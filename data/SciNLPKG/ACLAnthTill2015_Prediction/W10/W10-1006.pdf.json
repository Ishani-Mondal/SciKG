{"title": [{"text": "Rethinking Grammatical Error Annotation and Evaluation with the Amazon Mechanical Turk", "labels": [], "entities": [{"text": "Rethinking Grammatical Error Annotation", "start_pos": 0, "end_pos": 39, "type": "TASK", "confidence": 0.9421602189540863}, {"text": "Amazon Mechanical Turk", "start_pos": 64, "end_pos": 86, "type": "DATASET", "confidence": 0.713624914487203}]}], "abstractContent": [{"text": "In this paper we present results from two pilot studies which show that using the Amazon Mechanical Turk for preposition error annotation is as effective as using trained raters, but at a fraction of the time and cost.", "labels": [], "entities": [{"text": "Amazon Mechanical Turk", "start_pos": 82, "end_pos": 104, "type": "DATASET", "confidence": 0.9370176196098328}, {"text": "preposition error annotation", "start_pos": 109, "end_pos": 137, "type": "TASK", "confidence": 0.6978117426236471}]}, {"text": "Based on these results, we propose anew evaluation method which makes it feasible to compare two error detection systems tested on different learner data sets.", "labels": [], "entities": [{"text": "error detection", "start_pos": 97, "end_pos": 112, "type": "TASK", "confidence": 0.7164593786001205}]}], "introductionContent": [{"text": "The last few years have seen an explosion in the development of NLP tools to detect and correct errors made by learners of English as a Second Language (ESL).", "labels": [], "entities": [{"text": "correct errors made by learners of English as a Second Language (ESL)", "start_pos": 88, "end_pos": 157, "type": "TASK", "confidence": 0.6338243058749607}]}, {"text": "While there has been considerable emphasis placed on the system development aspect of the field, with researchers tackling some of the toughest ESL errors such as those involving articles) and prepositions (,, there has been a woeful lack of attention paid to developing best practices for annotation and evaluation.", "labels": [], "entities": []}, {"text": "Annotation in the field of ESL error detection has typically relied on just one trained rater, and that rater's judgments then become the gold standard for evaluating a system.", "labels": [], "entities": [{"text": "ESL error detection", "start_pos": 27, "end_pos": 46, "type": "TASK", "confidence": 0.9110988775889078}]}, {"text": "So it is very rare that inter-rater reliability is reported, although, in other NLP subfields, reporting reliability is the norm.", "labels": [], "entities": [{"text": "reliability", "start_pos": 36, "end_pos": 47, "type": "METRIC", "confidence": 0.8061522841453552}]}, {"text": "Time and cost are probably the two most important reasons why past work has relied on only one rater because using multiple annotators on the same ESL texts would obviously increase both considerably.", "labels": [], "entities": []}, {"text": "This is especially problematic for this field of research since some ESL errors, such as preposition usage, occur at error rates as low as 10%.", "labels": [], "entities": []}, {"text": "This means that to collect a corpus of 1,000 preposition errors, an annotator would have to check over 10,000 prepositions.", "labels": [], "entities": []}, {"text": "() challenged the view that using one rater is adequate by showing that preposition usage errors actually do not have high inter-annotator reliability.", "labels": [], "entities": []}, {"text": "For example, trained raters typically annotate preposition errors with a kappa around 0.60.", "labels": [], "entities": []}, {"text": "This low rater reliability has repercussions for system evaluation: Their experiments showed that system precision could vary as much as 10% depending on which rater's judgments they used as the gold standard.", "labels": [], "entities": [{"text": "system evaluation", "start_pos": 49, "end_pos": 66, "type": "TASK", "confidence": 0.8512943089008331}, {"text": "precision", "start_pos": 105, "end_pos": 114, "type": "METRIC", "confidence": 0.9729041457176208}]}, {"text": "For some grammatical errors such as subject-verb agreement, where rules are clearly defined, it maybe acceptable to use just one rater.", "labels": [], "entities": []}, {"text": "But for usage errors, the rules are less clearly defined and two native speakers can have very different judgments of what is acceptable.", "labels": [], "entities": []}, {"text": "One way to address this is by aggregating a multitude of judgments for each preposition and treating this as the gold standard, however such a tactic has been impractical due to time and cost limitations.", "labels": [], "entities": []}, {"text": "While annotation is a problem in this field, comparing one system to another has also been a major issue.", "labels": [], "entities": []}, {"text": "To date, none of the preposition and article error detection systems in the literature have been evaluated on the same corpus.", "labels": [], "entities": [{"text": "article error detection", "start_pos": 37, "end_pos": 60, "type": "TASK", "confidence": 0.5448716978232065}]}, {"text": "This is mostly due to the fact that learner corpora are difficult to acquire (and then annotate), but also to the fact that they are usually proprietary and cannot be shared.", "labels": [], "entities": []}, {"text": "Examples include the Cambridge Learners Corpus 2 used in, and TOEFL data, used in.", "labels": [], "entities": [{"text": "Cambridge Learners Corpus 2", "start_pos": 21, "end_pos": 48, "type": "DATASET", "confidence": 0.9684273153543472}, {"text": "TOEFL data", "start_pos": 62, "end_pos": 72, "type": "DATASET", "confidence": 0.8459149897098541}]}, {"text": "This makes it difficult to compare systems since learner corpora can be quite different.", "labels": [], "entities": []}, {"text": "For example, the \"difficulty\" of a corpus can be affected by the L1 of the writers, the number of years they have been learning English, their age, and also where they learn English (in a native-speaking country or a non-native speaking country).", "labels": [], "entities": [{"text": "L1", "start_pos": 65, "end_pos": 67, "type": "METRIC", "confidence": 0.9443745613098145}]}, {"text": "In essence, learner corpora are not equal, so a system that performs at 50% precision in one corpus may actually perform at 80% precision on a different one.", "labels": [], "entities": [{"text": "precision", "start_pos": 76, "end_pos": 85, "type": "METRIC", "confidence": 0.9949572682380676}, {"text": "precision", "start_pos": 128, "end_pos": 137, "type": "METRIC", "confidence": 0.9892725944519043}]}, {"text": "Such an inability to compare systems makes it difficult for this NLP research area to progress as quickly as it otherwise might.", "labels": [], "entities": []}, {"text": "In this paper we show that the Amazon Mechanical Turk (AMT), a fast and cheap source of untrained raters, can be used to alleviate several of the evaluation and annotation issues described above.", "labels": [], "entities": [{"text": "Amazon Mechanical Turk (AMT)", "start_pos": 31, "end_pos": 59, "type": "DATASET", "confidence": 0.8551965852578481}]}, {"text": "Specifically we show: \u2022 In terms of cost and time, AMT is an effective alternative to trained raters on the tasks of preposition selection in well-formed text and preposition error annotation in ESL text.", "labels": [], "entities": [{"text": "AMT", "start_pos": 51, "end_pos": 54, "type": "TASK", "confidence": 0.9074000716209412}, {"text": "preposition selection", "start_pos": 117, "end_pos": 138, "type": "TASK", "confidence": 0.7656304836273193}]}, {"text": "\u2022 With AMT, it is possible to efficiently collect multiple judgments fora target construction.", "labels": [], "entities": []}, {"text": "Given this, we propose anew method for evaluation that finally allows two systems to be compared to one another even if they are tested on different corpora.", "labels": [], "entities": []}], "datasetContent": [{"text": "We contend that the Amazon Mechanical Turk cannot only be used as an effective alternative annotation source, but can also be used to revamp evaluation since multiple judgments are now easily acquired.", "labels": [], "entities": [{"text": "Amazon Mechanical Turk", "start_pos": 20, "end_pos": 42, "type": "DATASET", "confidence": 0.9222528338432312}]}, {"text": "Instead of treating the task of error detection as a \"black or white\" distinction, where a preposition is either corrector incorrect, cases of preposition use can now be grouped into bins based on the level of agreement of the Turkers.", "labels": [], "entities": [{"text": "error detection", "start_pos": 32, "end_pos": 47, "type": "TASK", "confidence": 0.723188728094101}, {"text": "Turkers", "start_pos": 227, "end_pos": 234, "type": "DATASET", "confidence": 0.9470120072364807}]}, {"text": "For example, if 90% or more judge a preposition to bean error, 47 the high agreement is strong evidence that this is a clear case of an error.", "labels": [], "entities": [{"text": "agreement", "start_pos": 75, "end_pos": 84, "type": "METRIC", "confidence": 0.9351298213005066}]}, {"text": "Conversely, agreement levels around 50% would indicate that the use of a particular preposition is highly contentious, and, most likely, it should not be flagged by an automated error detection system.", "labels": [], "entities": [{"text": "agreement", "start_pos": 12, "end_pos": 21, "type": "METRIC", "confidence": 0.9622302651405334}]}, {"text": "The current standard method treats all cases of preposition usage equally, however, some are clearly harder to annotate than others.", "labels": [], "entities": []}, {"text": "By breaking an evaluation set into agreement bins, it should be possible to separate the \"easy\" cases from the \"hard\" cases and report precision and recall results for the different levels of human agreement represented by different bins.", "labels": [], "entities": [{"text": "precision", "start_pos": 135, "end_pos": 144, "type": "METRIC", "confidence": 0.998896598815918}, {"text": "recall", "start_pos": 149, "end_pos": 155, "type": "METRIC", "confidence": 0.9965842962265015}]}, {"text": "This method not only gives a clearer picture of how a system is faring, but it also ameliorates the problem of cross-system evaluation when two systems are evaluated on different corpora.", "labels": [], "entities": [{"text": "cross-system evaluation", "start_pos": 111, "end_pos": 134, "type": "TASK", "confidence": 0.7448035180568695}]}, {"text": "If each evaluation corpus is annotated by the same number of Turkers and with the same annotation scheme, it will now be possible to compare systems by simply comparing their performance on each respective bin.", "labels": [], "entities": []}, {"text": "The assumption here is that prepositions which show X% agreement in corpus A are of equivalent difficulty to those that show X% agreement in corpus B.", "labels": [], "entities": [{"text": "X% agreement", "start_pos": 52, "end_pos": 64, "type": "METRIC", "confidence": 0.8502668738365173}]}], "tableCaptions": [{"text": " Table 1: AMT Experiment Statistics", "labels": [], "entities": [{"text": "AMT Experiment", "start_pos": 10, "end_pos": 24, "type": "TASK", "confidence": 0.6729135513305664}]}, {"text": " Table 1. In the task of  preposition selection, only three Turkers are needed  to match the reliability of two trained raters; in the  more complicated task of error detection, up to 13  Turkers are needed. However, it should be noted  that these numbers can be viewed as upper bounds.  The error annotation scheme that was used is a very  simple one. We intend to experiment with different", "labels": [], "entities": [{"text": "preposition selection", "start_pos": 26, "end_pos": 47, "type": "TASK", "confidence": 0.9096608757972717}, {"text": "Turkers", "start_pos": 60, "end_pos": 67, "type": "METRIC", "confidence": 0.9558679461479187}, {"text": "error detection", "start_pos": 161, "end_pos": 176, "type": "TASK", "confidence": 0.6904934048652649}]}]}