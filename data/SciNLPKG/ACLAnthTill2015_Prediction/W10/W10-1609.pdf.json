{"title": [{"text": "A Machine Learning Approach for Recognizing Textual Entailment in Spanish", "labels": [], "entities": [{"text": "Recognizing Textual Entailment", "start_pos": 32, "end_pos": 62, "type": "TASK", "confidence": 0.8947940071423849}]}], "abstractContent": [{"text": "This paper presents a system that uses machine learning algorithms for the task of recognizing textual entailment in Spanish language.", "labels": [], "entities": [{"text": "recognizing textual entailment in Spanish language", "start_pos": 83, "end_pos": 133, "type": "TASK", "confidence": 0.8285918037096659}]}, {"text": "The datasets used include SPARTE Corpus and a translated version to Spanish of RTE3, RTE4 and RTE5 datasets.", "labels": [], "entities": [{"text": "SPARTE Corpus", "start_pos": 26, "end_pos": 39, "type": "DATASET", "confidence": 0.8266151547431946}, {"text": "RTE3", "start_pos": 79, "end_pos": 83, "type": "DATASET", "confidence": 0.9583854675292969}, {"text": "RTE4", "start_pos": 85, "end_pos": 89, "type": "DATASET", "confidence": 0.8806527256965637}, {"text": "RTE5 datasets", "start_pos": 94, "end_pos": 107, "type": "DATASET", "confidence": 0.9492389261722565}]}, {"text": "The features chosen quantify lexical, syntactic and semantic level matching between text and hypothesis sentences.", "labels": [], "entities": []}, {"text": "We analyze how the different sizes of datasets and classifiers could impact on the final overall performance of the RTE classification of two-way task in Spanish.", "labels": [], "entities": [{"text": "RTE classification", "start_pos": 116, "end_pos": 134, "type": "TASK", "confidence": 0.957112193107605}]}, {"text": "The RTE system yields 60.83% of accuracy and a competitive result of 66.50% of accuracy is reported by train and test set taken from SPARTE Corpus with 70% split.", "labels": [], "entities": [{"text": "RTE", "start_pos": 4, "end_pos": 7, "type": "TASK", "confidence": 0.4704509973526001}, {"text": "accuracy", "start_pos": 32, "end_pos": 40, "type": "METRIC", "confidence": 0.9996135830879211}, {"text": "accuracy", "start_pos": 79, "end_pos": 87, "type": "METRIC", "confidence": 0.9994670748710632}, {"text": "SPARTE Corpus", "start_pos": 133, "end_pos": 146, "type": "DATASET", "confidence": 0.9471205174922943}]}], "introductionContent": [{"text": "The objective of the Recognizing Textual Entailment Challenge is determining whether the meaning of the Hypothesis (H) can be inferred from a text (T)).", "labels": [], "entities": [{"text": "Recognizing Textual Entailment Challenge", "start_pos": 21, "end_pos": 61, "type": "TASK", "confidence": 0.8434676826000214}]}, {"text": "This challenge has been organized by NIST in recent years.", "labels": [], "entities": [{"text": "NIST", "start_pos": 37, "end_pos": 41, "type": "DATASET", "confidence": 0.9448695778846741}]}, {"text": "Another related antecedent was Answer Validation Exercise (AVE), part of Cross Language Evaluation Forum (CLEF), whose objective is to develop systems which are able to decide whether the answer to a question is corrector not ( ).", "labels": [], "entities": [{"text": "Answer Validation Exercise (AVE)", "start_pos": 31, "end_pos": 63, "type": "TASK", "confidence": 0.793932780623436}]}, {"text": "It was a three year-old track, from 2006 to 2008.", "labels": [], "entities": []}, {"text": "AVE challenge was an evaluation framework for Question Answering (QA) systems to promote the development and evaluation of subsystems aimed at validating the correctness of the answers given by a QA system.", "labels": [], "entities": [{"text": "Question Answering (QA)", "start_pos": 46, "end_pos": 69, "type": "TASK", "confidence": 0.8174540102481842}]}, {"text": "The Answer Validation task must select the best answer for the final output.", "labels": [], "entities": [{"text": "Answer Validation task", "start_pos": 4, "end_pos": 26, "type": "TASK", "confidence": 0.9056541124979655}]}, {"text": "There is a subtask for each language involved in QA, the Spanish is one of these.", "labels": [], "entities": [{"text": "QA", "start_pos": 49, "end_pos": 51, "type": "TASK", "confidence": 0.8989052176475525}]}, {"text": "Thus, AVE task is very similar to RTE (Recognition of Textual Entailments).", "labels": [], "entities": [{"text": "AVE", "start_pos": 6, "end_pos": 9, "type": "METRIC", "confidence": 0.9271348714828491}, {"text": "RTE", "start_pos": 34, "end_pos": 37, "type": "METRIC", "confidence": 0.8416454792022705}]}, {"text": "In this paper, we address the RTE task problem of determining the entailment value between Text and Hypothesis pairs in Spanish, applying machine learning techniques.", "labels": [], "entities": [{"text": "RTE task", "start_pos": 30, "end_pos": 38, "type": "TASK", "confidence": 0.9070612490177155}]}, {"text": "In the past, RTEs Challenges machine learning algorithms were widely used for the task of recognizing textual entailment and they have reported goods results for English language.", "labels": [], "entities": [{"text": "RTEs Challenges machine learning", "start_pos": 13, "end_pos": 45, "type": "TASK", "confidence": 0.8592865467071533}, {"text": "recognizing textual entailment", "start_pos": 90, "end_pos": 120, "type": "TASK", "confidence": 0.7944869796435038}]}, {"text": "Also, our system applies machine learning algorithms to the Spanish.", "labels": [], "entities": []}, {"text": "We built a set of datasets based on public available datasets for English, together to SPARTE (), an available Corpus in Spanish.", "labels": [], "entities": []}, {"text": "This corpus contains 2962 hypothesis with a document label and a True/False value indicating whether the document entails the hypothesis or not.", "labels": [], "entities": []}, {"text": "Up to our knowledge, SPARTE corpus in the only corpus aimed at evaluating RTE systems in Spanish.", "labels": [], "entities": [{"text": "SPARTE corpus", "start_pos": 21, "end_pos": 34, "type": "DATASET", "confidence": 0.7145181596279144}]}, {"text": "Finally, we generated a feature vector with the following components for both Text and Hypothesis: Levenshtein distance, a lexical distance based on Levenshtein, a semantic similarity measure Wordnet based, and the LCS (longest common substring) metric; in order to characterize the relationships between the Text and the Hypothesis.", "labels": [], "entities": []}, {"text": "The remainder of the paper is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 shows the system description, whereas Section 3 describes the results of experimental evaluation and discussion of them.", "labels": [], "entities": []}, {"text": "Section 4 discusses opportunities of collaboration.", "labels": [], "entities": []}, {"text": "Finally, Section 5 summarizes the conclusions and lines for future work.", "labels": [], "entities": []}], "datasetContent": [{"text": "With the aim of exploring the differences among training sets and machine learning algorithms, we did many experiments looking for the best result to our system.", "labels": [], "entities": []}, {"text": "First, we converted the RTE4 and RTE5 datasets with Contradiction/Unknown/Entailment pair information to a binary True/False problem, named two-way problem.", "labels": [], "entities": [{"text": "RTE4", "start_pos": 24, "end_pos": 28, "type": "DATASET", "confidence": 0.8964671492576599}, {"text": "RTE5 datasets", "start_pos": 33, "end_pos": 46, "type": "DATASET", "confidence": 0.838009387254715}]}, {"text": "Then, we used the following combination of datasets: RTE3-Sp, RTE4-Sp, RTE3-Sp+RTE4-Sp, SPARTE-Bal (balanced SPARTE Corpus with the same number of true and false cases), and SPARTE-Bal+ RTE3-Sp+RTE4-Sp.", "labels": [], "entities": [{"text": "RTE3-Sp", "start_pos": 53, "end_pos": 60, "type": "DATASET", "confidence": 0.8659494519233704}]}, {"text": "The training set SPARTE-Balanced was created by taking all true cases and randomly taking false cases, and then we build a balanced training set containing 1352 pairs, with 676 true and 676 false pairs.", "labels": [], "entities": []}, {"text": "We used four classifiers to learn every development set: (1) Support Vector Machine, (2) Ada Boost, (3) Multilayer Perceptron (MLP) and (4) Decision Tree using the open source WEKA Data Mining Software).", "labels": [], "entities": [{"text": "WEKA Data Mining Software", "start_pos": 176, "end_pos": 201, "type": "DATASET", "confidence": 0.8746110796928406}]}, {"text": "In all the tables results we show only the accuracy of the best classifier.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 43, "end_pos": 51, "type": "METRIC", "confidence": 0.9995248317718506}]}, {"text": "The results obtained to predict RTE5-Sp in a two-way classification task are summarized in shows our results reported in RTE two-way classification task by using with Cross Validation technique with 10 folds.", "labels": [], "entities": [{"text": "RTE5-Sp", "start_pos": 32, "end_pos": 39, "type": "TASK", "confidence": 0.46649274230003357}, {"text": "RTE two-way classification task", "start_pos": 121, "end_pos": 152, "type": "TASK", "confidence": 0.7918922305107117}]}, {"text": "Classifier Accuracy% RTE3-Sp+RTE4-Sp SVM 60.83% RTE3-Sp SVM 60.50% RTE4-Sp MLP 60.50% SPARTE-Bal+ RTE3-Sp+RTE4-Sp MLP 60.17% SPARTE-Bal DT 50% Baseline -50%  The performance in all cases was clearly above those baselines.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 11, "end_pos": 19, "type": "METRIC", "confidence": 0.9675691723823547}]}, {"text": "Only when using SPARTE-Bal we obtained a result equal to the baseline (50% true pairs and 50% false pairs).", "labels": [], "entities": [{"text": "SPARTE-Bal", "start_pos": 16, "end_pos": 26, "type": "DATASET", "confidence": 0.7787481546401978}]}, {"text": "The SPARTE-Balanced dataset yields the worst results, maybe because this dataset contains only pairs with QA task, and an additional reason, could be that SPARTE is syntactically simpler than PASCAL RTE.", "labels": [], "entities": [{"text": "SPARTE-Balanced dataset", "start_pos": 4, "end_pos": 27, "type": "DATASET", "confidence": 0.7229986935853958}]}, {"text": "In that sense, some authors have reported low performance when using syntactically simpler datasets; for instance, by using BPI 4 dataset to predict RTEs datasets in English.", "labels": [], "entities": [{"text": "BPI 4 dataset", "start_pos": 124, "end_pos": 137, "type": "DATASET", "confidence": 0.8572360078493754}]}, {"text": "Therefore, SPARTE seems to be not enough good training set to predict RTEs test sets.", "labels": [], "entities": [{"text": "SPARTE", "start_pos": 11, "end_pos": 17, "type": "METRIC", "confidence": 0.7002880573272705}, {"text": "RTEs", "start_pos": 70, "end_pos": 74, "type": "TASK", "confidence": 0.6976845264434814}]}, {"text": "The best performance of our system was achieved with SVM classifier with RTE3-Sp+RTE4-Sp dataset; it was 60.83% of accuracy.", "labels": [], "entities": [{"text": "RTE4-Sp dataset", "start_pos": 81, "end_pos": 96, "type": "DATASET", "confidence": 0.7755506634712219}, {"text": "accuracy", "start_pos": 115, "end_pos": 123, "type": "METRIC", "confidence": 0.9990490078926086}]}, {"text": "In the majority of the cases, SVM or MLP classifiers appear as 'favorite' in all classification tasks.", "labels": [], "entities": []}, {"text": "Surprisingly, in the two-way task, a slight and not statistical significant difference of 0.66% between the best and worst combination (except for SPARTE-Bal) of datasets and classifiers is found.", "labels": [], "entities": []}, {"text": "So, it suggests that the combination of dataset and classifiers do not produce a strong impact predicting RTE5-Sp, at least, for these feature sets.", "labels": [], "entities": [{"text": "RTE5-Sp", "start_pos": 106, "end_pos": 113, "type": "TASK", "confidence": 0.5805977582931519}]}, {"text": "Also, we observed that by including SPARTE-Bal to RTE3-Sp+RTE4-Sp dataset, the performance slightly decreases, although this difference was not statistical significant) with SPARTE-Bal and decision tree algorithm, are the best for cross-validation experiments.", "labels": [], "entities": [{"text": "RTE4-Sp dataset", "start_pos": 58, "end_pos": 73, "type": "DATASET", "confidence": 0.7481468915939331}]}, {"text": "In fact, an accuracy of 68.19% was obtained, which is 18.19% bigger than the result obtained in table 1, and was statistical significant.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 12, "end_pos": 20, "type": "METRIC", "confidence": 0.9997314810752869}]}, {"text": "Finally, we assessed our system only over the SPARTE Corpus.", "labels": [], "entities": [{"text": "SPARTE Corpus", "start_pos": 46, "end_pos": 59, "type": "DATASET", "confidence": 0.8481440544128418}]}, {"text": "First, we used cross validation technique with ten folds over SPARTE-Bal, testing over our four classifiers.", "labels": [], "entities": [{"text": "cross validation", "start_pos": 15, "end_pos": 31, "type": "TASK", "confidence": 0.693521112203598}, {"text": "SPARTE-Bal", "start_pos": 62, "end_pos": 72, "type": "DATASET", "confidence": 0.8163720369338989}]}, {"text": "Then, we tested SPARTE-Bal by splitting the corpus in training set (70%), and test set (30%).", "labels": [], "entities": []}, {"text": "The results are shown in the tables 4 and 5 below.", "labels": [], "entities": []}, {"text": "The results on cross-validation are better than those obtained on test set, which is most probably due to overfitting of classifiers.", "labels": [], "entities": []}, {"text": "shows a good performance of 66.50%, predicting test set and using Decision trees.", "labels": [], "entities": []}, {"text": "These results are opposed to the bad performance reported by SPARTE to predict RTEs datasets.", "labels": [], "entities": [{"text": "RTEs datasets", "start_pos": 79, "end_pos": 92, "type": "DATASET", "confidence": 0.713336318731308}]}, {"text": "Here, in fact, the syntactic complexity and original task do not change between train and test set; and it seems to be the main problem with the low performance of SPARTE in.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1.Results obtained in two-way classification task.", "labels": [], "entities": []}, {"text": " Table 2.Results obtained with Cross Validation 10 folds  in two-way task.", "labels": [], "entities": [{"text": "Cross Validation", "start_pos": 31, "end_pos": 47, "type": "TASK", "confidence": 0.6548473834991455}]}, {"text": " Table 4.Results obtained with Cross Validation 10 folds  in two-way task to predict SPARTE.", "labels": [], "entities": [{"text": "Cross Validation", "start_pos": 31, "end_pos": 47, "type": "TASK", "confidence": 0.6215454339981079}, {"text": "SPARTE", "start_pos": 85, "end_pos": 91, "type": "TASK", "confidence": 0.5204378366470337}]}, {"text": " Table 5.Results obtained with SPARTE with split 70%.", "labels": [], "entities": [{"text": "SPARTE", "start_pos": 31, "end_pos": 37, "type": "DATASET", "confidence": 0.6347194910049438}]}]}