{"title": [{"text": "An Enriched MT Grammar for Under $100", "labels": [], "entities": [{"text": "MT Grammar", "start_pos": 12, "end_pos": 22, "type": "DATASET", "confidence": 0.7800745666027069}]}], "abstractContent": [{"text": "We propose a framework for improving output quality of machine translation systems, by operating on the level of grammar rule features.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 55, "end_pos": 74, "type": "TASK", "confidence": 0.7372990250587463}]}, {"text": "Our framework aims to give a boost to grammar rules that appear in the derivations of translation candidates that are deemed to be of good quality, hence making those rules more preferable by the system.", "labels": [], "entities": []}, {"text": "To that end, we ask human annotators on Amazon Mechanical Turk to compare translation candidates, and then interpret their preferences of one candidate over another as an implicit preference for one derivation over another, and therefore as an implicit preference for one or more grammar rules.", "labels": [], "entities": [{"text": "Amazon Mechanical Turk", "start_pos": 40, "end_pos": 62, "type": "DATASET", "confidence": 0.9694737990697225}]}, {"text": "Our framework also allows us to generalize these preferences to grammar rules corresponding to a previously unseen test set, namely rules for which no candidates have been judged.", "labels": [], "entities": []}], "introductionContent": [{"text": "When translating between two languages, stateof-the-art statistical machine translation systems ( generate candidate translations by relying on a set of relevant grammar (or phrase table) entries.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 56, "end_pos": 87, "type": "TASK", "confidence": 0.6176900764306387}]}, {"text": "Each of those entries, or rules, associates a string in the source language with a string in the target language, with these associations typically learned by examining a large parallel bitext.", "labels": [], "entities": []}, {"text": "This score is the dot product: where \u03d5(e) is a feature vector characterizing e, and w is a system-specific weight vector characterizing the system's belief of how much the different features reflect translation quality.", "labels": [], "entities": []}, {"text": "The features of a candidate e are computed by examining the way e is constructed (or derived), and so if we let d(e) be the derivation of e, the feature vector can be denoted: \u03d5(d(e)) = \u03d5 1 (d(e)), . .", "labels": [], "entities": []}, {"text": ", \u03d5 m (d(e)) where \u03d5 i (d(e)) is the value of ith feature function of d(e) (with a corresponding weight w i in w).", "labels": [], "entities": []}, {"text": "To compute the score fora candidate, we examine its derivation d(e), enumerating the grammar rules used to construct e: d(e) = (r 1 , . .", "labels": [], "entities": []}, {"text": "Typically, each of the rules will itself have a vector of m features, and we calculate the value of a derivation feature \u03d5 i (d(e)) as the sum of the ith feature overall rules in the derivation: These features are usually either relative frequencies estimated from the training corpus, relating the rule's source and target sides, or features that characterize the structure of the rule itself, independently from the corpus.", "labels": [], "entities": []}, {"text": "Either way, the weight w i is chosen so as to reflect some belief regarding the correlation between the ith feature and translation quality.", "labels": [], "entities": []}, {"text": "This is usually done by choosing weights that maximize performance on a tuning set separate from the training bitext.", "labels": [], "entities": []}, {"text": "Unlike system weights, the grammar rule feature values are fixed once extracted, and are not modified during this tuning phase.", "labels": [], "entities": []}, {"text": "In this paper, we propose a framework to augment the feature set to incorporate additional intuition about how likely a rule is to produce a translation preferred by a human annotator.", "labels": [], "entities": []}, {"text": "This knowledge is acquired by directly asking human judges to compare candidate translations, therefore determining which subset of grammar rules annotators seem to prefer over others.", "labels": [], "entities": []}, {"text": "We also seek to generalize this intuition to rules for which no candidates were judged, hence allowing us to impact a much larger set of rules than just those used in translating the tuning set.", "labels": [], "entities": []}, {"text": "The paper is organized as follows.", "labels": [], "entities": []}, {"text": "We first give a general description of our framework.", "labels": [], "entities": []}, {"text": "We then discuss our data collection efforts on Amazon Mechanical Turk for an Urdu-English translation task, and make explicit the type of judgments we collect and how they can be used to augment grammar rules.", "labels": [], "entities": [{"text": "Amazon Mechanical Turk", "start_pos": 47, "end_pos": 69, "type": "DATASET", "confidence": 0.9398664832115173}, {"text": "Urdu-English translation task", "start_pos": 77, "end_pos": 106, "type": "TASK", "confidence": 0.717682828505834}]}, {"text": "Before concluding, we propose a framework for generalizing judgments to unseen grammar rules, and analyze the data collection process.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": []}