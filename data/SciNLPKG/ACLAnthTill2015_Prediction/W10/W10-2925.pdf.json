{"title": [{"text": "Distributed Asynchronous Online Learning for Natural Language Processing", "labels": [], "entities": [{"text": "Distributed Asynchronous Online Learning", "start_pos": 0, "end_pos": 40, "type": "TASK", "confidence": 0.8737540543079376}]}], "abstractContent": [{"text": "Recent speed-ups for training large-scale models like those found in statistical NLP exploit distributed computing (either on multicore or \"cloud\" architectures) and rapidly converging online learning algorithms.", "labels": [], "entities": []}, {"text": "Here we aim to combine the two.", "labels": [], "entities": []}, {"text": "We focus on distributed, \"mini-batch\" learners that make frequent updates asyn-chronously (Nedic et al., 2001; Langford et al., 2009).", "labels": [], "entities": []}, {"text": "We generalize existing asyn-chronous algorithms and experiment extensively with structured prediction problems from NLP, including discriminative, unsupervised, and non-convex learning scenarios.", "labels": [], "entities": []}, {"text": "Our results show asynchronous learning can provide substantial speed-ups compared to distributed and single-processor mini-batch algorithms with no signs of error arising from the approximate nature of the technique.", "labels": [], "entities": []}], "introductionContent": [{"text": "Modern statistical NLP models are notoriously expensive to train, requiring the use of generalpurpose or specialized numerical optimization algorithms (e.g., gradient and coordinate ascent algorithms and variations on them like L-BFGS and EM) that iterate over training data many times.", "labels": [], "entities": []}, {"text": "Two developments have led to major improvements in training time for NLP models: \u2022 online learning algorithms (, which update the parameters of a model more frequently, processing only one or a small number of training examples, called a \"minibatch,\" between updates; and \u2022 distributed computing, which divides training data among multiple CPUs for faster processing between updates (e.g.,).", "labels": [], "entities": []}, {"text": "Online algorithms offer fast convergence rates and scalability to large datasets, but distributed computing is a more natural fit for algorithms that require a lot of computation-e.g., processing a large batch of training examples-to be done between updates.", "labels": [], "entities": []}, {"text": "Typically, distributed online learning has been done in asynchronous setting, meaning that a mini-batch of data is divided among multiple CPUs, and the model is updated when they have all completed processing (.", "labels": [], "entities": []}, {"text": "Each mini-batch is processed only after the previous one has completed.", "labels": [], "entities": []}, {"text": "Synchronous frameworks are appealing in that they simulate the same algorithms that work on a single processor, but they have the drawback that the benefits of parallelism are only obtainable within one mini-batch iteration.", "labels": [], "entities": []}, {"text": "Moreover, empirical evaluations suggest that online methods only converge faster than batch algorithms when using very small mini-batches (.", "labels": [], "entities": []}, {"text": "In this case, synchronous parallelization will not offer much benefit.", "labels": [], "entities": []}, {"text": "In this paper, we focus our attention on asynchronous algorithms that generalize those presented by and.", "labels": [], "entities": []}, {"text": "In these algorithms, multiple mini-batches are processed simultaneously, each using potentially different and typically stale parameters.", "labels": [], "entities": []}, {"text": "The key advantage of an asynchronous framework is that it allows processors to remain in near-constant use, preventing them from wasting cycles waiting for other processors to complete their portion of the current mini-batch.", "labels": [], "entities": []}, {"text": "In this way, asynchronous algorithms allow more frequent parameter updates, which speeds convergence.", "labels": [], "entities": []}, {"text": "Our contributions are as follows: \u2022 We describe a framework for distributed asynchronous optimization ( \u00a75) similar to those described by and, but permitting mini-batch learning.", "labels": [], "entities": [{"text": "distributed asynchronous optimization", "start_pos": 64, "end_pos": 101, "type": "TASK", "confidence": 0.6447702348232269}]}, {"text": "The prior work contains convergence results for asynchronous online stochastic gradient descent for convex functions (discussed in brief in \u00a75.2).", "labels": [], "entities": [{"text": "stochastic gradient descent", "start_pos": 68, "end_pos": 95, "type": "TASK", "confidence": 0.6310672163963318}]}, {"text": "\u2022 We report experiments on three structured NLP tasks, including one problem that matches the conditions for convergence (named entity recognition; NER) and two that depart from theoretical foundations, namely the use of asynchronous stepwise EM for both convex and non-convex optimization.", "labels": [], "entities": [{"text": "entity recognition; NER", "start_pos": 128, "end_pos": 151, "type": "TASK", "confidence": 0.7762735709547997}]}, {"text": "\u2022 We directly compare asynchronous algorithms with multiprocessor synchronous mini-batch algorithms (e.g.,) and traditional batch algorithms.", "labels": [], "entities": []}, {"text": "\u2022 We experiment with adding artificial delays to simulate the effects of network or hardware traffic that could cause updates to be made with extremely stale parameters.", "labels": [], "entities": []}, {"text": "\u2022 Our experimental settings include both individual 4-processor machines as well as large clusters of commodity machines implementing the MapReduce programming model).", "labels": [], "entities": []}, {"text": "We also explore effects of mini-batch size.", "labels": [], "entities": []}, {"text": "Our main conclusion is that, when small minibatches work well, asynchronous algorithms offer substantial speed-ups without introducing error.", "labels": [], "entities": []}, {"text": "When large mini-batches work best, asynchronous learning does not hurt.", "labels": [], "entities": []}], "datasetContent": [{"text": "We performed experiments to measure speed-ups obtainable through distributed online optimization.", "labels": [], "entities": []}, {"text": "Since we will be considering different optimization algorithms and computing environments, we will primarily be interested in the wall-clock time required to obtain particular levels of performance on metrics appropriate to each task.", "labels": [], "entities": []}, {"text": "We consider three tasks, detailed in.", "labels": [], "entities": []}, {"text": "For experiments on a single node, we used a 64-bit machine with two 2.6GHz dual-core CPUs (i.e., 4 processors in all) with a total of 8GB of RAM.", "labels": [], "entities": []}, {"text": "This was a dedicated machine that was not available for any other jobs.", "labels": [], "entities": []}, {"text": "We also conducted experiments using a cluster architecture running Hadoop 0.20 (an implementation of MapReduce), consisting of 400 machines, each having 2 quadcore 1.86GHz CPUs with a total of 6GB of RAM.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Our experiments consider three tasks.", "labels": [], "entities": []}, {"text": " Table 2: Alignment error rates and wall time after 20 itera- tions of EM for various settings. See text for details.", "labels": [], "entities": [{"text": "Alignment error rates", "start_pos": 10, "end_pos": 31, "type": "METRIC", "confidence": 0.8585637807846069}]}]}