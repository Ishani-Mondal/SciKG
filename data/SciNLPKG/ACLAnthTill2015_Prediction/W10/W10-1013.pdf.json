{"title": [{"text": "Off-topic essay detection using short prompt texts", "labels": [], "entities": [{"text": "Off-topic essay detection", "start_pos": 0, "end_pos": 25, "type": "TASK", "confidence": 0.6921550035476685}]}], "abstractContent": [{"text": "Our work addresses the problem of predicting whether an essay is off-topic to a given prompt or question without any previously-seen essays as training data.", "labels": [], "entities": [{"text": "predicting whether an essay is off-topic to a given prompt or question", "start_pos": 34, "end_pos": 104, "type": "TASK", "confidence": 0.6952014267444611}]}, {"text": "Prior work has used similarity between essay vocabulary and prompt words to estimate the degree of on-topic content.", "labels": [], "entities": []}, {"text": "In our corpus of opinion essays , prompts are very short, and using similarity with such prompts to detect off-topic essays yields error rates of about 10%.", "labels": [], "entities": [{"text": "error rates", "start_pos": 131, "end_pos": 142, "type": "METRIC", "confidence": 0.9835614860057831}]}, {"text": "We propose two methods to enable better comparison of prompt and essay text.", "labels": [], "entities": []}, {"text": "We automatically expand short prompts before comparison , with words likely to appear in an essay to that prompt.", "labels": [], "entities": []}, {"text": "We also apply spelling correction to the essay texts.", "labels": [], "entities": [{"text": "spelling correction", "start_pos": 14, "end_pos": 33, "type": "TASK", "confidence": 0.585035115480423}]}, {"text": "Both methods reduce the error rates during off-topic essay detection and turnout to be complementary, leading to even better performance when used in unison.", "labels": [], "entities": [{"text": "essay detection", "start_pos": 53, "end_pos": 68, "type": "TASK", "confidence": 0.6707881689071655}, {"text": "turnout", "start_pos": 73, "end_pos": 80, "type": "METRIC", "confidence": 0.9888994097709656}]}], "introductionContent": [{"text": "It is important to limit the opportunity to submit uncooperative responses to educational software ().", "labels": [], "entities": []}, {"text": "We address the task of detecting essays that are irrelevant to a given prompt (essay question) when training data is not available and the prompt text is very short.", "labels": [], "entities": []}, {"text": "When example essays fora prompt are available, they can be used to learn word patterns to distinguish on-topic from off-topic essays.", "labels": [], "entities": []}, {"text": "Alternatively, prior work () has motivated using similarity between essay and prompt vocabularies to detect off-topic essays.", "labels": [], "entities": []}, {"text": "In Section 2, we examine the performance of prompt-essay comparison for four different essay types.", "labels": [], "entities": [{"text": "prompt-essay comparison", "start_pos": 44, "end_pos": 67, "type": "TASK", "confidence": 0.8154723942279816}]}, {"text": "We show that in the case of prompts with 9 or 13 content words on average, the error rates are higher compared to those with 60 or more content words.", "labels": [], "entities": [{"text": "error rates", "start_pos": 79, "end_pos": 90, "type": "METRIC", "confidence": 0.9846085906028748}]}, {"text": "In addition, more errors are observed when the method is used on essays written by English language learners compared to more advanced test takers.", "labels": [], "entities": []}, {"text": "An example short prompt from our opinion essays' corpus is shown below.", "labels": [], "entities": []}, {"text": "Testtakers provided arguments for/or against the opinion expressed by the prompt.", "labels": [], "entities": []}, {"text": "[1] \"In the past, people were more friendly than they are today.\"", "labels": [], "entities": []}, {"text": "To address this problem, we propose two enhancements.", "labels": [], "entities": []}, {"text": "We use unsupervised methods to expand the prompt text with words likely to appear in essays to that prompt.", "labels": [], "entities": []}, {"text": "Our approach is based on the intuition that regularities exist in the words which appear in essays, beyond the prevalence of actual prompt words.", "labels": [], "entities": []}, {"text": "Ina similar vein, misspellings in the essays, particulary of the prompt words, are also problematic for prompt-based methods.", "labels": [], "entities": []}, {"text": "Therefore we apply spelling correction to the essay text before comparison.", "labels": [], "entities": [{"text": "spelling correction", "start_pos": 19, "end_pos": 38, "type": "METRIC", "confidence": 0.8286437690258026}]}, {"text": "Our results show that both methods lower the error rates.", "labels": [], "entities": [{"text": "error rates", "start_pos": 45, "end_pos": 56, "type": "METRIC", "confidence": 0.9776490330696106}]}, {"text": "The relative performance of the two methods varies depending on the essay type; however, their combination gives the overall best results regardless of essay type.", "labels": [], "entities": []}], "datasetContent": [{"text": "We use the approach for off-topic essay detection suggested in prior work by.", "labels": [], "entities": [{"text": "off-topic essay detection", "start_pos": 24, "end_pos": 49, "type": "TASK", "confidence": 0.6192474861939748}]}, {"text": "The method uses cosine overlap between tf*idf vectors of prompt and essay content words to measure the similarity between a prompt-essay pair.", "labels": [], "entities": []}, {"text": "An essay is compared with the target prompt (prompt with which topicality must be checked) together with a set of reference prompts, different from the target.", "labels": [], "entities": []}, {"text": "The reference prompts are also chosen to be different from the actual prompts of the negative examples in our dataset.", "labels": [], "entities": []}, {"text": "If the target prompt is ranked as most similar 2 in the list of compared prompts, the essay is classified as on-topic.", "labels": [], "entities": []}, {"text": "9 reference prompts were used in our experiments.", "labels": [], "entities": []}, {"text": "We compute two error rates.", "labels": [], "entities": []}, {"text": "FALSE POSITIVE -percentage of on-topic essays incorrectly flagged as off-topic.", "labels": [], "entities": [{"text": "FALSE POSITIVE", "start_pos": 0, "end_pos": 14, "type": "METRIC", "confidence": 0.7520120441913605}]}, {"text": "FALSE NEGATIVE -percentage of off-topic essays which the system failed to flag.", "labels": [], "entities": [{"text": "FALSE NEGATIVE", "start_pos": 0, "end_pos": 14, "type": "METRIC", "confidence": 0.7427060604095459}]}, {"text": "In this task, it is of utmost importance to maintain very low false positive rates, as incorrect labeling of an on-topic essay as off-topic is undesirable.", "labels": [], "entities": [{"text": "false positive rates", "start_pos": 62, "end_pos": 82, "type": "METRIC", "confidence": 0.7449754675229391}]}], "tableCaptions": [{"text": " Table 1: Effect of essay types: average prompt length,  false positive and false negative rates", "labels": [], "entities": [{"text": "average prompt length", "start_pos": 33, "end_pos": 54, "type": "METRIC", "confidence": 0.7547463973363241}]}, {"text": " Table 2: Average error rates after prompt expansion and  spelling correction", "labels": [], "entities": [{"text": "Average error rates", "start_pos": 10, "end_pos": 29, "type": "METRIC", "confidence": 0.8179487188657125}, {"text": "prompt expansion", "start_pos": 36, "end_pos": 52, "type": "TASK", "confidence": 0.8509689569473267}, {"text": "spelling correction", "start_pos": 58, "end_pos": 77, "type": "TASK", "confidence": 0.6969949901103973}]}]}