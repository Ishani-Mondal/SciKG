{"title": [], "abstractContent": [{"text": "We build a model for speech disfluency detection based on conditional random fields (CRFs) using the Switchboard corpus.", "labels": [], "entities": [{"text": "speech disfluency detection", "start_pos": 21, "end_pos": 48, "type": "TASK", "confidence": 0.8193056782086691}, {"text": "Switchboard corpus", "start_pos": 101, "end_pos": 119, "type": "DATASET", "confidence": 0.9290185272693634}]}, {"text": "This model is then applied to anew domain without any adaptation.", "labels": [], "entities": []}, {"text": "We show that a technique for detecting speech disfluencies based on Integer Linear Programming (ILP) (Georgila, 2009) significantly outperforms CRFs.", "labels": [], "entities": [{"text": "detecting speech disfluencies", "start_pos": 29, "end_pos": 58, "type": "TASK", "confidence": 0.7853367328643799}]}, {"text": "In particular, in terms of F-score and NIST Error Rate the absolute improvement of ILP over CRFs exceeds 20% and 25% respectively.", "labels": [], "entities": [{"text": "F-score", "start_pos": 27, "end_pos": 34, "type": "METRIC", "confidence": 0.9986370205879211}, {"text": "NIST", "start_pos": 39, "end_pos": 43, "type": "DATASET", "confidence": 0.5423455238342285}, {"text": "Error Rate", "start_pos": 44, "end_pos": 54, "type": "METRIC", "confidence": 0.8676024377346039}, {"text": "ILP", "start_pos": 83, "end_pos": 86, "type": "METRIC", "confidence": 0.877517580986023}]}, {"text": "We conclude that ILP is an approach with great potential for speech disfluency detection when there is alack or shortage of in-domain data for training.", "labels": [], "entities": [{"text": "speech disfluency detection", "start_pos": 61, "end_pos": 88, "type": "TASK", "confidence": 0.8448230028152466}]}], "introductionContent": [{"text": "Speech disfluencies (also known as speech repairs) occur frequently in spontaneous speech and can pose difficulties to natural language processing (NLP) since most NLP tools (e.g. parsers and part-of-speech taggers) are traditionally trained on written language.", "labels": [], "entities": [{"text": "Speech disfluencies (also known as speech repairs)", "start_pos": 0, "end_pos": 50, "type": "TASK", "confidence": 0.6171748903062608}, {"text": "natural language processing (NLP)", "start_pos": 119, "end_pos": 152, "type": "TASK", "confidence": 0.7084534615278244}, {"text": "part-of-speech taggers", "start_pos": 192, "end_pos": 214, "type": "TASK", "confidence": 0.698249563574791}]}, {"text": "However, speech disfluencies are not noise.", "labels": [], "entities": [{"text": "speech disfluencies", "start_pos": 9, "end_pos": 28, "type": "TASK", "confidence": 0.7157151997089386}]}, {"text": "They are an integral part of how humans speak, may provide valuable information about the speaker's cognitive state, and can be critical for successful turn-taking.", "labels": [], "entities": []}, {"text": "Speech disfluencies have been the subject of much research in the field of spoken language processing, e.g. (. Speech disfluencies can be divided into three intervals, the reparandum, the editing term, and the correction ().", "labels": [], "entities": [{"text": "spoken language processing", "start_pos": 75, "end_pos": 101, "type": "TASK", "confidence": 0.6870952049891154}, {"text": "correction", "start_pos": 210, "end_pos": 220, "type": "METRIC", "confidence": 0.9926234483718872}]}, {"text": "In the example below, \"it left\" is the reparandum (the part that will be repaired), \"I mean\" is the editing term, and \"it came\" is the correction: (it left) * (I mean) it came The asterisk marks the interruption point at which the speaker halts the original utterance in order to start the repair.", "labels": [], "entities": [{"text": "correction", "start_pos": 135, "end_pos": 145, "type": "METRIC", "confidence": 0.9645092487335205}]}, {"text": "The editing term is optional and consists of one or more filled pauses (e.g. uh, um) or discourse markers (e.g. you know, well).", "labels": [], "entities": []}, {"text": "Our goal here is to automatically detect repetitions (the speaker repeats some part of the utterance), revisions (the speaker modifies the original utterance), or restarts (the speaker abandons an utterance and starts over).", "labels": [], "entities": []}, {"text": "We also deal with complex disfluencies, i.e. a series of disfluencies in succession (\"it it was it is sounds great\").", "labels": [], "entities": []}, {"text": "In previous work many different approaches to detecting speech disfluencies have been proposed.", "labels": [], "entities": [{"text": "detecting speech disfluencies", "start_pos": 46, "end_pos": 75, "type": "TASK", "confidence": 0.8625624378522238}]}, {"text": "Different types of features have been used, e.g. lexical features only, acoustic and prosodic features only, or a combination of both ().", "labels": [], "entities": []}, {"text": "Furthermore, a number of studies have been conducted on human transcriptions while other efforts have focused on detecting disfluencies from the speech recognition output.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 145, "end_pos": 163, "type": "TASK", "confidence": 0.7445407509803772}]}, {"text": "In our previous work, we proposed a novel two-stage technique for speech disfluency detection based on Integer Linear Programming (ILP).", "labels": [], "entities": [{"text": "speech disfluency detection", "start_pos": 66, "end_pos": 93, "type": "TASK", "confidence": 0.8225489854812622}]}, {"text": "ILP has been applied successfully to several NLP problems, e.g..", "labels": [], "entities": []}, {"text": "In the first stage of our method, we trained state-of-the-art classifiers for speech disfluency detection, in particular, HiddenEvent Language Models (HELMs), Maximum Entropy (ME) models, and Conditional Random Fields (CRFs) ().", "labels": [], "entities": [{"text": "speech disfluency detection", "start_pos": 78, "end_pos": 105, "type": "TASK", "confidence": 0.7634133199850718}]}, {"text": "Then in the second stage and during testing, each classifier proposed possible labels which were then assessed in the presence of local and global constraints using ILP.", "labels": [], "entities": []}, {"text": "These constraints are hand-crafted and encode common disfluency patterns.", "labels": [], "entities": []}, {"text": "ILP makes the final decision taking into account both the output of the classifier and the constraints.", "labels": [], "entities": [{"text": "ILP", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.8712171912193298}]}, {"text": "Our approach is similar to the work of) in the sense that they also combine machine learning with hand-crafted rules.", "labels": [], "entities": []}, {"text": "However, we use different machine learning techniques and ILP.", "labels": [], "entities": []}, {"text": "When we evaluated this approach on the Switchboard corpus (available from LDC and manually annotated with disfluencies) using lexical features, we found that ILP significantly improves the performance of HELMs and ME models with negligible cost in processing time.", "labels": [], "entities": [{"text": "Switchboard corpus", "start_pos": 39, "end_pos": 57, "type": "DATASET", "confidence": 0.8562950193881989}]}, {"text": "However, the improvement of ILP over CRFs was only marginal.", "labels": [], "entities": [{"text": "ILP", "start_pos": 28, "end_pos": 31, "type": "METRIC", "confidence": 0.9611409902572632}]}, {"text": "These results were achieved when each classifier was trained on approx. 35,000 occurrences of disfluencies.", "labels": [], "entities": []}, {"text": "Then we experimented with varying training set sizes in Switchboard.", "labels": [], "entities": [{"text": "Switchboard", "start_pos": 56, "end_pos": 67, "type": "DATASET", "confidence": 0.8563045859336853}]}, {"text": "As soon as we started reducing the amount of data for training the classifiers, the improvement of ILP over CRFs rose and became very significant, approx. 4% absolute reduction of error rate with 25% of the training set (approx. 9,000 occurrences of disfluencies).", "labels": [], "entities": [{"text": "ILP", "start_pos": 99, "end_pos": 102, "type": "METRIC", "confidence": 0.8044596910476685}, {"text": "absolute reduction of error rate", "start_pos": 158, "end_pos": 190, "type": "METRIC", "confidence": 0.8414617657661438}]}, {"text": "This result showed that ILP is particularly helpful when there is no much training data available.", "labels": [], "entities": []}, {"text": "However, Switchboard is a unique corpus because the amount of disfluencies that it contains is very large.", "labels": [], "entities": []}, {"text": "Thus even 25% of our training set contains more disfluencies than atypical corpus of human-human or human-machine interactions.", "labels": [], "entities": []}, {"text": "In this paper, we investigate what happens when we move to anew domain when there is no indomain data annotated with disfluencies to be used for training.", "labels": [], "entities": []}, {"text": "This is usually the case when we start developing a dialogue system in anew domain, when the system has not been fully implemented yet, and thus no data from users interacting with the system has been collected.", "labels": [], "entities": []}, {"text": "Since the improvement of ILP over HELMs and ME models was very large even when the models were both trained and tested on Switchboard (approx. 15% and 20% absolute reduction of error rate when 100% and 25% of the training set was used for training the classifiers respectively), in this paper we focus only on comparing CRFs versus CRFs+ILP.", "labels": [], "entities": [{"text": "Switchboard", "start_pos": 122, "end_pos": 133, "type": "DATASET", "confidence": 0.9055948257446289}, {"text": "absolute reduction of error rate", "start_pos": 155, "end_pos": 187, "type": "METRIC", "confidence": 0.8899291157722473}]}, {"text": "Our goal is to evaluate if and how much ILP improves CRFs in the case that no training data is available at all.", "labels": [], "entities": [{"text": "CRFs", "start_pos": 53, "end_pos": 57, "type": "TASK", "confidence": 0.9012745022773743}]}, {"text": "The structure of the paper is as follows: In section 2 we describe our data sets.", "labels": [], "entities": []}, {"text": "In section 3 we concisely describe our approach.", "labels": [], "entities": []}, {"text": "Then in section 4 we present our experiments.", "labels": [], "entities": []}, {"text": "Finally in section 5 we present our conclusion.", "labels": [], "entities": []}], "datasetContent": [{"text": "For building the CRF model we use the CRF++ toolkit (available from sourceforge).", "labels": [], "entities": []}, {"text": "We used only lexical features, i.e. words and part-ofspeech (POS) tags.", "labels": [], "entities": []}, {"text": "Switchboard includes POS information but to annotate the Rapport corpus with POS labels we used the Stanford POS tagger).", "labels": [], "entities": [{"text": "Rapport corpus", "start_pos": 57, "end_pos": 71, "type": "DATASET", "confidence": 0.9514820277690887}]}, {"text": "We experimented with different sets of features and we achieved the best results with the following setup (i is the location of the word or POS in the sentence): Our word features are w i , w i+1 , w i\u22121 , w i , w i , w i+1 , w i\u22122 , w i\u22121 , w i , w i , w i+1 , w i+2 . Our POS features have the same structure as the word features.", "labels": [], "entities": []}, {"text": "For ILP we use the lp solve software also available from sourceforge.", "labels": [], "entities": []}, {"text": "We train on Switchboard and test on the Rapport corpus.", "labels": [], "entities": [{"text": "Switchboard", "start_pos": 12, "end_pos": 23, "type": "DATASET", "confidence": 0.8881996870040894}, {"text": "Rapport corpus", "start_pos": 40, "end_pos": 54, "type": "DATASET", "confidence": 0.9598974585533142}]}, {"text": "For evaluating the performance of our models we use standard metrics proposed in the literature, i.e. Precision, Recall, F-score, and NIST Error Rate.", "labels": [], "entities": [{"text": "Precision", "start_pos": 102, "end_pos": 111, "type": "METRIC", "confidence": 0.9994353652000427}, {"text": "Recall", "start_pos": 113, "end_pos": 119, "type": "METRIC", "confidence": 0.9961919784545898}, {"text": "F-score", "start_pos": 121, "end_pos": 128, "type": "METRIC", "confidence": 0.9984245300292969}, {"text": "NIST Error Rate", "start_pos": 134, "end_pos": 149, "type": "METRIC", "confidence": 0.7417729695638021}]}, {"text": "We report results for BE and IP.", "labels": [], "entities": [{"text": "BE", "start_pos": 22, "end_pos": 24, "type": "METRIC", "confidence": 0.7517659068107605}, {"text": "IP", "start_pos": 29, "end_pos": 31, "type": "DATASET", "confidence": 0.7378898859024048}]}, {"text": "F-score is the harmonic mean of Precision and Recall (we equally weight Precision and Recall).", "labels": [], "entities": [{"text": "F-score", "start_pos": 0, "end_pos": 7, "type": "METRIC", "confidence": 0.9846011400222778}, {"text": "Precision", "start_pos": 32, "end_pos": 41, "type": "METRIC", "confidence": 0.79100501537323}, {"text": "Recall", "start_pos": 46, "end_pos": 52, "type": "METRIC", "confidence": 0.9339020252227783}, {"text": "Recall", "start_pos": 86, "end_pos": 92, "type": "METRIC", "confidence": 0.9565704464912415}]}, {"text": "Precision is the ratio of the correctly identified tags X to all the tags X detected by the model (where X is BE or IP).", "labels": [], "entities": [{"text": "Precision", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.9923394918441772}, {"text": "BE", "start_pos": 110, "end_pos": 112, "type": "METRIC", "confidence": 0.9865241050720215}]}, {"text": "Recall is the ratio of the correctly identified tags X to all the tags X that appear in the reference: Comparative results between our models. utterance.", "labels": [], "entities": []}, {"text": "The NIST Error Rate is the sum of insertions, deletions and substitutions divided by the total number of reference tags ().", "labels": [], "entities": [{"text": "NIST Error Rate", "start_pos": 4, "end_pos": 19, "type": "METRIC", "confidence": 0.7544368306795756}]}, {"text": "presents comparative results between our models.", "labels": [], "entities": []}, {"text": "As we can see, now the improvement of ILP over CRFs is not marginal as in Switchboard.", "labels": [], "entities": [{"text": "Switchboard", "start_pos": 74, "end_pos": 85, "type": "DATASET", "confidence": 0.815034031867981}]}, {"text": "In fact, in terms of F-score and NIST Error Rate the absolute improvement of ILP over CRFs exceeds 20% and 25% respectively.", "labels": [], "entities": [{"text": "F-score", "start_pos": 21, "end_pos": 28, "type": "METRIC", "confidence": 0.9988183379173279}, {"text": "NIST", "start_pos": 33, "end_pos": 37, "type": "DATASET", "confidence": 0.5588725805282593}, {"text": "Error Rate", "start_pos": 38, "end_pos": 48, "type": "METRIC", "confidence": 0.8777337968349457}, {"text": "ILP", "start_pos": 77, "end_pos": 80, "type": "METRIC", "confidence": 0.937659740447998}]}, {"text": "The results are statistically significant (p<10 \u22128 , Wilcoxon signed-rank test).", "labels": [], "entities": []}, {"text": "The main gain of ILP comes from the large improvement in Recall.", "labels": [], "entities": [{"text": "ILP", "start_pos": 17, "end_pos": 20, "type": "DATASET", "confidence": 0.5306831002235413}, {"text": "Recall", "start_pos": 57, "end_pos": 63, "type": "DATASET", "confidence": 0.565593421459198}]}, {"text": "This result shows that using ILP has great potential for speech disfluency detection when there is alack of in-domain data for training, and when we use lexical features and human transcriptions.", "labels": [], "entities": [{"text": "speech disfluency detection", "start_pos": 57, "end_pos": 84, "type": "TASK", "confidence": 0.8175056974093119}]}, {"text": "Furthermore, the cost of applying ILP is negligible since the process is fast and applied during testing.", "labels": [], "entities": []}, {"text": "Note that the improvement of ILP over CRFs is significant even though the two corpora, Switchboard and Rapport, differ in genre (conversation versus narrative).", "labels": [], "entities": []}, {"text": "The reason for the large improvement of ILP over CRFs is the fact that as explained above ILP takes into account common disfluency patterns and generalizes from them.", "labels": [], "entities": []}, {"text": "CRFs can potentially learn similar patterns from the data but do not generalize that well.", "labels": [], "entities": []}, {"text": "For example, if the CRF model learns that \"she she\" is a repetition it will not necessarily infer that any sequence of the same two words is a repetition (e.g. \"and and\").", "labels": [], "entities": []}, {"text": "Of course here, since we deal with human transcriptions we do not worry about speech recognition errors.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 78, "end_pos": 96, "type": "TASK", "confidence": 0.7433938980102539}]}, {"text": "Preliminary results with speech recognition output showed that ILP retains its advantages but more modestly.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 25, "end_pos": 43, "type": "TASK", "confidence": 0.8047566115856171}]}, {"text": "In this case, when deciding which boosting rules to apply, it makes sense to consider speech recognition confidence scores per word.", "labels": [], "entities": []}, {"text": "For example, a possible repetition \"to to\" could be the result of a misrecognition of \"to do\".", "labels": [], "entities": []}, {"text": "But these types of problems also affect plain CRFs, so in the end ILP is expected to continue outperforming CRFs.", "labels": [], "entities": []}, {"text": "This is one of the issues for future work together with using prosodic features.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Comparative results between our models.", "labels": [], "entities": []}]}