{"title": [{"text": "Context Adaptation in Statistical Machine Translation Using Models with Exponentially Decaying Cache", "labels": [], "entities": [{"text": "Context Adaptation", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.9060218334197998}, {"text": "Statistical Machine Translation", "start_pos": 22, "end_pos": 53, "type": "TASK", "confidence": 0.7589022119839987}]}], "abstractContent": [{"text": "We report results from a domain adaptation task for statistical machine translation (SMT) using cache-based adaptive language and translation models.", "labels": [], "entities": [{"text": "statistical machine translation (SMT)", "start_pos": 52, "end_pos": 89, "type": "TASK", "confidence": 0.798272579908371}]}, {"text": "We apply an exponential decay factor and integrate the cache models in a standard phrase-based SMT decoder.", "labels": [], "entities": [{"text": "SMT decoder", "start_pos": 95, "end_pos": 106, "type": "TASK", "confidence": 0.8708500266075134}]}, {"text": "Without the need for any domain-specific resources we obtain a 2.6% relative improvement on average in BLEU scores using our dynamic adaptation procedure.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 103, "end_pos": 107, "type": "METRIC", "confidence": 0.9984007477760315}]}], "introductionContent": [{"text": "Most data-driven approaches to natural language processing (NLP) are subject to the wellknown problem of lack of portability to new domains/genres.", "labels": [], "entities": [{"text": "natural language processing (NLP)", "start_pos": 31, "end_pos": 64, "type": "TASK", "confidence": 0.8355894883473715}]}, {"text": "Usually there is a substantial drop in performance when testing on data from a domain different to the training data.", "labels": [], "entities": []}, {"text": "Statistical machine translation is no exception.", "labels": [], "entities": [{"text": "Statistical machine translation", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.6722423732280731}]}, {"text": "Despite its popularity, standard SMT approaches fail to provide a framework for general application across domains unless appropriate training data is available and used in parameter estimation and tuning.", "labels": [], "entities": [{"text": "SMT", "start_pos": 33, "end_pos": 36, "type": "TASK", "confidence": 0.9929662942886353}, {"text": "parameter estimation", "start_pos": 173, "end_pos": 193, "type": "TASK", "confidence": 0.6638046950101852}]}, {"text": "The main problem is the general assumption of independent and identically distributed (i.i.d.) variables in machine learning approaches applied in the estimation of static global models.", "labels": [], "entities": []}, {"text": "Recently, there has been quite some attention to the problem of domain switching in SMT ( but groundbreaking success is still missing.", "labels": [], "entities": [{"text": "domain switching", "start_pos": 64, "end_pos": 80, "type": "TASK", "confidence": 0.8421511054039001}, {"text": "SMT", "start_pos": 84, "end_pos": 87, "type": "TASK", "confidence": 0.887035608291626}]}, {"text": "In this paper we report our findings in dynamic model adaptation using cache-based techniques when applying a standard model to the task of translating documents from a very different domain.", "labels": [], "entities": [{"text": "dynamic model adaptation", "start_pos": 40, "end_pos": 64, "type": "TASK", "confidence": 0.6987245877583822}]}, {"text": "The remaining part of the paper is organized as follows: First, we will motivate the chosen approach by reviewing the general phenomenon of repetition and consistency in natural language text.", "labels": [], "entities": []}, {"text": "Thereafter, we will briefly discuss the dynamic extensions to language and translation models applied in the experiments presented in the second last section followed by some final conclusions.", "labels": [], "entities": []}], "datasetContent": [{"text": "Our experiments are focused on the unsupervised dynamic adaptation of language and translation models to anew domain using the cache-based mixture models as described above.", "labels": [], "entities": []}, {"text": "We apply these techniques to a standard task of translating French to English using a model trained on the publicly available Europarl corpus) using standard settings and tools such as the Moses toolkit ( , GIZA++ (Och and Ney, 2003) and SRILM).", "labels": [], "entities": [{"text": "Europarl corpus", "start_pos": 126, "end_pos": 141, "type": "DATASET", "confidence": 0.989319920539856}, {"text": "SRILM", "start_pos": 238, "end_pos": 243, "type": "DATASET", "confidence": 0.7069445252418518}]}, {"text": "The log-linear model is then tuned as usual with minimum error rate training) on a separate development set coming from the same domain (Europarl).", "labels": [], "entities": [{"text": "Europarl", "start_pos": 137, "end_pos": 145, "type": "DATASET", "confidence": 0.9788632988929749}]}, {"text": "We modified SRILM to include a decaying cache model and implemented the phrase translation cache within the Moses decoder.", "labels": [], "entities": [{"text": "SRILM", "start_pos": 12, "end_pos": 17, "type": "DATASET", "confidence": 0.7101329565048218}, {"text": "phrase translation", "start_pos": 72, "end_pos": 90, "type": "TASK", "confidence": 0.7512596547603607}]}, {"text": "Furthermore, we added the caching procedures and other features for testing the adaptive approach.", "labels": [], "entities": []}, {"text": "Now we can simply switch the cache models on or off using additional command-line arguments when running Moses as usual.", "labels": [], "entities": []}, {"text": "For testing we chose to use documents from the medical domain coming from the EMEA corpus that is part of the freely available collection of parallel corpora OPUS 2 (Tiedemann, 2009).", "labels": [], "entities": [{"text": "EMEA corpus", "start_pos": 78, "end_pos": 89, "type": "DATASET", "confidence": 0.964823842048645}]}, {"text": "The reason for selecting this domain is that these documents include very consistent instructions and repetitive texts which ought to favor our caching techniques.", "labels": [], "entities": []}, {"text": "Furthermore, they are very different from the training data and, thus, domain adaptation is very important for proper translations.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 71, "end_pos": 88, "type": "TASK", "confidence": 0.6914512366056442}, {"text": "translations", "start_pos": 118, "end_pos": 130, "type": "TASK", "confidence": 0.9691986441612244}]}, {"text": "We randomly selected 102 pairs of documents with altogether 5,478 sentences.", "labels": [], "entities": []}, {"text": "Sentences have an average length of about 19 tokens with a lot of variation among them.", "labels": [], "entities": []}, {"text": "Documents are compiled from the European Public Assessment Reports (EPAR) which reflect scientific conclusions at the end of a centralized evaluation procedure for medical products.", "labels": [], "entities": [{"text": "European Public Assessment Reports (EPAR)", "start_pos": 32, "end_pos": 73, "type": "DATASET", "confidence": 0.7540063517434257}]}, {"text": "They include a lot of domain-specific terminology, short facts, lists and tables but also detailed textual descriptions of medicines and their use.", "labels": [], "entities": []}, {"text": "The overall lowercased type/token ratio in the English part of our test collection is about 0.045 which indicates quite substantial repetitions in the text.", "labels": [], "entities": []}, {"text": "This ratio is, however, much higher for individual documents.", "labels": [], "entities": []}, {"text": "In the experiment each document is processed individually in order to apply appropriate discourse breaks.", "labels": [], "entities": []}, {"text": "The baseline score for applying a standard phrase-based SMT model yields an average score of 28.67 BLEU per document (28.60 per sentence) which is quite reasonable for an out-ofdomain test.", "labels": [], "entities": [{"text": "SMT", "start_pos": 56, "end_pos": 59, "type": "TASK", "confidence": 0.87385493516922}, {"text": "BLEU", "start_pos": 99, "end_pos": 103, "type": "METRIC", "confidence": 0.9982818365097046}]}, {"text": "Intuitively, the baseline performance should be crucial for the adaptation.", "labels": [], "entities": []}, {"text": "As discussed earlier the cache-based approach assumes correct history and better baseline performance should increase the chance of adding appropriate items to the cache.", "labels": [], "entities": []}], "tableCaptions": []}