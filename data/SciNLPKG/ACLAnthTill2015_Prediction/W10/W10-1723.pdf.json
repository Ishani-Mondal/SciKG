{"title": [{"text": "The LIG machine translation system for WMT 2010", "labels": [], "entities": [{"text": "LIG machine translation", "start_pos": 4, "end_pos": 27, "type": "TASK", "confidence": 0.5406623582045237}, {"text": "WMT", "start_pos": 39, "end_pos": 42, "type": "TASK", "confidence": 0.764743983745575}]}], "abstractContent": [{"text": "This paper describes the system submitted by the Laboratory of Informatics of Grenoble (LIG) for the fifth Workshop on Statistical Machine Translation.", "labels": [], "entities": [{"text": "Statistical Machine Translation", "start_pos": 119, "end_pos": 150, "type": "TASK", "confidence": 0.7978445490201315}]}, {"text": "We participated to the news shared translation task for the French-English language pair.", "labels": [], "entities": [{"text": "news shared translation task", "start_pos": 23, "end_pos": 51, "type": "TASK", "confidence": 0.6662779003381729}]}, {"text": "We investigated differents techniques to simply deal with Out-Of-Vocabulary words in a statistical phrase-based machine translation system and analyze their impact on translation quality.", "labels": [], "entities": [{"text": "statistical phrase-based machine translation", "start_pos": 87, "end_pos": 131, "type": "TASK", "confidence": 0.6110789254307747}]}, {"text": "The final submission is a combination between a standard phrase-based system using the Moses decoder, with appropriate setups and pre-processing, and a lemmatized system to deal with Out-Of-Vocabulary conjugated verbs.", "labels": [], "entities": []}], "introductionContent": [{"text": "We participated, for the first time, to the shared news translation task of the fifth Workshop on Machine Translation) for the FrenchEnglish language pair.", "labels": [], "entities": [{"text": "shared news translation task of the fifth Workshop on Machine Translation", "start_pos": 44, "end_pos": 117, "type": "TASK", "confidence": 0.6386629966172305}, {"text": "FrenchEnglish language pair", "start_pos": 127, "end_pos": 154, "type": "DATASET", "confidence": 0.9291048248608907}]}, {"text": "The submission was performed using a standard phrase-based translation system with appropriate setups and preprocessings in order to deal with system's unknown words.", "labels": [], "entities": [{"text": "phrase-based translation", "start_pos": 46, "end_pos": 70, "type": "TASK", "confidence": 0.6565613001585007}]}, {"text": "Indeed, as shown in, and, handling Ou-of-Vocabulary words with techniques like lemmatization, phrase table extension or morphological pre-processing is away to improve translation quality.", "labels": [], "entities": [{"text": "phrase table extension", "start_pos": 94, "end_pos": 116, "type": "TASK", "confidence": 0.6911911765734354}]}, {"text": "After a short presentation of our baseline system setups we discuss the effect of Out-Of-Vocabulary words in the system and introduce some ideas we chose to implement.", "labels": [], "entities": []}, {"text": "In the last part, we evaluate their impact on translation quality using automatic and human evaluations.", "labels": [], "entities": [{"text": "translation quality", "start_pos": 46, "end_pos": 65, "type": "TASK", "confidence": 0.9236428141593933}]}], "datasetContent": [{"text": "In the automatic evaluation, the reported evaluation metric is the BLEU score () computed by MTEval version 13a.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 67, "end_pos": 77, "type": "METRIC", "confidence": 0.9793650507926941}, {"text": "MTEval version 13a", "start_pos": 93, "end_pos": 111, "type": "DATASET", "confidence": 0.8422764738400778}]}, {"text": "The results are reported in table 1.", "labels": [], "entities": []}, {"text": "Note that in our experiments, according to the resampling method of), there are significative variations (improvement or deterioration), with 95% certainty, only if the difference between two BLEU scores represent, at least, 0.33 points.", "labels": [], "entities": [{"text": "certainty", "start_pos": 146, "end_pos": 155, "type": "METRIC", "confidence": 0.9507359862327576}, {"text": "BLEU", "start_pos": 192, "end_pos": 196, "type": "METRIC", "confidence": 0.9982333183288574}]}, {"text": "To complete this automatic evaluation, we performed a human analysis of the systems outputs.", "labels": [], "entities": []}, {"text": "We compared two data set.", "labels": [], "entities": []}, {"text": "The first set (selected sent.) contains 301 sentences selected from test data by the combined system to be translated by the lemmatized system (6) whereas the second set (random sent.) contains 301 sentences randomly picked up.", "labels": [], "entities": []}, {"text": "The latter is our control data set.", "labels": [], "entities": [{"text": "control data set", "start_pos": 18, "end_pos": 34, "type": "DATASET", "confidence": 0.8114858667055765}]}, {"text": "We compared for both groups the translation hypothesis given by the lemmatized system and the standard one.", "labels": [], "entities": [{"text": "translation", "start_pos": 32, "end_pos": 43, "type": "TASK", "confidence": 0.9687789082527161}]}, {"text": "We performed a subjective evaluation with the NIST five points scales to measure fluency and adequacy of each sentences through SECtra w interface ().We involved a total of 6 volunteers judges (3 for each set).", "labels": [], "entities": [{"text": "NIST", "start_pos": 46, "end_pos": 50, "type": "DATASET", "confidence": 0.925449550151825}, {"text": "SECtra w interface", "start_pos": 128, "end_pos": 146, "type": "DATASET", "confidence": 0.6862921118736267}]}, {"text": "We evaluated the inter-annotator agreement using a generalized version of Kappa.", "labels": [], "entities": []}, {"text": "The results show a slight to fair agreement according.", "labels": [], "entities": []}, {"text": "The evaluation results, detailled in table 5 and 6, showed that both fluency and adequacy were improved using our combined system.", "labels": [], "entities": []}, {"text": "Indeed, fora random input (random sent.), the lemmatized system lowers the translations quality (fluency and adequacy are degraded for, respectively, 35.8% and 37.5% of the sentences), while it improves the quality for sentences selected by the combined system (for \"selected sent.\", fluency and adequacy are improved or stable for 81% of the sentences).", "labels": [], "entities": []}, {"text": "(6) \u2265 (4) 81% 62.4% (6) < 18.9% 37.5%: Subjective evaluation of sentences adequacy ((6) lemmatized system -(4) standard system) Fluency selected sent.", "labels": [], "entities": []}, {"text": "(6) \u2265 (4) 81% 64.1% (6)< 18.9% 35.8%: Subjective evaluation of sentences fluency ((6) lemmatized system -(4) standard system)", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Standard systems BLEU scores with tuning (without tuning)/ LM 5-gram", "labels": [], "entities": [{"text": "BLEU", "start_pos": 27, "end_pos": 31, "type": "METRIC", "confidence": 0.9949122071266174}]}, {"text": " Table 3: Systems's results on test set with differents language models", "labels": [], "entities": []}]}