{"title": [{"text": "The RWTH System Combination System for WMT 2010", "labels": [], "entities": [{"text": "WMT", "start_pos": 39, "end_pos": 42, "type": "TASK", "confidence": 0.8942370414733887}]}], "abstractContent": [{"text": "RWTH participated in the System Combination task of the Fifth Workshop on Statistical Machine Translation (WMT 2010).", "labels": [], "entities": [{"text": "RWTH", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.9233277440071106}, {"text": "System Combination task", "start_pos": 25, "end_pos": 48, "type": "TASK", "confidence": 0.8123378753662109}, {"text": "Statistical Machine Translation (WMT 2010)", "start_pos": 74, "end_pos": 116, "type": "TASK", "confidence": 0.8213301641600472}]}, {"text": "For 7 of the 8 language pairs, we combine 5 to 13 systems into a single consensus translation, using additional n-best reranking techniques in two of these language pairs.", "labels": [], "entities": []}, {"text": "Depending on the language pair, improvements versus the best single system are in the range of +0.5 and +1.7 on BLEU, and between \u22120.4 and \u22122.3 on TER.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 112, "end_pos": 116, "type": "METRIC", "confidence": 0.9552651047706604}, {"text": "TER", "start_pos": 147, "end_pos": 150, "type": "DATASET", "confidence": 0.8202771544456482}]}, {"text": "Novel techniques compared with RWTH's submission to WMT 2009 include the utilization of n-best reranking techniques, a consensus true casing approach , a different tuning algorithm, and the separate selection of input systems for CN construction, primary/skeleton hypotheses , HypLM, and true casing.", "labels": [], "entities": [{"text": "RWTH's submission to WMT 2009", "start_pos": 31, "end_pos": 60, "type": "DATASET", "confidence": 0.8561162253220876}, {"text": "CN construction", "start_pos": 230, "end_pos": 245, "type": "TASK", "confidence": 0.8091397285461426}, {"text": "HypLM", "start_pos": 277, "end_pos": 282, "type": "METRIC", "confidence": 0.7543516159057617}]}], "introductionContent": [{"text": "The RWTH approach to MT system combination is a refined version of the ROVER approach in ASR, with additional steps to cope with reordering between different hypotheses, and to use true casing information from the input hypotheses.", "labels": [], "entities": [{"text": "MT system combination", "start_pos": 21, "end_pos": 42, "type": "TASK", "confidence": 0.8985584576924642}, {"text": "ASR", "start_pos": 89, "end_pos": 92, "type": "TASK", "confidence": 0.9479209780693054}]}, {"text": "The basic concept of the approach has been described by.", "labels": [], "entities": []}, {"text": "Several improvements have been added later (.", "labels": [], "entities": []}, {"text": "This approach includes an enhanced alignment and reordering framework.", "labels": [], "entities": []}, {"text": "In contrast to existing approaches (Jayaraman and, the context of the whole corpus rather than a single sentence is considered in this iterative, unsupervised procedure, yielding a more reliable alignment.", "labels": [], "entities": []}, {"text": "Majority voting on the generated lattice is performed using prior weights for each system as well as other statistical models such as a special n-gram language model.", "labels": [], "entities": [{"text": "Majority voting", "start_pos": 0, "end_pos": 15, "type": "TASK", "confidence": 0.7935046851634979}]}, {"text": "In addition to lattice rescoring, n-best list reranking techniques can be applied ton best paths of this lattice.", "labels": [], "entities": [{"text": "lattice rescoring", "start_pos": 15, "end_pos": 32, "type": "TASK", "confidence": 0.7008731067180634}]}, {"text": "True casing is considered a separate step in RWTH's approach, which also takes the input hypotheses into account.", "labels": [], "entities": [{"text": "RWTH", "start_pos": 45, "end_pos": 49, "type": "DATASET", "confidence": 0.6738738417625427}]}, {"text": "The pipeline, and consequently the description of the pipeline given in this paper, is based on our pipeline for WMT 2009 (, with several extensions as described.", "labels": [], "entities": [{"text": "WMT 2009", "start_pos": 113, "end_pos": 121, "type": "DATASET", "confidence": 0.9004707336425781}]}], "datasetContent": [{"text": "Each language pair and each direction in WMT 2010 had its own set of systems, so we selected and tuned for each direction separately.", "labels": [], "entities": [{"text": "WMT 2010", "start_pos": 41, "end_pos": 49, "type": "DATASET", "confidence": 0.8582749664783478}]}, {"text": "After submission of our system combination output to WMT 2010, we also calculated scores on the test set (TEST), to validate our results, and as a preparation for this report.", "labels": [], "entities": [{"text": "WMT 2010", "start_pos": 53, "end_pos": 61, "type": "DATASET", "confidence": 0.9250971078872681}, {"text": "TEST)", "start_pos": 106, "end_pos": 111, "type": "METRIC", "confidence": 0.9573273062705994}]}, {"text": "Note that the scores reported for DEV are calculated on the full DEV set, but not on any combination of the one-fifth \"cross validation\" subcorpora.", "labels": [], "entities": [{"text": "DEV", "start_pos": 34, "end_pos": 37, "type": "TASK", "confidence": 0.5589337348937988}, {"text": "DEV set", "start_pos": 65, "end_pos": 72, "type": "DATASET", "confidence": 0.9318923652172089}]}], "tableCaptions": [{"text": " Table 2: Results for FR-EN.", "labels": [], "entities": [{"text": "FR-EN", "start_pos": 22, "end_pos": 27, "type": "DATASET", "confidence": 0.40883466601371765}]}, {"text": " Table 3: Results for EN-FR.", "labels": [], "entities": [{"text": "EN-FR", "start_pos": 22, "end_pos": 27, "type": "DATASET", "confidence": 0.684026837348938}]}, {"text": " Table 5: Results for DE-EN.", "labels": [], "entities": [{"text": "DE-EN", "start_pos": 22, "end_pos": 27, "type": "DATASET", "confidence": 0.5560322403907776}]}, {"text": " Table 6: Results for EN-DE.", "labels": [], "entities": [{"text": "EN-DE", "start_pos": 22, "end_pos": 27, "type": "DATASET", "confidence": 0.787588894367218}]}, {"text": " Table 8: Results for CZ-EN and EN-CZ.  TUNE  TEST  BLEU TER BLEU TER  CZ-EN  Best single  21.8 58.4  22.9 57.5  Primary SC  22.4 59.1  23.4 57.9  EN-CZ  Best single  17.0 67.1  16.6 66.4  Primary SC  16.7 65.4  17.4 63.6", "labels": [], "entities": [{"text": "TUNE  TEST  BLEU TER BLEU TER  CZ-EN", "start_pos": 40, "end_pos": 76, "type": "METRIC", "confidence": 0.7579753484044757}]}, {"text": " Table 10: Results for EN-ES.", "labels": [], "entities": [{"text": "EN-ES", "start_pos": 23, "end_pos": 28, "type": "DATASET", "confidence": 0.6333799362182617}]}]}