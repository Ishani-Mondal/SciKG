{"title": [{"text": "Preservation of Recognizability for Synchronous Tree Substitution Grammars", "labels": [], "entities": [{"text": "Preservation of Recognizability", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.8082690238952637}, {"text": "Synchronous Tree Substitution Grammars", "start_pos": 36, "end_pos": 74, "type": "TASK", "confidence": 0.6337828785181046}]}], "abstractContent": [{"text": "We consider synchronous tree substitution grammars (STSG).", "labels": [], "entities": [{"text": "synchronous tree substitution grammars (STSG)", "start_pos": 12, "end_pos": 57, "type": "TASK", "confidence": 0.7829153878348214}]}, {"text": "With the help of a characterization of the expressive power of STSG in terms of weighted tree bimor-phisms, we show that both the forward and the backward application of an STSG preserve recognizability of weighted tree languages in all reasonable cases.", "labels": [], "entities": []}, {"text": "As a consequence , both the domain and the range of an STSG without chain rules are recognizable weighted tree languages.", "labels": [], "entities": []}], "introductionContent": [{"text": "The syntax-based approach to statistical machine translation) becomes more and more competitive in machine translation, which is a subfield of natural language processing (NLP).", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 29, "end_pos": 60, "type": "TASK", "confidence": 0.6433184941609701}, {"text": "machine translation", "start_pos": 99, "end_pos": 118, "type": "TASK", "confidence": 0.816260814666748}, {"text": "natural language processing (NLP)", "start_pos": 143, "end_pos": 176, "type": "TASK", "confidence": 0.7500441074371338}]}, {"text": "In this approach the full parse trees of the involved sentences are available to the translation model, which can base its decisions on this rich structure.", "labels": [], "entities": []}, {"text": "In the competing phrase-based approach () the translation model only has access to the linear sentence structure.", "labels": [], "entities": []}, {"text": "There are two major classes of syntax-based translation models: tree transducers and synchronous grammars.", "labels": [], "entities": []}, {"text": "Examples in the former class are the top-down tree transducer, the extended top-down tree transducer, and the extended multi bottom-up tree transducer).", "labels": [], "entities": []}, {"text": "The latter class contains the syntax-directed transductions of, the generalized syntax-directed transductions, the synchronous tree substitution grammar (STSG) by and the synchronous tree adjoining grammar (STAG) by and.", "labels": [], "entities": []}, {"text": "The first bridge between those two classes were established in.", "labels": [], "entities": []}, {"text": "Further comparisons can be found in) for STSG and in) for STAG.", "labels": [], "entities": [{"text": "STSG", "start_pos": 41, "end_pos": 45, "type": "TASK", "confidence": 0.779112696647644}, {"text": "STAG", "start_pos": 58, "end_pos": 62, "type": "TASK", "confidence": 0.40333256125450134}]}, {"text": "One of the main challenges in NLP is the ambiguity that is inherent in natural languages.", "labels": [], "entities": []}, {"text": "For instance, the sentence \"I saw the man with the telescope\" has several different meanings.", "labels": [], "entities": []}, {"text": "Some of them can be distinguished by the parse tree, so that probabilistic parsers () for natural languages can (partially) achieve the disambiguation.", "labels": [], "entities": []}, {"text": "Such a parser returns a set of parse trees for each input sentence, and in addition, each returned parse tree is assigned a likelihood.", "labels": [], "entities": []}, {"text": "Thus, the result can be seen as a mapping from parse trees to probabilities where the impossible parses are assigned the probability 0.", "labels": [], "entities": []}, {"text": "Such mappings are called weighted tree languages, of which some can be finitely represented by weighted regular tree grammars.", "labels": [], "entities": []}, {"text": "Those weighted tree languages are recognizable and there exist algorithms) that efficiently extract the k-best parse trees (i.e., those with the highest probability) for further processing.", "labels": [], "entities": []}, {"text": "In this paper we consider synchronized tree substitution grammars (STSG).", "labels": [], "entities": [{"text": "synchronized tree substitution grammars (STSG)", "start_pos": 26, "end_pos": 72, "type": "TASK", "confidence": 0.7806370769228254}]}, {"text": "To overcome a technical difficulty we add (grammar) nonterminals to them.", "labels": [], "entities": []}, {"text": "Since an STSG often uses the nonterminals of a context-free grammar as terminal symbols (i.e., its derived trees contain both terminal and nonterminal symbols of the context-free grammar), we call the newly added (grammar) nonterminals of the STSG states.", "labels": [], "entities": []}, {"text": "Substitution does no longer take place at synchronized nonterminals (of the context-free grammar) but at synchronized states (one for the input and one for the output side).", "labels": [], "entities": []}, {"text": "The states themselves will not appear in the final derived trees, which yields that it is sufficient to assume that only identical states are synchro-nized.", "labels": [], "entities": []}, {"text": "Under those conventions a rule of an STSG has the form q \u2192 (s, t, V, a) where q is a state, a \u2208 R \u22650 is the rule weight, sis an input tree that can contain states at the leaves, and t is an output tree that can also contain states.", "labels": [], "entities": []}, {"text": "Finally, the synchronization is defined by V , which is a bijection between the state-labeled leaves of sand t.", "labels": [], "entities": []}, {"text": "We require that V only relates identical states.", "labels": [], "entities": []}, {"text": "The rules of an STSG are applied in a step-wise manner.", "labels": [], "entities": []}, {"text": "Here we use a derivation relation to define the semantics of an STSG.", "labels": [], "entities": []}, {"text": "It can be understood as the synchronization of the derivation relations of two regular tree grammars) where the synchronization is done on nonterminals (or states) in the spirit of syntax-directed transductions.", "labels": [], "entities": []}, {"text": "Thus each sentential form is a pair of (nonterminal-) connected trees.", "labels": [], "entities": []}, {"text": "An STSG G computes a mapping \u03c4 G , called its weighted tree transformation, that assigns a weight to each pair of input and output trees, where both the input and output tree may not contain any state.", "labels": [], "entities": []}, {"text": "This transformation is obtained as follows: We start with two copies of the initial state that are synchronized.", "labels": [], "entities": []}, {"text": "Given a connected tree pair (\u03be, \u03b6), we can apply the rule q \u2192 (s, t, V, a) to each pair of synchronized states q.", "labels": [], "entities": []}, {"text": "Such an application replaces the selected state q in \u03be by sand the corresponding state q in \u03b6 by t.", "labels": [], "entities": []}, {"text": "All the remaining synchronized states and the synchronized states of V remain synchronized.", "labels": [], "entities": []}, {"text": "The result is anew connected tree pair.", "labels": [], "entities": []}, {"text": "This step charges the weight a.", "labels": [], "entities": []}, {"text": "The weights of successive applications (or steps) are multiplied to obtain the weight of the derivation.", "labels": [], "entities": []}, {"text": "The weighted tree transformation \u03c4 G assigns to each pair of trees the sum of all weights of derivations that derive that pair.", "labels": [], "entities": []}, {"text": "showed that for every classical unweighted STSG there exists an equivalent bimorphism.", "labels": [], "entities": []}, {"text": "The converse result only holds up to deterministic relabelings, which remove the state information from the input and output tree.", "labels": [], "entities": []}, {"text": "It is this difference that motivates us to add states to STSG.", "labels": [], "entities": []}, {"text": "We generalize the result of Shieber (2004) and prove that every weighted tree transformation that is computable by an STSG can also be computed by a weighted bimorphism and vice versa.", "labels": [], "entities": []}, {"text": "Given an STSG and a recognizable weighted tree language \u03d5 of input trees, we investigate under which conditions the weighted tree language obtained by applying G to \u03d5 is again recognizable.", "labels": [], "entities": []}, {"text": "In other words, we investigate under which conditions the forward application of G preserves recognizability.", "labels": [], "entities": []}, {"text": "The same question is investigated for backward application, which is the corresponding operation given a recognizable weighted tree language of output trees.", "labels": [], "entities": []}, {"text": "Since STSG are symmetric (i.e., input and output can be exchanged), the results for backward application can be obtained easily from the results for forward application.", "labels": [], "entities": []}, {"text": "Our main result is that forward application preserves recognizability if the STSG G is outputproductive, which means that each rule of G contains at least one output symbol that is not a state.", "labels": [], "entities": []}, {"text": "Dually, backward application preserves recognizability if G is input-productive, which is the analogous property for the input side.", "labels": [], "entities": []}, {"text": "In fact, those results hold for weights taken from an arbitrary commutative semiring), but we present the results only for probabilities.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": []}