{"title": [], "abstractContent": [{"text": "We consider morphology learning in a semi-supervised setting, where a small set of linguistic gold standard analyses is available.", "labels": [], "entities": [{"text": "morphology learning", "start_pos": 12, "end_pos": 31, "type": "TASK", "confidence": 0.865783154964447}]}, {"text": "We extend Morfessor Base-line, which is a method for unsupervised morphological segmentation, to this task.", "labels": [], "entities": [{"text": "morphological segmentation", "start_pos": 66, "end_pos": 92, "type": "TASK", "confidence": 0.7291688323020935}]}, {"text": "We show that known linguistic segmenta-tions can be exploited by adding them into the data likelihood function and optimizing separate weights for unlabeled and labeled data.", "labels": [], "entities": []}, {"text": "Experiments on English and Finnish are presented with varying amount of labeled data.", "labels": [], "entities": []}, {"text": "Results of the linguistic evaluation of Morpho Challenge improve rapidly already with small amounts of labeled data, surpassing the state-of-the-art unsupervised methods at 1000 labeled words for English and at 100 labeled words for Finnish.", "labels": [], "entities": []}], "introductionContent": [{"text": "Morphological analysis is required in many natural language processing problems.", "labels": [], "entities": [{"text": "Morphological analysis", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.9158313870429993}, {"text": "natural language processing", "start_pos": 43, "end_pos": 70, "type": "TASK", "confidence": 0.6344937185446421}]}, {"text": "Especially, in agglutinative and compounding languages, where each word form consists of a combination of stems and affixes, the number of unique word forms in a corpus is very large.", "labels": [], "entities": []}, {"text": "This leads to problems in word-based statistical language modeling: Even with a large training corpus, many of the words encountered when applying the model did not occur in the training corpus, and thus there is no information available on how to process them.", "labels": [], "entities": [{"text": "statistical language modeling", "start_pos": 37, "end_pos": 66, "type": "TASK", "confidence": 0.6699116329352061}]}, {"text": "Using morphological units, such as stems and affixes, instead of complete word forms alleviates this problem.", "labels": [], "entities": []}, {"text": "Unfortunately, for many languages morphological analysis tools either do not exist or they are not freely available.", "labels": [], "entities": [{"text": "morphological analysis", "start_pos": 34, "end_pos": 56, "type": "TASK", "confidence": 0.7203115373849869}]}, {"text": "In many cases, the problems of availability also apply to morphologically annotated corpora, making supervised learning infeasible.", "labels": [], "entities": []}, {"text": "In consequence, there has been a need for approaches for morphological processing that would require little language-dependent resources.", "labels": [], "entities": [{"text": "morphological processing", "start_pos": 57, "end_pos": 81, "type": "TASK", "confidence": 0.7065113484859467}]}, {"text": "Due to this need, as well as the general interest in language acquisition and unsupervised language learning, the research on unsupervised learning of morphology has been active during the past ten years.", "labels": [], "entities": [{"text": "language acquisition", "start_pos": 53, "end_pos": 73, "type": "TASK", "confidence": 0.7167961597442627}]}, {"text": "Especially, methods that perform morphological segmentation have been studied extensively.", "labels": [], "entities": [{"text": "morphological segmentation", "start_pos": 33, "end_pos": 59, "type": "TASK", "confidence": 0.8725202977657318}]}, {"text": "These methods have shown to produce results that improve performance in several applications, such as speech recognition and information retrieval.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 102, "end_pos": 120, "type": "TASK", "confidence": 0.8801395297050476}, {"text": "information retrieval", "start_pos": 125, "end_pos": 146, "type": "TASK", "confidence": 0.840395450592041}]}, {"text": "While unsupervised methods often work quite well across different languages, it is difficult to avoid biases toward certain kinds of languages and analyses.", "labels": [], "entities": []}, {"text": "For example, in isolating languages, the average amount of morphemes per word is low, whereas in synthetic languages the amount maybe very high.", "labels": [], "entities": []}, {"text": "Also, different applications may need a particular bias, for example, not analyzing frequent compound words as consisting of smaller parts could be beneficial in information retrieval.", "labels": [], "entities": [{"text": "information retrieval", "start_pos": 162, "end_pos": 183, "type": "TASK", "confidence": 0.8118736147880554}]}, {"text": "In many cases, even a small amount of labeled data can be used to adapt a method to a particular language and task.", "labels": [], "entities": []}, {"text": "Methodologically, this is referred to as semi-supervised learning.", "labels": [], "entities": []}, {"text": "In semi-supervised learning, the learning system has access to both labeled and unlabeled data.", "labels": [], "entities": []}, {"text": "Typically, the labeled data set is too small for supervised methods to be effective, but there is a large amount of unlabeled data available.", "labels": [], "entities": []}, {"text": "There are many different approaches to this class of problems, as presented by.", "labels": [], "entities": []}, {"text": "One approach is to use generative models, which specify a join distribution overall variables in the model.", "labels": [], "entities": []}, {"text": "They can be utilized both in unsupervised and supervised learning.", "labels": [], "entities": []}, {"text": "In contrast, discriminative models only specify the conditional distribution between input data and labels, and therefore require labeled data.", "labels": [], "entities": []}, {"text": "Both, however, can be extended to the semi-supervised case.", "labels": [], "entities": []}, {"text": "For generative models, it is, in principle, very easy to use both labeled and unlabeled data.", "labels": [], "entities": [{"text": "generative", "start_pos": 4, "end_pos": 14, "type": "TASK", "confidence": 0.9651491045951843}]}, {"text": "For unsupervised learning one can consider the labels as missing data and estimate their values using the Expectation Maximization (EM) algorithm.", "labels": [], "entities": [{"text": "Expectation Maximization (EM)", "start_pos": 106, "end_pos": 135, "type": "TASK", "confidence": 0.7491026043891906}]}, {"text": "In the semi-supervised case, some labels are available, and the rest are considered missing and estimated with EM.", "labels": [], "entities": [{"text": "EM", "start_pos": 111, "end_pos": 113, "type": "METRIC", "confidence": 0.986377477645874}]}, {"text": "In this paper, we extend the Morfessor Baseline method for the semi-supervised case.", "labels": [], "entities": []}, {"text": "Morfessor () is one of the well-established methods for morphological segmentation.", "labels": [], "entities": [{"text": "morphological segmentation", "start_pos": 56, "end_pos": 82, "type": "TASK", "confidence": 0.8915169835090637}]}, {"text": "It applies a simple generative model.", "labels": [], "entities": []}, {"text": "The basic idea, inspired by the Minimum Description Length principle, is to encode the words in the training data with a lexicon of morphs, that are segments of the words.", "labels": [], "entities": []}, {"text": "The number of bits needed to encode both the morph lexicon and the data using the lexicon should be minimized.", "labels": [], "entities": []}, {"text": "Morfessor does not limit the number of morphemes per word form, making it suitable for modeling a large variety of agglutinative languages irrespective of them being more isolating or synthetic.", "labels": [], "entities": []}, {"text": "We show that the model can be trained in a similar fashion in the semi-supervised case as in the unsupervised case.", "labels": [], "entities": []}, {"text": "However, with a large set of unlabeled data, the effect of the supervision on the results tends to be small.", "labels": [], "entities": []}, {"text": "Thus, we add a discriminative weighting scheme, where a small set of word forms with gold standard analyzes are used for tuning the respective weights of the labeled and unlabeled data.", "labels": [], "entities": []}, {"text": "The paper is organized as follows: First, we discuss related work on semi-supervised learning.", "labels": [], "entities": []}, {"text": "Then we describe the Morfessor Baseline model and the unsupervised algorithm, followed by our semi-supervised extension.", "labels": [], "entities": []}, {"text": "Finally, we present experimental results for English and Finnish using the Morpho Challenge data sets ().", "labels": [], "entities": [{"text": "Morpho Challenge data sets", "start_pos": 75, "end_pos": 101, "type": "DATASET", "confidence": 0.9728678613901138}]}], "datasetContent": [{"text": "In the experiments, we compare six different variants of the Morfessor Baseline algorithm: \u2022 Unsupervised: The classic, unsupervised Morfessor baseline.", "labels": [], "entities": []}, {"text": "\u2022 Unsupervised + weighting: A held-out set is used for adjusting the weight of the likelihood \u03b1.", "labels": [], "entities": []}, {"text": "When \u03b1 = 1 the method is equivalent to the unsupervised baseline.", "labels": [], "entities": []}, {"text": "The main effect of adjusting \u03b1 is to control how many segments per word the algorithm prefers.", "labels": [], "entities": []}, {"text": "Higher \u03b1 leads to fewer and lower \u03b1 to more segments per word.", "labels": [], "entities": []}, {"text": "\u2022 Supervised: The semi-supervised method trained with only the labeled data.", "labels": [], "entities": []}, {"text": "\u2022 Supervised + weighting: As above, but the weight of the likelihood \u03b2 is optimized on the held-out set.", "labels": [], "entities": []}, {"text": "The weight can only affect which segmentations are selected from the possible alternative segmentations in the labeled data.", "labels": [], "entities": []}, {"text": "\u2022 Semi-supervised: The semi-supervised method trained with both labeled and unlabeled data.", "labels": [], "entities": []}, {"text": "\u2022 Semi-supervised + weighting: As above, but the parameters \u03b1 and \u03b2 are optimized using the the held-out set.", "labels": [], "entities": []}, {"text": "All variations are evaluated using the linguistic gold standard evaluation of Morpho Challenge 2009.", "labels": [], "entities": [{"text": "Morpho Challenge 2009", "start_pos": 78, "end_pos": 99, "type": "DATASET", "confidence": 0.8431886831919352}]}, {"text": "For supervised and semi-supervised methods, the amount of labeled data is varied between 100 and 10 000 words, whereas the heldout set has 500 gold standard analyzes.", "labels": [], "entities": []}, {"text": "To obtain precision-recall curves, we calculated weighted F0.5 and F2 scores in addition to the normal F1 score.", "labels": [], "entities": [{"text": "precision-recall", "start_pos": 10, "end_pos": 26, "type": "METRIC", "confidence": 0.9977486729621887}, {"text": "F0.5", "start_pos": 58, "end_pos": 62, "type": "METRIC", "confidence": 0.9929417371749878}, {"text": "F2", "start_pos": 67, "end_pos": 69, "type": "METRIC", "confidence": 0.9524709582328796}, {"text": "F1 score", "start_pos": 103, "end_pos": 111, "type": "METRIC", "confidence": 0.9762857854366302}]}, {"text": "The parameters \u03b1 and \u03b2 were optimized also for those.", "labels": [], "entities": []}, {"text": "We used the English and Finnish data sets from Competition 1 of Morpho Challenge 2009 ().", "labels": [], "entities": [{"text": "Finnish data sets from Competition 1 of Morpho Challenge 2009", "start_pos": 24, "end_pos": 85, "type": "DATASET", "confidence": 0.8595841854810715}]}, {"text": "Both are extracted from a three million sentence corpora.", "labels": [], "entities": []}, {"text": "For English, there were 62 185 728 word tokens and 384 903 word types.", "labels": [], "entities": []}, {"text": "For Finnish, there were 36 207 308 word tokens and 2 206 719 word types.", "labels": [], "entities": []}, {"text": "The complexity of Finnish morphology is indicated by the almost ten times larger number of word types than in English, while the number of word tokens is smaller.", "labels": [], "entities": []}, {"text": "We applied also the evaluation method of the Morpho Challenge 2009: The results of the morphological segmentation were compared to a linguistic gold standard analysis.", "labels": [], "entities": [{"text": "Morpho Challenge 2009", "start_pos": 45, "end_pos": 66, "type": "DATASET", "confidence": 0.8204212188720703}, {"text": "morphological segmentation", "start_pos": 87, "end_pos": 113, "type": "TASK", "confidence": 0.7835645079612732}]}, {"text": "Precision measures whether the words that share morphemes in the proposed analysis have common morphemes also in the gold standard, and recall measures the opposite.", "labels": [], "entities": [{"text": "Precision", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.9765750765800476}, {"text": "recall", "start_pos": 136, "end_pos": 142, "type": "METRIC", "confidence": 0.9988868832588196}]}, {"text": "The final score to optimize was F-measure, i.e, the harmonic mean of the precision and recall.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 32, "end_pos": 41, "type": "METRIC", "confidence": 0.9984471201896667}, {"text": "precision", "start_pos": 73, "end_pos": 82, "type": "METRIC", "confidence": 0.9992120265960693}, {"text": "recall", "start_pos": 87, "end_pos": 93, "type": "METRIC", "confidence": 0.9941190481185913}]}, {"text": "In addition to the unweighted F1 score, we have applied F2 and F0.5 scores, which give more weight to recall and precision, respectively.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 30, "end_pos": 38, "type": "METRIC", "confidence": 0.9833236932754517}, {"text": "F2", "start_pos": 56, "end_pos": 58, "type": "METRIC", "confidence": 0.988552451133728}, {"text": "F0.5", "start_pos": 63, "end_pos": 67, "type": "METRIC", "confidence": 0.977358341217041}, {"text": "recall", "start_pos": 102, "end_pos": 108, "type": "METRIC", "confidence": 0.9989690780639648}, {"text": "precision", "start_pos": 113, "end_pos": 122, "type": "METRIC", "confidence": 0.9990247488021851}]}, {"text": "Finnish gold standards are based on FINT-WOL morphological analyzer from Lingsoft, Inc., that applies the two-level model by.", "labels": [], "entities": [{"text": "FINT-WOL", "start_pos": 36, "end_pos": 44, "type": "METRIC", "confidence": 0.6234167218208313}]}, {"text": "English gold standards are from the CELEX English database.", "labels": [], "entities": [{"text": "English gold standards", "start_pos": 0, "end_pos": 22, "type": "DATASET", "confidence": 0.836107869942983}, {"text": "CELEX English database", "start_pos": 36, "end_pos": 58, "type": "DATASET", "confidence": 0.9773560166358948}]}, {"text": "The final test sets are the same as in Morpho Challenge, based on 10 000 English word forms and 200 000 Finnish word forms.", "labels": [], "entities": []}, {"text": "The test sets are divided into ten parts for calculating deviations and statistical significances.", "labels": [], "entities": []}, {"text": "For parameter tuning, we applied a small held-out set containing 500 word forms that were not included in the test set.", "labels": [], "entities": [{"text": "parameter tuning", "start_pos": 4, "end_pos": 20, "type": "TASK", "confidence": 0.8027901649475098}]}, {"text": "For supervised and semi-supervised training, we created sets of five different sizes: 100, 300, 1 000, 3 000, and 10 000.", "labels": [], "entities": []}, {"text": "They did not contain any of the word forms in the final test set, but were otherwise randomly selected from the words for which the gold standard analyses were available.", "labels": [], "entities": []}, {"text": "In order to use them for training Morfessor, the morpheme analyses were converted to segmentations using the Hutmegs package by. shows a comparison of the unsupervised, supervised and semi-supervised Morfessor Baseline for English.", "labels": [], "entities": [{"text": "Hutmegs package", "start_pos": 109, "end_pos": 124, "type": "DATASET", "confidence": 0.9606919288635254}]}, {"text": "It can be seen that optimizing the likelihood weight \u03b1 alone does not improve much over the unsupervised case, implying that the Morfessor Baseline is well suited for English morphology.", "labels": [], "entities": []}, {"text": "Without weighting of the likelihood function, semi-supervised training improves the results somewhat, but it outperforms weighted unsupervised model only barely.", "labels": [], "entities": []}, {"text": "With weighting, however, semi-supervised training improves the results significantly already for only 100 labeled training samples.", "labels": [], "entities": []}, {"text": "For comparison, in Morpho Challenges (), the unsupervised Morfessor Baseline and Morfessor Categories-MAP by have achieved F-measures of 59.84% and 50.50%, respectively, and the all time best unsupervised result by a method that does not provide alternative analyses for words is 66.24%, obtained by Bernhard.", "labels": [], "entities": [{"text": "F-measures", "start_pos": 123, "end_pos": 133, "type": "METRIC", "confidence": 0.9981004595756531}]}, {"text": "This best unsupervised result is surpassed by the semi-supervised algorithm at 1000 labeled samples.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: The F0.5, F1 and F2 measures for the  semi-supervised + weighting method.", "labels": [], "entities": [{"text": "F0.5", "start_pos": 14, "end_pos": 18, "type": "METRIC", "confidence": 0.950128972530365}, {"text": "F1", "start_pos": 20, "end_pos": 22, "type": "METRIC", "confidence": 0.9965524673461914}, {"text": "F2", "start_pos": 27, "end_pos": 29, "type": "METRIC", "confidence": 0.8633220791816711}]}, {"text": " Table 2: The values for the weights \u03b1 and \u03b2  that the semisupervised algorithm chose for differ- ent amounts of labeled data when optimizing F1- measure.", "labels": [], "entities": [{"text": "F1- measure", "start_pos": 142, "end_pos": 153, "type": "METRIC", "confidence": 0.9627110362052917}]}, {"text": " Table 3: Results of a simple morph labeling after  segmentation with semi-supervised Morfessor.", "labels": [], "entities": []}]}