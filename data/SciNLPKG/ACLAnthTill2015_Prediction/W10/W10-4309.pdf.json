{"title": [{"text": "Towards Semi-Supervised Classification of Discourse Relations using Feature Correlations", "labels": [], "entities": [{"text": "Semi-Supervised Classification of Discourse Relations", "start_pos": 8, "end_pos": 61, "type": "TASK", "confidence": 0.7040592908859253}]}], "abstractContent": [{"text": "Two of the main corpora available for training discourse relation classifiers are the RST Discourse Treebank (RST-DT) and the Penn Discourse Treebank (PDTB), which are both based on the Wall Street Journal corpus.", "labels": [], "entities": [{"text": "discourse relation classifiers", "start_pos": 47, "end_pos": 77, "type": "TASK", "confidence": 0.6295139392217001}, {"text": "RST Discourse Treebank (RST-DT", "start_pos": 86, "end_pos": 116, "type": "DATASET", "confidence": 0.7405829548835754}, {"text": "Penn Discourse Treebank (PDTB)", "start_pos": 126, "end_pos": 156, "type": "DATASET", "confidence": 0.9348279039065043}, {"text": "Wall Street Journal corpus", "start_pos": 186, "end_pos": 212, "type": "DATASET", "confidence": 0.9266669452190399}]}, {"text": "Most recent work using discourse relation classifiers have employed fully-supervised methods on these corpora.", "labels": [], "entities": [{"text": "discourse relation classifiers", "start_pos": 23, "end_pos": 53, "type": "TASK", "confidence": 0.637543241182963}]}, {"text": "However, certain discourse relations have little labeled data, causing low classification performance for their associated classes.", "labels": [], "entities": []}, {"text": "In this paper, we attempt to tackle this problem by employing a semi-supervised method for discourse relation classification.", "labels": [], "entities": [{"text": "discourse relation classification", "start_pos": 91, "end_pos": 124, "type": "TASK", "confidence": 0.766978919506073}]}, {"text": "The proposed method is based on the analysis of feature co-occurrences in unlabeled data.", "labels": [], "entities": []}, {"text": "This information is then used as a basis to extend the feature vectors during training.", "labels": [], "entities": []}, {"text": "The proposed method is evaluated on both RST-DT and PDTB, where it significantly outperformed baseline classifiers.", "labels": [], "entities": [{"text": "PDTB", "start_pos": 52, "end_pos": 56, "type": "DATASET", "confidence": 0.8699718713760376}]}, {"text": "We believe that the proposed method is a first step towards improving classification performance , particularly for discourse relations lacking annotated data.", "labels": [], "entities": []}], "introductionContent": [{"text": "The RST Discourse Treebank (RST-DT)), based on the Rhetorical Structure Theory (RST) ( framework, and the Penn Discourse Treebank (PDTB) (, are two of the most widely-used corpora for training discourse relation classifiers.", "labels": [], "entities": [{"text": "RST Discourse Treebank (RST-DT))", "start_pos": 4, "end_pos": 36, "type": "DATASET", "confidence": 0.79857869942983}, {"text": "Rhetorical Structure Theory (RST)", "start_pos": 51, "end_pos": 84, "type": "TASK", "confidence": 0.6787374367316564}, {"text": "Penn Discourse Treebank (PDTB)", "start_pos": 106, "end_pos": 136, "type": "DATASET", "confidence": 0.9393716355164846}, {"text": "discourse relation classifiers", "start_pos": 193, "end_pos": 223, "type": "TASK", "confidence": 0.6347460746765137}]}, {"text": "They are both based on the Wall Street Journal (WSJ) corpus, although there are substantial differences in the relation taxonomy used to annotate the corpus.", "labels": [], "entities": [{"text": "Wall Street Journal (WSJ) corpus", "start_pos": 27, "end_pos": 59, "type": "DATASET", "confidence": 0.9516339557511466}]}, {"text": "These corpora have been used inmost of the recent work employing discourse relation classifiers, which are based on fully-supervised machine learning approaches.", "labels": [], "entities": [{"text": "discourse relation classifiers", "start_pos": 65, "end_pos": 95, "type": "TASK", "confidence": 0.6394359568754832}]}, {"text": "Still, when building a discourse relation classifier on either corpus, one is faced with the same practical issue: Certain relations are very prevalent, such as ELABORATION[S] (RST-DT), with more than 4000 instances, whereas other occur rarely, such as EVALUATION[N] 1 (RST-DT), with three instances, or COMPARI-SON.PRAGMATIC CONCESSION (PDTB), with 12 instances.", "labels": [], "entities": []}, {"text": "This lack of training data causes poor classification performance on the classes associated to these relations.", "labels": [], "entities": []}, {"text": "In this paper, we try to tackle this problem by using feature co-occurrence information, extracted from unlabeled data, as away to inform the classifier when unseen features are found in test vectors.", "labels": [], "entities": []}, {"text": "The advantage of the method is that it relies solely on unlabeled data, which is abundant, and cheap to collect.", "labels": [], "entities": []}, {"text": "The contributions of this paper are the following: First, we propose a semi-supervised method that exploits the abundant, freely-available unlabeled data, which is harvested for feature cooccurrence information, and used as a basis to extend feature vectors to help classification for cases where unknown features are found in test vectors.", "labels": [], "entities": []}, {"text": "Second, the proposed method is evaluated on the RST-DT and PDTB corpus, where it significantly improves F-score when trained on moderately small datasets.", "labels": [], "entities": [{"text": "PDTB corpus", "start_pos": 59, "end_pos": 70, "type": "DATASET", "confidence": 0.9266566634178162}, {"text": "F-score", "start_pos": 104, "end_pos": 111, "type": "METRIC", "confidence": 0.9986796975135803}]}, {"text": "For instance, when trained on a dataset with around 1000 instances, the proposed method increases the macro-average F-score up to 30%, compared to a baseline classifier.", "labels": [], "entities": [{"text": "F-score", "start_pos": 116, "end_pos": 123, "type": "METRIC", "confidence": 0.9259992837905884}]}], "datasetContent": [{"text": "It is worth noting that the proposed method is independent of any particular classification algorithm.", "labels": [], "entities": []}, {"text": "As our goal is strictly to evaluate the relative benefit of employing the proposed method, we select a logistic regression classifier, for its simplicity.", "labels": [], "entities": []}, {"text": "We used the multi-class logistic regression (maximum entropy model) implemented in Classias.", "labels": [], "entities": []}, {"text": "Regularization parameters are set to their default value of one.", "labels": [], "entities": []}, {"text": "Unlabeled instances are created by selecting texts of the WSJ, and segmenting them into elementary discourse units (EDUs) using our sequential discourse segmenter.", "labels": [], "entities": [{"text": "WSJ", "start_pos": 58, "end_pos": 61, "type": "DATASET", "confidence": 0.8596187829971313}]}, {"text": "As there is no segmentation tool for the PDTB framework, we assumed that feature correlation information taken from EDUs created using a RST segmenter is also useful for extending feature vectors of PDTB relations.", "labels": [], "entities": []}, {"text": "Since we are interested in measuring the overall performance of a discourse relation classifier across all relation types, we use macro-averaged F-score as the preferred evaluation metric for this task.", "labels": [], "entities": [{"text": "F-score", "start_pos": 145, "end_pos": 152, "type": "METRIC", "confidence": 0.9106127023696899}]}, {"text": "We train a multi-class logistic regression model without extending the feature vectors as a baseline method.", "labels": [], "entities": []}, {"text": "This baseline is expected to show the effect of using the proposed feature extension approach for the task of discourse relation learning.", "labels": [], "entities": [{"text": "discourse relation learning", "start_pos": 110, "end_pos": 137, "type": "TASK", "confidence": 0.7536675333976746}]}, {"text": "Experimental results on RST-DT and PDTB datasets are depicted in.", "labels": [], "entities": [{"text": "PDTB datasets", "start_pos": 35, "end_pos": 48, "type": "DATASET", "confidence": 0.9300428032875061}]}, {"text": "We observe that the proposed feature extension method outperforms the baseline for both RST-DT and PDTB datasets for the full range of training dataset sizes.", "labels": [], "entities": [{"text": "PDTB datasets", "start_pos": 99, "end_pos": 112, "type": "DATASET", "confidence": 0.8102658987045288}]}, {"text": "However, the difference between the two methods decreases as we increase the amount of training data.", "labels": [], "entities": []}, {"text": "Specifically, with 200 training instances, for RST-DT, the baseline method has a macro-averaged F-score of 0.079, whereas the the proposed method has a macro-averaged F-score of 0.159 (around 101% increase in F-score).", "labels": [], "entities": [{"text": "RST-DT", "start_pos": 47, "end_pos": 53, "type": "TASK", "confidence": 0.9469705820083618}, {"text": "F-score", "start_pos": 96, "end_pos": 103, "type": "METRIC", "confidence": 0.9695348143577576}, {"text": "F-score", "start_pos": 167, "end_pos": 174, "type": "METRIC", "confidence": 0.9532622694969177}, {"text": "F-score", "start_pos": 209, "end_pos": 216, "type": "METRIC", "confidence": 0.9985296726226807}]}, {"text": "For 1000 training instances, the F-score for RST-DT increases by 29.2%, from 0.143 to 0.185, while the F-score for PDTB increases by 27.9%, from 0.109 to 0.139.", "labels": [], "entities": [{"text": "F-score", "start_pos": 33, "end_pos": 40, "type": "METRIC", "confidence": 0.999160885810852}, {"text": "RST-DT", "start_pos": 45, "end_pos": 51, "type": "TASK", "confidence": 0.711308479309082}, {"text": "F-score", "start_pos": 103, "end_pos": 110, "type": "METRIC", "confidence": 0.9974735379219055}, {"text": "PDTB", "start_pos": 115, "end_pos": 119, "type": "DATASET", "confidence": 0.8659006953239441}]}, {"text": "However, the difference between the two methods diminishes beyond 10000 training instances.", "labels": [], "entities": []}], "tableCaptions": []}