{"title": [{"text": "Crowdsourcing Document Relevance Assessment with Mechanical Turk", "labels": [], "entities": [{"text": "Crowdsourcing Document Relevance Assessment", "start_pos": 0, "end_pos": 43, "type": "TASK", "confidence": 0.8424477577209473}]}], "abstractContent": [{"text": "We investigate human factors involved in designing effective Human Intelligence Tasks (HITs) for Amazon's Mechanical Turk 1.", "labels": [], "entities": [{"text": "Amazon's Mechanical Turk 1", "start_pos": 97, "end_pos": 123, "type": "DATASET", "confidence": 0.9347769021987915}]}, {"text": "In particular, we assess document relevance to search queries via MTurk in order to evaluate search engine accuracy.", "labels": [], "entities": [{"text": "MTurk", "start_pos": 66, "end_pos": 71, "type": "DATASET", "confidence": 0.8552026748657227}, {"text": "accuracy", "start_pos": 107, "end_pos": 115, "type": "METRIC", "confidence": 0.9691847562789917}]}, {"text": "Our study varies four human factors and measures resulting experimental outcomes of cost, time, and accuracy of the assessments.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 100, "end_pos": 108, "type": "METRIC", "confidence": 0.9989386200904846}]}, {"text": "While results are largely inconclusive, we identify important obstacles encountered, lessons learned, related work, and interesting ideas for future investigation.", "labels": [], "entities": []}], "introductionContent": [{"text": "Evaluating accuracy of new search algorithms on ever-growing information repositories has become increasingly challenging in terms of the time and expense required by traditional evaluation techniques.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 11, "end_pos": 19, "type": "METRIC", "confidence": 0.9959576725959778}]}, {"text": "In particular, while the Cranfield evaluation paradigm has proven remarkably effective for decades, enormous manual effort is involved in assessing topic relevance of many different documents to many different queries.", "labels": [], "entities": []}, {"text": "Consequently, there has been significant recent interest in developing more scalable evaluation methodology.", "labels": [], "entities": []}, {"text": "This has included developing robust accuracy metrics using few assessments (), inferring implicit relevance assessments from user behavior), more carefully selecting documents for assessment), and leveraging crowdsourcing (.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 36, "end_pos": 44, "type": "METRIC", "confidence": 0.9789971709251404}]}, {"text": "We build on this line of work to investigating crowdsourcing-based relevance assessment via MTurk.", "labels": [], "entities": [{"text": "crowdsourcing-based relevance assessment", "start_pos": 47, "end_pos": 87, "type": "TASK", "confidence": 0.7002152601877848}, {"text": "MTurk", "start_pos": 92, "end_pos": 97, "type": "DATASET", "confidence": 0.9068793654441833}]}, {"text": "While MTurk has quickly become popular as a means of obtaining data annotations quickly and inexpensively, relatively little attention has been given to addressing human-factors involved in crowdsourcing and their impact on resultant cost, time, and accuracy of the annotations obtained (.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 250, "end_pos": 258, "type": "METRIC", "confidence": 0.9977235198020935}]}, {"text": "The advent of crowdsourcing has led to many researchers, whose work might otherwise fall outside the realm of human-computer interaction (HCI), suddenly finding themselves creating HITs for MTurk and thereby directly confronting important issues of interface design and usability which could significantly impact the quality or quantity of annotations they obtain.", "labels": [], "entities": []}, {"text": "A similar observation has been made recently regarding the importance of effective HCI for obtaining quality answers from users in asocial search setting.", "labels": [], "entities": []}, {"text": "Our overarching hypothesis is that better addressing human factors in HIT design can yield significantly reduce cost, reduce time, and/or increase accuracy of the annotations obtained via crowdsourcing.", "labels": [], "entities": [{"text": "HIT design", "start_pos": 70, "end_pos": 80, "type": "TASK", "confidence": 0.9356345534324646}, {"text": "accuracy", "start_pos": 147, "end_pos": 155, "type": "METRIC", "confidence": 0.9980701804161072}]}, {"text": "Such improvement could come through a variety of complimentary effects, such as attracting more or better workers, incentivizing them to do better work, better explaining the task to be performed and reducing confusion, etc.", "labels": [], "entities": []}, {"text": "While the results of this study are largely inconclusive with regard to our experimental hypothesis, other contributions of the work are identified in the abstract above.", "labels": [], "entities": []}], "datasetContent": [{"text": "We varied four simple aspects of HIT design: \u2022 Query: <title> vs. <desc> \u2022 Terminology: HIT title of \"binary relevance judgment\" (technical) vs. \"yes/no decision\" (layman) \u2022 Pay: $0.01 vs. $0.02 \u2022 Bonus: no bonus offered vs. $0.02 The Query is clearly central to relevance assessment since it provides the annotator's primary basis for judging relevance.", "labels": [], "entities": [{"text": "Pay", "start_pos": 174, "end_pos": 177, "type": "METRIC", "confidence": 0.9738572835922241}, {"text": "relevance assessment", "start_pos": 263, "end_pos": 283, "type": "TASK", "confidence": 0.8864005506038666}]}, {"text": "Since altering a query can have enormous impact on the assessment, and because we were testing the ability of Mechanical Turk workers to replicate assessments made previously by TREC assessors, we preserved wording of the queries as they appeared in the original TREC topics (see \u00a72).", "labels": [], "entities": [{"text": "TREC topics", "start_pos": 263, "end_pos": 274, "type": "DATASET", "confidence": 0.770158588886261}]}, {"text": "We hypothesized that the greater detail found in the topic description vs. the title would improve accuracy with some corresponding increase in HIT completion time (longer query to read, at times with more stilted language, and more specific relevance criteria requiring more careful reading of documents).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 99, "end_pos": 107, "type": "METRIC", "confidence": 0.9988334774971008}, {"text": "HIT completion time", "start_pos": 144, "end_pos": 163, "type": "METRIC", "confidence": 0.9140461484591166}]}, {"text": "An alternative hypothesis would be that a very conscientious worker might take longer wrestling with a vague title query.", "labels": [], "entities": []}, {"text": "Terminology: the HIT title is arguably one of a HIT's more prominent features since it is one of the first (and often the only) description of a HIT a potential worker sees.", "labels": [], "entities": []}, {"text": "An attractive title could conceivably draw workers to a task while an unattractive one could repel them.", "labels": [], "entities": []}, {"text": "Besides the simple variation studied here, future experiments could test other aspects of title formulation.", "labels": [], "entities": [{"text": "title formulation", "start_pos": 90, "end_pos": 107, "type": "TASK", "confidence": 0.8534106314182281}]}, {"text": "For example, greater specificity as to the content of documents or topics within the HIT could attract workers that are knowledgeable or interested in a particular subject.", "labels": [], "entities": []}, {"text": "Additionally, a title that indicates a task is for research purposes might attract workers motivated to contribute to society.", "labels": [], "entities": []}, {"text": "Pay: the base pay rate has obvious implications for attracting workers and incentivizing them to do quality work.", "labels": [], "entities": []}, {"text": "While anecdotal knowledge suggested the \"going rate\" for simple HITs was about $0.02, we started at the lowest possible rate and increased from there.", "labels": [], "entities": [{"text": "going rate", "start_pos": 41, "end_pos": 51, "type": "METRIC", "confidence": 0.9730904996395111}]}, {"text": "Although higher pay rates are certainly more attractive to legitimate workers, they also tend to attract more spammers, so determining appropriate pay is something of a careful balancing act.", "labels": [], "entities": []}, {"text": "Bonus: Two important questions are 1) How does knowing that one could receive a bonus affect performance on the current HIT?, and 2) How does actually receiving a bonus affect performance on future HITs?", "labels": [], "entities": []}, {"text": "We focused on the first question.", "labels": [], "entities": []}, {"text": "When bonuses were offered, we both advertised this fact in the HIT title (see Title 4 above) and appended the following statement to the instructions: \"[b]onuses will be given for good work with good explanations of the reasoning behind your relevance assessment.\"", "labels": [], "entities": [{"text": "HIT title", "start_pos": 63, "end_pos": 72, "type": "DATASET", "confidence": 0.8526841700077057}]}, {"text": "If a worker's explanation made clear why she made the relevance judgment she did, bonuses were awarded regardless of the assessment's correctness with regard to ground truth.", "labels": [], "entities": []}, {"text": "Decisions to award bonus pay were made manually (see \u00a75).", "labels": [], "entities": [{"text": "bonus pay", "start_pos": 19, "end_pos": 28, "type": "METRIC", "confidence": 0.6350124776363373}]}, {"text": "Various factors kept constant in our study could also be interesting to investigate in future work: \u2022 Description: the worker may optionally view a brief description of the task before accepting the HIT.", "labels": [], "entities": []}, {"text": "For all HITs, our description was simply: \"(1) Decide whether a document is relevant to a topic, 2) Click 'relevant' or 'not relevant', and 3) Submit\".", "labels": [], "entities": []}, {"text": "It would be very interesting to analyze the query logs for keywords used by Workers in searching for HITs of interest.", "labels": [], "entities": []}, {"text": "Omar Alonso suggests workers should always be paid (personal communication).", "labels": [], "entities": []}, {"text": "Given the low cost involved, keeping Workers individually happy avoids the effort of having to justify rejections to angry Workers, maintains one's reputation for attracting Workers, and still allows problematic workers to be filtered out in future batches.", "labels": [], "entities": []}, {"text": "With regard to outcomes, we were principally interested in measuring accuracy, time, and expense.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 69, "end_pos": 77, "type": "METRIC", "confidence": 0.998664140701294}]}, {"text": "Base statistics, such as the completion time of a particular HIT, allowed us to compute derived statistics like averages per topic, per Worker, per Batch, per experimental variable, etc.", "labels": [], "entities": []}, {"text": "We could then also look for correlations between outcomes as well as between experimental variables and outcomes.", "labels": [], "entities": []}, {"text": "Accuracy was measured by simply computing the annotator mean accuracy with regard to \"ground truth\" binary relevance labels from NIST.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 0, "end_pos": 8, "type": "METRIC", "confidence": 0.9953882694244385}, {"text": "accuracy", "start_pos": 61, "end_pos": 69, "type": "METRIC", "confidence": 0.7427870035171509}, {"text": "NIST", "start_pos": 129, "end_pos": 133, "type": "DATASET", "confidence": 0.9303674697875977}]}, {"text": "A variety of other possibilities exist, such as deciding binary annotations by majority vote and comparing these to ground truth.", "labels": [], "entities": []}, {"text": "Recent work has explored ensemble methods for weighting and combining anno-   tations () which also could have been used like majority vote.", "labels": [], "entities": []}, {"text": "As for time, we measured HIT completion time (from acceptance to completion) and Batch completion time (from publishing the Batch to all its HITs being completed).", "labels": [], "entities": [{"text": "HIT completion time", "start_pos": 25, "end_pos": 44, "type": "METRIC", "confidence": 0.8282873034477234}, {"text": "Batch completion time", "start_pos": 81, "end_pos": 102, "type": "METRIC", "confidence": 0.7110641797383627}]}, {"text": "We only anecdotally measured our own time required to generate HIT designs, shepherd the Batches, assess outcomes, etc.", "labels": [], "entities": []}, {"text": "Cost was measured solely with respect to what was paid to Workers and does not include overhead costs charged by Amazon ( \u00a72).", "labels": [], "entities": [{"text": "Cost", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.9684850573539734}, {"text": "Amazon", "start_pos": 113, "end_pos": 119, "type": "DATASET", "confidence": 0.9394038915634155}]}, {"text": "We also did not account for the cost of our own salaries, equipment, or other indirect expenses associated with the work.", "labels": [], "entities": []}, {"text": "We performed five batch evaluations, shown in Table 3.", "labels": [], "entities": []}, {"text": "For each of the four topics shown in Table 1, five documents were assessed, and ten assessments (one per HIT) were collected for each document.", "labels": [], "entities": []}, {"text": "Each batch therefore consisted of 4 * 5 * 10 = 200 HITs, for an overall total of 1000 HITs.", "labels": [], "entities": []}, {"text": "Document length varied from 162 words to 2129 words per document (including HTML tags and single-character tokens).", "labels": [], "entities": []}, {"text": "Each HIT required the worker to make a single binary relevance judgment (i.e. relevant or non-relevant) fora given querydocument pair.", "labels": [], "entities": []}, {"text": "In all cases, \"ground truth\" was available to us in the form of prior relevance assessments created by NIST.", "labels": [], "entities": [{"text": "NIST", "start_pos": 103, "end_pos": 107, "type": "DATASET", "confidence": 0.9467036724090576}]}, {"text": "149 unique Workers com- pleted the 1000 HITs, with some Workers completing far more HITs than others).", "labels": [], "entities": []}, {"text": "We did not restrict Workers from accepting HITs from different batches, and some Workers even participated in all 5 Batches.", "labels": [], "entities": []}, {"text": "Since in some cases a single Worker assessed the same query-document pair multiple times, our results likely reflect unanticipated effects of training or fatigue (see \u00a75).", "labels": [], "entities": []}, {"text": "Statistical significance was measured via a twotailed unpaired t-test.", "labels": [], "entities": [{"text": "Statistical significance", "start_pos": 0, "end_pos": 24, "type": "METRIC", "confidence": 0.8044341206550598}]}, {"text": "The only significant outcomes observed were increase in comment length and number of comments for higher-paying or bonus batches.", "labels": [], "entities": []}, {"text": "We note p-values < 0.05 where they occur.", "labels": [], "entities": []}, {"text": "Maximum accuracy of 70.5% was achieved with Batch 3, which featured use of Title query and yes/no response.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 8, "end_pos": 16, "type": "METRIC", "confidence": 0.99931800365448}]}, {"text": "Similar accuracy of 69.5% was also achieved in both Batch 1 and 2.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 8, "end_pos": 16, "type": "METRIC", "confidence": 0.9997090697288513}, {"text": "Batch", "start_pos": 52, "end_pos": 57, "type": "DATASET", "confidence": 0.7082334756851196}]}, {"text": "Accuracy fell in Batch 4 (using the Description query) to 66.5%, and fell further to 64% in Batch 5, which featured bonuses.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 0, "end_pos": 8, "type": "METRIC", "confidence": 0.9973459243774414}]}, {"text": "With regard to varying use of Title vs. Description query (Batches 1-3,5 vs. 4), accuracy for the Title query HITs was 68.4% vs. the 66.5% reported above for Batch 4.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 81, "end_pos": 89, "type": "METRIC", "confidence": 0.9996384382247925}, {"text": "HITs", "start_pos": 110, "end_pos": 114, "type": "METRIC", "confidence": 0.7971835136413574}]}, {"text": "Thus use of Description queries was not observed to lead to more accurate assessments.", "labels": [], "entities": []}, {"text": "HIT completion time was also highest for Batch 4, with workers taking an average of 72s to complete a HIT, vs. mean HIT completion time of 63s over the four Title query batches.", "labels": [], "entities": [{"text": "HIT completion time", "start_pos": 0, "end_pos": 19, "type": "METRIC", "confidence": 0.8560892740885416}, {"text": "HIT completion time", "start_pos": 116, "end_pos": 135, "type": "METRIC", "confidence": 0.9241837461789449}]}, {"text": "The number of unique workers (UW) per Batch gives some sense of how attractive a Batch was, where a high number could alternatively suggest many workers were attracted (positive) or incentives were too weak to encourage a few Workers to do many HITs (negative).", "labels": [], "entities": [{"text": "HITs", "start_pos": 245, "end_pos": 249, "type": "METRIC", "confidence": 0.7692264318466187}]}, {"text": "UW in batches 1-4 ranged from 64-72.", "labels": [], "entities": [{"text": "UW", "start_pos": 0, "end_pos": 2, "type": "DATASET", "confidence": 0.7184029221534729}]}, {"text": "This fell to 38 UW in Batch 5 (bonus batch), perhaps indicating that workers were incentivized to do more HITs to earn bonuses.", "labels": [], "entities": []}, {"text": "At the same time that the number of workers went down, the accuracy per worker went up, with the average worker judging 3.37 documents correctly, compared to a range of 2.10 -2.20 correct answers per average worker for Batches 1-3 and 1.85 correct answers per average worker for Batch 4 (which, interestingly, had slightly more UWs than the other batches).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 59, "end_pos": 67, "type": "METRIC", "confidence": 0.9993950128555298}]}, {"text": "Recall that bonuses were awarded whenever Workers provided clear justification of their judgments (whether or not those judgments matched ground truth).", "labels": [], "entities": []}, {"text": "In 74% of these cases (17 of the 23 HITs awarded bonuses), relevance assessments were correct.", "labels": [], "entities": [{"text": "relevance", "start_pos": 59, "end_pos": 68, "type": "METRIC", "confidence": 0.982772171497345}]}, {"text": "Thus there maybe a useful correlation to exploit provided practical heuristics exist for automatically distinguishing quality feedback from spam.", "labels": [], "entities": []}, {"text": "Feedback length might serve as a more practical alternative to measuring quality while still correlating with accuracy.", "labels": [], "entities": [{"text": "Feedback length", "start_pos": 0, "end_pos": 15, "type": "METRIC", "confidence": 0.6634438037872314}, {"text": "accuracy", "start_pos": 110, "end_pos": 118, "type": "METRIC", "confidence": 0.9981889128684998}]}, {"text": "Mean comment length for Batches 2 and 5 was 38.6 and 28.1 characters per comment, whereas Batches 1, 3, and 4 had mean comment lengths of 13.9, 12.7, and 19.3 characters per comment.", "labels": [], "entities": []}, {"text": "The mean difference in comment length between Batch 2 and Batch 1 was 24.7 characters (p<0.01), 25.9 characters between Batches 2 and 3 (p<0.01), and 19.3 characters between Batches 2 and 4 (p<0.01).", "labels": [], "entities": []}, {"text": "Batch 5 and Batch 1 had a mean comment-length difference of 14.2 characters (p<0.01), and Batches 5 and 3 differed by 15.4 characters (p<0.01).", "labels": [], "entities": []}, {"text": "Thus higher-paying HITs or HITs with bonus opportunities may correlate with greater Worker effort.", "labels": [], "entities": []}, {"text": "Batches 2 (pay=$0.02) and 5 (bonus batch) garnered the highest number of comments, with each averaging 0.37 comments per HIT.", "labels": [], "entities": [{"text": "HIT", "start_pos": 121, "end_pos": 124, "type": "DATASET", "confidence": 0.9482570290565491}]}, {"text": "In contrast, Batches 1, 3, and 4 averaged only 0.21, 0.18, and 0.23 comments per HIT, or a difference of 0.16 (p<0.01 ), 0.19 (p<0.01), and 0.14 (p<0.01) comments, respectively.", "labels": [], "entities": [{"text": "HIT", "start_pos": 81, "end_pos": 84, "type": "DATASET", "confidence": 0.7895078659057617}]}], "tableCaptions": [{"text": " Table 2: Documents assessed per topic, along with \"true\"  binary relevance judgments according to official TREC  NIST annotation. Document prefixes used in table: (3  and 13) WSJ920324-, except *WSJ920323-0193*,  (68 and 78) AP901231-except *FBIS4-9978* and  *WSJ920324-0062*. Only one document, 84, was  shared across queries (3 and 13).", "labels": [], "entities": [{"text": "TREC  NIST annotation", "start_pos": 108, "end_pos": 129, "type": "DATASET", "confidence": 0.7965948780377706}, {"text": "FBIS4-9978", "start_pos": 243, "end_pos": 253, "type": "DATASET", "confidence": 0.5088721513748169}, {"text": "WSJ920324-0062", "start_pos": 261, "end_pos": 275, "type": "DATASET", "confidence": 0.9308156967163086}]}, {"text": " Table 4: Preliminary analysis 1. Column labels: #B: Number of Batches, # HITs, noB: Cost without bonuses, withB:  Cost with bonuses, Total, MeanH/B: Mean per-HIT/Batch, sdB/H: std-deviation across Batches/HITs.", "labels": [], "entities": [{"text": "Total", "start_pos": 134, "end_pos": 139, "type": "METRIC", "confidence": 0.9900079965591431}, {"text": "MeanH/B: Mean per-HIT", "start_pos": 141, "end_pos": 162, "type": "METRIC", "confidence": 0.8275890151659647}]}, {"text": " Table 5: Preliminary analysis 2. Column labels: HPW: HITs per worker, MeanH/B: Mean per-HIT/Batch, sd(H):  std-deviation (across HITs), Acc: mean worker accuracy. Feedback length is in characters.", "labels": [], "entities": [{"text": "HITs", "start_pos": 54, "end_pos": 58, "type": "METRIC", "confidence": 0.9172325134277344}, {"text": "MeanH/B: Mean per-HIT/Batch", "start_pos": 71, "end_pos": 98, "type": "METRIC", "confidence": 0.8854203447699547}, {"text": "Acc", "start_pos": 137, "end_pos": 140, "type": "METRIC", "confidence": 0.9984242916107178}, {"text": "accuracy", "start_pos": 154, "end_pos": 162, "type": "METRIC", "confidence": 0.7069337368011475}, {"text": "Feedback length", "start_pos": 164, "end_pos": 179, "type": "METRIC", "confidence": 0.8767215609550476}]}]}