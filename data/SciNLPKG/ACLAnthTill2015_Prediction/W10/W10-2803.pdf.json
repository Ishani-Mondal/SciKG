{"title": [], "abstractContent": [{"text": "In this paper, we argue in favor of reconsidering models for word meaning, using as a basis results from cognitive science on human concept representation.", "labels": [], "entities": [{"text": "human concept representation", "start_pos": 126, "end_pos": 154, "type": "TASK", "confidence": 0.765247106552124}]}, {"text": "More specifically, we argue fora more flexible representation of word meaning than the assignment of a single best-fitting dictionary sense to each occurrence: Either use dictionary senses, but view them as having fuzzy boundaries, and assume that an occurrence can activate multiple senses to different degrees.", "labels": [], "entities": []}, {"text": "Or move away from dictionary senses completely, and only model similarities between individual word usages.", "labels": [], "entities": []}, {"text": "We argue that distri-butional models provide a flexible framework for experimenting with alternative models of word meanings, and discuss example models.", "labels": [], "entities": []}], "introductionContent": [{"text": "Word sense disambiguation (WSD) is one of the oldest problems in computational linguistics and still remains challenging today.", "labels": [], "entities": [{"text": "Word sense disambiguation (WSD)", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.8501315265893936}]}, {"text": "State-of-the-art performance on WSD for WordNet senses is at only around 70-80% accuracy).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 80, "end_pos": 88, "type": "METRIC", "confidence": 0.9989783763885498}]}, {"text": "The use of coarse-grained sense groups ( ) has led to considerable advances in WSD performance, with accuracies of around 90% (.", "labels": [], "entities": [{"text": "WSD", "start_pos": 79, "end_pos": 82, "type": "TASK", "confidence": 0.9812713265419006}, {"text": "accuracies", "start_pos": 101, "end_pos": 111, "type": "METRIC", "confidence": 0.9949383735656738}]}, {"text": "But this figure averages over lemmas, and the problem remains that while WSD works well for some lemmas, others continue to be tough.", "labels": [], "entities": [{"text": "WSD", "start_pos": 73, "end_pos": 76, "type": "TASK", "confidence": 0.6821603775024414}]}, {"text": "In WSD, polysemy is typically modeled through a list of dictionary senses thought to be mutually disjoint, such that each occurrence of a word is characterized through one best-fitting dictionary sense.", "labels": [], "entities": [{"text": "WSD", "start_pos": 3, "end_pos": 6, "type": "TASK", "confidence": 0.9730017185211182}]}, {"text": "Accordingly, WSD is typically framed as a classification task.", "labels": [], "entities": [{"text": "WSD", "start_pos": 13, "end_pos": 16, "type": "TASK", "confidence": 0.9827624559402466}]}, {"text": "Interestingly, the task of assigning a single best word sense is very hard for human annotators, not just machines.", "labels": [], "entities": [{"text": "assigning a single best word sense", "start_pos": 27, "end_pos": 61, "type": "TASK", "confidence": 0.7817457020282745}]}, {"text": "In this paper we advocate the exploration of alternative computational models of word meaning.", "labels": [], "entities": []}, {"text": "After all, one possible reason for the continuing difficulty of (manual as well as automatic) word sense assignment is that the prevailing model might be suboptimal.", "labels": [], "entities": [{"text": "word sense assignment", "start_pos": 94, "end_pos": 115, "type": "TASK", "confidence": 0.7697433034578959}]}, {"text": "We explore three main hypotheses.", "labels": [], "entities": []}, {"text": "The first builds on research on the human concept representation that has shown that concepts in the human mind do notwork like sets with clear-cut boundaries; they show graded membership, and there are typical members as well as borderline cases.", "labels": [], "entities": []}, {"text": "Accordingly, (A) we will suggest that word meaning maybe better modeled using a graded notion of sense membership than through concepts with hard boundaries.", "labels": [], "entities": []}, {"text": "Second, even if senses have soft boundaries, the question remains of whether they are disjoint.", "labels": [], "entities": []}, {"text": "(B) We will argue in favor of a framework where multiple senses may apply to a single occurrence, to different degrees.", "labels": [], "entities": []}, {"text": "This can be viewed as a dynamical grouping of senses for each occurrence, in contrast to static sense groups as in . The first two hypotheses still rely on an existing sense list.", "labels": [], "entities": []}, {"text": "However, there is no universal agreement across dictionaries and across tasks on the number of senses that words have.", "labels": [], "entities": []}, {"text": "even argues that general, task-independent word senses do not exist.", "labels": [], "entities": []}, {"text": "(C) By focusing on individual occurrences (usages) of a lemma and their degree of similarity, we can model word meaning without recourse to dictionary senses.", "labels": [], "entities": []}, {"text": "In this paper, we are going to argue in favor of the use of vector space as a basis for alternative models of word meaning.", "labels": [], "entities": []}, {"text": "Vector space models have been used widely to model word sense, their central property being that proximity in space can be used to predict semantic similarity.", "labels": [], "entities": [{"text": "word sense", "start_pos": 51, "end_pos": 61, "type": "TASK", "confidence": 0.7155395299196243}]}, {"text": "By viewing word occurrences as points in vector space, we can model word meaning without recourse to senses.", "labels": [], "entities": []}, {"text": "An additional advantage of vector space models is that they are also widely used inhuman concept representation models, yielding many modeling ideas that can be exploited for computational models.", "labels": [], "entities": []}, {"text": "In Section 2 we review the evidence that word sense is a tough phenomenon to model, and we layout findings that support hypotheses (A)-(C).", "labels": [], "entities": [{"text": "word sense", "start_pos": 41, "end_pos": 51, "type": "TASK", "confidence": 0.783673107624054}]}, {"text": "Section 4 considers distributional models that represent word meaning without recourse to dictionary senses, following (C).", "labels": [], "entities": []}, {"text": "In Section 5 we discuss possibilities for embedding dictionary senses in vector space in away that respects points (A) and (B).", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": []}