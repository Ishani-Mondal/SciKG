{"title": [{"text": "An Information-Retrieval Approach to Language Modeling: Applications to Social Data", "labels": [], "entities": [{"text": "Language Modeling", "start_pos": 37, "end_pos": 54, "type": "TASK", "confidence": 0.7393506765365601}]}], "abstractContent": [{"text": "In this paper we propose the IR-LM (Information Retrieval Language Model) which is an approach to carrying out language modeling based on large volumes of", "labels": [], "entities": [{"text": "Information Retrieval Language Model)", "start_pos": 36, "end_pos": 73, "type": "TASK", "confidence": 0.8139345288276673}]}], "introductionContent": [{"text": "We describe the Information Retrieval Language Model (IR-LM) which is a novel approach to language modeling motivated by domains with constantly changing large volumes of linguistic data.", "labels": [], "entities": [{"text": "Information Retrieval Language Model (IR-LM)", "start_pos": 16, "end_pos": 60, "type": "TASK", "confidence": 0.8348096609115601}, {"text": "language modeling", "start_pos": 90, "end_pos": 107, "type": "TASK", "confidence": 0.7430115640163422}]}, {"text": "Our approach is based on information retrieval methods and constitutes a departure from the traditional statistical n-gram language modeling (SLM) approach.", "labels": [], "entities": [{"text": "information retrieval", "start_pos": 25, "end_pos": 46, "type": "TASK", "confidence": 0.7955473959445953}, {"text": "statistical n-gram language modeling (SLM)", "start_pos": 104, "end_pos": 146, "type": "TASK", "confidence": 0.7310614330427987}]}, {"text": "We believe the IR-LM is more adequate than SLM when: (a) language models need to be updated constantly, (b) very large volumes of data are constantly being generated and (c) it is possible and likely that the sentence we are trying to score has been observed in the data (albeit with small possible variations).", "labels": [], "entities": []}, {"text": "These three characteristics are inherent of social domains such as blogging and micro-blogging.", "labels": [], "entities": []}], "datasetContent": [{"text": "We carried out experiments using the blog corpus provided by Spinn3r (Burton et al).", "labels": [], "entities": [{"text": "Spinn3r", "start_pos": 61, "end_pos": 68, "type": "DATASET", "confidence": 0.734591543674469}]}, {"text": "It consists of 44 million blog posts that originated during August and September 2008 from which we selected, cleaned, normalized and segmented 2 million English language blogs.", "labels": [], "entities": []}, {"text": "We reserved the segments originating from blogs dated September 30 for testing.", "labels": [], "entities": []}, {"text": "We took 1000 segments from the test subset and for each of these segments we built a 16-hypothesis cohort (by creating 16 overlapping subsegments of the constant length from the segment).", "labels": [], "entities": []}, {"text": "We built a 5-gram SLM using a 20k word dictionary and Knesser-Ney smoothing using the SRILM toolkit).", "labels": [], "entities": [{"text": "SRILM toolkit", "start_pos": 86, "end_pos": 99, "type": "DATASET", "confidence": 0.8548458218574524}]}, {"text": "We then ranked each of the 1000 test cohorts using each of the model's n-gram levels.", "labels": [], "entities": []}, {"text": "Our goal is to determine to what extent our approach correlates with an n-gram SLM-based rescoring.", "labels": [], "entities": []}, {"text": "For testing purposes we re-ranked each of the test cohorts using the IR-LM approach.", "labels": [], "entities": [{"text": "IR-LM", "start_pos": 69, "end_pos": 74, "type": "METRIC", "confidence": 0.7636707425117493}]}, {"text": "We then compared the rankings produced by n-grams and by IR-LM for every n-gram order and several IR configurations.", "labels": [], "entities": [{"text": "IR-LM", "start_pos": 57, "end_pos": 62, "type": "METRIC", "confidence": 0.7279627919197083}]}, {"text": "For this, we computed the Spearman rank correlation coefficient (SRCC).", "labels": [], "entities": [{"text": "Spearman rank correlation coefficient (SRCC)", "start_pos": 26, "end_pos": 70, "type": "METRIC", "confidence": 0.8703657048089164}]}, {"text": "SRCC averages for each configuration are shown in table 1.", "labels": [], "entities": [{"text": "SRCC", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.642447292804718}]}, {"text": "Row 1 shows the SRCC for the best overall IR configuration and row 2 shows the SRCC for the IR configuration producing the best results for each particular n-gram model.", "labels": [], "entities": []}, {"text": "We can see that albeit simple, IR-LM can produce results consistent with a language model based on fundamentally different assumptions.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1. Spearman rank correlation coefficient for  several n-gram IR configurations", "labels": [], "entities": [{"text": "Spearman rank correlation coefficient", "start_pos": 10, "end_pos": 47, "type": "METRIC", "confidence": 0.7141580581665039}]}]}