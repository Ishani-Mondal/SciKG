{"title": [{"text": "Parallel Active Learning: Eliminating Wait Time with Minimal Staleness", "labels": [], "entities": []}], "abstractContent": [{"text": "A practical concern for Active Learning (AL) is the amount of time human experts must wait for the next instance to label.", "labels": [], "entities": [{"text": "Active Learning (AL)", "start_pos": 24, "end_pos": 44, "type": "TASK", "confidence": 0.6752254486083984}]}, {"text": "We propose a method for eliminating this wait time independent of specific learning and scoring algorithms by making scores always available for all instances, using old (stale) scores when necessary.", "labels": [], "entities": []}, {"text": "The time during which the expert is annotating is used to train models and score instances-in parallel-to maximize the recency of the scores.", "labels": [], "entities": [{"text": "recency", "start_pos": 119, "end_pos": 126, "type": "METRIC", "confidence": 0.9566279053688049}]}, {"text": "Our method can be seen as a parameterless, dynamic batch AL algorithm.", "labels": [], "entities": []}, {"text": "We analyze the amount of staleness introduced by various AL schemes and then examine the effect of the staleness on performance on a part-of-speech tagging task on the Wall Street Journal.", "labels": [], "entities": [{"text": "part-of-speech tagging task", "start_pos": 133, "end_pos": 160, "type": "TASK", "confidence": 0.7590826153755188}, {"text": "Wall Street Journal", "start_pos": 168, "end_pos": 187, "type": "DATASET", "confidence": 0.967408816019694}]}, {"text": "Empirically, the parallel AL algorithm effectively has a batch size of one and a large candidate set size but eliminates the time an annotator would have to wait fora similarly parameterized batch scheme to select instances.", "labels": [], "entities": []}, {"text": "The exact performance of our method on other tasks will depend on the relative ratios of time spent annotating, training, and scoring, but in general we expect our pa-rameterless method to perform favorably compared to batch when accounting for wait time.", "labels": [], "entities": [{"text": "scoring", "start_pos": 126, "end_pos": 133, "type": "METRIC", "confidence": 0.9513213634490967}]}], "introductionContent": [{"text": "Recent emphasis has been placed on evaluating the effectiveness of active learning (AL) based on realistic cost estimates (.", "labels": [], "entities": [{"text": "active learning (AL)", "start_pos": 67, "end_pos": 87, "type": "TASK", "confidence": 0.7321505904197693}]}, {"text": "However, to our knowledge, no previous work has included in the cost measure the amount of time that an expert annotator must wait for the active learner to provide instances.", "labels": [], "entities": []}, {"text": "In fact, according to the standard approach to cost measurement, there is no reason not to use the theoretically optimal (w.r.t. a model, training procedure, and utility function) (but intractable) approach (see . In order to more fairly compare complex and time-consuming (but presumably superior) selection algorithms with simpler (but presumably inferior) algorithms, we describe \"best-case\" (minimum, from the standpoint of the payer) and \"worstcase\" (maximum) cost scenarios for each algorithm.", "labels": [], "entities": []}, {"text": "In the best-case cost scenario, annotators are paid only for the time they spend actively annotating.", "labels": [], "entities": []}, {"text": "The worst-case cost scenario additionally assumes that annotators are always on-the-clock, either annotating or waiting for the AL framework to provide them with instances.", "labels": [], "entities": []}, {"text": "In reality, human annotators work on a schedule and are not always annotating or waiting, but in general they expect to be paid for the time they spend waiting for the next instance.", "labels": [], "entities": []}, {"text": "In some cases, the annotator is not paid directly for waiting, but there are always opportunity costs associated with time-consuming algorithms, such as time to complete a project.", "labels": [], "entities": []}, {"text": "In reality, the true cost usually lies between the two extremes.", "labels": [], "entities": []}, {"text": "However, simply analyzing only the best-case cost, as is the current practice, can be misleading, as illustrated in.", "labels": [], "entities": []}, {"text": "When excluding waiting time fora particular selection algorithm 1 (\"AL Annotation Cost Only\"), the performance is much bet- Side-by-side comparison of best-case and worstcase cost measurement scenarios reveals that not accounting for the time required by AL to select instances affects the evaluation of an AL algorithm.", "labels": [], "entities": []}, {"text": "ter than the cost of random selection (\"Random Total Cost\"), but once waiting time is accounted for (\"AL Total cost\"), the AL approach can be worse than random.", "labels": [], "entities": [{"text": "AL Total cost", "start_pos": 102, "end_pos": 115, "type": "METRIC", "confidence": 0.8472506602605184}]}, {"text": "Given only the best-case cost, this algorithm would appear to be very desirable.", "labels": [], "entities": []}, {"text": "Yet, practitioners would be much less inclined to adopt this algorithm knowing that the worst-case cost is potentially no better than random.", "labels": [], "entities": []}, {"text": "Ina sense, waiting time serves as a natural penalty for expensive selection algorithms.", "labels": [], "entities": []}, {"text": "Therefore, conclusions about the usefulness of AL selection algorithms should take both best-case and worst-case costs into consideration.", "labels": [], "entities": [{"text": "AL selection", "start_pos": 47, "end_pos": 59, "type": "TASK", "confidence": 0.877281665802002}]}, {"text": "Although it is current practice to measure only best-case costs, mention as a desideratum for practical AL algorithms the need for what they call fast selection time cycles, i.e., algorithms that minimize the amount of time annotators wait for instances.", "labels": [], "entities": []}, {"text": "They address this by employing the batch selection technique of.", "labels": [], "entities": []}, {"text": "In fact, most AL practitioners and researchers implicitly acknowledge the importance of wait time by employing batch selection.", "labels": [], "entities": []}, {"text": "However, batch selection is not a perfect solution.", "labels": [], "entities": [{"text": "batch selection", "start_pos": 9, "end_pos": 24, "type": "TASK", "confidence": 0.7818871438503265}]}, {"text": "First, using the tradtional implementation, a \"good\" batch size must be specified beforehand.", "labels": [], "entities": []}, {"text": "In research, it is easy to try multiple batch sizes, but in practice where there is only one chance with live annotators, specifying a batch size is a much more difficult problem; ideally, the batch size would beset during the process of AL.", "labels": [], "entities": []}, {"text": "Second, traditional methods use the same batch size throughout the entire learning process.", "labels": [], "entities": []}, {"text": "However, in the beginning stages of AL, models have access to very little training data and retraining is often much less costly (in terms of time) than in the latter stages of AL in which models are trained on large amounts of data.", "labels": [], "entities": [{"text": "AL", "start_pos": 36, "end_pos": 38, "type": "TASK", "confidence": 0.9794062972068787}]}, {"text": "Intuitively, small batch sizes are acceptable in the beginning stages, whereas large batch sizes are desirable in the latter stages in order to mitigate the time cost of training.", "labels": [], "entities": []}, {"text": "In fact,  mention the use of an increasing batch size to speedup their simulations, but details are scant and the choice of parameters for their approach is task-and dataset-dependent.", "labels": [], "entities": []}, {"text": "Also, the use of batch AL causes instances to be chosen without the benefit of all of the most recently annotated instances, a phenomenon we call staleness and formally define in Section 2.", "labels": [], "entities": []}, {"text": "Finally, in batch AL, the computer is left idle while the annotator is working and vice-verse.", "labels": [], "entities": []}, {"text": "We present a parallel, parameterless solution that can eliminate wait time irrespective of the scoring alogrithm and training method.", "labels": [], "entities": []}, {"text": "Our approach is based on the observation that instances can always be available for annotation if we are willing to serve instances that may have been selected without the benefit of the most recent annotations.", "labels": [], "entities": []}, {"text": "By having the computer learner do work while the annotator is busy annotating, we are able to mitigate the effects of using these older annotations.", "labels": [], "entities": []}, {"text": "The rest of this paper will proceed as follows: Section 2 defines staleness and presents a progression of four AL algorithms that strike different balances between staleness and wait time, culminating in our parallelized algorithm.", "labels": [], "entities": []}, {"text": "We explain our methodology and experimental parameters in Section 3 and then present experimental results and compare the four AL algorithms in Section 4.", "labels": [], "entities": []}, {"text": "Conclusions and future work are presented in Section 5.", "labels": [], "entities": []}], "datasetContent": [{"text": "Because the performance of the parallel algorithm and the \"worst-case\" cost analysis depend on wait time, we hold computing resources constant, running all experiments on a cluster of Dell PowerEdge M610 servers equipped with two 2.8 GHz quad-core Intel Nehalem processors and 24 GB of memory.", "labels": [], "entities": []}, {"text": "All experiments were on English part of speech (POS) tagging on the POS-tagged Wall Street Journal text in the Penn Treebank (PTB) version 3.", "labels": [], "entities": [{"text": "English part of speech (POS) tagging", "start_pos": 24, "end_pos": 60, "type": "TASK", "confidence": 0.600543487817049}, {"text": "Wall Street Journal text in the Penn Treebank (PTB) version 3", "start_pos": 79, "end_pos": 140, "type": "DATASET", "confidence": 0.9193102213052603}]}, {"text": "We use sections 2-21 as initially unannotated data and randomly select 100 sentences to seed the models.", "labels": [], "entities": []}, {"text": "We employ section 24 as the set on which tag accuracy is computed, but do not count evaluation as part of the wait time.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 45, "end_pos": 53, "type": "METRIC", "confidence": 0.9734146595001221}]}, {"text": "We use the same model for pre-annotation as for scoring.", "labels": [], "entities": []}, {"text": "We employ the return on investment (ROI) AL framework introduced by . This framework requires that one define both a cost and benefit estimate and selects instances that maximize benef it(x)\u2212cost(x) cost . For simplicity, we estimate cost as the length of a sentence.", "labels": [], "entities": []}, {"text": "Our benefit model estimates the utility of each sentence as follows: benef it(s) = \u2212 log (max t p(t|s)) where p(t|s) is the probability of a tagging given a sentence.", "labels": [], "entities": []}, {"text": "Thus, sentences having low average (in the geometric mean sense) per-tag probability are favored.", "labels": [], "entities": []}, {"text": "We use a maximum entropy Markov model to estimate these probabilities, to pre-annotate instances, and to evaluate accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 114, "end_pos": 122, "type": "METRIC", "confidence": 0.9985200762748718}]}], "tableCaptions": []}