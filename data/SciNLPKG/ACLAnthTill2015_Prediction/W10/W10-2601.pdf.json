{"title": [{"text": "Adaptive Parameters for Entity Recognition with Perceptron HMMs", "labels": [], "entities": [{"text": "Entity Recognition", "start_pos": 24, "end_pos": 42, "type": "TASK", "confidence": 0.7549434900283813}]}], "abstractContent": [{"text": "We discuss the problem of model adaptation for the task of named entity recognition with respect to the variation of label distributions in data from different domains.", "labels": [], "entities": [{"text": "model adaptation", "start_pos": 26, "end_pos": 42, "type": "TASK", "confidence": 0.7761971354484558}, {"text": "named entity recognition", "start_pos": 59, "end_pos": 83, "type": "TASK", "confidence": 0.6251288751761118}]}, {"text": "We investigate an adaptive extension of the sequence perceptron, where the adaptive component includes parameters estimated from unlabelled data in combination with background knowledge in the form of gazetteers.", "labels": [], "entities": []}, {"text": "We apply this idea empirically on adaptation experiments involving two newswire datasets from different domains and compare with other popular methods such as self training and structural correspondence learning.", "labels": [], "entities": []}], "introductionContent": [{"text": "Model adaptation is a central problem in learningbased natural language processing.", "labels": [], "entities": [{"text": "Model adaptation", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.8120672106742859}, {"text": "learningbased natural language processing", "start_pos": 41, "end_pos": 82, "type": "TASK", "confidence": 0.5770317018032074}]}, {"text": "In the typical setting a model is trained on annotated in domain, or source, data, and is used on out of domain, or target, data.", "labels": [], "entities": []}, {"text": "The main difference with respect to similar problems such as semi-supervised learning is that source and target data are not assumed to be drawn from the same distribution, which might actually differ in relevant distributional properties: topic, domain, genre, style, etc.", "labels": [], "entities": []}, {"text": "In some formulations of the problem a few target labeled data is assumed to be available.", "labels": [], "entities": []}, {"text": "However, we are interested in the casein which no labeled data is available from the target domainexcept for evaluation purposes and fine tuning of hyperparameters.", "labels": [], "entities": []}, {"text": "Most of the work in adaptation has focused so far on the input side; e.g, proposing solutions based on generating shared source-target representations ().", "labels": [], "entities": []}, {"text": "Here we focus instead on the output aspect.", "labels": [], "entities": []}, {"text": "We hypothesize that * This work was carried out while the first author was working at Yahoo!", "labels": [], "entities": []}, {"text": "Research part of the loss incurred in using a model out of domain is due to its built-in class priors which do not match the class distribution in the target data.", "labels": [], "entities": []}, {"text": "Thus we attempt to explicitly correct the prediction of a pre-trained model fora given label by taking into account a noisy estimate of the label frequency in the target data.", "labels": [], "entities": []}, {"text": "The correction is carried out by means of adaptive parameters, estimated from unlabelled target data and background \"world knowledge\" in the form of gazetteers, and taken in consideration in the decoding phase.", "labels": [], "entities": []}, {"text": "We built a suitable dataset for experimenting with different adaptation approaches for named entity recognition (NER).", "labels": [], "entities": [{"text": "named entity recognition (NER)", "start_pos": 87, "end_pos": 117, "type": "TASK", "confidence": 0.7777181913455328}]}, {"text": "The main findings from our experiments are as follows.", "labels": [], "entities": []}, {"text": "First, the problem is challenging and only marginal improvements are possible under all evaluated frameworks.", "labels": [], "entities": []}, {"text": "Second, we found that our method compares well with current state-of-the-art approaches such as self training and structural correspondence learning () and taps on an interesting aspect which seems worth of further research.", "labels": [], "entities": []}, {"text": "Although we concentrate on a segmentation task within a specific framework, the perceptron HMM introduced by, we speculate that the same intuition could be straightforwardly applied in other learning frameworks (e.g., Support Vector Machines) and different tasks (e.g., standard classification).", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 2. BBN and CoNLL datasets.", "labels": [], "entities": [{"text": "BBN", "start_pos": 10, "end_pos": 13, "type": "DATASET", "confidence": 0.9090554118156433}, {"text": "CoNLL datasets", "start_pos": 18, "end_pos": 32, "type": "DATASET", "confidence": 0.9259256720542908}]}, {"text": " Table 3. Results of baselines and adaptive models.", "labels": [], "entities": []}]}