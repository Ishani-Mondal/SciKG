{"title": [{"text": "Using entity features to classify implicit discourse relations", "labels": [], "entities": []}], "abstractContent": [{"text": "We report results on predicting the sense of implicit discourse relations between adjacent sentences in text.", "labels": [], "entities": []}, {"text": "Our investigation concentrates on the association between discourse relations and properties of the referring expressions that appear in the related sentences.", "labels": [], "entities": []}, {"text": "The properties of interest include coreference information, grammatical role, information status and syntactic form of referring expressions.", "labels": [], "entities": []}, {"text": "Predicting the sense of implicit discourse relations based on these features is considerably better than a random baseline and several of the most discriminative features conform with linguistic intuitions.", "labels": [], "entities": []}, {"text": "However , these features do not perform as well as lexical features traditionally used for sense prediction.", "labels": [], "entities": [{"text": "sense prediction", "start_pos": 91, "end_pos": 107, "type": "TASK", "confidence": 0.779442697763443}]}], "introductionContent": [{"text": "Coherent text is described in terms of discourse relations such as \"cause\" and \"contrast\" between its constituent clauses.", "labels": [], "entities": []}, {"text": "It is also characterized by entity coherence, where the connectedness of the text is created by virtue of the mentioned entities and the properties of referring expressions.", "labels": [], "entities": []}, {"text": "We aim to investigate the association between discourse relations and the way in which references to entities are realized.", "labels": [], "entities": []}, {"text": "In our work, we employ features related to entity realization to automatically identify discourse relations in text.", "labels": [], "entities": [{"text": "entity realization", "start_pos": 43, "end_pos": 61, "type": "TASK", "confidence": 0.7681235373020172}]}, {"text": "We focus on implicit relations that hold between adjacent sentences in the absence of discourse connectives such as \"because\" or \"but\".", "labels": [], "entities": []}, {"text": "Previous studies on this task have zeroed in on lexical indicators of relation sense: dependencies between words () and the semantic orientation of words (, or on general syntactic regularities ().", "labels": [], "entities": []}, {"text": "The role of entities has also been hypothesized as important for this task and entity-related features have been used alongside others.", "labels": [], "entities": []}, {"text": "Corpus studies and reading time experiments performed by have in fact demonstrated that the type of discourse relation linking two clauses influences the resolution of pronouns in them.", "labels": [], "entities": []}, {"text": "However, the predictive power of entity-related features has not been studied independently of other factors.", "labels": [], "entities": []}, {"text": "Further motivation for studying this type of features comes from new corpus evidence (, that about a quarter of all adjacent sentences are linked purely by entity coherence, solely because they talk about the same entity.", "labels": [], "entities": []}, {"text": "Entity-related features would be expected to better separate out such relations.", "labels": [], "entities": []}, {"text": "We present the first comprehensive study of the connection between entity features and discourse relations.", "labels": [], "entities": []}, {"text": "We show that there are notable differences in properties of referring expressions across the different relations.", "labels": [], "entities": []}, {"text": "Sense prediction can be done with results better than random baseline using only entity realization information.", "labels": [], "entities": [{"text": "Sense prediction", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.8466261029243469}]}, {"text": "Their performance, however, is lower than a knowledgepoor approach using only the words in the sentences as features.", "labels": [], "entities": []}, {"text": "The addition of entity features to these basic word features is also not beneficial.", "labels": [], "entities": []}], "datasetContent": [{"text": "We define five classification tasks which disambiguate if a specific PDTB relation holds between adjacent sentences.", "labels": [], "entities": []}, {"text": "In each task, we classify the relation of interest (positive) versus a category with a naturally occurring distribution of all of the other relations (negative).", "labels": [], "entities": []}, {"text": "Sentence pairs from sections 0 to 22 of WSJ are used as training data and we test on sections 23 and 24.", "labels": [], "entities": [{"text": "WSJ", "start_pos": 40, "end_pos": 43, "type": "DATASET", "confidence": 0.9255512356758118}]}, {"text": "Given the skewed distribution of positive and negative examples for each task, we randomly downsample the negative instances in the training set to be equal to the positive examples., NoRel 1 (7).", "labels": [], "entities": [{"text": "NoRel 1", "start_pos": 184, "end_pos": 191, "type": "DATASET", "confidence": 0.7534461319446564}]}, {"text": "We do not downsample our test set.", "labels": [], "entities": []}, {"text": "Instead, we evaluate our predictions on the natural distribution present in the data to get a realistic estimate of performance.", "labels": [], "entities": []}, {"text": "We train a linear SVM classifier (LIBLIN-EAR 2 ) for each task.", "labels": [], "entities": [{"text": "SVM classifier", "start_pos": 18, "end_pos": 32, "type": "TASK", "confidence": 0.8334914743900299}, {"text": "LIBLIN-EAR 2", "start_pos": 34, "end_pos": 46, "type": "METRIC", "confidence": 0.954021543264389}]}, {"text": "The optimum regularization parameter was chosen using cross validation on the training data.", "labels": [], "entities": []}], "tableCaptions": []}