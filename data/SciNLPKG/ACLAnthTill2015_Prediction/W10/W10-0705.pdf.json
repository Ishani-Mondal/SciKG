{"title": [{"text": "Rating Computer-Generated Questions with Mechanical Turk", "labels": [], "entities": [{"text": "Rating Computer-Generated Questions", "start_pos": 0, "end_pos": 35, "type": "TASK", "confidence": 0.8414090077082316}]}], "abstractContent": [{"text": "We use Amazon Mechanical Turk to rate computer-generated reading comprehension questions about Wikipedia articles.", "labels": [], "entities": []}, {"text": "Such application-specific ratings can be used to train statistical rankers to improve systems' final output, or to evaluate technologies that generate natural language.", "labels": [], "entities": []}, {"text": "We discuss the question rating scheme we developed, assess the quality of the ratings that we gathered through Amazon Mechanical Turk, and show evidence that these ratings can be used to improve question generation.", "labels": [], "entities": [{"text": "Amazon Mechanical Turk", "start_pos": 111, "end_pos": 133, "type": "DATASET", "confidence": 0.9149850209554037}, {"text": "question generation", "start_pos": 195, "end_pos": 214, "type": "TASK", "confidence": 0.7828328013420105}]}], "introductionContent": [{"text": "This paper discusses the use of Amazon Mechanical Turk (MTurk) to rate computer-generated reading comprehension questions about Wikipedia articles.", "labels": [], "entities": []}, {"text": "We have developed a question generation system) that uses the overgenerate-and-rank paradigm).", "labels": [], "entities": [{"text": "question generation", "start_pos": 20, "end_pos": 39, "type": "TASK", "confidence": 0.7310923784971237}]}, {"text": "In the the overgenerate-and-rank approach, many systemgenerated outputs are ranked in order to select higher quality outputs.", "labels": [], "entities": []}, {"text": "While the approach has had considerable success in natural language generation), it often requires human labels on system output for the purpose of learning to rank.", "labels": [], "entities": [{"text": "natural language generation", "start_pos": 51, "end_pos": 78, "type": "TASK", "confidence": 0.6680900851885477}]}, {"text": "We employ MTurk to reduce the time and cost of acquiring these labels.", "labels": [], "entities": [{"text": "MTurk", "start_pos": 10, "end_pos": 15, "type": "DATASET", "confidence": 0.7371355295181274}]}, {"text": "For many problems, large labeled datasets do not exist.", "labels": [], "entities": []}, {"text": "One alternative is to build rule-based systems, but it is often difficult and time-consuming to accurately encode relevant linguistic knowledge in rules.", "labels": [], "entities": []}, {"text": "Another alternative, unsupervised or semisupervised learning, usually requires clever formulations of bias that guide the learning process; such intuitions are not always available.", "labels": [], "entities": []}, {"text": "Thus, small, application-specific labeled datasets, which can be cheaply constructed using MTurk, may provide considerable benefits by enabling the use of supervised learning.", "labels": [], "entities": []}, {"text": "In addition to using MTurk ratings to train a learned ranking component, we could also use MTurk ratings to evaluate the final top-ranked output of our system.", "labels": [], "entities": []}, {"text": "More generally, MTurk can be a useful evaluation tool for systems that output natural language (e.g., systems for natural language generation, summarization, translation).", "labels": [], "entities": [{"text": "natural language generation", "start_pos": 114, "end_pos": 141, "type": "TASK", "confidence": 0.7237897912661234}, {"text": "summarization, translation", "start_pos": 143, "end_pos": 169, "type": "TASK", "confidence": 0.6067653000354767}]}, {"text": "For example, used MTurk to evaluate machine translations.", "labels": [], "entities": [{"text": "MTurk", "start_pos": 18, "end_pos": 23, "type": "DATASET", "confidence": 0.8149455785751343}, {"text": "machine translations", "start_pos": 36, "end_pos": 56, "type": "TASK", "confidence": 0.6518352925777435}]}, {"text": "MTurk facilitates the efficient measurement and understanding of errors made by such technologies, and could be used to complement automatic evaluation metrics such as BLEU) and ROUGE).", "labels": [], "entities": [{"text": "MTurk", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.6621228456497192}, {"text": "BLEU", "start_pos": 168, "end_pos": 172, "type": "METRIC", "confidence": 0.9983258843421936}, {"text": "ROUGE", "start_pos": 178, "end_pos": 183, "type": "METRIC", "confidence": 0.9714394807815552}]}, {"text": "It is true that, for our task, MTurk workers annotate computer-generated rather than humangenerated natural language.", "labels": [], "entities": []}, {"text": "Thus, the data will not be as generally useful as other types of annotations, such as parse trees, which could be used to build general purpose syntactic parsers.", "labels": [], "entities": []}, {"text": "However, for the reasons described above, we believe the use of MTurk to rate computer-generated output can be useful for the training, development, and evaluation of language technologies.", "labels": [], "entities": []}, {"text": "The remainder of the paper is organized as follows: \u00a72 and \u00a73 briefly describe the question generation system and corpora used in our experiments.", "labels": [], "entities": [{"text": "question generation", "start_pos": 83, "end_pos": 102, "type": "TASK", "confidence": 0.7539573013782501}]}, {"text": "\u00a74 provides the details of our rating scheme.", "labels": [], "entities": []}, {"text": "\u00a75 discusses the quantity, cost, speed, and quality of the ratings we gathered.", "labels": [], "entities": [{"text": "quantity", "start_pos": 17, "end_pos": 25, "type": "METRIC", "confidence": 0.9603106379508972}, {"text": "speed", "start_pos": 33, "end_pos": 38, "type": "METRIC", "confidence": 0.9973896145820618}]}, {"text": "\u00a76 presents preliminary experiments showing that the MTurk ratings improve question ranking.", "labels": [], "entities": [{"text": "MTurk", "start_pos": 53, "end_pos": 58, "type": "TASK", "confidence": 0.8467462658882141}]}, {"text": "Finally, in \u00a77, we conclude.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 2: Example computer-generated questions, along with their mean ratings from Mechanical Turk.", "labels": [], "entities": [{"text": "Mechanical Turk", "start_pos": 83, "end_pos": 98, "type": "DATASET", "confidence": 0.8135190010070801}]}]}