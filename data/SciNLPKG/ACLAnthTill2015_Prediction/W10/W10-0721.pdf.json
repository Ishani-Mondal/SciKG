{"title": [{"text": "Collecting Image Annotations Using Amazon's Mechanical Turk", "labels": [], "entities": [{"text": "Collecting Image Annotations", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.8763443430264791}, {"text": "Amazon's Mechanical Turk", "start_pos": 35, "end_pos": 59, "type": "DATASET", "confidence": 0.9132886230945587}]}], "abstractContent": [{"text": "Crowd-sourcing approaches such as Ama-zon's Mechanical Turk (MTurk) make it possible to annotate or collect large amounts of linguistic data at a relatively low cost and high speed.", "labels": [], "entities": [{"text": "Ama-zon's Mechanical Turk (MTurk", "start_pos": 34, "end_pos": 66, "type": "DATASET", "confidence": 0.723051110903422}]}, {"text": "However, MTurk offers only limited control over who is allowed to particpate in a particular task.", "labels": [], "entities": []}, {"text": "This is particularly problematic for tasks requiring free-form text entry.", "labels": [], "entities": []}, {"text": "Unlike multiple-choice tasks there is no correct answer, and therefore control items for which the correct answer is known cannot be used.", "labels": [], "entities": []}, {"text": "Furthermore, MTurk has no effective built-in mechanism to guarantee workers are proficient English writers.", "labels": [], "entities": [{"text": "MTurk", "start_pos": 13, "end_pos": 18, "type": "DATASET", "confidence": 0.8113372325897217}]}, {"text": "We describe our experience in creating corpora of images annotated with multiple one-sentence descriptions on MTurk and explore the effectiveness of different quality control strategies for collecting linguistic data using Mechanical MTurk.", "labels": [], "entities": []}, {"text": "We find that the use of a qualification test provides the highest improvement of quality, whereas refining the annotations through follow-up tasks works rather poorly.", "labels": [], "entities": []}, {"text": "Using our best setup, we construct two image corpora, totaling more than 40,000 descriptive captions for 9000 images.", "labels": [], "entities": []}], "introductionContent": [{"text": "Although many generic NLP applications can be developed by using existing corpora or text collections as test and training data, there are many areas where NLP could be useful if there was a suitable corpus available.", "labels": [], "entities": []}, {"text": "For example, computer vision researchers are becoming interested in developing methods that can predict not just the presence and location of certain objects in an image, but also the relations between objects, their attributes, or the actions and events they participate in.", "labels": [], "entities": []}, {"text": "Such information can neither be obtained from standard computer vision data sets such as the COREL collection nor from the user-provided keyword tag annotations or captions on photo-sharing sites such as Flickr.", "labels": [], "entities": [{"text": "COREL collection", "start_pos": 93, "end_pos": 109, "type": "DATASET", "confidence": 0.9264765381813049}, {"text": "Flickr", "start_pos": 204, "end_pos": 210, "type": "DATASET", "confidence": 0.9580657482147217}]}, {"text": "Similarly, although the text near an image on a website may provide cues about the entities depicted in the image, an explicit description of the image content itself is typically only provided if it is not immediately obvious to a human what is depicted (in which case we may not expect a computer vision system to be able to recognize the image content either).", "labels": [], "entities": []}, {"text": "We therefore set out to collect a corpus of images annotated with simple full-sentence descriptions of their content.", "labels": [], "entities": []}, {"text": "To obtain these descriptions, we used Amazon's Mechanical Turk (MTurk).", "labels": [], "entities": [{"text": "Amazon's Mechanical Turk (MTurk)", "start_pos": 38, "end_pos": 70, "type": "DATASET", "confidence": 0.9442274570465088}]}, {"text": "MTurk is an online framework that allows researchers to post annotation tasks, called HITs (\"Human Intelligence Task\"), then, fora small fee, be completed by thousands of anonymous non-expert users (Turkers).", "labels": [], "entities": [{"text": "MTurk", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.9240145683288574}]}, {"text": "Although MTurk has been used fora variety of tasks in NLP, our use of MTurk differs from other research in NLP that uses MTurk mostly for annotation of existing text.", "labels": [], "entities": []}, {"text": "Similar to crowdsourcing-based annotation, quality control is an essential component of crowdsourcing-based data collection efforts, and needs to be factored into the overall costs.", "labels": [], "entities": [{"text": "crowdsourcing-based data collection", "start_pos": 88, "end_pos": 123, "type": "TASK", "confidence": 0.6922496954600016}]}, {"text": "For us, the quality of the text produced by the Turkers is particularly important since we are interested in us-ing this corpus for future research at the intersection of computer vision and natural language processing.", "labels": [], "entities": []}, {"text": "However, MTurk provides limited ways to implement such quality control directly.", "labels": [], "entities": [{"text": "MTurk", "start_pos": 9, "end_pos": 14, "type": "DATASET", "confidence": 0.9177841544151306}]}, {"text": "For example, our initial experiments yielded a data set that contained many sentences that were clearly not written by native speakers.", "labels": [], "entities": []}, {"text": "We learned that several steps must betaken to ensure that Turkers both understand the task and produce quality data.", "labels": [], "entities": []}, {"text": "This paper describes our experiences with Turk (based on data collection efforts in spring and summer 2009), comparing two different approaches to quality control.", "labels": [], "entities": [{"text": "Turk", "start_pos": 42, "end_pos": 46, "type": "DATASET", "confidence": 0.8599576354026794}]}, {"text": "Although we did not set out to run a scientific experiment comparing different strategies of how to collect linguistic data on Turk, our experience points towards certain recommendations for how to collect linguistic data on Turk.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Quality control by Turkers and Experts. The three experts judged 600 sentences from each data set. 565  sentences produced by unqualified workers were also judged by three Turkers.", "labels": [], "entities": []}, {"text": " Table 2: Quality control: Agreement between Turker and Expert votes, depending on the average number of control  items the Turker voters got right.", "labels": [], "entities": []}]}