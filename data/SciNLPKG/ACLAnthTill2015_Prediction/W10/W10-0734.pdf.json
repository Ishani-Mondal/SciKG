{"title": [{"text": "Creating a Bi-lingual Entailment Corpus through Translations with Mechanical Turk: $100 fora 10-day Rush", "labels": [], "entities": []}], "abstractContent": [{"text": "This paper reports on experiments in the creation of a bilingual Textual Entailment corpus , using non-experts' workforce under strict cost and time limitations ($100, 10 days).", "labels": [], "entities": []}, {"text": "To this aim workers have been hired for translation and validation tasks, through the Crowd-Flower channel to Amazon Mechanical Turk.", "labels": [], "entities": [{"text": "translation and validation", "start_pos": 40, "end_pos": 66, "type": "TASK", "confidence": 0.681761900583903}, {"text": "Amazon Mechanical Turk", "start_pos": 110, "end_pos": 132, "type": "DATASET", "confidence": 0.904877761999766}]}, {"text": "As a result, an accurate and reliable corpus of 426 English/Spanish entailment pairs has been produced in a more cost-effective way compared to other methods for the acquisition of translations based on crowdsourcing.", "labels": [], "entities": []}, {"text": "Focus-ing on two orthogonal dimensions (i.e. reliability of annotations made by non experts, and overall corpus creation costs), we summarize the methodology we adopted, the achieved results , the main problems encountered, and the lessons learned.", "labels": [], "entities": []}], "introductionContent": [{"text": "Textual Entailment (TE) () has been proposed as a generic framework for modelling language variability.", "labels": [], "entities": [{"text": "Textual Entailment (TE)", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.788054597377777}]}, {"text": "Given a text T and an hypothesis H, the task consists in deciding if the meaning of H can be inferred from the meaning of T.", "labels": [], "entities": []}, {"text": "At the monolingual level, the great potential of integrating TE recognition (RTE) components into NLP architectures has been demonstrated in several areas, including question answering, information retrieval, information extraction, and document summarization.", "labels": [], "entities": [{"text": "TE recognition (RTE)", "start_pos": 61, "end_pos": 81, "type": "TASK", "confidence": 0.8592797040939331}, {"text": "question answering", "start_pos": 166, "end_pos": 184, "type": "TASK", "confidence": 0.8722943961620331}, {"text": "information retrieval", "start_pos": 186, "end_pos": 207, "type": "TASK", "confidence": 0.8066743314266205}, {"text": "information extraction", "start_pos": 209, "end_pos": 231, "type": "TASK", "confidence": 0.8320529758930206}, {"text": "document summarization", "start_pos": 237, "end_pos": 259, "type": "TASK", "confidence": 0.6759884357452393}]}, {"text": "In contrast, mainly due to the absence of cross-lingual TE (CLTE) recognition components, similar improvements have not been achieved yet in any cross-lingual application.", "labels": [], "entities": [{"text": "cross-lingual TE (CLTE) recognition", "start_pos": 42, "end_pos": 77, "type": "TASK", "confidence": 0.5109551300605139}]}, {"text": "Along such direction, focusing on feasibility and architectural issues, () recently proposed baseline results demonstrating the potential of a simple approach that integrates Machine Translation and monolingual TE components.", "labels": [], "entities": [{"text": "Machine Translation", "start_pos": 175, "end_pos": 194, "type": "TASK", "confidence": 0.7484137415885925}]}, {"text": "As a complementary research problem, this paper addresses the data collection issue, focusing on the definition of a fast, cheap, and reliable methodology to create CLTE corpora.", "labels": [], "entities": [{"text": "data collection", "start_pos": 62, "end_pos": 77, "type": "TASK", "confidence": 0.7147939801216125}]}, {"text": "The main motivation is that, as in many other NLP areas, the availability of large quantities of annotated data represents a critical bottleneck in the systems' development/evaluation cycle.", "labels": [], "entities": []}, {"text": "Our first step in this direction takes advantage of an already available monolingual corpus, casting the problem as a translation one.", "labels": [], "entities": []}, {"text": "The challenge consists in taking a publicly available RTE dataset of English T-H pairs (i.e. the PASCAL-RTE3 dataset 1 ), and create its English-Spanish CLTE equivalent by translating the hypotheses into Spanish.", "labels": [], "entities": [{"text": "PASCAL-RTE3 dataset 1", "start_pos": 97, "end_pos": 118, "type": "DATASET", "confidence": 0.8769787748654684}]}, {"text": "To this aim non-expert workers have been hired through the CrowdFlower 2 channel to Amazon Mechanical Turk 3 (MTurk), a crowdsourcing marketplace recently used with success fora variety of NLP tasks.", "labels": [], "entities": [{"text": "CrowdFlower 2 channel", "start_pos": 59, "end_pos": 80, "type": "DATASET", "confidence": 0.8406952818234762}, {"text": "Amazon Mechanical Turk 3 (MTurk)", "start_pos": 84, "end_pos": 116, "type": "DATASET", "confidence": 0.7746774554252625}]}, {"text": "The following sections overview our experiments, carried out under strict time (10 days) and cost ($100) limitations.", "labels": [], "entities": []}, {"text": "In particular, Section 2 describes our data acquisition process; Section 3 summarizes the successive approximations that led to the definition of our methodology, and the lessons learned at each step; Section 4 concludes the paper and provides directions for future work.", "labels": [], "entities": [{"text": "data acquisition", "start_pos": 39, "end_pos": 55, "type": "TASK", "confidence": 0.817497730255127}]}], "datasetContent": [{"text": "The overall methodology, and the definition of the HITs described in Section 2, are the result of successive approximations that took into account two correlated aspects: the quality of the collected translations, and the current limitations of the CrowdFlower service.", "labels": [], "entities": []}, {"text": "On one side, simpler, cheaper, and faster jobs launched in the beginning of our experiments had to be refined to improve the quality of the retained translations.", "labels": [], "entities": []}, {"text": "On the other side, ad-hoc solutions had to be found to cope with the limited quality control functionalities provided by CrowdFlower.", "labels": [], "entities": []}, {"text": "In particular, the lack of regional qualifications of the workers, and of any qualification tests mechanism (useful features of MTurk) raised the need of defining more controlled, but also more expensive jobs. and the rest of this section summarize the progress of our work in defining the methodology adopted, the main improvements experimented at each step, the overall costs, and the lessons learned.", "labels": [], "entities": [{"text": "MTurk", "start_pos": 128, "end_pos": 133, "type": "DATASET", "confidence": 0.5539904236793518}]}, {"text": "Step 1: a na\u00a8\u0131vena\u00a8\u0131ve approach.", "labels": [], "entities": []}, {"text": "Initially, translation/validation jobs were defined without using qualification mechanisms, giving permission to any worker to complete our HITs.", "labels": [], "entities": [{"text": "translation/validation", "start_pos": 11, "end_pos": 33, "type": "TASK", "confidence": 0.8893278042475382}]}, {"text": "In this phase, our goal was to estimate the trade-off between the required development time, the overall costs, and the quality of translations collected in the most na\u00a8\u0131vena\u00a8\u0131ve conditions.", "labels": [], "entities": []}, {"text": "As expected, the job accomplishment time was negligible, and the overall cost very low.", "labels": [], "entities": []}, {"text": "More specifically, it took about 1 hour for translating the 800 hypotheses at the cost of $12, and less than 6 hours to obtain 5 validations per each translation at the same cost of $12.", "labels": [], "entities": []}, {"text": "Nevertheless, as revealed by further experiments with the introduction of gold units, the quality of the collected translations was poor.", "labels": [], "entities": []}, {"text": "In particular, 61% of them should have been rejected, often due to gross mistakes.", "labels": [], "entities": []}, {"text": "As an example, among the collected material several translations in languages other than English revealed a massive and defective use of on-line translation tools by untrusted workers, as also observed by).", "labels": [], "entities": []}, {"text": "Step 2: reducing validation errors.", "labels": [], "entities": []}, {"text": "A first improvement addressed the validation phase, where we introduced gold units as a mechanism to qualify the workers, and consequently prune the untrusted ones.", "labels": [], "entities": [{"text": "validation", "start_pos": 34, "end_pos": 44, "type": "TASK", "confidence": 0.9885452389717102}]}, {"text": "To this aim, we launched the validation HIT described in Section 2, adding around 50 English-Spanish control pairs.", "labels": [], "entities": [{"text": "validation HIT", "start_pos": 29, "end_pos": 43, "type": "TASK", "confidence": 0.7674196660518646}]}, {"text": "The pairs (equally distributed into positive and negative samples) have been extracted from the collected data, and manually checked by a Spanish native speaker.", "labels": [], "entities": []}, {"text": "The positive effect of using gold units has been verified in two ways.", "labels": [], "entities": []}, {"text": "First, we checked the quality of the translations collected in the first na\u00a8\u0131vena\u00a8\u0131ve translation job, by counting the number of rejections (61%) after running the improved validation job.", "labels": [], "entities": []}, {"text": "Then, we manually checked the quality of the translations retained with the new job.", "labels": [], "entities": []}, {"text": "A manual check on 20% of the retained translations was carried out by a Spanish native speaker, resulting in 97% Accuracy.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 113, "end_pos": 121, "type": "METRIC", "confidence": 0.9994915723800659}]}, {"text": "The 3% errors encountered are equally divided into minor translation errors, and controversial (but substantially acceptable) cases due to regional Spanish variations.", "labels": [], "entities": []}, {"text": "The considerable quality improvement observed has been obtained with a small increase of 25% in the cost (less than $3).", "labels": [], "entities": []}, {"text": "However, as regards the accomplishment time, adding the gold units to qualify workers led to a considerable increase in duration (about 4 days for the first iteration).", "labels": [], "entities": [{"text": "duration", "start_pos": 120, "end_pos": 128, "type": "METRIC", "confidence": 0.9939404726028442}]}, {"text": "This is mainly due to the high number of automatically rejected judgments, obtained from untrusted workers missing the gold units.", "labels": [], "entities": []}, {"text": "Because of the discrepancy between trusted and untrusted judgments, we faced another limitation of the CrowdFlower service, which further delayed our experiments.", "labels": [], "entities": []}, {"text": "Often, in fact, the rapid growth of untrusted judgments activates automatic pausing mechanisms, based on the assumption that gold units are not accurate.", "labels": [], "entities": []}, {"text": "This, however, is a strong assumption which does not take into account the huge amount of non-qualified workers accepting (or even just playing with) the HITs.", "labels": [], "entities": [{"text": "HITs", "start_pos": 154, "end_pos": 158, "type": "DATASET", "confidence": 0.8551450967788696}]}, {"text": "For instance, in our case the vast majority of errors came from workers located in specific regions where the native language is not Spanish nor English.", "labels": [], "entities": []}, {"text": "Step 3: reducing translation errors.", "labels": [], "entities": [{"text": "translation", "start_pos": 17, "end_pos": 28, "type": "TASK", "confidence": 0.6642576456069946}]}, {"text": "The observed improvement obtained by introducing gold units in the validation phase, led us to the definition of anew translation task, also involving a similar qualification mechanism.", "labels": [], "entities": [{"text": "validation phase", "start_pos": 67, "end_pos": 83, "type": "TASK", "confidence": 0.8916626870632172}]}, {"text": "To this aim, due to language variability, it was clearly impossible to use reference translations as gold units.", "labels": [], "entities": []}, {"text": "Taking into account the limitations of the CrowdFlower interface, which does not allow to set qualification tests or split the jobs into sequential subtasks (other effective and widely used features of MTurk), we solved the problem by defining the translation HITs as described in Section 2.", "labels": [], "entities": []}, {"text": "This solution combines a validity check and a translation task, and proved to be effective with a decrease in the translations eventually rejected (45%).", "labels": [], "entities": [{"text": "translation", "start_pos": 46, "end_pos": 57, "type": "TASK", "confidence": 0.9702568650245667}]}, {"text": "Step 4: reducing time.", "labels": [], "entities": []}, {"text": "Considering the extra time required by using gold units, we decided to spend more money on each HIT to boost the speed of our jobs.", "labels": [], "entities": [{"text": "HIT", "start_pos": 96, "end_pos": 99, "type": "DATASET", "confidence": 0.6119850277900696}]}, {"text": "In addition, to overcome the delays caused by the automatic pausing mechanism, we obtained from CrowdFlower the possibility to pose regional qualification, as commonly used in MTurk.", "labels": [], "entities": [{"text": "CrowdFlower", "start_pos": 96, "end_pos": 107, "type": "DATASET", "confidence": 0.9113696217536926}, {"text": "qualification", "start_pos": 141, "end_pos": 154, "type": "METRIC", "confidence": 0.8166332840919495}, {"text": "MTurk", "start_pos": 176, "end_pos": 181, "type": "DATASET", "confidence": 0.8739065527915955}]}, {"text": "As expected, both solutions proved to be effective, and contributed to the final definition of our methodology.", "labels": [], "entities": []}, {"text": "On one side, doubling the payment for each task (from $0.01 to $0.02 for each translation and from from $0.002 to $0.005 for each validation), we halved the required time to finish each job.", "labels": [], "entities": []}, {"text": "On the other side, by imposing the regional qualification, we eventually avoided unexpected automatic pauses.", "labels": [], "entities": [{"text": "regional qualification", "start_pos": 35, "end_pos": 57, "type": "METRIC", "confidence": 0.4493615925312042}]}], "tableCaptions": []}