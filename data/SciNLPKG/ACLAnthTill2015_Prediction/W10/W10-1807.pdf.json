{"title": [{"text": "Influence of Pre-annotation on POS-tagged Corpus Development", "labels": [], "entities": [{"text": "POS-tagged Corpus Development", "start_pos": 31, "end_pos": 60, "type": "TASK", "confidence": 0.6612705886363983}]}], "abstractContent": [{"text": "This article details a series of carefully designed experiments aiming at evaluating the influence of automatic pre-annotation on the manual part-of-speech annotation of a corpus, both from the quality and the time points of view, with a specific attention drawn to biases.", "labels": [], "entities": []}, {"text": "For this purpose, we manually annotated parts of the Penn Tree-bank corpus (Marcus et al., 1993) under various experimental setups, either from scratch or using various pre-annotations.", "labels": [], "entities": [{"text": "Penn Tree-bank corpus (Marcus et al., 1993)", "start_pos": 53, "end_pos": 96, "type": "DATASET", "confidence": 0.9539490520954133}]}, {"text": "These experiments confirm and detail the gain in quality observed before (Marcus et al., 1993; Dandapat et al., 2009; Rehbein et al., 2009), while showing that biases do appear and should betaken into account.", "labels": [], "entities": []}, {"text": "They finally demonstrate that even a not so accurate tagger can help improving annotation speed.", "labels": [], "entities": []}], "introductionContent": [{"text": "Training a machine-learning based part-of-speech (POS) tagger implies manually tagging a significant amount of text.", "labels": [], "entities": [{"text": "part-of-speech (POS) tagger", "start_pos": 34, "end_pos": 61, "type": "TASK", "confidence": 0.7016278862953186}]}, {"text": "The cost of this, in terms of human effort, slows down the development of taggers for under-resourced languages.", "labels": [], "entities": []}, {"text": "One usual way to improve this situation is to automatically pre-annotate the corpus, so that the work of the annotators is limited to the validation of this pre-annotation.", "labels": [], "entities": []}, {"text": "This method proved quite efficient in a number of POS-annotated corpus development projects, allowing fora significant gain not only in annotation time but also in consistency.", "labels": [], "entities": [{"text": "POS-annotated corpus development", "start_pos": 50, "end_pos": 82, "type": "TASK", "confidence": 0.5556311011314392}]}, {"text": "However, the influence of the pre-tagging quality on the error rate in the resulting annotated corpus and the bias introduced by the pre-annotation has been little examined.", "labels": [], "entities": []}, {"text": "This is what we propose to do here, using different parts of the Penn Treebank to train various instances of a POS tagger and experiment on pre-annotation.", "labels": [], "entities": [{"text": "Penn Treebank", "start_pos": 65, "end_pos": 78, "type": "DATASET", "confidence": 0.9960950613021851}, {"text": "POS tagger", "start_pos": 111, "end_pos": 121, "type": "TASK", "confidence": 0.5908406972885132}]}, {"text": "Our goal is to assess the impact of the quality (i.e., accuracy) of the POS tagger used for pre-annotating and to compare the use of pre-annotation with purely manual tagging, while minimizing all kinds of biases.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 55, "end_pos": 63, "type": "METRIC", "confidence": 0.999146580696106}]}, {"text": "We quantify the results in terms of error rate in the resulting annotated corpus, manual annotation time and inter-annotator agreement.", "labels": [], "entities": []}, {"text": "This article is organized as follows.", "labels": [], "entities": []}, {"text": "In Section 2, we mention some related work, while Section 3 describes the experimental setup, followed by a discussion on the obtained results (Section 4) and a conclusion.", "labels": [], "entities": []}], "datasetContent": [{"text": "The idea underlying our experiments is the following.", "labels": [], "entities": []}, {"text": "We split the Penn Treebank corpus) in a usual manner, namely we use Sections 2 to 21 to train various instances of a POS tagger, and Section 23 to perform the actual experiments.", "labels": [], "entities": [{"text": "Penn Treebank corpus", "start_pos": 13, "end_pos": 33, "type": "DATASET", "confidence": 0.9948214491208395}]}, {"text": "In order to measure the impact of the POS tagger's quality, we trained it on subcorpora of increasing sizes, and pre-annotated Section 23 with these various POS taggers.", "labels": [], "entities": [{"text": "POS tagger", "start_pos": 38, "end_pos": 48, "type": "TASK", "confidence": 0.6585850268602371}]}, {"text": "Then, we manually annotated parts of Section 23 under various experimental setups, either from scratch or using various pre-annotations, as explained below.", "labels": [], "entities": []}, {"text": "We designed different experimental setups to evaluate the impact of pre-annotation and preannotation accuracy on the quality of the resulting corpus.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 101, "end_pos": 109, "type": "METRIC", "confidence": 0.7574443817138672}]}, {"text": "The subparts of Section 23 that we used for these experiments are identified by sentence ids (e.g., 1-100 denotes the 100 first sentences in Section 23).", "labels": [], "entities": []}, {"text": "Two annotators were involved in the experiments.", "labels": [], "entities": []}, {"text": "They both have a good knowledge of linguistics, without being linguists themselves and had only little prior knowledge of the Penn Treebank POS tagset.", "labels": [], "entities": [{"text": "Penn Treebank POS tagset", "start_pos": 126, "end_pos": 150, "type": "DATASET", "confidence": 0.987121120095253}]}, {"text": "One of them had previous expertise in POS tagging (Annotator1).", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 38, "end_pos": 49, "type": "TASK", "confidence": 0.8393486440181732}]}, {"text": "It should also be noticed that, though they speak fluent English, they are not native speakers of the language.", "labels": [], "entities": []}, {"text": "They were asked to keep track of their annotation time, noting the time it took them to annotate or correct each series of 10 sentences.", "labels": [], "entities": []}, {"text": "They were also asked to use only a basic text editor, with no macro or specific feature that could help them, apart from 2 More precisely, MElt i en is trained on the i first sentences of the overall training corpus, i.e. Sections 2 to 21.", "labels": [], "entities": []}, {"text": "the usual ones, like Find, Replace, etc.", "labels": [], "entities": [{"text": "Find, Replace", "start_pos": 21, "end_pos": 34, "type": "TASK", "confidence": 0.8201837738355001}]}, {"text": "The set of 36 tags used in the Penn Treebank and quite a number of particular cases is a lotto keep in mind.", "labels": [], "entities": [{"text": "Penn Treebank", "start_pos": 31, "end_pos": 44, "type": "DATASET", "confidence": 0.9678582549095154}]}, {"text": "This implies a heavy cognitive load in shortterm memory, especially as no specific interface was used to help annotating or correcting the preannotations.", "labels": [], "entities": []}, {"text": "It was demonstrated that training improves the quality of manual annotation in a significant way as well as allows fora significant gain in time ().", "labels": [], "entities": []}, {"text": "In particular, observed that it took the Penn Treebank annotators 1 month to get fully efficient on the POS-tagging correction task, reaching a speed of 20 minutes per 1,000 words.", "labels": [], "entities": [{"text": "Penn Treebank annotators", "start_pos": 41, "end_pos": 65, "type": "DATASET", "confidence": 0.9889575640360514}, {"text": "POS-tagging correction task", "start_pos": 104, "end_pos": 131, "type": "TASK", "confidence": 0.8674685557683309}]}, {"text": "The speed of annotation in our experiments cannot be compared to this, as our annotators only annotated and corrected small samples of the Penn Treebank.", "labels": [], "entities": [{"text": "speed", "start_pos": 4, "end_pos": 9, "type": "METRIC", "confidence": 0.9666699171066284}, {"text": "Penn Treebank", "start_pos": 139, "end_pos": 152, "type": "DATASET", "confidence": 0.9762357175350189}]}, {"text": "However, the annotators' speed and correctness did improve with practice.", "labels": [], "entities": [{"text": "speed", "start_pos": 25, "end_pos": 30, "type": "METRIC", "confidence": 0.9937718510627747}, {"text": "correctness", "start_pos": 35, "end_pos": 46, "type": "METRIC", "confidence": 0.9871379137039185}]}, {"text": "As explained below, we took this learning curve into account, as previous work showed it has an significant impact on the results.", "labels": [], "entities": []}, {"text": "Also, during each experiment, sentences were annotated sequentially.", "labels": [], "entities": []}, {"text": "Moreover, the experiments were conducted in the order we describe them below.", "labels": [], "entities": []}, {"text": "For example, both annotators started their first annotation task (sentences 1-100) with sentence 1.", "labels": [], "entities": []}, {"text": "We conducted the following experiments: 1.", "labels": [], "entities": []}, {"text": "Impact of the pre-annotation accuracy on precision and inter-annotator agreement: In this experiment, we used sentences 1-400 with random pre-annotation: for each sentence, one pre-annotation is randomly selected among its possible pre-annotations (one for each tagger instance).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 29, "end_pos": 37, "type": "METRIC", "confidence": 0.9645398855209351}, {"text": "precision", "start_pos": 41, "end_pos": 50, "type": "METRIC", "confidence": 0.9993425011634827}]}, {"text": "The aim of this is to eliminate the bias caused by the annotators' learning curve.", "labels": [], "entities": []}, {"text": "Annotation time for each series of 10 consecutive sentences was gathered, as well as precision w.r.t. the reference and inter-annotator agreement (both annotators annotated sentences 1-100 and 301-400, while only one annotated 101-200 and the other 201-300).", "labels": [], "entities": [{"text": "Annotation time", "start_pos": 0, "end_pos": 15, "type": "METRIC", "confidence": 0.9692278206348419}, {"text": "precision", "start_pos": 85, "end_pos": 94, "type": "METRIC", "confidence": 0.9969936609268188}]}, {"text": "2. Impact of the pre-annotation accuracy on annotation time: This experiment is based on sentences 601-760, with pre-annotation.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 32, "end_pos": 40, "type": "METRIC", "confidence": 0.7938994765281677}, {"text": "annotation time", "start_pos": 44, "end_pos": 59, "type": "METRIC", "confidence": 0.8621933162212372}]}, {"text": "We divided them in series of 10 sentences.", "labels": [], "entities": []}, {"text": "For each series, one pre-annotation is selected (i.e., the pre-annotation produced by one of the 8 taggers), in such away that each pre-annotation is used for 2 series.", "labels": [], "entities": []}, {"text": "We measured the manual annotation time for each series and each annotator.", "labels": [], "entities": []}, {"text": "3. Bias induced by pre-annotation: In this experiment, both annotators annotated sentences 451-500 fully manually.", "labels": [], "entities": []}, {"text": "Later, they annotated sentences 451-475 with the pre-annotation from MElt ALL en (the best tagger) and sentences 476-500 with the preannotation from MElt 50 en (the second-worst tagger).", "labels": [], "entities": []}, {"text": "We then compared the fully manual annotations with those based on preannotations to check if and how they diverge from the Penn Treebank \"gold-standard\"; we also compared annotation times, in order to get a confirmation of the gain in time observed in previous experiments.", "labels": [], "entities": [{"text": "Penn Treebank \"gold-standard", "start_pos": 123, "end_pos": 151, "type": "DATASET", "confidence": 0.9762924462556839}]}], "tableCaptions": [{"text": " Table 1: Accuracy of the created taggers evaluated  on Section 23 of the Penn Treebank", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.998301088809967}, {"text": "Section 23 of the Penn Treebank", "start_pos": 56, "end_pos": 87, "type": "DATASET", "confidence": 0.8248122831185659}]}, {"text": " Table 2: Inter-annotator agreement on subcorpora", "labels": [], "entities": []}, {"text": " Table 3: Inter-annotator agreement on subcorpora  used to evaluate bias", "labels": [], "entities": []}, {"text": " Table 4: Accuracy with or without pre-annotation  with MElt ALL  en (sentences 451-475)", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9947697520256042}, {"text": "MElt ALL  en", "start_pos": 56, "end_pos": 68, "type": "METRIC", "confidence": 0.8904209534327189}]}, {"text": " Table 5: Accuracy with or without pre-annotation  with MElt 50  en (sentences 476-500)", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9940180778503418}, {"text": "MElt", "start_pos": 56, "end_pos": 60, "type": "METRIC", "confidence": 0.5660348534584045}]}, {"text": " Table 6: Excerpts of the contingency tables for  sentences 451-457 (512 tokens) with MElt ALL", "labels": [], "entities": [{"text": "MElt ALL", "start_pos": 86, "end_pos": 94, "type": "METRIC", "confidence": 0.7755583226680756}]}, {"text": " Table 7: Excerpts of the contingency tables for  sentences 476-500 (523 tokens) with MElt 50  en pre- annotation", "labels": [], "entities": [{"text": "MElt", "start_pos": 86, "end_pos": 90, "type": "METRIC", "confidence": 0.9189484715461731}]}, {"text": " Table 8: Excerpts of the contingency tables for  sentences 450-500 (1,035 tokens) without pre- annotation", "labels": [], "entities": []}]}