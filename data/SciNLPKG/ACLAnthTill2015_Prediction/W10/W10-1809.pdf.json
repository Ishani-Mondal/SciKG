{"title": [], "abstractContent": [{"text": "Many noun phrases in text are ambiguously quantified: syntax doesn't explicitly tell us whether they refer to a single entity or to several, and what portion of the set denoted by the Nbar actually takes part in the event expressed by the verb.", "labels": [], "entities": []}, {"text": "We describe this ambiguity phenomenon in terms of underspecification, or rather un-derquantification.", "labels": [], "entities": []}, {"text": "We attempt to validate the underquantification hypothesis by producing and testing an annotation scheme for quantification resolution, the aim of which is to associate a single quantifier with each noun phrase in our corpus.", "labels": [], "entities": [{"text": "quantification resolution", "start_pos": 108, "end_pos": 133, "type": "TASK", "confidence": 0.9096685349941254}]}, {"text": "1 Quantification resolution We are concerned with ambiguously quantified noun phrases (NPs) and their interpretation, as illustrated by the following examples: 1.", "labels": [], "entities": [{"text": "Quantification resolution", "start_pos": 2, "end_pos": 27, "type": "TASK", "confidence": 0.9270102977752686}]}, {"text": "Cats are mammals = All cats...", "labels": [], "entities": []}, {"text": "2. Cats have four legs = Most cats...", "labels": [], "entities": []}, {"text": "3. Cats were sleeping by the fire = Some cats...", "labels": [], "entities": []}, {"text": "4. The beans spilt out of the bag = Most/All of the beans...", "labels": [], "entities": []}, {"text": "5. Water was dripping through the ceiling = Some water...", "labels": [], "entities": []}, {"text": "We are interested in quantification resolution, that is, the process of giving an ambiguously quantified NP a formalisation which expresses a unique set relation appropriate to the semantics of the utterance.", "labels": [], "entities": [{"text": "quantification resolution", "start_pos": 21, "end_pos": 46, "type": "TASK", "confidence": 0.7377442568540573}]}, {"text": "For instance, we wish to arrive at: 6.", "labels": [], "entities": []}, {"text": "|\u03c6\u2229\u03c8| = |\u03c6| where \u03c6 is the set of all cats and \u03c8 the set of all mammals.", "labels": [], "entities": []}, {"text": "Resolving the quantification value of NPs is important for many NLP tasks.", "labels": [], "entities": []}, {"text": "Let us imagine an information extraction system having retrieved the triples 'cat-is-mammal' and 'cat-chase-mouse' for inclusion in a factual database about felines.", "labels": [], "entities": [{"text": "information extraction", "start_pos": 18, "end_pos": 40, "type": "TASK", "confidence": 0.7849768400192261}]}, {"text": "The problem with those representation-poor triples is that they do not contain the necessary information about quantification to answer such questions as 'Are all cats mammals?' or 'Do all cats chase mice?'", "labels": [], "entities": []}, {"text": "Or if they attempt to answer those queries, they give the same answer to both.", "labels": [], "entities": []}, {"text": "Ideally, we would like to annotate such triples with quantifiers which have a direct mapping to probability adverbs: 7.", "labels": [], "entities": []}, {"text": "All cats are mammals AND Tom is a cat \u2192 Tom is definitely a mammal.", "labels": [], "entities": []}, {"text": "8. Some cats chase mice AND Tom is a cat \u2192 Tom possibly chases mice.", "labels": [], "entities": []}, {"text": "Adequate quantification is also necessary for inference based on word-level entailment: an exis-tentially quantified NP can be replaced by a suitable hypernym but this is not possible in non-existential cases: (Some) cats are in my garden entails (Some) animals are in my garden but (All) cats are mammals doesn't imply that (All) animals are mammals.", "labels": [], "entities": []}, {"text": "In Herbelot (to appear), we provide a formal semantics for ambiguously quantified NPs, which relies on the idea that those NPs exhibit an under-specified quantifier, i.e. that for each NP in a corpus , a set relation can be agreed upon.", "labels": [], "entities": []}, {"text": "Our formal-isation includes a placeholder for the quantifier's set relation.", "labels": [], "entities": []}, {"text": "In line with inference requirements, we assume a threefold partitioning of the quan-tificational space, corresponding to the natural language quantifiers some, most and all (in addition to one, for the description of singular, unique entities).", "labels": [], "entities": []}, {"text": "The corresponding set relations are: 9.", "labels": [], "entities": []}, {"text": "some(\u03c6, \u03c8) is true iff 0 < |\u03c6 \u2229 \u03c8| < |\u03c6 \u2212 \u03c8| 10.", "labels": [], "entities": []}, {"text": "most(\u03c6, \u03c8) is true iff |\u03c6\u2212\u03c8| \u2264 |\u03c6\u2229\u03c8| < |\u03c6| 11.", "labels": [], "entities": []}, {"text": "all(\u03c6, \u03c8) is true iff |\u03c6 \u2229 \u03c8| = |\u03c6| This paper is an attempt to show that our for-malisation lends itself to evaluation by human annotation.", "labels": [], "entities": []}, {"text": "The labels produced will also serve as training and test sets for an automatic quantifica-tion resolution system.", "labels": [], "entities": [{"text": "quantifica-tion resolution", "start_pos": 79, "end_pos": 105, "type": "TASK", "confidence": 0.671458289027214}]}], "introductionContent": [], "datasetContent": [{"text": "We made an independence assumption between quantification value and kind value, and evaluated agreement separately for each type of annotation.", "labels": [], "entities": []}, {"text": "Intra-annotator agreement was calculated over the set of annotations produced by one of the authors.", "labels": [], "entities": []}, {"text": "The original annotation experiment was reproduced at three months' interval and Kappa was Class Kind Quantification Kappa 0.85 0.84: Inter-annotator agreements for both tasks computed between the original set and the new set.", "labels": [], "entities": [{"text": "Class Kind Quantification Kappa 0.85 0.84", "start_pos": 90, "end_pos": 131, "type": "METRIC", "confidence": 0.6228351593017578}]}, {"text": "shows results over 0.8 for both tasks, corresponding to 'perfect agreement' according to the.", "labels": [], "entities": [{"text": "perfect agreement", "start_pos": 57, "end_pos": 74, "type": "METRIC", "confidence": 0.7262155413627625}]}, {"text": "This indicates that the stability of the scheme is high.", "labels": [], "entities": []}, {"text": "shows inter-annotator agreements of over 0.6 for both tasks, which correspond to 'substantial agreement'.", "labels": [], "entities": []}, {"text": "This result must betaken with caution, though.", "labels": [], "entities": []}, {"text": "Although it shows good agreement overall, it is important to ascertain in what measure it holds for separate classes.", "labels": [], "entities": [{"text": "agreement", "start_pos": 23, "end_pos": 32, "type": "METRIC", "confidence": 0.9747475385665894}]}, {"text": "In an effort to report such per class agreement, we calculate Kappa values for each label by evaluating each class against all others collapsed together (as suggested by. indicates that substantial agreement is maintained for separate classes in the kind annotation task., however, suggests that, if agreement is perfect for the ONE and QUANT classes, it is very much lower for the SOME, MOST and ALL classes.", "labels": [], "entities": []}, {"text": "While it is clear that the latter three are the most complex to analyse, we can show that the lower results attached to them are partly due to issues related to Kappa as a measure of agreement., followed by Di proved that Kappa is subject to the effect of prevalence and that different marginal distributions can lead to very different Kappa values for the same observed agreement.", "labels": [], "entities": [{"text": "prevalence", "start_pos": 256, "end_pos": 266, "type": "METRIC", "confidence": 0.9578479528427124}]}, {"text": "It can be shown, in particular, that an unbalanced, symmetrical distribution of the data produces much lower figures than balanced or unbalanced, asymmetrical distributions because the expected agreement gets inflated.", "labels": [], "entities": []}, {"text": "Our confusion matrices indicate that our data falls into the category of unbalanced, symmetrical distribution: the classes are not evenly distributed but annotators agree on the relative prevalence of each class.", "labels": [], "entities": []}, {"text": "Moreover, in the quantification task itself, the ONE class covers roughly 50% of the data.", "labels": [], "entities": [{"text": "ONE", "start_pos": 49, "end_pos": 52, "type": "METRIC", "confidence": 0.8584945797920227}]}, {"text": "This means that, when calculating per class agree-: Per class inter-annotator agreement for the quantification annotation ment, we get an approximately balanced distribution for the ONE label and an unbalanced, but still symmetrical, distribution for the other labels.", "labels": [], "entities": []}, {"text": "This leads to the expected agreement being rather low for the ONE class and very high for the other classes.", "labels": [], "entities": [{"text": "agreement", "start_pos": 27, "end_pos": 36, "type": "METRIC", "confidence": 0.8864191174507141}]}, {"text": "reproduces the per class agreement figures obtained for the quantification task but shows, in addition, the observed and expected agreements for each label.", "labels": [], "entities": []}, {"text": "Although the observed agreement is consistently close to, or over, 0.9, the Kappa values differ widely in conjunction with expected agreement.", "labels": [], "entities": [{"text": "agreement", "start_pos": 22, "end_pos": 31, "type": "METRIC", "confidence": 0.9235709309577942}, {"text": "Kappa", "start_pos": 76, "end_pos": 81, "type": "METRIC", "confidence": 0.9421923756599426}]}, {"text": "This results in relatively low results for SOME, MOST and ALL (the QUANT label has nearly perfect agreement and therefore doesn't suffer from prevalence).: The effect of prevalence on per class agreement, quantification task.", "labels": [], "entities": [{"text": "prevalence", "start_pos": 142, "end_pos": 152, "type": "METRIC", "confidence": 0.9694822430610657}]}, {"text": "P r(a) is the observed agreement between annotators, P r(e) the expected agreement.", "labels": [], "entities": [{"text": "P r(a)", "start_pos": 0, "end_pos": 6, "type": "METRIC", "confidence": 0.9509516477584838}]}], "tableCaptions": [{"text": " Table 3: Per class inter-annotator agreement for  the kind annotation", "labels": [], "entities": []}, {"text": " Table 4: Per class inter-annotator agreement for  the quantification annotation", "labels": [], "entities": []}, {"text": " Table 5: The effect of prevalence on per class  agreement, quantification task. P r(a) is the ob- served agreement between annotators, P r(e) the  expected agreement.", "labels": [], "entities": [{"text": "prevalence", "start_pos": 24, "end_pos": 34, "type": "METRIC", "confidence": 0.9608179926872253}, {"text": "P r(a)", "start_pos": 81, "end_pos": 87, "type": "METRIC", "confidence": 0.9286314845085144}]}]}