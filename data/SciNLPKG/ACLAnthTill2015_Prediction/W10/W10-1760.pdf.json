{"title": [{"text": "Integration of Multiple Bilingually-Learned Segmentation Schemes into Statistical Machine Translation", "labels": [], "entities": [{"text": "Statistical Machine Translation", "start_pos": 70, "end_pos": 101, "type": "TASK", "confidence": 0.8063149253527323}]}], "abstractContent": [{"text": "This paper proposes an unsupervised word segmentation algorithm that identifies word boundaries in continuous source language text in order to improve the translation quality of statistical machine translation (SMT) approaches.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 36, "end_pos": 53, "type": "TASK", "confidence": 0.737823411822319}, {"text": "statistical machine translation (SMT)", "start_pos": 178, "end_pos": 215, "type": "TASK", "confidence": 0.7766428838173548}]}, {"text": "The method can be applied to any language pair where the source language is unseg-mented and the target language segmen-tation is known.", "labels": [], "entities": []}, {"text": "First, an iterative boot-strap method is applied to learn multiple segmentation schemes that are consistent with the phrasal segmentations of an SMT system trained on the resegmented bitext.", "labels": [], "entities": [{"text": "SMT", "start_pos": 145, "end_pos": 148, "type": "TASK", "confidence": 0.9831007719039917}]}, {"text": "In the second step, multiple seg-mentation schemes are integrated into a single SMT system by characterizing the source language side and merging identical translation pairs of differently segmented SMT models.", "labels": [], "entities": [{"text": "SMT", "start_pos": 80, "end_pos": 83, "type": "TASK", "confidence": 0.9858890771865845}]}, {"text": "Experimental results translating five Asian languages into English revealed that the method of integrating multiple segmentation schemes outperforms SMT models trained on any of the learned word segmentations and performs comparably to available state-of-the-art monolingually-built segmentation tools.", "labels": [], "entities": [{"text": "SMT", "start_pos": 149, "end_pos": 152, "type": "TASK", "confidence": 0.9895771741867065}]}], "introductionContent": [{"text": "The task of word segmentation, i.e., identifying word boundaries in continuous text, is one of the fundamental preprocessing steps of data-driven NLP applications like Machine Translation (MT).", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 12, "end_pos": 29, "type": "TASK", "confidence": 0.7365638166666031}, {"text": "identifying word boundaries in continuous text", "start_pos": 37, "end_pos": 83, "type": "TASK", "confidence": 0.8044881920019785}, {"text": "Machine Translation (MT)", "start_pos": 168, "end_pos": 192, "type": "TASK", "confidence": 0.8556712865829468}]}, {"text": "In contrast to Indo-European languages like English, many Asian languages like Chinese do not use a whitespace character to separate meaningful word units.", "labels": [], "entities": []}, {"text": "The problems of word segmentation are: (1) ambiguity, e.g., for Chinese, a single character can be a word component in one context, but a word by itself in another context.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 16, "end_pos": 33, "type": "TASK", "confidence": 0.742800772190094}]}, {"text": "(2) unknown words, i.e., existing words can be combined into new words such as proper nouns, e.g. \"White House\".", "labels": [], "entities": []}, {"text": "Purely dictionary-based approaches like) addressed these problems by maximum matching heuristics.", "labels": [], "entities": []}, {"text": "Recent research on unsupervised word segmentation focuses on approaches based on probabilistic methods.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 32, "end_pos": 49, "type": "TASK", "confidence": 0.7227822095155716}]}, {"text": "For example,) proposes a probabilistic segmentation model based on unigram word distributions, whereas) uses standard n-gram language models.", "labels": [], "entities": []}, {"text": "An alternative nonparametric Bayesian inference approach based on the Dirichlet process incorporating unigram and bigram word dependencies is introduced in).", "labels": [], "entities": []}, {"text": "The focus of this paper, however, is to learn word segmentations that are consistent with phrasal segmentations of SMT translation models.", "labels": [], "entities": [{"text": "word segmentations", "start_pos": 46, "end_pos": 64, "type": "TASK", "confidence": 0.7140598893165588}, {"text": "SMT translation", "start_pos": 115, "end_pos": 130, "type": "TASK", "confidence": 0.9847958981990814}]}, {"text": "In case of small translation units, e.g. single Chinese or Japanese characters, it is likely that such tokens have been seen in the training corpus, thus these tokens can be translated by an SMT engine.", "labels": [], "entities": [{"text": "SMT", "start_pos": 191, "end_pos": 194, "type": "TASK", "confidence": 0.9820272326469421}]}, {"text": "However, the contextual information provided by these tokens might not be enough to obtain a good translation.", "labels": [], "entities": [{"text": "translation", "start_pos": 98, "end_pos": 109, "type": "TASK", "confidence": 0.9599945545196533}]}, {"text": "For example, a Japanese-English SMT engine might translate the two successive characters \" \" (\"white\") and \"\u00a1 \" (\"bird\") as \"white bird\", while a human would translate \" \u00a1 \" as \"swan\".", "labels": [], "entities": [{"text": "SMT", "start_pos": 32, "end_pos": 35, "type": "TASK", "confidence": 0.9435532093048096}]}, {"text": "Therefore, the longer the translation unit, the more context can be exploited to find a meaningful translation.", "labels": [], "entities": []}, {"text": "On the other hand, the longer the translation unit, the less likely it is that such a token will occur in the training data due to data sparseness of the language resources utilized to train the statistical translation models.", "labels": [], "entities": []}, {"text": "Therefore, a word segmentation that is \"consistent with SMT models\" is one that identifies translation units that are small enough to be translatable, but large enough to be meaningful in the context of the given input sentence, achieving a trade-off between the coverage and the translation task complexity of the statistical models in order to improve translation quality.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 13, "end_pos": 30, "type": "TASK", "confidence": 0.7331444323062897}, {"text": "SMT", "start_pos": 56, "end_pos": 59, "type": "TASK", "confidence": 0.9835646152496338}]}, {"text": "The use of monolingual probabilistic models does not necessarily yield a better MT performance (.", "labels": [], "entities": [{"text": "MT", "start_pos": 80, "end_pos": 82, "type": "TASK", "confidence": 0.9750667810440063}]}, {"text": "However, improvements have been reported for approaches taking into account not only monolingual, but also bilingual information, to derive a word segmentation suitable for SMT.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 142, "end_pos": 159, "type": "TASK", "confidence": 0.7281806468963623}, {"text": "SMT", "start_pos": 173, "end_pos": 176, "type": "TASK", "confidence": 0.9947519898414612}]}, {"text": "Due to the availability of language resources, most recent research has focused on optimizing Chinese word segmentation (CWS) for Chinese-to-English SMT.", "labels": [], "entities": [{"text": "Chinese word segmentation (CWS)", "start_pos": 94, "end_pos": 125, "type": "TASK", "confidence": 0.7250109861294428}, {"text": "SMT", "start_pos": 149, "end_pos": 152, "type": "TASK", "confidence": 0.6886396408081055}]}, {"text": "For example, ( proposes a Bayesian Semi-Supervised approach for CWS that builds on ().", "labels": [], "entities": []}, {"text": "The generative model first segments Chinese text using an off-the-shelf segmenter and then learns new word types and word distributions suitable for SMT.", "labels": [], "entities": [{"text": "SMT", "start_pos": 149, "end_pos": 152, "type": "TASK", "confidence": 0.9935230016708374}]}, {"text": "Similarly, a dynamic programmingbased variational Bayes approach using bilingual information to improve MT is proposed in.", "labels": [], "entities": [{"text": "MT", "start_pos": 104, "end_pos": 106, "type": "TASK", "confidence": 0.9935221076011658}]}, {"text": "Concerning other languages, for example, () extended Hidden-Markov-Models, where hidden ngram probabilities were affected by co-occurring words in the target language part for Japanese word segmentation.", "labels": [], "entities": [{"text": "Japanese word segmentation", "start_pos": 176, "end_pos": 202, "type": "TASK", "confidence": 0.6258786221345266}]}, {"text": "Recent research on SMT is also focusing on the usage of multiple word segmentation schemes for the source language to improve translation quality.", "labels": [], "entities": [{"text": "SMT", "start_pos": 19, "end_pos": 22, "type": "TASK", "confidence": 0.9922159314155579}]}, {"text": "For example, () combines dictionary-based and CRF-based approaches for Chinese word segmentation in order to avoid outof-vocabulary (OOV) words.", "labels": [], "entities": [{"text": "Chinese word segmentation", "start_pos": 71, "end_pos": 96, "type": "TASK", "confidence": 0.6063794593016306}]}, {"text": "Moreover, the combination of different morphological decomposition of highly inflected languages like Arabic or Finnish is proposed in (de) to reduce the data sparseness problem of SMT approaches.", "labels": [], "entities": [{"text": "SMT", "start_pos": 181, "end_pos": 184, "type": "TASK", "confidence": 0.9918274879455566}]}, {"text": "Similarly, () utilizes SMT engines trained on different word segmentation schemes and combines the translation outputs using system combination techniques as a postprocess to SMT decoding.", "labels": [], "entities": [{"text": "SMT", "start_pos": 23, "end_pos": 26, "type": "TASK", "confidence": 0.9839196801185608}, {"text": "word segmentation", "start_pos": 56, "end_pos": 73, "type": "TASK", "confidence": 0.729934573173523}, {"text": "SMT decoding", "start_pos": 175, "end_pos": 187, "type": "TASK", "confidence": 0.905573844909668}]}, {"text": "In order to integrate multiple word segmentation schemes into the SMT decoder, ( proposed to generate word lattices covering all possible segmentations of the input sentence and to decode the lattice input.", "labels": [], "entities": [{"text": "SMT", "start_pos": 66, "end_pos": 69, "type": "TASK", "confidence": 0.9541712403297424}]}, {"text": "An extended version of the lattice approach that does not require the use (and existence) of monolingual segmentation tools was proposed in where a maximum entropy model is used to assign probabilities to the segmentations of an input word to generate diverse segmentation lattices from a single automatically learned model.", "labels": [], "entities": []}, {"text": "The method of) also uses a word lattice decoding approach, but they iteratively extract multiple word segmentation schemes from the training bitext.", "labels": [], "entities": []}, {"text": "This dictionary-based approach uses heuristics based on the maximum matching algorithm to obtain an agglomeration of segments that are covered by the dictionary.", "labels": [], "entities": []}, {"text": "It uses all possible source segmentations that are consistent with the extracted dictionary to create a word lattice for decoding.", "labels": [], "entities": []}, {"text": "The method proposed in this papers differs from previous approaches in the following points: \u2022 it works for any language pair where the source language is unsegmented and the target language segmentation is known.", "labels": [], "entities": []}, {"text": "\u2022 it can be applied for the translation of a source language where no linguistically motivated word segmentation tools are available.", "labels": [], "entities": []}, {"text": "\u2022 it applies machine learning techniques to identify segmentation schemes that improve translation quality fora given language pair.", "labels": [], "entities": []}, {"text": "\u2022 it decodes directly from unsegmented text using segmentation information implicit in the phrase-table to generate the target and thus avoids issues of consistency between phrasetable and input representation.", "labels": [], "entities": []}, {"text": "\u2022 it uses segmentations at alliterative levels of the bootstrap process, rather than only those from the final iteration allowing the consideration of segmentations from many levels of granularity.", "labels": [], "entities": []}, {"text": "Word segmentations are learned using a parallel corpus by aligning character-wise source language sentences to word units separated by a whitespace in the target language.", "labels": [], "entities": [{"text": "Word segmentations", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.679212749004364}]}, {"text": "Successive characters aligned to the same target words are merged into a larger source language unit.", "labels": [], "entities": []}, {"text": "Therefore, the granularity of the translation unit is defined in the given bitext context.", "labels": [], "entities": []}, {"text": "In order to minimize the side effects of alignment errors and to achieve segmentation consistency, a Maximum-Entropy (ME) algorithm is applied to learn the source language word segmentation that is consistent with the translation model of an SMT system trained on the resegmented bitext.", "labels": [], "entities": [{"text": "source language word segmentation", "start_pos": 156, "end_pos": 189, "type": "TASK", "confidence": 0.6738537922501564}, {"text": "SMT", "start_pos": 242, "end_pos": 245, "type": "TASK", "confidence": 0.9724986553192139}]}, {"text": "The process is iterated until no further improvement in translation quality is achieved.", "labels": [], "entities": [{"text": "translation", "start_pos": 56, "end_pos": 67, "type": "TASK", "confidence": 0.9587367177009583}]}, {"text": "In order to integrate multiple word segmentation into a single SMT system, the statistical translation models trained on differently segmented source language corpora are merged by characterizing the source side of each translation model, summing up the probabilities of identical phrase translation pairs, and rescoring the merged translation model (see Section 2).", "labels": [], "entities": [{"text": "SMT", "start_pos": 63, "end_pos": 66, "type": "TASK", "confidence": 0.9889510273933411}]}, {"text": "The proposed segmentation method is applied to the translation of five Asian languages, i.e., Japanese, Korean, Thai, and two Chinese dialects (Standard Mandarin and Taiwanese Mandarin), into English.", "labels": [], "entities": [{"text": "segmentation", "start_pos": 13, "end_pos": 25, "type": "TASK", "confidence": 0.9745550155639648}]}, {"text": "The utilized language resources and the outline of the experiments are summarized in Section 3.", "labels": [], "entities": []}, {"text": "The experimental results revealed that the proposed method outperforms not only a baseline system that translates characterized source language sentences, but also all SMT models trained on any of the learned word segmentations.", "labels": [], "entities": [{"text": "SMT", "start_pos": 168, "end_pos": 171, "type": "TASK", "confidence": 0.9838790893554688}]}, {"text": "In addition, the proposed method achieves translation results comparable to SMT models trained on linguistically segmented bitext.", "labels": [], "entities": [{"text": "SMT", "start_pos": 76, "end_pos": 79, "type": "TASK", "confidence": 0.9921739101409912}]}], "datasetContent": [{"text": "The effects of using different word segmentations and integrating them into an SMT engine are investigated using the multilingual Basic Travel Expressions Corpus (BTEC), which is a collection of sentences that bilingual travel experts consider useful for people going to or coming from other countries ( summarizes the characteristics of the BTEC corpus used for the training (train) of the SMT models, the tuning of model weights and stop conditions of the iterative bootstrap method (dev), and the evaluation of translation quality (test).", "labels": [], "entities": [{"text": "SMT", "start_pos": 79, "end_pos": 82, "type": "TASK", "confidence": 0.9908822774887085}, {"text": "Basic Travel Expressions Corpus (BTEC)", "start_pos": 130, "end_pos": 168, "type": "DATASET", "confidence": 0.687822801726205}, {"text": "BTEC corpus", "start_pos": 342, "end_pos": 353, "type": "DATASET", "confidence": 0.9049070775508881}, {"text": "SMT", "start_pos": 391, "end_pos": 394, "type": "TASK", "confidence": 0.9753192663192749}]}, {"text": "Besides the number of sentences (sen) and the vocabulary (voc), the sentence length (len) is also given as the average number of words per sentence.", "labels": [], "entities": [{"text": "sentence length (len)", "start_pos": 68, "end_pos": 89, "type": "METRIC", "confidence": 0.8127466976642609}]}, {"text": "The given statistics are obtained using commonly-used linguistic segmentation tools available for the respective language, i.e., CHASEN (ja), WORDCUT (th), ICTCLAS (zh), HanTagger (ko).", "labels": [], "entities": [{"text": "WORDCUT", "start_pos": 142, "end_pos": 149, "type": "METRIC", "confidence": 0.8535672426223755}]}, {"text": "No segmentation was available for Taiwanese Mandarin and therefore no meaningful statistics could be obtained.", "labels": [], "entities": [{"text": "segmentation", "start_pos": 3, "end_pos": 15, "type": "TASK", "confidence": 0.9618973731994629}]}, {"text": "For the training of the SMT models, standard word alignment and language modeling) tools were used.", "labels": [], "entities": [{"text": "SMT", "start_pos": 24, "end_pos": 27, "type": "TASK", "confidence": 0.9957341551780701}, {"text": "word alignment", "start_pos": 45, "end_pos": 59, "type": "TASK", "confidence": 0.7529676258563995}]}, {"text": "Minimum error rate training (MERT) was used to tune the decoder's parameters and performed on the dev set using the technique proposed in.", "labels": [], "entities": [{"text": "Minimum error rate training (MERT", "start_pos": 0, "end_pos": 33, "type": "METRIC", "confidence": 0.8162582516670227}]}, {"text": "For the translation, a multi-stack phrase-based decoder was used.", "labels": [], "entities": [{"text": "translation", "start_pos": 8, "end_pos": 19, "type": "TASK", "confidence": 0.9885667562484741}]}, {"text": "For the evaluation of translation quality, we applied standard automatic metrics, i.e., BLEU) and METEOR).", "labels": [], "entities": [{"text": "translation", "start_pos": 22, "end_pos": 33, "type": "TASK", "confidence": 0.9644238948822021}, {"text": "BLEU", "start_pos": 88, "end_pos": 92, "type": "METRIC", "confidence": 0.9992067217826843}, {"text": "METEOR", "start_pos": 98, "end_pos": 104, "type": "METRIC", "confidence": 0.9889524579048157}]}, {"text": "We have tested the statistical signifcance of our results 2 using the bootstrap method reported in () that (1) performs a random sampling with replacement from the evaluation data set, (2) calculates the evaluation metric score of each engine for the sampled test sentences and the difference between the two MT system scores, (3) repeats the sampling/scoring step itera-tively, and (4) applies the Student's t-test at a significance level of 95% confidence to test whether the score differences are significant.", "labels": [], "entities": []}, {"text": "In addition, human assessment of translation quality was carried out using the Ranking metrics.", "labels": [], "entities": [{"text": "translation", "start_pos": 33, "end_pos": 44, "type": "TASK", "confidence": 0.9578596353530884}]}, {"text": "For the Ranking evaluation, a human grader was asked to \"rank each whole sentence translation from Best to Worst relative to the other choices (ties are allowed)\".", "labels": [], "entities": []}, {"text": "The Ranking scores were obtained as the average number of times that a system was judged better than any other system and the normalized ranks (NormRank) were calculated on a per-judge basis for each translation task using the method of (.", "labels": [], "entities": [{"text": "NormRank)", "start_pos": 144, "end_pos": 153, "type": "METRIC", "confidence": 0.7268007695674896}]}, {"text": "Section 3.1 compares the proposed method to the baseline system that translates characterized source language sentences and to the SMT engines that are trained on iteratively learned as well as language-dependent linguistic word segmentations.", "labels": [], "entities": [{"text": "SMT", "start_pos": 131, "end_pos": 134, "type": "TASK", "confidence": 0.9857337474822998}, {"text": "language-dependent linguistic word segmentations", "start_pos": 194, "end_pos": 242, "type": "TASK", "confidence": 0.6495503038167953}]}, {"text": "The effects of the iterative learning method are summarized in Section 3.2.", "labels": [], "entities": []}], "tableCaptions": []}