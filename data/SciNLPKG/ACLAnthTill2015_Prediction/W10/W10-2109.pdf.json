{"title": [{"text": "No sentence is too confusing to ignore", "labels": [], "entities": []}], "abstractContent": [{"text": "We consider sentences of the form No X is too Y to Z, in which X is a noun phrase, Y is an adjective phrase, and Z is a verb phrase.", "labels": [], "entities": []}, {"text": "Such constructions are ambiguous, with two possible (and opposite !) interpretations, roughly meaning either that \"Every X Zs\", or that \"No X Zs\".", "labels": [], "entities": []}, {"text": "The interpretations have been noted to depend on semantic and pragmatic factors.", "labels": [], "entities": []}, {"text": "We show here that automatic disambigua-tion of this pragmatically complex construction can be largely achieved by using features of the lexical semantic properties of the verb (i.e., Z) participating in the construction.", "labels": [], "entities": []}, {"text": "We discuss our experimental findings in the context of construction grammar, which suggests a possible account of this phenomenon.", "labels": [], "entities": [{"text": "construction grammar", "start_pos": 55, "end_pos": 75, "type": "TASK", "confidence": 0.9333978891372681}]}], "introductionContent": [], "datasetContent": [{"text": "# sentences # instances: The number of sentences containing the target construction, and the number of resulting instances.", "labels": [], "entities": []}, {"text": "We also consider specific instances of the target construction that are judged inconsistently to establish some of the causes of disagreement.", "labels": [], "entities": []}, {"text": "One of the three experts who annotated the development items (discussed in Section 3.2) also annotated twenty items selected at random from the testing data.", "labels": [], "entities": []}, {"text": "In this case two instances are judged differently than the majority judgement obtained from AMT.", "labels": [], "entities": [{"text": "AMT", "start_pos": 92, "end_pos": 95, "type": "TASK", "confidence": 0.5528555512428284}]}, {"text": "These instances are given below with the noun, adjective and verb in the target construction underlined.", "labels": [], "entities": []}, {"text": "(4) When it comes to the clash of candidates on national television, no detail, it seems, is too minor for negotiation, no risk too small to eliminate.", "labels": [], "entities": []}, {"text": "(5) Lectures by big-name Wall Street felons will show why no swindler is too big to beat the rap by peaching on small-timers.", "labels": [], "entities": []}, {"text": "For sentence (4), the AMT judgements were unanimously for the \"no\" interpretation whereas the expert annotator chose the \"every\" interpretation.", "labels": [], "entities": [{"text": "AMT", "start_pos": 22, "end_pos": 25, "type": "TASK", "confidence": 0.4688134789466858}]}, {"text": "We are uncertain as to the reason for this disagreement, but are convinced that the \"every\" interpretation is the intended one.", "labels": [], "entities": []}, {"text": "In the case of sentence, the AMT judgements were split four-three for the \"every\" and \"no\" interpretations, respectively, while the expert annotator chose the \"no\" interpretation.", "labels": [], "entities": [{"text": "AMT", "start_pos": 29, "end_pos": 32, "type": "TASK", "confidence": 0.7234399318695068}]}, {"text": "For this sentence the provided paraphrases were Every swindler can beat the rap and No swindler can beat the rap.", "labels": [], "entities": []}, {"text": "If attention in the sentence is restricted to the target construction-i.e., no swindler is too big to beat the rap by peaching on small-timers-either of the \"no\" and \"every\" interpretations is possible.", "labels": [], "entities": []}, {"text": "That is, this clause alone can mean that \"no swindler is 'big' enough to be able to beat the rap\" (the \"no\" interpretation), or that \"no swindler is 'big' enough that they are above peaching on small-timers\" (or in other words, \"every swindler is able to beat the rap by peaching on small-timers\", the \"every\" interpretation).", "labels": [], "entities": []}, {"text": "However, the intention of the sentence as the \"no\" interpretation is clear from the referral in the main clause to big-name Wall Street felons, which implies that \"big\" swindlers have not beaten the rap.", "labels": [], "entities": []}, {"text": "Since the AMT annotators may not be devoting a large amount of attention to the task, they may focus only on the target construction and not the preliminary disambiguating material.", "labels": [], "entities": []}, {"text": "In this event, they maybe choosing between the \"every\" and \"no\" interpretations based on how cynical they are of the ability (or lack thereof) of the American legal system to punish Wall Street criminals.", "labels": [], "entities": []}, {"text": "We also examine a small number of examples in the testing set which do not receive a clear majority judgement from AMT.", "labels": [], "entities": [{"text": "AMT", "start_pos": 115, "end_pos": 118, "type": "DATASET", "confidence": 0.8277117609977722}]}, {"text": "(7) No neighborhood is too remote to diminish Mr. Levine's determination to discover and announce some previously unheralded treat.", "labels": [], "entities": []}, {"text": "(8) No one is too remote anymore to be concerned about style, Ms.", "labels": [], "entities": []}, {"text": "In example (6) the author is using the target construction to express somebody else's viewpoint that \"any amount should be spent on the war against terror\".", "labels": [], "entities": []}, {"text": "Therefore the literal reading of the target construction appears to be the \"every\" interpretation.", "labels": [], "entities": []}, {"text": "However, this construction is being used rhetorically (as part of the overall sentence) to express the author's belief that \"too much money is being spent on the war against terror\", which is close in meaning to the \"no\" interpretation.", "labels": [], "entities": []}, {"text": "It appears that the annotators are split between these two readings.", "labels": [], "entities": []}, {"text": "For sentence (7) the atypicality of neighbourhood as the subject of diminish may make this instance particularly difficult for the judges.", "labels": [], "entities": []}, {"text": "Sentence (8) appears to us to be a clear example of the \"every\" interpretation.", "labels": [], "entities": []}, {"text": "The paraphrases for this usage are \"Everyone should be concerned about style\" and \"No one should be concerned about style\".", "labels": [], "entities": []}, {"text": "In this case it is possible that the judges are biased by their beliefs about whether one should be concerned about style, and that this is giving rise to the lack of agreement.", "labels": [], "entities": [{"text": "style", "start_pos": 116, "end_pos": 121, "type": "METRIC", "confidence": 0.9491920471191406}]}, {"text": "These examples illustrate that some of these usages are clearly complex for people to annotate.", "labels": [], "entities": []}, {"text": "Such complex examples may require more context to be annotated with confidence.", "labels": [], "entities": []}, {"text": "To evaluate our model we conduct a 5-fold crossvalidation experiment using the items in the test-ing dataset.", "labels": [], "entities": []}, {"text": "When partitioning the items in the testing dataset into the five parts necessary for the cross-validation experiment, we ensure that all the instances of the target construction from a single sentence are in the same part.", "labels": [], "entities": []}, {"text": "This ensures that no instance used for training is from the same sentence as an instance used for testing.", "labels": [], "entities": []}, {"text": "We further ensure that the proportion of items in each class is roughly the same in each split.", "labels": [], "entities": []}, {"text": "For each of the five runs, we linearly scale the training data to be in the range, and apply the same transformation to the testing data.", "labels": [], "entities": []}, {"text": "We train a support vector machine (LIBSVM version 2.9,) with a radial basis function kernel on the training portion in each run, setting the cost and gamma parameters using cross-validation on just the training portion, and then test the classifier on the testing portion for that run using the same parameter settings.", "labels": [], "entities": []}, {"text": "We micro-average the accuracy obtained on each of the five runs.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 21, "end_pos": 29, "type": "METRIC", "confidence": 0.9995513558387756}]}, {"text": "Finally, we repeat each 5-fold crossvalidation experiment five times, with five random splits, and report the average accuracy over these trials.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 118, "end_pos": 126, "type": "METRIC", "confidence": 0.9994719624519348}]}], "tableCaptions": [{"text": " Table 2: The number of sentences containing the  target construction, and the number of resulting in- stances.", "labels": [], "entities": []}, {"text": " Table 3: % accuracy on testing data for each exper- imental condition and the majority baseline. Ac- curacies in boldface are statistically significantly  different from the baseline.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 12, "end_pos": 20, "type": "METRIC", "confidence": 0.9988806843757629}, {"text": "Ac- curacies", "start_pos": 98, "end_pos": 110, "type": "METRIC", "confidence": 0.9649001558621725}]}]}