{"title": [{"text": "Active Learning for Constrained Dirichlet Process Mixture Models", "labels": [], "entities": [{"text": "Constrained Dirichlet Process Mixture", "start_pos": 20, "end_pos": 57, "type": "TASK", "confidence": 0.6278826147317886}]}], "abstractContent": [{"text": "Recent work applied Dirichlet Process Mixture Models to the task of verb clustering , incorporating supervision in the form of must-links and cannot-links constraints between instances.", "labels": [], "entities": [{"text": "verb clustering", "start_pos": 68, "end_pos": 83, "type": "TASK", "confidence": 0.7179483622312546}]}, {"text": "In this work, we introduce an active learning approach for constraint selection employing uncertainty-based sampling.", "labels": [], "entities": [{"text": "constraint selection", "start_pos": 59, "end_pos": 79, "type": "TASK", "confidence": 0.7563174366950989}]}, {"text": "We achieve substantial improvements over random selection on two datasets.", "labels": [], "entities": []}], "introductionContent": [{"text": "Bayesian non-parametric mixture models have the attractive property that the number of components used to model the data is not fixed in advance but is determined by the model and the data.", "labels": [], "entities": []}, {"text": "This property is particularly interesting for NLP where many tasks are aimed at discovering novel information.", "labels": [], "entities": []}, {"text": "Recent work has applied such models to various tasks with promising results, e.g. and.", "labels": [], "entities": []}, {"text": "applied the basic model of this class, the Dirichlet Process Mixture Model (DPMM), to lexical-semantic verb clustering with encouraging results.", "labels": [], "entities": [{"text": "lexical-semantic verb clustering", "start_pos": 86, "end_pos": 118, "type": "TASK", "confidence": 0.6973478198051453}]}, {"text": "The task involves discovering classes of verbs similar in terms of their syntactic-semantic properties (e.g. MOTION class for travel, walk, run, etc.).", "labels": [], "entities": [{"text": "MOTION", "start_pos": 109, "end_pos": 115, "type": "METRIC", "confidence": 0.8848352432250977}]}, {"text": "Such classes can provide important support for other tasks, such as word sense disambiguation, parsing and semantic role labeling.)", "labels": [], "entities": [{"text": "word sense disambiguation", "start_pos": 68, "end_pos": 93, "type": "TASK", "confidence": 0.6833275953928629}, {"text": "parsing", "start_pos": 95, "end_pos": 102, "type": "TASK", "confidence": 0.9286960959434509}, {"text": "semantic role labeling", "start_pos": 107, "end_pos": 129, "type": "TASK", "confidence": 0.60597163438797}]}, {"text": "Although some fixed classifications are available these are not comprehensive and are inadequate for specific domains.", "labels": [], "entities": []}, {"text": "Furthermore, used a constrained version of the DPMM in order to guide clustering towards some prior intuition or considerations relevant to the specific task at hand.", "labels": [], "entities": []}, {"text": "This supervision was modelled as pairwise constraints between instances and it informs the model of relations between them that cannot be recovered by the model on the basis of the feature representation used.", "labels": [], "entities": []}, {"text": "Like other forms of supervision, these constraints require manual annotation and it is important to maximize the benefits obtained from it.", "labels": [], "entities": []}, {"text": "Therefore it is natural to consider active learning) in order to focus the supervision on clusterings on which the model is uncertain.", "labels": [], "entities": []}, {"text": "In this work, we propose a simple yet effective active learning method employing uncertainty based sampling.", "labels": [], "entities": []}, {"text": "The effectiveness of the AL method is demonstrated on two datasets, one of which has multiple gold standards.", "labels": [], "entities": []}], "datasetContent": [{"text": "In our experiments we used two verb clustering datasets, one from general English ( and one from the biomedical domain ().", "labels": [], "entities": []}, {"text": "In both datasets the features for each verb are its subcategorization frames (SCFs) which capture the syntactic context in which it occurs.", "labels": [], "entities": []}, {"text": "They were acquired automatically using a domain-independent statistical parsing toolkit, RASP, and a classifier which identifies verbal SCFs.", "labels": [], "entities": [{"text": "RASP", "start_pos": 89, "end_pos": 93, "type": "METRIC", "confidence": 0.7577887177467346}]}, {"text": "As a consequence, they include some noise due to standard text processing and parsing errors and due to the subtlety of the argument-adjunct distinction.", "labels": [], "entities": []}, {"text": "The general English dataset contains 204 verbs belonging to 17 fine-grained classes in taxonomy so that each class contains 12 verbs.", "labels": [], "entities": []}, {"text": "The biomedical dataset consists of 193 medium to high frequency verbs from a corpus of 2230 full-text articles from 3 biomedical journals.", "labels": [], "entities": []}, {"text": "A team of linguists and biologists created a three-level gold standard with 16, 34 and 50 classes.", "labels": [], "entities": []}, {"text": "Both datasets were pre-processed using non-negative matrix factorization which decomposes a large sparse matrix into two dense matrices (of lower dimensionality) with non-negative values.", "labels": [], "entities": []}, {"text": "In all experiments 35 dimensions were kept.", "labels": [], "entities": []}, {"text": "Preliminary experiments with different number of dimensions kept did not affect the performance substantially.", "labels": [], "entities": []}, {"text": "We evaluate our results using three information theoretic measures: Variation of Information (Meil\u02d8 a, 2007), V-measure ( and V-beta ().", "labels": [], "entities": []}, {"text": "All three assess the two desirable properties that a clustering should have with respect to a gold standard, homogeneity and completeness.", "labels": [], "entities": []}, {"text": "Homogeneity reflects the degree to which each cluster contains instances from a single class and is defined as the conditional entropy of the class distribution of the gold standard given the clustering.", "labels": [], "entities": []}, {"text": "Completeness reflects the degree to which each class is contained in a single cluster and is defined as the conditional entropy of clustering given the class distribution in the gold standard.", "labels": [], "entities": []}, {"text": "V-beta balances these properties explicitly by taking into account the ratio of the number of cluster discovered over the number of classes in the gold standard.", "labels": [], "entities": []}, {"text": "While an ideal clustering should have both properties, naively improving one of them can be harmful for the other.", "labels": [], "entities": []}, {"text": "Compared to the more commonly used F-measure (, these measures have the advantage that they do not assume a mapping between clusters and classes.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 35, "end_pos": 44, "type": "METRIC", "confidence": 0.8211015462875366}]}, {"text": "We performed experiments in order to assess the effectiveness of the AL algorithm for the constrained DPMM comparing it to random selection.", "labels": [], "entities": []}, {"text": "In each AL round, we run the Gibbs sampler for the (constrained) DPMM five times, using 100 iterations for burn-in, draw 20 samples from each run with 5 iterations lag between samples and select the most uncertain link to be labeled.", "labels": [], "entities": []}, {"text": "Following, the concentration parameter is inferred from the data using Gibbs sampling.", "labels": [], "entities": []}, {"text": "The performances were averaged across the collected samples.", "labels": [], "entities": []}, {"text": "Random selection was repeated three times.", "labels": [], "entities": [{"text": "Random selection", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.6650174111127853}]}, {"text": "The three levels of the biomedical gold standard were used independently and together with the general English dataset result in four experimental setups.", "labels": [], "entities": [{"text": "English dataset", "start_pos": 103, "end_pos": 118, "type": "DATASET", "confidence": 0.691740557551384}]}, {"text": "The comparison between AL and random selection for each dataset is shown in graphs 1(a)-1(d) using V-beta, noting that the observations made hold with all evaluation metrics used.", "labels": [], "entities": []}, {"text": "Constraints selected via AL improve the performance rapidly.", "labels": [], "entities": []}, {"text": "Indicatively, the performance reached using 1000 randomly chosen constraints is obtained using only 110 actively selected ones in the bio-50 dataset.", "labels": [], "entities": [{"text": "bio-50 dataset", "start_pos": 134, "end_pos": 148, "type": "DATASET", "confidence": 0.8775624632835388}]}, {"text": "AL performance levels out in later stages with performance superior to the one achieved using random selection with the same number of constraints.", "labels": [], "entities": []}, {"text": "The poor performance of random selection is expected, since the unsupervised DPMM predicts more than 90% of the binary links correctly.", "labels": [], "entities": []}, {"text": "Another interesting observation is that, during AL, homogeneity increased faster than completeness (graphs 1(g) and 1(h)).", "labels": [], "entities": [{"text": "AL", "start_pos": 48, "end_pos": 50, "type": "TASK", "confidence": 0.9065735936164856}]}, {"text": "This suggests that the features used lead the model towards finergrained clusters, which is further confirmed by the fact that the highest scores on the biomedical dataset are achieved when comparing against the finest-grained version of the gold standard.", "labels": [], "entities": []}, {"text": "While it is possible to choose constraints to the model that would increase completeness with respect to the gold standard, we argue that this would not allow us to obtain obtain insights on the model and the features used.", "labels": [], "entities": []}, {"text": "We also noticed that the choice of batch size has a significant effect on the learning rate of the model.", "labels": [], "entities": []}, {"text": "This phenomenon occurs in varying degrees in many applications of AL.", "labels": [], "entities": [{"text": "AL", "start_pos": 66, "end_pos": 68, "type": "TASK", "confidence": 0.9568554162979126}]}, {"text": "Manual inspection of the links chosen at each round revealed that batches often contained links involving the same instances.", "labels": [], "entities": []}, {"text": "This is expected due to transitivity: if the link between instances A and B is uncertain but the link between instances B and C is certain, then the link between A and C will be uncertain too.", "labels": [], "entities": []}, {"text": "While reducing the batch size leads to better learning rates, it requires estimating the model more often.", "labels": [], "entities": []}, {"text": "In order to ameliorate this issue, after obtaining the label of the most uncertain link, we remove the samples that disagreed with it and re-calculate the uncertainty of the remaining links given the remaining samples.", "labels": [], "entities": []}, {"text": "This is repeated until the intended batch size is reached.", "labels": [], "entities": []}, {"text": "Thus, we avoid selecting links involving the same instance, unless their uncertainty was not reduced by the constraints added.", "labels": [], "entities": []}, {"text": "A consideration that arises is that by reducing the number of samples used for uncertainty estimation, progressively we are left with fewer samples to rank the remaining links.", "labels": [], "entities": [{"text": "uncertainty estimation", "start_pos": 79, "end_pos": 101, "type": "TASK", "confidence": 0.640078529715538}]}, {"text": "Each labeled link reduces the number of samples approximately by half since the most uncertain link is likely to be a must-link in half the samples and a cannot-lnk in the remaining half.", "labels": [], "entities": []}, {"text": "As a result, fora batch with size |B| the uncertainty of the last link will be estimated using |S|/2 |B|\u22121 samples.", "labels": [], "entities": [{"text": "uncertainty", "start_pos": 42, "end_pos": 53, "type": "METRIC", "confidence": 0.9798597693443298}]}, {"text": "A crude solution would be to generate enough samples for the desired batch size.", "labels": [], "entities": []}, {"text": "However, obtaining a very large number of samples can be computationally expensive.", "labels": [], "entities": []}, {"text": "Therefore, we set a threshold for the minimum number of samples to be used to estimate the link uncertainty and when it is reached, more samples are generated using the constraints selected.", "labels": [], "entities": []}, {"text": "In graphs 1(e) and 1(f) we demonstrate the effectiveness of the batch selection method proposed (labeled \"batch\") compared to naive batch selection (labeled \"active10\").", "labels": [], "entities": []}], "tableCaptions": []}