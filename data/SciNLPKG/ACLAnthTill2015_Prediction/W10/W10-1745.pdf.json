{"title": [{"text": "CMU System Combination via Hypothesis Selection for WMT'10", "labels": [], "entities": [{"text": "WMT'10", "start_pos": 52, "end_pos": 58, "type": "TASK", "confidence": 0.5645393133163452}]}], "abstractContent": [{"text": "This paper describes the CMU entry for the system combination shared task at WMT'10.", "labels": [], "entities": [{"text": "WMT'10", "start_pos": 77, "end_pos": 83, "type": "DATASET", "confidence": 0.944866955280304}]}, {"text": "Our combination method is hypothesis selection, which uses information from n-best lists from the input MT systems , where available.", "labels": [], "entities": [{"text": "hypothesis selection", "start_pos": 26, "end_pos": 46, "type": "TASK", "confidence": 0.8274708390235901}]}, {"text": "The sentence level features used are independent from the MT systems involved.", "labels": [], "entities": [{"text": "MT", "start_pos": 58, "end_pos": 60, "type": "TASK", "confidence": 0.9535346031188965}]}, {"text": "Compared to the baseline we added source-to-target word alignment based features and trained system weights to our feature set.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 51, "end_pos": 65, "type": "TASK", "confidence": 0.683621808886528}]}, {"text": "We combined MT systems for French-English and German-English using provided data only.", "labels": [], "entities": [{"text": "MT", "start_pos": 12, "end_pos": 14, "type": "TASK", "confidence": 0.9441115856170654}]}], "introductionContent": [{"text": "For the combination of machine translation systems there have been several approaches described in recent publications.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 23, "end_pos": 42, "type": "TASK", "confidence": 0.7363801002502441}]}, {"text": "One uses confusion networks formed along a skeleton sentence to combine translation systems as described in and.", "labels": [], "entities": []}, {"text": "A different approach described in) is not keeping the skeleton fixed when aligning the systems.", "labels": [], "entities": []}, {"text": "Another approach selects whole hypotheses from a combined n-best list.", "labels": [], "entities": []}, {"text": "Our setup follows the latter approach.", "labels": [], "entities": []}, {"text": "We combine the output from the submitted translation systems, including n-best lists where available, into one joint n-best list, then calculate a set of features consistently for all hypotheses.", "labels": [], "entities": []}, {"text": "We use MER training on the provided development data to determine feature weights and re-rank the joint nbest list.", "labels": [], "entities": []}, {"text": "We train to maximize BLEU.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 21, "end_pos": 25, "type": "METRIC", "confidence": 0.9800849556922913}]}], "datasetContent": [{"text": "In the WMT shared translation task only a very small number of participants submitted n-best lists, e.g. in the German-English track there were only four n-best lists among the 16 submissions.", "labels": [], "entities": [{"text": "WMT shared translation task", "start_pos": 7, "end_pos": 34, "type": "TASK", "confidence": 0.830174520611763}]}, {"text": "Our combination method is proven to work significantly better when n-best lists are available.", "labels": [], "entities": []}, {"text": "For all our experiments on the data from WMT'09, which was available for system combination development as well as the WMT'10 shared task data we used the same setup and the same statistical models.", "labels": [], "entities": [{"text": "WMT'09", "start_pos": 41, "end_pos": 47, "type": "DATASET", "confidence": 0.9698665738105774}, {"text": "system combination development", "start_pos": 73, "end_pos": 103, "type": "TASK", "confidence": 0.7228620251019796}, {"text": "WMT'10 shared task data", "start_pos": 119, "end_pos": 142, "type": "DATASET", "confidence": 0.9048191606998444}]}, {"text": "To train our language models and word lexica we only used provided data.", "labels": [], "entities": []}, {"text": "We trained the statistical word lexica on the parallel data provided for each language pair 1 . For each combination we used three language models: a 4-gram language model trained on the English part of the parallel training data, a 1.2 giga-word 3-gram language model trained on the provided monolingual English data, and an interpolated 5-gram language model trained on the English GigaWord corpus.", "labels": [], "entities": [{"text": "English GigaWord corpus", "start_pos": 376, "end_pos": 399, "type": "DATASET", "confidence": 0.8944962819417318}]}, {"text": "We used the SRILM toolkit) for training.", "labels": [], "entities": [{"text": "SRILM toolkit", "start_pos": 12, "end_pos": 25, "type": "DATASET", "confidence": 0.848423421382904}]}, {"text": "We chose to train three separate LMs for the three corpora, so the feature weight training can automatically determine the importance of each corpus for this task.", "labels": [], "entities": []}, {"text": "The reason for training only a 3-gram LM from the wmt10 monolingual data was simply that there were not sufficient time and resources available to train a bigger model.", "labels": [], "entities": [{"text": "wmt10 monolingual data", "start_pos": 50, "end_pos": 72, "type": "DATASET", "confidence": 0.8950188954671224}]}, {"text": "For each of the two language pairs we compared a combination that used the word alignment features, or trained system weights or both of these feature groups in addition to the features described in) which serves a baseline for this set of experiments.", "labels": [], "entities": []}, {"text": "For combination we tokenized and lowercased all data, because the n-best lists were submitted in various formats.", "labels": [], "entities": []}, {"text": "Therefore we report the case insensitive scores here.", "labels": [], "entities": []}, {"text": "The combination was optimized toward the BLEU metric, therefore TER results might not be very meaningful here and are only reported for completeness.", "labels": [], "entities": [{"text": "BLEU metric", "start_pos": 41, "end_pos": 52, "type": "METRIC", "confidence": 0.9741551280021667}, {"text": "TER", "start_pos": 64, "end_pos": 67, "type": "METRIC", "confidence": 0.9987118244171143}]}], "tableCaptions": [{"text": " Table 1: French-English Results: BLEU / TER", "labels": [], "entities": [{"text": "BLEU / TER", "start_pos": 34, "end_pos": 44, "type": "METRIC", "confidence": 0.7713359793027242}]}, {"text": " Table 2: German-English Results: BLEU / TER", "labels": [], "entities": [{"text": "BLEU / TER", "start_pos": 34, "end_pos": 44, "type": "METRIC", "confidence": 0.7750027577082316}]}, {"text": " Table 4: German-English Results: BLEU / TER /  MaxSim", "labels": [], "entities": [{"text": "BLEU", "start_pos": 34, "end_pos": 38, "type": "METRIC", "confidence": 0.9987887740135193}, {"text": "TER", "start_pos": 41, "end_pos": 44, "type": "METRIC", "confidence": 0.7170782685279846}, {"text": "MaxSim", "start_pos": 48, "end_pos": 54, "type": "DATASET", "confidence": 0.7468856573104858}]}]}