{"title": [{"text": "A Chinese LPCFG Parser with Hybrid Character Information", "labels": [], "entities": [{"text": "Chinese LPCFG Parser", "start_pos": 2, "end_pos": 22, "type": "DATASET", "confidence": 0.820362369219462}]}], "abstractContent": [{"text": "We present anew probabilistic model based on the lexical PCFG model, which can easily utilize the Chinese character information to solve the lexical information sparseness in lexical PCFG model.", "labels": [], "entities": []}, {"text": "We discuss in particular some important features that can improve the parsing performance , and describe the strategy of modifying original label structure to reduce the label ambiguities.", "labels": [], "entities": [{"text": "parsing", "start_pos": 70, "end_pos": 77, "type": "TASK", "confidence": 0.9803516268730164}]}, {"text": "Final experiment demonstrates that the character information and label modification improve the parsing performance.", "labels": [], "entities": [{"text": "parsing", "start_pos": 96, "end_pos": 103, "type": "TASK", "confidence": 0.9759970903396606}]}], "introductionContent": [{"text": "Parsing is an important and fundamental task in natural language processing.", "labels": [], "entities": [{"text": "natural language processing", "start_pos": 48, "end_pos": 75, "type": "TASK", "confidence": 0.6447724799315134}]}, {"text": "The challenge of Chinese parser has been the focus of attention in recent years, and many different kinds of Chinese parsing models are investigated.", "labels": [], "entities": []}, {"text": "adopts Head-Driven model to parse analyzes the difficulties of Chinese parsing through comparing the differences between Chinese and English.", "labels": [], "entities": [{"text": "parse", "start_pos": 28, "end_pos": 33, "type": "TASK", "confidence": 0.9853576421737671}, {"text": "Chinese parsing", "start_pos": 63, "end_pos": 78, "type": "TASK", "confidence": 0.6327774524688721}]}, {"text": "() utilizes shift-reduce approach, dramatically improved the decoding speed of parsing.", "labels": [], "entities": [{"text": "parsing", "start_pos": 79, "end_pos": 86, "type": "TASK", "confidence": 0.9634431600570679}]}, {"text": "All these research adopted the same models which are also used in English parser -the models based on the words.", "labels": [], "entities": []}, {"text": "However, there is a big difference between English and Chinese: the expressing unit in English is word, while character is the smallest unit in Chinese.", "labels": [], "entities": []}, {"text": "Due to difficulties of word segmentation, especially for different segmenting criteria, many researchers explored parsing Chinese based on characters.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 23, "end_pos": 40, "type": "TASK", "confidence": 0.7411740273237228}]}, {"text": "The parser of received sentence as input and conducted word segmentation and syntactic parsing at the same time, but they did not utilize the character information in generating subtree;'s dependency parsing tree totally abandoned the word concept, so the dependency relations are the relations between characters.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 55, "end_pos": 72, "type": "TASK", "confidence": 0.7059948891401291}, {"text": "syntactic parsing", "start_pos": 77, "end_pos": 94, "type": "TASK", "confidence": 0.7687130272388458}, {"text": "dependency parsing", "start_pos": 189, "end_pos": 207, "type": "TASK", "confidence": 0.7134840488433838}]}, {"text": "We combine both word and character information to gain better performance of parsing.", "labels": [], "entities": [{"text": "parsing", "start_pos": 77, "end_pos": 84, "type": "TASK", "confidence": 0.9655190110206604}]}, {"text": "Although the criteria of segmentation are difficult to be unified, different criteria conflict only within the phrases which have little influence on the structure between phrases.", "labels": [], "entities": []}, {"text": "So we still use word as our basic unit of parsing.", "labels": [], "entities": []}, {"text": "Although word has been proved to be effective in head-driven parser, the data of word dependence is very sparse.", "labels": [], "entities": [{"text": "word dependence", "start_pos": 81, "end_pos": 96, "type": "TASK", "confidence": 0.6798704266548157}]}, {"text": "While it is worthy to note that words with similar concept always share the same characters in Chinese.", "labels": [], "entities": []}, {"text": "For instance, \" (scientist)\", \"(historian)\", etc., share the same character \"(expert)\", since they belong to the same concept \"expert in a certain field\".", "labels": [], "entities": []}, {"text": "So the problem of word sparseness can be solved by combining the character information to some extent.", "labels": [], "entities": []}, {"text": "Throughout this paper, we use TCT Treebank) as experimental data.", "labels": [], "entities": [{"text": "TCT Treebank", "start_pos": 30, "end_pos": 42, "type": "DATASET", "confidence": 0.758506566286087}]}, {"text": "TCT mainly consists of binary trees, with a few of multibranch and single-branch trees.", "labels": [], "entities": [{"text": "TCT", "start_pos": 0, "end_pos": 3, "type": "TASK", "confidence": 0.7812308669090271}]}, {"text": "Thus, we first transfer all trees to binary trees.", "labels": [], "entities": []}, {"text": "Then we use Lexical-PCFG model to exploit the word and character information, and Maximum Entropy Model to calculate the probability of induced trees as).", "labels": [], "entities": []}, {"text": "Finally we use CKY-based decoder.", "labels": [], "entities": []}, {"text": "In the following section, we will introduce how to utilize character information in our parsing model and the other features in detail.", "labels": [], "entities": []}, {"text": "Section 3 gives experiment results and analysis, which show improvement of our parsing approach.", "labels": [], "entities": [{"text": "parsing", "start_pos": 79, "end_pos": 86, "type": "TASK", "confidence": 0.9729294180870056}]}, {"text": "Section 4 presents the conclusion and future work.", "labels": [], "entities": []}], "datasetContent": [{"text": "Our experiments are conducted on the TCT corpus, which is used as the standard data of the CIPS-SIGHAN Parser 2010 bakeoff.", "labels": [], "entities": [{"text": "TCT corpus", "start_pos": 37, "end_pos": 47, "type": "DATASET", "confidence": 0.8293395936489105}, {"text": "CIPS-SIGHAN Parser 2010 bakeoff", "start_pos": 91, "end_pos": 122, "type": "DATASET", "confidence": 0.940755695104599}]}, {"text": "We omit the sentences with length 1 during training and testing.", "labels": [], "entities": []}, {"text": "Performance on the test corpus is evaluated with the standard measures from (SIGHAN RE-PORT, 2010).", "labels": [], "entities": [{"text": "SIGHAN", "start_pos": 77, "end_pos": 83, "type": "DATASET", "confidence": 0.5153005719184875}, {"text": "RE-PORT", "start_pos": 84, "end_pos": 91, "type": "METRIC", "confidence": 0.8398814797401428}]}, {"text": "We submit two results for the parsing bakeoff: one is single model we described in Section 2, another is reranking model, which is an attempt to apply a perceptron algorithm to rerank the 50-best result produced by the ME model.", "labels": [], "entities": [{"text": "parsing bakeoff", "start_pos": 30, "end_pos": 45, "type": "TASK", "confidence": 0.8877543807029724}]}, {"text": "1 shows the result of our parser compared with the top one in this bakeoff.", "labels": [], "entities": []}, {"text": "Since the parser we built is strictly dependent on the POS tags, the precision of POS tagging has a harsh effect on the overall parsing performance.", "labels": [], "entities": [{"text": "precision", "start_pos": 69, "end_pos": 78, "type": "METRIC", "confidence": 0.9978790283203125}]}, {"text": "The performance of the rerank model is lightly lower than that of the single model.", "labels": [], "entities": []}, {"text": "The most likely reason is that the features we count on are far from enough, and the informative features proved to be useful in are not yet included in our discriminative ranker.", "labels": [], "entities": []}, {"text": "Besides, the rank model we used is a simple perceptron learner, more delicated model, such as ME model used in (Charniak and Johnson,), might improve the result.", "labels": [], "entities": [{"text": "ME", "start_pos": 94, "end_pos": 96, "type": "METRIC", "confidence": 0.9581863284111023}]}, {"text": "In order to make clear how different features effect the parser performance, we conducted experiments on the TCT data provided by CIPSParEval-2009 for Chinese parser bakeoff 2 , since the sentences in CIPSParEval-2009 are given with head words and gold-standard POS tags.", "labels": [], "entities": []}, {"text": "The results of our parser are given in.", "labels": [], "entities": []}, {"text": "From Table 3 we can see that character features bring the improvement of F score for 1.75 compared with the basic features (line 2 vs line 3), and for 0.8 after adding the context features (line 4 vs line 6).", "labels": [], "entities": [{"text": "F score", "start_pos": 73, "end_pos": 80, "type": "METRIC", "confidence": 0.990708738565445}]}, {"text": "These results show that character features can improve the model with basic features very well.", "labels": [], "entities": []}, {"text": "After applying the context features, character features can still bring improvement, which states that character features can solve the ambiguities that cannot be solved by the context features.", "labels": [], "entities": []}, {"text": "One likely reason why character information is helpful is that the character can partly represent the meaning of word and can partly resolve the sparseness problem of word dependence as been observed in the work of.", "labels": [], "entities": [{"text": "word dependence", "start_pos": 167, "end_pos": 182, "type": "TASK", "confidence": 0.7069611102342606}]}, {"text": "C is a totally new meaning and D represents an ad-: Results of different features with no limit sentence length.", "labels": [], "entities": []}, {"text": "\"B+C\"-P \"B+C\"-R \"B+C\"-F1 \"B+C+H\"-P \"B+C+H\"-R \"B+C+H\"-F1 POS-P  ditional meaning.", "labels": [], "entities": [{"text": "F1 \"B+C+H\"-P \"B+C+H\"-R \"B+C+H\"-F1 POS-P  ditional meaning", "start_pos": 22, "end_pos": 79, "type": "TASK", "confidence": 0.5489149508731705}]}, {"text": "The expression after the first \"=\" is the meaning of the word, and the symbol \"+\" indicates the melding of meaning.", "labels": [], "entities": []}, {"text": "For example, A+B=A+D indicates that the word retains the meaning of character A, and adds new meaning D.", "labels": [], "entities": []}, {"text": "The distribution of each type in the dataset is shown in.", "labels": [], "entities": []}, {"text": "From we can see that type 4, i.e., there are no relation between characters and word, occupies only 8.02%.", "labels": [], "entities": []}, {"text": "This data proves that the word inherits the meaning from the characters which are used to construct the word.", "labels": [], "entities": []}, {"text": "However, the relations are really complicated.", "labels": [], "entities": []}, {"text": "For example, some words only inherit the meaning of formal characters and others of the last characters.", "labels": [], "entities": []}, {"text": "This might be the reason why character information does not have very obvious effect as expected.", "labels": [], "entities": []}, {"text": "In our parsing model, context features are really helpful to the parsing accuracy.", "labels": [], "entities": [{"text": "parsing", "start_pos": 7, "end_pos": 14, "type": "TASK", "confidence": 0.978702187538147}, {"text": "parsing", "start_pos": 65, "end_pos": 72, "type": "TASK", "confidence": 0.9643153548240662}, {"text": "accuracy", "start_pos": 73, "end_pos": 81, "type": "METRIC", "confidence": 0.7585117816925049}]}, {"text": "Different with the decision method in and), and reranking in) which all can utilize the context of current subtree very well (not only the POS tag), the CYK decoding algorithm restricts our context features.", "labels": [], "entities": []}, {"text": "However, we can conveniently exploit the POS tags around the current subtree without increasing the complexity of decoding and thus improve the performance.", "labels": [], "entities": []}, {"text": "Commonly, each subtree has only one headword.", "labels": [], "entities": []}, {"text": "However, we notice that the two head words of two coordinate children are equivalent, as illustrated in.", "labels": [], "entities": []}, {"text": "We assume that the parent node of these two children is A and the two headword are all the head words of A.", "labels": [], "entities": []}, {"text": "When A is the child of the parent node B, all the head words in A can be dependent with the other head words of another child C.", "labels": [], "entities": []}, {"text": "When A is still the head child of B, the head words of B are also the same as A.", "labels": [], "entities": []}, {"text": "Then we can extract more word dependence data.", "labels": [], "entities": [{"text": "word dependence", "start_pos": 25, "end_pos": 40, "type": "TASK", "confidence": 0.6731006950139999}]}, {"text": "For example, A have two head words \"(construct)\" and \"(complete)\", and \" (rule)\" is the headword of C, then we consider that \"(construct)\" and \"(complete)\" are all dependent with \"(rule)\".", "labels": [], "entities": []}, {"text": "Meanwhile, A is also the head child of B, and the head words of B are also \"(construct)\" and \"(complete)\".", "labels": [], "entities": []}, {"text": "During the decoding, we choose the most probable dependence as the dependence probability of B.", "labels": [], "entities": []}, {"text": "From the result, we can see that this strategy yields 0.17 improvements in the F score.", "labels": [], "entities": [{"text": "F score", "start_pos": 79, "end_pos": 86, "type": "METRIC", "confidence": 0.9866896271705627}]}, {"text": "Label splitting can also improve the performance.", "labels": [], "entities": [{"text": "Label splitting", "start_pos": 0, "end_pos": 15, "type": "TASK", "confidence": 0.7845672070980072}]}, {"text": "However, modifying the labels need much linguistic knowledge and manual work.) proposed an automated splitting and merging method.", "labels": [], "entities": [{"text": "splitting and merging", "start_pos": 101, "end_pos": 122, "type": "TASK", "confidence": 0.6812034845352173}]}, {"text": "As an attempt, we tested the effectiveness of it in our parser empirically.", "labels": [], "entities": []}, {"text": "When tested on the TCT data provided by CIPSParsEval-2009 for Chinese parser, bakeoff the label spitting improve the F1 measure from 0.864 to 0.869.", "labels": [], "entities": [{"text": "TCT data", "start_pos": 19, "end_pos": 27, "type": "DATASET", "confidence": 0.719325065612793}, {"text": "F1 measure", "start_pos": 117, "end_pos": 127, "type": "METRIC", "confidence": 0.989325076341629}]}], "tableCaptions": [{"text": " Table 3: Results of different features with no limit  sentence length.", "labels": [], "entities": []}, {"text": " Table 2: Results of different features with no limit sentence length.", "labels": [], "entities": []}, {"text": " Table 4: The relation between the meaning of  words and characters.", "labels": [], "entities": []}]}