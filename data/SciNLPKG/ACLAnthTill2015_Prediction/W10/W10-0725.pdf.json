{"title": [], "abstractContent": [{"text": "This paper describes our experiments of using Amazon's Mechanical Turk to generate (counter-)facts from texts for certain named-entities.", "labels": [], "entities": [{"text": "Amazon's Mechanical Turk", "start_pos": 46, "end_pos": 70, "type": "DATASET", "confidence": 0.875409334897995}]}, {"text": "We give the human annotators a paragraph of text and a highlighted named-entity.", "labels": [], "entities": []}, {"text": "They will write down several (counter-)facts about this named-entity in that context.", "labels": [], "entities": []}, {"text": "The analysis of the results is performed by comparing the acquired data with the recognizing textual entailment (RTE) challenge dataset.", "labels": [], "entities": [{"text": "recognizing textual entailment (RTE) challenge", "start_pos": 81, "end_pos": 127, "type": "TASK", "confidence": 0.6840520415987287}]}], "introductionContent": [], "datasetContent": [{"text": "The texts we use in our experiments are the development set of the RTE-5 challenge (Bentivogli et al., 2009), and we preprocess the data using the Stanford named-entity recognizer ().", "labels": [], "entities": [{"text": "RTE-5 challenge", "start_pos": 67, "end_pos": 82, "type": "TASK", "confidence": 0.5807792544364929}]}, {"text": "In all, it contains 600 T-H pairs, and we use the texts to generate facts and counter-facts and hypotheses as references.", "labels": [], "entities": []}, {"text": "We put our task online through CrowdFlower 2 , and on average, we pay one cent for each (counter-)fact to the Turkers.", "labels": [], "entities": [{"text": "Turkers", "start_pos": 110, "end_pos": 117, "type": "DATASET", "confidence": 0.9316047430038452}]}, {"text": "CrowdFlower can help with finding trustful Turkers and the data were collected within a few hours.", "labels": [], "entities": []}, {"text": "To get a sense of the quality of the data we collect, we mainly focus on analyzing the following three aspects: 1) the statistics of the datasets themselves; 2) the comparison between the data we collect and the original RTE dataset; and 3) the comparison between the facts and the counter-facts.", "labels": [], "entities": [{"text": "RTE dataset", "start_pos": 221, "end_pos": 232, "type": "DATASET", "confidence": 0.9431005120277405}]}, {"text": "show some basic statistics of the data we collect.", "labels": [], "entities": []}, {"text": "After excluding invalid and trivial ones 3 , we acquire 790 facts and 203 counter-facts.", "labels": [], "entities": []}, {"text": "In general, the counter-facts seem to be more difficult to obtain than the facts, since both the total number and the average number of the counter-facts are less than those of the facts.", "labels": [], "entities": []}, {"text": "Notice that the NEs are not many since they have to appear in both T and H.", "labels": [], "entities": []}, {"text": "The comparison between our data and the original RTE data is shown in.", "labels": [], "entities": [{"text": "RTE data", "start_pos": 49, "end_pos": 57, "type": "DATASET", "confidence": 0.8850622177124023}]}, {"text": "The average length of the generated hypotheses is longer than the original hypotheses, for both the facts and the counter-facts.", "labels": [], "entities": []}, {"text": "Counter-facts seem to be more verbose, since additional (contradictory) information is added.", "labels": [], "entities": []}, {"text": "For instance, example ID 425 in, Counter Fact 1 can be viewed as the more informative but contradictory version of Fact 1 (and the original hypoth-esis).", "labels": [], "entities": []}, {"text": "The average bag-of-words similarity scores are calculated by dividing the number of overlapping words of T and H by the total number of words in H.", "labels": [], "entities": []}, {"text": "In the original RTE dataset, the entailed hypotheses have a higher BoW score than the contradictory ones; while in our data, facts have a lower score than the counter-facts.", "labels": [], "entities": [{"text": "RTE dataset", "start_pos": 16, "end_pos": 27, "type": "DATASET", "confidence": 0.8973498344421387}, {"text": "BoW score", "start_pos": 67, "end_pos": 76, "type": "METRIC", "confidence": 0.9693245589733124}]}, {"text": "This might be caused by the greater variety of the facts than the counterfacts.", "labels": [], "entities": []}, {"text": "Fact 1 of example ID 425 in is almost the same as the original hypothesis, and Fact 2 of example ID 374 as well, though the latter has some slight differences which make the answer different from the original one.", "labels": [], "entities": []}, {"text": "The NE position in the sentence is another aspect to look at.", "labels": [], "entities": []}, {"text": "We find that people tend to put the NEs at the beginning of the sentences more than other positions, while in the RTE datasets, NEs appear in the middle more frequently.", "labels": [], "entities": [{"text": "RTE datasets", "start_pos": 114, "end_pos": 126, "type": "DATASET", "confidence": 0.8971495628356934}]}, {"text": "In order to get a feeling of the quality of the data, we randomly sampled 50 generated facts and counter-facts and manually compared them with the original hypotheses.", "labels": [], "entities": []}, {"text": "shows that generated facts are easier for the systems to recognize, and the counter-facts have the same difficulty on average.", "labels": [], "entities": []}, {"text": "Although it is subjective to evaluate the difficulty of the data by human reading, in general, we follow the criteria that 1.", "labels": [], "entities": []}, {"text": "Abstraction is more difficult than extraction; 2.", "labels": [], "entities": [{"text": "Abstraction", "start_pos": 0, "end_pos": 11, "type": "METRIC", "confidence": 0.985126793384552}]}, {"text": "Inference is more difficult than the direct entailment; 3.", "labels": [], "entities": []}, {"text": "The more sentences in T are involved, the more difficult that T-H pair is.", "labels": [], "entities": []}, {"text": "Therefore, we view the Counter Fact 1 in example ID 16 in is more difficult than the original hypothesis, since it requires more inference than the direct fact validation.", "labels": [], "entities": []}, {"text": "However, in example ID 374, Fact 1 is easier to be verified than the original hypothesis, and same as those facts in example ID 506.", "labels": [], "entities": [{"text": "Fact 1", "start_pos": 28, "end_pos": 34, "type": "METRIC", "confidence": 0.8582153916358948}]}, {"text": "Similar hypotheses (e.g. Fact 1 in example ID 425 and the original hypothesis) are treated as being at the same level of difficulty.", "labels": [], "entities": [{"text": "Fact 1 in example ID 425", "start_pos": 25, "end_pos": 49, "type": "DATASET", "confidence": 0.5958822617928187}]}, {"text": "After the quantitive analysis, let's take a closer look at the examples in.", "labels": [], "entities": []}, {"text": "The facts are usually constructed by rephrasing some parts of the text (e.g. in ID 425, \"after a brief inspection\" is paraphrased by \"investigated by\" in Fact 2) or making a short Valid Harder Easier Same Facts 76% 16% 24% 36% Counter-Facts 84% 36% 36% 12%: The comparison of the generated (counter-)facts with the original hypotheses.", "labels": [], "entities": [{"text": "ID 425", "start_pos": 80, "end_pos": 86, "type": "DATASET", "confidence": 0.8351561725139618}]}, {"text": "The Valid column shows the percentage of the valid (counter-)facts; and other columns present the distribution of harder, easier cases than the original hypotheses or with the same difficulty.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: The statistics of the (valid) data we collect. The  Total column presents the number of extracted NEs and  generated hypotheses and the Average column shows the  average numbers per text respectively.", "labels": [], "entities": [{"text": "Total", "start_pos": 62, "end_pos": 67, "type": "METRIC", "confidence": 0.9701073169708252}, {"text": "Average", "start_pos": 146, "end_pos": 153, "type": "METRIC", "confidence": 0.9737690091133118}]}, {"text": " Table 4. The facts are usually  constructed by rephrasing some parts of the text (e.g.  in ID 425, \"after a brief inspection\" is paraphrased  by \"investigated by\" in Fact 2) or making a short", "labels": [], "entities": [{"text": "ID 425", "start_pos": 92, "end_pos": 98, "type": "DATASET", "confidence": 0.8346001505851746}]}, {"text": " Table 2: The comparison between the generated (counter-)facts and the original hypotheses from the RTE dataset. The  Ave. Length column represents the average number of words in each hypothesis; The Ave. BoW shows the average  bag-of-words similarity compared with the text. The three columns on the right are all about the position of the NE  appearing in the sentence, how likely it is at the head, middle, or tail of the sentence.", "labels": [], "entities": [{"text": "RTE dataset", "start_pos": 100, "end_pos": 111, "type": "DATASET", "confidence": 0.9409970045089722}, {"text": "Ave. Length column", "start_pos": 118, "end_pos": 136, "type": "METRIC", "confidence": 0.674604132771492}, {"text": "Ave. BoW", "start_pos": 200, "end_pos": 208, "type": "DATASET", "confidence": 0.4024803638458252}]}]}