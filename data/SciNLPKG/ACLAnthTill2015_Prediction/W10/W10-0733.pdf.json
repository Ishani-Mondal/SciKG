{"title": [{"text": "Using Mechanical Turk to Build Machine Translation Evaluation Sets", "labels": [], "entities": [{"text": "Build Machine Translation Evaluation Sets", "start_pos": 25, "end_pos": 66, "type": "TASK", "confidence": 0.7028299570083618}]}], "abstractContent": [{"text": "Building machine translation (MT) test sets is a relatively expensive task.", "labels": [], "entities": [{"text": "machine translation (MT) test", "start_pos": 9, "end_pos": 38, "type": "TASK", "confidence": 0.8556288381417593}]}, {"text": "As MT becomes increasingly desired for more and more language pairs and more and more domains, it becomes necessary to build test sets for each case.", "labels": [], "entities": [{"text": "MT", "start_pos": 3, "end_pos": 5, "type": "TASK", "confidence": 0.9888536930084229}]}, {"text": "In this paper, we investigate using Ama-zon's Mechanical Turk (MTurk) to make MT test sets cheaply.", "labels": [], "entities": [{"text": "Ama-zon's Mechanical Turk (MTurk)", "start_pos": 36, "end_pos": 69, "type": "DATASET", "confidence": 0.8871279188564846}, {"text": "MT", "start_pos": 78, "end_pos": 80, "type": "TASK", "confidence": 0.9657375812530518}]}, {"text": "We find that MTurk can be used to make test sets much cheaper than professionally-produced test sets.", "labels": [], "entities": [{"text": "MTurk", "start_pos": 13, "end_pos": 18, "type": "DATASET", "confidence": 0.7564790844917297}]}, {"text": "More importantly , in experiments with multiple MT systems, we find that the MTurk-produced test sets yield essentially the same conclusions regarding system performance as the professionally-produced test sets yield.", "labels": [], "entities": [{"text": "MT", "start_pos": 48, "end_pos": 50, "type": "TASK", "confidence": 0.9502519965171814}, {"text": "MTurk-produced test sets", "start_pos": 77, "end_pos": 101, "type": "DATASET", "confidence": 0.9252613186836243}]}], "introductionContent": [{"text": "Machine translation (MT) research is empirically evaluated by comparing system output against reference human translations, typically using automatic evaluation metrics.", "labels": [], "entities": [{"text": "Machine translation (MT) research", "start_pos": 0, "end_pos": 33, "type": "TASK", "confidence": 0.9148728549480438}]}, {"text": "One method for establishing a translation test set is to holdout part of the training set to be used for testing.", "labels": [], "entities": []}, {"text": "However, this practice typically overestimates system quality when compared to evaluating on a test set drawn from a different domain.", "labels": [], "entities": []}, {"text": "Therefore, it's necessary to make new test sets not only for new language pairs but also for new domains.", "labels": [], "entities": []}, {"text": "Creating reasonable sized test sets for new domains can be expensive.", "labels": [], "entities": []}, {"text": "For example, the Workshop on Statistical Machine Translation (WMT) uses a mix of non-professional and professional translators to create the test sets for its annual shared translation tasks).", "labels": [], "entities": [{"text": "Statistical Machine Translation (WMT)", "start_pos": 29, "end_pos": 66, "type": "TASK", "confidence": 0.7612034529447556}]}, {"text": "For WMT09, the total cost of creating the test sets consisting of roughly 80,000 words across 3027 sentences in seven European languages was approximately $39,800 USD, or slightly more than $0.08 USD/word.", "labels": [], "entities": [{"text": "WMT09", "start_pos": 4, "end_pos": 9, "type": "DATASET", "confidence": 0.853001058101654}]}, {"text": "For WMT08, creating test sets consisting of 2,051 sentences in six languages was approximately $26,500 USD or slightly more than $0.10 USD/word.", "labels": [], "entities": [{"text": "WMT08", "start_pos": 4, "end_pos": 9, "type": "DATASET", "confidence": 0.8306241035461426}]}, {"text": "In this paper we examine the use of Amazon's Mechanical Turk (MTurk) to create translation test sets for statistical machine translation research.", "labels": [], "entities": [{"text": "Amazon's Mechanical Turk (MTurk)", "start_pos": 36, "end_pos": 68, "type": "DATASET", "confidence": 0.9168516482625689}, {"text": "statistical machine translation research", "start_pos": 105, "end_pos": 145, "type": "TASK", "confidence": 0.7486980929970741}]}, {"text": "showed that MTurk can be useful for creating data fora variety of NLP tasks, and that a combination of judgments from non-experts can attain expert-level quality in many cases.", "labels": [], "entities": []}, {"text": "showed that MTurk could be used for low-cost manual evaluation of machine translation quality, and suggested that it might be possible to use MTurk to create MT test sets after an initial pilot study where turkers (the people who complete the work assignments posted on MTurk) produced translations of 50 sentences in five languages.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 66, "end_pos": 85, "type": "TASK", "confidence": 0.7756194770336151}, {"text": "MT", "start_pos": 158, "end_pos": 160, "type": "TASK", "confidence": 0.9596320390701294}]}, {"text": "This paper explores this in more detail by asking turkers to translate the Urdu sentences of the Urdu-English test set used in the 2009 NIST Machine Translation Evaluation Workshop.", "labels": [], "entities": [{"text": "NIST Machine Translation Evaluation Workshop", "start_pos": 136, "end_pos": 180, "type": "TASK", "confidence": 0.7822042167186737}]}, {"text": "We evaluate multiple MT systems on both the professionallyproduced NIST2009 test set and our MTurkproduced test set and find that the MTurk-produced test set yields essentially the same conclusions about system performance as the NIST2009 set yields.", "labels": [], "entities": [{"text": "MT", "start_pos": 21, "end_pos": 23, "type": "TASK", "confidence": 0.962795615196228}, {"text": "NIST2009 test set", "start_pos": 67, "end_pos": 84, "type": "DATASET", "confidence": 0.9843369722366333}, {"text": "MTurkproduced test set", "start_pos": 93, "end_pos": 115, "type": "DATASET", "confidence": 0.9486864606539408}, {"text": "MTurk-produced test set", "start_pos": 134, "end_pos": 157, "type": "DATASET", "confidence": 0.9249840378761292}, {"text": "NIST2009 set", "start_pos": 230, "end_pos": 242, "type": "DATASET", "confidence": 0.9504345059394836}]}], "datasetContent": [{"text": "A main purpose of an MT test set is to evaluate various MT systems' performances relative to each other and assist in drawing conclusions about the relative quality of the translations produced by the systems.", "labels": [], "entities": [{"text": "MT", "start_pos": 21, "end_pos": 23, "type": "TASK", "confidence": 0.9699479937553406}]}, {"text": "Therefore, if a given system, say System A, outperforms another given system, say System B, on a high-quality professionally-produced test set, then we would want to see that System A also outperforms System B on our MTurk-produced test set.", "labels": [], "entities": [{"text": "MTurk-produced test set", "start_pos": 217, "end_pos": 240, "type": "DATASET", "confidence": 0.9590439399083456}]}, {"text": "It is also desirable that the magnitudes of the differences in performance between systems also be maintained.", "labels": [], "entities": []}, {"text": "In order to measure the differences in performance, using the differences in the absolute magnitudes of the BLEU scores will notwork well because the magnitudes of the BLEU scores are affected by many factors of the test set being used, such as the number of reference translations per foreign sentence.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 108, "end_pos": 112, "type": "METRIC", "confidence": 0.9432780742645264}, {"text": "BLEU", "start_pos": 168, "end_pos": 172, "type": "METRIC", "confidence": 0.9688771367073059}]}, {"text": "For determining performance differences between systems and especially for comparing them across different test sets, we use percentage of baseline performance.", "labels": [], "entities": []}, {"text": "To compute percentage of baseline performance, we designate one system as the baseline system and use percentage of that baseline system's performance.", "labels": [], "entities": []}, {"text": "For example, shows both absolute BLEU scores and percentage performance for three MT systems when tested on five different test sets.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 33, "end_pos": 37, "type": "METRIC", "confidence": 0.9186342358589172}, {"text": "MT", "start_pos": 82, "end_pos": 84, "type": "TASK", "confidence": 0.9689486622810364}]}, {"text": "The first test set in the table is the NIST-2009 set with all four reference translations per Urdu sentence.", "labels": [], "entities": [{"text": "NIST-2009 set", "start_pos": 39, "end_pos": 52, "type": "DATASET", "confidence": 0.9767848253250122}]}, {"text": "The next four test sets use only a single reference translation per Urdu sentence (ref 1 uses the first reference translation only, ref 2 the second only, etc.).", "labels": [], "entities": []}, {"text": "Note that the BLEU scores for the single-reference translation test sets are much lower than for the test set with all four reference translations and the difference in the absolute magnitudes of the BLEU scores between the three different systems are different for the different test sets.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 14, "end_pos": 18, "type": "METRIC", "confidence": 0.9985068440437317}, {"text": "BLEU", "start_pos": 200, "end_pos": 204, "type": "METRIC", "confidence": 0.9908374547958374}]}, {"text": "However, the percentage performance of the MT systems is maintained (both the ordering of the systems and the amount of the difference between them) across the different test sets.", "labels": [], "entities": [{"text": "MT", "start_pos": 43, "end_pos": 45, "type": "TASK", "confidence": 0.983889102935791}]}, {"text": "We evaluated three different MT systems on the NIST2009 test set and on our two MTurk-produced test sets (MTurk-NoEditing and MTurk-Edited).", "labels": [], "entities": [{"text": "MT", "start_pos": 29, "end_pos": 31, "type": "TASK", "confidence": 0.9762946963310242}, {"text": "NIST2009 test set", "start_pos": 47, "end_pos": 64, "type": "DATASET", "confidence": 0.9914173483848572}, {"text": "MTurk-produced test sets", "start_pos": 80, "end_pos": 104, "type": "DATASET", "confidence": 0.9255487322807312}, {"text": "MTurk-NoEditing", "start_pos": 106, "end_pos": 121, "type": "DATASET", "confidence": 0.8058491349220276}, {"text": "MTurk-Edited", "start_pos": 126, "end_pos": 138, "type": "DATASET", "confidence": 0.8255220055580139}]}, {"text": "Two of the MT systems (ISI Syntax (  2004;) and JHU Syntax () augmented with ()) were chosen because they represent stateof-the-art performance, having achieved the highest scores on NIST2009 to our knowledge.", "labels": [], "entities": [{"text": "MT", "start_pos": 11, "end_pos": 13, "type": "TASK", "confidence": 0.8529788851737976}, {"text": "NIST2009", "start_pos": 183, "end_pos": 191, "type": "DATASET", "confidence": 0.9589846730232239}]}, {"text": "They also have very similar performance on NIST2009 so we want to see if that similar performance is maintained as we evaluate on our MTurk-produced test sets.", "labels": [], "entities": [{"text": "NIST2009", "start_pos": 43, "end_pos": 51, "type": "DATASET", "confidence": 0.9761677980422974}, {"text": "MTurk-produced test sets", "start_pos": 134, "end_pos": 158, "type": "DATASET", "confidence": 0.9629607995351156}]}, {"text": "The third MT system (Joshua-Hierarchical) (, an open source implementation of, was chosen because though it is a competitive system, it had clear, markedly lower performance on NIST2009 than the other two systems and we want to see if that difference in performance is also maintained if we were to shift evaluation to our MTurk-produced test sets.", "labels": [], "entities": [{"text": "MT", "start_pos": 10, "end_pos": 12, "type": "TASK", "confidence": 0.9367809295654297}, {"text": "NIST2009", "start_pos": 177, "end_pos": 185, "type": "DATASET", "confidence": 0.9738001823425293}, {"text": "MTurk-produced test sets", "start_pos": 323, "end_pos": 347, "type": "DATASET", "confidence": 0.9159402847290039}]}, {"text": "There area number of observations to make.", "labels": [], "entities": []}, {"text": "One is that the absolute magnitude of the BLEU scores is much lower for all systems on the MTurk-produced test sets than on  the NIST2009 test set.", "labels": [], "entities": [{"text": "absolute magnitude", "start_pos": 16, "end_pos": 34, "type": "METRIC", "confidence": 0.9705462157726288}, {"text": "BLEU", "start_pos": 42, "end_pos": 46, "type": "METRIC", "confidence": 0.9676275253295898}, {"text": "MTurk-produced test sets", "start_pos": 91, "end_pos": 115, "type": "DATASET", "confidence": 0.9726521174112955}, {"text": "NIST2009 test set", "start_pos": 129, "end_pos": 146, "type": "DATASET", "confidence": 0.987415095170339}]}, {"text": "This is primarily because the NIST2009 set had four translations per foreign sentence whereas the MTurk-produced sets only have one translation per foreign sentence.", "labels": [], "entities": [{"text": "NIST2009 set", "start_pos": 30, "end_pos": 42, "type": "DATASET", "confidence": 0.9537791907787323}, {"text": "MTurk-produced sets", "start_pos": 98, "end_pos": 117, "type": "DATASET", "confidence": 0.9328121542930603}]}, {"text": "Due to this different scale of BLEU scores, we compare performances using percentage of baseline performance.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 31, "end_pos": 35, "type": "METRIC", "confidence": 0.9983912110328674}]}, {"text": "We use the ISI Syntax system as the baseline since it achieved the highest results on NIST2009.", "labels": [], "entities": [{"text": "NIST2009", "start_pos": 86, "end_pos": 94, "type": "DATASET", "confidence": 0.9772875308990479}]}, {"text": "The main observation of the results in is that both the relative performance of the various MT systems and the amount of the differences in performance (in terms of percentage performance of the baseline) are maintained when we use the MTurkproduced test sets as when we use the NIST2009 test set.", "labels": [], "entities": [{"text": "MT", "start_pos": 92, "end_pos": 94, "type": "TASK", "confidence": 0.9426273107528687}, {"text": "MTurkproduced test sets", "start_pos": 236, "end_pos": 259, "type": "DATASET", "confidence": 0.9542701840400696}, {"text": "NIST2009 test set", "start_pos": 279, "end_pos": 296, "type": "DATASET", "confidence": 0.9847418467203776}]}, {"text": "In particular, we can see that whether using the NIST2009 test set or the MTurk-produced test sets, one would conclude that ISI Syntax and JHU Syntax perform about the same and Joshua-Hierarchical delivers about 80% of the performance of the two syntax systems.", "labels": [], "entities": [{"text": "NIST2009 test set", "start_pos": 49, "end_pos": 66, "type": "DATASET", "confidence": 0.9835406144460043}, {"text": "MTurk-produced test sets", "start_pos": 74, "end_pos": 98, "type": "DATASET", "confidence": 0.970956563949585}]}, {"text": "The post-edited test set did not yield different conclusions than the non-edited test set yielded so the value of post-editing for test set creation remains an open question.", "labels": [], "entities": []}], "tableCaptions": []}