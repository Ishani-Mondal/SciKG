{"title": [{"text": "Towards Learning Rules from Natural Texts \u00a3", "labels": [], "entities": []}], "abstractContent": [{"text": "In this paper, we consider the problem of in-ductively learning rules from specific facts extracted from texts.", "labels": [], "entities": []}, {"text": "This problem is challenging due to two reasons.", "labels": [], "entities": []}, {"text": "First, natural texts are radically incomplete since there are always too many facts to mention.", "labels": [], "entities": []}, {"text": "Second, natural texts are systematically biased towards novelty and surprise, which presents an unrepresentative sample to the learner.", "labels": [], "entities": []}, {"text": "Our solutions to these two problems are based on building a generative observation model of what is mentioned and what is extracted given what is true.", "labels": [], "entities": [{"text": "generative observation", "start_pos": 60, "end_pos": 82, "type": "TASK", "confidence": 0.8954171538352966}]}, {"text": "We first present a Multiple-predicate Bootstrapping approach that consists of iteratively learning if-then rules based on an implicit observation model and then imput-ing new facts implied by the learned rules.", "labels": [], "entities": []}, {"text": "Second, we present an iterative ensemble co-learning approach, where multiple decision-trees are learned from bootstrap samples of the incomplete training data, and facts are im-puted based on weighted majority.", "labels": [], "entities": []}], "introductionContent": [{"text": "One of the principal goals of learning by reading is to make the vast amount of natural language text \u00a3 This material is based upon work supported by the Defense Advanced Research Projects Agency (DARPA) under Contract No. FA8750-09-C-0179.", "labels": [], "entities": []}, {"text": "Any opinions, findings and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the DARPA, or the Air Force Research Laboratory (AFRL).", "labels": [], "entities": [{"text": "DARPA", "start_pos": 160, "end_pos": 165, "type": "DATASET", "confidence": 0.886589765548706}, {"text": "Air Force Research Laboratory (AFRL)", "start_pos": 174, "end_pos": 210, "type": "DATASET", "confidence": 0.7967219012124198}]}, {"text": "We thank the reviewers for their insightful comments and helpful suggestions.", "labels": [], "entities": []}, {"text": "which is on the web accessible to automatic processing.", "labels": [], "entities": []}, {"text": "There are at least three different ways in which this can be done.", "labels": [], "entities": []}, {"text": "First, factual knowledge on the web can be extracted as formal relations or tuples of a database.", "labels": [], "entities": []}, {"text": "A number of information extraction systems, starting from the WebKb project (), to Whirl) to the TextRunner () project are of this kind.", "labels": [], "entities": [{"text": "information extraction", "start_pos": 12, "end_pos": 34, "type": "TASK", "confidence": 0.7795851826667786}]}, {"text": "They typically learn patterns or rules that can be applied to text to extract instances of relations.", "labels": [], "entities": []}, {"text": "A second possibility is to learn general knowledge, rules, or general processes and procedures by reading natural language descriptions of them, for example, extracting formal descriptions of the rules of the United States Senate or a recipe to make a dessert.", "labels": [], "entities": []}, {"text": "A third instance of machine reading is to generalize the facts extracted from the text to learn more general knowledge.", "labels": [], "entities": [{"text": "machine reading", "start_pos": 20, "end_pos": 35, "type": "TASK", "confidence": 0.7674883604049683}, {"text": "generalize the facts extracted from the text", "start_pos": 42, "end_pos": 86, "type": "TASK", "confidence": 0.8395129953111921}]}, {"text": "For example, one might learn by generalizing from reading the obituaries that most people live less than 90 years, or people tend to live and die in the countries they were born in.", "labels": [], "entities": []}, {"text": "In this paper, we consider the problem of learning such general rules by reading about specific facts.", "labels": [], "entities": []}, {"text": "At first blush, learning rules by reading specific facts appears to be a composition of information extraction followed by rule induction.", "labels": [], "entities": [{"text": "information extraction", "start_pos": 88, "end_pos": 110, "type": "TASK", "confidence": 0.7068827450275421}]}, {"text": "In the above example of learning from obituaries, there is reason to believe that this reductionist approach would work well.", "labels": [], "entities": []}, {"text": "However, there are two principal reasons why this approach of learning directly from natural texts is problematic.", "labels": [], "entities": []}, {"text": "One is that, unlike databases, the natural texts are radically incomplete.", "labels": [], "entities": []}, {"text": "By this we mean that many of the facts that are relevant to predicting the target relation might be missing in the text.", "labels": [], "entities": []}, {"text": "This 70 is so because inmost cases the set of relevant facts is open ended.", "labels": [], "entities": []}, {"text": "The second problem, in some ways more worrisome, is that the natural language texts are systematically biased towards newsworthiness, which correlates with infrequency or novelty.", "labels": [], "entities": []}, {"text": "This is sometimes called \"the man bites a dog phenomenon.\"", "labels": [], "entities": []}, {"text": "1 Unfortunately the novelty bias violates the most common assumption of machine learning that the training data is representative of the underlying truth, or equivalently, that any missing information is missing at random.", "labels": [], "entities": [{"text": "novelty", "start_pos": 20, "end_pos": 27, "type": "METRIC", "confidence": 0.9459795355796814}]}, {"text": "In particular, since natural langauge texts are written for people who already possess avast amount of prior knowledge, communication efficiency demands that facts that can be easily inferred by most people are left out of the text.", "labels": [], "entities": []}, {"text": "To empirically validate our two hypotheses of radical incompleteness and systematic bias of natural texts, we have examined a collection of 248 documents related to the topics of people, organizations, and relationships collected by the Linguistic Data Consortium (LDC).", "labels": [], "entities": [{"text": "Linguistic Data Consortium (LDC)", "start_pos": 237, "end_pos": 269, "type": "DATASET", "confidence": 0.8255999684333801}]}, {"text": "We chose the target relationship of the birthplace of a person.", "labels": [], "entities": []}, {"text": "It turned out that the birthplace of some person is only mentioned 23 times in the 248 documents, illustrating the radical incompleteness of texts mentioned earlier.", "labels": [], "entities": []}, {"text": "Moreover, in 14 out of the 23 mentions of the birthplace, the information violates some default inferences.", "labels": [], "entities": []}, {"text": "For example, one of the sentences reads: \"Ahmed Said Khadr, an Egyptian-born Canadian, was killed last October in Pakistan.\"", "labels": [], "entities": []}, {"text": "Presumably the phrase \"Egyptian-born\" was considered important by the reporter because it violates our expectation that most Canadians are born in Canada.", "labels": [], "entities": []}, {"text": "If Khadr was instead born in Canada, the reporter would mostly likely have left out \"Canadian-born\" because it is too obvious to mention given he is a Canadian.", "labels": [], "entities": []}, {"text": "In all the 9 cases where the birthplace does not violate the default assumptions, the story is biographical, e.g., an obituary.", "labels": [], "entities": []}, {"text": "In general, only a small part of the whole truth is ever mentioned in a given document.", "labels": [], "entities": []}, {"text": "Thus, the reporter has to make some choices as to what to mention and what to leave out.", "labels": [], "entities": []}, {"text": "The key insight of this paper is that considering how these choices are made is important in making correct statistical inferences.", "labels": [], "entities": []}, {"text": "In the above example, wrong probabilities would be derived if one assumes that the birthplace information is missing at random.", "labels": [], "entities": []}, {"text": "In this paper we introduce the notion of a \"mention model,\" which models the generative process of what is mentioned in a document.", "labels": [], "entities": [{"text": "generative process of what is mentioned in a document", "start_pos": 77, "end_pos": 130, "type": "TASK", "confidence": 0.7854910294214884}]}, {"text": "We also extend this using an \"extraction model,\" which represents the errors in the process of extracting facts from the text documents.", "labels": [], "entities": []}, {"text": "The mention model and the extraction model together represent the probability that some facts are extracted given the true facts.", "labels": [], "entities": []}, {"text": "For learning, we could use an explicit mention model to score hypothesized rules by calculating the probability that a rule is satisfied by the observed evidence and then pick the rules that are most likely given the evidence.", "labels": [], "entities": []}, {"text": "In this paper, we take the simpler approach of directly adapting the learning algorithms to an implicit mention model, by changing the way a rule is scored by the available evidence.", "labels": [], "entities": []}, {"text": "Since each text document involves multiple predicates with relationships between them, we learn rules to predict each predicate from the other predicates.", "labels": [], "entities": []}, {"text": "Thus, the goal of the system is to learn a sufficiently large set of rules to infer all the missing information as accurately as possible.", "labels": [], "entities": []}, {"text": "To effectively bootstrap the learning process, the learned rules are used on the incomplete training data to impute new facts, which are then used to induce more rules in subsequent iterations.", "labels": [], "entities": []}, {"text": "This approach is most similar to the coupled semi-supervised learning of) and general bootstrapping approaches in natural language processing.", "labels": [], "entities": []}, {"text": "Since this is in the context of multiplepredicate learning in inductive logic programming (ILP), we call this approach \"Multiple-predicate Bootstrapping.\"", "labels": [], "entities": [{"text": "multiplepredicate learning in inductive logic programming (ILP)", "start_pos": 32, "end_pos": 95, "type": "TASK", "confidence": 0.6646889117028978}]}, {"text": "One problem with Multiple-predicate Bootstrapping is potentially large variance.", "labels": [], "entities": []}, {"text": "To mitigae this, we consider the bagging approach, where multiple rule sets are learned from bootstrap samples of the training data with an implicit mention model to score the rules.", "labels": [], "entities": []}, {"text": "We then use these sets of rules as an ensemble to impute new facts, and repeat the process.", "labels": [], "entities": []}, {"text": "We evaluate both of these approaches on real world data processed through synthetic observation models.", "labels": [], "entities": []}, {"text": "Our results indicate that when the assump-71 tions of the learner suit the observation model, the learner's performance is quite good.", "labels": [], "entities": []}, {"text": "Further, we show that the ensemble approach significantly improves the performance of Multiple-predicate Bootstrapping.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluated Multiple-predicate Bootstrapping with implicit mention models on the schema-based NFL database retrieved from www.databasefootball.com.", "labels": [], "entities": [{"text": "NFL database", "start_pos": 95, "end_pos": 107, "type": "DATASET", "confidence": 0.8351785242557526}]}, {"text": "We developed two different synthetic observation models. with the natural interpretations.", "labels": [], "entities": []}, {"text": "To simplify arithmetic reasoning we replaced the numeric team scores in the real database with two defined predicates teamSmallerScore(Team, Game) and teamGreaterScore(Team, Game) to indicate the teams with the smaller and the greater scores.", "labels": [], "entities": [{"text": "arithmetic reasoning", "start_pos": 12, "end_pos": 32, "type": "TASK", "confidence": 0.8106763064861298}]}, {"text": "We generate two sets of synthetic data as follows.", "labels": [], "entities": []}, {"text": "In the Random mention model, each predicate except the teamInGame predicate is omitted independently with probability \u00d4.", "labels": [], "entities": []}, {"text": "The Novelty mention model, on the other hand, relies on the fact that gameWinner, gameLoser, and teamFinalScore are mutually correlated, as are homeTeam and awayTeam.", "labels": [], "entities": []}, {"text": "Thus, it picks one predicate 73   from the first group to mention its values, and omitseach of the other predicates independently with some probability \u00d5.", "labels": [], "entities": []}, {"text": "Similarly it gives a value to one of the two predicates in the second group and omits the other predicate with probability \u00d5.", "labels": [], "entities": []}, {"text": "One consequence of this model is that it always has one of the predicates in the first group and one of the predicates in the second group, which is sufficient to infer everything if one knew the correct domain rules.", "labels": [], "entities": []}, {"text": "We evaluate two scoring methods: the aggressive scoring and the conservative scoring.", "labels": [], "entities": []}, {"text": "We employed two learning systems: Quinlan's FOIL, which learns relational rules using a greedy covering algorithm, and Nijssen and Kok's FARMER, which is a relational data mining algorithm that searches for conjunctions of literals of large support using a bottom-up depth first search.", "labels": [], "entities": [{"text": "FOIL", "start_pos": 44, "end_pos": 48, "type": "METRIC", "confidence": 0.8429535031318665}, {"text": "FARMER", "start_pos": 137, "end_pos": 143, "type": "METRIC", "confidence": 0.8283582329750061}]}, {"text": "Both systems were applied to learn rules for all target predicates.", "labels": [], "entities": []}, {"text": "One important difference to note here is that while FARMER seeks all rules that exceed the necessary support threshold, FOIL only learns rules that are sufficient to classify all training instances into those that satisfy the target predicate and those that do not.", "labels": [], "entities": [{"text": "FOIL", "start_pos": 120, "end_pos": 124, "type": "METRIC", "confidence": 0.5157989859580994}]}, {"text": "Secondly, FOIL tries to learn maximally deterministic rules, while FARMER is parameterized by the minimum precision of a rule.", "labels": [], "entities": [{"text": "FOIL", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.6228362917900085}, {"text": "FARMER", "start_pos": 67, "end_pos": 73, "type": "METRIC", "confidence": 0.994411051273346}, {"text": "precision", "start_pos": 106, "end_pos": 115, "type": "METRIC", "confidence": 0.9753261804580688}]}, {"text": "We have not modified the way they interpret missing features during learning.", "labels": [], "entities": []}, {"text": "However, after the learning is complete, the rules learned by both approaches are scored by interpreting the missing data either aggressively or conservatively as described in the previous section.", "labels": [], "entities": []}, {"text": "We ran both systems on synthetic data generated using different parameters that control the fraction of missing data and the minimum support thresholdneeded for promotion.", "labels": [], "entities": []}, {"text": "In, the X and Y-axes show the fraction of missing predicates and the support threshold for the novelty mention model and aggressive scoring of rules for FOIL and FARMER.", "labels": [], "entities": [{"text": "FOIL", "start_pos": 153, "end_pos": 157, "type": "DATASET", "confidence": 0.5008411407470703}, {"text": "FARMER", "start_pos": 162, "end_pos": 168, "type": "DATASET", "confidence": 0.4658964276313782}]}, {"text": "On the Z-axis is the accuracy of predictions on the missing data, which is the fraction of the total number of initially missing entries that are correctly imputed.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 21, "end_pos": 29, "type": "METRIC", "confidence": 0.9995668530464172}]}, {"text": "We can see that aggressive scoring of rules with the novelty mention model performs very well even for large numbers of missing values for both FARMER and FOIL.", "labels": [], "entities": [{"text": "FARMER", "start_pos": 144, "end_pos": 150, "type": "METRIC", "confidence": 0.828484058380127}, {"text": "FOIL", "start_pos": 155, "end_pos": 159, "type": "METRIC", "confidence": 0.7695338129997253}]}, {"text": "FARMER's performance is more robust than FOIL's because FARMER learns all correct rules and uses whichever rule fires.", "labels": [], "entities": [{"text": "FOIL", "start_pos": 41, "end_pos": 45, "type": "DATASET", "confidence": 0.5384520292282104}]}, {"text": "For example, in the NFL domain, it could infer gameWinner from gameLoser or teamSmallerScore or teamGreaterScore.", "labels": [], "entities": []}, {"text": "In contrast, FOIL's covering strategy prevents it from learning more than one rule if it finds one perfect rule.", "labels": [], "entities": []}, {"text": "The results show that FOIL's performance degrades at larger fractions of missing data and large support thresholds.", "labels": [], "entities": [{"text": "FOIL", "start_pos": 22, "end_pos": 26, "type": "TASK", "confidence": 0.40955987572669983}]}, {"text": "Figures 1(c) and 1(d) show the accuracy of prediction vs. percentage of missing predicates for each of the mention models and the scoring methods for FARMER and FOIL fora support threshold of 120.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 31, "end_pos": 39, "type": "METRIC", "confidence": 0.999584972858429}, {"text": "FARMER", "start_pos": 150, "end_pos": 156, "type": "METRIC", "confidence": 0.5002999305725098}, {"text": "FOIL", "start_pos": 161, "end_pos": 165, "type": "METRIC", "confidence": 0.47948959469795227}]}, {"text": "They show that agressive scoring clearly outperforms conservative scoring for data generated using the novelty mention model.", "labels": [], "entities": []}, {"text": "In FOIL, aggressive scoring also seems to outperform conservative scoring on the dataset generated by the random mention model at high levels of missing data.", "labels": [], "entities": [{"text": "FOIL", "start_pos": 3, "end_pos": 7, "type": "TASK", "confidence": 0.5482157468795776}]}, {"text": "In FARMER, the two methods perform similarly.", "labels": [], "entities": [{"text": "FARMER", "start_pos": 3, "end_pos": 9, "type": "DATASET", "confidence": 0.5371867418289185}]}, {"text": "However, these results should be interpreted cautiously as they are derived from a single dataset which enjoys deterministic rules.", "labels": [], "entities": []}, {"text": "We are working towards a more robust evaluation in multiple domains as well as data extracted from natural texts.", "labels": [], "entities": []}], "tableCaptions": []}