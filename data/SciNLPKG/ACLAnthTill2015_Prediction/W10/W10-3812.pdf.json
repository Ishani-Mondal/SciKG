{"title": [{"text": "A Discriminative Syntactic Model for Source Permutation via Tree Transduction", "labels": [], "entities": [{"text": "Source Permutation via Tree Transduction", "start_pos": 37, "end_pos": 77, "type": "TASK", "confidence": 0.7588986754417419}]}], "abstractContent": [{"text": "A major challenge in statistical machine translation is mitigating the word order differences between source and target strings.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 21, "end_pos": 52, "type": "TASK", "confidence": 0.6715492407480875}]}, {"text": "While reordering and lexical translation choices are often conducted in tandem, source string permutation prior to translation is attractive for studying reordering using hierarchical and syntactic structure.", "labels": [], "entities": []}, {"text": "This work contributes an approach for learning source string permutation via transfer of the source syntax tree.", "labels": [], "entities": [{"text": "learning source string permutation", "start_pos": 38, "end_pos": 72, "type": "TASK", "confidence": 0.6820510029792786}]}, {"text": "We present a novel discriminative, probabilistic tree transduction model, and contribute a set of empirical upperbounds on translation performance for English-to-Dutch source string permutation under sequence and parse tree constraints.", "labels": [], "entities": []}, {"text": "Finally , the translation performance of our learning model is shown to outperform the state-of-the-art phrase-based system significantly .", "labels": [], "entities": [{"text": "translation", "start_pos": 14, "end_pos": 25, "type": "TASK", "confidence": 0.9605314135551453}]}], "introductionContent": [{"text": "From its beginnings, statistical machine translation (SMT) has faced a word reordering challenge that has a major impact on translation quality.", "labels": [], "entities": [{"text": "statistical machine translation (SMT)", "start_pos": 21, "end_pos": 58, "type": "TASK", "confidence": 0.8474045793215433}, {"text": "word reordering", "start_pos": 71, "end_pos": 86, "type": "TASK", "confidence": 0.7482037842273712}]}, {"text": "While standard mechanisms embedded in phrasebased SMT systems, e.g. (), deal efficiently with word reordering within a limited window of words, they are still not expected to handle all possible reorderings that involve words beyond this relatively narrow window, e.g.,).", "labels": [], "entities": [{"text": "SMT", "start_pos": 50, "end_pos": 53, "type": "TASK", "confidence": 0.751799464225769}]}, {"text": "More recent work handles word order differences between source and target languages using hierarchical methods that draw on Inversion Transduction Grammar (ITG), e.g.,).", "labels": [], "entities": []}, {"text": "In principle, the latter approach explores reordering defined by the choice of swapping the order of sibling subtrees under each node in a binary parse-tree of the source/target sentence.", "labels": [], "entities": []}, {"text": "An alternative approach aims at minimizing the need for reordering during translation by permuting the source sentence as a pre-translation step, e.g.,.", "labels": [], "entities": []}, {"text": "In effect, the translation process works with a model for source permutation (s \u2192 s \ud97b\udf59 ) followed by translation model (s \ud97b\udf59 \u2192 t), where sand tare source and target strings and s \ud97b\udf59 is the target-like permuted source string.", "labels": [], "entities": [{"text": "translation", "start_pos": 15, "end_pos": 26, "type": "TASK", "confidence": 0.9696173071861267}]}, {"text": "In how far can source permutation reduce the need for reordering in conjunction with translation is an empirical question.", "labels": [], "entities": []}, {"text": "In this paper we define source permutation as the problem of learning how to transfer a given source parse-tree into a parse-tree that minimizes the divergence from target word-order.", "labels": [], "entities": []}, {"text": "We model the tree transfer \u03c4 s \u2192 \u03c4 s \ud97b\udf59 as a sequence of local, independent transduction operations, each transforming the current intermediate tree A transduction operation merely permutes the sequence of n > 1 children of a single node in an intermediate tree, i.e., unlike previous work, we do not binarize the trees.", "labels": [], "entities": []}, {"text": "The number of permutations is factorial inn, and learning a sequence of transductions for explaining a source permutation can be computationally rather challenging (see).", "labels": [], "entities": []}, {"text": "Yet, from the limited perspective of source string permutation (s \u2192 s ), another challenge is to integrate a figure of merit that measures in how far s resembles a plausible target word-order.", "labels": [], "entities": []}, {"text": "We contribute solutions to these challenging problems.", "labels": [], "entities": []}, {"text": "Firstly, we learn the transduction operations using a discriminative estimate of where N x is the label of node (address) x, N x \u2192 \u03b1 x is the contextfree production under x, \u03c0(\u03b1 x ) is a permutation of \u03b1 x and context x represents a surrounding syntactic context.", "labels": [], "entities": []}, {"text": "As a result, this constrains {\u03c0(\u03b1 x )} only to those found in the training data, and it conditions the transduction application probability on its specific contexts.", "labels": [], "entities": []}, {"text": "Secondly, in every sequence s 0 = s, . .", "labels": [], "entities": []}, {"text": ", s n = s resulting from a tree transductions, we prefer those local transductions on \u03c4 s i\u22121 that lead to source string permutation s i that are closer to target word order than s i\u22121 ; we employ s language model probability ratios as a measure of word order improvement.", "labels": [], "entities": []}, {"text": "In how far does the assumption of source permutation provide any window for improvement over a phrase-based translation system?", "labels": [], "entities": [{"text": "phrase-based translation", "start_pos": 95, "end_pos": 119, "type": "TASK", "confidence": 0.6654648780822754}]}, {"text": "We conduct experiments on translating from English into Dutch, two languages which are characterized by a number of systematic divergences between them.", "labels": [], "entities": [{"text": "translating from English", "start_pos": 26, "end_pos": 50, "type": "TASK", "confidence": 0.8988529642422994}]}, {"text": "Initially, we conduct oracle experiments with varying constraints on source permutation to set upperbounds on performance relative to a state-of-the-art system.", "labels": [], "entities": []}, {"text": "Translating the oracle source string permutation (obtained by untangling the crossing alignments) offers a large margin of improvement, whereas the oracle parse tree permutation provides afar smaller improvement.", "labels": [], "entities": []}, {"text": "A minor change to the latter to also permute constituents that include words aligned with NULL, offers further improvement, yet lags bahind bare string permutation.", "labels": [], "entities": []}, {"text": "Subsequently, we present translation results using our learning approach, and exhibit a significant improvement in BLEU score over the state-of-the-art baseline system.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 115, "end_pos": 125, "type": "METRIC", "confidence": 0.9774061441421509}]}, {"text": "Our analysis shows that syntactic structure can provide important clues for reordering in translation, especially for dealing with long distance cases found in, e.g., English and Dutch.", "labels": [], "entities": [{"text": "translation", "start_pos": 90, "end_pos": 101, "type": "TASK", "confidence": 0.875910758972168}]}, {"text": "Yet, tree transduction by merely permuting the order of sister subtrees might turnout insufficient.", "labels": [], "entities": [{"text": "tree transduction", "start_pos": 5, "end_pos": 22, "type": "TASK", "confidence": 0.6702145338058472}]}], "datasetContent": [{"text": "The SMT system used in the experiments was implemented within the open-source MOSES toolkit ( . Standard training and weight tuning procedures which were used to build our system are explained in details on the MOSES web page 1 . The MSD model was used together with a distance-based reordering model.", "labels": [], "entities": [{"text": "SMT", "start_pos": 4, "end_pos": 7, "type": "TASK", "confidence": 0.9917005896568298}, {"text": "MOSES toolkit", "start_pos": 78, "end_pos": 91, "type": "DATASET", "confidence": 0.9425414800643921}, {"text": "MOSES web page 1", "start_pos": 211, "end_pos": 227, "type": "DATASET", "confidence": 0.9478562474250793}]}, {"text": "Word alignment was estimated with GIZA++ tool 2, coupled with mkcls 3 (Och, 1999), which allows for statistical word clustering for better generalization.", "labels": [], "entities": [{"text": "Word alignment", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.7501090466976166}, {"text": "statistical word clustering", "start_pos": 100, "end_pos": 127, "type": "TASK", "confidence": 0.6131702860196432}]}, {"text": "An 5-gram target language model was estimated using the SRI LM toolkit) and smoothed with modified Kneser-Ney discounting.", "labels": [], "entities": [{"text": "SRI LM toolkit", "start_pos": 56, "end_pos": 70, "type": "DATASET", "confidence": 0.9276015361150106}]}, {"text": "We use the Stanford parser 4 ( as a source-side parsing engine.", "labels": [], "entities": []}, {"text": "The parser was trained on the English treebank set provided with 14 syntactic categories and 48 POS tags.", "labels": [], "entities": [{"text": "English treebank set", "start_pos": 30, "end_pos": 50, "type": "DATASET", "confidence": 0.9576582511266073}]}, {"text": "The evaluation conditions were case-sensitive and included punctuation marks.", "labels": [], "entities": []}, {"text": "For Maximum Entropy modeling we used the maxent toolkit 5 . Data The experiment results were obtained using the English-Dutch corpus of the European Parliament Plenary Session transcription (EuroParl).", "labels": [], "entities": [{"text": "English-Dutch corpus of the European Parliament Plenary Session transcription (EuroParl)", "start_pos": 112, "end_pos": 200, "type": "DATASET", "confidence": 0.8563399314880371}]}, {"text": "Training corpus statistics can be found in  The development and test datasets were randomly chosen from the corpus and consisted of 500 and 1,000 sentences, respectively.", "labels": [], "entities": []}, {"text": "Both were provided with one reference translation.", "labels": [], "entities": []}, {"text": "Results Evaluation of the system performance is twofold.", "labels": [], "entities": []}, {"text": "In the first step, we analyze the quality of reordering method itself.", "labels": [], "entities": []}, {"text": "In the next step we look at the automatic translation scores and evaluate the impact which the choice of reordering strategy has on the translation quality.", "labels": [], "entities": []}, {"text": "In both stages of evaluation, the results are contrasted with the performance shown by the standard phrase-based SMT system (baseline) and with oracle results.", "labels": [], "entities": [{"text": "SMT", "start_pos": 113, "end_pos": 116, "type": "TASK", "confidence": 0.8951675295829773}]}, {"text": "Source reordering analysis shows the parameters of the reordered system allowing to assess the effectiveness of reordering permutations, namely: (1) a total number of crossings found in the word alignment (#C), (2) the size of the resulting phrase table (PT), (3) BLEU, NIST, and WER scores obtained using monotonized parallel corpus (oracle) as a reference.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 264, "end_pos": 268, "type": "METRIC", "confidence": 0.9990501999855042}, {"text": "NIST", "start_pos": 270, "end_pos": 274, "type": "METRIC", "confidence": 0.5530067682266235}, {"text": "WER", "start_pos": 280, "end_pos": 283, "type": "METRIC", "confidence": 0.9967586398124695}]}, {"text": "All the numbers are calculated on the re-aligned corpora.", "labels": [], "entities": []}, {"text": "Calculations are done on the basis of the 100,000 line extraction from the corpus", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Translation scores of oracle systems.", "labels": [], "entities": [{"text": "Translation", "start_pos": 10, "end_pos": 21, "type": "TASK", "confidence": 0.9713944792747498}]}, {"text": " Table 2: Basic statistics of the English-Dutch Eu- roParl training corpus.", "labels": [], "entities": [{"text": "English-Dutch Eu- roParl training corpus", "start_pos": 34, "end_pos": 74, "type": "DATASET", "confidence": 0.5446245521306992}]}, {"text": " Table 3: Main parameters of the tree-based re- ordering system. Translation scores The evaluation results for  the development and test corpora are reported in  Table 4. They include two baseline configurations  (dist and MSD), oracle results and contrasts them  with the performance shown by different combi- nations of single-category tree-based reordering  models. Best scores within each experimental sec- tion are placed in cells filled with grey.", "labels": [], "entities": [{"text": "Translation", "start_pos": 65, "end_pos": 76, "type": "TASK", "confidence": 0.9170022010803223}]}]}