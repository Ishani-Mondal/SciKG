{"title": [{"text": "Overview of the Chinese Word Sense Induction Task at CLP2010", "labels": [], "entities": [{"text": "Chinese Word Sense Induction", "start_pos": 16, "end_pos": 44, "type": "TASK", "confidence": 0.7163606733083725}, {"text": "CLP2010", "start_pos": 53, "end_pos": 60, "type": "DATASET", "confidence": 0.8474803566932678}]}], "abstractContent": [{"text": "In this paper, we describe the Chinese word sense induction task at CLP2010.", "labels": [], "entities": [{"text": "Chinese word sense induction task", "start_pos": 31, "end_pos": 64, "type": "TASK", "confidence": 0.6618101894855499}, {"text": "CLP2010", "start_pos": 68, "end_pos": 75, "type": "DATASET", "confidence": 0.9658542275428772}]}, {"text": "Seventeen teams participated in this task and nineteen system results were submitted.", "labels": [], "entities": []}, {"text": "All participant systems are evaluated on a dataset containing 100 target words and 5000 instances using the standard cluster evaluation.", "labels": [], "entities": []}, {"text": "We will describe the participating systems and the evaluation results, and then find the most suitable method by comparing the different Chinese word sense induction systems.", "labels": [], "entities": []}], "introductionContent": [{"text": "Word Sense Disambiguation (WSD) is an important task in natural language proceeding research and is critical to many applications which require language understanding.", "labels": [], "entities": [{"text": "Word Sense Disambiguation (WSD)", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.7785918762286504}, {"text": "natural language proceeding", "start_pos": 56, "end_pos": 83, "type": "TASK", "confidence": 0.656637171904246}]}, {"text": "In traditional evaluations, the supervised methods usually can achieve a better WSD performance than the unsupervised methods.", "labels": [], "entities": [{"text": "WSD", "start_pos": 80, "end_pos": 83, "type": "TASK", "confidence": 0.905911386013031}]}, {"text": "But the supervised WSD methods have some drawbacks: Firstly, they need large annotated dataset which is expensive to manually annotate.", "labels": [], "entities": [{"text": "WSD", "start_pos": 19, "end_pos": 22, "type": "TASK", "confidence": 0.9533942341804504}]}, {"text": "Secondly, the supervised WSD methods are based on the \"fixed-list of senses\" paradigm, i.e., the senses of a target word are represented as a closed list coming from a manually constructed dictionary ().", "labels": [], "entities": [{"text": "WSD", "start_pos": 25, "end_pos": 28, "type": "TASK", "confidence": 0.9286177158355713}]}, {"text": "Such a \"Fixed-list of senses\" paradigm suffers from the lack of explicit and topic relations between word senses, are usually cannot reflect the exact context of the target word).", "labels": [], "entities": []}, {"text": "Furthermore, because the \"fixed-list of senses\" paradigm make the fix granularity assumption of the senses distinction, it may not be suitable in different situations (.", "labels": [], "entities": []}, {"text": "Thirdly, since most supervised WSD methods assign senses based on dictionaries or other lexical resources, it will be difficult to adapt them to new domains or languages when such resources are scare.", "labels": [], "entities": [{"text": "WSD", "start_pos": 31, "end_pos": 34, "type": "TASK", "confidence": 0.9727161526679993}]}, {"text": "To overcome the deficiencies of the supervised WSD methods, many unsupervised WSD methods have been developed in recent years, which can induce word senses directly from the unannotated dataset, i.e., Word Sense Induction (WSI).", "labels": [], "entities": [{"text": "WSD", "start_pos": 47, "end_pos": 50, "type": "TASK", "confidence": 0.958160400390625}, {"text": "Word Sense Induction (WSI)", "start_pos": 201, "end_pos": 227, "type": "TASK", "confidence": 0.7306289970874786}]}, {"text": "In this sense, WSI could be treat as a clustering task, which groups the instances of the target word according to their contextual similarity, with each resulting cluster corresponding to a specific \"word sense\" or \"word use\" of the target word (in the task of WSI, the term \"word use\" is more suitable than \"word sense\").", "labels": [], "entities": []}, {"text": "Although traditional clustering techniques can be directly employed in WSI, in recent years some new methods have been proposed to enhance the WSI performance, such as the Bayesian approach and the collocation graph approach.", "labels": [], "entities": [{"text": "WSI", "start_pos": 71, "end_pos": 74, "type": "TASK", "confidence": 0.9752575755119324}, {"text": "WSI", "start_pos": 143, "end_pos": 146, "type": "TASK", "confidence": 0.9823607206344604}]}, {"text": "Both the traditional and the new methods can achieve a good performance in the task of English word sense induction.", "labels": [], "entities": [{"text": "English word sense induction", "start_pos": 87, "end_pos": 115, "type": "TASK", "confidence": 0.6750863417983055}]}, {"text": "However, the methods work well in English may not be suitable for Chinese due to the difference between Chinese and English.", "labels": [], "entities": []}, {"text": "So it is both important and critical to provide a standard testbed for the task of Chinese word sense induction (CWSI), in order to compare the performance of different Chinese WSI methods and find the methods which are suitable for the Chinese word sense induction task.", "labels": [], "entities": [{"text": "Chinese word sense induction (CWSI)", "start_pos": 83, "end_pos": 118, "type": "TASK", "confidence": 0.736803582736424}, {"text": "Chinese word sense induction task", "start_pos": 237, "end_pos": 270, "type": "TASK", "confidence": 0.6581854581832886}]}, {"text": "In this paper, we describe the Chinese word sense induction task at CLP2010.", "labels": [], "entities": [{"text": "Chinese word sense induction task", "start_pos": 31, "end_pos": 64, "type": "TASK", "confidence": 0.6618101894855499}, {"text": "CLP2010", "start_pos": 68, "end_pos": 75, "type": "DATASET", "confidence": 0.9658542275428772}]}, {"text": "The goal of this task is to provide a standard testbed for Chinese WSI task.", "labels": [], "entities": [{"text": "WSI task", "start_pos": 67, "end_pos": 75, "type": "TASK", "confidence": 0.5852055549621582}]}, {"text": "By comparing the different Chinese WSI methods, we can find the suitable methods for the Chinese word sense induction task.", "labels": [], "entities": [{"text": "Chinese word sense induction task", "start_pos": 89, "end_pos": 122, "type": "TASK", "confidence": 0.6835182487964631}]}, {"text": "This paper is organized as follow.", "labels": [], "entities": []}, {"text": "Section 2 describes the evaluation dataset in detail.", "labels": [], "entities": []}, {"text": "Section 3 demonstrates the evaluation criteria.", "labels": [], "entities": []}, {"text": "Section 3 describes the participated systems and their results.", "labels": [], "entities": []}, {"text": "The conclusions are drawn in section 4.", "labels": [], "entities": []}], "datasetContent": [{"text": "Two datasets are provided to the participants: the trial dataset and the test dataset.", "labels": [], "entities": []}, {"text": "The trial dataset contains 50 Chinese words, and for each Chinese word, a set of 50 word instances are provided.", "labels": [], "entities": []}, {"text": "All word instances are extracted from the Web and the newspapers like the Xinhua newspaper and the Renmin newspaper, and the HowNet senses of target words were manually annotated (Dong).", "labels": [], "entities": [{"text": "Xinhua newspaper", "start_pos": 74, "end_pos": 90, "type": "DATASET", "confidence": 0.9244383871555328}, {"text": "Renmin newspaper", "start_pos": 99, "end_pos": 115, "type": "DATASET", "confidence": 0.8289287686347961}]}, {"text": "shows an example of the trial data without hand-annotated tag.", "labels": [], "entities": []}, {"text": "shows an example of the trial data with hand-annotated tag.", "labels": [], "entities": []}, {"text": "In, the tag \"snum=2\" indicates that the target word \"\u675c\u9e43\" has two different senses in this dataset.", "labels": [], "entities": []}, {"text": "In each instance, the target word is marked between the tag \"<head>\" and the tag \"</head>\".", "labels": [], "entities": []}, {"text": "In, all instances between the tag \"<sense s=S0>\" and the tag \"</sense>\" are belong to the same sense class.", "labels": [], "entities": []}, {"text": "The case of the test dataset is similar to the trial dataset, but with little different in the number of target words.", "labels": [], "entities": []}, {"text": "The test dataset contains 100 target words (22 Chinese words containing one Chinese character and 78 Chinese words containing two or more Chinese ideographs).", "labels": [], "entities": []}, {"text": "shows an example of a system's output.", "labels": [], "entities": []}, {"text": "In, the first column represents the identifiers of target word, the second column represents the identifiers of instances, and the third column represents the identifiers of the resulting clusters and their weight (1.0 by default) generated by Chinese WSI systems.", "labels": [], "entities": []}, {"text": "As described in Section 1, WSI could be conceptualized as a clustering problem.", "labels": [], "entities": [{"text": "WSI", "start_pos": 27, "end_pos": 30, "type": "TASK", "confidence": 0.966437816619873}]}, {"text": "So we can measure the performance of WSI systems using the standard cluster evaluation metrics.", "labels": [], "entities": [{"text": "WSI", "start_pos": 37, "end_pos": 40, "type": "TASK", "confidence": 0.8890489339828491}]}, {"text": "As the same as, we use the FScore measure as the primary measure for assessing different WSI methods.", "labels": [], "entities": [{"text": "FScore measure", "start_pos": 27, "end_pos": 41, "type": "METRIC", "confidence": 0.9650199115276337}, {"text": "WSI", "start_pos": 89, "end_pos": 92, "type": "TASK", "confidence": 0.9720479846000671}]}, {"text": "The FScore is used in a similar way as at Information Retrieval field.", "labels": [], "entities": [{"text": "FScore", "start_pos": 4, "end_pos": 10, "type": "DATASET", "confidence": 0.4847213923931122}, {"text": "Information Retrieval field", "start_pos": 42, "end_pos": 69, "type": "TASK", "confidence": 0.841293474038442}]}, {"text": "In this case, the results of the WSI systems are treated as clusters of instances and the gold standard senses are classes.", "labels": [], "entities": []}, {"text": "Then the precision of a class with respect to a cluster is defined as the number of their mutual instances divided by the total cluster size, and the recall of a class with respect to a cluster is defined as the number of their mutual instances divided by the total class size.", "labels": [], "entities": [{"text": "precision", "start_pos": 9, "end_pos": 18, "type": "METRIC", "confidence": 0.9987429976463318}, {"text": "recall", "start_pos": 150, "end_pos": 156, "type": "METRIC", "confidence": 0.9987381100654602}]}, {"text": "The detailed definition is as bellows.", "labels": [], "entities": []}, {"text": "shows an example of a contingency table of classes and clusters, which can be used to calculate FScore.", "labels": [], "entities": [{"text": "FScore", "start_pos": 96, "end_pos": 102, "type": "DATASET", "confidence": 0.5737314224243164}]}, {"text": "Cluster 1 Cluster 2 Class 1 100 500 Class 2 400 200 Using this contingency table, we can calculate the FScore of this example is 0.7483.", "labels": [], "entities": [{"text": "FScore", "start_pos": 103, "end_pos": 109, "type": "METRIC", "confidence": 0.9979898929595947}]}, {"text": "It is easy to know the FScore of a perfect clustering solution will be equal to one, where each cluster has exactly the same instances as one of the classes, and vice versa.", "labels": [], "entities": [{"text": "FScore", "start_pos": 23, "end_pos": 29, "type": "METRIC", "confidence": 0.9988677501678467}]}, {"text": "This means that the higher the FScore, the better the clustering performance.", "labels": [], "entities": [{"text": "FScore", "start_pos": 31, "end_pos": 37, "type": "METRIC", "confidence": 0.7412979006767273}, {"text": "clustering", "start_pos": 54, "end_pos": 64, "type": "TASK", "confidence": 0.949833869934082}]}, {"text": "Purity and entropy) are also used to measure the performance of the clustering solution.", "labels": [], "entities": []}, {"text": "Compared to FScore, they have some disadvantages.", "labels": [], "entities": [{"text": "FScore", "start_pos": 12, "end_pos": 18, "type": "DATASET", "confidence": 0.8003320693969727}]}, {"text": "FScore uses two complementary concepts, precision and recall, to assess the quality of a clustering solution.", "labels": [], "entities": [{"text": "FScore", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.9006465077400208}, {"text": "precision", "start_pos": 40, "end_pos": 49, "type": "METRIC", "confidence": 0.9994271993637085}, {"text": "recall", "start_pos": 54, "end_pos": 60, "type": "METRIC", "confidence": 0.9989104270935059}]}, {"text": "Precision indicates the degree of the instances that makeup a cluster, which belong to a single class.", "labels": [], "entities": [{"text": "Precision", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.9838064908981323}]}, {"text": "On the other hand, recall indicates the degree of the instances that makeup a class, which belong to a single cluster.", "labels": [], "entities": [{"text": "recall", "start_pos": 19, "end_pos": 25, "type": "METRIC", "confidence": 0.9982494711875916}]}, {"text": "But purity and entropy only consider one factor and discard another.", "labels": [], "entities": [{"text": "purity", "start_pos": 4, "end_pos": 10, "type": "METRIC", "confidence": 0.9671980142593384}]}, {"text": "So we use FScore measure to assess a clustering solution.", "labels": [], "entities": [{"text": "FScore measure", "start_pos": 10, "end_pos": 24, "type": "METRIC", "confidence": 0.8889101147651672}]}, {"text": "For the sake of completeness, we also employ the V-Measure to assess different clustering solutions.", "labels": [], "entities": []}, {"text": "V-Measure assesses a cluster solution by considering its homogeneity and its completeness.", "labels": [], "entities": []}, {"text": "Homogeneity measures the degree that each cluster contains data points which belong to a single Gold Standard class.", "labels": [], "entities": [{"text": "Gold Standard class", "start_pos": 96, "end_pos": 115, "type": "DATASET", "confidence": 0.905891478061676}]}, {"text": "And completeness measures the degree that each Gold Standard class contains data points assigned to a single cluster.", "labels": [], "entities": [{"text": "Gold Standard class contains data", "start_pos": 47, "end_pos": 80, "type": "DATASET", "confidence": 0.9098030209541321}]}, {"text": "In general, the larger the V-Measure, the better the clustering performance.", "labels": [], "entities": [{"text": "clustering", "start_pos": 53, "end_pos": 63, "type": "TASK", "confidence": 0.9526870846748352}]}, {"text": "More details can be referred to).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: FScore of main systems on the test  dataset including one baseline -1c1w.", "labels": [], "entities": [{"text": "FScore", "start_pos": 10, "end_pos": 16, "type": "METRIC", "confidence": 0.9761292934417725}]}, {"text": " Table 4: FScore and V-Measure of all systems,  including one baseline.", "labels": [], "entities": [{"text": "FScore", "start_pos": 10, "end_pos": 16, "type": "METRIC", "confidence": 0.7226932644844055}]}, {"text": " Table 5: FScore of all systems on the dataset  only containing either single characters or  words respectively.", "labels": [], "entities": [{"text": "FScore", "start_pos": 10, "end_pos": 16, "type": "METRIC", "confidence": 0.9155315160751343}]}]}