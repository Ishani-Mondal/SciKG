{"title": [{"text": "Sentiment Classification using Automatically Extracted Subgraph Features", "labels": [], "entities": [{"text": "Sentiment Classification", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.9848823249340057}]}], "abstractContent": [{"text": "In this work, we propose a novel representation of text based on patterns derived from linguistic annotation graphs.", "labels": [], "entities": []}, {"text": "We use a subgraph mining algorithm to automatically derive features as frequent subgraphs from the annotation graph.", "labels": [], "entities": [{"text": "subgraph mining", "start_pos": 9, "end_pos": 24, "type": "TASK", "confidence": 0.7741804718971252}]}, {"text": "This process generates a very large number of features, many of which are highly correlated.", "labels": [], "entities": []}, {"text": "We propose a genetic programming based approach to feature construction which creates a fixed number of strong classification predictors from these subgraphs.", "labels": [], "entities": [{"text": "feature construction", "start_pos": 51, "end_pos": 71, "type": "TASK", "confidence": 0.7469639480113983}]}, {"text": "We evaluate the benefit gained from evolved struc-tured features, when used in addition to the bag-of-words features, fora sentiment classification task.", "labels": [], "entities": [{"text": "sentiment classification task", "start_pos": 123, "end_pos": 152, "type": "TASK", "confidence": 0.9061482151349386}]}], "introductionContent": [{"text": "In recent years, the topic of sentiment analysis has been one of the more popular directions in the field of language technologies.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 30, "end_pos": 48, "type": "TASK", "confidence": 0.9525773823261261}]}, {"text": "Recent work in supervised sentiment analysis has focused on innovative approaches to feature creation, with the greatest improvements in performance with features that insightfully capture the essence of the linguistic constructions used to express sentiment, e.g. (), In this spirit, we present a novel approach that leverages subgraphs automatically extracted from linguistic annotation graphs using efficient subgraph mining algorithms).", "labels": [], "entities": [{"text": "supervised sentiment analysis", "start_pos": 15, "end_pos": 44, "type": "TASK", "confidence": 0.6774010062217712}, {"text": "feature creation", "start_pos": 85, "end_pos": 101, "type": "TASK", "confidence": 0.7421389520168304}]}, {"text": "The difficulty with automatically deriving complex features comes with the increased feature space size.", "labels": [], "entities": []}, {"text": "Many of these features are highly correlated and do not provide any new information to the model.", "labels": [], "entities": []}, {"text": "For example, a feature of type unigram POS (e.g. \"camera NN\") doesn't provide any additional information beyond the unigram feature (e.g. \"camera\"), for words that are often used with the same part of speech.", "labels": [], "entities": []}, {"text": "However, alongside several redundant features, there are also features that provide new information.", "labels": [], "entities": []}, {"text": "It is these features that we aim to capture.", "labels": [], "entities": []}, {"text": "In this work, we propose an evolutionary approach that constructs complex features from subgraphs extracted from an annotation graph.", "labels": [], "entities": []}, {"text": "A constant number of these features are added to the unigram feature space, adding much of the representational benefits without the computational cost of a drastic increase in feature space size.", "labels": [], "entities": []}, {"text": "In the remainder of the paper, we review prior work on features commonly used for sentiment analysis.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 82, "end_pos": 100, "type": "TASK", "confidence": 0.9724183678627014}]}, {"text": "We then describe the annotation graph representation proposed by.", "labels": [], "entities": []}, {"text": "Following this, we describe the frequent subgraph mining algorithm proposed in, and used in this work to extract frequent subgraphs from the annotation graphs.", "labels": [], "entities": [{"text": "frequent subgraph mining", "start_pos": 32, "end_pos": 56, "type": "TASK", "confidence": 0.6675018668174744}]}, {"text": "We then introduce our novel feature evolution approach, and discuss our experimental setup and results.", "labels": [], "entities": [{"text": "feature evolution", "start_pos": 28, "end_pos": 45, "type": "TASK", "confidence": 0.7238013744354248}]}, {"text": "Subgraph features combined with the feature evolution approach gives promising results, with an improvement in performance over the baseline.", "labels": [], "entities": [{"text": "feature evolution", "start_pos": 36, "end_pos": 53, "type": "TASK", "confidence": 0.6832771748304367}]}], "datasetContent": [{"text": "We evaluate our approach on a sentiment classification task, where the goal is to classify a movie review sentence as expressing positive or negative sentiment towards the movie.", "labels": [], "entities": [{"text": "sentiment classification task", "start_pos": 30, "end_pos": 59, "type": "TASK", "confidence": 0.8863997658093771}]}, {"text": "Data: The dataset consists of snippets from Rotten Tomatoes (Pang and . It consists of 10662 snippets/sentences total with equal number positive and negative sentences (5331 each).", "labels": [], "entities": [{"text": "Rotten Tomatoes", "start_pos": 44, "end_pos": 59, "type": "DATASET", "confidence": 0.9480017721652985}, {"text": "Pang", "start_pos": 61, "end_pos": 65, "type": "DATASET", "confidence": 0.8951950669288635}]}, {"text": "This dataset was created and used by to train a classifier for identifying positive sentences in a full length review.", "labels": [], "entities": []}, {"text": "We use the first 8000 (4000 positive, 4000 negative) sentences as training data and evaluate on remaining 2662 (1331 positive, 1331 negative) sentences.", "labels": [], "entities": []}, {"text": "We added part of speech and dependency triple annotations to this data using the Stanford parser (.", "labels": [], "entities": []}, {"text": "Annotation Graph: For the annotation graph representation, we used Unigrams (U), Part of Speech (P) and Dependency Relation Type (D) as labels for the nodes, and ParentOfGov and ParentOfDep as labels for the edges.", "labels": [], "entities": []}, {"text": "For a dependency triple such as \"amod good movie\", five nodes are added to the annotation graph as shown in.", "labels": [], "entities": []}, {"text": "ParentOfGov and ParentOfDep edges are added from the 6 http://www.cs.cornell.edu/people/pabo/ movie-review-data/rt-polaritydata.tar.gz dependency relation node D amod to the unigram nodes U good and U movie.", "labels": [], "entities": []}, {"text": "These edges are also added for the part of speech nodes that correspond to the two unigrams in the dependency relation, as shown in.", "labels": [], "entities": []}, {"text": "This allows the algorithm to find general patterns, based on a dependency relation between two part of speech nodes, two unigram nodes or a combination of the two.", "labels": [], "entities": []}, {"text": "For example, a subgraph in(b) captures a general pattern where good modifies a noun.", "labels": [], "entities": []}, {"text": "This feature exists in \"amod good movie\", \"amod good camera\" and other similar dependency triples.", "labels": [], "entities": []}, {"text": "This feature is similar to the the dependency back-off features proposed in.", "labels": [], "entities": []}, {"text": "The extra edges are an alternative to putting wild cards on words, as proposed in section 3.", "labels": [], "entities": []}, {"text": "On the other hand, putting a wildcard on every word in the annotation graph for our example), will only give features based on dependency relations between part of speech annotations.", "labels": [], "entities": []}, {"text": "Thus, the wildcard based approach is more restrictive than 136 adding more edges.", "labels": [], "entities": []}, {"text": "However, with lots of edges, the complexity of the subgraph mining algorithm and the number of subgraph features increases tremendously.", "labels": [], "entities": [{"text": "subgraph mining", "start_pos": 51, "end_pos": 66, "type": "TASK", "confidence": 0.8136919140815735}]}, {"text": "Classifier: For our experiments we use Support Vector Machines (SVM) with a linear kernel.", "labels": [], "entities": []}, {"text": "We use the SVM-light 7 implementation of SVM with default settings.", "labels": [], "entities": []}, {"text": "Parameters: The gSpan algorithm requires setting the minimum support threshold (minsup) for the subgraph patterns to extract.", "labels": [], "entities": [{"text": "support threshold (minsup)", "start_pos": 61, "end_pos": 87, "type": "METRIC", "confidence": 0.6534364342689514}]}, {"text": "Support fora subgraph is the number of graphs in the dataset that contain the subgraph.", "labels": [], "entities": []}, {"text": "We experimented with several values for minimum support and minsup = 2 gave us the best performance.", "labels": [], "entities": []}, {"text": "For Genetic Programming, we used the same parameter settings as described in, which were tuned on a different dataset 8 than one used in this work, but it is from the same movie review domain.", "labels": [], "entities": [{"text": "Genetic Programming", "start_pos": 4, "end_pos": 23, "type": "TASK", "confidence": 0.8613848090171814}]}, {"text": "We also consider one alteration to these settings.", "labels": [], "entities": []}, {"text": "As we are introducing many new and highly correlated features to our feature space through subgraphs, we believe that a stricter constraint must be placed on correlation between features.", "labels": [], "entities": []}, {"text": "To accomplish this, we can set our correlation penalty cutoff to 0.3, lower than the 0.5 cutoff used in prior work.", "labels": [], "entities": [{"text": "correlation penalty cutoff", "start_pos": 35, "end_pos": 61, "type": "METRIC", "confidence": 0.9808190663655599}]}, {"text": "Results for both settings are reported.", "labels": [], "entities": []}, {"text": "Baselines: To the best of our knowledge, there is no supervised machine learning result published on this dataset.", "labels": [], "entities": []}, {"text": "We compare our results with the following baselines: \u2022 Unigram-only Baseline: In sentiment analysis, unigram-only features have been a strong baseline ().", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 81, "end_pos": 99, "type": "TASK", "confidence": 0.9389410316944122}]}, {"text": "We only use unigrams that occur in at least two sentences of the training data same as.", "labels": [], "entities": []}, {"text": "We also filter out stop words using a small stop word list 9 . \u2022 \u03c7 2 Baseline: For our training data, after filtering infrequent unigrams and stop words, we get 8424 features.", "labels": [], "entities": []}, {"text": "Adding subgraph features increases the total number of features to 44, 161, a factor of 5 increase in size.", "labels": [], "entities": []}, {"text": "Feature selection can be used to reduce this size by selecting the most discriminative features.", "labels": [], "entities": [{"text": "Feature selection", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.7613974213600159}]}, {"text": "\u03c7 2 feature selection () is commonly used in the literature.", "labels": [], "entities": []}, {"text": "We compare two methods of feature selection with \u03c7 2 , one which rejects features if their \u03c7 2 score is not significant at the 0.05 level, and one that reduces the number of features to match the size of our feature space with GP.", "labels": [], "entities": [{"text": "feature selection", "start_pos": 26, "end_pos": 43, "type": "TASK", "confidence": 0.6919759064912796}]}, {"text": "\u2022 Feature Subsumption (FS): Following the idea in, a complex feature where IG is Information Gain and S is a simple feature that representationally subsumes C, i.e. the text spans that match S area superset of the text spans that match C.", "labels": [], "entities": [{"text": "Feature Subsumption (FS)", "start_pos": 2, "end_pos": 26, "type": "TASK", "confidence": 0.7373399257659912}]}, {"text": "In our work, complex features are subgraph features and simple features are unigram features contained in them.", "labels": [], "entities": []}, {"text": "For example, (D amod) Edge P arentOf Dep (U bad) is a complex feature for which U bad is a simple feature.", "labels": [], "entities": [{"text": "D amod) Edge P arentOf Dep (U bad)", "start_pos": 14, "end_pos": 48, "type": "METRIC", "confidence": 0.6866128173741427}]}, {"text": "We tried same values for \u03b4 \u2208 {0.002, 0.001, 0.0005}, as suggested in.", "labels": [], "entities": []}, {"text": "Since all values gave us same number of features, we only report a single result for feature subsumption.", "labels": [], "entities": []}, {"text": "\u2022 Correlation (Corr): As mentioned earlier, some of the subgraph features are highly correlated with unigram features and do not provide new knowledge.", "labels": [], "entities": [{"text": "Correlation (Corr)", "start_pos": 2, "end_pos": 20, "type": "METRIC", "confidence": 0.8556616455316544}]}, {"text": "A correlation based filter for subgraph features can be used to discard a complex feature C if its absolute correlation with its simpler feature (unigram feature) is more than a certain threshold.", "labels": [], "entities": []}, {"text": "We use the same threshold as used in the GP criterion, but as a hard filter instead of a penalty.", "labels": [], "entities": [{"text": "GP", "start_pos": 41, "end_pos": 43, "type": "METRIC", "confidence": 0.771166980266571}]}], "tableCaptions": []}