{"title": [{"text": "Adaptive Chinese Word Segmentation with Online Passive-Aggressive Algorithm", "labels": [], "entities": [{"text": "Adaptive Chinese Word Segmentation", "start_pos": 0, "end_pos": 34, "type": "TASK", "confidence": 0.7605927437543869}]}], "abstractContent": [{"text": "In this paper, we describe our system 1 for CIPS-SIGHAN-2010 bake-off task of Chinese word segmentation, which fo-cused on the cross-domain performance of Chinese word segmentation algorithms.", "labels": [], "entities": [{"text": "Chinese word segmentation", "start_pos": 78, "end_pos": 103, "type": "TASK", "confidence": 0.5806844830513}, {"text": "Chinese word segmentation", "start_pos": 155, "end_pos": 180, "type": "TASK", "confidence": 0.6724029183387756}]}, {"text": "We use the online passive-aggressive algorithm with domain invariant information for cross-domain Chinese word seg-mentation.", "labels": [], "entities": []}], "introductionContent": [{"text": "In recent years, Chinese word segmentation (CWS) has undergone great development).", "labels": [], "entities": [{"text": "Chinese word segmentation (CWS)", "start_pos": 17, "end_pos": 48, "type": "TASK", "confidence": 0.7605462322632471}]}, {"text": "The popular method is to regard word segmentation as a sequence labeling problems.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 32, "end_pos": 49, "type": "TASK", "confidence": 0.757914811372757}]}, {"text": "The goal of sequence labeling is to assign labels to all elements of a sequence.", "labels": [], "entities": [{"text": "sequence labeling", "start_pos": 12, "end_pos": 29, "type": "TASK", "confidence": 0.6572625637054443}]}, {"text": "Due to the exponential size of the output space, sequence labeling problems tend to be more challenging than the conventional classification problems.", "labels": [], "entities": [{"text": "sequence labeling", "start_pos": 49, "end_pos": 66, "type": "TASK", "confidence": 0.8018371760845184}]}, {"text": "Many algorithms have been proposed and the progress has been encouraging, such as SVM struct), conditional random fields (CRF) (, maximum margin Markov networks (M3N) () and soon.", "labels": [], "entities": []}, {"text": "After years of intensive researches, Chinese word segmentation achieves a quite high precision.", "labels": [], "entities": [{"text": "Chinese word segmentation", "start_pos": 37, "end_pos": 62, "type": "TASK", "confidence": 0.5845452050367991}, {"text": "precision", "start_pos": 85, "end_pos": 94, "type": "METRIC", "confidence": 0.9958125948905945}]}, {"text": "However, the performance of segmentation is not so satisfying for out-of-domain text.", "labels": [], "entities": [{"text": "segmentation", "start_pos": 28, "end_pos": 40, "type": "TASK", "confidence": 0.9710219502449036}]}, {"text": "There are two domains in domain adaption problem, a source domain and a target domain.", "labels": [], "entities": [{"text": "domain adaption problem", "start_pos": 25, "end_pos": 48, "type": "TASK", "confidence": 0.7617414792378744}]}, {"text": "When we use the machine learning methods for Chinese word segmentation, we assume that training and test data are drawn from the same distribution.", "labels": [], "entities": [{"text": "Chinese word segmentation", "start_pos": 45, "end_pos": 70, "type": "TASK", "confidence": 0.5931397477785746}]}, {"text": "This assumption underlies both theoretical analysis and experimental evaluations of learning algorithms.", "labels": [], "entities": []}, {"text": "However, the assumption does not hold for domain adaptation).", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 42, "end_pos": 59, "type": "TASK", "confidence": 0.7504211068153381}]}, {"text": "The challenge is the difference of distribution between the source and target domains.", "labels": [], "entities": []}, {"text": "In this paper, we use online margin maximization algorithm and domain invariant features for domain adaptive CWS.", "labels": [], "entities": []}, {"text": "The online learning algorithm is Passive-Aggressive (PA) algorithm(), which passively accepts a solution whose loss is zero, while it aggressively forces the new prototype vector to stay as close as possible to the one previously learned.", "labels": [], "entities": []}, {"text": "The rest of the paper is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 introduces the related works.", "labels": [], "entities": []}, {"text": "Then we describe our algorithm in section 3 and 4.", "labels": [], "entities": []}, {"text": "The feature templates are described in section 5.", "labels": [], "entities": []}, {"text": "Section 6 gives the experimental analysis.", "labels": [], "entities": []}, {"text": "Section 7 concludes the paper.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 2: Evaluation results on simplified corpus  R  P  F1  OOV RR IV RR  Best 0.945 0.946 0.946  0.816  0.954  Literature Our 0.915 0.925 0.92  0.577  0.94  Best 0.953 0.95 0.951  0.827  0.975  Computer Our 0.934 0.919 0.926  0.739  0.969  Best 0.942 0.936 0.939  0.75  0.965  Medicine Our 0.927 0.924 0.925  0.714  0.953  Best 0.959 0.96 0.959  0.827  0.972  Finance  Our  0.94 0.942 0.941  0.719  0.961", "labels": [], "entities": [{"text": "simplified corpus  R  P  F1  OOV RR IV RR", "start_pos": 32, "end_pos": 73, "type": "DATASET", "confidence": 0.640566349029541}]}, {"text": " Table 3: Evaluation results on traditional corpus  R  P  F1  OOV RR IV RR  Best  0.942 0.942 0.942  0.788  0.958  Literature Our  0.869  0.91 0.889  0.698  0.887  Best  0.948 0.957 0.952  0.666  0.977  Computer Our  0.933 0.949 0.941  0.791  0.948  Best  0.953 0.957 0.955  0.798  0.966  Medicine Our  0.908 0.932 0.92  0.771  0.919  Best  0.964 0.962 0.963  0.812  0.975  Finance  Our 00.925 0.939 0.932  0.793  0.935", "labels": [], "entities": [{"text": "traditional corpus  R  P  F1  OOV RR IV RR", "start_pos": 32, "end_pos": 74, "type": "DATASET", "confidence": 0.7348672151565552}]}]}