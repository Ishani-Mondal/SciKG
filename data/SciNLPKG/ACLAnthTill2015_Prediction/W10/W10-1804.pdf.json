{"title": [{"text": "Agile Corpus Annotation in Practice: An Overview of Manual and Automatic Annotation of CVs", "labels": [], "entities": []}], "abstractContent": [{"text": "This paper describes work testing agile data annotation by moving away from the traditional, linear phases of corpus creation towards iterative ones and by recognizing the potential for sources of error occurring throughout the annotation process.", "labels": [], "entities": []}], "introductionContent": [{"text": "Annotated data sets are an important resources for various research fields, including natural language processing (NLP) and text mining (TM).", "labels": [], "entities": [{"text": "natural language processing (NLP)", "start_pos": 86, "end_pos": 119, "type": "TASK", "confidence": 0.8008889953295389}, {"text": "text mining (TM)", "start_pos": 124, "end_pos": 140, "type": "TASK", "confidence": 0.8599585056304931}]}, {"text": "While the detection of annotation inconsistencies in different data sets has been investigated (e.g. and their effect on NLP performance has been studied (e.g.), very little work has been done on deriving better methods of annotation as a whole process in order to maximize both the quality and quantity of annotated data.", "labels": [], "entities": []}, {"text": "This paper describes our annotation project in which we tested the relatively new approach of agile corpus annotation ( of moving away from the traditional, linear phases of corpus creation towards iterative ones and of recognizing the fact that sources of error can occur throughout the annotation process.", "labels": [], "entities": []}, {"text": "We explain agile annotation and discuss related work in Section 2.", "labels": [], "entities": []}, {"text": "Section 3 describes the entire annotation process and all its aspects.", "labels": [], "entities": []}, {"text": "We provide details on the data collection and preparation, the annotation tool, the annotators and the annotation phases.", "labels": [], "entities": []}, {"text": "Section 4 describes the final annotation scheme and Section 5 presents inter-annotatoragreement (IAA) figures measured throughout the annotation.", "labels": [], "entities": [{"text": "inter-annotatoragreement (IAA)", "start_pos": 71, "end_pos": 101, "type": "METRIC", "confidence": 0.9477016925811768}]}, {"text": "In Section 6, we summarize the performance of the machine-learning (ML)-based TM components which were trained and evaluated on the annotated data.", "labels": [], "entities": []}, {"text": "We discuss our findings and conclude in Section 7.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Number of files and annotated files in each  section of the CV data set.", "labels": [], "entities": [{"text": "CV data set", "start_pos": 70, "end_pos": 81, "type": "DATASET", "confidence": 0.964176078637441}]}, {"text": " Table 4: IAA for NEs, zone titles and relations in precision (P), recall (R) and F 1 at two stages in the  annotation process: (1) at the end of the second pilot annotation and (2) at the end of the main annotation  phase; as well as automatic annotation scores (3) on the blind TEST set. The total number of true positives  (TPs) is shown to provide an idea of the quantities of markables in each set.", "labels": [], "entities": [{"text": "IAA", "start_pos": 10, "end_pos": 13, "type": "METRIC", "confidence": 0.9933183193206787}, {"text": "precision (P)", "start_pos": 52, "end_pos": 65, "type": "METRIC", "confidence": 0.9252822995185852}, {"text": "recall (R)", "start_pos": 67, "end_pos": 77, "type": "METRIC", "confidence": 0.9406808018684387}, {"text": "F 1", "start_pos": 82, "end_pos": 85, "type": "METRIC", "confidence": 0.9808627367019653}]}]}