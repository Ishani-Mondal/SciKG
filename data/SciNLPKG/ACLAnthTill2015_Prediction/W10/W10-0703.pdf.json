{"title": [{"text": "Clustering dictionary definitions using Amazon Mechanical Turk", "labels": [], "entities": [{"text": "Amazon Mechanical Turk", "start_pos": 40, "end_pos": 62, "type": "DATASET", "confidence": 0.9192232688268026}]}], "abstractContent": [{"text": "Vocabulary tutors need word sense disambig-uation (WSD) in order to provide exercises and assessments that match the sense of words being taught.", "labels": [], "entities": [{"text": "word sense disambig-uation (WSD)", "start_pos": 23, "end_pos": 55, "type": "TASK", "confidence": 0.655663917462031}]}, {"text": "Using expert annotators to build a WSD training set for all the words supported would be too expensive.", "labels": [], "entities": [{"text": "WSD", "start_pos": 35, "end_pos": 38, "type": "TASK", "confidence": 0.909326434135437}]}, {"text": "Crowdsourcing that task seems to be a good solution.", "labels": [], "entities": []}, {"text": "However, a first required step is to define what the possible sense labels to assign to word occurrence are.", "labels": [], "entities": []}, {"text": "This can be viewed as a clustering task on dictionary definitions.", "labels": [], "entities": []}, {"text": "This paper evaluates the possibility of using Amazon Mechanical Turk (MTurk) to carryout that prerequisite step to WSD.", "labels": [], "entities": [{"text": "Amazon Mechanical Turk (MTurk)", "start_pos": 46, "end_pos": 76, "type": "DATASET", "confidence": 0.8397351205348969}, {"text": "WSD", "start_pos": 115, "end_pos": 118, "type": "TASK", "confidence": 0.9248289465904236}]}, {"text": "We propose two different approaches to using a crowd to accomplish clustering: one where the worker has a global view of the task, and one where only a local view is available.", "labels": [], "entities": []}, {"text": "We discuss how we can aggregate multiple workers\" clusters together, as well as pros and cons of our two approaches.", "labels": [], "entities": []}, {"text": "We show that either approach has an inte-rannotator agreement with experts that corresponds to the agreement between experts , and so using MTurk to cluster dictionary definitions appears to be a reliable approach.", "labels": [], "entities": []}], "introductionContent": [{"text": "For some applications it is useful to disambiguate the meanings of a polysemous word.", "labels": [], "entities": []}, {"text": "For example, if we show a student a text containing a word like \"bank\" and then automatically generate questions about the meaning of that word as it appeared in the text (say as the bank of a river), we would like to have the meaning of the word in the questions match the text meaning.", "labels": [], "entities": []}, {"text": "Teachers do this each time they assess a student on vocabulary knowledge.", "labels": [], "entities": []}, {"text": "For intelligent tutoring systems, two options are available.", "labels": [], "entities": []}, {"text": "The first one is to ask a teacher to go through all the material and label each appearance of a polysemous word with its sense.", "labels": [], "entities": []}, {"text": "This option is used only if there is a relatively small quantity of material.", "labels": [], "entities": []}, {"text": "Beyond that, automatic processing, known as Word Sense Disambiguation (WSD) is essential.", "labels": [], "entities": [{"text": "Word Sense Disambiguation (WSD)", "start_pos": 44, "end_pos": 75, "type": "TASK", "confidence": 0.7526070773601532}]}, {"text": "Most approaches are supervised and need large amounts of data to train the classifier for each and every word that is to be taught and assessed.", "labels": [], "entities": []}, {"text": "Amazon Mechanical Turk (MTurk) has been used for the purpose of word sense disambiguation ().", "labels": [], "entities": [{"text": "Amazon Mechanical Turk (MTurk)", "start_pos": 0, "end_pos": 30, "type": "DATASET", "confidence": 0.8602388103802999}, {"text": "word sense disambiguation", "start_pos": 64, "end_pos": 89, "type": "TASK", "confidence": 0.7892162799835205}]}, {"text": "The results show that nonexperts do very well (100% accuracy) when asked to identify the correct sense of a word out of a finite set of labels created by an expert.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 52, "end_pos": 60, "type": "METRIC", "confidence": 0.9974332451820374}]}, {"text": "It is therefore possible to use MTurk to build a training corpus for WSD.", "labels": [], "entities": [{"text": "MTurk", "start_pos": 32, "end_pos": 37, "type": "DATASET", "confidence": 0.8182714581489563}, {"text": "WSD", "start_pos": 69, "end_pos": 72, "type": "TASK", "confidence": 0.937633216381073}]}, {"text": "In order to extend the Snow et al crowdsourced disambiguation to a large number of words, we need an efficient way to create the set of senses of a word.", "labels": [], "entities": []}, {"text": "Asking an expert to do this is costly in time and money.", "labels": [], "entities": []}, {"text": "Thus it is necessary to have an efficient Word Sense Induction (WSI) system.", "labels": [], "entities": [{"text": "Word Sense Induction (WSI)", "start_pos": 42, "end_pos": 68, "type": "TASK", "confidence": 0.7274881651004156}]}, {"text": "A WSI system induces the different senses of a word and provides the corresponding sense labels.", "labels": [], "entities": []}, {"text": "This is the first step to crowdsourcing WSD on a large scale.", "labels": [], "entities": [{"text": "crowdsourcing WSD", "start_pos": 26, "end_pos": 43, "type": "TASK", "confidence": 0.7353297770023346}]}, {"text": "While many studies have shown that MTurk can be used for labeling tasks, to rate automatically constructed artifacts) and to transcribe speech (), to our knowledge, there has not been much work on evaluating the use of MTurk for clustering tasks.", "labels": [], "entities": [{"text": "labeling tasks", "start_pos": 57, "end_pos": 71, "type": "TASK", "confidence": 0.8968048095703125}]}, {"text": "The goal of this paper is to investigate different options available to crowdsource a clustering task and evaluate their efficiency in the concrete application of word sense induction.", "labels": [], "entities": [{"text": "word sense induction", "start_pos": 163, "end_pos": 183, "type": "TASK", "confidence": 0.7584484020868937}]}], "datasetContent": [{"text": "In order to evaluate our two approaches, we created a gold-standard (GS).", "labels": [], "entities": [{"text": "gold-standard (GS)", "start_pos": 54, "end_pos": 72, "type": "METRIC", "confidence": 0.6589235737919807}]}, {"text": "Since the task of WSI is strongly influenced by an annotator\"s grain size preference for the senses, four expert annotators were asked to create the GS.", "labels": [], "entities": [{"text": "WSI", "start_pos": 18, "end_pos": 21, "type": "TASK", "confidence": 0.7977017164230347}]}, {"text": "The literature offers many metrics to compare two annotators\" clustering solutions (Purity and Entropy), clustering F-Measure () and many others).", "labels": [], "entities": []}, {"text": "SemEval-2 includes a WSI task where V-Measure () is used to evaluate the clustering solutions.", "labels": [], "entities": [{"text": "WSI task", "start_pos": 21, "end_pos": 29, "type": "TASK", "confidence": 0.8424606323242188}]}, {"text": "V-Measure involves two metrics, homogeneity and completeness, that can bethought of as precision and recall.", "labels": [], "entities": [{"text": "completeness", "start_pos": 48, "end_pos": 60, "type": "METRIC", "confidence": 0.9558782577514648}, {"text": "precision", "start_pos": 87, "end_pos": 96, "type": "METRIC", "confidence": 0.9994891881942749}, {"text": "recall", "start_pos": 101, "end_pos": 107, "type": "METRIC", "confidence": 0.9962745904922485}]}, {"text": "Perfect homogeneity is obtained if the solutions have clusters whose data points belong to a single cluster in the GS.", "labels": [], "entities": [{"text": "GS", "start_pos": 115, "end_pos": 117, "type": "DATASET", "confidence": 0.9406014084815979}]}, {"text": "Perfect completeness is obtained if the clusters in the GS contain data points that belong to a single cluster in the evaluated solution.", "labels": [], "entities": []}, {"text": "The V-Measure is a (weighted) harmonic mean of the homogeneity and of the completeness metrics.", "labels": [], "entities": []}, {"text": "shows interannotator agreement (ITA) among four experts on the test dataset, using the average V-Measure overall the 50 sense clusters.", "labels": [], "entities": [{"text": "interannotator agreement (ITA)", "start_pos": 6, "end_pos": 36, "type": "METRIC", "confidence": 0.921620535850525}, {"text": "V-Measure", "start_pos": 95, "end_pos": 104, "type": "METRIC", "confidence": 0.8375114798545837}]}, {"text": "GS #1 GS #2 GS #3 GS #4 GS #1 1,000 0,850 0,766 0,770 GS #2 0,850 1,000 0,763 0,796 GS #3 0,766 0,763 1,000 0,689 GS #4 0,770 0,796 0,689 1,000 We can obtain the agreement between one expert and the three others by averaging the three VMeasures.", "labels": [], "entities": [{"text": "VMeasures", "start_pos": 235, "end_pos": 244, "type": "DATASET", "confidence": 0.7021530270576477}]}, {"text": "We finally obtain an \"Experts vs. Experts\" ITA of 0.772 by averaging this value for all of our experts.", "labels": [], "entities": [{"text": "ITA", "start_pos": 43, "end_pos": 46, "type": "METRIC", "confidence": 0.9686699509620667}]}, {"text": "The standard deviation for this ITA is 0.031.To be considered reliable, non-expert clustering would have to agree with the 4 experts with a similar result.", "labels": [], "entities": []}], "tableCaptions": []}