{"title": [{"text": "LDA Based Similarity Modeling for Question Answering", "labels": [], "entities": [{"text": "LDA Based Similarity Modeling", "start_pos": 0, "end_pos": 29, "type": "TASK", "confidence": 0.5396575331687927}, {"text": "Question Answering", "start_pos": 34, "end_pos": 52, "type": "TASK", "confidence": 0.7606762051582336}]}], "abstractContent": [{"text": "We present an exploration of generative mod-eling for the question answering (QA) task to rank candidate passages.", "labels": [], "entities": [{"text": "question answering (QA) task", "start_pos": 58, "end_pos": 86, "type": "TASK", "confidence": 0.8244563639163971}]}, {"text": "We investigate Latent Dirichlet Allocation (LDA) models to obtain ranking scores based on a novel similarity measure between a natural language question posed by the user and a candidate passage.", "labels": [], "entities": []}, {"text": "We construct two models each one introducing deeper evaluations on latent characteristics of passages together with given question.", "labels": [], "entities": []}, {"text": "With the new representation of topical structures on QA datasets, using a limited amount of world knowledge, we show improvements on performance of a QA ranking system.", "labels": [], "entities": [{"text": "QA datasets", "start_pos": 53, "end_pos": 64, "type": "DATASET", "confidence": 0.7891414165496826}]}], "introductionContent": [{"text": "Question Answering (QA) is a task of automatic retrieval of an answer given a question.", "labels": [], "entities": [{"text": "Question Answering (QA)", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.8642055094242096}, {"text": "automatic retrieval of an answer given a question", "start_pos": 37, "end_pos": 86, "type": "TASK", "confidence": 0.6966217309236526}]}, {"text": "Typically the question is linguistically processed and search phrases are extracted, which are then used to retrieve the candidate documents, passages or sentences.", "labels": [], "entities": []}, {"text": "A typical QA system has a pipeline structure starting from extraction of candidate sentences to ranking true answers.", "labels": [], "entities": []}, {"text": "Some approaches to QA use keyword-based techniques to locate candidate passages/sentences in the retrieved documents and then filter based on the presence of the desired answer type in candidate text.", "labels": [], "entities": [{"text": "QA", "start_pos": 19, "end_pos": 21, "type": "TASK", "confidence": 0.9329856038093567}]}, {"text": "Ranking is then done using syntactic features to characterize similarity to query.", "labels": [], "entities": []}, {"text": "In cases where simple question formulation is not satisfactory, many advanced QA systems implement more sophisticated syntactic, semantic and contextual processing such as named-entity recognition (), coreference resolution), logical inferences (abduction or entailment) () translation (, etc., to improve answer ranking.", "labels": [], "entities": [{"text": "question formulation", "start_pos": 22, "end_pos": 42, "type": "TASK", "confidence": 0.7236291021108627}, {"text": "named-entity recognition", "start_pos": 172, "end_pos": 196, "type": "TASK", "confidence": 0.7012892961502075}, {"text": "coreference resolution", "start_pos": 201, "end_pos": 223, "type": "TASK", "confidence": 0.9054087400436401}]}, {"text": "For instance, how questions, or spatially constrained questions, etc., require such types of deeper understanding of the question and the retrieved documents/passages.", "labels": [], "entities": []}, {"text": "Many studies on QA have focused on discriminative models to predict a function of matching features between each question and candidate passage (set of sentences), namely q/a pairs, e.g.,.", "labels": [], "entities": []}, {"text": "Despite their success, they have some room for improvement which are not usually raised, e.g., they require hand engineered features; or cascade features learnt separately from other modules in a QA pipeline, thus propagating errors.", "labels": [], "entities": []}, {"text": "The structures to be learned can become more complex than the amount of training data, e.g., alignment, entailment, translation, etc.", "labels": [], "entities": [{"text": "alignment", "start_pos": 93, "end_pos": 102, "type": "TASK", "confidence": 0.9630337953567505}, {"text": "translation", "start_pos": 116, "end_pos": 127, "type": "TASK", "confidence": 0.7538223266601562}]}, {"text": "In such cases, other source of information, e.g., unlabeled examples, or human prior knowledge, should be used to improve performance.", "labels": [], "entities": []}, {"text": "Generative modeling is away of encoding this additional information, providing a natural way to use unlabeled data.", "labels": [], "entities": []}, {"text": "In this work, we present new similarity measures to discover deeper relationship between q/a pairs based on a probabilistic model.", "labels": [], "entities": []}, {"text": "We investigate two methods using Latent Dirichlet Allocation (LDA) in \u00a7 3, and hierarchical LDA (hLDA)) in \u00a7 4 to discover hidden concepts.", "labels": [], "entities": [{"text": "Latent Dirichlet Allocation (LDA)", "start_pos": 33, "end_pos": 66, "type": "METRIC", "confidence": 0.7355261296033859}]}, {"text": "We present ways of utilizing this information within a discriminative classifier in \u00a7 5.", "labels": [], "entities": []}, {"text": "With empirical experiments in \u00a7 6, we analyze the effects of generative model outcome on a QA system.", "labels": [], "entities": [{"text": "generative model", "start_pos": 61, "end_pos": 77, "type": "TASK", "confidence": 0.9145555198192596}]}, {"text": "With the new representation of conceptual structures on QA 1 datasets, using a limited amount of world knowledge, we show performance improvements.", "labels": [], "entities": [{"text": "QA 1 datasets", "start_pos": 56, "end_pos": 69, "type": "DATASET", "confidence": 0.8050909241040548}]}], "datasetContent": [{"text": "We demonstrate the results of our experiments on exploration of the effect of different generative models presented in \u00a75 on TREC QA datasets.", "labels": [], "entities": [{"text": "TREC QA datasets", "start_pos": 125, "end_pos": 141, "type": "DATASET", "confidence": 0.8986554741859436}]}, {"text": "We performed experiments on the datasets used in ( . Their train dataset composes of a set of 1449 questions from TREC-99-03.", "labels": [], "entities": [{"text": "TREC-99-03", "start_pos": 114, "end_pos": 124, "type": "DATASET", "confidence": 0.8108267784118652}]}, {"text": "For each question, the 5 top-ranked candidate sentences are extracted from a large newswire corpora (Acquaint corpus) through a search engine, i.e., Lucene 2 . The q/a pairs are labeled as true/false depending on the containment of the true answer string in retrieved passages.", "labels": [], "entities": [{"text": "Acquaint corpus", "start_pos": 101, "end_pos": 116, "type": "DATASET", "confidence": 0.881822258234024}, {"text": "Lucene", "start_pos": 149, "end_pos": 155, "type": "DATASET", "confidence": 0.8987866640090942}]}, {"text": "Additionally, to calculate the LDA and hLDA similarity measures for each candidate passage, we also extract around 100 documents in the same fashion using Lucene and identify passages to build the probabilistic models.", "labels": [], "entities": [{"text": "LDA", "start_pos": 31, "end_pos": 34, "type": "METRIC", "confidence": 0.9229755401611328}, {"text": "hLDA similarity", "start_pos": 39, "end_pos": 54, "type": "METRIC", "confidence": 0.8262919485569}, {"text": "Lucene", "start_pos": 155, "end_pos": 161, "type": "DATASET", "confidence": 0.9620664715766907}]}, {"text": ", and the degree of similarity values, i.e., DES LDA (q, s) and DES hLDA (q, s) for each of the 5 top-ranked candidate sentences in training dataset at inference time.", "labels": [], "entities": [{"text": "DES LDA", "start_pos": 45, "end_pos": 52, "type": "METRIC", "confidence": 0.8949248790740967}, {"text": "DES hLDA (q", "start_pos": 64, "end_pos": 75, "type": "METRIC", "confidence": 0.8836275786161423}]}, {"text": "Around 7200 q/a pairs are compiled accordingly.", "labels": [], "entities": []}, {"text": "The provided testing data contains a set of 202 questions from TREC2004 along with 20 candidate sentences for each question, which are labeled as true/false.", "labels": [], "entities": [{"text": "TREC2004", "start_pos": 63, "end_pos": 71, "type": "DATASET", "confidence": 0.9158008694648743}]}, {"text": "To calculate the similarities for the 20 candidate sentences, we extract around 100 documents for each question and build LDA and hLDA models.", "labels": [], "entities": []}, {"text": "4037 testing q/a pairs are compiled.", "labels": [], "entities": []}, {"text": "We report the retrieval performance of our models in terms of Mean Reciprocal Rank (MRR), top 1 (Top1) and top 5 prediction accuracies (Top5)).", "labels": [], "entities": [{"text": "Mean Reciprocal Rank (MRR)", "start_pos": 62, "end_pos": 88, "type": "METRIC", "confidence": 0.9681451718012491}]}, {"text": "We performed parameter optimization during training based on prediction accuracy to find the best C = 10 \u22122 , .., 10 2 and \u0393 = 2 \u22122 , .., 2 3 for RBF kernel SVM.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 72, "end_pos": 80, "type": "METRIC", "confidence": 0.9348215460777283}, {"text": "RBF kernel SVM", "start_pos": 146, "end_pos": 160, "type": "DATASET", "confidence": 0.6792932947476705}]}, {"text": "For the LDA models we present the results with 10 topics.", "labels": [], "entities": []}, {"text": "In hLDA models, we use four levels for the tree construction and set the topic Dirichlet hyperparameters in decreasing order of levels at \u03b7 = {1.0, 0.75, 0.5, 0.25} to encourage as many terms in the mid to low levels as the higher levels in the hierarchy, fora better comparison between q/a pairs.", "labels": [], "entities": [{"text": "tree construction", "start_pos": 43, "end_pos": 60, "type": "TASK", "confidence": 0.7871778011322021}]}, {"text": "The nested CRP parameter \u03b3 is fixed at 1.0.", "labels": [], "entities": []}, {"text": "We evaluated n-sliding-window size of sentences in sequence, n = {1, 3, 5}, to compile candidate passages for probabilistic models.", "labels": [], "entities": []}, {"text": "The output scores for SVM models are normalized to.", "labels": [], "entities": []}, {"text": "As our baseline (in \u00a75), we consider supervised classifier based QA presented in ( . The baseline MRR on TREC-2004 dataset is MRR=%67.6, Top1=%58, Top5=%82.2.", "labels": [], "entities": [{"text": "MRR", "start_pos": 98, "end_pos": 101, "type": "METRIC", "confidence": 0.784633457660675}, {"text": "TREC-2004 dataset", "start_pos": 105, "end_pos": 122, "type": "DATASET", "confidence": 0.9645205140113831}, {"text": "MRR", "start_pos": 126, "end_pos": 129, "type": "METRIC", "confidence": 0.9959235191345215}]}, {"text": "The results of the new models on testing dataset are reported in.", "labels": [], "entities": []}, {"text": "Incorporating the generative model output to the classifier model as input features, i.e., M-3.1 and M-3-2, performs consistently better than the rest of the models and the baseline, where MRR result is statistically significant based on t-test statistics (at p = 0.95 confidence level).", "labels": [], "entities": [{"text": "MRR", "start_pos": 189, "end_pos": 192, "type": "METRIC", "confidence": 0.8902196288108826}]}, {"text": "When combined with the textual entailment scores, i.e., M-2.1 and M-2.2, they provide a slightly better ranking, a minor improvement compared to the baseline.", "labels": [], "entities": []}, {"text": "However, using the generative model outcome as sole ranking scores in 7 M-1.1 and M-1.2 do not reveal as good results as the other models, suggesting room for improvement.", "labels": [], "entities": []}, {"text": "In, Top1 MRR yields better improvement compared to the other two MRRs, especially for models M-3.1 and M-3.2.", "labels": [], "entities": [{"text": "MRR", "start_pos": 9, "end_pos": 12, "type": "TASK", "confidence": 0.7497824430465698}, {"text": "M-3.1", "start_pos": 93, "end_pos": 98, "type": "DATASET", "confidence": 0.9114962220191956}]}, {"text": "This suggests that the probabilistic model outcome rewards the candidate sentences containing the true answer by estimating higher scores and moves them up to the higher levels of the rank.", "labels": [], "entities": []}, {"text": "The analysis of different passage sizes suggest that the 1-window size yields best results and no significant performance improvement is observed when window size is increased.", "labels": [], "entities": []}, {"text": "Thus, the similarity between q/a pairs can be better explained if the candidate passage contains less redundant sentences.", "labels": [], "entities": []}, {"text": "The fact that the similarity scores obtained from the hLDA models are significantly better than LDA models in indicates an important property of hierarchal topic models.", "labels": [], "entities": [{"text": "similarity", "start_pos": 18, "end_pos": 28, "type": "METRIC", "confidence": 0.9712508320808411}]}, {"text": "With the hLDA specific and generic topics can be identified on different levels of the hierarchy.", "labels": [], "entities": []}, {"text": "Two candidate passages can be characterized with different abstract and specific topics enabling representation of better features to identify similarity measures between them.", "labels": [], "entities": []}, {"text": "Whereas in LDA, each candidate passage has a proportion in each topic.", "labels": [], "entities": []}, {"text": "Rewarding the similarities on specific topics with the hLDA models help improve the QA rank performance.", "labels": [], "entities": []}, {"text": "In M-3.1 and M-3.2 we use probabilistic similarities and DES as inputs to the classifier.", "labels": [], "entities": [{"text": "DES", "start_pos": 57, "end_pos": 60, "type": "METRIC", "confidence": 0.975040853023529}]}, {"text": "In   and DES hLDA features reveal slightly better results.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: The MRR results of the models presented in  \u00a75 on testing dataset (TREC 2004) using different window sizes  of candidate passages. The statistically significant model results in each corresponding MRR category are bolded.  Baseline MRR=%67.6, Top1=%58, Top5=%82.2.", "labels": [], "entities": [{"text": "MRR", "start_pos": 14, "end_pos": 17, "type": "TASK", "confidence": 0.9292393922805786}, {"text": "testing dataset (TREC 2004)", "start_pos": 60, "end_pos": 87, "type": "DATASET", "confidence": 0.7455022682746252}, {"text": "Baseline MRR", "start_pos": 233, "end_pos": 245, "type": "METRIC", "confidence": 0.8504734933376312}]}]}