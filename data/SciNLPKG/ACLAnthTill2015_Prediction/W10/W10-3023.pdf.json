{"title": [{"text": "Hedge Classification with Syntactic Dependency Features based on an Ensemble Classifier", "labels": [], "entities": [{"text": "Hedge Classification", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.7195986211299896}]}], "abstractContent": [{"text": "We present our CoNLL-2010 Shared Task system in the paper.", "labels": [], "entities": []}, {"text": "The system operates in three steps: sequence labeling, syntactic dependency parsing, and classification.", "labels": [], "entities": [{"text": "sequence labeling", "start_pos": 36, "end_pos": 53, "type": "TASK", "confidence": 0.6667061001062393}, {"text": "syntactic dependency parsing", "start_pos": 55, "end_pos": 83, "type": "TASK", "confidence": 0.6293284296989441}]}, {"text": "We have participated in the Shared Task 1.", "labels": [], "entities": [{"text": "Shared Task 1", "start_pos": 28, "end_pos": 41, "type": "TASK", "confidence": 0.628916064898173}]}, {"text": "Our experimental results measured by the in-domain and cross-domain F-scores on the biological domain are 81.11% and 67.99%, and on the Wikipedia domain 55.48% and 55.41%.", "labels": [], "entities": [{"text": "F-scores", "start_pos": 68, "end_pos": 76, "type": "METRIC", "confidence": 0.8978002071380615}]}], "introductionContent": [{"text": "The goals of the Shared Task ( are: (1) learning to detect sentences containing uncertainty and (2) learning to resolve the insentence scope of hedge cues.", "labels": [], "entities": []}, {"text": "We have participated in the in-domain and cross-domain challenges of Task 1.", "labels": [], "entities": []}, {"text": "Specifically, the aim of Task 1 is to identify sentences in texts that contain unreliable or uncertain information, and it is formulated as a binary classification problem.", "labels": [], "entities": []}, {"text": "Similar to, we use the BIO-cue labels for all tokens in a sentence to predict whether a token is the first one of a hedge cue (B-cue), inside a hedge cue (I-cue), or outside of a hedge cue (O-cue).", "labels": [], "entities": [{"text": "BIO-cue", "start_pos": 23, "end_pos": 30, "type": "METRIC", "confidence": 0.9034983515739441}]}, {"text": "Thus we formulate the problem at the token level, and our task is to label tokens in every sentence with BIO-cue.", "labels": [], "entities": [{"text": "BIO-cue", "start_pos": 105, "end_pos": 112, "type": "METRIC", "confidence": 0.9522545337677002}]}, {"text": "Finally, sentences that contain at least one B-cue or I-cue are considered as uncertain.", "labels": [], "entities": [{"text": "B-cue", "start_pos": 45, "end_pos": 50, "type": "METRIC", "confidence": 0.979385256767273}, {"text": "I-cue", "start_pos": 54, "end_pos": 59, "type": "METRIC", "confidence": 0.8051716089248657}]}, {"text": "Our system operates in three steps: sequence labeling, syntactic dependency parsing, and classification.", "labels": [], "entities": [{"text": "sequence labeling", "start_pos": 36, "end_pos": 53, "type": "TASK", "confidence": 0.6775481998920441}, {"text": "syntactic dependency parsing", "start_pos": 55, "end_pos": 83, "type": "TASK", "confidence": 0.6293673316637675}]}, {"text": "Sequence labeling is a preprocessing step for splitting sentence into tokens and obtaining features of tokens.", "labels": [], "entities": [{"text": "Sequence labeling", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.9294838309288025}]}, {"text": "Then a syntactic dependency parser is applied to obtain the dependency information of tokens.", "labels": [], "entities": []}, {"text": "Finally, we employ an ensemble classifier based on combining CRF (conditional random field) and MaxEnt (maximum entropy) classifiers to label each token with the BIO-cue.", "labels": [], "entities": []}, {"text": "Our experiments are conducted on two training data sets: one is the abstracts and full articles from BioScope (biomedical domain) corpus 1 , the other one is paragraphs from Wikipedia possibly containing weasel information.", "labels": [], "entities": [{"text": "BioScope (biomedical domain) corpus 1", "start_pos": 101, "end_pos": 138, "type": "DATASET", "confidence": 0.7499652334621975}]}, {"text": "Both training data sets have been annotated manually for hedge/weasel cues.", "labels": [], "entities": []}, {"text": "The annotation of weasel/hedge cues is carried out at the phrase level.", "labels": [], "entities": []}, {"text": "Sentences containing at least one hedge/weasel cue are considered as uncertain, while sentences with no hedge/weasel cues are considered as factual.", "labels": [], "entities": []}, {"text": "The results show that employing the ensemble classifier outperforms the single classifier system on the Wikipedia data set, and using the syntactic dependency information in the feature set outperform the system without syntactic dependency information on the biological data set (in-domain).", "labels": [], "entities": [{"text": "Wikipedia data set", "start_pos": 104, "end_pos": 122, "type": "DATASET", "confidence": 0.9721475839614868}]}, {"text": "In related work, Szarvas (2008) extended the methodology of, and presented a hedge detection method in biomedical texts with a weakly supervised selection of keywords.", "labels": [], "entities": [{"text": "hedge detection", "start_pos": 77, "end_pos": 92, "type": "TASK", "confidence": 0.7711572647094727}]}, {"text": "proposed an approach for automatic detection of sentences containing linguistic hedges using Wikipedia weasel tags and syntactic patterns.", "labels": [], "entities": [{"text": "automatic detection of sentences containing linguistic hedges", "start_pos": 25, "end_pos": 86, "type": "TASK", "confidence": 0.8385519215038845}]}, {"text": "The remainder of this paper is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 presents the technical details of our system.", "labels": [], "entities": []}, {"text": "Section 3 presents experimental results and performance analysis.", "labels": [], "entities": []}, {"text": "Section 4 presents our discussion of the experiments.", "labels": [], "entities": []}, {"text": "Section 5 concludes the paper and proposes future work.", "labels": [], "entities": []}], "datasetContent": [{"text": "We have participated in four subtasks, biological in-domain challenge, biological cross-domain challenge (Bio-cross-domain), Wikipedia in-domain challenge and Wikipedia cross-domain challenge (Wiki-cross-domain).", "labels": [], "entities": []}, {"text": "In all the experiments, TP, FP, FN and F-Score for the uncertainty class are used as the performance measures.", "labels": [], "entities": [{"text": "TP", "start_pos": 24, "end_pos": 26, "type": "METRIC", "confidence": 0.9670349955558777}, {"text": "FP", "start_pos": 28, "end_pos": 30, "type": "METRIC", "confidence": 0.9893654584884644}, {"text": "FN", "start_pos": 32, "end_pos": 34, "type": "METRIC", "confidence": 0.9664508700370789}, {"text": "F-Score", "start_pos": 39, "end_pos": 46, "type": "METRIC", "confidence": 0.9856648445129395}]}, {"text": "We have http://crfpp.sourceforge.net/ 7 http://maxent.sourceforge.net/ tested our system with the test data set and obtained official results as shown in.", "labels": [], "entities": []}, {"text": "In addition, we have performed several internal experiments on the training data set and several experiments on the test data set, which we describe in the next two subsections.", "labels": [], "entities": [{"text": "training data set", "start_pos": 67, "end_pos": 84, "type": "DATASET", "confidence": 0.7777557273705801}, {"text": "test data set", "start_pos": 116, "end_pos": 129, "type": "DATASET", "confidence": 0.84194415807724}]}, {"text": "The feature sets used for each subtask in our system are shown in, where each column denotes a feature set named after the title of the column (\"System\", \"dep\", \u2026).", "labels": [], "entities": []}, {"text": "Actually, for different subtasks, we make use of the same feature set named \"system\".: Official results of our system.", "labels": [], "entities": []}, {"text": "Initially we only used a single classifier instead of an ensemble classifier.", "labels": [], "entities": []}, {"text": "We performed 10-fold cross validation experiments on the training data set at the sentence level with different feature sets.", "labels": [], "entities": []}, {"text": "The results of these experiments are shown in.", "labels": [], "entities": []}, {"text": "In internal experiments, we mainly focus on the results of different models and different feature sets.", "labels": [], "entities": []}, {"text": "In, CRF and ME (MaxEnt) indicate the two classifiers; ENSMB stands for the ensemble classifier obtained by combining CRF and MaxEnt classifiers; the three words \"dep\", \"neighbor\" and \"together\" indicate the feature sets for different experiments shown in, and \"together\" is the union set of \"dep\" and \"neighbor\".", "labels": [], "entities": [{"text": "ENSMB", "start_pos": 54, "end_pos": 59, "type": "METRIC", "confidence": 0.7760515213012695}]}, {"text": "The results of ME and CRF experiments (third column of show that the individual classifier wrongly predicts many uncertain sentences ascertain ones.", "labels": [], "entities": []}, {"text": "The number of such errors is much greater than the number of errors of predicting certain ones as uncertain.", "labels": [], "entities": []}, {"text": "In other words, FN is greater than FP in our experiments and the recall ratio is very low, especially for the Wikipedia data set.", "labels": [], "entities": [{"text": "FN", "start_pos": 16, "end_pos": 18, "type": "METRIC", "confidence": 0.998938262462616}, {"text": "FP", "start_pos": 35, "end_pos": 37, "type": "METRIC", "confidence": 0.9976475834846497}, {"text": "recall ratio", "start_pos": 65, "end_pos": 77, "type": "METRIC", "confidence": 0.9929479658603668}, {"text": "Wikipedia data set", "start_pos": 110, "end_pos": 128, "type": "DATASET", "confidence": 0.9738396803538004}]}, {"text": "Based on this analysis, we propose an ensemble classifier approach to decrease FN in order to improve the recall ratio.", "labels": [], "entities": [{"text": "FN", "start_pos": 79, "end_pos": 81, "type": "METRIC", "confidence": 0.9948912858963013}, {"text": "recall ratio", "start_pos": 106, "end_pos": 118, "type": "METRIC", "confidence": 0.9883197844028473}]}, {"text": "The results of the ensemble classifier show that: along with the decreasing of FN, FP and TP are both increasing.", "labels": [], "entities": [{"text": "FN", "start_pos": 79, "end_pos": 81, "type": "METRIC", "confidence": 0.9808894991874695}, {"text": "FP", "start_pos": 83, "end_pos": 85, "type": "METRIC", "confidence": 0.9951032400131226}, {"text": "TP", "start_pos": 90, "end_pos": 92, "type": "METRIC", "confidence": 0.9840694069862366}]}, {"text": "Although the recall ratio increases, the precision ratio decreases at the same time.", "labels": [], "entities": [{"text": "recall ratio", "start_pos": 13, "end_pos": 25, "type": "METRIC", "confidence": 0.9870222210884094}, {"text": "precision ratio", "start_pos": 41, "end_pos": 56, "type": "METRIC", "confidence": 0.993614137172699}]}, {"text": "Therefore, the ensemble classifier approach is a trade-off between precision and recall.", "labels": [], "entities": [{"text": "precision", "start_pos": 67, "end_pos": 76, "type": "METRIC", "confidence": 0.999202311038971}, {"text": "recall", "start_pos": 81, "end_pos": 87, "type": "METRIC", "confidence": 0.9962708950042725}]}, {"text": "For data sets with low recall ratio, such as Wikipedia, the ensemble classifier outperforms each single classifier in terms of F-score, just as the ME, CRF and ENSMB experiments show in.", "labels": [], "entities": [{"text": "recall ratio", "start_pos": 23, "end_pos": 35, "type": "METRIC", "confidence": 0.9910496771335602}, {"text": "F-score", "start_pos": 127, "end_pos": 134, "type": "METRIC", "confidence": 0.9957589507102966}, {"text": "ME", "start_pos": 148, "end_pos": 150, "type": "METRIC", "confidence": 0.6871250867843628}]}, {"text": "In addition, we have performed simple feature selection in the internal experiments.", "labels": [], "entities": [{"text": "feature selection", "start_pos": 38, "end_pos": 55, "type": "TASK", "confidence": 0.6593015640974045}]}, {"text": "The comparison of \"dep\", \"neighbor\" and \"together\" experiments shown in demonstrates that the dependency and neighbor features are both beneficial only for the biological in-domain experiment.", "labels": [], "entities": []}, {"text": "This maybe because that sentences of the biological data are more regular than those of the Wikipedia data.", "labels": [], "entities": [{"text": "Wikipedia data", "start_pos": 92, "end_pos": 106, "type": "DATASET", "confidence": 0.8480424582958221}]}, {"text": "We have also performed experiments on the test data set, and the results are shown in.", "labels": [], "entities": [{"text": "test data set", "start_pos": 42, "end_pos": 55, "type": "DATASET", "confidence": 0.8579360644022623}]}, {"text": "With the same set of features of our sys-tem as shown in, we have performed three experiments: System-ME (ME denotes MaxEnt classifier), System-CRF (CRF denotes CRF classifier) and System-ENSMB (ENSMB denotes ensemble classifier), where \"System\" denotes the feature set in.", "labels": [], "entities": []}, {"text": "The meanings of these words are similar to internal experiments.", "labels": [], "entities": []}, {"text": "As show, for the Wikipedia test data set, the ensemble classifier outperforms each single classifier in terms of F-score by improving the recall ratio with a larger extent than the extent of the decreasing of the precision ratio.", "labels": [], "entities": [{"text": "Wikipedia test data set", "start_pos": 17, "end_pos": 40, "type": "DATASET", "confidence": 0.9655795842409134}, {"text": "F-score", "start_pos": 113, "end_pos": 120, "type": "METRIC", "confidence": 0.996894359588623}, {"text": "recall ratio", "start_pos": 138, "end_pos": 150, "type": "METRIC", "confidence": 0.9901819229125977}, {"text": "precision", "start_pos": 213, "end_pos": 222, "type": "METRIC", "confidence": 0.9970853924751282}]}, {"text": "For the biological test data set, the ensemble classifier outperforms System-ME but underperforms System-CRF.", "labels": [], "entities": []}, {"text": "This maybe due to the relatively high values of the precision and recall ratios already obtained by each single classifier.", "labels": [], "entities": [{"text": "precision", "start_pos": 52, "end_pos": 61, "type": "METRIC", "confidence": 0.9993816614151001}, {"text": "recall ratios", "start_pos": 66, "end_pos": 79, "type": "METRIC", "confidence": 0.9801977276802063}]}], "tableCaptions": [{"text": " Table 1: Official results of our system.", "labels": [], "entities": []}, {"text": " Table 2: Results of internal experiments on the biological training data set.", "labels": [], "entities": [{"text": "biological training data set", "start_pos": 49, "end_pos": 77, "type": "DATASET", "confidence": 0.7424392178654671}]}, {"text": " Table 3: Results of internal experiments on the Wikipedia training data set.", "labels": [], "entities": [{"text": "Wikipedia training data set", "start_pos": 49, "end_pos": 76, "type": "DATASET", "confidence": 0.9432771354913712}]}, {"text": " Table 4: Results of additional experiment of biological test data set.", "labels": [], "entities": [{"text": "biological test data set", "start_pos": 46, "end_pos": 70, "type": "DATASET", "confidence": 0.7664363831281662}]}, {"text": " Table 5: Results of additional experiment of Wikipedia test data set.", "labels": [], "entities": [{"text": "Wikipedia test data set", "start_pos": 46, "end_pos": 69, "type": "DATASET", "confidence": 0.9789163917303085}]}]}