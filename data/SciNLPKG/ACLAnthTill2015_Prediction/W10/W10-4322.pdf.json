{"title": [{"text": "Sparse Approximate Dynamic Programming for Dialog Management", "labels": [], "entities": [{"text": "Dialog Management", "start_pos": 43, "end_pos": 60, "type": "TASK", "confidence": 0.8526074886322021}]}], "abstractContent": [{"text": "Spoken dialogue management strategy optimization by means of Reinforcement Learning (RL) is now part of the state of the art.", "labels": [], "entities": [{"text": "Spoken dialogue management strategy optimization", "start_pos": 0, "end_pos": 48, "type": "TASK", "confidence": 0.8734827876091004}]}, {"text": "Yet, there is still a clear mis-match between the complexity implied by the required naturalness of dialogue systems and the inability of standard RL algorithms to scale up.", "labels": [], "entities": []}, {"text": "Another issue is the sparsity of the data available for training in the dialogue domain which cannot ensure convergence of most of RL algorithms.", "labels": [], "entities": []}, {"text": "In this paper, we propose to combine a sample-efficient generalization framework for RL with a feature selection algorithm for the learning of an optimal spoken dialogue management strategy.", "labels": [], "entities": [{"text": "spoken dialogue management", "start_pos": 154, "end_pos": 180, "type": "TASK", "confidence": 0.680969754854838}]}], "introductionContent": [{"text": "Optimization of dialogue management strategies by means of Reinforcement Learning (RL)) is now part of the state of the art in the research area of Spoken Dialogue Systems (SDS) (.", "labels": [], "entities": [{"text": "Spoken Dialogue Systems (SDS)", "start_pos": 148, "end_pos": 177, "type": "TASK", "confidence": 0.7873308757940928}]}, {"text": "It consists in casting the dialogue management problem into the Markov Decision Processes (MDP) paradigm and solving the associated optimization problem.", "labels": [], "entities": [{"text": "dialogue management problem", "start_pos": 27, "end_pos": 54, "type": "TASK", "confidence": 0.8247522711753845}]}, {"text": "Yet, there is still a clear mismatch between the complexity implied by the required naturalness of the dialogue systems and the inability of standard RL algorithms to scale up.", "labels": [], "entities": []}, {"text": "Another issue is the sparsity of the data available for training in the dialogue domain because collecting and annotating data is very time consuming.", "labels": [], "entities": []}, {"text": "Yet, RL algorithms are very data demanding and low amounts of data cannot ensure convergence of most of RL algorithms.", "labels": [], "entities": [{"text": "convergence", "start_pos": 81, "end_pos": 92, "type": "METRIC", "confidence": 0.9501603841781616}]}, {"text": "This latter problem has been extensively studied in the recent years and is addressed by simulating new dialogues thanks to a statistical model of human-machine interaction) and user modeling (;).", "labels": [], "entities": []}, {"text": "However, this results in a variability of the learned strategy depending on the user modeling method () and no common agreement exists on the best user model.", "labels": [], "entities": []}, {"text": "The former problem, that is dealing with complex dialogue systems within the RL framework, has received much less attention.", "labels": [], "entities": []}, {"text": "Although some works can be found in the SDS literature it is far from taking advantage of the large amount of machine learning literature devoted to this problem.", "labels": [], "entities": [{"text": "SDS", "start_pos": 40, "end_pos": 43, "type": "TASK", "confidence": 0.9302375912666321}]}, {"text": "In), the authors reduce the complexity of the problem (which is actually a Partially Observable MDP) by automatically condensing the continuous state space in a so-called summary space.", "labels": [], "entities": []}, {"text": "This results in a clustering of the state space in a discrete set of states on which standard RL algorithms are applied.", "labels": [], "entities": []}, {"text": "In (, the authors use a linear approximation scheme and apply the SARSA(\u03bb) algorithm) in a batch setting (from data and not from interactions or simulations).", "labels": [], "entities": [{"text": "SARSA(\u03bb) algorithm", "start_pos": 66, "end_pos": 84, "type": "METRIC", "confidence": 0.9307609796524048}]}, {"text": "This algorithm was actually designed for online learning and is known to converge very slowly.", "labels": [], "entities": []}, {"text": "It therefore requires a lot of data and especially in large state spaces.", "labels": [], "entities": []}, {"text": "Moreover, the choice of the features used for the linear approximation is particularly simple since features are the state variables themselves.", "labels": [], "entities": []}, {"text": "The approximated function can therefore not be more complex than an hyper-plane in the state variables space.", "labels": [], "entities": []}, {"text": "This drawback is shared by the approach of () where a batch algorithm (Least Square Policy Iteration or LSPI) is combined to a pruning method to only keep the most meaningful features.", "labels": [], "entities": []}, {"text": "In addition the complexity of LSPI is O(p 3 ).", "labels": [], "entities": [{"text": "O", "start_pos": 38, "end_pos": 39, "type": "METRIC", "confidence": 0.9743086099624634}]}, {"text": "In the machine learning community, this issue is actually addressed by function approximation accompanied with dimensionality reduction.", "labels": [], "entities": [{"text": "function approximation", "start_pos": 71, "end_pos": 93, "type": "TASK", "confidence": 0.7932837605476379}]}, {"text": "The data sparsity problem is also widely addressed in this literature, and sample-efficiency is one main trend of research in this field.", "labels": [], "entities": []}, {"text": "In this paper, we propose to combine a sample-efficient batch RL algorithm (namely the Fitted Value Iteration (FVI) algorithm) with a feature selection method in a novel manner and to apply this original combination to the learning of an optimal spoken dialogue strategy.", "labels": [], "entities": []}, {"text": "Although the algorithm uses a linear combination of features (or basis functions), these features are much richer in their ability of representing complex functions.", "labels": [], "entities": []}, {"text": "The ultimate goal of this research is to provide away of learning optimal dialogue policies fora large set of situations from a small and fixed set of annotated data in a tractable way.", "labels": [], "entities": []}, {"text": "The rest of this paper is structured as follows.", "labels": [], "entities": []}, {"text": "Section 2 gives a formal insight of MDP and briefly reminds the casting of the dialogue problem into the MDP framework.", "labels": [], "entities": []}, {"text": "Section 3.2 provides a description of approximate Dynamic Programming along with LSPI and FVI algorithms.", "labels": [], "entities": []}, {"text": "Section 4 provides an overview on how LSPI and FVI can be combined with a feature selection scheme (which is employed to learn the representation of the Q-function from the dialogue corpus).", "labels": [], "entities": [{"text": "FVI", "start_pos": 47, "end_pos": 50, "type": "DATASET", "confidence": 0.7367343306541443}]}, {"text": "Our experimental set-up, results and a comparison with state-of-the-art methods are presented in Section 5.", "labels": [], "entities": []}, {"text": "Eventually, Section 6 concludes.", "labels": [], "entities": []}], "datasetContent": [{"text": "We ran a set of learning iterations using two different representations of Q-function and with different numbers of training samples (one sample is a dialogue turn, that is a state transition {s, a, r, s }).", "labels": [], "entities": []}, {"text": "The number of samples used for training ranged The policies learned were then tested using a unigram user simulation and the DIPPER dialogue management framework.", "labels": [], "entities": []}, {"text": "show the average discounted sum of rewards of policies tested over 8\u00d725 dialogue episodes.", "labels": [], "entities": []}, {"text": "Our experimental results show that the dialogue policies learned using sparse SLFQ and LSPI with the two different Q-function representations perform significantly better than the hand-coded policy.", "labels": [], "entities": []}, {"text": "Most importantly it can be observed from and 2 that the performance of sparse LSFQ and sparse LSPI (which uses the dictionary method for feature selection) are nearly as good as LSFQ and LSPI (which employs more numerous hand-selected basis functions).", "labels": [], "entities": []}, {"text": "This shows the effectiveness of using the dictionary method for learning the representation of the Q-function from the dialogue corpora.", "labels": [], "entities": []}, {"text": "For this specific problem the set of hand selected features seem to perform better than sparse LSPI and sparse LSFQ, but this may not be always the case.", "labels": [], "entities": []}, {"text": "For complex dialogue management problems feature selection methods such as the one studied here will be handy since the option of manually selecting a good set of features will cease to exist.", "labels": [], "entities": [{"text": "dialogue management", "start_pos": 12, "end_pos": 31, "type": "TASK", "confidence": 0.7666341960430145}]}, {"text": "Secondly it can be concluded that, similar to LSFQ and LSPI, the sparse LSFQ and sparse LSPI based dialogue management are also sample effi- Figure 2: LSPI policy evaluation statistics cient and needs only few thousand samples (recall that a sample is a dialogue turn and not a dialogue episode) to learn fairly good policies, thus exhibiting a possibility to learn a good policy directly from very limited amount of dialogue examples.", "labels": [], "entities": []}, {"text": "We believe this is a significant improvement when compared to the corpora requirement for dialogue management using other RL algorithms such as SARSA.", "labels": [], "entities": [{"text": "dialogue management", "start_pos": 90, "end_pos": 109, "type": "TASK", "confidence": 0.8469285368919373}]}, {"text": "However, sparse LSPI seems to result in poorer performance compared to sparse LSFQ.", "labels": [], "entities": []}, {"text": "One key advantage of using the dictionary method is that only mandatory basis functions are selected to be part of the dictionary.", "labels": [], "entities": []}, {"text": "This results in fewer feature weights ensuring faster convergence during training.", "labels": [], "entities": []}, {"text": "From it can also be observed that the performance of both LSFQ and LSPI (using hand selected features) are nearly identical.", "labels": [], "entities": []}, {"text": "From a computational complexity point of view, LSFQ and LSPI roughly need the same number of iterations before the stopping criterion is met.", "labels": [], "entities": []}, {"text": "However, reminding that the proposed LSFQ complexity is O(p) 2 per iteration whereas LSPI complexity is O(p 3 ) per iteration, LSFQ is computationally less intensive.", "labels": [], "entities": [{"text": "O", "start_pos": 56, "end_pos": 57, "type": "METRIC", "confidence": 0.9538304805755615}]}], "tableCaptions": []}