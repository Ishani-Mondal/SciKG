{"title": [{"text": "Experts' Retrieval with Multiword-Enhanced Author Topic Model", "labels": [], "entities": [{"text": "Experts' Retrieval", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.6480012983083725}]}], "abstractContent": [{"text": "In this paper, we propose a multiword-enhanced author topic model that clusters authors with similar interests and expertise, and apply it to an information retrieval system that returns a ranked list of authors related to a keyword.", "labels": [], "entities": []}, {"text": "For example, we can retrieve Eugene Charniak via search for statistical parsing.", "labels": [], "entities": [{"text": "statistical parsing", "start_pos": 60, "end_pos": 79, "type": "TASK", "confidence": 0.807620644569397}]}, {"text": "The existing works on author topic model-ing assume a \"bag-of-words\" representation.", "labels": [], "entities": []}, {"text": "However, many semantic atomic concepts are represented by multiwords in text documents.", "labels": [], "entities": []}, {"text": "This paper presents a pre-computation step as away to discover these multiwords in the corpus automatically and tags them in the term-document matrix.", "labels": [], "entities": []}, {"text": "The key advantage of this method is that it retains the simplicity and the computational efficiency of the unigram model.", "labels": [], "entities": []}, {"text": "In addition to a qualitative evaluation, we evaluate the results by using the topic models as a component in a search engine.", "labels": [], "entities": []}, {"text": "We exhibit improved retrieval scores when the documents are represented via sets of latent topics and authors.", "labels": [], "entities": []}], "introductionContent": [{"text": "This paper addresses the problem of searching people with similar interests and expertise without inputting personal names as queries.", "labels": [], "entities": []}, {"text": "Many existing people search engines need people's names to do a \"keyword\" style search, using a person's name as a query.", "labels": [], "entities": []}, {"text": "However, in many situations, such information is impossible to know beforehand.", "labels": [], "entities": []}, {"text": "Imagine a scenario where the statistics department of a university invited a world-wide known expert in Bayesian statistics and machine learning to give a keynote speech; how can the department head notify all the people on campus who are interested without spamming those who are not?", "labels": [], "entities": []}, {"text": "Our paper proposes a solution to the aforementioned scenario by providing a search engine which goes beyond \"keyword\" search and can retrieve such information semantically.", "labels": [], "entities": []}, {"text": "The department head would only need to input the domain keyword of the keynote speaker, i.e. Bayesian statistics, machine learning, and all professors and students who are interested in this topic will be retrieved.", "labels": [], "entities": []}, {"text": "Specifically, we propose a Multiwordenhanced Author-Topic Model (MATM), a probabilistic generative model which assumes two steps of generation process when producing a document.", "labels": [], "entities": []}, {"text": "Statistical topical modeling) has attracted much attention recently due to its broad applications in machine learning, text mining and information retrieval.", "labels": [], "entities": [{"text": "Statistical topical modeling", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.8375352819760641}, {"text": "text mining", "start_pos": 119, "end_pos": 130, "type": "TASK", "confidence": 0.8471060395240784}, {"text": "information retrieval", "start_pos": 135, "end_pos": 156, "type": "TASK", "confidence": 0.8114684224128723}]}, {"text": "In these models, semantic topics are represented by multinomial distribution over words.", "labels": [], "entities": []}, {"text": "Typically, the content of each topic is visualized by simply listing the words in order of decreasing probability and the \"meaning\" of each topic is reflected by the top 10 to 20 words in that list.", "labels": [], "entities": []}, {"text": "The Author-Topic Model (ATM) () extends the basic topical models to include author information in which topics and authors are modeled jointly.", "labels": [], "entities": []}, {"text": "Each author is a multinomial distribution over topics and each topic is a multinomial distribution over words.", "labels": [], "entities": []}, {"text": "Our contribution to this paper is two-fold.", "labels": [], "entities": []}, {"text": "First of all, our model, MATM, extends the original ATM by adding semantically coherent multiwords into the term-document matrix to relax the model's \"bag-of-words\" assumption.", "labels": [], "entities": []}, {"text": "Each multiword is discovered via statistical measurement and filtered by its part of speech pattern via an off-line way.", "labels": [], "entities": []}, {"text": "One key advantage of tagging these semantic atomic units off-line, is the retention of the flexibility and computational efficiency in using the simpler word exchangeable model, while providing better interpretation of the topics author distribution.", "labels": [], "entities": []}, {"text": "Secondly, to the best of our knowledge, this is the first proposal to apply the enhanced author topic modeling in a semantic retrieval scenario, where searching people is associated with a set of hidden semantically meaningful topics instead of their names.", "labels": [], "entities": []}, {"text": "While current search engines cannot support interactive and exploratory search effectively, search based on our model serves very well to answer a range of exploratory queries about the document collections by semantically linking the interests of the authors to the topics of the collection, and ultimately to the distribution of the words in the documents.", "labels": [], "entities": []}, {"text": "The rest of the paper is organized as follows.", "labels": [], "entities": []}, {"text": "We present some related work on topic modeling, the original author-topic model and automatic phrase discovery methods in Sec.", "labels": [], "entities": [{"text": "topic modeling", "start_pos": 32, "end_pos": 46, "type": "TASK", "confidence": 0.8550439178943634}, {"text": "phrase discovery", "start_pos": 94, "end_pos": 110, "type": "TASK", "confidence": 0.7457454204559326}]}, {"text": "2. Then our model is described in Sec.", "labels": [], "entities": []}, {"text": "4 presents our experiments and the evaluation of our method on expert search.", "labels": [], "entities": []}, {"text": "We conclude this paper in Sec.", "labels": [], "entities": []}, {"text": "5 with some discussion and several further developments.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we describe the empirical evaluation of our model qualitatively and quantitatively by applying our model to a text retrieval system we call Expert Search.", "labels": [], "entities": []}, {"text": "This search engine is intended to retrieve groups of experts with similar interests and expertise by inputting only general domain key words, such as syntactic parsing, information retrieval.", "labels": [], "entities": [{"text": "syntactic parsing", "start_pos": 150, "end_pos": 167, "type": "TASK", "confidence": 0.7124574184417725}, {"text": "information retrieval", "start_pos": 169, "end_pos": 190, "type": "TASK", "confidence": 0.7935275137424469}]}, {"text": "We first describe the data set, the retrieval system and the evaluation metrics.", "labels": [], "entities": []}, {"text": "Then we present the empirical results both qualitatively and quantitatively.", "labels": [], "entities": []}, {"text": "We designed a preliminary retrieval system to evaluate our model.", "labels": [], "entities": []}, {"text": "to associate words with individual authors, i.e., we rank the joint probability of the query words and the target author P (W, a).", "labels": [], "entities": []}, {"text": "This probability is marginalized overall topics in the model to rank all authors in our corpus.", "labels": [], "entities": []}, {"text": "In addition, the model assumes that the word and the author is conditionally independent given the topic.", "labels": [], "entities": []}, {"text": "Formally, we define the ranking function of our retrieval system in Equation 5: Wis the input query, which may contain one or more words.", "labels": [], "entities": []}, {"text": "If a multiword is detected within the query, it is added into the query.", "labels": [], "entities": []}, {"text": "The final score is the sum of all words in this query weighted by their inverse document frequency \u03b1 i The inverse document frequency is defined as Equation 6.", "labels": [], "entities": [{"text": "Equation", "start_pos": 148, "end_pos": 156, "type": "METRIC", "confidence": 0.9374253749847412}]}, {"text": "In our experiments, we chose ten queries which covers several most popular research areas in computational linguistics and natural language processing.", "labels": [], "entities": [{"text": "natural language processing", "start_pos": 123, "end_pos": 150, "type": "TASK", "confidence": 0.6362329125404358}]}, {"text": "In our unigram model, query words are treated token by token.", "labels": [], "entities": []}, {"text": "However, in our multiword model, if the query contains a multiword inside our vocabulary, it is treated as an additional token to expand the query.", "labels": [], "entities": []}, {"text": "For each query, top 10 authors are returned from the system.", "labels": [], "entities": []}, {"text": "We manually label the relevance of these 10 authors based on the papers they submitted to these seven-year ACL conferences collected in our corpus.", "labels": [], "entities": [{"text": "ACL conferences collected in our corpus", "start_pos": 107, "end_pos": 146, "type": "DATASET", "confidence": 0.7629610697428385}]}, {"text": "Two evaluation metrics are used to measure the precision of the retrieving results.", "labels": [], "entities": [{"text": "precision", "start_pos": 47, "end_pos": 56, "type": "METRIC", "confidence": 0.9987120628356934}]}, {"text": "First we evaluate the precision at a given cut-off rank, namely precision at K with K ranging from 1 to 10.", "labels": [], "entities": [{"text": "precision", "start_pos": 22, "end_pos": 31, "type": "METRIC", "confidence": 0.9995397329330444}, {"text": "precision", "start_pos": 64, "end_pos": 73, "type": "METRIC", "confidence": 0.9990023970603943}]}, {"text": "We also calculate the average precision (AP) for each query and the mean average precision (MAP) for all the 10 queries.", "labels": [], "entities": [{"text": "average precision (AP)", "start_pos": 22, "end_pos": 44, "type": "METRIC", "confidence": 0.8530799150466919}, {"text": "mean average precision (MAP)", "start_pos": 68, "end_pos": 96, "type": "METRIC", "confidence": 0.9435323178768158}]}, {"text": "Average precision not only takes ranking as consideration but also emphasizes ranking relevant documents higher.", "labels": [], "entities": [{"text": "precision", "start_pos": 8, "end_pos": 17, "type": "METRIC", "confidence": 0.9923079013824463}]}, {"text": "Different from precision at K, it is sensitive to the ranking and captures some recall information since it assumes the precision of the non-retrieved documents to be zero.", "labels": [], "entities": [{"text": "precision", "start_pos": 15, "end_pos": 24, "type": "METRIC", "confidence": 0.9992448091506958}, {"text": "recall", "start_pos": 80, "end_pos": 86, "type": "METRIC", "confidence": 0.9987096786499023}, {"text": "precision", "start_pos": 120, "end_pos": 129, "type": "METRIC", "confidence": 0.9973013997077942}]}, {"text": "It is defined as the average of precisions computed at the point of each of the relevant documents in the ranked list as shown in equation 7.", "labels": [], "entities": [{"text": "precisions", "start_pos": 32, "end_pos": 42, "type": "METRIC", "confidence": 0.983322024345398}]}, {"text": "relevant documents Currently in our experiments, we do not have a pool of labeled authors to do a good evaluation of recall of our system.", "labels": [], "entities": [{"text": "recall", "start_pos": 117, "end_pos": 123, "type": "METRIC", "confidence": 0.99860018491745}]}, {"text": "However, as in the web browsing activity, many users only care about the first several hits of the retrieving results and precision at K and MAP measurements are robust measurements for this purpose.", "labels": [], "entities": [{"text": "precision", "start_pos": 122, "end_pos": 131, "type": "METRIC", "confidence": 0.9949014186859131}, {"text": "MAP", "start_pos": 141, "end_pos": 144, "type": "METRIC", "confidence": 0.9272881150245667}]}], "tableCaptions": [{"text": " Table 4: Papers in our ACL corpus for three authors related to the \"machine translation\" topic in", "labels": [], "entities": [{"text": "ACL corpus", "start_pos": 24, "end_pos": 34, "type": "DATASET", "confidence": 0.8046873807907104}, {"text": "machine translation\" topic", "start_pos": 69, "end_pos": 95, "type": "TASK", "confidence": 0.8627256453037262}]}, {"text": " Table 5: Precision at K evaluation of the multiword- enhanced model and the unigram model.", "labels": [], "entities": [{"text": "Precision", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9716376066207886}]}, {"text": " Table 6: Average Precision (AP) for each query and Mean  Average Precision (MAP) of the multiword-enhanced  model and the unigram model.", "labels": [], "entities": [{"text": "Average Precision (AP)", "start_pos": 10, "end_pos": 32, "type": "METRIC", "confidence": 0.9656412243843079}, {"text": "Mean  Average Precision (MAP)", "start_pos": 52, "end_pos": 81, "type": "METRIC", "confidence": 0.9843269685904185}]}]}