{"title": [{"text": "Learning Simple Wikipedia: A Cogitation in Ascertaining Abecedarian Language", "labels": [], "entities": []}], "abstractContent": [{"text": "Text simplification is the process of changing vocabulary and grammatical structure to create a more accessible version of the text while maintaining the underlying information and content.", "labels": [], "entities": [{"text": "Text simplification", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.7836575210094452}]}, {"text": "Automated tools for text simplification area practical way to make large corpora of text accessible to a wider audience lacking high levels of fluency in the corpus language.", "labels": [], "entities": [{"text": "text simplification", "start_pos": 20, "end_pos": 39, "type": "TASK", "confidence": 0.7804274559020996}]}, {"text": "In this work, we investigate the potential of Simple Wikipedia to assist automatic text simplification by building a statistical classification system that discriminates simple English from ordinary English.", "labels": [], "entities": [{"text": "automatic text simplification", "start_pos": 73, "end_pos": 102, "type": "TASK", "confidence": 0.6069842875003815}]}, {"text": "Most text simplification systems are based on handwritten rules (e.g., PEST (Carroll et al., 1999) and its module SYSTAR (Canning et al., 2000)), and therefore face limitations scaling and transferring across domains.", "labels": [], "entities": [{"text": "text simplification", "start_pos": 5, "end_pos": 24, "type": "TASK", "confidence": 0.7137278616428375}]}, {"text": "The potential for using Simple Wikipedia for text simplification is significant; it contains nearly 60,000 articles with revision histories and aligned articles to ordinary English Wikipedia.", "labels": [], "entities": [{"text": "text simplification", "start_pos": 45, "end_pos": 64, "type": "TASK", "confidence": 0.7674888968467712}]}, {"text": "Using articles from Simple Wikipedia and ordinary Wikipedia, we evaluated different classi-fiers and feature sets to identify the most dis-criminative features of simple English for use across domains.", "labels": [], "entities": []}, {"text": "These findings help further understanding of what makes text simple and can be applied as a tool to help writers craft simple text.", "labels": [], "entities": []}], "introductionContent": [{"text": "The availability of large collections of electronic texts is a boon to information seekers, however, advanced texts often require fluency in the language.", "labels": [], "entities": []}, {"text": "Text simplification (TS) is an emerging area of textto-text generation that focuses on increasing the readability of a given text.", "labels": [], "entities": [{"text": "Text simplification (TS)", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.8874171853065491}, {"text": "textto-text generation", "start_pos": 48, "end_pos": 70, "type": "TASK", "confidence": 0.7786023020744324}]}, {"text": "Potential applications can increase the accessibility of text, which has great value in education, public health, and safety, and can aid natural language processing tasks such as machine translation and text generation.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 180, "end_pos": 199, "type": "TASK", "confidence": 0.7799258530139923}, {"text": "text generation", "start_pos": 204, "end_pos": 219, "type": "TASK", "confidence": 0.7592999339103699}]}, {"text": "Corresponding to these applications, TS can be broken down into two rough categories depending on the target \"reader.\"", "labels": [], "entities": [{"text": "TS", "start_pos": 37, "end_pos": 39, "type": "TASK", "confidence": 0.9656230807304382}]}, {"text": "The first type of TS aims to increase human readability for people lacking highlevel language skills, either because of age, education level, unfamiliarity with the language, or disability.", "labels": [], "entities": [{"text": "TS", "start_pos": 18, "end_pos": 20, "type": "TASK", "confidence": 0.9614352583885193}]}, {"text": "Historically, generating this text has been done by hand, which is time consuming and expensive, especially when dealing with material that requires expertise, such as legal documents.", "labels": [], "entities": []}, {"text": "Most current automatic TS systems rely on handwritten rules, e.g.,), its SYSTAR module (), and the method described by.", "labels": [], "entities": [{"text": "TS", "start_pos": 23, "end_pos": 25, "type": "TASK", "confidence": 0.960739016532898}]}, {"text": "Systems using handwritten rules can be susceptible to changes in domains and need to be modified for each new domain or language.", "labels": [], "entities": []}, {"text": "There has been some research into automatically learning the rules for simplifying text using aligned corpora (, but these have yet to match the performance hand-crafted rule systems.", "labels": [], "entities": []}, {"text": "An example of a manually simplified sentence can be found in table 1.", "labels": [], "entities": []}, {"text": "The second type of TS has the goal of increasing the machine readability of text to aid tasks such as information extraction, machine translation, generative summarization, and other text generation tasks for selecting and evaluating the best candidate output text.", "labels": [], "entities": [{"text": "TS", "start_pos": 19, "end_pos": 21, "type": "TASK", "confidence": 0.9351188540458679}, {"text": "information extraction", "start_pos": 102, "end_pos": 124, "type": "TASK", "confidence": 0.7891504764556885}, {"text": "machine translation", "start_pos": 126, "end_pos": 145, "type": "TASK", "confidence": 0.8109593987464905}, {"text": "generative summarization", "start_pos": 147, "end_pos": 171, "type": "TASK", "confidence": 0.9539800584316254}, {"text": "text generation", "start_pos": 183, "end_pos": 198, "type": "TASK", "confidence": 0.7275586873292923}]}, {"text": "In machine translation, the evaluation tool most commonly used for evaluating output, the BLEU score (), rates the \"goodness\" of output based on n-gram overlap with human-generated text.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 3, "end_pos": 22, "type": "TASK", "confidence": 0.7652388513088226}, {"text": "BLEU score", "start_pos": 90, "end_pos": 100, "type": "METRIC", "confidence": 0.9823586940765381}]}, {"text": "However this metric has been criticized for not accurately measuring the fluency of text and there is active research into other metrics.", "labels": [], "entities": []}, {"text": "Previous studies suggest that text simplified for machine and human comprehension are categorically different.", "labels": [], "entities": []}, {"text": "Our research considers text simplified for human readers, but the findings can be used to identify features that discriminate simple text for both applications.", "labels": [], "entities": []}, {"text": "The process of TS can be divided into three aspects: removing extraneous or superfluous text, substituting more complex lexical and syntactic forms, and inserting information to offer further clarification where needed.", "labels": [], "entities": [{"text": "TS", "start_pos": 15, "end_pos": 17, "type": "TASK", "confidence": 0.9905375838279724}]}, {"text": "In this regard, TS is related to several different natural language processing tasks such as text summarization, compression, machine translation, and paraphrasing.", "labels": [], "entities": [{"text": "TS", "start_pos": 16, "end_pos": 18, "type": "TASK", "confidence": 0.9786645174026489}, {"text": "text summarization", "start_pos": 93, "end_pos": 111, "type": "TASK", "confidence": 0.7586632072925568}, {"text": "machine translation", "start_pos": 126, "end_pos": 145, "type": "TASK", "confidence": 0.7653932571411133}]}, {"text": "While none of these tasks alone directly provide a solution to text simplification, techniques can be drawn from each.", "labels": [], "entities": [{"text": "text simplification", "start_pos": 63, "end_pos": 82, "type": "TASK", "confidence": 0.7946500480175018}]}, {"text": "Summarization techniques can be used to identify the crucial, most informative parts of a text and compression can be used to remove superfluous words and phrases.", "labels": [], "entities": []}, {"text": "In fact, in the Wikipedia documents analyzed for this research, the average length of a \"simple\" document is only 21% the length of an \"ordinary\" English document (although this maybe an unintentional byproduct of how articles were simplified, as discussed in section 6.1).", "labels": [], "entities": []}, {"text": "In this paper we study the properties of language that differentiate simple from ordinary text for human readers.", "labels": [], "entities": []}, {"text": "Specifically, we use statistical learning techniques to identify the most discriminative features of simple English and \"ordinary\" English using articles from Simple Wikipedia and English Wikipedia.", "labels": [], "entities": [{"text": "Simple Wikipedia", "start_pos": 159, "end_pos": 175, "type": "DATASET", "confidence": 0.9128042459487915}, {"text": "English Wikipedia", "start_pos": 180, "end_pos": 197, "type": "DATASET", "confidence": 0.8037564754486084}]}, {"text": "We use cognitively motivated features as well as statistical measurements of a document's lexical, syntactic, and surface features.", "labels": [], "entities": []}, {"text": "Our study demonstrates the validity and potential benefits of using Simple Wikipedia as a resource for TS research.", "labels": [], "entities": [{"text": "validity", "start_pos": 27, "end_pos": 35, "type": "METRIC", "confidence": 0.9546357989311218}, {"text": "TS research", "start_pos": 103, "end_pos": 114, "type": "TASK", "confidence": 0.9180660843849182}]}], "datasetContent": [{"text": "Using the feature sets described above, we evaluated a simple/ordinary text classifier in several settings on each category.", "labels": [], "entities": []}, {"text": "First, we considered the task of document classification, where a classifier determines whether a full Wikipedia article was from ordinary English Wikipedia or Simple Wikipedia.", "labels": [], "entities": [{"text": "document classification", "start_pos": 33, "end_pos": 56, "type": "TASK", "confidence": 0.7383206784725189}]}, {"text": "For each category of articles, we measured accuracy on this binary classification task using 10-fold cross-validation.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 43, "end_pos": 51, "type": "METRIC", "confidence": 0.999505877494812}]}, {"text": "In the second setting, we consid-   ered the performance of a sentence-level classifier.", "labels": [], "entities": []}, {"text": "The classifier labeled each sentence as either ordinary or simple and we report results using 10-fold cross-validation on a random split of the sentences.", "labels": [], "entities": []}, {"text": "For both settings we also evaluated a single classifier trained on all categories.", "labels": [], "entities": []}, {"text": "We next considered cross-category performance: how would a classifier trained to detect differences between simple and ordinary examples from one category do when tested on another category.", "labels": [], "entities": []}, {"text": "In this experiment, we trained a single classifier on data from a single category and used the classifier to label examples from each of the other categories.", "labels": [], "entities": []}, {"text": "We report the accuracy on each category in these transfer experiments.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 14, "end_pos": 22, "type": "METRIC", "confidence": 0.9996353387832642}]}, {"text": "For learning we require a binary classifier training algorithm.", "labels": [], "entities": []}, {"text": "We evaluated several learning algorithms for classification and report results for each one: a) MIRA-a large margin online learning algorithm).", "labels": [], "entities": [{"text": "MIRA-a", "start_pos": 96, "end_pos": 102, "type": "METRIC", "confidence": 0.9063802361488342}]}, {"text": "Online learning algorithms observe examples sequentially and up-date the current hypothesis after each observation; b) Confidence Weighted (CW) learning-a probabilistic large margin online learning algorithm (; c) Maximum Entropy-a log-linear discriminative classifier; and d) Support Vector Machines (SVM)-a large margin discriminator.", "labels": [], "entities": []}, {"text": "For each experiment, we used default settings of the parameters and 10 online iterations for the online methods (MIRA, CW).", "labels": [], "entities": [{"text": "MIRA", "start_pos": 113, "end_pos": 117, "type": "METRIC", "confidence": 0.9453802108764648}]}, {"text": "To create a fair comparison for each category, we limited the number of examples to a maximum of 2000.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 4: A comparison of the article \"Stephen Hawking\"  from Simple and ordinary Wikipedia.", "labels": [], "entities": []}, {"text": " Table 5: The number of examples available in each cate- gory. To compare experiments in each category we used  at most 2000 instances in each experiment.", "labels": [], "entities": []}, {"text": " Table 6: The number of features in each feature class.", "labels": [], "entities": []}, {"text": " Table 7: Mean accuracy of all classifiers on the document classification task.", "labels": [], "entities": [{"text": "Mean", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9848483204841614}, {"text": "accuracy", "start_pos": 15, "end_pos": 23, "type": "METRIC", "confidence": 0.8795567750930786}, {"text": "document classification task", "start_pos": 50, "end_pos": 78, "type": "TASK", "confidence": 0.803593099117279}]}, {"text": " Table 8: Mean accuracy of all classifiers on the sentence classification task.", "labels": [], "entities": [{"text": "Mean", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9729156494140625}, {"text": "accuracy", "start_pos": 15, "end_pos": 23, "type": "METRIC", "confidence": 0.8706945180892944}, {"text": "sentence classification task", "start_pos": 50, "end_pos": 78, "type": "TASK", "confidence": 0.8070195317268372}]}]}