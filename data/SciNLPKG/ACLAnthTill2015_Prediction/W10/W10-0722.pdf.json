{"title": [{"text": "Non-Expert Evaluation of Summarization Systems is Risky", "labels": [], "entities": [{"text": "Summarization", "start_pos": 25, "end_pos": 38, "type": "TASK", "confidence": 0.8023506999015808}]}], "abstractContent": [{"text": "We provide evidence that intrinsic evaluation of summaries using Amazon's Mechanical Turk is quite difficult.", "labels": [], "entities": [{"text": "Amazon's Mechanical Turk", "start_pos": 65, "end_pos": 89, "type": "DATASET", "confidence": 0.9570539444684982}]}, {"text": "Experiments mirroring evaluation at the Text Analysis Con-ference's summarization track show that non-expert judges are notable to recover system rankings derived from experts.", "labels": [], "entities": [{"text": "Text Analysis Con-ference's summarization track", "start_pos": 40, "end_pos": 87, "type": "DATASET", "confidence": 0.7515899141629537}]}], "introductionContent": [{"text": "Automatic summarization is a particularly difficult task to evaluate.", "labels": [], "entities": [{"text": "Automatic summarization", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.6021443605422974}]}, {"text": "What makes a good summary?", "labels": [], "entities": []}, {"text": "Is it possible to separate information content from linguistic quality?", "labels": [], "entities": []}, {"text": "Besides subjectivity issues, evaluation is timeconsuming.", "labels": [], "entities": []}, {"text": "Ideally, a judge would read the original set of documents before deciding how well the important aspects are conveyed by a summary.", "labels": [], "entities": []}, {"text": "A typical 10-document problem could reasonably involve 25 minutes of reading or skimming and 5 more minutes for assessing a 100-word summary.", "labels": [], "entities": []}, {"text": "Since summary output can be quite variable, at least 30 topics should be evaluated to get a robust estimate of performance.", "labels": [], "entities": []}, {"text": "Assuming a single judge evaluates all summaries fora topic (more redundancy would be better), we get a rough time estimate: 17.5 hours to evaluate two systems.", "labels": [], "entities": []}, {"text": "Thus it is of great interest to find ways of speeding up evaluation while minimizing subjectivity.", "labels": [], "entities": []}, {"text": "Amazon's Mechanical Turk (MTurk) system has been used fora variety of labeling and annotation tasks (), but such crowd-sourcing has not been tested for summarization.", "labels": [], "entities": [{"text": "Amazon", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.914135754108429}, {"text": "labeling and annotation tasks", "start_pos": 70, "end_pos": 99, "type": "TASK", "confidence": 0.8196059167385101}, {"text": "summarization", "start_pos": 152, "end_pos": 165, "type": "TASK", "confidence": 0.9897379279136658}]}, {"text": "We describe an experiment to test whether MTurk is able to reproduce system-level rankings that match expert opinion.", "labels": [], "entities": [{"text": "MTurk", "start_pos": 42, "end_pos": 47, "type": "DATASET", "confidence": 0.6852883100509644}]}, {"text": "Unlike the results of other crowd-sourcing annotations for natural language tasks, we find that non-expert judges are unable to provide expert-like scores and tend to disagree significantly with each other.", "labels": [], "entities": []}, {"text": "This paper is organized as follows: Section 2 introduces the particular summarization task and data we use in our experiments; Section 3 describes the design of our Human Intelligence Task (HIT).", "labels": [], "entities": []}, {"text": "Section 4 shows experimental results and gives some analysis.", "labels": [], "entities": []}, {"text": "Section 5 reviews our main findings and provides suggestions for researchers wishing to conduct their own crowd-sourcing evaluations.", "labels": [], "entities": []}], "datasetContent": [{"text": "One way to dramatically speedup evaluation is to use the experts' reference summaries as a gold standard, leaving the source documents out entirely.", "labels": [], "entities": []}, {"text": "This is the idea behind automatic evaluation with ROUGE), which measures ngram overlap with the references, and assisted evaluation with Pyramid (), which measures overlap of facts or \"Semantic Content Units\" with the references.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 50, "end_pos": 55, "type": "METRIC", "confidence": 0.979830801486969}]}, {"text": "The same idea has also been employed in various manual evaluations, for example by, to directly compare the summaries of two different systems.", "labels": [], "entities": []}, {"text": "The potential bias introduced by such abbreviated evaluation has not been explored.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Identical summaries often were given different  scores by the same expert human judge at TAC 2009.", "labels": [], "entities": [{"text": "Identical summaries", "start_pos": 10, "end_pos": 29, "type": "TASK", "confidence": 0.6874364912509918}, {"text": "TAC 2009", "start_pos": 99, "end_pos": 107, "type": "DATASET", "confidence": 0.922405481338501}]}, {"text": " Table 3: Comparison of Overall Quality (OQ) and Lin- guistic Quality (LQ) scores between the TAC and MTurk  evaluations. Content (C) is evaluated by MTurk workers  as well. Note that system F is the lead baseline.", "labels": [], "entities": [{"text": "Comparison of Overall Quality (OQ)", "start_pos": 10, "end_pos": 44, "type": "METRIC", "confidence": 0.771064532654626}, {"text": "Lin- guistic Quality (LQ)", "start_pos": 49, "end_pos": 74, "type": "METRIC", "confidence": 0.9492822459765843}]}, {"text": " Table 4: Linear regression is used to model Overall Qual- ity scores as a function of judges, topics, and systems, re- spectively, for each data set. The R 2 values, which give  the fraction of variance explained by each of the six mod- els, are shown.", "labels": [], "entities": []}]}