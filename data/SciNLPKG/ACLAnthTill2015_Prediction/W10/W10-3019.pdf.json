{"title": [{"text": "Uncertainty Learning Using SVMs and CRFs", "labels": [], "entities": []}], "abstractContent": [{"text": "In this work, we explore the use of SVMs and CRFs in the problem of predicting certainty in sentences.", "labels": [], "entities": [{"text": "predicting certainty in sentences", "start_pos": 68, "end_pos": 101, "type": "TASK", "confidence": 0.7888538092374802}]}, {"text": "We consider this as a task of tagging uncertainty cues in context, for which we used lexical, wordlist-based and deep-syntactic features.", "labels": [], "entities": [{"text": "tagging uncertainty cues", "start_pos": 30, "end_pos": 54, "type": "TASK", "confidence": 0.8395337462425232}]}, {"text": "Results show that the syntactic context of the tokens in conjunction with the wordlist-based features turned out to be useful in predicting uncertainty cues.", "labels": [], "entities": [{"text": "predicting uncertainty cues", "start_pos": 129, "end_pos": 156, "type": "TASK", "confidence": 0.8781050443649292}]}], "introductionContent": [{"text": "Extracting factual information from text is a critical NLP task which has important applications in Information Extraction, Textual Entailment etc.", "labels": [], "entities": [{"text": "Extracting factual information from text", "start_pos": 0, "end_pos": 40, "type": "TASK", "confidence": 0.8828391551971435}, {"text": "Information Extraction", "start_pos": 100, "end_pos": 122, "type": "TASK", "confidence": 0.7983271181583405}, {"text": "Textual Entailment", "start_pos": 124, "end_pos": 142, "type": "TASK", "confidence": 0.8003591001033783}]}, {"text": "It is found that linguistic devices such as hedge phrases help to distinguish facts from uncertain information.", "labels": [], "entities": []}, {"text": "Hedge phrases usually indicate that authors do not or cannot backup their opinions/statements with facts.", "labels": [], "entities": []}, {"text": "As part of the CoNLL shared task, we explored the applicability of different machine learning approaches and feature sets to learn to detect sentences containing uncertainty.", "labels": [], "entities": []}, {"text": "In Section 2, we present the task formally and describe the data used.", "labels": [], "entities": []}, {"text": "Section 3 presents the system description and explains the features used in the task in detail.", "labels": [], "entities": []}, {"text": "We investigated two different machine learning frameworks in this task and did experiments on various feature configurations.", "labels": [], "entities": []}, {"text": "Section 4 presents those experiments and analyzes the results.", "labels": [], "entities": []}, {"text": "Section 5 describes the system used for the shared task final submission and presents the results obtained in the evaluation.", "labels": [], "entities": []}, {"text": "Section 6 concludes the paper and discusses a few future directions to extend this work.", "labels": [], "entities": []}], "datasetContent": [{"text": "To find the best configuration, we used 10% of the training data as the development set to tune parameters.", "labels": [], "entities": []}, {"text": "Since even the development set was fairly large, we used 9-fold cross validation to evaluate each models.", "labels": [], "entities": []}, {"text": "The development set was divided into 9 folds of which 8 folds were used to train a model which was tested on the 9th fold.", "labels": [], "entities": []}, {"text": "All the reported results in this section are averaged over the 9 folds.", "labels": [], "entities": []}, {"text": "We report F \u03b2=1 (F)-measure as the harmonic mean between (P)recision and (R)ecall.", "labels": [], "entities": [{"text": "F \u03b2=1 (F)-measure", "start_pos": 10, "end_pos": 27, "type": "METRIC", "confidence": 0.7419989630579948}]}, {"text": "We categorized the experiments into three distinct classes as shown in: Experiment Sets (linear) context widths.", "labels": [], "entities": []}, {"text": "Here, context width denotes the window of tokens whose features are considered.", "labels": [], "entities": []}, {"text": "For example, a context width of 2 means that the feature vector of any given token includes, in addition to its own features, those of 2 tokens before and after it as well as the prediction for 2 tokens before it.", "labels": [], "entities": []}, {"text": "We varied the context widths from 1 to 5, and found that the best results were obtained for context width of 1 and 2.", "labels": [], "entities": []}, {"text": "In this section, we present the results of experiments conducted on the development set as part of this task.", "labels": [], "entities": []}, {"text": "The results for the system using Yamcha and Mallet are given in  In this section, we explain in detail the system which was used for the results submitted in the shared task evaluation.", "labels": [], "entities": []}, {"text": "For predicting the cue phrases on evaluation dataset for the shared task, we trained a model using the best performing configuration (feature set and machinery) from the experiments described in Section 4.", "labels": [], "entities": []}, {"text": "The best configuration used the feature set <POS, parentPOS, modalMe, isDaughterAux, leftSisPOS, mpos, isUncertainAdj, isGenericAdj, myNNSparentIsGeneric> with a context width of 1 and it was trained using Mallet's CRF.", "labels": [], "entities": [{"text": "Mallet's CRF", "start_pos": 206, "end_pos": 218, "type": "DATASET", "confidence": 0.9202686548233032}]}, {"text": "The cross validation results of this configuration is reported in (First feature set in the Mallet section).", "labels": [], "entities": [{"text": "Mallet section", "start_pos": 92, "end_pos": 106, "type": "DATASET", "confidence": 0.9797216951847076}]}, {"text": "This model was trained on the entire Wikipedia training set provided for Task 1.", "labels": [], "entities": [{"text": "Wikipedia training set", "start_pos": 37, "end_pos": 59, "type": "DATASET", "confidence": 0.8340949416160583}]}, {"text": "We used this model to tag the evaluation dataset with uncertainty cues and any sentence where a cue phrase was tagged was classified as an uncertain sentence.: Overall Results  This section presents the results obtained on the shared task evaluation in detail.", "labels": [], "entities": []}, {"text": "The sentence level results are given in presents the cue level results for the task.", "labels": [], "entities": []}, {"text": "Our system had a cue level prediction precision of 67.14% with a low recall of 16.70% and F-measure of 26.75%, which is the 3rd best Fmeasure result among the published cue level results . We ran the best model trained on Wikipedia corpus on the biomedical evaluation dataset.", "labels": [], "entities": [{"text": "cue level prediction precision", "start_pos": 17, "end_pos": 47, "type": "METRIC", "confidence": 0.6668037623167038}, {"text": "recall", "start_pos": 69, "end_pos": 75, "type": "METRIC", "confidence": 0.999106228351593}, {"text": "F-measure", "start_pos": 90, "end_pos": 99, "type": "METRIC", "confidence": 0.9997149109840393}, {"text": "Fmeasure", "start_pos": 133, "end_pos": 141, "type": "METRIC", "confidence": 0.991292417049408}, {"text": "Wikipedia corpus", "start_pos": 222, "end_pos": 238, "type": "DATASET", "confidence": 0.8667123913764954}]}, {"text": "As expected, the results were much lower.", "labels": [], "entities": []}, {"text": "It obtained a precision of 67.54% with a low recall of", "labels": [], "entities": [{"text": "precision", "start_pos": 14, "end_pos": 23, "type": "METRIC", "confidence": 0.9996739625930786}, {"text": "recall", "start_pos": 45, "end_pos": 51, "type": "METRIC", "confidence": 0.9995087385177612}]}], "tableCaptions": [{"text": " Table 2: Features which turned out to be not useful", "labels": [], "entities": []}, {"text": " Table 6: Cue level Results -Across feature classes", "labels": [], "entities": []}, {"text": " Table 7. Our system obtained  a high precision of 87.95% with a low recall of  28.42% and F-measure of 42.96% on the task.  This was the 3rd best precision reported for the  Wikipedia task 1.", "labels": [], "entities": [{"text": "precision", "start_pos": 38, "end_pos": 47, "type": "METRIC", "confidence": 0.9991518259048462}, {"text": "recall", "start_pos": 69, "end_pos": 75, "type": "METRIC", "confidence": 0.9992842078208923}, {"text": "F-measure", "start_pos": 91, "end_pos": 100, "type": "METRIC", "confidence": 0.9996844530105591}, {"text": "precision", "start_pos": 147, "end_pos": 156, "type": "METRIC", "confidence": 0.9912375211715698}]}, {"text": " Table 7: Evaluation -Cue Level Results", "labels": [], "entities": []}, {"text": " Table 8: Evaluation -Cue Level Results", "labels": [], "entities": []}]}