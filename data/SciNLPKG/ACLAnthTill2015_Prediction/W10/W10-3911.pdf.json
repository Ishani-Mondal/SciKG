{"title": [{"text": "Adverse-Effect Relations Extraction from Massive Clinical Records", "labels": [], "entities": [{"text": "Adverse-Effect Relations Extraction", "start_pos": 0, "end_pos": 35, "type": "TASK", "confidence": 0.8329261740048727}, {"text": "Massive Clinical Records", "start_pos": 41, "end_pos": 65, "type": "DATASET", "confidence": 0.7157156467437744}]}], "abstractContent": [{"text": "The rapid spread of electronic health records raised an interest to large-scale information extraction from clinical texts.", "labels": [], "entities": [{"text": "large-scale information extraction from clinical texts", "start_pos": 68, "end_pos": 122, "type": "TASK", "confidence": 0.8403164645036062}]}, {"text": "Considering such a background, we are developing a method that can extract adverse drug event and effect (adverse-effect) relations from massive clinical records.", "labels": [], "entities": []}, {"text": "Adverse-effect relations share some features with relations proposed in previous relation extraction studies, but they also have unique characteristics.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 81, "end_pos": 100, "type": "TASK", "confidence": 0.7536899149417877}]}, {"text": "Adverse-effect relations are usually uncertain.", "labels": [], "entities": []}, {"text": "Not even medical experts can usually determine whether a symptom that arises after a medication represents an adverse-effect relation or not.", "labels": [], "entities": []}, {"text": "We propose a method to extract adverse-effect relations using a machine-learning technique with dependency features.", "labels": [], "entities": []}, {"text": "We performed experiments to extract adverse effect relations from 2,577 clinical texts, and obtained F 1-score of 37.54 with an optimal parameters and F 1-score of 34.90 with automatically tuned parameters.", "labels": [], "entities": [{"text": "F 1-score", "start_pos": 101, "end_pos": 110, "type": "METRIC", "confidence": 0.9921959936618805}, {"text": "F 1-score", "start_pos": 151, "end_pos": 160, "type": "METRIC", "confidence": 0.9912721812725067}]}, {"text": "The results also show that dependency features increase the extraction F 1-score by 3.59.", "labels": [], "entities": [{"text": "F 1-score", "start_pos": 71, "end_pos": 80, "type": "METRIC", "confidence": 0.8671686351299286}]}], "introductionContent": [{"text": "The widespread use of electronic health records (EHR) made clinical texts to be stored as computer processable data.", "labels": [], "entities": []}, {"text": "EHRs contain important information about patients' health.", "labels": [], "entities": []}, {"text": "However, extracting clinical information from EHRs is not easy because they are likely to be written in a natural language.", "labels": [], "entities": [{"text": "extracting clinical information from EHRs", "start_pos": 9, "end_pos": 50, "type": "TASK", "confidence": 0.8159807682037353}]}, {"text": "We are working on a task to extract adverse drug event and effect relations from clinical records.", "labels": [], "entities": [{"text": "extract adverse drug event and effect relations from clinical", "start_pos": 28, "end_pos": 89, "type": "TASK", "confidence": 0.7157093220286899}]}, {"text": "Usually, the association between a drug and its adverse-effect relation is investigated using numerous human resources, costing much time and money.", "labels": [], "entities": []}, {"text": "The motivation of our task comes from this situation.", "labels": [], "entities": []}, {"text": "An example of the task is presented in.", "labels": [], "entities": []}, {"text": "We defined an adverse-effect relation as a relation that holds between a drug entity and a symptom entity.", "labels": [], "entities": []}, {"text": "The sentence illustrates the occurrence of the adverse-effect hepatic disorder by the Singulair medication.", "labels": [], "entities": []}, {"text": "A hepatic disorder found was suspected drug-induced and the Singulair was stopped.", "labels": [], "entities": [{"text": "Singulair", "start_pos": 60, "end_pos": 69, "type": "DATASET", "confidence": 0.873330295085907}]}, {"text": "adverse-effect relation symptom drug A salient characteristic of adverse-effect relations is that they are usually uncertain.", "labels": [], "entities": []}, {"text": "The sentence in the example states that the hepatic disorder is suspected drug-induced, which means the hepatic disorder is likely to present an adverse-effect relation.", "labels": [], "entities": []}, {"text": "presents an example in which an adverse-effect relation is suspected, but words to indicate the suspicion are not stated.", "labels": [], "entities": []}, {"text": "The two effects of the drug--the recovery of HbA1c and the appearance of the edema--are expressed merely as observation results in this sentence.", "labels": [], "entities": []}, {"text": "The recovery of HbA1c is an expected effect of the drug and the appearance of the edema probably represents an adverse-effect case.", "labels": [], "entities": []}, {"text": "The uncertain nature of adverse-effect relations often engenders the statement of an adverse-effect relation as an observed fact.", "labels": [], "entities": []}, {"text": "A sentence including an adverse-effect relation occasionally becomes long to list all observations that appeared after administration of a medication.", "labels": [], "entities": []}, {"text": "Whether an interpretation that expresses an adverse-effect relation, such as drug-induced or suspected to bean adverse-effect, exists in a clinical record or not depends on a person who writes it.", "labels": [], "entities": []}, {"text": "However, an adverse-effect relation is associated with an undesired effect of a medication.", "labels": [], "entities": []}, {"text": "Its appearance would engender an extra action (e.g. stopped in the first example) or lead to an extra indication (e.g. but \u2026 appeared in the second example).", "labels": [], "entities": []}, {"text": "Proper handling of this extra information is likely to boost the extraction accuracy.", "labels": [], "entities": [{"text": "extraction", "start_pos": 65, "end_pos": 75, "type": "TASK", "confidence": 0.9236949682235718}, {"text": "accuracy", "start_pos": 76, "end_pos": 84, "type": "METRIC", "confidence": 0.9786433577537537}]}, {"text": "The challenge of this study is to capture relations with various certainties.", "labels": [], "entities": []}, {"text": "To establish this goal, we used a dependency structure for the adverse-effect relation extraction method.", "labels": [], "entities": [{"text": "adverse-effect relation extraction", "start_pos": 63, "end_pos": 97, "type": "TASK", "confidence": 0.6423974831899008}]}, {"text": "Adverse-effect statements are assumed to share a dependency structure to a certain degree.", "labels": [], "entities": []}, {"text": "For example, if we obtain the dependency structures as shown in, then we can easily determine that the structures are similar.", "labels": [], "entities": []}, {"text": "Of course, obtaining such perfect parsing results is not always possible.", "labels": [], "entities": [{"text": "parsing", "start_pos": 34, "end_pos": 41, "type": "TASK", "confidence": 0.9663464426994324}]}, {"text": "A statistical syntactic parser is known to perform badly if a text to be parsed belongs to a domain which differs from a domain on which the parser is trained.", "labels": [], "entities": [{"text": "statistical syntactic parser", "start_pos": 2, "end_pos": 30, "type": "TASK", "confidence": 0.6010108689467112}]}, {"text": "A statistical parser will likely output incomplete results in these texts and will likely have a negative effect on relation extraction methods which depend on it.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 116, "end_pos": 135, "type": "TASK", "confidence": 0.8486976027488708}]}, {"text": "The specified research topic of this study is to investigate whether incomplete dependency structures are effective and how they behave in the extraction of uncertain relations.", "labels": [], "entities": [{"text": "extraction of uncertain relations", "start_pos": 143, "end_pos": 176, "type": "TASK", "confidence": 0.8144267648458481}]}, {"text": "ACTOS 30 recovered HbA1c to 6.5% but an edema appeared after the medication.", "labels": [], "entities": []}, {"text": "A suspected drug-induced hepatic disorder found and the Singulair was stopped.", "labels": [], "entities": [{"text": "Singulair", "start_pos": 56, "end_pos": 65, "type": "DATASET", "confidence": 0.8356041312217712}]}, {"text": "conjunct nominal subject nominal subject nominal subject nominal subject conjunct was ACTOS 30 recovered HbA1c to 6.5% but an edema appeared after the medication.", "labels": [], "entities": [{"text": "ACTOS", "start_pos": 86, "end_pos": 91, "type": "METRIC", "confidence": 0.9706856608390808}]}], "datasetContent": [{"text": "We performed two experiments to evaluate the extraction method.", "labels": [], "entities": []}, {"text": "Experiment 1 aimed to observe the effects of the presented features.", "labels": [], "entities": []}, {"text": "Five combinations of the features were evaluated with a five-fold cross validation assuming that an optimal parameter combination was obtained.", "labels": [], "entities": []}, {"text": "The experiment conditions are described below: A.", "labels": [], "entities": [{"text": "A", "start_pos": 47, "end_pos": 48, "type": "METRIC", "confidence": 0.9598500728607178}]}, {"text": "Data 7,690 drug-symptom pairs were extracted from the corpus.", "labels": [], "entities": []}, {"text": "Manually annotated information was used to identify drugs and symptoms.", "labels": [], "entities": []}, {"text": "Within 7,690 pairs, 149 pairs failed to extract the dependency chain feature.", "labels": [], "entities": []}, {"text": "We removed these 149 pairs and used the remaining 7,541 pairs in the experiment.", "labels": [], "entities": []}, {"text": "The 7,541 pairs consisted of 367 positive samples and 7,174 negative samples.", "labels": [], "entities": []}, {"text": "We evaluated the extraction method with all combinations of SVM parameters in certain 36. 33.05 1,2,3,4,5   ranges.", "labels": [], "entities": []}, {"text": "We used LIBSVM 3 ver.", "labels": [], "entities": [{"text": "LIBSVM 3 ver", "start_pos": 8, "end_pos": 20, "type": "METRIC", "confidence": 0.5685030619303385}]}, {"text": "2.89 as an implementation of SVM.", "labels": [], "entities": []}, {"text": "The radial basis function (RBF) was used as the kernel function of SVM.", "labels": [], "entities": [{"text": "radial basis function (RBF)", "start_pos": 4, "end_pos": 31, "type": "METRIC", "confidence": 0.8206981718540192}]}, {"text": "The probability estimates option of LIBSVM was used to obtain the confidence value of discrimination.", "labels": [], "entities": [{"text": "LIBSVM", "start_pos": 36, "end_pos": 42, "type": "DATASET", "confidence": 0.7045383453369141}]}, {"text": "The gamma parameter of the RBF kernel was chosen from the range of [2 -20 , 2 0 ].", "labels": [], "entities": [{"text": "RBF kernel", "start_pos": 27, "end_pos": 37, "type": "DATASET", "confidence": 0.8397090137004852}]}, {"text": "The C parameter of SVM was chosen from the range of [2 -10 , 2 10 ].", "labels": [], "entities": [{"text": "SVM", "start_pos": 19, "end_pos": 22, "type": "TASK", "confidence": 0.8210888504981995}]}, {"text": "The SVM was trained and tested on 441 combinations of gamma and C. In testing, the probability threshold parameter p between [0.05, 0.95] was also chosen, and the F 1 -scores of all combination of gamma, C, and p were calculated with five-fold cross validation.", "labels": [], "entities": [{"text": "probability threshold parameter p", "start_pos": 83, "end_pos": 116, "type": "METRIC", "confidence": 0.8187061995267868}, {"text": "F 1 -scores", "start_pos": 163, "end_pos": 174, "type": "METRIC", "confidence": 0.9490276426076889}]}, {"text": "The best F 1 -scores and their parameter values for each combination of features (optimal F 1 -scores in this setting) are portrayed in.", "labels": [], "entities": [{"text": "F 1 -scores", "start_pos": 9, "end_pos": 20, "type": "METRIC", "confidence": 0.9266582727432251}, {"text": "F 1 -scores", "start_pos": 90, "end_pos": 101, "type": "METRIC", "confidence": 0.8960728943347931}]}, {"text": "The precision-recall distribution of F 1 -scores with feature combination C is presented in.", "labels": [], "entities": [{"text": "precision-recall", "start_pos": 4, "end_pos": 20, "type": "METRIC", "confidence": 0.997989296913147}, {"text": "F 1 -scores", "start_pos": 37, "end_pos": 48, "type": "METRIC", "confidence": 0.9181237518787384}]}, {"text": "Experiment 2 aimed to observe the performance of our extraction method when SVM parameters were automatically tuned.", "labels": [], "entities": []}, {"text": "In this experiment, we performed two cross validations: across validation to tune SVM parameters and another cross validation to evaluate the extraction method.", "labels": [], "entities": []}, {"text": "The experiment conditions are described below:  3 http://www.csie.ntu.edu.tw/~cjlin/libsvm Two five-fold cross validations were performed.", "labels": [], "entities": []}, {"text": "The first cross validation divided the data to 5 sets (A, B, C, D, and E) each consisting of development set and test set with the ratio of 4:1.", "labels": [], "entities": []}, {"text": "The second cross validation train and test all combination of SVM parameters (C, gamma, and p) in certain ranges and decide the optimal parameter combination(s) for the development sets of A, B, C, D, and E.", "labels": [], "entities": []}, {"text": "The second cross validation denotes the execution of Experiment 1 for each development set.", "labels": [], "entities": []}, {"text": "For each optimal parameter combination of A, B, C, D, and E, the corresponding development set was trained and the trained model was tested on the corresponding test set.", "labels": [], "entities": []}, {"text": "The average F 1 -score on five test sets marked 34.90, which is 2.64 lower than the F 1 -score of Experiment 1 with the same feature combination.", "labels": [], "entities": [{"text": "F 1 -score", "start_pos": 12, "end_pos": 22, "type": "METRIC", "confidence": 0.9879322350025177}, {"text": "F 1 -score", "start_pos": 84, "end_pos": 94, "type": "METRIC", "confidence": 0.9837043136358261}]}], "tableCaptions": [{"text": " Table 4. Best F 1 -scores and their parameters.", "labels": [], "entities": [{"text": "F 1 -scores", "start_pos": 15, "end_pos": 26, "type": "METRIC", "confidence": 0.8988642543554306}]}]}