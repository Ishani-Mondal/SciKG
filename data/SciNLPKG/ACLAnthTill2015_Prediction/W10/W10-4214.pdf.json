{"title": [{"text": "Towards an Extrinsic Evaluation of Referring Expressions in Situated Dialogs", "labels": [], "entities": [{"text": "Extrinsic Evaluation of Referring Expressions in Situated Dialogs", "start_pos": 11, "end_pos": 76, "type": "TASK", "confidence": 0.6812493056058884}]}], "abstractContent": [{"text": "In the field of referring expression generation , while in the static domain both intrinsic and extrinsic evaluations have been considered, extrinsic evaluation in the dynamic domain, such as in a situated col-laborative dialog, has not been discussed in depth.", "labels": [], "entities": [{"text": "referring expression generation", "start_pos": 16, "end_pos": 47, "type": "TASK", "confidence": 0.7821603616078695}]}, {"text": "Ina dynamic domain, a crucial problem is that referring expressions do not make sense without an appropriate preceding dialog context.", "labels": [], "entities": []}, {"text": "It is unrealistic for an evaluation to simply show a human evaluator the whole period from the beginning of a dialog up to the time point at which a referring expression is used.", "labels": [], "entities": []}, {"text": "Hence, to make evaluation feasible it is indispensable to determine an appropriate shorter context.", "labels": [], "entities": []}, {"text": "In order to investigate the context necessary to understand a referring expression in a situated collaborative dialog , we carried out an experiment with 33 evaluators and a Japanese referring expression corpus.", "labels": [], "entities": []}, {"text": "The results contribute to finding the proper contexts for extrinsic evalu-tion in dynamic domains.", "labels": [], "entities": []}], "introductionContent": [{"text": "In recent years, the NLG community has paid significant attention to the task of generating referring expressions, reflected in the seting-up of several competitive events such as the TUNA and GIVEChallenges at ENLG 2009 (.", "labels": [], "entities": [{"text": "TUNA and GIVEChallenges at ENLG 2009", "start_pos": 184, "end_pos": 220, "type": "DATASET", "confidence": 0.6079371720552444}]}, {"text": "With the development of increasingly complex generation systems, there has been heightened interest in and an ongoing significant discussion on different evaluation measures for referring expressions.", "labels": [], "entities": []}, {"text": "This discussion is carried out broadly in the field of generation, including in the multi-modal domain, e.g. (. There are two different evaluation methods corresponding to the bottom and the top of the vertical axis in: intrinsic and extrinsic evaluations.", "labels": [], "entities": []}, {"text": "Intrinsic methods often measure similarity between the system output and the gold standard corpora using metrics such as tree similarity, string-editdistance and BLEU ().", "labels": [], "entities": [{"text": "BLEU", "start_pos": 162, "end_pos": 166, "type": "METRIC", "confidence": 0.9986510872840881}]}, {"text": "Intrinsic methods have recently become popular in the NLG community.", "labels": [], "entities": []}, {"text": "In contrast, extrinsic methods evaluate generated expressions based on an external metric, such as its impact on human task performance.", "labels": [], "entities": []}], "datasetContent": [{"text": "The aim of our experiment is to investigate the \"context\" (content of the time span of the recorded interaction prior to the uttering of the referring expression) necessary to enable successful identification of the referent of a referring expression.", "labels": [], "entities": [{"text": "identification of the referent of a referring expression", "start_pos": 194, "end_pos": 250, "type": "TASK", "confidence": 0.79984200745821}]}, {"text": "Our method is to vary the context presented to evaluators and then to study the impact on human referent identification.", "labels": [], "entities": [{"text": "referent identification", "start_pos": 96, "end_pos": 119, "type": "TASK", "confidence": 0.7640254199504852}]}, {"text": "In order to realize this, for each instance of a referring expression, we vary the length of the video shown to the evaluator.", "labels": [], "entities": []}, {"text": "shows a screenshot of the interface prepared for this experiment.", "labels": [], "entities": []}, {"text": "The test data consists of three types of referring expressions: DPs (demonstrative pronouns), AMEs (action-mentioning expressions), and OTHERs (any other expression that is neither a DP nor AME, e.g intrinsic attributes and spatial relations).", "labels": [], "entities": [{"text": "AMEs", "start_pos": 94, "end_pos": 98, "type": "METRIC", "confidence": 0.9237829446792603}, {"text": "OTHERs", "start_pos": 136, "end_pos": 142, "type": "METRIC", "confidence": 0.8829135298728943}]}, {"text": "DPs are the most frequent type of referring expression in the corpus.", "labels": [], "entities": []}, {"text": "AMEs are expressions that utilize an action on the referent such as \"the triangle you put away to the top right\" (see) . As we pointed out in our previous paper, they are also a fundamental type of referring expression in this domain.", "labels": [], "entities": []}, {"text": "The basic question in investigating a suitable context is what information to consider about the preceding interaction; i.e. over what parameters to vary the context.", "labels": [], "entities": []}, {"text": "In previous work on the generation of demonstrative pronouns in a situated domain (), we investigated the role of linguistic and extra-linguistic information, and found that time distance from the last action (LA) on the referent as well as the last mention (LM) to the referent had a significant influence on the usage of referring expressions.", "labels": [], "entities": [{"text": "time distance from the last action (LA)", "start_pos": 174, "end_pos": 213, "type": "METRIC", "confidence": 0.6173329916265275}]}, {"text": "Based on those results, we focus on the information on the referent, namely LA and LM.", "labels": [], "entities": [{"text": "LA", "start_pos": 76, "end_pos": 78, "type": "METRIC", "confidence": 0.9960063099861145}]}, {"text": "For both AMEs and OTHERs, we only consider two possibilities of the order in which LM and LA appear before a referring expression (REX), depending on which comes first.", "labels": [], "entities": [{"text": "LA", "start_pos": 90, "end_pos": 92, "type": "METRIC", "confidence": 0.8479858040809631}, {"text": "REX", "start_pos": 131, "end_pos": 134, "type": "METRIC", "confidence": 0.7963996529579163}]}, {"text": "These are shown in, context patterns (a) LA-LM and (b) LM-LA.", "labels": [], "entities": []}, {"text": "Towards the very beginning of a dialog, some referring expressions have no LM and LA; those expressions are not considered in this research.", "labels": [], "entities": [{"text": "LA", "start_pos": 82, "end_pos": 84, "type": "METRIC", "confidence": 0.8195958733558655}]}, {"text": "All instances of AMEs and OTHERs in our test data belong to either the LA-LM or the LM-LA In contrast, DPs tend to be utilized in a deictic way in such situated dialogs.", "labels": [], "entities": []}, {"text": "We further noted in, that DPs in a collaborative task are also frequently used when the referent is under operation.", "labels": [], "entities": []}, {"text": "While they belong neither to the LA-LM nor the LM-LA pattern, it would be inappropriate to exclude those cases.", "labels": [], "entities": []}, {"text": "Hence, for DPs we consider another situation where the last action on the referent overlaps with the utterance of the DP (c) LM-LA' pattern).", "labels": [], "entities": []}, {"text": "In this case, we consider an ongoing operation on the referent as a \"last action\".", "labels": [], "entities": []}, {"text": "Another peculiarity of the LM-LA' pattern is that we have no None context in this case, since there is noway to show a video without showing LA (the current operation).", "labels": [], "entities": [{"text": "LA", "start_pos": 141, "end_pos": 143, "type": "METRIC", "confidence": 0.9555476903915405}]}, {"text": "Given the three basic variations of context, we recruited 33 university students as evaluators and divided them equally into three groups, i.e. 11 evaluators per group.", "labels": [], "entities": []}, {"text": "As for the referring expressions to evaluate, we selected 60 referring expressions used by the solver from the REX-J corpus (20 from each category), ensuring all were correctly understood by the operator during the recorded dialog.", "labels": [], "entities": [{"text": "REX-J corpus", "start_pos": 111, "end_pos": 123, "type": "DATASET", "confidence": 0.8439379930496216}]}, {"text": "We selected those 60 instances from expressions where both LM and LA appeared within the last 30 secs previous to the referring expression.", "labels": [], "entities": [{"text": "LA", "start_pos": 66, "end_pos": 68, "type": "METRIC", "confidence": 0.9442489743232727}]}, {"text": "This selection excludes initial mentions, as well as expressions where only LA or only LM exists or they do not appear within 30 secs.", "labels": [], "entities": [{"text": "LA", "start_pos": 76, "end_pos": 78, "type": "METRIC", "confidence": 0.9895081520080566}]}, {"text": "Hence the data utilized for this experiment is limited in this sense.", "labels": [], "entities": []}, {"text": "We need further experiments to investigate the relation between the time length of contexts and the accuarcy of evaluators.", "labels": [], "entities": []}, {"text": "We will return to this issue in the conclusion.", "labels": [], "entities": []}, {"text": "We combined 60 referring expressions and the three contexts to make the test instances.", "labels": [], "entities": []}, {"text": "Following the Latin square design, we divided these test instances into three groups, distributing each of the three contexts for every referring expression to each group.", "labels": [], "entities": []}, {"text": "The number of contexts was uniformly distributed over the groups.", "labels": [], "entities": []}, {"text": "Each instance group was assigned to each evaluator group.", "labels": [], "entities": []}, {"text": "For each referring expression instance, we record whether the evaluator was able to correctly identify the referent, how long it took them to identify it and whether they repeated the video (and if so how many times).", "labels": [], "entities": []}, {"text": "Reflecting the distribution of the data available in our corpus, the number of instances per context pattern differs for each type of referring expression.", "labels": [], "entities": []}, {"text": "For AMEs, overwhelmingly the last action on the referent was more recent than the last mention.", "labels": [], "entities": [{"text": "AMEs", "start_pos": 4, "end_pos": 8, "type": "TASK", "confidence": 0.8595481514930725}]}, {"text": "Hence we have only two LA-LM patterns among the 20 AMEs in our data.", "labels": [], "entities": []}, {"text": "For OTHERs, the balance is 8 to 12, with a slight majority of LM-LA patterns.", "labels": [], "entities": []}, {"text": "For DPs, there is a strong tendency to use a DP when apiece is under operation ().", "labels": [], "entities": []}, {"text": "Of the 20 DPs in the data, 2 were LA-LM, 5 were LM-LA pattern while 13 were of the LM-LA' pattern (i.e. their referents were under operation at the time of the utterance).", "labels": [], "entities": []}, {"text": "For these 13 instances of LM-LA' we do not have a None context.", "labels": [], "entities": []}, {"text": "The average stimulus times, i.e. time period of presented context, were 7.48 secs for None, 11.04 secs for LM/LA and 18.10 secs for Both.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Accuracy of referring expression identification per type and context", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9960428476333618}, {"text": "referring expression identification", "start_pos": 22, "end_pos": 57, "type": "TASK", "confidence": 0.7847785552342733}]}]}