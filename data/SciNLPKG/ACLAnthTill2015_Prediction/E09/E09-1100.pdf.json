{"title": [{"text": "Character-Level Dependencies in Chinese: Usefulness and Learning", "labels": [], "entities": []}], "abstractContent": [{"text": "We investigate the possibility of exploiting character-based dependency for Chi-nese information processing.", "labels": [], "entities": [{"text": "Chi-nese information processing", "start_pos": 76, "end_pos": 107, "type": "TASK", "confidence": 0.6423007249832153}]}, {"text": "As Chinese text is made up of character sequences rather than word sequences, word in Chi-nese is not so natural a concept as in En-glish, nor is word easy to be defined without argument for such a language.", "labels": [], "entities": []}, {"text": "Therefore we propose a character-level dependency scheme to represent primary linguistic relationships within a Chinese sentence.", "labels": [], "entities": []}, {"text": "The usefulness of character dependencies are verified through two specialized dependency parsing tasks.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 78, "end_pos": 96, "type": "TASK", "confidence": 0.7160141617059708}]}, {"text": "The first is to handle trivial character dependencies that are equally transformed from traditional word boundaries.", "labels": [], "entities": []}, {"text": "The second furthermore considers the case that annotated internal character dependencies inside a word are involved.", "labels": [], "entities": []}, {"text": "Both of these results from character-level dependency parsing are positive.", "labels": [], "entities": [{"text": "character-level dependency parsing", "start_pos": 27, "end_pos": 61, "type": "TASK", "confidence": 0.716137965520223}]}, {"text": "This study provides an alternative way to formularize basic character-and word-level representation for Chinese.", "labels": [], "entities": [{"text": "formularize basic character-and word-level representation", "start_pos": 42, "end_pos": 99, "type": "TASK", "confidence": 0.6660061240196228}]}], "introductionContent": [{"text": "In many human languages, word can be naturally identified from writing.", "labels": [], "entities": []}, {"text": "However, this is not the case for Chinese, for Chinese is born to be written in character 1 sequence rather than word sequence, namely, no natural separators such as blanks exist between words.", "labels": [], "entities": []}, {"text": "As word does not appear in a natural way as most European languages 2 , it brings the argument about how to determine the word-hood in Chinese.", "labels": [], "entities": []}, {"text": "Linguists' views about what is a Chinese word diverge so greatly that multiple word segmentation standards have been proposed for computational linguistics tasks since the first Bakeoff (Bakeoff-1, or (Sproat and.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 79, "end_pos": 96, "type": "TASK", "confidence": 0.7535975575447083}]}, {"text": "Up to Bakeoff-4, seven word segmentation standards have been proposed.", "labels": [], "entities": [{"text": "Bakeoff-4", "start_pos": 6, "end_pos": 15, "type": "DATASET", "confidence": 0.8485259413719177}, {"text": "word segmentation", "start_pos": 23, "end_pos": 40, "type": "TASK", "confidence": 0.6709869205951691}]}, {"text": "However, this does not effectively solve the open problem what a Chinese word should exactly be but raises another issue: what a segmentation standard should be selected for the successive application.", "labels": [], "entities": []}, {"text": "As word often plays a basic role for the further language processing, if it cannot be determined in a unified way, then all successive tasks will be affected more or less.", "labels": [], "entities": []}, {"text": "Motivated by dependency representation for syntactic parsing since) that has been drawn more and more interests in recent years, we suggest that character-level dependencies can be adopted to alleviate this difficulty in Chinese processing.", "labels": [], "entities": [{"text": "syntactic parsing", "start_pos": 43, "end_pos": 60, "type": "TASK", "confidence": 0.7047141492366791}]}, {"text": "If we regard traditional word boundary as a linear representation for neighbored characters, then character-level dependencies can provide away to represent non-linear relations between non-neighbored characters.", "labels": [], "entities": []}, {"text": "To show that character dependencies can be useful, we develop a parsing scheme for the related learning task and demonstrate its effectiveness.", "labels": [], "entities": []}, {"text": "The rest of the paper is organized as follows.", "labels": [], "entities": []}, {"text": "The next section shows the drawbacks of the current word boundary representation through some language examples.", "labels": [], "entities": []}, {"text": "Section 3 describes a character-level dependency parsing scheme for traditional word segmentation task and reports its evaluation results.", "labels": [], "entities": [{"text": "character-level dependency parsing", "start_pos": 22, "end_pos": 56, "type": "TASK", "confidence": 0.6167410612106323}, {"text": "word segmentation task", "start_pos": 80, "end_pos": 102, "type": "TASK", "confidence": 0.7987450063228607}]}, {"text": "Section 4 verifies the usefulness of annotated character dependencies inside a word.", "labels": [], "entities": []}, {"text": "Section 5 looks into a few issues concern-ing the role of character dependencies.", "labels": [], "entities": []}, {"text": "Section 6 concludes the paper.", "labels": [], "entities": []}], "datasetContent": [{"text": "Traditionally, word segmentation performance is measured by F-score ( F = 2RP/(R + P ) ), where the recall (R) and precision (P ) are the proportions of the correctly segmented words to all words in, respectively, the gold-standard segmentation and a segmenter's output.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 15, "end_pos": 32, "type": "TASK", "confidence": 0.7522801458835602}, {"text": "F-score", "start_pos": 60, "end_pos": 67, "type": "METRIC", "confidence": 0.9949418902397156}, {"text": "F", "start_pos": 70, "end_pos": 71, "type": "METRIC", "confidence": 0.8926882147789001}, {"text": "recall (R)", "start_pos": 100, "end_pos": 110, "type": "METRIC", "confidence": 0.9650753885507584}, {"text": "precision (P )", "start_pos": 115, "end_pos": 129, "type": "METRIC", "confidence": 0.954915389418602}]}, {"text": "To compute the word F-score, all parsing results will be restored to word boundaries according to the direction of output arcs.", "labels": [], "entities": [{"text": "F-score", "start_pos": 20, "end_pos": 27, "type": "METRIC", "confidence": 0.855036199092865}]}, {"text": "Our comparison with existing work will be conducted in closed test of Bakeoff.", "labels": [], "entities": [{"text": "Bakeoff", "start_pos": 70, "end_pos": 77, "type": "DATASET", "confidence": 0.8877391219139099}]}, {"text": "The rule for the closed testis that no additional information beyond training corpus is allowed, while open test of Bakeoff is without such restrict.", "labels": [], "entities": [{"text": "Bakeoff", "start_pos": 116, "end_pos": 123, "type": "DATASET", "confidence": 0.7928090691566467}]}, {"text": "The results with different dependency schemes are in.", "labels": [], "entities": []}, {"text": "As the feature preact is involved, abeam search algorithm with width 5 is used to decode, otherwise, a simple shift-reduce decoding is used.", "labels": [], "entities": []}, {"text": "We see that the performance given by Scheme E is much better than that by Scheme B. The results of character-based classification and tagging methods are at the bottom of 6 . It is observed that the parsing method outperforms classification and tagging method without Markovian features or decoding throughout the whole sequence.", "labels": [], "entities": [{"text": "character-based classification and tagging", "start_pos": 99, "end_pos": 141, "type": "TASK", "confidence": 0.6084102094173431}]}, {"text": "As full features are used, the former and the latter provide the similar performance.", "labels": [], "entities": []}, {"text": "Due to using a global model like CRFs, our previous work in () reported the best results over the evaluated corpora of Bakeoff-2 until now 7 . Though those results are slightly better than the results here, we still see that the results of character-level dependency parsing approach (Scheme E) are comparable to those state-of-the-art ones on each evaluated corpus.", "labels": [], "entities": [{"text": "character-level dependency parsing", "start_pos": 240, "end_pos": 274, "type": "TASK", "confidence": 0.6434441606203715}]}], "tableCaptions": [{"text": " Table 3: Corpus size of Bakeoff-2 in number of  words", "labels": [], "entities": [{"text": "Corpus size of", "start_pos": 10, "end_pos": 24, "type": "METRIC", "confidence": 0.8146658142407736}, {"text": "Bakeoff-2", "start_pos": 25, "end_pos": 34, "type": "DATASET", "confidence": 0.3977069556713104}]}, {"text": " Table 4: The results of parsing and classifica- tion/tagging approaches using different feature  combinations", "labels": [], "entities": [{"text": "classifica- tion/tagging", "start_pos": 37, "end_pos": 61, "type": "TASK", "confidence": 0.5839351654052735}]}, {"text": " Table 6. To evalu- ate the results with word F-score, all external de- pendencies in outputs are restored as word bound- aries. There are three models are evaluated in Ta- ble 6. It is shown that there is a significant perfor- mance enhancement as annotated internal charac- ter dependency is introduced. This positive result  shows that annotated internal character dependen- cies are meaningful.", "labels": [], "entities": [{"text": "F-score", "start_pos": 46, "end_pos": 53, "type": "METRIC", "confidence": 0.558997392654419}]}, {"text": " Table 6: Comparison of different methods", "labels": [], "entities": []}]}