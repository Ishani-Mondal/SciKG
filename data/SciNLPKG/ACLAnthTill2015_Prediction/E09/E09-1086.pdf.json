{"title": [{"text": "Unsupervised Recognition of Literal and Non-Literal Use of Idiomatic Expressions", "labels": [], "entities": [{"text": "Unsupervised Recognition of Literal and Non-Literal Use of Idiomatic Expressions", "start_pos": 0, "end_pos": 80, "type": "TASK", "confidence": 0.8368877589702606}]}], "abstractContent": [{"text": "We propose an unsupervised method for distinguishing literal and non-literal usages of idiomatic expressions.", "labels": [], "entities": [{"text": "distinguishing literal and non-literal usages of idiomatic expressions", "start_pos": 38, "end_pos": 108, "type": "TASK", "confidence": 0.6885298267006874}]}, {"text": "Our method determines how well a literal interpretation is linked to the overall cohesive structure of the discourse.", "labels": [], "entities": []}, {"text": "If strong links can be found, the expression is classified as literal, otherwise as idiomatic.", "labels": [], "entities": []}, {"text": "We show that this method can help to tell apart literal and non-literal usages, even for idioms which occur in canonical form.", "labels": [], "entities": []}], "introductionContent": [{"text": "Texts frequently contain expressions whose meaning is not strictly literal, such as metaphors or idioms.", "labels": [], "entities": []}, {"text": "Non-literal expressions pose a major challenge to natural language processing as they often exhibit lexical and syntactic idiosyncrasies.", "labels": [], "entities": []}, {"text": "For example, idioms can violate selectional restrictions (as in push one's luck under the assumption that only concrete things can normally be pushed), disobey typical subcategorisation constraints (e.g., inline without a determiner before line), or change the default assignments of semantic roles to syntactic categories (e.g., in break sth with X the argument X would typically bean instrument but for the idiom break the ice it is more likely to fill a patient role, as in break the ice with Russia).", "labels": [], "entities": []}, {"text": "To avoid erroneous analyses, a natural language processing system should recognise if an expression is used non-literally.", "labels": [], "entities": []}, {"text": "While there has been a lot of work on recognising idioms (see Section 2), most previous approaches have focused on a typebased classification, dividing expressions into \"idiom\" or \"not an idiom\" irrespective of their actual use in a discourse context.", "labels": [], "entities": []}, {"text": "However, while some expressions, such as by and large, always have a non-compositional, idiomatic meaning, many idioms, such as break the ice or spill the beans, share their linguistic form with perfectly literal expressions (see examples and, respectively).", "labels": [], "entities": []}, {"text": "For some expressions, such as drop the ball, the literal usage can even dominate in some domains.", "labels": [], "entities": []}, {"text": "Hence, whether a potentially ambiguous expression has literal or non-literal meaning has to be inferred from the discourse context.", "labels": [], "entities": []}, {"text": "(1) Dad had to break the ice on the chicken troughs so that they could get water.", "labels": [], "entities": []}, {"text": "(2) Somehow I always end up spilling the beans allover the floor and looking foolish when the clerk comes to sweep them up.", "labels": [], "entities": []}, {"text": "Type-based idiom classification thus only addresses part of the problem.", "labels": [], "entities": [{"text": "Type-based idiom classification", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.6882394750912985}]}, {"text": "While it can automatically compile lists of potentially idiomatic expressions, it does not say anything about the idiomaticity of an expression in a particular context.", "labels": [], "entities": []}, {"text": "In this paper, we propose a novel, cohesion-based approach for detecting non-literal usages (token-based idiom classification).", "labels": [], "entities": [{"text": "token-based idiom classification", "start_pos": 93, "end_pos": 125, "type": "TASK", "confidence": 0.6214381456375122}]}, {"text": "Our approach is unsupervised and similar in spirit to method for detecting malapropisms.", "labels": [], "entities": [{"text": "detecting malapropisms", "start_pos": 65, "end_pos": 87, "type": "TASK", "confidence": 0.8817757666110992}]}, {"text": "Like them, we rely on the presence or absence of cohesive links between the words in a text.", "labels": [], "entities": []}, {"text": "However, unlike Hirst and St-Onge we do not require a hand-crafted resource like WordNet or Roget's Thesaurus; our approach is knowledgelean.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 81, "end_pos": 88, "type": "DATASET", "confidence": 0.9603201746940613}]}], "datasetContent": [{"text": "For the lexical chain classifier we ran two experiments.", "labels": [], "entities": []}, {"text": "In the first, we used the data for one expression (break the ice) as a development set for optimising the two parameters (the relatedness threshold and the classification threshold).", "labels": [], "entities": []}, {"text": "To find good thresholds, a simple hill-climbing search was implemented during which we increased the relatedness threshold in steps of 0.02 and the classification threshold (governing the minimum chain length needed) in steps of 1.", "labels": [], "entities": []}, {"text": "We optimised the FScore for the literal class, though we found that the selected parameters varied only minimally when optimising for accuracy.", "labels": [], "entities": [{"text": "FScore", "start_pos": 17, "end_pos": 23, "type": "METRIC", "confidence": 0.7806113362312317}, {"text": "accuracy", "start_pos": 134, "end_pos": 142, "type": "METRIC", "confidence": 0.9960601925849915}]}, {"text": "We then used the parameter values determined in this way and applied the classifier to the remainder of the data.", "labels": [], "entities": []}, {"text": "The results obtained in this way depend to some extent on the data set used for the parameter setting.", "labels": [], "entities": []}, {"text": "To control this factor, we also ran another experiment in which we used an oracle to set the parameters (i.e., the parameters were optimised for the complete set).", "labels": [], "entities": []}, {"text": "While this is not a realistic scenario as it assumes that the labels of the test data are known during parameter setting, it does provide an upper bound for the lexical chain method.", "labels": [], "entities": []}, {"text": "For comparison, we also implemented an informed baseline classifier, which employs a simple model of cohesion, classifying expressions as literal if the noun inside the expression (e.g., ice for break the ice) is repeated elsewhere in the context, and non-literal otherwise.", "labels": [], "entities": []}, {"text": "One would expect this classifier to have a high precision for literal expressions but a low recall.", "labels": [], "entities": [{"text": "precision", "start_pos": 48, "end_pos": 57, "type": "METRIC", "confidence": 0.9978523254394531}, {"text": "recall", "start_pos": 92, "end_pos": 98, "type": "METRIC", "confidence": 0.9984737038612366}]}, {"text": "Finally, we implemented a supervised classifier.", "labels": [], "entities": []}, {"text": "Supervised classifiers have been used before for this task, notably by.", "labels": [], "entities": []}, {"text": "Our approach is slightly different: instead of creating meaning vectors we look at the word overlap 11 of a test instance with the literal and non-literal instances in the training set (for the same expression) and then assign the label of the closest set.", "labels": [], "entities": []}, {"text": "That such an approach might be promising becomes clear when one looks at some examples of literal and non-literal usage.", "labels": [], "entities": []}, {"text": "For instance, nonliteral examples of break the ice occur frequently with words such as diplomacy, relations, dialogue etc.", "labels": [], "entities": []}, {"text": "Effectively these words form lexical chains with the idiomatic meaning of break the ice.", "labels": [], "entities": []}, {"text": "They are absent for literal usages.", "labels": [], "entities": []}, {"text": "A supervised classifier can learn which terms are indicative of which usage.", "labels": [], "entities": []}, {"text": "Note that this information is expressionspecific, i.e., it is not possible to train a classifier for play with fire on labelled examples for break the ice.", "labels": [], "entities": []}, {"text": "This makes the supervised approach quite expensive in terms of annotation effort as data has to be labelled for each expression.", "labels": [], "entities": []}, {"text": "Nonetheless, it is instructive to see how well one could do with this approach.", "labels": [], "entities": []}, {"text": "In the experiments, we ran the supervised classifier in leave-one-out mode on each expression for which we had literal examples.", "labels": [], "entities": []}, {"text": "shows the results for the five classifiers discussed above: the informed baseline classifier (Rep), the cohesion graph (Graph), the lexical chain classifier with the parameters optimised on break the ice (LC), the lexical chain classifier with the parameters set by an oracle (LC-O), and the supervised classifier (Super).", "labels": [], "entities": []}, {"text": "The table also shows the accuracy that would be obtained by a CForm classifier, To appear) with gold standard canonical forms.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 25, "end_pos": 33, "type": "METRIC", "confidence": 0.9995081424713135}]}, {"text": "This classifier would label all examples in our data set as \"non-literal\" (it is thus equivalent to a majority class baseline).", "labels": [], "entities": []}, {"text": "Since the majority of examples is indeed used idiomatically, this classifier achieves a relatively high accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 104, "end_pos": 112, "type": "METRIC", "confidence": 0.9958518743515015}]}, {"text": "However, accuracy is not the best evaluation measure here be- We used the Dice coefficient as implemented in Ted Pedersen's Text::Similarity module: http://www.d.umn.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 9, "end_pos": 17, "type": "METRIC", "confidence": 0.9992806315422058}, {"text": "Dice coefficient", "start_pos": 74, "end_pos": 90, "type": "METRIC", "confidence": 0.822313129901886}]}, {"text": "edu/ \u02dc tpederse/text-similarity.html.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Accuracy, literal precision (P l ), recall  (R l ), and F-Score (F l ) for the classifiers", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9991093277931213}, {"text": "literal precision (P l )", "start_pos": 20, "end_pos": 44, "type": "METRIC", "confidence": 0.8431905011336008}, {"text": "recall  (R l )", "start_pos": 46, "end_pos": 60, "type": "METRIC", "confidence": 0.9592600226402282}, {"text": "F-Score (F l )", "start_pos": 66, "end_pos": 80, "type": "METRIC", "confidence": 0.9633729815483093}]}, {"text": " Table 3: Accuracies of the graph-based classifier  on each of the expressions (* indicates a dominant  literal usage)", "labels": [], "entities": [{"text": "Accuracies", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9945641756057739}]}]}