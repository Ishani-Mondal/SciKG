{"title": [{"text": "Fast Full Parsing by Linear-Chain Conditional Random Fields", "labels": [], "entities": []}], "abstractContent": [{"text": "This paper presents a chunking-based dis-criminative approach to full parsing.", "labels": [], "entities": [{"text": "full parsing", "start_pos": 65, "end_pos": 77, "type": "TASK", "confidence": 0.7763014733791351}]}, {"text": "We convert the task of full parsing into a series of chunking tasks and apply a conditional random field (CRF) model to each level of chunking.", "labels": [], "entities": []}, {"text": "The probability of an entire parse tree is computed as the product of the probabilities of individual chunk-ing results.", "labels": [], "entities": []}, {"text": "The parsing is performed in a bottom-up manner and the best derivation is efficiently obtained by using a depth-first search algorithm.", "labels": [], "entities": [{"text": "parsing", "start_pos": 4, "end_pos": 11, "type": "TASK", "confidence": 0.9775937795639038}]}, {"text": "Experimental results demonstrate that this simple parsing framework produces a fast and reasonably accurate parser.", "labels": [], "entities": [{"text": "parsing", "start_pos": 50, "end_pos": 57, "type": "TASK", "confidence": 0.964531421661377}]}], "introductionContent": [{"text": "Full parsing analyzes the phrase structure of a sentence and provides useful input for many kinds of high-level natural language processing such as summarization), pronoun resolution), and information extraction (.", "labels": [], "entities": [{"text": "Full parsing", "start_pos": 0, "end_pos": 12, "type": "TASK", "confidence": 0.8048475384712219}, {"text": "summarization", "start_pos": 148, "end_pos": 161, "type": "TASK", "confidence": 0.9801693558692932}, {"text": "pronoun resolution", "start_pos": 164, "end_pos": 182, "type": "TASK", "confidence": 0.7102905660867691}, {"text": "information extraction", "start_pos": 189, "end_pos": 211, "type": "TASK", "confidence": 0.8961243629455566}]}, {"text": "One of the major obstacles that discourage the use of full parsing in large-scale natural language processing applications is its computational cost.", "labels": [], "entities": []}, {"text": "For example, the MEDLINE corpus, a collection of abstracts of biomedical papers, consists of 70 million sentences and would require more than two years of processing time if the parser needs one second to process a sentence.", "labels": [], "entities": [{"text": "MEDLINE corpus", "start_pos": 17, "end_pos": 31, "type": "DATASET", "confidence": 0.9239257574081421}]}, {"text": "Generative models based on lexicalized PCFGs enjoyed great success as the machine learning framework for full parsing), but recently discriminative models attract more attention due to their superior accuracy and adaptability to new grammars and languages.", "labels": [], "entities": [{"text": "full parsing", "start_pos": 105, "end_pos": 117, "type": "TASK", "confidence": 0.6063589751720428}, {"text": "accuracy", "start_pos": 200, "end_pos": 208, "type": "METRIC", "confidence": 0.9921582937240601}]}, {"text": "A traditional approach to discriminative full parsing is to convert a full parsing task into a series of classification problems.", "labels": [], "entities": [{"text": "discriminative full parsing", "start_pos": 26, "end_pos": 53, "type": "TASK", "confidence": 0.5716257691383362}]}, {"text": "performs full parsing in a bottom-up and left-toright manner and uses a maximum entropy classifier to make decisions to construct individual phrases.", "labels": [], "entities": []}, {"text": "use the shiftreduce parsing framework and a maximum entropy model for local classification to decide parsing actions.", "labels": [], "entities": []}, {"text": "These approaches are often called history-based approaches.", "labels": [], "entities": []}, {"text": "A more recent approach to discriminative full parsing is to treat the task as a single structured prediction problem.", "labels": [], "entities": [{"text": "discriminative full parsing", "start_pos": 26, "end_pos": 53, "type": "TASK", "confidence": 0.553919772307078}]}, {"text": "incorporated rich local features into a tree CRF model and built a competitive parser.", "labels": [], "entities": []}, {"text": "proposed to use a parse forest to incorporate non-local features.", "labels": [], "entities": []}, {"text": "They used a perceptron algorithm to optimize the weights of the features and achieved state-of-the-art accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 103, "end_pos": 111, "type": "METRIC", "confidence": 0.9977402687072754}]}, {"text": "introduced latent variables in tree CRFs and proposed a caching mechanism to speedup the computation.", "labels": [], "entities": []}, {"text": "In general, the latter whole-sentence approaches give better accuracy than history-based approaches because they can better trade off decisions made in different parts in a parse tree.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 61, "end_pos": 69, "type": "METRIC", "confidence": 0.9971201419830322}]}, {"text": "However, the whole-sentence approaches tend to require a large computational cost both in training and parsing.", "labels": [], "entities": [{"text": "parsing", "start_pos": 103, "end_pos": 110, "type": "TASK", "confidence": 0.9598247408866882}]}, {"text": "In contrast, history-based approaches are less computationally intensive and usually produce fast parsers.", "labels": [], "entities": []}, {"text": "In this paper, we present a history-based parser using CRFs, by treating the task of full parsing as a series of chunking problems where it recognizes chunks in a flat input sequence.", "labels": [], "entities": []}, {"text": "We use the linear- chain CRF model to perform chunking.", "labels": [], "entities": []}, {"text": "Although our parsing model falls into the category of history-based approaches, it is one step closer to the whole-sentence approaches because the parser uses a whole-sequence model (i.e. CRFs) for individual chunking tasks.", "labels": [], "entities": []}, {"text": "In other words, our parser could be located somewhere between traditional history-based approaches and whole-sentence approaches.", "labels": [], "entities": []}, {"text": "One of our motivations for this work was that our parsing model may achieve a better balance between accuracy and speed than existing parsers.", "labels": [], "entities": [{"text": "parsing", "start_pos": 50, "end_pos": 57, "type": "TASK", "confidence": 0.979510486125946}, {"text": "accuracy", "start_pos": 101, "end_pos": 109, "type": "METRIC", "confidence": 0.9987149238586426}, {"text": "speed", "start_pos": 114, "end_pos": 119, "type": "METRIC", "confidence": 0.9288855195045471}]}, {"text": "It is also worth mentioning that our approach is similar in spirit to supertagging for parsing with lexicalized grammar formalisms such as CCG and HPSG), in which significant speed-ups for parsing time are achieved.", "labels": [], "entities": []}, {"text": "In this paper, we show that our approach is indeed appealing in that the parser runs very fast and gives competitive accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 117, "end_pos": 125, "type": "METRIC", "confidence": 0.9875136017799377}]}, {"text": "We evaluate our parser on the standard data set for parsing experiments (i.e. the Penn Treebank) and compare it with existing approaches to full parsing.", "labels": [], "entities": [{"text": "parsing experiments", "start_pos": 52, "end_pos": 71, "type": "TASK", "confidence": 0.8973911106586456}, {"text": "Penn Treebank)", "start_pos": 82, "end_pos": 96, "type": "DATASET", "confidence": 0.9808213710784912}]}, {"text": "This paper is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 presents the overall chunk parsing strategy.", "labels": [], "entities": [{"text": "chunk parsing", "start_pos": 31, "end_pos": 44, "type": "TASK", "confidence": 0.8612370491027832}]}, {"text": "Section 3 describes the CRF model used to perform individual chunking steps.", "labels": [], "entities": []}, {"text": "Section 4 describes the depth-first algorithm for finding the best derivation of a parse tree.", "labels": [], "entities": []}, {"text": "The part-of-speech tagger used in the parser is described in section 5.", "labels": [], "entities": [{"text": "part-of-speech tagger", "start_pos": 4, "end_pos": 25, "type": "TASK", "confidence": 0.7066546082496643}]}, {"text": "Experimental results on the Penn Treebank corpus are provided in Section 6.", "labels": [], "entities": [{"text": "Penn Treebank corpus", "start_pos": 28, "end_pos": 48, "type": "DATASET", "confidence": 0.9952905178070068}]}, {"text": "Section 7 discusses possible improvements and extensions of our work.", "labels": [], "entities": []}, {"text": "Section 8 offers some concluding remarks.", "labels": [], "entities": []}], "datasetContent": [{"text": "We ran parsing experiments using the Wall Street Journal corpus.", "labels": [], "entities": [{"text": "Wall Street Journal corpus", "start_pos": 37, "end_pos": 63, "type": "DATASET", "confidence": 0.9804504960775375}]}, {"text": "Sections 2-21 were used as the training data.", "labels": [], "entities": []}, {"text": "Section 22 was used as the development data, with which we tuned the feature set and parameters for learning and parsing.", "labels": [], "entities": []}, {"text": "Section 23 was reserved for the final accuracy report.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 38, "end_pos": 46, "type": "METRIC", "confidence": 0.9958220720291138}]}, {"text": "The training data for the CRF chunkers were created by converting each parse tree in the training data into a list of chunking sequences like the ones presented in.", "labels": [], "entities": [{"text": "CRF chunkers", "start_pos": 26, "end_pos": 38, "type": "TASK", "confidence": 0.6976042985916138}]}, {"text": "We trained three CRF models, i.e., the POS tagging model, the base chunking model, and the non-base chunking model.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 39, "end_pos": 50, "type": "TASK", "confidence": 0.6272024810314178}]}, {"text": "The training took about two days on a single CPU.", "labels": [], "entities": []}, {"text": "We used the evalb script provided by Sekine and Collins for evaluating the labeled recall/precision of the parser outputs 2 . All experiments were carried out on a server with 2.2 GHz AMD Opteron processors and 16GB memory.", "labels": [], "entities": [{"text": "recall", "start_pos": 83, "end_pos": 89, "type": "METRIC", "confidence": 0.9362754821777344}, {"text": "precision", "start_pos": 90, "end_pos": 99, "type": "METRIC", "confidence": 0.6376916170120239}]}], "tableCaptions": [{"text": " Table 4: Chunking performance (section 22, all  sentences).", "labels": [], "entities": []}, {"text": " Table 5: Beam width and parsing performance  (section 22, all sentences).", "labels": [], "entities": [{"text": "Beam width", "start_pos": 10, "end_pos": 20, "type": "TASK", "confidence": 0.5151260793209076}, {"text": "parsing", "start_pos": 25, "end_pos": 32, "type": "TASK", "confidence": 0.9555081725120544}]}, {"text": " Table 6: Parsing performance on section 23 (all sentences). * estimated from the parsing time on the  training data. ** reported in (Sagae and Lavie, 2006) where Pentium 4 3.2GHz was used to run the  parsers.", "labels": [], "entities": []}]}