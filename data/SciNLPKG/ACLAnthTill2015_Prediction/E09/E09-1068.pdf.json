{"title": [{"text": "Using Cycles and Quasi-Cycles to Disambiguate Dictionary Glosses", "labels": [], "entities": []}], "abstractContent": [{"text": "We present a novel graph-based algorithm for the automated disambiguation of glosses in lexical knowledge resources.", "labels": [], "entities": [{"text": "automated disambiguation of glosses in lexical knowledge resources", "start_pos": 49, "end_pos": 115, "type": "TASK", "confidence": 0.7334201671183109}]}, {"text": "A dictionary graph is built starting from senses (vertices) and explicit or implicit relations in the dictionary (edges).", "labels": [], "entities": []}, {"text": "The approach is based on the identification of edge sequences which constitute cycles in the dictionary graph (possibly with one edge reversed) and relate a source to a target word sense.", "labels": [], "entities": []}, {"text": "Experiments are performed on the disambiguation of ambiguous words in the glosses of WordNet and two machine-readable dictionaries.", "labels": [], "entities": [{"text": "disambiguation of ambiguous words", "start_pos": 33, "end_pos": 66, "type": "TASK", "confidence": 0.8376533091068268}, {"text": "WordNet", "start_pos": 85, "end_pos": 92, "type": "DATASET", "confidence": 0.9747231602668762}]}], "introductionContent": [{"text": "In the last two decades, we have witnessed an increasing availability of wide-coverage lexical knowledge resources in electronic format, most notably thesauri (such as Roget's Thesaurus, the Macquarie Thesaurus, etc.), machine-readable dictionaries (e.g., the Longman Dictionary of Contemporary English), computational lexicons (e.g. WordNet), etc.", "labels": [], "entities": [{"text": "Macquarie Thesaurus", "start_pos": 191, "end_pos": 210, "type": "DATASET", "confidence": 0.9385256469249725}, {"text": "Longman Dictionary of Contemporary English", "start_pos": 260, "end_pos": 302, "type": "DATASET", "confidence": 0.8969221830368042}]}, {"text": "The information contained in such resources comprises (depending on their kind) sense inventories, paradigmatic relations (e.g. flesh n is a kind of plant tissue 1 n ), 1 text definitions (e.g. flesh 3 n is defined as \"a soft moist part of a fruit\"), usage examples, and soon.", "labels": [], "entities": []}, {"text": "Unfortunately, not all the semantics are made explicit within lexical resources.", "labels": [], "entities": []}, {"text": "Even WordNet, the most widespread computational lexicon of English, provides explanatory information in the form of textual glosses, i.e. strings of text We denote as w i p the ith sense in a reference dictionary of a word w with part of speech p. which explain the meaning of concepts in terms of possibly ambiguous words.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 5, "end_pos": 12, "type": "DATASET", "confidence": 0.947028636932373}]}, {"text": "Moreover, while computational lexicons like WordNet contain semantically explicit information such as, among others, hypernymy and meronymy relations, most thesauri, glossaries, and machine-readable dictionaries are often just electronic transcriptions of their paper counterparts.", "labels": [], "entities": []}, {"text": "As a result, for each entry (e.g. a word sense or thesaurus entry) they mostly provide implicit information in the form of free text.", "labels": [], "entities": []}, {"text": "The production of semantically richer lexical resources can help alleviate the knowledge acquisition bottleneck and potentially enable advanced Natural Language Processing applications (Cuadros and).", "labels": [], "entities": []}, {"text": "However, in order to reduce the high cost of manual annotation), and to avoid the repetition of this effort for each knowledge resource, this task must be supported by wide-coverage automated techniques which do not rely on the specific resource at hand.", "labels": [], "entities": []}, {"text": "In this paper, we aim to make explicit large quantities of semantic information implicitly contained in the glosses of existing widecoverage lexical knowledge resources (specifically, machine-readable dictionaries and computational lexicons).", "labels": [], "entities": []}, {"text": "To this end, we present a method for Gloss Word Sense Disambiguation (WSD), called the Cycles and Quasi-Cycles (CQC) algorithm.", "labels": [], "entities": [{"text": "Gloss Word Sense Disambiguation (WSD)", "start_pos": 37, "end_pos": 74, "type": "TASK", "confidence": 0.664854713848659}]}, {"text": "The algorithm is based on a novel notion of cycles in the dictionary graph (possibly with one edge reversed) which support a disambiguation choice.", "labels": [], "entities": []}, {"text": "First, a dictionary graph is built from the input lexical knowledge resource.", "labels": [], "entities": []}, {"text": "Next, the method explicitly disambiguates the information associated with sense entries (i.e. gloss words) by associating senses for which the richest sets of paths can be found in the dictionary graph.", "labels": [], "entities": []}, {"text": "In Section 2, we provide basic definitions, present the gloss disambiguation algorithm, and il-lustrate the approach with an example.", "labels": [], "entities": []}, {"text": "In Section 3, we present a set of experiments performed on a variety of lexical knowledge resources, namely WordNet and two machine-readable dictionaries.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 108, "end_pos": 115, "type": "DATASET", "confidence": 0.9714075326919556}]}, {"text": "Results are discussed in Section 4, and related work is presented in Section 5.", "labels": [], "entities": []}, {"text": "We give our conclusions in Section 6.", "labels": [], "entities": []}], "datasetContent": [{"text": "To test and compare the performance of our algorithm, we performed a set of experiments on a variety of resources.", "labels": [], "entities": []}, {"text": "First, we summarize the resources (Section 3.1) and algorithms (Section 3.2) that we adopted.", "labels": [], "entities": []}, {"text": "In Section 3.3 we report our experimental results.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: The Cycles and Quasi-Cycles (CQC) al- gorithm in pseudocode.", "labels": [], "entities": []}, {"text": " Table 2: Gloss WSD performance on WordNet.", "labels": [], "entities": [{"text": "Gloss", "start_pos": 10, "end_pos": 15, "type": "METRIC", "confidence": 0.9072751402854919}, {"text": "WSD", "start_pos": 16, "end_pos": 19, "type": "TASK", "confidence": 0.6726243495941162}, {"text": "WordNet", "start_pos": 35, "end_pos": 42, "type": "DATASET", "confidence": 0.9516155123710632}]}, {"text": " Table 3: Gloss WSD performance on Macquarie  Concise.", "labels": [], "entities": [{"text": "Gloss", "start_pos": 10, "end_pos": 15, "type": "METRIC", "confidence": 0.9833849668502808}, {"text": "WSD", "start_pos": 16, "end_pos": 19, "type": "TASK", "confidence": 0.7432693243026733}, {"text": "Macquarie  Concise", "start_pos": 35, "end_pos": 53, "type": "DATASET", "confidence": 0.9941278696060181}]}, {"text": " Table 4: Gloss WSD performance on Ragazz- ini/Biagi.", "labels": [], "entities": [{"text": "Gloss", "start_pos": 10, "end_pos": 15, "type": "METRIC", "confidence": 0.9267557263374329}, {"text": "WSD", "start_pos": 16, "end_pos": 19, "type": "TASK", "confidence": 0.6089202165603638}, {"text": "Ragazz- ini/Biagi", "start_pos": 35, "end_pos": 52, "type": "DATASET", "confidence": 0.9109953761100769}]}]}