{"title": [{"text": "Generating a Non-English Subjectivity Lexicon: Relations That Matter", "labels": [], "entities": [{"text": "Generating a Non-English Subjectivity Lexicon", "start_pos": 0, "end_pos": 45, "type": "TASK", "confidence": 0.6414170026779175}]}], "abstractContent": [{"text": "We describe a method for creating a non-English subjectivity lexicon based on an English lexicon, an online translation service and a general purpose thesaurus: Wordnet.", "labels": [], "entities": [{"text": "Wordnet", "start_pos": 161, "end_pos": 168, "type": "DATASET", "confidence": 0.967331051826477}]}, {"text": "We use a PageRank-like algorithm to bootstrap from the translation of the English lexicon and rank the words in the thesaurus by polarity using the network of lexical relations in Wordnet.", "labels": [], "entities": [{"text": "Wordnet", "start_pos": 180, "end_pos": 187, "type": "DATASET", "confidence": 0.9803786277770996}]}, {"text": "We apply our method to the Dutch language.", "labels": [], "entities": []}, {"text": "The best results are achieved when using synonymy and antonymy relations only, and ranking positive and negative words simultaneously.", "labels": [], "entities": []}, {"text": "Our method achieves an accuracy of 0.82 at the top 3,000 negative words, and 0.62 at the top 3,000 positive words.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 23, "end_pos": 31, "type": "METRIC", "confidence": 0.9995253086090088}]}], "introductionContent": [{"text": "One of the key tasks in subjectivity analysis is the automatic detection of subjective (as opposed to objective, factual) statements in written documents ().", "labels": [], "entities": [{"text": "subjectivity analysis", "start_pos": 24, "end_pos": 45, "type": "TASK", "confidence": 0.7338546514511108}, {"text": "automatic detection of subjective (as opposed to objective, factual) statements in written documents", "start_pos": 53, "end_pos": 153, "type": "TASK", "confidence": 0.7140998430550098}]}, {"text": "This task is essential for applications such as online marketing research, where companies want to know what customers say about the companies, their products, specific products' features, and whether comments made are positive or negative.", "labels": [], "entities": [{"text": "online marketing research", "start_pos": 48, "end_pos": 73, "type": "TASK", "confidence": 0.7076031963030497}]}, {"text": "Another application is in political research, where public opinion could be assessed by analyzing usergenerated online data (blogs, discussion forums, etc.).", "labels": [], "entities": []}, {"text": "Most current methods for subjectivity identification rely on subjectivity lexicons, which list words that are usually associated with positive or negative sentiments or opinions (i.e., words with polarity).", "labels": [], "entities": [{"text": "subjectivity identification", "start_pos": 25, "end_pos": 52, "type": "TASK", "confidence": 0.7506669461727142}]}, {"text": "Such a lexicon can be used, e.g., to classify individual sentences or phrases as subjective or not, and as bearing positive or negative sentiments ().", "labels": [], "entities": []}, {"text": "For English, manually created subjectivity lexicons have been available fora while, but for many other languages such resources are still missing.", "labels": [], "entities": []}, {"text": "We describe a language-independent method for automatically bootstrapping a subjectivity lexicon, and apply and evaluate it for the Dutch language.", "labels": [], "entities": []}, {"text": "The method starts with an English lexicon of positive and negative words, automatically translated into the target language (Dutch in our case).", "labels": [], "entities": []}, {"text": "A PageRank-like algorithm is applied to the Dutch wordnet in order to filter and expand the set of words obtained through translation.", "labels": [], "entities": [{"text": "Dutch wordnet", "start_pos": 44, "end_pos": 57, "type": "DATASET", "confidence": 0.9028524458408356}]}, {"text": "The Dutch lexicon is then created from the resulting ranking of the wordnet nodes.", "labels": [], "entities": []}, {"text": "Our method has several benefits: \u2022 It is applicable to any language for which a wordnet and an automatic translation service or a machine-readable dictionary (from English) are available.", "labels": [], "entities": []}, {"text": "For example, the EuroWordnet project), e.g., provides wordnets for 7 languages, and free online translation services such as the one we have used in this paper are available for many other languages as well.", "labels": [], "entities": [{"text": "EuroWordnet project", "start_pos": 17, "end_pos": 36, "type": "DATASET", "confidence": 0.9699168801307678}]}, {"text": "\u2022 The method ranks all (or almost all) entries of a wordnet by polarity (positive or negative), which makes it possible to experiment with different settings of the precision/coverage threshold in applications that use the lexicon.", "labels": [], "entities": [{"text": "precision", "start_pos": 165, "end_pos": 174, "type": "METRIC", "confidence": 0.9949965476989746}]}, {"text": "We apply our method to the most recent version of, an extension of the Dutch WordNet, and we experiment with various parameters of the algorithm, in order to arrive at a good setting for porting the method to other languages.", "labels": [], "entities": [{"text": "Dutch WordNet", "start_pos": 71, "end_pos": 84, "type": "DATASET", "confidence": 0.8788579702377319}]}, {"text": "Specifically, we evaluate the quality of the resulting Dutch subjectivity lexicon using different subsets of wordnet relations and information in the glosses (definitions).", "labels": [], "entities": []}, {"text": "We also examine the effect of the number of iterations on the performance of our method.", "labels": [], "entities": []}, {"text": "We find that best performance is achieved when using only synonymy and antonymy relations and, moreover, the algorithm converges after about 10 iterations.", "labels": [], "entities": []}, {"text": "The remainder of the paper is organized as follows.", "labels": [], "entities": []}, {"text": "We summarize related work in section 2, present our method in section 3 and describe the manual assessment of the lexicon in section 4.", "labels": [], "entities": []}, {"text": "We discuss experimental results in section 5 and conclude in section 6.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluated several versions of the method of section 3 in order to find the best setting.", "labels": [], "entities": []}, {"text": "Our baseline is a ranking of all words in the wordnet with the weight -1 assigned to the translations of English negative polarity words, 1 assigned to the translations of positive words, and 0 assigned to the remaining words.", "labels": [], "entities": []}, {"text": "This corresponds to simply translating the English subjectivity lexicon.", "labels": [], "entities": []}, {"text": "In the run all.100 we applied our method to all words, synsets and relations from the Dutch Wordnet to create a graph with 153,386 nodes words) and 362,868 directed arcs (103,734 word-to-synset, 103,734 synset-to-word, 155,400 synset-to-synset relations).", "labels": [], "entities": [{"text": "Dutch Wordnet", "start_pos": 86, "end_pos": 99, "type": "DATASET", "confidence": 0.8549348711967468}]}, {"text": "We used 100 iterations of the PageRank algorihm for this run (and all runs below, unless indicated otherwise).", "labels": [], "entities": [{"text": "PageRank algorihm", "start_pos": 30, "end_pos": 47, "type": "DATASET", "confidence": 0.9502221345901489}]}, {"text": "In the run syn.100 we only used synset-toword, word-to-synset relations and 2,850 nearsynonymy relations between synsets.", "labels": [], "entities": []}, {"text": "We added 1,459 near-antonym relations to the graph to produce the run syn+ant.100.", "labels": [], "entities": []}, {"text": "In the run syn+hyp.100 we added 66,993 hyponymy and 66,993 hyperonymy relations to those used in run syn.100.", "labels": [], "entities": []}, {"text": "We also experimented with the information provided in the definitions (glosses) of synset.", "labels": [], "entities": []}, {"text": "The glosses were available for 68,122 of the 70,192 synsets.", "labels": [], "entities": []}, {"text": "Following, we assumed that there is a semantic relationship between a synset and each word used in its gloss.", "labels": [], "entities": []}, {"text": "Thus, the run gloss.100 uses a graph with words and 350,855 directed arcs from synsets to lemmas of all words in their glosses.", "labels": [], "entities": []}, {"text": "To create these arcs, glosses were lemmatized and lemmas not found in the wordnet were ignored.", "labels": [], "entities": []}, {"text": "To see if the information in the glosses can complement the wordnet relations, we also generated a hybrid run syn+ant+gloss.100 that used arcs derived from word-to-synset, synset-to-word, synonymy, antonymy relations and glosses.", "labels": [], "entities": []}, {"text": "Finally, we experimented with the number of iterations of PageRank in two setting: using all wordnet relations and using only synonyms and antonyms.", "labels": [], "entities": []}, {"text": "We used several measures to evaluate the quality of the word rankings produced by our method.", "labels": [], "entities": []}, {"text": "We consider the evaluation of a ranking parallel to the evaluation fora binary classification problem, where words are classified as positive (resp.", "labels": [], "entities": []}, {"text": "negative) if the assigned score exceeds a certain threshold value.", "labels": [], "entities": []}, {"text": "We can select a specific threshold and classify all words exceeding this score as positive.", "labels": [], "entities": []}, {"text": "There will be a certain amount of correctly classified words (true positives), and some incorrectly classified words (false positives).", "labels": [], "entities": []}, {"text": "As we move the threshold to include a larger portion of the ranking, both the number of true positives and the number of false positives increase.", "labels": [], "entities": []}, {"text": "We can visualize the quality of rankings by plotting their ROC curves, which show the relation between true positive rate (portion of the data correctly labeled as positive instances) and false positive rate (portion of the data incorrectly labeled as positive instances) at all possible threshold settings.", "labels": [], "entities": [{"text": "ROC", "start_pos": 59, "end_pos": 62, "type": "METRIC", "confidence": 0.9383203983306885}]}, {"text": "To compare rankings, we compute the area under the ROC curve (AUC), a measure frequently used to evaluate the performance of ranking classifiers.", "labels": [], "entities": [{"text": "ROC curve (AUC)", "start_pos": 51, "end_pos": 66, "type": "METRIC", "confidence": 0.9641757011413574}]}, {"text": "The AUC value corresponds to the probability that a randomly drawn positive instance will be ranked higher than a randomly drawn negative instance.", "labels": [], "entities": [{"text": "AUC", "start_pos": 4, "end_pos": 7, "type": "METRIC", "confidence": 0.9956217408180237}]}, {"text": "Thus, an AUC of 0.5 corresponds to random performance, a value of 1.0 corresponds to perfect performance.", "labels": [], "entities": [{"text": "AUC", "start_pos": 9, "end_pos": 12, "type": "METRIC", "confidence": 0.9940555095672607}]}, {"text": "When evaluating word rankings, we compute AU C \u2212 and AU C + as evalua-.", "labels": [], "entities": []}, {"text": "When comparing rankings, Kendall's measures look at the number of pairs of ranked items that agree or disagree with the ordering in the gold standard.", "labels": [], "entities": []}, {"text": "The measures can deal with partially ordered sets (i.e., rankings with ties): only pairs that are ordered in the gold standard are used.", "labels": [], "entities": []}, {"text": "Let T = {(a i , bi )} i denote the set of pairs ordered in the gold standard, i.e., a i g bi . Let C = {(a, b) \u2208 T | a r b} be the set of concordant pairs, i.e., pairs ordered the same way in the gold standard and in the ranking.", "labels": [], "entities": []}, {"text": "Let D = {(a, b) \u2208 T | b r a} be the set of discordant pairs and U = T \\ (C \u222a D) the set of pairs ordered in the gold standard, but tied in the ranking.", "labels": [], "entities": []}, {"text": "Kendall's rank correlation coefficient \u03c4 k and Kendall's distance D k are defined as follows: where p is a penalization factor for ties, which we set to 0.5, following.", "labels": [], "entities": [{"text": "rank correlation coefficient \u03c4 k", "start_pos": 10, "end_pos": 42, "type": "METRIC", "confidence": 0.7621944129467011}, {"text": "Kendall's distance D k", "start_pos": 47, "end_pos": 69, "type": "METRIC", "confidence": 0.6644821882247924}]}, {"text": "The value of \u03c4 k ranges from -1 (perfect disagreement) to 1 (perfect agreement), with 0 indicating an almost random ranking.", "labels": [], "entities": []}, {"text": "The value of D k ranges from 0 (perfect agreement) to 1 (perfect disagreement).", "labels": [], "entities": []}, {"text": "When applying Kendall's measures we assume that the gold standard defines a partial order: for two words a and b, a g b holds when a \u2208 N g , b \u2208 U g \u222a P g or when a \u2208 U g , b \u2208 P g ; here N g , U g , P g are sets of words judged as negative, neutral and positive, respectively, by human assessors.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Inter-annotator agreement per part-of- speech.", "labels": [], "entities": []}, {"text": " Table 2: Contingency table for all words assessed  by two annotators.", "labels": [], "entities": []}, {"text": " Table 4: Comparison of separate and simultaneous  rankings of negative and positive words.", "labels": [], "entities": []}]}