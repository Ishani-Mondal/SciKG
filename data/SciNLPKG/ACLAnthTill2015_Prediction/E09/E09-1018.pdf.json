{"title": [{"text": "EM Works for Pronoun Anaphora Resolution", "labels": [], "entities": [{"text": "Pronoun Anaphora", "start_pos": 13, "end_pos": 29, "type": "TASK", "confidence": 0.6573120057582855}]}], "abstractContent": [{"text": "We present an algorithm for pronoun-anaphora (in English) that uses Expectation Maximization (EM) to learn virtually all of its parameters in an unsupervised fashion.", "labels": [], "entities": [{"text": "Expectation Maximization (EM)", "start_pos": 68, "end_pos": 97, "type": "METRIC", "confidence": 0.8344057559967041}]}, {"text": "While EM frequently fails to find good models for the tasks to which it is set, in this case it works quite well.", "labels": [], "entities": [{"text": "EM", "start_pos": 6, "end_pos": 8, "type": "TASK", "confidence": 0.921655535697937}]}, {"text": "We have compared it to several systems available on the web (all we have found so far).", "labels": [], "entities": []}, {"text": "Our program significantly outperforms all of them.", "labels": [], "entities": []}, {"text": "The algorithm is fast and robust, and has been made publically available for downloading.", "labels": [], "entities": []}], "introductionContent": [{"text": "We present anew system for resolving (personal) pronoun anaphora . We believe it is of interest for two reasons.", "labels": [], "entities": [{"text": "resolving (personal) pronoun anaphora", "start_pos": 27, "end_pos": 64, "type": "TASK", "confidence": 0.70201542476813}]}, {"text": "First, virtually all of its parameters are learned via the expectationmaximization algorithm (EM).", "labels": [], "entities": []}, {"text": "While EM has worked quite well fora few tasks, notably machine translations (starting with the IBM models 1-5 (), it has not had success inmost others, such as part-of-speech tagging, named-entity recognition) and context-free-grammar induction (numerous attempts, too many to mention).", "labels": [], "entities": [{"text": "machine translations", "start_pos": 55, "end_pos": 75, "type": "TASK", "confidence": 0.7980776727199554}, {"text": "part-of-speech tagging", "start_pos": 160, "end_pos": 182, "type": "TASK", "confidence": 0.7630670666694641}, {"text": "named-entity recognition", "start_pos": 184, "end_pos": 208, "type": "TASK", "confidence": 0.7798848450183868}, {"text": "context-free-grammar induction", "start_pos": 214, "end_pos": 244, "type": "TASK", "confidence": 0.7291684746742249}]}, {"text": "Thus understanding the abilities and limitations of EM is very much a topic of interest.", "labels": [], "entities": [{"text": "EM", "start_pos": 52, "end_pos": 54, "type": "TASK", "confidence": 0.9266266226768494}]}, {"text": "We present this work as a positive data-point in this ongoing discussion.", "labels": [], "entities": []}, {"text": "Secondly, and perhaps more importantly, is the system's performance.", "labels": [], "entities": []}, {"text": "Remarkably, there are very few systems for actually doing pronoun anaphora available on the web.", "labels": [], "entities": []}, {"text": "By emailing the corporalist the other members of the list pointed us to The system, the Ge corpus, and the model described here can be downloaded from http://bllip.cs.brown.edu/download/emPronoun.tar.gz. four.", "labels": [], "entities": [{"text": "Ge corpus", "start_pos": 88, "end_pos": 97, "type": "DATASET", "confidence": 0.7909671664237976}]}, {"text": "We present ahead to head evaluation and find that our performance is significantly better than the competition.", "labels": [], "entities": []}], "datasetContent": [{"text": "To develop and test our program we use the dataset annotated by Niyu Ge (.", "labels": [], "entities": [{"text": "Niyu Ge", "start_pos": 64, "end_pos": 71, "type": "DATASET", "confidence": 0.7610621452331543}]}, {"text": "This consists of sections 0 and 1 of the Penn treebank.", "labels": [], "entities": [{"text": "Penn treebank", "start_pos": 41, "end_pos": 54, "type": "DATASET", "confidence": 0.9927418231964111}]}, {"text": "Ge marked every personal pronoun and all noun phrases that were coreferent with these pronouns.", "labels": [], "entities": []}, {"text": "We used section 0 as our development set, and section 1 for testing.", "labels": [], "entities": []}, {"text": "We reparsed the sentences using the Charniak and Johnson parser) rather than using the gold-parses that Ge marked up.", "labels": [], "entities": []}, {"text": "We hope thereby to make the results closer to those a user will experience.", "labels": [], "entities": []}, {"text": "(Generally the gold trees perform about 0.005 higher than the machine parsed version.)", "labels": [], "entities": []}, {"text": "The test set has 1119 personal pronouns of which 246 are non-anaphoric.", "labels": [], "entities": []}, {"text": "Our selection of this dataset, rather than the widely used MUC-6 corpus, is motivated by this large number of pronouns.", "labels": [], "entities": [{"text": "MUC-6 corpus", "start_pos": 59, "end_pos": 71, "type": "DATASET", "confidence": 0.9704290926456451}]}, {"text": "We compared our results to four currentlyavailable anaphora programs from the web.", "labels": [], "entities": []}, {"text": "These four were selected by sending a request to a commonly used mailing list (the \"corpora-list\") asking for such programs.", "labels": [], "entities": []}, {"text": "We received four leads: JavaRAP, Open-NLP, BART and GuiTAR.", "labels": [], "entities": [{"text": "BART", "start_pos": 43, "end_pos": 47, "type": "METRIC", "confidence": 0.9443227648735046}]}, {"text": "Of course, these systems represent the best available work, not the state of the art.", "labels": [], "entities": []}, {"text": "We presume that more recent supervised systems () per-form better.", "labels": [], "entities": []}, {"text": "Unfortunately, we were unable to obtain a comparison unsupervised learning system at all.", "labels": [], "entities": []}, {"text": "Only one of the four is explicitly aimed at personal-pronoun anaphora -RAP (Resolution of Anaphora Procedure).", "labels": [], "entities": [{"text": "personal-pronoun anaphora -RAP", "start_pos": 44, "end_pos": 74, "type": "METRIC", "confidence": 0.5098851546645164}]}, {"text": "It is a non-statistical system originally implemented in Prolog.", "labels": [], "entities": [{"text": "Prolog", "start_pos": 57, "end_pos": 63, "type": "DATASET", "confidence": 0.9404599070549011}]}, {"text": "The version we used is JavaRAP, a later reimplementation in Java (Long).", "labels": [], "entities": []}, {"text": "It only handles third person pronouns.", "labels": [], "entities": []}, {"text": "The other three are more general in that they handle all NP anaphora.", "labels": [], "entities": []}, {"text": "The GuiTAR system () is designed to work in an \"off the shelf\" fashion on general text GUI-TAR resolves pronouns using the algorithm of (), which filters candidate antecedents and then ranks them using morphosyntactic features.", "labels": [], "entities": [{"text": "GUI-TAR resolves pronouns", "start_pos": 87, "end_pos": 112, "type": "TASK", "confidence": 0.8414544264475504}]}, {"text": "Due to a bug in version 3, GUI-TAR does not currently handle possessive pronouns.GUITAR also has an optional discoursenew classification step, which cannot be used as it requires a discontinued Google search API.", "labels": [], "entities": [{"text": "discoursenew classification", "start_pos": 109, "end_pos": 136, "type": "TASK", "confidence": 0.6996950060129166}]}, {"text": "OpenNLP) uses a maximum-entropy classifier to rank potential antecedents for pronouns.", "labels": [], "entities": [{"text": "OpenNLP", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.9184768795967102}]}, {"text": "However despite being the best-performing (on pronouns) of the existing systems, there is a remarkable lack of published information on its innards.", "labels": [], "entities": []}, {"text": "BART) also uses a maximum-entropy model, based on.", "labels": [], "entities": [{"text": "BART", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.8124665021896362}]}, {"text": "The BART system also provides a more sophisticated feature set than is available in the basic model, including tree-kernel features and a variety of web-based knowledge sources.", "labels": [], "entities": [{"text": "BART", "start_pos": 4, "end_pos": 8, "type": "TASK", "confidence": 0.4225654602050781}]}, {"text": "Unfortunately we were notable to get the basic version working.", "labels": [], "entities": []}, {"text": "More precisely we were able to run the program, but the results we got were substantially lower than any of the other models and we believe that the program as shipped is not working properly.", "labels": [], "entities": []}, {"text": "Some of these systems provide their own preprocessing tools.", "labels": [], "entities": []}, {"text": "However, these were bypassed, so that all systems ran on the Charniak parse trees (with gold sentence segmentation).", "labels": [], "entities": [{"text": "Charniak parse trees", "start_pos": 61, "end_pos": 81, "type": "DATASET", "confidence": 0.7744950254758199}]}, {"text": "Systems with named-entity detectors were allowed to run them as a preprocess.", "labels": [], "entities": []}, {"text": "All systems were run using the models included in their standard distribution; typically these models are trained on annotated news articles (like MUC-6), which should be relatively similar to our WSJ documents.", "labels": [], "entities": [{"text": "MUC-6", "start_pos": 147, "end_pos": 152, "type": "DATASET", "confidence": 0.8911078572273254}, {"text": "WSJ documents", "start_pos": 197, "end_pos": 210, "type": "DATASET", "confidence": 0.9145666658878326}]}], "tableCaptions": [{"text": " Table 1: Words and their probabilities of generat- ing masculine, feminine and neuter pronouns", "labels": [], "entities": []}, {"text": " Table 2: The probability of an antecedent genera- tion a singular pronoun as a function of its number", "labels": [], "entities": []}, {"text": " Table 3: Probability of an antecedent generating a  first,second or third person pronoun as a function  of the antecedents person  Person of Pronoun  Person of Ante First  Second Third  First  0.089 0.021  0.889  Second  0.163 0.132  0.705  Third  0.025 0.011  0.964", "labels": [], "entities": []}, {"text": " Table 4: Same, but when the antecedent is in  quoted material but the pronoun is not", "labels": [], "entities": []}]}