{"title": [{"text": "TBL-Improved Non-Deterministic Segmentation and POS Tagging fora Chinese Parser", "labels": [], "entities": [{"text": "TBL-Improved Non-Deterministic Segmentation", "start_pos": 0, "end_pos": 43, "type": "TASK", "confidence": 0.5889406104882559}, {"text": "POS Tagging", "start_pos": 48, "end_pos": 59, "type": "TASK", "confidence": 0.5913870632648468}]}], "abstractContent": [{"text": "Although a lot of progress has been made recently in word segmentation and POS tagging for Chinese, the output of current state-of-the-art systems is too inaccurate to allow for syntactic analysis based on it.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 53, "end_pos": 70, "type": "TASK", "confidence": 0.7685303092002869}, {"text": "POS tagging", "start_pos": 75, "end_pos": 86, "type": "TASK", "confidence": 0.7877026498317719}]}, {"text": "We present an experiment in improving the output of an off-the-shelf module that performs segmentation and tagging , the tokenizer-tagger from Beijing University (PKU).", "labels": [], "entities": [{"text": "segmentation", "start_pos": 90, "end_pos": 102, "type": "TASK", "confidence": 0.9648719429969788}, {"text": "Beijing University (PKU)", "start_pos": 143, "end_pos": 167, "type": "DATASET", "confidence": 0.9294561743736267}]}, {"text": "Our approach is based on transformation-based learning (TBL).", "labels": [], "entities": []}, {"text": "Unlike in other TBL-based approaches to the problem, however, both obligatory and optional transformation rules are learned, so that the final system can output multiple segmentation and POS tagging analyses fora given input.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 187, "end_pos": 198, "type": "TASK", "confidence": 0.5614155381917953}]}, {"text": "By allowing fora small amount of ambiguity in the output of the tokenizer-tagger, we achieve a very considerable improvement inaccuracy.", "labels": [], "entities": []}, {"text": "Compared to the PKU tokenizer-tagger, we improve segmentation F-score from 94.18% to 96.74%, tagged word F-score from 84.63% to 92.44%, segmented sentence accuracy from 47.15% to 65.06% and tagged sentence accuracy from 14.07% to 31.47%.", "labels": [], "entities": [{"text": "F-score", "start_pos": 62, "end_pos": 69, "type": "METRIC", "confidence": 0.6072456240653992}, {"text": "F-score", "start_pos": 105, "end_pos": 112, "type": "METRIC", "confidence": 0.5483929514884949}, {"text": "accuracy", "start_pos": 155, "end_pos": 163, "type": "METRIC", "confidence": 0.9186751246452332}, {"text": "accuracy", "start_pos": 206, "end_pos": 214, "type": "METRIC", "confidence": 0.9669287800788879}]}], "introductionContent": [{"text": "Word segmentation and tagging are the necessary initial steps for almost any language processing system, and Chinese parsers are no exception.", "labels": [], "entities": [{"text": "Word segmentation", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.69231316447258}]}, {"text": "However, automatic Chinese word segmentation and tagging has been recognized as a very difficult task, for the following reasons: First, Chinese text provides few cues for word boundaries and part-ofspeech (POS) information.", "labels": [], "entities": [{"text": "automatic Chinese word segmentation and tagging", "start_pos": 9, "end_pos": 56, "type": "TASK", "confidence": 0.586144283413887}]}, {"text": "With the exception of punctuation marks, Chinese does not have word delimiters such as the whitespace used in English text, and unlike other languages without whitespaces such as Japanese, Chinese lacks morphological inflections that could provide cues for word boundaries and POS information.", "labels": [], "entities": []}, {"text": "In fact, the lack of word boundary marks and morphological inflection contributes not only to mistakes in machine processing of Chinese; it has also been identified as a factor for parsing miscues in Chinese children's reading behavior ().", "labels": [], "entities": [{"text": "parsing miscues", "start_pos": 181, "end_pos": 196, "type": "TASK", "confidence": 0.8980408310890198}]}, {"text": "Second, in addition to the two problems described above, segmentation and tagging also suffer from the fact that the notion of a word is very unclear in Chinese (.", "labels": [], "entities": [{"text": "segmentation", "start_pos": 57, "end_pos": 69, "type": "TASK", "confidence": 0.9652674794197083}]}, {"text": "While the word is an intuitive and salient notion in English, it is by no means a clear notion in Chinese.", "labels": [], "entities": []}, {"text": "Instead, for historical reasons, the intuitive and clear notion in Chinese language and culture is the character rather than the word.", "labels": [], "entities": []}, {"text": "Classical Chinese is in general monosyllabic, with each syllable corresponding to an independent morpheme that can be visually rendered with a written character.", "labels": [], "entities": []}, {"text": "In other words, characters did represent the basic syntactic unit in Classical Chinese, and thus became the sociologically intuitive notion.", "labels": [], "entities": []}, {"text": "However, although colloquial Chinese quickly evolved throughout Chinese history to be disyllabic or multi-syllabic, monosyllabic Classical Chinese has been considered more elegant and proper and was commonly used in written text until the early 20th century in China.", "labels": [], "entities": []}, {"text": "Even in Modern Chinese written text, Classical Chinese elements are not rare.", "labels": [], "entities": []}, {"text": "Consequently, even if a morpheme represented by a character is no longer used independently in Modern colloquial Chinese, it might still appear to be a free morpheme in modern written text, because it contains Classical Chinese elements.", "labels": [], "entities": []}, {"text": "This fact leads to a phenomenon in which Chinese speakers have difficulty differentiating whether a character represents abound or free morpheme, which in turn affects their judgment regarding where the word boundaries should be.", "labels": [], "entities": []}, {"text": "As pointed out by Hoosain, the varying knowledge of Classical Chinese among native Chinese speakers in fact affects their judgments about what is or is not a word.", "labels": [], "entities": []}, {"text": "In summary, due to the influence of Classical Chinese, the notion of a word and the boundary between abound and free morpheme is very unclear for Chinese speakers, which in turn leads to a fuzzy perception of where word boundaries should be.", "labels": [], "entities": []}, {"text": "Consequently, automatic segmentation and tagging in Chinese faces a serious challenge from prevalent ambiguities.", "labels": [], "entities": [{"text": "automatic segmentation and tagging", "start_pos": 14, "end_pos": 48, "type": "TASK", "confidence": 0.49467792361974716}]}, {"text": "For example 1 , the string \"\" can be segmented as (1a) or (1b), depending on the context.", "labels": [], "entities": []}, {"text": "y\u02c7ouy`\u0131y\u02c7ouy`y\u02c7ouy`\u0131 j\u00ec an have the intention meet The contrast shown in (2) illustrates that even a string that is not ambiguous in terms of segmentation can still be ambiguous in terms of tagging.", "labels": [], "entities": []}, {"text": "Even Chinese speakers cannot resolve such ambiguities without using further information from a bigger context, which suggests that resolving segmentation and tagging ambiguities probably should not be a task or goal at the word level.", "labels": [], "entities": [{"text": "resolving segmentation and tagging ambiguities", "start_pos": 131, "end_pos": 177, "type": "TASK", "confidence": 0.6792970180511475}]}, {"text": "Instead, we should preserve such ambiguities in this level and leave them to be resolved in a later stage, when more information is available.", "labels": [], "entities": []}, {"text": "To summarize, the word as a notion and hence word boundaries are very unclear; segmentation and tagging are prevalently ambiguous in Chinese.", "labels": [], "entities": []}, {"text": "These facts suggest that Chinese segmentation and part-of-speech identification are probably inherently non-deterministic at the word level.", "labels": [], "entities": [{"text": "Chinese segmentation", "start_pos": 25, "end_pos": 45, "type": "TASK", "confidence": 0.737971842288971}, {"text": "part-of-speech identification", "start_pos": 50, "end_pos": 79, "type": "TASK", "confidence": 0.7303188145160675}]}, {"text": "However most of the current segmentation and/or tagging systems output a single result.", "labels": [], "entities": [{"text": "segmentation and/or tagging", "start_pos": 28, "end_pos": 55, "type": "TASK", "confidence": 0.7740851521492005}]}, {"text": "While a deterministic approach to Chinese segmentation and POS tagging might be appropriate and necessary for certain tasks or applications, it has been shown to suffer from a problem of low accuracy.", "labels": [], "entities": [{"text": "Chinese segmentation", "start_pos": 34, "end_pos": 54, "type": "TASK", "confidence": 0.6391051411628723}, {"text": "POS tagging", "start_pos": 59, "end_pos": 70, "type": "TASK", "confidence": 0.7546238005161285}, {"text": "accuracy", "start_pos": 191, "end_pos": 199, "type": "METRIC", "confidence": 0.9938690066337585}]}, {"text": "As pointed out by Yu (), although the segmentation and tagging accuracy for certain types of text can reach as high as 95%, the accuracy for open domain text is only slightly higher than 80%.", "labels": [], "entities": [{"text": "segmentation and tagging", "start_pos": 38, "end_pos": 62, "type": "TASK", "confidence": 0.6732619305451711}, {"text": "accuracy", "start_pos": 63, "end_pos": 71, "type": "METRIC", "confidence": 0.9314063787460327}, {"text": "accuracy", "start_pos": 128, "end_pos": 136, "type": "METRIC", "confidence": 0.9990413784980774}]}, {"text": "Furthermore, Chinese segmentation (SIGHAN) bakeoff results also show that the performance of the Chinese segmentation systems has not improved a whole lot since 2003.", "labels": [], "entities": [{"text": "Chinese segmentation (SIGHAN) bakeoff", "start_pos": 13, "end_pos": 50, "type": "TASK", "confidence": 0.6893927355607351}]}, {"text": "This fact also indicates that deterministic approaches to Chinese segmentation have hit a bottleneck in terms of accuracy.", "labels": [], "entities": [{"text": "Chinese segmentation", "start_pos": 58, "end_pos": 78, "type": "TASK", "confidence": 0.6531324088573456}, {"text": "accuracy", "start_pos": 113, "end_pos": 121, "type": "METRIC", "confidence": 0.9975441098213196}]}, {"text": "The system for which we improved the output of the Beijing tokenizer-tagger is a hand-crafted Chinese grammar.", "labels": [], "entities": [{"text": "Beijing tokenizer-tagger", "start_pos": 51, "end_pos": 75, "type": "DATASET", "confidence": 0.8660165965557098}]}, {"text": "For such a system, as probably for any parsing system that presupposes segmented (and tagged) input, the accuracy of the segmentation and POS tagging analyses is critical.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 105, "end_pos": 113, "type": "METRIC", "confidence": 0.9994526505470276}, {"text": "POS tagging", "start_pos": 138, "end_pos": 149, "type": "TASK", "confidence": 0.673181563615799}]}, {"text": "However, as described in detail in the following section, even current state-of-art systems cannot provide satisfactory results for our application.", "labels": [], "entities": []}, {"text": "Based on the experiments presented in section 3, we believe that a proper amount of non-deterministic results can significantly improve the Chinese segmentation and tagging accuracy, which in turn improves the performance of the grammar.", "labels": [], "entities": [{"text": "Chinese segmentation", "start_pos": 140, "end_pos": 160, "type": "TASK", "confidence": 0.5986349731683731}, {"text": "accuracy", "start_pos": 173, "end_pos": 181, "type": "METRIC", "confidence": 0.9313861727714539}]}], "datasetContent": [{"text": "We started outwith a corpus of thirty goldsegmented and -tagged daily editions of the Xinhua Daily, which were provided by the Institute of Computational Linguistics at Beijing University.", "labels": [], "entities": [{"text": "Xinhua Daily", "start_pos": 86, "end_pos": 98, "type": "DATASET", "confidence": 0.8093127012252808}]}, {"text": "Three daily editions, which comprise 5,054 sentences with 129,377 words and 213,936 characters, were set aside for testing purposes; the remaining 27 editions were used for training.", "labels": [], "entities": []}, {"text": "With the idea of learning both obligatory and optional transformational rules in mind, we then split the training data into two roughly equally sized subsets.", "labels": [], "entities": []}, {"text": "All the data were broken into sentences using a very simple method: The end of a paragraph was always considered a sentence boundary.", "labels": [], "entities": []}, {"text": "Within paragraphs, sentence-final punctuation marks such as periods (which are unambiguous in Chinese), question marks and exclamation marks, potentially followed by a closing parenthesis, bracket or quote mark, were considered sentence boundaries.", "labels": [], "entities": []}, {"text": "We then had to come up with away of casting the problem of combined segmentation and POS tagging as a TBL problem.", "labels": [], "entities": [{"text": "segmentation", "start_pos": 68, "end_pos": 80, "type": "TASK", "confidence": 0.8949152231216431}, {"text": "POS tagging", "start_pos": 85, "end_pos": 96, "type": "TASK", "confidence": 0.7675304710865021}]}, {"text": "Following a strategy widely used in Chinese word segmentation, we did this by regarding the problem as a character tagging problem.", "labels": [], "entities": [{"text": "Chinese word segmentation", "start_pos": 36, "end_pos": 61, "type": "TASK", "confidence": 0.6324346462885538}, {"text": "character tagging problem", "start_pos": 105, "end_pos": 130, "type": "TASK", "confidence": 0.7892472545305887}]}, {"text": "However, since we intended to learn rules that deal with segmentation and POS tagging simultaneously, we could not adopt the BIO-coding approach.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 74, "end_pos": 85, "type": "TASK", "confidence": 0.7265014946460724}]}, {"text": "6 Also, since the TBLinduced transformational rules were to be converted into FST rules, we had to keep our character tagging scheme one-dimensional, unlike, who used a multi-dimensional TBL approach to solve the problem of combined segmentation and POS tagging.", "labels": [], "entities": [{"text": "character tagging", "start_pos": 108, "end_pos": 125, "type": "TASK", "confidence": 0.7095068097114563}, {"text": "POS tagging", "start_pos": 250, "end_pos": 261, "type": "TASK", "confidence": 0.7583034634590149}]}, {"text": "The character tagging scheme that we finally chose is illustrated in (6), where a. and b. show the character tags that we used for the analyses in (1a) and (1b) respectively.", "labels": [], "entities": [{"text": "character tagging", "start_pos": 4, "end_pos": 21, "type": "TASK", "confidence": 0.8048853278160095}]}, {"text": "The scheme consists in tagging the last character of a word with the part-ofspeech of the entire word; all non-final characters are tagged with '-'.", "labels": [], "entities": []}, {"text": "The main advantages of this character tagging scheme are that it expresses both word boundaries and parts-of-speech and that, at the same time, it is always consistent; inconsistencies between BIO tags indicating word boundaries and part-of-speech tags, which, for example, have to resolve, can simply not arise.", "labels": [], "entities": [{"text": "character tagging", "start_pos": 28, "end_pos": 45, "type": "TASK", "confidence": 0.7142603248357773}]}, {"text": "Both of the training data subsets were tagged according to our character tagging scheme and In this character tagging approach to word segmentation, characters are tagged as the beginning of a word (B), inside (or at the end) of a multi-character word (I) or a word of their own (O).", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 130, "end_pos": 147, "type": "TASK", "confidence": 0.7315001636743546}]}, {"text": "Their are numerous variations of this approach.", "labels": [], "entities": []}, {"text": "converted to the data format expected by \u00b5-TBL.", "labels": [], "entities": []}, {"text": "The first training data subset was used for learning obligatory resegmentation and retagging rules.", "labels": [], "entities": []}, {"text": "The corresponding rule templates, which define the space of possible rules to be explored, are given in.", "labels": [], "entities": []}, {"text": "The training parameters of \u00b5-TBL, which are an accuracy threshold and a score threshold, were set to 0.75 and 5 respectively; this means that a potential rule was only retained if at least 75% of the samples to which it would have applied were actually modified in the sense of the gold standard and not in some other way and that the learning process was terminated when no more rule could be found that applied to at least 5 samples in the first training data subset.", "labels": [], "entities": [{"text": "accuracy threshold", "start_pos": 47, "end_pos": 65, "type": "METRIC", "confidence": 0.9776915907859802}]}, {"text": "With these training parameters, 3,319 obligatory rules were learned by \u00b5-TBL.", "labels": [], "entities": []}, {"text": "Once the obligatory rules had been learned on the first training data subset, they were applied to the second training data subset.", "labels": [], "entities": []}, {"text": "Then, optional rules were learned on this second training data subset.", "labels": [], "entities": []}, {"text": "The rule templates used for optional rules are very similar to the ones used for obligatory rules; a few templates of optional rules are given in.", "labels": [], "entities": []}, {"text": "The difference between obligatory rules and optional rules is that the former replace one character tag by another, whereas the latter add character tags.", "labels": [], "entities": []}, {"text": "They hence introduce ambiguity, which is why we call them optional rules.", "labels": [], "entities": []}, {"text": "Like in the learning of the obligatory rules, the accuracy threshold used was 0.75; the score theshold was set to 7 because the training software seemed to hit a bug below that threshold.", "labels": [], "entities": [{"text": "accuracy threshold", "start_pos": 50, "end_pos": 68, "type": "METRIC", "confidence": 0.980777770280838}, {"text": "theshold", "start_pos": 94, "end_pos": 102, "type": "METRIC", "confidence": 0.6819486618041992}]}, {"text": "753 optional rules were learned.", "labels": [], "entities": []}, {"text": "We did not experiment with the adjustment of the training parameters on a separate held-out set.", "labels": [], "entities": []}, {"text": "Finally, the rule sets learned were converted into the fst) notation for transformational rules, so that they could be tested and used in the FST cascade used for preprocessing the input of the Chinese LFG.", "labels": [], "entities": [{"text": "Chinese LFG", "start_pos": 194, "end_pos": 205, "type": "DATASET", "confidence": 0.8884468078613281}]}, {"text": "For evaluation, the converted rules were applied to our test data set of 5,054 sentences.", "labels": [], "entities": [{"text": "test data set of 5,054 sentences", "start_pos": 56, "end_pos": 88, "type": "DATASET", "confidence": 0.8684733013312022}]}, {"text": "A few example rules learned by \u00b5-TBL with the set-up described above are given in; we show them both in \u00b5-TBL notation and in fst notation.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Evaluation figures achieved by four different systems on the 5,054 sentences of our test set", "labels": [], "entities": [{"text": "Evaluation", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9558383226394653}]}]}