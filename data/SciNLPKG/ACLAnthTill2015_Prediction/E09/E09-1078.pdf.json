{"title": [{"text": "Natural Language Generation as Planning Under Uncertainty for Spoken Dialogue Systems", "labels": [], "entities": [{"text": "Natural Language Generation", "start_pos": 0, "end_pos": 27, "type": "TASK", "confidence": 0.6099226673444113}]}], "abstractContent": [{"text": "We present and evaluate anew model for Natural Language Generation (NLG) in Spoken Dialogue Systems, based on statistical planning, given noisy feedback from the current generation context (e.g. a user and a surface realiser).", "labels": [], "entities": [{"text": "Natural Language Generation (NLG)", "start_pos": 39, "end_pos": 72, "type": "TASK", "confidence": 0.8193735380967458}]}, {"text": "We study its use in a standard NLG problem: how to present information (in this case a set of search results) to users, given the complex trade-offs between utterance length, amount of information conveyed, and cognitive load.", "labels": [], "entities": []}, {"text": "We set these trade-offs by analysing existing MATCH data.", "labels": [], "entities": [{"text": "MATCH data", "start_pos": 46, "end_pos": 56, "type": "DATASET", "confidence": 0.8026200830936432}]}, {"text": "We then train a NLG policy using Reinforcement Learning (RL), which adapts its behaviour to noisy feedback from the current generation context.", "labels": [], "entities": []}, {"text": "This policy is compared to several base-lines derived from previous work in this area.", "labels": [], "entities": []}, {"text": "The learned policy significantly out-performs all the prior approaches.", "labels": [], "entities": []}], "introductionContent": [{"text": "Natural language allows us to achieve the same communicative goal (\"what to say\") using many different expressions (\"how to say it\").", "labels": [], "entities": []}, {"text": "Ina Spoken Dialogue System (SDS), an abstract communicative goal (CG) can be generated in many different ways.", "labels": [], "entities": []}, {"text": "For example, the CG to present database results to the user can be realized as a summary), or by comparing items ( ), or by picking one item and recommending it to the user (.", "labels": [], "entities": []}, {"text": "Previous work has shown that it is useful to adapt the generated output to certain features of the dialogue context, for example user preferences, e.g. (), user knowledge, e.g., or predicted TTS quality, e.g. ().", "labels": [], "entities": []}, {"text": "In extending this previous work we treat NLG as a statistical sequential planning problem, analogously to current statistical approaches to Dialogue Management (DM), e.g. ( and \"conversation as action under uncertainty\").", "labels": [], "entities": [{"text": "Dialogue Management (DM)", "start_pos": 140, "end_pos": 164, "type": "TASK", "confidence": 0.8511402368545532}]}, {"text": "In NLG we have similar trade-offs and unpredictability as in DM, and in some systems the content planning and DM tasks are overlapping.", "labels": [], "entities": []}, {"text": "Clearly, very long system utterances with many actions in them are to be avoided, because users may become confused or impatient, but each individual NLG action will convey some (potentially) useful information to the user.", "labels": [], "entities": []}, {"text": "There is therefore an optimization problem to be solved.", "labels": [], "entities": []}, {"text": "Moreover, the user judgements or next (most likely) action after each NLG action are unpredictable, and the behaviour of the surface realizer may also be variable (see Section 6.2).", "labels": [], "entities": []}, {"text": "NLG could therefore fruitfully be approached as a sequential statistical planning task, where there are trade-offs and decisions to make, such as whether to choose another NLG action (and which one to choose) or to instead stop generating.", "labels": [], "entities": [{"text": "NLG", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.8737921118736267}]}, {"text": "Reinforcement Learning (RL) allows us to optimize such trade-offs in the presence of uncertainty, i.e. the chances of achieving a better state, while engaging in the risk of choosing another action.", "labels": [], "entities": [{"text": "Reinforcement Learning (RL)", "start_pos": 0, "end_pos": 27, "type": "TASK", "confidence": 0.747137063741684}]}, {"text": "In this paper we present and evaluate anew model for NLG in Spoken Dialogue Systems as planning under uncertainty.", "labels": [], "entities": []}, {"text": "In Section 2 we argue for applying RL to NLG problems and explain the overall framework.", "labels": [], "entities": [{"text": "RL", "start_pos": 35, "end_pos": 37, "type": "TASK", "confidence": 0.9588463306427002}]}, {"text": "In Section 3 we discuss challenges for NLG for Information Presentation.", "labels": [], "entities": [{"text": "Information Presentation", "start_pos": 47, "end_pos": 71, "type": "TASK", "confidence": 0.78068807721138}]}, {"text": "In Section 4 we present results from our analysis of the MATCH corpus ( ).", "labels": [], "entities": [{"text": "MATCH corpus", "start_pos": 57, "end_pos": 69, "type": "DATASET", "confidence": 0.9246836006641388}]}, {"text": "In Section 5 we present a detailed example of our proposed NLG method.", "labels": [], "entities": []}, {"text": "In Section 6 we report on experimental results using this framework for exploring Information Presentation policies.", "labels": [], "entities": []}, {"text": "In Section 7 we conclude and discuss future directions.", "labels": [], "entities": []}], "datasetContent": [{"text": "We now report on a proof-of-concept study where we train our policy in a simulated learning environment based on the results from the MATCH corpus analysis in Section 4.", "labels": [], "entities": [{"text": "MATCH corpus analysis", "start_pos": 134, "end_pos": 155, "type": "DATASET", "confidence": 0.8961822191874186}]}, {"text": "Simulation-based RL allows to explore unseen actions which are not in the data, and thus less initial data is needed ().", "labels": [], "entities": [{"text": "Simulation-based RL", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.697613000869751}]}, {"text": "Note, that we cannot directly learn from the MATCH data, as therefore we would need data from an interactive dialogue.", "labels": [], "entities": [{"text": "MATCH data", "start_pos": 45, "end_pos": 55, "type": "DATASET", "confidence": 0.8382590413093567}]}, {"text": "We are currently collecting such data in a Wizard-of-Oz experiment.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: NLG strategies present in the MATCH corpus with average no. attributes and sentences as found  in the data.", "labels": [], "entities": [{"text": "MATCH corpus", "start_pos": 40, "end_pos": 52, "type": "DATASET", "confidence": 0.8562330901622772}]}, {"text": " Table 3: NLG bi-gram user simulation", "labels": [], "entities": []}, {"text": " Table 5: Evaluation Results (p < .001 )", "labels": [], "entities": []}]}