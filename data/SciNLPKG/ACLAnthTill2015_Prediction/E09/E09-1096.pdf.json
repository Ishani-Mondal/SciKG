{"title": [{"text": "Feature-based Method for Document Alignment in Comparable News Corpora", "labels": [], "entities": [{"text": "Document Alignment", "start_pos": 25, "end_pos": 43, "type": "TASK", "confidence": 0.9262601137161255}]}], "abstractContent": [{"text": "In this paper, we present a feature-based method to align documents with similar content across two sets of bilingual comparable corpora from daily news texts.", "labels": [], "entities": []}, {"text": "We evaluate the contribution of each individual feature and investigate the incorporation of these diverse statistical and heuristic features for the task of bilingual document alignment.", "labels": [], "entities": [{"text": "bilingual document alignment", "start_pos": 158, "end_pos": 186, "type": "TASK", "confidence": 0.6617581943670908}]}, {"text": "Experimental results on the English-Chinese and English-Malay comparable news corpora show that our proposed Discrete Fourier Transform-based term frequency distribution feature is very effective.", "labels": [], "entities": []}, {"text": "It contributes 4.1% and 8% to performance improvement over Pearson's correlation method on the two comparable corpora.", "labels": [], "entities": []}, {"text": "In addition, when more heuristic and statistical features as well as a bilingual dictionary are utilized, our method shows an absolute performance improvement of 23.2% and 15.3% on the two sets of bilingual corpora when comparing with a prior information retrieval-based method.", "labels": [], "entities": []}], "introductionContent": [{"text": "The problem of document alignment is described as the task of aligning documents, news articles for instance, across two corpora based on content similarity.", "labels": [], "entities": [{"text": "document alignment", "start_pos": 15, "end_pos": 33, "type": "TASK", "confidence": 0.7246986329555511}]}, {"text": "The groups of corpora can be in the same or in different languages, depending on the purpose of one's task.", "labels": [], "entities": []}, {"text": "In our study, we attempt to align similar documents across comparable corpora which are bilingual, each set written in a different language but having similar content and domain coverage for different communication needs.", "labels": [], "entities": []}, {"text": "Previous works on monolingual document alignment focus on automatic alignment between documents and their presentation slides or between documents and their abstracts.", "labels": [], "entities": [{"text": "monolingual document alignment", "start_pos": 18, "end_pos": 48, "type": "TASK", "confidence": 0.6253591875235239}]}, {"text": "uses two similarity measures, Cosine and Jaccard, to calculate the candidate alignment score in his SlideSeer system, a digital library software that retrieves documents and their narrated slide presentations.", "labels": [], "entities": [{"text": "Jaccard", "start_pos": 41, "end_pos": 48, "type": "METRIC", "confidence": 0.8892956972122192}]}, {"text": "use a phrase-based HMM model to mine the alignment between documents and their human-written abstracts.", "labels": [], "entities": []}, {"text": "The main purpose of this work is to increase the size of the training corpus fora statistical-based summarization system.", "labels": [], "entities": []}, {"text": "The research on similarity calculation for multilingual comparable corpora has attracted more attention than monolingual comparable corpora.", "labels": [], "entities": [{"text": "similarity calculation", "start_pos": 16, "end_pos": 38, "type": "TASK", "confidence": 0.6949511021375656}]}, {"text": "However, the purpose and scenario of these works are rather varied.", "labels": [], "entities": []}, {"text": "represent document contents using descriptor terms of a multilingual thesaurus EUROVOC 1 , and calculate the semantic similarity based on the distance between the two documents' representations.", "labels": [], "entities": [{"text": "EUROVOC", "start_pos": 79, "end_pos": 86, "type": "DATASET", "confidence": 0.8523244857788086}]}, {"text": "The assignment of descriptors is trained by log-likelihood test and computed by \ud97b\udf59\ud97b\udf59\ud97b\udf59\ud97b\udf59\ud97b\udf59, Cosine, and Okapi.", "labels": [], "entities": []}, {"text": "Similarly, use a linear combination of three types of knowledge: cognates, geographical place names reference, and map documents based on the EUROVOC.", "labels": [], "entities": [{"text": "EUROVOC", "start_pos": 142, "end_pos": 149, "type": "DATASET", "confidence": 0.9667567014694214}]}, {"text": "The major limitation of these works is the use of EUROVOC, which is a specific resource workable only for European languages.", "labels": [], "entities": [{"text": "EUROVOC", "start_pos": 50, "end_pos": 57, "type": "DATASET", "confidence": 0.8791468143463135}]}, {"text": "Aligning documents across parallel corpora is another area of interest.", "labels": [], "entities": [{"text": "Aligning documents across parallel corpora", "start_pos": 0, "end_pos": 42, "type": "TASK", "confidence": 0.8729867100715637}]}, {"text": "use three similarity scores, Cosine, Normalized Edit Distance, and Sentence Alignment Score, to compute the similarity between two parallel documents.", "labels": [], "entities": [{"text": "Cosine", "start_pos": 29, "end_pos": 35, "type": "METRIC", "confidence": 0.9801692366600037}, {"text": "Normalized Edit Distance", "start_pos": 37, "end_pos": 61, "type": "METRIC", "confidence": 0.6681177318096161}]}, {"text": "An Adaboost classifier is trained on a list of scored text pairs labeled as parallel or nonparallel.", "labels": [], "entities": []}, {"text": "Then, the learned classifier is used to check the correctness of each alignment candidate.", "labels": [], "entities": []}, {"text": "Their method is simple but effective.", "labels": [], "entities": []}, {"text": "However, the features used in this method are only suitable for parallel corpora as the measurement is mainly based on structural similarity.", "labels": [], "entities": []}, {"text": "One goal of document alignment is for parallel sentence extraction for applications like statistical machine translation.", "labels": [], "entities": [{"text": "document alignment", "start_pos": 12, "end_pos": 30, "type": "TASK", "confidence": 0.7873310148715973}, {"text": "parallel sentence extraction", "start_pos": 38, "end_pos": 66, "type": "TASK", "confidence": 0.6463108658790588}, {"text": "statistical machine translation", "start_pos": 89, "end_pos": 120, "type": "TASK", "confidence": 0.7472095489501953}]}, {"text": "highlight that most of the current sentence alignment models are applicable for parallel documents, rather than comparable documents.", "labels": [], "entities": [{"text": "sentence alignment", "start_pos": 35, "end_pos": 53, "type": "TASK", "confidence": 0.7407777309417725}]}, {"text": "In addition, they argue that document alignment should be done before parallel sentence extraction.", "labels": [], "entities": [{"text": "document alignment", "start_pos": 29, "end_pos": 47, "type": "TASK", "confidence": 0.7677937746047974}, {"text": "parallel sentence extraction", "start_pos": 70, "end_pos": 98, "type": "TASK", "confidence": 0.6016541123390198}]}, {"text": "propose a general method to extract comparable bilingual text without using any linguistic resources.", "labels": [], "entities": []}, {"text": "The main feature of this method is the frequency correlation of words in different languages.", "labels": [], "entities": []}, {"text": "They assume that those words in different languages should have similar frequency correlation if they are actually translations of each other.", "labels": [], "entities": []}, {"text": "The association between two documents is then calculated based on this information using Pearson's correlation together with two monolingual features \ud97b\udf59\ud97b\udf5925 , a term frequency normalization, and \ud97b\udf59\ud97b\udf59\ud97b\udf59.", "labels": [], "entities": [{"text": "Pearson's correlation", "start_pos": 89, "end_pos": 110, "type": "METRIC", "confidence": 0.6988793015480042}]}, {"text": "The main advantages of this approach are that it is purely statistical-based and it is language-independent.", "labels": [], "entities": []}, {"text": "However, its performance maybe compromised due to the lack of linguistic knowledge, particularly across corpora which are linguistically very different.", "labels": [], "entities": []}, {"text": "Recently, Munteanu (2006) introduces a rather simple way to get the group of similar content document in multilingual comparable corpus by using the).", "labels": [], "entities": []}, {"text": "This method first pushes all the target documents into the database of the Lemur, and then uses a word-byword translation of each source document as a query to retrieve similar content target documents.", "labels": [], "entities": []}, {"text": "This paper will leverage on previous work, and propose and explore diverse range of features in our system.", "labels": [], "entities": []}, {"text": "Our document alignment system consists of three stages: candidate generation, feature extraction and feature combination.", "labels": [], "entities": [{"text": "document alignment", "start_pos": 4, "end_pos": 22, "type": "TASK", "confidence": 0.694627583026886}, {"text": "candidate generation", "start_pos": 56, "end_pos": 76, "type": "TASK", "confidence": 0.6991299092769623}, {"text": "feature extraction", "start_pos": 78, "end_pos": 96, "type": "TASK", "confidence": 0.7148148864507675}, {"text": "feature combination", "start_pos": 101, "end_pos": 120, "type": "TASK", "confidence": 0.668342262506485}]}, {"text": "We verify our method on two set of bilingual news comparable corpora English-Chinese and English-Malay.", "labels": [], "entities": []}, {"text": "Experimental results show that 1) when only using Fourier Transform-based term frequency, our method outperforms our reimplementation of's method by 4.1% and 8% for the top 100 alignment candidates and, 2) when using all features, our method significantly outperforms our implementation of Munteanu's (2006) method by 23.2% and 15.3%.", "labels": [], "entities": []}, {"text": "The paper is organized as follows.", "labels": [], "entities": []}, {"text": "In section 2, we describe the overall architecture of our system.", "labels": [], "entities": []}, {"text": "Section 3 discusses our improved frequency correlation-based feature, while Section 4 describes in detail the document relationship heuristics used in our model.", "labels": [], "entities": []}, {"text": "Section 5 reports the experimental results.", "labels": [], "entities": []}, {"text": "Finally, we conclude our work in section 6.", "labels": [], "entities": []}, {"text": "shows the general architecture of our document alignment system.", "labels": [], "entities": [{"text": "document alignment", "start_pos": 38, "end_pos": 56, "type": "TASK", "confidence": 0.7013614773750305}]}, {"text": "It consists of three components: candidate generation, feature extraction, and feature combination.", "labels": [], "entities": [{"text": "candidate generation", "start_pos": 33, "end_pos": 53, "type": "TASK", "confidence": 0.7423600256443024}, {"text": "feature extraction", "start_pos": 55, "end_pos": 73, "type": "TASK", "confidence": 0.7773882448673248}, {"text": "feature combination", "start_pos": 79, "end_pos": 98, "type": "TASK", "confidence": 0.6881971806287766}]}, {"text": "Our system works on two sets of monolingual corpora to derive a set of document alignments that are comparable in their content.", "labels": [], "entities": []}], "datasetContent": [{"text": "The experiments were conducted on two sets of comparable corpora namely English-Chinese and English-Malay.", "labels": [], "entities": []}, {"text": "The data are from three news publications in Singapore: the Strait Times (ST, English), Lian He Zao Bao (ZB, Chinese), and Berita Harian (BH, Malay).", "labels": [], "entities": [{"text": "Strait Times (ST", "start_pos": 60, "end_pos": 76, "type": "DATASET", "confidence": 0.8569192588329315}]}, {"text": "Since these languages are from different language families 5 , our model can be considered as language independent.", "labels": [], "entities": []}, {"text": "The evaluation is conducted based on a set of manually aligned documents prepared by a group of bilingual students.", "labels": [], "entities": []}, {"text": "It is done by carefully reading through each article in the month of June (2006) for both sets of corpora and trying to find articles of similar content in the other language within the given time window.", "labels": [], "entities": []}, {"text": "Alignment is based on similarity of content where the same story or event is mentioned.", "labels": [], "entities": [{"text": "Alignment", "start_pos": 0, "end_pos": 9, "type": "TASK", "confidence": 0.9731947779655457}]}, {"text": "Any two bilingual articles with at least 50% content overlapping are considered as comparable.", "labels": [], "entities": []}, {"text": "This set of reference data is cross-validated between annotators.", "labels": [], "entities": []}, {"text": "shows the statistics of our reference data for document alignment.", "labels": [], "entities": [{"text": "document alignment", "start_pos": 47, "end_pos": 65, "type": "TASK", "confidence": 0.7331820726394653}]}, {"text": "Note that although there are 438 alignments for ST-ZB, the number of unique ST articles are 396, implying that the mapping is not one-to-one.", "labels": [], "entities": []}, {"text": "Evaluation is performed on two levels to reflect performance from two different perspectives.", "labels": [], "entities": []}, {"text": "\"Macro evaluation\" is conducted to assess the correctness of the alignment candidates given their rank among all the alignment candidates.", "labels": [], "entities": []}, {"text": "\"Micro evaluation\" concerns about the correctness of the aligned documents returned fora given source document.", "labels": [], "entities": []}, {"text": "Macro evaluation: we present the performance for macro evaluation using average precision.", "labels": [], "entities": [{"text": "precision", "start_pos": 80, "end_pos": 89, "type": "METRIC", "confidence": 0.9580684900283813}]}, {"text": "It is used to evaluate the performance of a ranked list and gives higher score for the list that returns more correct alignment in the top.", "labels": [], "entities": []}, {"text": "Micro evaluation: for micro evaluation, we evaluate the F-Score, calculated from recall and precision, based on the number of correct alignments for the top of alignment candidates for each source document.", "labels": [], "entities": [{"text": "F-Score", "start_pos": 56, "end_pos": 63, "type": "METRIC", "confidence": 0.9946093559265137}, {"text": "recall", "start_pos": 81, "end_pos": 87, "type": "METRIC", "confidence": 0.9992194175720215}, {"text": "precision", "start_pos": 92, "end_pos": 101, "type": "METRIC", "confidence": 0.9970537424087524}]}, {"text": "First we implement the method of as the baseline.", "labels": [], "entities": []}, {"text": "Basically, this method does not depend on any linguistic resources and calculates the similarity between two documents purely by comparing all possible pairs of words.", "labels": [], "entities": []}, {"text": "In addition to this, we also implement Munteanu's (2006) method which uses Okapi scoring function from the Lemur Toolkit) to obtain the similarity score.", "labels": [], "entities": [{"text": "Lemur Toolkit", "start_pos": 107, "end_pos": 120, "type": "DATASET", "confidence": 0.8713888227939606}, {"text": "similarity score", "start_pos": 136, "end_pos": 152, "type": "METRIC", "confidence": 0.9698834121227264}]}, {"text": "This approach relies heavily on bilingual dictionaries.", "labels": [], "entities": []}, {"text": "To assess performances more fairly, the result from baseline method of Tao and Zhai are compared against the results of the following list of incremental approaches: the baseline (A); the baseline using term instead of word (B); replacing \ud97b\udf59\ud97b\udf59, \ud97b\udf59 by \ud97b\udf59\ud97b\udf59, \ud97b\udf59 for \ud97b\udf59\ud97b\udf59\ud97b\udf59 feature, with and without bilingual dictionaries in (C) and (D) respectively; and including \ud97b\udf59\ud97b\udf59\ud97b\udf59 and \ud97b\udf59\ud97b\udf59\ud97b\udf59 for our final model in (E).", "labels": [], "entities": []}, {"text": "Our model is also compared our model with results from the implementation of Munteanu (2006) using Okapi (F), and the results from a combination of our model with Okapi (G).", "labels": [], "entities": []}, {"text": "show the experimental results for two language pairs EnglishChinese (ST-ZB) and English -Malay (ST-BH), respectively.", "labels": [], "entities": []}, {"text": "Each row displays the result of each experiment at a certain cut-off among the top returned alignments.", "labels": [], "entities": []}, {"text": "The \"Top\" columns reflect the cut-off threshold.", "labels": [], "entities": []}, {"text": "The first three cases (A), (B) and (C), which do not rely on linguistic resources, suggest that our new features lead to better performance improvement over the baseline.", "labels": [], "entities": []}, {"text": "It can be seen that the use of term and \ud97b\udf59\ud97b\udf59\ud97b\udf59 significantly improves the performance.", "labels": [], "entities": []}, {"text": "The improvement indicated by a sharp increase in all cases from (C) to (D) shows that dictionaries can indeed help \ud97b\udf59\ud97b\udf59\ud97b\udf59 features.", "labels": [], "entities": []}, {"text": "Based on the result of (E), our final model significantly outperforms the model of Munteanu (F) in both macro and micro evaluation.", "labels": [], "entities": []}, {"text": "It is noted that our features rely less heavily on dictionaries as it only makes use of this resource to translate term words and title words of a document while needs to translate entire documents, exclude stopword, and relying on an IR system.", "labels": [], "entities": []}, {"text": "It is also observed that the performance of (G) shows that although the incorporation of Okapi score in our final model (E) improves the average precision performance of ST-ZB slightly, it does not appear to be helpful for our ST-BH data.", "labels": [], "entities": [{"text": "precision", "start_pos": 145, "end_pos": 154, "type": "METRIC", "confidence": 0.9963597655296326}, {"text": "ST-BH data", "start_pos": 227, "end_pos": 237, "type": "DATASET", "confidence": 0.7683719992637634}]}, {"text": "However, Okapi does help in the F-Measure on both corpora..", "labels": [], "entities": [{"text": "F-Measure", "start_pos": 32, "end_pos": 41, "type": "METRIC", "confidence": 0.9901276230812073}]}, {"text": "Performance of Strait Times -Berita Harian.", "labels": [], "entities": [{"text": "Strait Times -Berita Harian", "start_pos": 15, "end_pos": 42, "type": "DATASET", "confidence": 0.8485723495483398}]}], "tableCaptions": [{"text": " Table 1. Statistics on evaluation data.", "labels": [], "entities": []}, {"text": " Table 2. Performance of Strait Times -Zao Bao.", "labels": [], "entities": []}, {"text": " Table 3. Performance of Strait Times -Berita Harian.", "labels": [], "entities": [{"text": "Performance of Strait Times -Berita Harian", "start_pos": 10, "end_pos": 52, "type": "DATASET", "confidence": 0.5604040580136436}]}]}