{"title": [{"text": "Flexible Answer Typing with Discriminative Preference Ranking", "labels": [], "entities": [{"text": "Flexible Answer Typing", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.7546537717183431}]}], "abstractContent": [{"text": "An important part of question answering is ensuring a candidate answer is plausible as a response.", "labels": [], "entities": [{"text": "question answering", "start_pos": 21, "end_pos": 39, "type": "TASK", "confidence": 0.9041566252708435}]}, {"text": "We present a flexible approach based on discriminative preference ranking to determine which of a set of candidate answers are appropriate.", "labels": [], "entities": []}, {"text": "Dis-criminative methods provide superior performance while at the same time allow the flexibility of adding new and diverse features.", "labels": [], "entities": []}, {"text": "Experimental results on a set of fo-cused What ...? and Which ...?", "labels": [], "entities": []}, {"text": "questions show that our learned preference ranking methods perform better than alternative solutions to the task of answer typing.", "labels": [], "entities": [{"text": "answer typing", "start_pos": 116, "end_pos": 129, "type": "TASK", "confidence": 0.7714542746543884}]}, {"text": "A gain of almost 0.2 in MRR for both the first appropriate and first correct answers is observed along with an increase in precision over the entire range of recall.", "labels": [], "entities": [{"text": "MRR", "start_pos": 24, "end_pos": 27, "type": "METRIC", "confidence": 0.9685757756233215}, {"text": "precision", "start_pos": 123, "end_pos": 132, "type": "METRIC", "confidence": 0.9995439648628235}, {"text": "recall", "start_pos": 158, "end_pos": 164, "type": "METRIC", "confidence": 0.9943740367889404}]}], "introductionContent": [{"text": "Question answering (QA) systems have received a great deal of attention because they provide both a natural means of querying via questions and because they return short, concise answers.", "labels": [], "entities": [{"text": "Question answering (QA)", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.912997841835022}]}, {"text": "These two advantages simplify the task of finding information relevant to a topic of interest.", "labels": [], "entities": []}, {"text": "Questions convey more than simply a natural language query; an implicit expectation of answer type is provided along with the question words.", "labels": [], "entities": []}, {"text": "The discovery and exploitation of this implicit expected type is called answer typing.", "labels": [], "entities": [{"text": "answer typing", "start_pos": 72, "end_pos": 85, "type": "TASK", "confidence": 0.8772945702075958}]}, {"text": "We introduce an answer typing method that is sufficiently flexible to use a wide variety of features while at the same time providing a high level of performance.", "labels": [], "entities": [{"text": "answer typing", "start_pos": 16, "end_pos": 29, "type": "TASK", "confidence": 0.8346335589885712}]}, {"text": "Our answer typing method avoids the use of pre-determined classes that are often lacking for unanticipated answer types.", "labels": [], "entities": [{"text": "answer typing", "start_pos": 4, "end_pos": 17, "type": "TASK", "confidence": 0.8777015507221222}]}, {"text": "Because answer typing is only part of the QA task, a flexible answer typing model ensures that answer typing can be easily and usefully incorporated into a complete QA system.", "labels": [], "entities": [{"text": "answer typing", "start_pos": 8, "end_pos": 21, "type": "TASK", "confidence": 0.918538361787796}]}, {"text": "A discriminative preference ranking model with a preference for appropriate answers is trained and applied to unseen questions.", "labels": [], "entities": []}, {"text": "In terms of Mean Reciprocal Rank (MRR), we observe improvements over existing systems of around 0.2 both in terms of the correct answer and in terms of appropriate responses.", "labels": [], "entities": [{"text": "Mean Reciprocal Rank (MRR)", "start_pos": 12, "end_pos": 38, "type": "METRIC", "confidence": 0.964646448691686}]}, {"text": "This increase in MRR brings the performance of our model to near the level of a full QA system on a subset of questions, despite the fact that we rely on answer typing features alone.", "labels": [], "entities": [{"text": "MRR", "start_pos": 17, "end_pos": 20, "type": "TASK", "confidence": 0.640856146812439}]}, {"text": "The amount of information given about the expected answer can vary by question.", "labels": [], "entities": []}, {"text": "If the question contains a question focus, which we define to be the head noun following the wh-word such as city in \"What city hosted the 1988 Winter Olympics?\", some of the typing information is explicitly stated.", "labels": [], "entities": []}, {"text": "In this instance, the answer is required to be a city.", "labels": [], "entities": []}, {"text": "However, there is often additional information available about the type.", "labels": [], "entities": []}, {"text": "In our example, the answer must plausibly host a Winter Olympic Games.", "labels": [], "entities": []}, {"text": "The focus, along with the additional information, give strong clues about what are appropriate as responses.", "labels": [], "entities": []}, {"text": "We define an appropriate candidate answer as one that a user, who does not necessarily know the correct answer, would identify as a plausible answer to a given question.", "labels": [], "entities": []}, {"text": "For most questions, there exist plausible responses that are not correct answers to the question.", "labels": [], "entities": []}, {"text": "For our above question, the city of Vancouver is plausible even though it is not correct.", "labels": [], "entities": []}, {"text": "For the purposes of this paper, we assume correct answers area subset of appropriate candidates.", "labels": [], "entities": []}, {"text": "Because answer typing is only intended to be a component of a full QA system, we rely on other components to help establish the true correctness of a candidate answer.", "labels": [], "entities": [{"text": "answer typing", "start_pos": 8, "end_pos": 21, "type": "TASK", "confidence": 0.8624469041824341}]}, {"text": "The remainder of the paper is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 presents the application of discriminative preference rank learning to answer typing.", "labels": [], "entities": [{"text": "answer typing", "start_pos": 81, "end_pos": 94, "type": "TASK", "confidence": 0.8560488224029541}]}, {"text": "Section 3 introduces the models we use for learning appropriate answer preferences.", "labels": [], "entities": []}, {"text": "Sections 4 and 5 discuss our experiments and their results, respectively.", "labels": [], "entities": []}, {"text": "Section 6 presents prior work on answer typing and the use of discriminative methods in QA.", "labels": [], "entities": [{"text": "answer typing", "start_pos": 33, "end_pos": 46, "type": "TASK", "confidence": 0.9016654193401337}]}, {"text": "Finally, concluding remarks and ideas for future work are presented in Section 7.", "labels": [], "entities": []}], "datasetContent": [{"text": "To compare with the prior approach of, we use a set of what and which questions with question focus (questions with a noun phrase following the wh-word).", "labels": [], "entities": []}, {"text": "These area subset of the more general what, which, and who questions dealt with by.", "labels": [], "entities": []}, {"text": "Although our model can accommodate a wide range of what, which, when, and who questions, the focused what and which questions are an easily identifiable subclass that are rarely definitional or otherwise complex in terms of the desired answer.", "labels": [], "entities": []}, {"text": "We take the set of focused what and which questions from) comprising a total of 385 questions and performed 9-fold cross-validation, with one dedicated development partition (the tenth partition).", "labels": [], "entities": []}, {"text": "The development partition was used to tune the regularization parameter of the SVM used for testing.", "labels": [], "entities": []}, {"text": "Candidates are obtained by submitting the question as-is to the Google search engine and chunking the top 20 snippets returned, resulting in an average of 140 candidates per question.", "labels": [], "entities": []}, {"text": "Google snippets create a better confusion set than simply random words for appropriate and inappropriate candidates; many of the terms found in Google snippets are related in someway to the question.", "labels": [], "entities": []}, {"text": "To ensure a correct answer is present (where possible), we append the list of correct answers to the list of candidates.", "labels": [], "entities": []}, {"text": "As a measure of performance, we adopt Mean Reciprocal Rank (MRR) for both correct and appropriate answers, as well as precision-recall for appropriate answers.", "labels": [], "entities": [{"text": "Mean Reciprocal Rank (MRR)", "start_pos": 38, "end_pos": 64, "type": "METRIC", "confidence": 0.9733546872933706}, {"text": "precision-recall", "start_pos": 118, "end_pos": 134, "type": "METRIC", "confidence": 0.9988877177238464}]}, {"text": "MRR is useful as a measure of overall QA system performance), but is based only on the top corrector appropriate answer encountered in a ranked list.", "labels": [], "entities": [{"text": "MRR", "start_pos": 0, "end_pos": 3, "type": "METRIC", "confidence": 0.578644335269928}]}, {"text": "For this reason, we also show the precision-recall curve to better understand how our models perform.", "labels": [], "entities": [{"text": "precision-recall curve", "start_pos": 34, "end_pos": 56, "type": "METRIC", "confidence": 0.9848393797874451}]}, {"text": "We compare our models with three alternative approaches, the simplest of which is random.", "labels": [], "entities": []}, {"text": "For random, the candidate answers are randomly shuffled and performance is averaged over a number of runs (100).", "labels": [], "entities": []}, {"text": "The snippet frequency approach orders candidates based on their frequency of occurrence in the Google snippets, and is simply the S(t) feature of our discriminative models in isolation.", "labels": [], "entities": []}, {"text": "We remove terms comprised solely of question words from all approaches to prevent question words (which tend to be very frequent in the snippets) from being selected as answers.", "labels": [], "entities": []}, {"text": "The last of our alternative systems is an implementation of the work of in which the output probabilities of their model are used to rank candidates.", "labels": [], "entities": []}, {"text": "show the MRR results and precision-recall curve of our correctness model against the alternative approaches.", "labels": [], "entities": [{"text": "MRR", "start_pos": 9, "end_pos": 12, "type": "METRIC", "confidence": 0.9034441113471985}, {"text": "precision-recall", "start_pos": 25, "end_pos": 41, "type": "METRIC", "confidence": 0.9987382292747498}]}, {"text": "In comparison to these alternative systems, we show two versions of our correctness model.", "labels": [], "entities": [{"text": "correctness", "start_pos": 72, "end_pos": 83, "type": "METRIC", "confidence": 0.9297701120376587}]}, {"text": "The first uses a linear kernel and is able to outperform the alternative approaches.", "labels": [], "entities": []}, {"text": "The second uses a radial basis function (RBF) kernel and exhibits performance superior to that of the linear kernel.", "labels": [], "entities": []}, {"text": "This suggests a degree of non-linearity present in the data that cannot be captured by the linear kernel alone.", "labels": [], "entities": []}, {"text": "Both the training and running times of the RBF kernel are considerably larger than that of the linear kernel.", "labels": [], "entities": [{"text": "RBF kernel", "start_pos": 43, "end_pos": 53, "type": "DATASET", "confidence": 0.7189938426017761}]}, {"text": "The accuracy gain of the RBF kernel must therefore be weighed against the increased time required to use the model.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.9993165731430054}, {"text": "RBF kernel", "start_pos": 25, "end_pos": 35, "type": "DATASET", "confidence": 0.7550163567066193}]}, {"text": "give the MRR results and precision-recall curves for our additional models in comparison with that of the correctness model.", "labels": [], "entities": [{"text": "MRR", "start_pos": 9, "end_pos": 12, "type": "METRIC", "confidence": 0.8950565457344055}, {"text": "precision-recall", "start_pos": 25, "end_pos": 41, "type": "METRIC", "confidence": 0.9978402853012085}]}, {"text": "Although losses in MRR and precision are observed for both the appropriate and combined model using the RBF kernel, the linear kernel versions of these models show slight performance gains.", "labels": [], "entities": [{"text": "MRR", "start_pos": 19, "end_pos": 22, "type": "METRIC", "confidence": 0.7012755274772644}, {"text": "precision", "start_pos": 27, "end_pos": 36, "type": "METRIC", "confidence": 0.999575674533844}, {"text": "RBF kernel", "start_pos": 104, "end_pos": 114, "type": "DATASET", "confidence": 0.801079124212265}]}, {"text": "These results show that our discriminative preference ranking approach creates a better model of both correctness and appropriateness via weighting of contexts, preference rank learning, and with the incorporation of additional related features.", "labels": [], "entities": []}, {"text": "The last feature, snippet frequency, is not particularly strong on its own, but can be easily incorporated into our discriminative model.", "labels": [], "entities": []}, {"text": "The ability to add a wide variety of potentially helpful features is one of the strengths of discriminative techniques in general.", "labels": [], "entities": []}, {"text": "By moving away from simply correct answers in the correctness model and incorporating labeled appropriate examples in various ways, we are able to further improve upon the performance of our approach.", "labels": [], "entities": []}, {"text": "Training on appropriateness labels instead of correct answers results in a loss in MRR for the first correct answer, but again in MRR for the first appropriate candidate.", "labels": [], "entities": [{"text": "MRR", "start_pos": 83, "end_pos": 86, "type": "METRIC", "confidence": 0.992874026298523}, {"text": "MRR", "start_pos": 130, "end_pos": 133, "type": "METRIC", "confidence": 0.8816012740135193}]}, {"text": "Unfortunately, this does not carryover to the entire range of precision over recall.", "labels": [], "entities": [{"text": "precision", "start_pos": 62, "end_pos": 71, "type": "METRIC", "confidence": 0.9994551539421082}, {"text": "recall", "start_pos": 77, "end_pos": 83, "type": "METRIC", "confidence": 0.9881367683410645}]}, {"text": "For the linear kernel, our three ad- show remarkable consistency across the full range of recall, despite the fact that candidates exist for which feature values cannot easily be obtained.", "labels": [], "entities": [{"text": "consistency", "start_pos": 53, "end_pos": 64, "type": "METRIC", "confidence": 0.9761731028556824}, {"text": "recall", "start_pos": 90, "end_pos": 96, "type": "METRIC", "confidence": 0.9956673383712769}]}, {"text": "Due to tagging and chunking errors, ill-formed candidates may exist that are judged appropriate by the annotators.", "labels": [], "entities": [{"text": "chunking", "start_pos": 19, "end_pos": 27, "type": "TASK", "confidence": 0.9103130102157593}]}, {"text": "For example, \"explorer Hernando Soto\" is a candidate marked appropriate by both annotators to the question \"What Spanish explorer discovered the Mississippi River?\"", "labels": [], "entities": []}, {"text": "However, our context database does not include the phrase \"explorer Hernando Soto\" meaning that only a few features will have non-zero values.", "labels": [], "entities": []}, {"text": "Despite these occasional problems, our models are able to rank most correct and appropriate candidates high in a ranked list.", "labels": [], "entities": []}], "tableCaptions": []}