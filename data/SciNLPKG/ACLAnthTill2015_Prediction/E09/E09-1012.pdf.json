{"title": [{"text": "Incremental Parsing Models for Dialog Task Structure", "labels": [], "entities": [{"text": "Incremental Parsing", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.831353098154068}, {"text": "Dialog Task", "start_pos": 31, "end_pos": 42, "type": "TASK", "confidence": 0.850252091884613}]}], "abstractContent": [{"text": "In this paper, we present an integrated model of the two central tasks of dialog management: interpreting user actions and generating system actions.", "labels": [], "entities": [{"text": "dialog management", "start_pos": 74, "end_pos": 91, "type": "TASK", "confidence": 0.8723388016223907}, {"text": "interpreting user actions", "start_pos": 93, "end_pos": 118, "type": "TASK", "confidence": 0.8764752944310507}]}, {"text": "We model the interpretation task as a classi\uf0decation problem and the generation task as a prediction problem.", "labels": [], "entities": []}, {"text": "These two tasks are inter-leaved in an incremental parsing-based dialog model.", "labels": [], "entities": []}, {"text": "We compare three alternative parsing methods for this dialog model using a corpus of human-human spoken dialog from a catalog ordering domain that has been annotated for dialog acts and task/subtask information.", "labels": [], "entities": []}, {"text": "We contrast the amount of context provided by each method and its impact on performance.", "labels": [], "entities": []}], "introductionContent": [{"text": "Corpora of spoken dialog are now widely available, and frequently come with annotations for tasks/games, dialog acts, named entities and elements of syntactic structure.", "labels": [], "entities": []}, {"text": "These types of information provide rich clues for building dialog models (.", "labels": [], "entities": []}, {"text": "Dialog models can be built of\uf0dfine (for dialog mining and summarization), or online (for dialog management).", "labels": [], "entities": [{"text": "dialog mining", "start_pos": 39, "end_pos": 52, "type": "TASK", "confidence": 0.8321910202503204}, {"text": "dialog management", "start_pos": 88, "end_pos": 105, "type": "TASK", "confidence": 0.7932080924510956}]}, {"text": "A dialog manager is the component of a dialog system that is responsible for interpreting user actions in the dialog context, and for generating system actions.", "labels": [], "entities": []}, {"text": "Needless to say, a dialog manager operates incrementally as the dialog progresses.", "labels": [], "entities": []}, {"text": "In typical commercial dialog systems, the interpretation and generation processes operate independently of each other, with only a small amount of shared context.", "labels": [], "entities": [{"text": "interpretation and generation", "start_pos": 42, "end_pos": 71, "type": "TASK", "confidence": 0.7620563904444376}]}, {"text": "By contrast, in this paper we describe a dialog model that (1) tightly integrates interpretation and generation, (2) makes explicit the type and amount of shared context, (3) includes the task structure of the dialog in the context, (4) can be trained from dialog data, and (5) runs incrementally, parsing the dialog as it occurs and interleaving generation and interpretation.", "labels": [], "entities": []}, {"text": "At the core of our model is a parser that incrementally builds the dialog task structure as the dialog progresses.", "labels": [], "entities": []}, {"text": "In this paper, we experiment with three different incremental tree-based parsing methods.", "labels": [], "entities": []}, {"text": "We compare these methods using a corpus of human-human spoken dialogs in a catalog ordering domain that has been annotated for dialog acts and task/subtask information.", "labels": [], "entities": []}, {"text": "We show that all these methods outperform a baseline method for recovering the dialog structure.", "labels": [], "entities": []}, {"text": "The rest of this paper is structured as follows: In Section 2, we review related work.", "labels": [], "entities": []}, {"text": "In Section 3, we present our view of the structure of taskoriented human-human dialogs.", "labels": [], "entities": []}, {"text": "In Section 4, we present the parsing approaches included in our experiments.", "labels": [], "entities": [{"text": "parsing", "start_pos": 29, "end_pos": 36, "type": "TASK", "confidence": 0.968549907207489}]}, {"text": "In Section 5, we describe our data and experiments.", "labels": [], "entities": []}, {"text": "Finally, in Section 6, we present conclusions and describe our current and future work.", "labels": [], "entities": []}, {"text": "not incremental; it used global features such as the number of turn changes.", "labels": [], "entities": []}, {"text": "Also, it focused strictly in interpretation of input utterances; it could not predict actions by either dialog partner.", "labels": [], "entities": [{"text": "interpretation of input utterances", "start_pos": 29, "end_pos": 63, "type": "TASK", "confidence": 0.8425229489803314}]}, {"text": "In contrast to other work on discourse parsing, we wish to use the parsing process directly for dialog management (rather than for information extraction or summarization).", "labels": [], "entities": [{"text": "discourse parsing", "start_pos": 29, "end_pos": 46, "type": "TASK", "confidence": 0.7210387289524078}, {"text": "dialog management", "start_pos": 96, "end_pos": 113, "type": "TASK", "confidence": 0.8218441903591156}, {"text": "information extraction", "start_pos": 131, "end_pos": 153, "type": "TASK", "confidence": 0.7765963077545166}, {"text": "summarization", "start_pos": 157, "end_pos": 170, "type": "TASK", "confidence": 0.6650736331939697}]}, {"text": "This in\uf0dfuences our approach to dialog modeling in two ways.", "labels": [], "entities": [{"text": "dialog modeling", "start_pos": 31, "end_pos": 46, "type": "TASK", "confidence": 0.9067508280277252}]}, {"text": "First, the subtask tree we build represents the functional task structure of the dialog (rather than the rhetorical structure of the dialog).", "labels": [], "entities": []}, {"text": "Second, our dialog parser must be entirely incremental.", "labels": [], "entities": [{"text": "dialog parser", "start_pos": 12, "end_pos": 25, "type": "TASK", "confidence": 0.7996046245098114}]}, {"text": "Plan-Based Dialog Models Plan-based approaches to dialog modeling, like ours, operate directly on the dialog's task structure.", "labels": [], "entities": [{"text": "dialog modeling", "start_pos": 50, "end_pos": 65, "type": "TASK", "confidence": 0.87205970287323}]}, {"text": "The process of task-oriented dialog is treated as a special case of AI-style plan recognition.", "labels": [], "entities": [{"text": "AI-style plan recognition", "start_pos": 68, "end_pos": 93, "type": "TASK", "confidence": 0.6709025601545969}]}, {"text": "Plan-based dialog models are used for both interpretation of user utterances and prediction of agent actions.", "labels": [], "entities": [{"text": "interpretation of user utterances", "start_pos": 43, "end_pos": 76, "type": "TASK", "confidence": 0.8505112081766129}, {"text": "prediction of agent actions", "start_pos": 81, "end_pos": 108, "type": "TASK", "confidence": 0.8235400915145874}]}, {"text": "In addition to the hand-crafted models listed above, researchers have built stochastic plan recognition models for interaction, including ones based on Hidden Markov Models) and on probabilistic context-free grammars In this area, the work most closely related to ours is that of, who build an incremental bottom-up parser to parse plans.", "labels": [], "entities": []}, {"text": "Their parser, however, was not probabilistic or targeted at dialog processing.", "labels": [], "entities": [{"text": "dialog processing", "start_pos": 60, "end_pos": 77, "type": "TASK", "confidence": 0.8389277458190918}]}], "datasetContent": [{"text": "To evaluate our parse-based dialog model, we used 817 two-party dialogs from the CHILD corpus of telephone-based dialogs in a catalog-purchasing domain.", "labels": [], "entities": [{"text": "CHILD corpus of telephone-based dialogs", "start_pos": 81, "end_pos": 120, "type": "DATASET", "confidence": 0.9254268169403076}]}, {"text": "Each dialog was transcribed by hand; all numbers (telephone, credit card, etc.) were removed for privacy reasons.", "labels": [], "entities": []}, {"text": "The average dialog in this data set had 60 turns.", "labels": [], "entities": []}, {"text": "The dialogs were automatically segmented into utterances and automatically annotated with part-ofspeech tag and supertag information and named entities.", "labels": [], "entities": []}, {"text": "They were annotated by hand for dialog acts and tasks/subtasks.", "labels": [], "entities": []}, {"text": "The dialog act and task/subtask labels are given in.", "labels": [], "entities": []}, {"text": "We evaluate dialog act classi\uf0decation and prediction by comparing the automatically assigned dialog act tags to the reference dialog act tags.", "labels": [], "entities": []}, {"text": "For these tasks we report accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 26, "end_pos": 34, "type": "METRIC", "confidence": 0.995979905128479}]}, {"text": "We evaluate subtask classi\uf0decation and prediction by comparing the subtask trees output by the different parsing methods to the reference subtask tree.", "labels": [], "entities": []}, {"text": "We use the labeled crossing bracket metric (typically used in the syntactic parsing literature (), which computes recall, precision and crossing brackets for the constituents (subtrees) in a hypothesized parse tree given the reference parse tree.", "labels": [], "entities": [{"text": "syntactic parsing", "start_pos": 66, "end_pos": 83, "type": "TASK", "confidence": 0.764968991279602}, {"text": "recall", "start_pos": 114, "end_pos": 120, "type": "METRIC", "confidence": 0.9975318908691406}, {"text": "precision", "start_pos": 122, "end_pos": 131, "type": "METRIC", "confidence": 0.9972254037857056}]}, {"text": "We report F-measure, which is a combination of recall and precision.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9974629878997803}, {"text": "recall", "start_pos": 47, "end_pos": 53, "type": "METRIC", "confidence": 0.9994644522666931}, {"text": "precision", "start_pos": 58, "end_pos": 67, "type": "METRIC", "confidence": 0.9983407258987427}]}, {"text": "For each task, performance is reported for 1, 3, 5, and 10-best dynamic decoding as well as oracle (Or) and for 0, 1 and 3 utterances of context.", "labels": [], "entities": []}, {"text": "Figure 5: Performance of parse-based methods for subtask tree building shows the performance of the different methods for determining the subtask tree of the dialog.", "labels": [], "entities": [{"text": "subtask tree building", "start_pos": 49, "end_pos": 70, "type": "TASK", "confidence": 0.6552935441335043}]}, {"text": "Wider beam widths do not lead to improved performance for any method.", "labels": [], "entities": []}, {"text": "One utterance of context is best for shift-reduce and start-join; three is best for the connection path method.", "labels": [], "entities": []}, {"text": "The shiftreduce method performs the best.", "labels": [], "entities": []}, {"text": "With 1 utterance of context, its 1-best f-score is 47.86, as compared with 34.91 for start-complete, 25.13 for the connection path method, and 21.32 for the chunkbased baseline.", "labels": [], "entities": []}, {"text": "These performance differences are statistically signi\uf0decant at p < .001.", "labels": [], "entities": []}, {"text": "However, the best performance for the shift-reduce method is still signi\uf0decantly worse than oracle.", "labels": [], "entities": []}], "tableCaptions": []}