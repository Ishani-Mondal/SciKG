{"title": [{"text": "Language ID in the Context of Harvesting Language Data off the Web", "labels": [], "entities": [{"text": "Language ID", "start_pos": 0, "end_pos": 11, "type": "TASK", "confidence": 0.6756986826658249}]}], "abstractContent": [{"text": "As the arm of NLP technologies extends beyond a small core of languages, techniques for working with instances of language data across hundreds to thousands of languages may require revisiting and re-calibrating the tried and true methods that are used.", "labels": [], "entities": []}, {"text": "Of the NLP techniques that has been treated as \"solved\" is language identification (language ID) of written text.", "labels": [], "entities": [{"text": "language identification (language ID) of written text", "start_pos": 59, "end_pos": 112, "type": "TASK", "confidence": 0.8515355520778232}]}, {"text": "However, we argue that language ID is far from solved when one considers input spanning not dozens of languages, but rather hundreds to thousands, a number that one approaches when harvesting language data found on the Web.", "labels": [], "entities": [{"text": "language ID", "start_pos": 23, "end_pos": 34, "type": "TASK", "confidence": 0.7232394963502884}]}, {"text": "We formulate language ID as a coreference resolution problem and apply it to a Web harvesting task fora specific linguistic datatype and achieve a much higher accuracy than long accepted language ID approaches.", "labels": [], "entities": [{"text": "formulate language ID", "start_pos": 3, "end_pos": 24, "type": "TASK", "confidence": 0.6970041692256927}, {"text": "coreference resolution", "start_pos": 30, "end_pos": 52, "type": "TASK", "confidence": 0.8961296081542969}, {"text": "Web harvesting task", "start_pos": 79, "end_pos": 98, "type": "TASK", "confidence": 0.7088314890861511}, {"text": "accuracy", "start_pos": 159, "end_pos": 167, "type": "METRIC", "confidence": 0.9979153275489807}]}], "introductionContent": [{"text": "A large number of the world's languages have been documented by linguists; it is now increasingly common to post current research and data to the Web, often in the form of language snippets embedded in scholarly papers.", "labels": [], "entities": []}, {"text": "A particularly common format for linguistic data posted to the Web is \"interlinearized text\", a format used to present language data and analysis relevant to a particular argument or investigation.", "labels": [], "entities": []}, {"text": "Since interlinear examples consist of orthographically or phonetically encoded language data aligned with an English translation, the \"corpus\" of interlinear examples found on the Web, when taken together, constitute a significant multilingual, parallel corpus covering hundreds to thousands of the world's languages.", "labels": [], "entities": []}, {"text": "Previous work has discussed methods for harvesting interlinear text off the Web (), enriching it via structural projections, and even making it available to typological analyses (  and search . One challenge with harvesting interlinear data off the Web is language identification of the harvested data.", "labels": [], "entities": [{"text": "language identification", "start_pos": 256, "end_pos": 279, "type": "TASK", "confidence": 0.7473743259906769}]}, {"text": "There have been extensive studies on language identification (language ID) of written text, and a review of previous research on this topic can be found in ().", "labels": [], "entities": [{"text": "language identification (language ID) of written text", "start_pos": 37, "end_pos": 90, "type": "TASK", "confidence": 0.8618503146701388}]}, {"text": "In general, a language ID method requires a collection of text for training, something on the order of a thousand or more characters.", "labels": [], "entities": [{"text": "language ID", "start_pos": 14, "end_pos": 25, "type": "TASK", "confidence": 0.6922613978385925}]}, {"text": "These methods work well for languages with rich language resources; for instance, Cavnar and Trenkle's N-gram-based algorithm achieved an accuracy as high as 99.8% when tested on newsgroup articles across eight languages.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 138, "end_pos": 146, "type": "METRIC", "confidence": 0.9991808533668518}]}, {"text": "However, the performance is much worse (with accuracy dropping to as low as 1.66%) if there is very little language data for training and the number of languages being evaluated reaches a few hundred.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 45, "end_pos": 53, "type": "METRIC", "confidence": 0.9992108345031738}]}, {"text": "In this paper, we treat the language ID of harvested linguistic data as a coreference resolution problem.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 74, "end_pos": 96, "type": "TASK", "confidence": 0.887434720993042}]}, {"text": "Our method, although narrowly focused on this very specific datatype, makes it possible to collect small snippets of language data across hundreds of languages and use the data for linguistic search and bootstrapping NLP tools.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we compare the two approaches to the language ID task: the CL approach and the CoRef approach.", "labels": [], "entities": [{"text": "language ID task", "start_pos": 54, "end_pos": 70, "type": "TASK", "confidence": 0.8069781064987183}]}, {"text": "In our experiments, we run 10-fold cross validation (90% for training and 10% for testing) on the data set in and report the average of language ID accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 148, "end_pos": 156, "type": "METRIC", "confidence": 0.8443782925605774}]}, {"text": "The two approaches have different upper bounds.", "labels": [], "entities": []}, {"text": "The upper bound of the CL approach is the percentage of IGTs in the test data that belong to a seen language.", "labels": [], "entities": []}, {"text": "The upper bound of the CoRef approach is the percentage of IGTs in the test data that belong to a language whose language name appears in the same document.", "labels": [], "entities": []}, {"text": "For the data set in, the upper bounds are 90.33% and 97.31% respectively.", "labels": [], "entities": []}, {"text": "When the training data is much smaller, the upper bound of the CL approach would decrease tremendously, whereas the upper bound of the CoRef approach remains the same.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: Various language name tables", "labels": [], "entities": []}, {"text": " Table 7: The performance of the CL approach (# of classes: about 600, # of training instances=13,723)", "labels": [], "entities": []}, {"text": " Table 8: The performance of the CoRef approach (# of classes=2, # of training instances=511,039)", "labels": [], "entities": []}, {"text": " Table 9: The performance of the CoRef approach with less training data (the upper bound of the Coref  approach remains 97.31%)", "labels": [], "entities": []}]}