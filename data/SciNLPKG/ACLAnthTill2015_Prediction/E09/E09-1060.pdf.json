{"title": [{"text": "Correcting a PoS-tagged corpus using three complementary methods", "labels": [], "entities": [{"text": "PoS-tagged corpus", "start_pos": 13, "end_pos": 30, "type": "DATASET", "confidence": 0.7678818702697754}]}], "abstractContent": [{"text": "The quality of the part-of-speech (PoS) annotation in a corpus is crucial for the development of PoS taggers.", "labels": [], "entities": [{"text": "PoS taggers", "start_pos": 97, "end_pos": 108, "type": "TASK", "confidence": 0.7345260679721832}]}, {"text": "In this paper , we experiment with three complementary methods for automatically detecting errors in the PoS annotation for the Ice-landic Frequency Dictionary corpus.", "labels": [], "entities": [{"text": "Ice-landic Frequency Dictionary corpus", "start_pos": 128, "end_pos": 166, "type": "DATASET", "confidence": 0.7796527594327927}]}, {"text": "The first two methods are language independent and we argue that the third method can be adapted to other morphologically complex languages.", "labels": [], "entities": []}, {"text": "Once possible errors have been detected, we examine each error candidate and hand-correct the corresponding PoS tag if necessary.", "labels": [], "entities": []}, {"text": "Overall , based on the three methods, we hand-correct the PoS tagging of 1,334 tokens (0.23% of the tokens) in the corpus.", "labels": [], "entities": [{"text": "PoS tagging", "start_pos": 58, "end_pos": 69, "type": "TASK", "confidence": 0.7427214086055756}]}, {"text": "Furthermore , we re-evaluate existing state-of-the-art PoS taggers on Icelandic text using the corrected corpus.", "labels": [], "entities": [{"text": "PoS taggers", "start_pos": 55, "end_pos": 66, "type": "TASK", "confidence": 0.7051148414611816}]}], "introductionContent": [{"text": "Part-of-speech (PoS) tagged corpora are valuable resources for developing PoS taggers, i.e. programs which automatically tag each word in running text with morphosyntactic information.", "labels": [], "entities": [{"text": "Part-of-speech (PoS) tagged corpora", "start_pos": 0, "end_pos": 35, "type": "TASK", "confidence": 0.6180593321720759}, {"text": "PoS taggers", "start_pos": 74, "end_pos": 85, "type": "TASK", "confidence": 0.7278202772140503}]}, {"text": "Corpora in various languages, such as the English Penn Treebank corpus (, the Swedish Stockholm-Ume\u00e5 corpus (, and the Icelandic Frequency Dictionary (IFD) corpus, have been used to train (in the case of data-driven methods) and develop (in the case of linguistic rule-based methods) different taggers, and to evaluate their accuracy, e.g.).", "labels": [], "entities": [{"text": "Penn Treebank corpus", "start_pos": 50, "end_pos": 70, "type": "DATASET", "confidence": 0.896553615729014}, {"text": "Icelandic Frequency Dictionary (IFD) corpus", "start_pos": 119, "end_pos": 162, "type": "DATASET", "confidence": 0.7079009200845446}, {"text": "accuracy", "start_pos": 325, "end_pos": 333, "type": "METRIC", "confidence": 0.998060405254364}]}, {"text": "Consequently, the quality of the PoS annotation in a corpus (the gold standard annotation) is crucial.", "labels": [], "entities": []}, {"text": "Many corpora are annotated semiautomatically.", "labels": [], "entities": []}, {"text": "First, a PoS tagger is run on the corpus text, and, then, the text is hand-corrected by humans.", "labels": [], "entities": [{"text": "PoS tagger", "start_pos": 9, "end_pos": 19, "type": "TASK", "confidence": 0.6786604225635529}]}, {"text": "Despite human post-editing, (large) tagged corpora are almost certain to contain errors, because humans make mistakes.", "labels": [], "entities": []}, {"text": "Thus, it is important to apply known methods and/or develop new methods for automatically detecting tagging errors in corpora.", "labels": [], "entities": [{"text": "automatically detecting tagging errors", "start_pos": 76, "end_pos": 114, "type": "TASK", "confidence": 0.6476878002285957}]}, {"text": "Once an error has been detected it can be corrected by humans or an automatic method.", "labels": [], "entities": []}, {"text": "In this paper, we experiment with three different methods of PoS error detection using the IFD corpus.", "labels": [], "entities": [{"text": "PoS error detection", "start_pos": 61, "end_pos": 80, "type": "TASK", "confidence": 0.8443653583526611}, {"text": "IFD corpus", "start_pos": 91, "end_pos": 101, "type": "DATASET", "confidence": 0.9297173917293549}]}, {"text": "First, we use the variation n-gram method proposed by.", "labels": [], "entities": []}, {"text": "Secondly, we run five different taggers on the corpus and examine those cases where all the taggers agree on a tag, but, at the same time, disagree with the gold standard annotation.", "labels": [], "entities": []}, {"text": "Lastly, we use IceParser ( to generate shallow parses of sentences in the corpus and then develop various patterns, based on feature agreement, for finding candidates for annotation errors.", "labels": [], "entities": []}, {"text": "Once error candidates have been detected by each method, we examine the candidates manually and correct the errors.", "labels": [], "entities": []}, {"text": "Overall, based on these methods, we hand-correct the PoS tagging of 1,334 tokens or 0.23% of the tokens in the IFD corpus.", "labels": [], "entities": [{"text": "PoS tagging", "start_pos": 53, "end_pos": 64, "type": "TASK", "confidence": 0.7187069356441498}, {"text": "IFD corpus", "start_pos": 111, "end_pos": 121, "type": "DATASET", "confidence": 0.8617079257965088}]}, {"text": "We are not aware of previous corpus error detection/correction work applying the last two methods above.", "labels": [], "entities": [{"text": "corpus error detection/correction", "start_pos": 29, "end_pos": 62, "type": "TASK", "confidence": 0.7296078324317932}]}, {"text": "Note that the first two methods are completely language-independent, and the third method can be tailored to the language at hand, assuming the existence of a shallow parser.", "labels": [], "entities": []}, {"text": "Our results show that the three methods are complementary.", "labels": [], "entities": []}, {"text": "A large ratio of the tokens that get hand-corrected based on each method is uniquely corrected by that method . After hand-correcting the corpus, we retrain and re-evaluate two of the best three performing taggers on Icelandic text, which results in up to 0.18% higher accuracy than reported previously.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 269, "end_pos": 277, "type": "METRIC", "confidence": 0.9986578226089478}]}, {"text": "The remainder of this paper is organised as follows.", "labels": [], "entities": []}, {"text": "In Section 2 we describe related work, with regard to error detection and PoS tagging of Icelandic text.", "labels": [], "entities": [{"text": "error detection", "start_pos": 54, "end_pos": 69, "type": "TASK", "confidence": 0.7330021262168884}, {"text": "PoS tagging of Icelandic text", "start_pos": 74, "end_pos": 103, "type": "TASK", "confidence": 0.8170135498046875}]}, {"text": "Our three methods of error detection are described in Section 3 and results are provided in Section 4.", "labels": [], "entities": [{"text": "error detection", "start_pos": 21, "end_pos": 36, "type": "TASK", "confidence": 0.7461193799972534}]}, {"text": "We re-evaluate taggers in Section 5 and we conclude with a summary in Section 6.", "labels": [], "entities": []}], "datasetContent": [{"text": "Earlier work on evaluation of tagging accuracy for Icelandic text has used the original IFD corpus (without any error correction attempts).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 38, "end_pos": 46, "type": "METRIC", "confidence": 0.7742497324943542}, {"text": "IFD corpus", "start_pos": 88, "end_pos": 98, "type": "DATASET", "confidence": 0.910633772611618}]}, {"text": "Since we were able to correct several errors in the corpus, we were confident that the tagging accuracy published hitherto had been underestimated.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 95, "end_pos": 103, "type": "METRIC", "confidence": 0.9547730088233948}]}, {"text": "To verify this, we used IceTagger and TnT, two of the three best performing taggers on Icelandic text.", "labels": [], "entities": [{"text": "IceTagger", "start_pos": 24, "end_pos": 33, "type": "DATASET", "confidence": 0.9438206553459167}, {"text": "Icelandic text", "start_pos": 87, "end_pos": 101, "type": "DATASET", "confidence": 0.8104133009910583}]}, {"text": "Additionally, we used a changed version of TnT, which utilises functionality from IceMorphy, the morphological analyser of IceTagger, and a changed version of IceTagger which uses a hidden Markov Model (HMM) to disambiguate words which cannot be further disambiguated by applying rules.", "labels": [], "entities": []}, {"text": "In tables 2 and 3 below, Ice denotes IceTagger, Ice* denotes IceTagger+HMM, and TnT* denotes TnT+IceMorphy.", "labels": [], "entities": [{"text": "IceTagger", "start_pos": 37, "end_pos": 46, "type": "DATASET", "confidence": 0.9664843678474426}, {"text": "IceTagger+HMM", "start_pos": 61, "end_pos": 74, "type": "DATASET", "confidence": 0.8213635087013245}, {"text": "IceMorphy", "start_pos": 97, "end_pos": 106, "type": "DATASET", "confidence": 0.8921350240707397}]}, {"text": "We ran 10-fold cross-validation, using the exact same data-splits as used in), both before error correction (i.e. on the original corpus) and after the error correction (i.e. on the corrected corpus).", "labels": [], "entities": []}, {"text": "Note that in these two steps we did not retrain the TnT tagger, i.e. it still used the language model derived from the original uncorrected corpus.", "labels": [], "entities": []}, {"text": "Using the original corpus, the average tagging accuracy results (using the first nine splits), for unknown words, known words, and all words, are shown in 6 . The average unknown word ratio is 6.8%.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 47, "end_pos": 55, "type": "METRIC", "confidence": 0.9417237043380737}]}, {"text": "Then we repeated the evaluation, now using the corrected corpus.", "labels": [], "entities": []}, {"text": "The results are shown in Table 3.", "labels": [], "entities": []}, {"text": "By comparing the tagging accuracy for all words in tables 2 and 3, it can be seen that the accuracy had be underestimated by 0.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 25, "end_pos": 33, "type": "METRIC", "confidence": 0.8148691654205322}, {"text": "accuracy", "start_pos": 91, "end_pos": 99, "type": "METRIC", "confidence": 0.9995099306106567}]}, {"text": "Since the TnT tagger is a data-driven tagger, it is interesting to see whether the corrected corpus changes the language model (to the better) of the tagger.", "labels": [], "entities": [{"text": "TnT tagger", "start_pos": 10, "end_pos": 20, "type": "TASK", "confidence": 0.6864656805992126}]}, {"text": "In other words, does retraining using the corrected corpus produce better results than using the language model generated from the original corpus?", "labels": [], "entities": []}, {"text": "The answer is yes, as can be seen by comparing the accuracy figures for TnT and TnT* in tables 3 and 4.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 51, "end_pos": 59, "type": "METRIC", "confidence": 0.9997496008872986}]}, {"text": "The tagging accuracy for all words increases by 0.10 and 0.07 percentage points for TnT and TnT*, respectively.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 12, "end_pos": 20, "type": "METRIC", "confidence": 0.9792807698249817}]}, {"text": "The re-evaluation of the above taggers, with or without retraining, clearly indicates that the quality of the PoS annotation in the IFD corpus has significant effect on the accuracy of the taggers.", "labels": [], "entities": [{"text": "IFD corpus", "start_pos": 132, "end_pos": 142, "type": "DATASET", "confidence": 0.885811060667038}, {"text": "accuracy", "start_pos": 173, "end_pos": 181, "type": "METRIC", "confidence": 0.9989393353462219}]}], "tableCaptions": [{"text": " Table 1: Results for the three error detection methods", "labels": [], "entities": [{"text": "error detection", "start_pos": 32, "end_pos": 47, "type": "TASK", "confidence": 0.6896432340145111}]}, {"text": " Table 2: Average tagging accuracy (%) using the  original IFD corpus", "labels": [], "entities": [{"text": "tagging", "start_pos": 18, "end_pos": 25, "type": "TASK", "confidence": 0.8790284395217896}, {"text": "accuracy", "start_pos": 26, "end_pos": 34, "type": "METRIC", "confidence": 0.9760634899139404}, {"text": "IFD", "start_pos": 59, "end_pos": 62, "type": "DATASET", "confidence": 0.6845843195915222}]}, {"text": " Table 3: Average tagging accuracy (%) using the  corrected IFD corpus", "labels": [], "entities": [{"text": "tagging", "start_pos": 18, "end_pos": 25, "type": "TASK", "confidence": 0.8559836149215698}, {"text": "accuracy", "start_pos": 26, "end_pos": 34, "type": "METRIC", "confidence": 0.9733328819274902}, {"text": "IFD", "start_pos": 60, "end_pos": 63, "type": "METRIC", "confidence": 0.322282612323761}]}]}