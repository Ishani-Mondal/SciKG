{"title": [{"text": "Structural Correspondence Learning for Parse Disambiguation", "labels": [], "entities": [{"text": "Parse Disambiguation", "start_pos": 39, "end_pos": 59, "type": "TASK", "confidence": 0.7939209043979645}]}], "abstractContent": [{"text": "The paper presents an application of Structural Correspondence Learning (SCL) (Blitzer et al., 2006) for domain adaptation of a stochastic attribute-value grammar (SAVG).", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 105, "end_pos": 122, "type": "TASK", "confidence": 0.7510577440261841}]}, {"text": "So far, SCL has been applied successfully in NLP for Part-of-Speech tagging and Sentiment Analysis (Blitzer et al., 2006; Blitzer et al., 2007).", "labels": [], "entities": [{"text": "Part-of-Speech tagging", "start_pos": 53, "end_pos": 75, "type": "TASK", "confidence": 0.7352606952190399}, {"text": "Sentiment Analysis", "start_pos": 80, "end_pos": 98, "type": "TASK", "confidence": 0.8822528123855591}]}, {"text": "An attempt was made in the CoNLL 2007 shared task to apply SCL to non-projective dependency parsing (Shimizu and Nakagawa, 2007), however, without any clear conclusions.", "labels": [], "entities": [{"text": "CoNLL 2007 shared task", "start_pos": 27, "end_pos": 49, "type": "DATASET", "confidence": 0.8871434181928635}, {"text": "SCL", "start_pos": 59, "end_pos": 62, "type": "TASK", "confidence": 0.9574437737464905}, {"text": "non-projective dependency parsing", "start_pos": 66, "end_pos": 99, "type": "TASK", "confidence": 0.6666585604349772}]}, {"text": "We report on our exploration of applying SCL to adapt a syntactic disambiguation model and show promising initial results on Wikipedia domains.", "labels": [], "entities": []}], "introductionContent": [{"text": "Many current, effective natural language processing systems are based on supervised Machine Learning techniques.", "labels": [], "entities": []}, {"text": "The parameters of such systems are estimated to best reflect the characteristics of the training data, at the cost of portability: a system will be successful only as long as the training material resembles the input that the model gets.", "labels": [], "entities": []}, {"text": "Therefore, whenever we have access to a large amount of labeled data from some \"source\" (out-of-domain), but we would like a model that performs well on some new \"target\" domain, we face the problem of domain adaptation.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 202, "end_pos": 219, "type": "TASK", "confidence": 0.7216242849826813}]}, {"text": "The need for domain adaptation arises in many NLP tasks: Part-of-Speech tagging, Sentiment Analysis, Semantic Role Labeling or Statistical Parsing, to name but a few.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 13, "end_pos": 30, "type": "TASK", "confidence": 0.7543847560882568}, {"text": "Part-of-Speech tagging", "start_pos": 57, "end_pos": 79, "type": "TASK", "confidence": 0.7705631256103516}, {"text": "Sentiment Analysis", "start_pos": 81, "end_pos": 99, "type": "TASK", "confidence": 0.8912930190563202}, {"text": "Semantic Role Labeling", "start_pos": 101, "end_pos": 123, "type": "TASK", "confidence": 0.7321178118387858}, {"text": "Statistical Parsing", "start_pos": 127, "end_pos": 146, "type": "TASK", "confidence": 0.8687721192836761}]}, {"text": "For example, the performance of a statistical parsing system drops in an appalling way when a model trained on the Wall Street Journal is applied to the more varied Brown corpus).", "labels": [], "entities": [{"text": "statistical parsing", "start_pos": 34, "end_pos": 53, "type": "TASK", "confidence": 0.6866212487220764}, {"text": "Wall Street Journal", "start_pos": 115, "end_pos": 134, "type": "DATASET", "confidence": 0.9426060716311137}, {"text": "Brown corpus", "start_pos": 165, "end_pos": 177, "type": "DATASET", "confidence": 0.8102776110172272}]}, {"text": "The problem itself has started to get attention only recently.", "labels": [], "entities": []}, {"text": "We distinguish two main approaches to domain adaptation that have been addressed in the literature: supervised and semi-supervised.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 38, "end_pos": 55, "type": "TASK", "confidence": 0.7631163597106934}]}, {"text": "In supervised domain adaptation, besides the labeled source data, we have access to a comparably small, but labeled amount of target data.", "labels": [], "entities": [{"text": "supervised domain adaptation", "start_pos": 3, "end_pos": 31, "type": "TASK", "confidence": 0.6779106855392456}]}, {"text": "In contrast, semi-supervised domain adaptation) is the scenario in which, in addition to the labeled source data, we only have unlabeled and no labeled target domain data.", "labels": [], "entities": [{"text": "semi-supervised domain adaptation", "start_pos": 13, "end_pos": 46, "type": "TASK", "confidence": 0.6991984645525614}]}, {"text": "Semi-supervised adaptation is a much more realistic situation, while at the same time also considerably more difficult.", "labels": [], "entities": []}, {"text": "Studies on the supervised task have shown that straightforward baselines (e.g. models based on source only, target only, or the union of the data) achieve a relatively high performance level and are \"surprisingly difficult to beat\").", "labels": [], "entities": []}, {"text": "Thus, one conclusion from that line of work is that as soon as there is a reasonable (often even small) amount of labeled target data, it is often more fruitful to either just use that, or to apply simple adaptation techniques).", "labels": [], "entities": []}], "datasetContent": [{"text": "The base (source domain) disambiguation model is trained on the Alpino Treebank) (newspaper text), which consists of approximately 7,000 sentences and 145,000 tokens.", "labels": [], "entities": [{"text": "Alpino Treebank) (newspaper text)", "start_pos": 64, "end_pos": 97, "type": "DATASET", "confidence": 0.9550473604883466}]}, {"text": "For parameter estimation of the disambiguation model, in all reported experiments we use the TADM 2 toolkit (toolkit for advanced discriminative training), with a Gaussian prior (\u03c3 2 =1000) and the (default) limited memory variable metric estimation technique).", "labels": [], "entities": [{"text": "TADM 2", "start_pos": 93, "end_pos": 99, "type": "METRIC", "confidence": 0.9227414727210999}]}, {"text": "For training the binary pivot predictors, we use the MegaM 3 Optimization Package with the socalled \"bernoulli implicit\" input format.", "labels": [], "entities": [{"text": "MegaM 3 Optimization Package", "start_pos": 53, "end_pos": 81, "type": "DATASET", "confidence": 0.8701238185167313}]}, {"text": "To compute the SVD, we use SVDLIBC.", "labels": [], "entities": [{"text": "SVDLIBC", "start_pos": 27, "end_pos": 34, "type": "DATASET", "confidence": 0.9501495361328125}]}, {"text": "The output of the parser is dependency structure.", "labels": [], "entities": []}, {"text": "A standard evaluation metric is to measure the amount of generated dependencies that are identical to the stored dependencies (correct labeled dependencies), expressed as f-score.", "labels": [], "entities": []}, {"text": "An alternative measure is concept accuracy (CA), which is similar to f-score, but allows possible discrepancy between the number of returned dependencies If we want to compare the performance of disambiguation models, we can employ the \u03c6 measure.", "labels": [], "entities": [{"text": "concept accuracy (CA)", "start_pos": 26, "end_pos": 47, "type": "METRIC", "confidence": 0.8201638340950013}]}, {"text": "Intuitively, it tells us how much of the disambiguation problem has been solved.", "labels": [], "entities": [{"text": "disambiguation problem", "start_pos": 41, "end_pos": 63, "type": "TASK", "confidence": 0.9242329299449921}]}, {"text": "In more detail, the \u03c6 measure incorporates an upper and lower bound: base measures the accuracy of a model that simply selects the first parse for each sentence; oracle represents the accuracy achieved by a model that always selects the best parse from the set of potential parses (within the coverage of the parser).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 87, "end_pos": 95, "type": "METRIC", "confidence": 0.9977561831474304}, {"text": "accuracy", "start_pos": 184, "end_pos": 192, "type": "METRIC", "confidence": 0.9946773052215576}]}, {"text": "In addition, we also report relative error reduction (rel.er), which is the relative difference in \u03c6 scores for two models.", "labels": [], "entities": [{"text": "relative error reduction (rel.er)", "start_pos": 28, "end_pos": 61, "type": "METRIC", "confidence": 0.8684117694695791}]}, {"text": "As target domain, we consider the Dutch part of Wikipedia as data collection, described in the following.", "labels": [], "entities": [{"text": "Dutch part of Wikipedia", "start_pos": 34, "end_pos": 57, "type": "DATASET", "confidence": 0.7693458497524261}]}], "tableCaptions": [{"text": " Table 1: Size of test datasets.", "labels": [], "entities": []}, {"text": " Table 2: Size of related unlabeled data; relation- ship indicates whether all related pages are used  or some are filtered out (see section 5.2).", "labels": [], "entities": [{"text": "Size of related unlabeled", "start_pos": 10, "end_pos": 35, "type": "TASK", "confidence": 0.8107969164848328}, {"text": "relation- ship", "start_pos": 42, "end_pos": 56, "type": "METRIC", "confidence": 0.7697205543518066}]}, {"text": " Table 4: Results of our instantiation of SCL (with  varying h parameter and no feature normaliza- tion).", "labels": [], "entities": []}, {"text": " Table 5: First result on increasing unlabeled data.", "labels": [], "entities": []}]}