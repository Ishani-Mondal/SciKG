{"title": [{"text": "Improving Mid-Range Reordering using Templates of Factors", "labels": [], "entities": [{"text": "Improving Mid-Range Reordering", "start_pos": 0, "end_pos": 30, "type": "TASK", "confidence": 0.7229124108950297}]}], "abstractContent": [{"text": "We extend the factored translation model (Koehn and Hoang, 2007) to allow translations of longer phrases composed of factors such as POS and morphological tags to act as templates for the selection and reordering of surface phrase translation.", "labels": [], "entities": [{"text": "surface phrase translation", "start_pos": 216, "end_pos": 242, "type": "TASK", "confidence": 0.5991186102231344}]}, {"text": "We also reintroduce the use of alignment information within the decoder, which forms an integral part of decoding in the Alignment Template System (Och, 2002), into phrase-based decoding.", "labels": [], "entities": [{"text": "Alignment Template System (Och, 2002)", "start_pos": 121, "end_pos": 158, "type": "TASK", "confidence": 0.49424514919519424}]}, {"text": "Results show an increase in translation performance of up to 1.0% BLEU for out-of-domain French-English translation.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 66, "end_pos": 70, "type": "METRIC", "confidence": 0.9996153116226196}]}, {"text": "We also show how this method compares and relates to lexicalized reordering.", "labels": [], "entities": []}], "introductionContent": [{"text": "One of the major issues in statistical machine translation is reordering due to systematic wordordering differences between languages.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 27, "end_pos": 58, "type": "TASK", "confidence": 0.6510224839051565}]}, {"text": "Often reordering is best explained by linguistic categories, such as part-of-speech tags.", "labels": [], "entities": []}, {"text": "In fact, prior work has examined the use of part-of-speech tags in pre-reordering schemes,.", "labels": [], "entities": []}, {"text": "Re-ordering can also be viewed as composing of a number of related problems which can be explained or solved by a variety of linguistic phenomena.", "labels": [], "entities": []}, {"text": "Firstly, differences between phrase ordering account for much of the long-range reordering.", "labels": [], "entities": [{"text": "phrase ordering", "start_pos": 29, "end_pos": 44, "type": "TASK", "confidence": 0.7108432203531265}]}, {"text": "Syntax-based and hierarchical models such as) attempts to address this problem.", "labels": [], "entities": []}, {"text": "Shorter range re-ordering, such as intraphrasal word re-ordering, can often be predicted from the underlying property of the words and its context, the most obvious property being POS tags.", "labels": [], "entities": []}, {"text": "In this paper, we tackle the issue of shorterrange re-ordering in phrase-based decoding by presenting an extension of the factored translation which directly models the translation of nonsurface factors such as POS tags.", "labels": [], "entities": []}, {"text": "We shall call this extension the factored template model.", "labels": [], "entities": []}, {"text": "We use the fact that factors such as POS-tags are less sparse than surface words to obtain longer phrase translations.", "labels": [], "entities": [{"text": "phrase translations", "start_pos": 98, "end_pos": 117, "type": "TASK", "confidence": 0.737361490726471}]}, {"text": "These translations are used to inform the re-ordering of surface phrases.", "labels": [], "entities": []}, {"text": "Despite the ability of phrase-based systems to use multi-word phrases, the majority of phrases used during decoding are one word phrases, which we will show in later sections.", "labels": [], "entities": []}, {"text": "Using word translations negates the implicit capability of phrases to re-order words.", "labels": [], "entities": []}, {"text": "We show that the proposed extension increases the number of multi-word phrases used during decoding, capturing the implicit ordering with the phrase translation, leading to overall better sentence translation.", "labels": [], "entities": [{"text": "sentence translation", "start_pos": 188, "end_pos": 208, "type": "TASK", "confidence": 0.7216338068246841}]}, {"text": "In our tests, we obtained 1.0% increase in absolute for French-English translation, and 0.8% increase for German-English translation, trained on News Commentary corpora 1 . We will begin by recounting the phrase-based and factored model in Section 2 and describe the language model and lexicalized re-ordering model and the advantages and disadvantages of using these models to influence re-ordering.", "labels": [], "entities": [{"text": "absolute", "start_pos": 43, "end_pos": 51, "type": "METRIC", "confidence": 0.9960152506828308}, {"text": "German-English translation", "start_pos": 106, "end_pos": 132, "type": "TASK", "confidence": 0.6941248029470444}, {"text": "News Commentary corpora 1", "start_pos": 145, "end_pos": 170, "type": "DATASET", "confidence": 0.9127599000930786}]}, {"text": "The proposed model is described in Section 4.", "labels": [], "entities": []}], "datasetContent": [{"text": "We performed our experiments on the news commentary corpus 2 which contains 60,000 parallel sentences for German-English and 43,000 sentences for French-English.", "labels": [], "entities": [{"text": "news commentary corpus 2", "start_pos": 36, "end_pos": 60, "type": "DATASET", "confidence": 0.7271841987967491}]}, {"text": "Tuning was done on a 2000 sentence subset of the Europarl corpus () and tested on a 2000 sentence Europarl subset for out-of-domain, and a 1064 news commentary sentences for in-domain.", "labels": [], "entities": [{"text": "Europarl corpus", "start_pos": 49, "end_pos": 64, "type": "DATASET", "confidence": 0.9821811616420746}, {"text": "Europarl subset", "start_pos": 98, "end_pos": 113, "type": "DATASET", "confidence": 0.8977468311786652}]}, {"text": "The training corpus is aligned using Giza++ (.", "labels": [], "entities": []}, {"text": "To create POS tag translation models, the surface forms on both source and target language training data are replaced with POS tags before phrases are extracted.", "labels": [], "entities": [{"text": "POS tag translation", "start_pos": 10, "end_pos": 29, "type": "TASK", "confidence": 0.7935504118601481}]}, {"text": "The taggers used were the Brill Tagger for English, the Treetagger for French, and the LoPar Tagger (Schmidt and Schulte im) for German.", "labels": [], "entities": [{"text": "Brill", "start_pos": 26, "end_pos": 31, "type": "METRIC", "confidence": 0.7324078679084778}, {"text": "LoPar Tagger", "start_pos": 87, "end_pos": 99, "type": "DATASET", "confidence": 0.8860192596912384}]}, {"text": "The training script supplied with the Moses toolkit (  was used, extended to enable alignment information of each phrase pair.", "labels": [], "entities": []}, {"text": "The vanilla Moses MERT tuning script was used throughout.", "labels": [], "entities": [{"text": "MERT tuning script", "start_pos": 18, "end_pos": 36, "type": "DATASET", "confidence": 0.7063260873158773}]}, {"text": "Results are also presented for models trained on the larger Europarl corpora 3 .", "labels": [], "entities": [{"text": "Europarl corpora 3", "start_pos": 60, "end_pos": 78, "type": "DATASET", "confidence": 0.9868720372517904}]}], "tableCaptions": [{"text": " Table 1: German-English results, in %BLEU", "labels": [], "entities": [{"text": "BLEU", "start_pos": 38, "end_pos": 42, "type": "METRIC", "confidence": 0.9967855215072632}]}, {"text": " Table 3: Extending the models with lexicalized re- ordering (LR)", "labels": [], "entities": [{"text": "re- ordering (LR", "start_pos": 48, "end_pos": 64, "type": "METRIC", "confidence": 0.6994966745376587}]}, {"text": " Table 4: French-English results, trained on Eu- roparl corpus", "labels": [], "entities": []}]}