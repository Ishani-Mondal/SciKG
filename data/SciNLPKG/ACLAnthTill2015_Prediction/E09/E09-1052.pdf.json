{"title": [{"text": "A Logic of Semantic Representations for Shallow Parsing", "labels": [], "entities": []}], "abstractContent": [{"text": "One way to construct semantic representations in a robust manner is to enhance shallow language processors with semantic components.", "labels": [], "entities": []}, {"text": "Here, we provide a model theory fora semantic formalism that is designed for this, namely Robust Minimal Recursion Semantics (RMRS).", "labels": [], "entities": [{"text": "Robust Minimal Recursion Semantics (RMRS", "start_pos": 90, "end_pos": 130, "type": "TASK", "confidence": 0.6885174264510473}]}, {"text": "We show that RMRS supports a notion of entailment that allows it to form the basis for comparing the semantic output of different parses of varying depth.", "labels": [], "entities": []}], "introductionContent": [{"text": "Representing semantics as a logical form that supports automated inference and model construction is vital for deeper language engineering tasks, such as dialogue systems.", "labels": [], "entities": [{"text": "model construction", "start_pos": 79, "end_pos": 97, "type": "TASK", "confidence": 0.7931172847747803}]}, {"text": "Logical forms can be obtained from hand-crafted deep grammars) but this lacks robustness: not all words and constructions are covered and by design ill-formed phrases fail to parse.", "labels": [], "entities": []}, {"text": "There has thus been a trend recently towards robust wide-coverage semantic construction (e.g.,).", "labels": [], "entities": [{"text": "wide-coverage semantic construction", "start_pos": 52, "end_pos": 87, "type": "TASK", "confidence": 0.6403999229272207}]}, {"text": "But there are certain semantic phenomena that these robust approaches don't capture reliably, including quantifier scope, optional arguments, and long-distance dependencies (for instance,  report that the parser used by yields 63% accuracy on object extraction; e.g., the man that I met.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 231, "end_pos": 239, "type": "METRIC", "confidence": 0.9963148236274719}, {"text": "object extraction", "start_pos": 243, "end_pos": 260, "type": "TASK", "confidence": 0.7198715060949326}]}, {"text": "). Forcing a robust parser to make a decision about these phenomena can therefore be error-prone.", "labels": [], "entities": []}, {"text": "Depending on the application, it maybe preferable to give the parser the option to leave a semantic decision open when it's not sufficiently informed-i.e., to compute a partial semantic representation and to complete it later, using information extraneous to the parser.", "labels": [], "entities": []}, {"text": "In this paper, we focus on an approach to semantic representation that supports this strategy: Robust Minimal Recursion Semantics).", "labels": [], "entities": []}, {"text": "RMRS is designed to support underspecification of lexical information, scope, and predicate-argument structure.", "labels": [], "entities": []}, {"text": "It is an emerging standard for representing partial semantics, and has been applied in several implemented systems.", "labels": [], "entities": []}, {"text": "For instance, and use it to specify semantic components to shallow parsers ranging in depth from POS taggers to chunk parsers and intermediate parsers such as RASP ().", "labels": [], "entities": []}, {"text": "MRS analyses () derived from deep grammars, such as the English Resource Grammar (ERG,) are special cases of RMRS.", "labels": [], "entities": [{"text": "MRS analyses", "start_pos": 0, "end_pos": 12, "type": "TASK", "confidence": 0.8870948255062103}, {"text": "English Resource Grammar (ERG", "start_pos": 56, "end_pos": 85, "type": "DATASET", "confidence": 0.7871388554573059}]}, {"text": "But RMRS, unlike MRS and related formalisms like dominance constraints (, is able to express semantic information in the absence of full predicate argument structure and lexical subcategorisation.", "labels": [], "entities": []}, {"text": "The key contribution we make is to cast RMRS, for the first time, as a logic with a well-defined model theory.", "labels": [], "entities": [{"text": "RMRS", "start_pos": 40, "end_pos": 44, "type": "TASK", "confidence": 0.6718961000442505}]}, {"text": "Previously, no such model theory existed, and so RMRS had to be used in a somewhat ad-hoc manner that left open exactly what any given RMRS representation actually means.", "labels": [], "entities": []}, {"text": "This has hindered practical progress, both in terms of understanding the relationship of RMRS to other frameworks such as MRS and predicate logic and in terms of the development of efficient algorithms.", "labels": [], "entities": [{"text": "MRS", "start_pos": 122, "end_pos": 125, "type": "TASK", "confidence": 0.8876654505729675}]}, {"text": "As one application of our formalisation, we use entailment to propose a novel way of characterising consistency of RMRS analyses across different parsers.", "labels": [], "entities": []}, {"text": "Section 2 introduces RMRS informally and illustrates why it is necessary and useful for representing semantic information across deep and shallow language processors.", "labels": [], "entities": [{"text": "RMRS", "start_pos": 21, "end_pos": 25, "type": "TASK", "confidence": 0.8660414218902588}]}, {"text": "Section 3 defines the syntax and model-theory of RMRS.", "labels": [], "entities": []}, {"text": "We finish in Section 4 by pointing out some avenues for future research.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": []}