{"title": [{"text": "Automatic Single-Document Key Fact Extraction from Newswire Articles", "labels": [], "entities": [{"text": "Automatic Single-Document Key Fact Extraction", "start_pos": 0, "end_pos": 45, "type": "TASK", "confidence": 0.5708481848239899}]}], "abstractContent": [{"text": "This paper addresses the problem of extracting the most important facts from a news article.", "labels": [], "entities": [{"text": "extracting the most important facts from a news article", "start_pos": 36, "end_pos": 91, "type": "TASK", "confidence": 0.6957497861650255}]}, {"text": "Our approach uses syntactic , semantic, and general statistical features to identify the most important sentences in a document.", "labels": [], "entities": []}, {"text": "The importance of the individual features is estimated using generalized iterative scaling methods trained on an annotated newswire corpus.", "labels": [], "entities": []}, {"text": "The performance of our approach is evaluated against 300 unseen news articles and shows that use of these features results in statistically significant improvements over a provenly robust baseline, as measured using metrics such as precision, recall and ROUGE.", "labels": [], "entities": [{"text": "precision", "start_pos": 232, "end_pos": 241, "type": "METRIC", "confidence": 0.9993009567260742}, {"text": "recall", "start_pos": 243, "end_pos": 249, "type": "METRIC", "confidence": 0.9989259839057922}, {"text": "ROUGE", "start_pos": 254, "end_pos": 259, "type": "METRIC", "confidence": 0.9917550086975098}]}], "introductionContent": [{"text": "The increasing amount of information that is available to both professional users (such as journalists, financial analysts and intelligence analysts) and lay users has called for methods condensing information, in order to make the most important content standout.", "labels": [], "entities": []}, {"text": "Several methods have been proposed over the last two decades, among which keyword extraction and summarization are the most prominent ones.", "labels": [], "entities": [{"text": "keyword extraction", "start_pos": 74, "end_pos": 92, "type": "TASK", "confidence": 0.824284166097641}, {"text": "summarization", "start_pos": 97, "end_pos": 110, "type": "TASK", "confidence": 0.9741471409797668}]}, {"text": "Keyword extraction aims to identify the most relevant words or phrases in a document, e.g.,), while summarization aims to provide a short (commonly 100 words), coherent full-text summary of the document, e.g.,).", "labels": [], "entities": [{"text": "Keyword extraction", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.8761806488037109}, {"text": "summarization", "start_pos": 100, "end_pos": 113, "type": "TASK", "confidence": 0.9728997349739075}]}, {"text": "Key fact extraction falls in between keyword extraction and summarization.", "labels": [], "entities": [{"text": "Key fact extraction", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.7533372243245443}, {"text": "keyword extraction", "start_pos": 37, "end_pos": 55, "type": "TASK", "confidence": 0.8116230666637421}, {"text": "summarization", "start_pos": 60, "end_pos": 73, "type": "TASK", "confidence": 0.98134845495224}]}, {"text": "Here, the challenge is to identify the most relevant facts in a document, but not necessarily in a coherent full-text form as is done in summarization.", "labels": [], "entities": [{"text": "summarization", "start_pos": 137, "end_pos": 150, "type": "TASK", "confidence": 0.9573174118995667}]}, {"text": "Evidence of the usefulness of key fact extraction is CNN's website which since 2006 has most of its news articles preceded by a list of story highlights, see.", "labels": [], "entities": [{"text": "key fact extraction", "start_pos": 30, "end_pos": 49, "type": "TASK", "confidence": 0.6295307675997416}, {"text": "CNN's website", "start_pos": 53, "end_pos": 66, "type": "DATASET", "confidence": 0.8357391158739725}]}, {"text": "The advantage of the news highlights as opposed to full-text summaries is that they are much 'easier on the eye' and are better suited for quick skimming.", "labels": [], "entities": []}, {"text": "So far, only CNN.com offers this service and we are interested in finding out to what extent it can be automated and thus applied to any newswire source.", "labels": [], "entities": [{"text": "CNN.com", "start_pos": 13, "end_pos": 20, "type": "DATASET", "confidence": 0.9304215312004089}]}, {"text": "Although these highlights could be easily generated by the respective journalists, many news organization shy away from introducing an additional manual stage into the workflow, where pushback times of minutes are considered unacceptable in an extremely competitive news business which competes in terms of seconds rather than minutes.", "labels": [], "entities": []}, {"text": "Automating highlight generation can help eliminate those delays.", "labels": [], "entities": [{"text": "highlight generation", "start_pos": 11, "end_pos": 31, "type": "TASK", "confidence": 0.7576156556606293}]}, {"text": "Journalistic training emphasizes that news articles should contain the most important information in the beginning, while less important information, such as background or additional details, appears further down in the article.", "labels": [], "entities": []}, {"text": "This is also the main reason why most summarization systems applied to news articles do not outperform a simple baseline that just uses the first 100 words of an article ().", "labels": [], "entities": [{"text": "summarization", "start_pos": 38, "end_pos": 51, "type": "TASK", "confidence": 0.9719047546386719}]}, {"text": "On the other hand, most of CNN's story highlights are not taken from the beginning of the articles.", "labels": [], "entities": [{"text": "CNN's story highlights", "start_pos": 27, "end_pos": 49, "type": "DATASET", "confidence": 0.8614376187324524}]}, {"text": "In fact, more than 50% of the highlights stem from sentences that are not among the first 100 words of the articles.", "labels": [], "entities": []}, {"text": "This makes identifying story highlights a much more challenging task than single-document summarization in the news domain.", "labels": [], "entities": [{"text": "identifying story highlights", "start_pos": 11, "end_pos": 39, "type": "TASK", "confidence": 0.876599391301473}, {"text": "single-document summarization", "start_pos": 74, "end_pos": 103, "type": "TASK", "confidence": 0.6372122764587402}]}, {"text": "In order to automate story highlight identification we automatically extract syntactic, semantic, and purely statistical features from the document.", "labels": [], "entities": [{"text": "story highlight identification", "start_pos": 21, "end_pos": 51, "type": "TASK", "confidence": 0.669035921494166}]}, {"text": "The weights of the features are estimated using machine learning techniques, trained on an annotated corpus.", "labels": [], "entities": []}, {"text": "In this paper, we focus on identifying the relevant sentences in the news article from which the highlights were generated.", "labels": [], "entities": []}, {"text": "The system we have implemented is named AURUM: AUtomatic Retrieval of Unique information with Machine learning.", "labels": [], "entities": [{"text": "AURUM", "start_pos": 40, "end_pos": 45, "type": "METRIC", "confidence": 0.9141777157783508}, {"text": "AUtomatic Retrieval of Unique information", "start_pos": 47, "end_pos": 88, "type": "TASK", "confidence": 0.870521080493927}]}, {"text": "A full system would also contain a sentence compression step), but since both steps are largely independent of each other, existing sentence compression or simplification techniques can be applied to the sentences identified by our approach.", "labels": [], "entities": [{"text": "sentence compression", "start_pos": 35, "end_pos": 55, "type": "TASK", "confidence": 0.7122598886489868}, {"text": "sentence compression", "start_pos": 132, "end_pos": 152, "type": "TASK", "confidence": 0.7533471584320068}]}, {"text": "The remainder of this paper is organized as follows: The next section describes the relevant work done to date in keyfact extraction and automatic summarization.", "labels": [], "entities": [{"text": "keyfact extraction", "start_pos": 114, "end_pos": 132, "type": "TASK", "confidence": 0.8037595748901367}, {"text": "summarization", "start_pos": 147, "end_pos": 160, "type": "TASK", "confidence": 0.8149356842041016}]}, {"text": "Section 3 lays out our features and explains how they were learned and estimated.", "labels": [], "entities": []}, {"text": "Section 4 presents the experimental setup and our results, and Section 5 concludes with a short discussion.", "labels": [], "entities": []}], "datasetContent": [{"text": "The CNN corpus was divided into a training set and a development and test set.", "labels": [], "entities": [{"text": "CNN corpus", "start_pos": 4, "end_pos": 14, "type": "DATASET", "confidence": 0.8965693116188049}]}, {"text": "As we had only 300 manually annotated news articles and we wanted to maximize the number of documents usable for parameter estimation, we applied crossfolding, which is commonly used for situations with limited data.", "labels": [], "entities": [{"text": "parameter estimation", "start_pos": 113, "end_pos": 133, "type": "TASK", "confidence": 0.7053764462471008}]}, {"text": "The dev/test set was randomly partitioned into five folds.", "labels": [], "entities": []}, {"text": "Four of the five folds were used as development data (i.e. for parameter estimation with YASMET), while the remaining fold was used for testing.", "labels": [], "entities": [{"text": "YASMET", "start_pos": 89, "end_pos": 95, "type": "METRIC", "confidence": 0.6070253252983093}]}, {"text": "The procedure was repeated five times, each time with four folds used for development and a separate one for testing.", "labels": [], "entities": []}, {"text": "Cross-folding is safe to use as long as there are no dependencies between the folds, which is safe to assume here.", "labels": [], "entities": []}, {"text": "Some statistics on our training and development/test data can be found in: Characteristics of the evaluation corpus.", "labels": [], "entities": []}, {"text": "Most summarization evaluation campaigns, such as NIST's Document Understanding Conferences (DUC), impose a maximum length on summaries (e.g., 75 characters for the headline generation task or 100 words for the summarization task).", "labels": [], "entities": [{"text": "summarization evaluation", "start_pos": 5, "end_pos": 29, "type": "TASK", "confidence": 0.9331568777561188}, {"text": "headline generation task", "start_pos": 164, "end_pos": 188, "type": "TASK", "confidence": 0.8056521614392599}, {"text": "summarization task", "start_pos": 210, "end_pos": 228, "type": "TASK", "confidence": 0.8851270377635956}]}, {"text": "When identifying sentences from which story highlights are generated, the situation is slightly different, as the number of story highlights is not fixed.", "labels": [], "entities": []}, {"text": "On the other hand, most stories have between three and four highlights, and on average between four and five sentences per story from which the highlights were generated.", "labels": [], "entities": []}, {"text": "This variation led to us to carryout two sets of experiments: In the first experiment (fixed), the number of highlight sources is fixed and our system always returns exactly four highlight sources.", "labels": [], "entities": []}, {"text": "In the second experiment (thresh), our system can return between three and six highlight sources, depending on whether a sentence score passes a given threshold.", "labels": [], "entities": []}, {"text": "The threshold \u03b8 was used to allocate sentences s i of article a to the highlight list HL by first finding the highest-scoring sentence for that article \u03c3(s h ).", "labels": [], "entities": []}, {"text": "The threshold score was thus \u03b8 * \u03c3(s h ) and sentences were judged accordingly.", "labels": [], "entities": []}, {"text": "The algorithm used is given in. initialize HL, sh sort si in s by \u03c3(si) set sh = s0 for each sentence si in article a: include si else discard si return HL: Procedure for selecting highlight sources.", "labels": [], "entities": []}, {"text": "All scores were compared to a baseline, which simply returns the first n sentences of a news article.", "labels": [], "entities": []}, {"text": "n = 4 in the fixed experiment.", "labels": [], "entities": []}, {"text": "For the thresh experiment, the baseline always selected the same number of sentences as AURUM-thresh, but from the beginning of the article.", "labels": [], "entities": [{"text": "AURUM-thresh", "start_pos": 88, "end_pos": 100, "type": "METRIC", "confidence": 0.9354109168052673}]}, {"text": "Although this is a very simple baseline, it is worth reiterating that it is also a very competitive baseline, which most single-document summarization systems fail to beat due to the nature of news articles.", "labels": [], "entities": []}, {"text": "Since we are mainly interested in determining to what extent our system is able to correctly identify the highlight sources, we chose precision and recall as evaluation metrics.", "labels": [], "entities": [{"text": "precision", "start_pos": 134, "end_pos": 143, "type": "METRIC", "confidence": 0.9991993308067322}, {"text": "recall", "start_pos": 148, "end_pos": 154, "type": "METRIC", "confidence": 0.9986176490783691}]}, {"text": "Precision is the percentage of all returned highlight sources which are correct: where R is the set of returned highlight sources and T is the set of manually identified true sources in the test set.", "labels": [], "entities": [{"text": "Precision", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.9944307208061218}]}, {"text": "Recall is defined as the percentage of all true highlight sources that have been correctly identified by the system: Precision and recall can be combined by using the F-measure, which is the harmonic mean of the two: shows the results for both experiments (fixed and thresh) as an average over the folds.", "labels": [], "entities": [{"text": "Recall", "start_pos": 0, "end_pos": 6, "type": "METRIC", "confidence": 0.9903848171234131}, {"text": "Precision", "start_pos": 117, "end_pos": 126, "type": "METRIC", "confidence": 0.9922324419021606}, {"text": "recall", "start_pos": 131, "end_pos": 137, "type": "METRIC", "confidence": 0.9986831545829773}, {"text": "F-measure", "start_pos": 167, "end_pos": 176, "type": "METRIC", "confidence": 0.9939067959785461}]}, {"text": "To determine whether the observed differences between two approaches are statistically significant and not just caused by chance, we applied statistical significance testing.", "labels": [], "entities": []}, {"text": "As we did not want to make the assumption that the score differences are normally distributed, we used the bootstrap method, a powerful non-parametric inference test).", "labels": [], "entities": []}, {"text": "Improvements at a confidence level of more than 95% are marked with \" * \".", "labels": [], "entities": []}, {"text": "We can see that our approach consistently outperforms the baseline, and most of the improvements-in particular the F-measure scores-are statistically significant at the 0.95 level.", "labels": [], "entities": [{"text": "F-measure scores-are statistically", "start_pos": 115, "end_pos": 149, "type": "METRIC", "confidence": 0.9561830163002014}]}, {"text": "As to be expected, AURUM-fixed achieves higher precision gains, while AURUM-thresh achieves higher recall gains.", "labels": [], "entities": [{"text": "AURUM-fixed", "start_pos": 19, "end_pos": 30, "type": "METRIC", "confidence": 0.8932107090950012}, {"text": "precision", "start_pos": 47, "end_pos": 56, "type": "METRIC", "confidence": 0.9988501071929932}, {"text": "AURUM-thresh", "start_pos": 70, "end_pos": 82, "type": "METRIC", "confidence": 0.7425629496574402}, {"text": "recall", "start_pos": 99, "end_pos": 105, "type": "METRIC", "confidence": 0.9990583062171936}]}, {"text": "In addition, for 83.3 percent of the documents, our system's F-measure score is higher than or equal to that of the baseline.", "labels": [], "entities": [{"text": "F-measure score", "start_pos": 61, "end_pos": 76, "type": "METRIC", "confidence": 0.9890229105949402}]}, {"text": "shows how far down in the documents our system was able to correctly identify highlight sources.", "labels": [], "entities": []}, {"text": "Although the distribution is still heavily skewed towards extracting sentences from the beginning of the document, it is so to a lesser extent than just using positional information as a prior; see.", "labels": [], "entities": []}, {"text": "Ina third set of experiments we measured the n-gram overlap between the sentences we have identified as highlight sources and the actual story highlights in the ground truth.", "labels": [], "entities": []}, {"text": "ROUGE), a recall-oriented evaluation package for automatic summarization.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 0, "end_pos": 5, "type": "METRIC", "confidence": 0.8932828307151794}, {"text": "recall-oriented", "start_pos": 10, "end_pos": 25, "type": "METRIC", "confidence": 0.9798682332038879}, {"text": "summarization", "start_pos": 59, "end_pos": 72, "type": "TASK", "confidence": 0.8346621990203857}]}, {"text": "ROUGE operates essentially by comparing n-gram cooccurrences between a candidate summary and a number of reference summaries, and comparing that number in turn to the total number of n-grams in the reference summaries: Where n is the length of the n-gram, with lengths of 1 and 2 words most commonly used in current evaluations.", "labels": [], "entities": []}, {"text": "ROUGE has become the standard tool for evaluating automatic summaries, though it is not the optimal system for this experiment.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 0, "end_pos": 5, "type": "METRIC", "confidence": 0.9456771016120911}, {"text": "summaries", "start_pos": 60, "end_pos": 69, "type": "TASK", "confidence": 0.7582038640975952}]}, {"text": "This is due to the fact that it is geared towards a different task-as ours is not automatic summarization per se-and that ROUGE works best judging between a number of candidate and model summaries.", "labels": [], "entities": [{"text": "summarization", "start_pos": 92, "end_pos": 105, "type": "TASK", "confidence": 0.9423782229423523}, {"text": "ROUGE", "start_pos": 122, "end_pos": 127, "type": "METRIC", "confidence": 0.9611664414405823}]}, {"text": "The ROUGE scores are shown in.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 4, "end_pos": 9, "type": "METRIC", "confidence": 0.9959748387336731}]}, {"text": "Similar to the precision and recall scores, our approach consistently outperforms the baseline, with all but one difference being statistically significant.", "labels": [], "entities": [{"text": "precision", "start_pos": 15, "end_pos": 24, "type": "METRIC", "confidence": 0.9996746778488159}, {"text": "recall", "start_pos": 29, "end_pos": 35, "type": "METRIC", "confidence": 0.9974080920219421}]}, {"text": "Furthermore, in 76.2 percent of the documents, our system's ROUGE-1 score is higher than or equal to that of the baseline, and likewise for 85.2 percent of ROUGE-2 scores.", "labels": [], "entities": [{"text": "ROUGE-1 score", "start_pos": 60, "end_pos": 73, "type": "METRIC", "confidence": 0.9844048023223877}, {"text": "ROUGE-2", "start_pos": 156, "end_pos": 163, "type": "METRIC", "confidence": 0.8918592929840088}]}, {"text": "Our ROUGE scores and their improvements over the baseline are comparable to the results of, who optimized their approach towards ROUGE and gained significant improvements from using third-party data resources, both of which our approach does not require.", "labels": [], "entities": []}, {"text": "5 shows the unique sentences extracted by every system, which are the number of sentences one system extracted correctly while the other did not; this is thus an intuitive measure of how much two systems differ.", "labels": [], "entities": []}, {"text": "Essentially, a system could simply pick the first two sentences of each article and might thus achieve higher precision scores, since it is less likely to return 'wrong' sentences.", "labels": [], "entities": [{"text": "precision scores", "start_pos": 110, "end_pos": 126, "type": "METRIC", "confidence": 0.9720213711261749}]}, {"text": "However, if the scores are similar but there is a difference in the number of unique sentences extracted, this means a system has gone beyond the first 4 sentences and extracted others from deeper down inside the text.", "labels": [], "entities": []}, {"text": "To get a better understanding of the importance of the individual features we examined the weights as determined by YASMET.", "labels": [], "entities": [{"text": "YASMET", "start_pos": 116, "end_pos": 122, "type": "DATASET", "confidence": 0.7472116351127625}]}, {"text": "contains example output from the development sets, with feature selection determined implicitly by the weights the MaxEnt model assigns, where non-discriminative features receive a low weight.", "labels": [], "entities": []}, {"text": "Clearly, sentence position is of highest importance, while trigram 'trigger' phrases were quite important as well.", "labels": [], "entities": []}, {"text": "Simple bigrams continued to be a good indicator of data value, as is often the case.", "labels": [], "entities": []}, {"text": "Proper nouns proved to be a valuable pointer to new information, but mention of the news agency's name had less of an impact than originally thought.", "labels": [], "entities": []}, {"text": "Other particularly significant features included temporal adjectives, superlatives and all n-gram measures.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 4: Characteristics of the evaluation corpus.", "labels": [], "entities": []}, {"text": " Table 5: Evaluation scores for the four extraction systems.", "labels": [], "entities": []}, {"text": " Table 6: ROUGE scores for AURUM-fixed, returning 4 sentences, and AURUM-thresh, returning  between 3 and 6 sentences.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 10, "end_pos": 15, "type": "METRIC", "confidence": 0.997975766658783}, {"text": "AURUM-fixed", "start_pos": 27, "end_pos": 38, "type": "METRIC", "confidence": 0.8072951436042786}, {"text": "AURUM-thresh", "start_pos": 67, "end_pos": 79, "type": "METRIC", "confidence": 0.7940890192985535}]}, {"text": " Table 7: Unique recall scores for the systems.", "labels": [], "entities": [{"text": "recall", "start_pos": 17, "end_pos": 23, "type": "METRIC", "confidence": 0.9937830567359924}]}, {"text": " Table 8: Typical weights learned from the data.", "labels": [], "entities": []}]}