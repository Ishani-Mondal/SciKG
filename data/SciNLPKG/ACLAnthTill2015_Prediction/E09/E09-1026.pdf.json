{"title": [], "abstractContent": [{"text": "Large scale annotated corpora are prerequisite to developing high-performance semantic role labeling systems.", "labels": [], "entities": []}, {"text": "Unfortunately , such corpora are expensive to produce, limited in size, and may not be representative.", "labels": [], "entities": []}, {"text": "Our work aims to reduce the annotation effort involved in creating resources for semantic role labeling via semi-supervised learning.", "labels": [], "entities": [{"text": "semantic role labeling", "start_pos": 81, "end_pos": 103, "type": "TASK", "confidence": 0.7205314834912618}]}, {"text": "Our algorithm augments a small number of manually labeled instances with unlabeled examples whose roles are inferred automatically via annotation projection.", "labels": [], "entities": []}, {"text": "We formulate the projection task as a generalization of the linear assignment problem.", "labels": [], "entities": []}, {"text": "We seek to find a role assignment in the un-labeled data such that the argument similarity between the labeled and unlabeled instances is maximized.", "labels": [], "entities": []}, {"text": "Experimental results on semantic role labeling show that the automatic annotations produced by our method improve performance over using hand-labeled instances alone.", "labels": [], "entities": [{"text": "semantic role labeling", "start_pos": 24, "end_pos": 46, "type": "TASK", "confidence": 0.7504430810610453}]}], "introductionContent": [{"text": "Recent years have seen a growing interest in the task of automatically identifying and labeling the semantic roles conveyed by sentential constituents ().", "labels": [], "entities": [{"text": "labeling the semantic roles conveyed by sentential constituents", "start_pos": 87, "end_pos": 150, "type": "TASK", "confidence": 0.7812504693865776}]}, {"text": "This is partly due to its relevance for applications ranging from information extraction () to question answering, paraphrase identification, and the modeling of textual entailment relations ().", "labels": [], "entities": [{"text": "information extraction", "start_pos": 66, "end_pos": 88, "type": "TASK", "confidence": 0.7675075232982635}, {"text": "question answering", "start_pos": 95, "end_pos": 113, "type": "TASK", "confidence": 0.8871890902519226}, {"text": "paraphrase identification", "start_pos": 115, "end_pos": 140, "type": "TASK", "confidence": 0.9427502453327179}]}, {"text": "Resources like FrameNet ( and PropBank () have also facilitated the development of semantic role labeling methods by providing high-quality annotations for use in training.", "labels": [], "entities": [{"text": "semantic role labeling", "start_pos": 83, "end_pos": 105, "type": "TASK", "confidence": 0.6281756460666656}]}, {"text": "Semantic role labelers are commonly developed using a supervised learning paradigm 1 where a classifier learns to predict role labels based on features extracted from annotated training data.", "labels": [], "entities": [{"text": "Semantic role labelers", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.6951996684074402}]}, {"text": "Examples of the annotations provided in FrameNet are given in (1).", "labels": [], "entities": [{"text": "FrameNet", "start_pos": 40, "end_pos": 48, "type": "DATASET", "confidence": 0.8904959559440613}]}, {"text": "Here, the meaning of predicates (usually verbs, nouns, or adjectives) is conveyed by frames, schematic representations of situations.", "labels": [], "entities": []}, {"text": "Semantic roles (or frame elements) are defined for each frame and correspond to salient entities present in the situation evoked by the predicate (or frame evoking element).", "labels": [], "entities": []}, {"text": "Predicates with similar semantics instantiate the same frame and are attested with the same roles.", "labels": [], "entities": []}, {"text": "In our example, the frame Cause harm has three core semantic roles, Agent, Victim, and Body part and can be instantiated with verbs such as punch, crush, slap, and injure.", "labels": [], "entities": []}, {"text": "The frame may also be attested with non-core (peripheral) roles that are more generic and often shared across frames (see the roles Degree, and (1d)).", "labels": [], "entities": [{"text": "Degree", "start_pos": 132, "end_pos": 138, "type": "METRIC", "confidence": 0.9421233534812927}]}, {"text": "[ The English FrameNet (version 1.3) contains 502 frames covering 5,866 lexical entries.", "labels": [], "entities": [{"text": "English FrameNet", "start_pos": 6, "end_pos": 22, "type": "DATASET", "confidence": 0.8428462445735931}]}, {"text": "It also comes with a set of manually annotated example sentences, taken mostly from the British National Corpus.", "labels": [], "entities": [{"text": "British National Corpus", "start_pos": 88, "end_pos": 111, "type": "DATASET", "confidence": 0.9188395142555237}]}, {"text": "These annotations are often used as training data for semantic role labeling systems.", "labels": [], "entities": [{"text": "semantic role labeling", "start_pos": 54, "end_pos": 76, "type": "TASK", "confidence": 0.6714456478754679}]}, {"text": "However, the applicability of these systems is limited to those words for which labeled data exists, and their accuracy is strongly correlated with the amount of labeled data available.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 111, "end_pos": 119, "type": "METRIC", "confidence": 0.9987108707427979}]}, {"text": "Despite the substantial annotation effort involved in the creation of FrameNet (spanning approximately twelve years), the number of annotated instances varies greatly across lexical items.", "labels": [], "entities": []}, {"text": "For instance, FrameNet contains annotations for 2,113 verbs; of these 12.3% have five or less annotated examples.", "labels": [], "entities": []}, {"text": "The average number of annotations per verb is 29.2.", "labels": [], "entities": []}, {"text": "Labeled data is thus scarce for individual predicates within FrameNet's target domain and would presumably be even scarcer across domains.", "labels": [], "entities": []}, {"text": "The problem is more severe for languages other than English, where training data on the scale of FrameNet is virtually non-existent.", "labels": [], "entities": [{"text": "FrameNet", "start_pos": 97, "end_pos": 105, "type": "DATASET", "confidence": 0.8725782036781311}]}, {"text": "Although FrameNets are being constructed for German, Spanish, and Japanese, these resources are substantially smaller than their English counterpart and of limited value for modeling purposes.", "labels": [], "entities": []}, {"text": "One simple solution, albeit expensive and timeconsuming, is to manually create more annotations.", "labels": [], "entities": []}, {"text": "A better alternative maybe to begin with an initial small set of labeled examples and augment it with unlabeled data sufficiently similar to the original labeled set.", "labels": [], "entities": []}, {"text": "Suppose we have manual annotations for sentence (1a).", "labels": [], "entities": []}, {"text": "We shall try and find in an unlabeled corpus other sentences that are both structurally and semantically similar.", "labels": [], "entities": []}, {"text": "For instance, we may think that Bill will punch me in the face and I punched her hard in the head resemble our initial sentence and are thus good examples to add to our database.", "labels": [], "entities": []}, {"text": "Now, in order to use these new sentences as training data we must somehow infer their semantic roles.", "labels": [], "entities": []}, {"text": "We can probably guess that constituents in the same syntactic position must have the same semantic role, especially if they refer to the same concept (e.g., \"body parts\") and thus label in the face and in the head with the role Body part.", "labels": [], "entities": []}, {"text": "Analogously, Bill and I would be labeled as Agent and me and her as Victim.", "labels": [], "entities": []}, {"text": "In this paper we formalize the method sketched above in order to expand a small number of FrameNet-style semantic role annotations with large amounts of unlabeled data.", "labels": [], "entities": []}, {"text": "We adopt a learning strategy where annotations are projected from labeled onto unlabeled instances via maximizing a similarity function measuring syntactic and semantic compatibility.", "labels": [], "entities": []}, {"text": "We formalize the annotation projection problem as a generalization of the linear assignment problem and solve it efficiently using the simplex algorithm.", "labels": [], "entities": [{"text": "annotation projection problem", "start_pos": 17, "end_pos": 46, "type": "TASK", "confidence": 0.7806586225827535}]}, {"text": "We evaluate our algorithm by comparing the performance of a semantic role labeler trained on the annotations produced by our method and on a smaller dataset consisting solely of hand-labeled instances.", "labels": [], "entities": []}, {"text": "Results in several experimental settings show that the automatic annotations, despite being noisy, bring significant performance improvements.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section we discuss our experimental setup for assessing the usefulness of the method presented above.", "labels": [], "entities": []}, {"text": "We give details on our training procedure and parameter estimation, describe the semantic labeler we used in our experiments and explain how its output was evaluated.", "labels": [], "entities": [{"text": "parameter estimation", "start_pos": 46, "end_pos": 66, "type": "TASK", "confidence": 0.6539202332496643}]}, {"text": "Corpora Our seed corpus was taken from FrameNet.", "labels": [], "entities": [{"text": "Corpora Our seed corpus", "start_pos": 0, "end_pos": 23, "type": "DATASET", "confidence": 0.9401519000530243}, {"text": "FrameNet", "start_pos": 39, "end_pos": 47, "type": "DATASET", "confidence": 0.8730496168136597}]}, {"text": "The latter contains approximately 2,000 verb entries out of which we randomly selected a sample of 100.", "labels": [], "entities": []}, {"text": "We next extracted all annotated sentences for each of these verbs.", "labels": [], "entities": []}, {"text": "These sentences formed our gold standard corpus, 20% of which was reserved as test data.", "labels": [], "entities": []}, {"text": "We used the remaining 80% as seeds for training purposes.", "labels": [], "entities": []}, {"text": "We generated seed corpora of various sizes by randomly reducing the number of annotation instances per verb to a maximum of n.", "labels": [], "entities": []}, {"text": "An additional (non-overlapping) random sample of 100 verbs was used as development set for tuning the parameters for our method.", "labels": [], "entities": []}, {"text": "We gathered unlabeled sentences from the BNC.", "labels": [], "entities": [{"text": "BNC", "start_pos": 41, "end_pos": 44, "type": "DATASET", "confidence": 0.7786888480186462}]}, {"text": "The seed and unlabeled corpora were parsed with RASP ().", "labels": [], "entities": [{"text": "RASP", "start_pos": 48, "end_pos": 52, "type": "METRIC", "confidence": 0.9256621599197388}]}, {"text": "The FrameNet annotations in the seed corpus were converted into dependency graphs (see) using the method described in.", "labels": [], "entities": [{"text": "FrameNet annotations in the seed corpus", "start_pos": 4, "end_pos": 43, "type": "DATASET", "confidence": 0.7418927848339081}]}, {"text": "Briefly, the method works by matching nodes in the dependency graph with role bearing substrings in FrameNet.", "labels": [], "entities": [{"text": "FrameNet", "start_pos": 100, "end_pos": 108, "type": "DATASET", "confidence": 0.9186428189277649}]}, {"text": "It first finds the node in the graph which most closely matches the frame evoking element in FrameNet.", "labels": [], "entities": [{"text": "FrameNet", "start_pos": 93, "end_pos": 101, "type": "DATASET", "confidence": 0.919687032699585}]}, {"text": "Next, individual graph nodes are compared against labeled substrings in FrameNet to transfer all roles onto their closest matching graph nodes.", "labels": [], "entities": [{"text": "FrameNet", "start_pos": 72, "end_pos": 80, "type": "DATASET", "confidence": 0.9134678244590759}]}, {"text": "Parameter Estimation The similarity function described in Section 3.2 has three free parameters.", "labels": [], "entities": [{"text": "Parameter", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.8578277826309204}]}, {"text": "These are the weight A which determines the relative importance of syntactic and semantic information, the parameter B which determines when two arguments cannot be aligned and the syntactic score a for almost identical grammatical roles.", "labels": [], "entities": []}, {"text": "We optimized these parameters on the development set using Powell's direction set method with F 1 as our loss function.", "labels": [], "entities": []}, {"text": "The optimal values for A, B and a were 1.76, 0.41 and 0.67, respectively.", "labels": [], "entities": []}, {"text": "Our similarity function is further parametrized in using a semantic space model to compute the similarity between two words.", "labels": [], "entities": []}, {"text": "Considerable latitude is allowed in specifying the parameters of vector-based models.", "labels": [], "entities": []}, {"text": "These involve the definition of the linguistic context over which cooccurrences are collected, the number of components used (e.g., the k most frequent words in a corpus), and their values (e.g., as raw cooccurrence frequencies or ratios of probabilities).", "labels": [], "entities": []}, {"text": "We created a vector-based model from a lemmatized version of the BNC.", "labels": [], "entities": [{"text": "BNC", "start_pos": 65, "end_pos": 68, "type": "DATASET", "confidence": 0.9563378095626831}]}, {"text": "Following previous work, we optimized the parameters of our model on a wordbased semantic similarity task.", "labels": [], "entities": [{"text": "wordbased semantic similarity task", "start_pos": 71, "end_pos": 105, "type": "TASK", "confidence": 0.67576102912426}]}, {"text": "The task involves examining the degree of linear relationship between the human judgments for two individual words and vector-based similarity values.", "labels": [], "entities": []}, {"text": "We experimented with a variety of dimensions (ranging from 50 to 500,000), vector component definitions (e.g., pointwise mutual information or log likelihood ratio) and similarity measures (e.g., cosine or confusion probability).", "labels": [], "entities": []}, {"text": "We used WordSim353, a benchmark dataset (), consisting of relatedness judgments (on a scale of 0 to 10) for 353 word pairs.", "labels": [], "entities": [{"text": "WordSim353", "start_pos": 8, "end_pos": 18, "type": "DATASET", "confidence": 0.9574850797653198}]}, {"text": "We obtained best results with a model using a context window of five words on either side of the target word, the cosine measure, and 2,000 vector dimensions.", "labels": [], "entities": []}, {"text": "The latter were the most common context words (excluding a stop list of function words).", "labels": [], "entities": []}, {"text": "Their values were set to the ratio of the probability of the context word given the target word to the probability of the context word overall.", "labels": [], "entities": []}, {"text": "This configuration gave high correlations with the WordSim353 similarity judgments using the cosine measure.", "labels": [], "entities": [{"text": "WordSim353 similarity judgments", "start_pos": 51, "end_pos": 82, "type": "DATASET", "confidence": 0.7328028877576193}]}, {"text": "Solving the Linear Program A variety of algorithms have been developed for solving the linear assignment problem efficiently.", "labels": [], "entities": [{"text": "linear assignment problem", "start_pos": 87, "end_pos": 112, "type": "TASK", "confidence": 0.7855698664983114}]}, {"text": "In our study, we used the simplex algorithm.", "labels": [], "entities": []}, {"text": "We generate and solve an LP of every unlabeled sentence we wish to annotate.", "labels": [], "entities": []}, {"text": "Semantic role labeler We evaluated our method on a semantic role labeling task.", "labels": [], "entities": [{"text": "Semantic role labeler", "start_pos": 0, "end_pos": 21, "type": "TASK", "confidence": 0.6132928927739462}, {"text": "semantic role labeling task", "start_pos": 51, "end_pos": 78, "type": "TASK", "confidence": 0.7229108959436417}]}, {"text": "Specifically, we compared the performance of a generic semantic role labeler trained on the seed corpus and a larger corpus expanded with annotations produced by our method.", "labels": [], "entities": []}, {"text": "Our semantic role labeler followed closely the implementation of Johansson and Nugues.", "labels": [], "entities": [{"text": "semantic role labeler", "start_pos": 4, "end_pos": 25, "type": "TASK", "confidence": 0.6353670756022135}]}, {"text": "We extracted features from dependency parses corresponding to those routinely used in the semantic role labeling literature (see for an overview).", "labels": [], "entities": [{"text": "semantic role labeling", "start_pos": 90, "end_pos": 112, "type": "TASK", "confidence": 0.6268495122591654}]}, {"text": "SVM classifiers were trained to identify the arguments and label them with appropriate roles.", "labels": [], "entities": [{"text": "SVM classifiers", "start_pos": 0, "end_pos": 15, "type": "TASK", "confidence": 0.8300583064556122}]}, {"text": "For the latter we performed multi-class classification following the one-versus-one method 3.", "labels": [], "entities": [{"text": "multi-class classification", "start_pos": 28, "end_pos": 54, "type": "TASK", "confidence": 0.7621888816356659}]}, {"text": "For the experiments reported in this paper we used the LIBLINEAR library.", "labels": [], "entities": [{"text": "LIBLINEAR library", "start_pos": 55, "end_pos": 72, "type": "DATASET", "confidence": 0.9062037467956543}]}, {"text": "The misclassification penalty C was set to 0.1.", "labels": [], "entities": [{"text": "misclassification penalty C", "start_pos": 4, "end_pos": 31, "type": "METRIC", "confidence": 0.943100094795227}]}, {"text": "To evaluate against the test set, we linearized the resulting dependency graphs in order to obtain labeled role bracketings like those in example (1) and measured labeled precision, labeled recall and labeled F 1 . (Since our focus is on role labeling and not frame prediction, we let our role labeler make use of gold standard frame annotations, i.e., labeling of frame evoking elements with frame names.)", "labels": [], "entities": [{"text": "precision", "start_pos": 171, "end_pos": 180, "type": "METRIC", "confidence": 0.6481707692146301}, {"text": "recall", "start_pos": 190, "end_pos": 196, "type": "METRIC", "confidence": 0.9188914895057678}, {"text": "F 1", "start_pos": 209, "end_pos": 212, "type": "METRIC", "confidence": 0.8681719601154327}, {"text": "frame prediction", "start_pos": 260, "end_pos": 276, "type": "TASK", "confidence": 0.7066346853971481}]}], "tableCaptions": [{"text": " Table 2: Semantic role labeling performance using  different amounts of training data; the seeds are  expanded with their k nearest neighbors;  *  : F 1 is  significantly different from 0-NN (p < 0.05).", "labels": [], "entities": [{"text": "Semantic role labeling", "start_pos": 10, "end_pos": 32, "type": "TASK", "confidence": 0.7514448761940002}, {"text": "F 1", "start_pos": 150, "end_pos": 153, "type": "METRIC", "confidence": 0.9773477911949158}]}, {"text": " Table 3: Semantic role labeling performance us- ing different numbers of seed instances per verb in  the training corpus; the seeds are expanded with  their k = 2 nearest neighbors;  *  : F 1 is signifi- cantly different from seed corpus (p < 0.05).", "labels": [], "entities": [{"text": "Semantic role labeling", "start_pos": 10, "end_pos": 32, "type": "TASK", "confidence": 0.7092473208904266}]}]}