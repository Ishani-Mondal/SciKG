{"title": [{"text": "On the use of Comparable Corpora to improve SMT performance", "labels": [], "entities": [{"text": "SMT", "start_pos": 44, "end_pos": 47, "type": "TASK", "confidence": 0.9967711567878723}]}], "abstractContent": [{"text": "We present a simple and effective method for extracting parallel sentences from comparable corpora.", "labels": [], "entities": []}, {"text": "We employ a statistical machine translation (SMT) system built from small amounts of parallel texts to translate the source side of the non-parallel corpus.", "labels": [], "entities": [{"text": "statistical machine translation (SMT)", "start_pos": 12, "end_pos": 49, "type": "TASK", "confidence": 0.7730135520299276}]}, {"text": "The target side texts are used, along with other corpora, in the language model of this SMT system.", "labels": [], "entities": [{"text": "SMT", "start_pos": 88, "end_pos": 91, "type": "TASK", "confidence": 0.9935699701309204}]}, {"text": "We then use information retrieval techniques and simple filters to create French/English parallel data from a comparable news corpora.", "labels": [], "entities": [{"text": "information retrieval", "start_pos": 12, "end_pos": 33, "type": "TASK", "confidence": 0.7383733689785004}]}, {"text": "We evaluate the quality of the extracted data by showing that it significantly improves the performance of an SMT systems.", "labels": [], "entities": [{"text": "SMT", "start_pos": 110, "end_pos": 113, "type": "TASK", "confidence": 0.9928939938545227}]}], "introductionContent": [{"text": "Parallel corpora have proved bean indispensable resource in Statistical Machine Translation (SMT).", "labels": [], "entities": [{"text": "Statistical Machine Translation (SMT)", "start_pos": 60, "end_pos": 97, "type": "TASK", "confidence": 0.8713542222976685}]}, {"text": "A parallel corpus, also called bitext, consists in bilingual texts aligned at the sentence level.", "labels": [], "entities": []}, {"text": "They have also proved to be useful in a range of natural language processing applications like automatic lexical acquisition, cross language information retrieval and annotation projection.", "labels": [], "entities": [{"text": "automatic lexical acquisition", "start_pos": 95, "end_pos": 124, "type": "TASK", "confidence": 0.6271161536375681}, {"text": "cross language information retrieval", "start_pos": 126, "end_pos": 162, "type": "TASK", "confidence": 0.7326375097036362}, {"text": "annotation projection", "start_pos": 167, "end_pos": 188, "type": "TASK", "confidence": 0.7128919363021851}]}, {"text": "Unfortunately, parallel corpora area limited resource, with insufficient coverage of many language pairs and application domains of interest.", "labels": [], "entities": []}, {"text": "The performance of an SMT system heavily depends on the parallel corpus used for training.", "labels": [], "entities": [{"text": "SMT", "start_pos": 22, "end_pos": 25, "type": "TASK", "confidence": 0.9940388202667236}]}, {"text": "Generally, more bitexts lead to better performance.", "labels": [], "entities": [{"text": "bitexts", "start_pos": 16, "end_pos": 23, "type": "METRIC", "confidence": 0.9729650616645813}]}, {"text": "Current resources of parallel corpora cover few language pairs and mostly come from one domain (proceedings of the Canadian or European Parliament, or of the United Nations).", "labels": [], "entities": []}, {"text": "This becomes specifically problematic when SMT systems trained on such corpora are used for general translations, as the language jargon heavily used in these corpora is not appropriate for everyday life translations or translations in some other domain.", "labels": [], "entities": [{"text": "SMT", "start_pos": 43, "end_pos": 46, "type": "TASK", "confidence": 0.9897157549858093}, {"text": "general translations", "start_pos": 92, "end_pos": 112, "type": "TASK", "confidence": 0.590899795293808}]}, {"text": "One option to increase this scarce resource could be to produce more human translations, but this is a very expensive option, in terms of both time and money.", "labels": [], "entities": []}, {"text": "In recent work less expensive but very productive methods of creating such sentence aligned bilingual corpora were proposed.", "labels": [], "entities": []}, {"text": "These are based on generating \"parallel\" texts from already available \"almost parallel\" or \"not much parallel\" texts.", "labels": [], "entities": []}, {"text": "The term \"comparable corpus\" is often used to define such texts.", "labels": [], "entities": []}, {"text": "A comparable corpus is a collection of texts composed independently in the respective languages and combined on the basis of similarity of content (.", "labels": [], "entities": []}, {"text": "The raw material for comparable documents is often easy to obtain but the alignment of individual documents is a challenging task.", "labels": [], "entities": []}, {"text": "Multilingual news reporting agencies like AFP, Xinghua, Reuters, CNN, BBC etc.", "labels": [], "entities": [{"text": "AFP", "start_pos": 42, "end_pos": 45, "type": "DATASET", "confidence": 0.9086556434631348}, {"text": "BBC", "start_pos": 70, "end_pos": 73, "type": "DATASET", "confidence": 0.9066100716590881}]}, {"text": "serve to be reliable producers of huge collections of such comparable corpora.", "labels": [], "entities": []}, {"text": "Such texts are widely available from LDC, in particular the Gigaword corpora, or over the WEB for many languages and domains, e.g. Wikipedia.", "labels": [], "entities": [{"text": "WEB", "start_pos": 90, "end_pos": 93, "type": "DATASET", "confidence": 0.9341350197792053}]}, {"text": "They often contain many sentences that are reasonable translations of each other, thus potential parallel sentences to be identified and extracted.", "labels": [], "entities": []}, {"text": "There has been considerable amount of work on bilingual comparable corpora to learn word translations as well as discovering parallel sentences.", "labels": [], "entities": [{"text": "learn word translations", "start_pos": 78, "end_pos": 101, "type": "TASK", "confidence": 0.6546455721060435}]}, {"text": "use an approach based on dynamic programming to identify potential parallel sentences in title pairs.", "labels": [], "entities": []}, {"text": "Longest common sub sequence, edit operations and match-based score functions are subsequently used to determine confidence scores.", "labels": [], "entities": []}, {"text": "propose their STRAND web-mining based system and show that their approach is able to find large numbers of similar document pairs.", "labels": [], "entities": []}, {"text": "Works aimed at discovering parallel sentences  include, who use cross-language information retrieval techniques and dynamic programming to extract sentences from an English-Japanese comparable corpus.", "labels": [], "entities": []}, {"text": "They identify similar article pairs, and then, treating these pairs as parallel texts, align their sentences on a sentence pair similarity score and use DP to find the least-cost alignment over the document pair.", "labels": [], "entities": [{"text": "DP", "start_pos": 153, "end_pos": 155, "type": "METRIC", "confidence": 0.9387192130088806}]}, {"text": "approach the problem by using a cosine similarity measure to match foreign and English documents.", "labels": [], "entities": []}, {"text": "They work on \"very non-parallel corpora\".", "labels": [], "entities": []}, {"text": "They then generate all possible sentence pairs and select the best ones based on a threshold on cosine similarity scores.", "labels": [], "entities": []}, {"text": "Using the extracted sentences they learn a dictionary and iterate over with more sentence pairs.", "labels": [], "entities": []}, {"text": "Recent work by uses a bilingual lexicon to translate some of the words of the source sentence.", "labels": [], "entities": []}, {"text": "These translations are then used to query the database to find matching translations using information retrieval (IR) techniques.", "labels": [], "entities": [{"text": "information retrieval (IR)", "start_pos": 91, "end_pos": 117, "type": "TASK", "confidence": 0.7681543111801148}]}, {"text": "Candidate sentences are determined based on word overlap and the decision whether a sentence pair is parallel or not is performed by a maximum entropy classifier trained on parallel sentences.", "labels": [], "entities": []}, {"text": "Bootstrapping is used and the size of the learned bilingual dictionary is increased over iterations to get better results.", "labels": [], "entities": []}, {"text": "Our technique is similar to that of (Munteanu and Marcu, 2005) but we bypass the need of the bilingual dictionary by using proper SMT translations and instead of a maximum entropy classifier we use simple measures like the word error rate (WER) and the translation error rate (TER) to decide whether sentences are parallel or not.", "labels": [], "entities": [{"text": "SMT translations", "start_pos": 130, "end_pos": 146, "type": "TASK", "confidence": 0.9374096095561981}, {"text": "word error rate (WER)", "start_pos": 223, "end_pos": 244, "type": "METRIC", "confidence": 0.8933190802733103}, {"text": "translation error rate (TER)", "start_pos": 253, "end_pos": 281, "type": "METRIC", "confidence": 0.9280155797799429}]}, {"text": "Using the full SMT sentences, we get an added advantage of being able to detect one of the major errors of this technique, also identified by), i.e, the cases where the initial sentences are identical but the retrieved sentence has a tail of extra words at sentence end.", "labels": [], "entities": [{"text": "SMT sentences", "start_pos": 15, "end_pos": 28, "type": "TASK", "confidence": 0.8934186697006226}]}, {"text": "We try to counter this problem as detailed in 4.1.", "labels": [], "entities": []}, {"text": "We apply this technique to create a parallel corpus for the French/English language pair using the LDC Gigaword comparable corpus.", "labels": [], "entities": [{"text": "LDC Gigaword comparable corpus", "start_pos": 99, "end_pos": 129, "type": "DATASET", "confidence": 0.9157879948616028}]}, {"text": "We show that we achieve significant improvements in the BLEU score by adding our extracted corpus to the already available human-translated corpora.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 56, "end_pos": 66, "type": "METRIC", "confidence": 0.9750155806541443}]}, {"text": "This paper is organized as follows.", "labels": [], "entities": []}, {"text": "In the next section we first describe the baseline SMT system trained on human-provided translations only.", "labels": [], "entities": [{"text": "SMT", "start_pos": 51, "end_pos": 54, "type": "TASK", "confidence": 0.9858574867248535}]}, {"text": "We then proceed by explaining our parallel sentence selection scheme and the post-processing.", "labels": [], "entities": []}, {"text": "Section 4 summarizes our experimental results and the paper concludes with a discussion and perspectives of this work.", "labels": [], "entities": []}], "datasetContent": [{"text": "Our main goal was to be able to create an additional parallel corpus to improve machine translation quality, especially for the domains where we have lessor no parallel data available.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 80, "end_pos": 99, "type": "TASK", "confidence": 0.783277153968811}]}, {"text": "In this section we report the results of adding these extracted parallel sentences to the already available humantranslated parallel sentences.", "labels": [], "entities": []}, {"text": "We conducted a range of experiments by adding our extracted corpus to various combinations of already available human-translated parallel corpora.", "labels": [], "entities": []}, {"text": "We experimented with WER and TER as filters to select the best scoring sentences.", "labels": [], "entities": [{"text": "WER", "start_pos": 21, "end_pos": 24, "type": "METRIC", "confidence": 0.9906850457191467}, {"text": "TER", "start_pos": 29, "end_pos": 32, "type": "METRIC", "confidence": 0.99391770362854}]}, {"text": "Generally, sentences selected based on TER filter showed better BLEU and TER scores than their WER counter parts.", "labels": [], "entities": [{"text": "TER filter", "start_pos": 39, "end_pos": 49, "type": "METRIC", "confidence": 0.9665967226028442}, {"text": "BLEU", "start_pos": 64, "end_pos": 68, "type": "METRIC", "confidence": 0.9996426105499268}, {"text": "TER", "start_pos": 73, "end_pos": 76, "type": "METRIC", "confidence": 0.996900200843811}]}, {"text": "So we chose TER filter as standard for French words for training news bitexts only TER filter WER: BLEU scores on the Test data using an WER or TER filter.", "labels": [], "entities": [{"text": "TER filter", "start_pos": 12, "end_pos": 22, "type": "METRIC", "confidence": 0.952898383140564}, {"text": "TER filter", "start_pos": 83, "end_pos": 93, "type": "METRIC", "confidence": 0.9389002025127411}, {"text": "WER", "start_pos": 94, "end_pos": 97, "type": "METRIC", "confidence": 0.5075895190238953}, {"text": "BLEU", "start_pos": 99, "end_pos": 103, "type": "METRIC", "confidence": 0.8742294311523438}, {"text": "Test data", "start_pos": 118, "end_pos": 127, "type": "DATASET", "confidence": 0.7587748467922211}, {"text": "WER", "start_pos": 137, "end_pos": 140, "type": "METRIC", "confidence": 0.9191581010818481}, {"text": "TER", "start_pos": 144, "end_pos": 147, "type": "METRIC", "confidence": 0.9118167161941528}]}, {"text": "our experiments with limited amounts of human translated corpus.", "labels": [], "entities": []}, {"text": "shows this WER vs TER comparison based on BLEU and TER scores on the test data in function of the size of training data.", "labels": [], "entities": [{"text": "WER", "start_pos": 11, "end_pos": 14, "type": "METRIC", "confidence": 0.9860196113586426}, {"text": "TER", "start_pos": 18, "end_pos": 21, "type": "METRIC", "confidence": 0.8126888275146484}, {"text": "BLEU", "start_pos": 42, "end_pos": 46, "type": "METRIC", "confidence": 0.9994577765464783}, {"text": "TER", "start_pos": 51, "end_pos": 54, "type": "METRIC", "confidence": 0.9974825978279114}]}, {"text": "These experiments were performed with only 1.56M words of human-provided translations (news-commentary corpus).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Effect on BLEU score of removing extra  sentence tails from otherwise parallel sentences.", "labels": [], "entities": [{"text": "Effect", "start_pos": 10, "end_pos": 16, "type": "METRIC", "confidence": 0.9714881777763367}, {"text": "BLEU score", "start_pos": 20, "end_pos": 30, "type": "METRIC", "confidence": 0.9813802540302277}]}, {"text": " Table 2: Summary of BLEU scores for the best systems on the Dev-data with the news-commentary  corpus and the bilingual dictionary.", "labels": [], "entities": [{"text": "Summary", "start_pos": 10, "end_pos": 17, "type": "METRIC", "confidence": 0.8936408162117004}, {"text": "BLEU", "start_pos": 21, "end_pos": 25, "type": "METRIC", "confidence": 0.9905197620391846}]}]}