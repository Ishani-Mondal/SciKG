{"title": [{"text": "Large-Coverage Root Lexicon Extraction for Hindi", "labels": [], "entities": [{"text": "Large-Coverage Root Lexicon Extraction", "start_pos": 0, "end_pos": 38, "type": "TASK", "confidence": 0.5305000469088554}]}], "abstractContent": [{"text": "This paper describes a method using morphological rules and heuristics, for the automatic extraction of large-coverage lexicons of stems and root word-forms from a raw text corpus.", "labels": [], "entities": [{"text": "automatic extraction of large-coverage lexicons of stems and root word-forms from a raw text corpus", "start_pos": 80, "end_pos": 179, "type": "TASK", "confidence": 0.8240683078765869}]}, {"text": "We cast the problem of high-coverage lexicon extraction as one of stemming followed by root word-form selection.", "labels": [], "entities": [{"text": "high-coverage lexicon extraction", "start_pos": 23, "end_pos": 55, "type": "TASK", "confidence": 0.6663854817549387}]}, {"text": "We examine the use of POS tagging to improve precision and recall of stemming and thereby the coverage of the lexicon.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 22, "end_pos": 33, "type": "TASK", "confidence": 0.7732281684875488}, {"text": "precision", "start_pos": 45, "end_pos": 54, "type": "METRIC", "confidence": 0.9992035031318665}, {"text": "recall", "start_pos": 59, "end_pos": 65, "type": "METRIC", "confidence": 0.9980164766311646}]}, {"text": "We present accuracy, precision and recall scores for the system on a Hindi corpus.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 11, "end_pos": 19, "type": "METRIC", "confidence": 0.9995513558387756}, {"text": "precision", "start_pos": 21, "end_pos": 30, "type": "METRIC", "confidence": 0.9995923638343811}, {"text": "recall", "start_pos": 35, "end_pos": 41, "type": "METRIC", "confidence": 0.9995368719100952}, {"text": "Hindi corpus", "start_pos": 69, "end_pos": 81, "type": "DATASET", "confidence": 0.7363704890012741}]}], "introductionContent": [{"text": "Large-coverage morphological lexicons are an essential component of morphological analysers.", "labels": [], "entities": []}, {"text": "Morphological analysers find application in language processing systems for tasks like tagging, parsing and machine translation.", "labels": [], "entities": [{"text": "tagging", "start_pos": 87, "end_pos": 94, "type": "TASK", "confidence": 0.9645477533340454}, {"text": "machine translation", "start_pos": 108, "end_pos": 127, "type": "TASK", "confidence": 0.7792270183563232}]}, {"text": "While raw text is an abundant and easily accessible linguistic resource, high-coverage morphological lexicons are scarce or unavailable in Hindi as in many other languages).", "labels": [], "entities": []}, {"text": "Thus, the development of better algorithms for the extraction of morphological lexicons from raw text corpora is a task of considerable importance.", "labels": [], "entities": [{"text": "extraction of morphological lexicons from raw text corpora", "start_pos": 51, "end_pos": 109, "type": "TASK", "confidence": 0.7430912852287292}]}, {"text": "A root word-form lexicon is an intermediate stage in the creation of a morphological lexicon.", "labels": [], "entities": []}, {"text": "In this paper, we consider the problem of extracting a large-coverage root word-form lexicon for the Hindi language, a highly inflectional and moderately agglutinative Indo-European language spoken widely in South Asia.", "labels": [], "entities": []}, {"text": "Since a POS tagger, another basic tool, was available along with POS tagged data to train it, and since the error patterns indicated that POS tagging could greatly improve the accuracy of the lexicon, we used the POS tagger in our experiments on lexicon extraction.", "labels": [], "entities": [{"text": "POS tagger", "start_pos": 8, "end_pos": 18, "type": "TASK", "confidence": 0.6463495194911957}, {"text": "accuracy", "start_pos": 176, "end_pos": 184, "type": "METRIC", "confidence": 0.9973911046981812}, {"text": "lexicon extraction", "start_pos": 246, "end_pos": 264, "type": "TASK", "confidence": 0.7494739890098572}]}, {"text": "Previous work in morphological lexicon extraction from a raw corpus often does not achieve very high precision and recall ().", "labels": [], "entities": [{"text": "morphological lexicon extraction", "start_pos": 17, "end_pos": 49, "type": "TASK", "confidence": 0.6836345295111338}, {"text": "precision", "start_pos": 101, "end_pos": 110, "type": "METRIC", "confidence": 0.9989087581634521}, {"text": "recall", "start_pos": 115, "end_pos": 121, "type": "METRIC", "confidence": 0.9985558390617371}]}, {"text": "In some previous work the process of lexicon extraction involves incremental or post-construction manual validation of the entire lexicon.", "labels": [], "entities": [{"text": "lexicon extraction", "start_pos": 37, "end_pos": 55, "type": "TASK", "confidence": 0.8232384026050568}]}, {"text": "Our method attempts to improve on and extend the previous work by increasing the precision and recall of the system to such a point that manual validation might even be rendered unnecessary.", "labels": [], "entities": [{"text": "precision", "start_pos": 81, "end_pos": 90, "type": "METRIC", "confidence": 0.9993966817855835}, {"text": "recall", "start_pos": 95, "end_pos": 101, "type": "METRIC", "confidence": 0.9959288239479065}]}, {"text": "Yet another difference, to our knowledge, is that in our method we cast the problem of lexicon extraction as two subproblems: that of stemming and following it, that of root word-form selection.", "labels": [], "entities": [{"text": "lexicon extraction", "start_pos": 87, "end_pos": 105, "type": "TASK", "confidence": 0.8024486303329468}]}, {"text": "The input resources for our system are as follows: a) raw text corpus, b) morphological rules, c) POS tagger and d) word-segmentation labelled data.", "labels": [], "entities": [{"text": "POS tagger", "start_pos": 98, "end_pos": 108, "type": "TASK", "confidence": 0.6548415273427963}]}, {"text": "We output a stem lexicon and a root wordform lexicon.", "labels": [], "entities": []}, {"text": "We take as input a raw text corpus and a set of morphological rules.", "labels": [], "entities": []}, {"text": "We first run a stemming algorithm that uses the morphological rules and some heuristics to obtain a stem dictionary.", "labels": [], "entities": []}, {"text": "We then create a root dictionary from the stem dictionary.", "labels": [], "entities": []}, {"text": "The last two input resources are optional but when a POS tagger is utilized, the F-score (harmonic mean of precision and recall) of the root lexicon can be as high as 94.6%.", "labels": [], "entities": [{"text": "F-score", "start_pos": 81, "end_pos": 88, "type": "METRIC", "confidence": 0.9989377856254578}, {"text": "precision", "start_pos": 107, "end_pos": 116, "type": "METRIC", "confidence": 0.7998408675193787}, {"text": "recall)", "start_pos": 121, "end_pos": 128, "type": "METRIC", "confidence": 0.9822051823139191}]}, {"text": "In the rest of the paper, we provide a brief overview of the morphological features of the Hindi language, followed by a description of our method including the specification of rules, the corpora and the heuristics for stemming and root word-form selection.", "labels": [], "entities": [{"text": "root word-form selection", "start_pos": 233, "end_pos": 257, "type": "TASK", "confidence": 0.6165391604105631}]}, {"text": "We then evaluate the system with and without the POS tagger.", "labels": [], "entities": []}], "datasetContent": [{"text": "The goal of our experiment was to build a highcoverage morphological lexicon for Hindi and to evaluate the same.", "labels": [], "entities": []}, {"text": "Having developed a multi-stage system for lexicon extraction with a POS tagging step following by stemming and root word-form discovery, we proceeded to evaluate it as follows.", "labels": [], "entities": [{"text": "lexicon extraction", "start_pos": 42, "end_pos": 60, "type": "TASK", "confidence": 0.7539867758750916}, {"text": "POS tagging", "start_pos": 68, "end_pos": 79, "type": "TASK", "confidence": 0.6263810396194458}, {"text": "root word-form discovery", "start_pos": 111, "end_pos": 135, "type": "TASK", "confidence": 0.6975862979888916}]}, {"text": "The stemming and the root discovery module were evaluated against the gold standard of 1000 word-forms.", "labels": [], "entities": [{"text": "root discovery", "start_pos": 21, "end_pos": 35, "type": "TASK", "confidence": 0.719550684094429}]}, {"text": "In the first experiment, the precision and recall of stemming using the HSE+Pos algorithm were measured at different POS tagging accuracies.", "labels": [], "entities": [{"text": "precision", "start_pos": 29, "end_pos": 38, "type": "METRIC", "confidence": 0.9994712471961975}, {"text": "recall", "start_pos": 43, "end_pos": 49, "type": "METRIC", "confidence": 0.9990596175193787}]}, {"text": "In the second experiment the root word-form discovery module was provided the entire raw word corpus to use in determining the best possible candidate fora root and tested using the gold standard.", "labels": [], "entities": []}, {"text": "The scores obtained reflect the performance of the overall system.", "labels": [], "entities": []}, {"text": "For stemming, the recall was calculated as the fraction of stems and suffixes in the gold standard that were returned by the stemmer for each wordform examined.", "labels": [], "entities": [{"text": "recall", "start_pos": 18, "end_pos": 24, "type": "METRIC", "confidence": 0.9986306428909302}]}, {"text": "The precision was calculated as the fraction of stems and suffixes returned by the stemmer that matched the gold standard.", "labels": [], "entities": [{"text": "precision", "start_pos": 4, "end_pos": 13, "type": "METRIC", "confidence": 0.9994001388549805}]}, {"text": "The Fscore was calculated as the harmonic mean of the precision and recall.", "labels": [], "entities": [{"text": "Fscore", "start_pos": 4, "end_pos": 10, "type": "METRIC", "confidence": 0.9951367974281311}, {"text": "precision", "start_pos": 54, "end_pos": 63, "type": "METRIC", "confidence": 0.9997339844703674}, {"text": "recall", "start_pos": 68, "end_pos": 74, "type": "METRIC", "confidence": 0.9987006187438965}]}, {"text": "The recall of the root lexicon was measured as the fraction of gold standard roots that were in the lexicon.", "labels": [], "entities": [{"text": "recall", "start_pos": 4, "end_pos": 10, "type": "METRIC", "confidence": 0.9993048906326294}]}, {"text": "The precision was calculated as the fraction of roots in the lexicon that were also in the gold standard.", "labels": [], "entities": [{"text": "precision", "start_pos": 4, "end_pos": 13, "type": "METRIC", "confidence": 0.9993025064468384}]}, {"text": "Accuracy was the percentage of gold word-forms' roots that were matched exactly.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 0, "end_pos": 8, "type": "METRIC", "confidence": 0.9840207695960999}]}, {"text": "In order to approximately estimate the accuracy of a stemmer or morphological analyzer that used such a lexicon, we also calculated the accuracy weighted by the frequency of the word-forms in a small corpus of running text.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 39, "end_pos": 47, "type": "METRIC", "confidence": 0.9989483952522278}, {"text": "accuracy", "start_pos": 136, "end_pos": 144, "type": "METRIC", "confidence": 0.9992256164550781}]}, {"text": "The gold standard tokens were seen in this corpus about 4400 times.", "labels": [], "entities": []}, {"text": "We only considered content words (nouns, verbs, adjectives and adverbs) in this calculation.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 6: % Frequency and Accuracy by BSE", "labels": [], "entities": [{"text": "Frequency", "start_pos": 12, "end_pos": 21, "type": "METRIC", "confidence": 0.9615239500999451}, {"text": "Accuracy", "start_pos": 26, "end_pos": 34, "type": "METRIC", "confidence": 0.9643650054931641}, {"text": "BSE", "start_pos": 38, "end_pos": 41, "type": "DATASET", "confidence": 0.8932918310165405}]}, {"text": " Table 7: Frequency by POS Category", "labels": [], "entities": []}, {"text": " Table 8: Comparison of Rules", "labels": [], "entities": []}, {"text": " Table 9: Comparison of Heuristics", "labels": [], "entities": []}, {"text": " Table 10: Errors by POS Category", "labels": [], "entities": [{"text": "Errors", "start_pos": 11, "end_pos": 17, "type": "METRIC", "confidence": 0.9333578944206238}]}, {"text": " Table 11: Stemming Performance Comparisons", "labels": [], "entities": [{"text": "Stemming Performance Comparisons", "start_pos": 11, "end_pos": 43, "type": "TASK", "confidence": 0.8806411027908325}]}, {"text": " Table 12: Stemming Performance at Different  POS Tagger Accuracies", "labels": [], "entities": [{"text": "Stemming", "start_pos": 11, "end_pos": 19, "type": "TASK", "confidence": 0.9857498407363892}, {"text": "POS Tagger Accuracies", "start_pos": 46, "end_pos": 67, "type": "TASK", "confidence": 0.6601593792438507}]}, {"text": " Table 12. We com- pare the performance with gold POS tags and a  baseline system which does not use POS tags. We  do not use labelled training data for this section of  the experiments and only evaluate against the first  gold standard.", "labels": [], "entities": []}, {"text": " Table 13: Root Finding Accuracy", "labels": [], "entities": [{"text": "Root Finding", "start_pos": 11, "end_pos": 23, "type": "TASK", "confidence": 0.71492800116539}, {"text": "Accuracy", "start_pos": 24, "end_pos": 32, "type": "METRIC", "confidence": 0.9057588577270508}]}, {"text": " Table 14: Weighted Stemming and Root Finding  Accuracies (only Content Words)", "labels": [], "entities": [{"text": "Root Finding", "start_pos": 33, "end_pos": 45, "type": "TASK", "confidence": 0.6559273302555084}, {"text": "Accuracies", "start_pos": 47, "end_pos": 57, "type": "METRIC", "confidence": 0.6565431356430054}]}]}