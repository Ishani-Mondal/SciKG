{"title": [{"text": "Text-to-text Semantic Similarity for Automatic Short Answer Grading", "labels": [], "entities": [{"text": "Semantic Similarity", "start_pos": 13, "end_pos": 32, "type": "TASK", "confidence": 0.7413907945156097}, {"text": "Automatic Short Answer Grading", "start_pos": 37, "end_pos": 67, "type": "TASK", "confidence": 0.6588426828384399}]}], "abstractContent": [{"text": "In this paper, we explore unsupervised techniques for the task of automatic short answer grading.", "labels": [], "entities": [{"text": "automatic short answer grading", "start_pos": 66, "end_pos": 96, "type": "TASK", "confidence": 0.577165424823761}]}, {"text": "We compare a number of knowledge-based and corpus-based measures of text similarity, evaluate the effect of domain and size on the corpus-based measures, and also introduce a novel technique to improve the performance of the system by integrating automatic feedback from the student answers.", "labels": [], "entities": []}, {"text": "Overall, our system significantly and consistently out-performs other unsupervised methods for short answer grading that have been proposed in the past.", "labels": [], "entities": [{"text": "short answer grading", "start_pos": 95, "end_pos": 115, "type": "TASK", "confidence": 0.6356060902277628}]}], "introductionContent": [{"text": "One of the most important aspects of the learning process is the assessment of the knowledge acquired by the learner.", "labels": [], "entities": []}, {"text": "Ina typical examination setting (e.g., an exam, assignment or quiz), this assessment implies an instructor or a grader who provides students with feedback on their answers to questions that are related to the subject matter.", "labels": [], "entities": []}, {"text": "There are, however, certain scenarios, such as the large number of worldwide sites with limited teacher availability, or the individual or group study sessions done outside of class, in which an instructor is not available and yet students need an assessment of their knowledge of the subject.", "labels": [], "entities": []}, {"text": "In these instances, we often have to turn to computerassisted assessment.", "labels": [], "entities": []}, {"text": "While some forms of computer-assisted assessment do not require sophisticated text understanding (e.g., multiple choice or true/false questions can be easily graded by a system if the correct solution is available), there are also student answers that consist of free text which require an analysis of the text in the answer.", "labels": [], "entities": []}, {"text": "Research to date has concentrated on two main subtasks of computerassisted assessment: the grading of essays, which is done mainly by checking the style, grammaticality, and coherence of the essay (cf. (), and the assessment of short student answers (e.g.,), which is the focus of this paper.", "labels": [], "entities": []}, {"text": "An automatic short answer grading system is one which automatically assigns a grade to an answer provided by a student through a comparison with one or more correct answers.", "labels": [], "entities": []}, {"text": "It is important to note that this is different from the related task of paraphrase detection, since a requirement in student answer grading is to provide a grade on a certain scale rather than a binary yes/no decision.", "labels": [], "entities": [{"text": "paraphrase detection", "start_pos": 72, "end_pos": 92, "type": "TASK", "confidence": 0.9819607734680176}]}, {"text": "In this paper, we explore and evaluate a set of unsupervised techniques for automatic short answer grading.", "labels": [], "entities": [{"text": "automatic short answer grading", "start_pos": 76, "end_pos": 106, "type": "TASK", "confidence": 0.5549739375710487}]}, {"text": "Unlike previous work, which has either required the availability of manually crafted patterns (), or large training data sets to bootstrap such patterns), we attempt to devise an unsupervised method that requires no human intervention.", "labels": [], "entities": []}, {"text": "We address the grading problem from a text similarity perspective and examine the usefulness of various textto-text semantic similarity measures for automatically grading short student answers.", "labels": [], "entities": []}, {"text": "Specifically, in this paper we seek answers to the following questions.", "labels": [], "entities": []}, {"text": "First, given a number of corpus-based and knowledge-based methods as previously proposed in the past for word and text semantic similarity, what are the measures that work best for the task of short answer grading?", "labels": [], "entities": [{"text": "word and text semantic similarity", "start_pos": 105, "end_pos": 138, "type": "TASK", "confidence": 0.6562466621398926}, {"text": "short answer grading", "start_pos": 193, "end_pos": 213, "type": "TASK", "confidence": 0.6473489006360372}]}, {"text": "Second, given a corpus-based measure of similarity, what is the impact of the domain and the size of the corpus on the accuracy of the measure?", "labels": [], "entities": [{"text": "accuracy", "start_pos": 119, "end_pos": 127, "type": "METRIC", "confidence": 0.9988188147544861}]}, {"text": "Finally, can we use the student answers themselves to improve the quality of the grading system?", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 2: Comparison of knowledge-based and  corpus-based measures of similarity for short an- swer grading", "labels": [], "entities": []}, {"text": " Table 4: Summary of results obtained with vari- ous similarity measures, with relevance feedback  based on six student answers. We also list the  tf*idf and the LSA trained on BNC baselines (no  feedback), as well as the annotator agreement up- per bound.", "labels": [], "entities": [{"text": "BNC baselines", "start_pos": 177, "end_pos": 190, "type": "DATASET", "confidence": 0.9047132432460785}]}]}