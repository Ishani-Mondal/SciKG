{"title": [{"text": "An Alignment Algorithm using Belief Propagation and a Structure-Based Distortion Model", "labels": [], "entities": []}], "abstractContent": [{"text": "In this paper, we first demonstrate the interest of the Loopy Belief Propagation algorithm to train and use a simple alignment model where the expected marginal values needed for an efficient EM-training are not easily computable.", "labels": [], "entities": []}, {"text": "We then improve this model with a distortion model based on structure conservation.", "labels": [], "entities": []}], "introductionContent": [], "datasetContent": [{"text": "The evaluation setting is the same as in the previous section.", "labels": [], "entities": []}, {"text": "We created syntactic trees for every sentences.", "labels": [], "entities": []}, {"text": "For English,we used the Dan Bikel implementation of the Collins parser.", "labels": [], "entities": []}, {"text": "For French, the SYGMART parser and for Japanese, the KNP parser.", "labels": [], "entities": []}, {"text": "The line SDM:Parsing (SDM standing for \"Structure-based Distortion Monolink\") shows the results obtained by using P-sets from the trees produced by these parsers.", "labels": [], "entities": []}, {"text": "The line SDM:Adjacency shows results obtained by using adjacent positions P-sets ,as described at the end of the previous section (therefore, SDM:Adjacency do not use any parser).", "labels": [], "entities": []}, {"text": "Several interesting observations can be made from the results.", "labels": [], "entities": []}, {"text": "First, our structure-based distortion model did improve the results of the monolink model.", "labels": [], "entities": []}, {"text": "There are however some surprising results.", "labels": [], "entities": []}, {"text": "In particular, SDM:Adjacency produced surprisingly good results.", "labels": [], "entities": [{"text": "SDM", "start_pos": 15, "end_pos": 18, "type": "TASK", "confidence": 0.9665256142616272}]}, {"text": "It comes close to the results of the IBM model 4 in both language pairs, while it actually uses exactly the same parameters as model 1.", "labels": [], "entities": []}, {"text": "The fact that an assumption as simple as \"allow permutations, penalize gaps\" can produce results almost on par with the complicated distortion model of model 4 might bean indication that this model is unnecessarily complex for languages with similar structure.Another surprising result is the fact that SDM:Adjacency gives better results for the English-French language pair than SDM:Parsing, while we expected that information provided by parsers would have been more relevant for the distortion model.", "labels": [], "entities": [{"text": "SDM:Parsing", "start_pos": 380, "end_pos": 391, "type": "TASK", "confidence": 0.6732517679532369}]}, {"text": "It might bean indication that the structure of English and French is so close that knowing it provide only moderate information for word reordering.", "labels": [], "entities": [{"text": "word reordering", "start_pos": 132, "end_pos": 147, "type": "TASK", "confidence": 0.7346355020999908}]}, {"text": "The contrast with the English-Japanese pair is, in this respect, very interesting.", "labels": [], "entities": []}, {"text": "For this language pair, SDM:Adjacency did provide a strong improve-: Results for Japanese/English.", "labels": [], "entities": []}, {"text": "ment, but significantly less so than SDM:Parsing.", "labels": [], "entities": []}, {"text": "This tend to show that for language pairs that have very different structures, the information provided by syntactic tree is much more relevant.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Results for Japanese/English.", "labels": [], "entities": []}]}