{"title": [{"text": "Growing Finely-Discriminating Taxonomies from Seeds of Varying Quality and Size", "labels": [], "entities": [{"text": "Size", "start_pos": 75, "end_pos": 79, "type": "TASK", "confidence": 0.6387145519256592}]}], "abstractContent": [{"text": "Concept taxonomies offer a powerful means for organizing knowledge, but this organization must allow for many overlapping and fine-grained perspectives if a general-purpose taxonomy is to reflect concepts as they are actually employed and reasoned about in everyday usage.", "labels": [], "entities": []}, {"text": "We present here a means of bootstrapping finely-discriminating tax-onomies from a variety of different starting points, or seeds, that are acquired from three different sources: WordNet, ConceptNet and the web at large.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 178, "end_pos": 185, "type": "DATASET", "confidence": 0.966907799243927}]}], "introductionContent": [{"text": "Taxonomies provide a natural and intuitive means of organizing information, from the biological taxonomies of the Linnaean system to the layout of supermarkets and bookstores to the organizational structure of companies.", "labels": [], "entities": []}, {"text": "Taxonomies also provide the structural backbone for ontologies in computer science, from common-sense ontologies like Cyc ( and SUMO) to lexical ontologies like).", "labels": [], "entities": []}, {"text": "Each of these uses is based on the same root-branch-leaf metaphor: the broadest terms with the widest scope occupy the highest positions of a taxonomy, near the root, while specific terms with the most local concerns are located lower in the hierarchy, nearest the leaves.", "labels": [], "entities": []}, {"text": "The more interior nodes that a taxonomy possesses, the finer the conceptual distinctions and the more gradated the similarity judgments it can make (e.g.,).", "labels": [], "entities": []}, {"text": "General-purpose computational taxonomies are called upon to perform both coarse-grained and fine-grained judgments.", "labels": [], "entities": []}, {"text": "In NLP, for instance, the semantics of \"eat\" requires just enough knowledge to discriminate foods like tofu and cheese from non-foods like wool and steel, while specific applications in the domain of cooking and recipes (e.g., CHEF) require enough discrimination to know that tofu can be replaced with clotted cheese in many recipes because each is a soft, white and bland food.", "labels": [], "entities": []}, {"text": "So while much depends on the domain of usage, it remains an open question as to how many nodes a good taxonomy should possess.", "labels": [], "entities": []}, {"text": "Princeton WordNet, for instance, strives for as many nodes as there are word senses in English, yet it also contains a substantial number of composite nodes that are lexicalized not as single words, but as complex phrases.", "labels": [], "entities": [{"text": "Princeton WordNet", "start_pos": 0, "end_pos": 17, "type": "DATASET", "confidence": 0.9240374267101288}]}, {"text": "Print dictionaries intended for human consumption aim for some economy of structure, and typically do not include the meaning of phrases that can be understood as straightforward compositions of the meaning of their parts.", "labels": [], "entities": []}, {"text": "But WordNet also serves another purpose, as a lexical knowledgebase for computers, not humans, a context in which concerns about space seem quaint.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 4, "end_pos": 11, "type": "DATASET", "confidence": 0.9211487770080566}]}, {"text": "When space is not a issue, there seems no good reason to exclude nodes from a concept taxonomy merely for being composites of other ideas; the real test of entry is whether a given node adds value to a taxonomy, by increasing its level of internal organization through the systematic dissection of overly broad categories into finer, more intuitive and manageable clusters.", "labels": [], "entities": []}, {"text": "In this paper we describe a means by which finely-discriminating taxonomies can be grown from a variety of different knowledge seeds.", "labels": [], "entities": []}, {"text": "These taxonomies comprise composite categories that can be lexicalized as phrases of the form \"ADJ NOUN\", such as Sharp-Instrument, which represents the set of all instruments that are typically considered sharp, such as knives, scissors, chisels and can-openers.", "labels": [], "entities": [{"text": "ADJ NOUN\"", "start_pos": 95, "end_pos": 104, "type": "METRIC", "confidence": 0.824959933757782}, {"text": "Sharp-Instrument", "start_pos": 114, "end_pos": 130, "type": "METRIC", "confidence": 0.9045001864433289}]}, {"text": "While WordNet already contains an equivalent category, named Edge-Tool, which it defines with the gloss \"any cutting tool with a sharp cutting edge\", it provides no structural basis for inferring that any member of this category can be considered sharp.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 6, "end_pos": 13, "type": "DATASET", "confidence": 0.9309473037719727}]}, {"text": "For the most part, if two ideas (word senses) belong to the same semantic category X in WordNet, the most we can infer is that both possess the trivial property X-ness.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 88, "end_pos": 95, "type": "DATASET", "confidence": 0.9361206293106079}]}, {"text": "Our goal here is to construct taxonomies whose form makes explicit the actual properties that accrue from membership in a category.", "labels": [], "entities": []}, {"text": "Past work on related approaches to taxonomy creation are discussed in section 2, while section 3 describes the different knowledge seeds that serve as the starting point for our bootstrapping process.", "labels": [], "entities": [{"text": "taxonomy creation", "start_pos": 35, "end_pos": 52, "type": "TASK", "confidence": 0.8735593557357788}]}, {"text": "In section 4 we describe the bootstrapping process in more detail; such processes are prone to noise, so we also discuss how the acquired categorizations are validated and filtered after each bootstrapping cycle.", "labels": [], "entities": []}, {"text": "An evaluation of the key ideas is then presented in section 5, to determine which seed yields the highest quality taxonomy once bootstrapping is completed.", "labels": [], "entities": []}, {"text": "The paper then concludes with some final remarks in section 6.", "labels": [], "entities": []}], "datasetContent": [{"text": "The WordNet near-miss filter thus ensures that the parent field (Pk) of every triple contains a value that is sensible for the given child concept (Ci), but does not ensure that the discriminating property (Dj) in each triple is equally sensible and apropos.", "labels": [], "entities": []}, {"text": "To see whether the bootstrapping process is simply padding the seed taxonomy with large quantities of noise, or whether the acquired Dj values do indeed mark out the implicit essence of the Ci terms they describe, we need an evaluation framework that can quantify the ontological usefulness of these Dj values.", "labels": [], "entities": []}, {"text": "For this, we use the experimental setup of Almuhareb and, who use information extraction from the web to acquire attribute values for different terms/concepts, and who then compare the taxonomy that can be induced by clustering these values with the taxonomic backbone of WordNet.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 272, "end_pos": 279, "type": "DATASET", "confidence": 0.955414354801178}]}, {"text": "Almuhareb and Poesio first created a balanced set of 402 nouns from 21 different semantic classes in WordNet.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 101, "end_pos": 108, "type": "DATASET", "confidence": 0.9466580748558044}]}, {"text": "They then acquired attested attribute values for these nouns (such as hot for coffee, red for car, etc.) using the query \"(a|an| the) * Ci (is|was)\" to find corresponding Dj values for each Ci.", "labels": [], "entities": []}, {"text": "Unlike our work, these authors did not seek to acquire hypernyms for each Ci during this search, and did not try to link the acquired attribute values to a particular branching point (Pk) in the taxonomy (they did, however, seek matching attributes for these values, such as Temperature for hot, but that aspect is not relevant here).", "labels": [], "entities": [{"text": "Temperature", "start_pos": 275, "end_pos": 286, "type": "METRIC", "confidence": 0.9441946744918823}]}, {"text": "They acquired 94,989 attribute values in all for the 402 test nouns.", "labels": [], "entities": []}, {"text": "These values were then used as features of the corresponding nouns in a clustering experiment, using the CLUTO system of.", "labels": [], "entities": []}, {"text": "By using attribute values as a basis for partitioning the set of 402 nouns into 21 different categories, Almuhareb and Poesio attempted to reconstruct the original 21 WordNet categories from which the nouns were drawn.", "labels": [], "entities": []}, {"text": "The more accurate the match to the original WordNet clustering, the more these attribute values can be seen (and used) as a representation of conceptual structure.", "labels": [], "entities": []}, {"text": "In their first attempt, they achieved just a 56.7% clustering accuracy against the original human-assigned categories of WordNet.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 62, "end_pos": 70, "type": "METRIC", "confidence": 0.9686782956123352}, {"text": "WordNet", "start_pos": 121, "end_pos": 128, "type": "DATASET", "confidence": 0.963744580745697}]}, {"text": "But after using a noise-filter to remove almost half of the web-harvested attribute values, they achieve a higher cluster accuracy of 62.7%.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 122, "end_pos": 130, "type": "METRIC", "confidence": 0.7794967293739319}]}, {"text": "More specifically, Poesio and Almuhareb achieve a cluster purity of 0.627 and a cluster entropy of 0.338 using 51,345 features to describe and cluster the 402 nouns.", "labels": [], "entities": []}, {"text": "We replicate the above experiments using the same 402 nouns, and assess the clustering accur\u00ad acy (again using WordNet as a gold\u00adstandard) after each bootstrapping cycle.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 111, "end_pos": 118, "type": "DATASET", "confidence": 0.9721363186836243}]}, {"text": "Recall that we use only the D j fields of each triple as features for the clustering process, so the comparison with the WordNet gold\u00adstandard is still a fair one.", "labels": [], "entities": [{"text": "WordNet gold\u00adstandard", "start_pos": 121, "end_pos": 142, "type": "DATASET", "confidence": 0.9794992804527283}]}, {"text": "Once again, the goal is to determine how much like the human\u00adcrafted WordNet taxonomy is the tax\u00ad onomy that is clustered automatically from the discriminating words D j only.", "labels": [], "entities": [{"text": "WordNet taxonomy", "start_pos": 69, "end_pos": 85, "type": "DATASET", "confidence": 0.9580959975719452}]}, {"text": "The clustering ac\u00ad curacy for all three seeds are shown in  The test-set of 402 nouns contains some low-frequency words, such as casuarina, cinchona, dodecahedron, and concavity, and Almuhareb and Poesio note that one third of their data-set has a low-frequency of between 5-100 occurrences in the British National Corpus.", "labels": [], "entities": [{"text": "British National Corpus", "start_pos": 298, "end_pos": 321, "type": "DATASET", "confidence": 0.9225004514058431}]}, {"text": "Looking to the coverage column of each table, we thus see that there are words in the Poesio and Almuhareb data set for which no triples can be acquired in 5 cycles of bootstrapping.", "labels": [], "entities": [{"text": "Poesio and Almuhareb data set", "start_pos": 86, "end_pos": 115, "type": "DATASET", "confidence": 0.6889789819717407}]}, {"text": "Interestingly, though each seed is quite different in origin and size (see again   Both the WordNet and ConceptNet seeds achieve comparable accuracies of 68% and 67% respectively after 5 cycles of bootstrapping, which compares well with the accuracy of 62.7% achieved by Poesio and Almuhareb.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 92, "end_pos": 99, "type": "DATASET", "confidence": 0.9527136087417603}, {"text": "accuracies", "start_pos": 140, "end_pos": 150, "type": "METRIC", "confidence": 0.9942470788955688}, {"text": "accuracy", "start_pos": 241, "end_pos": 249, "type": "METRIC", "confidence": 0.9992606043815613}]}, {"text": "However, the simile seed clearly yields the best accuracy of 84.3%, which also exceeds the accuracy of 66.4% achieved by Poesio and Almuhareb when using both values and attributes (such as Temperature, Color, etc.) for clustering, or the accuracy of 70.9% they achieve when using attributes alone.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 49, "end_pos": 57, "type": "METRIC", "confidence": 0.9993410706520081}, {"text": "accuracy", "start_pos": 91, "end_pos": 99, "type": "METRIC", "confidence": 0.9992362260818481}, {"text": "accuracy", "start_pos": 238, "end_pos": 246, "type": "METRIC", "confidence": 0.9987989664077759}]}, {"text": "Furthermore, bootstrapping from the simile seed yields higher cluster accuracy on the 402-noun data-set than themselves achieve with their simile data on the same test-set (69.85%).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 70, "end_pos": 78, "type": "METRIC", "confidence": 0.9071546792984009}]}, {"text": "But most striking of all is the concision of the representations that are acquired using bootstrapping.", "labels": [], "entities": []}, {"text": "The simile seed yields a high cluster accuracy using a pool of just 2,614 fine discriminators, while Poesio and Almuhareb use 51,345 features even after their feature-set has been filtered for noise.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 38, "end_pos": 46, "type": "METRIC", "confidence": 0.9606677293777466}]}, {"text": "Though starting from different initial scales, each seed converges toward a feature-set that is roughly twenty times smaller than that used by Poesio and Almuhareb.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: The size of seed collections yielded from  different sources.", "labels": [], "entities": []}, {"text": " Table 2: Clustering accuracy using the WordNet seed  collection (E denotes Entropy and P stands for Purity)", "labels": [], "entities": [{"text": "accuracy", "start_pos": 21, "end_pos": 29, "type": "METRIC", "confidence": 0.9522556662559509}, {"text": "WordNet seed  collection", "start_pos": 40, "end_pos": 64, "type": "DATASET", "confidence": 0.9786156813303629}]}]}