{"title": [{"text": "Rule Filtering by Pattern for Efficient Hierarchical Translation", "labels": [], "entities": [{"text": "Hierarchical Translation", "start_pos": 40, "end_pos": 64, "type": "TASK", "confidence": 0.7084205150604248}]}], "abstractContent": [{"text": "We describe refinements to hierarchical translation search procedures intended to reduce both search errors and memory usage through modifications to hypothesis expansion in cube pruning and reductions in the size of the rule sets used in translation.", "labels": [], "entities": [{"text": "translation search", "start_pos": 40, "end_pos": 58, "type": "TASK", "confidence": 0.7967812418937683}]}, {"text": "Rules are put into syntactic classes based on the number of non-terminals and the pattern, and various filtering strategies are then applied to assess the impact on translation speed and quality.", "labels": [], "entities": []}, {"text": "Results are reported on the 2008 NIST Arabic-to-English evaluation task.", "labels": [], "entities": [{"text": "2008 NIST Arabic-to-English evaluation task", "start_pos": 28, "end_pos": 71, "type": "DATASET", "confidence": 0.8266116976737976}]}], "introductionContent": [{"text": "Hierarchical phrase-based translation) has emerged as one of the dominant current approaches to statistical machine translation.", "labels": [], "entities": [{"text": "Hierarchical phrase-based translation)", "start_pos": 0, "end_pos": 38, "type": "TASK", "confidence": 0.7799355983734131}, {"text": "statistical machine translation", "start_pos": 96, "end_pos": 127, "type": "TASK", "confidence": 0.7194176216920217}]}, {"text": "Hiero translation systems incorporate many of the strengths of phrase-based translation systems, such as feature-based translation and strong target language models, while also allowing flexible translation and movement based on hierarchical rules extracted from aligned parallel text.", "labels": [], "entities": [{"text": "Hiero translation", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.8826107680797577}]}, {"text": "The approach has been widely adopted and reported to be competitive with other large-scale data driven approaches, e.g. (. Large-scale hierarchical SMT involves automatic rule extraction from aligned parallel text, model parameter estimation, and the use of cube pruning k-best list generation in hierarchical translation.", "labels": [], "entities": [{"text": "SMT", "start_pos": 148, "end_pos": 151, "type": "TASK", "confidence": 0.6391580700874329}, {"text": "rule extraction from aligned parallel text", "start_pos": 171, "end_pos": 213, "type": "TASK", "confidence": 0.8348457664251328}, {"text": "model parameter estimation", "start_pos": 215, "end_pos": 241, "type": "TASK", "confidence": 0.6211671829223633}, {"text": "cube pruning k-best list generation", "start_pos": 258, "end_pos": 293, "type": "TASK", "confidence": 0.6274367570877075}, {"text": "hierarchical translation", "start_pos": 297, "end_pos": 321, "type": "TASK", "confidence": 0.6627984195947647}]}, {"text": "The number of hierarchical rules extracted far exceeds the number of phrase translations typically found in aligned text.", "labels": [], "entities": []}, {"text": "While this may lead to improved translation quality, there is also the risk of lengthened translation times and increased memory usage, along with possible search errors due to the pruning procedures needed in search.", "labels": [], "entities": [{"text": "translation", "start_pos": 32, "end_pos": 43, "type": "TASK", "confidence": 0.9629546403884888}]}, {"text": "We describe several techniques to reduce memory usage and search errors in hierarchical translation.", "labels": [], "entities": []}, {"text": "Memory usage can be reduced in cube pruning through smart memoization, and spreading neighborhood exploration can be used to reduce search errors.", "labels": [], "entities": []}, {"text": "However, search errors can still remain even when implementing simple phrase-based translation.", "labels": [], "entities": [{"text": "phrase-based translation", "start_pos": 70, "end_pos": 94, "type": "TASK", "confidence": 0.7111337184906006}]}, {"text": "We describe a 'shallow' search through hierarchical rules which greatly speeds translation without any effect on quality.", "labels": [], "entities": []}, {"text": "We then describe techniques to analyze and reduce the set of hierarchical rules.", "labels": [], "entities": []}, {"text": "We do this based on the structural properties of rules and develop strategies to identify and remove redundant or harmful rules.", "labels": [], "entities": []}, {"text": "We identify groupings of rules based on non-terminals and their patterns and assess the impact on translation quality and computational requirements for each given rule group.", "labels": [], "entities": []}, {"text": "We find that with appropriate filtering strategies rule sets can be greatly reduced in size without impact on translation performance.", "labels": [], "entities": []}], "datasetContent": [{"text": "Finally, in this section we report results of our shallow hierarchical system with the 2.5 mincount=1 configuration from, after including the following N-best list rescoring steps.", "labels": [], "entities": []}, {"text": "We build sentencespecific zero-cutoff stupid-backoff () 5-gram language models, estimated using \u223c4.7B words of English newswire text, and apply them to rescore each 10000-best list.", "labels": [], "entities": []}, {"text": "\u2022 Minimum Bayes Risk (MBR).", "labels": [], "entities": [{"text": "Minimum Bayes Risk (MBR)", "start_pos": 2, "end_pos": 26, "type": "METRIC", "confidence": 0.8813425799210867}]}, {"text": "We then rescore the first 1000-best hypotheses with MBR, taking the negative sentence level BLEU score as the loss function to minimise).", "labels": [], "entities": [{"text": "MBR", "start_pos": 52, "end_pos": 55, "type": "METRIC", "confidence": 0.9600749611854553}, {"text": "BLEU score", "start_pos": 92, "end_pos": 102, "type": "METRIC", "confidence": 0.9172108471393585}]}, {"text": "shows results for mt02-05-tune, mt02-05-test, the NIST subsets from the MT06 evaluation (mt06-nist-nw for newswire data and mt06-nist-ng for newsgroup) and mt08, as measured by lowercased IBM BLEU and TER).", "labels": [], "entities": [{"text": "MT06 evaluation", "start_pos": 72, "end_pos": 87, "type": "DATASET", "confidence": 0.9034265577793121}, {"text": "BLEU", "start_pos": 192, "end_pos": 196, "type": "METRIC", "confidence": 0.9290194511413574}, {"text": "TER", "start_pos": 201, "end_pos": 204, "type": "METRIC", "confidence": 0.9024702310562134}]}, {"text": "Mixed case NIST BLEU for this system on mt08 is 42.5.", "labels": [], "entities": [{"text": "NIST", "start_pos": 11, "end_pos": 15, "type": "METRIC", "confidence": 0.8781487345695496}, {"text": "BLEU", "start_pos": 16, "end_pos": 20, "type": "METRIC", "confidence": 0.8678792715072632}, {"text": "mt08", "start_pos": 40, "end_pos": 44, "type": "DATASET", "confidence": 0.956264853477478}]}, {"text": "This is directly comparable to official MT08 evaluation results 1 .", "labels": [], "entities": [{"text": "MT08 evaluation", "start_pos": 40, "end_pos": 55, "type": "DATASET", "confidence": 0.6086350977420807}]}], "tableCaptions": [{"text": " Table 2: Phrase-based TTM and Hiero perfor- mance on mt02-05-tune for TTM (a), Hiero (b),  Hiero with spreading neighborhood exploration  (c). SE is the number of Hiero hypotheses with  search errors.", "labels": [], "entities": [{"text": "SE", "start_pos": 144, "end_pos": 146, "type": "METRIC", "confidence": 0.9991044402122498}]}, {"text": " Table 3: Hierarchical rule patterns classed by  number of non-terminals, N nt , number of ele- ments N e , source and target patterns, and types in  the rule set extracted for mt02-05-tune.", "labels": [], "entities": []}, {"text": " Table 4: Rules excluded from the initial rule set.", "labels": [], "entities": []}, {"text": " Table 6: Impact of general rule filters on transla- tion (IBM BLEU), time (in seconds per word) and  number of rules (in millions).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 63, "end_pos": 67, "type": "METRIC", "confidence": 0.7752759456634521}, {"text": "time", "start_pos": 70, "end_pos": 74, "type": "METRIC", "confidence": 0.9667344093322754}]}, {"text": " Table 7: Effect of pattern-based rule filters. Time in seconds per word. Rules in millions.", "labels": [], "entities": [{"text": "Time", "start_pos": 48, "end_pos": 52, "type": "METRIC", "confidence": 0.9689366817474365}]}, {"text": " Table 8: Arabic-to-English translation results (lower-cased IBM BLEU / TER) with large language mod- els and MBR decoding.", "labels": [], "entities": [{"text": "BLEU / TER", "start_pos": 65, "end_pos": 75, "type": "METRIC", "confidence": 0.8043925762176514}]}]}