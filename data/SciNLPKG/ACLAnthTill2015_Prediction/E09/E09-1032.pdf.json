{"title": [{"text": "Who is \"You\"? Combining Linguistic and Gaze Features to Resolve Second-Person References in Dialogue *", "labels": [], "entities": [{"text": "Resolve Second-Person References in Dialogue", "start_pos": 56, "end_pos": 100, "type": "TASK", "confidence": 0.7579380095005035}]}], "abstractContent": [{"text": "We explore the problem of resolving the second person English pronoun you in multi-party dialogue, using a combination of linguistic and visual features.", "labels": [], "entities": []}, {"text": "First, we distinguish generic and referential uses, then we classify the referential uses as either plural or singular, and finally, for the latter cases, we identify the addressee.", "labels": [], "entities": []}, {"text": "In our first set of experiments, the linguistic and visual features are derived from manual transcriptions and annotations, but in the second set, they are generated through entirely automatic means.", "labels": [], "entities": []}, {"text": "Results show that a multimodal system is often preferable to a unimodal one.", "labels": [], "entities": []}], "introductionContent": [{"text": "The English pronoun you is the second most frequent word in unrestricted conversation (after I and right before it).", "labels": [], "entities": []}, {"text": "1 Despite this, with the exception of, its resolution has received very little attention in the literature.", "labels": [], "entities": [{"text": "resolution", "start_pos": 43, "end_pos": 53, "type": "TASK", "confidence": 0.9820370078086853}]}, {"text": "This is perhaps not surprising since the vast amount of work on anaphora and reference resolution has focused on text or discourse -mediums where second-person deixis is perhaps not as prominent as it is in dialogue.", "labels": [], "entities": [{"text": "reference resolution", "start_pos": 77, "end_pos": 97, "type": "TASK", "confidence": 0.7299678772687912}]}, {"text": "For spoken dialogue pronoun resolution modules however, resolving you is an essential task that has an important impact on the capabilities of dialogue summarization systems.", "labels": [], "entities": [{"text": "spoken dialogue pronoun resolution", "start_pos": 4, "end_pos": 38, "type": "TASK", "confidence": 0.6568173989653587}, {"text": "dialogue summarization", "start_pos": 143, "end_pos": 165, "type": "TASK", "confidence": 0.6291967183351517}]}, {"text": "Besides being important for computational implementations, resolving you is also an interesting and challenging research problem.", "labels": [], "entities": []}, {"text": "As for third person pronouns such as it, some uses of you are not strictly referential.", "labels": [], "entities": []}, {"text": "These include discourse marker uses such as you know in example (1), and generic uses like (2), where you does not refer to the addressee as it does in (3).", "labels": [], "entities": []}, {"text": "(1) It's not just, you know, noises like something hitting.", "labels": [], "entities": []}, {"text": "(2) Often, you need to know specific button sequences to get certain functionalities done.", "labels": [], "entities": []}, {"text": "(3) I think it's good.", "labels": [], "entities": []}, {"text": "You've done a good review.", "labels": [], "entities": []}, {"text": "However, unlike it, you is ambiguous between singular and plural interpretations -an issue that is particularly problematic in multi-party conversations.", "labels": [], "entities": []}, {"text": "While you clearly has a plural referent in (4), in (3) the number of its referent is ambiguous.", "labels": [], "entities": []}, {"text": "(4) I don't know if you guys have any questions.", "labels": [], "entities": []}, {"text": "When an utterance contains a singular referential you, resolving the you amounts to identifying the individual to whom the utterance is addressed.", "labels": [], "entities": []}, {"text": "This is trivial in two-person dialogue since the current listener is always the addressee, but in conversations with multiple participants, it is a complex problem where different kinds of linguistic and visual information play important roles.", "labels": [], "entities": []}, {"text": "One of the issues we investigate here is how this applies to the more concrete problem of resolving the second person pronoun you.", "labels": [], "entities": []}, {"text": "We approach this issue as a three-step problem.", "labels": [], "entities": []}, {"text": "Using the AMI Meeting Corpus () of multi-party dialogues, we first discriminate between referential and generic uses of you.", "labels": [], "entities": [{"text": "AMI Meeting Corpus", "start_pos": 10, "end_pos": 28, "type": "DATASET", "confidence": 0.8904052575429281}]}, {"text": "Then, within the referential uses, we distinguish between singular and plural, and finally, we resolve the singular referential instances by identifying the intended addressee.", "labels": [], "entities": []}, {"text": "We use multimodal features: initially, we extract discourse features from manual transcriptions and use visual information derived from manual annotations, but then we move to a fully automatic approach, using 1-best transcriptions produced by an automatic speech recognizer (ASR) and visual features automatically extracted from raw video.", "labels": [], "entities": []}, {"text": "In the next section of this paper, we give a brief overview of related work.", "labels": [], "entities": []}, {"text": "We describe our data in Section 3, and explain how we extract visual and linguistic features in Sections 4 and 5 respectively.", "labels": [], "entities": []}, {"text": "Section 6 then presents our experiments with manual transcriptions and annotations, while Section 7, those with automatically extracted information.", "labels": [], "entities": []}, {"text": "We end with conclusions in Section 8.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section we report our experiments and results when using manual transcriptions and annotations.", "labels": [], "entities": []}, {"text": "In Section 7 we will present the results obtained using ASR output and automatically extracted visual information.", "labels": [], "entities": [{"text": "ASR", "start_pos": 56, "end_pos": 59, "type": "METRIC", "confidence": 0.47656768560409546}]}, {"text": "All experiments (here and in the next section) are performed using a Bayesian Network classifier with 10-fold crossvalidation.", "labels": [], "entities": []}, {"text": "In each task, we give raw overall accuracy results and then F-scores for each of the classes.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 34, "end_pos": 42, "type": "METRIC", "confidence": 0.9902578592300415}, {"text": "F-scores", "start_pos": 60, "end_pos": 68, "type": "METRIC", "confidence": 0.9986899495124817}]}, {"text": "We computed measures of information gain in order to assess the predictive power of the various features, and did some experimentation with Correlation-based Feature Selection (CFS)).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Distribution of you interpretations", "labels": [], "entities": []}, {"text": " Table 5: Generic vs. referential uses", "labels": [], "entities": []}, {"text": " Table 6: Singular vs. plural reference; * = with Correlation-", "labels": [], "entities": []}, {"text": " Table 7: Addressee detection for singular references; * =", "labels": [], "entities": [{"text": "Addressee detection", "start_pos": 10, "end_pos": 29, "type": "TASK", "confidence": 0.8947505354881287}]}]}