{"title": [{"text": "User Simulations for context-sensitive speech recognition in Spoken Dialogue Systems", "labels": [], "entities": [{"text": "User Simulations", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.6663094609975815}, {"text": "context-sensitive speech recognition", "start_pos": 21, "end_pos": 57, "type": "TASK", "confidence": 0.618739108244578}]}], "abstractContent": [{"text": "We use a machine learner trained on a combination of acoustic and contextual features to predict the accuracy of incoming n-best automatic speech recognition (ASR) hypotheses to a spoken dialogue system (SDS).", "labels": [], "entities": [{"text": "accuracy of incoming n-best automatic speech recognition (ASR) hypotheses", "start_pos": 101, "end_pos": 174, "type": "TASK", "confidence": 0.6722115549174222}]}, {"text": "Our novel approach is to use a simple statistical User Simulation (US) for this task, which measures the likelihood that the user would say each hypothesis in the current context.", "labels": [], "entities": [{"text": "statistical User Simulation (US", "start_pos": 38, "end_pos": 69, "type": "TASK", "confidence": 0.6201968729496002}]}, {"text": "Such US models are now common in machine learning approaches to SDS, are trained on real dialogue data, and are related to theories of \"alignment\" in psycholinguistics.", "labels": [], "entities": [{"text": "SDS", "start_pos": 64, "end_pos": 67, "type": "TASK", "confidence": 0.9822373986244202}]}, {"text": "We use a US to predict the user's next dialogue move and thereby re-rank n-best hypotheses of a speech recognizer fora corpus of 2564 user utterances.", "labels": [], "entities": []}, {"text": "The method achieved a significant relative reduction of Word Error Rate (WER) of 5% (this is 44% of the possible WER improvement on this data), and 62% of the possible semantic improvement (Dialogue Move Accuracy), compared to the baseline policy of selecting the topmost ASR hypothesis.", "labels": [], "entities": [{"text": "Word Error Rate (WER)", "start_pos": 56, "end_pos": 77, "type": "METRIC", "confidence": 0.8789195617039999}, {"text": "WER", "start_pos": 113, "end_pos": 116, "type": "METRIC", "confidence": 0.8967280983924866}]}, {"text": "The majority of the improvement is attributable to the User Simulation feature, as shown by Information Gain analysis.", "labels": [], "entities": [{"text": "User Simulation", "start_pos": 55, "end_pos": 70, "type": "TASK", "confidence": 0.791435569524765}]}], "introductionContent": [{"text": "A crucial problem in the design of spoken dialogue systems (SDS) is to decide for incoming recognition hypotheses whether a system should accept (consider correctly recognized), reject (assume misrecognition), or ignore (classify as noise or speech not directed to the system) them.", "labels": [], "entities": [{"text": "spoken dialogue systems (SDS)", "start_pos": 35, "end_pos": 64, "type": "TASK", "confidence": 0.6724288413921992}]}, {"text": "Obviously, incorrect decisions at this point can have serious negative effects on system usability and user satisfaction.", "labels": [], "entities": []}, {"text": "On the one hand, accepting misrecognized hypotheses leads to misunderstandings and unintended system behaviors which are usually difficult to recover from.", "labels": [], "entities": []}, {"text": "On the other hand, users might get frustrated with a system that behaves too cautiously and rejects or ignores too many utterances.", "labels": [], "entities": []}, {"text": "Thus an important feature in dialogue system engineering is the tradeoff between avoiding task failure (due to misrecognitions) and promoting overall dialogue efficiency, flow, and naturalness.", "labels": [], "entities": [{"text": "dialogue system engineering", "start_pos": 29, "end_pos": 56, "type": "TASK", "confidence": 0.7636863986651102}]}, {"text": "In this paper, we investigate the use of machine learning trained on a combination of acoustic features and features computed from dialogue context to predict the quality of incoming n-best recognition hypotheses to a SDS.", "labels": [], "entities": [{"text": "SDS", "start_pos": 218, "end_pos": 221, "type": "TASK", "confidence": 0.9205051064491272}]}, {"text": "These predictions are then used to select a \"best\" hypothesis and to decide on appropriate system reactions.", "labels": [], "entities": []}, {"text": "We evaluate this approach in comparison with a baseline system that works in the standard way: always choosing the topmost hypothesis in the n-best list.", "labels": [], "entities": []}, {"text": "In such systems, complex repair strategies are required when the top hypothesis is incorrect.", "labels": [], "entities": []}, {"text": "The main novelty of this work is that we explore the use of predictions from simple statistical User Simulations to re-rank n-best lists of ASR hypotheses.", "labels": [], "entities": [{"text": "ASR hypotheses", "start_pos": 140, "end_pos": 154, "type": "TASK", "confidence": 0.8785055875778198}]}, {"text": "These User Simulations are now commonly used in statistical learning approaches to dialogue management), but they have not been used for context-sensitive ASR before.", "labels": [], "entities": [{"text": "dialogue management", "start_pos": 83, "end_pos": 102, "type": "TASK", "confidence": 0.8385177254676819}, {"text": "ASR", "start_pos": 155, "end_pos": 158, "type": "TASK", "confidence": 0.8089814186096191}]}, {"text": "In our model, the system's \"belief\" b(h) in a recognition hypothesis h is factored in two parts: the observation probability P (o|h) (approximated by the ASR confidence score) and the User Simulation probability P (h|us, C) of the hypothesis: b(h) = P (o|h).P (h|us, C) where us is the state of the User Simulation in context C.", "labels": [], "entities": [{"text": "ASR confidence score", "start_pos": 154, "end_pos": 174, "type": "METRIC", "confidence": 0.8711178104082743}]}, {"text": "The context is simply a window of di-alogue acts in the dialogue history, that the US is sensitive to (see section 3).", "labels": [], "entities": []}, {"text": "The paper is organized as follows.", "labels": [], "entities": []}, {"text": "After a short relation to previous work, we describe the data (Section 5) and derive baseline results (Section 6).", "labels": [], "entities": []}, {"text": "Section 3 describes the User Simulations that we use for re-ranking hypotheses.", "labels": [], "entities": [{"text": "User Simulations", "start_pos": 24, "end_pos": 40, "type": "TASK", "confidence": 0.7546644806861877}]}, {"text": "Section 7 describes our learning experiments for classifying and selecting from n-best recognition hypotheses and Section 9 reports our results.", "labels": [], "entities": []}], "datasetContent": [{"text": "To evaluate performance we use Dialogue Move Accuracy (DMA), a strict variant of Concept Error Rate (CER) as defined by, which takes into account the semantic aspects of the difference between the classified utterance and the true transcription.", "labels": [], "entities": [{"text": "Dialogue Move Accuracy (DMA)", "start_pos": 31, "end_pos": 59, "type": "METRIC", "confidence": 0.7203363279501597}, {"text": "Concept Error Rate (CER)", "start_pos": 81, "end_pos": 105, "type": "METRIC", "confidence": 0.805962453285853}]}, {"text": "CER is similar to WER, since it takes into account deletions, insertions and substitutions on the semantic (rather than the word) level of the utterance.", "labels": [], "entities": [{"text": "CER", "start_pos": 0, "end_pos": 3, "type": "METRIC", "confidence": 0.7653147578239441}, {"text": "WER", "start_pos": 18, "end_pos": 21, "type": "METRIC", "confidence": 0.5043071508407593}]}, {"text": "DMA is stricter than CER in the sense that it does not allow for partial matches in the semantic representation.", "labels": [], "entities": [{"text": "DMA", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.7573934197425842}]}, {"text": "In other words, if the classified utterance corresponds to the same semantic representation as the transcribed then we have 100% DMA, otherwise 0%.", "labels": [], "entities": [{"text": "DMA", "start_pos": 129, "end_pos": 132, "type": "METRIC", "confidence": 0.9190329909324646}]}, {"text": "Sentence Accuracy (SA) is the alignment of a single hypothesis in the n-best list with the true transcription.", "labels": [], "entities": [{"text": "Sentence Accuracy (SA)", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.6868793129920959}]}, {"text": "Similarly to DMA, it accounts for perfect alignment between the hypothesis and the transcription, i.e. if they match perfectly we have 100% SA, otherwise 0%.", "labels": [], "entities": [{"text": "SA", "start_pos": 140, "end_pos": 142, "type": "METRIC", "confidence": 0.9740570783615112}]}, {"text": "Experiments were conducted in two layers: the first layer concerns only the classifier, i.e. the ability of the system to correctly classify each hypothesis to either of the four labels 'opt', 'pos', 'neg', 'ign' and the second layer the re-ranker, i.e. the ability of the system to boost the speech recogniser's accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 313, "end_pos": 321, "type": "METRIC", "confidence": 0.9449661374092102}]}, {"text": "All results are drawn from the TiMBL classifier trained with the Weighted Overlap metric and k = 1 nearest neighbours settings.", "labels": [], "entities": []}, {"text": "Both layers are trained on 75% of the same Town-Info Corpus of 126 dialogues containing 60-best lists for 1554 user utterances or a total of 93240 hypotheses.", "labels": [], "entities": [{"text": "Town-Info Corpus", "start_pos": 43, "end_pos": 59, "type": "DATASET", "confidence": 0.9586013555526733}]}, {"text": "The first layer was tested against a separate Town-Info Corpus of 58 dialogues containing 510 user utterances or a total of 30600 hypotheses, while the second was tested on the whole training set with 10-fold cross-validation.", "labels": [], "entities": [{"text": "Town-Info Corpus", "start_pos": 46, "end_pos": 62, "type": "DATASET", "confidence": 0.9035158753395081}]}, {"text": "Using this corpus, a series of experiments was carried out using different sets of features in order to both determine and illustrate the increasing performance of the classifier.", "labels": [], "entities": []}, {"text": "These sets were determined not only by the literature but also by the Information Gain measures that were calculated on the training set using WEKA, as shown in table 3.", "labels": [], "entities": [{"text": "WEKA", "start_pos": 143, "end_pos": 147, "type": "DATASET", "confidence": 0.9099130630493164}]}, {"text": "We performed two series of experiments in two layers: the first corresponds to the training of the classifier alone and the second to the system as a whole measuring the re-ranker's output.: Results for the 'pos' category  In these series of experiments we measure precision, recall and F1-measure for each of the four labels and overall F1-measure and accuracy of the classifier.", "labels": [], "entities": [{"text": "precision", "start_pos": 265, "end_pos": 274, "type": "METRIC", "confidence": 0.9994444251060486}, {"text": "recall", "start_pos": 276, "end_pos": 282, "type": "METRIC", "confidence": 0.9991612434387207}, {"text": "F1-measure", "start_pos": 287, "end_pos": 297, "type": "METRIC", "confidence": 0.9987988471984863}, {"text": "F1-measure", "start_pos": 338, "end_pos": 348, "type": "METRIC", "confidence": 0.998363196849823}, {"text": "accuracy", "start_pos": 353, "end_pos": 361, "type": "METRIC", "confidence": 0.9991175532341003}]}, {"text": "In order to have a better view of the classifier's performance we have also included the confusion matrix for the final experiment with all 13 attributes.", "labels": [], "entities": []}, {"text": "show per class and per attribute set measures, while shows a collective view of the results for the four sets of attributes and the baseline being the majority class label 'neg'.", "labels": [], "entities": []}, {"text": "shows the confusion matrix for the final experiment.", "labels": [], "entities": [{"text": "confusion", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9578334093093872}]}, {"text": "In tables 4 -8 we generally notice an increase in precision, recall and F1-measure as we progressively add more attributes to the system with the exception of the addition of the Acoustic Features which seem to impair the classifier's performance.", "labels": [], "entities": [{"text": "precision", "start_pos": 50, "end_pos": 59, "type": "METRIC", "confidence": 0.9996731281280518}, {"text": "recall", "start_pos": 61, "end_pos": 67, "type": "METRIC", "confidence": 0.9997535347938538}, {"text": "F1-measure", "start_pos": 72, "end_pos": 82, "type": "METRIC", "confidence": 0.9993983507156372}]}, {"text": "We also make note of the fact that in the case of the 4th attribute set the classifier can distinguish very well the 'neg' and 'ign' categories with 86.3% and 99.9% F1-measure respectively.", "labels": [], "entities": [{"text": "F1-measure", "start_pos": 165, "end_pos": 175, "type": "METRIC", "confidence": 0.9967696666717529}]}, {"text": "Most importantly, we observe a remarkable boost in F1-measure and accuracy with the addition of the User Simulation score.", "labels": [], "entities": [{"text": "F1-measure", "start_pos": 51, "end_pos": 61, "type": "METRIC", "confidence": 0.999460756778717}, {"text": "accuracy", "start_pos": 66, "end_pos": 74, "type": "METRIC", "confidence": 0.9991760849952698}, {"text": "User Simulation", "start_pos": 100, "end_pos": 115, "type": "TASK", "confidence": 0.6495988070964813}]}, {"text": "We find a 37.36% relative increase in F1-measure and 34.02% increase.", "labels": [], "entities": [{"text": "F1-measure", "start_pos": 38, "end_pos": 48, "type": "METRIC", "confidence": 0.9993841648101807}]}, {"text": "Taking a closer look at the 4th experiment with all 13 features we notice in table 9 that most errors occur between the 'pos' and 'neg' category.", "labels": [], "entities": []}, {"text": "In fact, for the 'neg' category the False Positive Rate (FPR) is 18.17% and for the 'pos' 8.9%, all in all a lot larger than for the other categories.", "labels": [], "entities": [{"text": "False Positive Rate (FPR)", "start_pos": 36, "end_pos": 61, "type": "METRIC", "confidence": 0.9579256276289622}]}, {"text": "In these experiments we measure WER, DMA and SA for the system as a whole.", "labels": [], "entities": [{"text": "WER", "start_pos": 32, "end_pos": 35, "type": "METRIC", "confidence": 0.9983246922492981}, {"text": "DMA", "start_pos": 37, "end_pos": 40, "type": "METRIC", "confidence": 0.9865374565124512}, {"text": "SA", "start_pos": 45, "end_pos": 47, "type": "METRIC", "confidence": 0.9982506632804871}]}, {"text": "In order to make sure that the improvement noted was really attributed to the classifier we computed the p-values for each of these measures using the Wilcoxon signed rank test for WER and McNemar chi-square test for the DMA and SA measures.", "labels": [], "entities": [{"text": "WER", "start_pos": 181, "end_pos": 184, "type": "DATASET", "confidence": 0.48944908380508423}, {"text": "DMA", "start_pos": 221, "end_pos": 224, "type": "DATASET", "confidence": 0.879020631313324}]}, {"text": "In    Finally, we also notice that the Oracle has a 80.20% for the DMA, which means that 19.80% of the n-best lists did not include at all a hypothesis that matched semantically to the true transcript.", "labels": [], "entities": [{"text": "DMA", "start_pos": 67, "end_pos": 70, "type": "DATASET", "confidence": 0.8504763245582581}]}, {"text": "We trained a Memory Based Classifier based only on the higher level features of merely the User Simulation score and the Grammar Parsability (US + GP).", "labels": [], "entities": [{"text": "User Simulation", "start_pos": 91, "end_pos": 106, "type": "TASK", "confidence": 0.6465799361467361}, {"text": "Grammar Parsability (US + GP)", "start_pos": 121, "end_pos": 150, "type": "METRIC", "confidence": 0.7695962190628052}]}, {"text": "The idea behind this choice is to try and find a combination of features that ignores low level characteristics of the user's utterances as well as features that heavily rely on the speech recogniser and thus by default are not considered to be very trustworthy.", "labels": [], "entities": []}, {"text": "Quite surprisingly, the results taken from an experiment with just the User Simulation score and the Grammar Parsability are very promising and comparable with those acquired from the 4th experiment with all 13 attributes.", "labels": [], "entities": [{"text": "User Simulation", "start_pos": 71, "end_pos": 86, "type": "TASK", "confidence": 0.6155647486448288}, {"text": "Parsability", "start_pos": 109, "end_pos": 120, "type": "METRIC", "confidence": 0.4633539915084839}]}, {"text": "shows the precision, recall and F1-measure per label and table 12 illustrates the classifier's performance in comparison with the 4th experiment.", "labels": [], "entities": [{"text": "precision", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9996534585952759}, {"text": "recall", "start_pos": 21, "end_pos": 27, "type": "METRIC", "confidence": 0.9993657469749451}, {"text": "F1-measure", "start_pos": 32, "end_pos": 42, "type": "METRIC", "confidence": 0.9987086057662964}]}, {"text": "erable decrease in the recall and a corresponding increase in the precision of the 'pos' and 'opt' categories compared to the LF + CHF + AF + US attribute set, which account for lower F1-measures.", "labels": [], "entities": [{"text": "erable", "start_pos": 0, "end_pos": 6, "type": "METRIC", "confidence": 0.967860758304596}, {"text": "recall", "start_pos": 23, "end_pos": 29, "type": "METRIC", "confidence": 0.9985688924789429}, {"text": "precision", "start_pos": 66, "end_pos": 75, "type": "METRIC", "confidence": 0.9993568062782288}, {"text": "F1-measures", "start_pos": 184, "end_pos": 195, "type": "METRIC", "confidence": 0.9925041198730469}]}, {"text": "However, all in all the US + GP set manages to classify correctly 207 more vectors and quite interestingly commits far fewer ties and manages to resolve more compared to the full 13 attribute set.", "labels": [], "entities": [{"text": "US + GP set", "start_pos": 24, "end_pos": 35, "type": "DATASET", "confidence": 0.9414671361446381}]}], "tableCaptions": [{"text": " Table 2: Baseline and Oracle results (statistically  significant at p < 0.001)", "labels": [], "entities": []}, {"text": " Table 4: Results for the 'opt' category", "labels": [], "entities": [{"text": "opt' category", "start_pos": 27, "end_pos": 40, "type": "TASK", "confidence": 0.8788711428642273}]}, {"text": " Table 5: Results for the 'pos' category", "labels": [], "entities": []}, {"text": " Table 6: Results for the 'neg' category", "labels": [], "entities": []}, {"text": " Table 7: Results for the 'ign' category", "labels": [], "entities": []}, {"text": " Table 8: F1-Measure and Accuracy for the four  attribute sets", "labels": [], "entities": [{"text": "F1-Measure", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9985498785972595}, {"text": "Accuracy", "start_pos": 25, "end_pos": 33, "type": "METRIC", "confidence": 0.9635394215583801}]}, {"text": " Table 9: Confusion Matrix for LF+CHF+AF+US", "labels": [], "entities": []}, {"text": " Table 10: Baseline, Classifier, and Oracle results  (*** = p < 0.001, ** = p < 0.01, * = p < 0.05)", "labels": [], "entities": []}, {"text": " Table 11: Precision, Recall and F1: high-level fea- tures", "labels": [], "entities": [{"text": "Precision", "start_pos": 11, "end_pos": 20, "type": "METRIC", "confidence": 0.9924190044403076}, {"text": "Recall", "start_pos": 22, "end_pos": 28, "type": "METRIC", "confidence": 0.9942077398300171}, {"text": "F1", "start_pos": 33, "end_pos": 35, "type": "METRIC", "confidence": 0.9986120462417603}]}, {"text": " Table 12: F1, Accuracy and number of ties cor- rectly resolved for LF+CHF+AF+US and US+GP  feature sets", "labels": [], "entities": [{"text": "F1", "start_pos": 11, "end_pos": 13, "type": "METRIC", "confidence": 0.9997664093971252}, {"text": "Accuracy", "start_pos": 15, "end_pos": 23, "type": "METRIC", "confidence": 0.9995439648628235}]}]}