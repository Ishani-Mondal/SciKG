{"title": [], "abstractContent": [{"text": "Sense induction seeks to automatically identify word senses directly from a corpus.", "labels": [], "entities": [{"text": "Sense induction", "start_pos": 0, "end_pos": 15, "type": "TASK", "confidence": 0.8128655552864075}]}, {"text": "A key assumption underlying previous work is that the context surrounding an ambiguous word is indicative of its meaning.", "labels": [], "entities": []}, {"text": "Sense induction is thus typically viewed as an unsupervised clustering problem where the aim is to partition a word's contexts into different classes, each representing a word sense.", "labels": [], "entities": [{"text": "Sense induction", "start_pos": 0, "end_pos": 15, "type": "TASK", "confidence": 0.8079149425029755}]}, {"text": "Our work places sense induction in a Bayesian context by modeling the contexts of the ambiguous word as samples from a multi-nomial distribution over senses which are in turn characterized as distributions over words.", "labels": [], "entities": [{"text": "sense induction", "start_pos": 16, "end_pos": 31, "type": "TASK", "confidence": 0.7525196969509125}]}, {"text": "The Bayesian framework provides a principled way to incorporate a wide range of features beyond lexical co-occurrences and to systematically assess their utility on the sense induction task.", "labels": [], "entities": [{"text": "sense induction task", "start_pos": 169, "end_pos": 189, "type": "TASK", "confidence": 0.8190993070602417}]}, {"text": "The proposed approach yields improvements over state-of-the-art systems on a benchmark dataset.", "labels": [], "entities": []}], "introductionContent": [{"text": "Sense induction is the task of discovering automatically all possible senses of an ambiguous word.", "labels": [], "entities": [{"text": "Sense induction", "start_pos": 0, "end_pos": 15, "type": "TASK", "confidence": 0.8089470565319061}]}, {"text": "It is related to, but distinct from, word sense disambiguation (WSD) where the senses are assumed to be known and the aim is to identify the intended meaning of the ambiguous word in context.", "labels": [], "entities": [{"text": "word sense disambiguation (WSD)", "start_pos": 37, "end_pos": 68, "type": "TASK", "confidence": 0.8169054090976715}]}, {"text": "Although the bulk of previous work has been devoted to the disambiguation problem 1 , there are good reasons to believe that sense induction maybe able to overcome some of the issues associated with WSD.", "labels": [], "entities": [{"text": "sense induction", "start_pos": 125, "end_pos": 140, "type": "TASK", "confidence": 0.8125956058502197}, {"text": "WSD", "start_pos": 199, "end_pos": 202, "type": "TASK", "confidence": 0.8598763346672058}]}, {"text": "Since most disambiguation methods assign senses according to, and with the aid of, dictionaries or other lexical resources, it is difficult to adapt them to new domains or to languages where such resources are scarce.", "labels": [], "entities": []}, {"text": "A related problem concerns the granularity of the sense distinctions which is fixed, and may not be entirely suitable for different applications.", "labels": [], "entities": []}, {"text": "In contrast, when sense distinctions are inferred directly from the data, they are more likely to represent the task and domain at hand.", "labels": [], "entities": []}, {"text": "There is little risk that an important sense will be left out, or that irrelevant senses will influence the results.", "labels": [], "entities": []}, {"text": "Furthermore, recent work in machine translation () and information retrieval indicates that induced senses can lead to improved performance in areas where methods based on a fixed sense inventory have previously failed.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 28, "end_pos": 47, "type": "TASK", "confidence": 0.8184199929237366}, {"text": "information retrieval", "start_pos": 55, "end_pos": 76, "type": "TASK", "confidence": 0.7860511243343353}]}, {"text": "Sense induction is typically treated as an unsupervised clustering problem.", "labels": [], "entities": [{"text": "Sense induction", "start_pos": 0, "end_pos": 15, "type": "TASK", "confidence": 0.8434379696846008}]}, {"text": "The input to the clustering algorithm are instances of the ambiguous word with their accompanying contexts (represented by co-occurrence vectors) and the output is a grouping of these instances into classes corresponding to the induced senses.", "labels": [], "entities": []}, {"text": "In other words, contexts that are grouped together in the same class represent a specific word sense.", "labels": [], "entities": []}, {"text": "In this paper we adopt a novel Bayesian approach and formalize the induction problem in a generative model.", "labels": [], "entities": []}, {"text": "For each ambiguous word we first draw a distribution over senses, and then generate context words according to this distribution.", "labels": [], "entities": []}, {"text": "It is thus assumed that different senses will correspond to distinct lexical distributions.", "labels": [], "entities": []}, {"text": "In this framework, sense distinctions arise naturally through the generative process: our model postulates that the observed data (word contexts) are explicitly intended to communicate a latent structure (their meaning).", "labels": [], "entities": []}, {"text": "Our work is related to Latent Dirichlet Allocation (LDA,), a probabilistic model of text generation.", "labels": [], "entities": [{"text": "Latent Dirichlet Allocation (LDA", "start_pos": 23, "end_pos": 55, "type": "METRIC", "confidence": 0.7263492345809937}, {"text": "text generation", "start_pos": 84, "end_pos": 99, "type": "TASK", "confidence": 0.7671680152416229}]}, {"text": "LDA models each document using a mixture over K topics, which are in turn characterized as distributions over words.", "labels": [], "entities": []}, {"text": "The words in the document are generated by repeatedly sampling a topic according to the topic distribution, and selecting a word given the chosen topic.", "labels": [], "entities": []}, {"text": "Whereas LDA generates words from global topics corresponding to the whole document, our model generates words from local topics chosen based on a context window around the ambiguous word.", "labels": [], "entities": []}, {"text": "Document-level topics resemble general domain labels (e.g., finance, education) and cannot faithfully model more fine-grained meaning distinctions.", "labels": [], "entities": []}, {"text": "In our work, therefore, we create an individual model for every (ambiguous) word rather than a global model for an entire document collection.", "labels": [], "entities": []}, {"text": "We also show how multiple information sources can be straightforwardly integrated without changing the underlying probabilistic model.", "labels": [], "entities": []}, {"text": "For instance, besides lexical information we may want to consider parts of speech or dependencies in our sense induction problem.", "labels": [], "entities": []}, {"text": "This is in marked contrast with previous LDA-based models which mostly take only word-based information into account.", "labels": [], "entities": []}, {"text": "We evaluate our model on a recently released benchmark dataset and demonstrate improvements over the state-of-the-art.", "labels": [], "entities": []}, {"text": "The remainder of this paper is structured as follows.", "labels": [], "entities": []}, {"text": "We first present an overview of related work (Section 2) and then describe our Bayesian model in more detail (Sections 3 and 4).", "labels": [], "entities": []}, {"text": "Section 5 describes the resources and evaluation methodology used in our experiments.", "labels": [], "entities": []}, {"text": "We discuss our results in Section 6, and conclude in Section 7.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section we discuss our experimental set-up for assessing the performance of the model presented above.", "labels": [], "entities": []}, {"text": "We give details on our training procedure, describe our features, and explain how our system output was evaluated.", "labels": [], "entities": []}, {"text": "Data In this work, we focus solely on inducing senses for nouns, since they constitute the largest portion of content words.", "labels": [], "entities": []}, {"text": "For example, nouns represent 45% of the content words in the British National Corpus.", "labels": [], "entities": [{"text": "British National Corpus", "start_pos": 61, "end_pos": 84, "type": "DATASET", "confidence": 0.9428415894508362}]}, {"text": "Moreover, for many tasks and applications (e.g., web queries, Jansen et al.", "labels": [], "entities": []}, {"text": "2000) nouns are the most frequent and most important part-of-speech.", "labels": [], "entities": []}, {"text": "For evaluation, we used the Semeval-2007 benchmark dataset released as part of the sense induction and discrimination task.", "labels": [], "entities": [{"text": "Semeval-2007 benchmark dataset", "start_pos": 28, "end_pos": 58, "type": "DATASET", "confidence": 0.6935470501581827}, {"text": "sense induction and discrimination task", "start_pos": 83, "end_pos": 122, "type": "TASK", "confidence": 0.7335627794265747}]}, {"text": "The dataset contains texts from the Penn Treebank II corpus, a collection of articles from the first half of the 1989 Wall Street Journal (WSJ).", "labels": [], "entities": [{"text": "Penn Treebank II corpus, a collection of articles from the first half of the 1989 Wall Street Journal (WSJ)", "start_pos": 36, "end_pos": 143, "type": "DATASET", "confidence": 0.8715124455365267}]}, {"text": "It is hand-annotated with OntoNotes senses () and has 35 nouns.", "labels": [], "entities": []}, {"text": "The average noun ambiguity is 3.9, with a high (almost 80%) skew towards the predominant sense.", "labels": [], "entities": [{"text": "ambiguity", "start_pos": 17, "end_pos": 26, "type": "METRIC", "confidence": 0.8297705054283142}]}, {"text": "This is not entirely surprising since OntoNotes senses are less fine-grained than WordNet senses.", "labels": [], "entities": []}, {"text": "We used two corpora for training as we wanted to evaluate our model's performance across different domains.", "labels": [], "entities": []}, {"text": "The British National Corpus (BNC) is a 100 million word collection of samples of written and spoken language from a wide range of sources including newspapers, magazines, books (both academic and fiction), letters, and school essays as well as spontaneous conversations.", "labels": [], "entities": [{"text": "British National Corpus (BNC) is a 100 million word collection of samples of written and spoken language from a wide range of sources including newspapers, magazines, books (both academic and fiction), letters, and school essays as well as spontaneous conversations", "start_pos": 4, "end_pos": 269, "type": "Description", "confidence": 0.8225810832033554}]}, {"text": "This served as our out-of-domain corpus, and contained approximately 730 thousand instances of the 35 target nouns in the Semeval lexical sample.", "labels": [], "entities": [{"text": "Semeval lexical sample", "start_pos": 122, "end_pos": 144, "type": "DATASET", "confidence": 0.6924446225166321}]}, {"text": "The second, in-domain, corpus was built from selected portions of the Wall Street Journal.", "labels": [], "entities": [{"text": "Wall Street Journal", "start_pos": 70, "end_pos": 89, "type": "DATASET", "confidence": 0.9667716224988302}]}, {"text": "We used all articles (excluding the Penn Treebank II portion used in the Semeval dataset) from the years 1987-89 and 1994 to create a corpus of similar size to the BNC, containing approximately 740 thousand instances of the target words.", "labels": [], "entities": [{"text": "Penn Treebank II portion", "start_pos": 36, "end_pos": 60, "type": "DATASET", "confidence": 0.981094092130661}, {"text": "Semeval dataset", "start_pos": 73, "end_pos": 88, "type": "DATASET", "confidence": 0.7961956262588501}, {"text": "BNC", "start_pos": 164, "end_pos": 167, "type": "DATASET", "confidence": 0.9620856046676636}]}, {"text": "Additionally, we used the Senseval 2 and 3 lexical sample data) as development sets, for experimenting with the hyper-parameters of our model (see Section 6).", "labels": [], "entities": [{"text": "Senseval 2 and 3 lexical sample data", "start_pos": 26, "end_pos": 62, "type": "DATASET", "confidence": 0.8167499899864197}]}, {"text": "Evaluation Methodology Agirre and Soroa (2007) present two evaluation schemes for assessing sense induction methods.", "labels": [], "entities": [{"text": "sense induction", "start_pos": 92, "end_pos": 107, "type": "TASK", "confidence": 0.7175257802009583}]}, {"text": "Under the first scheme, the system output is compared to the gold standard using standard clustering evaluation metrics (e.g., purity, entropy).", "labels": [], "entities": []}, {"text": "Here, no attempt is made to match the induced senses against the labels of the gold standard.", "labels": [], "entities": [{"text": "gold standard", "start_pos": 79, "end_pos": 92, "type": "DATASET", "confidence": 0.829542338848114}]}, {"text": "Under the second scheme, the gold standard is partitioned into a test and training corpus.", "labels": [], "entities": []}, {"text": "The latter is used to derive a mapping of the induced senses to the gold standard labels.", "labels": [], "entities": []}, {"text": "The mapping is then used to calculate the system's F-Score on the test corpus.", "labels": [], "entities": [{"text": "F-Score", "start_pos": 51, "end_pos": 58, "type": "METRIC", "confidence": 0.9958840012550354}]}, {"text": "Unfortunately, the first scheme failed to discriminate among participating systems.", "labels": [], "entities": []}, {"text": "The onecluster-per-word baseline outperformed all systems, except one, which was only marginally better.", "labels": [], "entities": []}, {"text": "The scheme ignores the actual labeling and due to the dominance of the first sense in the data, encourages a single-sense approach which is further amplified by the use of a coarse-grained sense inventory.", "labels": [], "entities": []}, {"text": "For the purposes of this work, therefore, we focused on the second evaluation scheme.", "labels": [], "entities": []}, {"text": "Here, most of the participating systems outperformed the most-frequent-sense baseline, and the rest obtained only slightly lower scores.", "labels": [], "entities": []}, {"text": "Feature Space Our experiments used a feature set designed to capture both immediate local context, wider context and syntactic context.", "labels": [], "entities": []}, {"text": "Specifically, we experimented with six feature categories: \u00b110-word window (10w), \u00b15-word window (5w), collocations (1w), word n-grams (ng), part-ofspeech n-grams (pg) and dependency relations (dp).", "labels": [], "entities": []}, {"text": "These features have been widely adopted in various WSD algorithms (see Lee and Ng 2002 fora detailed evaluation).", "labels": [], "entities": [{"text": "WSD", "start_pos": 51, "end_pos": 54, "type": "TASK", "confidence": 0.9879930019378662}]}, {"text": "In all cases, we use the lemmatized version of the word(s).", "labels": [], "entities": []}, {"text": "The Semeval workshop organizers provided a small amount of context for each instance (usually a sentence or two surrounding the sentence containing the target word).", "labels": [], "entities": []}, {"text": "This context, as well as the text in the training corpora, was parsed using RASP (), to extract part-of-speech tags, lemmas, and dependency information.", "labels": [], "entities": [{"text": "RASP", "start_pos": 76, "end_pos": 80, "type": "METRIC", "confidence": 0.8410875797271729}]}, {"text": "For instances containing more than one occurrence of the target word, we disambiguate the first occurrence.", "labels": [], "entities": []}, {"text": "Instances which were not correctly recognized by the parser (e.g., a target word labeled with the wrong lemma or part-of-speech), were automatically assigned to the largest sensecluster.", "labels": [], "entities": []}, {"text": "3 This was the case for less than 1% of the instances.", "labels": [], "entities": []}, {"text": "Model Selection The framework presented in Section 3 affords great flexibility in modeling the empirical data.", "labels": [], "entities": [{"text": "Model Selection", "start_pos": 0, "end_pos": 15, "type": "TASK", "confidence": 0.7757671475410461}]}, {"text": "This however entails that several parameters must be instantiated.", "labels": [], "entities": []}, {"text": "More precisely, our model is conditioned on the Dirichlet hyperparameters \u03b1 and \u03b2 and the number of senses S.", "labels": [], "entities": []}, {"text": "Additional parameters include the number of iterations for the Gibbs sampler and whether or not the layers are assigned different weights.", "labels": [], "entities": []}, {"text": "Our strategy in this paper is to fix \u03b1 and \u03b2 and explore the consequences of varying S.", "labels": [], "entities": []}, {"text": "The value for the \u03b1 hyperparameter was set to 0.02.", "labels": [], "entities": []}, {"text": "This was optimized in an independent tuning experiment which used the Senseval 2 (Preiss and Yarowsky, 2001) and Senseval 3 (Mihalcea and Edmonds, 2004) datasets.", "labels": [], "entities": [{"text": "Senseval 3 (Mihalcea and Edmonds, 2004) datasets", "start_pos": 113, "end_pos": 161, "type": "DATASET", "confidence": 0.624846214056015}]}, {"text": "We experimented with \u03b1 values ranging from 0.005 to 1.", "labels": [], "entities": []}, {"text": "The \u03b2 parameter was set to 0.1 (in all layers).", "labels": [], "entities": []}, {"text": "This value is often considered optimal in LDA-related models).", "labels": [], "entities": []}, {"text": "For simplicity, we used uniform weights for the layers.", "labels": [], "entities": []}, {"text": "The Gibbs sampler was run for 2,000 iterations.", "labels": [], "entities": [{"text": "Gibbs sampler", "start_pos": 4, "end_pos": 17, "type": "DATASET", "confidence": 0.8696762323379517}]}, {"text": "Due to the randomized nature of the inference procedure, all reported results are average scores over ten runs.", "labels": [], "entities": []}, {"text": "Our experiments used the same number of senses for all the words, since tuning this number individually for each word would be prohibitive.", "labels": [], "entities": []}, {"text": "We experimented with values ranging from three to nine senses.", "labels": [], "entities": []}, {"text": "shows the results obtained for different numbers of senses when the model is trained on the WSJ (in-domain) and BNC (out-ofdomain) corpora, respectively.", "labels": [], "entities": [{"text": "WSJ", "start_pos": 92, "end_pos": 95, "type": "DATASET", "confidence": 0.6607177257537842}, {"text": "BNC", "start_pos": 112, "end_pos": 115, "type": "METRIC", "confidence": 0.9670656323432922}]}, {"text": "Here, we are using the optimal combination of layers for each system (which we discuss in the following section in de-: Senses inferred for the word drug from the WSJ and BNC corpora. tail).", "labels": [], "entities": [{"text": "WSJ and BNC corpora. tail", "start_pos": 163, "end_pos": 188, "type": "DATASET", "confidence": 0.7040975272655488}]}, {"text": "For the model trained on WSJ, performance peaks at four senses, which is similar to the average ambiguity in the test data.", "labels": [], "entities": [{"text": "WSJ", "start_pos": 25, "end_pos": 28, "type": "TASK", "confidence": 0.615856409072876}]}, {"text": "For the model trained on the BNC, however, the best results are obtained using twice as many senses.", "labels": [], "entities": [{"text": "BNC", "start_pos": 29, "end_pos": 32, "type": "DATASET", "confidence": 0.9467790126800537}]}, {"text": "Using fewer senses with the BNC-trained system can result in a drop inaccuracy of almost 2%.", "labels": [], "entities": [{"text": "BNC-trained", "start_pos": 28, "end_pos": 39, "type": "DATASET", "confidence": 0.711867094039917}, {"text": "inaccuracy", "start_pos": 68, "end_pos": 78, "type": "METRIC", "confidence": 0.6642680764198303}]}, {"text": "This is due to the shift in domain.", "labels": [], "entities": []}, {"text": "As the sense-divisions of the learning domain do not match those of the target domain, finer granularity is required in order to encompass all the relevant distinctions.", "labels": [], "entities": []}, {"text": "illustrates the senses inferred for the word drug when using the in-domain and out-ofdomain corpora, respectively.", "labels": [], "entities": []}, {"text": "The most probable words for each sense are also shown.", "labels": [], "entities": []}, {"text": "Firstly, note that the model infers some plausible senses for drug on the WSJ corpus (top half of).", "labels": [], "entities": [{"text": "WSJ corpus", "start_pos": 74, "end_pos": 84, "type": "DATASET", "confidence": 0.9682497680187225}]}, {"text": "Sense 1 corresponds to the \"enforcement\" sense of drug, Sense 2 refers to \"medication\", Sense 3 to the \"drug industry\" and Sense 4 to \"drugs research\".", "labels": [], "entities": []}, {"text": "The inferred senses for drug on the BNC (bottom half of) are more fine grained.", "labels": [], "entities": [{"text": "BNC", "start_pos": 36, "end_pos": 39, "type": "METRIC", "confidence": 0.6162815093994141}]}, {"text": "For example, the model finds distinct senses for \"medication\" (Sense 1 and 7) and \"illegal substance\" (Senses 2, 4, 6, 7).", "labels": [], "entities": []}, {"text": "It also finds a separate sense for \"drug dealing\" (Sense 5) and \"enforcement\" (Sense 8).", "labels": [], "entities": [{"text": "drug dealing", "start_pos": 36, "end_pos": 48, "type": "TASK", "confidence": 0.6904204040765762}]}, {"text": "Because the BNC has a broader focus, finer distinctions are needed to cover as many senses as possible that are relevant to the target domain (WSJ).", "labels": [], "entities": []}, {"text": "Layer Analysis We next examine which individual feature categories are most informative in our sense induction task.", "labels": [], "entities": [{"text": "sense induction task", "start_pos": 95, "end_pos": 115, "type": "TASK", "confidence": 0.833417812983195}]}, {"text": "We also investigate whether their combination, through our layered 1-Layer 10w 86.9 5w 86.8 1w 84.6 ng 83.6 pg 82.5 dp 82.2 MFS 80.9 5-Layers -10w 83.1 -5w 83.0 -1w 83.0 -ng 83.0 -pg 82.7 -dp: Model performance (F-score) on the WSJ with one layer (left), five layers (middle), and selected combinations of layers (right).", "labels": [], "entities": [{"text": "F-score", "start_pos": 212, "end_pos": 219, "type": "METRIC", "confidence": 0.9905666708946228}, {"text": "WSJ", "start_pos": 228, "end_pos": 231, "type": "DATASET", "confidence": 0.9658668637275696}]}, {"text": "model (see), yields performance improvements.", "labels": [], "entities": []}, {"text": "We used 4 senses for the system trained on WSJ and 8 for the system trained on the BNC (\u03b1 was set to 0.02 and \u03b2 to 0.1) (left side) shows the performance of our model when using only one layer.", "labels": [], "entities": [{"text": "WSJ", "start_pos": 43, "end_pos": 46, "type": "DATASET", "confidence": 0.946404218673706}, {"text": "BNC", "start_pos": 83, "end_pos": 86, "type": "METRIC", "confidence": 0.5010032653808594}]}, {"text": "The layer composed of words co-occurring within a \u00b110-word window (10w), and representing wider, topical, information gives the highest scores on its own.", "labels": [], "entities": []}, {"text": "It is followed by the \u00b15 (5w) and \u00b11 (1w) word windows, which represent more immediate, local context.", "labels": [], "entities": []}, {"text": "Part-of-speech n-grams (pg) and word ngrams (ng), on their own, achieve lower scores, largely due to over-generalization and data sparseness, respectively.", "labels": [], "entities": []}, {"text": "The lowest-scoring single layer is the dependency layer (dp), with performance only slightly above the most-frequent-sense baseline (MFS).", "labels": [], "entities": []}, {"text": "Dependency information is very informative when present, but extremely sparse.", "labels": [], "entities": []}, {"text": "(middle) also shows the results obtained when running the layered model with all but one of the layers as input.", "labels": [], "entities": []}, {"text": "We can use this information to determine the contribution of each layer by comparing to the combined model with all layers (all).", "labels": [], "entities": []}, {"text": "Because we are dealing with multiple layers, there is an element of overlap involved.", "labels": [], "entities": []}, {"text": "Therefore, each of the word-window layers, despite relatively high informativeness on its own, does not cause as much damage when it is absent, since the other layers compensate for the topical and local information.", "labels": [], "entities": []}, {"text": "The absence of the word n-gram layer, which provides specific local information, does not make a great impact when the 1w and pg layers are present.", "labels": [], "entities": []}, {"text": "Finally, we can see that the extremely sparse dependency layer is detrimental to the multi-layer model as a whole, and its removal increases performance.", "labels": [], "entities": []}, {"text": "The sparsity of the data in this layer means that there is often little information on which to base a decision.", "labels": [], "entities": []}, {"text": "In these cases, the layer contributes a close-to-uniform estimation 1-Layer 10w 84.6 5w 84.6 1w 83.6 pg 83.1 ng 82.8 dp 81.1 MFS 80.9: Model performance (F-score) on the BNC with one layer (left), five layers (middle), and selected combinations of layers (right).", "labels": [], "entities": [{"text": "estimation 1-Layer 10w 84.6 5w 84.6 1w 83.6 pg 83.1 ng 82.8 dp 81.1 MFS 80.9", "start_pos": 57, "end_pos": 133, "type": "METRIC", "confidence": 0.7123540621250868}, {"text": "F-score", "start_pos": 154, "end_pos": 161, "type": "METRIC", "confidence": 0.9489343166351318}]}], "tableCaptions": [{"text": " Table 2: Model performance (F-score) on the WSJ  with one layer (left), five layers (middle), and se- lected combinations of layers (right).", "labels": [], "entities": [{"text": "F-score", "start_pos": 29, "end_pos": 36, "type": "METRIC", "confidence": 0.9933664202690125}, {"text": "WSJ", "start_pos": 45, "end_pos": 48, "type": "DATASET", "confidence": 0.9379404187202454}]}, {"text": " Table 3: Model performance (F-score) on the BNC  with one layer (left), five layers (middle), and se- lected combinations of layers (right).", "labels": [], "entities": [{"text": "F-score", "start_pos": 29, "end_pos": 36, "type": "METRIC", "confidence": 0.9904392957687378}, {"text": "BNC", "start_pos": 45, "end_pos": 48, "type": "DATASET", "confidence": 0.8738386631011963}]}]}