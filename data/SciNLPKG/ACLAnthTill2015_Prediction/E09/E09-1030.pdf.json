{"title": [{"text": "Reconstructing false start errors in spontaneous speech text", "labels": [], "entities": [{"text": "Reconstructing false", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.8324534595012665}]}], "abstractContent": [{"text": "This paper presents a conditional random field-based approach for identifying speaker-produced disfluencies (i.e. if and where they occur) in spontaneous speech transcripts.", "labels": [], "entities": []}, {"text": "We emphasize false start regions , which are often missed in current disfluency identification approaches as they lack lexical or structural similarity to the speech immediately following.", "labels": [], "entities": [{"text": "disfluency identification", "start_pos": 69, "end_pos": 94, "type": "TASK", "confidence": 0.722322940826416}]}, {"text": "We find that combining lexical, syntactic , and language model-related features with the output of a state-of-the-art disflu-ency identification system improves overall word-level identification of these and other errors.", "labels": [], "entities": [{"text": "word-level identification", "start_pos": 169, "end_pos": 194, "type": "TASK", "confidence": 0.6602164208889008}]}, {"text": "Improvements are reinforced under a stricter evaluation metric requiring exact matches between cleaned sentences annotator-produced reconstructions, and altogether show promise for general reconstruction efforts.", "labels": [], "entities": []}], "introductionContent": [{"text": "The output of an automatic speech recognition (ASR) system is often not what is required for subsequent processing, in part because speakers themselves often make mistakes (e.g. stuttering, selfcorrecting, or using filler words).", "labels": [], "entities": [{"text": "automatic speech recognition (ASR)", "start_pos": 17, "end_pos": 51, "type": "TASK", "confidence": 0.8114215234915415}]}, {"text": "A cleaner speech transcript would allow for more accurate language processing as needed for natural language processing tasks such as machine translation and conversation summarization which often assume a grammatical sentence as input.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 134, "end_pos": 153, "type": "TASK", "confidence": 0.7945708334445953}, {"text": "conversation summarization", "start_pos": 158, "end_pos": 184, "type": "TASK", "confidence": 0.696127250790596}]}, {"text": "A system would accomplish reconstruction of its spontaneous speech input if its output were to represent, in flawless, fluent, and contentpreserving text, the message that the speaker intended to convey.", "labels": [], "entities": []}, {"text": "Such a system could also be applied not only to spontaneous English speech, but to correct common mistakes made by non-native speakers (), and possibly extended to non-English speaker errors.", "labels": [], "entities": []}, {"text": "A key motivation for this work is the hope that a cleaner, reconstructed speech transcript will allow for simpler and more accurate human and natural language processing, as needed for applications like machine translation, question answering, text summarization, and paraphrasing which often assume a grammatical sentence as input.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 203, "end_pos": 222, "type": "TASK", "confidence": 0.7880789041519165}, {"text": "question answering", "start_pos": 224, "end_pos": 242, "type": "TASK", "confidence": 0.884844571352005}, {"text": "text summarization", "start_pos": 244, "end_pos": 262, "type": "TASK", "confidence": 0.7562861442565918}]}, {"text": "This benefit has been directly demonstrated for statistical machine translation (SMT).", "labels": [], "entities": [{"text": "statistical machine translation (SMT)", "start_pos": 48, "end_pos": 85, "type": "TASK", "confidence": 0.8373254934946696}]}, {"text": "gave evidence that simple disfluency removal from transcripts can improve BLEU (a standard SMT evaluation metric) up to 8% for sentences with disfluencies.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 74, "end_pos": 78, "type": "METRIC", "confidence": 0.9988840222358704}, {"text": "SMT evaluation", "start_pos": 91, "end_pos": 105, "type": "TASK", "confidence": 0.8932910263538361}]}, {"text": "The presence of disfluencies were found to hurt SMT in two ways: making utterances longer without adding semantic content (and sometimes adding false content) and exacerbating the data mismatch between the spontaneous input and the clean text training data.", "labels": [], "entities": [{"text": "SMT", "start_pos": 48, "end_pos": 51, "type": "TASK", "confidence": 0.992028534412384}]}, {"text": "While full speech reconstruction would likely require a range of string transformations and potentially deep syntactic and semantic analysis of the errorful text, in this work we will first attempt to resolve less complex errors, corrected by deletion alone, in a given manuallytranscribed utterance.", "labels": [], "entities": [{"text": "full speech reconstruction", "start_pos": 6, "end_pos": 32, "type": "TASK", "confidence": 0.6469971040884653}]}, {"text": "We build on efforts from ), aiming to improve overall recall -especially of false start or non-copy errors -while concurrently maintaining or improving precision.", "labels": [], "entities": [{"text": "recall", "start_pos": 54, "end_pos": 60, "type": "METRIC", "confidence": 0.9909260272979736}, {"text": "precision", "start_pos": 152, "end_pos": 161, "type": "METRIC", "confidence": 0.9986594915390015}]}], "datasetContent": [{"text": "In these experiments, we attempt to label the following word-boundary classes as annotated in SSR corpus: \u2022 fillers (FL), including filled pauses and discourse markers (\u223c5.6% of words) \u2022 rough copy (RC) edit (reparandum incorporates the same or very similar words in roughly the same word order, including repetitions and some revisions) (\u223c4.6% of words) \u2022 non-copy (NC) edit (a speaker error where the reparandum has no lexical or structural relationship to the repair region following, as seen in restart fragments and some revisions) (\u223c3.2% of words) Other labels annotated in the SSR corpus (such as insertions and word reorderings), have been ignored for these error tagging experiments.", "labels": [], "entities": [{"text": "SSR corpus", "start_pos": 94, "end_pos": 104, "type": "DATASET", "confidence": 0.8728828728199005}, {"text": "SSR corpus", "start_pos": 584, "end_pos": 594, "type": "DATASET", "confidence": 0.8892923891544342}]}, {"text": "We approach our training of CRFs in several ways, detailed in.", "labels": [], "entities": []}, {"text": "In half of our experiments (#1, 3, and 4), we trained a single model to predict all three annotated classes (as defined at the beginning of Section 3.3), and in the other half (#2, 5, and 6), we trained the model to predict NCs only, NCs and FLs, RCs only, or RCs and FLs (as FLs often serve as interregnum, we predict that these will be a valuable cue for other edits).", "labels": [], "entities": []}, {"text": "We varied the subcorpus utterances used in training.", "labels": [], "entities": []}, {"text": "In some experiments (#1 and 2) we trained with the entire training set 3 , including sentences without speaker errors, and in others (#3-6) we trained only on those sentences containing the relevant deletion errors (and no additionally complex errors) to produce a densely errorful training set.", "labels": [], "entities": []}, {"text": "Likewise, in some experiments we produced output only for those test sentences which we knew to contain simple errors (#3 and 5).", "labels": [], "entities": []}, {"text": "This was meant to emulate the ideal condition where we could perfectly predict which sentences contain errors before identifying where exactly those errors occurred.", "labels": [], "entities": []}, {"text": "The JC04-edit feature was included to help us build on previous efforts for error classification.", "labels": [], "entities": [{"text": "JC04-edit", "start_pos": 4, "end_pos": 13, "type": "DATASET", "confidence": 0.7883519530296326}, {"text": "error classification", "start_pos": 76, "end_pos": 96, "type": "TASK", "confidence": 0.6166891157627106}]}, {"text": "To confirm that the model is not simply replicating these results and is indeed learning on its own with the other features detailed, we also trained models without this JC04-edit feature.", "labels": [], "entities": [{"text": "JC04-edit", "start_pos": 170, "end_pos": 179, "type": "DATASET", "confidence": 0.9042064547538757}]}, {"text": "We first evaluate edit detection accuracy on a perword basis.", "labels": [], "entities": [{"text": "edit detection", "start_pos": 18, "end_pos": 32, "type": "TASK", "confidence": 0.7746436595916748}, {"text": "accuracy", "start_pos": 33, "end_pos": 41, "type": "METRIC", "confidence": 0.8743626475334167}]}, {"text": "To evaluate our progress identifying word-level error classes, we calculate precision, recall and F-scores for each labeled class c in each experimental scenario.", "labels": [], "entities": [{"text": "precision", "start_pos": 76, "end_pos": 85, "type": "METRIC", "confidence": 0.9996651411056519}, {"text": "recall", "start_pos": 87, "end_pos": 93, "type": "METRIC", "confidence": 0.9982494115829468}, {"text": "F-scores", "start_pos": 98, "end_pos": 106, "type": "METRIC", "confidence": 0.9943607449531555}]}, {"text": "As usual, these metrics are calculated as ratios of correct, false, and missed predictions.", "labels": [], "entities": []}, {"text": "However, to take advantage of the double reconstruction annotations provided in SSR (and more importantly, in recognition of the occasional ambiguities of reconstruction) we mod-3 Using both annotated SSR reference reconstructions for each utterance ified these calculations slightly as shown below.", "labels": [], "entities": [{"text": "SSR", "start_pos": 80, "end_pos": 83, "type": "DATASET", "confidence": 0.835996150970459}]}, {"text": "where cw i is the hypothesized class for w i and cg 1 ,i and cg 2 ,i are the two reference classes.: Word-level error prediction F 1 -score results: Data variation.", "labels": [], "entities": [{"text": "Word-level error prediction", "start_pos": 101, "end_pos": 128, "type": "TASK", "confidence": 0.6049779852231344}, {"text": "F 1 -score", "start_pos": 129, "end_pos": 139, "type": "METRIC", "confidence": 0.9187250733375549}]}, {"text": "The first column identifies which data setup was used for each experiment.", "labels": [], "entities": []}, {"text": "The highest performing result for each class in the first set of experiments has been highlighted.", "labels": [], "entities": []}, {"text": "Analysis: Experimental results can be seen in.  training models for individual features and of constraining training data to contain only those utterances known to contain errors.", "labels": [], "entities": []}, {"text": "It also demonstrates the potential impact on error classification after prefiltering test data to those SUs with errors.", "labels": [], "entities": [{"text": "error classification", "start_pos": 45, "end_pos": 65, "type": "TASK", "confidence": 0.6812142431735992}]}, {"text": "demonstrates the contribution of each group of features to our CRF models.", "labels": [], "entities": []}, {"text": "Our results demonstrate the impact of varying our training data and the number of label classes trained for.", "labels": [], "entities": []}, {"text": "We see in from setup #5 experiments that training and testing on error-containing utterances led to a dramatic improvement in F 1 -score.", "labels": [], "entities": [{"text": "F 1 -score", "start_pos": 126, "end_pos": 136, "type": "METRIC", "confidence": 0.9894015341997147}]}, {"text": "On the other hand, our results for experiments using setup #6 (where training data was filtered to contain errorful data but test data was fully preserved) are consistently worse than those of either setup #2 (where both train and test data was untouched) or setup #5 (where both train and test data were prefiltered).", "labels": [], "entities": []}, {"text": "The output appears to suffer from sample bias, as the prior of an error occurring in training is much higher than in testing.", "labels": [], "entities": [{"text": "prior", "start_pos": 54, "end_pos": 59, "type": "METRIC", "confidence": 0.9394025206565857}]}, {"text": "This demonstrates that a densely errorful training set alone cannot improve our results when testing data conditions do not match training data conditions.", "labels": [], "entities": []}, {"text": "However, efforts to identify errorful sentences before determining where errors occur in those sentences maybe worthwhile in preventing false positives in error-less utterances.", "labels": [], "entities": []}, {"text": "We next consider the impact of the four feature groups on our prediction results.", "labels": [], "entities": []}, {"text": "The CRF model appears competitive even without the advantage of building on JC04 results, as seen in 4 . Interestingly and encouragingly, the NT bounds features which indicate the linguistic phrase structures beginning and ending at each word according to an automatic parse were also found to be highly contribututive for both fillers and non-copy identification.", "labels": [], "entities": []}, {"text": "We believe that further pursuit of syntactic features, especially those which can take advantage of the context-free weakness of statistical parsers like) will be promising in future research.", "labels": [], "entities": []}, {"text": "It was unexpected that NC classification would be so sensitive to the loss of lexical features while RC labeling was generally resilient to the dropping of any feature group.", "labels": [], "entities": [{"text": "NC classification", "start_pos": 23, "end_pos": 40, "type": "TASK", "confidence": 0.7472518086433411}]}, {"text": "We hypothesize that for rough copies, the information lost from the removal of the lexical items might have been compensated for by the JC04 features as JC04 performed most strongly on this error type.", "labels": [], "entities": []}, {"text": "This should be further investigated in the future.", "labels": [], "entities": []}, {"text": "Depending on the downstream task of speech reconstruction, it could be imperative not only to identify many of the errors in a given spoken utterance, but indeed to identify all errors (and only those errors), yielding the precise cleaned sentence that a human annotator might provide.", "labels": [], "entities": [{"text": "speech reconstruction", "start_pos": 36, "end_pos": 57, "type": "TASK", "confidence": 0.7216236889362335}]}, {"text": "In these experiments we apply simple cleanup (as described in Section 1.1) to both JC04 output and the predicted output for each experimental setup in, deleting words when their right boundary class is a filled pause, rough copy or non-copy.", "labels": [], "entities": []}, {"text": "Taking advantage of the dual annotations for each sentence in the SSR corpus, we can report both single-reference and double-reference evaluation.", "labels": [], "entities": [{"text": "SSR corpus", "start_pos": 66, "end_pos": 76, "type": "DATASET", "confidence": 0.8857590854167938}]}, {"text": "Thus, we judge that if a hypothesized cleaned sentence exactly matches either reference sentence cleaned in the same manner, we count the cleaned utterance as correct and otherwise assign no credit.", "labels": [], "entities": []}, {"text": "Analysis: We seethe outcome of this set of experiments in.", "labels": [], "entities": []}, {"text": "While the unfiltered test sets of JC04-1, setup #1 and setup #2 appear to have much higher sentence-level cleanup accuracy than the other experiments, we recall that this is natural also due to the fact that the majority of these sentences should not be cleaned at all, besides culate precision and thus F1 score precisely.", "labels": [], "entities": [{"text": "JC04-1", "start_pos": 34, "end_pos": 40, "type": "DATASET", "confidence": 0.8832788467407227}, {"text": "accuracy", "start_pos": 114, "end_pos": 122, "type": "METRIC", "confidence": 0.822177529335022}, {"text": "precision", "start_pos": 285, "end_pos": 294, "type": "METRIC", "confidence": 0.7640088796615601}, {"text": "F1 score", "start_pos": 304, "end_pos": 312, "type": "METRIC", "confidence": 0.9687033295631409}]}, {"text": "Instead, here we show the resultant F1 for the best case and worst case precision range.: Word-level error predictions: exact SU match results.", "labels": [], "entities": [{"text": "F1", "start_pos": 36, "end_pos": 38, "type": "METRIC", "confidence": 0.9788923263549805}, {"text": "precision", "start_pos": 72, "end_pos": 81, "type": "METRIC", "confidence": 0.944595217704773}]}, {"text": "JC04-2 was run only on test sentences known to contain some error to match the conditions of Setup #3 and #5 (from).", "labels": [], "entities": [{"text": "JC04-2", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.9307581782341003}]}, {"text": "For the baselines, we delete only filled pause filler words like \"eh\" and \"um\".", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Disfluency detection performance on the SSR test subcorpus using JC04 system.", "labels": [], "entities": [{"text": "Disfluency detection", "start_pos": 10, "end_pos": 30, "type": "TASK", "confidence": 0.8185456395149231}, {"text": "SSR test subcorpus", "start_pos": 50, "end_pos": 68, "type": "DATASET", "confidence": 0.93878573179245}, {"text": "JC04", "start_pos": 75, "end_pos": 79, "type": "DATASET", "confidence": 0.8729531168937683}]}, {"text": " Table 2: Deeper analysis of edit detection performance on the SSR test subcorpus using JC04 system.", "labels": [], "entities": [{"text": "edit detection", "start_pos": 29, "end_pos": 43, "type": "TASK", "confidence": 0.8041116297245026}, {"text": "SSR test subcorpus", "start_pos": 63, "end_pos": 81, "type": "DATASET", "confidence": 0.9661316871643066}, {"text": "JC04", "start_pos": 88, "end_pos": 92, "type": "DATASET", "confidence": 0.8712751865386963}]}, {"text": " Table 2.  The reader will recall that precision P is defined  as P =", "labels": [], "entities": [{"text": "precision P", "start_pos": 39, "end_pos": 50, "type": "METRIC", "confidence": 0.9232853353023529}]}, {"text": " Table 4: Word-level error prediction F 1 -score re- sults: Data variation. The first column identifies  which data setup was used for each experiment", "labels": [], "entities": [{"text": "Word-level error prediction", "start_pos": 10, "end_pos": 37, "type": "TASK", "confidence": 0.6451620856920878}, {"text": "F 1 -score re- sults", "start_pos": 38, "end_pos": 58, "type": "METRIC", "confidence": 0.9298614859580994}]}, {"text": " Table 5: Word-level error prediction F-score re- sults: Feature variation. All models were trained  with experimental setup #1 and with the set of fea- tures identified.", "labels": [], "entities": [{"text": "Word-level error prediction", "start_pos": 10, "end_pos": 37, "type": "TASK", "confidence": 0.579025020202001}, {"text": "F-score re- sults", "start_pos": 38, "end_pos": 55, "type": "METRIC", "confidence": 0.9077438861131668}]}, {"text": " Table 6: Word-level error predictions: exact SU match results. JC04-2 was run only on test sentences  known to contain some error to match the conditions of Setup #3 and #5 (from", "labels": [], "entities": []}]}