{"title": [{"text": "Sequential Labeling with Latent Variables: An Exact Inference Algorithm and Its Efficient Approximation", "labels": [], "entities": [{"text": "Sequential Labeling", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.8994307518005371}]}], "abstractContent": [{"text": "Latent conditional models have become popular recently in both natural language processing and vision processing communities.", "labels": [], "entities": []}, {"text": "However, establishing an effective and efficient inference method on latent conditional models remains a question.", "labels": [], "entities": []}, {"text": "In this paper, we describe the latent-dynamic inference (LDI), which is able to produce the optimal label sequence on latent conditional models by using efficient search strategy and dynamic programming.", "labels": [], "entities": []}, {"text": "Furthermore , we describe a straightforward solution on approximating the LDI, and show that the approximated LDI performs as well as the exact LDI, while the speed is much faster.", "labels": [], "entities": []}, {"text": "Our experiments demonstrate that the proposed inference algorithm out-performs existing inference methods on a variety of natural language processing tasks.", "labels": [], "entities": []}], "introductionContent": [{"text": "When data have distinct sub-structures, models exploiting latent variables are advantageous in learning (.", "labels": [], "entities": []}, {"text": "Actually, discriminative probabilistic latent variable models (DPLVMs) have recently become popular choices for performing a variety of tasks with sub-structures, e.g., vision recognition (, syntactic parsing (, and syntactic chunking (.", "labels": [], "entities": [{"text": "vision recognition", "start_pos": 169, "end_pos": 187, "type": "TASK", "confidence": 0.7570576071739197}, {"text": "syntactic parsing", "start_pos": 191, "end_pos": 208, "type": "TASK", "confidence": 0.8026663064956665}, {"text": "syntactic chunking", "start_pos": 216, "end_pos": 234, "type": "TASK", "confidence": 0.7752043306827545}]}, {"text": "demonstrated that DPLVM models could efficiently learn sub-structures of natural problems, and outperform several widelyused conventional models, e.g., support vector machines (SVMs), conditional random fields and hidden Markov models.", "labels": [], "entities": []}, {"text": "reported on a syntactic parsing task that DPLVM models can learn more compact and accurate grammars than the conventional techniques without latent variables.", "labels": [], "entities": [{"text": "syntactic parsing task", "start_pos": 14, "end_pos": 36, "type": "TASK", "confidence": 0.7605140507221222}]}, {"text": "The effectiveness of DPLVMs was also shown on a syntactic chunking task by.", "labels": [], "entities": [{"text": "syntactic chunking task", "start_pos": 48, "end_pos": 71, "type": "TASK", "confidence": 0.6867417593797048}]}, {"text": "DPLVMs outperform conventional learning models, as described in the aforementioned publications.", "labels": [], "entities": []}, {"text": "However, inferences on the latent conditional models are remaining problems.", "labels": [], "entities": []}, {"text": "In conventional models such as CRFs, the optimal label path can be efficiently obtained by the dynamic programming.", "labels": [], "entities": []}, {"text": "However, for latent conditional models such as DPLVMs, the inference is not straightforward because of the inclusion of latent variables.", "labels": [], "entities": []}, {"text": "In this paper, we propose anew inference algorithm, latent dynamic inference (LDI), by systematically combining an efficient search strategy with the dynamic programming.", "labels": [], "entities": []}, {"text": "The LDI is an exact inference method producing the most probable label sequence.", "labels": [], "entities": []}, {"text": "In addition, we also propose an approximated LDI algorithm for faster speed.", "labels": [], "entities": []}, {"text": "We show that the approximated LDI performs as well as the exact one.", "labels": [], "entities": []}, {"text": "We will also discuss a post-processing method for the LDI algorithm: the minimum bayesian risk reranking.", "labels": [], "entities": []}, {"text": "The subsequent section describes an overview of DPLVM models.", "labels": [], "entities": []}, {"text": "We discuss the probability distribution of DPLVM models, and present the LDI inference in Section 3.", "labels": [], "entities": [{"text": "LDI", "start_pos": 73, "end_pos": 76, "type": "METRIC", "confidence": 0.7436667680740356}]}, {"text": "Finally, we report experimental results and begin our discussions in Section 4 and Section 5.: Comparison between CRF models and DPLVM models on the training stage.", "labels": [], "entities": []}, {"text": "x represents the observation sequence, y represents labels and h represents the latent variables assigned to the labels.", "labels": [], "entities": []}, {"text": "Note that only the white circles are observed variables.", "labels": [], "entities": []}, {"text": "Also, only the links with the current observations are shown, but for both models, long range dependencies are possible.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we choose Bio-NER and NPchunking tasks for experiments.", "labels": [], "entities": []}, {"text": "First, we describe the implementations and settings.", "labels": [], "entities": []}, {"text": "We implemented DPLVMs by extending the HCRF library developed by.", "labels": [], "entities": [{"text": "HCRF library", "start_pos": 39, "end_pos": 51, "type": "DATASET", "confidence": 0.9400608837604523}]}, {"text": "We added a Limited-Memory BFGS optimizer (L-BFGS), and reimplemented the code on training and inference for higher efficiency.", "labels": [], "entities": []}, {"text": "To reduce overfitting, we employed a Gaussian prior).", "labels": [], "entities": []}, {"text": "We varied the the variance of the Gaussian prior (with values 10 k , k from -3 to 3), and we found that \u03c3 2 = 1.0 is optimal for DPLVMs on the development data, and used it throughout the experiments in this section.", "labels": [], "entities": []}, {"text": "The training stage was kept the same as.", "labels": [], "entities": []}, {"text": "In other words, there is no need to change the conventional parameter estimation method on DPLVM models for adapting the various inference algorithms in this paper.", "labels": [], "entities": []}, {"text": "For more information on training DPLVMs, refer to and.", "labels": [], "entities": []}, {"text": "Since the CRF model is one of the most successful models in sequential labeling tasks (, in this paper, we choosed CRFs as a baseline model for the comparison.", "labels": [], "entities": [{"text": "sequential labeling tasks", "start_pos": 60, "end_pos": 85, "type": "TASK", "confidence": 0.7792994379997253}]}, {"text": "Note that the feature sets were kept the same in DPLVMs and CRFs.", "labels": [], "entities": []}, {"text": "Also, the optimizer and fine tuning strategy were kept the same.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: On the test data of the Bio-NER task, ex- perimental comparisons among various inference  algorithms on DPLVMs, and the performance of  CRFs. S.A. signifies sentence accuracy. As can  be seen, at a much lower cost, the LDI-A (A signi- fies approximation) performed slightly better than  the LDI-E (E signifies exact).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 176, "end_pos": 184, "type": "METRIC", "confidence": 0.9435810446739197}]}, {"text": " Table 4: Experimental comparisons among differ- ent inference algorithms on DPLVMs, and the per- formance of CRFs using the same feature set on  the word features.", "labels": [], "entities": []}, {"text": " Table 5: The effect of MBR reranking on the NP- chunking task. As can be seen, MBR-reranking  improved the performance of the LDI.", "labels": [], "entities": [{"text": "NP- chunking task", "start_pos": 45, "end_pos": 62, "type": "TASK", "confidence": 0.7707794457674026}]}]}