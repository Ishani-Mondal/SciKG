{"title": [{"text": "Re-Ranking Models For Spoken Language Understanding", "labels": [], "entities": []}], "abstractContent": [{"text": "Spoken Language Understanding aims at mapping a natural language spoken sentence into a semantic representation.", "labels": [], "entities": [{"text": "Spoken Language Understanding", "start_pos": 0, "end_pos": 29, "type": "TASK", "confidence": 0.879281759262085}]}, {"text": "In the last decade two main approaches have been pursued: generative and discrimi-native models.", "labels": [], "entities": [{"text": "generative", "start_pos": 58, "end_pos": 68, "type": "TASK", "confidence": 0.9703984260559082}]}, {"text": "The former is more robust to overfitting whereas the latter is more robust to many irrelevant features.", "labels": [], "entities": []}, {"text": "Additionally, the way in which these approaches encode prior knowledge is very different and their relative performance changes based on the task.", "labels": [], "entities": []}, {"text": "In this paper we describe a machine learning framework where both models are used: a gen-erative model produces a list of ranked hypotheses whereas a discriminative model based on structure kernels and Support Vector Machines, re-ranks such list.", "labels": [], "entities": []}, {"text": "We tested our approach on the MEDIA corpus (human-machine dialogs) and on anew corpus (human-machine and human-human dialogs) produced in the Euro-pean LUNA project.", "labels": [], "entities": [{"text": "MEDIA corpus", "start_pos": 30, "end_pos": 42, "type": "DATASET", "confidence": 0.7742780447006226}, {"text": "Euro-pean LUNA project", "start_pos": 142, "end_pos": 164, "type": "DATASET", "confidence": 0.9539392789204916}]}, {"text": "The results show a large improvement on the state-of-the-art in concept segmentation and labeling.", "labels": [], "entities": [{"text": "concept segmentation", "start_pos": 64, "end_pos": 84, "type": "TASK", "confidence": 0.7213600277900696}]}], "introductionContent": [{"text": "In Spoken Dialog Systems, the Language Understanding module performs the task of translating a spoken sentence into its meaning representation based on semantic constituents.", "labels": [], "entities": [{"text": "Spoken Dialog Systems", "start_pos": 3, "end_pos": 24, "type": "TASK", "confidence": 0.8973064422607422}]}, {"text": "These are the units for meaning representation and are often referred to as concepts.", "labels": [], "entities": [{"text": "meaning representation", "start_pos": 24, "end_pos": 46, "type": "TASK", "confidence": 0.7748642861843109}]}, {"text": "Concepts are instantiated by sequences of words, therefore a Spoken Language Understanding (SLU) module finds the association between words and concepts.", "labels": [], "entities": []}, {"text": "In the last decade two major approaches have been proposed to find this correlation: (i) generative models, whose parameters refer to the joint probability of concepts and constituents; and (ii) discriminative models, which learn a classification function to map words into concepts based on geometric and statistical properties.", "labels": [], "entities": []}, {"text": "An example of generative model is the Hidden Vector State model (HVS)).", "labels": [], "entities": []}, {"text": "This approach extends the discrete Markov model encoding the context of each state as a vector.", "labels": [], "entities": []}, {"text": "State transitions are performed as stack shift operations followed by a push of a preterminal semantic category label.", "labels": [], "entities": []}, {"text": "In this way the model can capture semantic hierarchical structures without the use of tree-structured data.", "labels": [], "entities": []}, {"text": "Another simpler but effective generative model is the one based on Finite State Transducers.", "labels": [], "entities": [{"text": "generative", "start_pos": 30, "end_pos": 40, "type": "TASK", "confidence": 0.9688755869865417}]}, {"text": "It performs SLU as a translation process from words to concepts using Finite State Transducers (FST).", "labels": [], "entities": [{"text": "SLU", "start_pos": 12, "end_pos": 15, "type": "TASK", "confidence": 0.9609061479568481}]}, {"text": "An example of discriminative model used for SLU is the one based on Support Vector Machines (SVMs), as shown in . In this approach, data are mapped into a vector space and SLU is performed as a classification problem using Maximal Margin Classifiers).", "labels": [], "entities": [{"text": "SLU", "start_pos": 44, "end_pos": 47, "type": "TASK", "confidence": 0.973326563835144}]}, {"text": "Generative models have the advantage to be more robust to overfitting on training data, while discriminative models are more robust to irrelevant features.", "labels": [], "entities": []}, {"text": "Both approaches, used separately, have shown a good performance ), but they have very different characteristics and the way they encode prior knowledge is very different, thus designing models able to take into account characteristics of both approaches are particularly promising.", "labels": [], "entities": []}, {"text": "In this paper we propose a method for SLU based on generative and discriminative models: the former uses FSTs to generate a list of SLU hypotheses, which are re-ranked by SVMs.", "labels": [], "entities": [{"text": "SLU", "start_pos": 38, "end_pos": 41, "type": "TASK", "confidence": 0.9717867374420166}]}, {"text": "These exploit all possible word/concept subsequences (with gaps) of the spoken sentence as features (i.e. all possible n-grams).", "labels": [], "entities": []}, {"text": "Gaps allow for the encod-ing of long distance dependencies between words in relatively small n-grams.", "labels": [], "entities": []}, {"text": "Given the huge size of this feature space, we adopted kernel methods and in particular sequence kernels) and tree kernels) to implicitly encode n-grams and other structural information in SVMs.", "labels": [], "entities": []}, {"text": "We experimented with different approaches for training the discriminative models and two different corpora: the well-known MEDIA corpus () and anew corpus acquired in the European project LUNA 1.", "labels": [], "entities": [{"text": "MEDIA corpus", "start_pos": 123, "end_pos": 135, "type": "DATASET", "confidence": 0.7198076546192169}]}, {"text": "The results show a great improvement with respect to both the FST-based model and the SVM model alone, which are the current state-of-the-art for concept classification on such corpora.", "labels": [], "entities": [{"text": "FST-based", "start_pos": 62, "end_pos": 71, "type": "DATASET", "confidence": 0.5476360321044922}, {"text": "concept classification", "start_pos": 146, "end_pos": 168, "type": "TASK", "confidence": 0.7189010381698608}]}, {"text": "The rest of the paper is organized as follows: Sections 2 and 3 show the generative and discriminative models, respectively.", "labels": [], "entities": [{"text": "generative", "start_pos": 73, "end_pos": 83, "type": "TASK", "confidence": 0.9686495661735535}]}, {"text": "The experiments and results are reported in Section 4 whereas the conclusions are drawn in Section 5.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we describe the corpora, parameters, models and results of our experiments of word chunking and concept classification.", "labels": [], "entities": [{"text": "word chunking", "start_pos": 95, "end_pos": 108, "type": "TASK", "confidence": 0.7672154307365417}, {"text": "concept classification", "start_pos": 113, "end_pos": 135, "type": "TASK", "confidence": 0.7320950776338577}]}, {"text": "Our baseline relates to the error rate of systems based on only FST and SVMs.", "labels": [], "entities": [{"text": "error rate", "start_pos": 28, "end_pos": 38, "type": "METRIC", "confidence": 0.9445898830890656}, {"text": "FST", "start_pos": 64, "end_pos": 67, "type": "DATASET", "confidence": 0.6655728816986084}]}, {"text": "The re-ranking models are built on the FST output.", "labels": [], "entities": [{"text": "FST output", "start_pos": 39, "end_pos": 49, "type": "DATASET", "confidence": 0.8853315711021423}]}, {"text": "Different ways of producing training data for the re-ranking models determine different results.", "labels": [], "entities": []}, {"text": "We defined two different training sets in the LUNA corpus: one using only the WOZ training dialogs and one merging them with the HH dialogs.", "labels": [], "entities": [{"text": "LUNA corpus", "start_pos": 46, "end_pos": 57, "type": "DATASET", "confidence": 0.9559400975704193}, {"text": "HH dialogs", "start_pos": 129, "end_pos": 139, "type": "DATASET", "confidence": 0.9269325435161591}]}, {"text": "Given the small size of LUNA corpus, we did not carried out parameterization on a development set but we used default or a priori parameters.", "labels": [], "entities": [{"text": "LUNA corpus", "start_pos": 24, "end_pos": 35, "type": "DATASET", "confidence": 0.8706126809120178}]}, {"text": "We experimented with LUNA WOZ and six rerankers obtained with the combination of SVMs and perceptron (PCT) with three different types of kernels: Syntactic Tree Kernel (STK), Partial Tree kernels (PTK) and the String Kernel (SK) described in Section 3.3.", "labels": [], "entities": [{"text": "LUNA WOZ", "start_pos": 21, "end_pos": 29, "type": "METRIC", "confidence": 0.6582906544208527}]}, {"text": "Given the high number and the cost of these experiments, we ran only one model, i.e. the one  We trained all the SCLMs used in our experiments with the SRILM toolkit) and we used an interpolated model for probability estimation with the Kneser-Ney discount).", "labels": [], "entities": [{"text": "SRILM toolkit", "start_pos": 152, "end_pos": 165, "type": "DATASET", "confidence": 0.884025901556015}]}, {"text": "We then converted the model in an FST as described in Section 2.1.", "labels": [], "entities": [{"text": "FST", "start_pos": 34, "end_pos": 37, "type": "DATASET", "confidence": 0.4907945692539215}]}, {"text": "The model used to obtain the SVM baseline for concept classification was trained using Yam-CHA ().", "labels": [], "entities": [{"text": "concept classification", "start_pos": 46, "end_pos": 68, "type": "TASK", "confidence": 0.6611785590648651}]}, {"text": "For the reranking models based on structure kernels, SVMs or perceptron, we used the SVM-Light-TK toolkit (available at dit.unitn.it/moschitti).", "labels": [], "entities": []}, {"text": "For \u03bb (see Section 3.2), cost-factor and trade-off parameters, we used, 0.4, 1 and 1, respectively.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Statistics on the LUNA corpus", "labels": [], "entities": []}, {"text": " Table 2: Statistics on the MEDIA corpus", "labels": [], "entities": [{"text": "MEDIA", "start_pos": 28, "end_pos": 33, "type": "DATASET", "confidence": 0.7397420406341553}]}, {"text": " Table 3: Results of experiments (CER) using FST  and SVMs with the Sytntactic Tree Kernel (STK)  on two different corpora: LUNA WOZ + HH, and  MEDIA.", "labels": [], "entities": [{"text": "MEDIA", "start_pos": 144, "end_pos": 149, "type": "METRIC", "confidence": 0.7066465616226196}]}, {"text": " Table 4: Results of experiments, in terms of Con- cept Error Rate (CER), on the LUNA WOZ corpus  using Monolithic Training approach. The baseline  with FST and SVMs used separately are 23.2%  and 26.7% respectively.", "labels": [], "entities": [{"text": "Con- cept Error Rate (CER)", "start_pos": 46, "end_pos": 72, "type": "METRIC", "confidence": 0.897073283791542}, {"text": "LUNA WOZ corpus", "start_pos": 81, "end_pos": 96, "type": "DATASET", "confidence": 0.9011061191558838}, {"text": "FST", "start_pos": 153, "end_pos": 156, "type": "METRIC", "confidence": 0.4427117705345154}]}, {"text": " Table 5: Results of experiments, in terms of Con- cept Error Rate (CER), on the LUNA WOZ cor- pus using Split Training approach. The baseline  with FST and SVMs used separately are 23.2%  and 26.7% respectively.", "labels": [], "entities": [{"text": "Con- cept Error Rate (CER)", "start_pos": 46, "end_pos": 72, "type": "METRIC", "confidence": 0.9382220730185509}, {"text": "LUNA WOZ cor- pus", "start_pos": 81, "end_pos": 98, "type": "DATASET", "confidence": 0.7153629064559937}, {"text": "FST", "start_pos": 149, "end_pos": 152, "type": "METRIC", "confidence": 0.4805041551589966}]}]}