{"title": [{"text": "Outclassing Wikipedia in Open-Domain Information Extraction: Weakly-Supervised Acquisition of Attributes over Conceptual Hierarchies", "labels": [], "entities": [{"text": "Open-Domain Information Extraction", "start_pos": 25, "end_pos": 59, "type": "TASK", "confidence": 0.684938907623291}]}], "abstractContent": [{"text": "A set of labeled classes of instances is extracted from text and linked into an existing conceptual hierarchy.", "labels": [], "entities": []}, {"text": "Besides a significant increase in the coverage of the class labels assigned to individual instances, the resulting resource of labeled classes is more effective than similar data derived from the manually-created Wikipedia, in the task of attribute extraction over conceptual hierarchies.", "labels": [], "entities": [{"text": "attribute extraction", "start_pos": 239, "end_pos": 259, "type": "TASK", "confidence": 0.7383766174316406}]}], "introductionContent": [{"text": "Motivation: Sharing basic intuitions and longterm goals with other tasks within the area of Webbased information extraction (, the task of acquiring class attributes relies on unstructured text available on the Web, as a data source for extracting generally-useful knowledge.", "labels": [], "entities": [{"text": "Webbased information extraction", "start_pos": 92, "end_pos": 123, "type": "TASK", "confidence": 0.8412867983182272}]}, {"text": "In the case of attribute extraction, the knowledge to be extracted consists in quantifiable properties of various classes (e.g., top speed, body style and gas mileage for the class of sports cars).", "labels": [], "entities": [{"text": "attribute extraction", "start_pos": 15, "end_pos": 35, "type": "TASK", "confidence": 0.7774651348590851}]}, {"text": "Existing work on large-scale attribute extraction focuses on producing ranked lists of attributes, for target classes of instances available in the form of flat sets of instances (e.g., ferrari modena, porsche carrera gt) sharing the same class label (e.g., sports cars).", "labels": [], "entities": [{"text": "large-scale attribute extraction", "start_pos": 17, "end_pos": 49, "type": "TASK", "confidence": 0.7121476332346598}]}, {"text": "Independently of how the input target classes are populated with instances (manually or automatically), and what type of textual data source is used for extracting attributes (Web documents or query logs), the extraction of attributes operates at a lexical rather than semantic level.", "labels": [], "entities": []}, {"text": "Indeed, the class labels of the target classes maybe not more than text surface strings (e.g., sports cars) or even artificially-created labels (e.g., CartoonChar in lieu of cartoon characters).", "labels": [], "entities": [{"text": "CartoonChar", "start_pos": 151, "end_pos": 162, "type": "DATASET", "confidence": 0.9239866733551025}]}, {"text": "Moreover, although it is commonly accepted that sports cars are also cars, which in turn are also motor vehicles, the presence of sports cars among the input target classes does not lead to any attributes being extracted for cars and motor vehicles, unless the latter two class labels are also present explicitly among the input target classes.", "labels": [], "entities": []}, {"text": "Contributions: The contributions of this paper are threefold.", "labels": [], "entities": []}, {"text": "First, we investigate the role of classes of instances acquired automatically from unstructured text, in the task of attribute extraction over concepts from existing conceptual hierarchies.", "labels": [], "entities": [{"text": "attribute extraction", "start_pos": 117, "end_pos": 137, "type": "TASK", "confidence": 0.7572053670883179}]}, {"text": "For this purpose, ranked lists of attributes are acquired from query logs for various concepts, after linking a set of more than 4,500 open-domain, automatically-acquired classes containing a total of around 250,000 instances into conceptual hierarchies available in WordNet).", "labels": [], "entities": [{"text": "WordNet", "start_pos": 267, "end_pos": 274, "type": "DATASET", "confidence": 0.949686586856842}]}, {"text": "In comparison, previous work extracts attributes for either manually-specified classes of instances, or for classes of instances derived automatically but considered as flat rather than hierarchical classes, and manually associated to existing semantic concepts).", "labels": [], "entities": []}, {"text": "Second, we expand the set of classes of instances acquired from text, thus increasing their usefulness in attribute extraction in particular and information extraction in general.", "labels": [], "entities": [{"text": "attribute extraction", "start_pos": 106, "end_pos": 126, "type": "TASK", "confidence": 0.7256993651390076}, {"text": "information extraction", "start_pos": 145, "end_pos": 167, "type": "TASK", "confidence": 0.7920395731925964}]}, {"text": "To this effect, additional class labels (e.g., motor vehicles) are identified for existing instances (e.g., ferrari modena) of existing class labels (e.g., sports cars), by exploiting IsA relations available within the conceptual hierarchy (e.g., sports cars are also motor vehicles).", "labels": [], "entities": []}, {"text": "Third, we show that large-scale, automatically-derived classes of in-stances can have as much as, or even bigger, practical impact in open-domain information extraction tasks than similar data from large-scale, highcoverage, manually-compiled resources.", "labels": [], "entities": [{"text": "open-domain information extraction tasks", "start_pos": 134, "end_pos": 174, "type": "TASK", "confidence": 0.698196679353714}]}, {"text": "Specifically, evaluation results indicate that the accuracy of the extracted lists of attributes is higher by 8% at rank 10, 13% at rank 30 and 18% at rank 50, when using the automatically-extracted classes of instances rather than the comparatively more numerous and a-priori more reliable, humangenerated, collaboratively-vetted classes of instances available within Wikipedia).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 51, "end_pos": 59, "type": "METRIC", "confidence": 0.9993940591812134}]}], "datasetContent": [{"text": "Coverage of Class Instances: In run N, the input class instances are the component phrases of synsets encoded via HasInstance relations under other synsets in WordNet.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 159, "end_pos": 166, "type": "DATASET", "confidence": 0.9375221729278564}]}, {"text": "For example, the synset corresponding to {search engine}, defined as \"a computer program that retrieves documents or files or data from a database or from a computer network\", has 3 HasInstance instances in WordNet, namely Ask Jeeves, Google and Yahoo.", "labels": [], "entities": []}, {"text": "Table 3 illustrates the coverage of the class instances extracted from unstructured text and linked to WordNet in runs E sand E a respectively, relative to all 945 WordNet synsets that contain HasInstance instances.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 103, "end_pos": 110, "type": "DATASET", "confidence": 0.9615543484687805}]}, {"text": "Note that the coverage scores are conservative assessments of actual coverage, since a run (i.e., E s or E a ) receives credit fora WordNet instance only if the run contains an instance that is a full-length, case-insensitive match (e.g., ask: Examples of additional class labels collected from WordNet, for existing instances of the original labeled classes extracted from text classes; or expands existing classes, if the class label already occurs in the original set of labeled classes but not in association to the instance; or creates new classes of instances, if the class label is not part of the original set.", "labels": [], "entities": []}, {"text": "The latter two cases aggregate to increases in coverage, relative to the pairs from the original sets of labeled classes, of 53% for E sand 304% for E a .  Target Hierarchy Concepts: The performance of attribute extraction is assessed over a set of 25 target concepts also used for evaluation in one of the target concepts, denoted Country, corresponds to a synset situated at the internal offset 08544813 in WordNet 3.0, which groups together the synonymous phrases country, state and land and associates them with the definition \"the territory occupied by a nation\".", "labels": [], "entities": [{"text": "attribute extraction", "start_pos": 202, "end_pos": 222, "type": "TASK", "confidence": 0.7744938731193542}]}, {"text": "The target concepts exhibit variation with respect to their depths within WordNet conceptual hierarchies, ranging from a minimum of 5 (e.g., for Food) to a maximum of 11 (for Flower), with a mean depth of 8 over the 25 concepts.", "labels": [], "entities": [{"text": "WordNet conceptual hierarchies", "start_pos": 74, "end_pos": 104, "type": "DATASET", "confidence": 0.8859357237815857}]}, {"text": "Evaluation Procedure: The measurement of recall requires knowledge of the complete set of items (in our case, attributes) to be extracted.", "labels": [], "entities": [{"text": "recall", "start_pos": 41, "end_pos": 47, "type": "METRIC", "confidence": 0.9966051578521729}]}, {"text": "Unfortunately, this number is often unavailable in information extraction tasks in general (, and attribute extraction in particular.", "labels": [], "entities": [{"text": "information extraction tasks", "start_pos": 51, "end_pos": 79, "type": "TASK", "confidence": 0.8419860402743021}, {"text": "attribute extraction", "start_pos": 98, "end_pos": 118, "type": "TASK", "confidence": 0.7237130701541901}]}, {"text": "Indeed, the manual enumeration of all attributes of each target concept, to measure recall, is unfeasible.", "labels": [], "entities": [{"text": "recall", "start_pos": 84, "end_pos": 90, "type": "METRIC", "confidence": 0.9958089590072632}]}, {"text": "Therefore, the evaluation focuses on the assessment of attribute accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 65, "end_pos": 73, "type": "METRIC", "confidence": 0.9235740900039673}]}, {"text": "To remove any bias towards higher-ranked attributes during the assessment of class attributes, the ranked lists of attributes produced by each run to be evaluated are sorted alphabetically into a merged list.", "labels": [], "entities": []}, {"text": "Each attribute of the merged list is manually assigned a correctness label within its respective class.", "labels": [], "entities": []}, {"text": "In accordance with previously introduced methodology, an attribute is vital if it must be present in an ideal list of attributes of the class (e.g., side effects for Drug); okay if it provides useful but non-essential information; and wrong if it is incorrect.", "labels": [], "entities": []}, {"text": "To compute the precision score over a ranked list of attributes, the correctness labels are converted to numeric values (vital to 1, okay to 0.5 and wrong to 0).", "labels": [], "entities": [{"text": "precision score", "start_pos": 15, "end_pos": 30, "type": "METRIC", "confidence": 0.9819488823413849}]}, {"text": "Precision at some rank N in the list is thus measured as the sum of the assigned values of the first N attributes, divided by N . Attribute Accuracy: plots the precision at ranks 1 through 50 for the ranked lists of attributes extracted by various runs as an average over the 25 target concepts, along two dimensions.", "labels": [], "entities": [{"text": "Precision", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.987260103225708}, {"text": "Accuracy", "start_pos": 140, "end_pos": 148, "type": "METRIC", "confidence": 0.734329342842102}, {"text": "precision", "start_pos": 160, "end_pos": 169, "type": "METRIC", "confidence": 0.9985431432723999}]}, {"text": "In the leftmost graphs, each of the 25 target concepts counts towards the computation of precision scores of a given run, regardless of whether any attributes were extracted or not for the target concept.", "labels": [], "entities": [{"text": "precision scores", "start_pos": 89, "end_pos": 105, "type": "METRIC", "confidence": 0.9651496708393097}]}, {"text": "In the rightmost graphs, only target concepts for which some attributes were extracted are included in the precision scores of a given run.", "labels": [], "entities": [{"text": "precision scores", "start_pos": 107, "end_pos": 123, "type": "METRIC", "confidence": 0.9749088287353516}]}, {"text": "Thus, the leftmost graphs properly penalize a run for failing to extract any attributes for some target concepts, whereas the rightmost graphs do not include any such penalties.", "labels": [], "entities": []}, {"text": "On the other dimension, in the graphs at the top of, seed attributes are provided only for one class (namely, european countries), fora total of 5 attributes overall classes.", "labels": [], "entities": []}, {"text": "In the graphs at the bottom of the figure, there are 5 seed attributes for each of the 25 target concepts in the graphs at the bottom of, fora total of 5\u00d725=125 attributes.", "labels": [], "entities": []}, {"text": "Several conclusions can be drawn after inspecting the results.", "labels": [], "entities": []}, {"text": "First, providing more supervision, in the form of seed attributes for all concepts rather than for only one concept, translates into higher attribute accuracy for all runs, as shown by the graphs at the top vs. graphs at the bottom of.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 150, "end_pos": 158, "type": "METRIC", "confidence": 0.981627881526947}]}, {"text": "Second, in the leftmost graphs, run N has the lowest precision scores, which is inline with the relatively small number of instances available in the original WordNet, as confirmed by the counts from: Comparative accuracy of the attributes extracted by various runs, for individual concepts, as an average over the entire set of 25 target concepts, and as an average over (variable) subsets of the 25 target concepts for which some attributes were extracted in each run.", "labels": [], "entities": [{"text": "precision", "start_pos": 53, "end_pos": 62, "type": "METRIC", "confidence": 0.9963890314102173}, {"text": "WordNet", "start_pos": 159, "end_pos": 166, "type": "DATASET", "confidence": 0.9637318253517151}, {"text": "accuracy", "start_pos": 213, "end_pos": 221, "type": "METRIC", "confidence": 0.9905489087104797}]}, {"text": "Seed attributes are provided as input for each target concept restrictions may improve precision but hurts recall of class instances, which results in lower average precision scores for the attributes.", "labels": [], "entities": [{"text": "precision", "start_pos": 87, "end_pos": 96, "type": "METRIC", "confidence": 0.9991057515144348}, {"text": "recall", "start_pos": 107, "end_pos": 113, "type": "METRIC", "confidence": 0.9981239438056946}, {"text": "precision", "start_pos": 165, "end_pos": 174, "type": "METRIC", "confidence": 0.9856237173080444}]}, {"text": "Fourth, in the leftmost graphs, the runs using the automaticallyextracted labeled classes (E sand E a ) not only outperform N, but one of them (E a ) also outperforms Y.", "labels": [], "entities": []}, {"text": "This is the most important result.", "labels": [], "entities": []}, {"text": "It shows that large-scale, automatically-derived classes of instances can have as much as, or even bigger, practical impact in attribute extraction than similar data from larger (cf), manually-compiled, collaboratively created and maintained resources such as Wikipedia.", "labels": [], "entities": [{"text": "attribute extraction", "start_pos": 127, "end_pos": 147, "type": "TASK", "confidence": 0.7426871955394745}]}, {"text": "Concretely, in the graph on the bottom left of, the precision scores at ranks 10, 30 and 50 are 0.71, 0.59 and 0.53 for run Y, but 0.77, 0.67 and 0.63 for run E a . The scores correspond to attribute accuracy improvements of 8% at rank 10, 13% at rank 30, and 18% at rank 50 for run E a overrun Y.", "labels": [], "entities": [{"text": "precision", "start_pos": 52, "end_pos": 61, "type": "METRIC", "confidence": 0.9995970129966736}, {"text": "accuracy", "start_pos": 200, "end_pos": 208, "type": "METRIC", "confidence": 0.9582871198654175}]}, {"text": "In fact, in the rightmost graphs, that is, without taking into account target concepts without any extracted attributes, the precision scores of both E sand E a are higher than for run Y across most, if not all, ranks from 1 through 50.", "labels": [], "entities": [{"text": "precision", "start_pos": 125, "end_pos": 134, "type": "METRIC", "confidence": 0.9992856383323669}]}, {"text": "In this case, it is E 1 that produces the most accurate attributes, in a task-based demonstration that the more cautious linking of class labels to WordNet concepts in E s vs. E a leads to less coverage but higher precision of the linked labeled classes, which translates into extracted attributes of higher accuracy but for fewer target concepts.", "labels": [], "entities": [{"text": "precision", "start_pos": 214, "end_pos": 223, "type": "METRIC", "confidence": 0.9956949949264526}, {"text": "accuracy", "start_pos": 308, "end_pos": 316, "type": "METRIC", "confidence": 0.9899930357933044}]}], "tableCaptions": [{"text": " Table 1: Examples of instances within labeled classes extracted from unstructured text, used as input for  attribute extraction experiments", "labels": [], "entities": [{"text": "attribute extraction", "start_pos": 108, "end_pos": 128, "type": "TASK", "confidence": 0.7382996380329132}]}, {"text": " Table 2: Source of class instances for various ex- perimental runs", "labels": [], "entities": []}, {"text": " Table 3: Coverage of class instances extracted from text and linked to WordNet (used as input in runs E s  and E a respectively), measured as the fraction of WordNet HasInstance instances (used as input in run  N) that occur among the class instances (Cvg=coverage)", "labels": [], "entities": [{"text": "WordNet", "start_pos": 72, "end_pos": 79, "type": "DATASET", "confidence": 0.9511997699737549}]}]}