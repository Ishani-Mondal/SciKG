{"title": [{"text": "Performance Confidence Estimation for Automatic Summarization", "labels": [], "entities": [{"text": "Performance Confidence Estimation", "start_pos": 0, "end_pos": 33, "type": "TASK", "confidence": 0.7446852326393127}, {"text": "Summarization", "start_pos": 48, "end_pos": 61, "type": "TASK", "confidence": 0.7012748122215271}]}], "abstractContent": [{"text": "We address the task of automatically predicting if summarization system performance will be good or bad based on features derived directly from either single-or multi-document inputs.", "labels": [], "entities": [{"text": "summarization system", "start_pos": 51, "end_pos": 71, "type": "TASK", "confidence": 0.8687374889850616}]}, {"text": "Our labelled corpus for the task is composed of data from large scale evaluations completed over the span of several years.", "labels": [], "entities": []}, {"text": "The variation of data between years allows fora comprehensive analysis of the robustness of features, but poses a challenge for building a combined corpus which can be used for training and testing.", "labels": [], "entities": []}, {"text": "Still, we find that the problem can be mitigated by appropriately normalizing for differences within each year.", "labels": [], "entities": []}, {"text": "We examine different formulations of the classification task which considerably influence performance.", "labels": [], "entities": []}, {"text": "The best results are 84% prediction accuracy for single-and 74% for multi-document summarization.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 36, "end_pos": 44, "type": "METRIC", "confidence": 0.95084547996521}]}], "introductionContent": [{"text": "The input to a summarization system significantly affects the quality of the summary that can be produced for it, by either a person or an automatic method.", "labels": [], "entities": []}, {"text": "Some inputs are difficult and summaries produced by any approach will tend to be poor, while other inputs are easy and systems will exhibit good performance.", "labels": [], "entities": []}, {"text": "User satisfaction with the summaries can be improved, for example by automatically flagging summaries for which a system expects to perform poorly.", "labels": [], "entities": [{"text": "summaries", "start_pos": 27, "end_pos": 36, "type": "TASK", "confidence": 0.9456334114074707}]}, {"text": "In such cases the user can ignore the summary and avoid the frustration of reading poor quality text.", "labels": [], "entities": []}, {"text": "() describes an intelligent summarizer system that could identify documents which would be difficult to summarize based on structural properties.", "labels": [], "entities": [{"text": "summarizer", "start_pos": 28, "end_pos": 38, "type": "TASK", "confidence": 0.975832462310791}]}, {"text": "Documents containing question/answer sessions, speeches, tables and embedded lists were identified based on patterns and these features were used to determine whether an acceptable summary can be produced.", "labels": [], "entities": []}, {"text": "If not, the inputs were flagged as unsuitable for automatic summarization.", "labels": [], "entities": [{"text": "summarization", "start_pos": 60, "end_pos": 73, "type": "TASK", "confidence": 0.9174914956092834}]}, {"text": "In our work, we provide deeper insight into how other characteristics of the text itself and properties of document clusters can be used to identify difficult inputs.", "labels": [], "entities": []}, {"text": "The task of predicting the confidence in system performance fora given input is in fact relevant not only for summarization, but in general for all applications aimed at facilitating information access.", "labels": [], "entities": [{"text": "summarization", "start_pos": 110, "end_pos": 123, "type": "TASK", "confidence": 0.9884679913520813}]}, {"text": "In question answering for example, a system maybe configured not to answer questions for which the confidence of producing a correct answer is low, and in this way increase the overall accuracy of the system whenever it does produce an answer (.", "labels": [], "entities": [{"text": "question answering", "start_pos": 3, "end_pos": 21, "type": "TASK", "confidence": 0.8584213554859161}, {"text": "accuracy", "start_pos": 185, "end_pos": 193, "type": "METRIC", "confidence": 0.9983488321304321}]}, {"text": "Similarly in machine translation, some sentences might contain difficult to translate phrases, that is, portions of the input are likely to lead to garbled output if automatic translation is attempted.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 13, "end_pos": 32, "type": "TASK", "confidence": 0.7890678644180298}]}, {"text": "Automatically identifying such phrases has the potential of improving MT as shown by an oracle study.", "labels": [], "entities": [{"text": "MT", "start_pos": 70, "end_pos": 72, "type": "TASK", "confidence": 0.9924919009208679}]}, {"text": "More recent work has shown that properties of reordering, source and target language complexity and relatedness can be used to predict translation quality.", "labels": [], "entities": []}, {"text": "In information retrieval, the problem of predicting system performance has generated considerable interest and has led to notably good results).", "labels": [], "entities": [{"text": "information retrieval", "start_pos": 3, "end_pos": 24, "type": "TASK", "confidence": 0.8143117725849152}, {"text": "predicting system", "start_pos": 41, "end_pos": 58, "type": "TASK", "confidence": 0.8887849450111389}]}], "datasetContent": [{"text": "In this section we explore how the alternative task formulations influence success of predicting system performance.", "labels": [], "entities": []}, {"text": "Obviously, the two classes of interest for the prediction will be \"good performance\" and \"poor performance\".", "labels": [], "entities": []}, {"text": "But separating the real valued coverage scores for inputs into these two classes can be done in different ways.", "labels": [], "entities": []}, {"text": "All the data can be used and the definition of \"good\" or \"bad\" can be determined in relation to the average performance on all inputs.", "labels": [], "entities": []}, {"text": "Or only the best and worst sets can be used as representative examples.", "labels": [], "entities": []}, {"text": "We explore the consequences of adopting either of these options.", "labels": [], "entities": []}, {"text": "For the first set of experiments, we divide all inputs based on the mean value of the average system scores as in.", "labels": [], "entities": []}, {"text": "All multi-document results reported in this paper are based on the use of the six significant features discussed in Section 4.", "labels": [], "entities": []}, {"text": "data was used for 10-fold cross validation.", "labels": [], "entities": []}, {"text": "We ex-perimented with three classifiers available in Rlogistic regression (LogR), decision tree (DTree) and support vector machines (SVM).", "labels": [], "entities": []}, {"text": "SVM and decision tree classifiers are libraries under CRAN packages e1071 and rpart.", "labels": [], "entities": []}, {"text": "Since our development set was very small (only 29 inputs), we did not perform any parameter tuning.", "labels": [], "entities": []}, {"text": "There is nearly equal number of inputs on either side of the average system performance and the random baseline performance in this case would give 50% accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 152, "end_pos": 160, "type": "METRIC", "confidence": 0.9986206293106079}]}], "tableCaptions": [{"text": " Table 1: Correlations between input features and average system performance for multi-document inputs  of DUC 2001-2003, 2004G (generic task), 2004B (biographical task), All data (2002-2004) -UNnor- malized and Normalized coverage scores. P-values smaller than 0.05 are marked by *.", "labels": [], "entities": [{"text": "DUC 2001-2003", "start_pos": 107, "end_pos": 120, "type": "DATASET", "confidence": 0.9328954219818115}, {"text": "UNnor", "start_pos": 193, "end_pos": 198, "type": "DATASET", "confidence": 0.79097580909729}, {"text": "Normalized coverage scores", "start_pos": 212, "end_pos": 238, "type": "METRIC", "confidence": 0.7152489821116129}]}, {"text": " Table 2: Correlations between input features and  average system performance for single doc. inputs  of DUC'01, '02, All ('01+'02) N-normalized. P- values smaller than 0.05 are marked by *.", "labels": [], "entities": []}, {"text": " Table 4: Single document input classification Pre- cision (P), Recall (R),and F score (F) for difficult  inputs on DUC'01 and '02 (total 432 examples)  divided into 2 classes based on the average cover- age score (217 difficult and 215 easy inputs).", "labels": [], "entities": [{"text": "Pre- cision (P)", "start_pos": 47, "end_pos": 62, "type": "METRIC", "confidence": 0.9463288287321726}, {"text": "Recall (R)", "start_pos": 64, "end_pos": 74, "type": "METRIC", "confidence": 0.9530668407678604}, {"text": "F score (F)", "start_pos": 79, "end_pos": 90, "type": "METRIC", "confidence": 0.981701111793518}, {"text": "DUC'01", "start_pos": 116, "end_pos": 122, "type": "DATASET", "confidence": 0.9552282094955444}, {"text": "cover- age score", "start_pos": 197, "end_pos": 213, "type": "METRIC", "confidence": 0.7722908854484558}]}, {"text": " Table 3: Multi-document input classification results on UNnormalized and Normalized data from DUC  2002 to 2004. Both Normalized and UNormalized data contain 109 difficult and 87 easy inputs. Since  the split is not balanced, the accuracy of classification as well as the Precision (P), Recall (R) and F score  (F) are reported for both classes of easy and diff(icult) inputs.", "labels": [], "entities": [{"text": "Multi-document input classification", "start_pos": 10, "end_pos": 45, "type": "TASK", "confidence": 0.6368417839209238}, {"text": "UNnormalized and Normalized data from DUC  2002", "start_pos": 57, "end_pos": 104, "type": "DATASET", "confidence": 0.7353942181382861}, {"text": "accuracy", "start_pos": 231, "end_pos": 239, "type": "METRIC", "confidence": 0.9994338154792786}, {"text": "Precision (P)", "start_pos": 273, "end_pos": 286, "type": "METRIC", "confidence": 0.9550808221101761}, {"text": "Recall (R)", "start_pos": 288, "end_pos": 298, "type": "METRIC", "confidence": 0.9154352098703384}, {"text": "F score  (F)", "start_pos": 303, "end_pos": 315, "type": "METRIC", "confidence": 0.9449115753173828}]}, {"text": " Table 5: Single-document-input classification Pre- cision (P), Recall (R), and F score (F) for difficult  inputs on a random sample of 196 observations (99  difficult/97 easy) from DUC'01 and '02.", "labels": [], "entities": [{"text": "Pre- cision (P)", "start_pos": 47, "end_pos": 62, "type": "METRIC", "confidence": 0.9629072745641073}, {"text": "Recall (R)", "start_pos": 64, "end_pos": 74, "type": "METRIC", "confidence": 0.9626856595277786}, {"text": "F score (F)", "start_pos": 80, "end_pos": 91, "type": "METRIC", "confidence": 0.9843904852867127}, {"text": "DUC'01", "start_pos": 182, "end_pos": 188, "type": "DATASET", "confidence": 0.9608762264251709}]}, {"text": " Table 6: Performance of multiple classifiers on extreme observations from single and multi-document  data (100% data = 196 data points in both cases divided into 2 classes on the basis of average coverge  score). Reported precision (P), recall (R) and F score (F) are for difficult inputs. Experiments on ex- tremes use equal number of examples from each class -baseline performance is 50%. Systems whose  performance is significantly better than the specified numbers are shown in brackets (S-SVM, D-Decision  Tree, L-Logistic Regression).", "labels": [], "entities": [{"text": "Reported precision (P)", "start_pos": 214, "end_pos": 236, "type": "METRIC", "confidence": 0.8632945656776428}, {"text": "recall (R)", "start_pos": 238, "end_pos": 248, "type": "METRIC", "confidence": 0.9525564312934875}, {"text": "F score (F)", "start_pos": 253, "end_pos": 264, "type": "METRIC", "confidence": 0.9646366477012634}]}]}