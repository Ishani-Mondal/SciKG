{"title": [{"text": "Deterministic shift-reduce parsing for unification-based grammars by using default unification", "labels": [], "entities": [{"text": "Deterministic shift-reduce parsing", "start_pos": 0, "end_pos": 34, "type": "TASK", "confidence": 0.8058057824770609}]}], "abstractContent": [{"text": "Many parsing techniques including parameter estimation assume the use of a packed parse forest for efficient and accurate parsing.", "labels": [], "entities": [{"text": "parsing", "start_pos": 5, "end_pos": 12, "type": "TASK", "confidence": 0.9780502915382385}, {"text": "parameter estimation", "start_pos": 34, "end_pos": 54, "type": "TASK", "confidence": 0.7169553637504578}]}, {"text": "However, they have several inherent problems deriving from the restriction of locality in the packed parse forest.", "labels": [], "entities": []}, {"text": "Deterministic parsing is one of solutions that can achieve simple and fast parsing without the mechanisms of the packed parse forest by accurately choosing search paths.", "labels": [], "entities": [{"text": "Deterministic parsing", "start_pos": 0, "end_pos": 21, "type": "TASK", "confidence": 0.6943327188491821}]}, {"text": "We propose (i) deter-ministic shift-reduce parsing for unification based grammars, and (ii) best-first shift-reduce parsing with beam threshold-ing for unification-based grammars.", "labels": [], "entities": []}, {"text": "De-terministic parsing cannot simply be applied to unification-based grammar parsing , which often fails because of its hard constraints.", "labels": [], "entities": [{"text": "unification-based grammar parsing", "start_pos": 51, "end_pos": 84, "type": "TASK", "confidence": 0.6976265112559}]}, {"text": "Therefore, it is developed by using default unification, which almost always succeeds in unification by over-writing inconsistent constraints in grammars .", "labels": [], "entities": []}], "introductionContent": [{"text": "Over the last few decades, probabilistic unification-based grammar parsing has been investigated intensively.", "labels": [], "entities": [{"text": "unification-based grammar parsing", "start_pos": 41, "end_pos": 74, "type": "TASK", "confidence": 0.7630606293678284}]}, {"text": "Previous studies;) defined a probabilistic model of unification-based grammars, including head-driven phrase structure grammar (HPSG), lexical functional grammar (LFG) and combinatory categorial grammar (CCG), as a maximum entropy model).", "labels": [], "entities": []}, {"text": "Geman and Johnson) and) proposed a feature forest, which is a dynamic programming algorithm for estimating the probabilities of all possible parse candidates.", "labels": [], "entities": []}, {"text": "A feature forest can estimate the model parameters without unpacking the parse forest, i.e., the chart and its edges.", "labels": [], "entities": []}, {"text": "Feature forests have been used successfully for probabilistic HPSG and CCG, and its parsing is empirically known to be fast and accurate, especially with supertagging).", "labels": [], "entities": [{"text": "parsing", "start_pos": 84, "end_pos": 91, "type": "TASK", "confidence": 0.9597642421722412}]}, {"text": "Both estimation and parsing with the packed parse forest, however, have several inherent problems deriving from the restriction of locality.", "labels": [], "entities": [{"text": "estimation", "start_pos": 5, "end_pos": 15, "type": "TASK", "confidence": 0.9519537687301636}, {"text": "parsing", "start_pos": 20, "end_pos": 27, "type": "TASK", "confidence": 0.9819751381874084}]}, {"text": "First, feature functions can be defined only for local structures, which limit the parser's performance.", "labels": [], "entities": []}, {"text": "This is because parsers segment parse trees into constituents and factor equivalent constituents into a single constituent (edge) in a chart to avoid the same calculation.", "labels": [], "entities": [{"text": "parsers segment parse trees into constituents", "start_pos": 16, "end_pos": 61, "type": "TASK", "confidence": 0.8267193337281545}]}, {"text": "This also means that the semantic structures must be segmented.", "labels": [], "entities": []}, {"text": "This is a crucial problem when we think of designing semantic structures other than predicate argument structures, e.g., synchronous grammars for machine translation.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 146, "end_pos": 165, "type": "TASK", "confidence": 0.7482225298881531}]}, {"text": "The size of the constituents will be exponential if the semantic structures are not segmented.", "labels": [], "entities": []}, {"text": "Lastly, we need delayed evaluation for evaluating feature functions.", "labels": [], "entities": []}, {"text": "The application of feature functions must be delayed until all the values in the segmented constituents are instantiated.", "labels": [], "entities": []}, {"text": "This is because values in parse trees can propagate anywhere throughout the parse tree by unification.", "labels": [], "entities": []}, {"text": "For example, values may propagate from the root node to terminal nodes, and the final form of the terminal nodes is unknown until the parser finishes constructing the whole parse tree.", "labels": [], "entities": []}, {"text": "Consequently, the design of grammars, semantic structures, and feature functions becomes complex.", "labels": [], "entities": []}, {"text": "To solve the problem of locality, several approaches, such as reranking), shift-reduce parsing, search optimization learning () and sampling methods, were studied.", "labels": [], "entities": [{"text": "shift-reduce parsing", "start_pos": 74, "end_pos": 94, "type": "TASK", "confidence": 0.5946361869573593}]}, {"text": "In this paper, we investigate shift-reduce parsing approach for unification-based grammars without the mechanisms of the packed parse forest.", "labels": [], "entities": []}, {"text": "Shift-reduce parsing for CFG and dependency parsing have recently been studied (, through approaches based essentially on deterministic parsing.", "labels": [], "entities": [{"text": "Shift-reduce parsing", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.7781481146812439}, {"text": "dependency parsing", "start_pos": 33, "end_pos": 51, "type": "TASK", "confidence": 0.7906390130519867}]}, {"text": "These techniques, however, cannot simply be applied to unification-based grammar parsing because it can fail as a result of its hard constraints in the grammar.", "labels": [], "entities": [{"text": "unification-based grammar parsing", "start_pos": 55, "end_pos": 88, "type": "TASK", "confidence": 0.7962002158164978}]}, {"text": "Therefore, in this study, we propose deterministic parsing for unification-based grammars by using default unification, which almost always succeeds in unification by overwriting inconsistent constraints in the grammars.", "labels": [], "entities": [{"text": "deterministic parsing", "start_pos": 37, "end_pos": 58, "type": "TASK", "confidence": 0.6745886206626892}]}, {"text": "We further pursue best-first shift-reduce parsing for unificationbased grammars.", "labels": [], "entities": []}, {"text": "Sections 2 and 3 explain unification-based grammars and default unification, respectively.", "labels": [], "entities": [{"text": "default unification", "start_pos": 56, "end_pos": 75, "type": "TASK", "confidence": 0.6395490616559982}]}, {"text": "Shift-reduce parsing for unification-based grammars is presented in Section 4.", "labels": [], "entities": [{"text": "Shift-reduce parsing", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.8577611148357391}]}, {"text": "Section 5 discusses our experiments, and Section 6 concludes the paper.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluated the speed and accuracy of parsing with Enju 2.3\u03b2, an HPSG for English).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 27, "end_pos": 35, "type": "METRIC", "confidence": 0.9995028972625732}, {"text": "parsing", "start_pos": 39, "end_pos": 46, "type": "TASK", "confidence": 0.9813246130943298}]}, {"text": "The lexicon for the grammar was extracted from Sections 02-21 of the Penn Treebank (39,832 sentences).", "labels": [], "entities": [{"text": "Penn Treebank", "start_pos": 69, "end_pos": 82, "type": "DATASET", "confidence": 0.830396831035614}]}, {"text": "The grammar consisted of 2,302 lexical entries for 11,187 words.", "labels": [], "entities": []}, {"text": "Two probabilistic classifiers for selecting shift-reduce actions were trained using the same portion of the treebank.", "labels": [], "entities": []}, {"text": "One is trained using normal unification, and the other is trained using default unification.", "labels": [], "entities": []}, {"text": "We measured the accuracy of the predicate argument relation output of the parser.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 16, "end_pos": 24, "type": "METRIC", "confidence": 0.9995564818382263}]}, {"text": "A predicate-argument relation is defined as a tuple \u2329\u00ed \u00b5\u00ed\u00bc\u008e, \u00ed \u00b5\u00ed\u00b1\u00a4 \ud97b\udf59 , \u00ed \u00b5\u00ed\u00b1\u008e, \u00ed \u00b5\u00ed\u00b1\u00a4 \ud97b\udf59 \u232a, where \u00ed \u00b5\u00ed\u00bc\u008e is the predicate type (e.g., adjective, intransitive verb), \u00ed \u00b5\u00ed\u00b1\u00a4 \ud97b\udf59 is the headword of the predicate, \u00ed \u00b5\u00ed\u00b1\u008e is the argument label (MOD-ARG, ARG1, \u2026, ARG4), and \u00ed \u00b5\u00ed\u00b1\u00a4 \ud97b\udf59 is the headword of the argument.", "labels": [], "entities": [{"text": "MOD-ARG", "start_pos": 238, "end_pos": 245, "type": "DATASET", "confidence": 0.8572911024093628}, {"text": "\u2026", "start_pos": 253, "end_pos": 254, "type": "METRIC", "confidence": 0.6735267639160156}, {"text": "ARG4", "start_pos": 256, "end_pos": 260, "type": "DATASET", "confidence": 0.6490584015846252}]}, {"text": "The labeled precision (LP) / labeled recall (LR) is the ratio of tuples correctly identified by the parser, and the labeled F-score (LF) is the harmonic mean of the LP and LR.", "labels": [], "entities": [{"text": "labeled precision (LP)", "start_pos": 4, "end_pos": 26, "type": "METRIC", "confidence": 0.8027008295059204}, {"text": "recall (LR)", "start_pos": 37, "end_pos": 48, "type": "METRIC", "confidence": 0.9225353300571442}, {"text": "F-score (LF)", "start_pos": 124, "end_pos": 136, "type": "METRIC", "confidence": 0.9443002045154572}]}, {"text": "This evaluation scheme was the same one used in previous evaluations of lexicalized grammars).", "labels": [], "entities": []}, {"text": "The experiments were conducted on an Intel Xeon 5160 server with 3.0-GHz CPUs.", "labels": [], "entities": []}, {"text": "Section 22 of the Penn Treebank was used as the development set, and the performance was evaluated using sentences of \u2264 100 words in Section 23.", "labels": [], "entities": [{"text": "Section 22 of the Penn Treebank", "start_pos": 0, "end_pos": 31, "type": "DATASET", "confidence": 0.7286142259836197}]}, {"text": "The LP, LR, and LF were evaluated for Section 23.", "labels": [], "entities": [{"text": "Section 23", "start_pos": 38, "end_pos": 48, "type": "TASK", "confidence": 0.8854227364063263}]}, {"text": "lists the results of parsing for Section 23.", "labels": [], "entities": [{"text": "parsing", "start_pos": 21, "end_pos": 28, "type": "TASK", "confidence": 0.9901166558265686}]}, {"text": "In the table, \"Avg.", "labels": [], "entities": []}, {"text": "time\" is the average parsing time for the tested sentences.", "labels": [], "entities": []}, {"text": "\"# of backtrack\" is the total number of backtracking steps that occurred during parsing.", "labels": [], "entities": []}, {"text": "# of states\" is the average number of states for the tested sentences.", "labels": [], "entities": []}, {"text": "\"# of dead end\" is the number of sentences for which parsing failed.", "labels": [], "entities": []}, {"text": "\"# of non-sentential success\" is the number of sentences for which parsing succeeded but did not generate a parse tree satisfying the sentential condition.", "labels": [], "entities": []}, {"text": "\"det\" means the deterministic shift-reduce parsing proposed in this paper.", "labels": [], "entities": [{"text": "deterministic shift-reduce parsing", "start_pos": 16, "end_pos": 50, "type": "TASK", "confidence": 0.6048397024472555}]}, {"text": "\"back\u00ed \u00b5\u00ed\u00b1\u009b\" means shift-reduce parsing with backtracking at most \u00ed \u00b5\u00ed\u00b1\u009b times for each sentence.", "labels": [], "entities": [{"text": "back\u00ed \u00b5\u00ed\u00b1\u009b\"", "start_pos": 1, "end_pos": 12, "type": "METRIC", "confidence": 0.8967359066009521}]}, {"text": "\"du\" indicates that default unification was used.", "labels": [], "entities": []}, {"text": "\"beam\u00ed \u00b5\u00ed\u00b1\u008f\" means best-first shift-reduce parsing with beam threshold \u00ed \u00b5\u00ed\u00b1\u008f.", "labels": [], "entities": [{"text": "beam\u00ed \u00b5\u00ed\u00b1\u008f\"", "start_pos": 1, "end_pos": 12, "type": "METRIC", "confidence": 0.6115059554576874}, {"text": "beam threshold \u00ed \u00b5\u00ed\u00b1\u008f", "start_pos": 56, "end_pos": 77, "type": "METRIC", "confidence": 0.836557948589325}]}, {"text": "The upper half of the table gives the results obtained using gold POSs, while the lower half gives the results obtained using an automatic POS tagger.", "labels": [], "entities": [{"text": "POS tagger", "start_pos": 139, "end_pos": 149, "type": "TASK", "confidence": 0.6949311792850494}]}, {"text": "The maximum number of backtracking steps and the beam threshold were determined by observing the performance for the development set (Section 22) such that the LF was maximized with a parsing time of less than 500 ms/sentence (except \"beam(403.4)\").", "labels": [], "entities": [{"text": "beam threshold", "start_pos": 49, "end_pos": 63, "type": "METRIC", "confidence": 0.9576823711395264}, {"text": "LF", "start_pos": 160, "end_pos": 162, "type": "METRIC", "confidence": 0.9211371541023254}]}, {"text": "The performance of \"beam(403.4)\" was evaluated to seethe limit of the performance of the beam-search parsing.", "labels": [], "entities": [{"text": "beam", "start_pos": 20, "end_pos": 24, "type": "METRIC", "confidence": 0.9856023192405701}, {"text": "beam-search parsing", "start_pos": 89, "end_pos": 108, "type": "TASK", "confidence": 0.7230129837989807}]}, {"text": "Deterministic parsing without default unification achieved accuracy with an LF of around 79.1% (Section 23, gold POS).", "labels": [], "entities": [{"text": "Deterministic parsing", "start_pos": 0, "end_pos": 21, "type": "TASK", "confidence": 0.5860943794250488}, {"text": "accuracy", "start_pos": 59, "end_pos": 67, "type": "METRIC", "confidence": 0.9995926022529602}, {"text": "LF", "start_pos": 76, "end_pos": 78, "type": "METRIC", "confidence": 0.9916587471961975}, {"text": "gold POS)", "start_pos": 108, "end_pos": 117, "type": "METRIC", "confidence": 0.7690118948618571}]}, {"text": "With backtracking, the LF increased to 83.6%.", "labels": [], "entities": [{"text": "the", "start_pos": 19, "end_pos": 22, "type": "METRIC", "confidence": 0.9900751709938049}, {"text": "LF", "start_pos": 23, "end_pos": 25, "type": "METRIC", "confidence": 0.5222748517990112}]}, {"text": "shows the relation between LF and parsing time for the development set (Section 22, gold POS).", "labels": [], "entities": [{"text": "LF", "start_pos": 27, "end_pos": 29, "type": "METRIC", "confidence": 0.9892016649246216}, {"text": "parsing", "start_pos": 34, "end_pos": 41, "type": "TASK", "confidence": 0.9442192316055298}, {"text": "gold POS)", "start_pos": 84, "end_pos": 93, "type": "METRIC", "confidence": 0.7512523929278055}]}, {"text": "As seen in the figure, the LF increased as the parsing time increased.", "labels": [], "entities": [{"text": "LF", "start_pos": 27, "end_pos": 29, "type": "METRIC", "confidence": 0.9813634157180786}, {"text": "parsing", "start_pos": 47, "end_pos": 54, "type": "TASK", "confidence": 0.9629037976264954}]}, {"text": "The increase in LF for deterministic parsing without default unification, however, seems to have saturated around 83.3%.", "labels": [], "entities": [{"text": "LF", "start_pos": 16, "end_pos": 18, "type": "METRIC", "confidence": 0.9868175983428955}, {"text": "deterministic parsing without default unification", "start_pos": 23, "end_pos": 72, "type": "TASK", "confidence": 0.6129181087017059}]}, {"text": "also shows that deterministic parsing with default unification achieved higher accuracy, with an LF of around 87.6% (Section 23, gold POS), without backtracking.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 79, "end_pos": 87, "type": "METRIC", "confidence": 0.9993477463722229}, {"text": "LF", "start_pos": 97, "end_pos": 99, "type": "METRIC", "confidence": 0.989734411239624}, {"text": "gold POS)", "start_pos": 129, "end_pos": 138, "type": "METRIC", "confidence": 0.7204772035280863}]}, {"text": "Default unification is effective: it ran faster and achieved higher accuracy than deterministic parsing with normal unification.", "labels": [], "entities": [{"text": "Default unification", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.7631941735744476}, {"text": "accuracy", "start_pos": 68, "end_pos": 76, "type": "METRIC", "confidence": 0.997858464717865}]}, {"text": "The beam-search parsing without default unification achieved high accuracy, with an LF of around 87.0%, but is still worse than deterministic parsing with default unification.", "labels": [], "entities": [{"text": "beam-search parsing", "start_pos": 4, "end_pos": 23, "type": "TASK", "confidence": 0.74481201171875}, {"text": "accuracy", "start_pos": 66, "end_pos": 74, "type": "METRIC", "confidence": 0.9996782541275024}, {"text": "LF", "start_pos": 84, "end_pos": 86, "type": "METRIC", "confidence": 0.9912788271903992}]}, {"text": "However, with default unification, it achieved the best performance, with an LF of around 88.5%, in the settings of parsing timeless than 500ms/sentence for Section 22.", "labels": [], "entities": [{"text": "LF", "start_pos": 77, "end_pos": 79, "type": "METRIC", "confidence": 0.9965521097183228}]}, {"text": "For comparison with previous studies using the packed parse forest, the performances of Miyao's parser, Ninomiya's parser, Matsuzaki's parser and Sagae's parser are also listed in.", "labels": [], "entities": []}, {"text": "Miyao's parser is based on a probabilistic model estimated only by a feature forest.", "labels": [], "entities": []}, {"text": "Ninomiya's parser is a mixture of the feature forest\" was very slow, but the accuracy was higher than any other parsers except Sagae's parser.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 77, "end_pos": 85, "type": "METRIC", "confidence": 0.9997554421424866}]}, {"text": "shows the behaviors of default unification for \"det+du.\"", "labels": [], "entities": []}, {"text": "The table shows the 20 most frequent path values that were overwritten by default unification in Section 22.", "labels": [], "entities": []}, {"text": "In most of the cases, the overwritten path values were in the selection features, i.e., subcategorization frames (COMPS:, SUBJ:, SPR:, CONJ:) and modifiee specification (MOD:).", "labels": [], "entities": []}, {"text": "The column of 'Default type' indicates the default types which were overwritten by the strict types in the column of 'Strict type,' and the last column is the frequency of overwriting.", "labels": [], "entities": [{"text": "frequency", "start_pos": 159, "end_pos": 168, "type": "METRIC", "confidence": 0.9639862179756165}]}, {"text": "'cons' means a non-empty list, and 'nil' means an empty list.", "labels": [], "entities": []}, {"text": "In most of the cases, modifiee and subcategorization frames were changed from empty to non-empty and vice versa.", "labels": [], "entities": []}, {"text": "From the table, overwriting of head information was also observed, e.g., 'noun' was changed to 'verb.'", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Experimental results for Section 23.", "labels": [], "entities": []}]}