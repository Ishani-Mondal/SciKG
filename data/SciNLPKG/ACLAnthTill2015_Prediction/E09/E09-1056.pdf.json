{"title": [{"text": "Improvements in Analogical Learning: Application to Translating multi-Terms of the Medical Domain", "labels": [], "entities": []}], "abstractContent": [{"text": "Handling terminology is an important matter in a translation workflow.", "labels": [], "entities": [{"text": "Handling terminology", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.7825236618518829}, {"text": "translation workflow", "start_pos": 49, "end_pos": 69, "type": "TASK", "confidence": 0.9199850261211395}]}, {"text": "However, current Machine Translation (MT) systems do not yet propose anything proactive upon tools which assist in managing termi-nological databases.", "labels": [], "entities": [{"text": "Machine Translation (MT)", "start_pos": 17, "end_pos": 41, "type": "TASK", "confidence": 0.8608352303504944}]}, {"text": "In this work, we investigate several enhancements to analog-ical learning and test our implementation on translating medical terms.", "labels": [], "entities": [{"text": "translating medical terms", "start_pos": 105, "end_pos": 130, "type": "TASK", "confidence": 0.9027963876724243}]}, {"text": "We show that the analogical engine works equally well when translating from and into a morphologically rich language, or when dealing with language pairs written in different scripts.", "labels": [], "entities": []}, {"text": "Combining it with a phrase-based statistical engine leads to significant improvements.", "labels": [], "entities": []}], "introductionContent": [{"text": "If machine translation is to meet commercial needs, it must offer a sensible approach to translating terms.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 3, "end_pos": 22, "type": "TASK", "confidence": 0.7849406898021698}, {"text": "translating terms", "start_pos": 89, "end_pos": 106, "type": "TASK", "confidence": 0.8994896709918976}]}, {"text": "Currently, MT systems offer at best database management tools which allow a human (typically a translator, a terminologist or even the vendor of the system) to specify bilingual terminological entries.", "labels": [], "entities": [{"text": "MT", "start_pos": 11, "end_pos": 13, "type": "TASK", "confidence": 0.962029755115509}]}, {"text": "More advanced tools are meant to identify inconsistencies in terminological translations and might prove useful in controlledlanguage situations (.", "labels": [], "entities": []}, {"text": "One approach to translate terms consists in using a domain-specific parallel corpus with standard alignment techniques () to mine new translations.", "labels": [], "entities": []}, {"text": "Massive amounts of parallel data are certainly available in several pairs of languages for domains such as parliament debates or the like.", "labels": [], "entities": []}, {"text": "However, having at our disposal a domain-specific (e.g. computer science) bitext with an adequate coverage is another issue.", "labels": [], "entities": []}, {"text": "One might argue that domain-specific comparable (or perhaps unrelated) corpora are easier to acquire, in which case context-vector techniques can be used to identify the translation of terms.", "labels": [], "entities": []}, {"text": "We certainly agree with that point of view to a certain extent, but as discussed by, for many specific domains and pairs of languages, such resources simply do not exist.", "labels": [], "entities": []}, {"text": "Furthermore, the task of translation identification is more difficult and error-prone.", "labels": [], "entities": [{"text": "translation identification", "start_pos": 25, "end_pos": 51, "type": "TASK", "confidence": 0.9905560910701752}]}, {"text": "Analogical learning has recently regained some interest in the NLP community.", "labels": [], "entities": [{"text": "Analogical learning", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.9306041598320007}]}, {"text": "proposed a machine translation system entirely based on the concept of formal analogy, that is, analogy on forms.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 11, "end_pos": 30, "type": "TASK", "confidence": 0.7411028742790222}]}, {"text": "applied analogical learning to several morphological tasks also involving analogies on words.", "labels": [], "entities": [{"text": "analogical learning", "start_pos": 8, "end_pos": 27, "type": "TASK", "confidence": 0.8702867031097412}]}, {"text": "applied it to the task of translating unknown words in several European languages, an idea investigated as well by fora Japanese to English translation task.", "labels": [], "entities": [{"text": "translating unknown words in several European languages", "start_pos": 26, "end_pos": 81, "type": "TASK", "confidence": 0.8368401868002755}, {"text": "English translation", "start_pos": 132, "end_pos": 151, "type": "TASK", "confidence": 0.6705948412418365}]}, {"text": "In this study, we improve the state-of-the-art of analogical learning by (i) proposing a simple yet effective implementation of an analogical solver; (ii) proposing an efficient solution to the search issue embedded in analogical learning, (iii) investigating whether a classifier can be trained to recognize bad candidates produced by analogical learning.", "labels": [], "entities": []}, {"text": "We evaluate our analogical engine on the task of translating terms of the medical domain; a domain well-known for its tendency to create new words, many of which being complex lexical constructions.", "labels": [], "entities": []}, {"text": "Our experiments involve five language pairs, including languages with very different morphological systems.", "labels": [], "entities": []}, {"text": "In the remainder of this paper, we first present in Section 2 the principle of analogical learning.", "labels": [], "entities": [{"text": "analogical learning", "start_pos": 79, "end_pos": 98, "type": "TASK", "confidence": 0.8826190829277039}]}, {"text": "Practical issues in analogical learning are discussed in Section 3 along with our solutions.", "labels": [], "entities": [{"text": "analogical learning", "start_pos": 20, "end_pos": 39, "type": "TASK", "confidence": 0.9474228322505951}]}, {"text": "In Section 4, we report on experiments we conducted with our analogical device.", "labels": [], "entities": []}, {"text": "We conclude this study and discuss future work in Section 5.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: The 3-most frequent solutions generated  by our solver, for different sampling sizes s, for  the equation [reader : readable = doer : ? ]. nb  indicates the number of (different) solutions gen- erated. According to our definition, there are 32  distinct solutions to this equation. Note that our  solver has no problem producing doable.", "labels": [], "entities": []}, {"text": " Table 3: Main characteristics of our datasets. nb  indicates the number of pairs of terms in a bi- text, u f % (u e %) stands for the percentage of uni- terms in the Foreign (English) part. oov% indi- cates the percentage of out-of-vocabulary forms  (space-separated forms of TEST unseen in TRAIN).", "labels": [], "entities": []}, {"text": " Table 4: Main characteristics of the generator, as a  function of the translation directions (TEST).", "labels": [], "entities": [{"text": "TEST", "start_pos": 95, "end_pos": 99, "type": "METRIC", "confidence": 0.9881979823112488}]}, {"text": " Table 5: Precision (p) and recall (r) of some classifiers on the TEST material.", "labels": [], "entities": [{"text": "Precision (p)", "start_pos": 10, "end_pos": 23, "type": "METRIC", "confidence": 0.9336451888084412}, {"text": "recall (r)", "start_pos": 28, "end_pos": 38, "type": "METRIC", "confidence": 0.9620392471551895}, {"text": "TEST material", "start_pos": 66, "end_pos": 79, "type": "DATASET", "confidence": 0.902240127325058}]}, {"text": " Table 6: Precision and recall at rank 1 and 10 for the Foreign-to-English translation tasks (TEST).", "labels": [], "entities": [{"text": "Precision", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9932017922401428}, {"text": "recall", "start_pos": 24, "end_pos": 30, "type": "METRIC", "confidence": 0.9975413084030151}, {"text": "Foreign-to-English translation tasks (TEST)", "start_pos": 56, "end_pos": 99, "type": "TASK", "confidence": 0.7800225615501404}]}, {"text": " Table 7: Translation performances on TEST. P smt  stands for the precision and recall of the SMT en- gine. \u2206B indicates the absolute gain in BLEU  score of the combined system.", "labels": [], "entities": [{"text": "Translation", "start_pos": 10, "end_pos": 21, "type": "TASK", "confidence": 0.9452729225158691}, {"text": "TEST", "start_pos": 38, "end_pos": 42, "type": "DATASET", "confidence": 0.6803688406944275}, {"text": "precision", "start_pos": 66, "end_pos": 75, "type": "METRIC", "confidence": 0.9996127486228943}, {"text": "recall", "start_pos": 80, "end_pos": 86, "type": "METRIC", "confidence": 0.9991982579231262}, {"text": "SMT en- gine", "start_pos": 94, "end_pos": 106, "type": "TASK", "confidence": 0.8385906517505646}, {"text": "\u2206B", "start_pos": 108, "end_pos": 110, "type": "METRIC", "confidence": 0.7568404078483582}, {"text": "BLEU  score", "start_pos": 142, "end_pos": 153, "type": "METRIC", "confidence": 0.9803707003593445}]}]}