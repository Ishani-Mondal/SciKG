{"title": [{"text": "N-gram-based Statistical Machine Translation versus Syntax Augmented Machine Translation: comparison and system combination", "labels": [], "entities": [{"text": "Statistical Machine Translation", "start_pos": 13, "end_pos": 44, "type": "TASK", "confidence": 0.6255278388659159}, {"text": "Syntax Augmented Machine Translation", "start_pos": 52, "end_pos": 88, "type": "TASK", "confidence": 0.5674255788326263}]}], "abstractContent": [{"text": "In this paper we compare and contrast two approaches to Machine Translation (MT): the CMU-UKA Syntax Augmented Machine Translation system (SAMT) and UPC-TALP N-gram-based Statistical Machine Translation (SMT).", "labels": [], "entities": [{"text": "Machine Translation (MT)", "start_pos": 56, "end_pos": 80, "type": "TASK", "confidence": 0.8860152721405029}, {"text": "CMU-UKA Syntax Augmented Machine Translation", "start_pos": 86, "end_pos": 130, "type": "TASK", "confidence": 0.7257427930831909}, {"text": "UPC-TALP N-gram-based Statistical Machine Translation (SMT)", "start_pos": 149, "end_pos": 208, "type": "TASK", "confidence": 0.6920415870845318}]}, {"text": "SAMT is a hierarchical syntax-driven translation system underlain by a phrase-based model and a target part parse tree.", "labels": [], "entities": []}, {"text": "In N-gram-based SMT, the translation process is based on bilingual units related to word-to-word alignment and statistical modeling of the bilingual context following a maximum-entropy framework.", "labels": [], "entities": [{"text": "SMT", "start_pos": 16, "end_pos": 19, "type": "TASK", "confidence": 0.7117674350738525}, {"text": "word-to-word alignment", "start_pos": 84, "end_pos": 106, "type": "TASK", "confidence": 0.709971472620964}]}, {"text": "We provide a step-by-step comparison of the systems and report results in terms of automatic evaluation metrics and required computational resources fora smaller Arabic-to-English translation task (1.5M tokens in the training corpus).", "labels": [], "entities": [{"text": "Arabic-to-English translation task", "start_pos": 162, "end_pos": 196, "type": "TASK", "confidence": 0.7491351962089539}]}, {"text": "Human error analysis clarifies advantages and disadvantages of the systems under consideration.", "labels": [], "entities": []}, {"text": "Finally, we combine the output of both systems to yield significant improvements in translation quality.", "labels": [], "entities": [{"text": "translation", "start_pos": 84, "end_pos": 95, "type": "TASK", "confidence": 0.9621413350105286}]}], "introductionContent": [{"text": "There is an ongoing controversy regarding whether or not information about the syntax of language can benefit MT or contribute to a hybrid system.", "labels": [], "entities": [{"text": "MT", "start_pos": 110, "end_pos": 112, "type": "TASK", "confidence": 0.9817891716957092}]}, {"text": "Classical IBM word-based models were recently augmented with a phrase translation capability, as shown in, or in more recent implementation, the MOSES MT system 1 (.", "labels": [], "entities": [{"text": "phrase translation", "start_pos": 63, "end_pos": 81, "type": "TASK", "confidence": 0.795196920633316}, {"text": "MOSES MT system 1", "start_pos": 145, "end_pos": 162, "type": "DATASET", "confidence": 0.7754836827516556}]}, {"text": "In parallel to the phrasebased approach, the N -gram-based approach appeared ( ).", "labels": [], "entities": []}, {"text": "It stemms from www.statmt.org/moses/ the Finite-State Transducers paradigm, and is extended to the log-linear modeling framework, as shown in ).", "labels": [], "entities": []}, {"text": "A system following this approach deals with bilingual units, called tuples, which are composed of one or more words from the source language and zero or more words from the target one.", "labels": [], "entities": []}, {"text": "The N -gram-based systems allow for linguistically motivated word reordering by implementing word order monotonization.", "labels": [], "entities": [{"text": "word reordering", "start_pos": 61, "end_pos": 76, "type": "TASK", "confidence": 0.7303203344345093}]}, {"text": "Prior to the SMT revolution, a major part of MT systems was developed using rule-based algorithms; however, starting from the 1990's, syntax-driven systems based on phrase hierarchy have gained popularity.", "labels": [], "entities": [{"text": "SMT", "start_pos": 13, "end_pos": 16, "type": "TASK", "confidence": 0.9928197264671326}, {"text": "MT", "start_pos": 45, "end_pos": 47, "type": "TASK", "confidence": 0.9911744594573975}]}, {"text": "A representative sample of modern syntax-based systems includes models based on bilingual synchronous grammar, parse tree-to-string translation models) and nonisomorphic tree-to-tree mappings.", "labels": [], "entities": [{"text": "parse tree-to-string translation", "start_pos": 111, "end_pos": 143, "type": "TASK", "confidence": 0.6238496502240499}]}, {"text": "The orthodox phrase-based model was enhanced in, where a hierarchical phrase model allowing for multiple generalizations within each phrase was introduced.", "labels": [], "entities": []}, {"text": "The open-source toolkit SAMT 2 () is a further evolution of this approach, in which syntactic categories extracted from the target side parse tree are directly assigned to the hierarchically structured phrases.", "labels": [], "entities": []}, {"text": "Several publications discovering similarities and differences between distinct translation models have been written over the last few years.", "labels": [], "entities": []}, {"text": "In, the N -gram-based system is contrasted with a state-of-the-art phrase-based framework, while in, the authors seek to estimate the advantages, weakest points and possible overlap between syntaxbased MT and phrase-based SMT.", "labels": [], "entities": []}, {"text": "In the comparison of phrase-based , \"Chiang's style\" hirearchical system and SAMT is pro-vided.", "labels": [], "entities": []}, {"text": "In this study, we intend to compare the differences and similarities of the statistical N -grambased SMT approach and the SAMT system.", "labels": [], "entities": [{"text": "SMT", "start_pos": 101, "end_pos": 104, "type": "TASK", "confidence": 0.9596003890037537}]}, {"text": "The comparison is performed on a small Arabic-toEnglish translation task from the news domain.", "labels": [], "entities": []}], "datasetContent": [{"text": "As training corpus, we used the 50K first-lines extraction from the Arabic-English corpus that was provided to the NIST'08 6 evaluation campaign and belongs to the news domain.", "labels": [], "entities": [{"text": "NIST'08 6 evaluation campaign", "start_pos": 115, "end_pos": 144, "type": "DATASET", "confidence": 0.8973397761583328}]}, {"text": "The corpus statistics can be found in.", "labels": [], "entities": []}, {"text": "The development and test sets were provided with 4 reference translations, belong to the same domain and contain 663 and 500 sentences, respectively.", "labels": [], "entities": []}, {"text": "Evaluation conditions were case-insensitive and sensitive to tokenization.", "labels": [], "entities": [{"text": "tokenization", "start_pos": 61, "end_pos": 73, "type": "TASK", "confidence": 0.9664369821548462}]}, {"text": "The word alignment is automatically computed by using GIZA++ () in both directions, which are made symmetric by using the grow-diag-final-and operation.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 4, "end_pos": 18, "type": "TASK", "confidence": 0.7081215232610703}]}, {"text": "The experiments were done on a dual-processor Pentium IV Intel Xeon Quad Core X5355 2.66 GHz machine with 24 G of RAM.", "labels": [], "entities": [{"text": "RAM", "start_pos": 114, "end_pos": 117, "type": "METRIC", "confidence": 0.9529343843460083}]}, {"text": "All computational times and memory size results are approximated.", "labels": [], "entities": []}, {"text": "The SAMT guideline was used to perform the experiments and is available on-line: http://www.cs.cmu.edu/\u223czollmann/samt/.", "labels": [], "entities": [{"text": "SAMT guideline", "start_pos": 4, "end_pos": 18, "type": "DATASET", "confidence": 0.780023843050003}]}, {"text": "Moses MT script was used to create the grow \u2212 diag \u2212 final word alignment and extract purely lexical phrases, which are then used to induce the SAMT grammar.", "labels": [], "entities": [{"text": "Moses MT script", "start_pos": 0, "end_pos": 15, "type": "DATASET", "confidence": 0.8310269117355347}]}, {"text": "The target side (English) of the training corpus was parsed with the Charniak's parser.", "labels": [], "entities": []}, {"text": "Rule extraction and filtering procedures were restricted to the concatenation of the development and test sets, allowing for rules with a maximal length of 12 elements in the source side and with a zero minimum occurrence criterion for both nonlexical and purely lexical rules.", "labels": [], "entities": [{"text": "Rule extraction", "start_pos": 0, "end_pos": 15, "type": "TASK", "confidence": 0.8198253512382507}]}, {"text": "Moses-style phrases extracted with a phrasebased system were 4.8M , while a number of generalized rules representing the hierarchical model grew dramatically to 22.9M . 10.8M of them were pruned out on the filtering step.", "labels": [], "entities": []}, {"text": "The vocabulary of the English Penn Treebank elementary non-terminals is 72, while a number of generalized elements, including additive and truncated categories, is 35.7K.", "labels": [], "entities": [{"text": "Penn Treebank elementary non-terminals", "start_pos": 30, "end_pos": 68, "type": "DATASET", "confidence": 0.9162261188030243}]}, {"text": "The F astT ranslateChart beam-search decoder was used as an engine of MER training aiming to tune the feature weight coefficients and produce final n-best and 1-best translations by combining the intensive search with a standard 4-gram LM as shown in.", "labels": [], "entities": [{"text": "F astT ranslateChart beam-search decoder", "start_pos": 4, "end_pos": 44, "type": "DATASET", "confidence": 0.8101593375205993}, {"text": "MER", "start_pos": 70, "end_pos": 73, "type": "TASK", "confidence": 0.9563128352165222}]}, {"text": "The iteration limit was set to 10 with 1000-best list and the highest BLEU score as optimization criteria.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 70, "end_pos": 80, "type": "METRIC", "confidence": 0.9780877828598022}]}, {"text": "We did not use completely abstract rules (without any source-side lexical utterance), since these rules significantly slowdown the decoding process (noAllowAbstractRules option).", "labels": [], "entities": []}, {"text": "shows a summary of computational time and RAM needed at each step of the translation.", "labels": [], "entities": [{"text": "RAM", "start_pos": 42, "end_pos": 45, "type": "METRIC", "confidence": 0.9991149306297302}, {"text": "translation", "start_pos": 73, "end_pos": 84, "type": "TASK", "confidence": 0.9648467302322388}]}, {"text": "Step  Evaluation scores including results of system combination (see subsection 4.6) are reported in.", "labels": [], "entities": []}, {"text": "The core model of the N -gram-based system is a 4-gram LM of bilingual units containing: 184.345 1-grams).", "labels": [], "entities": []}, {"text": "The number of non-unique initially extracted tuples is 1.1M , which were pruned according to the maximum number of translation options per tuple on the source side.", "labels": [], "entities": []}, {"text": "Tuples with a NULL on the source side were attached to either the previous or the next unit ( ).", "labels": [], "entities": [{"text": "NULL", "start_pos": 14, "end_pos": 18, "type": "METRIC", "confidence": 0.9435948133468628}]}, {"text": "The feature models weights were optimized according to the same optimization criteria as in the SAMT experiments (the highest BLEU score).", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 126, "end_pos": 136, "type": "METRIC", "confidence": 0.9733888506889343}]}, {"text": "Stage-by-stage RAM and time requirements are presented in, while translation quality evaluation results can be found in.", "labels": [], "entities": [{"text": "RAM", "start_pos": 15, "end_pos": 18, "type": "METRIC", "confidence": 0.9989746809005737}]}, {"text": "Step: Tuple-based SMT: Computational resources.", "labels": [], "entities": [{"text": "Tuple-based SMT", "start_pos": 6, "end_pos": 21, "type": "TASK", "confidence": 0.5387594401836395}]}], "tableCaptions": [{"text": " Table 1: Basic statistics of the training corpus.", "labels": [], "entities": []}, {"text": " Table 5: Human made error statistics for a representative test set.", "labels": [], "entities": []}]}