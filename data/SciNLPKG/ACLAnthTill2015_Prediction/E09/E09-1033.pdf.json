{"title": [{"text": "Rich bitext projection features for parse reranking", "labels": [], "entities": [{"text": "parse reranking", "start_pos": 36, "end_pos": 51, "type": "TASK", "confidence": 0.9544506072998047}]}], "abstractContent": [{"text": "Many different types of features have been shown to improve accuracy in parse reranking.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 60, "end_pos": 68, "type": "METRIC", "confidence": 0.9991206526756287}, {"text": "parse reranking", "start_pos": 72, "end_pos": 87, "type": "TASK", "confidence": 0.9460828006267548}]}, {"text": "A class of features that thus far has not been considered is based on a projection of the syntactic structure of a translation of the text to be parsed.", "labels": [], "entities": []}, {"text": "The intuition for using this type of bitext projection feature is that ambiguous structures in one language often correspond to un-ambiguous structures in another.", "labels": [], "entities": []}, {"text": "We show that reranking based on bitext projection features increases parsing accuracy significantly .", "labels": [], "entities": [{"text": "parsing", "start_pos": 69, "end_pos": 76, "type": "TASK", "confidence": 0.9692134261131287}, {"text": "accuracy", "start_pos": 77, "end_pos": 85, "type": "METRIC", "confidence": 0.9673648476600647}]}], "introductionContent": [{"text": "Parallel text or bitext is an important knowledge source for solving many problems such as machine translation, cross-language information retrieval, and the projection of linguistic resources from one language to another.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 91, "end_pos": 110, "type": "TASK", "confidence": 0.7858452796936035}, {"text": "cross-language information retrieval", "start_pos": 112, "end_pos": 148, "type": "TASK", "confidence": 0.7345649401346842}]}, {"text": "In this paper, we show that bitext-based features are effective in addressing another NLP problem, increasing the accuracy of statistical parsing.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 114, "end_pos": 122, "type": "METRIC", "confidence": 0.9991061091423035}, {"text": "statistical parsing", "start_pos": 126, "end_pos": 145, "type": "TASK", "confidence": 0.7148109078407288}]}, {"text": "We pursue this approach fora number of reasons.", "labels": [], "entities": []}, {"text": "First, one limiting factor for syntactic approaches to statistical machine translation is parse quality).", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 55, "end_pos": 86, "type": "TASK", "confidence": 0.7375154991944631}]}, {"text": "Improved parses of bitext should result in improved machine translation.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 52, "end_pos": 71, "type": "TASK", "confidence": 0.6995498090982437}]}, {"text": "Second, as more and more texts are available in several languages, it will be increasingly the case that a text to be parsed is itself part of a bitext.", "labels": [], "entities": []}, {"text": "Third, we hope that the improved parses of bitext will serve as higher quality training data for improving monolingual parsing using a process similar to self-training ().", "labels": [], "entities": [{"text": "monolingual parsing", "start_pos": 107, "end_pos": 126, "type": "TASK", "confidence": 0.5162982195615768}]}, {"text": "It is well known that different languages encode different types of grammatical information (agreement, case, tense etc.) and that what can be left unspecified in one language must be made explicit in another.", "labels": [], "entities": []}, {"text": "This information can be used for syntactic disambiguation.", "labels": [], "entities": [{"text": "syntactic disambiguation", "start_pos": 33, "end_pos": 57, "type": "TASK", "confidence": 0.9020424485206604}]}, {"text": "However, it is surprisingly hard to do this well.", "labels": [], "entities": []}, {"text": "We use parses and alignments that are automatically generated and hence imperfect.", "labels": [], "entities": []}, {"text": "German parse quality is considered to be worse than English parse quality, and the annotation style is different, e.g., NP structure in German is flatter.", "labels": [], "entities": []}, {"text": "We conduct our research in the framework of N-best parse reranking, but apply it to bitext and add only features based on syntactic projection from German to English.", "labels": [], "entities": [{"text": "N-best parse reranking", "start_pos": 44, "end_pos": 66, "type": "TASK", "confidence": 0.7563207348187765}]}, {"text": "We test the idea that, generally, English parses with more isomorphism with respect to the projected German parse are better.", "labels": [], "entities": []}, {"text": "The system takes as input (i) English sentences with a list of automatically generated syntactic parses, (ii) a translation of the English sentences into German, (iii) an automatically generated parse of the German translation, and (iv) an automatically generated word alignment.", "labels": [], "entities": []}, {"text": "We achieve a significant improvement of 0.66 F 1 (absolute) on test data.", "labels": [], "entities": [{"text": "F 1", "start_pos": 45, "end_pos": 48, "type": "METRIC", "confidence": 0.9943366050720215}]}, {"text": "The paper is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 outlines our approach and section 3 introduces the model.", "labels": [], "entities": []}, {"text": "Section 4 describes training and section 5 presents the data and experimental results.", "labels": [], "entities": []}, {"text": "In section 6, we discuss previous work.", "labels": [], "entities": []}, {"text": "Section 7 analyzes our results and section 8 concludes.", "labels": [], "entities": []}], "datasetContent": [{"text": "We used the subset of the Wall Street Journal investigated in for our experiments, which consists of all sentences that have at least one prepositional phrase attachment ambiguity.", "labels": [], "entities": [{"text": "Wall Street Journal", "start_pos": 26, "end_pos": 45, "type": "DATASET", "confidence": 0.9565461874008179}]}, {"text": "This difficult subset of sentences seems particularly interesting when investigating the potential of information in bitext for improving parsing performance.", "labels": [], "entities": [{"text": "parsing", "start_pos": 138, "end_pos": 145, "type": "TASK", "confidence": 0.9619820713996887}]}, {"text": "The first 500 sentences of this set were translated from English to German by a graduate student and an additional 3218 sen-1: Algorithm TRAIN(\u03bb) 2: repeat 3: add \u03bb to the set s 4: let t be a set of 1000 randomly generated vectors 5: let \u03bb = argmax \u03c1\u2208(s\u222at) F1(\u03c1) 6: let \u03bb \u2032 = \u03bb 7: repeat 8: repeatedly run one-dimensional error minimization step (updating a single scalar of the vector \u03bb) until no further error reduction 9: adjust each scalar of \u03bb in turn towards 0 such that there is no increase in error (if possible) 10: until no scalar in \u03bb changes in last two steps (8 and 9) 11: until \u03bb = \u03bb \u2032 12: return \u03bb: Sketch of the training algorithm tences by a translation bureau.", "labels": [], "entities": [{"text": "TRAIN", "start_pos": 137, "end_pos": 142, "type": "METRIC", "confidence": 0.7503032088279724}, {"text": "F1", "start_pos": 257, "end_pos": 259, "type": "METRIC", "confidence": 0.9561243653297424}]}, {"text": "We withheld these 3718 English sentences (and an additional 1000 reserved sentences) when we trained BitPar on the Penn treebank.", "labels": [], "entities": [{"text": "Penn treebank", "start_pos": 115, "end_pos": 128, "type": "DATASET", "confidence": 0.9788693189620972}]}, {"text": "We use the BitPar parser which is based on a bit-vector implementation (cf. () of the Cocke-Younger-Kasami algorithm.", "labels": [], "entities": []}, {"text": "It computes a compact parse forest for all possible analyses.", "labels": [], "entities": []}, {"text": "As all possible analyses are computed, any number of best parses can be extracted.", "labels": [], "entities": []}, {"text": "In contrast, other treebank parsers use sophisticated search strategies to find the most probable analysis without examining the set of all possible analyses (.", "labels": [], "entities": []}, {"text": "BitPar is particularly useful for N-best parsing as the N-best parses can be computed efficiently.", "labels": [], "entities": []}, {"text": "For the 3718 sentences in the translated set, we created 100-best English parses and 1-best German parses.", "labels": [], "entities": []}, {"text": "The German parser was trained on the TIGER treebank.", "labels": [], "entities": [{"text": "TIGER treebank", "start_pos": 37, "end_pos": 51, "type": "DATASET", "confidence": 0.7443403899669647}]}, {"text": "For the Europarl corpus, we created 1-best parses for both languages.", "labels": [], "entities": [{"text": "Europarl corpus", "start_pos": 8, "end_pos": 23, "type": "DATASET", "confidence": 0.9908760190010071}]}, {"text": "We use a word alignment of the translated sentences from the Penn treebank, as well as a word alignment of the Europarl corpus.", "labels": [], "entities": [{"text": "Penn treebank", "start_pos": 61, "end_pos": 74, "type": "DATASET", "confidence": 0.9928925931453705}, {"text": "Europarl corpus", "start_pos": 111, "end_pos": 126, "type": "DATASET", "confidence": 0.9943400025367737}]}, {"text": "We align these two data sets together with data from the JRC Acquis () to try to obtain better quality alignments (it is well known that alignment quality improves as the amount of data increases).", "labels": [], "entities": [{"text": "JRC Acquis", "start_pos": 57, "end_pos": 67, "type": "DATASET", "confidence": 0.9441473484039307}]}, {"text": "We aligned approximately 3.08 million sentence pairs.", "labels": [], "entities": []}, {"text": "We tried to obtain better alignment quality as alignment quality is a problem in many cases where syntactic projection would otherwise work well To generate the alignments, we used Model 4 (, as implemented in GIZA++.", "labels": [], "entities": []}, {"text": "As is standard practice, we trained Model 4 with English as the source language, and then trained Model 4 with German as the source language, resulting in two Viterbi alignments.", "labels": [], "entities": []}, {"text": "These were combined using the Grow Diag Final And symmetrization heuristic (.", "labels": [], "entities": []}, {"text": "We perform 7-way crossvalidation on 3718 sentences.", "labels": [], "entities": []}, {"text": "In each fold of the cross-validation, the training set is 3186 sentences, while the test set is 532 sentences.", "labels": [], "entities": []}, {"text": "Our results are shown in table 1.", "labels": [], "entities": []}, {"text": "In row 1, we take the hypothesis ranked best by BitPar.", "labels": [], "entities": []}, {"text": "In row 2, we train using the algorithm outlined in section 4.", "labels": [], "entities": []}, {"text": "To cancel out any effect caused by a particularly effective or ineffective starting \u03bb value, we perform 5 trials each time.", "labels": [], "entities": []}, {"text": "Columns 3 and 5 report the improvement over the baseline on train and test respectively.", "labels": [], "entities": []}, {"text": "We reach an improvement of 0.56 over the baseline using the algorithm as described in section 4.", "labels": [], "entities": []}, {"text": "Our initial experiments used many highly correlated features.", "labels": [], "entities": []}, {"text": "For our next experiment we use greedy feature selection.", "labels": [], "entities": []}, {"text": "We start with a \u03bb vector that is zero for all features, and then run the error minimization without the random generation of vectors ().", "labels": [], "entities": []}, {"text": "This means that we add one feature at a time.", "labels": [], "entities": []}, {"text": "This greedy algorithm winds up producing a vector with many zero weights.", "labels": [], "entities": []}, {"text": "In row 3 of table 1, we used the greedy feature selection algorithm and trained using F 1 , resulting in a performance of 0.66 over the baseline which is our best result.", "labels": [], "entities": [{"text": "F 1", "start_pos": 86, "end_pos": 89, "type": "METRIC", "confidence": 0.9808974862098694}]}, {"text": "We performed a planned one-tailed paired t-test on the F 1 scores of the parses selected by the baseline and this system for the 3718 sentences (parses were taken from the test portion of each fold).", "labels": [], "entities": [{"text": "F 1 scores", "start_pos": 55, "end_pos": 65, "type": "METRIC", "confidence": 0.9620955983797709}]}, {"text": "We found that there is a significant difference with the baseline (t(3717) = 6.42, p < .01).", "labels": [], "entities": []}, {"text": "We believe that using the full set of 34 features (many of which are very similar to one another) made the training problem harder without improving the fit to the training data, and that greedy feature selection helps with this (see also section 7).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Average F 1 of 7-way cross-validation", "labels": [], "entities": [{"text": "Average F 1", "start_pos": 10, "end_pos": 21, "type": "METRIC", "confidence": 0.8400434255599976}]}]}