{"title": [{"text": "Predicting the fluency of text with shallow structural features: case studies of machine translation and human-written text", "labels": [], "entities": [{"text": "machine translation", "start_pos": 81, "end_pos": 100, "type": "TASK", "confidence": 0.7273001670837402}]}], "abstractContent": [{"text": "Sentence fluency is an important component of overall text readability but few studies in natural language processing have sought to understand the factors that define it.", "labels": [], "entities": [{"text": "Sentence fluency", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.8977294862270355}]}, {"text": "We report the results of an initial study into the predictive power of surface syntactic statistics for the task; we use fluency assessments done for the purpose of evaluating machine translation.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 176, "end_pos": 195, "type": "TASK", "confidence": 0.793863445520401}]}, {"text": "We find that these features are weakly but significantly correlated with fluency.", "labels": [], "entities": []}, {"text": "Machine and human translations can be distinguished with accuracy over 80%.", "labels": [], "entities": [{"text": "Machine and human translations", "start_pos": 0, "end_pos": 30, "type": "TASK", "confidence": 0.5829766318202019}, {"text": "accuracy", "start_pos": 57, "end_pos": 65, "type": "METRIC", "confidence": 0.9993163347244263}]}, {"text": "The performance of pairwise comparison of fluency is also very high-over 90% fora multi-layer perceptron classifier.", "labels": [], "entities": []}, {"text": "We also test the hypothesis that the learned models capture general fluency properties applicable to human-written text.", "labels": [], "entities": []}, {"text": "The results do not support this hypothesis: prediction accuracy on the new data is only 57%.", "labels": [], "entities": [{"text": "prediction", "start_pos": 44, "end_pos": 54, "type": "METRIC", "confidence": 0.8462249636650085}, {"text": "accuracy", "start_pos": 55, "end_pos": 63, "type": "METRIC", "confidence": 0.910916268825531}]}, {"text": "This finding suggests that developing a dedicated , task-independent corpus of fluency judgments will be beneficial for further investigations of the problem.", "labels": [], "entities": []}], "introductionContent": [{"text": "Numerous natural language applications involve the task of producing fluent text.", "labels": [], "entities": []}, {"text": "This is a core problem for surface realization in natural language generation), as well as an important step in machine translation.", "labels": [], "entities": [{"text": "surface realization", "start_pos": 27, "end_pos": 46, "type": "TASK", "confidence": 0.7370457351207733}, {"text": "natural language generation", "start_pos": 50, "end_pos": 77, "type": "TASK", "confidence": 0.6547701954841614}, {"text": "machine translation", "start_pos": 112, "end_pos": 131, "type": "TASK", "confidence": 0.8211491107940674}]}, {"text": "Considerations of sentence fluency are also key in sentence simplification, sentence compression), text re-generation for summarization) and headline generation (.", "labels": [], "entities": [{"text": "sentence simplification", "start_pos": 51, "end_pos": 74, "type": "TASK", "confidence": 0.7115191519260406}, {"text": "sentence compression", "start_pos": 76, "end_pos": 96, "type": "TASK", "confidence": 0.7723118364810944}, {"text": "summarization", "start_pos": 122, "end_pos": 135, "type": "TASK", "confidence": 0.9773493409156799}, {"text": "headline generation", "start_pos": 141, "end_pos": 160, "type": "TASK", "confidence": 0.8698825836181641}]}, {"text": "Despite its importance for these popular applications, the factors contributing to sentence level fluency have not been researched indepth.", "labels": [], "entities": []}, {"text": "Much more attention has been devoted to discourse-level constraints on adjacent sentences indicative of coherence and good text flow.", "labels": [], "entities": []}, {"text": "In many applications fluency is assessed in combination with other qualities.", "labels": [], "entities": []}, {"text": "For example, in machine translation evaluation, approaches such as BLEU () use n-gram overlap comparisons with a model to judge overall \"goodness\", with higher n-grams meant to capture fluency considerations.", "labels": [], "entities": [{"text": "machine translation evaluation", "start_pos": 16, "end_pos": 46, "type": "TASK", "confidence": 0.8612486322720846}, {"text": "BLEU", "start_pos": 67, "end_pos": 71, "type": "METRIC", "confidence": 0.9939163327217102}]}, {"text": "More sophisticated ways to compare a system production and a model involve the use of syntax, but even in these cases fluency is only indirectly assessed and the main advantage of the use of syntax is better estimation of the semantic overlap between a model and an output.", "labels": [], "entities": []}, {"text": "Similarly, the metrics proposed for text generation by ( ) (simple accuracy, generation accuracy) are based on string-edit distance from an ideal output.", "labels": [], "entities": [{"text": "text generation", "start_pos": 36, "end_pos": 51, "type": "TASK", "confidence": 0.7686825394630432}, {"text": "accuracy", "start_pos": 67, "end_pos": 75, "type": "METRIC", "confidence": 0.5966197848320007}, {"text": "accuracy", "start_pos": 88, "end_pos": 96, "type": "METRIC", "confidence": 0.5948206186294556}]}, {"text": "In contrast, the work of () and () directly sets as a goal the assessment of sentence-level fluency, regardless of content.", "labels": [], "entities": []}, {"text": "In () the main premise is that syntactic information from a parser can more robustly capture fluency than language models, giving more direct indications of the degree of ungrammaticality.", "labels": [], "entities": []}, {"text": "The idea is extended in (, where four parsers are used and artificially generated sentences with varying level of fluency are evaluated with impressive success.", "labels": [], "entities": []}, {"text": "The fluency models hold promise for actual improvements in machine translation output quality (.", "labels": [], "entities": [{"text": "machine translation output", "start_pos": 59, "end_pos": 85, "type": "TASK", "confidence": 0.8092025518417358}]}, {"text": "In that work, only simple parser features are used for the prediction of fluency, but no actual syntactic properties of the sentences.", "labels": [], "entities": [{"text": "prediction of fluency", "start_pos": 59, "end_pos": 80, "type": "TASK", "confidence": 0.833965023358663}]}, {"text": "But certainly, problems with sentence fluency are expected to be manifested in syntax.", "labels": [], "entities": []}, {"text": "We would expect for example that syntactic tree features that capture common parse configurations and that are used in discriminative parsing should be useful for predicting sentence fluency as well.", "labels": [], "entities": [{"text": "predicting sentence fluency", "start_pos": 163, "end_pos": 190, "type": "TASK", "confidence": 0.7684444189071655}]}, {"text": "Indeed, early work has demonstrated that syntactic features, and branching properties in particular, are helpful features for automatically distinguishing human translations from machine translations).", "labels": [], "entities": [{"text": "distinguishing human translations from machine translations", "start_pos": 140, "end_pos": 199, "type": "TASK", "confidence": 0.7344721257686615}]}, {"text": "The exploration of branching properties of human and machine translations was motivated by the observations during failure analysis that MT system output tends to favor right-branching structures over noun compounding.", "labels": [], "entities": [{"text": "MT system", "start_pos": 137, "end_pos": 146, "type": "TASK", "confidence": 0.9162391424179077}, {"text": "noun compounding", "start_pos": 201, "end_pos": 217, "type": "TASK", "confidence": 0.7339369207620621}]}, {"text": "Branching preference mismatch manifest themselves in the English output when translating from languages whose branching properties are radically different from English.", "labels": [], "entities": []}, {"text": "Accuracy close to 80% was achieved for distinguishing human translations from machine translations.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 0, "end_pos": 8, "type": "METRIC", "confidence": 0.966962993144989}, {"text": "distinguishing human translations from machine translations", "start_pos": 39, "end_pos": 98, "type": "TASK", "confidence": 0.753710021575292}]}, {"text": "In our work we continue the investigation of sentence level fluency based on features that capture surface statistics of the syntactic structure in a sentence.", "labels": [], "entities": []}, {"text": "We revisit the task of distinguishing machine translations from human translations, but also further our understanding of fluency by providing comprehensive analysis of the association between fluency assessments of translations and surface syntactic features.", "labels": [], "entities": [{"text": "distinguishing machine translations from human translations", "start_pos": 23, "end_pos": 82, "type": "TASK", "confidence": 0.7370186050732931}]}, {"text": "We also demonstrate that based on the same class of features, it is possible to distinguish fluent machine translations from disfluent machine translations.", "labels": [], "entities": []}, {"text": "Finally, we test the models on human written text in order to verify if the classifiers trained on data coming from machine translation evaluations can be used for general predictions of fluency and readability.", "labels": [], "entities": []}, {"text": "For our experiments we use the evaluations of Chinese to English translations distributed by LDC (catalog number LDC2003T17), for which both machine and human translations are available.", "labels": [], "entities": [{"text": "LDC (catalog number LDC2003T17)", "start_pos": 93, "end_pos": 124, "type": "DATASET", "confidence": 0.8124547203381857}]}, {"text": "Machine translations have been assessed by evaluators for fluency on a five point scale (5: flawless English; 4: good English; 3: non-native English; 2: disfluent English; 1: incomprehensible).", "labels": [], "entities": []}, {"text": "Assessments by different annotators were averaged to assign overall fluency assessment for each machine-translated sentence.", "labels": [], "entities": []}, {"text": "For each segment (sentence), there are four human and three machine translations.", "labels": [], "entities": []}, {"text": "In this setting we address four tasks with increasing difficulty: \u2022 Distinguish human and machine translations.", "labels": [], "entities": [{"text": "Distinguish human and machine translations", "start_pos": 68, "end_pos": 110, "type": "TASK", "confidence": 0.8651107549667358}]}, {"text": "\u2022 Distinguish fluent machine translations from poor machine translations.", "labels": [], "entities": [{"text": "Distinguish fluent machine translations", "start_pos": 2, "end_pos": 41, "type": "TASK", "confidence": 0.7295680195093155}]}, {"text": "\u2022 Distinguish the better (in terms of fluency) translation among two translations of the same input segment.", "labels": [], "entities": []}, {"text": "\u2022 Use the models trained on data from MT evaluations to predict potential fluency problems of human-written texts (from the Wall Street Journal).", "labels": [], "entities": [{"text": "MT evaluations", "start_pos": 38, "end_pos": 52, "type": "TASK", "confidence": 0.9136734902858734}, {"text": "Wall Street Journal)", "start_pos": 124, "end_pos": 144, "type": "DATASET", "confidence": 0.9638016223907471}]}, {"text": "Even for the last most challenging task results are promising, with prediction accuracy almost 10% better than a random baseline.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 79, "end_pos": 87, "type": "METRIC", "confidence": 0.9097180366516113}]}, {"text": "For the other tasks accuracies are high, exceeding 80%.", "labels": [], "entities": [{"text": "accuracies", "start_pos": 20, "end_pos": 30, "type": "METRIC", "confidence": 0.9974520802497864}]}, {"text": "It is important to note that the purpose of our study is not evaluation of machine translation per se.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 75, "end_pos": 94, "type": "TASK", "confidence": 0.7523482441902161}]}, {"text": "Our goal is more general and the interest is in finding predictors of sentence fluency.", "labels": [], "entities": []}, {"text": "No general corpora exist with fluency assessments, so it seems advantageous to use the assessments done in the context of machine translation for preliminary investigations of fluency.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 122, "end_pos": 141, "type": "TASK", "confidence": 0.7196434140205383}]}, {"text": "Nevertheless, our findings are also potentially beneficial for sentence-level evaluation of machine translation.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 92, "end_pos": 111, "type": "TASK", "confidence": 0.7960807085037231}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 2: Pearson's correlation coefficient between fluency and syntactic phrasing features. P-values are  given in parenthesis.", "labels": [], "entities": [{"text": "Pearson's correlation coefficient", "start_pos": 10, "end_pos": 43, "type": "METRIC", "confidence": 0.6767194718122482}]}, {"text": " Table 3: Accuracy for the task of distinguishing machine and human translations.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9966084957122803}, {"text": "distinguishing machine and human translations", "start_pos": 35, "end_pos": 80, "type": "TASK", "confidence": 0.7837747931480408}]}, {"text": " Table 4: Accuracy for pairwise fluency comparison. \"Same sentence\" are comparisons constrained  between different translations of the same sentences, \"any pair\" contains comparisons of sentences with  different fluency over the entire data set.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9838910698890686}]}, {"text": " Table 5: The five features with highest weights in the support vector machine model for the different  tasks.", "labels": [], "entities": []}, {"text": " Table 6: Accuracy, precision and recall (for fluent  class) for each model when test on WSJ sentences.  The gold-standard is assessment by a single reader  of the text.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9989563226699829}, {"text": "precision", "start_pos": 20, "end_pos": 29, "type": "METRIC", "confidence": 0.9993284940719604}, {"text": "recall", "start_pos": 34, "end_pos": 40, "type": "METRIC", "confidence": 0.9995143413543701}]}]}