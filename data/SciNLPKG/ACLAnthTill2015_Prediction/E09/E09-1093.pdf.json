{"title": [], "abstractContent": [{"text": "A corpus-based technique is described to improve the efficiency of wide-coverage high-accuracy parsers.", "labels": [], "entities": []}, {"text": "By keeping track of the derivation steps which lead to the best parse fora very large collection of sentences, the parser learns which parse steps can be filtered without significant loss in parsing accuracy, but with an important increase in parsing efficiency.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 199, "end_pos": 207, "type": "METRIC", "confidence": 0.9591935276985168}]}, {"text": "An interesting characteristic of our approach is that it is self-learning, in the sense that it uses unannotated corpora.", "labels": [], "entities": []}], "introductionContent": [{"text": "We consider wide-coverage high-accuracy parsing systems such as Alpino, a parser for Dutch which contains a grammar based on HPSG and a maximum entropy disambiguation component trained on a treebank.", "labels": [], "entities": [{"text": "HPSG", "start_pos": 125, "end_pos": 129, "type": "DATASET", "confidence": 0.9132897257804871}]}, {"text": "Even if such parsing systems now obtain satisfactory accuracy fora variety of text types, a drawback concerns the computational properties of such parsers: they typically require lots of memory and are often very slow for longer and very ambiguous sentences.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 53, "end_pos": 61, "type": "METRIC", "confidence": 0.9974313378334045}]}, {"text": "We present a very simple, fairly general, corpus-based method to improve upon the practical efficiency of such parsers.", "labels": [], "entities": []}, {"text": "We use the accurate, slow, parser to parse many (unannotated) input sentences.", "labels": [], "entities": []}, {"text": "For each sentence, we keep track of sequences of derivation steps that were required to find the best parse of that sentence (i.e., the parse that obtained the best score, highest probability, according to the parser itself).", "labels": [], "entities": []}, {"text": "Given a large set of successful derivation step sequences, we experimented with a variety of simple heuristics to filter unpromising derivation steps.", "labels": [], "entities": []}, {"text": "A heuristic that works remarkably well simply states that fora new input sentence, the parser can only consider derivation step sequences in which any sub-sequence of length N has been observed at least once in the training data.", "labels": [], "entities": []}, {"text": "Experimental results are provided for various heuristics and amounts of training data.", "labels": [], "entities": []}, {"text": "It is hard to compare fast, accurate, parsers with slow, slightly more accurate parsers.", "labels": [], "entities": []}, {"text": "In section 3 we propose both an on-line and an off-line application scenario, introducing a time-out per sentence, which leads to metrics for choosing between parser variants.", "labels": [], "entities": []}, {"text": "In the experimental part we show that, in an online scenario, the most successful heuristic leads to a parser that is more accurate than the baseline system, except for unrealistic time-outs per sentence of more than 15 minutes.", "labels": [], "entities": []}, {"text": "Furthermore, we show that, in an off-line scenario, the most successful heuristic leads to a parser that is more than four times faster than the base-line variant with the same accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 177, "end_pos": 185, "type": "METRIC", "confidence": 0.9966132044792175}]}], "datasetContent": [{"text": "Some of the experiments have been performed with the Alpino Treebank.", "labels": [], "entities": [{"text": "Alpino Treebank", "start_pos": 53, "end_pos": 68, "type": "DATASET", "confidence": 0.964121550321579}]}, {"text": "The Alpino Treebank (van der) consists of manually verified dependency structures for the cdbl (newspaper) part of the Eindhoven corpus.", "labels": [], "entities": [{"text": "Alpino Treebank (van der)", "start_pos": 4, "end_pos": 29, "type": "DATASET", "confidence": 0.9225958784421285}, {"text": "Eindhoven corpus", "start_pos": 119, "end_pos": 135, "type": "DATASET", "confidence": 0.8197014331817627}]}, {"text": "The treebank contains 7137 sentences.", "labels": [], "entities": []}, {"text": "Average sentence length is about 20 tokens.", "labels": [], "entities": []}, {"text": "Some further experiments are performed on the basis of the D-Coi corpus).", "labels": [], "entities": [{"text": "D-Coi corpus", "start_pos": 59, "end_pos": 71, "type": "DATASET", "confidence": 0.7182105630636215}]}, {"text": "From this corpus, we used the manually verified syntactic annotations of the P-P-H and P-P-L parts.", "labels": [], "entities": []}, {"text": "The P-P-H part consists of over 2200 sentences from the Dutch daily newspaper Trouw from 2001.", "labels": [], "entities": [{"text": "Dutch daily newspaper Trouw from 2001", "start_pos": 56, "end_pos": 93, "type": "DATASET", "confidence": 0.7391400138537089}]}, {"text": "Average sentence length is about 16.5 tokens.", "labels": [], "entities": []}, {"text": "The P-P-L part contains 1115 sentences taken from information brochures of Dutch Ministries.", "labels": [], "entities": [{"text": "Dutch Ministries", "start_pos": 75, "end_pos": 91, "type": "DATASET", "confidence": 0.8213852345943451}]}, {"text": "Average sentence length is about 18.5 tokens.", "labels": [], "entities": []}, {"text": "For training data, we used newspaper text from the TwNC (Twente Newspaper) corpus.", "labels": [], "entities": [{"text": "TwNC (Twente Newspaper) corpus", "start_pos": 51, "end_pos": 81, "type": "DATASET", "confidence": 0.8730960289637247}]}, {"text": "In addition, we used Volkskrant 1997 newspaper data extracted from the Volkskrant 1997 CDROM.", "labels": [], "entities": [{"text": "Volkskrant 1997 newspaper data extracted from the Volkskrant 1997 CDROM", "start_pos": 21, "end_pos": 92, "type": "DATASET", "confidence": 0.8828096061944961}]}, {"text": "line parser improves upon the prefix filter only for unrealistic time-outs larger than fifteen minutes of CPU-time.", "labels": [], "entities": [{"text": "line parser", "start_pos": 0, "end_pos": 11, "type": "TASK", "confidence": 0.6311922371387482}]}, {"text": "The difference inaccuracy fora given time-out value can be considerable: as much as 12% for time-outs around 30 seconds of CPUtime.", "labels": [], "entities": []}, {"text": "In this section, we confirm the experimental results obtained on the Alpino Treebank by performing similar experiments on the D-Coi data.", "labels": [], "entities": [{"text": "Alpino Treebank", "start_pos": 69, "end_pos": 84, "type": "DATASET", "confidence": 0.9793497025966644}]}, {"text": "The purpose of this confirmation is twofold.", "labels": [], "entities": []}, {"text": "On the one hand, the Alpino Treebank might not be a reliable test set for the Alpino parser, because it has been used quite intensively during the development of various components of the system.", "labels": [], "entities": [{"text": "Alpino Treebank", "start_pos": 21, "end_pos": 36, "type": "DATASET", "confidence": 0.9608446359634399}]}, {"text": "On the other hand, we might regard the experiments in the previous section as development experiments from which we learn the best parameters of the approach.", "labels": [], "entities": []}, {"text": "The real evaluation of the technique is now performed using only the best method found on the development set, which is the prefix filter with \u03c4 = 0.", "labels": [], "entities": []}, {"text": "We performed experiments with two parts of the D-Coi corpus.", "labels": [], "entities": [{"text": "D-Coi corpus", "start_pos": 47, "end_pos": 59, "type": "DATASET", "confidence": 0.7344323694705963}]}, {"text": "The first data set, P-P-H, contains newspaper data, and is therefore comparable both with the Alpino Treebank, and more importantly, with the training data that we used to develop the filters.", "labels": [], "entities": [{"text": "newspaper data", "start_pos": 36, "end_pos": 50, "type": "DATASET", "confidence": 0.8638388812541962}, {"text": "Alpino Treebank", "start_pos": 94, "end_pos": 109, "type": "DATASET", "confidence": 0.9601495265960693}]}, {"text": "In order to check if the success of the filtering methods requires that training data and test data need to betaken from similar texts, we also provide experimental results on a test set consisting of different material: the P-P-L part of the D-Coi corpus, which contains text extracted from information brochures published by Dutch Ministries.", "labels": [], "entities": [{"text": "Dutch Ministries", "start_pos": 327, "end_pos": 343, "type": "DATASET", "confidence": 0.8437857925891876}]}, {"text": "The third and fourth graphs in provide results obtained on the P-P-H corpus.", "labels": [], "entities": [{"text": "P-P-H corpus", "start_pos": 63, "end_pos": 75, "type": "DATASET", "confidence": 0.7167954742908478}]}, {"text": "The increased efficiency of the prefix filter is slightly less pronounced.", "labels": [], "entities": []}, {"text": "This maybe due to the smaller mean sentence length of this data set.", "labels": [], "entities": []}, {"text": "Still, the prefix filtering method performs much better fora large variety of time-outs.", "labels": [], "entities": [{"text": "prefix filtering", "start_pos": 11, "end_pos": 27, "type": "TASK", "confidence": 0.8251140415668488}]}, {"text": "Only for very high, unrealistic, time-outs, the baseline parser obtains better accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 79, "end_pos": 87, "type": "METRIC", "confidence": 0.9982660412788391}]}, {"text": "The same general trend is observed in the P-P-L data-set.", "labels": [], "entities": [{"text": "P-P-L data-set", "start_pos": 42, "end_pos": 56, "type": "DATASET", "confidence": 0.6913926005363464}]}, {"text": "From these results we tentatively conclude that the proposed technique is applicable across text types and domains.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Hypothetical result of parser on a test set  of four sentences. The columns labeled precision,  recall, f-score and accuracy represent aggregates  over sentences 1 . . . i.", "labels": [], "entities": [{"text": "precision", "start_pos": 94, "end_pos": 103, "type": "METRIC", "confidence": 0.999363362789154}, {"text": "recall", "start_pos": 106, "end_pos": 112, "type": "METRIC", "confidence": 0.9954700469970703}, {"text": "f-score", "start_pos": 114, "end_pos": 121, "type": "METRIC", "confidence": 0.926112949848175}, {"text": "accuracy", "start_pos": 126, "end_pos": 134, "type": "METRIC", "confidence": 0.9973061084747314}]}]}