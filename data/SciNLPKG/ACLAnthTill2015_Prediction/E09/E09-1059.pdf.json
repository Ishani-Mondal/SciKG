{"title": [{"text": "Sentiment Summarization: Evaluating and Learning User Preferences", "labels": [], "entities": [{"text": "Sentiment Summarization", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.9436783790588379}]}], "abstractContent": [{"text": "We present the results of a large-scale, end-to-end human evaluation of various sentiment summarization models.", "labels": [], "entities": [{"text": "sentiment summarization", "start_pos": 80, "end_pos": 103, "type": "TASK", "confidence": 0.833835631608963}]}, {"text": "The evaluation shows that users have a strong preference for summarizers that model sentiment over non-sentiment baselines, but have no broad overall preference between any of the sentiment-based models.", "labels": [], "entities": []}, {"text": "However, an analysis of the human judgments suggests that there are identifiable situations where one summarizer is generally preferred over the others.", "labels": [], "entities": []}, {"text": "We exploit this fact to build anew summarizer by training a ranking SVM model over the set of human preference judgments that were collected during the evaluation, which results in a 30% relative reduction in error over the previous best summarizer.", "labels": [], "entities": [{"text": "summarizer", "start_pos": 35, "end_pos": 45, "type": "TASK", "confidence": 0.9760134220123291}, {"text": "error", "start_pos": 209, "end_pos": 214, "type": "METRIC", "confidence": 0.5684756636619568}]}], "introductionContent": [{"text": "The growth of the Internet as a commerce medium, and particularly the Web 2.0 phenomenon of user-generated content, have resulted in the proliferation of massive numbers of product, service and merchant reviews.", "labels": [], "entities": []}, {"text": "While this means that users have plenty of information on which to base their purchasing decisions, in practice this is often too much information fora user to absorb.", "labels": [], "entities": []}, {"text": "To alleviate this information overload, research on systems that automatically aggregate and summarize opinions have been gaining interest (Hu and Liu, 2004a; Hu and Liu, 2004b;;.", "labels": [], "entities": []}, {"text": "Evaluating these systems has been a challenge, however, due to the number of human judgments required to draw meaningful conclusions.", "labels": [], "entities": []}, {"text": "Often systems are evaluated piecemeal, selecting pieces that can be evaluated easily and automatically.", "labels": [], "entities": []}, {"text": "While this technique produces meaningful evaluations of the selected components, other components remain untested, and the overall effectiveness of the entire system as a whole remains unknown.", "labels": [], "entities": []}, {"text": "When systems are evaluated end-to-end by human judges, the studies are often small, consisting of only a handful of judges and data points).", "labels": [], "entities": []}, {"text": "Furthermore, automated summarization metrics like ROUGE ( are non-trivial to adapt to this domain as they require human curated outputs.", "labels": [], "entities": [{"text": "summarization", "start_pos": 23, "end_pos": 36, "type": "TASK", "confidence": 0.8172889351844788}, {"text": "ROUGE", "start_pos": 50, "end_pos": 55, "type": "METRIC", "confidence": 0.8254225254058838}]}, {"text": "We present the results of a large-scale, end-toend human evaluation of three sentiment summarization models applied to user reviews of consumer products.", "labels": [], "entities": []}, {"text": "The evaluation shows that there is no significant difference in rater preference between any of the sentiment summarizers, but that raters do prefer sentiment summarizers over nonsentiment baselines.", "labels": [], "entities": []}, {"text": "This indicates that even simple sentiment summarizers provide users utility.", "labels": [], "entities": [{"text": "sentiment summarizers", "start_pos": 32, "end_pos": 53, "type": "TASK", "confidence": 0.7843851149082184}]}, {"text": "An analysis of the rater judgments also indicates that there are identifiable situations where one sentiment summarizer is generally preferred over the others.", "labels": [], "entities": [{"text": "sentiment summarizer", "start_pos": 99, "end_pos": 119, "type": "TASK", "confidence": 0.6886986941099167}]}, {"text": "We attempt to learn these preferences by training a ranking SVM that exploits the set of preference judgments collected during the evaluation.", "labels": [], "entities": []}, {"text": "Experiments show that the ranking SVM summarizer's cross-validation error decreases by as much as 30% over the previous best model.", "labels": [], "entities": []}, {"text": "Human evaluations of text summarization have been undertaken in the past.", "labels": [], "entities": [{"text": "text summarization", "start_pos": 21, "end_pos": 39, "type": "TASK", "confidence": 0.7886757552623749}]}, {"text": "presented a task-driven evaluation in the news domain in order to understand the utility of different systems.", "labels": [], "entities": []}, {"text": "Also in the news domain, the Document Understanding Conference 1 has run a number of multi-document and query-driven summarization shared-tasks that have used a wide iPod Shuffle: 4/5 stars \"In final analysis the iPod Shuffle is a decent player that offers a sleek compact form factor an excessively simple user interface and a low price\" ...", "labels": [], "entities": []}, {"text": "\"It's not good for carrying a lot of music but fora little bit of music you can quickly grab and go with this nice little toy\" ...", "labels": [], "entities": []}, {"text": "\"Mine came in a nice bright orange color that makes it easy to locate.\" range of automatic and human-based evaluation criteria.", "labels": [], "entities": []}, {"text": "This year, the new Text Analysis Conference 2 is running a shared-task that contains an opinion component.", "labels": [], "entities": [{"text": "Text Analysis Conference", "start_pos": 19, "end_pos": 43, "type": "TASK", "confidence": 0.8246805667877197}]}, {"text": "The goal of that evaluation is to summarize answers to opinion questions about entities mentioned in blogs.", "labels": [], "entities": [{"text": "summarize answers to opinion questions about entities mentioned in blogs", "start_pos": 34, "end_pos": 106, "type": "TASK", "confidence": 0.8378832340240479}]}, {"text": "Our work most closely resembles the evaluations in. had raters evaluate extractive and abstractive summarization systems.", "labels": [], "entities": []}, {"text": "Mirroring our results, they show that both extractive and abstractive summarization outperform a baseline, but that overall, humans have no preference between the two.", "labels": [], "entities": []}, {"text": "Again mirroring our results, their analysis indicates that even though there is no overall difference, there are situations where one system generally outperforms the other.", "labels": [], "entities": []}, {"text": "In particular, show that an entity's controversiality, e.g., mid-range star rating, is correlated with which summary has highest value.", "labels": [], "entities": []}, {"text": "The study presented here differs from Carenini et al. in many respects: First, our evaluation is over different extractive summarization systems in an attempt to understand what model properties are correlated with human preference irrespective of presentation; Secondly, our evaluation is on a larger scale including hundreds of judgments by hundreds of raters; Finally, we take a major next step and show that it is possible to automatically learn significantly improved models by leveraging data collected in a large-scale evaluation.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluated summary performance for reviews of consumer electronics.", "labels": [], "entities": []}, {"text": "In this setting an entity to be summarized is one particular product, Dis a set of user reviews about that product, and R is the normalized aggregate star ratings left by users.", "labels": [], "entities": [{"text": "R", "start_pos": 120, "end_pos": 121, "type": "METRIC", "confidence": 0.9908323884010315}]}, {"text": "We gathered reviews for 165 electronics products from several online review aggregators.", "labels": [], "entities": []}, {"text": "The products covered a variety of electronics, such as MP3 players, digital cameras, printers, wireless routers, and video game systems.", "labels": [], "entities": []}, {"text": "Each product had a minimum of four reviews and up to a maximum of nearly 3000.", "labels": [], "entities": []}, {"text": "The mean number of reviews per product was 148, and the median was 70.", "labels": [], "entities": [{"text": "mean number of reviews", "start_pos": 4, "end_pos": 26, "type": "METRIC", "confidence": 0.9433521032333374}]}, {"text": "We ran each of our algorithms over the review corpus and generated summaries for each product with K = 650.", "labels": [], "entities": []}, {"text": "All summaries were roughly equal length to avoid length-based rater bias 3 . In total we ran four experiments fora combined number of 1980 rater judgments (plus additional judgments during the development phase of this study).", "labels": [], "entities": []}, {"text": "Our initial set of experiments were over the three opinion-based summarization systems: SM, SMAC, and SAM.", "labels": [], "entities": []}, {"text": "We ran three experiments comparing SMAC to SM, SAM to SM, and SAM to SMAC.", "labels": [], "entities": [{"text": "SMAC to SM", "start_pos": 35, "end_pos": 45, "type": "TASK", "confidence": 0.7635665734608968}, {"text": "SMAC", "start_pos": 69, "end_pos": 73, "type": "DATASET", "confidence": 0.7396803498268127}]}, {"text": "In each experiment two summaries of the same product were placed side-by-side in a random order.", "labels": [], "entities": []}, {"text": "Raters were also shown an overall rating, R, for each product (these ratings are often provided in a form such as \"3.5 of 5 stars\").", "labels": [], "entities": [{"text": "R", "start_pos": 42, "end_pos": 43, "type": "METRIC", "confidence": 0.974452018737793}]}, {"text": "The two summaries on either side were shown below this information with links to the full text of the reviews for the raters to explore.", "labels": [], "entities": []}, {"text": "Raters were asked to express their preference for one summary over the other.", "labels": [], "entities": []}, {"text": "For two summaries SA and S B they could answer, Raters were free to choose any rating, but were specifically instructed that their rating should account fora summaries representativeness of the overall set of reviews.", "labels": [], "entities": [{"text": "SA", "start_pos": 18, "end_pos": 20, "type": "METRIC", "confidence": 0.7919436097145081}]}, {"text": "Raters were also asked to provide a brief comment justifying their rating.", "labels": [], "entities": []}, {"text": "Over 100 raters participated in each study, and each comparison was evaluated by three raters with no rater making more than five judgments.: Results of side-by-side experiments.", "labels": [], "entities": []}, {"text": "Agreement is the percentage of items for which all raters agreed on a positive/negative/no-preference rating.", "labels": [], "entities": [{"text": "Agreement", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.9756286144256592}]}, {"text": "No Preference is the percentage of agreement items in which the raters had no preference.", "labels": [], "entities": [{"text": "Preference", "start_pos": 3, "end_pos": 13, "type": "METRIC", "confidence": 0.8236136436462402}]}, {"text": "Preferred A/B is the percentage of agreement items in which the raters preferred either A or B respectively.", "labels": [], "entities": [{"text": "Preferred A/B", "start_pos": 0, "end_pos": 13, "type": "METRIC", "confidence": 0.9469279795885086}]}, {"text": "Mean Numeric is the average of the numeric ratings (converted from discreet preference decisions) indicating on average the raters preferred system A over B on a scale of -1 to 1.", "labels": [], "entities": [{"text": "Mean", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.9761762022972107}]}, {"text": "Positive scores indicate a preference for system A.", "labels": [], "entities": []}, {"text": "\u2020 significant at a 95% confidence interval for the mean numeric score.", "labels": [], "entities": [{"text": "mean numeric score", "start_pos": 51, "end_pos": 69, "type": "METRIC", "confidence": 0.9031573534011841}]}, {"text": "We chose to have raters leave pairwise preferences, rather than evaluate each candidate summary in isolation, because raters can make a preference decisions more quickly than a valuation judgment, which allowed for collection of more data points.", "labels": [], "entities": []}, {"text": "Furthermore, there is evidence that rater agreement is much higher in preference decisions than in value judgments.", "labels": [], "entities": [{"text": "rater agreement", "start_pos": 36, "end_pos": 51, "type": "METRIC", "confidence": 0.8271472752094269}]}, {"text": "Results are shown in the first three rows of table 1.", "labels": [], "entities": []}, {"text": "The first column of the table indicates the experiment that was run.", "labels": [], "entities": []}, {"text": "The second column indicates the percentage of judgments for which the raters were in agreement.", "labels": [], "entities": []}, {"text": "Agreement here is a weak agreement, where three raters are defined to be in agreement if they all gave a no preference rating, or if there was a preference rating, but no two preferences conflicted.", "labels": [], "entities": []}, {"text": "The next three columns indicate the percentage of judgments for each preference category, grouped here into three coarse assignments.", "labels": [], "entities": []}, {"text": "The final column indicates a numeric average for the experiment.", "labels": [], "entities": []}, {"text": "This was calculated by converting users ratings to a scale of 1 (strongly preferred SA ) to -1 (strongly preferred S B ) at 0.33 intervals.", "labels": [], "entities": []}, {"text": "shows only results for items in which the raters had agreement in order to draw reliable conclusions, though the results change little when all items are taken into account.", "labels": [], "entities": []}, {"text": "Ultimately, the results indicate that none of the sentiment summarizers are strongly preferred over any other.", "labels": [], "entities": [{"text": "sentiment summarizers", "start_pos": 50, "end_pos": 71, "type": "TASK", "confidence": 0.7107697576284409}]}, {"text": "Only the SAM v SMAC model has a difference that can be considered statistically significant.", "labels": [], "entities": [{"text": "SAM v SMAC", "start_pos": 9, "end_pos": 19, "type": "TASK", "confidence": 0.5901468396186829}]}, {"text": "In terms of order we might conclude that SAM is the most preferred, followed by SM, followed by SMAC.", "labels": [], "entities": [{"text": "SAM", "start_pos": 41, "end_pos": 44, "type": "TASK", "confidence": 0.6841034889221191}, {"text": "SM", "start_pos": 80, "end_pos": 82, "type": "METRIC", "confidence": 0.9467985033988953}, {"text": "SMAC", "start_pos": 96, "end_pos": 100, "type": "METRIC", "confidence": 0.6089180707931519}]}, {"text": "However, the slight differences make any such conclusions tenuous at best.", "labels": [], "entities": []}, {"text": "This leads one to wonder whether raters even require any complex modeling when summarizing opinions.", "labels": [], "entities": [{"text": "summarizing opinions", "start_pos": 79, "end_pos": 99, "type": "TASK", "confidence": 0.8896318078041077}]}, {"text": "To test this we took the lowest scoring model overall, SMAC, and compared it to a leading text baseline (LT) that simply selects the first sentence from a ranked list of reviews until the length constraint is violated.", "labels": [], "entities": [{"text": "SMAC", "start_pos": 55, "end_pos": 59, "type": "TASK", "confidence": 0.5054875612258911}]}, {"text": "The results are given in the last row of 1.", "labels": [], "entities": []}, {"text": "Here there is a clear distinction as raters preferred SMAC to LT, indicating that they did find usefulness in systems that modeled aspects and sentiment.", "labels": [], "entities": []}, {"text": "However, there are still 25.5% of agreement items where the raters did choose a simple leading text baseline.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Results of side-by-side experiments. Agreement is the percentage of items for which all raters  agreed on a positive/negative/no-preference rating. No Preference is the percentage of agreement items  in which the raters had no preference. Preferred A/B is the percentage of agreement items in which the  raters preferred either A or B respectively. Mean Numeric is the average of the numeric ratings (converted  from discreet preference decisions) indicating on average the raters preferred system A over B on a scale  of -1 to 1. Positive scores indicate a preference for system A.  \u2020 significant at a 95% confidence interval  for the mean numeric score.", "labels": [], "entities": [{"text": "Mean", "start_pos": 359, "end_pos": 363, "type": "METRIC", "confidence": 0.9615631699562073}]}, {"text": " Table 2: Accuracies for learned summarizers.", "labels": [], "entities": [{"text": "Accuracies", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9832007884979248}]}]}