{"title": [{"text": "Human Evaluation of a German Surface Realisation Ranker", "labels": [], "entities": [{"text": "Human Evaluation of a German Surface Realisation Ranker", "start_pos": 0, "end_pos": 55, "type": "TASK", "confidence": 0.774199552834034}]}], "abstractContent": [{"text": "In this paper we present a human-based evaluation of surface realisation alternatives.", "labels": [], "entities": [{"text": "surface realisation", "start_pos": 53, "end_pos": 72, "type": "TASK", "confidence": 0.7434630990028381}]}, {"text": "We examine the relative rankings of naturally occurring corpus sentences and automatically generated strings chosen by statistical models (language model, log-linear model), as well as the naturalness of the strings chosen by the log-linear model.", "labels": [], "entities": []}, {"text": "We also investigate to what extent preceding context has an effect on choice.", "labels": [], "entities": []}, {"text": "We show that native speakers do accept quite some variation in word order, but there are also clearly factors that make certain real-isation alternatives more natural.", "labels": [], "entities": []}], "introductionContent": [{"text": "An important component of research on surface realisation (the task of generating strings fora given abstract representation) is evaluation, especially if we want to be able to compare across systems.", "labels": [], "entities": [{"text": "surface realisation", "start_pos": 38, "end_pos": 57, "type": "TASK", "confidence": 0.7665952146053314}]}, {"text": "There is consensus that exact match with respect to an actually observed corpus sentence is too strict a metric and that BLEU score measured against corpus sentences can only give a rough impression of the quality of the system output.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 121, "end_pos": 131, "type": "METRIC", "confidence": 0.9735036194324493}]}, {"text": "It is unclear, however, what kind of metric would be most suitable for the evaluation of string realisations, so that, as a result, there have been a range of automatic metrics applied including inter alia exact match, string edit distance, NIST SSA, BLEU, NIST, ROUGE, generation string accuracy, generation tree accuracy, word accuracy ().", "labels": [], "entities": [{"text": "evaluation of string realisations", "start_pos": 75, "end_pos": 108, "type": "TASK", "confidence": 0.6511911898851395}, {"text": "NIST SSA", "start_pos": 241, "end_pos": 249, "type": "TASK", "confidence": 0.5071290731430054}, {"text": "BLEU", "start_pos": 251, "end_pos": 255, "type": "METRIC", "confidence": 0.9980582594871521}, {"text": "NIST", "start_pos": 257, "end_pos": 261, "type": "DATASET", "confidence": 0.8348145484924316}, {"text": "ROUGE", "start_pos": 263, "end_pos": 268, "type": "METRIC", "confidence": 0.9491730332374573}, {"text": "accuracy", "start_pos": 288, "end_pos": 296, "type": "METRIC", "confidence": 0.8918511867523193}, {"text": "accuracy", "start_pos": 314, "end_pos": 322, "type": "METRIC", "confidence": 0.707958459854126}, {"text": "accuracy", "start_pos": 329, "end_pos": 337, "type": "METRIC", "confidence": 0.559177577495575}]}, {"text": "It is not always clear how appropriate these metrics are, especially at the level of individual sentences.", "labels": [], "entities": []}, {"text": "Using automatic evaluation metrics cannot be avoided, but ideally, a metric for the evaluation of realisation rankers would rank alternative realisations in the same way as native speakers of the language for which the surface realisation system is developed, and not only globally, but also at the level of individual sentences.", "labels": [], "entities": []}, {"text": "Another major consideration in evaluation is what to take as the gold standard.", "labels": [], "entities": []}, {"text": "The easiest option is to take the original corpus string that was used to produce the abstract representation from which we generate.", "labels": [], "entities": []}, {"text": "However, there may well be other realisations of the same input that are as suitable in the given context.", "labels": [], "entities": []}, {"text": "argue that while we should take advantage of large corpora in NLG, we also need to take care that we do not introduce errors by learning from incorrect data present in corpora.", "labels": [], "entities": []}, {"text": "In order to better understand what makes good evaluation data (and metrics), we designed and implemented an experiment in which human judges evaluated German string realisations.", "labels": [], "entities": []}, {"text": "The main aims of this experiment were: (i) to establish how much variation in German word order is acceptable for human judges, (ii) to find an automatic evaluation metric that mirrors the findings of the human evaluation, (iii) to provide detailed feedback for the designers of the surface realisation ranking model and (iv) to establish what effect preceding context has on the choice of realisation.", "labels": [], "entities": []}, {"text": "In this paper, we concentrate on points (i) and (iv).", "labels": [], "entities": []}, {"text": "The remainder of the paper is structured as follows: In Section 2 we outline the realisation ranking system that provided the data for the experiment.", "labels": [], "entities": []}, {"text": "In Section 3 we outline the design of the experiment and in Section 4 we present our findings.", "labels": [], "entities": []}, {"text": "In Section 5 we relate this to other work and finally we conclude in Section 6.", "labels": [], "entities": []}], "datasetContent": [{"text": "The experiment was divided into three parts.", "labels": [], "entities": []}, {"text": "Each part took between 30 and 45 minutes to complete, and participants were asked to leave sometime (e.g. a week) between each part.", "labels": [], "entities": []}, {"text": "In total, 24 participants completed the experiment.", "labels": [], "entities": []}, {"text": "All were native German speakers (mostly from South-Western Germany) and almost all had a linguistic background.", "labels": [], "entities": []}, {"text": "gives a breakdown of the items in each part of the experiment.", "labels": [], "entities": []}, {"text": "5 \"Williams war in der britischen Politik \u00e4u\u00dferst umstritten.\": Statistics for each experiment part", "labels": [], "entities": [{"text": "Williams", "start_pos": 3, "end_pos": 11, "type": "DATASET", "confidence": 0.74679034948349}]}], "tableCaptions": [{"text": " Table 1: Results achieved by trigram LM ranker  and log-linear model ranker in Cahill et al. (2007)", "labels": [], "entities": []}, {"text": " Table 3: Task 1a: Ranks for each system", "labels": [], "entities": []}, {"text": " Table 4: The 4 alternatives given by the grammar  for (2) and their frequencies", "labels": [], "entities": []}, {"text": " Table 6: Task 3a: Ranks for each system (com- pared to ranks in Task 1a)", "labels": [], "entities": []}, {"text": " Table 7: How often did a participant make the  same choice?", "labels": [], "entities": []}, {"text": " Table 8: Inter-Annotator Agreement for each ex- periment", "labels": [], "entities": []}]}