{"title": [{"text": "Text Summarization Model based on Maximum Coverage Problem and its Variant", "labels": [], "entities": [{"text": "Text Summarization", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.6886430978775024}]}], "abstractContent": [{"text": "We discuss text summarization in terms of maximum coverage problem and its variant.", "labels": [], "entities": [{"text": "text summarization", "start_pos": 11, "end_pos": 29, "type": "TASK", "confidence": 0.6938587427139282}]}, {"text": "We explore some decoding algorithms including the ones never used in this sum-marization formulation, such as a greedy algorithm with performance guarantee, a randomized algorithm, and a branch-and-bound method.", "labels": [], "entities": []}, {"text": "On the basis of the results of comparative experiments, we also augment the summarization model so that it takes into account the relevance to the document cluster.", "labels": [], "entities": [{"text": "summarization", "start_pos": 76, "end_pos": 89, "type": "TASK", "confidence": 0.9523939490318298}]}, {"text": "Through experiments, we showed that the augmented model is superior to the best-performing method of DUC'04 on ROUGE-1 without stopwords.", "labels": [], "entities": []}], "introductionContent": [{"text": "Automatic text summarization is one of the tasks that have long been studied in natural language processing.", "labels": [], "entities": [{"text": "Automatic text summarization", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.7153539061546326}, {"text": "natural language processing", "start_pos": 80, "end_pos": 107, "type": "TASK", "confidence": 0.6477404336134592}]}, {"text": "This task is to create a summary, or a short and concise document that describes the content of a given set of documents.", "labels": [], "entities": []}, {"text": "One well-known approach to text summarization is the extractive method, which selects some linguistic units (e.g., sentences) from given documents in order to generate a summary.", "labels": [], "entities": [{"text": "text summarization", "start_pos": 27, "end_pos": 45, "type": "TASK", "confidence": 0.674673318862915}]}, {"text": "The extractive method has an advantage that the grammaticality is guaranteed at least at the level of the linguistic units.", "labels": [], "entities": []}, {"text": "Since the actual generation of linguistic expressions has not achieved the level of the practical use, we focus on the extractive method in this paper, especially the method based on the sentence extraction.", "labels": [], "entities": [{"text": "sentence extraction", "start_pos": 187, "end_pos": 206, "type": "TASK", "confidence": 0.7199500054121017}]}, {"text": "Most of the extractive summarization methods rely on sequentially solving binary classification problems of determining whether each sentence should be selected or not.", "labels": [], "entities": [{"text": "extractive summarization", "start_pos": 12, "end_pos": 36, "type": "TASK", "confidence": 0.6767723858356476}]}, {"text": "In such sequential methods, however, the viewpoint regarding whether the summary is good as a whole, is not taken into consideration, although a summary conveys information as a whole.", "labels": [], "entities": []}, {"text": "We represent text summarization as an optimization problem and attempt to globally solve the problem.", "labels": [], "entities": [{"text": "text summarization", "start_pos": 13, "end_pos": 31, "type": "TASK", "confidence": 0.6703480333089828}]}, {"text": "In particular, we represent text summarization as a maximum coverage problem with knapsack constraint (MCKP).", "labels": [], "entities": [{"text": "text summarization", "start_pos": 28, "end_pos": 46, "type": "TASK", "confidence": 0.7484906017780304}]}, {"text": "One of the advantages of this representation is that MCKP can directly model whether each concept in the given documents is covered by the summary or not, and can dispense with rather counter-intuitive approaches such as giving penalty to each pair of two similar sentences.", "labels": [], "entities": []}, {"text": "By formally apprehending the target problem, we can use a lot of knowledge and techniques developed in the combinatorial mathematics, and also analyse results more precisely.", "labels": [], "entities": []}, {"text": "In fact, on the basis of the results of the experiments, we augmented the summarization model.", "labels": [], "entities": []}, {"text": "The contributions of this paper are as follows.", "labels": [], "entities": []}, {"text": "We are not the first to represent text summarization as MCKP.", "labels": [], "entities": [{"text": "text summarization", "start_pos": 34, "end_pos": 52, "type": "TASK", "confidence": 0.6770624816417694}]}, {"text": "However, no researchers have exploited the decoding algorithms for solving MCKP in the summarization task.", "labels": [], "entities": [{"text": "summarization task", "start_pos": 87, "end_pos": 105, "type": "TASK", "confidence": 0.9322280585765839}]}, {"text": "We conduct comprehensive comparative experiments of those algorithms.", "labels": [], "entities": []}, {"text": "Specifically, we test the greedy algorithm, the greedy algorithm with performance guarantee, the stack decoding, the linear relaxation problem with randomized decoding, and the branch-andbound method.", "labels": [], "entities": []}, {"text": "On the basis of the experimental results, we then propose an augmented model that takes into account the relevance to the document cluster.", "labels": [], "entities": []}, {"text": "We empirically show that the augmented model is superior to the best-performing method of DUC'04 on ROUGE-1 without stopwords.", "labels": [], "entities": []}], "datasetContent": [{"text": "We conducted experiments on the dataset of DUC'04 (2004) with settings of task 2, which is a multi-document summarization task.", "labels": [], "entities": [{"text": "DUC'04 (2004)", "start_pos": 43, "end_pos": 56, "type": "DATASET", "confidence": 0.8909670561552048}]}, {"text": "50 document clusters, each of which consists of 10 documents, are given.", "labels": [], "entities": []}, {"text": "One summary is to be generated for each cluster.", "labels": [], "entities": []}, {"text": "Following the most relevant previous method, we set the target length to 100 words.", "labels": [], "entities": []}, {"text": "dataset was used as the training dataset for trained weights.", "labels": [], "entities": []}, {"text": "All the documents were segmented into sentences using a script distributed by DUC.", "labels": [], "entities": [{"text": "DUC", "start_pos": 78, "end_pos": 81, "type": "DATASET", "confidence": 0.9758870601654053}]}, {"text": "Words are stemmed by Porter's stemmer.", "labels": [], "entities": []}, {"text": "ROUGE version 1.5.5) was used for evaluation.", "labels": [], "entities": []}, {"text": "Among others, we focus on ROUGE-1 in the discussion of the result, because ROUGE-1 has proved to have strong correlation with human annotation.", "labels": [], "entities": [{"text": "ROUGE-1", "start_pos": 26, "end_pos": 33, "type": "METRIC", "confidence": 0.9335684180259705}]}, {"text": "Wilcoxon signed rank test for paired samples with significance level 0.05 was used for the significance test of the difference in ROUGE-1.", "labels": [], "entities": [{"text": "Wilcoxon signed rank test", "start_pos": 0, "end_pos": 25, "type": "METRIC", "confidence": 0.6944311931729317}, {"text": "significance level 0.05", "start_pos": 50, "end_pos": 73, "type": "METRIC", "confidence": 0.9311077197392782}, {"text": "significance", "start_pos": 91, "end_pos": 103, "type": "METRIC", "confidence": 0.9763618111610413}, {"text": "ROUGE-1", "start_pos": 130, "end_pos": 137, "type": "METRIC", "confidence": 0.9781660437583923}]}, {"text": "The simplex method and the branch-and-bound method implemented in GLPK) were used to solve respectively linear and integer programming problems.", "labels": [], "entities": [{"text": "GLPK", "start_pos": 66, "end_pos": 70, "type": "DATASET", "confidence": 0.9284425973892212}]}, {"text": "The methods that are compared here are the greedy algorithm (greedy), the greedy algorithm with performance guarantee (g-greedy), the randomized algorithm (rand), the stack decoding (stack), and the branch-and-bound method (exact).", "labels": [], "entities": []}, {"text": "We ran greedy, g-greedy, rand100k, stack30 and exact to solve MCKP-Rel.", "labels": [], "entities": [{"text": "exact", "start_pos": 47, "end_pos": 52, "type": "METRIC", "confidence": 0.9959092140197754}, {"text": "MCKP-Rel", "start_pos": 62, "end_pos": 70, "type": "DATASET", "confidence": 0.862846314907074}]}, {"text": "We experimented on DUC'04 with the same experimental setting as the previous ones.", "labels": [], "entities": [{"text": "DUC'04", "start_pos": 19, "end_pos": 25, "type": "DATASET", "confidence": 0.963643491268158}]}, {"text": "We determined the value of \u03bb for each method using DUC'03 as development data.", "labels": [], "entities": [{"text": "DUC'03", "start_pos": 51, "end_pos": 57, "type": "DATASET", "confidence": 0.9412502646446228}]}, {"text": "Specifically, we conducted experiments on DUC'03 with different \u03bb (\u2208 {0.0, 0.1, \u00b7 \u00b7 \u00b7 , 1.0}) and simply selected the one with the highest ROUGE-1 value.", "labels": [], "entities": [{"text": "DUC'03", "start_pos": 42, "end_pos": 48, "type": "DATASET", "confidence": 0.9601829648017883}, {"text": "ROUGE-1", "start_pos": 139, "end_pos": 146, "type": "METRIC", "confidence": 0.9920234680175781}]}, {"text": "The results with these predicted \u03bb are shown in.", "labels": [], "entities": []}, {"text": "Only ROUGE-1 values are shown.", "labels": [], "entities": [{"text": "ROUGE-1", "start_pos": 5, "end_pos": 12, "type": "METRIC", "confidence": 0.995322048664093}]}, {"text": "Method exact opt is exact with the optimal \u03bb, and can be regarded as the upperbound of MCKP-Rel.", "labels": [], "entities": [{"text": "MCKP-Rel", "start_pos": 87, "end_pos": 95, "type": "DATASET", "confidence": 0.9319323301315308}]}, {"text": "To evaluate the appropriateness of models without regard to search quality, we first focused on exact and found that MCKP-Rel outperformed MCKP with exact.", "labels": [], "entities": [{"text": "exact", "start_pos": 96, "end_pos": 101, "type": "METRIC", "confidence": 0.9757300615310669}]}, {"text": "This means that MCKP-Rel model is superior to MCKP model.", "labels": [], "entities": []}, {"text": "Among the algorithms, stack30 and exact performed well.", "labels": [], "entities": [{"text": "exact", "start_pos": 34, "end_pos": 39, "type": "METRIC", "confidence": 0.9918978214263916}]}, {"text": "All methods except for greedy yielded significantly better ROUGE values compared with the corresponding results in Figures 1 and 2 show ROUGE-1 for different values of \u03bb.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 59, "end_pos": 64, "type": "METRIC", "confidence": 0.9969890713691711}, {"text": "ROUGE-1", "start_pos": 136, "end_pos": 143, "type": "METRIC", "confidence": 0.9686209559440613}]}, {"text": "The leftmost part (\u03bb = 0.0) corresponds to MCKP.", "labels": [], "entities": [{"text": "MCKP", "start_pos": 43, "end_pos": 47, "type": "DATASET", "confidence": 0.9686346054077148}]}, {"text": "We can see from the figures, that MCKP-Rel at the best \u03bb always outperforms MCKP, and that MCKP-Rel tends to degrade for very large \u03bb.", "labels": [], "entities": []}, {"text": "This means that excessive weight on relevance has an adversative effect on performance and therefore the coverage is important.", "labels": [], "entities": [{"text": "coverage", "start_pos": 105, "end_pos": 113, "type": "METRIC", "confidence": 0.8310253024101257}]}, {"text": "In the experiments above, we found that \u03bb = 0.2 is the optimal value for exact with interpolated weights.", "labels": [], "entities": []}, {"text": "We suppose that this \u03bb gives the best model, and examined search errors as we did in Section 5.2.", "labels": [], "entities": []}, {"text": "We obtained, which shows that search errors in MCKP-Rel counterintuitively increase (\u21d1) ROUGE-1 scoreless often than MCKP did in.", "labels": [], "entities": [{"text": "ROUGE-1 scoreless", "start_pos": 88, "end_pos": 105, "type": "METRIC", "confidence": 0.9726602435112}]}, {"text": "This was the case also for trained weights.", "labels": [], "entities": []}, {"text": "This result suggests that MCKP-Rel is more suitable to text summarization than MCKP is.", "labels": [], "entities": [{"text": "text summarization", "start_pos": 55, "end_pos": 73, "type": "TASK", "confidence": 0.7565578520298004}]}, {"text": "However, exact with trained weights at the optimal \u03bb(= 0.4) in was outperformed by stack30.", "labels": [], "entities": [{"text": "exact", "start_pos": 9, "end_pos": 14, "type": "METRIC", "confidence": 0.8728858232498169}, {"text": "stack30", "start_pos": 83, "end_pos": 90, "type": "DATASET", "confidence": 0.8784934282302856}]}, {"text": "It suggests that there is still room for future improvement in the model.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: ROUGE of MCKP with interpolated  weights. Underlined ROUGE-1 scores are signif- icantly different from the score of exact. Compu- tational time was measured in seconds.  ROUGE  time  1  2  SU4  (sec)  greedy  0.283 0.083 0.123 <0.01  g-greedy 0.294 0.080 0.121  0.01  rand100k 0.300 0.079 0.119  1.88  stack30  0.304 0.078 0.120  4.53  exact  0.305 0.081 0.121  4.04", "labels": [], "entities": [{"text": "Compu- tational time", "start_pos": 133, "end_pos": 153, "type": "METRIC", "confidence": 0.9395646154880524}]}, {"text": " Table 2: ROUGE of MCKP with trained weights.  Underlined ROUGE-1 scores are significantly dif- ferent from the score of exact. Computational time  was measured in seconds.  ROUGE  time  1  2  SU4  (sec)  greedy  0.283 0.080 0.121 < 0.01  g-greedy 0.310 0.077 0.118  0.01  rand100k 0.299 0.077 0.117  1.93  stack30  0.309 0.080 0.120  4.23  exact  0.307 0.078 0.119  4.56", "labels": [], "entities": [{"text": "Computational time", "start_pos": 128, "end_pos": 146, "type": "METRIC", "confidence": 0.9591383039951324}]}, {"text": " Table 3: ROUGE of stack with various stacksizes  size  10  20  30  50  100  inter 0.304 0.304 0.304 0.304 0.303  train 0.308 0.310 0.309 0.308 0.307", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 10, "end_pos": 15, "type": "METRIC", "confidence": 0.9855912923812866}]}, {"text": " Table 4: Search errors of MCKP with interpolated  weights", "labels": [], "entities": [{"text": "MCKP", "start_pos": 27, "end_pos": 31, "type": "DATASET", "confidence": 0.8782595992088318}]}, {"text": " Table 6: Search errors of MCKP-Rel with interpo- lated weights (\u03bb = 0.2).", "labels": [], "entities": [{"text": "MCKP-Rel", "start_pos": 27, "end_pos": 35, "type": "DATASET", "confidence": 0.9176189303398132}, {"text": "interpo- lated weights", "start_pos": 41, "end_pos": 63, "type": "METRIC", "confidence": 0.891446053981781}]}, {"text": " Table 7: ROUGE-1 of MCKP-Rel with byte con- straints, evaluated without stopwords. Underlined  are the values significantly different from peer65.", "labels": [], "entities": [{"text": "ROUGE-1", "start_pos": 10, "end_pos": 17, "type": "METRIC", "confidence": 0.9818437695503235}, {"text": "MCKP-Rel", "start_pos": 21, "end_pos": 29, "type": "DATASET", "confidence": 0.9532573819160461}]}, {"text": " Table 8: ROUGE-1 of MCKP-Rel with byte con- straints, evaluated with stopwords. Underlined are  the values significantly different from peer65.", "labels": [], "entities": [{"text": "ROUGE-1", "start_pos": 10, "end_pos": 17, "type": "METRIC", "confidence": 0.9822820425033569}, {"text": "MCKP-Rel", "start_pos": 21, "end_pos": 29, "type": "DATASET", "confidence": 0.9523221850395203}]}]}