{"title": [], "abstractContent": [{"text": "We present an extensive study on the problem of detecting polarity of words.", "labels": [], "entities": [{"text": "detecting polarity of words", "start_pos": 48, "end_pos": 75, "type": "TASK", "confidence": 0.8277345448732376}]}, {"text": "We consider the polarity of a word to be either positive or negative.", "labels": [], "entities": []}, {"text": "For example, words such as good, beautiful, and wonderful are considered as positive words; whereas words such as bad, ugly, and sad are considered negative words.", "labels": [], "entities": []}, {"text": "We treat polarity detection as a semi-supervised label propagation problem in a graph.", "labels": [], "entities": [{"text": "polarity detection", "start_pos": 9, "end_pos": 27, "type": "TASK", "confidence": 0.7316159307956696}, {"text": "label propagation", "start_pos": 49, "end_pos": 66, "type": "TASK", "confidence": 0.7759684026241302}]}, {"text": "In the graph, each node represents a word whose polarity is to be determined.", "labels": [], "entities": []}, {"text": "Each weighted edge encodes a relation that exists between two words.", "labels": [], "entities": []}, {"text": "Each node (word) can have two labels: positive or negative.", "labels": [], "entities": []}, {"text": "We study this framework in two different resource availability scenarios using WordNet and OpenOffice thesaurus when WordNet is not available.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 79, "end_pos": 86, "type": "DATASET", "confidence": 0.9742023348808289}, {"text": "WordNet", "start_pos": 117, "end_pos": 124, "type": "DATASET", "confidence": 0.943713903427124}]}, {"text": "We report our results on three different languages: En-glish, French, and Hindi.", "labels": [], "entities": []}, {"text": "Our results indicate that label propagation improves significantly over the baseline and other semi-supervised learning methods like Mincuts and Randomized Mincuts for this task.", "labels": [], "entities": [{"text": "label propagation", "start_pos": 26, "end_pos": 43, "type": "TASK", "confidence": 0.78084796667099}]}], "introductionContent": [{"text": "Opinionated texts are characterized by words or phrases that communicate positive or negative sentiment.", "labels": [], "entities": []}, {"text": "Consider the following example of two movie reviews 1 shown in.", "labels": [], "entities": []}, {"text": "The positive review is peppered with words such as enjoyable, likeable, decent, breathtakingly and the negative * Work done as a summer intern at Google Inc.", "labels": [], "entities": []}, {"text": "comment uses words like ear-shattering, humorless, unbearable.", "labels": [], "entities": []}, {"text": "These terms and prior knowledge of their polarity could be used as features in a supervised classification framework to determine the sentiment of the opinionated text (E.g.,).", "labels": [], "entities": []}, {"text": "Thus lexicons indicating polarity of such words are indispensable resources not only in automatic sentiment analysis but also in other natural language understanding tasks like textual entailment.", "labels": [], "entities": [{"text": "automatic sentiment analysis", "start_pos": 88, "end_pos": 116, "type": "TASK", "confidence": 0.6558529237906138}, {"text": "textual entailment", "start_pos": 177, "end_pos": 195, "type": "TASK", "confidence": 0.6738570630550385}]}, {"text": "This motivation was seen in the General Enquirer effort by and several others who manually construct such lexicons for the English language.", "labels": [], "entities": []}, {"text": "While it is possible to manually build these resources fora language, the ensuing effort is onerous.", "labels": [], "entities": []}, {"text": "This motivates the need for automatic language-agnostic methods for building sentiment lexicons.", "labels": [], "entities": []}, {"text": "The importance of this problem has warranted several efforts in the past, some of which will be reviewed here.", "labels": [], "entities": []}, {"text": "We demonstrate the application of graph-based semi-supervised learning for induction of polarity lexicons.", "labels": [], "entities": [{"text": "induction of polarity lexicons", "start_pos": 75, "end_pos": 105, "type": "TASK", "confidence": 0.8506667613983154}]}, {"text": "We try several graph-based semi-supervised learning methods like Mincuts, Randomized Mincuts, and Label Propagation.", "labels": [], "entities": []}, {"text": "In particular, we define a graph with nodes consisting of the words or phrases to be classified either as positive or negative.", "labels": [], "entities": []}, {"text": "The edges between the nodes encode some notion of similarity.", "labels": [], "entities": []}, {"text": "Ina transductive fashion, a few of these nodes are labeled using seed examples and the labels for the remaining nodes are derived using these seeds.", "labels": [], "entities": []}, {"text": "We explore natural word-graph sources like WordNet and exploit different relations within WordNet like synonymy and hypernymy.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 43, "end_pos": 50, "type": "DATASET", "confidence": 0.9752217531204224}, {"text": "WordNet", "start_pos": 90, "end_pos": 97, "type": "DATASET", "confidence": 0.951834499835968}]}, {"text": "Our method is not just confined to WordNet; any source listing synonyms could be used.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 35, "end_pos": 42, "type": "DATASET", "confidence": 0.9649377465248108}]}, {"text": "To demonstrate this, we show the use of OpenOffice thesaurus -a free resource available in several languages.", "labels": [], "entities": []}, {"text": "We begin by discussing some related work in Section 2 and briefly describe the learning methods we use, in Section 3.", "labels": [], "entities": []}, {"text": "Section 4 details our evaluation methodology along with detailed experiments for English.", "labels": [], "entities": []}, {"text": "In Section 5 we demonstrate results in French and Hindi, as an example of how the method could be easily applied to other languages as well.", "labels": [], "entities": []}], "datasetContent": [{"text": "We use the General Inquirer (GI) 8 data for evaluation.", "labels": [], "entities": [{"text": "General Inquirer (GI) 8 data", "start_pos": 11, "end_pos": 39, "type": "DATASET", "confidence": 0.697957728590284}]}, {"text": "General Inquirer is lexicon of English words hand-labeled with categorical information along several dimensions.", "labels": [], "entities": [{"text": "General Inquirer", "start_pos": 0, "end_pos": 16, "type": "DATASET", "confidence": 0.8280948400497437}]}, {"text": "One such dimension is called valence, with 1915 words labeled \"Positiv\" (sic) and 2291 words labeled \"Negativ\" for words with positive and negative sentiments respectively.", "labels": [], "entities": []}, {"text": "Since we want to evaluate the performance of the algorithms alone and not the recall issues in using WordNet, we only consider words from GI that also occur in WordNet.", "labels": [], "entities": [{"text": "recall", "start_pos": 78, "end_pos": 84, "type": "METRIC", "confidence": 0.9983250498771667}, {"text": "WordNet", "start_pos": 101, "end_pos": 108, "type": "DATASET", "confidence": 0.9526051878929138}, {"text": "WordNet", "start_pos": 160, "end_pos": 167, "type": "DATASET", "confidence": 0.9631410837173462}]}, {"text": "This leaves us the distribution of words as enumerated in All experiments reported in Sections 4.1 to 4.5 use the data described above with a 50-50 split so that the first half is used as seeds and the second half is used for test.", "labels": [], "entities": []}, {"text": "Note that all the experiments described below did not involve any parameter tuning thus obviating the need fora separate development test set.", "labels": [], "entities": []}, {"text": "The effect of number of seeds on learning is described in Section 4.6.", "labels": [], "entities": []}, {"text": "enrich their sentiment lexicon from WordNet as follows.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 36, "end_pos": 43, "type": "DATASET", "confidence": 0.9650600552558899}]}, {"text": "Synonyms of a positive word are positive while antonyms are treated as negative.", "labels": [], "entities": []}, {"text": "This basic version suffers from a very poor recall as shown in the for adjectives (see iteration 1).", "labels": [], "entities": [{"text": "recall", "start_pos": 44, "end_pos": 50, "type": "METRIC", "confidence": 0.9990455508232117}]}, {"text": "The recall can be improved fora slight trade-off in precision if we re-run the above algorithm on the output produced at the previous level.", "labels": [], "entities": [{"text": "recall", "start_pos": 4, "end_pos": 10, "type": "METRIC", "confidence": 0.99951171875}, {"text": "precision", "start_pos": 52, "end_pos": 61, "type": "METRIC", "confidence": 0.9994562268257141}]}, {"text": "This could be repeated iteratively until there is no noticeable change in precision/recall.", "labels": [], "entities": [{"text": "precision", "start_pos": 74, "end_pos": 83, "type": "METRIC", "confidence": 0.9991673231124878}, {"text": "recall", "start_pos": 84, "end_pos": 90, "type": "METRIC", "confidence": 0.9086352586746216}]}, {"text": "We consider this as the best possible F1-score produced by the Kim-Hovy method.", "labels": [], "entities": [{"text": "F1-score", "start_pos": 38, "end_pos": 46, "type": "METRIC", "confidence": 0.9983820915222168}]}, {"text": "The classwise F1 for this method is shown in.", "labels": [], "entities": [{"text": "F1", "start_pos": 14, "end_pos": 16, "type": "METRIC", "confidence": 0.9114570617675781}]}, {"text": "We use these scores as our baseline.: Precision/Recall/F1-scores for KimHovy method", "labels": [], "entities": [{"text": "Precision", "start_pos": 38, "end_pos": 47, "type": "METRIC", "confidence": 0.9889149069786072}, {"text": "Recall", "start_pos": 48, "end_pos": 54, "type": "METRIC", "confidence": 0.7038810849189758}, {"text": "F1-scores", "start_pos": 55, "end_pos": 64, "type": "METRIC", "confidence": 0.9101220965385437}, {"text": "KimHovy", "start_pos": 69, "end_pos": 76, "type": "DATASET", "confidence": 0.9337348937988281}]}], "tableCaptions": [{"text": " Table 1: English evaluation data from General In- quirer", "labels": [], "entities": [{"text": "General In- quirer", "start_pos": 39, "end_pos": 57, "type": "DATASET", "confidence": 0.8840576559305191}]}, {"text": " Table 2: Precision/Recall/F1-scores for Kim- Hovy method", "labels": [], "entities": [{"text": "Precision", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9634035229682922}, {"text": "Recall", "start_pos": 20, "end_pos": 26, "type": "METRIC", "confidence": 0.7204048037528992}, {"text": "F1-scores", "start_pos": 27, "end_pos": 36, "type": "METRIC", "confidence": 0.8768947124481201}]}, {"text": " Table 3. Note that  the seed data was fully unused except for the ex- amples \"good\" and \"bad\". We still test on the  same test data as earlier for comparing results.  Also note that the recall need not be 100 in this  case as we refrain from making a decision when  d(t, good) = d(t, bad).", "labels": [], "entities": [{"text": "recall", "start_pos": 187, "end_pos": 193, "type": "METRIC", "confidence": 0.9991125464439392}]}, {"text": " Table 3: Precision/Recall/F1-scores for prototype  method", "labels": [], "entities": [{"text": "Precision", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9545103311538696}, {"text": "Recall", "start_pos": 20, "end_pos": 26, "type": "METRIC", "confidence": 0.555945873260498}, {"text": "F1-scores", "start_pos": 27, "end_pos": 36, "type": "METRIC", "confidence": 0.6778534650802612}]}, {"text": " Table 4: Precision/Recall/F1-scores using mincuts  and randomized mincuts", "labels": [], "entities": [{"text": "Recall", "start_pos": 20, "end_pos": 26, "type": "METRIC", "confidence": 0.5901532769203186}, {"text": "F1-scores", "start_pos": 27, "end_pos": 36, "type": "METRIC", "confidence": 0.6218177080154419}]}, {"text": " Table 5: Precision/Recall/F1-scores for Label Pro- pogation", "labels": [], "entities": [{"text": "Precision", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9778984189033508}, {"text": "Recall", "start_pos": 20, "end_pos": 26, "type": "METRIC", "confidence": 0.6966068148612976}, {"text": "F1-scores", "start_pos": 27, "end_pos": 36, "type": "METRIC", "confidence": 0.7961527705192566}]}, {"text": " Table 6: Effect of adding hypernyms", "labels": [], "entities": []}, {"text": " Table 8: Evaluation on French dataset", "labels": [], "entities": [{"text": "French dataset", "start_pos": 24, "end_pos": 38, "type": "DATASET", "confidence": 0.9590160548686981}]}]}