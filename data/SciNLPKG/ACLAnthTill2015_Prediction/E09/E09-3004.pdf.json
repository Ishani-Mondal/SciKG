{"title": [{"text": "Finding Word Substitutions Using a Distributional Similarity Baseline and Immediate Context Overlap", "labels": [], "entities": [{"text": "Finding Word Substitutions", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.7394787073135376}]}], "abstractContent": [{"text": "This paper deals with the task of finding generally applicable substitutions fora given input term.", "labels": [], "entities": []}, {"text": "We show that the output of a distributional similarity system base-line can be filtered to obtain terms that are not simply similar but frequently substi-tutable.", "labels": [], "entities": []}, {"text": "Our filter relies on the fact that when two terms are in a common entail-ment relation, it should be possible to substitute one for the other in their most frequent surface contexts.", "labels": [], "entities": []}, {"text": "Using the Google 5-gram corpus to find such characteristic contexts, we show that for the given task, our filter improves the precision of a distributional similarity system from 41% to 56% on a test set comprising common transitive verbs.", "labels": [], "entities": [{"text": "Google 5-gram corpus", "start_pos": 10, "end_pos": 30, "type": "DATASET", "confidence": 0.6307328542073568}, {"text": "precision", "start_pos": 126, "end_pos": 135, "type": "METRIC", "confidence": 0.9994472861289978}]}], "introductionContent": [{"text": "This paper looks at the task of finding word substitutions for simple statements in the context of KB querying.", "labels": [], "entities": [{"text": "KB querying", "start_pos": 99, "end_pos": 110, "type": "TASK", "confidence": 0.8075980246067047}]}, {"text": "Let us assume that we have a knowledge base made of statements of the type 'subject -verb -object': 1.", "labels": [], "entities": []}, {"text": "Bank of America -acquire -Merrill Lynch 2.", "labels": [], "entities": [{"text": "Bank of America", "start_pos": 0, "end_pos": 15, "type": "DATASET", "confidence": 0.9031064112981161}, {"text": "Merrill Lynch 2", "start_pos": 26, "end_pos": 41, "type": "DATASET", "confidence": 0.8675883809725443}]}, {"text": "Lloyd's -buy -HBOS", "labels": [], "entities": [{"text": "Lloyd's -buy -", "start_pos": 0, "end_pos": 14, "type": "DATASET", "confidence": 0.9494032740592957}, {"text": "HBOS", "start_pos": 14, "end_pos": 18, "type": "DATASET", "confidence": 0.7601845264434814}]}], "datasetContent": [{"text": "The evaluation of KB or ontology extraction systems is typically done by presenting human judges with a subset of extracted data and asking them to annotate it according to certain correctness criteria.", "labels": [], "entities": [{"text": "KB or ontology extraction", "start_pos": 18, "end_pos": 43, "type": "TASK", "confidence": 0.6880515068769455}]}, {"text": "For entailment systems, the annotation usually relies on two tests: whether the meaning of one word entails the other one in some senses of those words, and whether the judges can come up with contexts in which the words are directly substitutable.", "labels": [], "entities": []}, {"text": "point out the difficulties in applying those criteria.", "labels": [], "entities": []}, {"text": "They note the low inter-annotator agreements obtained in previous studies and propose anew evaluation method based on precise judgement questions applied to a set of relevant contexts.", "labels": [], "entities": []}, {"text": "Using their methods, they evaluate the DIRT () and TEASE () algorithms and obtain upper bound precisions of 44% and 38% respectively on 646 entailment rules for 30 transitive verbs.", "labels": [], "entities": [{"text": "DIRT", "start_pos": 39, "end_pos": 43, "type": "METRIC", "confidence": 0.8251770734786987}, {"text": "TEASE", "start_pos": 51, "end_pos": 56, "type": "METRIC", "confidence": 0.992775559425354}, {"text": "precisions", "start_pos": 94, "end_pos": 104, "type": "METRIC", "confidence": 0.7746344804763794}]}, {"text": "We follow here their methodology to check the results obtained via the traditional annotation.", "labels": [], "entities": []}, {"text": "We then recalculate our best precision following the method introduced in.", "labels": [], "entities": [{"text": "precision", "start_pos": 29, "end_pos": 38, "type": "METRIC", "confidence": 0.9991481304168701}]}, {"text": "This approach consists in extracting, for each potential entailment relation X-verb 1 -Y \u21d2 X-verb 2 -Y, 15 sentences in which verb1 appears and ask annotators to provide answers to three questions: 1.", "labels": [], "entities": []}, {"text": "Is the left-hand side of the relation entailed by the sentence?", "labels": [], "entities": []}, {"text": "3. Does the sentence with verb 1 entail the sentence with verb 2 ? We show in some potential annotations at various stages of the process.", "labels": [], "entities": []}, {"text": "For each pair, Szpektor et al. then calculate a lower-bound precision as where n Entailed is the number of entailed sentence pairs (the annotator has answered 'yes' to the third question) and n Lef tHandEntailed is the number of sentences where the left-hand relation is entailed (the annotator has answered 'yes' to the first question).", "labels": [], "entities": [{"text": "precision", "start_pos": 60, "end_pos": 69, "type": "METRIC", "confidence": 0.9927061200141907}, {"text": "Entailed", "start_pos": 81, "end_pos": 89, "type": "METRIC", "confidence": 0.9829027652740479}]}, {"text": "They also calculate an upper-bound precision as where n Acceptable is the number of acceptable verb 2 sentences (the annotator has answered 'yes' to the second question).", "labels": [], "entities": [{"text": "precision", "start_pos": 35, "end_pos": 44, "type": "METRIC", "confidence": 0.9831243753433228}, {"text": "Acceptable", "start_pos": 56, "end_pos": 66, "type": "METRIC", "confidence": 0.9963337182998657}]}, {"text": "A pair is deemed to contain an entailment relation if the precision for that particular pair is over 80%.", "labels": [], "entities": [{"text": "precision", "start_pos": 58, "end_pos": 67, "type": "METRIC", "confidence": 0.9990835189819336}]}, {"text": "The authors comment that a large proportion of extracted sentences lead to a 'left-hand side not entailed' answer.", "labels": [], "entities": []}, {"text": "In order to counteract that effect, we only extract sentences without modals or negation from our Wikipedia corpus and consequently only require 10 sentences per relation (only 11% of our sentences have a 'non-entailed' left-hand side relation against 43% for.", "labels": [], "entities": []}, {"text": "We obtain an upper bound precision of 52%, which is slightly lower than the one initially calculated using our broad definition of entailment, showing that the more stringent evaluation is useful when checking for general substitutability in the returned pairs.", "labels": [], "entities": [{"text": "precision", "start_pos": 25, "end_pos": 34, "type": "METRIC", "confidence": 0.9873570203781128}]}, {"text": "When we calculate the lower bound precision, however, we obtain a low 10% precision due to the large number of sentences judged as 'unlikely English sentences' after substitution (they amount to 33% of all examples with a left-hand side judged 'entailed').", "labels": [], "entities": [{"text": "precision", "start_pos": 34, "end_pos": 43, "type": "METRIC", "confidence": 0.9867129325866699}, {"text": "precision", "start_pos": 74, "end_pos": 83, "type": "METRIC", "confidence": 0.9979848861694336}]}, {"text": "This result illustrates the need fora module able to check sentence acceptability when applying the system to true substitution tasks.", "labels": [], "entities": []}, {"text": "Fortunately, as we explain in the next section, it also takes into account requirements that are only necessary for generation tasks, and are therefore irrelevant to our querying task.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Results using 50 immediate contexts", "labels": [], "entities": []}, {"text": " Table 3: Results using 200 immediate contexts", "labels": [], "entities": []}]}