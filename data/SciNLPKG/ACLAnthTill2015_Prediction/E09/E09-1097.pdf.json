{"title": [{"text": "Improving Grammaticality in Statistical Sentence Generation: Introducing a Dependency Spanning Tree Algorithm with an Argument Satisfaction Model", "labels": [], "entities": [{"text": "Improving Grammaticality", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.935642659664154}, {"text": "Statistical Sentence Generation", "start_pos": 28, "end_pos": 59, "type": "TASK", "confidence": 0.7783602674802145}]}], "abstractContent": [{"text": "like text summarisation requires a means of producing novel summary sentences.", "labels": [], "entities": [{"text": "text summarisation", "start_pos": 5, "end_pos": 23, "type": "TASK", "confidence": 0.5862206071615219}]}, {"text": "In order to improve the grammati-cality of the generated sentence, we model a global (sentence) level syntactic structure.", "labels": [], "entities": []}, {"text": "We couch statistical sentence generation as a spanning tree problem in order to search for the best dependency tree spanning a set of chosen words.", "labels": [], "entities": [{"text": "statistical sentence generation", "start_pos": 9, "end_pos": 40, "type": "TASK", "confidence": 0.6723937392234802}]}, {"text": "We also introduce anew search algorithm for this task that models argument satisfaction to improve the linguistic validity of the generated tree.", "labels": [], "entities": []}, {"text": "We treat the allocation of modi-fiers to heads as a weighted bipartite graph matching (or assignment) problem, a well studied problem in graph theory.", "labels": [], "entities": [{"text": "weighted bipartite graph matching (or assignment)", "start_pos": 52, "end_pos": 101, "type": "TASK", "confidence": 0.7043194994330406}]}, {"text": "Using BLEU to measure performance on a string regeneration task, we found an improvement , illustrating the benefit of the spanning tree approach armed with an argument satisfaction model.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 6, "end_pos": 10, "type": "METRIC", "confidence": 0.9974002838134766}, {"text": "string regeneration task", "start_pos": 39, "end_pos": 63, "type": "TASK", "confidence": 0.8153550426165262}]}], "introductionContent": [{"text": "Research in statistical novel sentence generation has the potential to extend the current capabilities of automatic text summarisation technology, moving from sentence extraction to abstract-like summarisation.", "labels": [], "entities": [{"text": "statistical novel sentence generation", "start_pos": 12, "end_pos": 49, "type": "TASK", "confidence": 0.6481632441282272}, {"text": "text summarisation", "start_pos": 116, "end_pos": 134, "type": "TASK", "confidence": 0.6565352380275726}, {"text": "sentence extraction", "start_pos": 159, "end_pos": 178, "type": "TASK", "confidence": 0.732869490981102}]}, {"text": "In this paper, we describe anew algorithm that improves upon the grammaticality of statistically generated sentences, evaluated on a string regeneration task, which was first proposed as a surrogate fora grammaticality test by.", "labels": [], "entities": []}, {"text": "In this task, a system must regenerate the original sentence which has had its word order scrambled.", "labels": [], "entities": []}, {"text": "As an evaluation task, string regeneration reflects the issues that challenge the sentence generation components of machine translation, paraphrase generation, and summarisation systems).", "labels": [], "entities": [{"text": "string regeneration", "start_pos": 23, "end_pos": 42, "type": "TASK", "confidence": 0.8047167956829071}, {"text": "sentence generation", "start_pos": 82, "end_pos": 101, "type": "TASK", "confidence": 0.7092583179473877}, {"text": "machine translation", "start_pos": 116, "end_pos": 135, "type": "TASK", "confidence": 0.7420044839382172}, {"text": "paraphrase generation", "start_pos": 137, "end_pos": 158, "type": "TASK", "confidence": 0.8190038204193115}, {"text": "summarisation", "start_pos": 164, "end_pos": 177, "type": "TASK", "confidence": 0.9630352258682251}]}, {"text": "Our research in summarisation utilises the statistical generation algorithms described in this paper to generate novel summary sentences.", "labels": [], "entities": [{"text": "summarisation", "start_pos": 16, "end_pos": 29, "type": "TASK", "confidence": 0.9954278469085693}, {"text": "statistical generation", "start_pos": 43, "end_pos": 65, "type": "TASK", "confidence": 0.7562661468982697}]}, {"text": "The goal of the string regeneration task is to recover a sentence once its words have been randomly ordered.", "labels": [], "entities": [{"text": "string regeneration", "start_pos": 16, "end_pos": 35, "type": "TASK", "confidence": 0.7304385453462601}]}, {"text": "Similarly, fora text-to-text generation scenario, the goal is to generate a sentence given an unordered list of words, typically using an n-gram language model to select the best word ordering.", "labels": [], "entities": [{"text": "text-to-text generation", "start_pos": 16, "end_pos": 39, "type": "TASK", "confidence": 0.7589659690856934}]}, {"text": "N-gram language models appear to do well at a local level when examining word sequences smaller than n.", "labels": [], "entities": []}, {"text": "However, beyond this window size, the sequence is often ungrammatical.", "labels": [], "entities": []}, {"text": "This is not surprising as these methods are unable to model grammaticality at the sentence level, unless the size of n is sufficiently large.", "labels": [], "entities": []}, {"text": "In practice, the lack of sufficient training data means that n is often smaller than the average sentence length.", "labels": [], "entities": []}, {"text": "Even if data exists, increasing the size of n corresponds to a higher degree polynomial complexity search for the best word sequence.", "labels": [], "entities": []}, {"text": "In response, we introduce an algorithm for searching for the best word sequence in away that attempts to model grammaticality at the sentence level.", "labels": [], "entities": []}, {"text": "Mirroring the use of spanning tree algorithms in parsing (), we present an approach to statistical sentence generation.", "labels": [], "entities": [{"text": "statistical sentence generation", "start_pos": 87, "end_pos": 118, "type": "TASK", "confidence": 0.6953421831130981}]}, {"text": "Given a set of scrambled words, the approach searches for the most probable dependency tree, as defined by some corpus, such that it contains each word of the input set.", "labels": [], "entities": []}, {"text": "The tree is then traversed to obtain the final word ordering.", "labels": [], "entities": []}, {"text": "In particular, we present two spanning tree algorithms.", "labels": [], "entities": []}, {"text": "We first adapt the Chu-Liu-Edmonds (CLE) algorithm (see and), used in, to include a basic argument model, added to keep track of linear precedence between heads and modifiers.", "labels": [], "entities": []}, {"text": "While our adapted version of the CLE algorithm finds an optimal spanning tree, this does not always correspond with a linguistically valid dependency tree, primarily because it does not attempt to ensure that words in the tree have plausible numbers of arguments.", "labels": [], "entities": []}, {"text": "We propose an alternative dependencyspanning tree algorithm which uses a more fine-grained argument model representing argument positions.", "labels": [], "entities": []}, {"text": "To find the best modifiers for argument positions, we treat the attachment of edges to the spanning tree as a weighted bipartite graph matching problem (or the assignment problem), a standard problem in graph theory.", "labels": [], "entities": []}, {"text": "The remainder of this paper is as follows.", "labels": [], "entities": []}, {"text": "Section 2 outlines the graph representation of the spanning tree problem.", "labels": [], "entities": []}, {"text": "We describe a standard spanning tree algorithm in Section 3.", "labels": [], "entities": []}, {"text": "Section 4 defines a finer-grained argument model and presents anew dependency spanning tree search algorithm.", "labels": [], "entities": []}, {"text": "We experiment to determine whether a global dependency structure, as found by our algorithm, improves performance on the string regeneration problem, presenting results in Section 5.", "labels": [], "entities": [{"text": "string regeneration problem", "start_pos": 121, "end_pos": 148, "type": "TASK", "confidence": 0.8004854023456573}]}, {"text": "Related work is presented in Section 6.", "labels": [], "entities": []}, {"text": "Section 7 concludes that an argument model improves the linguistic plausibility of the generated trees, thus improving grammaticality in text generation.", "labels": [], "entities": [{"text": "text generation", "start_pos": 137, "end_pos": 152, "type": "TASK", "confidence": 0.714621514081955}]}], "datasetContent": [], "tableCaptions": []}