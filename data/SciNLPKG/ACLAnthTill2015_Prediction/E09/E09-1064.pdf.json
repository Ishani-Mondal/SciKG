{"title": [{"text": "Evaluating the Inferential Utility of Lexical-Semantic Resources", "labels": [], "entities": []}], "abstractContent": [{"text": "Lexical-semantic resources are used extensively for applied semantic inference, yet a clear quantitative picture of their current utility and limitations is largely missing.", "labels": [], "entities": []}, {"text": "We propose system-and application-independent evaluation and analysis methodologies for resources' performance , and systematically apply them to seven prominent resources.", "labels": [], "entities": []}, {"text": "Our findings identify the currently limited recall of available resources, and indicate the potential to improve performance by examining non-standard relation types and by distilling the output of distributional methods.", "labels": [], "entities": [{"text": "recall", "start_pos": 44, "end_pos": 50, "type": "METRIC", "confidence": 0.985680341720581}]}, {"text": "Further, our results stress the need to include auxiliary information regarding the lexical and logical contexts in which a lexical inference is valid, as well as its prior validity likelihood.", "labels": [], "entities": []}], "introductionContent": [{"text": "Lexical information plays a major role in semantic inference, as the meaning of one term is often inferred form another.", "labels": [], "entities": [{"text": "semantic inference", "start_pos": 42, "end_pos": 60, "type": "TASK", "confidence": 0.8414582312107086}]}, {"text": "Lexical-semantic resources, which provide the needed knowledge for lexical inference, are commonly utilized by applied inference systems ( and applications such as Information Retrieval and Question Answering ().", "labels": [], "entities": [{"text": "Information Retrieval", "start_pos": 164, "end_pos": 185, "type": "TASK", "confidence": 0.7843957841396332}, {"text": "Question Answering", "start_pos": 190, "end_pos": 208, "type": "TASK", "confidence": 0.8106988966464996}]}, {"text": "Beyond WordNet), a wide range of resources has been developed and utilized, including extensions to WordNet) and resources based on automatic distributional similarity methods.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 7, "end_pos": 14, "type": "DATASET", "confidence": 0.9476836919784546}, {"text": "WordNet", "start_pos": 100, "end_pos": 107, "type": "DATASET", "confidence": 0.9628658890724182}]}, {"text": "Recently, Wikipedia is emerging as a source for extracting semantic relationships ().", "labels": [], "entities": [{"text": "extracting semantic relationships", "start_pos": 48, "end_pos": 81, "type": "TASK", "confidence": 0.8274788657824198}]}, {"text": "As of today, only a partial comparative picture is available regarding the actual utility and limitations of available resources for lexical-semantic inference.", "labels": [], "entities": []}, {"text": "Works that do provide quantitative information regarding resources utility have focused on few particular resources ( and evaluated their impact on a specific system.", "labels": [], "entities": []}, {"text": "Most often, works which utilized lexical resources do not provide information about their isolated contribution; rather, they only report overall performance for systems in which lexical resources serve as components.", "labels": [], "entities": []}, {"text": "Our paper provides a step towards clarifying this picture.", "labels": [], "entities": []}, {"text": "We propose a system-and application-independent evaluation methodology that isolates resources' performance, and systematically apply it to seven prominent lexicalsemantic resources.", "labels": [], "entities": []}, {"text": "The evaluation and analysis methodology is specified within the Textual Entailment framework, which has become popular in recent years for modeling practical semantic inference in a generic manner (.", "labels": [], "entities": []}, {"text": "To that end, we assume certain definitions that extend the textual entailment paradigm to the lexical level.", "labels": [], "entities": []}, {"text": "The findings of our work provide useful insights and suggested directions for two research communities: developers of applied inference systems and researchers addressing lexical acquisition and resource construction.", "labels": [], "entities": [{"text": "lexical acquisition and resource construction", "start_pos": 171, "end_pos": 216, "type": "TASK", "confidence": 0.7790923714637756}]}, {"text": "Beyond the quantitative mapping of resources' performance, our analysis points at issues concerning their effective utilization and major characteristics.", "labels": [], "entities": []}, {"text": "Even more importantly, the results highlight current gaps in existing resources and point at directions towards filling them.", "labels": [], "entities": []}, {"text": "We show that the coverage of most resources is quite limited, where a substantial part of recall is attributable to semantic relations that are typically not available to inference systems.", "labels": [], "entities": [{"text": "recall", "start_pos": 90, "end_pos": 96, "type": "METRIC", "confidence": 0.9979013204574585}]}, {"text": "Notably, distributional acquisition methods are shown to provide many useful relationships which are missing from other resources, but these are embedded amongst many irrelevant ones.", "labels": [], "entities": [{"text": "distributional acquisition", "start_pos": 9, "end_pos": 35, "type": "TASK", "confidence": 0.7514152526855469}]}, {"text": "Additionally, the results highlight the need to represent and inference over various aspects of contextual information, which affect the applicability of lexical inferences.", "labels": [], "entities": []}, {"text": "We suggest that these gaps should be addressed by future research.", "labels": [], "entities": []}], "datasetContent": [{"text": "Figure 1: Evaluation methodology flowchart The input for our evaluation methodology is a lexical-semantic resource R, which contains lexical entailment rules.", "labels": [], "entities": []}, {"text": "We evaluate R's utility by testing how useful it is for inferring a sample of test hypotheses H from a corpus.", "labels": [], "entities": []}, {"text": "Each hypothesis in H contains more than one lexical element in order to provide implicit context validation for rule applications, e.g. h: water pollution.", "labels": [], "entities": []}, {"text": "We next describe the steps of our evaluation methodology, as illustrated in.", "labels": [], "entities": []}, {"text": "We refer to the examples in the figure when needed: 1) Fetch rules: For each h \u2208 H and each lexical element e \u2208 h (e.g. water), we fetch all rules e' \u21d2 e in R that might be applied to entail e (e.g. lake \u21d2 water).", "labels": [], "entities": []}, {"text": "2) Generate intermediate hypotheses h': For each rule r: e' \u21d2 e, we generate an intermediate hypothesis h by replacing e in h withe (e.g. h 1 : lake pollution).", "labels": [], "entities": []}, {"text": "From a text t entailing h , h can be further entailed by the single application of r.", "labels": [], "entities": []}, {"text": "We thus simulate the process by which an entailment system would infer h from t using r.", "labels": [], "entities": []}, {"text": "3) Retrieve matching texts: For each h we retrieve from a corpus all texts that contain the lemmatized words of h (not necessarily as a single phrase).", "labels": [], "entities": [{"text": "Retrieve matching texts", "start_pos": 3, "end_pos": 26, "type": "TASK", "confidence": 0.8895426988601685}]}, {"text": "These texts may entail h . We discard texts that also match h since entailing h from them might not require the application of any rule from the evaluated resource.", "labels": [], "entities": []}, {"text": "In our example, the retrieved texts contain lake and pollution but do not contain water.", "labels": [], "entities": []}, {"text": "4) Annotation: A sample of the retrieved texts is presented to human annotators.", "labels": [], "entities": []}, {"text": "The annotators are asked to answer the following two questions for each text, simulating the typical inference process of an entailment system: a) Does t entail h'?", "labels": [], "entities": []}, {"text": "If t does not entail h then the text would not provide a useful example for the application of r.", "labels": [], "entities": []}, {"text": "For instance, t 1 (in) does not entail h 1 and thus we cannot deduce h from it by applying the ruler.", "labels": [], "entities": []}, {"text": "Such texts are discarded from further evaluation.", "labels": [], "entities": []}, {"text": "b) Does t entail h?", "labels": [], "entities": []}, {"text": "If t is annotated as entailing h , an entailment system would then infer h from h by applying r.", "labels": [], "entities": []}, {"text": "If h is not entailed from t even though h is, the rule application is considered invalid.", "labels": [], "entities": []}, {"text": "For instance, t 2 does not entail h even though it entails h 2 . Indeed, the application of r 2 : *soil \u21d2 water 2 , from which h 2 was constructed, yields incorrect inference.", "labels": [], "entities": []}, {"text": "If the answer is 'yes', as in the case oft 3 , the application of r fort is considered valid.", "labels": [], "entities": []}, {"text": "The above process yields a sample of annotated rule applications for each test hypothesis, from which we can measure resources performance, as described in Section 5.", "labels": [], "entities": []}, {"text": "Current available state-of-the-art lexical-semantic resources mainly deal with nouns.", "labels": [], "entities": []}, {"text": "Therefore, we used nominal hypotheses for our experiment . We chose TREC 1-8 (excluding 4) as our test corpus and randomly sampled 25 ad-hoc queries of two-word compounds as our hypotheses.", "labels": [], "entities": []}, {"text": "We did not use longer hypotheses to ensure that enough texts containing the intermediate hypotheses are found in the corpus.", "labels": [], "entities": []}, {"text": "For annotation simplicity, we retrieved single sentences as our texts.", "labels": [], "entities": []}, {"text": "For each rule applied for an hypothesis h, we sampled 10 sentences from the sentences retrieved for that rule.", "labels": [], "entities": []}, {"text": "As a baseline, we also sampled 10 sentences for each original hypothesis h in which both words of hare found.", "labels": [], "entities": []}, {"text": "In total, 1550 unique sentences were sampled and annotated by two annotators.", "labels": [], "entities": []}, {"text": "To assess the validity of our evaluation methodology, the annotators first judged a sample of 220 sentences.", "labels": [], "entities": []}, {"text": "The Kappa scores for inter-annotator agreement were 0.74 and 0.64 for judging hand h, respectively.", "labels": [], "entities": []}, {"text": "These figures correspond to substantial agreement) and are comparable with related semantic annotations ().", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Lexical resources performance", "labels": [], "entities": []}, {"text": " Table 2: The distribution of invalid and valid rule applications by rule types: incorrect rules (INCOR), correct rules requiring", "labels": [], "entities": [{"text": "incorrect rules (INCOR)", "start_pos": 81, "end_pos": 104, "type": "METRIC", "confidence": 0.6721082329750061}]}]}