{"title": [], "abstractContent": [{"text": "Multi-source statistical machine translation is the process of generating a single translation from multiple inputs.", "labels": [], "entities": [{"text": "Multi-source statistical machine translation", "start_pos": 0, "end_pos": 44, "type": "TASK", "confidence": 0.5672827214002609}]}, {"text": "Previous work has focused primarily on selecting from potential outputs of separate translation systems, and solely on multi-parallel corpora and test sets.", "labels": [], "entities": []}, {"text": "We demonstrate how multi-source translation can be adapted for multiple monolingual inputs.", "labels": [], "entities": [{"text": "multi-source translation", "start_pos": 19, "end_pos": 43, "type": "TASK", "confidence": 0.6727569848299026}]}, {"text": "We also examine different approaches to dealing with multiple sources, including consensus decoding , and we present a novel method of input combination to generate lattices for multi-source translation within a single translation model.", "labels": [], "entities": []}], "introductionContent": [{"text": "Multi-source statistical machine translation was first formally defined by as the process of translating multiple meaningequivalent source language texts into a single target language.", "labels": [], "entities": [{"text": "Multi-source statistical machine translation", "start_pos": 0, "end_pos": 44, "type": "TASK", "confidence": 0.6201496720314026}]}, {"text": "Multi-source translation is of particular use when translating a document that has already been translated into several languages, either by humans or machines, and needs to be further translated into other target languages.", "labels": [], "entities": [{"text": "Multi-source translation", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.8021883368492126}]}, {"text": "This situation occurs often in large multi-lingual organisations such as the United Nations and the European Parliament, which must translate their proceedings into the languages of the member institutions.", "labels": [], "entities": []}, {"text": "It is also common in multi-national companies, which need to translate product and marketing documentation for their different markets.", "labels": [], "entities": []}, {"text": "Clearly, any existing translations fora document can help automatic translation into other languages.", "labels": [], "entities": []}, {"text": "These different versions of the input can resolve deficiencies and ambiguities (e.g., syntactic and semantic ambiguity) present in a single input, resulting in higher quality translation output.", "labels": [], "entities": []}, {"text": "In this paper, we present three models of multisource translation, with increasing degrees of sophistication, which we compare empirically on a number of different corpora.", "labels": [], "entities": [{"text": "multisource translation", "start_pos": 42, "end_pos": 65, "type": "TASK", "confidence": 0.7884678840637207}]}, {"text": "We generalize the definition of multi-source translation to include any translation case with multiple inputs and a single output, allowing for, e.g., multiple paraphrased inputs in a single language.", "labels": [], "entities": [{"text": "multi-source translation", "start_pos": 32, "end_pos": 56, "type": "TASK", "confidence": 0.692457765340805}]}, {"text": "Our methods include simple output selection, which treats the multisource translation task as many independent translation steps followed by selection of one of their outputs, and output combination, which uses consensus decoding to construct a string from n-gram fragments of the translation outputs ().", "labels": [], "entities": [{"text": "multisource translation task", "start_pos": 62, "end_pos": 90, "type": "TASK", "confidence": 0.8000513116518656}]}, {"text": "We also present a novel method, input combination, in which we compile the input texts into a compact lattice, over which we perform a single decoding pass.", "labels": [], "entities": []}, {"text": "We show that as we add additional inputs, the simplest output selection method performs quite poorly relative to a single input translation system, while the latter two methods are able to make better use of the additional inputs.", "labels": [], "entities": []}, {"text": "The paper is structured as follows.", "labels": [], "entities": []}, {"text": "\u00a72 presents the three methods for multi-source translation in detail: output selection, output combination, and our novel lattice-based method for input combination.", "labels": [], "entities": [{"text": "multi-source translation", "start_pos": 34, "end_pos": 58, "type": "TASK", "confidence": 0.6374751329421997}]}, {"text": "We report experiments applying these techniques to three different corpora, with both monolingual inputs ( \u00a73) and multilingual inputs ( \u00a74).", "labels": [], "entities": []}, {"text": "We finish in \u00a75 by analyzing the benefits and drawbacks of these approaches.", "labels": [], "entities": []}], "datasetContent": [{"text": "We start our experimental evaluation by translating multiple monolingual inputs into a foreign language.", "labels": [], "entities": []}, {"text": "This is a best-case scenario for testing and analytic purposes because we have a single translation model from one source language to one target language.", "labels": [], "entities": []}, {"text": "While translating from multiple monolingual inputs is not a common use for machine translation, it could be useful in situations where we have a number of paraphrases of the input text, e.g., cross-language information retrieval and summarization.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 75, "end_pos": 94, "type": "TASK", "confidence": 0.747891753911972}, {"text": "cross-language information retrieval", "start_pos": 192, "end_pos": 228, "type": "TASK", "confidence": 0.6910440524419149}, {"text": "summarization", "start_pos": 233, "end_pos": 246, "type": "TASK", "confidence": 0.9842618703842163}]}, {"text": "Data sets for this condition are readily available in the form of test sets created for machine translation evaluation, which contains multiple target references for each source sentence.", "labels": [], "entities": [{"text": "machine translation evaluation", "start_pos": 88, "end_pos": 118, "type": "TASK", "confidence": 0.8800845742225647}]}, {"text": "By flipping these test sets around, we create multiple monolingual inputs (the original references) and a single reference output (the original source text).", "labels": [], "entities": []}, {"text": "We examine two datasets: the BTEC Italian-English corpus (), and the Multiple Translation Chinese to English (MTC) corpora, as used in past years' NIST MT evaluations.", "labels": [], "entities": [{"text": "BTEC Italian-English corpus", "start_pos": 29, "end_pos": 56, "type": "DATASET", "confidence": 0.9196134805679321}, {"text": "Multiple Translation Chinese to English (MTC)", "start_pos": 69, "end_pos": 114, "type": "TASK", "confidence": 0.6918161995708942}, {"text": "NIST MT evaluations", "start_pos": 147, "end_pos": 166, "type": "TASK", "confidence": 0.5280628204345703}]}, {"text": "All of our translation experiments use the Moses decoder (, and are evaluated using BLEU-4.", "labels": [], "entities": [{"text": "BLEU-4", "start_pos": 84, "end_pos": 90, "type": "METRIC", "confidence": 0.9920380115509033}]}, {"text": "Moses is a phrase-based decoder with features for lexicalized reordering, distance-based reordering, phrase and word translation probabilities, phrase and word counts, and an n-gram language model.", "labels": [], "entities": [{"text": "phrase and word translation probabilities", "start_pos": 101, "end_pos": 142, "type": "TASK", "confidence": 0.6255816161632538}]}, {"text": "Multilingual cases are the traditional realm of multi-source translation.", "labels": [], "entities": [{"text": "multi-source translation", "start_pos": 48, "end_pos": 72, "type": "TASK", "confidence": 0.6934811174869537}]}, {"text": "We no longer have directly comparable translation models; instead each input language has a separate set of rules for translating to the output language.", "labels": [], "entities": []}, {"text": "However, the availability of (and demand for) multi-parallel corpora makes this form of multi-source translation of great practical use.", "labels": [], "entities": [{"text": "multi-source translation", "start_pos": 88, "end_pos": 112, "type": "TASK", "confidence": 0.6688769161701202}]}], "tableCaptions": [{"text": " Table 1: Example minimum TER edit script.", "labels": [], "entities": [{"text": "TER", "start_pos": 26, "end_pos": 29, "type": "METRIC", "confidence": 0.9205705523490906}]}, {"text": " Table 2: BLEU scores on the BTEC test set for  translating English inputs into Italian.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9993495345115662}, {"text": "BTEC test set", "start_pos": 29, "end_pos": 42, "type": "DATASET", "confidence": 0.976784348487854}]}, {"text": " Table 3: BLEU scores using single inputs from  each different team on the MTC. Bold indicates  the better score between All and Self tuning.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9983277916908264}, {"text": "MTC", "start_pos": 75, "end_pos": 78, "type": "DATASET", "confidence": 0.8866173028945923}]}, {"text": " Table 4: BLEU scores for multi-source translations  of MTC test sets. Better score for each output- based multi-source method is shown in bold.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9991958737373352}, {"text": "MTC test sets", "start_pos": 56, "end_pos": 69, "type": "DATASET", "confidence": 0.7232182621955872}]}, {"text": " Table 5: BLEU scores for individual translation  systems into English trained on Europarl, from  best to worst.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9994507431983948}, {"text": "Europarl", "start_pos": 82, "end_pos": 90, "type": "DATASET", "confidence": 0.9917090535163879}]}, {"text": " Table 6: BLEU scores for multi-source translation  systems into English trained on Europarl. Single  source French decoding is shown as a baseline.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9993336796760559}, {"text": "Europarl", "start_pos": 84, "end_pos": 92, "type": "DATASET", "confidence": 0.9935094714164734}]}]}