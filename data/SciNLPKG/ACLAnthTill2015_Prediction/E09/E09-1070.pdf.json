{"title": [{"text": "Analysing Wikipedia and Gold-Standard Corpora for NER Training", "labels": [], "entities": [{"text": "Analysing Wikipedia", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.6367703527212143}, {"text": "NER", "start_pos": 50, "end_pos": 53, "type": "TASK", "confidence": 0.888703465461731}]}], "abstractContent": [{"text": "Named entity recognition (NER) for En-glish typically involves one of three gold standards: MUC, CoNLL, or BBN, all created by costly manual annotation.", "labels": [], "entities": [{"text": "Named entity recognition (NER)", "start_pos": 0, "end_pos": 30, "type": "TASK", "confidence": 0.8016029993693033}, {"text": "MUC", "start_pos": 92, "end_pos": 95, "type": "DATASET", "confidence": 0.7283645272254944}, {"text": "BBN", "start_pos": 107, "end_pos": 110, "type": "METRIC", "confidence": 0.8640124797821045}]}, {"text": "Recent work has used Wikipedia to automatically create a massive corpus of named entity annotated text.", "labels": [], "entities": []}, {"text": "We present the first comprehensive cross-corpus evaluation of NER.", "labels": [], "entities": [{"text": "NER", "start_pos": 62, "end_pos": 65, "type": "TASK", "confidence": 0.9302575588226318}]}, {"text": "We identify the causes of poor cross-corpus performance and demonstrate ways of making them more compatible.", "labels": [], "entities": []}, {"text": "Using our process, we develop a Wikipedia corpus which out-performs gold standard corpora on cross-corpus evaluation by up to 11%.", "labels": [], "entities": [{"text": "Wikipedia corpus", "start_pos": 32, "end_pos": 48, "type": "DATASET", "confidence": 0.912488579750061}]}], "introductionContent": [{"text": "Named Entity Recognition (NER), the task of identifying and classifying the names of people, organisations and other entities within text, is central to many NLP systems.", "labels": [], "entities": [{"text": "Named Entity Recognition (NER)", "start_pos": 0, "end_pos": 30, "type": "TASK", "confidence": 0.7761795918146769}, {"text": "identifying and classifying the names of people, organisations and other entities within text", "start_pos": 44, "end_pos": 137, "type": "TASK", "confidence": 0.6758478581905365}]}, {"text": "NER developed from information extraction in the Message Understanding Conferences (MUC) of the 1990s.", "labels": [], "entities": [{"text": "NER", "start_pos": 0, "end_pos": 3, "type": "TASK", "confidence": 0.974097728729248}, {"text": "information extraction", "start_pos": 19, "end_pos": 41, "type": "TASK", "confidence": 0.7648516595363617}, {"text": "Message Understanding Conferences (MUC)", "start_pos": 49, "end_pos": 88, "type": "TASK", "confidence": 0.817358672618866}]}, {"text": "By MUC 6 and 7, NER had become a distinct task: tagging proper names, and temporal and numerical expressions.", "labels": [], "entities": [{"text": "MUC 6", "start_pos": 3, "end_pos": 8, "type": "DATASET", "confidence": 0.7692050635814667}, {"text": "NER", "start_pos": 16, "end_pos": 19, "type": "TASK", "confidence": 0.9842022657394409}, {"text": "tagging proper names", "start_pos": 48, "end_pos": 68, "type": "TASK", "confidence": 0.8952000141143799}]}, {"text": "Statistical machine learning systems have proven successful for NER.", "labels": [], "entities": [{"text": "NER", "start_pos": 64, "end_pos": 67, "type": "TASK", "confidence": 0.9843153357505798}]}, {"text": "These learn patterns associated with individual entity classes, making use of many contextual, orthographic, linguistic and external knowledge features.", "labels": [], "entities": []}, {"text": "However, they rely heavily on large annotated training corpora.", "labels": [], "entities": []}, {"text": "This need for costly expert annotation hinders the creation of more task-adaptable, highperformance named entity recognisers.", "labels": [], "entities": []}, {"text": "In acquiring new sources for annotated corpora, we require an analysis of training data as a variable in NER.", "labels": [], "entities": []}, {"text": "This paper compares the three main goldstandard corpora.", "labels": [], "entities": []}, {"text": "We found that tagging models built on each corpus perform relatively poorly when tested on the others.", "labels": [], "entities": []}, {"text": "We therefore present three methods for analysing internal and intercorpus inconsistencies.", "labels": [], "entities": []}, {"text": "Our analysis demonstrates that seemingly minor variations in the text itself, starting right from tokenisation can have a huge impact on practical NER performance.", "labels": [], "entities": [{"text": "NER", "start_pos": 147, "end_pos": 150, "type": "TASK", "confidence": 0.9867371916770935}]}, {"text": "We take this experience and apply it to a corpus created automatically using Wikipedia.", "labels": [], "entities": []}, {"text": "This corpus was created following the method of.", "labels": [], "entities": []}, {"text": "By training the C&C tagger) on the gold-standard corpora and our new Wikipedia-derived training data, we evaluate the usefulness of the latter and explore the nature of the training corpus as a variable in NER.", "labels": [], "entities": []}, {"text": "Our Wikipedia-derived corpora exceed the performance of non-corresponding training and test sets by up to 11% F -score, and can be engineered to automatically produce models consistent with various NE-annotation schema.", "labels": [], "entities": [{"text": "F -score", "start_pos": 110, "end_pos": 118, "type": "METRIC", "confidence": 0.9925547043482462}]}, {"text": "We show that it is possible to automatically create large, free, named entity-annotated corpora for general or domain specific tasks.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluated our annotation process by building separate NER models learned from Wikipediaderived and gold-standard data.", "labels": [], "entities": [{"text": "Wikipediaderived and gold-standard data", "start_pos": 81, "end_pos": 120, "type": "DATASET", "confidence": 0.7852527722716331}]}, {"text": "Our results are given as micro-averaged precision, recall and Fscores both in terms of MUC-style and CoNLL-style (exact-match) scoring.", "labels": [], "entities": [{"text": "precision", "start_pos": 40, "end_pos": 49, "type": "METRIC", "confidence": 0.9776324033737183}, {"text": "recall", "start_pos": 51, "end_pos": 57, "type": "METRIC", "confidence": 0.9997870326042175}, {"text": "Fscores", "start_pos": 62, "end_pos": 69, "type": "METRIC", "confidence": 0.9997318387031555}, {"text": "MUC-style", "start_pos": 87, "end_pos": 96, "type": "METRIC", "confidence": 0.7684544920921326}, {"text": "CoNLL-style", "start_pos": 101, "end_pos": 112, "type": "DATASET", "confidence": 0.5129354596138}]}, {"text": "We evaluated all experiments with and without the MISC category.", "labels": [], "entities": [{"text": "MISC", "start_pos": 50, "end_pos": 54, "type": "TASK", "confidence": 0.7558152079582214}]}, {"text": "Wikipedia's articles are freely available for download.", "labels": [], "entities": []}, {"text": "We have used data from the 2008 May 22 dump of English Wikipedia which includes 2.3 million articles.", "labels": [], "entities": [{"text": "May 22 dump of English Wikipedia", "start_pos": 32, "end_pos": 64, "type": "DATASET", "confidence": 0.7759216924508413}]}, {"text": "Splitting this into sentences and tokenising produced 32 million sentences each containing an average of 24 tokens.", "labels": [], "entities": [{"text": "tokenising", "start_pos": 34, "end_pos": 44, "type": "TASK", "confidence": 0.9732205271720886}]}, {"text": "Our experiments were performed with a Wikipedia corpus of 3.5 million tokens.", "labels": [], "entities": [{"text": "Wikipedia corpus", "start_pos": 38, "end_pos": 54, "type": "DATASET", "confidence": 0.8810966610908508}]}, {"text": "Although we had up to 294 million tokens available, we were limited by the RAM required by the C&C tagger training software.", "labels": [], "entities": [{"text": "RAM", "start_pos": 75, "end_pos": 78, "type": "METRIC", "confidence": 0.9858784675598145}, {"text": "C&C tagger training", "start_pos": 95, "end_pos": 114, "type": "TASK", "confidence": 0.7211532235145569}]}, {"text": "match and MUC-style evaluations (which are typically a few percent higher).", "labels": [], "entities": [{"text": "match", "start_pos": 0, "end_pos": 5, "type": "METRIC", "confidence": 0.9824157953262329}, {"text": "MUC-style", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.7919045090675354}]}, {"text": "The cross-corpus gold standard experiments on the DEV sets are shown first in both tables.", "labels": [], "entities": [{"text": "DEV sets", "start_pos": 50, "end_pos": 58, "type": "DATASET", "confidence": 0.9839938282966614}]}, {"text": "As in, the performance drops significantly when the training and test corpus are from different sources.", "labels": [], "entities": []}, {"text": "The corresponding TEST set scores are given in.", "labels": [], "entities": [{"text": "TEST set scores", "start_pos": 18, "end_pos": 33, "type": "METRIC", "confidence": 0.8290094335873922}]}, {"text": "The second group of experiments in these tables show the performance of Wikipedia corpora with increasing levels of link inference (described in Section 6.1).", "labels": [], "entities": []}, {"text": "Links inferred upon matching article titles (WP1) and disambiguation titles (WP2) consistently increase F -score by \u223c5%, while surnames for PER entities (WP3) and all link texts (WP4) tend to introduce error.", "labels": [], "entities": [{"text": "F -score", "start_pos": 104, "end_pos": 112, "type": "METRIC", "confidence": 0.99468994140625}]}, {"text": "A key result of our work is that the performance of noncorresponding gold standards is often significantly exceeded by our Wikipedia training data.", "labels": [], "entities": [{"text": "Wikipedia training data", "start_pos": 123, "end_pos": 146, "type": "DATASET", "confidence": 0.8004152377446493}]}], "tableCaptions": [{"text": " Table 1: Gold-standard NE-annotated corpora", "labels": [], "entities": []}, {"text": " Table 2: Gold standard F -scores (exact-match)", "labels": [], "entities": [{"text": "F -scores", "start_pos": 24, "end_pos": 33, "type": "METRIC", "confidence": 0.9703864852587382}, {"text": "exact-match)", "start_pos": 35, "end_pos": 47, "type": "METRIC", "confidence": 0.8574854731559753}]}, {"text": " Table 3: Examples of n-gram tag variations in BBN (top) and CoNLL (bottom). Nucleus is in bold.", "labels": [], "entities": [{"text": "BBN", "start_pos": 47, "end_pos": 50, "type": "DATASET", "confidence": 0.5969838500022888}]}, {"text": " Table 4: Tag sequence confusion on BBN DEV when training on gold-standard corpora (no MISC)", "labels": [], "entities": [{"text": "BBN DEV", "start_pos": 36, "end_pos": 43, "type": "DATASET", "confidence": 0.9518944919109344}]}, {"text": " Table 6: Tag sequence confusion on BBN DEV with training on BBN and the Wikipedia baseline", "labels": [], "entities": [{"text": "BBN DEV", "start_pos": 36, "end_pos": 43, "type": "DATASET", "confidence": 0.9450623691082001}, {"text": "BBN", "start_pos": 61, "end_pos": 64, "type": "DATASET", "confidence": 0.9758102893829346}, {"text": "Wikipedia baseline", "start_pos": 73, "end_pos": 91, "type": "DATASET", "confidence": 0.9196092188358307}]}, {"text": " Table 7: Exact-match DEV F -scores", "labels": [], "entities": [{"text": "Exact-match DEV F -scores", "start_pos": 10, "end_pos": 35, "type": "METRIC", "confidence": 0.6558081805706024}]}, {"text": " Table 8: MUC-style DEV F -scores", "labels": [], "entities": [{"text": "MUC-style DEV F -scores", "start_pos": 10, "end_pos": 33, "type": "METRIC", "confidence": 0.6572446644306182}]}, {"text": " Table 9: Wikipedia as additional training data", "labels": [], "entities": []}, {"text": " Table 10: Exact-match TEST results for WP2", "labels": [], "entities": [{"text": "Exact-match", "start_pos": 11, "end_pos": 22, "type": "METRIC", "confidence": 0.6531798243522644}, {"text": "TEST", "start_pos": 23, "end_pos": 27, "type": "METRIC", "confidence": 0.837445080280304}, {"text": "WP2", "start_pos": 40, "end_pos": 43, "type": "TASK", "confidence": 0.6553995013237}]}, {"text": " Table 11: MUC-eval TEST results for WP2", "labels": [], "entities": [{"text": "MUC-eval", "start_pos": 11, "end_pos": 19, "type": "DATASET", "confidence": 0.43365278840065}, {"text": "TEST", "start_pos": 20, "end_pos": 24, "type": "METRIC", "confidence": 0.7800015807151794}, {"text": "WP2", "start_pos": 37, "end_pos": 40, "type": "TASK", "confidence": 0.5765476822853088}]}, {"text": " Table 12: Tokens in BBN DEV that our Wikipedia model frequently mis-tagged", "labels": [], "entities": [{"text": "BBN DEV", "start_pos": 21, "end_pos": 28, "type": "DATASET", "confidence": 0.9330861270427704}]}]}