{"title": [], "abstractContent": [{"text": "We present several algorithms for assigning heads in phrase structure trees, based on different linguistic intuitions on the role of heads in natural language syntax.", "labels": [], "entities": [{"text": "assigning heads in phrase structure trees", "start_pos": 34, "end_pos": 75, "type": "TASK", "confidence": 0.7509624361991882}]}, {"text": "Starting point of our approach is the observation that a head-annotated treebank defines a unique lexicalized tree substitution grammar.", "labels": [], "entities": []}, {"text": "This allows us to go back and forth between the two representations, and define objective functions for the unsu-pervised learning of head assignments in terms of features of the implicit lexical-ized tree grammars.", "labels": [], "entities": []}, {"text": "We evaluate algorithms based on the match with gold standard head-annotations, and the comparative parsing accuracy of the lexicalized grammars they give rise to.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 107, "end_pos": 115, "type": "METRIC", "confidence": 0.9263642430305481}]}, {"text": "On the first task, we approach the accuracy of hand-designed heuristics for English and inter-annotation-standard agreement for Ger-man.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 35, "end_pos": 43, "type": "METRIC", "confidence": 0.998985230922699}]}, {"text": "On the second task, the implied lex-icalized grammars score 4% points higher on parsing accuracy than lexicalized grammars derived by commonly used heuris-tics.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 88, "end_pos": 96, "type": "METRIC", "confidence": 0.9637343883514404}]}], "introductionContent": [{"text": "The head of a phrasal constituent is a central concept inmost current grammatical theories and many syntax-based NLP techniques.", "labels": [], "entities": []}, {"text": "The term is used to mark, for any nonterminal node in a syntactic tree, the specific daughter node that fulfills a special role; however, theories and applications differ widely in what that special role is supposed to be.", "labels": [], "entities": []}, {"text": "In descriptive grammatical theories, the role of the head can range from the determinant of agreement or the locus of inflections, to the governor that selects the morphological form of its sister nodes or the constituent that is distributionally equivalent to its parent ().", "labels": [], "entities": []}, {"text": "In computational linguistics, heads mainly serve to select the lexical content on which the probability of a production should depend).", "labels": [], "entities": []}, {"text": "With the increased popularity of dependency parsing, head annotations have also become a crucial level of syntactic information for transforming constituency treebanks to dependency structures (  or richer syntactic representations (e.g.,.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 33, "end_pos": 51, "type": "TASK", "confidence": 0.8896349668502808}]}, {"text": "For the WSJ-section of the Penn Treebank, a set of heuristic rules for assigning heads has emerged from the work of and) that has been employed in a wide variety of studies and proven extremely useful, even in rather different applications from what the rules were originally intended for.", "labels": [], "entities": [{"text": "WSJ-section of the Penn Treebank", "start_pos": 8, "end_pos": 40, "type": "DATASET", "confidence": 0.8891847372055054}]}, {"text": "However, the rules are specific to English and the treebank's syntactic annotation, and do not offer much insights into how headedness can be learned in principle or in practice.", "labels": [], "entities": []}, {"text": "Moreover, the rules are heuristic and might still leave room for improvement with respect to recovering linguistic head assignment even on the Penn WSJ corpus; in fact, we find that the headassignments according to the Magerman-Collins rules correspond only in 85% of the cases to dependencies such as annotated in PARC 700 Dependency Bank (see section 5).", "labels": [], "entities": [{"text": "Penn WSJ corpus", "start_pos": 143, "end_pos": 158, "type": "DATASET", "confidence": 0.9714045723279318}, {"text": "PARC 700 Dependency Bank", "start_pos": 315, "end_pos": 339, "type": "DATASET", "confidence": 0.9700739085674286}]}, {"text": "Automatic methods for identifying heads are therefore of interest, both for practical and more fundamental linguistic reasons.", "labels": [], "entities": []}, {"text": "In this paper we investigate possible ways of finding heads based on lexicalized tree structures that can be extracted from an available treebank.", "labels": [], "entities": []}, {"text": "The starting point of our approach is the observation that a headannotated treebank (obeying the constraint that every nonterminal node has exactly one daughter marked as head) defines a unique lexicalized tree substitution grammar (obeying the constraint that every elementary tree has exactly one lexical anchor).", "labels": [], "entities": []}, {"text": "This allows us to go back and forth between the two representations, and define objective functions for the unsupervised learning of head assignments in terms of features of the implicit Lexicalized Tree Substitution Grammars.", "labels": [], "entities": []}, {"text": "Using this grammar formalism (LTSGs) we will investigate which objective functions we should optimize for recovering heads.", "labels": [], "entities": []}, {"text": "Should we try to reduce uncertainty about the grammatical frames that can be associated with a particular lexical item?", "labels": [], "entities": []}, {"text": "Or should we assume that linguistic head assignments are based on the occurrence frequencies of the productive units they imply?", "labels": [], "entities": []}, {"text": "We present two new algorithms for unsupervised recovering of heads -entropy minimization and a greedy technique we call \"familiarity maximization\" -that can be seen as ways to operationalize these last two linguistic intuitions.", "labels": [], "entities": []}, {"text": "Both algorithms are unsupervised, in the sense that they are trained on data without head annotations, but both take labeled phrase-structure trees as input.", "labels": [], "entities": []}, {"text": "Our work fits well with several recent approaches aimed at completely unsupervised learning of the key aspects of syntactic structure: lexical categories, phrase-structure (, phrasal categories ( and dependencies ().", "labels": [], "entities": []}, {"text": "For the specific task addressed in this paperassigning heads in treebanks -we only know of one earlier paper:.", "labels": [], "entities": []}, {"text": "These authors investigated a technique for identifying heads in constituency trees based on maximizing likelihood, using EM, under a Tree Insertion Grammar (TIG)model . In this approach, headedness in some sense becomes a state-split, allowing for grammars that more closely match empirical distributions over trees.", "labels": [], "entities": []}, {"text": "The authors report somewhat disappointing results, however: the automatically induced head-annotations do not lead to significantly more accurate parsers than simple leftmost or rightmost head assignment schemes 2 . In section 2 we define the grammar model we will use.", "labels": [], "entities": []}, {"text": "In section 3 we describe the headassignment algorithms.", "labels": [], "entities": [{"text": "headassignment", "start_pos": 29, "end_pos": 43, "type": "TASK", "confidence": 0.9701544046401978}]}, {"text": "In section 4, 5 and 6 we The space over the possible head assignments that these authors consider -essentially regular expressions over CFG rules -is more restricted than in the current work where we consider a larger \"domain of locality\".", "labels": [], "entities": []}, {"text": "2 However, the authors' approach of using EM for inducing latent information in treebanks has led to extremely accurate constituency parsers, that neither make use of nor produce headedness information; see ( then describe our evaluations of these algorithms.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 2: Parsing accuracy on WSJ20 of the LTSGs  extracted from various head assignments, when  computing the most probable derivations for ev- ery sentence in the test set. The Labeled F-Score  (LFS) and unlabeled F-Score (UFS) results are re- ported. The final column gives the total number of  extracted elementary trees (in thousands).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 18, "end_pos": 26, "type": "METRIC", "confidence": 0.9691603183746338}, {"text": "WSJ20", "start_pos": 30, "end_pos": 35, "type": "DATASET", "confidence": 0.9222643971443176}]}, {"text": " Table 3: Evaluation on WSJ20 of various head as- signments on Bikel's parser.", "labels": [], "entities": [{"text": "WSJ20", "start_pos": 24, "end_pos": 29, "type": "DATASET", "confidence": 0.8592700958251953}]}, {"text": " Table 1: Percentage of correct head assignments against gold standard in Penn WSJ and Tiger.   \u2020 The Tiger treebank already comes with built-in head labels, but not for all categories. In this case the  score is computed only for the internal nodes that conform to the one head per node constraint.", "labels": [], "entities": [{"text": "Penn WSJ", "start_pos": 74, "end_pos": 82, "type": "DATASET", "confidence": 0.9334435760974884}, {"text": "Tiger", "start_pos": 87, "end_pos": 92, "type": "DATASET", "confidence": 0.8523228764533997}, {"text": "Tiger treebank", "start_pos": 102, "end_pos": 116, "type": "DATASET", "confidence": 0.96210977435112}]}]}