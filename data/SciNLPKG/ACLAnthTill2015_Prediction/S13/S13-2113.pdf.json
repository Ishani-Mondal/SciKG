{"title": [{"text": "UoS: A Graph-Based System for Graded Word Sense Induction", "labels": [], "entities": [{"text": "Graded Word Sense Induction", "start_pos": 30, "end_pos": 57, "type": "TASK", "confidence": 0.6349364668130875}]}], "abstractContent": [{"text": "This paper presents UoS, a graph-based Word Sense Induction system which attempts to find all applicable senses of a target word given its context, grading each sense according to its suitability to the context.", "labels": [], "entities": [{"text": "Word Sense Induction", "start_pos": 39, "end_pos": 59, "type": "TASK", "confidence": 0.5640533864498138}]}, {"text": "Senses of a target word are induced through use of a non-parameterised, linear-time clustering algorithm that returns maximal quasi-strongly connected components of a target word graph in which vertex pairs are assigned to the same cluster if either vertex has the highest edge weight to the other.", "labels": [], "entities": []}, {"text": "UoS participated in SemEval-2013 Task 13: Word Sense Induction for Graded and Non-Graded Senses.", "labels": [], "entities": [{"text": "UoS", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.9016591906547546}, {"text": "SemEval-2013 Task 13", "start_pos": 20, "end_pos": 40, "type": "TASK", "confidence": 0.7887802918752035}]}, {"text": "Two system were submitted; both systems returned results comparable with those of the best performing systems.", "labels": [], "entities": []}], "introductionContent": [{"text": "Word Sense Induction (WSI) is the task of automatically discovering word senses from text.", "labels": [], "entities": [{"text": "Word Sense Induction (WSI)", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.7578999946514765}]}, {"text": "In principle, WSI avoids reliance on a pre-defined sense inventory.", "labels": [], "entities": [{"text": "WSI", "start_pos": 14, "end_pos": 17, "type": "TASK", "confidence": 0.9793611168861389}]}, {"text": "Whereas the related task of Word Sense Disambiguation (WSD) can only assign pre-defined senses to words on the basis of context, WSI follows the dictum that \"The meaning of a word is its use in the language.\" to discover senses through examination of context of use in large text corpora.", "labels": [], "entities": [{"text": "Word Sense Disambiguation (WSD)", "start_pos": 28, "end_pos": 59, "type": "TASK", "confidence": 0.7790402223666509}]}, {"text": "WSI, therefore, maybe applied to discover new, rare, or domain specific senses; senses undefined in existing sense inventories.", "labels": [], "entities": []}, {"text": "Previous WSI evaluations) have approached sense induction in terms of finding the single most salient sense of a target word given its context.", "labels": [], "entities": [{"text": "WSI", "start_pos": 9, "end_pos": 12, "type": "TASK", "confidence": 0.906669020652771}, {"text": "sense induction", "start_pos": 42, "end_pos": 57, "type": "TASK", "confidence": 0.7382280230522156}]}, {"text": "However, as shown in, a graded notion of sense maybe more applicable, as multiple senses of the target word maybe perceived by readers.", "labels": [], "entities": []}, {"text": "The SemEval-2013 WSI evaluation described in this paper is designed to explore the possibility of finding all perceived senses of a target word in a single contextual instance.", "labels": [], "entities": [{"text": "SemEval-2013 WSI", "start_pos": 4, "end_pos": 20, "type": "TASK", "confidence": 0.5692935585975647}]}, {"text": "The aim for participants in the task is therefore to design a system that will induce a set of graded (weighted) senses of a target word in a particular context.", "labels": [], "entities": []}, {"text": "The paper is organised as follows: Section 2 introduces SemEval-2013 Task 13: Word Sense Induction for Graded and Non-Graded Senses; Section 3 presents UoS, the system that participated in the task; Section 4 reports evaluation results, showing that UoS returns scores comparable with those of the best performing systems.", "labels": [], "entities": []}], "datasetContent": [{"text": "Systems are evaluated in two ways: (1) in a WSD task and (2), a clustering task.", "labels": [], "entities": [{"text": "WSD task", "start_pos": 44, "end_pos": 52, "type": "TASK", "confidence": 0.8290904760360718}]}, {"text": "In the first evaluation, systems are assessed by their ability to correctly identify which WordNet 3.1 senses of the target word are applicable in a given instance, and to quantify, and so, rank, senses according to their level of applicability.", "labels": [], "entities": []}, {"text": "The supervised evaluation method of previous SemEval WSI tasks) is applied to map induced senses to WordNet 3.1 senses, with the mapping function of Jurgens (2012) used to account for the applicability weights.", "labels": [], "entities": [{"text": "SemEval WSI tasks", "start_pos": 45, "end_pos": 62, "type": "TASK", "confidence": 0.8087548414866129}]}, {"text": "Three evaluation metrics are used - \u2022 Jaccard Index: measures the overlap between gold standard senses and those returned by a WSI system.", "labels": [], "entities": [{"text": "Jaccard Index", "start_pos": 38, "end_pos": 51, "type": "METRIC", "confidence": 0.8699874579906464}]}, {"text": "\u2022 Positionally-Weighted Kendall's Tau: measures the ability of a system to rank senses by their applicability.", "labels": [], "entities": [{"text": "Positionally-Weighted Kendall's Tau", "start_pos": 2, "end_pos": 37, "type": "METRIC", "confidence": 0.7394486591219902}]}, {"text": "\u2022 Weighted Normalized Discounted Cumulative Gain (NDCG): measures the agreement in applicability ratings, accounting for both the ranking and difference in weights assigned to senses.", "labels": [], "entities": [{"text": "Weighted Normalized Discounted Cumulative Gain (NDCG)", "start_pos": 2, "end_pos": 55, "type": "METRIC", "confidence": 0.6984254755079746}]}, {"text": "In the second evaluation, similarity between a participant's clustering solution and that of the gold standard set of senses is measured using two metrics - \u2022 Fuzzy Normalised Mutual Information (NMI): extends the method of to compute NMI between overlapping (fuzzy) clusters.", "labels": [], "entities": []}, {"text": "Fuzzy NMI measures the alignment of system and gold standard senses independently of the cluster sizes, so returns a measure of how well a WSI system would perform regardless of the sense distribution in a corpus.", "labels": [], "entities": []}, {"text": "\u2022 Fuzzy B-Cubed: adapts the overlapping BCubed measure defined in Amig\u00f3 et al. to the fuzzy clustering setting.", "labels": [], "entities": []}, {"text": "As an itembased, rather than cluster-based, measure, Fuzzy B-Cubed is sensitive to cluster size skew, thus captures the expected performance of a WSI system on anew corpus where the sense distribution is the same.", "labels": [], "entities": []}, {"text": "Two sets of results were submitted.", "labels": [], "entities": []}, {"text": "The first, UoS (top 3), returns the three highest scoring senses for each instance; the second, UoS (# WN senses), returns then = number of target word senses in WordNet 3.1 most cohesive clusters, as defined by Equation (2).", "labels": [], "entities": []}, {"text": "Results for the seven participating WSI systems are reported in \u2022 SemCor, Most Frequent Sense (MFS): labels each instance with the MFS in SemCor.", "labels": [], "entities": [{"text": "Most Frequent Sense (MFS)", "start_pos": 74, "end_pos": 99, "type": "METRIC", "confidence": 0.7430983583132426}]}, {"text": "9 \u2022 SemCor, All Senses: labels each instance with all SemCor senses, weighting each according to its frequency in SemCor.", "labels": [], "entities": []}, {"text": "\u2022 1 sense per instance: labels each instance with a unique induced sense, equivalent to the 1 cluster per instance baseline of the SemEval-2010 WSI task ().", "labels": [], "entities": [{"text": "SemEval-2010 WSI task", "start_pos": 131, "end_pos": 152, "type": "TASK", "confidence": 0.49489330252011615}]}, {"text": "\u2022 One sense: labels each instance with the same induced sense, equivalent to the MFS baseline of the SemEval-2010 WSI task.", "labels": [], "entities": [{"text": "SemEval-2010 WSI task", "start_pos": 101, "end_pos": 122, "type": "TASK", "confidence": 0.6104553441206614}]}, {"text": "\u2022 Most Frequent Sense: labels each instance with the sense that is most frequently selected by annotators for all target word instances.", "labels": [], "entities": []}, {"text": "\u2022 Senses Avg.Weighted: labels each instance with all senses.", "labels": [], "entities": [{"text": "Avg.Weighted", "start_pos": 9, "end_pos": 21, "type": "METRIC", "confidence": 0.8581924438476562}]}, {"text": "Each sense is scored according to its average applicability rating from the gold standard labelling.", "labels": [], "entities": []}, {"text": "Weighted: labels each instance with all senses, equally weighted.", "labels": [], "entities": []}, {"text": "\u2022 1 of 2 random senses: labels each instance with one of two randomly selected induced senses.", "labels": [], "entities": []}, {"text": "\u2022 1 of 3 random senses: labels each instance with one of three randomly selected induced senses.", "labels": [], "entities": []}, {"text": "\u2022 1 of n random senses: labels each instance with one of n randomly selected induced senses, where n is the number of senses for the target word in WordNet 3.1. 10 As noted by the task's organisers 11 , the SemCor scores are the fairest baselines for participating systems to compare against as they have no knowledge of the test set sense distribution; the other baselines are more challenging as they have knowledge of the test set sense distribution and annotator grading.", "labels": [], "entities": [{"text": "WordNet 3.1.", "start_pos": 148, "end_pos": 160, "type": "DATASET", "confidence": 0.9295045137405396}]}, {"text": "Given the number of evaluation metrics (16 in total on the task website), individual analysis of system results per metric is beyond the scope of this paper.", "labels": [], "entities": []}, {"text": "However, a ranking of systems maybe obtained by taking a summed ranked score; that is, by adding  one would expect a system that performs well in the main set of results (all instances), as UoS (top 3) does, to do so in at least one of the single-sense / multi-sense splits: this is clearly not the case.", "labels": [], "entities": []}, {"text": "Indeed, the results suggest that UoS (# WN senses), found to perform poorly overall instances, is better suited to the task's aim of finding graded senses.", "labels": [], "entities": [{"text": "UoS", "start_pos": 33, "end_pos": 36, "type": "METRIC", "confidence": 0.9889039397239685}]}], "tableCaptions": [{"text": " Table 1: Results for the WSD evaluation: all instances.", "labels": [], "entities": [{"text": "WSD", "start_pos": 26, "end_pos": 29, "type": "TASK", "confidence": 0.9619743824005127}]}, {"text": " Table 2: Results for the cluster-based evaluation: all instances.", "labels": [], "entities": []}]}