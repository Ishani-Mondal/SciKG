{"title": [{"text": "Montague Meets Markov: Deep Semantics with Probabilistic Logical Form", "labels": [], "entities": []}], "abstractContent": [{"text": "We combine logical and distributional representations of natural language meaning by transforming distributional similarity judgments into weighted inference rules using Markov Logic Networks (MLNs).", "labels": [], "entities": []}, {"text": "We show that this framework supports both judging sentence similarity and recognizing tex-tual entailment by appropriately adapting the MLN implementation of logical connectives.", "labels": [], "entities": [{"text": "judging sentence similarity", "start_pos": 42, "end_pos": 69, "type": "TASK", "confidence": 0.8289523124694824}]}, {"text": "We also show that distributional phrase similarity , used as textual inference rules created on the fly, improves its performance.", "labels": [], "entities": [{"text": "distributional phrase similarity", "start_pos": 18, "end_pos": 50, "type": "TASK", "confidence": 0.645119289557139}]}], "introductionContent": [{"text": "Tasks in natural language semantics are very diverse and pose different requirements on the underlying formalism for representing meaning.", "labels": [], "entities": []}, {"text": "Some tasks require a detailed representation of the structure of complex sentences.", "labels": [], "entities": []}, {"text": "Some tasks require the ability to recognize near-paraphrases or degrees of similarity between sentences.", "labels": [], "entities": []}, {"text": "Some tasks require logical inference, either exact or approximate.", "labels": [], "entities": []}, {"text": "Often it is necessary to handle ambiguity and vagueness in meaning.", "labels": [], "entities": []}, {"text": "Finally, we frequently want to be able to learn relevant knowledge automatically from corpus data.", "labels": [], "entities": []}, {"text": "There is no single representation for natural language meaning at this time that fulfills all requirements.", "labels": [], "entities": []}, {"text": "But there are representations that meet some of the criteria.", "labels": [], "entities": []}, {"text": "Logic-based representations) provide an expressive and flexible formalism to express even complex propositions, and they come with standardized inference mechanisms.", "labels": [], "entities": []}, {"text": "Distributional mod- Figure 1: Turning distributional similarity into a weighted inference rule els) use contextual similarity to predict semantic similarity of words and phrases), and to model polysemy.", "labels": [], "entities": []}, {"text": "This suggests that distributional models and logicbased representations of natural language meaning are complementary in their strengths (), which encourages developing new techniques to combine them.", "labels": [], "entities": []}, {"text": "propose a framework for combining logic and distributional models in which logical form is the primary meaning representation.", "labels": [], "entities": []}, {"text": "Distributional similarity between pairs of words is converted into weighted inference rules that are added to the logical form, as illustrated in.", "labels": [], "entities": []}, {"text": "Finally, Markov Logic Networks () (MLNs) are used to perform weighted inference on the resulting knowledge base.", "labels": [], "entities": []}, {"text": "However, they only employed single-word distributional similarity rules, and only evaluated on a small set of short, hand-crafted test sentences.", "labels": [], "entities": []}, {"text": "In this paper, we extend Garrette et al.'s approach and adapt it to handle two existing semantic tasks: recognizing textual entailment (RTE) and semantic textual similarity (STS).", "labels": [], "entities": [{"text": "recognizing textual entailment (RTE)", "start_pos": 104, "end_pos": 140, "type": "TASK", "confidence": 0.7606091052293777}, {"text": "semantic textual similarity (STS)", "start_pos": 145, "end_pos": 178, "type": "TASK", "confidence": 0.5988227923711141}]}, {"text": "We show how this single semantic framework using probabilistic logical form in Markov logic can be adapted to support both of these important tasks.", "labels": [], "entities": []}, {"text": "This is possible because MLNs constitute a flexible programming language based on probabilistic logic) that can be easily adapted to support multiple types of linguistically useful inference.", "labels": [], "entities": []}, {"text": "At the word and short phrase level, our approach model entailment through \"distributional\" similarity (.", "labels": [], "entities": []}, {"text": "If X and Y occur in similar contexts, we assume that they describe similar entities and thus there is some degree of entailment between them.", "labels": [], "entities": []}, {"text": "At the sentence level, however, we hold that a stricter, logic-based view of entailment is beneficial, and we even model sentence similarity (in STS) as entailment.", "labels": [], "entities": []}, {"text": "There are two main innovations in the formalism that make it possible for us to work with naturally occurring corpus data.", "labels": [], "entities": []}, {"text": "First, we use more expressive distributional inference rules based on the similarity of phrases rather than just individual words.", "labels": [], "entities": []}, {"text": "In comparison to existing methods for creating textual inference rules (, these rules are computed on the fly as needed, rather than pre-compiled.", "labels": [], "entities": []}, {"text": "Second, we use more flexible probabilistic combinations of evidence in order to compute degrees of sentence similarity for STS and to help compensate for parser errors.", "labels": [], "entities": []}, {"text": "We replace deterministic conjunction by an average combiner, which encodes causal independence (.", "labels": [], "entities": []}, {"text": "We show that our framework is able to handle both sentence similarity (STS) and textual entailment (RTE) by making some simple adaptations to the MLN when switching between tasks.", "labels": [], "entities": [{"text": "textual entailment (RTE)", "start_pos": 80, "end_pos": 104, "type": "TASK", "confidence": 0.6217521965503693}]}, {"text": "The framework achieves reasonable results on both tasks.", "labels": [], "entities": []}, {"text": "On STS, we obtain a correlation of r = 0.66 with full logic, r = 0.73 in a system with weakened variable binding, and r = 0.85 in an ensemble model.", "labels": [], "entities": [{"text": "STS", "start_pos": 3, "end_pos": 6, "type": "TASK", "confidence": 0.9187865853309631}]}, {"text": "On RTE-1 we obtain an accuracy of 0.57.", "labels": [], "entities": [{"text": "RTE-1", "start_pos": 3, "end_pos": 8, "type": "DATASET", "confidence": 0.7974766492843628}, {"text": "accuracy", "start_pos": 22, "end_pos": 30, "type": "METRIC", "confidence": 0.9997491240501404}]}, {"text": "We show that the distributional inference rules benefit both tasks and that more flexible probabilistic combinations of evidence are crucial for STS.", "labels": [], "entities": [{"text": "STS", "start_pos": 145, "end_pos": 148, "type": "TASK", "confidence": 0.9680404663085938}]}, {"text": "Although other approaches could be adapted to handle both RTE and STS, we do not know of any other methods that have been explicitly tested on both problems.", "labels": [], "entities": [{"text": "RTE", "start_pos": 58, "end_pos": 61, "type": "TASK", "confidence": 0.892650842666626}, {"text": "STS", "start_pos": 66, "end_pos": 69, "type": "TASK", "confidence": 0.5472985506057739}]}], "datasetContent": [{"text": "The dataset we use in our experiments is the MSR Video Paraphrase Corpus (MSR-Vid) subset of the STS 2012 task, consisting of 1,500 sentence pairs.", "labels": [], "entities": [{"text": "MSR Video Paraphrase Corpus (MSR-Vid) subset of the STS 2012 task", "start_pos": 45, "end_pos": 110, "type": "DATASET", "confidence": 0.6055760956727542}]}, {"text": "The corpus itself was built by asking annotators from Amazon Mechanical Turk to describe very short video fragments ().", "labels": [], "entities": [{"text": "Amazon Mechanical Turk", "start_pos": 54, "end_pos": 76, "type": "DATASET", "confidence": 0.9602792064348856}]}, {"text": "The organizers of the STS 2012 task () sampled video descriptions and asked Turkers to assign similarity scores (ranging from 0 to 5) to pairs of sentences, without access to the video.", "labels": [], "entities": [{"text": "STS 2012 task", "start_pos": 22, "end_pos": 35, "type": "TASK", "confidence": 0.5541837215423584}]}, {"text": "The gold standard score is the average of the Turkers' annotations.", "labels": [], "entities": [{"text": "gold standard score", "start_pos": 4, "end_pos": 23, "type": "METRIC", "confidence": 0.7251308063666025}, {"text": "Turkers'", "start_pos": 46, "end_pos": 54, "type": "DATASET", "confidence": 0.9111225605010986}]}, {"text": "In addition to the MSR Video Paraphrase Corpus subset, the STS 2012 task involved data from machine translation and sense descriptions.", "labels": [], "entities": [{"text": "MSR Video Paraphrase Corpus", "start_pos": 19, "end_pos": 46, "type": "TASK", "confidence": 0.7875973731279373}, {"text": "machine translation", "start_pos": 92, "end_pos": 111, "type": "TASK", "confidence": 0.6776507347822189}]}, {"text": "We do not use these because they do not consist of full grammatical sentences, which the parser does not handle well.", "labels": [], "entities": []}, {"text": "In addition, the STS 2012 data included sentences from the MSR Paraphrase Corpus, which we also do not currently use because some sentences are long and create intractable MLN inference problems.", "labels": [], "entities": [{"text": "STS 2012 data", "start_pos": 17, "end_pos": 30, "type": "DATASET", "confidence": 0.829074760278066}, {"text": "MSR Paraphrase Corpus", "start_pos": 59, "end_pos": 80, "type": "DATASET", "confidence": 0.92656542857488}]}, {"text": "This issue is discussed further in section 6.", "labels": [], "entities": []}, {"text": "Following STS standards, our evaluation compares a system's similarity judgments to the gold standard scores using Pearson's correlation coefficient r.", "labels": [], "entities": [{"text": "Pearson's correlation coefficient r", "start_pos": 115, "end_pos": 150, "type": "METRIC", "confidence": 0.7590864837169647}]}], "tableCaptions": [{"text": " Table 1: Results on the RTE-1 Test Set.", "labels": [], "entities": [{"text": "RTE-1 Test Set", "start_pos": 25, "end_pos": 39, "type": "DATASET", "confidence": 0.9340043862660726}]}, {"text": " Table 2: Results on the STS video dataset.", "labels": [], "entities": [{"text": "STS video dataset", "start_pos": 25, "end_pos": 42, "type": "DATASET", "confidence": 0.9143937428792318}]}]}