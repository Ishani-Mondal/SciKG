{"title": [{"text": "sielers : Feature Analysis and Polarity Classification of Expressions from Twitter and SMS Data", "labels": [], "entities": []}], "abstractContent": [{"text": "In this paper, we describe our system for the SemEval-2013 Task 2, Sentiment Analysis in Twitter.", "labels": [], "entities": [{"text": "SemEval-2013 Task 2", "start_pos": 46, "end_pos": 65, "type": "TASK", "confidence": 0.8304109970728556}, {"text": "Sentiment Analysis in Twitter", "start_pos": 67, "end_pos": 96, "type": "TASK", "confidence": 0.8594507724046707}]}, {"text": "We formed features that take into account the context of the expression and take a supervised approach towards subjectivity and polarity classification.", "labels": [], "entities": [{"text": "polarity classification", "start_pos": 128, "end_pos": 151, "type": "TASK", "confidence": 0.7708333134651184}]}, {"text": "Experiments were performed on the features to find out whether they were more suited for subjectivity or polarity Classification.", "labels": [], "entities": [{"text": "polarity Classification", "start_pos": 105, "end_pos": 128, "type": "TASK", "confidence": 0.7476731240749359}]}, {"text": "We tested our model for sentiment polarity classification on Twitter as well as SMS chat expressions, analyzed their F-measure scores and drew some interesting conclusions from them.", "labels": [], "entities": [{"text": "sentiment polarity classification", "start_pos": 24, "end_pos": 57, "type": "TASK", "confidence": 0.9249813755353292}, {"text": "F-measure", "start_pos": 117, "end_pos": 126, "type": "METRIC", "confidence": 0.9857866764068604}]}], "introductionContent": [{"text": "In recent years there has been a huge growth in popularity of vaious social media microblogging platforms like Twitter.", "labels": [], "entities": []}, {"text": "Users freely share their personal opinions on various events and entities on these platforms.", "labels": [], "entities": []}, {"text": "However, while character constraints make sure the opinions are short and to the point, they also contribute to the noisy nature of Twitter data.", "labels": [], "entities": []}, {"text": "The contextual polarity of the phrase in which a particular instance of a word appears maybe quite different from the word's prior polarity.", "labels": [], "entities": []}, {"text": "Positive words are used in phrases expressing negative sentiments, or vice versa.", "labels": [], "entities": []}, {"text": "Also, quite often words that are positive or negative out of context are neutral in context, meaning they are not even being used to express a sentiment.", "labels": [], "entities": []}, {"text": "This is evident from the example of underlined phrase in the following tweet: Lana Del Rey at Hammersmith Apollo in May..", "labels": [], "entities": [{"text": "Hammersmith Apollo in May.", "start_pos": 94, "end_pos": 120, "type": "DATASET", "confidence": 0.9044670015573502}]}], "datasetContent": [{"text": "Our goal for these experiments is two-fold.", "labels": [], "entities": []}, {"text": "First, we want to evaluate the effectiveness of our features when using them for subjectivity classification as compared to sentiment polarity classification.", "labels": [], "entities": [{"text": "subjectivity classification", "start_pos": 81, "end_pos": 108, "type": "TASK", "confidence": 0.7178606986999512}, {"text": "sentiment polarity classification", "start_pos": 124, "end_pos": 157, "type": "TASK", "confidence": 0.8086313406626383}]}, {"text": "Second, we want to evaluate and compare the performance of our learnt model when tested upon Twitter and SMS expression data.", "labels": [], "entities": []}, {"text": "We use Naive Bayes classifier in Weka) as the learning algorithm.", "labels": [], "entities": [{"text": "Weka", "start_pos": 33, "end_pos": 37, "type": "DATASET", "confidence": 0.948302686214447}]}, {"text": "Feature Analysis between Subjectivity and Polarity Classification For our first set of experiments, we re-label all positive, negative and neutral expressions as subjective for subjectivity classification in the training dataset.", "labels": [], "entities": [{"text": "subjectivity classification", "start_pos": 177, "end_pos": 204, "type": "TASK", "confidence": 0.7145980894565582}]}, {"text": "For polarity classification we remove all objective expressions from the training dataset and perform 3-way classification between positive, negative and neutral expressions.", "labels": [], "entities": [{"text": "polarity classification", "start_pos": 4, "end_pos": 27, "type": "TASK", "confidence": 0.8116878271102905}]}, {"text": "In both cases we perform 10-fold cross validation on the training dataset.", "labels": [], "entities": [{"text": "training dataset", "start_pos": 57, "end_pos": 73, "type": "DATASET", "confidence": 0.6810719668865204}]}, {"text": "For subjectivity classification we have 24939 tweet expressions with 15565 objective and 9374 subjective expressions.", "labels": [], "entities": [{"text": "subjectivity classification", "start_pos": 4, "end_pos": 31, "type": "TASK", "confidence": 0.8018398284912109}]}, {"text": "Subjective expressions contain 5787 positive, 3131 negative and 456 neutral expressions.", "labels": [], "entities": []}, {"text": "shows the accuracy of subjectivity and sentiment polarity classification results and improvement due to each feature.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9994687438011169}, {"text": "sentiment polarity classification", "start_pos": 39, "end_pos": 72, "type": "TASK", "confidence": 0.8203453222910563}]}, {"text": "It is fairly evident from that phrase prior polarity features are equally important for both subjectivity and sentiment polarity classification.", "labels": [], "entities": [{"text": "sentiment polarity classification", "start_pos": 110, "end_pos": 143, "type": "TASK", "confidence": 0.8300855755805969}]}, {"text": "The same however, doesn't completely hold true for the other two feature types.", "labels": [], "entities": []}, {"text": "While POS Tag pattern features provide an improvement of 1.89% in subjectivity classification accuracy, they only provide a 0.64% increase inaccuracy in polarity classification.", "labels": [], "entities": [{"text": "subjectivity classification", "start_pos": 66, "end_pos": 93, "type": "TASK", "confidence": 0.6896017491817474}, {"text": "accuracy", "start_pos": 94, "end_pos": 102, "type": "METRIC", "confidence": 0.9660911560058594}, {"text": "polarity classification", "start_pos": 153, "end_pos": 176, "type": "TASK", "confidence": 0.7257475852966309}]}, {"text": "Many inferences can be drawn from this result and a deeper analysis is required on POS tag patterns to prove that this wasn't a mere aberration.", "labels": [], "entities": []}, {"text": "Emoticon and interjection feature too give lower improvement in accuracies during sentiment polarity classification (0.44%) as compared to subjectivity classification (0.83%).", "labels": [], "entities": [{"text": "accuracies", "start_pos": 64, "end_pos": 74, "type": "METRIC", "confidence": 0.9976758360862732}, {"text": "sentiment polarity classification", "start_pos": 82, "end_pos": 115, "type": "TASK", "confidence": 0.8515099287033081}]}, {"text": "This, however, is expected since most common emoticons and interjections with prior polarities are already covered in the total score of the expression.", "labels": [], "entities": []}, {"text": "Thus, the noisy data based binary features have significant contribution only when the emoticons and interjections aren't present in the lexicon.", "labels": [], "entities": []}, {"text": "This implies that these binary features only: Accuracies for all three features used for Subjectivity and Sentiment Polarity Classification.", "labels": [], "entities": [{"text": "Accuracies", "start_pos": 46, "end_pos": 56, "type": "METRIC", "confidence": 0.9981538653373718}, {"text": "Sentiment Polarity Classification", "start_pos": 106, "end_pos": 139, "type": "TASK", "confidence": 0.762053112188975}]}, {"text": "hint towards the expression being subjective.", "labels": [], "entities": []}, {"text": "The context features, i.e., phrase prior polarity and POS tag pattern features defined for 2 words before and after the expression also carry more significance during subjectivity classification than in sentiment polarity classification.", "labels": [], "entities": [{"text": "subjectivity classification", "start_pos": 167, "end_pos": 194, "type": "TASK", "confidence": 0.6672819256782532}, {"text": "sentiment polarity classification", "start_pos": 203, "end_pos": 236, "type": "TASK", "confidence": 0.7647613286972046}]}, {"text": "Polarity Classification comparison for Twitter and SMS expression data For the second set of experiments comparing the performance of polarity classification in Twitter expressions and SMS expressions, we use the polarity classification model learnt in the above experiment.", "labels": [], "entities": [{"text": "Polarity Classification comparison", "start_pos": 0, "end_pos": 34, "type": "TASK", "confidence": 0.7748499711354574}, {"text": "polarity classification", "start_pos": 134, "end_pos": 157, "type": "TASK", "confidence": 0.7911742627620697}]}, {"text": "Tables 2(a) and 2(b) shows the precision, recall and F-measure scores for both Twitter and SMS expressions.", "labels": [], "entities": [{"text": "precision", "start_pos": 31, "end_pos": 40, "type": "METRIC", "confidence": 0.9995991587638855}, {"text": "recall", "start_pos": 42, "end_pos": 48, "type": "METRIC", "confidence": 0.998430073261261}, {"text": "F-measure", "start_pos": 53, "end_pos": 62, "type": "METRIC", "confidence": 0.998284637928009}]}, {"text": "The polarity classification accuracies for Twitter and SMS expressions are 74.76% and 70.82%, respectively.", "labels": [], "entities": []}, {"text": "Closer inspection of test data shows that SMS expressions exhibit more aggressive usage of abbreviations and slangs and are in general noisier than Twitter expressions.", "labels": [], "entities": []}, {"text": "This is probably due to the fact that typing on a cellphone is more cumbersome than on a keyboard.", "labels": [], "entities": []}, {"text": "The quantitative distribution of positive, negative and neutral classes in both datasets affects the F-measure scores of individual classes.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 101, "end_pos": 110, "type": "METRIC", "confidence": 0.9766613841056824}]}, {"text": "This is evident from the difference in positive and negative F-measures of Twitter and SMS expressions data.", "labels": [], "entities": [{"text": "F-measures", "start_pos": 61, "end_pos": 71, "type": "METRIC", "confidence": 0.9909160733222961}]}, {"text": "In both datasets, neutral class F-measure is extremely low.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 32, "end_pos": 41, "type": "METRIC", "confidence": 0.7964335680007935}]}, {"text": "This is partially expected due to the low quantity of neutral class expressions in Twitter   seems that more fine-grained analysis of neutral expressions is required for better polarity classification accuracy.", "labels": [], "entities": [{"text": "polarity classification", "start_pos": 177, "end_pos": 200, "type": "TASK", "confidence": 0.6740267127752304}, {"text": "accuracy", "start_pos": 201, "end_pos": 209, "type": "METRIC", "confidence": 0.8641861081123352}]}, {"text": "Our method ranks 16th (F-measure: 0.7441) out of 28 participating systems for Twitter data and 12th (F-measure: 0.7348) out of 26 participating systems for SMS data.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 23, "end_pos": 32, "type": "METRIC", "confidence": 0.9947689771652222}, {"text": "F-measure", "start_pos": 101, "end_pos": 110, "type": "METRIC", "confidence": 0.9736358523368835}]}, {"text": "The best performing system have 0.8893(NRC-Canada) and 0.8837(GUMLTLT) averaged(positive, negative) F-measure score for Twitter and SMS data, respectively.", "labels": [], "entities": [{"text": "NRC-Canada", "start_pos": 39, "end_pos": 49, "type": "DATASET", "confidence": 0.8962039351463318}, {"text": "GUMLTLT) averaged(positive, negative) F-measure score", "start_pos": 62, "end_pos": 115, "type": "METRIC", "confidence": 0.8676094353199005}]}], "tableCaptions": [{"text": " Table 1: Accuracies for all three features used for  Subjectivity and Sentiment Polarity Classification.", "labels": [], "entities": [{"text": "Accuracies", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9989444613456726}, {"text": "Sentiment Polarity Classification", "start_pos": 71, "end_pos": 104, "type": "TASK", "confidence": 0.7530753215154012}]}, {"text": " Table 2: Precision, Recall and F-measure scores for  positive, negative and neutral classes computed on  Twitter and SMS expressions data.", "labels": [], "entities": [{"text": "Precision", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.999152421951294}, {"text": "Recall", "start_pos": 21, "end_pos": 27, "type": "METRIC", "confidence": 0.9921687245368958}, {"text": "F-measure", "start_pos": 32, "end_pos": 41, "type": "METRIC", "confidence": 0.9985793828964233}]}]}