{"title": [{"text": "LIMSIILES: Basic English Substitution for Student Answer Assessment at SemEval 2013", "labels": [], "entities": [{"text": "Student Answer Assessment at SemEval 2013", "start_pos": 42, "end_pos": 83, "type": "TASK", "confidence": 0.7691467305024465}]}], "abstractContent": [{"text": "In this paper, we describe a method for assessing student answers, modeled as a paraphrase identification problem, based on substitution by Basic English variants.", "labels": [], "entities": [{"text": "paraphrase identification", "start_pos": 80, "end_pos": 105, "type": "TASK", "confidence": 0.7325802743434906}]}, {"text": "Basic En-glish paraphrases are acquired from the Simple English Wiktionary.", "labels": [], "entities": [{"text": "Simple English Wiktionary", "start_pos": 49, "end_pos": 74, "type": "DATASET", "confidence": 0.7448321183522543}]}, {"text": "Substitutions are applied both on reference answers and student answers in order to reduce the diversity of their vocabulary and map them to a common vocabulary.", "labels": [], "entities": []}, {"text": "The evaluation of our approach on the SemEval 2013 Joint Student Response Analysis and 8th Recognizing Textual Entail-ment Challenge data shows promising results, and this work is a first step toward an open-domain system able to exhibit deep text understanding capabilities.", "labels": [], "entities": [{"text": "SemEval 2013 Joint Student Response Analysis and 8th Recognizing Textual Entail-ment Challenge data", "start_pos": 38, "end_pos": 137, "type": "DATASET", "confidence": 0.5713310585572169}]}], "introductionContent": [{"text": "Automatically assessing student answers is a challenging natural language processing task (NLP).", "labels": [], "entities": [{"text": "natural language processing task (NLP)", "start_pos": 57, "end_pos": 95, "type": "TASK", "confidence": 0.7982333302497864}]}, {"text": "It is away to make test grading easier and improve adaptive tutoring (, and is the goal of the SemEval 2013's task 7, titled Joint Student Response Analysis.", "labels": [], "entities": [{"text": "SemEval 2013's task 7", "start_pos": 95, "end_pos": 116, "type": "TASK", "confidence": 0.6451266646385193}, {"text": "Joint Student Response Analysis", "start_pos": 125, "end_pos": 156, "type": "TASK", "confidence": 0.6046842709183693}]}, {"text": "More specifically, given a question, a known correct \"reference answer\" and a 1-or 2-sentence student answer, the goal is to determine the student's answer accuracy (.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 156, "end_pos": 164, "type": "METRIC", "confidence": 0.9335351586341858}]}, {"text": "This can be seen as a paraphrase identification problem between student answers and reference answers.", "labels": [], "entities": [{"text": "paraphrase identification", "start_pos": 22, "end_pos": 47, "type": "TASK", "confidence": 0.7713843584060669}]}, {"text": "Paraphrase identification searches whether two sentences have essentially the same meaning.", "labels": [], "entities": [{"text": "Paraphrase identification", "start_pos": 0, "end_pos": 25, "type": "TASK", "confidence": 0.9207944273948669}]}, {"text": "Automatically generating or extracting semantic equivalences for the various units of language -words, phrases, and sentences -is an important problem in NLP and is being increasingly employed to improve the performance of several NLP applications, like question-answering and machine translation.", "labels": [], "entities": [{"text": "Automatically generating or extracting semantic equivalences", "start_pos": 0, "end_pos": 60, "type": "TASK", "confidence": 0.6883412649234136}, {"text": "machine translation", "start_pos": 277, "end_pos": 296, "type": "TASK", "confidence": 0.8012098371982574}]}, {"text": "Paraphrase identification would benefit from a precise and broad-coverage semantic language model.", "labels": [], "entities": [{"text": "Paraphrase identification", "start_pos": 0, "end_pos": 25, "type": "TASK", "confidence": 0.9607408344745636}]}, {"text": "This is unfortunately difficult to obtain to its full extent for any natural language, due to the size of atypical lexicon and the complexity of grammatical constructions.", "labels": [], "entities": []}, {"text": "Our hypothesis is that the simpler the language lexicon is, the easier it will be to access and compare meaning of sentences.", "labels": [], "entities": []}, {"text": "This assumption is justified by the multiple attempts at controlled natural languages and especially simplified forms of English.", "labels": [], "entities": []}, {"text": "One of them, Basic English, has been adopted by the Wikipedia Project as the preferred language of the Simple English Wikipedia 1 and its sister project the Simple English Wiktionary 2 . Our method starts with acquiring paraphrases from the Simple English Wiktionary's definitions.", "labels": [], "entities": [{"text": "Simple English Wikipedia 1", "start_pos": 103, "end_pos": 129, "type": "DATASET", "confidence": 0.6995181292295456}]}, {"text": "Using those, we generate variants of both sentences whose meanings are to be compared.", "labels": [], "entities": []}, {"text": "Finally, we compute traditional lexical and semantic similarity measures on those two sets of variants to produce features to train a classifier on the SemEval 2013 datasets in order to take the final decision.", "labels": [], "entities": [{"text": "SemEval 2013 datasets", "start_pos": 152, "end_pos": 173, "type": "DATASET", "confidence": 0.8622776667277018}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 3: SemEval 2013 evaluation results.", "labels": [], "entities": [{"text": "SemEval 2013 evaluation", "start_pos": 10, "end_pos": 33, "type": "TASK", "confidence": 0.7138770421346029}]}]}