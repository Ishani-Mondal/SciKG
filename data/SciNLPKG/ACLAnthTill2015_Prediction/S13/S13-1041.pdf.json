{"title": [{"text": "Using the text to evaluate short answers for reading comprehension exercises", "labels": [], "entities": []}], "abstractContent": [{"text": "Short answer questions for reading comprehension area common task in foreign language learning.", "labels": [], "entities": []}, {"text": "Automatic short answer scoring is the task of automatically assessing the semantic content of a student's answer, marking it e.g. as corrector incorrect.", "labels": [], "entities": [{"text": "short answer scoring", "start_pos": 10, "end_pos": 30, "type": "TASK", "confidence": 0.5749961038430532}]}, {"text": "While previous approaches mainly focused on comparing a learner answer to some reference answer provided by the teacher, we explore the use of the underlying reading texts as additional evidence for the classification.", "labels": [], "entities": []}, {"text": "First, we conduct a corpus study targeting the links between sentences in reading texts for learners of German and answers to reading comprehension questions based on those texts.", "labels": [], "entities": []}, {"text": "Second, we use the reading text directly for classification , considering three different models: an answer-based classifier extended with textual features, a simple text-based classifier, and a model that combines the two according to confidence of the text-based classification.", "labels": [], "entities": []}, {"text": "The most promising approach is the first one, results for which show that textual features improve classification accuracy.", "labels": [], "entities": [{"text": "classification", "start_pos": 99, "end_pos": 113, "type": "TASK", "confidence": 0.9699589014053345}, {"text": "accuracy", "start_pos": 114, "end_pos": 122, "type": "METRIC", "confidence": 0.8915433287620544}]}, {"text": "While the other two models do not improve classification accuracy , they do investigate the role of the text and suggest possibilities for developing automatic answer scoring systems with less supervision needed from instructors.", "labels": [], "entities": [{"text": "classification", "start_pos": 42, "end_pos": 56, "type": "TASK", "confidence": 0.9532504677772522}, {"text": "accuracy", "start_pos": 57, "end_pos": 65, "type": "METRIC", "confidence": 0.871084451675415}]}], "introductionContent": [{"text": "Reading comprehension exercises area common means of assessment for language teaching: students read a text in the language they are learning and are then asked to answer questions about the text.", "labels": [], "entities": []}, {"text": "The types of questions asked of the learner may vary in their scope and in the type of answers they are designed to elicit; in this work we focus on \"short answer\" responses, which are generally in the range of 1-3 sentences.", "labels": [], "entities": []}, {"text": "The nature of the reading comprehension task is that the student is asked to show that he or she has understood the text at hand.", "labels": [], "entities": []}, {"text": "Questions focus on one or more pieces of information from the text, and correct responses should contain the relevant semantic content.", "labels": [], "entities": []}, {"text": "In the language learning context, responses classified as correct might still contain grammatical or spelling errors; the focus lies on the content rather than the form of the learner answer.", "labels": [], "entities": []}, {"text": "Automatic scoring of short answer responses to reading comprehension questions is in essence a textual entailment task, with the additional complication that, in order to answer a question correctly, the learner must have identified the right portion of the text.", "labels": [], "entities": [{"text": "Automatic scoring of short answer responses to reading comprehension questions", "start_pos": 0, "end_pos": 78, "type": "TASK", "confidence": 0.7882253289222717}]}, {"text": "It isn't enough that a student answer is entailed by some part of the reading text; it must be entailed by the part of the text which is responsive to the question under discussion.", "labels": [], "entities": []}, {"text": "Previous approaches to automatic short answer scoring have seldom considered the reading text itself, instead comparing student answers to target answers supplied by instructors; we will refer to these as answer-based models.", "labels": [], "entities": [{"text": "automatic short answer scoring", "start_pos": 23, "end_pos": 53, "type": "TASK", "confidence": 0.5996755808591843}]}, {"text": "In this paper we explore the role of the text for short answer scoring, evaluating several models for considering the text in automatic scoring, and presenting results of an annotation study regarding the semantic links between reading texts and answers to reading comprehension questions.", "labels": [], "entities": [{"text": "short answer scoring", "start_pos": 50, "end_pos": 70, "type": "TASK", "confidence": 0.6402756969134012}]}], "datasetContent": [{"text": "This section discusses experiments on short answer scoring (binary classification) for German, in the context of reading comprehension for language learning.", "labels": [], "entities": [{"text": "short answer scoring (binary classification)", "start_pos": 38, "end_pos": 82, "type": "TASK", "confidence": 0.6775209265095847}]}, {"text": "Specifically, we investigate the text-based models described in Section 4.2.", "labels": [], "entities": []}, {"text": "In all cases, features and parameter settings were tuned on a development set which was extracted from the larger CREG corpus.", "labels": [], "entities": [{"text": "CREG corpus", "start_pos": 114, "end_pos": 125, "type": "DATASET", "confidence": 0.8756398558616638}]}, {"text": "In other words, there is no overlap between test and development data.", "labels": [], "entities": []}, {"text": "For testing, we perform leave-one-out cross-validation on the slightly-smaller subset of the corpus which was used for annotation.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Inter-annotator agreement for linking answers to source sentences in text", "labels": [], "entities": []}, {"text": " Table 2: Classification accuracy for answer-based base- line (baseline), answer-based plus textual features (text),  and classifier combination (combined). +syn indicates  expanded synonymy features, goldlink indicates identi- fying the source sentences via annotated links, autolink  indicates determining source sentences using the align- ment model, k=number of neighbors. Results marked  with * are significant compared to the best baseline  model. See Section 5.2.1 for details.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 25, "end_pos": 33, "type": "METRIC", "confidence": 0.9885464906692505}]}]}