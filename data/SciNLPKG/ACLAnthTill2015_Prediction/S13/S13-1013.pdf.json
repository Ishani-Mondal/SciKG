{"title": [{"text": "HENRY-CORE: Domain Adaptation and Stacking for Text Similarity *", "labels": [], "entities": [{"text": "Text Similarity", "start_pos": 47, "end_pos": 62, "type": "TASK", "confidence": 0.7103858292102814}]}], "abstractContent": [{"text": "This paper describes a system for automatically measuring the semantic similarity between two texts, which was the aim of the 2013 Semantic Textual Similarity (STS) task (Agirre et al., 2013).", "labels": [], "entities": [{"text": "Semantic Textual Similarity (STS) task", "start_pos": 131, "end_pos": 169, "type": "TASK", "confidence": 0.718465873173305}]}, {"text": "For the 2012 STS task, Heilman and Madnani (2012) submitted the PERP system, which performed competitively in relation to other submissions.", "labels": [], "entities": [{"text": "2012 STS task", "start_pos": 8, "end_pos": 21, "type": "TASK", "confidence": 0.4638230800628662}, {"text": "PERP", "start_pos": 64, "end_pos": 68, "type": "METRIC", "confidence": 0.9556458592414856}]}, {"text": "However, approaches including word and n-gram features also performed well (B\u00e4r et al., 2012; \u02c7 Sari\u00b4cSari\u00b4c et al., 2012), and the 2013 STS task fo-cused more on predicting similarity for text pairs from new domains.", "labels": [], "entities": []}, {"text": "Therefore, for the three variations of our system that we were allowed to submit, we used stacking (Wolpert, 1992) to combine PERP with word and n-gram features and applied the domain adaptation approach outlined by Daume III (2007) to facilitate generalization to new domains.", "labels": [], "entities": []}, {"text": "Our submissions performed well at most sub-tasks, particularly at measuring the similarity of news headlines, where one of our submissions ranked 2nd among 89 from 34 teams, but there is still room for improvement.", "labels": [], "entities": []}], "introductionContent": [{"text": "We aim to develop an automatic measure of the semantic similarity between two short texts (e.g., sentences).", "labels": [], "entities": []}, {"text": "Such a measure could be useful for various applications, including automated short answer scoring (, question answering (, * System description papers for this task were required to have a team ID and task ID (e.g., \"HENRY-CORE\") as a prefix. and machine translation evaluation (.", "labels": [], "entities": [{"text": "automated short answer scoring", "start_pos": 67, "end_pos": 97, "type": "TASK", "confidence": 0.5818688720464706}, {"text": "question answering", "start_pos": 101, "end_pos": 119, "type": "TASK", "confidence": 0.8777155876159668}, {"text": "machine translation evaluation", "start_pos": 247, "end_pos": 277, "type": "TASK", "confidence": 0.8661226630210876}]}, {"text": "In this paper, we describe our submissions to the 2013 Semantic Textual Similarity (STS) task, which evaluated implementations of text-to-text similarity measures.", "labels": [], "entities": [{"text": "Semantic Textual Similarity (STS) task", "start_pos": 55, "end_pos": 93, "type": "TASK", "confidence": 0.7537304418427604}]}, {"text": "Submissions were evaluated according to Pearson correlations between gold standard similarity values acquired from human raters and machine-produced similarity values.", "labels": [], "entities": [{"text": "gold standard similarity values acquired", "start_pos": 69, "end_pos": 109, "type": "METRIC", "confidence": 0.6635389506816864}]}, {"text": "Teams were allowed to submit up to three submissions.", "labels": [], "entities": []}, {"text": "For each submission, correlations were calculated separately for four subtasks: measuring similarity between news headlines (\"headlines\"), between machine translation outputs and human reference translations (\"SMT\"), between word glosses from OntoNotes ( and WordNet) (\"OnWN\"), and between frame descriptions from FrameNet () and glosses from WordNet (\"FNWN\").", "labels": [], "entities": [{"text": "WordNet", "start_pos": 343, "end_pos": 350, "type": "DATASET", "confidence": 0.9785958528518677}, {"text": "FNWN", "start_pos": 353, "end_pos": 357, "type": "DATASET", "confidence": 0.5708705186843872}]}, {"text": "A weighted mean of the correlations was also computed as an overall evaluation metric (the OnWn and FNWN datasets were smaller than the headlines and SMT datasets).", "labels": [], "entities": [{"text": "OnWn", "start_pos": 91, "end_pos": 95, "type": "DATASET", "confidence": 0.8942965269088745}, {"text": "FNWN datasets", "start_pos": 100, "end_pos": 113, "type": "DATASET", "confidence": 0.729429617524147}, {"text": "headlines and SMT datasets", "start_pos": 136, "end_pos": 162, "type": "DATASET", "confidence": 0.7439486235380173}]}, {"text": "The suggested training data for the 2013 STS task was the data from the 2012 STS task (Agirre et al., 2012), including both the training and test sets for that year.", "labels": [], "entities": [{"text": "STS task", "start_pos": 41, "end_pos": 49, "type": "TASK", "confidence": 0.7180711925029755}]}, {"text": "The 2012 task was similar except that the data were from a different set of subtasks: measuring similarity between sentences from the Microsoft Research Paraphrase corpus () (\"MSRpar\"), between sentences from the Microsoft Research Video Description corpus (, and between human and machine translations of parliamentary proceedings (\"SMTeuroparl\").", "labels": [], "entities": [{"text": "Microsoft Research Paraphrase corpus", "start_pos": 134, "end_pos": 170, "type": "DATASET", "confidence": 0.674196258187294}, {"text": "Microsoft Research Video Description corpus", "start_pos": 213, "end_pos": 256, "type": "DATASET", "confidence": 0.6933847069740295}]}, {"text": "The 2012 task provided training and test sets for those three subtasks and also included two additional tasks with just test sets: a similar OnWN task, and measuring similarity between human and machine translations of news broadcasts (\"SMTnews\").", "labels": [], "entities": []}, {"text": "Heilman and Madnani (2012) described the PERP system and submitted it to the 2012 STS task.", "labels": [], "entities": [{"text": "2012 STS task", "start_pos": 77, "end_pos": 90, "type": "DATASET", "confidence": 0.7229858636856079}]}, {"text": "PERP measures the similarity of a sentence pair by finding a sequence of edit operations (e.g., insertions, deletions, substitutions, and shifts) that converts one sentence to the other.", "labels": [], "entities": []}, {"text": "It then uses various features of the edits, with weights learned from labeled sentence pairs, to assign a similarity score.", "labels": [], "entities": [{"text": "similarity score", "start_pos": 106, "end_pos": 122, "type": "METRIC", "confidence": 0.9716911017894745}]}, {"text": "PERP performed well, ranking 7th out of 88 submissions from 35 teams according to the weighted mean correlation.", "labels": [], "entities": [{"text": "PERP", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.745095431804657}, {"text": "weighted mean correlation", "start_pos": 86, "end_pos": 111, "type": "METRIC", "confidence": 0.6525544921557108}]}, {"text": "However, PERP lacked some of the useful word and n-gram overlap features included in some of the other top-performing submissions.", "labels": [], "entities": []}, {"text": "In addition, domain adaptation seemed more relevant for the STS 2013 task since in-domain data was available only for one (OnWN) of the four subtasks.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 13, "end_pos": 30, "type": "TASK", "confidence": 0.7491050958633423}, {"text": "STS 2013 task", "start_pos": 60, "end_pos": 73, "type": "TASK", "confidence": 0.761865238348643}]}, {"text": "Therefore, in this work, we combine the PERP system with various word and n-gram features.", "labels": [], "entities": []}, {"text": "We also apply the domain adaptation technique of to support generalization beyond the domains in the training data.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 18, "end_pos": 35, "type": "TASK", "confidence": 0.6920886039733887}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Pearson correlations for STS 2012 data for each subtask and then the weighted mean across subtasks. \"UKP\"  was submitted by", "labels": [], "entities": [{"text": "Pearson correlations", "start_pos": 10, "end_pos": 30, "type": "METRIC", "confidence": 0.9356517493724823}, {"text": "STS 2012 data", "start_pos": 35, "end_pos": 48, "type": "DATASET", "confidence": 0.7135908007621765}, {"text": "UKP\"", "start_pos": 111, "end_pos": 115, "type": "DATASET", "confidence": 0.942670464515686}]}]}