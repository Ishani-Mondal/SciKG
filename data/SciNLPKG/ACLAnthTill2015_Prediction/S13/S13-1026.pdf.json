{"title": [{"text": "KLUE-CORE: A regression model of semantic textual similarity", "labels": [], "entities": [{"text": "semantic textual similarity", "start_pos": 33, "end_pos": 60, "type": "TASK", "confidence": 0.6644514302412668}]}], "abstractContent": [{"text": "This paper describes our system entered for the *SEM 2013 shared task on Semantic Textual Similarity (STS).", "labels": [], "entities": [{"text": "SEM 2013 shared task on Semantic Textual Similarity (STS)", "start_pos": 49, "end_pos": 106, "type": "TASK", "confidence": 0.8024761378765106}]}, {"text": "We focus on the core task of predicting the semantic textual similarity of sentence pairs.", "labels": [], "entities": [{"text": "predicting the semantic textual similarity of sentence pairs", "start_pos": 29, "end_pos": 89, "type": "TASK", "confidence": 0.8271287605166435}]}, {"text": "The current system utilizes machine learning techniques trained on semantic similarity ratings from the *SEM 2012 shared task; it achieved rank 20 out of 90 submissions from 35 different teams.", "labels": [], "entities": [{"text": "SEM 2012 shared task", "start_pos": 105, "end_pos": 125, "type": "TASK", "confidence": 0.6226709187030792}]}, {"text": "Given the simple nature of our approach, which uses only WordNet and unannotated corpus data as external resources, we consider this a remarkably good result, making the system an interesting tool fora wide range of practical applications.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 57, "end_pos": 64, "type": "DATASET", "confidence": 0.9562378525733948}]}], "introductionContent": [{"text": "The *SEM 2013 shared task on Semantic Textual Similarity () required participants to implement a software system that is able to predict the semantic textual similarity (STS) of sentence pairs.", "labels": [], "entities": [{"text": "SEM 2013 shared task", "start_pos": 5, "end_pos": 25, "type": "TASK", "confidence": 0.762820690870285}, {"text": "Semantic Textual Similarity", "start_pos": 29, "end_pos": 56, "type": "TASK", "confidence": 0.6423602799574534}, {"text": "semantic textual similarity (STS)", "start_pos": 141, "end_pos": 174, "type": "METRIC", "confidence": 0.6944965074459711}]}, {"text": "Being able to reliably measure semantic similarity can be beneficial for many applications, e.g. in the domains of MT evaluation, information extraction, question answering, and summarization.", "labels": [], "entities": [{"text": "MT evaluation", "start_pos": 115, "end_pos": 128, "type": "TASK", "confidence": 0.9896742105484009}, {"text": "information extraction", "start_pos": 130, "end_pos": 152, "type": "TASK", "confidence": 0.8871244788169861}, {"text": "question answering", "start_pos": 154, "end_pos": 172, "type": "TASK", "confidence": 0.9191663563251495}, {"text": "summarization", "start_pos": 178, "end_pos": 191, "type": "TASK", "confidence": 0.992809534072876}]}, {"text": "For the shared task, STS was measured on a scale ranging from 0 (indicating no similarity at all) to 5 (semantic equivalence).", "labels": [], "entities": [{"text": "STS", "start_pos": 21, "end_pos": 24, "type": "METRIC", "confidence": 0.9980039000511169}]}, {"text": "The system predictions were evaluated against manually annotated data.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we describe some post-hoc experiments on the STS 2013 test data, which we performed in order to find out whether we made good decisions regarding the machine learning method, training data, similarity features, and other parameters.", "labels": [], "entities": [{"text": "STS 2013 test data", "start_pos": 62, "end_pos": 80, "type": "DATASET", "confidence": 0.8629831820726395}]}, {"text": "Results of our submitted system are typeset in italics, the best results in each column are typeset in bold font.", "labels": [], "entities": []}], "tableCaptions": []}