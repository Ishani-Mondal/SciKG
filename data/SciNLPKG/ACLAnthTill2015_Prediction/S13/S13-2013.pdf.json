{"title": [], "abstractContent": [{"text": "We analyze the performance of SUTIME, a temporal tagger for recognizing and normalizing temporal expressions, on TempEval-3 Task A for English.", "labels": [], "entities": []}, {"text": "SUTIME is available as part of the Stanford CoreNLP pipeline and can be used to annotate documents with temporal information.", "labels": [], "entities": [{"text": "Stanford CoreNLP pipeline", "start_pos": 35, "end_pos": 60, "type": "DATASET", "confidence": 0.909241795539856}]}, {"text": "Testing on the TempEval-3 evaluation corpus showed that this system is competitive with state-of-the-art techniques.", "labels": [], "entities": [{"text": "TempEval-3 evaluation corpus", "start_pos": 15, "end_pos": 43, "type": "DATASET", "confidence": 0.8074620962142944}]}], "introductionContent": [{"text": "The importance of modeling temporal information is increasingly apparent in natural language applications, such as information extraction and question answering.", "labels": [], "entities": [{"text": "information extraction", "start_pos": 115, "end_pos": 137, "type": "TASK", "confidence": 0.827218621969223}, {"text": "question answering", "start_pos": 142, "end_pos": 160, "type": "TASK", "confidence": 0.9056280553340912}]}, {"text": "Extracting temporal information requires the ability to recognize temporal expressions, and to convert them from text to a normalized form that is easy to process.", "labels": [], "entities": [{"text": "Extracting temporal information", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.8797742327054342}]}, {"text": "Temporal tagging systems are designed to address this problem.", "labels": [], "entities": [{"text": "Temporal tagging", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.8990654647350311}]}, {"text": "In this paper, we evaluate the performance of the SUTIME (Chang and Manning, 2012) rule-based temporal tagging system.", "labels": [], "entities": [{"text": "SUTIME (Chang and Manning, 2012) rule-based temporal tagging", "start_pos": 50, "end_pos": 110, "type": "TASK", "confidence": 0.5785869739272378}]}, {"text": "We evaluate the performance of SUTIME on extracting temporal information in TempEval-3 (Uz-, which requires systems to automatically annotate documents with temporal information using TimeML ().", "labels": [], "entities": []}, {"text": "The TempEval-3 training data contains gold human annotated data from TimeBank, AQUAINT, and anew dataset of silver data automatically annotated using a combination of TipSem ( and TRIOS), two of the best performing systems from TempEval-2 ().", "labels": [], "entities": [{"text": "TempEval-3 training data", "start_pos": 4, "end_pos": 28, "type": "DATASET", "confidence": 0.8732821146647135}, {"text": "TimeBank", "start_pos": 69, "end_pos": 77, "type": "DATASET", "confidence": 0.9769195318222046}, {"text": "AQUAINT", "start_pos": 79, "end_pos": 86, "type": "METRIC", "confidence": 0.6619776487350464}, {"text": "TRIOS", "start_pos": 180, "end_pos": 185, "type": "METRIC", "confidence": 0.8950360417366028}]}], "datasetContent": [{"text": "We evaluated SUTIME's performance on the TempEval-3 Task A for English.", "labels": [], "entities": [{"text": "TempEval-3 Task A", "start_pos": 41, "end_pos": 58, "type": "TASK", "confidence": 0.5802555878957113}]}, {"text": "Task A consists of determining the extent of time expressions as defined by the TimeML TIMEX3 tag, as well as providing normalized attributes for type and value.", "labels": [], "entities": [{"text": "TimeML TIMEX3 tag", "start_pos": 80, "end_pos": 97, "type": "DATASET", "confidence": 0.8400631546974182}]}, {"text": "Extracted temporal expressions from the system and the gold are matched, and precision, recall, and F 1 are computed.", "labels": [], "entities": [{"text": "precision", "start_pos": 77, "end_pos": 86, "type": "METRIC", "confidence": 0.9997476935386658}, {"text": "recall", "start_pos": 88, "end_pos": 94, "type": "METRIC", "confidence": 0.9995806813240051}, {"text": "F 1", "start_pos": 100, "end_pos": 103, "type": "METRIC", "confidence": 0.989484578371048}]}, {"text": "For the evaluation of extents, there are two metrics: a relaxed match score for identifying a matching temporal expression, and a strict match that requires the text to be matched exactly.", "labels": [], "entities": []}, {"text": "For example, identifying the twentieth century when the gold is twentieth centry will give a relaxed match but not a strict match.", "labels": [], "entities": []}, {"text": "For the type and value attributes, an accuracy and a measure of the F 1 with respect to the relaxed match is given.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 38, "end_pos": 46, "type": "METRIC", "confidence": 0.9997709393501282}, {"text": "F 1", "start_pos": 68, "end_pos": 71, "type": "METRIC", "confidence": 0.9912014305591583}]}, {"text": "We compare SUTIME's performance with several other top systems on the English TempEval-3 Task A. We also include TIPSem which was used to create the silver data for TempEval-3 as a baseline.", "labels": [], "entities": [{"text": "English TempEval-3 Task A.", "start_pos": 70, "end_pos": 96, "type": "DATASET", "confidence": 0.878643348813057}, {"text": "TIPSem", "start_pos": 113, "end_pos": 119, "type": "METRIC", "confidence": 0.9659096002578735}]}, {"text": "Of the systems that prepared multiple runs, we selected the best performing run to report.", "labels": [], "entities": []}, {"text": "gives the results for these systems on the TempEval-3 evaluation set.", "labels": [], "entities": [{"text": "TempEval-3 evaluation set", "start_pos": 43, "end_pos": 68, "type": "DATASET", "confidence": 0.889310379823049}]}, {"text": "Interestingly, NavyTime which uses SU-TIME for Task A, actually did better than SUTIME in the value normalization and is effectively the 2nd best system in Task A. The performance of NavyTime is otherwise identical to SUTIME.", "labels": [], "entities": [{"text": "NavyTime", "start_pos": 15, "end_pos": 23, "type": "DATASET", "confidence": 0.943621814250946}, {"text": "value normalization", "start_pos": 94, "end_pos": 113, "type": "TASK", "confidence": 0.6822848916053772}, {"text": "NavyTime", "start_pos": 183, "end_pos": 191, "type": "DATASET", "confidence": 0.8807373046875}]}, {"text": "In NavyTime the normalization was tuned to the TimeBank annotation whereas the SUTIME submission was untuned.", "labels": [], "entities": [{"text": "NavyTime", "start_pos": 3, "end_pos": 11, "type": "DATASET", "confidence": 0.9598389863967896}, {"text": "TimeBank", "start_pos": 47, "end_pos": 55, "type": "DATASET", "confidence": 0.8888257145881653}, {"text": "SUTIME submission", "start_pos": 79, "end_pos": 96, "type": "DATASET", "confidence": 0.5389392971992493}]}, {"text": "SUTIME has the highest recall in discovering temporal expressions.", "labels": [], "entities": [{"text": "recall", "start_pos": 23, "end_pos": 29, "type": "METRIC", "confidence": 0.9995385408401489}]}, {"text": "It also has the highest overall relaxed F 1 , slightly higher than HeidelTime (Str\u00f6tgen and Gertz, 2010) (cleartk had the highest strict F 1 of 82.71).", "labels": [], "entities": [{"text": "relaxed F 1", "start_pos": 32, "end_pos": 43, "type": "METRIC", "confidence": 0.899993360042572}, {"text": "strict F 1", "start_pos": 130, "end_pos": 140, "type": "METRIC", "confidence": 0.784840444723765}]}, {"text": "Not surprisingly, the system used to generate the silver data, TIPSem, had the highest precision when extracting temporal expressions.", "labels": [], "entities": [{"text": "TIPSem", "start_pos": 63, "end_pos": 69, "type": "METRIC", "confidence": 0.5880001783370972}, {"text": "precision", "start_pos": 87, "end_pos": 96, "type": "METRIC", "confidence": 0.9978826642036438}]}, {"text": "For normalization, HeidelTime had the overall best performance on value and type.", "labels": [], "entities": [{"text": "normalization", "start_pos": 4, "end_pos": 17, "type": "TASK", "confidence": 0.9726440906524658}]}, {"text": "Both SUTIME and HeidelTime are rule-based, indicating the effectiveness of using rules for this domain.", "labels": [], "entities": [{"text": "HeidelTime", "start_pos": 16, "end_pos": 26, "type": "DATASET", "confidence": 0.9027786254882812}]}, {"text": "Another top performing system, ManTime used conditional random fields, a machine learning approach, for identifying temporal expressions and rules for normalization.", "labels": [], "entities": [{"text": "ManTime", "start_pos": 31, "end_pos": 38, "type": "DATASET", "confidence": 0.9458504915237427}]}], "tableCaptions": [{"text": " Table 1: TempEval-3; English Platinum Test set.", "labels": [], "entities": [{"text": "TempEval-3", "start_pos": 10, "end_pos": 20, "type": "DATASET", "confidence": 0.7125500440597534}, {"text": "English Platinum Test set", "start_pos": 22, "end_pos": 47, "type": "DATASET", "confidence": 0.9248137474060059}]}, {"text": " Table 2: Summary of errors made by SUTIME on the  platinum data set", "labels": [], "entities": [{"text": "platinum data set", "start_pos": 51, "end_pos": 68, "type": "DATASET", "confidence": 0.9583755532900492}]}, {"text": " Table 3: Break down of value errors made by SUTime on  the platinum data set", "labels": [], "entities": [{"text": "Break", "start_pos": 10, "end_pos": 15, "type": "METRIC", "confidence": 0.8876003623008728}, {"text": "platinum data set", "start_pos": 60, "end_pos": 77, "type": "DATASET", "confidence": 0.9396173556645712}]}]}