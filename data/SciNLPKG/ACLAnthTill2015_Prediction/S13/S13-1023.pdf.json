{"title": [{"text": "LIPN-CORE: Semantic Text Similarity using n-grams, WordNet, Syntactic Analysis, ESA and Information Retrieval based Features", "labels": [], "entities": [{"text": "Semantic Text Similarity", "start_pos": 11, "end_pos": 35, "type": "TASK", "confidence": 0.6601275602976481}, {"text": "WordNet", "start_pos": 51, "end_pos": 58, "type": "DATASET", "confidence": 0.9088419079780579}, {"text": "Syntactic Analysis", "start_pos": 60, "end_pos": 78, "type": "TASK", "confidence": 0.7470390498638153}, {"text": "Information Retrieval", "start_pos": 88, "end_pos": 109, "type": "TASK", "confidence": 0.6799367964267731}]}], "abstractContent": [{"text": "This paper describes the system used by the LIPN team in the Semantic Textual Similarity task at *SEM 2013.", "labels": [], "entities": [{"text": "Semantic Textual Similarity task at *SEM 2013", "start_pos": 61, "end_pos": 106, "type": "TASK", "confidence": 0.8559873402118683}]}, {"text": "It uses a support vector regression model, combining different text similarity measures that constitute the features.", "labels": [], "entities": []}, {"text": "These measures include simple distances like Levenshtein edit distance, cosine, Named Entities overlap and more complex distances like Explicit Semantic Analysis, WordNet-based similarity, IR-based similarity, and a similarity measure based on syntactic dependencies.", "labels": [], "entities": []}], "introductionContent": [{"text": "The Semantic Textual Similarity task (STS) at *SEM 2013 requires systems to grade the degree of similarity between pairs of sentences.", "labels": [], "entities": [{"text": "Semantic Textual Similarity task (STS) at *SEM 2013", "start_pos": 4, "end_pos": 55, "type": "TASK", "confidence": 0.7157933766191656}]}, {"text": "It is closely related to other well known tasks in NLP such as textual entailment, question answering or paraphrase detection.", "labels": [], "entities": [{"text": "textual entailment", "start_pos": 63, "end_pos": 81, "type": "TASK", "confidence": 0.6981801837682724}, {"text": "question answering", "start_pos": 83, "end_pos": 101, "type": "TASK", "confidence": 0.878824919462204}, {"text": "paraphrase detection", "start_pos": 105, "end_pos": 125, "type": "TASK", "confidence": 0.8156437873840332}]}, {"text": "However, as noticed in), the major difference is that STS systems must give a graded, as opposed to binary, answer.", "labels": [], "entities": []}, {"text": "One of the most successful systems in *SEM 2012 STS,, managed to grade pairs of sentences accurately by combining focused measures, either simple ones based on surface features (ie n-grams), more elaborate ones based on lexical semantics, or measures requiring external corpora such as Explicit Semantic Analysis, into a robust measure by using a log-linear regression model.", "labels": [], "entities": [{"text": "*SEM 2012 STS", "start_pos": 38, "end_pos": 51, "type": "DATASET", "confidence": 0.6579774543642998}]}, {"text": "The LIPN-CORE system is built upon this idea of combining simple measures with a regression model to obtain a robust and accurate measure of textual similarity, using the individual measures as features for the global system.", "labels": [], "entities": []}, {"text": "These measures include simple distances like Levenshtein edit distance, cosine, Named Entities overlap and more complex distances like Explicit Semantic Analysis, WordNetbased similarity, IR-based similarity, and a similarity measure based on syntactic dependencies.", "labels": [], "entities": []}, {"text": "The paper is organized as follows.", "labels": [], "entities": []}, {"text": "Measures are presented in Section 2.", "labels": [], "entities": []}, {"text": "Then the regression model, based on Support Vector Machines, is described in Section 3.", "labels": [], "entities": []}, {"text": "Finally we discuss the results of the system in Section 4.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Ablation test for the different features on the  whole 2013 test set.", "labels": [], "entities": [{"text": "Ablation", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9982761144638062}, {"text": "2013 test set", "start_pos": 65, "end_pos": 78, "type": "DATASET", "confidence": 0.9057544271151224}]}, {"text": " Table 2: Ablation test for the different features on the different parts of the 2013 test set.", "labels": [], "entities": [{"text": "Ablation", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9983491897583008}, {"text": "2013 test set", "start_pos": 81, "end_pos": 94, "type": "DATASET", "confidence": 0.8770339488983154}]}, {"text": " Table 3: Pearson correlation calculated on individual features.", "labels": [], "entities": [{"text": "Pearson correlation", "start_pos": 10, "end_pos": 29, "type": "METRIC", "confidence": 0.7669610977172852}]}]}