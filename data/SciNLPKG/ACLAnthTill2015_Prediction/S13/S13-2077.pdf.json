{"title": [{"text": "FBK: Sentiment Analysis in Twitter with Tweetsted", "labels": [], "entities": [{"text": "FBK", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.9014192819595337}, {"text": "Sentiment Analysis", "start_pos": 5, "end_pos": 23, "type": "TASK", "confidence": 0.9524613916873932}]}], "abstractContent": [{"text": "This paper presents the Tweetsted system implemented for the SemEval 2013 task on Sentiment Analysis in Twitter.", "labels": [], "entities": [{"text": "SemEval 2013 task on Sentiment Analysis in Twitter", "start_pos": 61, "end_pos": 111, "type": "TASK", "confidence": 0.7950416728854179}]}, {"text": "In particular, we participated in Task B on Message Polarity Classification in the Constrained setting.", "labels": [], "entities": [{"text": "Message Polarity Classification", "start_pos": 44, "end_pos": 75, "type": "TASK", "confidence": 0.8390385707219442}]}, {"text": "The approach is based on the exploitation of various resources such as SentiWordNet and LIWC.", "labels": [], "entities": [{"text": "LIWC", "start_pos": 88, "end_pos": 92, "type": "DATASET", "confidence": 0.9080782532691956}]}, {"text": "Official results show that our approach yields a F-score of 0.5976 for Twitter messages (11th out of 35) and a F-score of 0.5487 for SMS messages (8th out of 28 participants).", "labels": [], "entities": [{"text": "F-score", "start_pos": 49, "end_pos": 56, "type": "METRIC", "confidence": 0.9996947050094604}, {"text": "F-score", "start_pos": 111, "end_pos": 118, "type": "METRIC", "confidence": 0.9996198415756226}]}], "introductionContent": [{"text": "Microblogging is currently a very popular communication tool where millions of users share opinions on different aspects of life.", "labels": [], "entities": [{"text": "Microblogging", "start_pos": 0, "end_pos": 13, "type": "DATASET", "confidence": 0.8597521781921387}]}, {"text": "For this reason it is a valuable source of data for opinion mining and sentiment analysis.", "labels": [], "entities": [{"text": "opinion mining", "start_pos": 52, "end_pos": 66, "type": "TASK", "confidence": 0.898634284734726}, {"text": "sentiment analysis", "start_pos": 71, "end_pos": 89, "type": "TASK", "confidence": 0.9425015151500702}]}, {"text": "Working with such type of texts presents challenges for NLP beyond those typically encountered when dealing with more traditional texts, such as newswire data.", "labels": [], "entities": []}, {"text": "Tweets are short, the language used is very informal, with creative spelling and punctuation, misspellings, slang, new words, URLs, genrespecific terminology and abbreviations, and #hash-tags.", "labels": [], "entities": []}, {"text": "These characteristics need to be handled with specific approaches.", "labels": [], "entities": []}, {"text": "This paper presents the approach adopted for the SemEval 2013 task on Sentiment Analysis in Twitter, in particular Task B on Message Polarity Classification in the Constrained setting (i.e., using the provided training data only).", "labels": [], "entities": [{"text": "SemEval 2013 task", "start_pos": 49, "end_pos": 66, "type": "TASK", "confidence": 0.8095276753107706}, {"text": "Sentiment Analysis in Twitter", "start_pos": 70, "end_pos": 99, "type": "TASK", "confidence": 0.8270587176084518}, {"text": "Message Polarity Classification", "start_pos": 125, "end_pos": 156, "type": "TASK", "confidence": 0.7254293859004974}]}, {"text": "The goal of Task B on Message Polarity Classification is the following: given a message, decide whether it expresses a positive, negative, or neutral sentiment.", "labels": [], "entities": [{"text": "Message Polarity Classification", "start_pos": 22, "end_pos": 53, "type": "TASK", "confidence": 0.8195369442303976}]}, {"text": "For messages conveying both a positive and a negative sentiment, whichever is the stronger sentiment should be chosen.", "labels": [], "entities": []}, {"text": "Two modalities are possible: (1) Constrained (using the provided training data only; other resources, such as lexica, are allowed; however, it is not allowed to use additional tweets/SMS messages or additional sentences with sentiment annotations); and (2) Unconstrained (using additional data for training, e.g., additional tweets/SMS messages or additional sentences annotated for sentiment).", "labels": [], "entities": []}, {"text": "We participated in the Constrained modality.", "labels": [], "entities": []}, {"text": "We adopted a supervised machine learning (ML) approach based on various contextual and semantic features.", "labels": [], "entities": [{"text": "supervised machine learning (ML)", "start_pos": 13, "end_pos": 45, "type": "TASK", "confidence": 0.6649935046831766}]}, {"text": "In particular, we exploited resources such as SentiWordNet (), LIWC, and the lexicons described in.", "labels": [], "entities": [{"text": "LIWC", "start_pos": 63, "end_pos": 67, "type": "DATASET", "confidence": 0.8538528084754944}]}, {"text": "Critical features include: whether the message contains intensifiers, adjectives, interjections, presence of positive or negative emoticons, possible message polarity based on SentiWordNet scores (, scores based on LIWC categories), negated words, etc.", "labels": [], "entities": []}], "datasetContent": [{"text": "In order to select the best performing feature set, we carried out several 5-fold cross validation experiments on the training data.", "labels": [], "entities": []}, {"text": "We report in the best performing feature set.", "labels": [], "entities": []}, {"text": "In particular, we adopted a 2 stage approach: 1.", "labels": [], "entities": []}, {"text": "during the first stage we performed a binary classification of messages according to the classes neutral vs subjective; 2.", "labels": [], "entities": []}, {"text": "in the second stage, we performed a binary classification of subjective messages according to the classes positive vs negative.", "labels": [], "entities": []}, {"text": "We opted fora two stage binary classification approach, since we observed that it produces slightly, neutral vs subjective, obtained with 5-fold cross validation on training set only, accounts for an accuracy of 69.6%.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 200, "end_pos": 208, "type": "METRIC", "confidence": 0.9995018243789673}]}, {"text": "Instead, the best result for stage (1), obtained with training on training data and testing on development data, accounts for an accuracy of 72.67%.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 129, "end_pos": 137, "type": "METRIC", "confidence": 0.9996737241744995}]}, {"text": "The list of best features is reported in.", "labels": [], "entities": []}, {"text": "Feature selection was performed by starting from a small set of basic features, and then by adding the remaining features incrementally.", "labels": [], "entities": [{"text": "Feature selection", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.7356384992599487}]}, {"text": "In stage (2), positive vs negative, we started from the best feature set obtained from stage (1) and added the remaining features one by one incrementally.", "labels": [], "entities": []}, {"text": "In this case, we kept SWNscoresMaximum without testing again other formulae; in particular, to compute words prior polarity, we also kept the first sense approach, that assigns to every word the SWN score of its most frequent sense and proved to be the most discriminative in the first stage neutral vs. subjective.", "labels": [], "entities": []}, {"text": "We found that none of the feature sets produced better results than that obtained using the best feature set selected from stage (1).", "labels": [], "entities": []}, {"text": "So, the best feature set for stage (2) is unchanged.", "labels": [], "entities": []}, {"text": "We trained the system on the training data and tested it on the development data, achieving an accuracy of 80.67%.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 95, "end_pos": 103, "type": "METRIC", "confidence": 0.9997004270553589}]}, {"text": "The SemEval task organizers () provided two test sets on which the systems were to be evaluated: one included Twitter messages, i.e. the same type of texts included in the training set, while the other comprised SMS messages, i.e. texts having more or less the same length as the Twitter data but (supposedly) a different style.", "labels": [], "entities": []}, {"text": "We applied the same model, trained both on the training and the development set, on the two types of data, without any specific adaptation.", "labels": [], "entities": []}, {"text": "The Twitter test set was composed of 3,813 tweets.", "labels": [], "entities": [{"text": "Twitter test set", "start_pos": 4, "end_pos": 20, "type": "DATASET", "confidence": 0.786345104376475}]}, {"text": "Official results show that our approach yields an F-score of 0.5976 for Twitter messages (11th out of 35), while the best performing system obtained an F-score of 0.6902.", "labels": [], "entities": [{"text": "F-score", "start_pos": 50, "end_pos": 57, "type": "METRIC", "confidence": 0.999546229839325}, {"text": "F-score", "start_pos": 152, "end_pos": 159, "type": "METRIC", "confidence": 0.9991193413734436}]}, {"text": "The confusion matrix is reported in, while the score details in.", "labels": [], "entities": [{"text": "confusion matrix", "start_pos": 4, "end_pos": 20, "type": "METRIC", "confidence": 0.9779360294342041}]}, {"text": "The latter table shows that our system achieves the lowest results on negative tweets, both in terms of precision and of recall.", "labels": [], "entities": [{"text": "precision", "start_pos": 104, "end_pos": 113, "type": "METRIC", "confidence": 0.9995928406715393}, {"text": "recall", "start_pos": 121, "end_pos": 127, "type": "METRIC", "confidence": 0.9981433153152466}]}, {"text": "The SMS test set for the competition was composed of 2,094 SMS.", "labels": [], "entities": []}, {"text": "Official results provided by the task organizers show that our approach yields an Fscore of 0.5487 for SMS messages (8th out of 28 participants), while the best performing system obtained an F-score of 0.6846.", "labels": [], "entities": [{"text": "Fscore", "start_pos": 82, "end_pos": 88, "type": "METRIC", "confidence": 0.9997180104255676}, {"text": "F-score", "start_pos": 191, "end_pos": 198, "type": "METRIC", "confidence": 0.9993547797203064}]}, {"text": "The confusion matrix is reported in, while the score details in Table 7.", "labels": [], "entities": [{"text": "confusion", "start_pos": 4, "end_pos": 13, "type": "METRIC", "confidence": 0.9938066005706787}]}, {"text": "Also in this case the recognition of negative messages achieves by far the poorest performance.", "labels": [], "entities": []}, {"text": "A comparison of the results on the two test sets shows that, as expected, our system performs better on tweets than on SMS.", "labels": [], "entities": []}, {"text": "However, precision achieved by the system on neutral SMS is 0.12 points better on text messages than on tweets.", "labels": [], "entities": [{"text": "precision", "start_pos": 9, "end_pos": 18, "type": "METRIC", "confidence": 0.9997320771217346}]}, {"text": "Interestingly, it appears from the results in Tables 5 and 7 (and from the distribution of the classes in the data sets) that there maybe a correlation between the number of tweets/SMS fora particular class and the performance obtained for such class.", "labels": [], "entities": []}, {"text": "We plan to further investigate this issue.", "labels": [], "entities": []}, {"text": "208 64 936 0.5487: Detailed results for SMS task", "labels": [], "entities": [{"text": "SMS task", "start_pos": 40, "end_pos": 48, "type": "TASK", "confidence": 0.6645919978618622}]}], "tableCaptions": [{"text": " Table 5. The latter table shows that our system  achieves the lowest results on negative tweets, both  in terms of precision and of recall.", "labels": [], "entities": [{"text": "precision", "start_pos": 116, "end_pos": 125, "type": "METRIC", "confidence": 0.9995830655097961}, {"text": "recall", "start_pos": 133, "end_pos": 139, "type": "METRIC", "confidence": 0.9978310465812683}]}, {"text": " Table 5: Detailed results for Twitter task", "labels": [], "entities": []}, {"text": " Table 6: Confusion matrix for SMS task", "labels": [], "entities": [{"text": "SMS task", "start_pos": 31, "end_pos": 39, "type": "TASK", "confidence": 0.9038560390472412}]}, {"text": " Table 7: Detailed results for SMS task", "labels": [], "entities": [{"text": "SMS task", "start_pos": 31, "end_pos": 39, "type": "TASK", "confidence": 0.947767823934555}]}]}