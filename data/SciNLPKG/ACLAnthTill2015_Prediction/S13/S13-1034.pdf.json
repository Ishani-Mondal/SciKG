{"title": [{"text": "CNGL-CORE: Referential Translation Machines for Measuring Semantic Similarity", "labels": [], "entities": [{"text": "Referential Translation Machines", "start_pos": 11, "end_pos": 43, "type": "TASK", "confidence": 0.822689930597941}, {"text": "Similarity", "start_pos": 67, "end_pos": 77, "type": "TASK", "confidence": 0.5836875438690186}]}], "abstractContent": [{"text": "We invent referential translation machines (RTMs), a computational model for identifying the translation acts between any two data sets with respect to a reference corpus selected in the same domain, which can be used for judging the semantic similarity between text.", "labels": [], "entities": [{"text": "referential translation machines (RTMs)", "start_pos": 10, "end_pos": 49, "type": "TASK", "confidence": 0.8393753667672476}]}, {"text": "RTMs make quality and semantic similarity judgments possible by using retrieved relevant training data as interpretants for reaching shared semantics.", "labels": [], "entities": [{"text": "RTMs", "start_pos": 0, "end_pos": 4, "type": "TASK", "confidence": 0.8912949562072754}, {"text": "semantic similarity judgments", "start_pos": 22, "end_pos": 51, "type": "TASK", "confidence": 0.6501124799251556}]}, {"text": "An MTPP (machine translation performance predictor) model derives features measuring the closeness of the test sentences to the training data, the difficulty of translating them, and the presence of acts of translation involved.", "labels": [], "entities": [{"text": "MTPP (machine translation performance predictor)", "start_pos": 3, "end_pos": 51, "type": "TASK", "confidence": 0.6634810652051654}]}, {"text": "We view semantic similarity as paraphrasing between any two given texts.", "labels": [], "entities": []}, {"text": "Each view is modeled by an RTM model, giving us anew perspective on the binary relationship between the two.", "labels": [], "entities": []}, {"text": "Our prediction model is the 15th on some tasks and 30th overall out of 89 submissions in total according to the official results of the Semantic Textual Similarity (STS 2013) challenge.", "labels": [], "entities": [{"text": "Semantic Textual Similarity (STS 2013) challenge", "start_pos": 136, "end_pos": 184, "type": "TASK", "confidence": 0.8501185849308968}]}, {"text": "1 Semantic Textual Similarity Judgments We introduce a fully automated judge for semantic similarity that performs well in the semantic textual similarity (STS) task (Agirre et al., 2013).", "labels": [], "entities": [{"text": "Semantic Textual Similarity Judgments", "start_pos": 2, "end_pos": 39, "type": "TASK", "confidence": 0.699230432510376}, {"text": "semantic textual similarity (STS) task", "start_pos": 127, "end_pos": 165, "type": "TASK", "confidence": 0.7297144149030957}]}, {"text": "STS is a degree of semantic equivalence between two texts based on the observations that \"vehicle\" and \"car\" are more similar than \"wave\" and \"car\".", "labels": [], "entities": [{"text": "STS", "start_pos": 0, "end_pos": 3, "type": "METRIC", "confidence": 0.9344425201416016}]}, {"text": "Accurate prediction of STS has a wide application area including: identifying whether two tweets are talking about the same thing, whether an answer is correct by comparing it with a reference answer, and whether a given shorter text is a valid summary of another text.", "labels": [], "entities": [{"text": "Accurate prediction of STS", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.8412089049816132}]}, {"text": "The translation quality estimation task (Callison-Burch et al., 2012) aims to develop quality indicators for translations at the sentence-level and predictors without access to a reference translation.", "labels": [], "entities": [{"text": "translation quality estimation", "start_pos": 4, "end_pos": 34, "type": "TASK", "confidence": 0.802069365978241}]}, {"text": "(2013) develop atop performing machine translation performance predictor (MTPP), which uses machine learning models over features measuring how well the test set matches the training set relying on extrinsic and language independent features.", "labels": [], "entities": [{"text": "atop performing machine translation performance predictor (MTPP)", "start_pos": 15, "end_pos": 79, "type": "TASK", "confidence": 0.7577985922495524}]}, {"text": "The semantic textual similarity (STS) task (Agirre et al., 2013) addresses the following problem.", "labels": [], "entities": [{"text": "semantic textual similarity (STS)", "start_pos": 4, "end_pos": 37, "type": "TASK", "confidence": 0.7070750941832861}]}, {"text": "Given two sentences S 1 and S 2 in the same language, quantify the degree of similarity with a similarity score, which is a number in the range [0, 5].", "labels": [], "entities": [{"text": "similarity score", "start_pos": 95, "end_pos": 111, "type": "METRIC", "confidence": 0.9618959724903107}]}, {"text": "The semantic textual similarity prediction problem involves finding a function f approximating the semantic textual similarity score given two sentences, S 1 and S 2 : f (S 1 , S 2) \u2248 q(S 1 , S 2).", "labels": [], "entities": [{"text": "semantic textual similarity prediction", "start_pos": 4, "end_pos": 42, "type": "TASK", "confidence": 0.6935631409287453}]}, {"text": "(1) We approach f as a supervised learning problem with (S 1 , S 2 , q(S 1 , S 2)) tuples being the training data and q(S 1 , S 2) being the target similarity score.", "labels": [], "entities": []}, {"text": "We model the problem as a translation task where one possible interpretation is obtained by translating S 1 (the source to translate, S) to S 2 (the target translation, T).", "labels": [], "entities": []}, {"text": "Since linguistic processing can reveal deeper similarity relationships, we also look at the translation task at different granularities of information: plain text (R for regular) , after lemmatiza-tion (L), after part-of-speech (POS) tagging (P), and after removing 128 English stop-words (S) 1.", "labels": [], "entities": [{"text": "part-of-speech (POS) tagging", "start_pos": 213, "end_pos": 241, "type": "TASK", "confidence": 0.6929044485092163}]}], "introductionContent": [], "datasetContent": [], "tableCaptions": [{"text": " Table 1: CV performance on the training set with tuning. Underlined are the settings we use in our submissions. RTM  models in directions S 1 \u2192 S 2 , S 2 \u2192 S 1 , and the bi-directional models S 1 S 2 are displayed.", "labels": [], "entities": []}, {"text": " Table 2: STS challenge r and ranking results ranked ac- cording to the mean r obtained. head is headlines and  mean is the mean of all submissions.", "labels": [], "entities": [{"text": "head", "start_pos": 89, "end_pos": 93, "type": "METRIC", "confidence": 0.9682217240333557}]}, {"text": " Table 3: Bi-directional STS challenge r and ranking re- sults ranked according to the mean r obtained. We com- bine the two directions by taking the mean, min, or the  max or use the bi-directional RTM model S 1 S 2 .", "labels": [], "entities": []}]}