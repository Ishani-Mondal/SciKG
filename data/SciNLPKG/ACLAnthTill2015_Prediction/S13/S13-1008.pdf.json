{"title": [{"text": "NTNU-CORE: Combining strong features for semantic similarity", "labels": [], "entities": [{"text": "NTNU-CORE", "start_pos": 0, "end_pos": 9, "type": "DATASET", "confidence": 0.8566305637359619}, {"text": "semantic similarity", "start_pos": 41, "end_pos": 60, "type": "TASK", "confidence": 0.7608233988285065}]}], "abstractContent": [{"text": "The paper outlines the work carried out at NTNU as part of the *SEM'13 shared task on Semantic Textual Similarity, using an approach which combines shallow textual, dis-tributional and knowledge-based features by a support vector regression model.", "labels": [], "entities": [{"text": "NTNU", "start_pos": 43, "end_pos": 47, "type": "DATASET", "confidence": 0.9334933161735535}, {"text": "Semantic Textual Similarity", "start_pos": 86, "end_pos": 113, "type": "TASK", "confidence": 0.6511995395024618}]}, {"text": "Feature sets include (1) aggregated similarity based on named entity recognition with WordNet and Levenshtein distance through the calculation of maximum weighted bipartite graphs; (2) higher order word co-occurrence similarity using a novel method called \"Multi-sense Random Indexing\"; (3) deeper semantic relations based on the RelEx semantic dependency relationship extraction system; (4) graph edit-distance on dependency trees; (5) reused features of the TakeLab and DKPro systems from the STS'12 shared task.", "labels": [], "entities": [{"text": "named entity recognition", "start_pos": 56, "end_pos": 80, "type": "TASK", "confidence": 0.7116299271583557}, {"text": "WordNet", "start_pos": 86, "end_pos": 93, "type": "DATASET", "confidence": 0.9464912414550781}, {"text": "RelEx semantic dependency relationship extraction", "start_pos": 330, "end_pos": 379, "type": "TASK", "confidence": 0.714455759525299}]}, {"text": "The NTNU systems obtained 9th place overall (5th best team) and 1st place on the SMT data set.", "labels": [], "entities": [{"text": "NTNU", "start_pos": 4, "end_pos": 8, "type": "DATASET", "confidence": 0.9169521927833557}, {"text": "SMT data set", "start_pos": 81, "end_pos": 93, "type": "DATASET", "confidence": 0.9458040793736776}]}], "introductionContent": [{"text": "Intuitively, two texts are semantically similar if they roughly mean the same thing.", "labels": [], "entities": []}, {"text": "The task of formally establishing semantic textual similarity clearly is more complex.", "labels": [], "entities": [{"text": "establishing semantic textual similarity", "start_pos": 21, "end_pos": 61, "type": "TASK", "confidence": 0.5433282256126404}]}, {"text": "For a start, it implies that we have away to formally represent the intended meaning of all texts in all possible contexts, and furthermore away to measure the degree of equivalence between two such representations.", "labels": [], "entities": []}, {"text": "This goes far beyond the state-of-the-art for arbitrary sentence pairs, and several restrictions must be imposed.", "labels": [], "entities": []}, {"text": "The Semantic Textual Similarity (STS) task limits the comparison to isolated sentences only (rather than complete texts), and defines similarity of a pair of sentences as the one assigned by human judges on a 0-5 scale (with 0 implying no relation and 5 complete semantic equivalence).", "labels": [], "entities": [{"text": "Semantic Textual Similarity (STS) task", "start_pos": 4, "end_pos": 42, "type": "TASK", "confidence": 0.8013632808412824}]}, {"text": "It is unclear, however, to what extent two judges would agree on the level of similarity between sentences; report figures on the agreement between the authors themselves of about 87-89%.", "labels": [], "entities": [{"text": "similarity", "start_pos": 78, "end_pos": 88, "type": "METRIC", "confidence": 0.9294543266296387}]}, {"text": "As inmost language processing tasks, there are two overall ways to measure sentence similarity, either by data-driven (distributional) methods or by knowledge-driven methods; in the STS'12 task the two approaches were used nearly equally much.", "labels": [], "entities": []}, {"text": "Distributional models normally measure similarity in terms of word or word co-occurrence statistics, or through concept relations extracted from a corpus.", "labels": [], "entities": []}, {"text": "The basic strategy taken by NTNU in the STS'13 task was to use something of a \"feature carpet bombing approach\" in the way of first automatically extracting as many potentially useful features as possible, using both knowledge and data-driven methods, and then evaluating feature combinations on the data sets provided by the organisers of the shared task.", "labels": [], "entities": [{"text": "feature carpet bombing", "start_pos": 79, "end_pos": 101, "type": "TASK", "confidence": 0.6798075437545776}]}, {"text": "To this end, four different types of features were extracted.", "labels": [], "entities": []}, {"text": "The first (Section 2) aggregates similarity based on named entity recognition with WordNet and Levenshtein distance by calculating maximum weighted bipartite graphs.", "labels": [], "entities": [{"text": "named entity recognition", "start_pos": 53, "end_pos": 77, "type": "TASK", "confidence": 0.6168932318687439}, {"text": "WordNet", "start_pos": 83, "end_pos": 90, "type": "DATASET", "confidence": 0.9669302105903625}]}, {"text": "The second set of features (Section 3) models higher order co-occurrence similarity relations using Random Indexing (), both in the form of a (standard) sliding window approach and through a novel method called \"Multi-sense Random Indexing\" which aims to separate the representation of different senses of a term from each other.", "labels": [], "entities": []}, {"text": "The third feature set (Section 4) aims to capture deeper semantic relations using either the output of the RelEx semantic dependency relationship extraction system ( or an in-house graph edit-distance matching system.", "labels": [], "entities": [{"text": "RelEx semantic dependency relationship extraction", "start_pos": 107, "end_pos": 156, "type": "TASK", "confidence": 0.7633767366409302}]}, {"text": "The final set (Section 5) is a straight-forward gathering of features from the systems that fared best in STS'12: TakeLab from University of Zagreb ( \u02c7 Sari\u00b4cSari\u00b4c et al., 2012) and DKPro from Darmstadt's Ubiquitous Knowledge Processing Lab.", "labels": [], "entities": [{"text": "TakeLab from University of Zagreb ( \u02c7 Sari\u00b4cSari\u00b4c et al., 2012)", "start_pos": 114, "end_pos": 178, "type": "DATASET", "confidence": 0.8415429500433115}]}, {"text": "As described in Section 6, Support Vector Regression () was used for solving the multi-dimensional regression problem of combining all the extracted feature values.", "labels": [], "entities": [{"text": "Support Vector Regression", "start_pos": 27, "end_pos": 52, "type": "METRIC", "confidence": 0.795651892820994}]}, {"text": "Three different systems were created based on feature performance on the supplied development data.", "labels": [], "entities": []}, {"text": "Section 7 discusses scores on the STS'12 and STS'13 test data.", "labels": [], "entities": [{"text": "STS'12 and STS'13 test data", "start_pos": 34, "end_pos": 61, "type": "DATASET", "confidence": 0.7746848225593567}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Correlation score on 2012 test data", "labels": [], "entities": [{"text": "2012 test data", "start_pos": 31, "end_pos": 45, "type": "DATASET", "confidence": 0.8632951577504476}]}, {"text": " Table 1. This basically shows that except  for the GateWordMatch, adding our other fea- tures tends to give slightly lower scores (cf. NTNU1  vs NTNU3). In addition, the table illustrates that op- timizing the SVR according to cross-validated grid  search on 2012 training data (here C = 200), rarely  pays off when testing on unseen data (cf. NTNU1  vs NTNU2).", "labels": [], "entities": [{"text": "GateWordMatch", "start_pos": 52, "end_pos": 65, "type": "DATASET", "confidence": 0.8881199955940247}]}, {"text": " Table 2: Correlation score and rank on 2013 test data", "labels": [], "entities": [{"text": "Correlation score", "start_pos": 10, "end_pos": 27, "type": "METRIC", "confidence": 0.9295012354850769}, {"text": "2013 test data", "start_pos": 40, "end_pos": 54, "type": "DATASET", "confidence": 0.8057423631350199}]}, {"text": " Table 3: Correlation score and rank of the best features", "labels": [], "entities": [{"text": "Correlation score", "start_pos": 10, "end_pos": 27, "type": "METRIC", "confidence": 0.9532699286937714}]}]}