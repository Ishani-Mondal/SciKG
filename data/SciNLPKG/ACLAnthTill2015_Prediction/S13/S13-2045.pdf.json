{"title": [{"text": "SemEval-2013 Task 7: The Joint Student Response Analysis and 8th Recognizing Textual Entailment Challenge", "labels": [], "entities": [{"text": "8th", "start_pos": 61, "end_pos": 64, "type": "METRIC", "confidence": 0.9777539968490601}, {"text": "Recognizing Textual Entailment", "start_pos": 65, "end_pos": 95, "type": "TASK", "confidence": 0.6439960499604543}]}], "abstractContent": [{"text": "We present the results of the Joint Student Response Analysis and 8th Recognizing Tex-tual Entailment Challenge, aiming to bring together researchers in educational NLP technology and textual entailment.", "labels": [], "entities": []}, {"text": "The task of giving feedback on student answers requires semantic inference and therefore is related to recognizing textual entailment.", "labels": [], "entities": [{"text": "recognizing textual entailment", "start_pos": 103, "end_pos": 133, "type": "TASK", "confidence": 0.7878377834955851}]}, {"text": "Thus, we offered to the community a 5-way student response labeling task, as well as 3-way and 2-way RTE-style tasks on educational data.", "labels": [], "entities": [{"text": "student response labeling task", "start_pos": 42, "end_pos": 72, "type": "TASK", "confidence": 0.6519755423069}]}, {"text": "In addition, a partial entailment task was piloted.", "labels": [], "entities": []}, {"text": "We present and compare results from 9 participating teams, and discuss future directions.", "labels": [], "entities": []}], "introductionContent": [{"text": "One of the tasks in educational NLP systems is providing feedback to students in the context of exam questions, homework or intelligent tutoring.", "labels": [], "entities": []}, {"text": "Much previous work has been devoted to the automated scoring of essays (, error detection and correction (, and classification of texts by grade level.", "labels": [], "entities": [{"text": "automated scoring of essays", "start_pos": 43, "end_pos": 70, "type": "TASK", "confidence": 0.6671634688973427}, {"text": "error detection and correction", "start_pos": 74, "end_pos": 104, "type": "TASK", "confidence": 0.756293922662735}]}, {"text": "In these applications, NLP methods based on shallow features and supervised learning are often highly effective.", "labels": [], "entities": []}, {"text": "However, for the assessment of responses to short-answer questions () and in tutorial dialog systems) deeper semantic processing is likely to be appropriate.", "labels": [], "entities": []}, {"text": "Since the task of making and testing a full educational dialog system is daunting, identified a key subtask and proposed it as anew shared task for the NLP community.", "labels": [], "entities": []}, {"text": "Student response analysis (henceforth SRA) is the task of labeling student answers with categories that could", "labels": [], "entities": [{"text": "Student response analysis (henceforth SRA)", "start_pos": 0, "end_pos": 42, "type": "TASK", "confidence": 0.8217072742325919}]}], "datasetContent": [{"text": "For each evaluation data set (test set), we computed the per-class precision, recall and F 1 score.", "labels": [], "entities": [{"text": "precision", "start_pos": 67, "end_pos": 76, "type": "METRIC", "confidence": 0.9551630616188049}, {"text": "recall", "start_pos": 78, "end_pos": 84, "type": "METRIC", "confidence": 0.9993438124656677}, {"text": "F 1 score", "start_pos": 89, "end_pos": 98, "type": "METRIC", "confidence": 0.9905271927515665}]}, {"text": "We also computed three main summary metrics: accuracy, macro-average F 1 and weighted average F 1 . Accuracy is the overall percentage of correctly classified examples.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 45, "end_pos": 53, "type": "METRIC", "confidence": 0.999625563621521}, {"text": "macro-average F 1", "start_pos": 55, "end_pos": 72, "type": "METRIC", "confidence": 0.7351347804069519}, {"text": "weighted average F 1", "start_pos": 77, "end_pos": 97, "type": "METRIC", "confidence": 0.9540223330259323}, {"text": "Accuracy", "start_pos": 100, "end_pos": 108, "type": "METRIC", "confidence": 0.9993267059326172}]}, {"text": "Macroaverage is the average value of each metric (precision, recall, F 1 ) across classes, without taking class size into account.", "labels": [], "entities": [{"text": "precision", "start_pos": 50, "end_pos": 59, "type": "METRIC", "confidence": 0.9987518787384033}, {"text": "recall", "start_pos": 61, "end_pos": 67, "type": "METRIC", "confidence": 0.9929648637771606}, {"text": "F 1 )", "start_pos": 69, "end_pos": 74, "type": "METRIC", "confidence": 0.9748626947402954}]}, {"text": "It is defined as 1/N cc metric(c), where N c is the number of classes (2, 3, or 5 depending on the task).", "labels": [], "entities": []}, {"text": "Note that in the 5-way SCIENTSBANK dataset the 'nondomain' class is severely underrepresented, with only 23 examples out of 4335 total (see).", "labels": [], "entities": [{"text": "SCIENTSBANK dataset", "start_pos": 23, "end_pos": 42, "type": "DATASET", "confidence": 0.7143133729696274}]}, {"text": "Therefore, we calculated macro-averaged P/R/F 1 over only 4 classes (i.e. excluding the 'non-domain' class) for SCIENTSBANK 5-way data.", "labels": [], "entities": [{"text": "macro-averaged P/R/F 1", "start_pos": 25, "end_pos": 47, "type": "METRIC", "confidence": 0.5792389341763088}]}, {"text": "Weighted Average (or simply weighted) is the average value for each metric weighted by class size, defined as 1/N c |c| * metric(c) where N is the total number of test items and |c| is the number of items labeled as c in gold-standard data.", "labels": [], "entities": []}, {"text": "In general, macro-averaging favors systems that perform well across all classes regardless of class size.", "labels": [], "entities": []}, {"text": "Accuracy and weighted average prefer systems that perform best on the largest number of examples, favoring higher performance on the most frequent classes.", "labels": [], "entities": []}, {"text": "In practice, only a small number of the systems were ranked differently by the different metrics.", "labels": [], "entities": []}, {"text": "We discuss this further in Section 4.7.", "labels": [], "entities": []}, {"text": "Results for all metrics are available online, and this paper focuses on two metrics for brevity: weighted and macro-average F 1 scores.", "labels": [], "entities": [{"text": "F 1 scores", "start_pos": 124, "end_pos": 134, "type": "METRIC", "confidence": 0.8775237202644348}]}, {"text": "We used a subset of the SCIENTSBANK Extra corpus () with the same problematic questions filtered out as the main task (see Section 3.3).", "labels": [], "entities": [{"text": "SCIENTSBANK Extra corpus", "start_pos": 24, "end_pos": 48, "type": "DATASET", "confidence": 0.6340575913588206}]}, {"text": "We further filtered out all the student answer facets which were labeled other than 'Expressed' or 'Unaddressed' in the gold standard annotation; the facets in which the relationship between the two key terms, as classified in the manual annotation, proved to be problematic to define and judge, namely Topic, Agent, Root, Cause, Quantifier, Neg; and inter-propositional facets, i.e. facets that expressed relations between higher-level propositions.", "labels": [], "entities": []}, {"text": "Finally, the facet relations were removed from the dataset, leaving the relationship between the two facet terms unspecified so as to allow a more fuzzy approach to the inference problem posed by the exercise.", "labels": [], "entities": []}, {"text": "We used the same training/test split as reported in Section 3.4.", "labels": [], "entities": []}, {"text": "The training set created from the Training SCIENTSBANK Extra corpus contains 13,145 reference answer facets, 5,939 of which were labeled as 'Expressed' in the student answers and 7,206 as 'Unaddressed'.", "labels": [], "entities": [{"text": "Training SCIENTSBANK Extra corpus", "start_pos": 34, "end_pos": 67, "type": "DATASET", "confidence": 0.7812598347663879}]}, {"text": "The Test set was created from the SCIENTSBANK Extra unseen data and is divided into the same subsets as the main task (Unseen Answers, Unseen Questions and Unseen Domains).", "labels": [], "entities": [{"text": "SCIENTSBANK Extra unseen data", "start_pos": 34, "end_pos": 63, "type": "DATASET", "confidence": 0.6216380968689919}]}, {"text": "It contains 16,263 facets total, with 5,945 instances labeled as 'Expressed', and 10,318 labeled as 'Unaddressed'.", "labels": [], "entities": []}, {"text": "The metrics used in the Pilot task were the same as in the Main task, i.e. Overall Accuracy, Macroaverage  . and Weighted Average Precision, Recall and F 1 , and computed as described in Section 4.2.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 83, "end_pos": 91, "type": "METRIC", "confidence": 0.8176522254943848}, {"text": "Macroaverage", "start_pos": 93, "end_pos": 105, "type": "METRIC", "confidence": 0.995026171207428}, {"text": "Weighted Average Precision", "start_pos": 113, "end_pos": 139, "type": "METRIC", "confidence": 0.7780857880910238}, {"text": "Recall", "start_pos": 141, "end_pos": 147, "type": "METRIC", "confidence": 0.9879922866821289}, {"text": "F 1", "start_pos": 152, "end_pos": 155, "type": "METRIC", "confidence": 0.9884045422077179}]}, {"text": "We used only a majority class baseline, which labeled all facets as 'Unaddressed'.", "labels": [], "entities": []}, {"text": "Its performance is presented in Section 5.4 jointly with the system results.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Label distribution. Percentages in parentheses. UA, UQ, UD correspond to individual test sets.", "labels": [], "entities": []}, {"text": " Table 2: Five-way task weighted-average F 1", "labels": [], "entities": []}, {"text": " Table 3: Five-way task macro-average F 1", "labels": [], "entities": []}, {"text": " Table 4: Three-way task weighted-average F 1", "labels": [], "entities": []}, {"text": " Table 5: Three-way task macro-average F 1", "labels": [], "entities": []}, {"text": " Table 6: Two-way task macro-average F 1", "labels": [], "entities": []}, {"text": " Table 7: Weighted-average and macro-average F 1 scores  (UA: Unseen Answers; UQ: Unseen Questions; UD Un- seen Domains)", "labels": [], "entities": [{"text": "F 1 scores", "start_pos": 45, "end_pos": 55, "type": "METRIC", "confidence": 0.9444157282511393}]}]}