{"title": [{"text": "Unsupervised Word Usage Similarity in Social Media Texts", "labels": [], "entities": [{"text": "Unsupervised Word Usage Similarity", "start_pos": 0, "end_pos": 34, "type": "TASK", "confidence": 0.6400531604886055}]}], "abstractContent": [{"text": "We propose an unsupervised method for automatically calculating word usage similarity in social media data based on topic modelling , which we contrast with a baseline dis-tributional method and Weighted Textual Matrix Factorization.", "labels": [], "entities": [{"text": "calculating word usage similarity in social media", "start_pos": 52, "end_pos": 101, "type": "TASK", "confidence": 0.7131009016718183}]}, {"text": "We evaluate these methods against a novel dataset made up of human ratings over 550 Twitter message pairs annotated for usage similarity fora set of 10 nouns.", "labels": [], "entities": []}, {"text": "The results show that our topic modelling approach outperforms the other two methods.", "labels": [], "entities": [{"text": "topic modelling", "start_pos": 26, "end_pos": 41, "type": "TASK", "confidence": 0.8193381726741791}]}], "introductionContent": [{"text": "In recent years, with the growing popularity of social media applications, there has been a steep rise in the amount of \"post\"-based user-generated text (including microblog posts, status updates and comments).", "labels": [], "entities": []}, {"text": "This data has been identified as having potential for applications ranging from trend analysis ( and event detection () to election outcome prediction).", "labels": [], "entities": [{"text": "trend analysis", "start_pos": 80, "end_pos": 94, "type": "TASK", "confidence": 0.7410152554512024}, {"text": "event detection", "start_pos": 101, "end_pos": 116, "type": "TASK", "confidence": 0.7142990231513977}, {"text": "election outcome prediction", "start_pos": 123, "end_pos": 150, "type": "TASK", "confidence": 0.6680983901023865}]}, {"text": "However, given that posts are generally very short, noisy and lacking in context, traditional NLP approaches tend to perform poorly over social media data.", "labels": [], "entities": []}, {"text": "This is the first paper to address the task of lexical semantic interpretation in microblog data based on word usage similarity.", "labels": [], "entities": [{"text": "lexical semantic interpretation", "start_pos": 47, "end_pos": 78, "type": "TASK", "confidence": 0.6772399246692657}]}, {"text": "Word usage similarity (USIM:) is a relatively new paradigm for capturing similarity in the usages of a given word independently of any lexicon or sense inventory.", "labels": [], "entities": [{"text": "Word usage similarity (USIM", "start_pos": 0, "end_pos": 27, "type": "TASK", "confidence": 0.6892769992351532}]}, {"text": "The task is to rate on an ordinal scale the similarity in usage between two different usages of the same word.", "labels": [], "entities": []}, {"text": "In doing so, it avoids common issues in conventional word sense disambiguation, relating to sense underspecification, the appropriateness of a static sense inventory to a given domain, and the inability to capture similarities/overlaps between word senses.", "labels": [], "entities": [{"text": "word sense disambiguation", "start_pos": 53, "end_pos": 78, "type": "TASK", "confidence": 0.6420660316944122}]}, {"text": "As an example of USIM, consider the following pairing of Twitter posts containing the target word paper:", "labels": [], "entities": [{"text": "USIM", "start_pos": 17, "end_pos": 21, "type": "TASK", "confidence": 0.48046788573265076}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 2: Spearman rank correlation (\u03c1) for each  method based on each background corpus. The best  result for each corpus is shown in bold.", "labels": [], "entities": [{"text": "Spearman rank correlation (\u03c1)", "start_pos": 10, "end_pos": 39, "type": "METRIC", "confidence": 0.8877565463383993}]}, {"text": " Table 3: Spearman's \u03c1 using LDA for the optimal T  for each lemma (Per lemma) and the best T over all  lemmas (Global) using ORIGINAL and EXPANDED.  \u03c1 values that are significant at the 0.05 level are  shown in bold.", "labels": [], "entities": [{"text": "LDA", "start_pos": 29, "end_pos": 32, "type": "METRIC", "confidence": 0.9016938209533691}, {"text": "ORIGINAL", "start_pos": 126, "end_pos": 134, "type": "METRIC", "confidence": 0.8732659816741943}]}]}