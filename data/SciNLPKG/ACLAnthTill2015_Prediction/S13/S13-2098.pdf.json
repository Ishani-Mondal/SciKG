{"title": [{"text": "CNGL: Grading Student Answers by Acts of Translation", "labels": [], "entities": [{"text": "Grading Student Answers by Acts of Translation", "start_pos": 6, "end_pos": 52, "type": "TASK", "confidence": 0.8443556087357658}]}], "abstractContent": [{"text": "We invent referential translation machines (RTMs), a computational model for identifying the translation acts between any two data sets with respect to a reference corpus selected in the same domain, which can be used for automatically grading student answers.", "labels": [], "entities": [{"text": "referential translation machines (RTMs)", "start_pos": 10, "end_pos": 49, "type": "TASK", "confidence": 0.841309110323588}]}, {"text": "RTMs make quality and semantic similarity judgments possible by using retrieved relevant training data as interpretants for reaching shared semantics.", "labels": [], "entities": [{"text": "RTMs", "start_pos": 0, "end_pos": 4, "type": "TASK", "confidence": 0.8912951350212097}, {"text": "semantic similarity judgments", "start_pos": 22, "end_pos": 51, "type": "TASK", "confidence": 0.6501124799251556}]}, {"text": "An MTPP (machine translation performance predictor) model derives features measuring the closeness of the test sentences to the training data, the difficulty of translating them, and the presence of acts of translation involved.", "labels": [], "entities": [{"text": "MTPP (machine translation performance predictor)", "start_pos": 3, "end_pos": 51, "type": "TASK", "confidence": 0.66348106946264}]}, {"text": "We view question answering as translation from the question to the answer, from the question to the reference answer, from the answer to the reference answer , or from the question and the answer to the reference answer.", "labels": [], "entities": [{"text": "question answering", "start_pos": 8, "end_pos": 26, "type": "TASK", "confidence": 0.7639915943145752}]}, {"text": "Each view is modeled by an RTM model, giving us anew perspective on the ternary relationship between the question , the answer, and the reference answer.", "labels": [], "entities": []}, {"text": "We show that all RTM models contribute and a prediction model based on all four perspectives performs the best.", "labels": [], "entities": [{"text": "RTM", "start_pos": 17, "end_pos": 20, "type": "TASK", "confidence": 0.8998830318450928}]}, {"text": "Our prediction model is the 2nd best system on some tasks according to the official results of the Student Response Analysis (SRA 2013) challenge.", "labels": [], "entities": [{"text": "Student Response Analysis (SRA 2013) challenge", "start_pos": 99, "end_pos": 145, "type": "TASK", "confidence": 0.5702793784439564}]}], "introductionContent": [], "datasetContent": [{"text": "SRA involves the prediction on Beetle (student interactions when learning conceptual knowledge in the basic electricity and electronics domain) and SciEntsBank (science assessment questions) datasets.", "labels": [], "entities": [{"text": "SRA", "start_pos": 0, "end_pos": 3, "type": "TASK", "confidence": 0.7807748913764954}, {"text": "Beetle", "start_pos": 31, "end_pos": 37, "type": "DATASET", "confidence": 0.9249005913734436}]}, {"text": "SciEntsBank is harder due to containing questions from multiple domains (.", "labels": [], "entities": []}, {"text": "SRA challenge results are evaluated with the weighted average ).", "labels": [], "entities": [{"text": "SRA challenge", "start_pos": 0, "end_pos": 13, "type": "TASK", "confidence": 0.6991076469421387}]}, {"text": "The lexical baseline system is based on measures of lexical overlap using 4 features: the number of overlapping words, F 1 , Lesk, and cosine scores over the words when comparing A and R ({4}) and Q and R ({4}).", "labels": [], "entities": [{"text": "F 1", "start_pos": 119, "end_pos": 122, "type": "METRIC", "confidence": 0.9847469329833984}]}, {"text": "Lesk score is calculated as: L(A, R) = p\u2208M |p| 2 /(|A||R|), where M contains the maximal overlapping phrases that match in A and Rand |p| is the length of a phrase . This lexical baseline is highly competitive: no submission performed better in the 2-way Beetle unseen questions task.", "labels": [], "entities": [{"text": "Beetle unseen questions task", "start_pos": 255, "end_pos": 283, "type": "TASK", "confidence": 0.7007475048303604}]}], "tableCaptions": [{"text": " Table 1: Performance on the training set without tuning.", "labels": [], "entities": []}, {"text": " Table 2. The CNGL system significantly outper- forms the lexical overlap baseline in all tasks for  Beetle and in the 2-way task for SciEntsBank. For  3-way and 5-way, CNGL performs slightly better.", "labels": [], "entities": [{"text": "Beetle", "start_pos": 101, "end_pos": 107, "type": "DATASET", "confidence": 0.928368330001831}]}, {"text": " Table 2: Optimized SVR results vs. lexical overlap base- line on the training set for 2-way, 3-way, or 5-way tasks.", "labels": [], "entities": []}, {"text": " Table 3: SRA challenge results: CNGL SVR submission,  the lexical overlap baseline, and the maximum and mean  results for 2-way, 3-way, or 5-way tasks. uA, uQ, and uD  correspond to unseen answers, questions, and domains.", "labels": [], "entities": [{"text": "SRA challenge", "start_pos": 10, "end_pos": 23, "type": "TASK", "confidence": 0.6802401542663574}, {"text": "CNGL SVR submission", "start_pos": 33, "end_pos": 52, "type": "TASK", "confidence": 0.5268324514230093}]}, {"text": " Table 4: Improved SVR performance on the training set  with tuning for 2-way, 3-way, or 5-way tasks.", "labels": [], "entities": [{"text": "SVR", "start_pos": 19, "end_pos": 22, "type": "TASK", "confidence": 0.8650780320167542}]}, {"text": " Table 1 and Table 2.", "labels": [], "entities": []}, {"text": " Table 6: Improved TREE results on the SRA task test set.", "labels": [], "entities": [{"text": "Improved", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9946655035018921}, {"text": "TREE", "start_pos": 19, "end_pos": 23, "type": "METRIC", "confidence": 0.9165791869163513}, {"text": "SRA task test set", "start_pos": 39, "end_pos": 56, "type": "DATASET", "confidence": 0.8264292329549789}]}]}