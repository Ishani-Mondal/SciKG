{"title": [{"text": "CU : Computational Assessment of Short Free Text Answers -A Tool for Evaluating Students' Understanding", "labels": [], "entities": [{"text": "CU", "start_pos": 0, "end_pos": 2, "type": "DATASET", "confidence": 0.9078028202056885}, {"text": "Computational Assessment of Short Free Text Answers", "start_pos": 5, "end_pos": 56, "type": "TASK", "confidence": 0.7960327182497297}, {"text": "Evaluating Students' Understanding", "start_pos": 69, "end_pos": 103, "type": "TASK", "confidence": 0.7693593700726827}]}], "abstractContent": [{"text": "Assessing student understanding by evaluating their free text answers to posed questions is a very important task.", "labels": [], "entities": [{"text": "Assessing student understanding", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.8340641458829244}]}, {"text": "However, manually, it is time-consuming and computationally, it is difficult.", "labels": [], "entities": []}, {"text": "This paper details our shallow NLP approach to computationally assessing student free text answers when a reference answer is provided.", "labels": [], "entities": []}, {"text": "For four out of the five test sets, our system achieved an overall accuracy above the median and mean.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 67, "end_pos": 75, "type": "METRIC", "confidence": 0.9994145631790161}, {"text": "mean", "start_pos": 97, "end_pos": 101, "type": "METRIC", "confidence": 0.9717091917991638}]}], "introductionContent": [{"text": "Assessing student understanding is one of the holy grails of education ().", "labels": [], "entities": [{"text": "Assessing student understanding", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.9046139717102051}]}, {"text": "If we (teachers, tutors, intelligent tutors, potential employers, parents and school administrators) know what and how much a student knows, then we know what the student still needs to learn.", "labels": [], "entities": []}, {"text": "And then, can efficiently and effectively educate the student.", "labels": [], "entities": []}, {"text": "However, the task of assessing what exactly a student understands about a particular topic can be expensive, difficult and subjective.", "labels": [], "entities": []}, {"text": "Using multiple choice questionnaires is one of the most prevalent forms of assessing student understanding because it is easy and fast, both manually and computationally.", "labels": [], "entities": [{"text": "assessing student understanding", "start_pos": 75, "end_pos": 106, "type": "TASK", "confidence": 0.5732686817646027}]}, {"text": "However there has been a lot of pushback from educators about the validity of results gotten from multiple choice questionnaires.", "labels": [], "entities": []}, {"text": "Assessing student understanding by evaluating student free text answers either written or spoken is one of the preferred alternatives to multiple choice questionnaires.", "labels": [], "entities": [{"text": "Assessing student understanding", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.8135807911554972}]}, {"text": "As an assessment tool, free text answers can illuminate what and how much a student knows since the student is forced to recall terms and make connections between those terms rather than just picking one out of several options.", "labels": [], "entities": []}, {"text": "However, assessing free text answers manually is tedious, expensive and time-consuming, hence the search fora computational option.", "labels": [], "entities": []}, {"text": "There are three main issues that can limit the computational approach and corresponding performance when assessing free text answers: (1) the unit of assessment, (2) the reference and (3) the level of assessment.", "labels": [], "entities": []}, {"text": "The unit of assessment can be words, facets, phrases, sentences, short answers or essays.", "labels": [], "entities": []}, {"text": "The reference is the correct answer and what is being compared to the student answer.", "labels": [], "entities": []}, {"text": "Most researchers generate the reference manually) but some have focused on automatically generating the reference.", "labels": [], "entities": []}, {"text": "The level of assessment can be coarse with 2 categories such as correct and incorrect or more finer-grained with up to 19 categories as in.", "labels": [], "entities": []}, {"text": "In general, the finergrained assessments are more difficult to assess.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 4. The results on our training data and  a breakdown of the contribution of each feature is  shown in", "labels": [], "entities": []}, {"text": " Table 1: Overall Accuracy results for CU system on the  test", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 18, "end_pos": 26, "type": "METRIC", "confidence": 0.984427273273468}]}, {"text": " Table 2: Accuracy results for 5X cross validation on the  training data", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9994760155677795}]}]}