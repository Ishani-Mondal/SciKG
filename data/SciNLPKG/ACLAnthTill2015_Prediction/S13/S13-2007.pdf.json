{"title": [{"text": "SemEval-2013 Task 5: Evaluating Phrasal Semantics", "labels": [], "entities": [{"text": "SemEval-2013 Task", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.8772563338279724}, {"text": "Evaluating Phrasal Semantics", "start_pos": 21, "end_pos": 49, "type": "TASK", "confidence": 0.7443889578183492}]}], "abstractContent": [{"text": "This paper describes the SemEval-2013 Task 5: \"Evaluating Phrasal Semantics\".", "labels": [], "entities": [{"text": "SemEval-2013 Task", "start_pos": 25, "end_pos": 42, "type": "TASK", "confidence": 0.9012441635131836}, {"text": "Evaluating Phrasal Semantics", "start_pos": 47, "end_pos": 75, "type": "TASK", "confidence": 0.8058648904164633}]}, {"text": "Its first subtask is about computing the semantic similarity of words and compositional phrases of minimal length.", "labels": [], "entities": [{"text": "computing the semantic similarity of words and compositional phrases", "start_pos": 27, "end_pos": 95, "type": "TASK", "confidence": 0.6465574271149106}]}, {"text": "The second one addresses deciding the compositionality of phrases in a given context.", "labels": [], "entities": [{"text": "deciding the compositionality of phrases", "start_pos": 25, "end_pos": 65, "type": "TASK", "confidence": 0.7740653991699219}]}, {"text": "The paper discusses the importance and background of these subtasks and their structure.", "labels": [], "entities": []}, {"text": "In succession, it introduces the systems that participated and discusses evaluation results.", "labels": [], "entities": []}], "introductionContent": [{"text": "Numerous past tasks have focused on leveraging the meaning of word types or words in context.", "labels": [], "entities": [{"text": "leveraging the meaning of word types or words in context", "start_pos": 36, "end_pos": 92, "type": "TASK", "confidence": 0.7901691913604736}]}, {"text": "Examples of the former are noun categorization and the TOEFL test, examples of the latter are word sense disambiguation, metonymy resolution, and lexical substitution.", "labels": [], "entities": [{"text": "noun categorization", "start_pos": 27, "end_pos": 46, "type": "TASK", "confidence": 0.8020192086696625}, {"text": "TOEFL test", "start_pos": 55, "end_pos": 65, "type": "METRIC", "confidence": 0.8554652035236359}, {"text": "word sense disambiguation", "start_pos": 94, "end_pos": 119, "type": "TASK", "confidence": 0.7122791806856791}, {"text": "metonymy resolution", "start_pos": 121, "end_pos": 140, "type": "TASK", "confidence": 0.8203384578227997}, {"text": "lexical substitution", "start_pos": 146, "end_pos": 166, "type": "TASK", "confidence": 0.7553160488605499}]}, {"text": "As these tasks have enjoyed a lot success, a natural progression is the pursuit of models that can perform similar tasks taking into account multiword expressions and complex compositional structure.", "labels": [], "entities": []}, {"text": "In this paper, we present two subtasks designed to evaluate such phrasal models: a.", "labels": [], "entities": []}, {"text": "Semantic similarity of words and compositional phrases b.", "labels": [], "entities": [{"text": "Semantic similarity of words and compositional phrases b", "start_pos": 0, "end_pos": 56, "type": "TASK", "confidence": 0.7639810480177402}]}, {"text": "Evaluating the compositionality of phrases in context For example, the first subtask addresses computing how similar the word \"valuation\" is to the compositional sequence \"price assessment\", while the second subtask addresses deciding whether the phrase \"piece of cake\" is used literally or figuratively in the sentence \"Labour was apiece of cake!\".", "labels": [], "entities": []}, {"text": "The aim of these subtasks is two-fold.", "labels": [], "entities": []}, {"text": "Firstly, considering that there is a spread interest lately in phrasal semantics in its various guises, they provide an opportunity to draw together approaches to numerous related problems under a common evaluation set.", "labels": [], "entities": []}, {"text": "It is intended that after the competition, the evaluation setting and the datasets will comprise an on-going benchmark for the evaluation of these phrasal models.", "labels": [], "entities": []}, {"text": "Secondly, the subtasks attempt to bridge the gap between established lexical semantics and fullblown linguistic inference.", "labels": [], "entities": []}, {"text": "Thus, we anticipate that they will stimulate an increased interest around the general issue of phrasal semantics.", "labels": [], "entities": []}, {"text": "We use the notion of phrasal semantics here as opposed to lexical compounds or compositional semantics.", "labels": [], "entities": []}, {"text": "Bridging the gap between lexical semantics and linguistic inference could provoke novel approaches to certain established tasks, such as lexical entailment and paraphrase identification.", "labels": [], "entities": [{"text": "paraphrase identification", "start_pos": 160, "end_pos": 185, "type": "TASK", "confidence": 0.8407458961009979}]}, {"text": "In addition, it could ul-timately lead to improvements in a wide range of applications in natural language processing, such as document retrieval, clustering and classification, question answering, query expansion, synonym extraction, relation extraction, automatic translation, or textual advertisement matching in search engines, all of which depend on phrasal semantics.", "labels": [], "entities": [{"text": "document retrieval", "start_pos": 127, "end_pos": 145, "type": "TASK", "confidence": 0.761866956949234}, {"text": "clustering and classification", "start_pos": 147, "end_pos": 176, "type": "TASK", "confidence": 0.7046051124731699}, {"text": "question answering", "start_pos": 178, "end_pos": 196, "type": "TASK", "confidence": 0.878539115190506}, {"text": "query expansion", "start_pos": 198, "end_pos": 213, "type": "TASK", "confidence": 0.7130301743745804}, {"text": "synonym extraction", "start_pos": 215, "end_pos": 233, "type": "TASK", "confidence": 0.8789682984352112}, {"text": "relation extraction", "start_pos": 235, "end_pos": 254, "type": "TASK", "confidence": 0.7996499240398407}, {"text": "automatic translation", "start_pos": 256, "end_pos": 277, "type": "TASK", "confidence": 0.689039871096611}, {"text": "textual advertisement matching", "start_pos": 282, "end_pos": 312, "type": "TASK", "confidence": 0.6476660271485647}]}, {"text": "The remainder of this paper is structured as follows: Section 2 presents details about the data sources and the variety of sources applicable to the task.", "labels": [], "entities": []}, {"text": "Section 3 discusses the first subtask, which is about semantic similarity of words and compositional phrases.", "labels": [], "entities": []}, {"text": "In subsection 3.1 the subtask is described in detail together with some information about its background.", "labels": [], "entities": []}, {"text": "Subsection 3.2 discusses the data creation process and subsection 3.3 discusses the participating systems and their results.", "labels": [], "entities": [{"text": "data creation process", "start_pos": 29, "end_pos": 50, "type": "TASK", "confidence": 0.7857474088668823}]}, {"text": "Section 4 introduces the second subtask, which is about evaluating the compositionality of phrases in context.", "labels": [], "entities": []}, {"text": "Subsection 4.1 explains the data creation process for this subtask.", "labels": [], "entities": []}, {"text": "In subsection 4.2 the evaluation statistics of participating systems are presented.", "labels": [], "entities": []}, {"text": "Section 5 is a discussion about the conclusions of the entire task.", "labels": [], "entities": []}, {"text": "Finally, in section 6 we summarize this presentation and discuss briefly our vision about challenges in distributional semantics.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Quantitative characteristics of the datasets", "labels": [], "entities": []}, {"text": " Table 2: Task 5a: Evaluation results. A, P, R, rej. and F 1 stand for accuracy, precision, recall, rejection and F 1 score,  respectively.", "labels": [], "entities": [{"text": "A", "start_pos": 39, "end_pos": 40, "type": "METRIC", "confidence": 0.9827514290809631}, {"text": "F 1", "start_pos": 57, "end_pos": 60, "type": "METRIC", "confidence": 0.9837777614593506}, {"text": "accuracy", "start_pos": 71, "end_pos": 79, "type": "METRIC", "confidence": 0.999510645866394}, {"text": "precision", "start_pos": 81, "end_pos": 90, "type": "METRIC", "confidence": 0.9978540539741516}, {"text": "recall", "start_pos": 92, "end_pos": 98, "type": "METRIC", "confidence": 0.9967492818832397}, {"text": "rejection", "start_pos": 100, "end_pos": 109, "type": "METRIC", "confidence": 0.9556232690811157}, {"text": "F 1 score", "start_pos": 114, "end_pos": 123, "type": "METRIC", "confidence": 0.9890285730361938}]}, {"text": " Table 3: Quantitative characteristics of the datasets", "labels": [], "entities": []}, {"text": " Table 4: Task 5b: Evaluation results for the known  phrases setting", "labels": [], "entities": []}, {"text": " Table 5: Task 5b: Evaluation results for the unseen  phrases setting", "labels": [], "entities": []}]}