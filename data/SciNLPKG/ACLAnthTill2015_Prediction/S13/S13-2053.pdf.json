{"title": [{"text": "NRC-Canada: Building the State-of-the-Art in Sentiment Analysis of Tweets", "labels": [], "entities": [{"text": "NRC-Canada", "start_pos": 0, "end_pos": 10, "type": "DATASET", "confidence": 0.9387435913085938}, {"text": "Sentiment Analysis of Tweets", "start_pos": 45, "end_pos": 73, "type": "TASK", "confidence": 0.8531083166599274}]}], "abstractContent": [{"text": "In this paper, we describe how we created two state-of-the-art SVM classifiers, one to detect the sentiment of messages such as tweets and SMS (message-level task) and one to detect the sentiment of a term within a message (term-level task).", "labels": [], "entities": []}, {"text": "Among submissions from 44 teams in a competition, our submissions stood first in both tasks on tweets, obtaining an F-score of 69.02 in the message-level task and 88.93 in the term-level task.", "labels": [], "entities": [{"text": "F-score", "start_pos": 116, "end_pos": 123, "type": "METRIC", "confidence": 0.9994052648544312}]}, {"text": "We implemented a variety of surface-form, semantic, and sentiment features.", "labels": [], "entities": []}, {"text": "We also generated two large word-sentiment association lexicons , one from tweets with sentiment-word hashtags, and one from tweets with emoticons.", "labels": [], "entities": []}, {"text": "In the message-level task, the lexicon-based features provided again of 5 F-score points overall others.", "labels": [], "entities": [{"text": "F-score", "start_pos": 74, "end_pos": 81, "type": "METRIC", "confidence": 0.9981638789176941}]}, {"text": "Both of our systems can be replicated using freely available resources.", "labels": [], "entities": []}], "introductionContent": [{"text": "Hundreds of millions of people around the world actively use microblogging websites such as Twitter.", "labels": [], "entities": []}, {"text": "Thus there is tremendous interest in sentiment analysis of tweets across a variety of domains such as commerce (), health, and disaster management.", "labels": [], "entities": [{"text": "sentiment analysis of tweets", "start_pos": 37, "end_pos": 65, "type": "TASK", "confidence": 0.9213793575763702}, {"text": "disaster management", "start_pos": 127, "end_pos": 146, "type": "TASK", "confidence": 0.7243023812770844}]}, {"text": "In this paper, we describe how we created two state-of-the-art SVM classifiers, one to detect the sentiment of messages such as tweets and SMS (message-level task) and one to detect the sentiment of a term within a message (term-level task).", "labels": [], "entities": []}, {"text": "The sentiment can be one out of three possibilities: positive, negative, or neutral.", "labels": [], "entities": []}, {"text": "We developed these classifiers to participate in an international competition organized by the Conference on Semantic Evaluation Exercises (SemEval-2013) ().", "labels": [], "entities": []}, {"text": "The organizers created and shared sentiment-labeled tweets for training, development, and testing.", "labels": [], "entities": []}, {"text": "The distributions of the labels in the different datasets is shown in.", "labels": [], "entities": []}, {"text": "The competition, officially referred to as Task 2: Sentiment Analysis in Twitter, had 44 teams (34 for the message-level task and 23 for the term-level task).", "labels": [], "entities": [{"text": "Sentiment Analysis", "start_pos": 51, "end_pos": 69, "type": "TASK", "confidence": 0.9133142232894897}]}, {"text": "Our submissions stood first in both tasks, obtaining a macro-averaged F-score of 69.02 in the message-level task and 88.93 in the term-level task.", "labels": [], "entities": [{"text": "F-score", "start_pos": 70, "end_pos": 77, "type": "METRIC", "confidence": 0.9568545818328857}]}, {"text": "The task organizers also provided a second test dataset, composed of Short Message Service (SMS) messages (no training data of SMS messages was provided).", "labels": [], "entities": []}, {"text": "We applied our classifiers on the SMS test set without any further tuning.", "labels": [], "entities": [{"text": "SMS test set", "start_pos": 34, "end_pos": 46, "type": "DATASET", "confidence": 0.9351639151573181}]}, {"text": "Nonetheless, the classifiers still obtained the first position in identifying sentiment of SMS messages (F-score of 68.46) and second position in detecting the sentiment of terms within SMS messages (F-score of 88.00, only 0.39 points behind the first ranked system).", "labels": [], "entities": [{"text": "F-score", "start_pos": 105, "end_pos": 112, "type": "METRIC", "confidence": 0.9977364540100098}, {"text": "detecting the sentiment of terms within SMS messages", "start_pos": 146, "end_pos": 198, "type": "TASK", "confidence": 0.6946437358856201}, {"text": "F-score", "start_pos": 200, "end_pos": 207, "type": "METRIC", "confidence": 0.9952377080917358}]}, {"text": "We implemented a number of surface-form, semantic, and sentiment features.", "labels": [], "entities": []}, {"text": "We also generated two large word-sentiment association lexicons, one from tweets with sentiment-word hashtags, and one from tweets with emoticons.", "labels": [], "entities": []}, {"text": "The automatically generated lexicons were particularly useful.", "labels": [], "entities": []}, {"text": "In the message-level task for tweets, they alone provided again of more than 5 F-score points over and above that obtained using all other features.", "labels": [], "entities": [{"text": "F-score", "start_pos": 79, "end_pos": 86, "type": "METRIC", "confidence": 0.9987932443618774}]}, {"text": "The lexicons are made freely available.", "labels": [], "entities": []}], "datasetContent": [{"text": "We trained an SVM classifier on the 8,891 annotated terms in tweets (7,756 terms in the training set and 1,135 terms in the development set).", "labels": [], "entities": []}, {"text": "We applied the model to 4,435 terms in the tweets test set.", "labels": [], "entities": [{"text": "tweets test set", "start_pos": 43, "end_pos": 58, "type": "DATASET", "confidence": 0.7509064475695292}]}, {"text": "The same model was applied unchanged to the other test set of 2,334 terms in unseen SMS messages as well.", "labels": [], "entities": []}, {"text": "The bottom-line score used by the task organizers was the macro-averaged F-score of the positive and negative classes.", "labels": [], "entities": [{"text": "F-score", "start_pos": 73, "end_pos": 80, "type": "METRIC", "confidence": 0.9241237640380859}]}, {"text": "The results on the training set (ten-fold crossvalidation), the development set (trained on the training set), and the test sets (trained on the combined set of tweets in the training and development sets) are shown in.", "labels": [], "entities": []}, {"text": "The table also shows baseline results obtained by a majority classifier that always predicts the most frequent class as output, and an additional baseline result obtained using an SVM and unigram features alone.", "labels": [], "entities": []}, {"text": "Our submission obtained a macro-averaged F-score of 88.93 on the tweet set and was ranked first among 29 submissions from 23 participating teams.", "labels": [], "entities": [{"text": "F-score", "start_pos": 41, "end_pos": 48, "type": "METRIC", "confidence": 0.9704769849777222}]}, {"text": "Even with no tuning specific to SMS data, our SMS submission still obtained second rank with an F-score of 88.00.", "labels": [], "entities": [{"text": "F-score", "start_pos": 96, "end_pos": 103, "type": "METRIC", "confidence": 0.9996629953384399}]}, {"text": "The score of the first ranking system on the SMS set was 88.39.", "labels": [], "entities": [{"text": "SMS set", "start_pos": 45, "end_pos": 52, "type": "DATASET", "confidence": 0.7928610444068909}]}, {"text": "A post-competition bug-fix in the bigram features resulted in a small improvement: F-score of 89.10 on the tweets set and 88.34 on the SMS set.", "labels": [], "entities": [{"text": "F-score", "start_pos": 83, "end_pos": 90, "type": "METRIC", "confidence": 0.9996693134307861}]}, {"text": "Note that the performance is significantly higher in the term-level task than in the message-level task.", "labels": [], "entities": []}, {"text": "This is largely because of the ngram features (see unigram baselines in).", "labels": [], "entities": []}, {"text": "We analyzed the labeled data provided to determine why ngrams performed so strongly in this task.", "labels": [], "entities": []}, {"text": "We found that the percentage of test tokens already seen within training data targets was 85.1%.", "labels": [], "entities": []}, {"text": "Further, the average ratio of instances pertaining to the most dominant polarity of a target term to the total number of instances of that target term was 0.808.", "labels": [], "entities": []}, {"text": "Observe that the ngram features were the most useful.", "labels": [], "entities": []}, {"text": "Note also that removing just the word ngram features or just the character ngram features results in only a small drop in performance.", "labels": [], "entities": []}, {"text": "This indicates that the two feature groups capture similar information.", "labels": [], "entities": []}, {"text": "The sentiment lexicon features are the next most useful group-removing them leads to a drop in Fscore of 3.95 points for the tweets set and 4.64 for the SMS set.", "labels": [], "entities": [{"text": "Fscore", "start_pos": 95, "end_pos": 101, "type": "METRIC", "confidence": 0.9991673231124878}]}, {"text": "Modeling negation improves the Fscore by 0.72 points on the tweets set and 1.57 points on the SMS set.", "labels": [], "entities": [{"text": "Modeling negation", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.8921040296554565}, {"text": "Fscore", "start_pos": 31, "end_pos": 37, "type": "METRIC", "confidence": 0.9910117387771606}]}, {"text": "The last two rows in show the results obtained when the features are extracted only from the target (and not from its context) and when they are extracted only from the context of the target (and not from the target itself).", "labels": [], "entities": []}, {"text": "Observe that even though the context may influence the polarity of the target, using target features alone is substantially more  useful than using context features alone.", "labels": [], "entities": []}, {"text": "Nonetheless, adding context features improves the F-scores by roughly 2 to 4 points.", "labels": [], "entities": [{"text": "F-scores", "start_pos": 50, "end_pos": 58, "type": "METRIC", "confidence": 0.9960988759994507}]}], "tableCaptions": [{"text": " Table 2: Message-level Task: The macro-averaged F- scores on different datasets.", "labels": [], "entities": [{"text": "F- scores", "start_pos": 49, "end_pos": 58, "type": "METRIC", "confidence": 0.9403672417004904}]}, {"text": " Table 3: Message-level Task: The macro-averaged F- scores obtained on the test sets with one of the feature  groups removed. The number in the brackets is the dif- ference with the all features score. The biggest drops are  shown in bold.", "labels": [], "entities": [{"text": "F- scores", "start_pos": 49, "end_pos": 58, "type": "METRIC", "confidence": 0.9425478378931681}]}, {"text": " Table 4: Term-level Task: The macro-averaged F-scores  on the datasets. The official scores of our submission are  shown in bold. SVM-all* shows results after a bug fix.", "labels": [], "entities": [{"text": "F-scores", "start_pos": 46, "end_pos": 54, "type": "METRIC", "confidence": 0.9235960245132446}]}, {"text": " Table 5: Term-level Task: The F-scores obtained on the  test sets with one of the feature groups removed. The  number in brackets is the difference with the all features  score. The biggest drops are shown in bold.", "labels": [], "entities": [{"text": "F-scores", "start_pos": 31, "end_pos": 39, "type": "METRIC", "confidence": 0.9897785782814026}]}]}