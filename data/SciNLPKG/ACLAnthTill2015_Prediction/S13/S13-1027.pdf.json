{"title": [{"text": "IBM_EG-CORE: Comparing multiple Lexical and NE matching features in measuring Semantic Textual similarity", "labels": [], "entities": [{"text": "IBM_EG-CORE", "start_pos": 0, "end_pos": 11, "type": "DATASET", "confidence": 0.8280372619628906}, {"text": "Semantic Textual similarity", "start_pos": 78, "end_pos": 105, "type": "TASK", "confidence": 0.6659786701202393}]}], "abstractContent": [{"text": "We present in this paper the systems we participated within the Semantic Textual Similarity task at SEM 2013.", "labels": [], "entities": [{"text": "Semantic Textual Similarity task at SEM 2013", "start_pos": 64, "end_pos": 108, "type": "TASK", "confidence": 0.8353954298155648}]}, {"text": "The Semantic Textual Similarity Core task (STS) computes the degree of semantic equivalence between two sentences where the participant systems will be compared to the manual scores, which range from 5 (semantic equivalence) to 0 (no relation).", "labels": [], "entities": [{"text": "Semantic Textual Similarity Core task (STS)", "start_pos": 4, "end_pos": 47, "type": "TASK", "confidence": 0.7713123001158237}]}, {"text": "We combined multiple text similarity measures of varying complexity.", "labels": [], "entities": []}, {"text": "The experiments illustrate the different effect of four feature types including direct lexical matching, idf-weighted lexical matching, modified BLEU N-gram matching and named entities matching.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 145, "end_pos": 149, "type": "METRIC", "confidence": 0.9923546314239502}]}, {"text": "Our team submitted three runs during the task evaluation period and they ranked number 11, 15 and 19 among the 90 participating systems according to the official Mean Pearson correlation metric for the task.", "labels": [], "entities": [{"text": "Mean Pearson correlation metric", "start_pos": 162, "end_pos": 193, "type": "METRIC", "confidence": 0.91590715944767}]}, {"text": "We also report an unofficial run with mean Pearson correlation of 0.59221 on STS2013 test dataset, ranking as the 3 rd best system among the 90 participating systems.", "labels": [], "entities": [{"text": "Pearson correlation", "start_pos": 43, "end_pos": 62, "type": "METRIC", "confidence": 0.9476388096809387}, {"text": "STS2013 test dataset", "start_pos": 77, "end_pos": 97, "type": "DATASET", "confidence": 0.9619480967521667}]}], "introductionContent": [{"text": "The Semantic Textual Similarity (STS) task at SEM 2013 is to measure the degree of semantic equivalence between pairs of sentences as a graded notion of similarity.", "labels": [], "entities": [{"text": "Semantic Textual Similarity (STS)", "start_pos": 4, "end_pos": 37, "type": "TASK", "confidence": 0.7779263158639272}, {"text": "SEM 2013", "start_pos": 46, "end_pos": 54, "type": "TASK", "confidence": 0.7486160695552826}]}, {"text": "Text Similarity is very important to many Natural Language Processing applications, like extractive summarization (), methods for automatic evaluation of machine translation), as well as text summarization (.", "labels": [], "entities": [{"text": "Text Similarity", "start_pos": 0, "end_pos": 15, "type": "TASK", "confidence": 0.6803265661001205}, {"text": "extractive summarization", "start_pos": 89, "end_pos": 113, "type": "TASK", "confidence": 0.6815708428621292}, {"text": "evaluation of machine translation", "start_pos": 140, "end_pos": 173, "type": "TASK", "confidence": 0.6352407783269882}, {"text": "text summarization", "start_pos": 187, "end_pos": 205, "type": "TASK", "confidence": 0.8304266035556793}]}, {"text": "In Text Coherence Detection (), sentences are linked together by similar or related words.", "labels": [], "entities": [{"text": "Text Coherence Detection", "start_pos": 3, "end_pos": 27, "type": "TASK", "confidence": 0.7279935876528422}]}, {"text": "For Word Sense Disambiguation, researchers) introduced a sense similarity measure using the sentence similarity of the sense definitions.", "labels": [], "entities": [{"text": "Word Sense Disambiguation", "start_pos": 4, "end_pos": 29, "type": "TASK", "confidence": 0.7469607392946879}]}, {"text": "In this paper we illustrate the different effect of four feature types including direct lexical matching, idf-weighted lexical matching, modified BLEU N-gram matching and named entities matching.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 146, "end_pos": 150, "type": "METRIC", "confidence": 0.9910955429077148}]}, {"text": "The rest of this paper will proceed as follows, Section 2 describes the four text similarity features used.", "labels": [], "entities": []}, {"text": "Section 3 illustrates the system description, data resources as well as Feature combination.", "labels": [], "entities": []}, {"text": "Experiments and Results are illustrated in section 4.", "labels": [], "entities": []}, {"text": "then we report our conclusion and future work.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": []}