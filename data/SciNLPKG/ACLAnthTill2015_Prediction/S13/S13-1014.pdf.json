{"title": [{"text": "DeepPurple: Lexical, String and Affective Feature Fusion for Sentence-Level Semantic Similarity Estimation", "labels": [], "entities": [{"text": "DeepPurple", "start_pos": 0, "end_pos": 10, "type": "DATASET", "confidence": 0.9268504977226257}, {"text": "Sentence-Level Semantic Similarity Estimation", "start_pos": 61, "end_pos": 106, "type": "TASK", "confidence": 0.7285965159535408}]}], "abstractContent": [{"text": "This paper describes our submission for the *SEM shared task of Semantic Textual Similarity.", "labels": [], "entities": [{"text": "SEM shared task", "start_pos": 45, "end_pos": 60, "type": "TASK", "confidence": 0.8934920430183411}, {"text": "Semantic Textual Similarity", "start_pos": 64, "end_pos": 91, "type": "TASK", "confidence": 0.7856393655141195}]}, {"text": "We estimate the semantic similarity between two sentences using regression models with features: 1) n-gram hit rates (lexical matches) between sentences, 2) lexical semantic similarity between non-matching words, 3) string similarity metrics, 4) affective content similarity and 5) sentence length.", "labels": [], "entities": []}, {"text": "Domain adaptation is applied in the form of independent models and a model selection strategy achieving a mean correlation of 0.47.", "labels": [], "entities": [{"text": "Domain adaptation", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.7887814044952393}]}], "introductionContent": [{"text": "Text semantic similarity estimation has been an active research area, thanks to a variety of potential applications and the wide availability of data afforded by the worldwide web.", "labels": [], "entities": [{"text": "Text semantic similarity estimation", "start_pos": 0, "end_pos": 35, "type": "TASK", "confidence": 0.8306951075792313}]}, {"text": "Semantic textual similarity (STS) estimates can be used for information extraction, question answering () and machine translation (.", "labels": [], "entities": [{"text": "information extraction", "start_pos": 60, "end_pos": 82, "type": "TASK", "confidence": 0.8278401494026184}, {"text": "question answering", "start_pos": 84, "end_pos": 102, "type": "TASK", "confidence": 0.9049413502216339}, {"text": "machine translation", "start_pos": 110, "end_pos": 129, "type": "TASK", "confidence": 0.8147590160369873}]}, {"text": "Term-level similarity has been successfully applied to problems like grammar induction) and affective text categorization.", "labels": [], "entities": [{"text": "Term-level similarity", "start_pos": 0, "end_pos": 21, "type": "TASK", "confidence": 0.7765121161937714}, {"text": "grammar induction", "start_pos": 69, "end_pos": 86, "type": "TASK", "confidence": 0.7594791352748871}]}, {"text": "In this work, we built on previous research and our submission to) to create a sentence-level STS model for the shared task of *SEM 2013 (.", "labels": [], "entities": [{"text": "*SEM 2013", "start_pos": 127, "end_pos": 136, "type": "DATASET", "confidence": 0.5258628129959106}]}, {"text": "Semantic similarity between words has been well researched, with a variety of knowledge-based) and corpus-based () metrics proposed.", "labels": [], "entities": [{"text": "Semantic similarity between words", "start_pos": 0, "end_pos": 33, "type": "TASK", "confidence": 0.8477384001016617}]}, {"text": "Moving to sentences increases the complexity exponentially and as a result has led to measurements of similarity at various levels: lexical, syntactic, and semantic ().", "labels": [], "entities": [{"text": "complexity", "start_pos": 34, "end_pos": 44, "type": "METRIC", "confidence": 0.9757830500602722}]}, {"text": "Machine translation evaluation metrics can be used to estimate lexical level similarity (), including BLEU (), a metric using word n-gram hit rates.", "labels": [], "entities": [{"text": "Machine translation evaluation", "start_pos": 0, "end_pos": 30, "type": "TASK", "confidence": 0.855146328608195}, {"text": "BLEU", "start_pos": 102, "end_pos": 106, "type": "METRIC", "confidence": 0.998235821723938}]}, {"text": "The pilot task of sentence STS in SemEval 2012) showed a similar trend towards multi-level similarity, with the top performing systems utilizing large amounts of partial similarity metrics and domain adaptation (the use of separate models for each input domain)).", "labels": [], "entities": [{"text": "sentence STS", "start_pos": 18, "end_pos": 30, "type": "TASK", "confidence": 0.814374178647995}, {"text": "domain adaptation", "start_pos": 193, "end_pos": 210, "type": "TASK", "confidence": 0.7046428471803665}]}, {"text": "Our approach is originally motivated by BLEU and primarily utilizes \"hard\" and \"soft\" n-gram hit rates to estimate similarity.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 40, "end_pos": 44, "type": "METRIC", "confidence": 0.9841862320899963}]}, {"text": "Compared to last year, we utilize different alignment strategies (to decide which n-grams should be compared with which).", "labels": [], "entities": []}, {"text": "We also include string similarities (at the token and character level) and similarity of affective content, expressed through the difference in sentence arousal and valence ratings.", "labels": [], "entities": [{"text": "similarity", "start_pos": 75, "end_pos": 85, "type": "METRIC", "confidence": 0.9670087099075317}]}, {"text": "Finally we added domain adaptation: the creation of separate models per domain and a strategy to select the most appropriate model.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 17, "end_pos": 34, "type": "TASK", "confidence": 0.7196086943149567}]}], "datasetContent": [{"text": "Initially all sentences are pre-processed by the CoreNLP ( suite of tools, a process that includes named entity recognition, normalization, part of speech tagging, lemmatization and stemming.", "labels": [], "entities": [{"text": "named entity recognition", "start_pos": 99, "end_pos": 123, "type": "TASK", "confidence": 0.6189511020978292}, {"text": "part of speech tagging", "start_pos": 140, "end_pos": 162, "type": "TASK", "confidence": 0.6404164880514145}]}, {"text": "We evaluated multiple types of preprocessing per unsupervised metric and chose different ones depending on the metric.", "labels": [], "entities": []}, {"text": "Word-level semantic similarities, used for soft comparisons and affective feature extraction, were computed over a corpus of 116 million web snippets collected by posing one query for every word in the Aspell spellchecker (asp, ) vocabulary to the Yahoo!", "labels": [], "entities": [{"text": "affective feature extraction", "start_pos": 64, "end_pos": 92, "type": "TASK", "confidence": 0.732469360033671}]}, {"text": "Word-level emotional ratings in continuous valence and arousal scales were produced by a model trained on the ANEW dataset and using contextual similarities.", "labels": [], "entities": [{"text": "ANEW dataset", "start_pos": 110, "end_pos": 122, "type": "DATASET", "confidence": 0.9825290441513062}]}, {"text": "Finally, string similarities were calculated over the original unmodified sentences.", "labels": [], "entities": []}, {"text": "Next, results are reported in terms of correlation between the generated scores and the ground truth, for each corpus in the shared task, as well as their weighted mean.", "labels": [], "entities": []}, {"text": "Feature selection is applied to the large candidate feature set using a wrapperbased backward selection approach on the training data.The final feature set contains 15 features: soft hit rates calculated over content word 1-to 4-grams (4 features), soft hit rates calculated over unigrams per part-of-speech, for adjectives, nouns, adverbs, verbs (4 features), BLEU unigram hit rates for all words and content words (2 features), skip and containment similarities, containment normalized by sum of sentence lengths or product of sentence lengths (3 features) and affective similarities for arousal and valence (2 features).", "labels": [], "entities": [{"text": "BLEU unigram hit rates", "start_pos": 361, "end_pos": 383, "type": "METRIC", "confidence": 0.8925302177667618}]}, {"text": "Domain adaptation methods are the only difference between the three submitted runs.", "labels": [], "entities": [{"text": "Domain adaptation", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.8143706321716309}]}, {"text": "For all three runs we train one linear model per training set and a fallback model.", "labels": [], "entities": []}, {"text": "For the first run, dubbed linear, the fallback model is linear and model selection during evaluation is performed by filename, therefore results for the OnWN set are produced by a model trained with OnWN data, while the rest are produced by the fallback model.", "labels": [], "entities": [{"text": "OnWN set", "start_pos": 153, "end_pos": 161, "type": "DATASET", "confidence": 0.9407036900520325}, {"text": "OnWN data", "start_pos": 199, "end_pos": 208, "type": "DATASET", "confidence": 0.9296539723873138}]}, {"text": "The second run, dubbed length, uses a hierarchical fallback model and model selection is performed by filename.", "labels": [], "entities": []}, {"text": "The third run, dubbed adapt, uses the same models as the first run and each test set is assigned to a model (i.e., the fallback model is never used).", "labels": [], "entities": []}, {"text": "The test setmodel (training) mapping for this run is: OnWN \u2192 OnWN, headlines \u2192 SMTnews, SMT \u2192 Europarl and FNWN \u2192 OnWN.", "labels": [], "entities": [{"text": "OnWN", "start_pos": 61, "end_pos": 65, "type": "DATASET", "confidence": 0.597305178642273}, {"text": "Europarl", "start_pos": 94, "end_pos": 102, "type": "DATASET", "confidence": 0.813809335231781}]}, {"text": "Results for the linear run using subsets of the final feature set are shown in.", "labels": [], "entities": []}, {"text": "Lexical features (hit rates) are obviously the most valuable features.", "labels": [], "entities": []}, {"text": "String similarities provided us with an improvement in the train-.", "labels": [], "entities": []}, {"text": "Our best run was the simplest one, using a purely linear model and effectively no adaptation.", "labels": [], "entities": []}, {"text": "Adding a more aggressive adaptation strategy improved results in the FNWN and SMT sets, so there is definitely some potential, however the improvement observed is nowhere near that observed in the training data or the same task of SemEval 2012.", "labels": [], "entities": [{"text": "FNWN", "start_pos": 69, "end_pos": 73, "type": "DATASET", "confidence": 0.9271445274353027}, {"text": "SMT", "start_pos": 78, "end_pos": 81, "type": "TASK", "confidence": 0.918910801410675}]}, {"text": "We have to question whether this improvement is an artifact of the rating distributions of these two sets (SMT contains virtually only high ratings, FNWN contains virtually only low ratings): such wild mismatches in priors among training and test sets can be mitigated using more elaborate machine learning algorithms (rather than employing better semantic similarity features or algorithms).", "labels": [], "entities": [{"text": "SMT", "start_pos": 107, "end_pos": 110, "type": "TASK", "confidence": 0.9209927320480347}, {"text": "FNWN", "start_pos": 149, "end_pos": 153, "type": "DATASET", "confidence": 0.818227231502533}]}, {"text": "Overall the system performs well in the two sets containing large similarity rating ranges.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Correlation performance for the linear model us- ing lexical (L), string (S) and affect (A) features  Feature headl. OnWN FNWN SMT mean  L  0.68  0.51  0.23 0.25  0.46  L+S  0.69  0.49  0.23 0.26  0.46  L+S+A  0.69  0.51  0.27 0.28  0.47", "labels": [], "entities": [{"text": "OnWN FNWN SMT mean  L", "start_pos": 127, "end_pos": 148, "type": "DATASET", "confidence": 0.6661324381828309}]}, {"text": " Table 1. Lexical features (hit rates) are  obviously the most valuable features. String similar- ities provided us with an improvement in the train-", "labels": [], "entities": []}, {"text": " Table 2: Correlation performance on the evaluation set.", "labels": [], "entities": []}, {"text": " Table 2. Our best run was the simplest one, using  a purely linear model and effectively no adaptation.  Adding a more aggressive adaptation strategy im- proved results in the FNWN and SMT sets, so there  is definitely some potential, however the improve- ment observed is nowhere near that observed in the  training data or the same task of SemEval 2012. We  have to question whether this improvement is an ar- tifact of the rating distributions of these two sets  (SMT contains virtually only high ratings, FNWN  contains virtually only low ratings): such wild mis- matches in priors among training and test sets can  be mitigated using more elaborate machine learning  algorithms (rather than employing better semantic  similarity features or algorithms). Overall the sys- tem performs well in the two sets containing large  similarity rating ranges.", "labels": [], "entities": [{"text": "FNWN", "start_pos": 177, "end_pos": 181, "type": "DATASET", "confidence": 0.9567853808403015}, {"text": "SMT", "start_pos": 186, "end_pos": 189, "type": "TASK", "confidence": 0.8820977210998535}, {"text": "SMT", "start_pos": 468, "end_pos": 471, "type": "TASK", "confidence": 0.9600593447685242}, {"text": "FNWN", "start_pos": 510, "end_pos": 514, "type": "DATASET", "confidence": 0.9380624294281006}]}]}