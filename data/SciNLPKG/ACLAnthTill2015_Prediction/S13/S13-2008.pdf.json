{"title": [{"text": "HsH: Estimating Semantic Similarity of Words and Short Phrases with Frequency Normalized Distance Measures", "labels": [], "entities": [{"text": "Estimating Semantic Similarity of Words and Short Phrases with Frequency Normalized Distance Measures", "start_pos": 5, "end_pos": 106, "type": "TASK", "confidence": 0.6871643456128927}]}], "abstractContent": [{"text": "This paper describes the approach of the Hochschule Hannover to the SemEval 2013 Task Evaluating Phrasal Semantics.", "labels": [], "entities": [{"text": "SemEval 2013 Task Evaluating Phrasal Semantics", "start_pos": 68, "end_pos": 114, "type": "TASK", "confidence": 0.8989832997322083}]}, {"text": "In order to compare a single word with a two word phrase we compute various distributional similarities , among which anew similarity measure , based on Jensen-Shannon Divergence with a correction for frequency effects.", "labels": [], "entities": []}, {"text": "The classification is done by a support vector machine that uses all similarities as features.", "labels": [], "entities": []}, {"text": "The approach turned out to be the most successful one in the task.", "labels": [], "entities": []}], "introductionContent": [{"text": "The task Evaluating Phrasal Semantics of the 2013 International Workshop on Semantic Evaluation consists of two subtasks.", "labels": [], "entities": [{"text": "Phrasal Semantics of the 2013 International Workshop on Semantic Evaluation", "start_pos": 20, "end_pos": 95, "type": "TASK", "confidence": 0.674940025806427}]}, {"text": "For the first subtask a list of pairs consisting of a single word and a two word phrase are given.", "labels": [], "entities": []}, {"text": "For the English task a labeled list of 11,722 pairs was provided for training and a test set with 3,906 unlabeled examples.", "labels": [], "entities": []}, {"text": "For German the training set contains 2,202 and the test set 732 pairs.", "labels": [], "entities": []}, {"text": "The system should be able to tell whether the two word phrase is a definition of the single word or not.", "labels": [], "entities": []}, {"text": "This task is somewhat different from the usual perspective of finding synonyms, since definitions are usually more general than the words they define.", "labels": [], "entities": []}, {"text": "In distributional semantics words are represented by context vectors and similarities of these context vectors are assumed to reflect similarities of the words they represent.", "labels": [], "entities": []}, {"text": "We compute context vectors for all words using the lemmatized version of the Wacky Corpora for English (UKWaC, approximately 2,2 billion words) and German (DeWaC, 1,7 billion words) ().", "labels": [], "entities": [{"text": "UKWaC", "start_pos": 104, "end_pos": 109, "type": "DATASET", "confidence": 0.8993093371391296}, {"text": "DeWaC", "start_pos": 156, "end_pos": 161, "type": "DATASET", "confidence": 0.9214326739311218}]}, {"text": "For the phrases we compute the context vectors as well directly on the base of occurrences of that phrase, as well as by construction from the context vectors of the two components.", "labels": [], "entities": []}, {"text": "For the similarities between the vectors we use Jensen-Shannon divergence (JSD) and cosine similarity.", "labels": [], "entities": [{"text": "Jensen-Shannon divergence (JSD)", "start_pos": 48, "end_pos": 79, "type": "METRIC", "confidence": 0.6598339855670929}]}, {"text": "Since the JSD is extremely dependent on the number of occurrences of the words, we define anew similarity measure that corrects for this dependency.", "labels": [], "entities": [{"text": "JSD", "start_pos": 10, "end_pos": 13, "type": "TASK", "confidence": 0.6783682107925415}]}, {"text": "Since none of the measures gives satisfactory results, we use all measures to train a support vector machine that classifies the pairs.", "labels": [], "entities": []}, {"text": "The remainder of this paper is organized as follows.", "labels": [], "entities": []}, {"text": "We start with an overview of related work.", "labels": [], "entities": []}, {"text": "In section 3 we discuss the dependence of JSD on word frequency and introduce anew similarity measure.", "labels": [], "entities": []}, {"text": "Section 4 then describes the system.", "labels": [], "entities": []}, {"text": "The results are given in section 5 and are discussed in section 6.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 2: Results for English and German (no names  dataset). Results on train sets are averaged results from  10-fold cross validation. Results on the test set are the  official task results.", "labels": [], "entities": []}, {"text": " Table 3: Results for English train set (average from 10- fold cross validation) using one feature", "labels": [], "entities": [{"text": "English train set", "start_pos": 22, "end_pos": 39, "type": "DATASET", "confidence": 0.9189849297205607}]}]}