{"title": [{"text": "SOFTCARDINALITY-CORE: Improving Text Overlap with Distributional Measures for Semantic Textual Similarity", "labels": [], "entities": [{"text": "Improving Text Overlap", "start_pos": 22, "end_pos": 44, "type": "TASK", "confidence": 0.7845521370569865}, {"text": "Semantic Textual Similarity", "start_pos": 78, "end_pos": 105, "type": "TASK", "confidence": 0.575509657462438}]}], "abstractContent": [{"text": "Soft cardinality has been shown to be a very strong text-overlapping baseline for the task of measuring semantic textual similarity (STS), obtaining 3 rd place in SemEval-2012.", "labels": [], "entities": [{"text": "measuring semantic textual similarity (STS)", "start_pos": 94, "end_pos": 137, "type": "TASK", "confidence": 0.6402799827711922}]}, {"text": "At *SEM-2013 shared task, beside the plain text-overlapping approach, we tested within soft cardinality two distributional word-similarity functions derived from the ukWack corpus.", "labels": [], "entities": [{"text": "SEM-2013 shared task", "start_pos": 4, "end_pos": 24, "type": "TASK", "confidence": 0.6582117279370626}, {"text": "ukWack corpus", "start_pos": 166, "end_pos": 179, "type": "DATASET", "confidence": 0.9916903078556061}]}, {"text": "Unfortunately, we combined these measures with other features using regression, obtaining positions 18 th , 22 nd and 23 rd among the 90 participants systems in the official ranking.", "labels": [], "entities": []}, {"text": "Already after the release of the gold standard annotations of the test data, we observed that using only the similarity measures without combining them with other features would have obtained positions 6 th , 7 th and 8 th ; moreover , an arithmetic average of these similarity measures would have been 4 th (mean=0.5747).", "labels": [], "entities": []}, {"text": "This paper describes both the 3 systems as they were submitted and the similarity measures that would obtained those better results.", "labels": [], "entities": []}], "introductionContent": [{"text": "The task of textual semantic similarity (STS) consists in providing a similarity function on pairs of texts that correlates with human judgments.", "labels": [], "entities": [{"text": "textual semantic similarity (STS)", "start_pos": 12, "end_pos": 45, "type": "TASK", "confidence": 0.8234429905811945}]}, {"text": "Such a function has many practical applications in NLP tasks (e.g. summarization, question answering, textual entailment, paraphrasing, machine translation evaluation, among others), which makes this task particularly important.", "labels": [], "entities": [{"text": "summarization", "start_pos": 67, "end_pos": 80, "type": "TASK", "confidence": 0.9814913272857666}, {"text": "question answering", "start_pos": 82, "end_pos": 100, "type": "TASK", "confidence": 0.8371817767620087}, {"text": "textual entailment", "start_pos": 102, "end_pos": 120, "type": "TASK", "confidence": 0.7033081948757172}, {"text": "machine translation evaluation", "start_pos": 136, "end_pos": 166, "type": "TASK", "confidence": 0.8604857722918192}]}, {"text": "Numerous efforts have been devoted to this task () and major evaluation campaigns have been held at) and in *SEM-2013 ().", "labels": [], "entities": [{"text": "SEM-2013", "start_pos": 109, "end_pos": 117, "type": "TASK", "confidence": 0.4885135889053345}]}, {"text": "The experimental setup of STS in 2012 consisted of three data sets, roughly divided in 50% for training and for testing, which contained text pairs manually annotated as a gold standard.", "labels": [], "entities": []}, {"text": "Furthermore, two data sets were provided for surprise testing.", "labels": [], "entities": []}, {"text": "The measure of performance was the average of the correlations per data set weighted by the number of pairs in each data set (mean).", "labels": [], "entities": []}, {"text": "The best performing systems were UKP () mean=0.6773, TakeLab () mean=0.6753 and soft cardinality ( ) mean=0.6708.", "labels": [], "entities": [{"text": "UKP", "start_pos": 33, "end_pos": 36, "type": "DATASET", "confidence": 0.7735576629638672}, {"text": "TakeLab () mean", "start_pos": 53, "end_pos": 68, "type": "METRIC", "confidence": 0.5742670198281606}]}, {"text": "UKP and TakeLab systems used a large number of resources (see) such as dictionaries, a distributional thesaurus, monolingual corpora, Wikipedia, WordNet, distributional similarity measures, KB similarity, POS tagger, machine learning and others.", "labels": [], "entities": [{"text": "UKP", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.9800043702125549}, {"text": "WordNet", "start_pos": 145, "end_pos": 152, "type": "DATASET", "confidence": 0.908389151096344}, {"text": "POS tagger", "start_pos": 205, "end_pos": 215, "type": "TASK", "confidence": 0.7178240120410919}]}, {"text": "Unlike those systems, the soft cardinality approach used mainly text overlapping and conventional text preprocessing such as removing of stop words, stemming and idf term weighting.", "labels": [], "entities": [{"text": "idf term weighting", "start_pos": 162, "end_pos": 180, "type": "TASK", "confidence": 0.597875694433848}]}, {"text": "This shows that the additional gain in performance from using external resources is small and that the soft cardinality approach is a very challenging baseline for the STS task.", "labels": [], "entities": [{"text": "STS task", "start_pos": 168, "end_pos": 176, "type": "TASK", "confidence": 0.9063154458999634}]}, {"text": "Soft cardinality has been previously shown ) to be also a good baseline for other applications such as information retrieval, entity matching, paraphrase detection and recognizing textual entailment.", "labels": [], "entities": [{"text": "information retrieval", "start_pos": 103, "end_pos": 124, "type": "TASK", "confidence": 0.8064172863960266}, {"text": "entity matching", "start_pos": 126, "end_pos": 141, "type": "TASK", "confidence": 0.8284225165843964}, {"text": "paraphrase detection", "start_pos": 143, "end_pos": 163, "type": "TASK", "confidence": 0.923256129026413}, {"text": "recognizing textual entailment", "start_pos": 168, "end_pos": 198, "type": "TASK", "confidence": 0.8333890438079834}]}, {"text": "Soft cardinality approach to constructing similarity functions ( consists in using any cardinality-based resemblance coefficient (such as Jaccard or Dice) but substituting the classical set cardinality with a softened counting function called soft cardinality.", "labels": [], "entities": []}, {"text": "For example, the soft cardinality of a set containing three very similar elements is close to (though larger than) 1, while for three very different elements it is close to (though less than) 3.", "labels": [], "entities": []}, {"text": "To use the soft cardinality with texts, they are represented as sets of words, and a word-similarity function is used for the soft counting of the words.", "labels": [], "entities": []}, {"text": "For the sake of completeness, we give a brief overview of the soft-cardinality method in Section 3.", "labels": [], "entities": []}, {"text": "The resemblance coefficient used in our participation is a modified version of Tversky's ratio model.", "labels": [], "entities": [{"text": "resemblance coefficient", "start_pos": 4, "end_pos": 27, "type": "METRIC", "confidence": 0.981256753206253}]}, {"text": "Apart from the two parameters of this coefficient, anew parameter was included and functions max and min were used to make it symmetrical.", "labels": [], "entities": []}, {"text": "The rationale for this new coefficient is given in Section 2.", "labels": [], "entities": []}, {"text": "Three word similarity features used in our systems are described in Section 4.", "labels": [], "entities": []}, {"text": "The one is a measure of character q-gram overlapping, which reuses the coefficient proposed in Section 2; this measure is described in subsection 4.1.", "labels": [], "entities": []}, {"text": "The other two ones are distributional measures obtained from the ukWack corpus (, which is a collection of web-crawled documents containing about 1.9 billion words in English.", "labels": [], "entities": [{"text": "ukWack corpus", "start_pos": 65, "end_pos": 78, "type": "DATASET", "confidence": 0.992855966091156}]}, {"text": "The second measure is, again, a reuse of the coefficient specified in Section 2, but using instead sets of occurrences (and co-occurrences) of words in sentences in the ukWack corpus; this measure is described in subsection 4.2.", "labels": [], "entities": [{"text": "ukWack corpus", "start_pos": 169, "end_pos": 182, "type": "DATASET", "confidence": 0.9929446876049042}]}, {"text": "Finally, the third one, which is a normalized version of pointwise mutual information (PMI), is described in subsection 4.3.", "labels": [], "entities": [{"text": "pointwise mutual information (PMI)", "start_pos": 57, "end_pos": 91, "type": "TASK", "confidence": 0.673280249039332}]}, {"text": "The parameters of the three text-similarity functions derived from the combination of the proposed coefficient of resemblance (Section 2), the soft cardinality (Section 3) and the three word-similarity measures (Section 4) were adjusted to maximize the correlation with the 2012 STS gold standard data.", "labels": [], "entities": [{"text": "2012 STS gold standard data", "start_pos": 274, "end_pos": 301, "type": "DATASET", "confidence": 0.8432555794715881}]}, {"text": "At this point, these soft-cardinality similarity functions can provide predictions for the test data.", "labels": [], "entities": []}, {"text": "However, we decided to test the approach of learning a resemblance function from the training data instead of using a preset resemblance coefficient.", "labels": [], "entities": []}, {"text": "Basically, most resemblance coefficients are ternary functions F (x, y, z) where x = |A|, y = |B| and z = |A\u2229B|: e.g. Dice coefficient is F (x, y, z) = 2z /x+y and Jaccard is F (x, y, z) = z /x+y\u2212z.", "labels": [], "entities": [{"text": "Dice coefficient", "start_pos": 118, "end_pos": 134, "type": "METRIC", "confidence": 0.8790430128574371}, {"text": "F", "start_pos": 175, "end_pos": 176, "type": "METRIC", "confidence": 0.9146738052368164}]}, {"text": "Thus, this function can be learned using a regression model, providing cardinalities x, y and z as features and the gold standard value as the target function.", "labels": [], "entities": []}, {"text": "The results obtained for the text-similarity functions and the regression approach are presented in Section 7.", "labels": [], "entities": []}, {"text": "Unfortunately, when using a regressor trained with 2012 STS data and tested with 2013 surprise data we observed that the results worsened rather than improved.", "labels": [], "entities": [{"text": "STS data", "start_pos": 56, "end_pos": 64, "type": "DATASET", "confidence": 0.656920775771141}]}, {"text": "A short explanation of this is overfitting.", "labels": [], "entities": []}, {"text": "A more detailed discussion of this, together with an assessment of the performance gain obtained by the use of distributional measures is provided in Section 8.", "labels": [], "entities": []}, {"text": "Finally, in Section 9 the conclusions of our participation in this evaluation campaign are presented.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 2: Unofficial results using text-similarity functions", "labels": [], "entities": []}, {"text": " Table 3: Unofficial results using linear regression", "labels": [], "entities": []}]}