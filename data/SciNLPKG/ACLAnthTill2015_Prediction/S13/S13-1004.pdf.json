{"title": [{"text": "*SEM 2013 shared task: Semantic Textual Similarity", "labels": [], "entities": [{"text": "SEM 2013 shared task", "start_pos": 1, "end_pos": 21, "type": "TASK", "confidence": 0.8354911059141159}, {"text": "Semantic Textual Similarity", "start_pos": 23, "end_pos": 50, "type": "TASK", "confidence": 0.6979275445143381}]}], "abstractContent": [{"text": "In Semantic Textual Similarity (STS), systems rate the degree of semantic equivalence, on a graded scale from 0 to 5, with 5 being the most similar.", "labels": [], "entities": [{"text": "Semantic Textual Similarity (STS)", "start_pos": 3, "end_pos": 36, "type": "TASK", "confidence": 0.7912297149499258}]}, {"text": "This year we setup two tasks: (i) a core task (CORE), and (ii) a typed-similarity task (TYPED).", "labels": [], "entities": [{"text": "CORE", "start_pos": 47, "end_pos": 51, "type": "METRIC", "confidence": 0.8814712762832642}]}, {"text": "CORE is similar in setup to SemEval STS 2012 task with pairs of sentences from sources related to those of 2012, yet different in genre from the 2012 set, namely, this year we included newswire headlines, machine translation evaluation datasets and multiple lexical resource glossed sets.", "labels": [], "entities": [{"text": "CORE", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.5006576776504517}, {"text": "SemEval STS 2012 task", "start_pos": 28, "end_pos": 49, "type": "TASK", "confidence": 0.732532262802124}, {"text": "machine translation evaluation", "start_pos": 205, "end_pos": 235, "type": "TASK", "confidence": 0.7485970358053843}]}, {"text": "TYPED, on the other hand, is novel and tries to characterize why two items are deemed similar, using cultural heritage items which are described with metadata such as title, author or description.", "labels": [], "entities": [{"text": "TYPED", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.6755805015563965}]}, {"text": "Several types of similarity have been defined, including similar author, similar time period or similar location.", "labels": [], "entities": []}, {"text": "The annotation for both tasks leverages crowdsourcing, with relative high inter-annotator correlation, ranging from 62% to 87%.", "labels": [], "entities": [{"text": "inter-annotator correlation", "start_pos": 74, "end_pos": 101, "type": "METRIC", "confidence": 0.6795922517776489}]}, {"text": "The CORE task attracted 34 participants with 89 runs, and the TYPED task attracted 6 teams with 14 runs.", "labels": [], "entities": [{"text": "CORE", "start_pos": 4, "end_pos": 8, "type": "DATASET", "confidence": 0.4013579487800598}]}], "introductionContent": [{"text": "Given two snippets of text, Semantic Textual Similarity (STS) captures the notion that some texts are more similar than others, measuring the degree of semantic equivalence.", "labels": [], "entities": [{"text": "Semantic Textual Similarity (STS)", "start_pos": 28, "end_pos": 61, "type": "TASK", "confidence": 0.7221216708421707}]}, {"text": "Textual similarity can range from exact semantic equivalence to complete unrelatedness, corresponding to quantified values between 5 and 0.", "labels": [], "entities": []}, {"text": "The graded similarity intuitively captures the notion of intermediate shades of similarity such as pairs of text differ only in some minor nuanced aspects of meaning only, to relatively important differences in meaning, to sharing only some details, or to simply being related to the same topic, as shown in.", "labels": [], "entities": []}, {"text": "One of the goals of the STS task is to create a unified framework for combining several semantic components that otherwise have historically tended to be evaluated independently and without characterization of impact on NLP applications.", "labels": [], "entities": []}, {"text": "By providing such a framework, STS will allow for an extrinsic evaluation for these modules.", "labels": [], "entities": [{"text": "STS", "start_pos": 31, "end_pos": 34, "type": "TASK", "confidence": 0.8870797753334045}]}, {"text": "Moreover, this STS framework itself could in turn be evaluated intrinsically and extrinsically as a grey/black box within various NLP applications such as Machine Translation (MT), Summarization, Generation, Question Answering (QA), etc.", "labels": [], "entities": [{"text": "Machine Translation (MT)", "start_pos": 155, "end_pos": 179, "type": "TASK", "confidence": 0.8340827465057373}, {"text": "Summarization, Generation, Question Answering (QA)", "start_pos": 181, "end_pos": 231, "type": "TASK", "confidence": 0.7621949215730032}]}, {"text": "STS is related to both Textual Entailment (TE) and Paraphrasing, but differs in a number of ways and it is more directly applicable to a number of NLP tasks.", "labels": [], "entities": [{"text": "Textual Entailment (TE)", "start_pos": 23, "end_pos": 46, "type": "TASK", "confidence": 0.7907772779464721}]}, {"text": "STS is different from TE inasmuch as it assumes bidirectional graded equivalence between the pair of textual snippets.", "labels": [], "entities": [{"text": "STS", "start_pos": 0, "end_pos": 3, "type": "TASK", "confidence": 0.5805178880691528}]}, {"text": "In the case of TE the equivalence is directional, e.g. a car is a vehicle, but a vehicle is not necessarily a car.", "labels": [], "entities": []}, {"text": "STS also differs from both TE and Paraphrasing (in as far as both tasks have been defined to date in the literature) in that, rather than being a binary yes/no decision (e.g. a vehicle is not a car), we define STS to be a graded similarity notion (e.g. a vehicle and a car are more similar than a wave and a car).", "labels": [], "entities": []}, {"text": "A quantifiable graded bidirectional notion of textual similarity is useful fora myriad of NLP tasks such as MT evaluation, information extraction, question answering, summarization, etc.", "labels": [], "entities": [{"text": "MT evaluation", "start_pos": 108, "end_pos": 121, "type": "TASK", "confidence": 0.9834253787994385}, {"text": "information extraction", "start_pos": 123, "end_pos": 145, "type": "TASK", "confidence": 0.8390674591064453}, {"text": "question answering", "start_pos": 147, "end_pos": 165, "type": "TASK", "confidence": 0.8834094405174255}, {"text": "summarization", "start_pos": 167, "end_pos": 180, "type": "TASK", "confidence": 0.958956241607666}]}, {"text": "\u2022 (5) The two sentences are completely equivalent, as they mean the same thing.", "labels": [], "entities": []}, {"text": "The bird is bathing in the sink.", "labels": [], "entities": []}, {"text": "Birdie is washing itself in the water basin.", "labels": [], "entities": [{"text": "Birdie", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.9051653146743774}]}, {"text": "\u2022 (4) The two sentences are mostly equivalent, but some unimportant details differ.", "labels": [], "entities": []}, {"text": "In May 2010, the troops attempted to invade Kabul.", "labels": [], "entities": []}, {"text": "The US army invaded Kabul on May 7th last year, 2010.", "labels": [], "entities": [{"text": "Kabul on May 7th last year", "start_pos": 20, "end_pos": 46, "type": "DATASET", "confidence": 0.8238009115060171}]}, {"text": "\u2022 (3) The two sentences are roughly equivalent, but some important information differs/missing.", "labels": [], "entities": []}, {"text": "John said he is considered a witness but not a suspect.", "labels": [], "entities": []}, {"text": "\"He is not a suspect anymore.\"", "labels": [], "entities": []}, {"text": "\u2022 (2) The two sentences are not equivalent, but share some details.", "labels": [], "entities": []}, {"text": "They flew out of the nest in groups.", "labels": [], "entities": []}, {"text": "They flew into the nest together.", "labels": [], "entities": []}, {"text": "\u2022 (1) The two sentences are not equivalent, but are on the same topic.", "labels": [], "entities": []}, {"text": "The woman is playing the violin.", "labels": [], "entities": []}, {"text": "The young lady enjoys listening to the guitar.", "labels": [], "entities": []}, {"text": "\u2022 (0) The two sentences are on different topics.", "labels": [], "entities": []}, {"text": "John went horseback riding at dawn with a whole group of friends.", "labels": [], "entities": [{"text": "horseback riding", "start_pos": 10, "end_pos": 26, "type": "TASK", "confidence": 0.8129718601703644}]}, {"text": "Sunrise at dawn is a magnificent view to take in if you wake up early enough for it.", "labels": [], "entities": []}, {"text": "In 2012 we held the first pilot task at SemEval 2012, as part of the *SEM 2012 conference, with great success: 35 teams participated with 88 system runs).", "labels": [], "entities": [{"text": "SEM 2012 conference", "start_pos": 70, "end_pos": 89, "type": "TASK", "confidence": 0.5766448577245077}]}, {"text": "In addition, we held a DARPA sponsored workshop at Columbia University . In 2013, STS was selected as the official Shared Task of the *SEM 2013 conference.", "labels": [], "entities": [{"text": "SEM 2013 conference", "start_pos": 135, "end_pos": 154, "type": "TASK", "confidence": 0.5206741591294607}]}, {"text": "Accordingly, in STS 2013, we setup two tasks: The core task CORE, which is similar to the 2012 task; and a pilot task on typed-similarity TYPED between semi-structured records.", "labels": [], "entities": [{"text": "CORE", "start_pos": 60, "end_pos": 64, "type": "METRIC", "confidence": 0.9784436225891113}]}, {"text": "For CORE, we provided all the STS 2012 data as training data, and the test data was drawn from related but different datasets.", "labels": [], "entities": [{"text": "CORE", "start_pos": 4, "end_pos": 8, "type": "DATASET", "confidence": 0.8394030332565308}, {"text": "STS 2012 data", "start_pos": 30, "end_pos": 43, "type": "DATASET", "confidence": 0.7957698702812195}]}, {"text": "This is in contrast to the STS 2012 task where the train/test data were drawn from the same datasets.", "labels": [], "entities": [{"text": "STS 2012 task", "start_pos": 27, "end_pos": 40, "type": "TASK", "confidence": 0.592182477315267}]}, {"text": "The 2012 datasets comprised the following: pairs of sentences from paraphrase datasets from news and video elicitation (MSRpar and MSRvid), machine translation evaluation data (SMTeuroparl, SMTnews) and pairs of glosses (OnWN).", "labels": [], "entities": [{"text": "machine translation evaluation", "start_pos": 140, "end_pos": 170, "type": "TASK", "confidence": 0.7453464070955912}]}, {"text": "The current STS 2013 dataset comprises the following: pairs of news headlines, SMT evaluation sentences (SMT) and pairs of glosses.", "labels": [], "entities": [{"text": "STS 2013 dataset", "start_pos": 12, "end_pos": 28, "type": "DATASET", "confidence": 0.8014965852101644}, {"text": "SMT evaluation sentences (SMT)", "start_pos": 79, "end_pos": 109, "type": "TASK", "confidence": 0.8532873789469401}]}, {"text": "The typed-similarity pilot task TYPED attempts to characterize, for the first time, the reason and/or type of similarity.", "labels": [], "entities": []}, {"text": "STS reduces the problem of judging similarity to a single number, but, in some applications, it is important to characterize why and how two items are deemed similar, hence the added nuance.", "labels": [], "entities": [{"text": "STS", "start_pos": 0, "end_pos": 3, "type": "TASK", "confidence": 0.9137737154960632}]}, {"text": "The dataset comprises pairs of Cultural Heritage items from Europeana, 2 a single access point to millions of books, paintings, films, museum objects and archival records that have been digitized throughout Europe.", "labels": [], "entities": [{"text": "Cultural Heritage items from Europeana", "start_pos": 31, "end_pos": 69, "type": "DATASET", "confidence": 0.6624851644039154}]}, {"text": "It is an authoritative source of information coming from European cultural and scientific institutions.", "labels": [], "entities": []}, {"text": "Typically, the items comprise meta-data describing a cultural heritage item and, sometimes, a thumbnail of the item itself.", "labels": [], "entities": []}, {"text": "Participating systems in the TYPED task need to compute the similarity between items, using the textual meta-data.", "labels": [], "entities": [{"text": "TYPED task", "start_pos": 29, "end_pos": 39, "type": "TASK", "confidence": 0.8511348068714142}]}, {"text": "In addition to general similarity, participants need to score specific kinds of similarity, like similar author, similar time period, etc.", "labels": [], "entities": []}, {"text": "The paper is structured as follows.", "labels": [], "entities": []}, {"text": "Section 2 reports the sources of the texts used in the two tasks.", "labels": [], "entities": []}, {"text": "Section 3 details the annotation procedure.", "labels": [], "entities": []}, {"text": "Section 4 presents the evaluation of the systems, followed by the results of CORE and TYPED tasks.", "labels": [], "entities": [{"text": "CORE", "start_pos": 77, "end_pos": 81, "type": "METRIC", "confidence": 0.5266669392585754}, {"text": "TYPED", "start_pos": 86, "end_pos": 91, "type": "METRIC", "confidence": 0.8872642517089844}]}, {"text": "Section 6 draws on some conclusions and forward projections.", "labels": [], "entities": []}], "datasetContent": [{"text": "Evaluation of STS is still an open issue.", "labels": [], "entities": [{"text": "STS", "start_pos": 14, "end_pos": 17, "type": "TASK", "confidence": 0.9191918969154358}]}, {"text": "STS experiments have traditionally used Pearson productmoment correlation, or, alternatively, Spearman rank order correlation.", "labels": [], "entities": [{"text": "STS", "start_pos": 0, "end_pos": 3, "type": "TASK", "confidence": 0.903115451335907}, {"text": "Pearson productmoment correlation", "start_pos": 40, "end_pos": 73, "type": "METRIC", "confidence": 0.7300759255886078}, {"text": "Spearman rank order correlation", "start_pos": 94, "end_pos": 125, "type": "METRIC", "confidence": 0.5432775393128395}]}, {"text": "In addition, we also need a method to aggregate the results from each dataset into an overall score.", "labels": [], "entities": []}, {"text": "The analysis performed in shows that Pearson and averaging across datasets are the best suited combination in general.", "labels": [], "entities": [{"text": "Pearson", "start_pos": 37, "end_pos": 44, "type": "METRIC", "confidence": 0.9932286143302917}]}, {"text": "In particular, Pearson is more informative than Spearman, in that Spearman only takes the rank differences into account, while Pearson does account for value differences as well.", "labels": [], "entities": []}, {"text": "The study also showed that other alternatives need to be considered, depending on the requirements of the target application.", "labels": [], "entities": []}, {"text": "We leave application-dependent evaluations for future work, and focus on average weighted Pearson correlation.", "labels": [], "entities": [{"text": "Pearson correlation", "start_pos": 90, "end_pos": 109, "type": "METRIC", "confidence": 0.9385429918766022}]}, {"text": "When averaging, we weight each individual correlation by the size of the dataset.", "labels": [], "entities": []}, {"text": "In addition, participants in the CORE task are allowed to provide a confidence score between 1 and 100 for each of their scores.", "labels": [], "entities": [{"text": "CORE task", "start_pos": 33, "end_pos": 42, "type": "TASK", "confidence": 0.6966458857059479}]}, {"text": "The evaluation script down-weights the pairs with low confidence, following weighted Pearson.", "labels": [], "entities": [{"text": "Pearson", "start_pos": 85, "end_pos": 92, "type": "METRIC", "confidence": 0.9210404753684998}]}, {"text": "In order to compute statistical significance among system results, we use a one-tailed parametric test based on Fisher's ztransformation (, equation 14.5.10).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Results on the CORE task. The first rows on the left correspond to the baseline and to two publicly available  systems, see text for details. Note:  \u2020 signals team involving one of the organizers,  \u2021 for systems submitting past the  120 hour window.", "labels": [], "entities": [{"text": "CORE task", "start_pos": 25, "end_pos": 34, "type": "TASK", "confidence": 0.7375083267688751}]}, {"text": " Table 3: Results on TYPED task. The first row corresponds to the baseline. Note:  \u2020 signals team involving one of the  organizers.", "labels": [], "entities": []}, {"text": " Table 4: CORE task: Resources and tools used by the systems that submitted a description file. Leftmost columns  correspond to the resources, and rightmost to tools, in alphabetic order.", "labels": [], "entities": []}]}