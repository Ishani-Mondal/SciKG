{"title": [{"text": "SemEval-2013 Task 1: TEMPEVAL-3: Evaluating Time Expressions, Events, and Temporal Relations", "labels": [], "entities": [{"text": "TEMPEVAL-3", "start_pos": 21, "end_pos": 31, "type": "METRIC", "confidence": 0.7607077956199646}, {"text": "Evaluating Time Expressions, Events, and Temporal Relations", "start_pos": 33, "end_pos": 92, "type": "TASK", "confidence": 0.730427645974689}]}], "abstractContent": [{"text": "Within the SemEval-2013 evaluation exercise, the TempEval-3 shared task aims to advance research on temporal information processing.", "labels": [], "entities": [{"text": "SemEval-2013 evaluation exercise", "start_pos": 11, "end_pos": 43, "type": "TASK", "confidence": 0.8260319828987122}, {"text": "temporal information processing", "start_pos": 100, "end_pos": 131, "type": "TASK", "confidence": 0.6211301982402802}]}, {"text": "It follows on from TempEval-1 and-2, with: a three-part structure covering temporal expression, event, and temporal relation extraction; a larger dataset; and new single measures to rank systems-in each task and in general.", "labels": [], "entities": [{"text": "temporal relation extraction", "start_pos": 107, "end_pos": 135, "type": "TASK", "confidence": 0.6362591485182444}]}, {"text": "In this paper, we describe the participants' approaches, results, and the observations from the results, which may guide future research in this area.", "labels": [], "entities": []}], "introductionContent": [{"text": "The TempEval task) was added as anew task in.", "labels": [], "entities": []}, {"text": "The ultimate aim of research in this area is the automatic identification of temporal expressions (timexes), events, and temporal relations within a text as specified in TimeML annotation ().", "labels": [], "entities": [{"text": "automatic identification of temporal expressions (timexes), events", "start_pos": 49, "end_pos": 115, "type": "TASK", "confidence": 0.7319534540176391}]}, {"text": "However, since addressing this aim in a first evaluation challenge was deemed too difficult a staged approach was suggested.", "labels": [], "entities": []}, {"text": "TempEval (henceforth TempEval-1) was an initial evaluation exercise focusing only on the categorization of temporal relations and only in English.", "labels": [], "entities": []}, {"text": "It included three relation types: event-timex, event-dct, 1 and relations between main events in consecutive sentences.", "labels": [], "entities": []}, {"text": "TempEval-2 () extended TempEval-1, growing into a multilingual task, and consisting of six subtasks rather than three.", "labels": [], "entities": []}, {"text": "This included event and timex extraction, as well as the three relation tasks from TempEval-1, with the addition of a relation task where one event subordinates another.", "labels": [], "entities": [{"text": "event and timex extraction", "start_pos": 14, "end_pos": 40, "type": "TASK", "confidence": 0.595846101641655}]}, {"text": "TempEval-3 () is a follow-up to TempEval 1 and 2, covering English and Spanish.", "labels": [], "entities": [{"text": "TempEval-3", "start_pos": 0, "end_pos": 10, "type": "DATASET", "confidence": 0.890331506729126}]}, {"text": "TempEval-3 is different from its predecessors in a few respects:", "labels": [], "entities": []}], "datasetContent": [{"text": "The metrics used to evaluate the participants are:  The aim of this evaluation is to provide a meaningful report of the performance obtained by the participants in the tasks defined in Section 3.", "labels": [], "entities": []}, {"text": "Furthermore, the results include TIPSem as reference for comparison.", "labels": [], "entities": [{"text": "TIPSem", "start_pos": 33, "end_pos": 39, "type": "METRIC", "confidence": 0.8388789892196655}]}, {"text": "This was used as a pre-annotation system in some cases.", "labels": [], "entities": []}, {"text": "TIPSem obtained the best results in event processing task in TempEval-2 and offered very competitive results in timex and relation processing.", "labels": [], "entities": [{"text": "TIPSem", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.751335084438324}, {"text": "event processing task", "start_pos": 36, "end_pos": 57, "type": "TASK", "confidence": 0.8111894130706787}, {"text": "relation processing", "start_pos": 122, "end_pos": 141, "type": "TASK", "confidence": 0.8788745105266571}]}, {"text": "The best timex processing system in TempEval-2 (HeidelTime) is participating in this edition as well, therefore we included TIPSem as a reference in all tasks.", "labels": [], "entities": [{"text": "TIPSem", "start_pos": 124, "end_pos": 130, "type": "METRIC", "confidence": 0.7167902588844299}]}, {"text": "We only report results in main measures.", "labels": [], "entities": []}, {"text": "Results are divided by language and shown per task.", "labels": [], "entities": []}, {"text": "Detailed scores can be found on the task website.", "labels": [], "entities": [{"text": "Detailed", "start_pos": 0, "end_pos": 8, "type": "METRIC", "confidence": 0.8731394410133362}]}, {"text": "For complete temporal annotation from raw text (Task ABC -Task C from raw text) and for temporal relation only tasks (Task C, Task C relation only), we had five participants in total.", "labels": [], "entities": []}, {"text": "For relation evaluation, we primarily evaluate on Task ABC (Task C from raw text), which requires joint entity extraction, link identification and relation classification.", "labels": [], "entities": [{"text": "relation evaluation", "start_pos": 4, "end_pos": 23, "type": "TASK", "confidence": 0.9481816589832306}, {"text": "joint entity extraction", "start_pos": 98, "end_pos": 121, "type": "TASK", "confidence": 0.6611867845058441}, {"text": "link identification", "start_pos": 123, "end_pos": 142, "type": "TASK", "confidence": 0.6942544728517532}, {"text": "relation classification", "start_pos": 147, "end_pos": 170, "type": "TASK", "confidence": 0.7365356832742691}]}, {"text": "The results for this task can be found in.", "labels": [], "entities": []}, {"text": "While TIPSem obtained the best results in task ABC, especially in recall, it was used by some annotators to pre-label data.", "labels": [], "entities": [{"text": "TIPSem", "start_pos": 6, "end_pos": 12, "type": "METRIC", "confidence": 0.9499486684799194}, {"text": "recall", "start_pos": 66, "end_pos": 72, "type": "METRIC", "confidence": 0.9929497241973877}]}, {"text": "In the interest of rigour and fairness, we separate out this system.", "labels": [], "entities": [{"text": "fairness", "start_pos": 30, "end_pos": 38, "type": "METRIC", "confidence": 0.98207688331604}]}, {"text": "For task C, for provided participants with entities and participants identified: between which entity pairs a relation exists (link identification); and the class of that relation.", "labels": [], "entities": []}, {"text": "We also evaluate the participants on the relation by providing the entities and the links (performance in) -TIPSem could not be evaluated in this setting since the system is not prepared to do categorization only unless the relations are divided as in TempEval-2.", "labels": [], "entities": []}, {"text": "For these Task C related tasks, we had only one new participant, who didn't participate in Task A and B: UTTime.", "labels": [], "entities": [{"text": "UTTime", "start_pos": 105, "end_pos": 111, "type": "DATASET", "confidence": 0.742352306842804}]}, {"text": "Identifying which pair of entities to consider for temporal relations is anew task in this TempEval challenge.", "labels": [], "entities": []}, {"text": "The participants approached the problems in data-driven, rule-based and also in hybrid ways (   the other hand, all the participants used data-driven approaches for temporal relations.", "labels": [], "entities": []}, {"text": "Observations: We collected the following observations from the results and from participants' experiments.", "labels": [], "entities": [{"text": "Observations", "start_pos": 0, "end_pos": 12, "type": "METRIC", "confidence": 0.9675129055976868}]}, {"text": "Strategy: For relation classification, all participants used partially or fully machine learning-based systems.", "labels": [], "entities": [{"text": "relation classification", "start_pos": 14, "end_pos": 37, "type": "TASK", "confidence": 0.9671204388141632}]}, {"text": "Data: None of the participants implemented their systems training on the silver data.", "labels": [], "entities": []}, {"text": "Most of the systems use the combined TimeBank and AQUAINT (TBAQ) corpus.", "labels": [], "entities": [{"text": "AQUAINT (TBAQ) corpus", "start_pos": 50, "end_pos": 71, "type": "DATASET", "confidence": 0.6687782287597657}]}, {"text": "Data: Adding additional high-quality relations, either Philippe Muller's closure-based inferences or the verb clause relations from, typically increased recall and the overall performance (ClearTK runs two and four).", "labels": [], "entities": [{"text": "recall", "start_pos": 153, "end_pos": 159, "type": "METRIC", "confidence": 0.9992889165878296}]}, {"text": "Features: Participants mostly used the morphosyntactic and lexical semantic information.", "labels": [], "entities": []}, {"text": "The best performing systems from TempEval-2 (TIPSem and TRIOS) additionally used sentence level semantic information.", "labels": [], "entities": []}, {"text": "One participant in TempEval-3 (UTTime) also did deep parsing for the sentence level semantic features.", "labels": [], "entities": [{"text": "deep parsing", "start_pos": 48, "end_pos": 60, "type": "TASK", "confidence": 0.6621532291173935}]}, {"text": "Features: Using more Linguistic knowledge is important for the task, but it is more important to execute it properly.", "labels": [], "entities": []}, {"text": "Many systems performed better using less linguistic knowledge.", "labels": [], "entities": []}, {"text": "Hence a system (e.g. ClearTK) with basic morphosyntactic features is hard to beat with more semantic features, if not used properly.", "labels": [], "entities": []}, {"text": "Classifier: Across the various tasks, ClearTK tried Mallet CRF, Mallet MaxEnt, OpenNLP MaxEnt, and LI-BLINEAR (SVMs and logistic regression).", "labels": [], "entities": [{"text": "Mallet CRF", "start_pos": 52, "end_pos": 62, "type": "DATASET", "confidence": 0.9190907776355743}]}, {"text": "They picked the final classifiers by running a grid search over models and parameters on the training data, and for all tasks, a LIBLINEAR model was at least as good as all the other models.", "labels": [], "entities": [{"text": "LIBLINEAR", "start_pos": 129, "end_pos": 138, "type": "METRIC", "confidence": 0.9912403225898743}]}, {"text": "As an added bonus, it was way faster to train than most of the other models.", "labels": [], "entities": []}, {"text": "There were two participants for Spanish.", "labels": [], "entities": []}, {"text": "Both participated in task A and only one of them in task B.", "labels": [], "entities": []}, {"text": "In this) to obtain the linguistic features automatically.", "labels": [], "entities": []}, {"text": "shows the results obtained for task A.", "labels": [], "entities": []}, {"text": "As it can be observed HeidelTime obtains the best results.", "labels": [], "entities": []}, {"text": "It improves the previous state-of-the-art results (TIPSemB-F), especially in normalization (value F1).", "labels": [], "entities": [{"text": "TIPSemB-F", "start_pos": 51, "end_pos": 60, "type": "METRIC", "confidence": 0.9902262687683105}, {"text": "F1", "start_pos": 98, "end_pos": 100, "type": "METRIC", "confidence": 0.8774919509887695}]}, {"text": "shows the results from event extraction.", "labels": [], "entities": [{"text": "event extraction", "start_pos": 23, "end_pos": 39, "type": "TASK", "confidence": 0.7239051461219788}]}, {"text": "In this case, the previous state-of-the-art is not improved.", "labels": [], "entities": []}, {"text": "only shows the results obtained in temporal awareness by the state-of-the-art system since there were not participants on this task.", "labels": [], "entities": []}, {"text": "We observe that TIPSemB-F approach offers competitive results, which is comparable to results obtained in TE3 English test set.", "labels": [], "entities": [{"text": "TE3 English test set", "start_pos": 106, "end_pos": 126, "type": "DATASET", "confidence": 0.9353282749652863}]}], "tableCaptions": [{"text": " Table 1: Platinum corpus entity inter-annotator agreement.", "labels": [], "entities": [{"text": "Platinum corpus entity inter-annotator agreement", "start_pos": 10, "end_pos": 58, "type": "DATASET", "confidence": 0.8959328770637512}]}, {"text": " Table 3: Task A -Temporal Expression Performance.", "labels": [], "entities": []}, {"text": " Table 6: Task B -Event Extraction Performance.", "labels": [], "entities": [{"text": "Event Extraction Performance", "start_pos": 18, "end_pos": 46, "type": "TASK", "confidence": 0.6717142661412557}]}, {"text": " Table 7: Task ABC -Temporal Awareness Evaluation (Task C  evaluation from raw text).", "labels": [], "entities": []}, {"text": " Table 8: Task C -TLINK Identification and Classification.", "labels": [], "entities": [{"text": "Task C -TLINK Identification and Classification", "start_pos": 10, "end_pos": 57, "type": "TASK", "confidence": 0.5583087035587856}]}, {"text": " Table 9: Task C -relation only: Relation Classification.", "labels": [], "entities": [{"text": "Relation Classification", "start_pos": 33, "end_pos": 56, "type": "TASK", "confidence": 0.8144506812095642}]}, {"text": " Table 12: Task A: Temporal Expression (Spanish).", "labels": [], "entities": [{"text": "Temporal Expression (Spanish)", "start_pos": 19, "end_pos": 48, "type": "TASK", "confidence": 0.8588998913764954}]}, {"text": " Table 13: Task B: Event Extraction (Spanish).", "labels": [], "entities": [{"text": "Event Extraction (Spanish)", "start_pos": 19, "end_pos": 45, "type": "TASK", "confidence": 0.7828106164932251}]}, {"text": " Table 14: Task ABC: Temporal Awareness (Spanish).", "labels": [], "entities": []}, {"text": " Table 15: Task A: TempEval-2 test set (Spanish).", "labels": [], "entities": [{"text": "TempEval-2 test set", "start_pos": 19, "end_pos": 38, "type": "DATASET", "confidence": 0.7585861086845398}]}, {"text": " Table 16: Task B: TempEval-2 test set (Spanish).", "labels": [], "entities": [{"text": "TempEval-2 test set", "start_pos": 19, "end_pos": 38, "type": "DATASET", "confidence": 0.7509855329990387}]}]}