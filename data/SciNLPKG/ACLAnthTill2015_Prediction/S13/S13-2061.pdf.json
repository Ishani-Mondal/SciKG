{"title": [{"text": "TJP: Using Twitter to Analyze the Polarity of Contexts", "labels": [], "entities": []}], "abstractContent": [{"text": "This paper presents our system, TJP, which participated in SemEval 2013 Task 2 part A: Contextual Polarity Disambiguation.", "labels": [], "entities": [{"text": "SemEval 2013 Task 2 part A", "start_pos": 59, "end_pos": 85, "type": "TASK", "confidence": 0.8342028558254242}, {"text": "Contextual Polarity Disambiguation", "start_pos": 87, "end_pos": 121, "type": "TASK", "confidence": 0.661955306927363}]}, {"text": "The goal of this task is to predict whether marked contexts are positive, neutral or negative.", "labels": [], "entities": []}, {"text": "However , only the scores of positive and negative class will be used to calculate the evaluation result using F-score.", "labels": [], "entities": [{"text": "F-score", "start_pos": 111, "end_pos": 118, "type": "METRIC", "confidence": 0.9921660423278809}]}, {"text": "We chose to work as 'constrained', which used only the provided training and development data without additional sentiment annotated resources.", "labels": [], "entities": []}, {"text": "Our approach considered unigram, bigram and trigram using Na\u00efve Bayes training model with the objective of establishing a simple-approach baseline.", "labels": [], "entities": []}, {"text": "Our system achieved F-score 81.23% and F-score 78.16% in the results for SMS messages and Tweets respectively .", "labels": [], "entities": [{"text": "F-score", "start_pos": 20, "end_pos": 27, "type": "METRIC", "confidence": 0.9994152784347534}, {"text": "F-score", "start_pos": 39, "end_pos": 46, "type": "METRIC", "confidence": 0.9990683197975159}]}], "introductionContent": [{"text": "Natural language processing (NLP) is a research area comprising various tasks; one of which is sentiment analysis.", "labels": [], "entities": [{"text": "Natural language processing (NLP)", "start_pos": 0, "end_pos": 33, "type": "TASK", "confidence": 0.7692239433526993}, {"text": "sentiment analysis", "start_pos": 95, "end_pos": 113, "type": "TASK", "confidence": 0.9686369895935059}]}, {"text": "The main goal of sentiment analysis is to identify the polarity of natural language text (.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 17, "end_pos": 35, "type": "TASK", "confidence": 0.9423975944519043}]}, {"text": "Sentiment analysis can be referred to as opinion mining, as study peoples' opinions, appraisals and emotions towards entities and events and their attributes.", "labels": [], "entities": [{"text": "Sentiment analysis", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.9419994056224823}, {"text": "opinion mining", "start_pos": 41, "end_pos": 55, "type": "TASK", "confidence": 0.7534612715244293}]}, {"text": "Sentiment analysis has become a popular research area in NLP with the purpose of identifying opinions or attitudes in terms of polarity.", "labels": [], "entities": [{"text": "Sentiment analysis", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.9626470506191254}]}, {"text": "This paper presents TJP, a system submitted to SemEval 2013 for Task 2 part A: Contextual Polarity Disambiguation (.", "labels": [], "entities": [{"text": "SemEval 2013", "start_pos": 47, "end_pos": 59, "type": "TASK", "confidence": 0.6577019393444061}, {"text": "Contextual Polarity Disambiguation", "start_pos": 79, "end_pos": 113, "type": "TASK", "confidence": 0.6813097794850668}]}, {"text": "TJP was focused on the 'constrained' task, which used only training and development data provided.", "labels": [], "entities": [{"text": "TJP", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.6566399335861206}]}, {"text": "This avoided both resource implications and potential advantages implied by the use of additional data containing sentiment annotations.", "labels": [], "entities": []}, {"text": "The objective was to explore the relative success of a simple approach that could be implemented easily with open-source software.", "labels": [], "entities": []}, {"text": "The TJP system was implemented using the Python Natural Language Toolkit (NLTK,).", "labels": [], "entities": []}, {"text": "We considered several basic approaches.", "labels": [], "entities": []}, {"text": "These used a preprocessing phase to expand contractions, eliminate stopwords, and identify emoticons.", "labels": [], "entities": []}, {"text": "The next phase used supervised machine learning and n-gram features.", "labels": [], "entities": []}, {"text": "Although we had two approaches that both used n-gram features, we were limited to submitting just one result.", "labels": [], "entities": []}, {"text": "Consequently, we chose to submit a unigram based approach followed by naive Bayes since this performed better on the data.", "labels": [], "entities": []}, {"text": "The remainder of this paper is structured as follows: section 2 provides some discussion on the related work.", "labels": [], "entities": []}, {"text": "The methodology of corpus collection and data classification are provided in section 3.", "labels": [], "entities": [{"text": "corpus collection", "start_pos": 19, "end_pos": 36, "type": "TASK", "confidence": 0.7150468230247498}, {"text": "data classification", "start_pos": 41, "end_pos": 60, "type": "TASK", "confidence": 0.7868711948394775}]}, {"text": "Section 4 outlines details of the experiment and results, followed by the conclusion and ideas for future work in section 5.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this experiment, we used the distributed data from Twitter messages and the F-measure for system evaluation.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 79, "end_pos": 88, "type": "METRIC", "confidence": 0.9815094470977783}]}, {"text": "As at first approach, the corpora were trained directly in the system, while stopwords (e.g. a, an, the) were removed before training using the python NLTK for the second approach.", "labels": [], "entities": []}, {"text": "The approaches are demonstrated on a sample context in and 3.", "labels": [], "entities": []}, {"text": "After comparing both approaches), we were able to obtain an F-score 84.62% of positive and 71.70% of negative after removing stopwords.", "labels": [], "entities": [{"text": "F-score", "start_pos": 60, "end_pos": 67, "type": "METRIC", "confidence": 0.9979552030563354}]}, {"text": "Then, the average F-score is 78.16%, which was increased from the first approach by 0.50%.", "labels": [], "entities": [{"text": "F-score", "start_pos": 18, "end_pos": 25, "type": "METRIC", "confidence": 0.9996600151062012}]}, {"text": "The results from both approaches showed that, unigram achieved higher scores than either bigrams or trigrams.", "labels": [], "entities": []}, {"text": "Moreover, these experiments have been tested with a set of SMS messages to assess how well our system trained on Twitter data can be generalized to other types of message data.", "labels": [], "entities": []}, {"text": "The second approach still achieved the better scores, where we were able to obtain an F-score of 77.81% of positive and 84.66% of negative; thus, the average F-score is 81.23%.", "labels": [], "entities": [{"text": "F-score", "start_pos": 86, "end_pos": 93, "type": "METRIC", "confidence": 0.9992947578430176}, {"text": "F-score", "start_pos": 158, "end_pos": 165, "type": "METRIC", "confidence": 0.9984129667282104}]}, {"text": "The results of unigram from the second approach submitted to SemEval 2013 can be found in.", "labels": [], "entities": [{"text": "SemEval 2013", "start_pos": 61, "end_pos": 73, "type": "DATASET", "confidence": 0.6715545952320099}]}, {"text": "After comparing them using the average F-score from positive and negative class, the results showed that our system works better for SMS messaging than for Twitter.", "labels": [], "entities": [{"text": "F-score", "start_pos": 39, "end_pos": 46, "type": "METRIC", "confidence": 0.9976065158843994}]}, {"text": "gonna miss some of my classes.", "labels": [], "entities": []}, {"text": "After analyzing the Twitter message and SMS messages, we were able to obtain an average F-score of 78.16% and 81.23% respectively during the SemEval 2013 task.", "labels": [], "entities": [{"text": "F-score", "start_pos": 88, "end_pos": 95, "type": "METRIC", "confidence": 0.9995799660682678}, {"text": "SemEval 2013 task", "start_pos": 141, "end_pos": 158, "type": "TASK", "confidence": 0.602383941411972}]}, {"text": "The reason that, our system achieved better scores with SMS message then Twitter message might be due to our use of Twitter messages as training data.", "labels": [], "entities": []}, {"text": "However this is still to be verified experimentally.", "labels": [], "entities": []}, {"text": "The experimental performance on the tasks demonstrates the advantages of simple approaches.", "labels": [], "entities": []}, {"text": "This provides a baseline performance set to which more sophisticated or resource intensive techniques maybe compared.", "labels": [], "entities": []}, {"text": "For future work, we intend to trace back to the root words and work with the suffix and prefix that imply negative semantics, such as 'dis-', 'un-', '-ness' and '-less'.", "labels": [], "entities": []}, {"text": "Moreover, we would like to collect more shorthand texts than that used commonly in microblogs, such as gr8 (great), btw (by the way), pov (point of view), gd (good) and ne1 (anyone).", "labels": [], "entities": []}, {"text": "We believe these could help to improve our system and achieve better accuracy when classifying the sentiment of context from microblogs.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 69, "end_pos": 77, "type": "METRIC", "confidence": 0.998335063457489}, {"text": "classifying the sentiment of context", "start_pos": 83, "end_pos": 119, "type": "TASK", "confidence": 0.8151681065559387}]}], "tableCaptions": [{"text": " Table 2: Example of context from first approach", "labels": [], "entities": []}, {"text": " Table 3: Example of context from second approach.  Note 'some' and 'of' are listed in NLTK stopwords.", "labels": [], "entities": []}]}