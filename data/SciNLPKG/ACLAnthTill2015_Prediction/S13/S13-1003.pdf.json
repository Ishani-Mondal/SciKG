{"title": [{"text": "Coarse to Fine Grained Sense Disambiguation in Wikipedia", "labels": [], "entities": [{"text": "Coarse to Fine Grained Sense Disambiguation", "start_pos": 0, "end_pos": 43, "type": "TASK", "confidence": 0.6108461568752924}, {"text": "Wikipedia", "start_pos": 47, "end_pos": 56, "type": "DATASET", "confidence": 0.8035348653793335}]}], "abstractContent": [{"text": "Wikipedia articles are annotated by volunteer contributors with numerous links that connect words and phrases to relevant titles.", "labels": [], "entities": []}, {"text": "Links to general senses of a word are used concurrently with links to more specific senses, without being distinguished explicitly.", "labels": [], "entities": []}, {"text": "We present an approach to training coarse to fine grained sense disambiguation systems in the presence of such annotation inconsistencies.", "labels": [], "entities": []}, {"text": "Experimental results show that accounting for annotation ambiguity in Wikipedia links leads to significant improvements in disambiguation.", "labels": [], "entities": []}], "introductionContent": [], "datasetContent": [{"text": "We ran disambiguation experiments on the 6 ambiguous words atmosphere, president, dollar, game, diamond and Corinth.", "labels": [], "entities": []}, {"text": "The corresponding Wikipedia sense repositories have been summarized in.", "labels": [], "entities": [{"text": "Wikipedia sense repositories", "start_pos": 18, "end_pos": 46, "type": "DATASET", "confidence": 0.870649258295695}]}, {"text": "All WSD classifiers used the same set of standard WSD features (), such as words and their part-ofspeech tags in a window of 3 words around the ambiguous keyword, the unigram and bigram content words that are within 2 sentences of the current sentence, the syntactic governor of the keyword, and its chains of syntactic dependencies of lengths up to two.", "labels": [], "entities": [{"text": "WSD classifiers", "start_pos": 4, "end_pos": 19, "type": "TASK", "confidence": 0.7231602966785431}]}, {"text": "Furthermore, for each example, a Wikipedia specific feature was computed as the cosine similarity between the context of the ambiguous word and the text of the article for the target sense or reference.", "labels": [], "entities": []}, {"text": "The Level 1 and Level 3 classifiers were trained using the SVM multi component of the SVM light package.", "labels": [], "entities": [{"text": "SVM light package", "start_pos": 86, "end_pos": 103, "type": "DATASET", "confidence": 0.8822056651115417}]}, {"text": "The WSD classifiers were evaluated in a 4-fold cross validation scenario in which 50% of the data was used for training, 25% for tuning the capacity parameter C, and 25% for testing.", "labels": [], "entities": [{"text": "WSD classifiers", "start_pos": 4, "end_pos": 19, "type": "TASK", "confidence": 0.7026419639587402}]}, {"text": "The final accuracy numbers, shown in  The evaluation of the binary classifiers at the second level follows the same 4-fold cross validation scheme that was used for Level 1 and Level 3 . The manual labels for specific senses and references in the unlabeled datasets are always ignored during training and tuning and used only during testing.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9991724491119385}]}, {"text": "We compare the Naive SVM, Biased SVM, and Weighted SVM in the two evaluation settings, using for all of them the same train/development/test splits of the data and the same features.", "labels": [], "entities": []}, {"text": "We emphasize that our manual labels are used only for testing purposes -the manual labels are ignored during training and tuning, when the data is assumed to contain only positive and unlabeled examples.", "labels": [], "entities": []}, {"text": "We implemented the Biased SVM approach on top of the binary SVM light package.", "labels": [], "entities": []}, {"text": "The C P and C U parameters of the Biased SVM were tuned through the c and j parameters of SVM light (c = C U and j = C P /C U ).", "labels": [], "entities": []}, {"text": "Eventually, all three methods use the development data for tuning the c and j parameters of the SVM.", "labels": [], "entities": []}, {"text": "However, whereas the Naive SVM tunes these parameters to optimize the accuracy with respect to the noisy label s(x), the Biased SVM tunes the same parameters to maximize an estimate of the accuracy or F-measure with respect to the true label y(x).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 70, "end_pos": 78, "type": "METRIC", "confidence": 0.9988510608673096}, {"text": "accuracy", "start_pos": 189, "end_pos": 197, "type": "METRIC", "confidence": 0.9981932044029236}, {"text": "F-measure", "start_pos": 201, "end_pos": 210, "type": "METRIC", "confidence": 0.994819700717926}]}, {"text": "The Weighted SVM approach was implemented on top of the LibSVM 2 package.", "labels": [], "entities": []}, {"text": "Even though the original Weighted SVM method of  does not specify tuning any parameters, we noticed it gave better results when the capacity c and weight j parameters were tuned for the first classifier g(x).", "labels": [], "entities": []}, {"text": "shows the accuracy results of the three methods for Level 2 , whereas shows the Fmeasure results.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9996645450592041}, {"text": "Fmeasure", "start_pos": 80, "end_pos": 88, "type": "METRIC", "confidence": 0.9630976915359497}]}, {"text": "The Biased SVM outperforms the Naive SVM on all the words, in terms of both accuracy and F-measure.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 76, "end_pos": 84, "type": "METRIC", "confidence": 0.9994342923164368}, {"text": "F-measure", "start_pos": 89, "end_pos": 98, "type": "METRIC", "confidence": 0.9945928454399109}]}, {"text": "The most dramatic increases are seen for the words atmosphere, game, diamond, and Corinth.", "labels": [], "entities": [{"text": "diamond", "start_pos": 69, "end_pos": 76, "type": "METRIC", "confidence": 0.9500331282615662}]}, {"text": "For these words, the number of positive examples is significantly smaller compared to the total number of positive and unlabeled examples.", "labels": [], "entities": []}, {"text": "Thus, the percentage of positive examples relative to the total number of positive and unlabeled examples is 31.9% for atmosphere, 29.1% for game, 9.0% for diamond, and 11.6% for Corinth.", "labels": [], "entities": [{"text": "Corinth", "start_pos": 179, "end_pos": 186, "type": "DATASET", "confidence": 0.9459632039070129}]}, {"text": "The positive to total ratio is however significantly larger for the other two words: 67.2% for president and 91.5% for dollar.", "labels": [], "entities": []}, {"text": "When the number of positive examples is large, the false negative noise from the unlabeled dataset in the Naive SVM approach will be relatively small, hence the good performance of Naive SVM in these cases.", "labels": [], "entities": []}, {"text": "To check whether this is the case, we have also run experiments where we used only half of the available positive examples for the word president and one tenth of the positive examples for the word dollar, such that the positive datasets became comparable in size with the unlabeled datasets.", "labels": [], "entities": []}, {"text": "The results for these experiments are shown in in the rows labeled president Sand dollar S . As expected, the difference between the performance of Naive SVM and Biased SVM gets larger on these smaller datasets, especially for the word dollar.", "labels": [], "entities": [{"text": "president Sand dollar", "start_pos": 67, "end_pos": 88, "type": "DATASET", "confidence": 0.7918322483698527}]}, {"text": "The Weighted SVM outperforms the Naive SVM on five out of the six words, the exception being the word president.", "labels": [], "entities": []}, {"text": "Comparatively, the Biased SVM has a more stable behavior and overall results in a more substantial improvement over the Naive SVM.", "labels": [], "entities": []}, {"text": "Based on these initial results, we seethe Biased SVM as the method of choice for learning with positive and unlabeled examples in the task of coarse to fine grained sense disambiguation in Wikipedia.", "labels": [], "entities": []}, {"text": "Ina final set of experiments, we compared the traditional flat classification approach and our proposed hierarchical classifier in terms of their overall disambiguation accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 169, "end_pos": 177, "type": "METRIC", "confidence": 0.9411242008209229}]}, {"text": "In these experiments, the sense repository contains all the leaf nodes as distinct sense categories.", "labels": [], "entities": []}, {"text": "For example, the word atmosphere would correspond to the sense repository R = {ATMOSPHERE (G), ATMOSPHERE OF EARTH, ATMOSPHERE OF MARS, ATMOSPHERE OF VENUS, STELLAR ATMOSPHERE, ATMO-SPHERE (UNIT), ATMOSPHERE (MUSIC GROUP)}.", "labels": [], "entities": [{"text": "EARTH", "start_pos": 109, "end_pos": 114, "type": "METRIC", "confidence": 0.536460816860199}, {"text": "MARS", "start_pos": 130, "end_pos": 134, "type": "METRIC", "confidence": 0.7474287152290344}, {"text": "VENUS", "start_pos": 150, "end_pos": 155, "type": "METRIC", "confidence": 0.6792604923248291}, {"text": "STELLAR ATMOSPHERE", "start_pos": 157, "end_pos": 175, "type": "METRIC", "confidence": 0.708348423242569}, {"text": "ATMOSPHERE (MUSIC GROUP)}", "start_pos": 197, "end_pos": 222, "type": "METRIC", "confidence": 0.7315592527389526}]}, {"text": "The overall accuracy results are shown in and confirm the utility of using the LPU framework in the hierarchical model, which outperforms the traditional flat model, especially on words with low ratio of positive to unlabeled examples.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 12, "end_pos": 20, "type": "METRIC", "confidence": 0.9995505213737488}]}], "tableCaptions": [{"text": " Table 1: Wiki (CAPS) and manual (italics) annotations.", "labels": [], "entities": []}, {"text": " Table 2: Wiki (CAPS) and manual (italics) annotations.", "labels": [], "entities": []}, {"text": " Table 3: Disambiguation accuracy at Levels 1 & 3.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 25, "end_pos": 33, "type": "METRIC", "confidence": 0.9576981067657471}]}, {"text": " Table 4: Disambiguation accuracy at Level 2 .", "labels": [], "entities": [{"text": "accuracy", "start_pos": 25, "end_pos": 33, "type": "METRIC", "confidence": 0.9656834006309509}]}, {"text": " Table 5: Disambiguation F-measure at Level 2 .", "labels": [], "entities": [{"text": "F-measure", "start_pos": 25, "end_pos": 34, "type": "METRIC", "confidence": 0.6146905422210693}]}, {"text": " Table 6: Flat vs. Hierarchical disambiguation accuracy.", "labels": [], "entities": [{"text": "Hierarchical disambiguation", "start_pos": 19, "end_pos": 46, "type": "TASK", "confidence": 0.6302836686372757}, {"text": "accuracy", "start_pos": 47, "end_pos": 55, "type": "METRIC", "confidence": 0.9506428837776184}]}]}