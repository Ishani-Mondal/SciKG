{"title": [{"text": "ALTN: Word Alignment Features for Cross-lingual Textual Entailment", "labels": [], "entities": [{"text": "Word Alignment", "start_pos": 6, "end_pos": 20, "type": "TASK", "confidence": 0.7250082492828369}, {"text": "Cross-lingual Textual Entailment", "start_pos": 34, "end_pos": 66, "type": "TASK", "confidence": 0.7361294428507487}]}], "abstractContent": [{"text": "We present a supervised learning approach to cross-lingual textual entailment that explores statistical word alignment models to predict entailment relations between sentences written in different languages.", "labels": [], "entities": [{"text": "cross-lingual textual entailment", "start_pos": 45, "end_pos": 77, "type": "TASK", "confidence": 0.6397679646809896}, {"text": "statistical word alignment", "start_pos": 92, "end_pos": 118, "type": "TASK", "confidence": 0.6799149115880331}]}, {"text": "Our approach is language independent, and was used to participate in the CLTE task (Task#8) organized within Semeval 2013 (Negri et al., 2013).", "labels": [], "entities": [{"text": "CLTE task (Task#8) organized within Semeval 2013 (Negri et al., 2013)", "start_pos": 73, "end_pos": 142, "type": "DATASET", "confidence": 0.7822319749328825}]}, {"text": "The four runs submitted, one for each language combination covered by the test data (i.e. Spanish/English, German/English, French/English and Italian/English), achieved encouraging results.", "labels": [], "entities": []}, {"text": "In terms of accuracy, performance ranges from 38.8% (for Ger-man/English) to 43.2% (for Italian/English).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 12, "end_pos": 20, "type": "METRIC", "confidence": 0.999546468257904}]}, {"text": "On the Italian/English and Spanish/English test sets our systems ranked second among five participants, close to the top results (re-spectively 43.4% and 45.4%).", "labels": [], "entities": [{"text": "Italian/English and Spanish/English test sets", "start_pos": 7, "end_pos": 52, "type": "DATASET", "confidence": 0.6201823718018002}]}], "introductionContent": [{"text": "Cross-lingual textual entailment (CLTE) is an extension of the Textual Entailment task () that consists in deciding, given two texts T and H written in different languages (respectively called text and hypothesis), if H can be inferred from T ().", "labels": [], "entities": [{"text": "Cross-lingual textual entailment (CLTE)", "start_pos": 0, "end_pos": 39, "type": "TASK", "confidence": 0.7661658376455307}]}, {"text": "In the case of SemEval 2013, the task is formulated as a multi-class classification problem in which there are four possible relations between T and H: forward (T \u2192 H), backward (T \u2190 H), bidirectional (T \u2194 H) and \"no entailment\".", "labels": [], "entities": []}, {"text": "Targeting the identification of semantic equivalence and information disparity between topically related sentences, CLTE recognition can be seen as a core task fora number of cross-lingual applications.", "labels": [], "entities": [{"text": "identification of semantic equivalence and information disparity between topically related sentences", "start_pos": 14, "end_pos": 114, "type": "TASK", "confidence": 0.7388514551249418}, {"text": "CLTE recognition", "start_pos": 116, "end_pos": 132, "type": "TASK", "confidence": 0.8920591473579407}]}, {"text": "Among others, multilingual content synchronization has been recently proposed as an ideal framework for the exploitation of CLTE components and the integration of semantics and machine translation (MT) technology).", "labels": [], "entities": [{"text": "multilingual content synchronization", "start_pos": 14, "end_pos": 50, "type": "TASK", "confidence": 0.6124081214269003}, {"text": "machine translation (MT)", "start_pos": 177, "end_pos": 201, "type": "TASK", "confidence": 0.838574481010437}]}, {"text": "In the last few years, several methods have been proposed for CLTE.", "labels": [], "entities": []}, {"text": "These can be roughly divided in two main groups ): i) those using a pivoting strategy by translating H into the language of T and then using monolingual TE components 1 , and those directly using cross-lingual strategies.", "labels": [], "entities": []}, {"text": "Among this second group, several sources of cross-lingual knowledge have been used, such as dictionaries (, phrase and paraphrase tables), GIZA++ word alignment models), MT of subsegments Esp\u00ec a-Gomis et al., 2012), or semantic Wordnets.", "labels": [], "entities": [{"text": "GIZA++ word alignment", "start_pos": 139, "end_pos": 160, "type": "TASK", "confidence": 0.49459321051836014}, {"text": "MT of subsegments Esp\u00ec a-Gomis et al., 2012", "start_pos": 170, "end_pos": 213, "type": "TASK", "confidence": 0.8497615456581116}]}, {"text": "In this work we propose a CLTE detection method based on anew set of features using word alignment as a source of cross-lingual knowledge.", "labels": [], "entities": [{"text": "CLTE detection", "start_pos": 26, "end_pos": 40, "type": "TASK", "confidence": 0.8636208474636078}, {"text": "word alignment", "start_pos": 84, "end_pos": 98, "type": "TASK", "confidence": 0.7212018817663193}]}, {"text": "This set, which is richer than the one by), is aimed not only at grasping information about the proportion of aligned words, but also about the distribution of the alignments in both H and T . This set of features is later used by two support vector machine (SVM) classifiers for detecting CLTE separately in both directions (T \u2192 H and T \u2190 H).", "labels": [], "entities": []}, {"text": "We use the combined output of both classifiers for performing the CLTE detection.", "labels": [], "entities": [{"text": "CLTE detection", "start_pos": 66, "end_pos": 80, "type": "TASK", "confidence": 0.7823973298072815}]}, {"text": "The paper is organized as follows: Section 2 describes the features used and the classification method; Section 3 explains the experimental framework and the results obtained for the different language-pair sets; finally, the conclusions obtained from the results are summarised in Section 4.", "labels": [], "entities": []}], "datasetContent": [{"text": "In our submission we experimented with three standard word alignment algorithms: the hidden Markov model (HMM) () and IBM models 3 and 4.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 54, "end_pos": 68, "type": "TASK", "confidence": 0.7803341746330261}]}, {"text": "They are implemented in the MGIZA++ package (.", "labels": [], "entities": [{"text": "MGIZA++ package", "start_pos": 28, "end_pos": 43, "type": "DATASET", "confidence": 0.9295176863670349}]}, {"text": "Building on a probabilistic lexical model to establish mappings between words in two languages, these models compute alignments between the word positions in two input sentences S 1 and S 2 . The models are trained incrementally: HMM is the base for IBM model 3, which is the base for IBM model 4.", "labels": [], "entities": [{"text": "IBM model 3", "start_pos": 250, "end_pos": 261, "type": "DATASET", "confidence": 0.8687535723050436}]}, {"text": "To train our models, we used 5 iterations of HMM, and 3 iterations of IBM models 3 and 4.", "labels": [], "entities": [{"text": "HMM", "start_pos": 45, "end_pos": 48, "type": "DATASET", "confidence": 0.8555178046226501}]}, {"text": "Word alignments produced by these models are asymmetric (S 1 \u2192 S 2 = S 2 \u2192 S 1 ).", "labels": [], "entities": [{"text": "Word alignments", "start_pos": 0, "end_pos": 15, "type": "TASK", "confidence": 0.6908825039863586}]}, {"text": "To cope with this, different heuristics ( ) have been proposed to obtain symmetric alignments from two asymmetric sets (S 1 \u2194 S 2 ).", "labels": [], "entities": []}, {"text": "We experimented with three symmetrization heuristics, namely: union, intersection, and grow-diag-finaland, a more complex symmetrization method which combines intersection with some alignments from the union.", "labels": [], "entities": []}, {"text": "To train the word alignment models we used the Europarl parallel corpus () concatenated with the News Commentary corpus 2 for three language pairs: English-German (2,079,049 sentences), English-Spanish (2,123,036 sentences), English-French (2,144,820 sentences).", "labels": [], "entities": [{"text": "word alignment", "start_pos": 13, "end_pos": 27, "type": "TASK", "confidence": 0.7414935529232025}, {"text": "Europarl parallel corpus", "start_pos": 47, "end_pos": 71, "type": "DATASET", "confidence": 0.9193266232808431}, {"text": "News Commentary corpus", "start_pos": 97, "end_pos": 119, "type": "DATASET", "confidence": 0.8736272056897482}]}, {"text": "For EnglishItalian we only used the parallel data available in Europarl (1,909,115 sentences) since this language pair is not covered by the News Commentary corpus.", "labels": [], "entities": [{"text": "Europarl", "start_pos": 63, "end_pos": 71, "type": "DATASET", "confidence": 0.9918082356452942}, {"text": "News Commentary corpus", "start_pos": 141, "end_pos": 163, "type": "DATASET", "confidence": 0.8990171750386556}]}, {"text": "For our submitted run the SVM classifiers were trained using the whole training set.", "labels": [], "entities": [{"text": "SVM classifiers", "start_pos": 26, "end_pos": 41, "type": "TASK", "confidence": 0.7535039782524109}]}, {"text": "Such dataset consists of 1,000 pairs for each of the four language combinations, resulting from a concatenation of the training and test sets used for the first round of evaluation at SemEval 2012 ( ).", "labels": [], "entities": []}, {"text": "We have set a polynomial kernel with parameters empirically estimated on the training set: C = 2.0, and d = 1.", "labels": [], "entities": []}, {"text": "After some preliminary experiments we have concluded that the HMM model in conjunction with the intersection symmetrization provides the best results.", "labels": [], "entities": []}, {"text": "Our results, calculated over the 500 test pairs provided for each language combination, are presented in.", "labels": [], "entities": []}, {"text": "As can be seen from the table, our system consistently outperforms the best average run of all participants and is the second best system for Spanish/English and Italian/English.", "labels": [], "entities": []}, {"text": "For the other two languages, French/English and German/English, it is the 3rd best system with a larger distance from top results.", "labels": [], "entities": []}, {"text": "The motivations for such lower results, currently under investigation, might be related to lower performance in terms of word alignment, the core of our approach.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 121, "end_pos": 135, "type": "TASK", "confidence": 0.7702783048152924}]}, {"text": "The first step of our analysis will hence address, and in case try to cope with, significant differences in word alignment performance affecting results.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 108, "end_pos": 122, "type": "TASK", "confidence": 0.7229541093111038}]}, {"text": "Overall, considering the small distance from top results, and the fact that our approach does not require deep linguistic processing to be reasonably effective for any language pair for which parallel corpora are available, our results are encouraging and motivate further research along such direction.", "labels": [], "entities": []}, {"text": "within a supervised learning setting.", "labels": [], "entities": []}, {"text": "In our approach, word alignment models obtained by statistical methods from parallel corpora leverage information about the number, the proportion, and the distribution of aligned terms in the input sentences.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 17, "end_pos": 31, "type": "TASK", "confidence": 0.7774912416934967}]}], "tableCaptions": [{"text": " Table 1: Accuracy results for the language pairs evaluated for the average of the best runs of the participating systems,  our submission and the best systems.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9987607002258301}]}]}