{"title": [{"text": "Bootstrapping Semantic Role Labelers from Parallel Data", "labels": [], "entities": [{"text": "Bootstrapping Semantic Role Labelers", "start_pos": 0, "end_pos": 36, "type": "TASK", "confidence": 0.5893517211079597}]}], "abstractContent": [{"text": "We present an approach which uses the similarity in semantic structure of bilingual parallel sentences to bootstrap a pair of semantic role labeling (SRL) models.", "labels": [], "entities": [{"text": "semantic role labeling (SRL)", "start_pos": 126, "end_pos": 154, "type": "TASK", "confidence": 0.7964814702669779}]}, {"text": "The setting is similar to co-training, except for the intermediate model required to convert the SRL structure between the two annotation schemes used for different languages.", "labels": [], "entities": []}, {"text": "Our approach can facilitate the construction of SRL models for resource-poor languages, while preserving the annotation schemes designed for the target language and making use of the limited resources available for it.", "labels": [], "entities": [{"text": "SRL", "start_pos": 48, "end_pos": 51, "type": "TASK", "confidence": 0.9682174324989319}]}, {"text": "We evaluate the model on four language pairs, English vs German, Spanish, Czech and Chinese.", "labels": [], "entities": []}, {"text": "Consistent improvements are observed over the self-training baseline.", "labels": [], "entities": []}], "introductionContent": [{"text": "The success of statistical modeling methods in a variety of natural language processing (NLP) tasks in the last decade depended crucially on the availability of annotated resources for their training.", "labels": [], "entities": [{"text": "natural language processing (NLP) tasks", "start_pos": 60, "end_pos": 99, "type": "TASK", "confidence": 0.7623729620661054}]}, {"text": "And while sizable resources for most standard tasks are only available fora few languages, the human effort required to achieve reasonable performance on such tasks for other languages maybe significantly reduced by leveraging existing resources and the similarities between languages.", "labels": [], "entities": []}, {"text": "This idea has lead to the development of crosslingual annotation projection approaches, which make use of parallel corpora, as well as attempts to adapt models directly to other languages.", "labels": [], "entities": [{"text": "crosslingual annotation projection", "start_pos": 41, "end_pos": 75, "type": "TASK", "confidence": 0.6401353577772776}]}, {"text": "In this paper we consider correspondences between SRL structures in translated sentences from a different perspective.", "labels": [], "entities": [{"text": "SRL structures", "start_pos": 50, "end_pos": 64, "type": "TASK", "confidence": 0.9231534898281097}]}, {"text": "Most cross-lingual annotation projection approaches transfer the source language annotation scheme to the target language without modification, which makes it hard to combine their output with existing target language resources, as annotation schemes may vary significantly.", "labels": [], "entities": [{"text": "cross-lingual annotation projection", "start_pos": 5, "end_pos": 40, "type": "TASK", "confidence": 0.6691760718822479}]}, {"text": "We instead address the problem of information transfer between two existing annotation schemes (figure 1) fora pair of languages using an intermediate model of role correspondence (RCM).", "labels": [], "entities": [{"text": "information transfer", "start_pos": 34, "end_pos": 54, "type": "TASK", "confidence": 0.7539360225200653}, {"text": "role correspondence (RCM", "start_pos": 160, "end_pos": 184, "type": "TASK", "confidence": 0.7436302900314331}]}, {"text": "An RCM models the probability of a pair of corresponding arguments being assigned a certain pair of roles.", "labels": [], "entities": []}, {"text": "We then use it to guide a pair of monolingual models toward compatible predictions on parallel data in order to extend the coverage and/or accuracy of one or both models.", "labels": [], "entities": [{"text": "coverage", "start_pos": 123, "end_pos": 131, "type": "METRIC", "confidence": 0.9717722535133362}, {"text": "accuracy", "start_pos": 139, "end_pos": 147, "type": "METRIC", "confidence": 0.9968994855880737}]}, {"text": "Romanian is not taught in their schools . Ve \u0161kol\u00e1ch se neu\u010d\u00ed rumunsky . The notion of compatibility here is highly nontrivial, even for sentences translated as close to the original as possible., for example, observe that in the English-Chinese parallel PropBank ( corresponding arguments often bear different labels, even though the same inventory of semantic roles is used for both languages and the annotation guidelines are similar.", "labels": [], "entities": []}, {"text": "When different annotation schemes are considered, the problem is further complicated by the difference in the granularity of semantic roles used and varying notions of what is an argument and what is not.", "labels": [], "entities": []}, {"text": "Manually annotated training data for such a model is hard to come by.", "labels": [], "entities": []}, {"text": "Instead, we propose an iterative procedure similar to bootstrapping, where the parameters of the RCM are initially estimated from a parallel corpus automatically annotated with semantic roles using the monolingual models independently, and then the RCM is used to refine these annotations via a joint inference procedure, serving to enforce consistency on the predictions of monolingual models on parallel sentences.", "labels": [], "entities": []}, {"text": "The obtained annotations on the parallel corpus are expected to be of higher quality than the independent predictions of the models, so they can be used to improve the SRL models' performance and/or coverage.", "labels": [], "entities": [{"text": "coverage", "start_pos": 199, "end_pos": 207, "type": "METRIC", "confidence": 0.968716025352478}]}, {"text": "We evaluate this approach by augmenting the original training data with the annotations obtained on parallel data and observing the change in the model's performance.", "labels": [], "entities": []}, {"text": "This is especially useful if one of the languages is relatively poor in resources, in which case the proposed procedure will help propagate information from the stronger model to the weaker one.", "labels": [], "entities": []}, {"text": "Even if the two models are comparable in their predictive power, we maybe able to benefit from the fact that certain semantic roles are realized less ambiguously in one language than in another.", "labels": [], "entities": []}, {"text": "We will henceforth refer to these two alternatives as the projection and symmetric setups.", "labels": [], "entities": []}, {"text": "The paper is structured as follows.", "labels": [], "entities": []}, {"text": "In the next section we present our approach and discuss the issues of role correspondence modeling, then describe the implementation and datasets used in evaluation in section 3, present the evaluation and results in section 4 and conclude with the discussion of related work in section 5.", "labels": [], "entities": [{"text": "role correspondence modeling", "start_pos": 70, "end_pos": 98, "type": "TASK", "confidence": 0.8621371984481812}]}], "datasetContent": [{"text": "We evaluate our approach on four language pairs, namely English vs German, Spanish, Czech and Chinese, which we will denote en-de, en-es, en-cz and en-zh respectively.", "labels": [], "entities": []}, {"text": "The first question we are interested in is how the joint inference affects the quality of the automatically obtained annotations on the parallel data.", "labels": [], "entities": []}, {"text": "To answer this, we will run the monolingual models independently and jointly, then train models on the output of these two procedures and compare their performance on a test set.", "labels": [], "entities": []}, {"text": "Note that we do not add the initial training data at this point, so the initial model scores are provided for reference, rather than as a baseline.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Projection setup results: self-training baseline,  refined model and the difference in their performance.  Asterisk indicates out-of-domain test set, statistically sig- nificant improvements are highlighted in bold.", "labels": [], "entities": []}, {"text": " Table 2: The effect of adding automatically obtained an- notation to the initial training set. Asterisk indicates out- of-domain test set, statistically significant improvements  are highlighted in bold.", "labels": [], "entities": []}, {"text": " Table 3: Comparing JOINT model against the self- training baseline in symmetric setup. Asterisk indicates  out-of-domain test set, statistically significant improve- ments are highlighted in bold.", "labels": [], "entities": [{"text": "statistically significant improve- ments", "start_pos": 132, "end_pos": 172, "type": "METRIC", "confidence": 0.697851550579071}]}, {"text": " Table 4: Oracle RCM performance, projection setup: ini- tial model, self-training baseline, refined model and its  improvement over the other two. Asterisk indicates out- of-domain test set, statistically significant improvements  are highlighted in bold.", "labels": [], "entities": []}]}