{"title": [{"text": "Semeval-2013 Task 8: Cross-lingual Textual Entailment for Content Synchronization", "labels": [], "entities": [{"text": "Cross-lingual Textual Entailment", "start_pos": 21, "end_pos": 53, "type": "TASK", "confidence": 0.6420874297618866}, {"text": "Content Synchronization", "start_pos": 58, "end_pos": 81, "type": "TASK", "confidence": 0.708480566740036}]}], "abstractContent": [{"text": "This paper presents the second round of the task on Cross-lingual Textual Entailment for Content Synchronization, organized within SemEval-2013.", "labels": [], "entities": [{"text": "Cross-lingual Textual Entailment", "start_pos": 52, "end_pos": 84, "type": "TASK", "confidence": 0.714417576789856}, {"text": "Content Synchronization", "start_pos": 89, "end_pos": 112, "type": "TASK", "confidence": 0.6803029924631119}]}, {"text": "The task was designed to promote research on semantic inference over texts written in different languages, targeting at the same time areal application scenario.", "labels": [], "entities": [{"text": "semantic inference", "start_pos": 45, "end_pos": 63, "type": "TASK", "confidence": 0.7993273735046387}]}, {"text": "Participants were presented with datasets for different language pairs, where multi-directional entailment relations (\"forward\", \"backward\", \"bidirectional\", \"no entailment\") had to be identified.", "labels": [], "entities": []}, {"text": "We report on the training and test data used for evaluation, the process of their creation, the participating systems (six teams, 61 runs), the approaches adopted and the results achieved.", "labels": [], "entities": []}], "introductionContent": [{"text": "The cross-lingual textual entailment task ) addresses textual entailment (TE) recognition () under the new dimension of cross-linguality, and within the new challenging application scenario of content synchronization.", "labels": [], "entities": [{"text": "textual entailment (TE) recognition", "start_pos": 54, "end_pos": 89, "type": "TASK", "confidence": 0.6641526619593302}]}, {"text": "Given two texts in different languages, the cross-lingual textual entailment (CLTE) task consists of deciding if the meaning of one text can be inferred from the meaning of the other text.", "labels": [], "entities": [{"text": "cross-lingual textual entailment (CLTE)", "start_pos": 44, "end_pos": 83, "type": "TASK", "confidence": 0.7420343359311422}]}, {"text": "Crosslinguality represents an interesting direction for research on recognizing textual entailment (RTE), especially due to its possible application in a variety of tasks.", "labels": [], "entities": [{"text": "recognizing textual entailment (RTE)", "start_pos": 68, "end_pos": 104, "type": "TASK", "confidence": 0.8624078830083212}]}, {"text": "Among others (e.g. question answering, information retrieval, information extraction, and document summarization), multilingual content synchronization represents a challenging application scenario to evaluate CLTE recognition components geared to the identification of sentence-level semantic relations.", "labels": [], "entities": [{"text": "question answering", "start_pos": 19, "end_pos": 37, "type": "TASK", "confidence": 0.853749543428421}, {"text": "information retrieval", "start_pos": 39, "end_pos": 60, "type": "TASK", "confidence": 0.7756393253803253}, {"text": "information extraction", "start_pos": 62, "end_pos": 84, "type": "TASK", "confidence": 0.7815951704978943}, {"text": "document summarization", "start_pos": 90, "end_pos": 112, "type": "TASK", "confidence": 0.6672659814357758}, {"text": "CLTE recognition", "start_pos": 210, "end_pos": 226, "type": "TASK", "confidence": 0.8710379600524902}, {"text": "identification of sentence-level semantic relations", "start_pos": 252, "end_pos": 303, "type": "TASK", "confidence": 0.7279892802238465}]}, {"text": "Given two documents about the same topic written in different languages (e.g. Wikipedia pages), the content synchronization task consists of automatically detecting and resolving differences in the information they provide, in order to produce aligned, mutually enriched versions of the two documents.", "labels": [], "entities": []}, {"text": "Towards this objective, a crucial requirement is to identify the information in one page that is either equivalent or novel (more informative) with respect to the content of the other.", "labels": [], "entities": []}, {"text": "The task can be naturally cast as an entailment recognition problem, where bidirectional and unidirectional entailment judgements for two text fragments are respectively mapped into judgements about semantic equivalence and novelty.", "labels": [], "entities": [{"text": "entailment recognition", "start_pos": 37, "end_pos": 59, "type": "TASK", "confidence": 0.7272911369800568}]}, {"text": "The task can also be seen as a machine translation evaluation problem, where judgements about semantic equivalence and novelty depend on the possibility to fully or partially translate a text fragment into the other.", "labels": [], "entities": [{"text": "machine translation evaluation", "start_pos": 31, "end_pos": 61, "type": "TASK", "confidence": 0.8006038268407186}]}, {"text": "The recent advances on monolingual TE on the one hand, and the methodologies used in Statistical Machine Translation (SMT) on the other, offer promising solutions to approach the CLTE task.", "labels": [], "entities": [{"text": "TE", "start_pos": 35, "end_pos": 37, "type": "TASK", "confidence": 0.7193036675453186}, {"text": "Statistical Machine Translation (SMT)", "start_pos": 85, "end_pos": 122, "type": "TASK", "confidence": 0.8461583256721497}]}, {"text": "In line with a number of systems that model the RTE task as a similarity problem (i.e. handling similarity scores between T and H as features contributing to the entailment decision), the standard sentence and word alignment programs used in SMT offer a strong baseline for CLTE (Mehdad et al., 2011;  ).", "labels": [], "entities": [{"text": "RTE task", "start_pos": 48, "end_pos": 56, "type": "TASK", "confidence": 0.9291883409023285}, {"text": "word alignment", "start_pos": 210, "end_pos": 224, "type": "TASK", "confidence": 0.6922124624252319}, {"text": "SMT", "start_pos": 242, "end_pos": 245, "type": "TASK", "confidence": 0.9918150305747986}]}, {"text": "However, although representing a solid starting point to approach the problem, similarity-based techniques are just approximations, open to significant improvements coming from semantic inference at the multilingual level (e.g. cross-lingual entailment rules such as \"perro\"\u2192\"animal\").", "labels": [], "entities": []}, {"text": "Taken in isolation, similaritybased techniques clearly fall short of providing an effective solution to the problem of assigning directions to the entailment relations (especially in the complex CLTE scenario, where entailment relations are multi-directional).", "labels": [], "entities": []}, {"text": "Thanks to the contiguity between CLTE, TE and SMT, the proposed task provides an interesting scenario to approach the issues outlined above from different perspectives, and offers large room for mutual improvement.", "labels": [], "entities": [{"text": "TE", "start_pos": 39, "end_pos": 41, "type": "METRIC", "confidence": 0.6901304125785828}, {"text": "SMT", "start_pos": 46, "end_pos": 49, "type": "TASK", "confidence": 0.932523250579834}]}, {"text": "Building on the success of the first CLTE evaluation organized within, the remainder of this paper describes the second evaluation round organized within SemEval-2013.", "labels": [], "entities": []}, {"text": "The following sections provide an overview of the datasets used, the participating systems, the approaches adopted, the achieved results, and the lessons learned.", "labels": [], "entities": []}], "datasetContent": [{"text": "The CLTE-2013 dataset is composed of four CLTE corpora created for the following language combinations: Spanish/English (SP-EN), Italian/English (IT-EN), French/English (FR-EN), German/English (DE-EN).", "labels": [], "entities": [{"text": "CLTE-2013 dataset", "start_pos": 4, "end_pos": 21, "type": "DATASET", "confidence": 0.9494088292121887}]}, {"text": "Each corpus consists of 1,500 sentence pairs (1,000 for training and 500 for test), balanced across the four entailment judgements.", "labels": [], "entities": []}, {"text": "In this year's evaluation, as training set we used the CLTE-2012 corpus 1 that was created for the SemEval-2012 evaluation exercise 2 (including both training and test sets).", "labels": [], "entities": [{"text": "CLTE-2012 corpus 1", "start_pos": 55, "end_pos": 73, "type": "DATASET", "confidence": 0.9556824167569479}, {"text": "SemEval-2012 evaluation exercise", "start_pos": 99, "end_pos": 131, "type": "TASK", "confidence": 0.7167715032895406}]}, {"text": "The CLTE-2013 test set was created from scratch, following the methodology described in the next section.", "labels": [], "entities": [{"text": "CLTE-2013 test set", "start_pos": 4, "end_pos": 22, "type": "DATASET", "confidence": 0.9612495104471842}]}, {"text": "As described in section 3.1, the methodology followed to create the training and test sets was the same except for the crowdsourced tasks.", "labels": [], "entities": []}, {"text": "This allowed us to obtain two datasets with the same balance across the entailment judgements, and to keep under control the distribution of the pairs for different length diff values in each language combination.", "labels": [], "entities": []}, {"text": "The training set is composed of 1,000 CLTE pairs for each language combination, balanced across the four entailment judgements (bidirectional, forward, backward, and no entailment).", "labels": [], "entities": []}, {"text": "As shown in, our data collection procedure led to a dataset where the majority of the pairs falls in the +5 -5 length diff range for each language pair (67.2% on average across the four language pairs).", "labels": [], "entities": []}, {"text": "This characteristic is particularly relevant as our assumption is that such data distribution makes entailment judgements based on mere surface features such as sentence length ineffective, thus encouraging the development of alternative, deeper processing strategies.", "labels": [], "entities": []}, {"text": "The test set is composed of 500 entailment pairs for each language combination, balanced across the four entailment judgements.", "labels": [], "entities": []}, {"text": "As shown in, also in this dataset the majority of the collected entailment pairs is uniformly distributed   and test sets, the ideal objective of a full homogeneity between the datasets for these two languages was difficult to reach.", "labels": [], "entities": []}, {"text": "Complete details about the distribution of the pairs in terms of length diff for the four crosslingual corpora in the test set are provided in.", "labels": [], "entities": []}, {"text": "Vertical bars represent, for each length diff value, the proportion of pairs belonging to the four entailment classes.", "labels": [], "entities": []}, {"text": "Evaluation results have been automatically computed by comparing the entailment judgements returned by each system with those manually assigned by human annotators in the gold standard.", "labels": [], "entities": []}, {"text": "The metrics used for systems' ranking is accuracy over the whole test set, i.e. the number of correct judgements out of the total number of judgements in the test set.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 41, "end_pos": 49, "type": "METRIC", "confidence": 0.9990242719650269}]}, {"text": "Additionally, we calculated precision, recall, and F1 measures for each of the four entailment judgement categories taken separately.", "labels": [], "entities": [{"text": "precision", "start_pos": 28, "end_pos": 37, "type": "METRIC", "confidence": 0.999731719493866}, {"text": "recall", "start_pos": 39, "end_pos": 45, "type": "METRIC", "confidence": 0.9995869994163513}, {"text": "F1", "start_pos": 51, "end_pos": 53, "type": "METRIC", "confidence": 0.9998685121536255}]}, {"text": "These scores aim at giving participants the possibility to gain clearer insights into their system's behaviour on the entailment phenomena relevant to the task.", "labels": [], "entities": []}, {"text": "To allow comparison with the CLTE-2012 results, the same three baselines were calculated on the CLTE-2013 test set for each language combination.", "labels": [], "entities": [{"text": "CLTE-2012", "start_pos": 29, "end_pos": 38, "type": "DATASET", "confidence": 0.9073929190635681}, {"text": "CLTE-2013 test set", "start_pos": 96, "end_pos": 114, "type": "DATASET", "confidence": 0.9742346803347269}]}, {"text": "The first one is the 0.25 accuracy score obtained by assigning each test pair in the balanced dataset to one of the four classes.", "labels": [], "entities": [{"text": "accuracy score", "start_pos": 26, "end_pos": 40, "type": "METRIC", "confidence": 0.9803177118301392}]}, {"text": "The other two baselines consider the length difference between T1 and T2: \u2022 Composition of binary judgements (Binary).", "labels": [], "entities": []}, {"text": "To calculate this baseline an SVM classifier is trained to take binary entailment decisions (\"YES\", \"NO\").", "labels": [], "entities": [{"text": "YES", "start_pos": 94, "end_pos": 97, "type": "METRIC", "confidence": 0.9961744546890259}, {"text": "NO", "start_pos": 101, "end_pos": 103, "type": "METRIC", "confidence": 0.8567237257957458}]}, {"text": "The classifier uses length(T1)/length(T2) and length(T2)/length(T1) as features respectively to check for entailment from T1 to T2 and viceversa.", "labels": [], "entities": [{"text": "length(T1)/length(T2)", "start_pos": 20, "end_pos": 41, "type": "METRIC", "confidence": 0.8774965927004814}, {"text": "length(T2)/length(T1)", "start_pos": 46, "end_pos": 67, "type": "METRIC", "confidence": 0.8742238059639931}]}, {"text": "For each test pair, the unidirectional judgements returned by the two classifiers are composed into a single multi-directional judgement (\"YES-YES\"=\"bidirectional\", \"YES-NO\"=\"forward\", \"NO-YES\"=\"backward\", \"NO-NO\"=\"no entailment\"); \u2022 Multi-class classification (Multi-class).", "labels": [], "entities": []}, {"text": "A single SVM classifier is trained with the same features to directly assign to each pair one of the four entailment judgements.", "labels": [], "entities": []}, {"text": "Both the baselines have been calculated with the LIBSVM package (Chang and Lin, 2011), using default parameters.", "labels": [], "entities": []}, {"text": "Baseline results are reported in.", "labels": [], "entities": [{"text": "Baseline", "start_pos": 0, "end_pos": 8, "type": "METRIC", "confidence": 0.9524142742156982}]}, {"text": "Although the four CLTE datasets are derived from the same monolingual EN-EN corpus, baseline results present slight differences due to the effect of translation into different languages.", "labels": [], "entities": [{"text": "CLTE datasets", "start_pos": 18, "end_pos": 31, "type": "DATASET", "confidence": 0.8860260546207428}]}, {"text": "With respect to last year's evaluation, we can observe a slight drop in the binary classification baseline results.", "labels": [], "entities": []}, {"text": "This might be due to the fact that the length distribution of examples is slightly different this year.", "labels": [], "entities": []}, {"text": "However, there are no significant differences between the multi-class baseline results of this year in comparison with the previous round results.", "labels": [], "entities": []}, {"text": "This might suggest that multi-class classification is a more robust approach for recognizing multi-directional entailment relations.", "labels": [], "entities": [{"text": "multi-class classification", "start_pos": 24, "end_pos": 50, "type": "TASK", "confidence": 0.7879316210746765}, {"text": "recognizing multi-directional entailment relations", "start_pos": 81, "end_pos": 131, "type": "TASK", "confidence": 0.7307533100247383}]}, {"text": "Moreover, both baselines failed in capturing the \"no-entailment\" examples in all datasets (F 1 no\u2212entailment = 0).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Training set pair distribution within the -5/+5  length diff range.", "labels": [], "entities": []}, {"text": " Table 3: Baseline accuracy results.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 19, "end_pos": 27, "type": "METRIC", "confidence": 0.9844610095024109}]}, {"text": " Table 4: CLTE-2013 accuracy results (61 runs) over the  4 language combinations. Highest, average, median and  lowest scores are calculated considering only the best run  for each team (*task organizers' system).", "labels": [], "entities": [{"text": "CLTE-2013", "start_pos": 10, "end_pos": 19, "type": "DATASET", "confidence": 0.7048345804214478}, {"text": "accuracy", "start_pos": 20, "end_pos": 28, "type": "METRIC", "confidence": 0.986515998840332}]}, {"text": " Table 5: CLTE-2012 accuracy results. Highest, average,  median and lowest scores are calculated considering only  the best run for each team.", "labels": [], "entities": [{"text": "CLTE-2012", "start_pos": 10, "end_pos": 19, "type": "DATASET", "confidence": 0.7803974747657776}, {"text": "accuracy", "start_pos": 20, "end_pos": 28, "type": "METRIC", "confidence": 0.9909462928771973}]}, {"text": " Table 6: Precision, recall and F1 scores, calculated for each team's best run for all the language combinations.", "labels": [], "entities": [{"text": "Precision", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9985796213150024}, {"text": "recall", "start_pos": 21, "end_pos": 27, "type": "METRIC", "confidence": 0.9977344274520874}, {"text": "F1 scores", "start_pos": 32, "end_pos": 41, "type": "METRIC", "confidence": 0.9751121997833252}]}]}