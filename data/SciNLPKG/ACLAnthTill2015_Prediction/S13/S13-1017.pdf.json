{"title": [{"text": "ECNUCS: Measuring Short Text Semantic Equivalence Using Multiple Similarity Measurements", "labels": [], "entities": [{"text": "ECNUCS", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.8602610230445862}, {"text": "Measuring Short Text Semantic Equivalence", "start_pos": 8, "end_pos": 49, "type": "TASK", "confidence": 0.7943307161331177}]}], "abstractContent": [{"text": "This paper reports our submissions to the Semantic Textual Similarity (STS) task in * SEM Shared Task 2013.", "labels": [], "entities": [{"text": "Semantic Textual Similarity (STS) task", "start_pos": 42, "end_pos": 80, "type": "TASK", "confidence": 0.7854157601084027}, {"text": "SEM Shared Task 2013", "start_pos": 86, "end_pos": 106, "type": "TASK", "confidence": 0.6907533258199692}]}, {"text": "We submitted three Support Vector Regression (SVR) systems in core task, using 6 types of similarity measures , i.e., string similarity, number similarity , knowledge-based similarity, corpus-based similarity, syntactic dependency similarity and machine translation similarity.", "labels": [], "entities": []}, {"text": "Our third system with different training data and different feature sets for each test data set performs the best and ranks 35 out of 90 runs.", "labels": [], "entities": []}, {"text": "We also submitted two systems in typed task using string based measure and Named Entity based measure.", "labels": [], "entities": []}, {"text": "Our best system ranks 5 out of 15 runs.", "labels": [], "entities": []}], "introductionContent": [{"text": "The task of semantic textual similarity (STS) is to measure the degree of semantic equivalence between two sentences, which plays an increasingly important role in natural language processing (NLP) applications.", "labels": [], "entities": [{"text": "semantic textual similarity (STS)", "start_pos": 12, "end_pos": 45, "type": "TASK", "confidence": 0.7617652714252472}]}, {"text": "For example, in text categorization, two documents which are more similar are more likely to be grouped in the same class.", "labels": [], "entities": [{"text": "text categorization", "start_pos": 16, "end_pos": 35, "type": "TASK", "confidence": 0.7377388775348663}]}, {"text": "In information retrieval), text similarity improves the effectiveness of a semantic search engine by providing information which holds high similarity with the input query.", "labels": [], "entities": [{"text": "information retrieval", "start_pos": 3, "end_pos": 24, "type": "TASK", "confidence": 0.7595483958721161}, {"text": "text similarity", "start_pos": 27, "end_pos": 42, "type": "TASK", "confidence": 0.7052189111709595}]}, {"text": "In machine translation), sentence similarity can be applied for automatic evaluation of the output translation and the reference translations.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 3, "end_pos": 22, "type": "TASK", "confidence": 0.7795709371566772}, {"text": "sentence similarity", "start_pos": 25, "end_pos": 44, "type": "TASK", "confidence": 0.7113577425479889}]}, {"text": "In question answering, once the question and the candidate answers are treated as two texts, the answer text which has a higher relevance with the question text may have higher probability to be the right one.", "labels": [], "entities": [{"text": "question answering", "start_pos": 3, "end_pos": 21, "type": "TASK", "confidence": 0.9100522100925446}]}, {"text": "The STS task in * SEM Shared Task 2013 consists of two subtasks, i.e., core task and typed task, and we participate in both of them.", "labels": [], "entities": [{"text": "SEM Shared Task 2013", "start_pos": 18, "end_pos": 38, "type": "TASK", "confidence": 0.6701546311378479}]}, {"text": "The core task aims to measure the semantic similarity of two sentences, resulting in a similarity score which ranges from 5 (semantic equivalence) to 0 (no relation).", "labels": [], "entities": [{"text": "similarity score", "start_pos": 87, "end_pos": 103, "type": "METRIC", "confidence": 0.9545632004737854}]}, {"text": "The typed task is a pilot task on typed-similarity between semistructured records.", "labels": [], "entities": []}, {"text": "The types of similarity to be measured include location, author, people involved, time, events or actions, subject and description as well as the general similarity of two texts.", "labels": [], "entities": []}, {"text": "In this work we present a Support Vector Regression (SVR) system to measure sentence semantic similarity by integrating multiple measurements, i.e., string similarity, knowledge based similarity, corpus based similarity, number similarity and machine translation metrics.", "labels": [], "entities": [{"text": "sentence semantic similarity", "start_pos": 76, "end_pos": 104, "type": "TASK", "confidence": 0.6874372363090515}]}, {"text": "Most of these similarities are borrowed from previous work, e.g.,), ( \u02c7 Saric et al., 2012) and (de).", "labels": [], "entities": []}, {"text": "We also propose a novel syntactic dependency similarity.", "labels": [], "entities": []}, {"text": "Our best system ranks 35 out of 90 runs in core task and ranks 5 out of 15 runs in typed task.", "labels": [], "entities": []}, {"text": "The rest of this paper is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 describes the similarity measurements used in this work in detail.", "labels": [], "entities": [{"text": "similarity", "start_pos": 24, "end_pos": 34, "type": "METRIC", "confidence": 0.9494678974151611}]}, {"text": "Section 3 presents experiments and the results of two tasks.", "labels": [], "entities": []}, {"text": "Conclusions and future work are given in Section 4. 124 To compute semantic textual similarity, previous work has adopted multiple semantic similarity measurements.", "labels": [], "entities": []}, {"text": "In this work, we adopt 6 types of measures, i.e., string similarity, number similarity, knowledge-based similarity, corpus-based similarity, syntactic dependency similarity and machine translation similarity.", "labels": [], "entities": []}, {"text": "Most of them are borrowed from previous work due to their superior performance reported.", "labels": [], "entities": []}, {"text": "Besides, we also propose two syntactic dependency similarity measures.", "labels": [], "entities": []}, {"text": "Totally we get 33 similarity measures.", "labels": [], "entities": [{"text": "similarity", "start_pos": 18, "end_pos": 28, "type": "METRIC", "confidence": 0.981002926826477}]}, {"text": "Generally, these similarity measures are represented as numerical values and combined using regression model.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Different training data sets used for each test data set", "labels": [], "entities": []}, {"text": " Table 2: Best feature combination for each data set", "labels": [], "entities": []}, {"text": " Table 3: Final results on STS core task", "labels": [], "entities": [{"text": "STS core task", "start_pos": 27, "end_pos": 40, "type": "DATASET", "confidence": 0.6655795772870382}]}, {"text": " Table 4: Pearson correlation of features of the six aspects  on MSRpar", "labels": [], "entities": [{"text": "Pearson correlation", "start_pos": 10, "end_pos": 29, "type": "METRIC", "confidence": 0.7847664952278137}, {"text": "MSRpar", "start_pos": 65, "end_pos": 71, "type": "TASK", "confidence": 0.7051500082015991}]}]}