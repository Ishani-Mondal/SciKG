{"title": [{"text": "BUT-TYPED: Using domain knowledge for computing typed similarity", "labels": [], "entities": [{"text": "BUT-TYPED", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.9608268141746521}]}], "abstractContent": [{"text": "This paper deals with knowledge-based text processing which aims at an intuitive notion of textual similarity.", "labels": [], "entities": [{"text": "text processing", "start_pos": 38, "end_pos": 53, "type": "TASK", "confidence": 0.695190042257309}]}, {"text": "Entities and relations relevant fora particular domain are identified and disambiguated by means of semi-supervised machine learning techniques and resulting annotations are applied for computing typed-similarity of individual texts.", "labels": [], "entities": []}, {"text": "The work described in this paper particularly shows effects of the mentioned processes in the context of the *SEM 2013 pilot task on typed-similarity, apart of the Semantic Tex-tual Similarity shared task.", "labels": [], "entities": [{"text": "SEM 2013 pilot task", "start_pos": 110, "end_pos": 129, "type": "TASK", "confidence": 0.49322302639484406}, {"text": "Semantic Tex-tual Similarity shared task", "start_pos": 164, "end_pos": 204, "type": "TASK", "confidence": 0.7307884812355041}]}, {"text": "The goal is to evaluate the degree of semantic similarity between semi-structured records.", "labels": [], "entities": []}, {"text": "As the evaluation dataset has been taken from Europeana-a collection of records on European cultural heritage objects-we focus on computing a semantic distance on field author which has the highest potential to benefit from the domain knowledge.", "labels": [], "entities": [{"text": "Europeana-a collection of records on European cultural heritage objects-we", "start_pos": 46, "end_pos": 120, "type": "DATASET", "confidence": 0.9318744473987155}]}, {"text": "Specific features that are employed in our system BUT-TYPED are briefly introduced together with a discussion on their efficient acquisition.", "labels": [], "entities": [{"text": "BUT-TYPED", "start_pos": 50, "end_pos": 59, "type": "METRIC", "confidence": 0.5641952157020569}]}, {"text": "Support Vector Regression is then used to combine the features and to provide a final similarity score.", "labels": [], "entities": [{"text": "Support Vector Regression", "start_pos": 0, "end_pos": 25, "type": "METRIC", "confidence": 0.7404907544453939}, {"text": "similarity score", "start_pos": 86, "end_pos": 102, "type": "METRIC", "confidence": 0.9646124243736267}]}, {"text": "The system ranked third on the attribute author among 15 submitted runs in the typed-similarity task.", "labels": [], "entities": []}], "introductionContent": [{"text": "The goal of the pilot typed-similarity task lied in measuring a degree of semantic similarity between semi-structured records.", "labels": [], "entities": []}, {"text": "The data came from the Europeana digital library 1 collecting millions of records on paintings, books, films, and other museum and archival objects that have been digitized throughout Europe.", "labels": [], "entities": [{"text": "Europeana digital library", "start_pos": 23, "end_pos": 48, "type": "DATASET", "confidence": 0.9619816144307455}]}, {"text": "More than 2,000 cultural and scientific institutions across Europe have contributed to Europeana.", "labels": [], "entities": []}, {"text": "There are many metadata fields attached to each item in the library, but only fields title, subject, description, creator, date and source were used in the task.", "labels": [], "entities": []}, {"text": "Having this collection, it is natural to expect that domain knowledge on relevant cultural heritage entities and their inter-relations will help to measure semantic closeness between particular items.", "labels": [], "entities": []}, {"text": "When focusing on similarities in a particular field (a semantic type) that clearly covers a domain-specific aspect (such as field author/creator in our case), the significance of the domain knowledge should be the highest.", "labels": [], "entities": []}, {"text": "Intuitively, the semantic similarity among authors of two artworks corresponds to strengths of links that can be identified among the two (groups of) authors.", "labels": [], "entities": []}, {"text": "As the gold standard for the task resulted from a Mechanical Turk experiment (, it could be expected that close fields correspond to authors that are well known to represent the same style, worked in the same time or the same art branch (e. g., Gabri\u00ebl Metsu and Johannes Vermeer), come from the same region (often guessed from the names), dealt with related topics (not necessarily in the artwork described by the record in question), etc.", "labels": [], "entities": []}, {"text": "In addition to necessary evaluation of the intersection and the union of two author fields (leading naturally to the Jaccard similarity coeffi-cient on normalized name records -see below), it is therefore crucial to integrate means measuring the above-mentioned semantic links between identified authors.", "labels": [], "entities": []}, {"text": "Unfortunately, there is a lot of noise in the data used in the task.", "labels": [], "entities": []}, {"text": "Since Europeana does not precisely define meaning and purpose of each particular field in the database, many mistakes come directly from the unmanaged importing process realized by participating institutions.", "labels": [], "entities": [{"text": "Europeana", "start_pos": 6, "end_pos": 15, "type": "DATASET", "confidence": 0.9185574054718018}]}, {"text": "Fields often mix content of various semantic nature and, occasionally, they are completely misinterpreted (e. g., field creator stands for the author, but, in many cases, it contains only the institution the data comes from).", "labels": [], "entities": []}, {"text": "Moreover, the data in records is rather sparse -many fields are left empty even though the information to be filled in is included in original museum records (e. g., the author of an artwork is known but not entered).", "labels": [], "entities": []}, {"text": "The low quality of underlying data can be also responsible for results reported in related studies.", "labels": [], "entities": []}, {"text": "For example, evaluate semantic similarity between semi-structured items from Europeana.", "labels": [], "entities": []}, {"text": "They use several measures including a simple normalized textual overlap, the extended Lesk measure, the cosine similarity, a Wikipedia-based model and the LDA (Latent Dirichlet Allocation).", "labels": [], "entities": [{"text": "LDA", "start_pos": 155, "end_pos": 158, "type": "METRIC", "confidence": 0.9615082144737244}]}, {"text": "The study, restricted to fields title, subject and description, shows that the best score is obtained by the normalized overlap applied only to the title field.", "labels": [], "entities": []}, {"text": "Any other combination of the fields decreased the performance.", "labels": [], "entities": []}, {"text": "Similarly, sophisticated methods did not bring any improvement.", "labels": [], "entities": []}, {"text": "The particular gold standard (training/test data) used in the typed-similarity task is also problematic.", "labels": [], "entities": []}, {"text": "For example, it provides estimates of location-based similarity even though it makes no sense for particular two records -no field mentions a location and it cannot be inferred from other parts).", "labels": [], "entities": []}, {"text": "A throughout analysis of the task data showed that creator is the only field we could reasonably use in our experiments (although many issues discussed in previous paragraphs apply for the field as well).", "labels": [], "entities": []}, {"text": "That is why we focus on similarities between author fields in this study.", "labels": [], "entities": []}, {"text": "While aplenty of measures for computing textual similarity have been proposed and there is an active research in the fields of Textual Entailment (), Paraphrase Identification ( and, recently, the Semantic Textual Similarity (, the semi-structured record similarity is a relatively new area of research.", "labels": [], "entities": [{"text": "Textual Entailment", "start_pos": 127, "end_pos": 145, "type": "TASK", "confidence": 0.6972870826721191}, {"text": "Paraphrase Identification", "start_pos": 150, "end_pos": 175, "type": "TASK", "confidence": 0.9447955191135406}]}, {"text": "Even though we focus on a particular domain-specific field in this study, our work builds on previous results () to pre-compute semantic closeness of authors based on available biographies and other related texts.", "labels": [], "entities": []}, {"text": "The rest of the paper is organized as follows: The next section introduces the key domain-knowledge processing step of our system which aims at recognizing and disambiguating entities relevant for the cultural heritage domain.", "labels": [], "entities": []}, {"text": "The realized system and its results are described in Section 3.", "labels": [], "entities": []}, {"text": "Finally, Section 4 briefly summarizes the achievements.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Number of art-related entities from various  sources", "labels": [], "entities": []}]}