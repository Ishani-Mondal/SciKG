{"title": [{"text": "UKP-BIU: Similarity and Entailment Metrics for Student Response Analysis", "labels": [], "entities": [{"text": "UKP-BIU", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.9641424417495728}, {"text": "Student Response Analysis", "start_pos": 47, "end_pos": 72, "type": "TASK", "confidence": 0.7505603631337484}]}], "abstractContent": [{"text": "Our system combines text similarity measures with a textual entailment system.", "labels": [], "entities": []}, {"text": "In the main task, we focused on the influence of lexical-ized versus unlexicalized features, and how they affect performance on unseen questions and domains.", "labels": [], "entities": []}, {"text": "We also participated in the pilot partial entailment task, where our system significantly outperforms a strong baseline.", "labels": [], "entities": []}], "introductionContent": [{"text": "The Joint Student Response Analysis and 8th Recognizing Textual Entailment Challenge () brings together two important dimensions of Natural Language Processing: real-world applications and semantic inference technologies.", "labels": [], "entities": [{"text": "8th Recognizing Textual Entailment Challenge", "start_pos": 40, "end_pos": 84, "type": "TASK", "confidence": 0.6532643735408783}]}, {"text": "The challenge focuses on the domain of middleschool quizzes, and attempts to emulate the meticulous marking process that teachers do on a daily basis.", "labels": [], "entities": []}, {"text": "Given a question, a reference answer, and a student's answer, the task is to determine whether the student answered correctly.", "labels": [], "entities": []}, {"text": "While this is not anew task in itself, the challenge focuses on employing textual entailment technologies as the backbone of this educational application.", "labels": [], "entities": []}, {"text": "As a consequence, we formalize the question \"Did the student answer correctly?\" as \"Can the reference answer be inferred from the student's answer?\".", "labels": [], "entities": []}, {"text": "This question can (hopefully) be answered by a textual entailment system ().", "labels": [], "entities": []}, {"text": "The challenge contains two tasks: In the main task, the system must analyze each answer as a whole.", "labels": [], "entities": []}, {"text": "There are three settings, where each one defines \"correct\" in a different resolution.", "labels": [], "entities": [{"text": "correct", "start_pos": 50, "end_pos": 57, "type": "METRIC", "confidence": 0.9746107459068298}]}, {"text": "The highestresolution setting defines five different classes or \"correctness values\": correct, partially correct, contradictory, irrelevant, non-domain.", "labels": [], "entities": []}, {"text": "In the pilot task, critical elements of the answer need to be analyzed separately.", "labels": [], "entities": []}, {"text": "Each such element is called a facet, and is defined as a pair of words that are critical in answering the question.", "labels": [], "entities": []}, {"text": "As there is a substantial difference between the two tasks, we designed sibling architectures for each task, and divide the main part of the paper accordingly.", "labels": [], "entities": []}, {"text": "Our goal is to provide a robust architecture for student response analysis, that can generalize and perform well in multiple domains.", "labels": [], "entities": [{"text": "student response analysis", "start_pos": 49, "end_pos": 74, "type": "TASK", "confidence": 0.6424377858638763}]}, {"text": "Moreover, we are interested in evaluating how well general-purpose technologies will perform in this setting.", "labels": [], "entities": []}, {"text": "We therefore approach the challenge by combining two such technologies: DKPro Similarity -an extensive suite of text similarity measures-that has been successfully applied in other settings like the SemEval 2012 task on semantic textual similarity or reuse detection.", "labels": [], "entities": [{"text": "DKPro Similarity", "start_pos": 72, "end_pos": 88, "type": "TASK", "confidence": 0.6174501776695251}, {"text": "SemEval 2012 task", "start_pos": 199, "end_pos": 216, "type": "TASK", "confidence": 0.8703490694363912}, {"text": "semantic textual similarity or reuse detection", "start_pos": 220, "end_pos": 266, "type": "TASK", "confidence": 0.565225159128507}]}, {"text": "BIUTEE, the Bar-Ilan University Textual Entailment Engine (, which has shown state-of-the-art performance on recognizing textual entailment challenges.", "labels": [], "entities": [{"text": "BIUTEE", "start_pos": 0, "end_pos": 6, "type": "METRIC", "confidence": 0.9215132594108582}]}, {"text": "Our systems use both technologies to extract features, and combine them in a supervised model.", "labels": [], "entities": []}, {"text": "Indeed, this approach works relatively well (with respect to other entries in the challenge), especially in unseen domains.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Main task performance for the SciEntsBank test  set. We show weighted average F 1 for the three scenarios.", "labels": [], "entities": [{"text": "SciEntsBank test  set", "start_pos": 40, "end_pos": 61, "type": "DATASET", "confidence": 0.8045140504837036}, {"text": "weighted average F 1", "start_pos": 71, "end_pos": 91, "type": "METRIC", "confidence": 0.7189514935016632}]}, {"text": " Table 2: Confusion matrix of Run 1 in the 5-way Unseen  Domains scenario. The vertical axis is the real class, the  horizontal axis is the predicted class.", "labels": [], "entities": []}, {"text": " Table 3: Pilot task performance across different scenar- ios. The scores are F 1 -measures (weighted average).", "labels": [], "entities": [{"text": "F 1 -measures", "start_pos": 78, "end_pos": 91, "type": "METRIC", "confidence": 0.9736467152833939}]}]}