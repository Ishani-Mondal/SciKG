{"title": [{"text": "unimelb: Topic Modelling-based Word Sense Induction for Web Snippet Clustering", "labels": [], "entities": [{"text": "Topic Modelling-based Word Sense Induction", "start_pos": 9, "end_pos": 51, "type": "TASK", "confidence": 0.7139124274253845}]}], "abstractContent": [{"text": "This paper describes our system for Task 11 of SemEval-2013.", "labels": [], "entities": []}, {"text": "In the task, participants are provided with a set of ambiguous search queries and the snippets returned by a search engine, and are asked to associate senses with the snippets.", "labels": [], "entities": []}, {"text": "The snippets are then clustered using the sense assignments and systems are evaluated based on the quality of the snippet clusters.", "labels": [], "entities": []}, {"text": "Our system adopts a pre-existing Word Sense Induction (WSI) methodology based on Hierarchical Dirichlet Process (HDP), a non-parametric topic model.", "labels": [], "entities": [{"text": "Word Sense Induction (WSI)", "start_pos": 33, "end_pos": 59, "type": "TASK", "confidence": 0.732001985112826}]}, {"text": "Our system is trained over extracts from the full text of English Wikipedia, and is shown to perform well in the shared task.", "labels": [], "entities": [{"text": "English Wikipedia", "start_pos": 58, "end_pos": 75, "type": "DATASET", "confidence": 0.8023064434528351}]}], "introductionContent": [{"text": "The basic premise behind research on word sense disambiguation (WSD) is that there exists a static, discrete set of word senses that can be used to label distinct usages of a given word.", "labels": [], "entities": [{"text": "word sense disambiguation (WSD)", "start_pos": 37, "end_pos": 68, "type": "TASK", "confidence": 0.8090406358242035}]}, {"text": "There are various pitfalls underlying this premise, including: (1) what sense inventory is appropriate fora particular task (given that sense inventories can vary considerably in their granularity and partitioning of word usages)?", "labels": [], "entities": []}, {"text": "given that word senses tend to take the form of prototypes, is discrete labelling a felicitous representation of word usages, especially for non-standard word usages?", "labels": [], "entities": []}, {"text": "(3) how should novel word usages be captured under this model? and given the rapid pace of language evolution on real-time social media such as Twitter and Facebook, is it reasonable to assume a static sense inventory?", "labels": [], "entities": []}, {"text": "Given this backdrop, there has been a recent growth of interest in the task of word sense induction (WSI), where the word sense representation fora given word is automatically inferred from a given data source, and word usages are labelled (often probabilistically) according to that data source.", "labels": [], "entities": [{"text": "word sense induction (WSI)", "start_pos": 79, "end_pos": 105, "type": "TASK", "confidence": 0.8417273362477621}]}, {"text": "While WSI has considerable appeal as a task, intrinsic cross-comparison of WSI systems is fraught with many of the same issues as WSD (, leading to a move towards task-based WSI evaluation, such as in Task 11 of SemEval-2013, titled \"Evaluating Word Sense Induction & Disambiguation within an End-User Application\".", "labels": [], "entities": [{"text": "WSI evaluation", "start_pos": 174, "end_pos": 188, "type": "TASK", "confidence": 0.8903439044952393}, {"text": "Evaluating Word Sense Induction & Disambiguation within an End-User Application", "start_pos": 234, "end_pos": 313, "type": "TASK", "confidence": 0.7606552451848984}]}, {"text": "This paper presents the UNIMELB system entry to SemEval-2013 Task 11.", "labels": [], "entities": [{"text": "SemEval-2013 Task 11", "start_pos": 48, "end_pos": 68, "type": "TASK", "confidence": 0.7741239865620931}]}, {"text": "Our method is based heavily on the WSI methodology proposed by for novel word sense detection.", "labels": [], "entities": [{"text": "word sense detection", "start_pos": 73, "end_pos": 93, "type": "TASK", "confidence": 0.7615656852722168}]}, {"text": "Largely the same methodology was also applied to SemEval-2013 Task 13 on WSI (Lau et al., to appear).", "labels": [], "entities": [{"text": "SemEval-2013 Task 13 on WSI", "start_pos": 49, "end_pos": 76, "type": "TASK", "confidence": 0.6584736287593842}]}], "datasetContent": [{"text": "Following, we use the default parameters (\u03b3 = 0.1 and \u03b1 0 = 1.0) for HDP.", "labels": [], "entities": []}, {"text": "For each search query, we apply HDP to induce the senses, and a distribution of senses is produced for each \"document\" in the model.", "labels": [], "entities": []}, {"text": "As the snippets in the test dataset correspond to the documents in the model and evaluation is based on \"hard\" clusters of snippets, we assign a sense to each snippet based on the sense (= topic) which has the highest probability for that snippet.", "labels": [], "entities": []}, {"text": "The task requires participants to produce a ranked list of snippets for each induced sense, based on the relative fit between the snippet and the sense.", "labels": [], "entities": []}, {"text": "We induce the ranking based on the sense probabilities assigned to the senses, such that snippets that have the highest probability of the induced sense are ranked highest, and snippets with lower sense probabilities are ranked lower.", "labels": [], "entities": []}, {"text": "Two classes of evaluation are used in the shared task: 1.", "labels": [], "entities": []}, {"text": "cluster quality measures: Jaccard Index (JI), RandIndex (RI), Adjusted RandIndex (ARI) and F1; 2.", "labels": [], "entities": [{"text": "Jaccard Index (JI)", "start_pos": 26, "end_pos": 44, "type": "METRIC", "confidence": 0.810952651500702}, {"text": "RandIndex (RI)", "start_pos": 46, "end_pos": 60, "type": "METRIC", "confidence": 0.8248210847377777}, {"text": "Adjusted RandIndex (ARI)", "start_pos": 62, "end_pos": 86, "type": "METRIC", "confidence": 0.9131263017654419}, {"text": "F1", "start_pos": 91, "end_pos": 93, "type": "METRIC", "confidence": 0.9963549375534058}]}, {"text": "diversification of search results: Subtopic Recall@K and Subtopic Precision@r.", "labels": [], "entities": []}, {"text": "Details of the evaluation measures are described in.", "labels": [], "entities": []}, {"text": "The idea behind the second form of evaluation (i.e. diversification of search results) is that search engine results should cluster the results based on senses (of the query term in the documents) given an ambiguous query.", "labels": [], "entities": []}, {"text": "For example, if a user searches for apple, the search engine may return results related to both the computer brand sense and the fruit sense of apple.", "labels": [], "entities": []}, {"text": "Given this assumption, the best WSI/WSD system is the one that can correctly identify the diversity of senses in the snippets.", "labels": [], "entities": []}, {"text": "Cluster quality, subtopic recall@K and subtopic precision@r results for all systems entered in the task are presented in, and, respectively.", "labels": [], "entities": [{"text": "recall", "start_pos": 26, "end_pos": 32, "type": "METRIC", "confidence": 0.934758722782135}, {"text": "precision@r", "start_pos": 48, "end_pos": 59, "type": "METRIC", "confidence": 0.8707835872968038}]}, {"text": "In terms of cluster quality, our systems (HDP-CLUSTERS-LEMMA and HDP-CLUSTERS-NOLEMMA) consistently outperform the other teams for all measures except for the Jaccard Index (where we rank second and third, by a narrow margin).", "labels": [], "entities": [{"text": "Jaccard Index", "start_pos": 159, "end_pos": 172, "type": "DATASET", "confidence": 0.7662724256515503}]}, {"text": "The average number of induced clusters and the average cluster size of our systems are similar to those of the gold standard system (GOLD), indicating that our systems are learning an appropriate sense granularity.", "labels": [], "entities": []}, {"text": "In terms of diversification of search results, our systems perform markedly better than most teams, other than RAKESH which trails closely behind our systems (despite a relatively low ranking in terms of the cluster quality evaluation).", "labels": [], "entities": []}, {"text": "Overall, the results are encouraging and our system performs very well over the task.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: An example of topic model features.", "labels": [], "entities": []}, {"text": " Table 2: Cluster quality results for all systems. The best result for each column is presented in boldface. SINGLETON  and ALLINONE are baseline systems and GOLD is the theoretical upper-bound for the task.", "labels": [], "entities": [{"text": "SINGLETON", "start_pos": 109, "end_pos": 118, "type": "METRIC", "confidence": 0.7799985408782959}, {"text": "ALLINONE", "start_pos": 124, "end_pos": 132, "type": "METRIC", "confidence": 0.9646545648574829}, {"text": "GOLD", "start_pos": 158, "end_pos": 162, "type": "METRIC", "confidence": 0.9933921694755554}]}]}