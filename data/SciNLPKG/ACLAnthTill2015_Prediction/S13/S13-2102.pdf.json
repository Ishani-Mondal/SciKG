{"title": [{"text": "CoMeT: Integrating different levels of linguistic modeling for meaning assessment", "labels": [], "entities": [{"text": "meaning assessment", "start_pos": 63, "end_pos": 81, "type": "TASK", "confidence": 0.9013279676437378}]}], "abstractContent": [{"text": "This paper describes the CoMeT system, our contribution to the SemEval 2013 Task 7 challenge , focusing on the task of automatically assessing student answers to factual questions.", "labels": [], "entities": [{"text": "SemEval 2013 Task 7 challenge", "start_pos": 63, "end_pos": 92, "type": "TASK", "confidence": 0.8082961678504944}]}, {"text": "CoMeT is based on a meta-classifier that uses the outputs of the subsystems we developed: CoMiC, CoSeC, and three shallower bag approaches.", "labels": [], "entities": [{"text": "CoMeT", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.8400714993476868}]}, {"text": "We sketch the functionality of all subsystems and evaluate their performance against the official test set of the challenge.", "labels": [], "entities": []}, {"text": "CoMeT obtained the best result (73.1% accuracy) for the 3-way unseen answers in Beetle among all challenge participants.", "labels": [], "entities": [{"text": "CoMeT", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.8302061557769775}, {"text": "accuracy", "start_pos": 38, "end_pos": 46, "type": "METRIC", "confidence": 0.9928722381591797}, {"text": "Beetle", "start_pos": 80, "end_pos": 86, "type": "DATASET", "confidence": 0.8865824341773987}]}, {"text": "We also discuss possible improvements and directions for future research.", "labels": [], "entities": []}], "introductionContent": [{"text": "Our contribution to the SemEval 2013 Task 7 challenge () presented here is based on our research in the A4 project 1 of the SFB 833, which is dedicated to the question how meaning can be computationally compared in realistic situations.", "labels": [], "entities": [{"text": "SemEval 2013 Task 7 challenge", "start_pos": 24, "end_pos": 53, "type": "TASK", "confidence": 0.8628920316696167}, {"text": "A4 project 1 of the SFB 833", "start_pos": 104, "end_pos": 131, "type": "DATASET", "confidence": 0.7281720680849892}]}, {"text": "In realistic situations, utterances are not necessarily well-formed or complete, there maybe individual differences in situative and world knowledge among the speakers.", "labels": [], "entities": []}, {"text": "This can complicate or even preclude a complete linguistic analysis, leading us to the following research question: Which linguistic representations can be used effectively and robustly for comparing the meaning of sentences and text fragments computationally?", "labels": [], "entities": [{"text": "comparing the meaning of sentences and text fragments computationally", "start_pos": 190, "end_pos": 259, "type": "TASK", "confidence": 0.7768131362067329}]}, {"text": "1 http://purl.org/dm/projects/sfb833-a4 In order to work on effective and robust processing, we base our work on reading comprehension exercises for foreign language learners, of which we are also collecting a large corpus ).", "labels": [], "entities": []}, {"text": "Our first system, CoMiC, is an alignment-based approach which exists in English and German variants).", "labels": [], "entities": []}, {"text": "CoMiC uses various levels of linguistic abstraction from surface tokens to dependency parses.", "labels": [], "entities": []}, {"text": "Further work that we are starting to tackle includes the utilization of Information Structure) in the system.", "labels": [], "entities": []}, {"text": "The second approach emerging from the research project is CoSeC (), a semantics-based system for meaning comparison that was developed for German from the start and was ported to operate on English for this shared task.", "labels": [], "entities": [{"text": "meaning comparison", "start_pos": 97, "end_pos": 115, "type": "TASK", "confidence": 0.6843001991510391}]}, {"text": "As a novel contribution in this paper, we present CoMeT (Comparing Meaning in T\u00fcbingen), a system that employs a meta-classifier for combining the output of CoMiC and CoSeC and three shallower bag approaches.", "labels": [], "entities": []}, {"text": "In terms of the general context of our work, short answer assessment essentially comes in the two flavors of meaning comparison and grading, the first trying to determine whether or not two utterances convey the same meaning, the latter aimed at grading the abilities of students (cf. ). Short answer assessment is also closely related to the field of Recognizing Textual Entailment (RTE,, which this year is directly reflected by the fact that SemEval 2013 Task 7 is the Joint Student Response Analysis and 8th Recognizing Textual Entailment Challenge.", "labels": [], "entities": [{"text": "short answer assessment", "start_pos": 45, "end_pos": 68, "type": "TASK", "confidence": 0.7865311900774637}, {"text": "meaning comparison", "start_pos": 109, "end_pos": 127, "type": "TASK", "confidence": 0.722704216837883}, {"text": "Recognizing Textual Entailment (RTE", "start_pos": 352, "end_pos": 387, "type": "TASK", "confidence": 0.7794050693511962}, {"text": "SemEval 2013 Task", "start_pos": 445, "end_pos": 462, "type": "TASK", "confidence": 0.707971473534902}]}, {"text": "Turning to the organization of this paper, section 2 introduces the three types of sub-systems and the meta-classifier.", "labels": [], "entities": []}, {"text": "In section 3, we report on the evaluation results of each sub-system both for our development set as well as for the official test set of the shared task.", "labels": [], "entities": []}, {"text": "We then discuss possible causes and implications of the findings we made by participating in the shared task.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we present the results for each of the sub-systems, both on the custom-made split of the training data we used in our development, as well as on the official test data of the SemEval 2013 Task 7 challenge.", "labels": [], "entities": [{"text": "SemEval 2013 Task 7 challenge", "start_pos": 192, "end_pos": 221, "type": "TASK", "confidence": 0.6291104435920716}]}, {"text": "Subsequently, we discuss possible causes for issues raised by our evaluation results.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: Development set: accuracy for 2-way task (uA:  unseen answers, uQ: unseen questions, uT: unseen topics)", "labels": [], "entities": [{"text": "accuracy", "start_pos": 27, "end_pos": 35, "type": "METRIC", "confidence": 0.99954754114151}]}, {"text": " Table 4: Official test set: overall accuracy of CoMeT (uA:  unseen answers, uQ: unseen questions, uT: unseen topics)", "labels": [], "entities": [{"text": "accuracy", "start_pos": 37, "end_pos": 45, "type": "METRIC", "confidence": 0.9961053729057312}]}, {"text": " Table 5: Official test set: accuracy for 2-way task (uA:  unseen answers, uQ: unseen questions, uT: unseen topics)", "labels": [], "entities": [{"text": "accuracy", "start_pos": 29, "end_pos": 37, "type": "METRIC", "confidence": 0.999339759349823}]}]}