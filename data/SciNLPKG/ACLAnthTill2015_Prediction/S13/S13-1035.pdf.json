{"title": [{"text": "A Dataset of Syntactic-Ngrams over Time from a Very Large Corpus of English Books", "labels": [], "entities": []}], "abstractContent": [{"text": "We created a dataset of syntactic-ngrams (counted dependency-tree fragments) based on a corpus of 3.5 million English books.", "labels": [], "entities": []}, {"text": "The dataset includes over 10 billion distinct items covering a wide range of syntactic configurations.", "labels": [], "entities": []}, {"text": "It also includes temporal information, facilitating new kinds of research into lexical semantics overtime.", "labels": [], "entities": [{"text": "lexical semantics overtime", "start_pos": 79, "end_pos": 105, "type": "TASK", "confidence": 0.7280735373497009}]}, {"text": "This paper describes the dataset, the syntactic representation, and the kinds of information provided.", "labels": [], "entities": []}], "introductionContent": [{"text": "The distributional hypothesis of states that properties of words can be captured based on their contexts.", "labels": [], "entities": []}, {"text": "The consequences of this hypothesis have been leveraged to a great effect by the NLP community, resulting in algorithms for inferring syntactic as well as semantic properties of words (see e.g. ( and the references therein).", "labels": [], "entities": []}, {"text": "In this paper, we describe a very large dataset of syntactic-ngrams, that is, structures in which the contexts of words are based on their respective position in a syntactic parse tree, and not on their sequential order in the sentence: the different words in the ngram maybe far apart from each other in the sentence, yet close to each other syntactically.", "labels": [], "entities": []}, {"text": "See for an example of a syntactic-ngram.", "labels": [], "entities": []}, {"text": "The utility of syntactic contexts of words for constructing vector-space models of word meanings is well established.", "labels": [], "entities": []}, {"text": "Syntactic relations are successfully used for modeling selectional preferences; * Work performed while at Google., and dependency paths are also used to infer binary relations between words (.", "labels": [], "entities": []}, {"text": "The use of syntacticngrams holds promise also for improving the accuracy of core NLP tasks such as syntactic languagemodeling () and syntactic-parsing (, though most successful attempts to improve syntactic parsing by using counts from large corpora are based on sequential rather than syntactic information (, we believe this is because large-scale datasets of syntactic counts are not readily available.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 64, "end_pos": 72, "type": "METRIC", "confidence": 0.9978062510490417}, {"text": "syntactic parsing", "start_pos": 197, "end_pos": 214, "type": "TASK", "confidence": 0.7526876926422119}]}, {"text": "Unfortunately, most work utilizing counts from large textual corpora does not use a standardized corpora for constructing their models, making it very hard to reproduce results and challenging to compare results across different studies.", "labels": [], "entities": []}, {"text": "Our aim in this work is not to present new methods or results, but rather to provide anew kind of a large-scale (based on corpora about 100 times larger than previous efforts) high-quality and standard resource for researchers to build upon.", "labels": [], "entities": []}, {"text": "Instead of focusing on a specific task, we aim to provide a flexible resource that could be adapted to many possible tasks.", "labels": [], "entities": []}, {"text": "Specifically, the contribution of this work is in creating a dataset of syntactic-ngrams which is: \u2022 Derived from a very large (345 billion words) corpus spanning along time period.", "labels": [], "entities": []}, {"text": "\u2022 Covers a wide range of syntactic phenomena and is adaptable to many use cases.", "labels": [], "entities": []}, {"text": "\u2022 Based on state-of-the-art syntactic processing in a modern syntactic representation.", "labels": [], "entities": []}, {"text": "\u2022 Broken down by year of occurrence, as well: A syntactic ngram appearing 112 times in the extended-biarcs set, which include structures containing three content words (see Section 4).", "labels": [], "entities": []}, {"text": "Grayed items are non-content words and are not included in the word count.", "labels": [], "entities": []}, {"text": "The dashed auxiliary \"have\" is a functional marker (see Section 3), appearing only in the extended-* sets.", "labels": [], "entities": []}, {"text": "as some coarse-grained regional and genre distinctions (British, American, Fiction).", "labels": [], "entities": []}, {"text": "\u2022 Freely available for non-commercial use.", "labels": [], "entities": []}, {"text": "After describing the underlying syntactic representation, we will present our definition of a syntacticngram, and detail the kinds of syntactic-ngrams we chose to include in the dataset.", "labels": [], "entities": []}, {"text": "Then, we present details of the corpus and the syntactic processing we performed.", "labels": [], "entities": []}, {"text": "With respect to previous efforts, the dataset has the following distinguishing characteristics: Temporal Dimension A unique aspect of our dataset is the temporal dimension, allowing inspection of how the contexts of different words vary overtime.", "labels": [], "entities": []}, {"text": "For example, one could examine how the meaning of a word evolves overtime by looking at the contexts it appears in within different time periods.", "labels": [], "entities": []}, {"text": "shows the cosine similarity between the word \"rock\" and the words \"stone\" and \"jazz\" from year 1930 to 2000, showing that rock acquired anew meaning around 1968.", "labels": [], "entities": []}, {"text": "Large syntactic contexts Previous efforts of providing syntactic counts from large scale corpora) focus on relations between two content words.", "labels": [], "entities": []}, {"text": "Our dataset include structures covering much larger tree fragments, some of them including 5 or more content words.", "labels": [], "entities": []}, {"text": "By including such structures we hope to encourage research exploring higher orders of interactions, for example modeling the relation between adjectives of two conjoined nouns, the interactions between subjects and objects of verbs, or fine-grained selectional preferences of verbs and nouns.", "labels": [], "entities": []}, {"text": "to become similar to \"jazz\" around 1968.", "labels": [], "entities": []}, {"text": "The plot shows the cosine similarity between the immediate syntactic contexts of of the word \"rock\" in each year, to the immediate syntactic contexts of the words \"jazz\" (in red) and \"stone\" (in blue) aggregated overall years.", "labels": [], "entities": []}, {"text": "A closely related effort to add syntactic annotation to the books corpus is described in.", "labels": [], "entities": []}, {"text": "That effort emphasize an interactive query interface covering several languages, in which the underlying syntactic representations are linearngrams enriched with universal part-of-speech tags, as well as first order unlabeled dependencies.", "labels": [], "entities": []}, {"text": "In contrast, our emphasis is not on an easy-to-use query interface but instead a useful and flexible resource for computational-minded researchers.", "labels": [], "entities": []}, {"text": "We focus on English and use finer-grained English-specific POS-tags.", "labels": [], "entities": []}, {"text": "The syntactic analysis is done using a more accurate parser, and we provide counts over labeled tree fragments, covering a diverse set of treefragments many of which include more than two content words.", "labels": [], "entities": []}, {"text": "Counted Fragments instead of complete trees While some efforts provide complete parse trees from large corpora), we instead provide counted tree fragments.", "labels": [], "entities": []}, {"text": "We believe that our form of aggregate information is of more immediate use than the raw parse trees.", "labels": [], "entities": []}, {"text": "While access to the parse trees may allow for somewhat greater flexibility in the kinds of questions one could ask, it also comes with a very hefty price tag in terms of the required computational resources: while counting seems trivial, it is, in fact, quite demanding computationally when done on such a scale, and requires a massive infrastructure.", "labels": [], "entities": []}, {"text": "By lifting this burden of NLP researchers, we hope to free them to tackle interesting research questions.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": []}