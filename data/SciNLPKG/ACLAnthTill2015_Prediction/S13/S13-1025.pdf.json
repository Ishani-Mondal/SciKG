{"title": [{"text": "DLS@CU-CORE: A Simple Machine Learning Model of Semantic Textual Similarity", "labels": [], "entities": [{"text": "DLS@CU-CORE", "start_pos": 0, "end_pos": 11, "type": "DATASET", "confidence": 0.8170048395792643}]}], "abstractContent": [{"text": "We present a system submitted in the Semantic Textual Similarity (STS) task at the Second Joint Conference on Lexical and Computational Semantics (*SEM 2013).", "labels": [], "entities": [{"text": "Semantic Textual Similarity (STS) task at the Second Joint Conference on Lexical and Computational Semantics (*SEM 2013)", "start_pos": 37, "end_pos": 157, "type": "TASK", "confidence": 0.8709138631820679}]}, {"text": "Given two short text fragments, the goal of the system is to determine their semantic similarity.", "labels": [], "entities": []}, {"text": "Our system makes use of three different measures of text similarity: word n-gram overlap, character n-gram overlap and semantic overlap.", "labels": [], "entities": []}, {"text": "Using these measures as features, it trains a support vector regression model on SemEval STS 2012 data.", "labels": [], "entities": [{"text": "SemEval STS 2012 data", "start_pos": 81, "end_pos": 102, "type": "DATASET", "confidence": 0.7438349276781082}]}, {"text": "This model is then applied on the STS 2013 data to compute textual similarities.", "labels": [], "entities": [{"text": "STS 2013 data", "start_pos": 34, "end_pos": 47, "type": "DATASET", "confidence": 0.9012355009714762}]}, {"text": "Two different selections of training data result in very different performance levels: while a correlation of 0.4135 with gold standards was observed in the official evaluation (ranked 63 rd among all systems) for one selection, the other resulted in a correlation of 0.5352 (that would rank 21 st).", "labels": [], "entities": []}], "introductionContent": [{"text": "Automatically identifying the semantic similarity between two short text fragments (e.g. sentences) is an important research problem having many important applications in natural language processing, information retrieval, and digital education.", "labels": [], "entities": [{"text": "Automatically identifying the semantic similarity between two short text fragments (e.g. sentences)", "start_pos": 0, "end_pos": 99, "type": "TASK", "confidence": 0.8444568174225944}, {"text": "natural language processing", "start_pos": 171, "end_pos": 198, "type": "TASK", "confidence": 0.6479659477869669}, {"text": "information retrieval", "start_pos": 200, "end_pos": 221, "type": "TASK", "confidence": 0.8444767892360687}]}, {"text": "Examples include automatic text summarization, question answering, essay grading, among others.", "labels": [], "entities": [{"text": "automatic text summarization", "start_pos": 17, "end_pos": 45, "type": "TASK", "confidence": 0.5744176010290781}, {"text": "question answering", "start_pos": 47, "end_pos": 65, "type": "TASK", "confidence": 0.910758912563324}, {"text": "essay grading", "start_pos": 67, "end_pos": 80, "type": "TASK", "confidence": 0.7112587541341782}]}, {"text": "However, despite having important applications, semantic similarity identification at the level of short text fragments is a relatively recent area of investigation.", "labels": [], "entities": [{"text": "semantic similarity identification", "start_pos": 48, "end_pos": 82, "type": "TASK", "confidence": 0.894559919834137}]}, {"text": "The problem was formally brought to attention and the first solutions were proposed in 2006 with the works reported in () and ().", "labels": [], "entities": []}, {"text": "Work prior to these focused primarily on large documents (or individual words) ().", "labels": [], "entities": []}, {"text": "But the sentencelevel granularity of the problem is characterized by factors like high specificity and low topicality of the expressed information, and potentially small lexical overlap even between very similar texts, asking for an approach different from those that were designed for larger texts.", "labels": [], "entities": []}, {"text": "Since its inception, the problem has seen a large number of solutions in a relatively small amount of time.", "labels": [], "entities": []}, {"text": "The central idea behind most solutions is the identification and alignment of semantically similar or related words across the two sentences, and the aggregation of these similarities to generate an overall similarity score (.", "labels": [], "entities": [{"text": "identification and alignment of semantically similar or related words", "start_pos": 46, "end_pos": 115, "type": "TASK", "confidence": 0.6793396373589834}]}, {"text": "The Semantic Textual Similarity task (STS) organized as part of the Semantic Evaluation Exercises (see) fora description of STS 2012) provides a common platform for evaluation of such systems via comparison with humanannotated similarity scores over a large dataset.", "labels": [], "entities": [{"text": "Semantic Textual Similarity task (STS)", "start_pos": 4, "end_pos": 42, "type": "TASK", "confidence": 0.7878562595163073}]}, {"text": "In this paper, we present a system which was submitted in STS 2013.", "labels": [], "entities": [{"text": "STS 2013", "start_pos": 58, "end_pos": 66, "type": "DATASET", "confidence": 0.8870677351951599}]}, {"text": "Our system is based on very simple measures of lexical and character-level overlap, semantic overlap between the two sentences based on word relatedness measures, and surface features like the sentences' lengths.", "labels": [], "entities": []}, {"text": "These measures are used as features fora support vector regression model that we train with annotated data from SemEval STS 2012.", "labels": [], "entities": [{"text": "SemEval STS 2012", "start_pos": 112, "end_pos": 128, "type": "DATASET", "confidence": 0.7016764481862386}]}, {"text": "Finally, the trained model is applied on the STS 2013 test pairs.", "labels": [], "entities": [{"text": "STS 2013 test pairs", "start_pos": 45, "end_pos": 64, "type": "DATASET", "confidence": 0.8724105060100555}]}, {"text": "Our approach is inspired by the success of similar systems in STS 2012: systems that combine multiple measures of similarity using a machine learning model to generate an overall score.", "labels": [], "entities": []}, {"text": "We wanted to investigate how a minimal system of this kind, making use of very few external resources, performs on a large dataset.", "labels": [], "entities": []}, {"text": "Our experiments reveal that the performance of such a system depends highly on the training data.", "labels": [], "entities": []}, {"text": "While training on one dataset yielded a best correlation (among our three runs, described later in this document) of only 0.4135 with the gold scores, training on another dataset showed a considerably higher correlation of 0.5352.", "labels": [], "entities": []}], "datasetContent": [{"text": "The STS 2013 test data consists of four datasets: two datasets consisting of gloss pairs (OnWN: 561 pairs and FNWN: 189 pairs), a dataset of machine translation evaluation pairs (SMT: 750 pairs) and a dataset consisting of news headlines (headlines: 750 pairs).", "labels": [], "entities": [{"text": "STS 2013 test data", "start_pos": 4, "end_pos": 22, "type": "DATASET", "confidence": 0.8090700581669807}, {"text": "OnWN", "start_pos": 90, "end_pos": 94, "type": "METRIC", "confidence": 0.5362809300422668}, {"text": "FNWN", "start_pos": 110, "end_pos": 114, "type": "METRIC", "confidence": 0.8155248761177063}]}, {"text": "For each dataset, the output of a system is evaluated via comparison with human-annotated similarity scores and measured using the Pearson Correlation Coefficient.", "labels": [], "entities": [{"text": "Pearson Correlation Coefficient", "start_pos": 131, "end_pos": 162, "type": "METRIC", "confidence": 0.8262674609820048}]}, {"text": "Then a weighted sum of the correlations for all datasets are taken to be the final score, where each dataset's weight is the proportion of sentence pairs in that dataset.", "labels": [], "entities": []}, {"text": "We computed the similarity scores using three different feature sets (for our three runs) for the support vector regression model: 1.", "labels": [], "entities": [{"text": "similarity scores", "start_pos": 16, "end_pos": 33, "type": "METRIC", "confidence": 0.9624795317649841}]}, {"text": "All features mentioned in Section 2.", "labels": [], "entities": []}, {"text": "This set of features were used in our run 1. 2. All features except word \u00ed \u00b5\u00ed\u00b1\u009b-gram overlap (experiments on STS 2012 test data revealed that using word n-grams actually lowers the performance of our model, hence this decision).", "labels": [], "entities": [{"text": "word \u00ed \u00b5\u00ed\u00b1\u009b-gram overlap", "start_pos": 68, "end_pos": 92, "type": "METRIC", "confidence": 0.8787807722886404}, {"text": "STS 2012 test data", "start_pos": 109, "end_pos": 127, "type": "DATASET", "confidence": 0.863766223192215}]}, {"text": "These are the features that were used in our run 2. 3. Only character \u00ed \u00b5\u00ed\u00b1\u009b-gram and length features (just to test the performance of the model without any semantic features).", "labels": [], "entities": [{"text": "character \u00ed \u00b5\u00ed\u00b1\u009b-gram", "start_pos": 60, "end_pos": 81, "type": "METRIC", "confidence": 0.6980039477348328}]}, {"text": "Our run 3 was based on these features.", "labels": [], "entities": []}, {"text": "We trained the support vector regression model on two different training datasets, both drawn from STS 2012 data: 1.", "labels": [], "entities": [{"text": "STS 2012 data", "start_pos": 99, "end_pos": 112, "type": "DATASET", "confidence": 0.9120213389396667}]}, {"text": "In the first setup, we chose the training datasets from STS 2012 that we considered the most similar to the test dataset.", "labels": [], "entities": [{"text": "training datasets from STS 2012", "start_pos": 33, "end_pos": 64, "type": "DATASET", "confidence": 0.7757964611053467}]}, {"text": "The only exception was the FNWN dataset, for which we selected the all the datasets from 2012 because no single dataset from STS 2012 seemed to have similarity with this dataset.", "labels": [], "entities": [{"text": "FNWN dataset", "start_pos": 27, "end_pos": 39, "type": "DATASET", "confidence": 0.9870943129062653}]}, {"text": "For the OnWN test dataset, we selected the OnWN dataset from STS 2012.", "labels": [], "entities": [{"text": "OnWN test dataset", "start_pos": 8, "end_pos": 25, "type": "DATASET", "confidence": 0.9487560391426086}, {"text": "OnWN dataset from STS 2012", "start_pos": 43, "end_pos": 69, "type": "DATASET", "confidence": 0.9638994097709656}]}, {"text": "For both headlines and SMT, we selected SMTnews and SMTeuroparl from STS 2012.", "labels": [], "entities": [{"text": "SMT", "start_pos": 23, "end_pos": 26, "type": "TASK", "confidence": 0.9169384837150574}, {"text": "SMTnews", "start_pos": 40, "end_pos": 47, "type": "DATASET", "confidence": 0.779206395149231}, {"text": "SMTeuroparl from STS 2012", "start_pos": 52, "end_pos": 77, "type": "DATASET", "confidence": 0.6335724368691444}]}, {"text": "The rationale behind this selection was to train the machine learning model on a distribution similar to the test data.", "labels": [], "entities": []}, {"text": "2. In the second setup, we aggregated all datasets (train and test) from STS 2012 and used this combined dataset to train the three models that were later applied on each STS 2013 test data.", "labels": [], "entities": [{"text": "STS 2013 test data", "start_pos": 171, "end_pos": 189, "type": "DATASET", "confidence": 0.722161054611206}]}, {"text": "Here the rationale is to train on as much data as possible.", "labels": [], "entities": []}, {"text": "shows the results for the first setup.", "labels": [], "entities": []}, {"text": "This is the performance of the set of scores which we actually submitted in STS 2013.", "labels": [], "entities": [{"text": "STS 2013", "start_pos": 76, "end_pos": 84, "type": "DATASET", "confidence": 0.7556383013725281}]}, {"text": "The first four columns show the correlations of our system with the gold standard for all runs.", "labels": [], "entities": []}, {"text": "The rightmost column shows the overall weighted correlations.", "labels": [], "entities": []}, {"text": "As we can see, run 1 with all the features demonstrated the best performance among the three runs.", "labels": [], "entities": []}, {"text": "There was a considerable drop in performance in run 3 which did not utilize any semantic similarity measure.", "labels": [], "entities": []}, {"text": "As evident from the table, evaluation results did not indicate a particularly promising system.", "labels": [], "entities": []}, {"text": "Our best system ranked 63 rd among the 90 systems evaluated in STS 2013.", "labels": [], "entities": [{"text": "STS 2013", "start_pos": 63, "end_pos": 71, "type": "DATASET", "confidence": 0.6521014273166656}]}, {"text": "We further investigated to find out the reason: is the set of our features insufficient to capture text semantic similarity, or were the training data inappropriate for their corresponding test data?", "labels": [], "entities": []}, {"text": "This is why we experimented with the second setup discussed above.", "labels": [], "entities": []}, {"text": "Following are the results: As we can see in, the correlations for all feature sets improved by more than 10% for each run.", "labels": [], "entities": []}, {"text": "In this case, the best system with correlation 0.5352 would rank 21 st among all systems in STS 2013.", "labels": [], "entities": [{"text": "correlation", "start_pos": 35, "end_pos": 46, "type": "METRIC", "confidence": 0.9737557768821716}, {"text": "STS 2013", "start_pos": 92, "end_pos": 100, "type": "DATASET", "confidence": 0.6796362400054932}]}, {"text": "These results indicate that the primary reason behind the system's previous bad performance (Table 1) was the selection of an inappropriate dataset.", "labels": [], "entities": []}, {"text": "Although it was not clear in the beginning which of the two options would be the better, this second experiment reveals that selecting the largest possible dataset to train is the better choice for this dataset.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1. Results for manually selected training data", "labels": [], "entities": []}, {"text": " Table 2. Results for combined training data", "labels": [], "entities": []}]}