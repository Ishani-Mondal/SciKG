{"title": [{"text": "IIRG: A Na\u00a8\u0131veNa\u00a8\u0131ve Approach to Evaluating Phrasal Semantics", "labels": [], "entities": [{"text": "IIRG", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.7915260195732117}, {"text": "Na\u00a8\u0131veNa\u00a8\u0131ve Approach", "start_pos": 8, "end_pos": 29, "type": "METRIC", "confidence": 0.46572697659333545}]}], "abstractContent": [{"text": "This paper describes the IIRG 1 system entered in SemEval-2013, the 7th International Workshop on Semantic Evaluation.", "labels": [], "entities": [{"text": "IIRG 1", "start_pos": 25, "end_pos": 31, "type": "TASK", "confidence": 0.6700957715511322}, {"text": "SemEval-2013, the 7th International Workshop on Semantic Evaluation", "start_pos": 50, "end_pos": 117, "type": "TASK", "confidence": 0.6335776348908743}]}, {"text": "We participated in Task 5 Evaluating Phrasal Semantics.", "labels": [], "entities": [{"text": "Evaluating Phrasal Semantics", "start_pos": 26, "end_pos": 54, "type": "TASK", "confidence": 0.6443771719932556}]}, {"text": "We have adopted a token-based approach to solve this task using 1) Na\u00a8\u0131veNa\u00a8\u0131ve Bayes methods and 2) Word Overlap methods, both of which rely on the extraction of syntactic features.", "labels": [], "entities": []}, {"text": "We found that the word overlap method significantly out-performs the Na\u00a8\u0131veNa\u00a8\u0131ve Bayes methods, achieving our highest overall score with an accuracy of approximately 78%.", "labels": [], "entities": [{"text": "word overlap", "start_pos": 18, "end_pos": 30, "type": "TASK", "confidence": 0.6824110150337219}, {"text": "accuracy", "start_pos": 141, "end_pos": 149, "type": "METRIC", "confidence": 0.9992726445198059}]}], "introductionContent": [{"text": "The Phrasal Semantics task consists of two related subtasks.", "labels": [], "entities": [{"text": "Phrasal Semantics task", "start_pos": 4, "end_pos": 26, "type": "TASK", "confidence": 0.7322028477986654}]}, {"text": "Task 5A requires systems to evaluate the semantic similarity of words and compositional phrases.", "labels": [], "entities": []}, {"text": "Task 5B requires systems to evaluate the compositionality of phrases in context.", "labels": [], "entities": []}, {"text": "We participated in Task 5B and submitted three runs for evaluation, two runs using the Na\u00a8\u0131veNa\u00a8\u0131ve Bayes Machine Learning Algorithm and a Word Overlap run using a simple bag-of-words approach.", "labels": [], "entities": [{"text": "Na\u00a8\u0131veNa\u00a8\u0131ve Bayes Machine Learning Algorithm", "start_pos": 87, "end_pos": 132, "type": "DATASET", "confidence": 0.7299828860494826}]}, {"text": "Identifying non-literal expressions poses a major challenge in NLP because they occur frequently and often exhibit irregular behavior by not adhering to grammatical constraints.", "labels": [], "entities": [{"text": "Identifying non-literal expressions", "start_pos": 0, "end_pos": 35, "type": "TASK", "confidence": 0.9057998259862264}]}, {"text": "Previous research in the area of identifying literal/non-literal use of expressions includes generating a wide range of different features for use with a machine learning prediction algorithm.", "labels": [], "entities": [{"text": "machine learning prediction", "start_pos": 154, "end_pos": 181, "type": "TASK", "confidence": 0.6357753773530325}]}, {"text": "() present a system 1 Intelligent Information Retrieval Group involving identifying the global and local contexts of a phrase.", "labels": [], "entities": [{"text": "Intelligent Information Retrieval", "start_pos": 22, "end_pos": 55, "type": "TASK", "confidence": 0.6160702407360077}]}, {"text": "Global context was determined by looking for occurrences of semantically related words in a given passage, while local context focuses on the words immediately preceding and following the phrase.", "labels": [], "entities": []}, {"text": "Windows of five words at each side of the target were taken as features.", "labels": [], "entities": []}, {"text": "More syntactic features were also used, including details of nodes from the dependency tree of each example.", "labels": [], "entities": []}, {"text": "The system produced approximately 90% accuracy when tested, for both idiom-specific and generic models.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 38, "end_pos": 46, "type": "METRIC", "confidence": 0.9994851350784302}]}, {"text": "It was found that the statistical features (global and local contexts) performed well, even on unseen phrases.", "labels": [], "entities": []}, {"text": "( found that similarities between words in the expression and its context indicate literal usage.", "labels": [], "entities": []}, {"text": "This is comparable to, which used cohesion-based classifiers based on lexical chains and graphs.", "labels": [], "entities": []}, {"text": "Unsupervised approaches to classifying idiomatic use include clustering, which classified data based on semantic analyzability (whether the meaning of the expression is similar to the meanings of its parts) and lexical and syntactic flexibility (measurements of how much variation exists within the expression).", "labels": [], "entities": [{"text": "classifying idiomatic use", "start_pos": 27, "end_pos": 52, "type": "TASK", "confidence": 0.8466388583183289}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Results of IIRG Training Runs", "labels": [], "entities": [{"text": "IIRG Training Runs", "start_pos": 21, "end_pos": 39, "type": "TASK", "confidence": 0.6684918204943339}]}, {"text": " Table 2: Results of Runs Submitted to Sem-Eval 2013", "labels": [], "entities": [{"text": "Runs Submitted to Sem-Eval 2013", "start_pos": 21, "end_pos": 52, "type": "TASK", "confidence": 0.6888665556907654}]}, {"text": " Table 3: Results of Runs Submitted to Sem-Eval 2013 (per phrase)", "labels": [], "entities": [{"text": "Sem-Eval 2013", "start_pos": 39, "end_pos": 52, "type": "DATASET", "confidence": 0.5547551661729813}]}]}