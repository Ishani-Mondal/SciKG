{"title": [{"text": "Kea: Expression-level Sentiment Analysis from Twitter Data", "labels": [], "entities": [{"text": "Sentiment Analysis", "start_pos": 22, "end_pos": 40, "type": "TASK", "confidence": 0.8525121808052063}]}], "abstractContent": [{"text": "This paper describes an expression-level sentiment detection system that participated in the subtask A of SemEval-2013 Task 2: Sentiment Analysis in Twitter.", "labels": [], "entities": [{"text": "expression-level sentiment detection", "start_pos": 24, "end_pos": 60, "type": "TASK", "confidence": 0.6765531897544861}, {"text": "SemEval-2013 Task 2: Sentiment Analysis in Twitter", "start_pos": 106, "end_pos": 156, "type": "TASK", "confidence": 0.7612002789974213}]}, {"text": "Our system uses a supervised approach to learn the features from the training data to classify expressions in new tweets as positive, negative or neutral.", "labels": [], "entities": []}, {"text": "The proposed approach helps to understand the relevant features that contribute most in this classification task.", "labels": [], "entities": [{"text": "classification task", "start_pos": 93, "end_pos": 112, "type": "TASK", "confidence": 0.8974872827529907}]}], "introductionContent": [{"text": "In recent years, Twitter has emerged as an ubiquitous and an opportune platform for social activity.", "labels": [], "entities": []}, {"text": "Analyzing the sentiments of the tweets expressed by an international user-base can provide an approximate view of how people feel.", "labels": [], "entities": []}, {"text": "One of the biggest challenges of working with tweets is their short length.", "labels": [], "entities": []}, {"text": "Additionally, the language used in tweets is very informal, with creative spellings and punctuation, misspellings, slang, new words, URLs, and genrespecific terminology and abbreviations, such as, RT for \"re-tweet\" and #hashtags, which area type of tagging for tweets.", "labels": [], "entities": [{"text": "RT", "start_pos": 197, "end_pos": 199, "type": "METRIC", "confidence": 0.8524323105812073}]}, {"text": "Although several systems tackle the task of analyzing sentiments from tweets, the task of analyzing sentiments at term or phrase-level within a tweet has remained largely unexplored.", "labels": [], "entities": []}, {"text": "This paper describes the details of our expressionlevel sentiment detection system that participated in the subtask A of SemEval-2013 Task 2: Sentiment Analysis in Twitter ().", "labels": [], "entities": [{"text": "expressionlevel sentiment detection", "start_pos": 40, "end_pos": 75, "type": "TASK", "confidence": 0.7240288654963175}, {"text": "SemEval-2013 Task 2", "start_pos": 121, "end_pos": 140, "type": "TASK", "confidence": 0.8467279076576233}, {"text": "Sentiment Analysis in Twitter", "start_pos": 142, "end_pos": 171, "type": "TASK", "confidence": 0.818438783288002}]}, {"text": "The goal is to mark expressions (a term or short phrases) in a tweet with their contextual polarity.", "labels": [], "entities": []}, {"text": "This is challenging given the fact that the entire length of a tweet is restricted to just 140 characters.", "labels": [], "entities": []}, {"text": "We describe the creation of an SVM classifier that is used to classify the contextual polarity of expressions within tweets.", "labels": [], "entities": []}, {"text": "A feature set derived from various linguistic features, parts-of-speech tagging and prior sentiment lexicons was used to train the classifier.", "labels": [], "entities": [{"text": "parts-of-speech tagging", "start_pos": 56, "end_pos": 79, "type": "TASK", "confidence": 0.7679934799671173}]}], "datasetContent": [{"text": "The task organizers made available a test data set composed of 4435 tweets where each tweet contained an instance of an expression whose sentiment was to be detected.", "labels": [], "entities": []}, {"text": "Another test corpus of 2334 SMS messages was also used in the evaluation to test how well a system trained on tweets generalizes on other data types.", "labels": [], "entities": []}, {"text": "The metric for evaluating the systems is Fmeasure.", "labels": [], "entities": [{"text": "Fmeasure", "start_pos": 41, "end_pos": 49, "type": "METRIC", "confidence": 0.995729386806488}]}, {"text": "We participated in the \"constrained\" version of the task which meant working with only the provided training data and no additional tweets/SMS messages or sentences with sentiment annotations were used.", "labels": [], "entities": []}, {"text": "However, other resources such as sentiment lexicons can be incorporated into the system., which presents the results of our submission in this task, lists the F-score of the positive, negative and neutral classes on the Twitter test data, whereas lists the results of the SMS message data.", "labels": [], "entities": [{"text": "F-score", "start_pos": 159, "end_pos": 166, "type": "METRIC", "confidence": 0.9975341558456421}, {"text": "Twitter test data", "start_pos": 220, "end_pos": 237, "type": "DATASET", "confidence": 0.7871575653553009}]}, {"text": "As it can be observed from the results, the negative sentiments are classified better than the positive ones.", "labels": [], "entities": []}, {"text": "We reckon this maybe due to the comparatively fewer ways of expressing a positive emotion, while the negative sentiment seems to have a much wider vocabulary (our sentiment lexicon has 25% less positive words than negative).", "labels": [], "entities": []}, {"text": "Whereas the positive class has a higher precision, the negative class seems to have a more notable recall.", "labels": [], "entities": [{"text": "precision", "start_pos": 40, "end_pos": 49, "type": "METRIC", "confidence": 0.9988805651664734}, {"text": "recall", "start_pos": 99, "end_pos": 105, "type": "METRIC", "confidence": 0.9986097812652588}]}, {"text": "The most striking observation, however, is the extremely low F-score for the neutral class.", "labels": [], "entities": [{"text": "F-score", "start_pos": 61, "end_pos": 68, "type": "METRIC", "confidence": 0.9992339611053467}]}, {"text": "This maybe due to the highly skewed proportion (less than 5%) of neutral instances in the training data.", "labels": [], "entities": []}, {"text": "In future work, it will be interesting to see how balancing out the proportions of the three classes affects the classification accuracy.", "labels": [], "entities": [{"text": "classification", "start_pos": 113, "end_pos": 127, "type": "TASK", "confidence": 0.9565017223358154}, {"text": "accuracy", "start_pos": 128, "end_pos": 136, "type": "METRIC", "confidence": 0.9516404867172241}]}, {"text": "We also ran some ablation experiments on the provided Twitter and SMS test data sets after the submission.", "labels": [], "entities": [{"text": "SMS test data sets", "start_pos": 66, "end_pos": 84, "type": "DATASET", "confidence": 0.8061947822570801}]}, {"text": "reports the findings of experiments where, for example, \"-prior polarities\" indicates a feature set excluding the prior polarities.", "labels": [], "entities": []}, {"text": "The metric used here is the macro-averaged F-score of the positive and the negative class.", "labels": [], "entities": [{"text": "F-score", "start_pos": 43, "end_pos": 50, "type": "METRIC", "confidence": 0.9374432563781738}]}, {"text": "The baseline measure implements a simple SVM classifier using only the words as unigram features in the expression.", "labels": [], "entities": [{"text": "SVM classifier", "start_pos": 41, "end_pos": 55, "type": "TASK", "confidence": 0.8175309598445892}]}, {"text": "Interestingly, contrary to our hypothesis dur-ing development time, using the POS of the entire tweet was the least helpful feature.", "labels": [], "entities": [{"text": "POS", "start_pos": 78, "end_pos": 81, "type": "METRIC", "confidence": 0.9512715339660645}]}, {"text": "Since this was an expression level classification task, it seems that using the POS features of the entire tweet may misguide the classifier.", "labels": [], "entities": [{"text": "expression level classification task", "start_pos": 18, "end_pos": 54, "type": "TASK", "confidence": 0.7133185118436813}]}, {"text": "Unsurprisingly, the prior polarities turned out to be the most important part of the feature set for this classification task as it seems that many of the expressions' contextual polarities remained same as their prior polarities.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Sample feature set for an expression (denoted in bold)", "labels": [], "entities": []}, {"text": " Table 2: Submitted results: Twitter test data", "labels": [], "entities": [{"text": "Submitted", "start_pos": 10, "end_pos": 19, "type": "TASK", "confidence": 0.973482072353363}, {"text": "Twitter test data", "start_pos": 29, "end_pos": 46, "type": "DATASET", "confidence": 0.9624640742937723}]}, {"text": " Table 3: Submitted results: SMS test data", "labels": [], "entities": [{"text": "Submitted", "start_pos": 10, "end_pos": 19, "type": "TASK", "confidence": 0.9709875583648682}, {"text": "SMS test data", "start_pos": 29, "end_pos": 42, "type": "DATASET", "confidence": 0.6316220760345459}]}, {"text": " Table 4: Macro-averaged F-score results using different  feature sets", "labels": [], "entities": [{"text": "F-score", "start_pos": 25, "end_pos": 32, "type": "METRIC", "confidence": 0.9651281237602234}]}]}