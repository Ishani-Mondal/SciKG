{"title": [{"text": "Choosing the Right Words: Characterizing and Reducing Error of the Word Count Approach", "labels": [], "entities": [{"text": "Characterizing and Reducing Error of the Word Count Approach", "start_pos": 26, "end_pos": 86, "type": "TASK", "confidence": 0.5356913771894243}]}], "abstractContent": [{"text": "Social scientists are increasingly using the vast amount of text available on social media to measure variation in happiness and other psychological states.", "labels": [], "entities": []}, {"text": "Such studies count words deemed to be indicators of happiness and track how the word frequencies change across locations or time.", "labels": [], "entities": []}, {"text": "This word count approach is simple and scalable, yet often picks up false signals, as words can appear in different contexts and take on different meanings.", "labels": [], "entities": []}, {"text": "We characterize the types of errors that occur using the word count approach, and find lexical ambiguity to be the most prevalent.", "labels": [], "entities": []}, {"text": "We then show that one can reduce error with a simple refinement to such lexica by automatically eliminating highly ambiguous words.", "labels": [], "entities": [{"text": "error", "start_pos": 33, "end_pos": 38, "type": "METRIC", "confidence": 0.963911235332489}]}, {"text": "The resulting refined lexica improve precision as measured by human judgments of word occurrences in Facebook posts.", "labels": [], "entities": [{"text": "precision", "start_pos": 37, "end_pos": 46, "type": "METRIC", "confidence": 0.9989226460456848}]}], "introductionContent": [{"text": "Massive social media corpora, such as blogs, tweets, and Facebook statuses have recently peaked the interest of social scientists.", "labels": [], "entities": []}, {"text": "Compared to traditional samples in tensor hundreds, social media sample sizes are orders of magnitude larger, often containing millions or billions of posts or queries.", "labels": [], "entities": []}, {"text": "Such text provides potential for unobtrusive, inexpensive, and real-time measurement of psychological states (such as positive or negative affect) and aspects of subjective well-being (such as happiness and engagement).", "labels": [], "entities": []}, {"text": "Social scientists have recently begun to use social media text in a variety of studies (.", "labels": [], "entities": []}, {"text": "One of the most popular approaches to estimate psychological states is by using the word count method (, where one tracks the frequency of words that have been judged to be associated with a given state.", "labels": [], "entities": []}, {"text": "Greater use of such words is taken to index the prevalence of the corresponding state.", "labels": [], "entities": []}, {"text": "For example, the use of the word 'happy' is taken to index positive emotion, and 'angry' to index negative emotion.", "labels": [], "entities": []}, {"text": "The most widely used tool to carryout such analysis, and the one we investigate in this paper, is Pennebaker's Linguistic Inquiry and Word Count, (LIWC) (.", "labels": [], "entities": [{"text": "Pennebaker's Linguistic Inquiry and Word Count", "start_pos": 98, "end_pos": 144, "type": "TASK", "confidence": 0.6149017683097294}]}, {"text": "LIWC, originally developed to analyze writing samples for emotion and control, has grown to include a variety of lexica for linguistic and psychosocial topics including positive and negative emotions, pronouns, money, work, and religion.", "labels": [], "entities": [{"text": "LIWC", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.8073570132255554}]}, {"text": "The word count approach has high appeal to social scientists in need of a tool to approach social media, and although others have been used (see, for example), LIWC's lexica are generally perceived as a \"tried-and-tested\" list of words.", "labels": [], "entities": []}, {"text": "Unfortunately, the word count approach has some drawbacks when used as indicators for psychological states.", "labels": [], "entities": []}, {"text": "Words are the unit of measurement, but words can carry many different meanings depending on context.", "labels": [], "entities": []}, {"text": "Consider the Facebook posts below containing instances of 'play', a word associated with positive emotion in LIWC.", "labels": [], "entities": []}, {"text": "1. so everyone should come to the play tomorrow...", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluate our refinement by comparing against human judgements of the emotion conveyed by words in individual posts.", "labels": [], "entities": []}, {"text": "In the case of human judgements, we find that the subset of humanannotated instances matching the refined lexica are more accurate than the complete set.", "labels": [], "entities": []}, {"text": "In section 3 we discussed the method we used to judge instances of LIWC POSEMO and NEGEMO words as to whether they contributed the associated affect.", "labels": [], "entities": []}, {"text": "Each of the 1,000 instances in our evaluation corpus were judged three times such that the majority was taken as truth.", "labels": [], "entities": []}, {"text": "In order to validate our refined lexica, we find the accuracy (precision) of the subset of instances which contain the refined lexica terms.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 53, "end_pos": 61, "type": "METRIC", "confidence": 0.9994232654571533}, {"text": "precision", "start_pos": 63, "end_pos": 72, "type": "METRIC", "confidence": 0.9516217112541199}]}, {"text": "shows the change in precision when using the refined lexica.", "labels": [], "entities": [{"text": "precision", "start_pos": 20, "end_pos": 29, "type": "METRIC", "confidence": 0.9992966651916504}]}, {"text": "size represents the number of instances from the full evaluation corpus matching words in the refined lexica.", "labels": [], "entities": []}, {"text": "One can see that initially precision increase as the size becomes smaller.", "labels": [], "entities": [{"text": "precision", "start_pos": 27, "end_pos": 36, "type": "METRIC", "confidence": 0.9996002316474915}]}, {"text": "This is more clearly seen in.", "labels": [], "entities": []}, {"text": "As discussed in the method section, our goal with the refinement is improving precision, making lexica more suitable to applications over massive social media where one can more readily afford to skip instances (i.e. smaller size) in order to achieve more accuracy.", "labels": [], "entities": [{"text": "precision", "start_pos": 78, "end_pos": 87, "type": "METRIC", "confidence": 0.998975396156311}, {"text": "accuracy", "start_pos": 256, "end_pos": 264, "type": "METRIC", "confidence": 0.9814144968986511}]}, {"text": "Still, removing more ambiguous words does not guarantee improved precision at capturing the intended psychological state; it is possible that that all senses of an ambiguous word do in fact carry intended signal or that the intended sense a low ambiguity word is not the most frequent.", "labels": [], "entities": [{"text": "precision", "start_pos": 65, "end_pos": 74, "type": "METRIC", "confidence": 0.995894193649292}]}, {"text": "Our maximum precision occurs with a threshold of 0.50, where things somewhat level-out.", "labels": [], "entities": [{"text": "precision", "start_pos": 12, "end_pos": 21, "type": "METRIC", "confidence": 0.9995573163032532}]}, {"text": "This represents approximately a 23% reduction in error, and verifies that we can increase precision through the automatic lexicon refinement based on lexical ambiguity.", "labels": [], "entities": [{"text": "error", "start_pos": 49, "end_pos": 54, "type": "METRIC", "confidence": 0.9655600190162659}, {"text": "precision", "start_pos": 90, "end_pos": 99, "type": "METRIC", "confidence": 0.9990503191947937}]}], "tableCaptions": [{"text": " Table 2: Inter-annotator agreement over 1,000 instances  of LIWC terms in Facebook posts. Base rate is the aver- age of how often an annotator answered true.", "labels": [], "entities": [{"text": "Base rate", "start_pos": 91, "end_pos": 100, "type": "METRIC", "confidence": 0.9798503518104553}, {"text": "aver- age", "start_pos": 108, "end_pos": 117, "type": "METRIC", "confidence": 0.9794570008913676}]}, {"text": " Table  5. The most common signal error was wrong word  sense, where the word did not signal emotional  state and some other sense or definition of the word  was intended (e.g. \"u feel like ur living in a mu- sic video\"; corresponding to the sense \"to inhabit\"  rather than the intended sense, \"to have life; be", "labels": [], "entities": []}, {"text": " Table 5: Frequency of the signal error types.", "labels": [], "entities": [{"text": "Frequency", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9631253480911255}]}, {"text": " Table 6: Precision (prec) and instance subset size (size)  of refinements to the LIWC POSEMO and NEGEMO lex- ica with various \u03b8 thresholds (0.10, 0.50, 0.90)", "labels": [], "entities": [{"text": "Precision", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9974004030227661}, {"text": "NEGEMO lex- ica", "start_pos": 98, "end_pos": 113, "type": "DATASET", "confidence": 0.8126351684331894}]}]}