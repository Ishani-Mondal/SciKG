{"title": [{"text": "SemEval-2013 Task 10: Cross-lingual Word Sense Disambiguation", "labels": [], "entities": [{"text": "SemEval-2013 Task 10", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.8401192625363668}, {"text": "Cross-lingual Word Sense Disambiguation", "start_pos": 22, "end_pos": 61, "type": "TASK", "confidence": 0.7380430102348328}]}], "abstractContent": [{"text": "The goal of the Cross-lingual Word Sense Disam-biguation task is to evaluate the viability of multilingual WSD on a benchmark lexical sample data set.", "labels": [], "entities": [{"text": "Cross-lingual Word Sense Disam-biguation task", "start_pos": 16, "end_pos": 61, "type": "TASK", "confidence": 0.6516212403774262}, {"text": "benchmark lexical sample data set", "start_pos": 116, "end_pos": 149, "type": "DATASET", "confidence": 0.8219420671463012}]}, {"text": "The traditional WSD task is transformed into a multilingual WSD task, where participants are asked to provide contextually correct translations of English ambiguous nouns into five target languages, viz.", "labels": [], "entities": [{"text": "WSD task", "start_pos": 16, "end_pos": 24, "type": "TASK", "confidence": 0.9094076454639435}, {"text": "WSD task", "start_pos": 60, "end_pos": 68, "type": "TASK", "confidence": 0.8602191507816315}]}, {"text": "French, Italian, English, German and Dutch.", "labels": [], "entities": []}, {"text": "We report results for the 12 official submissions from 5 different research teams, as well as for the ParaSense system that was developed by the task organizers.", "labels": [], "entities": []}], "introductionContent": [{"text": "Lexical ambiguity remains one of the major problems for current machine translation systems.", "labels": [], "entities": [{"text": "Lexical ambiguity", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.8770078718662262}, {"text": "machine translation", "start_pos": 64, "end_pos": 83, "type": "TASK", "confidence": 0.7198501825332642}]}, {"text": "In the following French sentence \"Je cherche des id\u00e9es pour manger de l'avocat\" 1 , the word \"avocat\" is clearly referring to the fruit, whereas both Google Translate 2 as well as Babelfish translate the word as \"lawyer\".", "labels": [], "entities": []}, {"text": "Although \"lawyer\" is a correct translation of the word \"avocat\", it is the wrong translation in this context.", "labels": [], "entities": []}, {"text": "Other language technology applications, such as Question Answering (QA) systems or information retrieval (IR) systems, also suffer from the poor contextual disambiguation of word senses.", "labels": [], "entities": [{"text": "Question Answering (QA)", "start_pos": 48, "end_pos": 71, "type": "TASK", "confidence": 0.8594088435173035}, {"text": "information retrieval (IR)", "start_pos": 83, "end_pos": 109, "type": "TASK", "confidence": 0.7361459851264953}]}, {"text": "Word sense disambiguation (WSD) is still considered one of the most challenging problems within language technology today.", "labels": [], "entities": [{"text": "Word sense disambiguation (WSD)", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.8149456779162089}]}, {"text": "It requires the construction of an artificial text understanding as the system should detect the correct word sense based on the context of the word.", "labels": [], "entities": []}, {"text": "Different methodologies have been investigated to solve the problem; see for instance and fora detailed overview of WSD algorithms and evaluation.", "labels": [], "entities": [{"text": "WSD", "start_pos": 116, "end_pos": 119, "type": "TASK", "confidence": 0.9689363241195679}]}, {"text": "This paper reports on the second edition of the \"Cross-Lingual Word Sense Disambiguation\" (CLWSD) task, that builds further on the insights we gained from the SemEval-2010 evaluation) and for which new test data were annotated.", "labels": [], "entities": [{"text": "Cross-Lingual Word Sense Disambiguation\" (CLWSD) task", "start_pos": 49, "end_pos": 102, "type": "TASK", "confidence": 0.7823555767536163}]}, {"text": "The task is an unsupervised Word Sense Disambiguation task for English nouns, the sense label of which is composed of translations in different target languages (viz. French, Italian, Spanish, Dutch and German).", "labels": [], "entities": [{"text": "Word Sense Disambiguation task", "start_pos": 28, "end_pos": 58, "type": "TASK", "confidence": 0.7271881774067879}]}, {"text": "The sense inventory is built upon the basis of the Europarl parallel corpus; all translations of a polysemous word were manually grouped into clusters, which constitute different senses of that given word.", "labels": [], "entities": [{"text": "Europarl parallel corpus", "start_pos": 51, "end_pos": 75, "type": "DATASET", "confidence": 0.9551706711451212}]}, {"text": "For the test data, native speakers assigned a translation cluster(s) to each test sentence and gave their top three translations from the predefined list of Europarl translations, in order to assign weights to the set of gold standard translations.", "labels": [], "entities": [{"text": "Europarl", "start_pos": 157, "end_pos": 165, "type": "DATASET", "confidence": 0.9779446721076965}]}, {"text": "The decision to recast the more traditional monolingual WSD task into a cross-lingual WSD task was motivated by the following arguments.", "labels": [], "entities": []}, {"text": "Firstly, using multilingual unlabeled parallel corpora contributes to clearing the data acquisition bottleneck for WSD, because using translations as sense labels excludes the need for manually created sense-tagged corpora and sense inventories such as WordNet.", "labels": [], "entities": [{"text": "data acquisition", "start_pos": 83, "end_pos": 99, "type": "TASK", "confidence": 0.7250640988349915}, {"text": "WSD", "start_pos": 115, "end_pos": 118, "type": "TASK", "confidence": 0.9428368210792542}, {"text": "WordNet", "start_pos": 253, "end_pos": 260, "type": "DATASET", "confidence": 0.9517448544502258}]}, {"text": "Moreover, as there is fairly little linguistic knowledge involved, the framework can be easily deployed fora variety of different languages.", "labels": [], "entities": []}, {"text": "Secondly, a cross-lingual approach also deals with the sense granularity problem; finer sense distinctions are only relevant as far as they get lexicalized in different translations of the word.", "labels": [], "entities": []}, {"text": "If we take the English word \"head\" as an example, we see that this word is always translated as \"hoofd\" in Dutch (both for the \"chief\" and for the 'body part\" sense of the word).", "labels": [], "entities": []}, {"text": "At the same time, the subjectivity problem is tackled that arises when lexicographers have to construct a fixed set of senses fora particular word that should fit all possible domains and applications.", "labels": [], "entities": []}, {"text": "In addition, the use of domain-specific corpora allows to derive sense inventories that are tailored towards a specific target domain or application and to train a dedicated CLWSD system using these particular sense inventories.", "labels": [], "entities": []}, {"text": "Thirdly, working immediately with translations instead of more abstract sense labels allows to bypass the need to map abstract sense labels to corresponding translations.", "labels": [], "entities": []}, {"text": "This makes it easier to integrate a dedicated WSD module into real multilingual applications such as machine translation or information retrieval ().", "labels": [], "entities": [{"text": "machine translation", "start_pos": 101, "end_pos": 120, "type": "TASK", "confidence": 0.8243687152862549}, {"text": "information retrieval", "start_pos": 124, "end_pos": 145, "type": "TASK", "confidence": 0.7408042848110199}]}, {"text": "Many studies have already shown the validity of a cross-lingual approach to Word Sense Disambiguation ().", "labels": [], "entities": [{"text": "Word Sense Disambiguation", "start_pos": 76, "end_pos": 101, "type": "TASK", "confidence": 0.707423746585846}]}, {"text": "The Cross-lingual WSD task contributes to this research domain by the construction of a dedicated benchmark data set where the ambiguous words were annotated with the senses from a multilingual sense inventory extracted from a parallel corpus.", "labels": [], "entities": [{"text": "WSD task", "start_pos": 18, "end_pos": 26, "type": "TASK", "confidence": 0.7525931894779205}]}, {"text": "This benchmark data sets allows a detailed comparison between different approaches to the CLWSD task.", "labels": [], "entities": []}, {"text": "The remainder of this paper is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 focuses on the task description and briefly recapitalizes the construction of the sense inventory and the annotation procedure of the test sentences.", "labels": [], "entities": []}, {"text": "Section 3 presents the participating systems to the task, whereas Section 4 gives an overview of the experimental setup and results.", "labels": [], "entities": []}, {"text": "Section 5 concludes this paper.", "labels": [], "entities": []}], "datasetContent": [{"text": "Two subtasks were proposed for the Cross-lingual WSD task: a best evaluation and an Out-of-five evaluation task.", "labels": [], "entities": [{"text": "WSD task", "start_pos": 49, "end_pos": 57, "type": "TASK", "confidence": 0.7986474931240082}]}, {"text": "For the best evaluation, systems can propose as many guesses as the system believes are correct, but the score is divided by the number of guesses.", "labels": [], "entities": []}, {"text": "In case of the Out-of-five evaluation, systems can propose up to five guesses per test instance without being penalized for wrong translation suggestions.", "labels": [], "entities": []}, {"text": "Both evaluation tasks are explained in more detail in Section 4.1.", "labels": [], "entities": []}, {"text": "Five different research teams participated to the CLWSD task and submitted up to three different runs of their system, resulting in 12 different submissions for the task.", "labels": [], "entities": []}, {"text": "All systems took part in both the best and the Out-of-five evaluation tasks.", "labels": [], "entities": []}, {"text": "These systems took very different approaches to solve the task, ranging from statistical machine translation, classification and sense clustering to topic model based approaches.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 77, "end_pos": 108, "type": "TASK", "confidence": 0.71007772286733}, {"text": "classification and sense clustering", "start_pos": 110, "end_pos": 145, "type": "TASK", "confidence": 0.6514158323407173}]}, {"text": "The XLING team) submitted three runs of their system for all five target languages.", "labels": [], "entities": []}, {"text": "The first version of the system presents a topic matching and translation approach to CLWSD (TnT run), where LDA is applied on the Europarl sentences containing the ambiguous focus word in order to train topic models.", "labels": [], "entities": [{"text": "topic matching", "start_pos": 43, "end_pos": 57, "type": "TASK", "confidence": 0.772835910320282}, {"text": "Europarl", "start_pos": 131, "end_pos": 139, "type": "DATASET", "confidence": 0.9782257676124573}]}, {"text": "Each sentence in the training corpus is assigned a topic that contains a list of associated words with the topic.", "labels": [], "entities": []}, {"text": "The topic of the test sentence is then inferred and compared to the matching training sentences by means of the cosine similarity between the training and test vectors.", "labels": [], "entities": []}, {"text": "WordNet (WN) is used as a fallback in case the system returns less than 5 answers.", "labels": [], "entities": [{"text": "WordNet (WN)", "start_pos": 0, "end_pos": 12, "type": "DATASET", "confidence": 0.906096026301384}]}, {"text": "The secondand best performing -flavor of the system (SnT run) calculates the cosine similarity between the context words of the test and training sentences.", "labels": [], "entities": []}, {"text": "The output of the system then contains the translation that results from running word alignment on the focus word in the training corpus.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 81, "end_pos": 95, "type": "TASK", "confidence": 0.7013005316257477}]}, {"text": "As a fallback, WordNet is again used.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 15, "end_pos": 22, "type": "DATASET", "confidence": 0.9636949300765991}]}, {"text": "The WN senses are sorted by frequency in the SemCor corpus and the corresponding translation is selected from the aligned WordNet in the target language.", "labels": [], "entities": [{"text": "SemCor corpus", "start_pos": 45, "end_pos": 58, "type": "DATASET", "confidence": 0.7578964829444885}, {"text": "WordNet", "start_pos": 122, "end_pos": 129, "type": "DATASET", "confidence": 0.9189440011978149}]}, {"text": "The third run of the system (merged) combines the output from the other two flavors of the system.", "labels": [], "entities": []}, {"text": "The LIMSI system applies an unsupervised CLWSD method that was proposed in for three target languages, viz.", "labels": [], "entities": []}, {"text": "Spanish, Italian and French.", "labels": [], "entities": []}, {"text": "First, word alignment is applied on the parallel corpus and three bilingual lexicons are built, containing for each focus word the translations in the three target languages.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 7, "end_pos": 21, "type": "TASK", "confidence": 0.7977628707885742}]}, {"text": "Ina next step, a vector is built for each translation of the English focus word, using the cooccurrences of the word in the sentences in which it gets this particular translation.", "labels": [], "entities": []}, {"text": "A clustering algorithm then groups the feature vectors using the Weighted Jaccard measure.", "labels": [], "entities": []}, {"text": "New instances containing the ambiguous focus word are then compared to the training feature vectors and assigned to one of the sense clusters.", "labels": [], "entities": []}, {"text": "In case the highest-ranked translation in the cluster has a score below the threshold, the system falls back to the most frequent translation.", "labels": [], "entities": []}, {"text": "Two very well performing systems take a classification-based approach to the CLWSD task: the HLTDI and WSD2 systems.", "labels": [], "entities": [{"text": "HLTDI", "start_pos": 93, "end_pos": 98, "type": "DATASET", "confidence": 0.8591280579566956}]}, {"text": "The HLTDI system () performs word alignment on the intersected Europarl corpus to locate training instances containing the ambiguous focus words.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 29, "end_pos": 43, "type": "TASK", "confidence": 0.7467442452907562}, {"text": "Europarl corpus", "start_pos": 63, "end_pos": 78, "type": "DATASET", "confidence": 0.9852129220962524}]}, {"text": "The first flavor of the system (l1) uses a maxent clas-sifier that is trained over local context features.", "labels": [], "entities": []}, {"text": "The L2 model (l2 run) also adds translations of the focus word into the four other target languages to the feature vector.", "labels": [], "entities": []}, {"text": "To disambiguate new test instances, these translations into the four other languages are estimated using the classifiers builtin the first version of the system (l1).", "labels": [], "entities": []}, {"text": "The third system run (mrf ) builds a Markov network of L1 classifiers in order to find the best translation into all five target languages jointly.", "labels": [], "entities": []}, {"text": "The nodes of this network correspond to the distribution produced by the L1 classifiers, while the edges contain pairwise potentials derived from the joint probabilities of translation labels occurring together in the training data.", "labels": [], "entities": []}, {"text": "Another classification-based approach is presented by the WSD2 system (van Gompel and van den, that uses a k-NN classifier to solve the CLWSD task.", "labels": [], "entities": [{"text": "WSD2", "start_pos": 58, "end_pos": 62, "type": "DATASET", "confidence": 0.7464175820350647}]}, {"text": "The first configuration of the system (c1l) uses local context features fora window of three words containing the focus word.", "labels": [], "entities": []}, {"text": "Parameters were optimized on the trial data.", "labels": [], "entities": [{"text": "Parameters", "start_pos": 0, "end_pos": 10, "type": "METRIC", "confidence": 0.9846248626708984}]}, {"text": "The second flavor of the system (c1lN) uses the same configuration of the system, but without parameter optimization.", "labels": [], "entities": []}, {"text": "The third configuration of the system (var) is heavily optimized on the trial data, selecting the winning configuration per trial word and evaluation metric.", "labels": [], "entities": []}, {"text": "In addition to the local context features, also global bag-of-word context features are considered for this version of the system.", "labels": [], "entities": []}, {"text": "A completely different approach is taken by the NRC-SMT system, that uses a statistical machine translation approach to tackle the CLWSD task.", "labels": [], "entities": [{"text": "NRC-SMT", "start_pos": 48, "end_pos": 55, "type": "DATASET", "confidence": 0.8870905041694641}, {"text": "statistical machine translation", "start_pos": 76, "end_pos": 107, "type": "TASK", "confidence": 0.6076712310314178}]}, {"text": "The baseline version of the system (SMTbasic) represents a standard phrase-based SMT baseline, that is trained only on the intersected Europarl corpus.", "labels": [], "entities": [{"text": "SMTbasic)", "start_pos": 36, "end_pos": 45, "type": "TASK", "confidence": 0.8080569803714752}, {"text": "SMT", "start_pos": 81, "end_pos": 84, "type": "TASK", "confidence": 0.8296934366226196}, {"text": "Europarl corpus", "start_pos": 135, "end_pos": 150, "type": "DATASET", "confidence": 0.9789981544017792}]}, {"text": "Translations for the test instances are extracted from the top hypothesis (for the best evaluation) or from the 100-best list (for the Out-of-five evaluation).", "labels": [], "entities": []}, {"text": "The optimized version of the system (SMTadapt2) is trained on the Europarl corpus and additional news data, and uses mixture models that are developed for domain adaptation in SMT.", "labels": [], "entities": [{"text": "SMTadapt2", "start_pos": 37, "end_pos": 46, "type": "TASK", "confidence": 0.880307137966156}, {"text": "Europarl corpus", "start_pos": 66, "end_pos": 81, "type": "DATASET", "confidence": 0.9952979385852814}, {"text": "domain adaptation", "start_pos": 155, "end_pos": 172, "type": "TASK", "confidence": 0.723443478345871}, {"text": "SMT", "start_pos": 176, "end_pos": 179, "type": "TASK", "confidence": 0.9306992292404175}]}, {"text": "In addition to the five systems that participated to the official evaluation campaign, the organizers also present results for their ParaSense system, which is described in the following section.", "labels": [], "entities": []}, {"text": "Test set The lexical sample contains 50 English sentences per ambiguous focus word.", "labels": [], "entities": []}, {"text": "All instances were manually annotated per language, which resulted in a set of gold standard translation labels per instance.", "labels": [], "entities": []}, {"text": "For the construction of the test dataset, we refer to Section 2.", "labels": [], "entities": []}, {"text": "Evaluation metric The BEST precision and recall metric was introduced by) in the framework of the SemEval-2007 competition.", "labels": [], "entities": [{"text": "BEST", "start_pos": 22, "end_pos": 26, "type": "METRIC", "confidence": 0.9973773956298828}, {"text": "precision", "start_pos": 27, "end_pos": 36, "type": "METRIC", "confidence": 0.8405774235725403}, {"text": "recall metric", "start_pos": 41, "end_pos": 54, "type": "METRIC", "confidence": 0.9664546251296997}]}, {"text": "The metric takes into account the frequency weights of the gold standard translations: translations that were picked by different annotators received a higher associated frequency which is incorporated in the formulas for calculating precision and recall.", "labels": [], "entities": [{"text": "precision", "start_pos": 234, "end_pos": 243, "type": "METRIC", "confidence": 0.9954954385757446}, {"text": "recall", "start_pos": 248, "end_pos": 254, "type": "METRIC", "confidence": 0.9897878170013428}]}, {"text": "For the BEST precision and recall evaluation, the system can propose as many guesses as the system believes are correct, but the resulting score is divided by the number of guesses.", "labels": [], "entities": [{"text": "BEST", "start_pos": 8, "end_pos": 12, "type": "METRIC", "confidence": 0.9794287085533142}, {"text": "precision", "start_pos": 13, "end_pos": 22, "type": "METRIC", "confidence": 0.7962821125984192}, {"text": "recall", "start_pos": 27, "end_pos": 33, "type": "METRIC", "confidence": 0.998292863368988}]}, {"text": "In this way, systems that output many guesses are not favored and systems can maximize their score by guessing the most frequent translation from the annotators.", "labels": [], "entities": []}, {"text": "We also calculate Mode precision and recall, where precision and recall are calculated against the translation that is preferred by the majority of annotators, provided that one translation is more frequent than the others.", "labels": [], "entities": [{"text": "Mode", "start_pos": 18, "end_pos": 22, "type": "METRIC", "confidence": 0.9788161516189575}, {"text": "precision", "start_pos": 23, "end_pos": 32, "type": "METRIC", "confidence": 0.524512767791748}, {"text": "recall", "start_pos": 37, "end_pos": 43, "type": "METRIC", "confidence": 0.9982514977455139}, {"text": "precision", "start_pos": 51, "end_pos": 60, "type": "METRIC", "confidence": 0.9990724325180054}, {"text": "recall", "start_pos": 65, "end_pos": 71, "type": "METRIC", "confidence": 0.998004138469696}]}, {"text": "The following variables are used for the BEST precision and recall formulas.", "labels": [], "entities": [{"text": "BEST", "start_pos": 41, "end_pos": 45, "type": "METRIC", "confidence": 0.9413737654685974}, {"text": "precision", "start_pos": 46, "end_pos": 55, "type": "METRIC", "confidence": 0.7343342304229736}, {"text": "recall", "start_pos": 60, "end_pos": 66, "type": "METRIC", "confidence": 0.9969112277030945}]}, {"text": "Let H be the set of annotators, T the set of test words and hi the set of translations for an item i \u2208 T for annotator h \u2208 H.", "labels": [], "entities": []}, {"text": "Let Abe the set of words from T where the system provides at least one answer and a i the set of guesses from the system for word i \u2208 A.", "labels": [], "entities": []}, {"text": "For each i, we calculate the multiset union (H i ) for all hi for all h \u2208 H and for each unique type (res) in H i that has an associated frequency (f req res ).", "labels": [], "entities": []}, {"text": "Equation 1 lists the BEST precision formula, whereas Equation 2 lists the formula for calculating the BEST recall score: Most Frequent translation baseline As a baseline, we selected the most frequent lemmatized translation that resulted from the automated word alignment (GIZA++) for all ambiguous nouns in the training data.", "labels": [], "entities": [{"text": "BEST precision", "start_pos": 21, "end_pos": 35, "type": "METRIC", "confidence": 0.7878448069095612}, {"text": "BEST recall score", "start_pos": 102, "end_pos": 119, "type": "METRIC", "confidence": 0.8323850433031718}]}, {"text": "This baseline is inspired by the most frequent sense baseline often used in WSD evaluations.", "labels": [], "entities": [{"text": "WSD evaluations", "start_pos": 76, "end_pos": 91, "type": "TASK", "confidence": 0.8792521953582764}]}, {"text": "The main difference between the most frequent sense baseline and our baseline is that the latter is corpus-dependent: we do not take into account the overall frequency of a word as it would be measured based on a large general purpose corpus, but calculate the most frequent sense (or translation in this case) based on our training corpus.", "labels": [], "entities": []}, {"text": "For the system evaluation results, we show precision and Mode precision figures for both evaluation types (best and Out-of-five).", "labels": [], "entities": [{"text": "precision", "start_pos": 43, "end_pos": 52, "type": "METRIC", "confidence": 0.9997710585594177}, {"text": "Mode", "start_pos": 57, "end_pos": 61, "type": "METRIC", "confidence": 0.998076319694519}, {"text": "precision", "start_pos": 62, "end_pos": 71, "type": "METRIC", "confidence": 0.5311673879623413}]}, {"text": "In our case, precision refers to the number of correct translations in relation to the total number of translations generated by the system, while recall refers to the number of correct translations generated by the classifier.", "labels": [], "entities": [{"text": "precision", "start_pos": 13, "end_pos": 22, "type": "METRIC", "confidence": 0.9993245601654053}, {"text": "recall", "start_pos": 147, "end_pos": 153, "type": "METRIC", "confidence": 0.9991348385810852}]}, {"text": "As all participating systems predict a translation label for all sentences in the test set, precision and recall will give identical results.", "labels": [], "entities": [{"text": "precision", "start_pos": 92, "end_pos": 101, "type": "METRIC", "confidence": 0.999678373336792}, {"text": "recall", "start_pos": 106, "end_pos": 112, "type": "METRIC", "confidence": 0.9994779229164124}]}, {"text": "As a consequence, we do not list the recall and Mode recall figures that are in this case identical to the corresponding precision scores.", "labels": [], "entities": [{"text": "recall", "start_pos": 37, "end_pos": 43, "type": "METRIC", "confidence": 0.9991476535797119}, {"text": "Mode recall", "start_pos": 48, "end_pos": 59, "type": "METRIC", "confidence": 0.8241548538208008}, {"text": "precision", "start_pos": 121, "end_pos": 130, "type": "METRIC", "confidence": 0.9975448250770569}]}, {"text": "lists the averaged best precision scores for all systems, while gives an overview of the best Mode precision figures for all five target languages, viz.", "labels": [], "entities": [{"text": "precision", "start_pos": 24, "end_pos": 33, "type": "METRIC", "confidence": 0.9701451063156128}, {"text": "Mode precision", "start_pos": 94, "end_pos": 108, "type": "METRIC", "confidence": 0.912945955991745}]}, {"text": "Spanish (Es), Dutch (Nl), German (De), Italian (It) and French (Fr).", "labels": [], "entities": []}, {"text": "We list scores for all participating systems in the official CLWSD evaluation campaign, as well as for the organizers' system ParaSense, that is not part of the official SemEval competition.", "labels": [], "entities": [{"text": "CLWSD evaluation campaign", "start_pos": 61, "end_pos": 86, "type": "DATASET", "confidence": 0.8771620194117228}, {"text": "ParaSense", "start_pos": 126, "end_pos": 135, "type": "DATASET", "confidence": 0.7550182342529297}, {"text": "SemEval competition", "start_pos": 170, "end_pos": 189, "type": "TASK", "confidence": 0.7476156651973724}]}, {"text": "The best results for the best precision evaluation are achieved by the NRCSMTadapt2 system for Spanish and by the WSD2 system for the other four target languages, closely followed by the HLTDI system.", "labels": [], "entities": [{"text": "precision", "start_pos": 30, "end_pos": 39, "type": "METRIC", "confidence": 0.997282862663269}]}, {"text": "The latter two systems also obtain the best results for the best Mode precision metric.", "labels": [], "entities": [{"text": "Mode precision metric", "start_pos": 65, "end_pos": 86, "type": "METRIC", "confidence": 0.8426634271939596}]}, {"text": "lists the averaged Out-of-five precision scores for all systems, while gives an overview of the Out-of-five Mode precision figures for all five target languages, viz.", "labels": [], "entities": [{"text": "Out-of-five precision scores", "start_pos": 19, "end_pos": 47, "type": "METRIC", "confidence": 0.8131284515062968}, {"text": "Out-of-five Mode precision", "start_pos": 96, "end_pos": 122, "type": "METRIC", "confidence": 0.7147379120190939}]}, {"text": "Spanish (Es), Dutch (Nl), German (De), Italian (It) and French (Fr).", "labels": [], "entities": []}, {"text": "For the Out-of-five evaluation, where systems are allowed to generate up to five unique translations without being penalized for wrong translations, again the HLTDI and WSD2 systems obtain the best classification performance.", "labels": [], "entities": [{"text": "HLTDI", "start_pos": 159, "end_pos": 164, "type": "DATASET", "confidence": 0.8601053953170776}, {"text": "WSD2", "start_pos": 169, "end_pos": 173, "type": "DATASET", "confidence": 0.7910958528518677}]}, {"text": "Although the winning systems use different approaches (statistical machine translation and classi-fication algorithms), they have in common that they only use a parallel corpus to extract disambiguating information, and do not use external resources such as WordNet.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 55, "end_pos": 86, "type": "TASK", "confidence": 0.6256940464178721}, {"text": "WordNet", "start_pos": 258, "end_pos": 265, "type": "DATASET", "confidence": 0.9702538251876831}]}, {"text": "As a consequence, this makes the systems very flexible and language-independent.", "labels": [], "entities": []}, {"text": "The ParaSense system, that incorporates translation information from four other languages, outperforms all other systems, except for the best precision metric in Spanish, where the NRC-SMT system obtains the overall best results.", "labels": [], "entities": [{"text": "precision", "start_pos": 142, "end_pos": 151, "type": "METRIC", "confidence": 0.9966639876365662}, {"text": "NRC-SMT", "start_pos": 181, "end_pos": 188, "type": "DATASET", "confidence": 0.8067504167556763}]}, {"text": "This confirms the hypothesis that a truly multilingual approach to WSD, which incorporates translation information from multiple languages into the feature vector, is more effective than only using monolingual or bilingual features.", "labels": [], "entities": [{"text": "WSD", "start_pos": 67, "end_pos": 70, "type": "TASK", "confidence": 0.9702020883560181}]}, {"text": "A possible explanation could be that the differences between the different languages that are integrated in the feature vector enable the system to refine the obtained sense distinctions.", "labels": [], "entities": []}, {"text": "We indeed see that the ParaSense system outperforms the classificationbased bilingual approaches which exploit similar information (e.g. training corpora and machine learning algorithms).", "labels": [], "entities": []}, {"text": "In general, we notice that French and Spanish have the highest scores, while Dutch and German seem harder to tackle.", "labels": [], "entities": []}, {"text": "Italian is situated somewhere in between the Romance and Germanic languages.", "labels": [], "entities": []}, {"text": "This trend confirms the results that were obtained during the first SemEval Cross-lingual WSD task ().", "labels": [], "entities": [{"text": "SemEval Cross-lingual WSD task", "start_pos": 68, "end_pos": 98, "type": "TASK", "confidence": 0.8160913586616516}]}, {"text": "As pointed out after the first competition, the discrepancy between the scores for the Romance and Germanic languages can probably be explained by the number of classes (or translations in this case) the systems have to choose from.", "labels": [], "entities": []}, {"text": "Germanic languages are typically characterized by a very productive compounding system, where compounds are joined together in one orthographic unit, which results in a much higher number of different class labels.", "labels": [], "entities": []}, {"text": "As the Romance languages typically write compounds in separate orthographic units, they dispose of a smaller number of different translations for each ambiguous noun.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: BEST precision scores averaged over all twenty  test words for Spanish (Es), Dutch (Nl), German (De),  Italian (It) and French (Fr).", "labels": [], "entities": [{"text": "BEST", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9984942674636841}, {"text": "precision", "start_pos": 15, "end_pos": 24, "type": "METRIC", "confidence": 0.8664551377296448}]}, {"text": " Table 2: BEST Mode precision scores averaged over all  twenty test words for Spanish (Es), Dutch (Nl), German  (De), Italian (It) and French (Fr).", "labels": [], "entities": [{"text": "BEST Mode precision scores", "start_pos": 10, "end_pos": 36, "type": "METRIC", "confidence": 0.8211873024702072}]}, {"text": " Table 3: OUT-OF-FIVE precision scores averaged over all  twenty test words for Spanish (Es), Dutch (Nl), German  (De), Italian (It) and French (Fr).", "labels": [], "entities": [{"text": "OUT-OF-FIVE", "start_pos": 10, "end_pos": 21, "type": "METRIC", "confidence": 0.9968582391738892}, {"text": "precision", "start_pos": 22, "end_pos": 31, "type": "METRIC", "confidence": 0.9342835545539856}]}, {"text": " Table 4: OUT-OF-FIVE Mode precision scores averaged  over all twenty test words for Spanish (Es), Dutch (Nl),  German (De), Italian (It) and French (Fr).", "labels": [], "entities": [{"text": "OUT-OF-FIVE", "start_pos": 10, "end_pos": 21, "type": "METRIC", "confidence": 0.9903609752655029}, {"text": "Mode", "start_pos": 22, "end_pos": 26, "type": "METRIC", "confidence": 0.6836103200912476}, {"text": "precision", "start_pos": 27, "end_pos": 36, "type": "METRIC", "confidence": 0.5558838844299316}]}]}