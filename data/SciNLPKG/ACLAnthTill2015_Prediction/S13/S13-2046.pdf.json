{"title": [{"text": "ETS: Domain Adaptation and Stacking for Short Answer Scoring *", "labels": [], "entities": [{"text": "Domain Adaptation", "start_pos": 5, "end_pos": 22, "type": "TASK", "confidence": 0.7339520454406738}, {"text": "Short Answer Scoring", "start_pos": 40, "end_pos": 60, "type": "TASK", "confidence": 0.6494225660959879}]}], "abstractContent": [{"text": "Automatic scoring of short text responses to educational assessment items is a challenging task, particularly because large amounts of labeled data (i.e., human-scored responses) mayor may not be available due to the variety of possible questions and topics.", "labels": [], "entities": [{"text": "Automatic scoring of short text responses to educational assessment items", "start_pos": 0, "end_pos": 73, "type": "TASK", "confidence": 0.801361957192421}]}, {"text": "As such, it seems desirable to integrate various approaches, making use of model answers from experts (e.g., to give higher scores to responses that are similar), prescored student responses (e.g., to learn direct associations between particular phrases and scores), etc.", "labels": [], "entities": []}, {"text": "Here, we describe a system that uses stacking (Wolpert, 1992) and domain adaptation (Daume III, 2007) to achieve this aim, allowing us to integrate item-specific n-gram features and more general text similarity measures (Heilman and Madnani, 2012).", "labels": [], "entities": [{"text": "domain adaptation (Daume III, 2007)", "start_pos": 66, "end_pos": 101, "type": "TASK", "confidence": 0.6033096797764301}]}, {"text": "We report encouraging results from the Joint Student Response Analysis and 8th Recognizing Textual Entailment Challenge.", "labels": [], "entities": [{"text": "8th Recognizing Textual Entailment Challenge", "start_pos": 75, "end_pos": 119, "type": "TASK", "confidence": 0.6398825466632843}]}], "introductionContent": [{"text": "In this paper, we address the problem of automatically scoring short text responses to educational assessment items for measuring content knowledge.", "labels": [], "entities": []}, {"text": "Many approaches can be and have been taken to this problem-e.g.,,, inter alia.", "labels": [], "entities": []}, {"text": "The effectiveness of any particular approach likely depends on the the availability of data (among other factors).", "labels": [], "entities": []}, {"text": "For example, if thousands of prescored responses are avail-able, then a simple classifier using n-gram features may suffice.", "labels": [], "entities": []}, {"text": "However, if only model answers (i.e., reference answers) or rubrics are available, more general semantic similarity measures (or even rulebased approaches) would be more effective.", "labels": [], "entities": []}, {"text": "It seems likely that, in many cases, there will be model answers as well as a modest number of prescored responses available, as was the case for the Joint Student Response Analysis and 8th Recognizing Textual Entailment Challenge ( \u00a72).", "labels": [], "entities": [{"text": "Joint Student Response Analysis and 8th Recognizing Textual Entailment Challenge", "start_pos": 150, "end_pos": 230, "type": "TASK", "confidence": 0.5096575886011123}]}, {"text": "Therefore, we desire to incorporate both task-specific features, such as n-grams, as well as more general features such as the semantic similarity of the response to model answers.", "labels": [], "entities": []}, {"text": "We also observe that some features may themselves require machine learning or tuning on data from the domain, in addition to any machine learning required for the overall system.", "labels": [], "entities": []}, {"text": "In this paper, we describe a machine learning approach to short answer scoring that allows us to incorporate both item-specific and general features by using the domain adaptation technique of Daume III.", "labels": [], "entities": [{"text": "short answer scoring", "start_pos": 58, "end_pos": 78, "type": "TASK", "confidence": 0.654652863740921}]}, {"text": "In addition, the approach employs stacking to support the integration of components that require tuning or machine learning.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Weighted average F 1 scores for 5-way classification for our SemEval 2013 task 7 submissions, along with  the maximum and mean performance, for comparison. \"A\" = unseen answers, \"Q\" = unseen questions, \"D\" = unseen  domains (see  \u00a72 for details). Results that were the maximum score among submissions for part of the task are in bold.", "labels": [], "entities": [{"text": "F 1", "start_pos": 27, "end_pos": 30, "type": "METRIC", "confidence": 0.9223967492580414}, {"text": "SemEval 2013 task", "start_pos": 71, "end_pos": 88, "type": "TASK", "confidence": 0.5764845212300619}]}]}