{"title": [{"text": "Semantic Parsing Freebase: Towards Open-domain Semantic Parsing", "labels": [], "entities": []}], "abstractContent": [{"text": "Existing semantic parsing research has steadily improved accuracy on a few domains and their corresponding databases.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 9, "end_pos": 25, "type": "TASK", "confidence": 0.7652728259563446}, {"text": "accuracy", "start_pos": 57, "end_pos": 65, "type": "METRIC", "confidence": 0.999005138874054}]}, {"text": "This paper introduces FreeParser, a system that trains on one domain and one set of predicate and constant symbols, and then can parse sentences for any new domain, including sentences that refer to symbols never seen during training.", "labels": [], "entities": []}, {"text": "FreeParser uses a domain-independent architecture to automatically identify sentences relevant to each new database symbol, which it uses to supplement its manually-annotated training data from the training domain.", "labels": [], "entities": [{"text": "FreeParser", "start_pos": 0, "end_pos": 10, "type": "DATASET", "confidence": 0.9147427082061768}]}, {"text": "In cross-domain experiments involving 23 domains, FreeParser can parse sentences for which it has seen comparable unannotated sentences with an F1 of 0.71.", "labels": [], "entities": [{"text": "F1", "start_pos": 144, "end_pos": 146, "type": "METRIC", "confidence": 0.9959052801132202}]}], "introductionContent": [{"text": "Semantic parsing is the task of converting a sentence into a representation of its meaning, usually in a logical form grounded in the symbols of some fixed ontology or relational database ().", "labels": [], "entities": [{"text": "Semantic parsing is the task of converting a sentence into a representation of its meaning, usually in a logical form grounded in the symbols of some fixed ontology or relational database", "start_pos": 0, "end_pos": 187, "type": "Description", "confidence": 0.8310364866629243}]}, {"text": "A growing body of research on semantic parsing has yielded consistent improvements in parsing accuracy.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 30, "end_pos": 46, "type": "TASK", "confidence": 0.8257512152194977}, {"text": "parsing", "start_pos": 86, "end_pos": 93, "type": "TASK", "confidence": 0.9584779739379883}, {"text": "accuracy", "start_pos": 94, "end_pos": 102, "type": "METRIC", "confidence": 0.9310647249221802}]}, {"text": "Yet existing semantic parsers have always been limited by the need for significant amounts of manually-annotated training data for each domain of discourse, or for each new database.", "labels": [], "entities": []}, {"text": "As a result, current semantic parsers have been constrained to small domains, like answering geography questions.", "labels": [], "entities": [{"text": "answering geography questions", "start_pos": 83, "end_pos": 112, "type": "TASK", "confidence": 0.8468558589617411}]}, {"text": "In an effort to breakout of these narrowlyconstrained domains, we investigate semantic parsers for Freebase, an online database of usercontributed facts divided into 86 domains, including everything from architecture to zoos.", "labels": [], "entities": []}, {"text": "Freebase is much larger than standard benchmark databases for semantic parsing; for example, it contains 300 times as many relations, and 75,000 times as many instances, as the GeoQuery database.", "labels": [], "entities": [{"text": "Freebase", "start_pos": 0, "end_pos": 8, "type": "DATASET", "confidence": 0.954700231552124}, {"text": "semantic parsing", "start_pos": 62, "end_pos": 78, "type": "TASK", "confidence": 0.7243106514215469}]}, {"text": "On average, the benchmark GeoQuery dataset has 125 training sentences per relation.", "labels": [], "entities": [{"text": "GeoQuery dataset", "start_pos": 26, "end_pos": 42, "type": "DATASET", "confidence": 0.9345238506793976}]}, {"text": "An equivalent dataset for Freebase would require labeling close to 40,000 training sentences, an expensive undertaking.", "labels": [], "entities": [{"text": "Freebase", "start_pos": 26, "end_pos": 34, "type": "DATASET", "confidence": 0.9802944660186768}]}, {"text": "The size and diversity of data in Freebase forces us to consider anew task of open-domain semantic parsing.", "labels": [], "entities": [{"text": "Freebase", "start_pos": 34, "end_pos": 42, "type": "DATASET", "confidence": 0.942890465259552}, {"text": "open-domain semantic parsing", "start_pos": 78, "end_pos": 106, "type": "TASK", "confidence": 0.6005473931630453}]}, {"text": "We introduce FreeParser, which trains on labeled examples from a select group of initial domains.", "labels": [], "entities": []}, {"text": "It also uses the information in Freebase to automatically find unlabeled training sentences from Wikipedia for every Freebase relation.", "labels": [], "entities": [{"text": "Freebase", "start_pos": 32, "end_pos": 40, "type": "DATASET", "confidence": 0.9453712105751038}]}, {"text": "Using a self-supervised architecture, FreeParser automatically labels these sentences, and then trains a semantic parser for all of Freebase.", "labels": [], "entities": []}, {"text": "The current restriction to Wikipedia has a downside: 44% of the test questions in our dataset contained a word that never appeared in our set of automatically-collected sentences, suggesting that significant further gains could be had by scaling to a larger corpus.", "labels": [], "entities": []}, {"text": "However, FreeParser is able to find correct parses for 70% of the questions from new domains where it could find relevant sentences in Wikipedia, at a precision of 72%.", "labels": [], "entities": [{"text": "precision", "start_pos": 151, "end_pos": 160, "type": "METRIC", "confidence": 0.9885773658752441}]}, {"text": "The next section provides background on semantic parsing for Freebase, and discusses related work.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 40, "end_pos": 56, "type": "TASK", "confidence": 0.7239904850721359}]}, {"text": "Section 3 describes the main modules of the FreeParser system.", "labels": [], "entities": []}, {"text": "Section 4 analyzes the performance of", "labels": [], "entities": []}], "datasetContent": [{"text": "Freebase is a free, online, user-contributed, relational database (www.freebase.com) covering many different domains of knowledge.", "labels": [], "entities": [{"text": "Freebase", "start_pos": 0, "end_pos": 8, "type": "DATASET", "confidence": 0.9658379554748535}]}, {"text": "The full schema and contents are available for download.", "labels": [], "entities": []}, {"text": "Freebase has a number of advantages for building an open-domain semantic parser.", "labels": [], "entities": [{"text": "Freebase", "start_pos": 0, "end_pos": 8, "type": "DATASET", "confidence": 0.934491753578186}]}, {"text": "Most obviously, it provides a much tougher test for semantic parsing than traditional benchmark databases like GeoQuery.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 52, "end_pos": 68, "type": "TASK", "confidence": 0.784952849149704}]}, {"text": "It also provides a testbed for semantic parsing across domains.", "labels": [], "entities": [{"text": "semantic parsing across domains", "start_pos": 31, "end_pos": 62, "type": "TASK", "confidence": 0.8332426100969315}]}, {"text": "As a reference point, the GeoQuery database contains a single domain (geography), 8 relations, and 698 total instances.", "labels": [], "entities": [{"text": "GeoQuery database", "start_pos": 26, "end_pos": 43, "type": "DATASET", "confidence": 0.9021560549736023}]}, {"text": "The \"Freebase Commons\" subset of Freebase, which is our focus, consists of 86 domains, an average of 25 relations per domain (total of 2134 relations), and 615,000 known instances per domain (53 million instances total).", "labels": [], "entities": [{"text": "Freebase", "start_pos": 33, "end_pos": 41, "type": "DATASET", "confidence": 0.5208091139793396}]}, {"text": "By dividing Freebase into different sub-databases according to domain, we can readily test the portability of our parser across domains, and its ability to handle relations and symbols that never occur in manually-labeled training data.", "labels": [], "entities": []}, {"text": "Our dataset contains 403 questions and a meaning representation for each question, written in a variant of lambda calculus . We believe the dataset in itself is an important contribution to the field, as it The data is available from the second author's webpage.", "labels": [], "entities": []}, {"text": "We now test FreeParser's ability to provide semantic parses in domains where it has seen no manuallylabeled training data.", "labels": [], "entities": []}, {"text": "We also empirically analyze the design decisions for FreeParser.", "labels": [], "entities": [{"text": "FreeParser", "start_pos": 53, "end_pos": 63, "type": "DATASET", "confidence": 0.9319422841072083}]}, {"text": "All of our experiments are conducted on the Freebase dataset described in Section 2.1.", "labels": [], "entities": [{"text": "Freebase dataset", "start_pos": 44, "end_pos": 60, "type": "DATASET", "confidence": 0.9928900003433228}]}, {"text": "To create Q: What is 'Big Daddy' rated?", "labels": [], "entities": []}, {"text": "Movie ratings are stored as special codes in Freebase, and are rarely observed 'as is' in text.", "labels": [], "entities": [{"text": "Freebase", "start_pos": 45, "end_pos": 53, "type": "DATASET", "confidence": 0.9723935723304749}]}, {"text": "Q: Who is the CEO of Apple?", "labels": [], "entities": []}, {"text": "Wikipedia regularly uses the full form 'Chief Executive Officer'; no retrieved sentence had 'CEO' together with the executive's name and company name.", "labels": [], "entities": []}, {"text": "Q: When did Jack Albertson die?", "labels": [], "entities": []}, {"text": "Many sentences contain \"person died on date\", but no retrieved sentence contained the morphological variant \"(did) die.\": Example infeasible questions, and why FreeParser had difficulty finding sentences in Wikipedia that contain the relevant keywords from the question.", "labels": [], "entities": []}, {"text": "manually-labeled training and test sets for domain adaptation, we divide the dataset into three groups of nearly-equal size by placing similar domains together in the same group.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 44, "end_pos": 61, "type": "TASK", "confidence": 0.7572479844093323}]}, {"text": "No domain has questions in more than one group.", "labels": [], "entities": []}, {"text": "We then perform 3-fold cross-validation across these three groups.", "labels": [], "entities": []}, {"text": "We run FreeParser's Sentence Retrieval Engine, AutoLabeler, and Assessor for all relations that appear in our dataset, and we include the automaticallylabeled data in the training data.", "labels": [], "entities": [{"text": "Sentence Retrieval", "start_pos": 20, "end_pos": 38, "type": "TASK", "confidence": 0.7943290174007416}]}], "tableCaptions": [{"text": " Table 1: Breakdown of our Freebase data set into do- mains. Several questions used symbols from multiple  Freebase domains, in which cases human judges selected  the best domain they could for that question's category.", "labels": [], "entities": [{"text": "Freebase data set", "start_pos": 27, "end_pos": 44, "type": "DATASET", "confidence": 0.9750578006108602}]}, {"text": " Table 4: FreeParser compared with variations that are  missing critical design components. All precision and  recall differences between the full system and its varia- tions are statistically significant (p < 0.01) using a two- tailed Fisher's exact test.", "labels": [], "entities": [{"text": "precision", "start_pos": 96, "end_pos": 105, "type": "METRIC", "confidence": 0.9990565180778503}, {"text": "recall", "start_pos": 111, "end_pos": 117, "type": "METRIC", "confidence": 0.9910248517990112}]}]}