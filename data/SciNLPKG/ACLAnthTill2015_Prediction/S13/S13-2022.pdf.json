{"title": [{"text": "BUAP: N -gram based Feature Evaluation for the Cross-Lingual Textual Entailment Task", "labels": [], "entities": [{"text": "BUAP", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.8579502105712891}, {"text": "Cross-Lingual Textual Entailment", "start_pos": 47, "end_pos": 79, "type": "TASK", "confidence": 0.6041671435038248}]}], "abstractContent": [{"text": "This paper describes the evaluation of different kinds of textual features for the Cross-Lingual Textual Entailment Task of SemEval 2013.", "labels": [], "entities": [{"text": "Cross-Lingual Textual Entailment Task of SemEval 2013", "start_pos": 83, "end_pos": 136, "type": "TASK", "confidence": 0.6642777110849108}]}, {"text": "We have counted the number of N-grams for three types of textual entities (char-acter, word and PoS tags) that exist in the pair of sentences from which we are interested in determining the judgment of textual entailment.", "labels": [], "entities": []}, {"text": "Difference, intersection and distance (Euclidian, Manhattan and Jaccard) of N-grams were considered for constructing a feature vector which is further introduced in a support vector machine classifier which allows to construct a classification model.", "labels": [], "entities": []}, {"text": "Five different runs were submitted, one of them considering voting system of the previous four approaches.", "labels": [], "entities": []}, {"text": "The results obtained show a performance below the median of six teams that have participated in the competition.", "labels": [], "entities": []}], "introductionContent": [{"text": "The cross-lingual textual entailment (CLTE), recently proposed by and, is an extension of the textual entailment task ().", "labels": [], "entities": [{"text": "cross-lingual textual entailment (CLTE)", "start_pos": 4, "end_pos": 43, "type": "TASK", "confidence": 0.6865605662266413}]}, {"text": "Formally speaking, given a pair of topically related text fragments (T 1 and T 2 which are assumed to be TRUE statements) written in different languages, the CLTE task consists of automatically annotating it with one of the following entailment judgments: \u2022 bidirectional (T 1 \u2192 T 2 & T 1 \u2190 T 2): the two fragments entail each other (semantic equivalence); \u2022 forward (T 1 \u2192 T 2 & T 1 T 2): unidirectional entailment from T 1 to T 2; \u2022 backward (T 1 T 2 & T 1 \u2190 T 2): unidirectional entailment from T 2 to T 1; \u2022 no entailment (T1 T 2 & T 1 T 2): there is no entailment between T 1 and T 2 in both directions; The Cross-lingual datasets evaluated were available for the following language combinations (T 1-T 2): \u2022 Spanish-English (SPA-ENG) \u2022 German-English (DEU-ENG) \u2022 Italian-English (ITA-ENG) \u2022 French-English In this paper we describe the evaluation of different features extracted from each pair of topically related sentences.", "labels": [], "entities": []}, {"text": "N -grams of characters, words and PoS tags were counted with the aim of constructing a representative vector for each judgment entailment (FORWARD, BACKWARD, BI-DIRECTIONAL or NO-ENTAILMENT).", "labels": [], "entities": [{"text": "FORWARD", "start_pos": 139, "end_pos": 146, "type": "METRIC", "confidence": 0.9976769089698792}, {"text": "BACKWARD", "start_pos": 148, "end_pos": 156, "type": "METRIC", "confidence": 0.9919093251228333}, {"text": "BI-DIRECTIONAL", "start_pos": 158, "end_pos": 172, "type": "METRIC", "confidence": 0.9926871061325073}]}, {"text": "The resulting vectors were fed into a supervised classifier based on Support Vector Machines (SVM) 1 which attempted to construct a classification model.", "labels": [], "entities": []}, {"text": "The description of the features and the vectorial representation is given in Section 2.", "labels": [], "entities": []}, {"text": "The obtained results are shown and dicussed in Section 3.", "labels": [], "entities": []}, {"text": "Finally, the findings of this work are given in Section 4.", "labels": [], "entities": []}], "datasetContent": [{"text": "We have considered the task as a classification problem using the pivot approach.", "labels": [], "entities": []}, {"text": "Thus, we have translated 2 each pair to their corresponding language in order to have two pairs of sentences written in the same language.", "labels": [], "entities": []}, {"text": "Let P air(T 1, T 2) be the original pair of topically related sentences.", "labels": [], "entities": []}, {"text": "Then, we have obtained the English translation of T 1, denoted by T 3, which will be aligned with T 2.", "labels": [], "entities": []}, {"text": "On the other hand, we have translated T 2 to the other language (Spanish, German, Italian or French), denoted by T 4, which will be aligned with T 1.", "labels": [], "entities": []}, {"text": "The two pairs of sentences, P air(T 2, T 3) (English) and P air(T 1, T 4) (other language), are now written in the same language, and we can proceed to calculate the textual features we are interested in.", "labels": [], "entities": []}, {"text": "The features used to represent both sentences are described below: \u2022 N -grams of characters, with N = 2, \u00b7 \u00b7 \u00b7 , 5.", "labels": [], "entities": []}, {"text": "\u2022 N -grams of words, with N = 2, \u00b7 \u00b7 \u00b7 , 4.", "labels": [], "entities": []}, {"text": "\u2022 N -grams of PoS tags, with N = 2, \u00b7 \u00b7 \u00b7 , 4.", "labels": [], "entities": []}, {"text": "\u2022 Euclidean measure between each pair of sentences (P air(T 1, T 4) and P air(T 2, T 3)).", "labels": [], "entities": [{"text": "Euclidean measure", "start_pos": 2, "end_pos": 19, "type": "METRIC", "confidence": 0.797035813331604}]}, {"text": "\u2022 Manhattan measure between each pair of sentences (P air(T 1, T 4) and P air(T 2, T 3)).", "labels": [], "entities": [{"text": "Manhattan measure", "start_pos": 2, "end_pos": 19, "type": "METRIC", "confidence": 0.96730175614357}]}, {"text": "\u2022 Jaccard coefficient, expanding English terms in both sentences, T 2 and T 3, with their corresponding synonyms (none disambiguation process was considered).", "labels": [], "entities": [{"text": "Jaccard coefficient", "start_pos": 2, "end_pos": 21, "type": "METRIC", "confidence": 0.7924881279468536}]}, {"text": "The manner we have used the above mentioned features is described in detail in the following subsections.", "labels": [], "entities": []}, {"text": "The results obtained in the competition are presented and discussed in this section.", "labels": [], "entities": []}, {"text": "First, we describe the training and test corpus, and thereafter, the results obtained with the different approaches submitted.", "labels": [], "entities": []}, {"text": "In order to train the different approaches already discussed, we have constructed a training corpus made up of two datasets: the training data provided by the task organizers the task 8 of, and the test dataset together with the gold standard of CLTE task of SemEval 2012).", "labels": [], "entities": [{"text": "CLTE task of SemEval 2012", "start_pos": 246, "end_pos": 271, "type": "DATASET", "confidence": 0.8374210715293884}]}, {"text": "Thus, the training corpus contains 4000 sentence pairs.", "labels": [], "entities": []}, {"text": "The test set provided in the competition contains 2000 sentence pairs.", "labels": [], "entities": []}, {"text": "The corpus is balanced, with 1000 pairs for each language in the training dataset, whereas, 500 pairs are given in the test set for each language (see).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Description of the dataset", "labels": [], "entities": []}, {"text": " Table 3: Overall statistics obtained in the Task-8 of Se- mEval 2013", "labels": [], "entities": [{"text": "Task-8 of Se- mEval 2013", "start_pos": 45, "end_pos": 69, "type": "DATASET", "confidence": 0.7567142148812612}]}, {"text": " Table 4: Statistics of the approach 4, detailed by entail- ment judgment", "labels": [], "entities": []}]}