{"title": [{"text": "UNITOR-CORE TYPED: Combining Text Similarity and Semantic Filters through SV Regression", "labels": [], "entities": [{"text": "UNITOR-CORE", "start_pos": 0, "end_pos": 11, "type": "DATASET", "confidence": 0.3690458834171295}, {"text": "TYPED", "start_pos": 12, "end_pos": 17, "type": "METRIC", "confidence": 0.6174803972244263}, {"text": "Combining Text Similarity", "start_pos": 19, "end_pos": 44, "type": "TASK", "confidence": 0.5702378849188486}]}], "abstractContent": [{"text": "This paper presents the UNITOR system that participated in the *SEM 2013 shared task on Semantic Textual Similarity (STS).", "labels": [], "entities": [{"text": "SEM 2013 shared task on Semantic Textual Similarity (STS)", "start_pos": 64, "end_pos": 121, "type": "TASK", "confidence": 0.7909724522720684}]}, {"text": "The task is modeled as a Support Vector (SV) regression problem, where a similarity scoring function between text pairs is acquired from examples.", "labels": [], "entities": []}, {"text": "The proposed approach has been implemented in a system that aims at providing high applicability and robustness, in order to reduce the risk of over-fitting over a specific datasets.", "labels": [], "entities": []}, {"text": "Moreover, the approach does not require any manually coded resource (e.g. WordNet), but mainly exploits distributional analysis of un-labeled corpora.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 74, "end_pos": 81, "type": "DATASET", "confidence": 0.9513453841209412}]}, {"text": "A good level of accuracy is achieved over the shared task: in the Typed STS task the proposed system ranks in 1st and 2nd position.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 16, "end_pos": 24, "type": "METRIC", "confidence": 0.9994736313819885}, {"text": "Typed STS task", "start_pos": 66, "end_pos": 80, "type": "TASK", "confidence": 0.6737593412399292}]}], "introductionContent": [{"text": "Semantic Textual Similarity (STS) measures the degree of semantic equivalence between two phrases or texts.", "labels": [], "entities": [{"text": "Semantic Textual Similarity (STS)", "start_pos": 0, "end_pos": 33, "type": "TASK", "confidence": 0.7894197255373001}]}, {"text": "An effective method to compute similarity between sentences or semi-structured material has many applications in Natural Language Processing () and related areas such as Information Retrieval, improving the effectiveness of semantic search engines (), or databases, using text similarity in schema matching to solve semantic heterogeneity.", "labels": [], "entities": [{"text": "Information Retrieval", "start_pos": 170, "end_pos": 191, "type": "TASK", "confidence": 0.7754546999931335}]}, {"text": "This paper describes the UNITOR system participating in both tasks of the *SEM 2013 shared task on Semantic Textual Similarity (STS), described in (: \u2022 the Core STS tasks: given two sentences, s 1 and s 2 , participants are asked to provide a score reflecting the corresponding text similarity.", "labels": [], "entities": [{"text": "SEM 2013 shared task on Semantic Textual Similarity (STS)", "start_pos": 75, "end_pos": 132, "type": "TASK", "confidence": 0.7166876738721674}]}, {"text": "It is the same task proposed in ().", "labels": [], "entities": []}, {"text": "\u2022 the Typed-similarity STS task: given two semi-structured records t 1 and t 2 , containing several typed fields with textual values, participants are asked to provide multiple similarity scores: the types of similarity to be studied include location, author, people involved, time, events or actions, subject and description.", "labels": [], "entities": [{"text": "Typed-similarity STS task", "start_pos": 6, "end_pos": 31, "type": "TASK", "confidence": 0.8572994470596313}]}, {"text": "In line with several participants of the STS 2012 challenge, such as (, STS is here modeled as a Support Vector (SV) regression problem, where a SV regressor learns the similarity function over text pairs.", "labels": [], "entities": []}, {"text": "The semantic relatedness between two sentences is first modeled in an unsupervised fashion by several similarity functions, each describing the analogy between the two texts according to a specific semantic perspective.", "labels": [], "entities": []}, {"text": "We aim at capturing separately syntactic and lexical equivalences between sentences and exploiting either topical relatedness or paradigmatic similarity between individual words.", "labels": [], "entities": []}, {"text": "Such information is then combined in a supervised schema through a scoring function y = f ( x) over individual measures from labeled data through SV regression: y is the gold similarity score (provided by human annotators), while x is the vector of the different individual scores, provided by the chosen similarity functions.", "labels": [], "entities": [{"text": "gold similarity score", "start_pos": 170, "end_pos": 191, "type": "METRIC", "confidence": 0.7109912236531576}]}, {"text": "For the Typed STS task, given the specificity of the involved information and the heterogeneity of target scores, individual measures are not applied to entire texts.", "labels": [], "entities": [{"text": "Typed STS task", "start_pos": 8, "end_pos": 22, "type": "TASK", "confidence": 0.9345306158065796}]}, {"text": "Specific phrases are filtered according to linguistic policies, e.g. words characterized by specific Part-of-Speech (POS), such as nouns and verbs, or Named Entity (NE) Category, i.e. men-tions to specific name classes, such as of a PER-SON, LOCATION or DATE.", "labels": [], "entities": []}, {"text": "The former allows to focus the similarity functions over entities (nouns) or actions (verbs), while the latter allows to focus on some aspects connected with the targeted similarity functions, such as person involved, location or time.", "labels": [], "entities": []}, {"text": "The proposed approach has been implemented in a system that aims at providing high applicability and robustness.", "labels": [], "entities": []}, {"text": "This objective is pursued by adopting four similarity measures designed to avoid the risk of over-fitting over each specific dataset.", "labels": [], "entities": []}, {"text": "Moreover, the approach does not require any manually coded resource (e.g. WordNet), but mainly exploits distributional analysis of unlabeled corpora.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 74, "end_pos": 81, "type": "DATASET", "confidence": 0.9473993182182312}]}, {"text": "Despite of its simplicity, a good level of accuracy is achieved over the 2013 STS challenge: in the Typed STS task the proposed system ranks 1 stand 2 nd position (out of 18); in the Core STS task, it ranks around the 37 th position (out of 90) and a simple refinement to our model makes it 19 th . In the rest of the paper, in Section 2, the employed similarity functions are described and the application of SV regression is presented.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 43, "end_pos": 51, "type": "METRIC", "confidence": 0.9994906187057495}]}, {"text": "Finally, Section 3 discusses results on the *SEM 2013 shared task.", "labels": [], "entities": [{"text": "SEM 2013 shared task", "start_pos": 45, "end_pos": 65, "type": "TASK", "confidence": 0.5789423882961273}]}], "datasetContent": [{"text": "In all experiments, sentences are processed with the Stanford CoreNLP 3 system, for Part-of-Speech tagging, lemmatization, named entity recognition and dependency parsing.", "labels": [], "entities": [{"text": "Stanford CoreNLP 3", "start_pos": 53, "end_pos": 71, "type": "DATASET", "confidence": 0.9014875690142313}, {"text": "Part-of-Speech tagging", "start_pos": 84, "end_pos": 106, "type": "TASK", "confidence": 0.7161737382411957}, {"text": "named entity recognition", "start_pos": 123, "end_pos": 147, "type": "TASK", "confidence": 0.6099256177743276}, {"text": "dependency parsing", "start_pos": 152, "end_pos": 170, "type": "TASK", "confidence": 0.807770699262619}]}, {"text": "In order to estimate the basic lexical similarity function employed in the SUM, SSC and SPTK operators, a co-occurrence Word Space is acquired through the distributional analysis of the UkWaC corpus (), a Web document collection made of about 2 billion tokens.", "labels": [], "entities": [{"text": "UkWaC corpus", "start_pos": 186, "end_pos": 198, "type": "DATASET", "confidence": 0.9356205761432648}]}, {"text": "The same setting of (Croce et al., 2012a) has been adopted for the space acquisition.", "labels": [], "entities": [{"text": "space acquisition", "start_pos": 67, "end_pos": 84, "type": "TASK", "confidence": 0.7553953528404236}]}, {"text": "The same setup described in () is applied to estimate the SSC function.", "labels": [], "entities": [{"text": "SSC", "start_pos": 58, "end_pos": 61, "type": "TASK", "confidence": 0.9323988556861877}]}, {"text": "The similarity between pairs of syntactically restricted word compound is evaluated through a Symmetric model: it selects the best 200 dimensions of the space, selected by maximizing the component-wise product of each compound as in ( , and combines the similarity scores measured in each couple subspace with the product function.", "labels": [], "entities": []}, {"text": "The similarity score in each subspace is obtained by summing the cosine similarity of the corresponding projected words.", "labels": [], "entities": [{"text": "similarity score", "start_pos": 4, "end_pos": 20, "type": "METRIC", "confidence": 0.9436391890048981}]}, {"text": "The \"soft cardinality\" is estimated with the parameter p = 2.", "labels": [], "entities": []}, {"text": "The estimation of the semantically Smoothed Partial Tree Kernel (SPTK) is made available by an extended version of SVM-LightTK software 5) implementing the smooth matching between tree nodes.", "labels": [], "entities": [{"text": "Smoothed Partial Tree Kernel (SPTK)", "start_pos": 35, "end_pos": 70, "type": "TASK", "confidence": 0.6111940826688494}]}, {"text": "Similarity between lexical nodes is estimated as the cosine similarity in the co-occurrence Word Space described above, as in).", "labels": [], "entities": []}, {"text": "Finally, SVM-LightTK is employed for the SV regression learning to combine specific similarity functions.", "labels": [], "entities": []}], "tableCaptions": []}