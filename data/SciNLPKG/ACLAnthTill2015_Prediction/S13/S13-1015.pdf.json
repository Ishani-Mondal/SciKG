{"title": [{"text": "UMCC_DLSI: Textual Similarity based on Lexical-Semantic features", "labels": [], "entities": [{"text": "UMCC_DLSI", "start_pos": 0, "end_pos": 9, "type": "DATASET", "confidence": 0.7534182469050089}]}], "abstractContent": [{"text": "This paper describes the specifications and results of UMCC_DLSI system, which participated in the Semantic Textual Similarity task (STS) of SemEval-2013.", "labels": [], "entities": [{"text": "Semantic Textual Similarity task (STS) of SemEval-2013", "start_pos": 99, "end_pos": 153, "type": "TASK", "confidence": 0.7896644473075867}]}, {"text": "Our supervised system uses different types of lexical and semantic features to train a Bagging classifier used to decide the correct option.", "labels": [], "entities": []}, {"text": "Related to the different features we can highlight the resource ISR-WN used to extract semantic relations among words and the use of different algorithms to establish semantic and lexical similarities.", "labels": [], "entities": []}, {"text": "In order to establish which features are the most appropriate to improve STS results we participated with three runs using different set of features.", "labels": [], "entities": [{"text": "STS", "start_pos": 73, "end_pos": 76, "type": "TASK", "confidence": 0.974003255367279}]}, {"text": "Our best run reached the position 44 in the official ranking, obtaining a general correlation coefficient of 0.61.", "labels": [], "entities": [{"text": "general correlation coefficient", "start_pos": 74, "end_pos": 105, "type": "METRIC", "confidence": 0.802964965502421}]}], "introductionContent": [{"text": ") presents the task Semantic Textual Similarity (STS) again.", "labels": [], "entities": [{"text": "Semantic Textual Similarity (STS)", "start_pos": 20, "end_pos": 53, "type": "TASK", "confidence": 0.8030157287915548}]}, {"text": "In STS, the participating systems must examine the degree of semantic equivalence between two sentences.", "labels": [], "entities": [{"text": "STS", "start_pos": 3, "end_pos": 6, "type": "TASK", "confidence": 0.9673009514808655}]}, {"text": "The goal of this task is to create a unified framework for the evaluation of semantic textual similarity modules and to characterize their impact on NLP applications.", "labels": [], "entities": []}, {"text": "STS is related to Textual Entailment (TE) and Paraphrase tasks.", "labels": [], "entities": [{"text": "Textual Entailment (TE)", "start_pos": 18, "end_pos": 41, "type": "TASK", "confidence": 0.7778306007385254}]}, {"text": "The main difference is that STS assumes bidirectional graded equivalence between the pair of textual snippets.", "labels": [], "entities": [{"text": "STS", "start_pos": 28, "end_pos": 31, "type": "TASK", "confidence": 0.9290279746055603}]}, {"text": "In case of TE, the equivalence is directional (e.g. a student is a person, but a person is not necessarily a student).", "labels": [], "entities": []}, {"text": "In addition, STS differs from TE and Paraphrase in that, rather than being a binary yes/no decision, STS is a similarity-graded notion (e.g. a student is more similar to a person than a dog to a person).", "labels": [], "entities": []}, {"text": "This graded bidirectional is useful for NLP tasks such as Machine Translation (MT), Information Extraction (IE), Question Answering (QA), and Summarization.", "labels": [], "entities": [{"text": "Machine Translation (MT)", "start_pos": 58, "end_pos": 82, "type": "TASK", "confidence": 0.8337949752807617}, {"text": "Information Extraction (IE)", "start_pos": 84, "end_pos": 111, "type": "TASK", "confidence": 0.823143208026886}, {"text": "Question Answering (QA)", "start_pos": 113, "end_pos": 136, "type": "TASK", "confidence": 0.865511691570282}, {"text": "Summarization", "start_pos": 142, "end_pos": 155, "type": "TASK", "confidence": 0.99196457862854}]}, {"text": "Several semantic tasks could be added as modules in the STS framework, \"such as Word Sense Disambiguation and Induction, Lexical Substitution, Semantic Role Labeling, Multiword Expression detection and handling, Anaphora and Co-reference resolution, Time and Date resolution and Named Entity, among others\" 1", "labels": [], "entities": [{"text": "Word Sense Disambiguation and Induction", "start_pos": 80, "end_pos": 119, "type": "TASK", "confidence": 0.6814427196979522}, {"text": "Semantic Role Labeling", "start_pos": 143, "end_pos": 165, "type": "TASK", "confidence": 0.7004332542419434}, {"text": "Multiword Expression detection and handling", "start_pos": 167, "end_pos": 210, "type": "TASK", "confidence": 0.894940447807312}, {"text": "Anaphora and Co-reference resolution", "start_pos": 212, "end_pos": 248, "type": "TASK", "confidence": 0.6424975842237473}, {"text": "Time and Date resolution", "start_pos": 250, "end_pos": 274, "type": "TASK", "confidence": 0.5591428801417351}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 1. Weights applied to WordNet relations.", "labels": [], "entities": []}, {"text": " Table 3. Features from the analyzed sentences.", "labels": [], "entities": []}, {"text": " Table 14. Official SemEval-2013 results over test  datasets. Ranking (R).", "labels": [], "entities": [{"text": "Official SemEval-2013 results over test  datasets", "start_pos": 11, "end_pos": 60, "type": "DATASET", "confidence": 0.7376725226640701}, {"text": "Ranking (R)", "start_pos": 62, "end_pos": 73, "type": "METRIC", "confidence": 0.8949592411518097}]}, {"text": " Table 15. Comparison with best run (SemEval 2013).", "labels": [], "entities": [{"text": "Comparison", "start_pos": 11, "end_pos": 21, "type": "METRIC", "confidence": 0.9531198740005493}, {"text": "SemEval 2013)", "start_pos": 37, "end_pos": 50, "type": "DATASET", "confidence": 0.8795263171195984}]}]}