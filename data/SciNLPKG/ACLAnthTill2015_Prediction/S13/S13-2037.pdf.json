{"title": [{"text": "SATTY : Word Sense Induction Application in Web Search Clustering *", "labels": [], "entities": [{"text": "SATTY", "start_pos": 0, "end_pos": 5, "type": "TASK", "confidence": 0.6629847288131714}]}], "abstractContent": [{"text": "The aim of this paper is to perform Word Sense induction (WSI); which clusters web search results and produces a diversified list of search results.", "labels": [], "entities": [{"text": "Word Sense induction (WSI)", "start_pos": 36, "end_pos": 62, "type": "TASK", "confidence": 0.7619600743055344}]}, {"text": "It describes the WSI system developed for Task 11 of SemEval-2013.", "labels": [], "entities": []}, {"text": "This paper implements the idea of monotone sub-modular function optimization using greedy algorithm.", "labels": [], "entities": [{"text": "monotone sub-modular function optimization", "start_pos": 34, "end_pos": 76, "type": "TASK", "confidence": 0.7108938694000244}]}], "introductionContent": [{"text": "Two different types of systems were submitted under Task 11 of).", "labels": [], "entities": []}, {"text": "The two system types are WSI (Word Sense Induction) and WSD (Word Sense Disambiguation).", "labels": [], "entities": [{"text": "Word Sense Disambiguation)", "start_pos": 61, "end_pos": 87, "type": "TASK", "confidence": 0.6429989337921143}]}, {"text": "WSD is the task of automatically associating meaning with words.", "labels": [], "entities": [{"text": "WSD", "start_pos": 0, "end_pos": 3, "type": "TASK", "confidence": 0.8732768297195435}]}, {"text": "In WSD the possible meanings fora given word are drawn from an existing sense inventory.", "labels": [], "entities": [{"text": "WSD", "start_pos": 3, "end_pos": 6, "type": "TASK", "confidence": 0.8716422915458679}]}, {"text": "In contrast, WSI aims at automatically identifying the meanings of a given word from raw text.", "labels": [], "entities": [{"text": "WSI", "start_pos": 13, "end_pos": 16, "type": "TASK", "confidence": 0.8770450353622437}]}, {"text": "A WSI system will be asked to identify the meaning of the input query and cluster the search results into semantically-related groups according to their meanings.", "labels": [], "entities": []}, {"text": "Instead, a WSD system will be requested to sense-tag the above search results with the appropriate senses of the input query and this, again, will implicitly determine a clustering of snippets (i.e., one cluster per sense).", "labels": [], "entities": []}, {"text": "* This system was designed and submitted in the competition SemEval-2013 under task 11 : Evaluating Word Sense Induction & Disambiguation within An End-User Application (Roberto Navigli and Daniele Vannella2013).", "labels": [], "entities": [{"text": "Evaluating Word Sense Induction & Disambiguation within An End-User Application", "start_pos": 89, "end_pos": 168, "type": "TASK", "confidence": 0.8025568276643753}]}, {"text": "http://www.cs.york.ac.uk/semeval-2013/.", "labels": [], "entities": []}, {"text": "Our system implements the idea given in (Jingrui He and Hanghang Tong and Qiaozhu.", "labels": [], "entities": []}, {"text": "This developed system uses the concept of submodularity.", "labels": [], "entities": []}, {"text": "The task is treated as a submodular function maximization which has its benefits.", "labels": [], "entities": []}, {"text": "On the one hand, there exists a simple greedy algorithm for monotone submodular function maximization where the solution obtained is guaranteed to be almost as good as the best possible solution according to an objective.", "labels": [], "entities": [{"text": "monotone submodular function maximization", "start_pos": 60, "end_pos": 101, "type": "TASK", "confidence": 0.6551947593688965}]}, {"text": "More precisely, the greedy algorithm is a constant factor approximation to the cardinality constrained version of the problem, so that the approximate soultion is in the bound of (1 \u2212 1/e) of optimal solution.", "labels": [], "entities": []}, {"text": "It is also important to note that this is a worst case bound, and inmost cases the quality of the solution obtained will be much better than this bound suggests.", "labels": [], "entities": []}, {"text": "In our system, monotone submodular objective of (Jingrui He and Hanghang) was implemented to find the top k simultaneously relevant and diversified list of search results.", "labels": [], "entities": []}, {"text": "Once these top k results are obtained, they are used as centroids to form clusters by classifying each of remaining search results to one of the centroid with maximum similarity, producing k clusters.", "labels": [], "entities": []}, {"text": "Those results which are not similar to any of the centroids are either put in a different cluster or are assigned to cluster with highest similarity.", "labels": [], "entities": []}], "datasetContent": [{"text": "The implemented system was tested on data given by SemEval -2013  Only title and snippet information was used.", "labels": [], "entities": [{"text": "SemEval -2013", "start_pos": 51, "end_pos": 64, "type": "DATASET", "confidence": 0.7262827555338541}]}, {"text": "The relevance between query and a search result is calculated using weighted Jaccard.", "labels": [], "entities": [{"text": "relevance", "start_pos": 4, "end_pos": 13, "type": "METRIC", "confidence": 0.9604306817054749}]}, {"text": "Cosine similarity is used to calculate the similarity between search results using only title and snippet.", "labels": [], "entities": [{"text": "Cosine similarity", "start_pos": 0, "end_pos": 17, "type": "METRIC", "confidence": 0.8106974363327026}]}, {"text": "It was just bag of words (i.e. unigram) approach and no other preprocessing of data was done.", "labels": [], "entities": []}, {"text": "In the first stage, system produces top 10 diversified search results which are then used as centroids to form 10 clusters.", "labels": [], "entities": []}, {"text": "Those results which are not similar to any of the centroids are put in a different cluster, sometimes resulting in 11 clusters.", "labels": [], "entities": []}, {"text": "The evaluation method required : (i) to rank the search results within each cluster according to the confidence with which they belong to that cluster, (ii) to rank the clusters according to their diversity.", "labels": [], "entities": []}, {"text": "The cluster ranking is kept same as the rank of their centroids in top 10 results returned in first stage of the system.", "labels": [], "entities": []}, {"text": "Also search results within each cluster are then ranked by their average similarity to rest of the search results in the same cluster, in descending order with respect to the ranking score.", "labels": [], "entities": []}, {"text": "The ranking score of search result xi in cluster C is calculated as below, which is used in our system : The other way of ranking search results within a cluster can be ranking by their relevance to the query.", "labels": [], "entities": []}, {"text": "In that case, it depends on how good the relevance scores are.", "labels": [], "entities": [{"text": "relevance", "start_pos": 41, "end_pos": 50, "type": "METRIC", "confidence": 0.9094924330711365}]}, {"text": "This ranking affects the ability of the system to diversify search results, i.e., Subtopic Recall@K and Subtopic Precision@r measures.", "labels": [], "entities": []}, {"text": "The clustering quality is measured by measures of Rand Index (RI), Adjusted Rand Index (ARI), F1-measure (F1) and Jaccard Index (JI).", "labels": [], "entities": [{"text": "Rand Index (RI)", "start_pos": 50, "end_pos": 65, "type": "METRIC", "confidence": 0.952194619178772}, {"text": "Adjusted Rand Index (ARI)", "start_pos": 67, "end_pos": 92, "type": "METRIC", "confidence": 0.9251608451207479}, {"text": "F1-measure (F1)", "start_pos": 94, "end_pos": 109, "type": "METRIC", "confidence": 0.918747216463089}, {"text": "Jaccard Index (JI)", "start_pos": 114, "end_pos": 132, "type": "METRIC", "confidence": 0.8534910202026367}]}, {"text": "All these evaluation metrics used are described in).", "labels": [], "entities": []}, {"text": "All the given evaluation metric values are obtained for the described data using the java evaluator provided by.", "labels": [], "entities": []}, {"text": "Our system's evaluation measures along with other systems, submitted in SemEval -2013 are shown in tables 1, 2 and 3.", "labels": [], "entities": [{"text": "SemEval -2013", "start_pos": 72, "end_pos": 85, "type": "DATASET", "confidence": 0.7244332234064738}]}, {"text": "Our system's name is task11-satty-approach1.", "labels": [], "entities": []}, {"text": "The clustering quality was found to be good as indicated by F1 and RI while scoring low for ARI, JI.", "labels": [], "entities": [{"text": "F1", "start_pos": 60, "end_pos": 62, "type": "METRIC", "confidence": 0.9995143413543701}, {"text": "RI", "start_pos": 67, "end_pos": 69, "type": "METRIC", "confidence": 0.9882723689079285}, {"text": "ARI", "start_pos": 92, "end_pos": 95, "type": "METRIC", "confidence": 0.8201887011528015}]}, {"text": "In terms of diversification of search results, it did not perform that well indicating that either ranking of search results within each cluster or cluster ranking or both were not that good.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: The best result for each column is presented in boldface. singleton and allinone are baseline systems and  gold is the theoretical upper-bound for the task. WSI : Word Sense Induction, WSD : Word Sense Disambiguation", "labels": [], "entities": [{"text": "Word Sense Disambiguation", "start_pos": 201, "end_pos": 226, "type": "TASK", "confidence": 0.5828046500682831}]}, {"text": " Table 2: S-recall@K for different values of K averaged over 100 queries.", "labels": [], "entities": []}]}