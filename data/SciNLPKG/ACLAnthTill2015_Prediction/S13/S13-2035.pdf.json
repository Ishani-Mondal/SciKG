{"title": [{"text": "SemEval-2013 Task 11: Word Sense Induction & Disambiguation within an End-User Application", "labels": [], "entities": [{"text": "SemEval-2013 Task 11", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.8221830129623413}, {"text": "Word Sense Induction & Disambiguation", "start_pos": 22, "end_pos": 59, "type": "TASK", "confidence": 0.6723846554756164}]}], "abstractContent": [{"text": "In this paper we describe our Semeval-2013 task on Word Sense Induction and Dis-ambiguation within an end-user application, namely Web search result clustering and diversification.", "labels": [], "entities": [{"text": "Word Sense Induction", "start_pos": 51, "end_pos": 71, "type": "TASK", "confidence": 0.6003303627173106}, {"text": "Web search result clustering", "start_pos": 131, "end_pos": 159, "type": "TASK", "confidence": 0.6029865890741348}]}, {"text": "Given a target query, induction and disambiguation systems are requested to cluster and diversify the search results returned by a search engine for that query.", "labels": [], "entities": []}, {"text": "The task enables the end-to-end evaluation and comparison of systems.", "labels": [], "entities": []}], "introductionContent": [{"text": "Word ambiguity is a pervasive issue in Natural Language Processing.", "labels": [], "entities": [{"text": "Word ambiguity", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.6867789328098297}, {"text": "Natural Language Processing", "start_pos": 39, "end_pos": 66, "type": "TASK", "confidence": 0.6360759337743124}]}, {"text": "Two main techniques in computational lexical semantics, i.e., Word Sense Disambiguation (WSD) and Word Sense Induction (WSI) address this issue from different perspectives: the former is aimed at assigning word senses from a predefined sense inventory to words in context, whereas the latter automatically identifies the meanings of a word of interest by clustering the contexts in which it occurs (see) fora survey).", "labels": [], "entities": [{"text": "computational lexical semantics", "start_pos": 23, "end_pos": 54, "type": "TASK", "confidence": 0.7130013306935629}, {"text": "Word Sense Disambiguation (WSD)", "start_pos": 62, "end_pos": 93, "type": "TASK", "confidence": 0.7287596712509791}, {"text": "Word Sense Induction (WSI", "start_pos": 98, "end_pos": 123, "type": "TASK", "confidence": 0.6391949474811554}]}, {"text": "Unfortunately, the paradigms of both WSD and WSI suffer from significant issues which hamper their success in real-world applications.", "labels": [], "entities": [{"text": "WSD", "start_pos": 37, "end_pos": 40, "type": "TASK", "confidence": 0.7081876993179321}]}, {"text": "In fact, the performance of WSD systems depends heavily on which sense inventory is chosen.", "labels": [], "entities": [{"text": "WSD", "start_pos": 28, "end_pos": 31, "type": "TASK", "confidence": 0.9471194744110107}]}, {"text": "For instance, the most popular computational lexicon of English, i.e., WordNet, provides fine-grained distinctions which make the disambiguation task quite difficult even for humans), although disagreements can be solved to some extent with graph-based methods.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 71, "end_pos": 78, "type": "DATASET", "confidence": 0.951006293296814}]}, {"text": "On the other hand, although WSI overcomes this issue by allowing unrestrained sets of senses, its evaluation is particularly arduous because there is no easy way of comparing and ranking different representations of senses.", "labels": [], "entities": [{"text": "WSI", "start_pos": 28, "end_pos": 31, "type": "TASK", "confidence": 0.7914099097251892}]}, {"text": "In fact, all the proposed measures in the literature tend to favour specific cluster shapes (e.g., singletons or all-in-one clusters) of the senses produced as output.", "labels": [], "entities": []}, {"text": "Indeed, WSI evaluation is actually an instance of the more general and difficult problem of evaluating clustering algorithms.", "labels": [], "entities": [{"text": "WSI evaluation", "start_pos": 8, "end_pos": 22, "type": "TASK", "confidence": 0.9564269483089447}]}, {"text": "Nonetheless, many everyday tasks carried out by online users would benefit from intelligent systems able to address the lexical ambiguity issue effectively.", "labels": [], "entities": []}, {"text": "A casein point is Web information retrieval, a task which is becoming increasingly difficult given the continuously growing pool of Web text of the most wildly disparate kinds.", "labels": [], "entities": [{"text": "Web information retrieval", "start_pos": 18, "end_pos": 43, "type": "TASK", "confidence": 0.6067701876163483}]}, {"text": "Recent work has addressed this issue by proposing a general evaluation framework for injecting WSI into Web search result clustering and diversification.", "labels": [], "entities": [{"text": "Web search result clustering", "start_pos": 104, "end_pos": 132, "type": "TASK", "confidence": 0.5898603200912476}]}, {"text": "In this task the search results returned by a search engine for an input query are grouped into clusters, and diversified by providing a reranking which maximizes the meaning heterogeneity of the top ranking results.", "labels": [], "entities": []}, {"text": "The Semeval-2013 task described in this paper 1 adopts the evaluation framework of, and extends it to both WSD and WSI systems.", "labels": [], "entities": []}, {"text": "The task is aimed at overcoming the wellknown limitations of in vitro evaluations, such as those of previous SemEval tasks on the topic, and enabling a fair comparison between the two disambiguation paradigms.", "labels": [], "entities": []}, {"text": "Key to our framework is the assumption that search results grouped into a given cluster are semantically related to each other and that each cluster is expected to represent a specific meaning of the input query (even though it is possible for more than one cluster to represent the same meaning).", "labels": [], "entities": []}, {"text": "For instance, consider the target query apple and the following 3 search result snippets: 1.", "labels": [], "entities": []}, {"text": "Apple Inc., formerly Apple Computer, Inc., is...", "labels": [], "entities": []}, {"text": "2. The science of apple growing is called pomology...", "labels": [], "entities": [{"text": "apple growing", "start_pos": 18, "end_pos": 31, "type": "TASK", "confidence": 0.7245353907346725}]}], "datasetContent": [{"text": "We created a dataset of 100 ambiguous queries.", "labels": [], "entities": []}, {"text": "The queries were randomly sampled from the AOL search logs so as to ensure that they had been used in real search sessions.", "labels": [], "entities": [{"text": "AOL search logs", "start_pos": 43, "end_pos": 58, "type": "DATASET", "confidence": 0.8999701340993246}]}, {"text": "Following previous work on the topic () we selected those queries for which a sense inventory exists as a disambiguation page in the English Wikipedia 2 . This guaranteed that the selected queries consisted of either a single word or a multiword expression for which we had a collaborativelyedited list of meanings, including lexicographic and encyclopedic ones.", "labels": [], "entities": [{"text": "English Wikipedia 2", "start_pos": 133, "end_pos": 152, "type": "DATASET", "confidence": 0.898970345656077}]}, {"text": "We discarded all queries made   up of > 4 words, since the length of the great majority of queries lay in the range.", "labels": [], "entities": []}, {"text": "In we compare the percentage distribution of 1-to 4-word queries in the AOL query logs against our dataset of queries.", "labels": [], "entities": [{"text": "AOL query logs", "start_pos": 72, "end_pos": 86, "type": "DATASET", "confidence": 0.856060266494751}]}, {"text": "Note that we increased the percentage of 3-and 4-word queries in order to have a significant coverage of those lengths.", "labels": [], "entities": []}, {"text": "Anyhow, in both cases most queries contained from 1 to 2 words.", "labels": [], "entities": []}, {"text": "Note that the reported percentage distributions of query length is different from recent statistics for two reasons: first, over the years users have increased the average number of words per query in order to refine their searches; second, we selected only queries which were either single words (e.g., apple) or multi-word expressions (e.g., mortal kombat), thereby discarding several long queries composed of different words (such as angelina jolie actress).", "labels": [], "entities": []}, {"text": "Finally, we submitted each query to Google search and retrieved the 64 top-ranking results returned for each query.", "labels": [], "entities": []}, {"text": "Therefore, overall the dataset consists of 100 queries and 6,400 results.", "labels": [], "entities": []}, {"text": "Each search result includes the following information: page title, URL of the page and snippet of the page text.", "labels": [], "entities": []}, {"text": "We show an example of search result for the apple query in.", "labels": [], "entities": []}, {"text": "For each query q we used Amazon Mechanical Turk to annotate each query result with the most suitable sense.", "labels": [], "entities": [{"text": "Amazon Mechanical Turk", "start_pos": 25, "end_pos": 47, "type": "DATASET", "confidence": 0.9333427747090658}]}, {"text": "The sense inventory for q was obtained by listing the senses available in the Wikipedia disambiguation page of q augmented with additional options from the classes obtained from the section headings of the disambiguation page plus the OTHER catch-all meaning.", "labels": [], "entities": [{"text": "OTHER", "start_pos": 235, "end_pos": 240, "type": "METRIC", "confidence": 0.5672824382781982}]}, {"text": "For instance, consider the apple query.", "labels": [], "entities": []}, {"text": "We show its disambiguation page in.", "labels": [], "entities": []}, {"text": "The sense inventory for apple was made up of the senses listed in that page (e.g., MALUS, APPLE INC., APPLE BANK, etc.) plus the set of generic classes OTHER PLANTS AND PLANT PARTS, OTHER COMPANIES, OTHER FILMS, plus OTHER.", "labels": [], "entities": [{"text": "MALUS", "start_pos": 83, "end_pos": 88, "type": "METRIC", "confidence": 0.9648919105529785}, {"text": "APPLE INC.", "start_pos": 90, "end_pos": 100, "type": "METRIC", "confidence": 0.795923262834549}, {"text": "APPLE BANK", "start_pos": 102, "end_pos": 112, "type": "METRIC", "confidence": 0.5721482336521149}, {"text": "OTHER COMPANIES", "start_pos": 182, "end_pos": 197, "type": "METRIC", "confidence": 0.5965659916400909}, {"text": "OTHER", "start_pos": 199, "end_pos": 204, "type": "METRIC", "confidence": 0.6581981182098389}, {"text": "FILMS", "start_pos": 205, "end_pos": 210, "type": "METRIC", "confidence": 0.5994144678115845}, {"text": "OTHER", "start_pos": 217, "end_pos": 222, "type": "METRIC", "confidence": 0.8083286881446838}]}, {"text": "For each query we ensured that three annotators tagged each of the 64 results for that query with the most suitable sense among those in the sense inventory (selecting OTHER if no sense was appropriate).", "labels": [], "entities": [{"text": "OTHER", "start_pos": 168, "end_pos": 173, "type": "METRIC", "confidence": 0.8401350975036621}]}, {"text": "Specifically, each Turker was provided with the following instructions: \"The goal is annotating the search result snippets returned by Google fora given query with the appropriate meaning among those available (obtained from the Wikipedia disambiguation page for the query).", "labels": [], "entities": []}, {"text": "You have to select the meaning that you consider most appropriate\".", "labels": [], "entities": []}, {"text": "No constraint on the age, gender and citizenship of the annotators was imposed.", "labels": [], "entities": []}, {"text": "However, in order to avoid random tagging of search results, we provided 3 gold-standard result annotations per query, which could be shown to the Turker more than once during the annotation process.", "labels": [], "entities": []}, {"text": "In the case (s)he failed to annotate the gold items, the annotator was automatically excluded.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Percentage distribution of AOL query lengths  (first row) vs. the queries sampled for our task (second  row).", "labels": [], "entities": []}, {"text": " Table 2: Percentage of snippets with full agreement, ma- jority agreement and full disagreement.", "labels": [], "entities": []}, {"text": " Table 4: Resources used for WSI/WSD.", "labels": [], "entities": [{"text": "WSI/WSD", "start_pos": 29, "end_pos": 36, "type": "TASK", "confidence": 0.7273604472478231}]}, {"text": " Table 6: Results for Jaccard Index (JI) and F1 measure.", "labels": [], "entities": [{"text": "Jaccard Index (JI)", "start_pos": 22, "end_pos": 40, "type": "METRIC", "confidence": 0.9180392622947693}, {"text": "F1 measure", "start_pos": 45, "end_pos": 55, "type": "METRIC", "confidence": 0.969330221414566}]}, {"text": " Table 7: Average number of clusters (# cl.) and average  cluster size (ACS).", "labels": [], "entities": [{"text": "Average", "start_pos": 10, "end_pos": 17, "type": "METRIC", "confidence": 0.9652028679847717}, {"text": "average  cluster size (ACS)", "start_pos": 49, "end_pos": 76, "type": "METRIC", "confidence": 0.8929431339104971}]}]}