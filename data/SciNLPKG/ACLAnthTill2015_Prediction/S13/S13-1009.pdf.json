{"title": [{"text": "SXUCFN-Core: STS Models Integrating FrameNet Parsing Information", "labels": [], "entities": []}], "abstractContent": [{"text": "This paper describes our system submitted to *SEM 2013 Semantic Textual Similarity (STS) core task which aims to measure semantic similarity of two given text snippets.", "labels": [], "entities": [{"text": "SEM 2013 Semantic Textual Similarity (STS) core task", "start_pos": 46, "end_pos": 98, "type": "TASK", "confidence": 0.851916229724884}]}, {"text": "In this shared task, we propose an interpolation STS model named Model_LIM integrating Fra-meNet parsing information, which has a good performance with low time complexity compared with former submissions.", "labels": [], "entities": [{"text": "Fra-meNet parsing information", "start_pos": 87, "end_pos": 116, "type": "TASK", "confidence": 0.7279849847157797}]}], "introductionContent": [{"text": "The goal of Semantic Textual Similarity (STS) is to measure semantic similarity of two given text snippets.", "labels": [], "entities": [{"text": "Semantic Textual Similarity (STS)", "start_pos": 12, "end_pos": 45, "type": "TASK", "confidence": 0.8021829078594843}]}, {"text": "STS has been recently proposed by as a pilot task, which has close relationship with both tasks of Textual Entailment and Paraphrase, but not equivalent with them and it is more directly applicable to a number of NLP tasks such as Question Answering (), Text Summarization (), etc.", "labels": [], "entities": [{"text": "Question Answering", "start_pos": 231, "end_pos": 249, "type": "TASK", "confidence": 0.8619721233844757}, {"text": "Text Summarization", "start_pos": 254, "end_pos": 272, "type": "TASK", "confidence": 0.7968681752681732}]}, {"text": "And yet, the acquiring of sentence similarity has been the most important and basic task in STS.", "labels": [], "entities": [{"text": "acquiring of sentence similarity", "start_pos": 13, "end_pos": 45, "type": "TASK", "confidence": 0.8823865503072739}, {"text": "STS", "start_pos": 92, "end_pos": 95, "type": "TASK", "confidence": 0.9813812971115112}]}, {"text": "Therefore, the STS core task of *SEM 2013 conference, is formally defined as the degree of semantic equivalence between two sentences as follows: \uf0b7 5: completely equivalent, as they mean the same thing.", "labels": [], "entities": [{"text": "STS core task of *SEM 2013 conference", "start_pos": 15, "end_pos": 52, "type": "DATASET", "confidence": 0.5528340823948383}]}, {"text": "\uf0b7 4: mostly equivalent, but some unimportant details differ.", "labels": [], "entities": []}, {"text": "\uf0b7 3: roughly equivalent, but some important information differs/missing.", "labels": [], "entities": []}, {"text": "\uf0b7 2: not equivalent, but share some details.", "labels": [], "entities": []}, {"text": "\uf0b7 1: not equivalent, but are on the same topic.", "labels": [], "entities": []}, {"text": "\uf0b7 0: on different topics.", "labels": [], "entities": []}, {"text": "In this paper, we attempt to integrate semantic information into STS task besides the lower-level word and syntactic information.", "labels": [], "entities": []}, {"text": "Evaluation results show that our STS model could benefit from semantic parsing information of two text snippets.", "labels": [], "entities": [{"text": "semantic parsing information", "start_pos": 62, "end_pos": 90, "type": "TASK", "confidence": 0.7731895248095194}]}, {"text": "The rest of the paper is organized as follows: Section 2 reviews prior researches on STS.", "labels": [], "entities": [{"text": "STS", "start_pos": 85, "end_pos": 88, "type": "TASK", "confidence": 0.9596556425094604}]}, {"text": "Section 3 illustrates three models measuring text similarity.", "labels": [], "entities": []}, {"text": "Section 4 describes the linear interpolation model in detail.", "labels": [], "entities": []}, {"text": "Section 5 provides the experimental results on the development set as well as the official results on all published datasets.", "labels": [], "entities": []}, {"text": "Finally, Section 6 summarizes our paper with direction for future works.", "labels": [], "entities": []}], "datasetContent": [{"text": "There is no new train data in 2013, so we use 2012 data as train data.", "labels": [], "entities": []}, {"text": "From  From, we notice that all the models except Model_FN, are apt to handle the SMTeuroparl that involves long sentences.", "labels": [], "entities": [{"text": "SMTeuroparl", "start_pos": 81, "end_pos": 92, "type": "TASK", "confidence": 0.8984880447387695}]}, {"text": "For Model_FN, it performs well in computing on short and similarly structured texts such as MSRvid (This will be confirmed in test data later).", "labels": [], "entities": [{"text": "MSRvid", "start_pos": 92, "end_pos": 98, "type": "DATASET", "confidence": 0.8671919703483582}]}, {"text": "Although WordNet and FrameNet model has a mere weight of 20% in Model_LIM (i.e. \u03c9 1 +\u03c9 2 = 0.2), the run which integrate more semantic information displays a consistent performance across the three train sets (especially in SMTeuroparl, the Pearson correlation rises from 0.5178 to 0.66808), when compared to the other three.: Performances of our three models as well as the baseline and UKP_run2 (that is ranked 1 in last STS task) results on 2012 test data.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 9, "end_pos": 16, "type": "DATASET", "confidence": 0.9418114423751831}, {"text": "Pearson correlation", "start_pos": 241, "end_pos": 260, "type": "METRIC", "confidence": 0.9093092679977417}, {"text": "UKP", "start_pos": 388, "end_pos": 391, "type": "DATASET", "confidence": 0.9019098877906799}, {"text": "2012 test data", "start_pos": 444, "end_pos": 458, "type": "DATASET", "confidence": 0.695737381776174}]}, {"text": "The highest correlation in each column is given in bold.", "labels": [], "entities": [{"text": "correlation", "start_pos": 12, "end_pos": 23, "type": "METRIC", "confidence": 0.9967411160469055}]}], "tableCaptions": [{"text": " Table 1: Different combinations of \u03b1, \u03b2, \u03b3 (\u03b1 + \u03b2 +  \u03b3 =1) with ID that is horizontal axis in", "labels": [], "entities": [{"text": "ID", "start_pos": 65, "end_pos": 67, "type": "METRIC", "confidence": 0.9576135873794556}]}, {"text": " Table 2: Performances of the four models on 2012 train  data. The highest correlation in each column is given in  bold.", "labels": [], "entities": [{"text": "2012 train  data", "start_pos": 45, "end_pos": 61, "type": "DATASET", "confidence": 0.889681875705719}, {"text": "correlation", "start_pos": 75, "end_pos": 86, "type": "METRIC", "confidence": 0.9869937300682068}]}, {"text": " Table 3: Performances of our three models as well as  the baseline and UKP_run2 (that is ranked 1 in last STS  task) results on 2012 test data. The highest correlation in  each column is given in bold.", "labels": [], "entities": [{"text": "UKP_run2", "start_pos": 72, "end_pos": 80, "type": "DATASET", "confidence": 0.7513940731684366}, {"text": "2012 test data", "start_pos": 129, "end_pos": 143, "type": "DATASET", "confidence": 0.7631504933039347}, {"text": "correlation", "start_pos": 157, "end_pos": 168, "type": "METRIC", "confidence": 0.9635076522827148}]}]}