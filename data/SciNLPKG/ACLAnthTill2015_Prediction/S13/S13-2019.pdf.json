{"title": [{"text": "ClaC: Semantic Relatedness of Words and Phrases", "labels": [], "entities": [{"text": "Semantic Relatedness of Words and Phrases", "start_pos": 6, "end_pos": 47, "type": "TASK", "confidence": 0.8423113028208414}]}], "abstractContent": [{"text": "The measurement of phrasal semantic relat-edness is an important metric for many natural language processing applications.", "labels": [], "entities": [{"text": "phrasal semantic relat-edness", "start_pos": 19, "end_pos": 48, "type": "TASK", "confidence": 0.5705071489016215}]}, {"text": "In this paper, we present three approaches for measuring phrasal semantics, one based on a semantic network model, another on a distribu-tional similarity model, and a hybrid between the two.", "labels": [], "entities": []}, {"text": "Our hybrid approach achieved an F-measure of 77.4% on the task of evaluating the semantic similarity of words and compo-sitional phrases.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 32, "end_pos": 41, "type": "METRIC", "confidence": 0.9996917247772217}]}], "introductionContent": [{"text": "Phrasal semantic relatedness is a measurement of how multiword expressions are related in meaning.", "labels": [], "entities": [{"text": "Phrasal semantic relatedness", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.8088226517041525}]}, {"text": "Many natural language processing applications such as textual entailment, question answering, or information retrieval require a robust measurement of phrasal semantic relatedness.", "labels": [], "entities": [{"text": "textual entailment", "start_pos": 54, "end_pos": 72, "type": "TASK", "confidence": 0.7326120883226395}, {"text": "question answering", "start_pos": 74, "end_pos": 92, "type": "TASK", "confidence": 0.8829382359981537}, {"text": "information retrieval", "start_pos": 97, "end_pos": 118, "type": "TASK", "confidence": 0.7945620119571686}]}, {"text": "Current approaches to address this problem can be categorized into three main categories: those that rely on a knowledge base and its structure, those that use the distributional hypothesis on a large corpus, and hybrid approaches.", "labels": [], "entities": []}, {"text": "In this paper, we propose supervised approaches for comparing phrasal semantics that are based on a semantic network model, a distributional similarity model, and a hybrid between the two.", "labels": [], "entities": []}, {"text": "Those approaches have been evaluated on the task of semantic similarity of words and compositional phrases and on the task of evaluating the compositionality of phrases in context.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluated our approaches for word-phrase semantic relatedness on the SemEval task of evaluating phrasal semantics, and more specifically on the sub-task of evaluating the semantic similarity between words and phrases.", "labels": [], "entities": [{"text": "word-phrase semantic relatedness", "start_pos": 32, "end_pos": 64, "type": "TASK", "confidence": 0.7566699981689453}]}, {"text": "The task provided an English dataset of 15,628 word-phrases, 60% annotated for training and 40% for testing, with the goal of classifying each word-phrase as either positive or negative.", "labels": [], "entities": []}, {"text": "To transform the semantic relatedness measure to a semantic similarity classification one, we first calculated the semantic relatedness of each word-phrase in the training set, and used JRip, WEKA's () implementation of Cohen's RIPPER rule learning algorithm, in order to learn a set of rules that can differentiate between a positive semantic similarity and a negative one.", "labels": [], "entities": []}, {"text": "The classifier resulted in rules for the semantic network model based relatedness that could be summarized as follows: If the semantic relatedness of the word-phrase is over 61% then the similarity is positive, otherwise it is negative.", "labels": [], "entities": []}, {"text": "So for the example Interview -Formal meeting, which resulted in a semantic relatedness of 66.7% in the semantic network approach, it will be classified positively by the generated rule.", "labels": [], "entities": []}, {"text": "This method was our first submitted test run to this task, which resulted in a recall of 63.79%, a precision of 91.01%, and an F-measure of 75.00% on the testing set.", "labels": [], "entities": [{"text": "recall", "start_pos": 79, "end_pos": 85, "type": "METRIC", "confidence": 0.9930856227874756}, {"text": "precision", "start_pos": 99, "end_pos": 108, "type": "METRIC", "confidence": 0.9751158952713013}, {"text": "F-measure", "start_pos": 127, "end_pos": 136, "type": "METRIC", "confidence": 0.9994994401931763}]}, {"text": "For the second run, we trained the distributional similarity model using the same classifier.", "labels": [], "entities": []}, {"text": "This resulted with the following rule that could be summarized as follows: If the semantic relatedness of the word-phrase is over 40% then the similarity is positive, otherwise it is negative.", "labels": [], "entities": []}, {"text": "It was obvious from the training set that the semantic network model was more accurate than the distributional similarity model, but the distributional model had more coverage.", "labels": [], "entities": []}, {"text": "So for our second submitted test run, we used the semantic network approach as the main result, but used the distributional model as a backup approach if one of the words in the phrase was not available in WordNet, thus combining the precision and coverage of both approaches.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 206, "end_pos": 213, "type": "DATASET", "confidence": 0.9543076157569885}, {"text": "precision", "start_pos": 234, "end_pos": 243, "type": "METRIC", "confidence": 0.9991945624351501}]}, {"text": "This method resulted in a recall of 69.48%, a precision of 86.70%, and an F-measure of 77.14% on the testing set.", "labels": [], "entities": [{"text": "recall", "start_pos": 26, "end_pos": 32, "type": "METRIC", "confidence": 0.9973487854003906}, {"text": "precision", "start_pos": 46, "end_pos": 55, "type": "METRIC", "confidence": 0.9852112531661987}, {"text": "F-measure", "start_pos": 74, "end_pos": 83, "type": "METRIC", "confidence": 0.9994235038757324}]}, {"text": "For the last run, we used the same classifier but this time we training it using two features: the semantic network model relatedness measure (SN), and the distributional similarity model (DS).", "labels": [], "entities": [{"text": "relatedness measure (SN)", "start_pos": 122, "end_pos": 146, "type": "METRIC", "confidence": 0.7354699492454528}, {"text": "distributional similarity model (DS)", "start_pos": 156, "end_pos": 192, "type": "METRIC", "confidence": 0.7078931480646133}]}, {"text": "This training resulted in a set of rules that could be summarized as follows: if SN > 61% then the similarity is positive, else if DS > 40% then the similarity is also positive, and lastly if SN > 53% and DS > 31% then also in this case the similarity is positive, otherwise the similarity is negative.", "labels": [], "entities": [{"text": "similarity", "start_pos": 99, "end_pos": 109, "type": "METRIC", "confidence": 0.9797053933143616}]}, {"text": "This was our third submitted test run, which resulted a recall of 70.66%, a precision of 85.55%, and an F-measure of 77.39% on the testing set.", "labels": [], "entities": [{"text": "recall", "start_pos": 56, "end_pos": 62, "type": "METRIC", "confidence": 0.9967303276062012}, {"text": "precision", "start_pos": 76, "end_pos": 85, "type": "METRIC", "confidence": 0.9712803959846497}, {"text": "F-measure", "start_pos": 104, "end_pos": 113, "type": "METRIC", "confidence": 0.9990660548210144}]}], "tableCaptions": []}