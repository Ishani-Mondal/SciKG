{"title": [{"text": "SRIUBC-Core: Multiword Soft Similarity Models for Textual Similarity", "labels": [], "entities": [{"text": "SRIUBC-Core", "start_pos": 0, "end_pos": 11, "type": "DATASET", "confidence": 0.8757433295249939}]}], "abstractContent": [{"text": "In this year's Semantic Textual Similarity evaluation, we explore the contribution of models that provide soft similarity scores across spans of multiple words, over the previous year's system.", "labels": [], "entities": [{"text": "Semantic Textual Similarity evaluation", "start_pos": 15, "end_pos": 53, "type": "TASK", "confidence": 0.8097580075263977}]}, {"text": "To this end, we explored the use of neural probabilistic language models and a TF-IDF weighted variant of Explicit Semantic Analysis.", "labels": [], "entities": [{"text": "Explicit Semantic Analysis", "start_pos": 106, "end_pos": 132, "type": "TASK", "confidence": 0.5412213603655497}]}, {"text": "The neural language model systems used vector representations of individual words, where these vectors were derived by training them against the context of words encountered, and thus reflect the dis-tributional characteristics of their usage.", "labels": [], "entities": []}, {"text": "To generate a similarity score between spans, we experimented with using tiled vectors and Restricted Boltzmann Machines to identify similar encodings.", "labels": [], "entities": []}, {"text": "We find that these soft similarity methods generally outperformed our previous year's systems, albeit they did not perform as well in the overall rankings.", "labels": [], "entities": []}, {"text": "A simple analysis of the soft similarity resources over two word phrases is provided, and future areas of improvement are described.", "labels": [], "entities": []}], "introductionContent": [{"text": "For this year's Semantic Textual Similarity (STS) evaluation, we built upon the best performing system we deployed last year with several methods for exploring the soft similarity between windows of words, instead of relying just on single token-totoken similarities.", "labels": [], "entities": [{"text": "Semantic Textual Similarity (STS)", "start_pos": 16, "end_pos": 49, "type": "TASK", "confidence": 0.7771691928307215}]}, {"text": "From the previous year's evaluation, we were impressed by the performance of features derived from bigrams and skip bigrams.", "labels": [], "entities": []}, {"text": "Bigrams capture the relationship between two concurrent words, while skip bigrams can capture longer distance relationships.", "labels": [], "entities": []}, {"text": "We found that characterizing the overlap in skip bigrams between the sentences in a STS problem pair proved to be a major contributor to last year's system's performance.", "labels": [], "entities": []}, {"text": "Skip bigrams were matched on two criteria, lexical matches, and via part of speech (POS).", "labels": [], "entities": []}, {"text": "Lexical matching is brittle, and even if the match were made on lemmas, we lose the ability to match against synonyms.", "labels": [], "entities": [{"text": "Lexical matching", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.8911706507205963}]}, {"text": "We could rely on the token-to-token similarity methods to account for these non-lexical similarities, but these do not account for sequence nor dependencies in the sentencees.", "labels": [], "entities": []}, {"text": "Using POS based matching allows fora level of generalization, but at a much broader level.", "labels": [], "entities": [{"text": "POS based matching", "start_pos": 6, "end_pos": 24, "type": "TASK", "confidence": 0.5936718881130219}]}, {"text": "What we would like to have is a model that can capture these long distance relationships at a level that is less broad than POS matching, but allows fora soft similarity scoring between words.", "labels": [], "entities": []}, {"text": "In addition, the ability to encompass a larger window without having to manually insert skips would be desirable as well.", "labels": [], "entities": []}, {"text": "To this end we decided to explore the use of neural probabilistic language models (NLPM) for capturing this kind of behavior (.", "labels": [], "entities": []}, {"text": "NLPMs represent individual words as real valued vectors, often at a much lower dimensionality than the original vocabulary.", "labels": [], "entities": []}, {"text": "By training these representations to maximize a criterion such as loglikelihood of target word given the other words in its neighborhood, the word vectors themselves can capture commonalities between words that have been used in similar contexts.", "labels": [], "entities": []}, {"text": "In previous studies, these vectors themselves can capture distributionally derived similarities, by directly comparing the word vectors themselves using simple measures such as Euclidean distance.", "labels": [], "entities": []}, {"text": "In addition, we fielded a variant of Explicit Semantic Analysis () that used TF-IDF weightings, instead of using the raw concept vectors themselves.", "labels": [], "entities": [{"text": "Explicit Semantic Analysis", "start_pos": 37, "end_pos": 63, "type": "TASK", "confidence": 0.5167911549409231}]}, {"text": "From previous experiments, we found that using TF-IDF weightings on the words in a pair gave a boost in performance over sentence length comparisons and above, so this simple modification was incorporated into our system.", "labels": [], "entities": []}, {"text": "In order to identify the contribution of these soft similarity methods against last year's system, we fielded three systems: 1.", "labels": [], "entities": []}, {"text": "System 1, the system from the previous year, incorporating semantic similarity resources, precision focused and Bilingual Evaluation Understudy (BLEU) overlaps (), and several types of skip-bigrams.", "labels": [], "entities": [{"text": "precision focused and Bilingual Evaluation Understudy (BLEU) overlaps", "start_pos": 90, "end_pos": 159, "type": "METRIC", "confidence": 0.8312637329101562}]}, {"text": "2. System 2, features just the new NLPM scores and TFIDF-ESA.", "labels": [], "entities": [{"text": "TFIDF-ESA", "start_pos": 51, "end_pos": 60, "type": "METRIC", "confidence": 0.7233241200447083}]}], "datasetContent": [{"text": "System 2, the TFIDF-ESA score fora pair is a feature.", "labels": [], "entities": [{"text": "TFIDF-ESA score", "start_pos": 14, "end_pos": 29, "type": "METRIC", "confidence": 0.8548470735549927}]}, {"text": "For each of the given ngram sizes, we treated the ngram similarity scores from the Vector Window and RBM methods as individual features.", "labels": [], "entities": [{"text": "ngram similarity scores", "start_pos": 50, "end_pos": 73, "type": "METRIC", "confidence": 0.8119488954544067}]}, {"text": "System 3 combines the features from System 2 with those from System 1.", "labels": [], "entities": []}, {"text": "For Systems 2 and 3, the SVR setup used by System 1 was used to develop scorers.", "labels": [], "entities": []}, {"text": "As no training immediate training sets were provided for the evaluation sets, we used the train and test partitions given in, training on both the 2012 train and test data, where gold scores were available.", "labels": [], "entities": [{"text": "2012 train and test data", "start_pos": 147, "end_pos": 171, "type": "DATASET", "confidence": 0.8433609724044799}]}], "tableCaptions": [{"text": " Table 2: Pearson correlation of systems against the test datasets (top). The test set performance for the new Neural  Probabilistic Language Model (NLPM) and TFIDF-ESA components are given, along with a lexical-only variant for  comparison (bottom).", "labels": [], "entities": [{"text": "Pearson correlation", "start_pos": 10, "end_pos": 29, "type": "METRIC", "confidence": 0.8817464411258698}]}, {"text": " Table 3: Cosine similarity of two input strings, as given by the vectors generated from the Vector Window size 2, RBM  Window size 2, and TFIDF-ESA.", "labels": [], "entities": []}]}