{"title": [{"text": "KLUE: Simple and robust methods for polarity classification", "labels": [], "entities": [{"text": "KLUE", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.5378277897834778}, {"text": "polarity classification", "start_pos": 36, "end_pos": 59, "type": "TASK", "confidence": 0.7923313081264496}]}], "abstractContent": [{"text": "This paper describes our approach to the SemEval-2013 task on \"Sentiment Analysis in Twitter\".", "labels": [], "entities": [{"text": "SemEval-2013 task", "start_pos": 41, "end_pos": 58, "type": "TASK", "confidence": 0.9141020476818085}, {"text": "Sentiment Analysis in Twitter", "start_pos": 63, "end_pos": 92, "type": "TASK", "confidence": 0.9017707854509354}]}, {"text": "We use simple bag-of-words models , a freely available sentiment dictionary automatically extended with distributionally similar terms, as well as lists of emoticons and inter-net slang abbreviations in conjunction with fast and robust machine learning algorithms.", "labels": [], "entities": []}, {"text": "The resulting system is resource-lean, making it relatively independent of a specific language.", "labels": [], "entities": []}, {"text": "Despite its simplicity, the system achieves competitive accuracies of 0.70-0.72 in detecting the sentiment of text messages.", "labels": [], "entities": [{"text": "detecting the sentiment of text messages", "start_pos": 83, "end_pos": 123, "type": "TASK", "confidence": 0.8178705076376597}]}, {"text": "We also apply our approach to the task of detecting the context-dependent sentiment of individual words and phrases within a message.", "labels": [], "entities": [{"text": "detecting the context-dependent sentiment of individual words and phrases within a message", "start_pos": 42, "end_pos": 132, "type": "TASK", "confidence": 0.7622276892264684}]}], "introductionContent": [{"text": "The SemEval-2013 task on \"Sentiment Analysis in Twitter\" () focuses on polarity classification, i. e. the problem of determining whether a textual unit, e. g. a document, paragraph, sentence or phrase, expresses a positive, negative or neutral sentiment (for a review of research topics and recent developments in the field of sentiment analysis see).", "labels": [], "entities": [{"text": "Sentiment Analysis in Twitter\"", "start_pos": 26, "end_pos": 56, "type": "TASK", "confidence": 0.8713975548744202}, {"text": "polarity classification", "start_pos": 71, "end_pos": 94, "type": "TASK", "confidence": 0.7675184011459351}, {"text": "sentiment analysis", "start_pos": 327, "end_pos": 345, "type": "TASK", "confidence": 0.7731595635414124}]}, {"text": "There are two subtasks: in task B, \"Message Polarity Classification\", whole messages have to be classified as being of positive, negative or neutral sentiment; in task A, \"Contextual Polarity Disambiguation\", a marked instance of a word or phrase has to be classified in the context of a whole message.", "labels": [], "entities": [{"text": "Message Polarity Classification\"", "start_pos": 36, "end_pos": 68, "type": "TASK", "confidence": 0.7311544790863991}, {"text": "Contextual Polarity Disambiguation", "start_pos": 172, "end_pos": 206, "type": "TASK", "confidence": 0.6624671320120493}]}, {"text": "The training data for task B consist of approximately 10 200 manually annotated Twitter messages, the training data for task A of approximately 9 500 marked instances in approximately 6 300 Twitter messages.", "labels": [], "entities": []}, {"text": "The test data consist of in-domain Twitter messages (3 813 messages for task B and 4 435 marked instances in 2 826 messages for task A) and out-of-domain SMS text messages (2 094 messages for task B, 2 334 marked instances in 1 437 messages for task A).", "labels": [], "entities": []}, {"text": "The distribution of messages and marked instances over sentiment categories in the training and test sets is shown in The main focus of the current paper lies on experimenting with resource-lean and robust methods for task B, the classification of whole messages.", "labels": [], "entities": [{"text": "classification of whole messages", "start_pos": 230, "end_pos": 262, "type": "TASK", "confidence": 0.8427280634641647}]}, {"text": "We do, however, apply our approach also to task A.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section we evaluate different classifiers (multinomial Naive Bayes, Linear SVM 8 and Maximum Entropy 9 ) and various combinations of features on the gold test sets.", "labels": [], "entities": [{"text": "gold test sets", "start_pos": 157, "end_pos": 171, "type": "DATASET", "confidence": 0.7864437301953634}]}, {"text": "We vary the bag-of-words model (bow), the use of AFINN (sent), our extensions to the sentiment dictionary (ext) and the list of emotion markers (emo).", "labels": [], "entities": [{"text": "AFINN", "start_pos": 49, "end_pos": 54, "type": "METRIC", "confidence": 0.9957226514816284}]}, {"text": "To present as clear a picture of the classifiers' performances as possible, we report Fscores for each of the three classes, the weighted average of all three F-scores (F w ), the (unweighted) average of the positive and negative F-scores (F pos+neg ; this is the value shown in the official task results and used for ranking systems), as well as accuracy.", "labels": [], "entities": [{"text": "Fscores", "start_pos": 86, "end_pos": 93, "type": "METRIC", "confidence": 0.9973406195640564}, {"text": "F-scores (F w )", "start_pos": 159, "end_pos": 174, "type": "METRIC", "confidence": 0.9269940257072449}, {"text": "accuracy", "start_pos": 347, "end_pos": 355, "type": "METRIC", "confidence": 0.9993683695793152}]}, {"text": "Results for submitted systems are typeset in italics, the best results in each column are typeset in bold font.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: The data sets for both tasks", "labels": [], "entities": []}, {"text": " Table 2: Some weak baselines for task B, Twitter test set", "labels": [], "entities": [{"text": "Twitter test set", "start_pos": 42, "end_pos": 58, "type": "DATASET", "confidence": 0.9193008740743002}]}, {"text": " Table 3: Evaluation results for task B on the Twitter test set", "labels": [], "entities": [{"text": "Twitter test set", "start_pos": 47, "end_pos": 63, "type": "DATASET", "confidence": 0.9637675086657206}]}, {"text": " Table 4: Evaluation results for task B on the SMS test set", "labels": [], "entities": [{"text": "SMS test set", "start_pos": 47, "end_pos": 59, "type": "DATASET", "confidence": 0.8635470867156982}]}, {"text": " Table 5: Evaluation results for task A on the Twitter test set", "labels": [], "entities": [{"text": "Twitter test set", "start_pos": 47, "end_pos": 63, "type": "DATASET", "confidence": 0.9582668344179789}]}, {"text": " Table 6: Evaluation results for task A on the SMS test set", "labels": [], "entities": [{"text": "SMS test set", "start_pos": 47, "end_pos": 59, "type": "DATASET", "confidence": 0.8675438165664673}]}, {"text": " Table 7: Task B, confusion matrix for tweets/SMS", "labels": [], "entities": []}, {"text": " Table 8: Task A, confusion matrix for tweets/SMS", "labels": [], "entities": []}]}