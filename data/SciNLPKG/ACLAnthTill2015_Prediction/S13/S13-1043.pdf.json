{"title": [{"text": "Automatically Identifying Implicit Arguments to Improve Argument Linking and Coherence Modeling", "labels": [], "entities": [{"text": "Improve Argument Linking", "start_pos": 48, "end_pos": 72, "type": "TASK", "confidence": 0.65644042690595}]}], "abstractContent": [{"text": "Implicit arguments area discourse-level phenomenon that has not been extensively studied in semantic processing.", "labels": [], "entities": []}, {"text": "One reason for this lies in the scarce amount of annotated data sets available.", "labels": [], "entities": []}, {"text": "We argue that more data of this kind would be helpful to improve existing approaches to linking implicit arguments in discourse and to enable more in-depth studies of the phenomenon itself.", "labels": [], "entities": []}, {"text": "In this paper, we present a range of studies that empirically validate this claim.", "labels": [], "entities": []}, {"text": "Our contributions are threefold: we present a heuristic approach to automatically identify implicit arguments and their antecedents by exploiting comparable texts; we show how the induced data can be used as training data for improving existing argument linking models; finally, we present a novel approach to modeling local coherence that extends previous approaches by taking into account non-explicit entity references.", "labels": [], "entities": []}], "introductionContent": [{"text": "Semantic role labeling systems traditionally process text in a sentence-by-sentence fashion, constructing local structures of semantic meaning . Information relevant to these structures, however, can be non-local in natural language texts.", "labels": [], "entities": [{"text": "Semantic role labeling", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.6963087320327759}]}, {"text": "In this paper, we view instances of this phenomenon, also referred to as implicit arguments, as elements of discourse.", "labels": [], "entities": []}, {"text": "Ina coherent discourse, each utterance focuses on a salient set of entities, also called \"foci\" or \"centers\".", "labels": [], "entities": []}, {"text": "According to the theory of, the salience of an entity in a discourse is reflected by linguistic factors such as choice of referring expression and syntactic form.", "labels": [], "entities": []}, {"text": "Both extremes of salience, i.e., contexts of referential continuity and irrelevance, can also be reflected by the non-realization of an entity.", "labels": [], "entities": []}, {"text": "Altough specific instances of non-realization, so-called zero anaphora, have been well-studied in discourse analysis (, inter alia), this phenomenon has widely been ignored in computational approaches to entitybased coherence modeling.", "labels": [], "entities": [{"text": "discourse analysis", "start_pos": 98, "end_pos": 116, "type": "TASK", "confidence": 0.7243736237287521}, {"text": "entitybased coherence modeling", "start_pos": 204, "end_pos": 234, "type": "TASK", "confidence": 0.6162048280239105}]}, {"text": "It could, however, provide an explanation for local coherence in cases that are not covered by current models of Centering (cf.).", "labels": [], "entities": []}, {"text": "In this work, we propose anew model to predict whether realizing an argument contributes to local coherence in a given position in discourse.", "labels": [], "entities": []}, {"text": "Example (1) shows a text fragment, in which argument realization is necessary in the first sentence but redundant in the second.", "labels": [], "entities": [{"text": "argument realization", "start_pos": 44, "end_pos": 64, "type": "TASK", "confidence": 0.7441689372062683}]}, {"text": "(1) El Salvador is now the only Latin American country which still has troops in.", "labels": [], "entities": []}, {"text": "Nicaragua, Honduras and the Dominican Republic have withdrawn their troops.", "labels": [], "entities": []}, {"text": "From a semantic processing perspective, a human reader can easily infer that \"Iraq\", the marked entity in the first sentence of Example (1), is also an implicit argument of the predicate \"withdraw\" in the second sentence.", "labels": [], "entities": []}, {"text": "This inference step is, however, difficult to model computationally as it involves an interplay of two challenging sub-tasks: first, a semantic processor has to determine that an argument is not realized (but inferrable); and second, a suit-able antecedent has to be found within the discourse context.", "labels": [], "entities": []}, {"text": "For the remainder of this paper, we refer to these steps as identifying and linking implicit arguments to discourse antecedents.", "labels": [], "entities": []}, {"text": "As indicated by Example (1), implicit arguments are an important aspect in semantic processing, yet they are not captured in traditional semantic role labeling systems.", "labels": [], "entities": [{"text": "semantic processing", "start_pos": 75, "end_pos": 94, "type": "TASK", "confidence": 0.8353675603866577}, {"text": "semantic role labeling", "start_pos": 137, "end_pos": 159, "type": "TASK", "confidence": 0.6933254798253378}]}, {"text": "The main reasons for this are the scarcity of annotated data, and the inherent difficulty of inferring discourse antecedents automatically.", "labels": [], "entities": []}, {"text": "In this paper, we propose to induce implicit arguments and discourse antecedents by exploiting complementary (explicit) information obtained from monolingual comparable texts (Section 3).", "labels": [], "entities": []}, {"text": "We apply the empirically acquired data in argument linking (Section 4) and coherence modeling (Section 5).", "labels": [], "entities": [{"text": "argument linking", "start_pos": 42, "end_pos": 58, "type": "TASK", "confidence": 0.7696622908115387}]}, {"text": "We conclude with a discussion on the advantages of our data set and outline directions for future work (Section 6).", "labels": [], "entities": []}], "datasetContent": [{"text": "Our first experiment assesses the utility of automatically induced implicit arguments and antecedent pairs for the task of implicit argument linking.", "labels": [], "entities": [{"text": "implicit argument linking", "start_pos": 123, "end_pos": 148, "type": "TASK", "confidence": 0.6830657124519348}]}, {"text": "For evaluation, we use the data sets from the SemEval 2010 task on Linking Events and their Participants in Discourse (, henceforth just SemEval).", "labels": [], "entities": [{"text": "SemEval 2010 task", "start_pos": 46, "end_pos": 63, "type": "TASK", "confidence": 0.6403194069862366}, {"text": "Linking Events and their Participants in Discourse", "start_pos": 67, "end_pos": 117, "type": "TASK", "confidence": 0.7898984466280256}]}, {"text": "For direct comparison with previous results and heuristic acquisition techniques (cf. Section 2), we apply the implicit argument identification and linking model by Silberer and Frank (2012, henceforth S&F) for training and testing.", "labels": [], "entities": [{"text": "heuristic acquisition", "start_pos": 48, "end_pos": 69, "type": "TASK", "confidence": 0.6913163363933563}, {"text": "argument identification and linking", "start_pos": 120, "end_pos": 155, "type": "TASK", "confidence": 0.7668875455856323}]}, {"text": "In our second experiment, we examine the effect of implicit arguments on local coherence, i.e., the question of how well a local argument (non-)realization fits into a given context.", "labels": [], "entities": []}, {"text": "We approach this question as follows: first, we assemble a data set of document pairs that differ only with respect to a single realization decision (Section 5.1).", "labels": [], "entities": []}, {"text": "Given each pair in this data set, we ask human annotators to indicate their preference for the implicit or explicit argument realization in the pre-specified context (Section 5.2).", "labels": [], "entities": []}, {"text": "Second, we attempt to emulate the decision process computationally using a discriminative model based on discourse and entity-specific features (Section 5.3).", "labels": [], "entities": []}, {"text": "The goal of this task is to correctly predict the realization type (implicit or explicit) of an argument that maximizes the coherence of the document.", "labels": [], "entities": []}, {"text": "As a proxy for coherence, we use the naturalness ratings given by our annotators.", "labels": [], "entities": []}, {"text": "We evaluate classification performance on the part of our test set for which clear preferences have been established.", "labels": [], "entities": []}, {"text": "We report results in terms of precision, recall and F 1 score.", "labels": [], "entities": [{"text": "precision", "start_pos": 30, "end_pos": 39, "type": "METRIC", "confidence": 0.999797523021698}, {"text": "recall", "start_pos": 41, "end_pos": 47, "type": "METRIC", "confidence": 0.9997819066047668}, {"text": "F 1 score", "start_pos": 52, "end_pos": 61, "type": "METRIC", "confidence": 0.990977942943573}]}, {"text": "We compute precision as the fraction of correct classifier decisions divided by the total number of classifications; and recall as the fraction of correct classifier decisions divided by the total number of test items.", "labels": [], "entities": [{"text": "precision", "start_pos": 11, "end_pos": 20, "type": "METRIC", "confidence": 0.9990960359573364}, {"text": "recall", "start_pos": 121, "end_pos": 127, "type": "METRIC", "confidence": 0.9995720982551575}]}, {"text": "Note that precision and recall are identical when the model provides a class label for every test item.", "labels": [], "entities": [{"text": "precision", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9995967745780945}, {"text": "recall", "start_pos": 24, "end_pos": 30, "type": "METRIC", "confidence": 0.9995585083961487}]}, {"text": "We compute F 1 as the harmonic mean between precision and recall.", "labels": [], "entities": [{"text": "F 1", "start_pos": 11, "end_pos": 14, "type": "METRIC", "confidence": 0.9888809025287628}, {"text": "precision", "start_pos": 44, "end_pos": 53, "type": "METRIC", "confidence": 0.9986369013786316}, {"text": "recall", "start_pos": 58, "end_pos": 64, "type": "METRIC", "confidence": 0.9899214506149292}]}, {"text": "For comparison with previous work, we further apply a couple of previously proposed local coherence models: the original entity grid model by, a modified version that uses topic models) and an extended version that includes entity-specific features).", "labels": [], "entities": []}, {"text": "We further apply the discourse-new model by and the pronoun-based model by.", "labels": [], "entities": []}, {"text": "For all of the aforementioned models, we use their respective implementation provided with the Brown Coherence Toolkit . Note that the toolkit only returns one coherence score for each document.", "labels": [], "entities": []}, {"text": "To use the toolkit for argument classification, we use two documents per data pointone that contains the affected argument explicitly and one that does not (implicit argument) -and treat the higher scoring variant as classification output.", "labels": [], "entities": [{"text": "argument classification", "start_pos": 23, "end_pos": 46, "type": "TASK", "confidence": 0.7422114610671997}]}, {"text": "If both documents achieve the same score, we neither count the test item as correctly nor as incorrectly classified.", "labels": [], "entities": []}, {"text": "In contrast, we apply our own model only on the document that contains the implicit argument, and use the classifier to predict whether this realization type fits into the given context or not.", "labels": [], "entities": []}, {"text": "Note that our model has an advantage here because it is specifically designed for this task.", "labels": [], "entities": []}, {"text": "Yet, all models compute local coherence ratings based on entity occurrences and should thus be able to predict which realization type coheres best with the given discourse context.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Examples of erroneous pairs of implicit arguments and antecedents. In (1), the parser did not recognize  \"Statistics\" as an argument of showed; in (2), the parser mislabeled \"French\" as a locative modifier; both errors lead  to incorrectly identified implicit arguments. In (3), the implicit argument is correct but the wrong antecedent was  identified because \"major\" had been mislabeled in the aligned predicate-argument structure", "labels": [], "entities": []}, {"text": " Table 3: Results in terms of precision (P), recall (R) and  F 1 score (F) for identifying and linking implicit argu- ments in the SemEval test set.", "labels": [], "entities": [{"text": "precision (P)", "start_pos": 30, "end_pos": 43, "type": "METRIC", "confidence": 0.9465615451335907}, {"text": "recall (R)", "start_pos": 45, "end_pos": 55, "type": "METRIC", "confidence": 0.9533393234014511}, {"text": "F 1 score (F)", "start_pos": 61, "end_pos": 74, "type": "METRIC", "confidence": 0.971344659725825}, {"text": "SemEval test set", "start_pos": 131, "end_pos": 147, "type": "DATASET", "confidence": 0.8000595569610596}]}, {"text": " Table 4: Results in terms of precision (P), recall (R) and  F 1 score for correctly predicting argument realization; re- sults that significantly differ from our (full) model are  marked with asterisks (* p<0.1; ** p<0.01)", "labels": [], "entities": [{"text": "precision (P)", "start_pos": 30, "end_pos": 43, "type": "METRIC", "confidence": 0.9274260252714157}, {"text": "recall (R)", "start_pos": 45, "end_pos": 55, "type": "METRIC", "confidence": 0.9360359907150269}, {"text": "F 1 score", "start_pos": 61, "end_pos": 70, "type": "METRIC", "confidence": 0.9871072967847189}, {"text": "predicting argument realization", "start_pos": 85, "end_pos": 116, "type": "TASK", "confidence": 0.8727773825327555}]}]}