{"title": [{"text": "NavyTime: Event and Time Ordering from Raw Text", "labels": [], "entities": [{"text": "NavyTime", "start_pos": 0, "end_pos": 8, "type": "DATASET", "confidence": 0.7039347290992737}, {"text": "Event and Time Ordering", "start_pos": 10, "end_pos": 33, "type": "TASK", "confidence": 0.5203662067651749}]}], "abstractContent": [{"text": "This paper describes a complete event/time ordering system that annotates raw text with events, times, and the ordering relations between them at the SemEval-2013 Task 1.", "labels": [], "entities": [{"text": "SemEval-2013 Task 1", "start_pos": 150, "end_pos": 169, "type": "TASK", "confidence": 0.596527894337972}]}, {"text": "Task 1 is a unique challenge because it starts from raw text, rather than pre-annotated text with known events and times.", "labels": [], "entities": []}, {"text": "A working system first identifies events and times, then identifies which events and times should be ordered, and finally labels the ordering relation between them.", "labels": [], "entities": []}, {"text": "We present a split classifier approach that breaks the ordering tasks into smaller decision points.", "labels": [], "entities": []}, {"text": "Experiments show that more specialized classifiers perform better than few joint classifiers.", "labels": [], "entities": []}, {"text": "The NavyTime system ranked second both overall and inmost subtasks like event extraction and relation labeling.", "labels": [], "entities": [{"text": "NavyTime", "start_pos": 4, "end_pos": 12, "type": "DATASET", "confidence": 0.8615546822547913}, {"text": "event extraction", "start_pos": 72, "end_pos": 88, "type": "TASK", "confidence": 0.7860517799854279}, {"text": "relation labeling", "start_pos": 93, "end_pos": 110, "type": "TASK", "confidence": 0.851978987455368}]}], "introductionContent": [{"text": "The SemEval-2013 Task 1 (TempEval-3) contest is the third instantiation of an event ordering challenge.", "labels": [], "entities": [{"text": "SemEval-2013 Task 1 (TempEval-3) contest", "start_pos": 4, "end_pos": 44, "type": "TASK", "confidence": 0.7440694059644427}]}, {"text": "However, it is the first to start from raw text with the challenge to create an end-to-end algorithm for event ordering.", "labels": [], "entities": [{"text": "event ordering", "start_pos": 105, "end_pos": 119, "type": "TASK", "confidence": 0.7457402646541595}]}, {"text": "Previous challenges included the individual aspects of such a system, including event extraction, timex extraction, and event/time ordering.", "labels": [], "entities": [{"text": "event extraction", "start_pos": 80, "end_pos": 96, "type": "TASK", "confidence": 0.7343321144580841}, {"text": "timex extraction", "start_pos": 98, "end_pos": 114, "type": "TASK", "confidence": 0.7790426909923553}, {"text": "event/time ordering", "start_pos": 120, "end_pos": 139, "type": "TASK", "confidence": 0.5756603702902794}]}, {"text": "However, neither task was dependent on the other.", "labels": [], "entities": []}, {"text": "This paper presents NavyTime, a system inspired partly by this previous breakup of the tasks.", "labels": [], "entities": [{"text": "NavyTime", "start_pos": 20, "end_pos": 28, "type": "DATASET", "confidence": 0.9461445212364197}]}, {"text": "We focus on breaking up the event/time ordering task further, and show that 5 classifiers yield better performance than the traditional 3 (or even 1).", "labels": [], "entities": [{"text": "event/time ordering task", "start_pos": 28, "end_pos": 52, "type": "TASK", "confidence": 0.6978727817535401}]}, {"text": "The first required steps to annotate a document are to extract its events and time expressions.", "labels": [], "entities": []}, {"text": "This paper describes anew event extractor with a rich set of contextual features that is atop performer for event attributes at Tempeval-3.", "labels": [], "entities": [{"text": "event extractor", "start_pos": 26, "end_pos": 41, "type": "TASK", "confidence": 0.7202571928501129}]}, {"text": "We then explore additions to SUTime, atop rule-based extractor for time.", "labels": [], "entities": []}, {"text": "However, the core challenge is to link these extracted events and times together.", "labels": [], "entities": []}, {"text": "We describe new models for these difficult tasks: (1) identifying ordered pairs, and (2) labeling the ordering relations.", "labels": [], "entities": [{"text": "labeling the ordering relations", "start_pos": 89, "end_pos": 120, "type": "TASK", "confidence": 0.8082232475280762}]}, {"text": "Relation identification is rarely addressed in the literature.", "labels": [], "entities": [{"text": "Relation identification", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.9862227141857147}]}, {"text": "Given a set of events, which pairs of events are temporally related?", "labels": [], "entities": []}, {"text": "Almost all previous work assumes we are given the pairs, and the task is to label the relation (before, after, etc.).", "labels": [], "entities": []}, {"text": "Raw text presents anew challenge: extract the relevant pairs before labeling them.", "labels": [], "entities": []}, {"text": "We present some of the first results that compare rule-based approaches to trained probabilistic classifiers.", "labels": [], "entities": []}, {"text": "These are the first such comparisons to our knowledge.", "labels": [], "entities": []}, {"text": "Finally, after relation identification, we label relations between the pairs.", "labels": [], "entities": [{"text": "relation identification", "start_pos": 15, "end_pos": 38, "type": "TASK", "confidence": 0.7916765511035919}]}, {"text": "This is the traditional event ordering task, although we now start from noisy pairs.", "labels": [], "entities": [{"text": "event ordering task", "start_pos": 24, "end_pos": 43, "type": "TASK", "confidence": 0.7927614947160085}]}, {"text": "Our main contribution is to build independent classifiers for intra-sentence event/time pairs.", "labels": [], "entities": []}, {"text": "We show improved performance when training these split classifiers.", "labels": [], "entities": []}, {"text": "NavyTime's approach is highly competitive, achieving 2nd place in relation labeling (and overall).", "labels": [], "entities": [{"text": "NavyTime", "start_pos": 0, "end_pos": 8, "type": "DATASET", "confidence": 0.9640520215034485}, {"text": "relation labeling", "start_pos": 66, "end_pos": 83, "type": "TASK", "confidence": 0.8785421252250671}]}], "datasetContent": [{"text": "All models are developed on the TimeBank (Pustejovsky et al., 2003) and AQUAINT corpora.", "labels": [], "entities": [{"text": "TimeBank", "start_pos": 32, "end_pos": 40, "type": "DATASET", "confidence": 0.9455644488334656}, {"text": "AQUAINT corpora", "start_pos": 72, "end_pos": 87, "type": "DATASET", "confidence": 0.7624166011810303}]}, {"text": "These labeled newspaper articles have fueled many years of event ordering research.", "labels": [], "entities": [{"text": "event ordering", "start_pos": 59, "end_pos": 73, "type": "TASK", "confidence": 0.8225008845329285}]}, {"text": "TimeBank includes 183 documents and AQUAINT includes 73.", "labels": [], "entities": [{"text": "TimeBank", "start_pos": 0, "end_pos": 8, "type": "DATASET", "confidence": 0.9756308794021606}, {"text": "AQUAINT", "start_pos": 36, "end_pos": 43, "type": "DATASET", "confidence": 0.48767155408859253}]}, {"text": "The annotators of each were given different guidance, so they provide unique distributions of relations.", "labels": [], "entities": []}, {"text": "Development of the algorithms in this paper were solely on 10-fold cross validation on the union of the two corpora.", "labels": [], "entities": []}, {"text": "The SemEval-2013 Task 1 (TempEval-3) provides unseen raw text to then evaluate the final systems.", "labels": [], "entities": []}, {"text": "Final results are from this set of unseen newspaper articles.", "labels": [], "entities": []}, {"text": "They were annotated by a different set of people who annotated TimeBank and AQUAINT.", "labels": [], "entities": [{"text": "TimeBank", "start_pos": 63, "end_pos": 71, "type": "DATASET", "confidence": 0.9604451656341553}, {"text": "AQUAINT", "start_pos": 76, "end_pos": 83, "type": "DATASET", "confidence": 0.7234797477722168}]}, {"text": "All models were created by using 10-fold cross validation on TimeBank+AQUAINT.", "labels": [], "entities": [{"text": "TimeBank+AQUAINT", "start_pos": 61, "end_pos": 77, "type": "DATASET", "confidence": 0.7589310606320699}]}, {"text": "The best model was then trained on the entire set.", "labels": [], "entities": []}, {"text": "Features seen only once were trimmed from training.", "labels": [], "entities": []}, {"text": "The relation labeling confidence threshold was set to 0.3.", "labels": [], "entities": [{"text": "relation labeling confidence threshold", "start_pos": 4, "end_pos": 42, "type": "TASK", "confidence": 0.7919966727495193}]}, {"text": "Final results are reported on the held out test set provided by.", "labels": [], "entities": []}, {"text": "Our first experiments focus on relation labeling.", "labels": [], "entities": [{"text": "relation labeling", "start_pos": 31, "end_pos": 48, "type": "TASK", "confidence": 0.8889475762844086}]}, {"text": "This is a simpler task than identification in that we start with known pairs of entities, and the task is to assign a label to them (Task C-relation at SemEval-2013 Task 1).", "labels": [], "entities": []}, {"text": "Our system initially ranked second with 46.83.", "labels": [], "entities": []}, {"text": "The next task is both relation identification and relation labeling combined (Task C).", "labels": [], "entities": [{"text": "relation identification", "start_pos": 22, "end_pos": 45, "type": "TASK", "confidence": 0.9300332367420197}, {"text": "relation labeling", "start_pos": 50, "end_pos": 67, "type": "TASK", "confidence": 0.8110513389110565}]}, {"text": "This is unfortunately a task that is difficult to define.", "labels": [], "entities": []}, {"text": "Without a completely labeled graph of events and times, it is not about true extraction, but matching human labeling decisions that were constrained by time and effort.", "labels": [], "entities": [{"text": "true extraction", "start_pos": 72, "end_pos": 87, "type": "TASK", "confidence": 0.6903090626001358}]}, {"text": "We experimented with rule-based vs datadriven extractors.", "labels": [], "entities": []}, {"text": "We held our relation labeling model constant, and swapped different identification models in and out.", "labels": [], "entities": []}, {"text": "Our best configuration was evaluated on test.", "labels": [], "entities": []}, {"text": "Results are shown in   stages of this paper, starting from event and temporal extraction, then applying relation ID and labeling.", "labels": [], "entities": [{"text": "event and temporal extraction", "start_pos": 59, "end_pos": 88, "type": "TASK", "confidence": 0.6442522630095482}, {"text": "relation ID", "start_pos": 104, "end_pos": 115, "type": "TASK", "confidence": 0.6798024922609329}]}, {"text": "Our system ranked 2nd of 4 systems.", "labels": [], "entities": []}, {"text": "Our best performing setup uses trained classifiers for relation identification of event-event and event-DCT links, but deterministic rules for eventtime links (Sec 5.1).", "labels": [], "entities": [{"text": "relation identification", "start_pos": 55, "end_pos": 78, "type": "TASK", "confidence": 0.7565822005271912}]}, {"text": "It then uses trained classifiers for relation labeling of all pair types.", "labels": [], "entities": [{"text": "relation labeling", "start_pos": 37, "end_pos": 54, "type": "TASK", "confidence": 0.8277426958084106}]}, {"text": "Training with TimeBank+AQUAINT outperformed just TimeBank.", "labels": [], "entities": [{"text": "TimeBank", "start_pos": 14, "end_pos": 22, "type": "DATASET", "confidence": 0.9107029438018799}, {"text": "AQUAINT", "start_pos": 23, "end_pos": 30, "type": "METRIC", "confidence": 0.8863681554794312}, {"text": "TimeBank", "start_pos": 49, "end_pos": 57, "type": "DATASET", "confidence": 0.9817941188812256}]}, {"text": "The split classifier approach for intra and inter-sentence event-event relations also outperformed a single event-event classifier.", "labels": [], "entities": []}, {"text": "We cannot give more specific results due to space constraints.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Task Crel, F1 scores of relation labeling.", "labels": [], "entities": [{"text": "F1", "start_pos": 21, "end_pos": 23, "type": "METRIC", "confidence": 0.9985772371292114}, {"text": "relation labeling", "start_pos": 34, "end_pos": 51, "type": "TASK", "confidence": 0.7568967342376709}]}, {"text": " Table 2. Navy- Time is the third best performer.  Finally, the full task from raw text requires all", "labels": [], "entities": [{"text": "Navy- Time", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.5657854676246643}]}, {"text": " Table 2: Task C, F1 scores of relation ID and labeling.", "labels": [], "entities": [{"text": "F1", "start_pos": 18, "end_pos": 20, "type": "METRIC", "confidence": 0.9990233182907104}, {"text": "relation ID", "start_pos": 31, "end_pos": 42, "type": "TASK", "confidence": 0.7451139688491821}]}, {"text": " Table 3: Task ABC, Extraction and labeling raw text.", "labels": [], "entities": [{"text": "labeling raw text", "start_pos": 35, "end_pos": 52, "type": "TASK", "confidence": 0.8951828281084696}]}]}