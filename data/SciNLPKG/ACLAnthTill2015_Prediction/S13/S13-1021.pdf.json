{"title": [{"text": "MayoClinicNLP-CORE: Semantic representations for textual similarity", "labels": [], "entities": [{"text": "MayoClinicNLP-CORE", "start_pos": 0, "end_pos": 18, "type": "DATASET", "confidence": 0.9666705131530762}, {"text": "textual similarity", "start_pos": 49, "end_pos": 67, "type": "TASK", "confidence": 0.7226389795541763}]}], "abstractContent": [{"text": "The Semantic Textual Similarity (STS) task examines semantic similarity at a sentence-level.", "labels": [], "entities": [{"text": "Semantic Textual Similarity (STS) task", "start_pos": 4, "end_pos": 42, "type": "TASK", "confidence": 0.8007784145218986}]}, {"text": "We explored three representations of semantics (implicit or explicit): named entities , semantic vectors, and structured vectorial semantics.", "labels": [], "entities": []}, {"text": "From a DKPro baseline, we also performed feature selection and used source-specific linear regression models to combine our features.", "labels": [], "entities": [{"text": "feature selection", "start_pos": 41, "end_pos": 58, "type": "TASK", "confidence": 0.6677225530147552}]}, {"text": "Our systems placed 5th, 6th, and 8th among 90 submitted systems.", "labels": [], "entities": []}], "introductionContent": [{"text": "The Semantic Textual Similarity (STS) task examines semantic similarity at a sentence-level.", "labels": [], "entities": [{"text": "Semantic Textual Similarity (STS) task", "start_pos": 4, "end_pos": 42, "type": "TASK", "confidence": 0.8007782825401851}]}, {"text": "While much work has compared the semantics of terms, concepts, or documents, this space has been relatively unexplored.", "labels": [], "entities": []}, {"text": "The 2013 STS task provided sentence pairs and a 0-5 human rating of their similarity, with training data from 5 sources and test data from 4 sources.", "labels": [], "entities": [{"text": "2013 STS task", "start_pos": 4, "end_pos": 17, "type": "DATASET", "confidence": 0.7603703141212463}]}, {"text": "We sought to explore and evaluate the usefulness of several semantic representations that have had recent significance in research or practice.", "labels": [], "entities": []}, {"text": "First, information extraction (IE) methods often implicitly consider named entities as ad hoc semantic representations, for example, in the clinical domain.", "labels": [], "entities": [{"text": "information extraction (IE)", "start_pos": 7, "end_pos": 34, "type": "TASK", "confidence": 0.8710263729095459}]}, {"text": "Therefore, we sought to evaluate similarity based on named entity-based features.", "labels": [], "entities": []}, {"text": "Second, in many applications, an effective means of incorporating distributional semantics is Random Indexing (RI).", "labels": [], "entities": [{"text": "Random Indexing (RI)", "start_pos": 94, "end_pos": 114, "type": "TASK", "confidence": 0.7072940349578858}]}, {"text": "Thus we consider three different representations possible within Random Indexing ().", "labels": [], "entities": []}, {"text": "Finally, because compositional distributional semantics is an important research topic (, we sought to evaluate a principled composition strategy: structured vectorial semantics (.", "labels": [], "entities": []}, {"text": "The remainder of this paper proceeds as follows.", "labels": [], "entities": []}, {"text": "Section 2 overviews our similarity metrics, and Section 3 overviews the systems that were defined on these metrics.", "labels": [], "entities": []}, {"text": "Competition results and additional analyses are in Section 4.", "labels": [], "entities": []}, {"text": "We end with discussion on the results in Section 5.", "labels": [], "entities": []}], "datasetContent": [{"text": "The MayoClinicNLP team submitted three systems to the STS-Core task.", "labels": [], "entities": [{"text": "MayoClinicNLP", "start_pos": 4, "end_pos": 17, "type": "DATASET", "confidence": 0.9771139025688171}]}, {"text": "We also include here a posthoc run that was considered as a possible submission.", "labels": [], "entities": []}, {"text": "r1wtCDT This run used the 47 metrics from DKPro, TakeLab, and MayoClinicNLP as a feature pool for feature selection.", "labels": [], "entities": [{"text": "DKPro", "start_pos": 42, "end_pos": 47, "type": "DATASET", "confidence": 0.968004047870636}, {"text": "TakeLab", "start_pos": 49, "end_pos": 56, "type": "DATASET", "confidence": 0.8444721698760986}, {"text": "MayoClinicNLP", "start_pos": 62, "end_pos": 75, "type": "DATASET", "confidence": 0.9826703667640686}]}, {"text": "Sourcespecific similarity metrics were combined with classifier-confidence-score weights.", "labels": [], "entities": []}, {"text": "r2CDT Same feature pool as run 1.", "labels": [], "entities": []}, {"text": "Best-match (as determined by classifier) source-specific similarity metric was used rather than a weighted combination.", "labels": [], "entities": []}, {"text": "r3wtCD TakeLab features were removed from the feature pool (before feature selection).", "labels": [], "entities": [{"text": "r3wtCD TakeLab", "start_pos": 0, "end_pos": 14, "type": "DATASET", "confidence": 0.8319395184516907}]}, {"text": "Same source combination as run 1.", "labels": [], "entities": []}, {"text": "r4ALL Post-hoc run using all 47 metrics, but training a single linear regression model rather than source-specific models.", "labels": [], "entities": [{"text": "r4ALL", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.8406577706336975}]}, {"text": "shows the top 10 runs of 90 submitted in the STS-Core task are shown, with our three systems placing 5th, 6th, and 8th.", "labels": [], "entities": []}, {"text": "Additionally, we can see that run 4 would have placed 4th.", "labels": [], "entities": []}, {"text": "Notice that there are significant source-specific differences between the runs.", "labels": [], "entities": []}, {"text": "For example, while run 4 is better overall, runs 1-3 outperform it on all but the headlines and FNWN datasets, i.e., the test datasets that were not present in the training data.", "labels": [], "entities": [{"text": "FNWN datasets", "start_pos": 96, "end_pos": 109, "type": "DATASET", "confidence": 0.9402534365653992}]}, {"text": "Thus, it is clear that the source-specific models are beneficial when the training data is in-domain, but a combined model is more beneficial when no such training data is available.", "labels": [], "entities": []}, {"text": "Due to the source-specific variability among the runs, it is important to know whether the forwardsearch feature selection performed as expected.", "labels": [], "entities": []}, {"text": "For source specific models (runs 1 and 3) and a combined model (run 4), shows the 10-fold cross-validation scores on the training set as the next feature is added to the model.", "labels": [], "entities": []}, {"text": "As we would expect, there is an initial growth region where the first features truly complement one another and improve performance significantly.", "labels": [], "entities": []}, {"text": "A plateau is reached for each of the models, and some (e.g., SMTnews) even decay if too many noisy features are added.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 4: Adding customized features one at a time into optimized DKPro feature set. Models are trained across all  sources.", "labels": [], "entities": []}]}