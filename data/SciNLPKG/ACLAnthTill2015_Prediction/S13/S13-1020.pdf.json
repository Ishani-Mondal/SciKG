{"title": [{"text": "UPC-CORE: What Can Machine Translation Evaluation Metrics and Wikipedia Do for Estimating Semantic Textual Similarity?", "labels": [], "entities": [{"text": "UPC-CORE", "start_pos": 0, "end_pos": 8, "type": "DATASET", "confidence": 0.9525712132453918}, {"text": "Machine Translation Evaluation Metrics", "start_pos": 19, "end_pos": 57, "type": "TASK", "confidence": 0.7569635808467865}]}], "abstractContent": [{"text": "In this paper we discuss our participation to the 2013 Semeval Semantic Textual Similarity task.", "labels": [], "entities": [{"text": "Semeval Semantic Textual Similarity task", "start_pos": 55, "end_pos": 95, "type": "TASK", "confidence": 0.6562954783439636}]}, {"text": "Our core features include (i) a set of met-rics borrowed from automatic machine translation , originally intended to evaluate automatic against reference translations and (ii) an instance of explicit semantic analysis, built upon opening paragraphs of Wikipedia 2010 articles.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 72, "end_pos": 91, "type": "TASK", "confidence": 0.7607173919677734}]}, {"text": "Our similarity estimator relies on a support vector regressor with RBF kernel.", "labels": [], "entities": []}, {"text": "Our best approach required 13 machine translation metrics + explicit semantic analysis and ranked 65 in the competition.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 30, "end_pos": 49, "type": "TASK", "confidence": 0.761124849319458}, {"text": "explicit semantic analysis", "start_pos": 60, "end_pos": 86, "type": "TASK", "confidence": 0.6590505341688792}]}, {"text": "Our post-competition analysis shows that the features have a good expression level, but overfitting and-mainly-normalization issues caused our correlation values to decrease.", "labels": [], "entities": []}], "introductionContent": [{"text": "Our participation to the 2013 Semantic Textual Similarity task (STS) ( 1 was focused on the CORE problem: GIVEN TWO SENTENCES, s 1 AND s 2 , QUANTIFIABLY INFORM ON HOW SIMI-LAR s 1 AND s 2 ARE.", "labels": [], "entities": [{"text": "Semantic Textual Similarity task (STS)", "start_pos": 30, "end_pos": 68, "type": "TASK", "confidence": 0.7927875603948321}, {"text": "CORE", "start_pos": 92, "end_pos": 96, "type": "METRIC", "confidence": 0.5053337216377258}, {"text": "GIVEN TWO SENTENCES", "start_pos": 106, "end_pos": 125, "type": "METRIC", "confidence": 0.7154661615689596}, {"text": "INFORM", "start_pos": 154, "end_pos": 160, "type": "METRIC", "confidence": 0.7829586267471313}, {"text": "ARE", "start_pos": 189, "end_pos": 192, "type": "METRIC", "confidence": 0.9748657941818237}]}, {"text": "We considered real-valued features from four different sources: (i) a set of linguistic measures computed with the Asiya Toolkit for Automatic MT Evaluation (), (ii) an instance of explicit semantic analysis (, built on top of Wikipedia articles, (iii) a dataset predictor, and (iv) a subset of the features available in Takelab's Semantic Text Similarity system ( \u02c7 Sari\u00b4c).", "labels": [], "entities": [{"text": "MT Evaluation", "start_pos": 143, "end_pos": 156, "type": "TASK", "confidence": 0.9447439908981323}, {"text": "Semantic Text Similarity", "start_pos": 331, "end_pos": 355, "type": "TASK", "confidence": 0.619717409213384}]}, {"text": "1 http://ixa2.si.ehu.es/sts/ Our approaches obtained an overall modest result compared to other participants (best position: 65 out of 89).", "labels": [], "entities": []}, {"text": "Nevertheless, our post-competition analysis shows that the low correlation was caused mainly by a deficient data normalization strategy.", "labels": [], "entities": [{"text": "correlation", "start_pos": 63, "end_pos": 74, "type": "METRIC", "confidence": 0.9492232799530029}]}, {"text": "The paper distribution is as follows.", "labels": [], "entities": []}, {"text": "Section 2 offers a brief overview of the task.", "labels": [], "entities": []}, {"text": "Section 3 describes our approach.", "labels": [], "entities": []}, {"text": "Section 4 discuss our experiments and obtained results.", "labels": [], "entities": []}], "datasetContent": [{"text": "We consider a set of linguistic measures originally intended to evaluate the quality of automatic translation systems.", "labels": [], "entities": []}, {"text": "These measures compute the quality of a translation by comparing it against one or several reference translations, considered as gold standard.", "labels": [], "entities": []}, {"text": "A straightforward application of these measures to the problem at hand is to consider s 1 as the reference and s 2 as the automatic translation, or vice versa.", "labels": [], "entities": []}, {"text": "Some of the metrics are not symmetric so we compute similarity between s 1 and s 2 in both directions and average the resulting scores.", "labels": [], "entities": [{"text": "similarity", "start_pos": 52, "end_pos": 62, "type": "METRIC", "confidence": 0.9717036485671997}]}, {"text": "The measures are computed with the Asiya Toolkit for Automatic MT Evaluation ().", "labels": [], "entities": [{"text": "Asiya Toolkit", "start_pos": 35, "end_pos": 48, "type": "DATASET", "confidence": 0.8906942009925842}, {"text": "MT Evaluation", "start_pos": 63, "end_pos": 76, "type": "TASK", "confidence": 0.9655933082103729}]}, {"text": "The only pre-processing carried out was tokenization (Asiya performs additional inbox pre-processing operations, though).", "labels": [], "entities": [{"text": "tokenization", "start_pos": 40, "end_pos": 52, "type": "TASK", "confidence": 0.9802072644233704}]}, {"text": "We consid-ered a sample from three similarity families, which was proposed in () as a varied and robust metric set, showing good correlation with human assessments.", "labels": [], "entities": []}, {"text": "3 Lexical Similarity Two metrics of Translation Error Rate () (i.e. the estimated human effort to convert s 1 into s 2 ): -TER and -TER pA . Two measures of lexical precision: BLEU () and NIST).", "labels": [], "entities": [{"text": "Translation Error Rate", "start_pos": 36, "end_pos": 58, "type": "METRIC", "confidence": 0.7020919422308604}, {"text": "TER", "start_pos": 123, "end_pos": 126, "type": "METRIC", "confidence": 0.9930094480514526}, {"text": "TER pA", "start_pos": 132, "end_pos": 138, "type": "METRIC", "confidence": 0.9749591946601868}, {"text": "precision", "start_pos": 165, "end_pos": 174, "type": "METRIC", "confidence": 0.9472646117210388}, {"text": "BLEU", "start_pos": 176, "end_pos": 180, "type": "METRIC", "confidence": 0.9977166652679443}, {"text": "NIST", "start_pos": 188, "end_pos": 192, "type": "DATASET", "confidence": 0.7495622038841248}]}, {"text": "One measure of lexical recall: ROUGE W ().", "labels": [], "entities": [{"text": "recall", "start_pos": 23, "end_pos": 29, "type": "METRIC", "confidence": 0.9794697761535645}, {"text": "ROUGE W", "start_pos": 31, "end_pos": 38, "type": "METRIC", "confidence": 0.9329788386821747}]}, {"text": "Finally, four variants of METEOR (Banerjee and) (exact, stemming, synonyms, and paraphrasing), a lexical metric accounting for F -Measure.", "labels": [], "entities": [{"text": "METEOR", "start_pos": 26, "end_pos": 32, "type": "METRIC", "confidence": 0.9816120266914368}, {"text": "F -Measure", "start_pos": 127, "end_pos": 137, "type": "METRIC", "confidence": 0.9623413681983948}]}, {"text": "Syntactic Similarity Three metrics that estimate the similarity of the sentences over dependency parse trees (): DP-HWCMi c -4 for grammatical categories chains, DP-HWCMi r -4 over grammatical relations, and DP-O r (\u22c6) over words ruled by non-terminal nodes.", "labels": [], "entities": [{"text": "Syntactic Similarity", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.885012686252594}]}, {"text": "Also, one measure that estimates the similarity over constituent parse trees: CP-STM 4 (Liu and).", "labels": [], "entities": [{"text": "similarity", "start_pos": 37, "end_pos": 47, "type": "METRIC", "confidence": 0.9516693949699402}]}, {"text": "Semantic Similarity Three measures that estimate the similarities over semantic roles (i.e. arguments and adjuncts): SR-O r , SR-M r (\u22c6), and SR-O r (\u22c6).", "labels": [], "entities": []}, {"text": "Additionally, two metrics that estimate similarities over discourse representations: DR-O r (\u22c6) and DR-O rp (\u22c6).", "labels": [], "entities": []}, {"text": "Given the similarity shifts in the different datasets (cf, we tried to predict what dataset an instance belonged to on the basis of its vocabulary.", "labels": [], "entities": []}, {"text": "We built binary maxent classifiers for each dataset in the development set, resulting in five dataset likelihood features: dMSRpar, dSMTeuroparl, dMSRvid, dOnWN, and dSMTnews.", "labels": [], "entities": []}, {"text": "Pre-processing consisted of tokenization and lemmatization.", "labels": [], "entities": [{"text": "tokenization", "start_pos": 28, "end_pos": 40, "type": "TASK", "confidence": 0.9719579219818115}]}, {"text": "Section 4.1 describes our model tuning strategy.", "labels": [], "entities": [{"text": "model tuning", "start_pos": 26, "end_pos": 38, "type": "TASK", "confidence": 0.7401381731033325}]}, {"text": "Sections 4.2 and 4.3 discuss the official and postcompetition results.", "labels": [], "entities": [{"text": "postcompetition", "start_pos": 46, "end_pos": 61, "type": "METRIC", "confidence": 0.932113528251648}]}], "tableCaptions": [{"text": " Table 1: Overview of sub-collections in the development and test datasets, including number of instances and distri- bution of similarity values (in percentage) as well as mean, minimum, and maximum lengths.", "labels": [], "entities": []}, {"text": " Table 2: Tuning process: parameter definition and feature  selection. Number of features at the beginning and end  of the feature selection step included.", "labels": [], "entities": [{"text": "Tuning", "start_pos": 10, "end_pos": 16, "type": "TASK", "confidence": 0.9675556421279907}, {"text": "parameter definition", "start_pos": 26, "end_pos": 46, "type": "TASK", "confidence": 0.7171820104122162}, {"text": "feature  selection", "start_pos": 51, "end_pos": 69, "type": "TASK", "confidence": 0.688396155834198}]}, {"text": " Table 4: Official results for the three runs (rank included).", "labels": [], "entities": []}, {"text": " Table 5: Post-competition experiments results", "labels": [], "entities": []}]}