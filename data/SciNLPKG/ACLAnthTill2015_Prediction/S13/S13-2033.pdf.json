{"title": [{"text": "WSD2: Parameter optimisation for Memory-based Cross-Lingual Word-Sense Disambiguation", "labels": [], "entities": [{"text": "WSD2", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.7361983060836792}, {"text": "Cross-Lingual Word-Sense Disambiguation", "start_pos": 46, "end_pos": 85, "type": "TASK", "confidence": 0.5615116755167643}]}], "abstractContent": [{"text": "We present our system WSD2 which participated in the Cross-Lingual Word-Sense Dis-ambiguation task for SemEval 2013 (Lefever and Hoste, 2013).", "labels": [], "entities": [{"text": "Cross-Lingual Word-Sense Dis-ambiguation task for SemEval 2013 (Lefever and Hoste, 2013)", "start_pos": 53, "end_pos": 141, "type": "TASK", "confidence": 0.5495103831802096}]}, {"text": "The system closely resembles our winning system for the same task in SemEval 2010.", "labels": [], "entities": []}, {"text": "It is based on k-nearest neighbour classifiers which map words with local and global context features onto their translation , i.e. their cross-lingual sense.", "labels": [], "entities": []}, {"text": "The system participated in the task for all five languages and obtained winning scores for four of them when asked to predict the best trans-lation(s).", "labels": [], "entities": []}, {"text": "We tested various configurations of our system, focusing on various levels of hy-perparameter optimisation and feature selection.", "labels": [], "entities": []}, {"text": "Our final results indicate that hyperpa-rameter optimisation did not lead to the best results, indicating overfitting by our optimisa-tion method in this aspect.", "labels": [], "entities": []}, {"text": "Feature selection does have a modest positive impact.", "labels": [], "entities": [{"text": "Feature selection", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.8286519050598145}]}], "introductionContent": [{"text": "WSD2 is a rewrite and extension of our previous system) that participated in the Cross-Lingual Word Sense Disambiguation task in.", "labels": [], "entities": [{"text": "WSD2", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.9457216858863831}, {"text": "Cross-Lingual Word Sense Disambiguation task", "start_pos": 81, "end_pos": 125, "type": "TASK", "confidence": 0.7910677313804626}]}, {"text": "In WSD2 we introduce and test anew level of hyperparameter optimisation.", "labels": [], "entities": [{"text": "WSD2", "start_pos": 3, "end_pos": 7, "type": "DATASET", "confidence": 0.7025237083435059}]}, {"text": "Unlike the previous occasion, we participate in all five target languages (Dutch, Spanish, Italian, French, and German).", "labels": [], "entities": []}, {"text": "The task presents twenty polysemous nouns with fifty instances each to be mapped onto normalised (lemmatised) translations in all languages.", "labels": [], "entities": []}, {"text": "The task is described in detail by.", "labels": [], "entities": []}, {"text": "Trial data is provided and has been used to optimise system parameters.", "labels": [], "entities": []}, {"text": "Due to the unsupervised nature of the task, no training data is provided.", "labels": [], "entities": []}, {"text": "However, given that the gold standard of the task is based exclusively on the Europarl parallel corpus, we select that same corpus to minimise our chances of delivering translations that the human annotators preparing the test data could have never picked.", "labels": [], "entities": [{"text": "Europarl parallel corpus", "start_pos": 78, "end_pos": 102, "type": "DATASET", "confidence": 0.9578062891960144}]}, {"text": "Systems may output several senses per instance, rather than producing just one sense prediction.", "labels": [], "entities": []}, {"text": "These are evaluated in two different ways.", "labels": [], "entities": []}, {"text": "The scoring type \"best\" expects that the system outputs the sense it considers the most likely, or a number of senses in the order of its confidence in these senses being correct.", "labels": [], "entities": []}, {"text": "Multiple guesses are penalised, however.", "labels": [], "entities": []}, {"text": "In contrast, the scoring type \"out of five\" expects five guesses, in which each answer carries the same weight.", "labels": [], "entities": []}, {"text": "These metrics are more extensively described in and.", "labels": [], "entities": []}], "datasetContent": [{"text": "To assess the accuracy of a certain configuration of our system as a whole, we take the average overall word experts.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 14, "end_pos": 22, "type": "METRIC", "confidence": 0.9988065958023071}]}, {"text": "An initial experiment on the trial data explores the impact of different context sizes, with hyperparameter optimisation on the classifiers.", "labels": [], "entities": []}, {"text": "The results, shown in, clearly indicate that on average the classifiers perform best with a local context of just one word to the left and one to the right of the word to be disambiguated.", "labels": [], "entities": []}, {"text": "Larger context sizes have a negative impact on average accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 55, "end_pos": 63, "type": "METRIC", "confidence": 0.9746026992797852}]}, {"text": "These tests include hyperparameter optimisation, but the same trend shows without.", "labels": [], "entities": []}, {"text": "We submitted three configurations of our system to the shared task, the maximum number of runs.", "labels": [], "entities": []}, {"text": "Adding lemma features to the local context window of three words proves beneficial in general, as shown in.", "labels": [], "entities": []}, {"text": "This is therefore the first configuration we submitted (c1l).", "labels": [], "entities": []}, {"text": "As second configuration (c1lN) we submitted the same configuration without parameter optimisation on the classifiers.", "labels": [], "entities": []}, {"text": "Note that neither of these include global context features.", "labels": [], "entities": []}, {"text": "The third configuration (var) we submitted includes feature selection, and selects per word expert the configuration that has the highest score on the trial data, and thus tests all kinds of configurations.", "labels": [], "entities": []}, {"text": "Note that hyperparameter optimisation is also enabled for this configuration.", "labels": [], "entities": []}, {"text": "Due to the feature selection on the trial data, we by definition obtain the highest scores on this trial data, but this carries the risk of overfitting.", "labels": [], "entities": []}, {"text": "Results on the trial data are shown in.", "labels": [], "entities": []}, {"text": "The hyperparameter optimisation on classifier accuracy has a slightly negative impact, suggesting overfitting on the training data.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 46, "end_pos": 54, "type": "METRIC", "confidence": 0.9629427194595337}]}, {"text": "Therefore a fourth configuration (varN) was tried later to indepen-dently assess the idea of feature selection, without hyperparameter optimisation on the classifiers.", "labels": [], "entities": []}, {"text": "This proves to be a good idea.", "labels": [], "entities": []}, {"text": "However, the fourth configuration was not yet available for the actual competition.", "labels": [], "entities": []}, {"text": "This incidentally would have had no impact on the final ranking between competitors.", "labels": [], "entities": []}, {"text": "When we run these systems on the actual test data of the shared task, we obtain the results in.", "labels": [], "entities": []}, {"text": "The best score amongst the other competitors is mentioned in the last row for reference, this is the HLTDI team) for all but Best-Spanish, which goes to the NRC contribution  A major factor in this task is the accuracy of lemmatisation, and to lesser extent of PoS tagging.", "labels": [], "entities": [{"text": "HLTDI", "start_pos": 101, "end_pos": 106, "type": "METRIC", "confidence": 0.6318404078483582}, {"text": "NRC contribution", "start_pos": 157, "end_pos": 173, "type": "DATASET", "confidence": 0.8488001227378845}, {"text": "accuracy", "start_pos": 210, "end_pos": 218, "type": "METRIC", "confidence": 0.9993394017219543}, {"text": "PoS tagging", "start_pos": 261, "end_pos": 272, "type": "TASK", "confidence": 0.818626195192337}]}, {"text": "We conducted additional experiments on German and French without lemmatisation, tested on the trial data.", "labels": [], "entities": []}, {"text": "Results immediately fell below baseline.", "labels": [], "entities": []}, {"text": "Another main factor is the quality of the word alignments, and the degree to which the found word alignments correspond with the translations the human annotators could choose from in preparing the gold standard.", "labels": [], "entities": [{"text": "word alignments", "start_pos": 42, "end_pos": 57, "type": "TASK", "confidence": 0.6967213600873947}]}, {"text": "An idea we tested is, instead of relying on the mere intersection of word alignments, to use a phrase-translation table generated by and for the Statistical Machine Translation system Moses (, which uses the grow-diag-final heuristic to extract phrase pairs.", "labels": [], "entities": [{"text": "Statistical Machine Translation", "start_pos": 145, "end_pos": 176, "type": "TASK", "confidence": 0.5769860446453094}]}, {"text": "This results in more phrases, and whilst this is a good idea for MT, in the current task it has a detrimental effect, as it creates too many translation options and we do not have an MT decoder to discard ineffective options in this task.", "labels": [], "entities": [{"text": "MT", "start_pos": 65, "end_pos": 67, "type": "TASK", "confidence": 0.9882768988609314}]}, {"text": "The grow-diag-final heuristic incorporates unaligned words to the end of a translation in the translation option, a bad idea for CLWSD.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Feature exploration on the trial data", "labels": [], "entities": []}, {"text": " Table 2: Results on the trial data", "labels": [], "entities": []}, {"text": " Table 3. The best score  amongst the other competitors is mentioned in the  last row for reference, this is the HLTDI team", "labels": [], "entities": [{"text": "HLTDI team", "start_pos": 113, "end_pos": 123, "type": "DATASET", "confidence": 0.7412222325801849}]}, {"text": " Table 3: Results on the test set", "labels": [], "entities": []}]}