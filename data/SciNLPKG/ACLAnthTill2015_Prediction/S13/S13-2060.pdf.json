{"title": [{"text": "UNITOR: Combining Syntactic and Semantic Kernels for Twitter Sentiment Analysis", "labels": [], "entities": [{"text": "Twitter Sentiment Analysis", "start_pos": 53, "end_pos": 79, "type": "TASK", "confidence": 0.6935168703397115}]}], "abstractContent": [{"text": "In this paper, the UNITOR system participating in the SemEval-2013 Sentiment Analysis in Twitter task is presented.", "labels": [], "entities": [{"text": "SemEval-2013 Sentiment Analysis in Twitter task", "start_pos": 54, "end_pos": 101, "type": "TASK", "confidence": 0.8253736396630605}]}, {"text": "The polarity detection of a tweet is modeled as a classification task, tackled through a Multiple Kernel approach.", "labels": [], "entities": [{"text": "polarity detection of a tweet", "start_pos": 4, "end_pos": 33, "type": "TASK", "confidence": 0.8335439205169678}]}, {"text": "It allows to combine the contribution of complex kernel functions, such as the Latent Semantic Kernel and Smoothed Partial Tree Kernel, to implicitly integrate syntactic and lexical information of annotated examples.", "labels": [], "entities": []}, {"text": "In the challenge, UNITOR system achieves good results, even considering that no manual feature engineering is performed and no manually coded resources are employed.", "labels": [], "entities": []}, {"text": "These kernels in-fact embed distri-butional models of lexical semantics to determine expressive generalization of tweets.", "labels": [], "entities": []}], "introductionContent": [{"text": "Web 2.0 and Social Networks technologies allow users to generate contents on blogs, forums and new forms of communication (such as micro-blogging) writing their opinion about facts, things, events.", "labels": [], "entities": []}, {"text": "The analysis of this information is crucial for companies, politicians or other users in order to learn what people think, and consequently to adjust their strategies.", "labels": [], "entities": []}, {"text": "In such a scenario, the interest in the analysis of the sentiment expressed by people is rapidly growing.", "labels": [], "entities": [{"text": "analysis of the sentiment expressed", "start_pos": 40, "end_pos": 75, "type": "TASK", "confidence": 0.7934803009033203}]}, {"text": "Twitter 1 represents an intriguing source of information as it is used to share opinions and sentiments about brands, products, or situations ().", "labels": [], "entities": []}, {"text": "1 http://www.twitter.com On the other hand, tweet analysis represents a challenging task for natural language processing systems.", "labels": [], "entities": [{"text": "tweet analysis", "start_pos": 44, "end_pos": 58, "type": "TASK", "confidence": 0.8390254080295563}]}, {"text": "Let us consider the following tweets, evoking a positive (1), and negative (2) polarity, respectively.", "labels": [], "entities": []}, {"text": "Porto amazing as the sun sets...", "labels": [], "entities": []}, {"text": "http://bit.ly/c28w (1) @knickfan82 Nooooo ;( they delayed the knicks game until Monday!", "labels": [], "entities": [{"text": "knickfan82 Nooooo", "start_pos": 24, "end_pos": 41, "type": "DATASET", "confidence": 0.6582668125629425}]}, {"text": "Tweets are short, informal and characterized by their own particular language with \"Twitter syntax\", e.g. retweets (\"RT\"), user references (\"@\"), hashtags (\"#\") or other typical web abbreviations, such as emoticons or acronyms.", "labels": [], "entities": []}, {"text": "Classical approaches to sentiment analysis ( are not directly applicable to tweets: most of them focus on relatively large texts, e.g. movie or product reviews, and performance drops are experimented in tweets scenario.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 24, "end_pos": 42, "type": "TASK", "confidence": 0.9568905532360077}]}, {"text": "Some recent works tried to model the sentiment in tweets ().", "labels": [], "entities": []}, {"text": "Specific approaches and feature modeling are used to achieve good accuracy levels in tweet polarity recognition.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 66, "end_pos": 74, "type": "METRIC", "confidence": 0.9937499165534973}, {"text": "tweet polarity recognition", "start_pos": 85, "end_pos": 111, "type": "TASK", "confidence": 0.8873689572016398}]}, {"text": "For example, the use of n-grams, POS tags, polarity lexicon and tweet specific features (e.g. hashtag, retweet) are some of the component exploited by these works in combination with different machine learning algorithms (e.g. Naive, k-NN strategies (), SVM and Tree).", "labels": [], "entities": []}, {"text": "In this paper, the UNITOR system participating in the SemEval-2013 Sentiment Analysis in Twitter task () models the sentiment analysis stage as a classification task.", "labels": [], "entities": [{"text": "SemEval-2013 Sentiment Analysis in Twitter task", "start_pos": 54, "end_pos": 101, "type": "TASK", "confidence": 0.7964661121368408}, {"text": "sentiment analysis", "start_pos": 116, "end_pos": 134, "type": "TASK", "confidence": 0.9151825308799744}]}, {"text": "A Support Vector Machine (SVM) classifier learns the association between short texts and polarity classes (i.e. positive, negative, neutral).", "labels": [], "entities": []}, {"text": "Different kernel functions) have been used: each kernel aims at capturing specific aspects of the semantic similarity between two tweets, according to syntactic and lexical information.", "labels": [], "entities": []}, {"text": "In particular, inline with the idea of using convolution tree kernels to model complex semantic tasks, e.g. (), we adopted the Smoothed Partial Tree Kernel (Croce et al., 2011) (SPTK).", "labels": [], "entities": [{"text": "Smoothed Partial Tree Kernel (Croce et al., 2011) (SPTK)", "start_pos": 127, "end_pos": 183, "type": "DATASET", "confidence": 0.5662295264857156}]}, {"text": "It is a state-of-the-art convolution kernel that allows to measure the similarity between syntactic structures, which are partially similar and whose nodes can differ but are nevertheless semantically related.", "labels": [], "entities": []}, {"text": "Moreover, a Bag-of-Word and a Latent Semantic Kernel () are also combined with the SPTK in a multi-kernel approach.", "labels": [], "entities": []}, {"text": "Our aim is to design a system that exhibits wide applicability and robustness.", "labels": [], "entities": []}, {"text": "This objective is pursued by adopting an approach that avoids the use of any manually coded resource (e.g. a polarity lexicon), but mainly exploits distributional analysis of unlabeled corpora: the generalization of words meaning is achieved through the construction of a Word Space, which provides an effective distributional model of lexical semantics.", "labels": [], "entities": []}, {"text": "In the rest of the paper, in Section 2 we will deeply explain our approach.", "labels": [], "entities": []}, {"text": "In Section 3 the results achieved by our system in the SemEval-2013 challenge are described and discussed.", "labels": [], "entities": [{"text": "SemEval-2013 challenge", "start_pos": 55, "end_pos": 77, "type": "TASK", "confidence": 0.6083243489265442}]}], "datasetContent": [{"text": "In the Sentiment Analysis in Twitter task, two subtasks are defined: Contextual Polarity Disambiguation (Task A), and Message Polarity Classification (Task B).", "labels": [], "entities": [{"text": "Sentiment Analysis in Twitter task", "start_pos": 7, "end_pos": 41, "type": "TASK", "confidence": 0.8937818527221679}, {"text": "Contextual Polarity Disambiguation", "start_pos": 69, "end_pos": 103, "type": "TASK", "confidence": 0.5592688719431559}, {"text": "Message Polarity Classification", "start_pos": 118, "end_pos": 149, "type": "TASK", "confidence": 0.6792154014110565}]}, {"text": "The former deals with the polarity classification (positive, negative or neutral) of a marked occurrence of a word or phrase in a tweet context.", "labels": [], "entities": []}, {"text": "For example the adjective \"amazing\" in example 1 expresses a positive marked word.", "labels": [], "entities": []}, {"text": "The latter deals with the classification of an entire tweet with respect to the three classes positive, negative and neutral.", "labels": [], "entities": [{"text": "classification of an entire tweet", "start_pos": 26, "end_pos": 59, "type": "TASK", "confidence": 0.7849004626274109}]}, {"text": "In both subtasks, we computed a fixed (80%-20%) split of the training data for classifiers parameter tuning.", "labels": [], "entities": [{"text": "classifiers parameter tuning", "start_pos": 79, "end_pos": 107, "type": "TASK", "confidence": 0.7123297154903412}]}, {"text": "Tuned parameters are the regularization parameter and the cost factor) of the SVM formulation.", "labels": [], "entities": []}, {"text": "The former represents the trade off between a training error and the margin.", "labels": [], "entities": [{"text": "margin", "start_pos": 69, "end_pos": 75, "type": "METRIC", "confidence": 0.9883493185043335}]}, {"text": "The latter controls the trade off between positive and negative examples.", "labels": [], "entities": []}, {"text": "The learning phase is made available by an extended version of SVM-LightTK 2 , implementing the smooth matching between tree nodes.", "labels": [], "entities": []}, {"text": "We built a Word Space based on about 1.5 million of tweets downloaded during the challenge period using the topic name from the trial material as and query terms.", "labels": [], "entities": []}, {"text": "We normalized and analyzed tweets as described in section 2.1.", "labels": [], "entities": []}, {"text": "Words occurring more than 100 times in the source corpus are represented as vectors.", "labels": [], "entities": []}, {"text": "The 10, 000 most frequent words in the corpus are considered as contexts and the co-occurrence scores are measured in a window of size n = \u00b15.", "labels": [], "entities": []}, {"text": "Vector components are weighted through the Pointwise Mutual Information (PMI), and dimensionality reduction is applied through SVD with a cut of k = 250.", "labels": [], "entities": []}, {"text": "The task requires to classify two different texts: tweets and sms.", "labels": [], "entities": []}, {"text": "Sms classification is intended to verify how well a system can scale on a different domain.", "labels": [], "entities": [{"text": "Sms classification", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.9838815629482269}]}, {"text": "In the testing phase two types of submissions are allowed.", "labels": [], "entities": []}, {"text": "Constrained results refer to the case where systems are trained only with the released data.", "labels": [], "entities": []}, {"text": "Unconstrained results refer to the case where additional training material is allowed.", "labels": [], "entities": []}, {"text": "Evaluation metrics adopted to compare systems are Precision, Recall and F1-Measure.", "labels": [], "entities": [{"text": "Precision", "start_pos": 50, "end_pos": 59, "type": "METRIC", "confidence": 0.9976624250411987}, {"text": "Recall", "start_pos": 61, "end_pos": 67, "type": "METRIC", "confidence": 0.9953376054763794}, {"text": "F1-Measure", "start_pos": 72, "end_pos": 82, "type": "METRIC", "confidence": 0.9975830316543579}]}, {"text": "Average F1 of the positive and negative classes is then used to generate ranks.", "labels": [], "entities": [{"text": "F1", "start_pos": 8, "end_pos": 10, "type": "METRIC", "confidence": 0.9925097227096558}]}, {"text": "Further information about the task is available in ().", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Task A results for the sms dataset", "labels": [], "entities": [{"text": "sms dataset", "start_pos": 33, "end_pos": 44, "type": "DATASET", "confidence": 0.7306614369153976}]}, {"text": " Table 2: Task A results for the twitter dataset", "labels": [], "entities": [{"text": "twitter dataset", "start_pos": 33, "end_pos": 48, "type": "DATASET", "confidence": 0.751083642244339}]}, {"text": " Table 3: Task B results for the sms dataset in the  constrained case", "labels": [], "entities": [{"text": "sms dataset", "start_pos": 33, "end_pos": 44, "type": "DATASET", "confidence": 0.6834605783224106}]}, {"text": " Table 4: Task B results for the twitter dataset in the  constrained case", "labels": [], "entities": [{"text": "twitter dataset", "start_pos": 33, "end_pos": 48, "type": "DATASET", "confidence": 0.706922709941864}]}, {"text": " Table 5: Task B results for the sms dataset in the  unconstrained case", "labels": [], "entities": [{"text": "sms dataset", "start_pos": 33, "end_pos": 44, "type": "DATASET", "confidence": 0.6771946102380753}]}, {"text": " Table 6: Task B results for the twitter dataset in the  unconstrained case", "labels": [], "entities": [{"text": "twitter dataset", "start_pos": 33, "end_pos": 48, "type": "DATASET", "confidence": 0.7150572389364243}]}]}