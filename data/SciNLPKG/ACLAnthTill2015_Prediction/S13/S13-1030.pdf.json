{"title": [{"text": "UniMelb NLP-CORE: Integrating predictions from multiple domains and feature sets for estimating semantic textual similarity", "labels": [], "entities": [{"text": "estimating semantic textual similarity", "start_pos": 85, "end_pos": 123, "type": "TASK", "confidence": 0.7043232768774033}]}], "abstractContent": [{"text": "In this paper we present our systems for calculating the degree of semantic similarity between two texts that we submitted to the Semantic Textual Similarity task at SemEval-2013.", "labels": [], "entities": [{"text": "Semantic Textual Similarity task at SemEval-2013", "start_pos": 130, "end_pos": 178, "type": "TASK", "confidence": 0.8452459474404653}]}, {"text": "Our systems predict similarity using a regression over features based on the following sources of information: string similarity, topic distributions of the texts based on latent Dirichlet allocation, and similarity between the documents returned by an information retrieval engine when the target texts are used as queries.", "labels": [], "entities": [{"text": "similarity", "start_pos": 205, "end_pos": 215, "type": "METRIC", "confidence": 0.9587703943252563}]}, {"text": "We also explore methods for integrating predictions using different training datasets and feature sets.", "labels": [], "entities": []}, {"text": "Our best system was ranked 17th out of 89 participating systems.", "labels": [], "entities": []}, {"text": "In our post-task analysis, we identify simple changes to our system that further improve our results.", "labels": [], "entities": []}], "introductionContent": [{"text": "Semantic Textual Similarity (STS) measures the degree of semantic similarity or equivalence between a pair of short texts.", "labels": [], "entities": [{"text": "Semantic Textual Similarity (STS)", "start_pos": 0, "end_pos": 33, "type": "TASK", "confidence": 0.7757691542307535}]}, {"text": "STS is related to many natural language processing applications such as text summarisation, machine translation, word sense disambiguation, and question answering.", "labels": [], "entities": [{"text": "STS", "start_pos": 0, "end_pos": 3, "type": "TASK", "confidence": 0.9608607888221741}, {"text": "text summarisation", "start_pos": 72, "end_pos": 90, "type": "TASK", "confidence": 0.7476475238800049}, {"text": "machine translation", "start_pos": 92, "end_pos": 111, "type": "TASK", "confidence": 0.8024371862411499}, {"text": "word sense disambiguation", "start_pos": 113, "end_pos": 138, "type": "TASK", "confidence": 0.7058196465174357}, {"text": "question answering", "start_pos": 144, "end_pos": 162, "type": "TASK", "confidence": 0.8971409797668457}]}, {"text": "Two short texts are considered similar if they both convey similar messages.", "labels": [], "entities": []}, {"text": "Often it is the case that similar texts will have a high degree of lexical overlap, although this isn't always so.", "labels": [], "entities": []}, {"text": "For example, SC dismissed government's review plea in Vodafone tax case and SC dismisses govt's review petition on Vodafone tax verdict are semantically similar.", "labels": [], "entities": [{"text": "Vodafone tax case", "start_pos": 54, "end_pos": 71, "type": "DATASET", "confidence": 0.9544676144917806}, {"text": "Vodafone tax verdict", "start_pos": 115, "end_pos": 135, "type": "DATASET", "confidence": 0.951831062634786}]}, {"text": "These texts have matches in terms of exact words (SC, Vodafone, tax), morphologically-related words (dismissed and dismisses), and abbreviations (government's and govt's).", "labels": [], "entities": [{"text": "Vodafone", "start_pos": 54, "end_pos": 62, "type": "DATASET", "confidence": 0.8978136777877808}]}, {"text": "However, the usages (senses) of plea and petition, and case and verdict are also similar.", "labels": [], "entities": []}, {"text": "One straightforward way of estimating semantic similarity of two texts is by using approaches based on the similarity of the surface forms of the words they contain.", "labels": [], "entities": []}, {"text": "However, such methods are not capable of capturing similarity or relatedness at the lexical level, and moreover, they do not exploit the context in which individual words are used in a target text.", "labels": [], "entities": []}, {"text": "Nevertheless, a variety of knowledge sources -including part-of-speech, collocations, syntax, and domain -can be used to identify the usage or sense of words in context) to address these issues.", "labels": [], "entities": []}, {"text": "Despite their limitations, string similarity measures have been widely used in previous semantic similarity tasks (.", "labels": [], "entities": []}, {"text": "Latent variable models have also been used to estimate the semantic similarity between words, word usages, and texts.", "labels": [], "entities": []}, {"text": "In this paper, we consider three different ways of measuring semantic similarity based on word and word usage similarity: 1.", "labels": [], "entities": []}, {"text": "String-based similarity to measure surfacelevel lexical similarity, taking into account morphology and abbreviations (e.g., dismisses and dismissed, and government's and govt's); 2.", "labels": [], "entities": []}, {"text": "Latent variable models of similarity to capture words that have different surface forms, but that have similar meanings or that can be used in similar contexts (e.g., petition and plea, verdict and case); and 3.", "labels": [], "entities": []}, {"text": "Topical/domain similarity of the texts with respect to the similarity of documents in an external corpus (based on information-retrieval methods) that are relevant to the target texts.", "labels": [], "entities": []}, {"text": "We develop features based on all three of these knowledge sources to capture semantic similarity from a variety of perspectives.", "labels": [], "entities": []}, {"text": "We build a regression model, trained on STS training data which has semantic similarity scores for pairs of texts, to learn weights for the features and rate the similarity of test instances.", "labels": [], "entities": []}, {"text": "Our approach to the task is to explore the utility of novel features or features that have not performed well in previous research, rather than combine these features with the myriad of features that have been proposed by others for the task.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluated our models based on a leave-one-out cross-validation across the 3 training datasets.", "labels": [], "entities": []}, {"text": "Thus, for each of the training datasets, we trained a separate model using features from the other two.", "labels": [], "entities": []}, {"text": "We considered approaches based on each individual feature set, with and without flagging.", "labels": [], "entities": []}, {"text": "We further considered combinations of feature sets using feature concatenation, as well as feature and domain stacking, again with and without flagging.", "labels": [], "entities": [{"text": "feature and domain stacking", "start_pos": 91, "end_pos": 118, "type": "TASK", "confidence": 0.6491333693265915}]}, {"text": "Results are: Pearson's \u03c1 for each feature set (FSet), as well as combinations of feature sets and adaptation strategies, on each test dataset, and the micro-average overall test datasets, using features from all 2012 data (test + train).", "labels": [], "entities": [{"text": "Pearson's \u03c1", "start_pos": 13, "end_pos": 24, "type": "METRIC", "confidence": 0.8983172575632731}, {"text": "FSet)", "start_pos": 47, "end_pos": 52, "type": "METRIC", "confidence": 0.9803409576416016}]}, {"text": "(*), (+), and (\u2212) denote Run1, Run2, and Run3, respectively, our submissions to the shared task; FL=Flagging, FS=Feature stacking, DS=Domain stacking.", "labels": [], "entities": [{"text": "FL", "start_pos": 97, "end_pos": 99, "type": "METRIC", "confidence": 0.9472694396972656}, {"text": "FS", "start_pos": 110, "end_pos": 112, "type": "METRIC", "confidence": 0.9875368475914001}, {"text": "Feature stacking", "start_pos": 113, "end_pos": 129, "type": "TASK", "confidence": 0.6543906033039093}, {"text": "Domain stacking", "start_pos": 134, "end_pos": 149, "type": "TASK", "confidence": 0.7488919496536255}]}, {"text": "\u03b4 denotes the difference in system performance after adding the additional training data.", "labels": [], "entities": []}, {"text": "The best results on the training data were achieved using only our SS feature set with flagging (Run1), with an average Pearson's \u03c1 of 0.549.", "labels": [], "entities": [{"text": "SS feature set", "start_pos": 67, "end_pos": 81, "type": "DATASET", "confidence": 0.7353893220424652}, {"text": "Pearson's \u03c1", "start_pos": 120, "end_pos": 131, "type": "METRIC", "confidence": 0.9824356834093729}]}, {"text": "This feature set also gave the best performance on MSRpar and SMTeuroparl, although the IR feature set was substantially better on MSRvid.", "labels": [], "entities": []}, {"text": "On the training datasets, our approaches that combine feature sets did not give an improvement over the best individual feature set on any dataset, or overall.", "labels": [], "entities": []}, {"text": "We re-trained all the above systems by extending the training data to include the 2012 test data.", "labels": [], "entities": [{"text": "2012 test data", "start_pos": 82, "end_pos": 96, "type": "DATASET", "confidence": 0.8253057797749838}]}, {"text": "Scores on the 2013 test datasets and the change in Pearson's \u03c1 after adding the extra training data (denoted \u03b4) are presented in.", "labels": [], "entities": [{"text": "2013 test datasets", "start_pos": 14, "end_pos": 32, "type": "DATASET", "confidence": 0.689515103896459}, {"text": "Pearson's \u03c1", "start_pos": 51, "end_pos": 62, "type": "METRIC", "confidence": 0.8574884136517843}]}, {"text": "In general, the addition of the 2012 test data to the training dataset improves the performance of the system, though this is often not the case for the flagging approach to domain adaptation, which in some instances drops in performance after adding the additional training data.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 174, "end_pos": 191, "type": "TASK", "confidence": 0.7322742491960526}]}, {"text": "The biggest improvements were seen for feature-domain stacking, particularly on FNWN.", "labels": [], "entities": [{"text": "feature-domain stacking", "start_pos": 39, "end_pos": 62, "type": "TASK", "confidence": 0.7546802461147308}, {"text": "FNWN", "start_pos": 80, "end_pos": 84, "type": "DATASET", "confidence": 0.9643335342407227}]}, {"text": "This suggests that feature-domain stacking is more sensitive to the similarity between training data and test data than flagging, but also that it is better able to cope with variety in training domains than flagging.", "labels": [], "entities": [{"text": "feature-domain stacking", "start_pos": 19, "end_pos": 42, "type": "TASK", "confidence": 0.6829375475645065}]}, {"text": "Given that the pool of annotated data for the STS task continues to increase, feature-domain stacking is a promising approach to exploiting the differences between domains to improve overall STS performance.", "labels": [], "entities": [{"text": "STS task", "start_pos": 46, "end_pos": 54, "type": "TASK", "confidence": 0.8817092478275299}, {"text": "feature-domain stacking", "start_pos": 78, "end_pos": 101, "type": "TASK", "confidence": 0.6984984278678894}]}, {"text": "To facilitate comparison with the published results for the 2013 STS task, we present a condensed summary of our results in, which shows the absolute score as well as the projected ranking of each of our systems.", "labels": [], "entities": [{"text": "2013 STS task", "start_pos": 60, "end_pos": 73, "type": "TASK", "confidence": 0.4586540957291921}, {"text": "absolute score", "start_pos": 141, "end_pos": 155, "type": "METRIC", "confidence": 0.9510276913642883}]}, {"text": "It also includes the median and baseline results for comparison.", "labels": [], "entities": [{"text": "median", "start_pos": 21, "end_pos": 27, "type": "METRIC", "confidence": 0.9796836376190186}]}], "tableCaptions": [{"text": " Table 1: Pearson's \u03c1 for each feature set (FSet),  as well as combinations of feature sets and adap- tation strategies, on each training dataset, and the  micro-average over all training datasets. (*), (+),  and (\u2212) denote Run1, Run2, and Run3, respectively,  our submissions to the shared task; FL=Flagging,  FS=Feature stacking, DS=Domain stacking.", "labels": [], "entities": [{"text": "Pearson's \u03c1", "start_pos": 10, "end_pos": 21, "type": "METRIC", "confidence": 0.6103047033150991}, {"text": "FL", "start_pos": 297, "end_pos": 299, "type": "METRIC", "confidence": 0.9736360907554626}, {"text": "FS", "start_pos": 311, "end_pos": 313, "type": "METRIC", "confidence": 0.9869740009307861}, {"text": "Feature stacking", "start_pos": 314, "end_pos": 330, "type": "TASK", "confidence": 0.6455222815275192}, {"text": "Domain stacking", "start_pos": 335, "end_pos": 350, "type": "TASK", "confidence": 0.7591687142848969}]}, {"text": " Table 2: Pearson's \u03c1 for each feature set (FSet),  as well as combinations of feature sets and adap- tation strategies, on each test dataset, and the  micro-average over all test datasets. (*), (+), and  (\u2212) denote Run1, Run2, and Run3, respectively,  our submissions to the shared task; FL=Flagging,  FS=Feature stacking, DS=Domain stacking.", "labels": [], "entities": [{"text": "Pearson's \u03c1", "start_pos": 10, "end_pos": 21, "type": "METRIC", "confidence": 0.6192540427049001}, {"text": "FL", "start_pos": 289, "end_pos": 291, "type": "METRIC", "confidence": 0.9736494421958923}, {"text": "FS", "start_pos": 303, "end_pos": 305, "type": "METRIC", "confidence": 0.9860666394233704}, {"text": "Feature stacking", "start_pos": 306, "end_pos": 322, "type": "TASK", "confidence": 0.6441101133823395}, {"text": "Domain stacking", "start_pos": 327, "end_pos": 342, "type": "TASK", "confidence": 0.7616415917873383}]}, {"text": " Table 4: Pearson's \u03c1 (and projected ranking) of runs.  The upper 4 runs are trained only on STS 2012 train- ing data. (+) denotes runs that were submitted for  evaluation. ( * ) denotes systems trained on STS 2012", "labels": [], "entities": [{"text": "Pearson's \u03c1", "start_pos": 10, "end_pos": 21, "type": "METRIC", "confidence": 0.6847191055615743}, {"text": "STS 2012 train- ing data", "start_pos": 93, "end_pos": 117, "type": "DATASET", "confidence": 0.8658778965473175}, {"text": "STS 2012", "start_pos": 206, "end_pos": 214, "type": "DATASET", "confidence": 0.7772771418094635}]}]}