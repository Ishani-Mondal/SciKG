{"title": [{"text": "NTNU: Domain Semi-Independent Short Message Sentiment Classification", "labels": [], "entities": [{"text": "NTNU", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.845855176448822}, {"text": "Domain Semi-Independent Short Message Sentiment Classification", "start_pos": 6, "end_pos": 68, "type": "TASK", "confidence": 0.6605674276749293}]}], "abstractContent": [{"text": "The paper describes experiments using grid searches over various combinations of machine learning algorithms, features and pre-processing strategies in order to produce the optimal systems for sentiment classification of microblog messages.", "labels": [], "entities": [{"text": "sentiment classification of microblog messages", "start_pos": 193, "end_pos": 239, "type": "TASK", "confidence": 0.9224156141281128}]}, {"text": "The approach is fairly domain independent, as demonstrated by the systems achieving quite competitive results when applied to short text message data, i.e., input they were not originally trained on.", "labels": [], "entities": []}], "introductionContent": [{"text": "The informal texts in microblogs such as Twitter and on other social media represent challenges for traditional language processing systems.", "labels": [], "entities": []}, {"text": "The posts (\"tweets\") are limited to 140 characters and often contain misspellings, slang and abbreviations.", "labels": [], "entities": []}, {"text": "On the other hand, the posts are often opinionated in nature as a very result of their informal character, which has led Twitter to being a goldmine for sentiment analysis (SA).", "labels": [], "entities": [{"text": "sentiment analysis (SA)", "start_pos": 153, "end_pos": 176, "type": "TASK", "confidence": 0.8685227632522583}]}, {"text": "SA for longer texts, such as movie reviews, has been explored since the 1990s; 1 however, the limited amount of attributes in tweets makes the feature vectors shorter than in documents and the task of analysing them closely related to phrase-and sentence-level SA (;.", "labels": [], "entities": []}, {"text": "Hence there are no guarantees that algorithms that perform well on document-level SA will do as well on tweets.", "labels": [], "entities": [{"text": "document-level SA", "start_pos": 67, "end_pos": 84, "type": "TASK", "confidence": 0.5444915741682053}]}, {"text": "On the other hand, it is possible to exploit some of the special features of the web language, e.g., emoticons See; for overviews. and emotionally loaded abbreviations.", "labels": [], "entities": []}, {"text": "Thus the data will normally go through some preprocessing before any classification is attempted, e.g., by filtering out Twitter specific symbols and functions, in particular retweets (reposting another user's tweet), mentions ('@', tags used to mention another user), hashtags ('#', used to tag a tweet to a certain topic), emoticons, and URLs (linking to an external resource, e.g., a news article or a photo).", "labels": [], "entities": []}, {"text": "The first system to really use Twitter as a corpus was created as a student course project at Stanford (.", "labels": [], "entities": []}, {"text": "experimented with sentiment classification of tweets using Support Vector Machines and Conditional Random Fields, benchmarked with a Na\u00a8\u0131veNa\u00a8\u0131ve Bayes Classifier baseline, but were unable to beat the baseline.", "labels": [], "entities": [{"text": "sentiment classification of tweets", "start_pos": 18, "end_pos": 52, "type": "TASK", "confidence": 0.9267340749502182}]}, {"text": "Later, and as Twitter has grown in popularity, many other systems for Twitter Sentiment Analysis (TSA) have been developed (see, e.g.,).", "labels": [], "entities": [{"text": "Twitter Sentiment Analysis (TSA)", "start_pos": 70, "end_pos": 102, "type": "TASK", "confidence": 0.7633796681960424}]}, {"text": "Clearly, it is possible to classify the sentiment of tweets in a single step; however, the approach to TSA most used so far is a two-step strategy where the first step is subjectivity classification and the second step is polarity classification.", "labels": [], "entities": [{"text": "classify the sentiment of tweets", "start_pos": 27, "end_pos": 59, "type": "TASK", "confidence": 0.8600553154945374}, {"text": "TSA", "start_pos": 103, "end_pos": 106, "type": "TASK", "confidence": 0.9190387725830078}, {"text": "subjectivity classification", "start_pos": 171, "end_pos": 198, "type": "TASK", "confidence": 0.7186688631772995}, {"text": "polarity classification", "start_pos": 222, "end_pos": 245, "type": "TASK", "confidence": 0.7591963112354279}]}, {"text": "The goal of subjectivity classification is to separate subjective and objective statements.", "labels": [], "entities": [{"text": "subjectivity classification", "start_pos": 12, "end_pos": 39, "type": "TASK", "confidence": 0.7235414385795593}]}, {"text": "counted word frequencies in a subjective vs an objective set of tweets; the results showed that interjections and personal pronouns are the strongest indicators of subjectivity.", "labels": [], "entities": []}, {"text": "In general, these word classes, adverbs and (in particular) adjectives) have shown to be good subjectivity indicators, which has made part-of-speech (POS) tagging a reasonable technique for filtering out objective tweets.", "labels": [], "entities": [{"text": "part-of-speech (POS) tagging", "start_pos": 134, "end_pos": 162, "type": "TASK", "confidence": 0.6407473742961883}]}, {"text": "Early research on TSA showed that the challenging vocabulary made it harder to accurately tag tweets; however, report on using a POS tagger for marking tweets, performing with almost 90% accuracy.", "labels": [], "entities": [{"text": "TSA", "start_pos": 18, "end_pos": 21, "type": "TASK", "confidence": 0.8820170760154724}, {"text": "accuracy", "start_pos": 187, "end_pos": 195, "type": "METRIC", "confidence": 0.9975723624229431}]}, {"text": "Polarity classification is the task of separating the subjective statements into positives and negatives.", "labels": [], "entities": [{"text": "Polarity classification", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.6599366664886475}]}, {"text": "tried different solutions for tweet polarity classification, and found that the best performance came from using n-grams together with lexicon and microblog features.", "labels": [], "entities": [{"text": "tweet polarity classification", "start_pos": 30, "end_pos": 59, "type": "TASK", "confidence": 0.9122708837191263}]}, {"text": "Interestingly, performance dropped when a POS tagger was included.", "labels": [], "entities": []}, {"text": "They speculate that this can be due to the accuracy of the POS tagger itself, or that POS tagging just is less effective for analysing tweet polarity.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 43, "end_pos": 51, "type": "METRIC", "confidence": 0.9989469647407532}, {"text": "POS tagging", "start_pos": 86, "end_pos": 97, "type": "TASK", "confidence": 0.6691576540470123}, {"text": "analysing tweet polarity", "start_pos": 125, "end_pos": 149, "type": "TASK", "confidence": 0.7276616891225179}]}, {"text": "In this paper we will explore the application of a set of machine learning algorithms to the task of Twitter sentiment classification, comparing one-step and two-step approaches, and investigate a range of different preprocessing methods.", "labels": [], "entities": [{"text": "Twitter sentiment classification", "start_pos": 101, "end_pos": 133, "type": "TASK", "confidence": 0.7207062542438507}]}, {"text": "What we explicitly will not do, is to utilise a sentiment lexicon, even though many methods in TSA rely on lexica with a sentiment score for each word.", "labels": [], "entities": [{"text": "TSA", "start_pos": 95, "end_pos": 98, "type": "TASK", "confidence": 0.9151245355606079}]}, {"text": "Nielsen (2011) manually built a sentiment lexicon specialized for Twitter, while others have tried to induce such lexica automatically with good results (.", "labels": [], "entities": []}, {"text": "However, sentiment lexica -and in particular specialized Twitter sentiment lexica -make the classification more domain dependent.", "labels": [], "entities": []}, {"text": "Here we will instead aim to exploit domain independent approaches as far as possible, and thus abstain from using sentiment lexica.", "labels": [], "entities": []}, {"text": "The rest of the paper is laid out as follows: Section 2 introduces the twitter data sets used in the study.", "labels": [], "entities": []}, {"text": "Then Section 3 describes the system built for carrying out the twitter sentiment classification experiments, which in turn are reported and discussed in Sections 4 and 5.", "labels": [], "entities": [{"text": "twitter sentiment classification", "start_pos": 63, "end_pos": 95, "type": "TASK", "confidence": 0.7218142847220103}]}], "datasetContent": [{"text": "In order to run sentiment classification experiments, a general system was built.", "labels": [], "entities": [{"text": "sentiment classification", "start_pos": 16, "end_pos": 40, "type": "TASK", "confidence": 0.9732591211795807}]}, {"text": "It has a Sentiment Analysis API Layer which works as a thin extension of the Twitter API, sending all tweets received in parallel to a Sentiment Analysis Classifier server.", "labels": [], "entities": [{"text": "Sentiment Analysis API", "start_pos": 9, "end_pos": 31, "type": "TASK", "confidence": 0.8281534711519877}]}, {"text": "After classification, the SA API returns the same JSON structure as the Twitter API sends out, only with an additional attribute denoting the tweet's sentiment.", "labels": [], "entities": []}, {"text": "The Sentiment Analysis Classifier system consists of preprocessing and classification, described below.", "labels": [], "entities": [{"text": "Sentiment Analysis Classifier", "start_pos": 4, "end_pos": 33, "type": "TASK", "confidence": 0.895031750202179}]}, {"text": "Experiments were carried out using the platform introduced in the previous section, with models built on the training set of Table 1.", "labels": [], "entities": []}, {"text": "The testing system generates and trains different models based on a set of parameters, such as classification algorithm, preprocessing methods, whether or not to use inverse document frequency (IDF) or stop words.", "labels": [], "entities": [{"text": "inverse document frequency (IDF)", "start_pos": 166, "end_pos": 198, "type": "METRIC", "confidence": 0.789484515786171}]}, {"text": "A grid search option can be activated, so that a model is generated with the best possible parameter set for the given algorithm, using 10-fold cross validation.", "labels": [], "entities": []}, {"text": "displays the precision, recall, F 1 -score, and accuracy for each of the thirteen classifiers with the Dev 2 data set (see) used for evaluation.", "labels": [], "entities": [{"text": "precision", "start_pos": 13, "end_pos": 22, "type": "METRIC", "confidence": 0.9996960163116455}, {"text": "recall", "start_pos": 24, "end_pos": 30, "type": "METRIC", "confidence": 0.9992995262145996}, {"text": "F 1 -score", "start_pos": 32, "end_pos": 42, "type": "METRIC", "confidence": 0.9882616996765137}, {"text": "accuracy", "start_pos": 48, "end_pos": 56, "type": "METRIC", "confidence": 0.9996815919876099}, {"text": "Dev 2 data set", "start_pos": 103, "end_pos": 117, "type": "DATASET", "confidence": 0.8197154402732849}]}, {"text": "Note that most classifiers involving the NB algorithm perform badly, both in terms of accuracy and F-score.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 86, "end_pos": 94, "type": "METRIC", "confidence": 0.9995723366737366}, {"text": "F-score", "start_pos": 99, "end_pos": 106, "type": "METRIC", "confidence": 0.9970752000808716}]}, {"text": "This was observed for the other data sets as well.", "labels": [], "entities": []}, {"text": "Further, we can see that one-step classifiers did better than two-step models, with MaxEnt obtaining the best accuracy, but SVM a slightly better F-score.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 110, "end_pos": 118, "type": "METRIC", "confidence": 0.9995766282081604}, {"text": "F-score", "start_pos": 146, "end_pos": 153, "type": "METRIC", "confidence": 0.997097373008728}]}, {"text": "all classifiers were trained on the training set of A second grid search with the two best classifiers from the first search was performed instead using the smaller Dev 1 data set for evaluation.", "labels": [], "entities": [{"text": "Dev 1 data set", "start_pos": 165, "end_pos": 179, "type": "DATASET", "confidence": 0.872080609202385}]}, {"text": "The results for both the SVM and MaxEnt classifiers are shown in.", "labels": [], "entities": [{"text": "SVM", "start_pos": 25, "end_pos": 28, "type": "DATASET", "confidence": 0.812126874923706}]}, {"text": "With the Dev 1 data set, SVM performs much better than MaxEnt.", "labels": [], "entities": [{"text": "Dev 1 data set", "start_pos": 9, "end_pos": 23, "type": "DATASET", "confidence": 0.7980889230966568}]}, {"text": "The larger Dev 2 development set contains more neutral tweets than the Dev 1 set, which gives us reasons to believe that evaluating on the Dev 2 set favours the MaxEnt classifier.", "labels": [], "entities": [{"text": "Dev 2 development set", "start_pos": 11, "end_pos": 32, "type": "DATASET", "confidence": 0.8420184701681137}, {"text": "Dev 1 set", "start_pos": 71, "end_pos": 80, "type": "DATASET", "confidence": 0.7645215193430582}, {"text": "Dev 2 set", "start_pos": 139, "end_pos": 148, "type": "DATASET", "confidence": 0.7555456558863322}]}], "tableCaptions": [{"text": " Table 1: The data sets used in the experiments", "labels": [], "entities": []}, {"text": " Table 3: Best classifier performance (bold=best score;", "labels": [], "entities": []}, {"text": " Table 4: The NTNU systems in SemEval'13", "labels": [], "entities": [{"text": "SemEval'13", "start_pos": 30, "end_pos": 40, "type": "TASK", "confidence": 0.6300361752510071}]}]}