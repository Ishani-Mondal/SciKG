{"title": [{"text": "WBI-NER: The impact of domain-specific features on the performance of identifying and classifying mentions of drugs", "labels": [], "entities": [{"text": "WBI-NER", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.8133044242858887}]}], "abstractContent": [{"text": "Named entity recognition (NER) systems are often based on machine learning techniques to reduce the labor-intensive development of hand-crafted extraction rules and domain-dependent dictionaries.", "labels": [], "entities": [{"text": "Named entity recognition (NER)", "start_pos": 0, "end_pos": 30, "type": "TASK", "confidence": 0.795886680483818}]}, {"text": "Nevertheless, time-consuming feature engineering is often needed to achieve state-of-the-art performance.", "labels": [], "entities": []}, {"text": "In this study, we investigate the impact of such domain-specific features on the performance of recognizing and classifying mentions of pharmacological substances.", "labels": [], "entities": [{"text": "recognizing and classifying mentions of pharmacological substances", "start_pos": 96, "end_pos": 162, "type": "TASK", "confidence": 0.7879517589296613}]}, {"text": "We compare the performance of a system based on general features , which have been successfully applied to a wide range of NER tasks, with a system that additionally uses features generated from the output of an existing chemical NER tool and a collection of domain-specific resources.", "labels": [], "entities": [{"text": "NER tasks", "start_pos": 123, "end_pos": 132, "type": "TASK", "confidence": 0.923829972743988}]}, {"text": "We demonstrate that acceptable results can be achieved with the former system.", "labels": [], "entities": []}, {"text": "Still, our experiments show that using domain-specific features outperforms this general approach.", "labels": [], "entities": []}, {"text": "Our system ranked first in the SemEval-2013 Task 9.1: Recognition and classification of pharmacological substances.", "labels": [], "entities": [{"text": "SemEval-2013 Task 9.1", "start_pos": 31, "end_pos": 52, "type": "TASK", "confidence": 0.7585871815681458}, {"text": "Recognition and classification of pharmacological substances", "start_pos": 54, "end_pos": 114, "type": "TASK", "confidence": 0.7372646282116572}]}], "introductionContent": [{"text": "The accurate identification of drug mentions in text is an important prerequisite for many applications, including the retrieval of information about substances in drug development (e.g.), the identification of adverse drug effects (e.g.) and the recognition of drug-drug interactions (e.g.).", "labels": [], "entities": [{"text": "accurate identification of drug mentions in text", "start_pos": 4, "end_pos": 52, "type": "TASK", "confidence": 0.782036053282874}, {"text": "identification of adverse drug", "start_pos": 193, "end_pos": 223, "type": "TASK", "confidence": 0.8526704162359238}, {"text": "recognition of drug-drug interactions", "start_pos": 247, "end_pos": 284, "type": "TASK", "confidence": 0.8750009089708328}]}, {"text": "Given that most of the information related to drug research is covered by medical reports and pharmacological publications, computational methods for information extraction should be used to support this task.", "labels": [], "entities": [{"text": "information extraction", "start_pos": 150, "end_pos": 172, "type": "TASK", "confidence": 0.7432783246040344}]}, {"text": "The SemEval-2013 Task 9.1 competition 1 (Segura-) aims at a fair assessment on the state-of-the-art of tools that recognize and classify mentions of pharmacological substances in natural language texts -a task referred to as drug named entity recognition (NER).", "labels": [], "entities": [{"text": "recognize and classify mentions of pharmacological substances in natural language texts", "start_pos": 114, "end_pos": 201, "type": "TASK", "confidence": 0.6971215849572961}, {"text": "drug named entity recognition (NER)", "start_pos": 225, "end_pos": 260, "type": "TASK", "confidence": 0.757714854819434}]}, {"text": "The goal of participating teams is to recreate the gold annotation on a heldout part of an annotated corpus.", "labels": [], "entities": []}, {"text": "Four classes of entities have to be identified: Drug, DrugN, Group and Brand.", "labels": [], "entities": [{"text": "DrugN", "start_pos": 54, "end_pos": 59, "type": "DATASET", "confidence": 0.9227438569068909}]}, {"text": "Entities of class Drug denote any kind of drug that is approved for use in humans, whereas DrugN denotes substances that are not approved.", "labels": [], "entities": [{"text": "DrugN", "start_pos": 91, "end_pos": 96, "type": "DATASET", "confidence": 0.9152804017066956}]}, {"text": "Group are terms describing a group of drugs and Brand stands for drug names introduced by a pharmaceutical company.", "labels": [], "entities": [{"text": "Brand", "start_pos": 48, "end_pos": 53, "type": "METRIC", "confidence": 0.8635351061820984}]}, {"text": "The aim of this study is to examine whether it is worthwhile to implement domain-specific features for supporting drug NER.", "labels": [], "entities": [{"text": "NER", "start_pos": 119, "end_pos": 122, "type": "TASK", "confidence": 0.8438268899917603}]}, {"text": "The question we attempt to answer is whether such features really help in identifying and classifying mentions of drugs or whether a mostly domain-independent feature set, which can be applied to many other tasks, achieves a comparable performance.", "labels": [], "entities": [{"text": "identifying and classifying mentions of drugs", "start_pos": 74, "end_pos": 119, "type": "TASK", "confidence": 0.8490314483642578}]}], "datasetContent": [{"text": "We perform document-level 10-fold cross-validation (CV) on the training corpus to measure the impact of domain-specific features.", "labels": [], "entities": []}, {"text": "To ensure comparability between Run 1 and Run 2, we use the same splits for evaluation.", "labels": [], "entities": []}, {"text": "Furthermore, we train models on the complete training corpus and evaluate on the test corpus of DDI Task 9.1 for each run respectively.", "labels": [], "entities": [{"text": "DDI Task 9.1", "start_pos": 96, "end_pos": 108, "type": "DATASET", "confidence": 0.9135594765345255}]}, {"text": "In addition, we train a third model based on the best feature set determined with CV and use the entity mentions of the Task 9.2 test corpus, which also contains annotations of drug-drug interactions, as additional training data.", "labels": [], "entities": [{"text": "CV", "start_pos": 82, "end_pos": 84, "type": "DATASET", "confidence": 0.9212294816970825}, {"text": "Task 9.2 test corpus", "start_pos": 120, "end_pos": 140, "type": "DATASET", "confidence": 0.7542855739593506}]}, {"text": "Following the SemEval-2013 Task 9.1 metrics, we evaluate exact matching performance (correct entity boundaries) and strict matching performance (correct boundaries and correct type).", "labels": [], "entities": []}, {"text": "shows micro-average CV results for identifying and classifying mentions of pharmacological substances in the training corpus.", "labels": [], "entities": []}, {"text": "The performance varies drastically between different entity classes regardless of the feature set, e.g., Run 1 achieves an F 1 of 91.0% for Drug, but only 15.9% F 1 for DrugN.", "labels": [], "entities": [{"text": "F 1", "start_pos": 123, "end_pos": 126, "type": "METRIC", "confidence": 0.9952096045017242}, {"text": "F 1", "start_pos": 161, "end_pos": 164, "type": "METRIC", "confidence": 0.9875988364219666}, {"text": "DrugN", "start_pos": 169, "end_pos": 174, "type": "DATASET", "confidence": 0.9617246985435486}]}], "tableCaptions": [{"text": " Table 1: Example label sequence for the tokenized sentence MedLine.d110.s4 of the training corpus.", "labels": [], "entities": []}, {"text": " Table 3: Document-level 10-fold cross-validation  micro-average results on the training corpus.", "labels": [], "entities": []}, {"text": " Table 4: Results on the test corpus. \u2206F 1 denotes the F 1 pp difference to the preceding Run and # the number  of annotated mentions", "labels": [], "entities": [{"text": "F 1", "start_pos": 39, "end_pos": 42, "type": "METRIC", "confidence": 0.9741848707199097}, {"text": "F 1 pp difference", "start_pos": 55, "end_pos": 72, "type": "METRIC", "confidence": 0.8389282673597336}]}]}