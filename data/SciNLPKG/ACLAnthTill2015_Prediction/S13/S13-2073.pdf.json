{"title": [{"text": "UMCC_DLSI-(SA): Using a ranking algorithm and informal features to solve Sentiment Analysis in Twitter", "labels": [], "entities": [{"text": "UMCC_DLSI-(SA)", "start_pos": 0, "end_pos": 14, "type": "METRIC", "confidence": 0.7493411898612976}, {"text": "Sentiment Analysis", "start_pos": 73, "end_pos": 91, "type": "TASK", "confidence": 0.9518921673297882}]}], "abstractContent": [{"text": "In this paper, we describe the development and performance of the supervised system UMCC_DLSI-(SA).", "labels": [], "entities": [{"text": "UMCC_DLSI-(SA)", "start_pos": 84, "end_pos": 98, "type": "DATASET", "confidence": 0.8379888335863749}]}, {"text": "This system uses corpora where phrases are annotated as Positive, Negative, Objective, and Neutral, to achieve new sentiment resources involving word dictionaries with their associated polarity.", "labels": [], "entities": []}, {"text": "As a result, new sentiment inventories are obtained and applied in conjunction with detected informal patterns, to tackle the challenges posted in Task 2b of the Semeval-2013 competition.", "labels": [], "entities": []}, {"text": "Assessing the effectiveness of our application in sentiment classification, we obtained a 69% F-Measure for neutral and an average of 43% F-Measure for positive and negative using Tweets and SMS messages.", "labels": [], "entities": [{"text": "sentiment classification", "start_pos": 50, "end_pos": 74, "type": "TASK", "confidence": 0.9667768478393555}, {"text": "F-Measure", "start_pos": 94, "end_pos": 103, "type": "METRIC", "confidence": 0.998400866985321}, {"text": "F-Measure", "start_pos": 138, "end_pos": 147, "type": "METRIC", "confidence": 0.9960599541664124}]}], "introductionContent": [{"text": "Textual information has become one of the most important sources of data to extract useful and heterogeneous knowledge from.", "labels": [], "entities": []}, {"text": "Texts can provide factual information, such as: descriptions, lists of characteristics, or even instructions to opinionbased information, which would include reviews, emotions, or feelings.", "labels": [], "entities": []}, {"text": "These facts have motivated dealing with the identification and extraction of opinions and sentiments in texts that require special attention.", "labels": [], "entities": [{"text": "identification and extraction of opinions and sentiments", "start_pos": 44, "end_pos": 100, "type": "TASK", "confidence": 0.8399693284715924}]}, {"text": "Many researchers, such as () and many others have been working on this and related areas.", "labels": [], "entities": []}, {"text": "Related to assessment Sentiment Analysis (SA) systems, some international competitions have taken place.", "labels": [], "entities": [{"text": "assessment Sentiment Analysis (SA)", "start_pos": 11, "end_pos": 45, "type": "TASK", "confidence": 0.7899477481842041}]}, {"text": "Some of those include: Semeval-2010 (Task 18: Disambiguating Sentiment Ambiguous Adjectives 1 ) NTCIR (Multilingual Opinion Analysis Task (MOAT 2 )) TASS 3 (Workshop on Sentiment Analysis at SEPLN workshop) and Semeval-2013 (Task 2 4 Sentiment Analysis in Twitter) (.", "labels": [], "entities": [{"text": "Sentiment Analysis at SEPLN workshop", "start_pos": 169, "end_pos": 205, "type": "TASK", "confidence": 0.7809546828269959}, {"text": "Sentiment Analysis in Twitter)", "start_pos": 234, "end_pos": 264, "type": "TASK", "confidence": 0.8092238485813141}]}, {"text": "In this paper, we introduce a system for Task 2 b) of the Semeval-2013 competition.", "labels": [], "entities": [{"text": "Semeval-2013 competition", "start_pos": 58, "end_pos": 82, "type": "TASK", "confidence": 0.5428687930107117}]}], "datasetContent": [{"text": "In the construction of the sentiment resource, we used the annotated sentences provided by the corpora described in.", "labels": [], "entities": []}, {"text": "The resources listed in were selected to test the functionality of the words annotation proposal with subjectivity and objectivity.", "labels": [], "entities": []}, {"text": "Note that the shadowed rows correspond to constrained runs corpora: tweeti-bsub.dist_out.tsv 8 (dist), b1_tweeti-objorneub.dist_out.tsv 9 (objorneu), twitter-dev-input-B.tsv 10 (dev).", "labels": [], "entities": []}, {"text": "The resources from that include unconstrained runs corpora are: all the previously mentioned ones, Computational-intelligence 11 (CI) and stno 12 corpora.", "labels": [], "entities": []}, {"text": "The used sentiment lexicons are from the WordNetAffect_Categories 13 and opinion-words 14 files as shown in detail in.", "labels": [], "entities": [{"text": "WordNetAffect_Categories 13", "start_pos": 41, "end_pos": 68, "type": "DATASET", "confidence": 0.9385443478822708}]}, {"text": "Some issues were taken into account throughout this process.", "labels": [], "entities": []}, {"text": "For instance, after obtaining a contextual graph \u00ed \u00b5\u00ed\u00b0\u00ba, factotum words are present inmost of the involved sentences (i.e., verb \"to be\").", "labels": [], "entities": [{"text": "contextual graph \u00ed \u00b5\u00ed\u00b0\u00ba", "start_pos": 32, "end_pos": 55, "type": "METRIC", "confidence": 0.5142005980014801}]}, {"text": "This issue becomes very dangerous after applying the PageRank algorithm because the algorithm Resources described in strengthens the nodes possessing many linked elements.", "labels": [], "entities": []}, {"text": "For that reason, the subtractions \u00ed \u00b5\u00ed\u00b1\u0083\u00ed \u00b5\u00ed\u00b1\u009c\u00ed \u00b5\u00ed\u00b1 \u2212 \u00ed \u00b5\u00ed\u00b1\u0081\u00ed \u00b5\u00ed\u00b1\u0092\u00ed \u00b5\u00ed\u00b1\u0094 and \u00ed \u00b5\u00ed\u00b1\u0081\u00ed \u00b5\u00ed\u00b1\u0092\u00ed \u00b5\u00ed\u00b1\u0094 \u2212 \u00ed \u00b5\u00ed\u00b1\u0083\u00ed \u00b5\u00ed\u00b1\u009c\u00ed \u00b5\u00ed\u00b1 are applied, where the most frequent words in all contexts obtain high values.", "labels": [], "entities": []}, {"text": "The subtraction becomes a dumping factor.", "labels": [], "entities": []}, {"text": "As an example, when we take the verb \"to be\", before applying equation, the verb achieves the highest values in each subjective context graph ( \u00ed \u00b5\u00ed\u00b0\u00ba\u00ed \u00b5\u00ed\u00b1\u009d\u00ed \u00b5\u00ed\u00b1\u009c\u00ed \u00b5\u00ed\u00b1 and \u00ed \u00b5\u00ed\u00b0\u00ba\u00ed \u00b5\u00ed\u00b1\u009b\u00ed \u00b5\u00ed\u00b1\u0092\u00ed \u00b5\u00ed\u00b1\u0094) namely, 9.94 and 18.67 rank values respectively.", "labels": [], "entities": []}, {"text": "These values, once equation and), the verb \"to be\" achieves \u00ed \u00b5\u00ed\u00b1\u0083\u00ed \u00b5\u00ed\u00b1\u009c\u00ed \u00b5\u00ed\u00b1 = 0 , \u00ed \u00b5\u00ed\u00b1\u0081\u00ed \u00b5\u00ed\u00b1\u0092\u00ed \u00b5\u00ed\u00b1\u0094 = 0 and therefore \u00ed \u00b5\u00ed\u00b1\u0082\u00ed \u00b5\u00ed\u00b1\u008f\u00ed \u00b5\u00ed\u00b1\u0097\u00ed \u00b5\u00ed\u00b0\u00b4\u00ed \u00b5\u00ed\u00b1\u0099\u00ed \u00b5\u00ed\u00b1\u00a1 = 1 . Through this example, it seems as though we subjectively discarded words that appear frequently in both contexts (Positive and Negative).).", "labels": [], "entities": []}, {"text": "Constrained (Run1), Unconstrained (Run2), Correct(C), Incorrect (Inc).", "labels": [], "entities": [{"text": "Correct", "start_pos": 42, "end_pos": 49, "type": "METRIC", "confidence": 0.9755592942237854}, {"text": "Incorrect", "start_pos": 54, "end_pos": 63, "type": "METRIC", "confidence": 0.9620988368988037}]}, {"text": "In order to assess the effectiveness of our trained classifiers, we performed some evaluation tests.", "labels": [], "entities": []}, {"text": "shows relevant results obtained after applying our system to an environment (specific domain).", "labels": [], "entities": []}, {"text": "The best results were obtained with the restricted corpus.", "labels": [], "entities": []}, {"text": "The information used to increase the knowledge was not balanced or perhaps is of poor quality.", "labels": [], "entities": []}, {"text": "The test dataset evaluation is shown in, where system results are compared with the best results in each case.", "labels": [], "entities": []}, {"text": "We notice that the constrained run is better in almost every aspect.", "labels": [], "entities": []}, {"text": "In the few cases where it was lower, there was a minimal difference.", "labels": [], "entities": []}, {"text": "This suggests that the information used to increase our Sentiment Resource was unbalanced (high difference between quantity of tagged types of annotated phrases), or was of poor quality.", "labels": [], "entities": []}, {"text": "By comparing these results with the ones obtained by our system on the test dataset, we notice that on the test dataset, the results fell in the middle of the effectiveness scores.", "labels": [], "entities": []}, {"text": "After seeing these results), we assumed that our system performance is better in a controlled environment (or specific domain).", "labels": [], "entities": []}, {"text": "To make it more realistic, the system must be trained with a bigger and more balanced dataset.", "labels": [], "entities": []}, {"text": "shows the results obtained by our system while comparing them to the best results of Task 2b of Semeval-2013.", "labels": [], "entities": []}, {"text": "In, we can seethe difference between the best systems.", "labels": [], "entities": []}, {"text": "They are the ones in bold and underlined as target results.", "labels": [], "entities": []}, {"text": "These results have a difference of around 20 percentage points.", "labels": [], "entities": []}, {"text": "The grayed out ones correspond to our runs.", "labels": [], "entities": []}, {"text": "Corrects(C), Incorrect (Inc).", "labels": [], "entities": [{"text": "Incorrect", "start_pos": 13, "end_pos": 22, "type": "METRIC", "confidence": 0.6555882692337036}]}, {"text": "run descriptions are as follows: \uf0b7 UMCC_DLSI_(SA)-B-twitter-constrained (1_tw), \uf0b7 NRC-Canada-B-twitter-constrained (1_tw_cnd), \uf0b7 UMCC_DLSI_(SA)-B-twitter-unconstrained (2_tw), \uf0b7 teragram-B-twitter-unconstrained (2_tw_ter), \uf0b7 UMCC_DLSI_(SA)-B-SMS-constrained (1_sms), \uf0b7 NRC-Canada-B-SMS-constrained (1_sms_cnd), UMCC_DLSI_(SA)-B-SMSunconstrained (2_sms), \uf0b7 AVAYA-B-sms-unconstrained (2_sms_ava).", "labels": [], "entities": [{"text": "\uf0b7", "start_pos": 80, "end_pos": 81, "type": "DATASET", "confidence": 0.957882285118103}, {"text": "NRC-Canada-B-twitter-constrained", "start_pos": 82, "end_pos": 114, "type": "DATASET", "confidence": 0.6693760752677917}, {"text": "\uf0b7", "start_pos": 223, "end_pos": 224, "type": "DATASET", "confidence": 0.8345506191253662}, {"text": "UMCC", "start_pos": 225, "end_pos": 229, "type": "DATASET", "confidence": 0.478053480386734}, {"text": "\uf0b7", "start_pos": 267, "end_pos": 268, "type": "DATASET", "confidence": 0.9619150161743164}, {"text": "NRC-Canada-B-SMS-constrained", "start_pos": 269, "end_pos": 297, "type": "DATASET", "confidence": 0.6768723726272583}, {"text": "\uf0b7", "start_pos": 354, "end_pos": 355, "type": "DATASET", "confidence": 0.48740968108177185}, {"text": "AVAYA-B-sms-unconstrained", "start_pos": 356, "end_pos": 381, "type": "DATASET", "confidence": 0.3384421169757843}]}, {"text": "As we can see in the training and testing evaluation tables, our training stage offered more relevant scores than the best scores in Task2b.", "labels": [], "entities": []}, {"text": "This means that we need to identify the missed features between both datasets (training and testing).", "labels": [], "entities": []}, {"text": "For that reason, we decided to check how many words our system (more concretely, our Sentiment Resource) missed..", "labels": [], "entities": []}, {"text": "Quantity of words used by our system over the test dataset.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3. Corpora used to apply RA-SR. Positive (P),  Negative (N), Objective (Obj/O), Unknow (Unk), Total  (T), Constrained (C), Unconstrained (UC).", "labels": [], "entities": [{"text": "Objective (Obj/O)", "start_pos": 68, "end_pos": 85, "type": "METRIC", "confidence": 0.8810042440891266}, {"text": "Total", "start_pos": 101, "end_pos": 106, "type": "METRIC", "confidence": 0.9448708891868591}, {"text": "Unconstrained (UC)", "start_pos": 130, "end_pos": 148, "type": "METRIC", "confidence": 0.8984164297580719}]}, {"text": " Table 4. Sentiment Lexicons. Positive (P), Negative  (N) and Total (T).", "labels": [], "entities": []}, {"text": " Table 5. Training dataset evaluation using cross- validation (Logistic classifier", "labels": [], "entities": []}, {"text": " Table 6. Test dataset evaluation using official scores.", "labels": [], "entities": []}, {"text": " Table 7. Quantity of words used by our system over  the test dataset.", "labels": [], "entities": []}]}