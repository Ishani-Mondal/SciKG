{"title": [{"text": "AI-KU: Using Substitute Vectors and Co-Occurrence Modeling for Word Sense Induction and Disambiguation", "labels": [], "entities": [{"text": "Word Sense Induction and Disambiguation", "start_pos": 63, "end_pos": 102, "type": "TASK", "confidence": 0.7481554210186004}]}], "abstractContent": [{"text": "Word sense induction aims to discover different senses of a word from a corpus by using unsupervised learning approaches.", "labels": [], "entities": [{"text": "Word sense induction", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.7878263791402181}]}, {"text": "Once a sense inventory is obtained for an ambiguous word, word sense discrimination approaches choose the best-fitting single sense fora given context from the induced sense inventory.", "labels": [], "entities": [{"text": "word sense discrimination", "start_pos": 58, "end_pos": 83, "type": "TASK", "confidence": 0.7019546826680502}]}, {"text": "However, there may not be a clear distinction between one sense and another, although fora context, more than one induced sense can be suitable.", "labels": [], "entities": []}, {"text": "Graded word sense method allows for labeling a word in more than one sense.", "labels": [], "entities": []}, {"text": "In contrast to the most common approach which is to apply clustering or graph partitioning on a representation of first or second order co-occurrences of a word, we propose a system that creates a substitute vector for each target word from the most likely substitutes suggested by a statistical language model.", "labels": [], "entities": []}, {"text": "Word samples are then taken according to probabilities of these substitutes and the results of the co-occurrence model are clustered.", "labels": [], "entities": []}, {"text": "This approach outperforms the other systems on graded word sense induction task in SemEval-2013.", "labels": [], "entities": [{"text": "word sense induction task", "start_pos": 54, "end_pos": 79, "type": "TASK", "confidence": 0.7197222858667374}]}], "introductionContent": [{"text": "There exists several drawbacks of representing the word senses with a fixed-list of definitions of a manually constructed lexical database.", "labels": [], "entities": []}, {"text": "There is no guarantee that they reflect the exact meaning of a target word in a given context since they usually contain definitions that are too general).", "labels": [], "entities": []}, {"text": "More so, lexical databases often include many rare senses while missing corpus/domain-specific senses ().", "labels": [], "entities": []}, {"text": "The goal of Word Sense Induction (WSI) is to solve these problems by automatically discovering the meanings of a target word from a text, not pre-defined sense inventories.", "labels": [], "entities": [{"text": "Word Sense Induction (WSI)", "start_pos": 12, "end_pos": 38, "type": "TASK", "confidence": 0.7974076320727667}]}, {"text": "Word Sense Discrimination (WSD) approaches determine best-fitting sense among the meanings that are discovered for an ambiguous word.", "labels": [], "entities": [{"text": "Word Sense Discrimination (WSD)", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.7682746946811676}]}, {"text": "However, suggested that annotators often gave high ratings to more than one WordNet sense for the same occurrence.", "labels": [], "entities": [{"text": "WordNet sense", "start_pos": 76, "end_pos": 89, "type": "DATASET", "confidence": 0.9193240106105804}]}, {"text": "They introduced a novel annotation paradigm allowing that words have more than one sense with a degree of applicability.", "labels": [], "entities": []}, {"text": "Unlike previous SemEval tasks in which systems labeled a target word's meaning with only one sense, word sense induction task in SemEval-2013 relaxes this by allowing a target word to have more than one sense if applicable.", "labels": [], "entities": [{"text": "word sense induction", "start_pos": 100, "end_pos": 120, "type": "TASK", "confidence": 0.7382373611132304}]}, {"text": "Word sense induction approaches can be categorized into graph based models, bayesian, and vectorspace ones.", "labels": [], "entities": [{"text": "Word sense induction", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.6960191627343496}]}, {"text": "In graph-based approaches, every context word is represented as a vertex and if two context words co-occur in one or more instances of a target word, then two vertices are connected with an edge.", "labels": [], "entities": []}, {"text": "When the graph is obtained, one of the graph clustering algorithm is employed.", "labels": [], "entities": []}, {"text": "As a result, different partitions indicate the different senses of a target word).", "labels": [], "entities": []}, {"text": "explored the use of two graph algorithms for unsupervised induction and tagging of nominal word senses based on corpora.", "labels": [], "entities": [{"text": "tagging of nominal word senses", "start_pos": 72, "end_pos": 102, "type": "TASK", "confidence": 0.8286295056343078}]}, {"text": "Recently, proposed a graph-based model which achieved good results on word sense induction and discrimination task in proposed a Bayesian approach modeling the contexts of the ambiguous word as samples from a multinomial distribution over senses which are in turn characterized as distributions over words.", "labels": [], "entities": [{"text": "word sense induction", "start_pos": 70, "end_pos": 90, "type": "TASK", "confidence": 0.7485569715499878}]}, {"text": "Vector-space models, on the other hand, typically create context vector by using first or second order co-occurrences.", "labels": [], "entities": []}, {"text": "Once context vector has been constructed, different clustering algorithms maybe applied.", "labels": [], "entities": []}, {"text": "However, representing the context with first or second order co-occurrences can be difficult since there are plenty of parameters to be considered such as the order of occurrence, context window size, statistical significance of words in the context window and soon.", "labels": [], "entities": []}, {"text": "Instead of dealing with these, we suggest representing the context with the most likely substitutes determined by a statistical language model.", "labels": [], "entities": []}, {"text": "Statistical language models based on large corpora has been examined in for unsupervised word sense disambiguation and lexical substitution.", "labels": [], "entities": [{"text": "word sense disambiguation", "start_pos": 89, "end_pos": 114, "type": "TASK", "confidence": 0.6200016438961029}, {"text": "lexical substitution", "start_pos": 119, "end_pos": 139, "type": "TASK", "confidence": 0.7406178712844849}]}, {"text": "Moreover, the best results in unsupervised part-of-speech induction achieved by using substitute vectors.", "labels": [], "entities": [{"text": "part-of-speech induction", "start_pos": 43, "end_pos": 67, "type": "TASK", "confidence": 0.6828896403312683}]}, {"text": "In this paper, we propose a system that represents the context of each target word by using high probability substitutes according to a statistical language model.", "labels": [], "entities": []}, {"text": "These substitute words and their probabilities are used to create word pairs (instance id -substitute word) to feed our co-occurrence model.", "labels": [], "entities": []}, {"text": "The output of the co-occurrence model is clustered by kmeans algorithm.", "labels": [], "entities": []}, {"text": "Our systems perform well among other submitted systems in Rest of the paper is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 describes the provided datasets and evaluation measures of the task.", "labels": [], "entities": []}, {"text": "Section 3 gives details of our algorithm and is divided into five contiguous subsections that correspond to each step of our system.", "labels": [], "entities": []}, {"text": "In Section 4 we present the differences between our three systems and their performances.", "labels": [], "entities": []}, {"text": "Finally, Section 5 summarizes our work in this task.", "labels": [], "entities": []}, {"text": "The code to replicate this work is available at http://goo.gl/jPTZQ.", "labels": [], "entities": []}], "datasetContent": [{"text": "The test data for the graded word sense induction task in SemEval-2013 includes 50 terms containing 20 verbs, 20 nouns and 10 adjectives.", "labels": [], "entities": [{"text": "graded word sense induction task", "start_pos": 22, "end_pos": 54, "type": "TASK", "confidence": 0.7299514651298523}]}, {"text": "There area total of 4664 test instances provided.", "labels": [], "entities": []}, {"text": "All evaluation was performed on test instances only.", "labels": [], "entities": []}, {"text": "In addition, the organizers provided sense labeled trial data which can be used for tuning.", "labels": [], "entities": [{"text": "tuning", "start_pos": 84, "end_pos": 90, "type": "TASK", "confidence": 0.9630111455917358}]}, {"text": "This trial data is a redistribution of the Graded Sense and Usage data set provided by.", "labels": [], "entities": [{"text": "Graded Sense and Usage data set", "start_pos": 43, "end_pos": 74, "type": "DATASET", "confidence": 0.8093942900498708}]}, {"text": "It consists of 8 terms; 3 verbs, 3 nouns, and 2 adjectives all with moderate polysemy (4-7 senses).", "labels": [], "entities": []}, {"text": "Each term in trial data has 50 contexts, in total 400 instances provided.", "labels": [], "entities": []}, {"text": "Lastly, participants can use ukWaC 1 , a 2-billion word web-gathered corpus, for sense induction.", "labels": [], "entities": [{"text": "ukWaC 1", "start_pos": 29, "end_pos": 36, "type": "DATASET", "confidence": 0.9213236570358276}, {"text": "sense induction", "start_pos": 81, "end_pos": 96, "type": "TASK", "confidence": 0.8872700929641724}]}, {"text": "Furthermore, unlike in previous WSI tasks, organizers allow participants to use additional contexts not found in the ukWaC under the condition that they submit systems for both using only the ukWaC and with their augmented corpora.", "labels": [], "entities": [{"text": "WSI tasks", "start_pos": 32, "end_pos": 41, "type": "TASK", "confidence": 0.926225334405899}, {"text": "ukWaC", "start_pos": 117, "end_pos": 122, "type": "DATASET", "confidence": 0.9796706438064575}, {"text": "ukWaC", "start_pos": 192, "end_pos": 197, "type": "DATASET", "confidence": 0.9736589193344116}]}, {"text": "The gold-standard of test data was prepared using WordNet 3.1 by 10 annotators.", "labels": [], "entities": [{"text": "WordNet 3.1 by 10 annotators", "start_pos": 50, "end_pos": 78, "type": "DATASET", "confidence": 0.9162468791007996}]}, {"text": "Since WSI systems report their annotations in a different sense inventory than WordNet 3.1, a mapping procedure should be used first.", "labels": [], "entities": []}, {"text": "The organizers use the sense mapping procedure explained in.", "labels": [], "entities": [{"text": "sense mapping", "start_pos": 23, "end_pos": 36, "type": "TASK", "confidence": 0.7623763680458069}]}, {"text": "This procedure has adopted the supervised evaluation setting of past SemEval WSI Tasks, but the main difference is that the former takes into account applicability weights for each sense which is a necessary for graded word sense.", "labels": [], "entities": [{"text": "SemEval WSI Tasks", "start_pos": 69, "end_pos": 86, "type": "TASK", "confidence": 0.8280800183614095}]}, {"text": "Evaluation can be divided into two categories: (1) a traditional WSD task for Unsupervised WSD and WSI systems, (2) a clustering comparison setting that evaluates the similarity of the sense inventories for WSI systems.", "labels": [], "entities": []}, {"text": "WSD evaluation is made according to three objectives: \u2022 Their ability to detect which senses are applicable (Jaccard Index is used) \u2022 Their ability to rank the applicable senses according to the level of applicability (Weighted Kendall's \u03c4 is used) \u2022 Their ability to quantify the level of applicability for each sense (Weighted Normalized Discounted Cumulative Gain is used) Clustering comparison is made by using: \u2022 Fuzzy Normalized Mutual Information: It captures the alignment of the two clusterings independent of the cluster sizes and therefore serves as an effective measure of the ability of an approach to accurately model rare senses.", "labels": [], "entities": [{"text": "WSD evaluation", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.8899352848529816}, {"text": "Weighted Kendall's \u03c4", "start_pos": 219, "end_pos": 239, "type": "METRIC", "confidence": 0.6356726810336113}]}, {"text": "\u2022 Fuzzy B-Cubed: It provides an item-based evaluation that is sensitive to the cluster size skew and effectively captures the expected performance of the system on a dataset where the cluster (i.e., sense) distribution would be equivalent.", "labels": [], "entities": [{"text": "Fuzzy B-Cubed", "start_pos": 2, "end_pos": 15, "type": "METRIC", "confidence": 0.841120719909668}]}, {"text": "More details can be found on the task website.", "labels": [], "entities": []}, {"text": "In this section, we will discuss evaluation scores and the characteristics of the test and the trial data.", "labels": [], "entities": []}, {"text": "All three AI-KU systems followed the same procedures described in Section 3.", "labels": [], "entities": []}, {"text": "After clustering, some basic post-processing operations were performed for ai-ku(a1000) and ai-ku(r5-a1000).", "labels": [], "entities": []}, {"text": "For ai-ku(a1000), we added 1000 to all sense labels which were obtained from the clustering procedure; for ai-ku(r5-a1000), those sense labels occurred less than 5 times in clustering were removed since we considered them to be unreliable labels, afterwards we added 1000 for all remaining sense labels.", "labels": [], "entities": []}, {"text": "Supervised Metrics: shows the performance of our systems on the test data using all instances (verbs, nouns, adjectives) for all supervised measures and in comparison with the systems that performed best and worst, most frequent sense (MFS), all senses equally weighted, all senses average weighted, random-3, and random-n base-   lines.", "labels": [], "entities": []}, {"text": "Bold numbers indicate that ai-ku achieved best scores among all submitted systems.", "labels": [], "entities": []}, {"text": "Our systems performed generally well for all three supervised measures and slightly better for all submitted systems.", "labels": [], "entities": []}, {"text": "On the other hand, baselines achieved better scores than all participants.", "labels": [], "entities": []}, {"text": "More precisely, on sense detection objective, MFS baseline obtained 0.552 which is the top score, while the best submitted system could reach only 0.244.", "labels": [], "entities": [{"text": "sense detection", "start_pos": 19, "end_pos": 34, "type": "TASK", "confidence": 0.779900074005127}, {"text": "MFS baseline", "start_pos": 46, "end_pos": 58, "type": "DATASET", "confidence": 0.6963227391242981}]}, {"text": "Why is it the case that MFS had one of the worst sense detection score on trial data (see), but best on test data?", "labels": [], "entities": [{"text": "MFS", "start_pos": 24, "end_pos": 27, "type": "TASK", "confidence": 0.6723422408103943}]}, {"text": "Unlike the trial data, test data largely consists of only one sense instances, MFS usually gives correct answer.", "labels": [], "entities": [{"text": "MFS", "start_pos": 79, "end_pos": 82, "type": "DATASET", "confidence": 0.6449899077415466}]}, {"text": "illustrates the characteristics of the test and trial data.", "labels": [], "entities": []}, {"text": "Instances annotated with multiple sense had a very small fraction in the test data.", "labels": [], "entities": []}, {"text": "In fact, 517 instances in the test set were annotated with two senses (11%) and only 25 were annotated with three senses (0.5%  tives, All-Sense-eq-weighted outperformed all other systems.", "labels": [], "entities": []}, {"text": "The reason is the same as the above.", "labels": [], "entities": []}, {"text": "This baseline ranks all senses equally and since most instances had been annotated only one sense, the other wrong senses were tied and placed at the second position in ranking.", "labels": [], "entities": []}, {"text": "As a result, this baseline achieved the highest score.", "labels": [], "entities": []}, {"text": "Finally, for quantifying the level of applicability for each sense, Weighted NDCG was employed.", "labels": [], "entities": [{"text": "NDCG", "start_pos": 77, "end_pos": 81, "type": "METRIC", "confidence": 0.5435240864753723}]}, {"text": "ai-ku outperformed other submitted systems, but top score was achieved by all-sense-avgweighted baseline.", "labels": [], "entities": []}, {"text": "Addition to these results, organizers provided scores for instances which have only one sense.", "labels": [], "entities": []}, {"text": "This setting contains 89% of the test data.", "labels": [], "entities": []}, {"text": "shows supervised and unsupervised scores for all single-sense instances.", "labels": [], "entities": []}, {"text": "Our base system, aiku, outperformed all other system and all baselines for FScore.", "labels": [], "entities": [{"text": "FScore", "start_pos": 75, "end_pos": 81, "type": "TASK", "confidence": 0.6060888171195984}]}, {"text": "Moreover, it also achieved the second best score (0.045) for Fuzzy NMI.", "labels": [], "entities": [{"text": "Fuzzy NMI", "start_pos": 61, "end_pos": 70, "type": "DATASET", "confidence": 0.8647105395793915}]}, {"text": "Only one baseline (one sense per instance) obtained slightly better score (0.048) for this metric.", "labels": [], "entities": []}, {"text": "For Fuzzy B-Cubed, ai-ku(r5-a1000) obtained 0.421 which is the third best score.", "labels": [], "entities": [{"text": "Fuzzy B-Cubed", "start_pos": 4, "end_pos": 17, "type": "TASK", "confidence": 0.5075637698173523}]}, {"text": "All instances ai-ku 7.72 ai-ku(a1000) 7.72 ai-ku(r5-a1000) 3.11 Net sense.", "labels": [], "entities": []}, {"text": "ai-ku performed best for Fuzzy NMI among other systems included baselines.", "labels": [], "entities": [{"text": "Fuzzy NMI", "start_pos": 25, "end_pos": 34, "type": "DATASET", "confidence": 0.7624379992485046}]}, {"text": "For Fuzzy B-Cubed, ai-ku(r5a1000) outperformed random-3 and random-n baselines.", "labels": [], "entities": []}, {"text": "depicts the performance of our systems, best and worst systems as well as the random baselines.", "labels": [], "entities": []}, {"text": "The best scores for the graded word sense induction task in SemEval-2013 are mostly achieved by baselines in supervised setting.", "labels": [], "entities": [{"text": "word sense induction task", "start_pos": 31, "end_pos": 56, "type": "TASK", "confidence": 0.7492018789052963}]}, {"text": "Major problem is that there is huge sense differences between test and trial data regarding to number of sense distribution.", "labels": [], "entities": []}, {"text": "Participants that used trial data as for parameter tuning and picking the best algorithm achieved lower scores than baselines since test data does not show properties of trial data.", "labels": [], "entities": [{"text": "parameter tuning", "start_pos": 41, "end_pos": 57, "type": "TASK", "confidence": 0.7416064441204071}]}, {"text": "Consequently, ai-ku systems produce significantly more senses than the gold-standard (see), and this mainly deteriorates our performance.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: The most likely substitutes for meet", "labels": [], "entities": []}, {"text": " Table 4: Supervised results on the trial set using median  gold-standard (JI: Jaccard Index FScore, WKT: Weighted  Kendall's Tau FScore, WNDCG: Weighted Normalized  Discounted Cumulative Gain FScore)", "labels": [], "entities": [{"text": "JI: Jaccard Index FScore", "start_pos": 75, "end_pos": 99, "type": "METRIC", "confidence": 0.5967331647872924}, {"text": "Weighted  Kendall's Tau FScore", "start_pos": 106, "end_pos": 136, "type": "METRIC", "confidence": 0.5947080373764038}, {"text": "WNDCG", "start_pos": 138, "end_pos": 143, "type": "METRIC", "confidence": 0.4845694303512573}]}, {"text": " Table 5: Supervised results on the test set. (Submitted- Best indicates the best scores among all submitted sys- tem. All-Best indicates the best scores among all sub- mitted systems and baselines. JI: Jaccard Index FS- core, WKT: Weighted Kendall's Tau FScore, WNDCG:  Weighted Normalized Discounted Cumulative Gain FS- core)", "labels": [], "entities": [{"text": "Jaccard Index FS- core", "start_pos": 203, "end_pos": 225, "type": "METRIC", "confidence": 0.5444383263587952}, {"text": "WKT: Weighted Kendall's Tau FScore", "start_pos": 227, "end_pos": 261, "type": "DATASET", "confidence": 0.6158255764416286}, {"text": "WNDCG", "start_pos": 263, "end_pos": 268, "type": "DATASET", "confidence": 0.6700620055198669}]}, {"text": " Table 6: Average number of senses and average sense  perplexity for trial and test data", "labels": [], "entities": []}, {"text": " Table 7: Supervised and unsupervised results on the test  set using instances which have only one sense. Bold num- bers indicate that ai-ku achieved the best submitted sys- tem scores. (FScore: Supervised FScore, FNMI: Fuzzy  Normalized Mutual Information, FB-Cubed: Fuzzy B- Cubed FScore)", "labels": [], "entities": [{"text": "FScore", "start_pos": 187, "end_pos": 193, "type": "METRIC", "confidence": 0.8680792450904846}, {"text": "FNMI", "start_pos": 214, "end_pos": 218, "type": "DATASET", "confidence": 0.7675150632858276}, {"text": "FB-Cubed", "start_pos": 258, "end_pos": 266, "type": "DATASET", "confidence": 0.7849183082580566}]}]}