{"title": [{"text": "HeidelTime: Tuning English and Developing Spanish Resources for TempEval-3", "labels": [], "entities": [{"text": "TempEval-3", "start_pos": 64, "end_pos": 74, "type": "TASK", "confidence": 0.5608431696891785}]}], "abstractContent": [{"text": "In this paper, we describe our participation in the TempEval-3 challenge.", "labels": [], "entities": [{"text": "TempEval-3 challenge", "start_pos": 52, "end_pos": 72, "type": "TASK", "confidence": 0.6541048884391785}]}, {"text": "With our multilingual temporal tagger HeidelTime, we addressed task A, the extraction and normaliza-tion of temporal expressions for English and Spanish.", "labels": [], "entities": []}, {"text": "Exploiting HeidelTime's strict separation between source code and language-dependent parts, we tuned HeidelTime's existing English resources and developed new Spanish resources.", "labels": [], "entities": []}, {"text": "For both languages, we achieved the best results among all participants for task A, the combination of extraction and normalization.", "labels": [], "entities": []}, {"text": "Both the improved English and the new Spanish resources are publicly available with HeidelTime.", "labels": [], "entities": [{"text": "HeidelTime", "start_pos": 84, "end_pos": 94, "type": "DATASET", "confidence": 0.9674252271652222}]}], "introductionContent": [{"text": "The task of temporal annotation, which is addressed in the TempEval-3 challenge, consists of three subtasks: (A) the extraction and normalization of temporal expressions, (B) event extraction, and (C) the annotation of temporal relations . This makes sub-task A, i.e., temporal tagging, a prerequisite for the full task of temporal annotating documents.", "labels": [], "entities": [{"text": "temporal annotation", "start_pos": 12, "end_pos": 31, "type": "TASK", "confidence": 0.6531314849853516}, {"text": "event extraction", "start_pos": 175, "end_pos": 191, "type": "TASK", "confidence": 0.7159127444028854}, {"text": "temporal tagging", "start_pos": 269, "end_pos": 285, "type": "TASK", "confidence": 0.6900085508823395}]}, {"text": "In addition, temporal tagging is important for many further natural language processing and understanding tasks, and can also be exploited for search and exploration scenarios in information retrieval).", "labels": [], "entities": [{"text": "temporal tagging", "start_pos": 13, "end_pos": 29, "type": "TASK", "confidence": 0.6987932622432709}, {"text": "natural language processing and understanding", "start_pos": 60, "end_pos": 105, "type": "TASK", "confidence": 0.6751985788345337}]}, {"text": "In the context of the TempEval-2 challenge, we developed our temporal tagger HeidelTime, which achieved the best results for the extraction and normalization of temporal expressions for English documents.", "labels": [], "entities": []}, {"text": "For our work on multilingual information retrieval (e.g., ), we extended HeidelTime with a focus on supporting the simple integration of further languages).", "labels": [], "entities": [{"text": "multilingual information retrieval", "start_pos": 16, "end_pos": 50, "type": "TASK", "confidence": 0.6167962650458018}]}, {"text": "For TempEval-3, we now tuned HeidelTime's English resources and developed new Spanish resources to address both languages that are part of TempEval-3.", "labels": [], "entities": []}, {"text": "As the evaluation results demonstrate, HeidelTime outperforms the systems of all other participants for the full task of temporal tagging by achieving high quality results for the extraction and normalization for English and Spanish.", "labels": [], "entities": [{"text": "temporal tagging", "start_pos": 121, "end_pos": 137, "type": "TASK", "confidence": 0.650023952126503}]}, {"text": "The remainder of the paper is structured as follows: We explain HeidelTime's system architecture in Section 2.", "labels": [], "entities": []}, {"text": "Section 3 covers the tuning of HeidelTime's English and the development of the Spanish resources.", "labels": [], "entities": [{"text": "HeidelTime's English", "start_pos": 31, "end_pos": 51, "type": "DATASET", "confidence": 0.9045537908871969}]}, {"text": "Finally, we discuss the evaluation results in Section 4, and conclude the paper in Section 5.", "labels": [], "entities": []}], "datasetContent": [{"text": "For the extraction task, precision (P), recall (R), and f 1 -score (F1) are used for strict and relaxed matching.", "labels": [], "entities": [{"text": "precision (P)", "start_pos": 25, "end_pos": 38, "type": "METRIC", "confidence": 0.9463241994380951}, {"text": "recall (R)", "start_pos": 40, "end_pos": 50, "type": "METRIC", "confidence": 0.9520127326250076}, {"text": "f 1 -score (F1)", "start_pos": 56, "end_pos": 71, "type": "METRIC", "confidence": 0.9038889578410557}]}, {"text": "The value F1 and type F1 measures combine relaxed matching with correct normalization.", "labels": [], "entities": [{"text": "F1", "start_pos": 10, "end_pos": 12, "type": "METRIC", "confidence": 0.9471750259399414}, {"text": "type F1", "start_pos": 17, "end_pos": 24, "type": "METRIC", "confidence": 0.9207589328289032}]}, {"text": "Systems are ranked by value F1 (value).", "labels": [], "entities": [{"text": "F1", "start_pos": 28, "end_pos": 30, "type": "METRIC", "confidence": 0.9971902966499329}]}, {"text": "shows the results on the Aquaint (a), TimeBank (b), and Spanish training corpora (c).", "labels": [], "entities": [{"text": "Aquaint", "start_pos": 25, "end_pos": 32, "type": "DATASET", "confidence": 0.5859699249267578}, {"text": "TimeBank", "start_pos": 38, "end_pos": 46, "type": "DATASET", "confidence": 0.8350563645362854}, {"text": "Spanish training corpora", "start_pos": 56, "end_pos": 80, "type": "DATASET", "confidence": 0.8193620244661967}]}, {"text": "On both English corpora, HeidelTime's TempEval-3 tuned version outperforms the other two versions.", "labels": [], "entities": []}, {"text": "The big differences between the two English corpora are rather due to the better annotation quality of TimeBank than due to different challenges in the documents of the two corpora.", "labels": [], "entities": [{"text": "TimeBank", "start_pos": 103, "end_pos": 111, "type": "DATASET", "confidence": 0.9697661399841309}]}, {"text": "The evaluation results on the test data are presented in.", "labels": [], "entities": []}, {"text": "For English, HeidelTime's TempEval-3 tuned version achieves the best results, and all three HeidelTime versions outperform the systems of the eight other participating teams with a total number of 21 submissions (task A ranking measure value F1).", "labels": [], "entities": [{"text": "task A ranking measure value F1", "start_pos": 213, "end_pos": 244, "type": "METRIC", "confidence": 0.6887555867433548}]}, {"text": "For comparison, the results of the next best system (NavyTime) is listed in(a).", "labels": [], "entities": [{"text": "NavyTime", "start_pos": 53, "end_pos": 61, "type": "DATASET", "confidence": 0.9648622870445251}]}, {"text": "For Spanish, we highly outperform the other two systems, as shown in(b).", "labels": [], "entities": []}, {"text": "In order to be able to interpret HeidelTime's results on the training and test data, we performed an error analysis (TimeBank and Spanish training corpus).", "labels": [], "entities": [{"text": "TimeBank", "start_pos": 117, "end_pos": 125, "type": "DATASET", "confidence": 0.9222824573516846}, {"text": "Spanish training corpus", "start_pos": 130, "end_pos": 153, "type": "DATASET", "confidence": 0.7374033629894257}]}, {"text": "The most important findings are: (i) For a rule-based system, HeidelTime's recall is relatively low (many false negatives; FN).", "labels": [], "entities": [{"text": "recall", "start_pos": 75, "end_pos": 81, "type": "METRIC", "confidence": 0.9952449202537537}, {"text": "FN", "start_pos": 123, "end_pos": 125, "type": "METRIC", "confidence": 0.9965747594833374}]}, {"text": "However, note that several FN are intentional.", "labels": [], "entities": [{"text": "FN", "start_pos": 27, "end_pos": 29, "type": "TASK", "confidence": 0.464192271232605}]}, {"text": "55% and 29% of 117 and 149 FN in the English and Spanish training corpora are due to imprecise expressions (some time; the latest period).", "labels": [], "entities": [{"text": "English and Spanish training corpora", "start_pos": 37, "end_pos": 73, "type": "DATASET", "confidence": 0.7386324644088745}]}, {"text": "These are difficult to normalize correctly, e.g., sometime can refer to seconds or years.", "labels": [], "entities": []}, {"text": "To guarantee high quality normalization, we do not extract expressions that cannot be normalized correctly with high probability.", "labels": [], "entities": []}, {"text": "(ii) There is a trade-off between precision and recall due to expressions referring to past, present, or future (X REF).", "labels": [], "entities": [{"text": "precision", "start_pos": 34, "end_pos": 43, "type": "METRIC", "confidence": 0.9993135929107666}, {"text": "recall", "start_pos": 48, "end_pos": 54, "type": "METRIC", "confidence": 0.9979208111763}, {"text": "X REF)", "start_pos": 113, "end_pos": 119, "type": "METRIC", "confidence": 0.7554951906204224}]}, {"text": "These are annotated either only in some contexts or inconsistently throughout the train-  The main sources for incorrect value normalization of underspecified expressions (Feb. 1; Monday) are wrongly detected reference times or relations to them (e.g., due to wrong tense identification), annotation errors in the corpora (e.g., last week annotated as WXX instead of the week it is referring to), granularity errors (e.g., a year ago can refer to a day, month, quarter, or year), and ambiguities (e.g., the year can be a duration or a specific year).", "labels": [], "entities": []}, {"text": "(iv) Some expressions in the Spanish test set were extracted and normalized correctly although no similar expressions exist in the Spanish training data.", "labels": [], "entities": [{"text": "Spanish test set", "start_pos": 29, "end_pos": 45, "type": "DATASET", "confidence": 0.8874290784200033}, {"text": "Spanish training data", "start_pos": 131, "end_pos": 152, "type": "DATASET", "confidence": 0.5863467156887054}]}, {"text": "Here, the Spanish resources highly benefited from the high quality English resources as starting point of the development process, and from HeidelTime's language-independent normalization strategies.", "labels": [], "entities": []}, {"text": "(v) A reoccurring error in the English test set is that HeidelTime matches and normalizes expressions such as two days earlier while only two days should be annotated according to TimeML.", "labels": [], "entities": [{"text": "English test set", "start_pos": 31, "end_pos": 47, "type": "DATASET", "confidence": 0.8445416291554769}, {"text": "TimeML", "start_pos": 180, "end_pos": 186, "type": "DATASET", "confidence": 0.9595672488212585}]}, {"text": "This results in a relaxed match with false type and value.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Results on training data ranked by value F1.", "labels": [], "entities": [{"text": "F1", "start_pos": 51, "end_pos": 53, "type": "METRIC", "confidence": 0.9918475151062012}]}, {"text": " Table 2: TempEval-3 task A evaluation results ranked by  value F1 (* next best: NavyTime).", "labels": [], "entities": [{"text": "NavyTime", "start_pos": 81, "end_pos": 89, "type": "DATASET", "confidence": 0.9419909119606018}]}]}