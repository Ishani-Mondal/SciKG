{"title": [{"text": "Beyond N-Grams: Can Linguistic Sophistication Improve Language Modeling?", "labels": [], "entities": [{"text": "Linguistic Sophistication Improve Language Modeling", "start_pos": 20, "end_pos": 71, "type": "TASK", "confidence": 0.7709211707115173}]}], "abstractContent": [{"text": "It seems obvious that a successful model of natural language would incorporate a great deal of both linguistic and world knowledge.", "labels": [], "entities": []}, {"text": "Interestingly, state of the art language models for speech recognition are based on a very crude linguistic model, namely conditioning the probability of a word on a small fixed number of preceding words.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 52, "end_pos": 70, "type": "TASK", "confidence": 0.7210316807031631}]}, {"text": "Despite many attempts to incorporate more sophisticated information into the models, the n-gram model remains the state of the art, used in virtually all speech recognition systems.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 154, "end_pos": 172, "type": "TASK", "confidence": 0.737824559211731}]}, {"text": "In this paper we address the question of whether there is hope in improving language modeling by incorporating more sophisticated linguistic and world knowledge, or whether the n-grams are already capturing the majority of the information that can be employed.", "labels": [], "entities": [{"text": "language modeling", "start_pos": 76, "end_pos": 93, "type": "TASK", "confidence": 0.7004057765007019}]}], "introductionContent": [{"text": "N-gram language models are very crude linguistic models that attempt to capture the constraints of language by simply conditioning the probability of a word on a small fixed number of predecessors.", "labels": [], "entities": []}, {"text": "It is rather frustrating to language engineers that the n-gram model is the workhorse of virtually every speech recognition system.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 105, "end_pos": 123, "type": "TASK", "confidence": 0.7185930758714676}]}, {"text": "Over the years, there have been many attempts to improve language models by utilizing linguistic information, but these methods have not been able to achieve significant improvements over the n-gram.", "labels": [], "entities": []}, {"text": "The insufficiency of Markov models has been known for many years (see).", "labels": [], "entities": []}, {"text": "It is easy to construct examples where a trigram model fails and a more sophisticated model could succeed.", "labels": [], "entities": []}, {"text": "For instance, in the sentence : The dog on the hill barked, the word barked would be assigned a low probability by a trigram model.", "labels": [], "entities": []}, {"text": "However, a linguistic model could determine that dog is the head of the noun phrase preceding barked and therefore assign barked a high probability, since P(barkedldog) is high.", "labels": [], "entities": [{"text": "P(barkedldog)", "start_pos": 155, "end_pos": 168, "type": "METRIC", "confidence": 0.8797870129346848}]}, {"text": "Using different sources of rich linguistic information will help speech recognition if the phenomena they capture are prevalent and they involve instances where the recognizer makes errors.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 65, "end_pos": 83, "type": "TASK", "confidence": 0.8589684963226318}]}, {"text": "~ In this paper we first give a brief overview of some recent attempts at incorporating linguistic information into language models.", "labels": [], "entities": []}, {"text": "Then we discuss experiments which give some insight into what aspects of language hold most promise for improving the accuracy of speech recognizers.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 118, "end_pos": 126, "type": "METRIC", "confidence": 0.9972158670425415}, {"text": "speech recognizers", "start_pos": 130, "end_pos": 148, "type": "TASK", "confidence": 0.6620834916830063}]}], "datasetContent": [{"text": "In an attempt to gain insight into what linguistic knowledge we should be exploring to improve language models for speech recognition, we ran experiments where people tried to improve the output of speech recognition systems and then recorded what types of knowledge they used in doing so.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 115, "end_pos": 133, "type": "TASK", "confidence": 0.7683313190937042}]}, {"text": "We hoped to both assess how much gain might be expected from very sophisticated models and to determine just what information sources could contribute to this gain.", "labels": [], "entities": []}, {"text": "People were given the ordered list of the ten most likely hypotheses for an utterance according to the recognizer.", "labels": [], "entities": []}, {"text": "They were then asked to choose from the ten-best list the hypothesis that they thought would have the lowest word error rate, in other words, to try to determine which hypothesis is closest to the truth.", "labels": [], "entities": [{"text": "word error rate", "start_pos": 109, "end_pos": 124, "type": "METRIC", "confidence": 0.8026938041051229}]}, {"text": "Often, the truth is not present in the 10-best list.", "labels": [], "entities": []}, {"text": "An example 5-best list from the Wall Street Journal corpus is shown in.", "labels": [], "entities": [{"text": "Wall Street Journal corpus", "start_pos": 32, "end_pos": 58, "type": "DATASET", "confidence": 0.9774747937917709}]}, {"text": "Four subjects were used in this experiment, and each subject was presented with 75 10-best lists from three different speech recognition systems (225 instances total per subject).", "labels": [], "entities": []}, {"text": "From this experiment, we hoped to gauge what the upper bound is on how much we could improve upon state of the art by using very rich models) For our experiments, we used three different speech recognizers, trained respectively on Switchboard (spontaneous speech), Broadcast News (recorded news broadcasts) and Wall Street Journal data.", "labels": [], "entities": [{"text": "Broadcast News (recorded news broadcasts", "start_pos": 265, "end_pos": 305, "type": "DATASET", "confidence": 0.8855170408884684}, {"text": "Wall Street Journal data", "start_pos": 311, "end_pos": 335, "type": "DATASET", "confidence": 0.8802617937326431}]}, {"text": "4 The word error rates of the recognizers for each corpus are shown in the first line of.", "labels": [], "entities": [{"text": "word error rates", "start_pos": 6, "end_pos": 22, "type": "METRIC", "confidence": 0.6787044207255045}]}, {"text": "The human subjects were presented with the ten-best lists.", "labels": [], "entities": []}, {"text": "Sentences within each ten-best list were aligned to make it easier to compare them.", "labels": [], "entities": []}, {"text": "In addition to choosing the most appropriate selection from the 10-best list, subjects were also allowed to posit a string not in the list by editing any of the strings in the 10-best list in anyway they chose.", "labels": [], "entities": []}, {"text": "For each sample, subjects were asked to determine what types of information were used in deciding.", "labels": [], "entities": []}, {"text": "This was done by presenting the subjects with a set of check boxes, and asking them to check all that applied.", "labels": [], "entities": []}, {"text": "A list of the options presented to the human can be found in.", "labels": [], "entities": []}, {"text": "Subjects were provided with a detailed explanation, as well as examples, for each of these options .5", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1  Word Error Rate: Recognizer,  Oracle and Human", "labels": [], "entities": [{"text": "Word Error Rate", "start_pos": 10, "end_pos": 25, "type": "METRIC", "confidence": 0.7005463242530823}]}, {"text": " Table 2 Human Gain Relative to Recognizer  and Oracle", "labels": [], "entities": []}]}