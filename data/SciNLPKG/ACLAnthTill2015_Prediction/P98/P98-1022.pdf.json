{"title": [{"text": "A Probabilistic Corpus-Driven Model for Lexical-Functional Analysis", "labels": [], "entities": [{"text": "Lexical-Functional Analysis", "start_pos": 40, "end_pos": 67, "type": "TASK", "confidence": 0.8511188626289368}]}], "abstractContent": [{"text": "We develop a Data-Oriented Parsing (DOP) model based on the syntactic representations of Lexical-Functional Grammar (LFG).", "labels": [], "entities": []}, {"text": "We start by summarizing the original DOP model for tree representations and then show how it can be extended with corresponding functional structures.", "labels": [], "entities": []}, {"text": "The resulting LFG-DOP model triggers anew, corpus-based notion of grammaticality, and its probability models exhibit interesting behavior with respect to specificity and the interpretation of ill-formed strings.", "labels": [], "entities": []}], "introductionContent": [{"text": "Data-Oriented Parsing (DOP) models of natural language embody the assumption that human language perception and production works with representations of past language experiences, rather than with abstract grammar rules (cf.).", "labels": [], "entities": []}, {"text": "DOP models therefore maintain large corpora of linguistic representations of previously occurring utterances.", "labels": [], "entities": []}, {"text": "New utterances are analyzed by combining (arbitrarily large) fragments from the corpus; the occurrence-frequencies of the fragments are used to determine which analysis is the most probable one.", "labels": [], "entities": []}, {"text": "In accordance with the general DOP architecture outlined by, a particular DOP model is described by specifying settings for the following four parameters: \u2022 a formal definition of a well-formed representation for utterance analyses, \u2022 a set of decomposition operations that divide a given utterance analysis into a set of fragments, \u2022 a set of composition operations by which such fragments maybe recombined to derive an analysis of anew utterance, and \u2022 a definition of a probability model that indicates how the probability of anew utterance analysis is computed on the basis of the probabilities of the fragments that combine to make it up.", "labels": [], "entities": [{"text": "utterance analyses", "start_pos": 213, "end_pos": 231, "type": "TASK", "confidence": 0.7936593890190125}]}, {"text": "Previous instantiations of the DOP architecture were based on utterance-analyses represented as surface phrase-structure trees CTree-DOP\", e.g.).", "labels": [], "entities": []}, {"text": "Tree-DOP uses two decomposition operations that produce connected subtrees of utterance representations: (1) the Root operation selects any node of a tree to be the root of the new subtree and erases all nodes except the selected node and the nodes it dominates; (2) the Frontier operation then chooses a set (possibly empty) of nodes in the new subtree different from its root and erases all subtrees dominated by the chosen nodes.", "labels": [], "entities": []}, {"text": "The only composition operation used by Tree-DOP is a node-substitution operation that replaces the left-most nonterminal frontier node in a subtree with a fragment whose root category matches the category of the frontier node.", "labels": [], "entities": []}, {"text": "Thus Tree-DOP provides treerepresentations for new utterances by combining fragments from a corpus of phrase structure trees.", "labels": [], "entities": []}, {"text": "A Tree-DOP representation R can typically be derived in many different ways.", "labels": [], "entities": []}, {"text": "If each derivation D has a probability P(D), then the probability of deriving R is the sum of the individual derivation probabilities: P(R) = ~D derives R P(D) A Tree-DOP derivation D = <tl, t2 ...", "labels": [], "entities": []}, {"text": "tk> is produced by a stochastic branching process.", "labels": [], "entities": []}, {"text": "It starts by randomly choosing a fragment tl labeled with the initial category (e.g. S).", "labels": [], "entities": []}, {"text": "At each subsequent step, a next fragment is chosen at random from among the set of competitors for composition into the current subtree.", "labels": [], "entities": []}, {"text": "The process stops when a tree results with no nonterminal leaves.", "labels": [], "entities": []}, {"text": "Let CP(tlCS) denote the probability of choosing a tree t from a competition set CS containing t.", "labels": [], "entities": [{"text": "CP(tlCS)", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.8810445964336395}]}, {"text": "Then the probability of a derivation is P(<tl, t2 ... tk>) = l']iCP where the competition probability CP(t ICS) is given by CP(t I CS) = P(t) / :El, e CS P(t') Here, P(t) is the fragment probability fort in a given corpus.", "labels": [], "entities": []}, {"text": "Let Ti-I = tj o t2 o ...", "labels": [], "entities": []}, {"text": "o ti.1 be the subanalysis just before the ith step of the process, let LNC(Ti.I ) denote the category of the leftmost nonterminal of Ti-l, and let r(t) denote the root categ.ory of a fragment t.", "labels": [], "entities": []}, {"text": "Then the competition set at the i th step is CS i = { t : r(t)=LNC(Ti.", "labels": [], "entities": []}, {"text": "1 ) } That is, the competition sets for Tree-DOP are determined by the category of the leftmost nonterminal of the current subanalysis.", "labels": [], "entities": []}, {"text": "This is not the only possible definition of competition set.", "labels": [], "entities": []}, {"text": "As have shown, the competition sets can be made dependent on the composition operation.", "labels": [], "entities": []}, {"text": "Their left-corner language model would also apply to Tree-DOP, yielding a different definition for the competition sets.", "labels": [], "entities": []}, {"text": "But the properties of such Tree-DOP models have not been investigated.", "labels": [], "entities": []}, {"text": "Experiments with Tree-DOP on the Penn Treebank and the OVIS corpus show a consistent increase in parse accuracy when larger and more complex subtrees are taken into account (cf..", "labels": [], "entities": [{"text": "Penn Treebank", "start_pos": 33, "end_pos": 46, "type": "DATASET", "confidence": 0.9936420917510986}, {"text": "OVIS corpus", "start_pos": 55, "end_pos": 66, "type": "DATASET", "confidence": 0.923041045665741}, {"text": "accuracy", "start_pos": 103, "end_pos": 111, "type": "METRIC", "confidence": 0.9026198983192444}]}, {"text": "However, Tree-DOP is limited in that it cannot account for underlying syntactic (and semantic) dependencies that are not reflected directly in a surface tree.", "labels": [], "entities": []}, {"text": "All modern linguistic theories propose more articulated representations and mechanisms in order to characterize such linguistic phenomena.", "labels": [], "entities": []}, {"text": "DOP models fora number of richer representations have been explored (van den, but these approaches have remained context-free in their generative power.", "labels": [], "entities": []}, {"text": "In contrast, Lexical-Functional Grammar, which assigns representations consisting of a surface constituent tree enriched with a corresponding functional structure, is known to be beyond context-free.", "labels": [], "entities": [{"text": "Lexical-Functional Grammar", "start_pos": 13, "end_pos": 39, "type": "TASK", "confidence": 0.8248308897018433}]}, {"text": "In the current work, we develop a DOP model based on representations defined by LFG theory CLFG-DOP\").", "labels": [], "entities": []}, {"text": "That is, we provide anew instantiation for the four parameters of the DOP architecture.", "labels": [], "entities": []}, {"text": "We will see that this basic LFG-DOP model triggers anew, corpus-based notion of grammaticality, and that it leads to a different class of its probability models which exhibit interesting properties with respect to specificity and the interpretation of ill-formed strings.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": []}