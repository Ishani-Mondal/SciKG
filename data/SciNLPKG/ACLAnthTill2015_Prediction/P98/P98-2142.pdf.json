{"title": [{"text": "Integrated Control of Chart Items for Error Repair", "labels": [], "entities": [{"text": "Error Repair", "start_pos": 38, "end_pos": 50, "type": "TASK", "confidence": 0.677900031208992}]}], "abstractContent": [{"text": "This paper describes a system that performs hierarchical error repair for ill-formed sentences, with heterarchical control of chart items produced at the lexical, syntactic, and semantic levels.", "labels": [], "entities": [{"text": "hierarchical error repair", "start_pos": 44, "end_pos": 69, "type": "TASK", "confidence": 0.6429698963960012}]}, {"text": "The system uses an augmented context-free grammar and employs a bidirectional chart parsing algorithm.", "labels": [], "entities": []}, {"text": "The system is composed of four subsystems: for lexical, syntactic, surface case, and semantic processing.", "labels": [], "entities": [{"text": "semantic processing", "start_pos": 85, "end_pos": 104, "type": "TASK", "confidence": 0.696620300412178}]}, {"text": "The subsystems are controlled by an integrated-agenda system.", "labels": [], "entities": []}, {"text": "The system employs a parser for well-formed sentences and a second parser for repairing single error sentences.", "labels": [], "entities": []}, {"text": "The system ranks possible repairs by penalty scores which are based on both grammar-dependent factors (e.g. the significance of the repaired constituent in a local tree) and grammar-independent factors (e.g. error types).", "labels": [], "entities": []}, {"text": "This paper focuses on the heterarchical processing of integrated-agenda items (i.e. chart items) at three levels, in the context of single error recovery.", "labels": [], "entities": [{"text": "single error recovery", "start_pos": 132, "end_pos": 153, "type": "TASK", "confidence": 0.6656146844228109}]}], "introductionContent": [{"text": "Weischedel and Sondheimer (1983) described two types of ill-formedness: relative (i.e. limitations of the computer system) and absolute (e.g. misspellings, mistyping, agreement violation etc).", "labels": [], "entities": []}, {"text": "These two types of problem cause ill-formedness of a sentence at various levels, including typographical, orthographical, morphological, phonological, syntactic, semantic, and pragmatic levels.", "labels": [], "entities": []}, {"text": "Typographical spelling errors have been studied by many people.", "labels": [], "entities": [{"text": "Typographical spelling errors", "start_pos": 0, "end_pos": 29, "type": "TASK", "confidence": 0.4867158631483714}]}, {"text": "found a large proportion of real-word errors were orthographical: to--> too, were ---> where.", "labels": [], "entities": []}, {"text": "At the sentential level, types of syntactic errors such as co-occurrence violations, ellipsis, conjunction errors, and extraneous terms have been studied.", "labels": [], "entities": []}, {"text": "In addition, found 0.6% of words misspelt (447/68966) in 300 email messages, leading to about 12.0% of the 3728 sentences havingerrors.", "labels": [], "entities": []}, {"text": "Various systems have focused on the recovery of ill-formed text at the morphosyntactic level, the syntactic level, and the semantic level.", "labels": [], "entities": []}, {"text": "Those systems identified and repaired errors in various ways, including using grammar-specific rules (metarules), least-cost error recovery based on chart parsing, semantic preferences, and heuristic approaches based on a shift-reduce parser.", "labels": [], "entities": []}, {"text": "Systems that focus on a particular level miss errors that can only be detected using higher level knowledge.", "labels": [], "entities": []}, {"text": "For example, at the lexical level, in I saw a man if the park, the misspelt word if is undetected.", "labels": [], "entities": []}, {"text": "At the syntactic level, in I saw a man in the pork, the misspelling of pork can only be detected using semantic information.", "labels": [], "entities": []}, {"text": "This paper describes the automatic correction of ill-formed sentences by using integrated information from three levels (lexical, syntactic, and semantic).", "labels": [], "entities": [{"text": "automatic correction of ill-formed sentences", "start_pos": 25, "end_pos": 69, "type": "TASK", "confidence": 0.8394673705101013}]}, {"text": "The CHAPTER system (CHArt Parser for Twostage Error Recovery), performs two-stage error recovery using generalised top-down chart parsing for the syntax phase (cf..", "labels": [], "entities": [{"text": "Twostage Error Recovery", "start_pos": 37, "end_pos": 60, "type": "TASK", "confidence": 0.7016347646713257}, {"text": "error recovery", "start_pos": 82, "end_pos": 96, "type": "TASK", "confidence": 0.758595198392868}]}, {"text": "It uses an augmented context-free grammar, which covers verb subcategorisations, passives, yes/no and WHquestions, finite relative clauses, and EQUI/SOR phenomena.", "labels": [], "entities": []}, {"text": "The semantic processing uses a conceptual hierarchy and act templates, that express semantic restrictions.", "labels": [], "entities": []}, {"text": "Surface case processing is used to help extract meaning by mapping surface cases to their corresponding conceptual cases.", "labels": [], "entities": [{"text": "Surface case processing", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.6298169990380605}]}, {"text": "Unlike other systems that have focused on error recovery at a particular level, CHAPTER uses an integrated agenda system, which integrates lexical, syntactic, surface case, and semantic processing.", "labels": [], "entities": [{"text": "error recovery", "start_pos": 42, "end_pos": 56, "type": "TASK", "confidence": 0.7223756313323975}, {"text": "CHAPTER", "start_pos": 80, "end_pos": 87, "type": "TASK", "confidence": 0.9626448750495911}]}, {"text": "CHAPTER uses syntactic and semantic information to correct spelling errors detected, including real-word errors.", "labels": [], "entities": [{"text": "CHAPTER", "start_pos": 0, "end_pos": 7, "type": "TASK", "confidence": 0.525034487247467}]}, {"text": "Section 2 gives test results for CHAPTER.", "labels": [], "entities": [{"text": "CHAPTER", "start_pos": 33, "end_pos": 40, "type": "TASK", "confidence": 0.966999888420105}]}, {"text": "Section 3 describes problems with CHAPTER and section 4 contains conclusions.", "labels": [], "entities": [{"text": "CHAPTER", "start_pos": 34, "end_pos": 41, "type": "TASK", "confidence": 0.9699325561523438}]}], "datasetContent": [{"text": "The test data included syntactic errors introduced by substitution of an unknown or known word, addition of an unknown or known word, deletion of a word, segmentation and punctuation problems, and semantic errors.", "labels": [], "entities": []}, {"text": "Data sets we used are identified as: NED (a mix of errors from Novels, Electronic mail, and an (electronic) Diary); Applingl, and Peters2 (the Birkbeck data from Oxford Text Archive); and Thesprev.", "labels": [], "entities": [{"text": "NED", "start_pos": 37, "end_pos": 40, "type": "DATASET", "confidence": 0.8043592572212219}, {"text": "Applingl", "start_pos": 116, "end_pos": 124, "type": "METRIC", "confidence": 0.5863496661186218}, {"text": "Peters2", "start_pos": 130, "end_pos": 137, "type": "DATASET", "confidence": 0.9524401426315308}, {"text": "Birkbeck data from Oxford Text Archive", "start_pos": 143, "end_pos": 181, "type": "DATASET", "confidence": 0.8231086879968643}, {"text": "Thesprev", "start_pos": 188, "end_pos": 196, "type": "DATASET", "confidence": 0.8840634822845459}]}, {"text": "Thesprev was a scanned version of an anonymous humorous article titled \"Thesis Prevention: Advice to PhD Supervisors: The Siblings of Perpetual Prototyping\".", "labels": [], "entities": [{"text": "Thesprev", "start_pos": 0, "end_pos": 8, "type": "DATASET", "confidence": 0.937882661819458}, {"text": "Thesis Prevention: Advice to PhD Supervisors: The Siblings", "start_pos": 72, "end_pos": 130, "type": "TASK", "confidence": 0.8029466927051544}]}, {"text": "In all, 258 ill-formed sentences were tested: 153 from the NED data, 13 from Thesprev, 74 from Applingl, and 18 from Peters2.", "labels": [], "entities": [{"text": "NED data", "start_pos": 59, "end_pos": 67, "type": "DATASET", "confidence": 0.9310451149940491}, {"text": "Thesprev", "start_pos": 77, "end_pos": 85, "type": "DATASET", "confidence": 0.6611227989196777}, {"text": "Applingl", "start_pos": 95, "end_pos": 103, "type": "DATASET", "confidence": 0.8699271082878113}]}, {"text": "The syntactic grammar covered 166 (64.3%) of the manually corrected versions of the 258 sentences.", "labels": [], "entities": []}, {"text": "The average parsing time was 3.2 seconds.", "labels": [], "entities": [{"text": "parsing", "start_pos": 12, "end_pos": 19, "type": "TASK", "confidence": 0.9701254963874817}]}, {"text": "Syntactic processing produced on average 1.7 parse trees 2, of which 0.4 syntactic parse trees were filtered out by semantic processing.", "labels": [], "entities": []}, {"text": "Semantic processing produced 9.3 concepts on average per S node, and 7.3 of them on average were ill-formed.", "labels": [], "entities": []}, {"text": "So many were produced because CHAPTER generated a semantic concept whether it was semantically ill-formed or not, to assist with the repair of illformed sentences.", "labels": [], "entities": [{"text": "CHAPTER generated a semantic concept", "start_pos": 30, "end_pos": 66, "type": "TASK", "confidence": 0.7634668707847595}]}, {"text": "Across the 4 data sets, about one-third of the (manually-corrected) sentences were outside the coverage of the grammar and lexicon.", "labels": [], "entities": []}, {"text": "The most common reasons were that the sentences included a conjunction (\"He places them face down so that they area surprise\"), a phrasal verb CI called out to Fred and went inside\"), or a compound noun (\"P C development tools are far ahead of Unix development tools\").", "labels": [], "entities": []}, {"text": "The remaining 182 sentences were used for testing: NED (98/153); Thesprev (12/13); Applingl (55/74); and Peters2 (17/18).", "labels": [], "entities": [{"text": "NED (98/153", "start_pos": 51, "end_pos": 62, "type": "DATASET", "confidence": 0.7546681642532349}, {"text": "Thesprev", "start_pos": 65, "end_pos": 73, "type": "METRIC", "confidence": 0.9680459499359131}, {"text": "Applingl", "start_pos": 83, "end_pos": 91, "type": "METRIC", "confidence": 0.9843440055847168}]}, {"text": "Compound and compoundcomplex sentences in NED were split into simple sentences to collect 13 more ill-formed sentences for testing.", "labels": [], "entities": []}, {"text": "2There are so few parse trees because of the use of subcategorisation and the augmented context-free grammar (the number of parse trees ranges from 1 to 7).", "labels": [], "entities": [{"text": "2There", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.9480277299880981}]}, {"text": "shows that 89.9% of these illformed sentences were repaired.", "labels": [], "entities": []}, {"text": "Among these, CHAPTER ranked the correct repair first or second in 79.3% of cases (see 'best repair' column in).", "labels": [], "entities": [{"text": "CHAPTER", "start_pos": 13, "end_pos": 20, "type": "METRIC", "confidence": 0.3670406937599182}, {"text": "repair", "start_pos": 40, "end_pos": 46, "type": "METRIC", "confidence": 0.5075347423553467}]}, {"text": "The ranking was based on penalty schemes at three levels: lexical, syntactic, and semantic.", "labels": [], "entities": []}, {"text": "If the correct repair was ranked lower than second among the repairs suggested, then it is counted under 'other repairs' in.", "labels": [], "entities": []}, {"text": "In the case of the NED data, the 'other repairs' include 11 cases of incorrect repairs introduced by: segmentation errors, apostrophe errors, semantic errors, and phrasal verbs.", "labels": [], "entities": [{"text": "NED data", "start_pos": 19, "end_pos": 27, "type": "DATASET", "confidence": 0.8905383050441742}]}, {"text": "Thus for about 71% of all ill-formed sentences tested, the correct repair ranked first or second among the repairs suggested.", "labels": [], "entities": []}, {"text": "For 19% of the sentences tested, incorrect repairs were ranked as the best repairs.", "labels": [], "entities": []}, {"text": "A sentence was considered to be \"correctly repaired\" if any of the suggested corrections was the same as the one obtained by manual correction shows further statistics on CHAPTER's performance.", "labels": [], "entities": [{"text": "CHAPTER", "start_pos": 171, "end_pos": 178, "type": "TASK", "confidence": 0.8797091245651245}]}, {"text": "CHAPTER took 18.8 seconds on average 3 to repair an illformed sentence; suggested an average of 6.4 repaired parse trees; an average of 3 repairs were filtered out by semantic processing.", "labels": [], "entities": []}, {"text": "During semantic processing, an average of 40.3 semantic concepts were suggested for each S node.", "labels": [], "entities": []}, {"text": "An average 34.3 concepts per S node were classified as ill-formed.", "labels": [], "entities": []}, {"text": "Twenty seven percent of the 'best' parse trees suggested by CHAPTER's ranking strategy at the syntactic level were filtered out by semantic processing.", "labels": [], "entities": []}, {"text": "The remaining 73% of the 'best' parse trees were judged semantically wellformed.", "labels": [], "entities": []}, {"text": "In the case of the NED data set, 90 illformed sentences were repaired.", "labels": [], "entities": [{"text": "NED data set", "start_pos": 19, "end_pos": 31, "type": "DATASET", "confidence": 0.9699627955754598}]}, {"text": "On average: recovery time per sentence was 23.9 seconds; 9.8 repaired S trees per sentence were produced; 4.5 of the 9.8 repaired S trees were semantically well-formed; 95.1 repaired concepts (ill-formed and well-formed) were produced; 8.5 of 95.1 repaired concepts were well-formed; and semantic processing filtered syntactically best repairs, removing 22% of repaired sentences.", "labels": [], "entities": [{"text": "recovery time", "start_pos": 12, "end_pos": 25, "type": "METRIC", "confidence": 0.9414940178394318}]}, {"text": "The number of repaired concepts for S is very large because semantic processing at present supports interpretation of only a single verbal (or verb phrasal) adjuncts.", "labels": [], "entities": []}, {"text": "For example, the template of the verb GO allows either a temporal or destination adjunct at present and ignores any second or later adjunct.", "labels": [], "entities": []}, {"text": "Thus a GO sentence would be interpreted using both and [THING GO TIME].", "labels": [], "entities": [{"text": "THING GO TIME", "start_pos": 56, "end_pos": 69, "type": "METRIC", "confidence": 0.8416070739428202}]}], "tableCaptions": [{"text": " Table 1. Performance of CHAPTER on ill-formed sentences  *Peters2 data are not considered in the averages because Peters2 consists of only the sentences that were covered by  CHAPTER's grammar, selected from more than 300 sentence fragments (simple sentences and phrases.)", "labels": [], "entities": []}, {"text": " Table 2. Results on CHAPTER's performance (average values per sentence)", "labels": [], "entities": [{"text": "CHAPTER's", "start_pos": 21, "end_pos": 30, "type": "TASK", "confidence": 0.8823560774326324}]}]}