{"title": [{"text": "Integration of Large-Scale Linguistic Resources in a Natural Language Understanding System", "labels": [], "entities": []}], "abstractContent": [{"text": "Knowledge acquisition is a serious bottleneck for natural language understanding systems.", "labels": [], "entities": [{"text": "Knowledge acquisition", "start_pos": 0, "end_pos": 21, "type": "TASK", "confidence": 0.8086487948894501}, {"text": "natural language understanding", "start_pos": 50, "end_pos": 80, "type": "TASK", "confidence": 0.6621157626310984}]}, {"text": "For this reason, large-scale linguistic resources have been compiled and made available by organizations such as the Linguistic Data Consortium (Comlex) and Princeton University (WordNet).", "labels": [], "entities": []}, {"text": "Systems making use of these resources can greatly accelerate the development process by avoiding the need for the developer to re-create this information.", "labels": [], "entities": []}, {"text": "In this paper we describe how we integrated these large scale linguistic resources into our natural language understanding system.", "labels": [], "entities": []}, {"text": "Clientserver architecture was used to make a large volume of lexical information and a large knowledge base available to the system at development and/or run time.", "labels": [], "entities": []}, {"text": "We discuss issues of achieving compatibility between these disparate resources.", "labels": [], "entities": []}], "introductionContent": [], "datasetContent": [{"text": "We analyzed a small corpus of 1330 sentences (on the subject of our NLU system) in order to give a quantitative description of the contribution of our lexicon and semantics servers.", "labels": [], "entities": []}, {"text": "Our corpus contained forms of 526 distinct roots.", "labels": [], "entities": []}, {"text": "Over 60% of these roots had definitions in our core vocabulary.", "labels": [], "entities": []}, {"text": "Definitions for an additional 25% were extracted from the lexicon server.", "labels": [], "entities": []}, {"text": "Analysis of the remaining 71 roots showed that a developer would have needed to enter definitions for 20 common nouns, 2 verbs, and 2 adjectives; the rest were truly proper nouns as assigned by default.", "labels": [], "entities": []}, {"text": "The 24 roots not covered were for the most part instances of technical jargon for our domain?", "labels": [], "entities": []}, {"text": "For the 215 verbs in our corpus, again over 60% had semantic rules in our core NL Engine.", "labels": [], "entities": []}, {"text": "Our semantics server contributed rules for an additional 38%, leaving our developer with the need to write rules (or rely on guessed default rules) for only 2 verbs.", "labels": [], "entities": []}], "tableCaptions": []}