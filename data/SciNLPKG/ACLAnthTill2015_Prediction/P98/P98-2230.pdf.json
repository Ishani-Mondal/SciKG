{"title": [{"text": "Machine Translation with a Stochastic Grammatical Channel", "labels": [], "entities": [{"text": "Machine Translation", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.785117506980896}]}], "abstractContent": [{"text": "We introduce a stochastic grammatical channel model for machine translation, that synthesizes several desirable characteristics of both statistical and grammatical machine translation.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 56, "end_pos": 75, "type": "TASK", "confidence": 0.8162728846073151}, {"text": "grammatical machine translation", "start_pos": 152, "end_pos": 183, "type": "TASK", "confidence": 0.6921213865280151}]}, {"text": "As with the pure statistical translation model described by Wu (1996) (in which a bracketing transduction grammar models the channel), alternative hypotheses compete probabilistically, exhaustive search of the translation hypothesis space can be performed in polynomial time, and robustness heuristics arise naturally from a language-independent inversion-transduction model.", "labels": [], "entities": []}, {"text": "However, unlike pure statistical translation models, the generated output string is guaranteed to conform to a given target grammar.", "labels": [], "entities": []}, {"text": "The model employs only (1) a translation lexicon, (2) a context-free grammar for the target language, and (3) a bigram language model.", "labels": [], "entities": []}, {"text": "The fact that no explicit bilingual translation rules are used makes the model easily portable to a variety of source languages.", "labels": [], "entities": []}, {"text": "Initial experiments show that it also achieves significant speed gains over our earlier model.", "labels": [], "entities": [{"text": "speed", "start_pos": 59, "end_pos": 64, "type": "METRIC", "confidence": 0.9742866158485413}]}, {"text": "1 Motivation Speed of statistical machine translation methods has long been an issue.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 22, "end_pos": 53, "type": "TASK", "confidence": 0.6519333124160767}]}, {"text": "A step was taken by Wu (Wu, 1996) who introduced a polynomial-time algorithm for the runtime search for an optimal translation.", "labels": [], "entities": []}, {"text": "To achieve this, Wu's method substituted a language-independent stochastic bracketing transduction grammar (SBTG) in place of the simpler word-alignment channel models reviewed in Section 2.", "labels": [], "entities": [{"text": "language-independent stochastic bracketing transduction", "start_pos": 43, "end_pos": 98, "type": "TASK", "confidence": 0.6476229131221771}]}, {"text": "The SBTG channel made exhaustive search possible through dynamic programming, instead of previous \"stack search\" heuristics.", "labels": [], "entities": []}, {"text": "Translation accuracy was not compromised, because the SBTG is apparently flexible enough to model word-order variation (between English and Chinese) even though it eliminates large portions of the space of 1408 word alignments.", "labels": [], "entities": [{"text": "Translation", "start_pos": 0, "end_pos": 11, "type": "TASK", "confidence": 0.9365605711936951}, {"text": "accuracy", "start_pos": 12, "end_pos": 20, "type": "METRIC", "confidence": 0.8956679105758667}]}, {"text": "The SBTG can be regarded as a model of the language-universal hypothesis that closely related arguments tend to stay together (Wu, 1995a; Wu, 1995b).", "labels": [], "entities": []}, {"text": "In this paper we introduce a generalization of Wu's method with the objectives of 1.", "labels": [], "entities": []}, {"text": "increasing translation speed further, 2.", "labels": [], "entities": [{"text": "translation", "start_pos": 11, "end_pos": 22, "type": "TASK", "confidence": 0.9156148433685303}]}, {"text": "improving meaning-preservation accuracy, 3.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 31, "end_pos": 39, "type": "METRIC", "confidence": 0.978727400302887}]}, {"text": "improving grammaticality of the output, and 4.", "labels": [], "entities": []}, {"text": "seeding a natural transition toward transduc-tion rule models, under the constraint of \u2022 employing no additional knowledge resources except a grammar for the target language.", "labels": [], "entities": []}, {"text": "To achieve these objectives, we: \u2022 replace Wu's SBTG channel with a full stochastic inversion transduction grammar or SITG channel, discussed in Section 3, and \u2022 (mis-)use the target language grammar as a SITG, discussed in Section 4.", "labels": [], "entities": []}, {"text": "In Wu's SBTG method, the burden of generating grammatical output rests mostly on the bigram language model; explicit grammatical knowledge cannot be used.", "labels": [], "entities": []}, {"text": "As a result, output grammaticality cannot be guaranteed.", "labels": [], "entities": []}, {"text": "The advantage is that language-dependent syntactic knowledge resources are not needed.", "labels": [], "entities": []}, {"text": "We relax those constraints hereby assuming a good (monolingual) context-free grammar for the target language.", "labels": [], "entities": []}, {"text": "Compared to other knowledge resources (such as transfer rules or semantic on-tologies), monolingual syntactic grammars are relatively easy to acquire or construct.", "labels": [], "entities": []}, {"text": "We use the grammar in the SITG channel, while retaining the bigram language model.", "labels": [], "entities": [{"text": "SITG channel", "start_pos": 26, "end_pos": 38, "type": "DATASET", "confidence": 0.821200966835022}]}, {"text": "The new model facilitates explicit coding of grammatical knowledge and finer control over channel probabilities.", "labels": [], "entities": []}, {"text": "Like Wu's SBTG model, the translation hypothesis space can be exhaustively searched in polynomial time, as shown in", "labels": [], "entities": []}], "introductionContent": [], "datasetContent": [], "tableCaptions": []}