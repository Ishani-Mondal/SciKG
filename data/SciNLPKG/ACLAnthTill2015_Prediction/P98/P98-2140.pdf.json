{"title": [{"text": "Feature Lattices for Maximum Entropy Modelling", "labels": [], "entities": []}], "abstractContent": [{"text": "Maximum entropy framework proved to be expressive and powerful for the statistical language modelling, but it suffers from the computational expensiveness of the model building.", "labels": [], "entities": [{"text": "statistical language modelling", "start_pos": 71, "end_pos": 101, "type": "TASK", "confidence": 0.7544570565223694}]}, {"text": "The iterative scaling algorithm that is used for the parameter estimation is computation-ally expensive while the feature selection process might require to estimate parameters for many candidate features many times.", "labels": [], "entities": [{"text": "parameter estimation", "start_pos": 53, "end_pos": 73, "type": "TASK", "confidence": 0.7026139795780182}]}, {"text": "In this paper we present a novel approach for building maximum entropy models.", "labels": [], "entities": []}, {"text": "Our approach uses the feature collocation lattice and builds complex candidate features without resorting to iterative scaling.", "labels": [], "entities": []}], "introductionContent": [{"text": "Maximum entropy modelling has been recently introduced to the NLP community and proved to bean expressive and powerful framework.", "labels": [], "entities": [{"text": "Maximum entropy modelling", "start_pos": 0, "end_pos": 25, "type": "TASK", "confidence": 0.6436872084935507}]}, {"text": "The maximum entropy model is a model which fits a set of pre-defined constraints and assumes maximum ignorance about everything which is not subject to its constraints thus assigning such cases with the most uniform distribution.", "labels": [], "entities": []}, {"text": "The most uniform distribution will have the entropy on its maximum Because of its ability to handle overlapping features the maximum entropy framework provides a principle way to incorporate information from multiple knowledge sources.", "labels": [], "entities": []}, {"text": "It is superior totraditionally used for this purpose linear interpolation and Katz back-off method.", "labels": [], "entities": []}, {"text": "evaluates in detail a maximum entropy language model which combines unigrams, bigrams, trigrams and long-distance trigger words, and provides a thorough analysis of all the merits of the approach.", "labels": [], "entities": []}, {"text": "* Now at Harlequin Ltd.", "labels": [], "entities": [{"text": "Harlequin Ltd", "start_pos": 9, "end_pos": 22, "type": "DATASET", "confidence": 0.9621730446815491}]}, {"text": "The iterative scaling algorithm applied for the parameter estimation of maximum entropy models computes a set of feature weights (As) which ensure that the model fits the reference distribution and does not make spurious assumptions (as required by the maximum entropy principle) about events beyond the reference distribution.", "labels": [], "entities": []}, {"text": "It, however, does not guarantee that the features employed by the model are good features and the model is useful.", "labels": [], "entities": []}, {"text": "Thus the most important part of the model building is the feature selection procedure.", "labels": [], "entities": [{"text": "feature selection", "start_pos": 58, "end_pos": 75, "type": "TASK", "confidence": 0.6768762022256851}]}, {"text": "The key idea of the feature selection is that if we notice an interaction between certain features we should build a more complex feature which will account for this interaction.", "labels": [], "entities": []}, {"text": "The newly added feature should improve the model: its Kullback-Leibler divergence from the reference distribution should decrease and the conditional maximum entropy model will also have the greatest log-likelihood (L) value: The basic feature induction algorithm presented in) starts with an empty feature space and iterativety tries all possible feature candidates.", "labels": [], "entities": []}, {"text": "These candidates are either atomic features or complex features produced as a combination of anatomic feature with the features already selected to the model's feature space.", "labels": [], "entities": []}, {"text": "For every feature from the candidate feature set the algorithm prescribes to compute the maximum entropy model using the iterative scaling algorithm described above, and select the feature which in the largest way minimizes the Kullback-Leibler divergence or maximizes the log-likelihood of the model.", "labels": [], "entities": []}, {"text": "This approach, however, is not computationally feasible since the iterative scaling is computationally expensive and to compute models for many candidate features many times is unreal.", "labels": [], "entities": []}, {"text": "To make feature ranking computationally tractable in and) a simplified process proposed: at the feature ranking stage when adding anew feature to the model, all previously computed parameters are kept fixed and, thus, we have to fit only one new constraint imposed by the candidate feature.", "labels": [], "entities": [{"text": "feature ranking", "start_pos": 8, "end_pos": 23, "type": "TASK", "confidence": 0.723924458026886}]}, {"text": "Then after the best ranked feature has been established, it is added to the feature space and the weights for all the features are recomputed.", "labels": [], "entities": []}, {"text": "This approach estimates good features relatively fast but it does not guarantee that at every single point we add the best feature because when we add anew feature to the model all its parameters can change.", "labels": [], "entities": []}, {"text": "In this paper we present a novel approach to feature selection for the maximum entropy models.", "labels": [], "entities": []}, {"text": "Our approach uses a feature collocation lattice and selects candidate features without resorting to the iterative scaling.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": []}