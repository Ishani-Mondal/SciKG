{"title": [{"text": "Evaluating a Focus-Based Approach to Anaphora Resolution*", "labels": [], "entities": [{"text": "Anaphora Resolution", "start_pos": 37, "end_pos": 56, "type": "TASK", "confidence": 0.9559088349342346}]}], "abstractContent": [{"text": "We present an approach to anaphora resolution based on a focusing algorithm, and implemented within an existing MUC (Message Understanding Conference) Information Extraction system, allowing quantitative evaluation against a substantial corpus of annotated real-world texts.", "labels": [], "entities": [{"text": "anaphora resolution", "start_pos": 26, "end_pos": 45, "type": "TASK", "confidence": 0.859032928943634}, {"text": "MUC (Message Understanding Conference) Information Extraction", "start_pos": 112, "end_pos": 173, "type": "TASK", "confidence": 0.7055439949035645}]}, {"text": "Extensions to the basic focusing mechanism can be easily tested, resulting in refinements to the mechanism and resolution rules.", "labels": [], "entities": [{"text": "resolution", "start_pos": 111, "end_pos": 121, "type": "METRIC", "confidence": 0.9317916631698608}]}, {"text": "Results show that the focusing algorithm is highly sensitive to the quality of syntactic-semantic analyses, when compared to a simpler heuristic-based approach .", "labels": [], "entities": []}], "introductionContent": [{"text": "Anaphora resolution is still present as a significant linguistic problem, both theoretically and practically, and interest has recently been renewed with the introduction of a quantitative evaluation regime as part of the Message Understanding Conference (MUC) evaluations of Information Extraction (IE) systems.", "labels": [], "entities": [{"text": "Anaphora resolution", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.8237052261829376}, {"text": "Message Understanding Conference (MUC) evaluations of Information Extraction (IE)", "start_pos": 222, "end_pos": 303, "type": "TASK", "confidence": 0.729144023014949}]}, {"text": "This has made it possible to evaluate different (implementable) theoretical approaches against sizable corpora of real-world texts, rather than the small collections of artificial examples typically discussed in the literature.", "labels": [], "entities": []}, {"text": "This paper describes an evaluation of a focusbased approach to pronoun resolution (not anaphora in general), based on an extension of Sidner's algorithm proposed in, with further refinements from development on real-world texts.", "labels": [], "entities": [{"text": "pronoun resolution", "start_pos": 63, "end_pos": 81, "type": "TASK", "confidence": 0.723120778799057}]}, {"text": "The approach * This work was carried out in the context of the EU AVENTINUS project, which aims to develop a multilingual IE system for drug enforcement, and including a language-independent coreference mechanism ( . is implemented within the general coreference mechanism provided by the LaSIE (Large Scale Information Extraction) system ( and ), Sheffield University's entry in the MUC-6 and 7 evaluations.", "labels": [], "entities": [{"text": "drug enforcement", "start_pos": 136, "end_pos": 152, "type": "TASK", "confidence": 0.6880151331424713}, {"text": "MUC-6", "start_pos": 384, "end_pos": 389, "type": "DATASET", "confidence": 0.9244758486747742}]}], "datasetContent": [{"text": "As part of MUC (, coreference resolution was evaluated as a sub-task of information extraction, which involved negotiating a definition of coreference relations that could be reliably evaluated.", "labels": [], "entities": [{"text": "MUC", "start_pos": 11, "end_pos": 14, "type": "TASK", "confidence": 0.4794440269470215}, {"text": "coreference resolution", "start_pos": 18, "end_pos": 40, "type": "TASK", "confidence": 0.9485391080379486}, {"text": "information extraction", "start_pos": 72, "end_pos": 94, "type": "TASK", "confidence": 0.762391984462738}]}, {"text": "The final definition included only 'identity' relations between text strings: proper nouns, common nouns and pronouns.", "labels": [], "entities": []}, {"text": "Other possible coreference relations, such as 'part-whole', and nontext strings (zero anaphora) were excluded.", "labels": [], "entities": []}, {"text": "The definition was used to manually annotate several corpora of newswire texts, using SGML markup to indicate relations between text strings.", "labels": [], "entities": []}, {"text": "Automatically annotated texts, produced by systems using the same markup scheme, were then compared with the manually annotated versions, using scoring software made available to MUC participants, based on (.", "labels": [], "entities": [{"text": "MUC", "start_pos": 179, "end_pos": 182, "type": "DATASET", "confidence": 0.7911128401756287}]}, {"text": "The scoring software calculates the standard Information Retrieval metrics of 'recall' and 'precision', 2 together with an overall f-measure.", "labels": [], "entities": [{"text": "Information Retrieval", "start_pos": 45, "end_pos": 66, "type": "TASK", "confidence": 0.7329829633235931}, {"text": "recall", "start_pos": 79, "end_pos": 85, "type": "METRIC", "confidence": 0.9966627955436707}, {"text": "precision", "start_pos": 92, "end_pos": 101, "type": "METRIC", "confidence": 0.9941956400871277}]}, {"text": "The following section presents the results obtained using the corpora and scorer provided for MUC-7 training (60 texts, average 581 words per text, 19 words per sentence) and evaluation (20 texts, average 605 words per text, 20 words per sentence), the latter provided for the formal MUC-7 run and kept blind during development.", "labels": [], "entities": [{"text": "MUC-7 training", "start_pos": 94, "end_pos": 108, "type": "TASK", "confidence": 0.6183306276798248}]}], "tableCaptions": []}