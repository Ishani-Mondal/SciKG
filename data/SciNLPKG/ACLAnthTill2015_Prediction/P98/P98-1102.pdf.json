{"title": [], "abstractContent": [{"text": "In order to realize their full potential, multimodal systems need to support not just input from multiple modes, but also synchronized integration of modes.", "labels": [], "entities": []}, {"text": "Johnston et al (1997) model this integration using a unification operation over typed feature structures.", "labels": [], "entities": []}, {"text": "This is an effective solution fora broad class of systems, but limits multimodal utterances to combinations of a single spoken phrase with a single gesture.", "labels": [], "entities": []}, {"text": "We show how the unification-based approach can be scaled up to provide a full multimodal grammar formalism.", "labels": [], "entities": []}, {"text": "In conjunction with a multidimen-sional chart parser, this approach supports integration of multiple elements distributed across the spatial, temporal, and acoustic dimensions of multimodal interaction.", "labels": [], "entities": []}, {"text": "Integration strategies are stated in a high level unification-based rule formalism supporting rapid prototyping and iterative development of multimodal systems.", "labels": [], "entities": []}], "introductionContent": [{"text": "Multimodal interfaces enable more natural and efficient interaction between humans and machines by providing multiple channels through which input or output may pass.", "labels": [], "entities": []}, {"text": "Our concern here is with multimodal input, such as interfaces which support simultaneous input from speech and pen.", "labels": [], "entities": []}, {"text": "Such interfaces have clear task performance and user preference advantages over speech only interfaces, in particular for spatial tasks such as those involving maps.", "labels": [], "entities": []}, {"text": "Our focus here is on the integration of input from multiple modes and the role this plays in the segmentation and parsing of natural human input.", "labels": [], "entities": [{"text": "segmentation and parsing of natural human input", "start_pos": 97, "end_pos": 144, "type": "TASK", "confidence": 0.7393734284809658}]}, {"text": "In the examples given here, the modes are speech and pen, but the architecture described is more general in that it can support more than two input modes and modes of other types such as 3D gestural input.", "labels": [], "entities": []}, {"text": "Our multimodal interface technology is implemented in, a working system which supports dynamic interaction with maps and other complex visual displays.", "labels": [], "entities": []}, {"text": "The initial applications of QuickSet are: setting up and interacting with distributed simulations, logistics planning, and navigation in virtual worlds.", "labels": [], "entities": []}, {"text": "The system is distributed; consisting of a series of agents () which communicate through a shared blackboard.", "labels": [], "entities": []}, {"text": "It runs on both desktop and handheld PCs, communicating over wired and wireless LANs.", "labels": [], "entities": []}, {"text": "The user interacts with a map displayed on a wireless hand-held unit ().", "labels": [], "entities": []}, {"text": "They can draw directly on the map and simultaneously issue spoken commands.", "labels": [], "entities": []}, {"text": "Different kinds of entities, lines, and areas maybe created by drawing the appropriate spatial features and speaking their type; for example, drawing an area and saying 'flood zone'.", "labels": [], "entities": []}, {"text": "Orders may also be specified; for example, by drawing a line and saying 'helicopterfollow this route'.", "labels": [], "entities": []}, {"text": "The speech signal is routed to an HMM-based continuous speaker-independent recognizer.", "labels": [], "entities": [{"text": "HMM-based continuous speaker-independent recognizer", "start_pos": 34, "end_pos": 85, "type": "TASK", "confidence": 0.5115173161029816}]}, {"text": "The electronic 'ink' is routed to a neural net-based gesture recognizer.", "labels": [], "entities": []}, {"text": "Both generate N-best lists of potential recognition results with associated probabilities.", "labels": [], "entities": []}, {"text": "These results are assigned semantic interpretations by natural language processing and gesture interpretation agents respectively.", "labels": [], "entities": [{"text": "gesture interpretation", "start_pos": 87, "end_pos": 109, "type": "TASK", "confidence": 0.7039526551961899}]}, {"text": "A multimodal integrator agent fields input from the natural language and gesture interpretation agents and selects the appropriate multimodal or unimodal commands to execute.", "labels": [], "entities": []}, {"text": "These are passed onto abridge agent which provides an API to the underlying applications the system is used to control.", "labels": [], "entities": []}, {"text": "In the approach to multimodal integration proposed by Johnston et al 1997, integration of spoken and gestural input is driven by a unification operation over typed feature structures representing the semantic contributions of the different modes.", "labels": [], "entities": [{"text": "multimodal integration", "start_pos": 19, "end_pos": 41, "type": "TASK", "confidence": 0.7645320892333984}]}, {"text": "This approach overcomes the limitations of previous approaches in that it allows fora full range of gestura~ input beyond simple deictic pointing gestures.", "labels": [], "entities": []}, {"text": "Unlike speech-driven systems, it is fully multimodal in that all elements of the content of a command can be in either mode.", "labels": [], "entities": []}, {"text": "Furthermore, compared to related framemerging strategies, it provides a well understood, generally applicable common meaning representation for the different modes and a formally well defined mechanism for multimodal integration.", "labels": [], "entities": [{"text": "multimodal integration", "start_pos": 206, "end_pos": 228, "type": "TASK", "confidence": 0.742508202791214}]}, {"text": "However, while this approach provides an efficient solution fora broad class of multimodal systems, there are significant limitations on the expressivity and generality of the approach.", "labels": [], "entities": []}, {"text": "A wide range of potential multimodal utterances fall outside the expressive potential of the previous architecture.", "labels": [], "entities": []}, {"text": "Empirical studies of multimodal interaction, utilizing wizard-of-oz techniques, have shown that when users are free to interact with any combination of speech and pen, a single spoken utterance maybe associated with more than one gesture.", "labels": [], "entities": []}, {"text": "For example, a number of deictic pointing gestures maybe associated with a single spoken utterance: ' calculate distance from hereto bere', 'put that there', 'move this team to here and prepare to rescue residents from this building'.", "labels": [], "entities": [{"text": "calculate", "start_pos": 102, "end_pos": 111, "type": "METRIC", "confidence": 0.9489663243293762}]}, {"text": "Speech may also be combined with a series of gestures of different types: the user circles a vehicle on the map, says 'follow this route', and draws an arrow indicating the route to be followed.", "labels": [], "entities": []}, {"text": "In addition to more complex multipart multimodal utterances, unimodal gestural utterances may contain several component gestures which compose to yield a command.", "labels": [], "entities": []}, {"text": "For example, to create an entity with a specific orientation, a user might draw the entity and then draw an arrow leading out from it).", "labels": [], "entities": []}, {"text": "To specify a movement, the user might draw an arrow indicating the extent of the move and indicate departure and arrival times by writing expressions at the base and head).", "labels": [], "entities": []}, {"text": "These I I z'\u00b0l: Complex Unimodal Gestures are specific examples of the more general problem of visual parsing, which has been a focus of attention in research on visual programming and pen-based interfaces for the creation of complex graphical objects such as mathematical equations and flowcharts.", "labels": [], "entities": [{"text": "visual parsing", "start_pos": 95, "end_pos": 109, "type": "TASK", "confidence": 0.7423090040683746}]}, {"text": "The approach of Johnston et al 1997 also faces fundamental architectural problems.", "labels": [], "entities": []}, {"text": "The multimodal integration strategy is hard-coded into the integration agent and there is no isolatable statement of the rules and constraints independent of the code itself.", "labels": [], "entities": []}, {"text": "As the range of multimodal utterances supported is extended, it becomes essential that there be a declarative statement of the grammar of multimodal utterances, separate from the algorithms and mechanisms of parsing.", "labels": [], "entities": []}, {"text": "This will enable system developers to describe integration strategies in a high level representation, facilitating rapid prototyping and iterative development of multimodal systems.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": []}