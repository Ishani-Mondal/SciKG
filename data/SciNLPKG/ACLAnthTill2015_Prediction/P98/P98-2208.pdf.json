{"title": [{"text": "Tagging English by Path Voting Constraints", "labels": [], "entities": [{"text": "Tagging English by Path Voting Constraints", "start_pos": 0, "end_pos": 42, "type": "TASK", "confidence": 0.768159419298172}]}], "abstractContent": [{"text": "We describe a constraint-based tagging approach where individual constraint rules vote on sequences of matching tokens and tags.", "labels": [], "entities": []}, {"text": "Disambiguation of all tokens in a sentence is performed at the very end by selecting tags that appear on the path that receives the highest vote.", "labels": [], "entities": [{"text": "Disambiguation of all tokens in a sentence", "start_pos": 0, "end_pos": 42, "type": "TASK", "confidence": 0.7876873527254377}]}, {"text": "This constraint application paradigm makes the outcome of the disambiguation independent of the rule sequence, and hence relieves the rule developer from worrying about potentially conflicting rule sequencing.", "labels": [], "entities": []}, {"text": "The approach can also combine statistically and manually obtained constraints, and incorporate negative constraint rules to rule out certain patterns.", "labels": [], "entities": []}, {"text": "We have applied this approach to tagging English text from the Wall Street Journal and the Brown Corpora.", "labels": [], "entities": [{"text": "tagging English text", "start_pos": 33, "end_pos": 53, "type": "TASK", "confidence": 0.881865660349528}, {"text": "Wall Street Journal and the Brown Corpora", "start_pos": 63, "end_pos": 104, "type": "DATASET", "confidence": 0.6859283447265625}]}, {"text": "Our results from the Wall Street Journal Corpus indicate that with 400 statistically derived constraint rules and about 800 hand-crafted constraint rules, we can attain an average accuracy of 9Z89~ on the training corpus and an average accuracy of g7.50~ on the testing corpus.", "labels": [], "entities": [{"text": "Wall Street Journal Corpus", "start_pos": 21, "end_pos": 47, "type": "DATASET", "confidence": 0.9808691591024399}, {"text": "accuracy", "start_pos": 180, "end_pos": 188, "type": "METRIC", "confidence": 0.9991575479507446}, {"text": "accuracy", "start_pos": 236, "end_pos": 244, "type": "METRIC", "confidence": 0.9983140230178833}]}, {"text": "We can also relax the single tag per token limitation and allow ambiguous tagging which lets us trade recall and precision.", "labels": [], "entities": [{"text": "recall", "start_pos": 102, "end_pos": 108, "type": "METRIC", "confidence": 0.9994827508926392}, {"text": "precision", "start_pos": 113, "end_pos": 122, "type": "METRIC", "confidence": 0.9982861876487732}]}], "introductionContent": [{"text": "Part-of-speech tagging is one of the preliminary steps in many natural language processing systems in which the proper part-of-speech tag of the tokens comprising the sentences are disambiguated using either statistical or symbolic local contextual information.", "labels": [], "entities": [{"text": "Part-of-speech tagging", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.7962628901004791}]}, {"text": "Tagging systems have used either a statistical approach where a large corpora is employed to train a probabilistic model which then is used to tag unseen text, (e.g.,,,), or a constraint-based approach which employs a large number of hand-crafted linguistic constraints that are used to eliminate impossible sequences or morphological parses fora given word in a given context, recently most prominently exemplified by the Constraint Grammar work (. has presented a transformationbased learning approach.", "labels": [], "entities": []}, {"text": "This paper extends a novel approach to constraint-based tagging first applied for Turkish, which relieves the rule developer from worrying about conflicting rule ordering requirements and constraints.", "labels": [], "entities": []}, {"text": "The approach depends on assigning votes to constraints via statistical and/or manual means, and then letting constraints vote on matching sequences on tokens, as depicted in.", "labels": [], "entities": []}, {"text": "This approach does not reflect the outcome of matching constraints to the set of morphological parses immediately as usually done in constraint-based systems.", "labels": [], "entities": []}, {"text": "Only after all applicable rules are applied to a sentence, tokens are disambiguated in parallel.", "labels": [], "entities": []}, {"text": "Thus, the outcome of the rule applications is independent of the order of rule applications.", "labels": [], "entities": []}, {"text": "We assume that sentences are delineated and that each token is assigned all possible tags by a lexicon or by a morphological analyzer.", "labels": [], "entities": []}, {"text": "We represent each sentence as a standard chart using a directed acyclic graph where nodes represent token boundaries and arcs are labeled with ambiguous interpretations of tokens.", "labels": [], "entities": []}, {"text": "For instance, the sentence I cancan the can.", "labels": [], "entities": []}, {"text": "would be represented as shown in, where bold arcs denote the correct tags.", "labels": [], "entities": []}, {"text": "We describe constraints on token sequences using rules of the sort R = (C1,C2,-.", "labels": [], "entities": []}, {"text": "\",Cn; V), where the Ci are, in general, feature constraints on a sequence of the ambiguous parses, and V is an integer denoting the vote of the rule.", "labels": [], "entities": []}, {"text": "For English, the features that we use are: (1) LEX: the lexical form, and (2) TAG: the tag.", "labels": [], "entities": [{"text": "LEX", "start_pos": 47, "end_pos": 50, "type": "METRIC", "confidence": 0.9905243515968323}, {"text": "TAG", "start_pos": 78, "end_pos": 81, "type": "METRIC", "confidence": 0.9878700375556946}]}, {"text": "It is certainly possible to extend the set of features used, by including features such as initial letter capitalization, any derivational information, etc.", "labels": [], "entities": []}, {"text": "For instance, (,, ; 100) is a rule with a high vote to promote modal followed by a verb with an intervening adverb.", "labels": [], "entities": []}, {"text": "The rule (, ; -100) demotes a singular determiner reading of that before a plural noun, while (, [TAG=J J, LEX=o'cher] ; 100) is a rule with a high vote that captures a collocation.", "labels": [], "entities": [{"text": "TAG=J J, LEX=o'cher", "start_pos": 98, "end_pos": 117, "type": "METRIC", "confidence": 0.8482332229614258}]}, {"text": "The constraints apply to a sentence in the following manner: Assume fora moment that all possible paths from the start node to the end node of a sentence graph are explicitly enumerated, and that after the enumeration, each path is augmented by a vote component.", "labels": [], "entities": []}, {"text": "For each path at hand, we apply each constraint to all possible sequences of token parses.", "labels": [], "entities": []}, {"text": "Let R = (C1,C2,'\",C,~;V) be a constraint and let wi,wi+l,-'-, wi+,~-i be a sequence of token parses labeling sequential arcs of the path.", "labels": [], "entities": []}, {"text": "We say rule R matches this sequence of parses, if wj, i _< j < i + n -1 is subsumed by the corresponding constraint Cj-i+l.", "labels": [], "entities": []}, {"text": "When such a match occurs, the vote of the path is incremented by V. When all constraints are applied to all possible sequences in all paths, we select the path with the maximum vote.", "labels": [], "entities": []}, {"text": "If there are multiple paths with the same maximum vote, the tokens whose parses are different in those paths are assumed to be left ambiguous.", "labels": [], "entities": []}, {"text": "Given that each token has on the average more than 2 possible tags, the procedural description above is very inefficient for all but very short sentences.", "labels": [], "entities": []}, {"text": "However, the observation that our constraints are localized to a window of a small number of tokens (say at most 5 tokens in a sequence), suggests a more efficient scheme originally used by.", "labels": [], "entities": []}, {"text": "Assume our constraint windows are allowed to look at a window of at most size k sequential parses.", "labels": [], "entities": []}, {"text": "Let us take the first k tokens of a sentence and generate all possible paths of k arcs (spanning k + 1 nodes), and apply all constraints to these \"short\" paths.", "labels": [], "entities": []}, {"text": "Now, if we discard the first token and consider the (k + 1) st token, we need to consider and extend only those paths that have accumulated the maximum vote among the paths whose last k -1 parses are the same.", "labels": [], "entities": []}, {"text": "The reason is that since the first token is now out of the context window, it cannot influence the application of any rules.", "labels": [], "entities": []}, {"text": "Hence only the highest scoring (partial) paths need to be extended, as lower scoring paths cannot later accumulate votes to surpass the current highest scoring paths.", "labels": [], "entities": []}, {"text": "In we describe the procedure in a more formal way where wl,,\" \", ws denotes a sequence of tokens in a sentence, amb(wi) denotes the number of ambiguous tags for token wi, and k denotes the maximum context window size (determined at run time).", "labels": [], "entities": []}, {"text": "1. P = { all I-I~_--: arnb(wj) paths of the first k-1 tokens } Results from Tagging English", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Results from tagging the WSJ and  Brown Corpora.", "labels": [], "entities": [{"text": "WSJ", "start_pos": 35, "end_pos": 38, "type": "DATASET", "confidence": 0.8812752366065979}, {"text": "Brown Corpora", "start_pos": 44, "end_pos": 57, "type": "DATASET", "confidence": 0.8438438773155212}]}]}