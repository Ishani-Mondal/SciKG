{"title": [{"text": "Learning Intonation Rules for Concept to Speech Generation", "labels": [], "entities": [{"text": "Concept to Speech Generation", "start_pos": 30, "end_pos": 58, "type": "TASK", "confidence": 0.7829378843307495}]}], "abstractContent": [{"text": "In this paper, we report on an effort to provide a general-purpose spoken language generation tool for Concept-to-Speech (CTS) applications by extending a widely used text generation package, FUF/SURGE, with an intonation generation component.", "labels": [], "entities": [{"text": "spoken language generation", "start_pos": 67, "end_pos": 93, "type": "TASK", "confidence": 0.7996704975763956}]}, {"text": "As a first step, we applied machine learning and statistical models to learn intonation rules based on the semantic and syntactic information typically represented in FUF/SURGE at the sentence level.", "labels": [], "entities": []}, {"text": "The results of this study area set of intonation rules learned automatically which can be directly implemented in our intonation generation component.", "labels": [], "entities": []}, {"text": "Through 5-fold cross-validation, we show that the learned rules achieve around 90% accuracy for break index, boundary tone and phrase accent and 80% accuracy for pitch accent.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 83, "end_pos": 91, "type": "METRIC", "confidence": 0.9992732405662537}, {"text": "break index", "start_pos": 96, "end_pos": 107, "type": "METRIC", "confidence": 0.8032051622867584}, {"text": "accuracy", "start_pos": 149, "end_pos": 157, "type": "METRIC", "confidence": 0.9990695118904114}]}, {"text": "Our study is unique in its use of features produced by language generation to control intonation.", "labels": [], "entities": []}, {"text": "The methodology adopted here can be employed directly when more discourse/pragmatic information is to be considered in the future.", "labels": [], "entities": []}, {"text": "1 Motivation Speech is rapidly becoming a viable medium for interaction with real-world applications.", "labels": [], "entities": [{"text": "1 Motivation Speech", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.567650298277537}]}, {"text": "Spoken language interfaces to on-line information , such as plane or train schedules, through display-less systems, such as telephone interfaces , are well underdevelopment.", "labels": [], "entities": []}, {"text": "Speech interfaces are also widely used in applications where eyes-free and hands-free communication is critical, such as car navigation.", "labels": [], "entities": [{"text": "car navigation", "start_pos": 121, "end_pos": 135, "type": "TASK", "confidence": 0.7762973606586456}]}, {"text": "Natural language generation (NLG) can enhance the ability of such systems to communicate naturally and effectively by allowing the system to tailor, reorganize, or summarize lengthy database responses.", "labels": [], "entities": [{"text": "Natural language generation (NLG", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.7281889021396637}, {"text": "summarize lengthy database responses", "start_pos": 164, "end_pos": 200, "type": "TASK", "confidence": 0.8510579615831375}]}, {"text": "For example, in our work on a mul-timedia generation system where speech and graphics generation techniques are used to au-tomaticaily summarize patient's pre-, during, and post-, operation status to different care-givers (Dalai et al., 1996), records relevant to patient status can easily number in the thousands.", "labels": [], "entities": [{"text": "mul-timedia generation", "start_pos": 30, "end_pos": 52, "type": "TASK", "confidence": 0.8048080801963806}]}, {"text": "Through content planning, sentence planning and lexical selection, ,the NLG component is able to provide a concise, yet informative , briefing automatically through spoken and written language coordinated with graphics (McKeown et al., 1997).", "labels": [], "entities": [{"text": "sentence planning", "start_pos": 26, "end_pos": 43, "type": "TASK", "confidence": 0.7646051645278931}]}, {"text": "Integrating language generation with speech synthesis within a Concept-to-Speech (CTS) system not only brings the individual benefits of each; as an integrated system, CTS can take advantage of the availability of rich structural information constructed by the underlying NLG component to improve the quality of synthesized speech.", "labels": [], "entities": []}, {"text": "Together, they have the potential of generating better speech than Text-to-Speech (TTS) systems.", "labels": [], "entities": []}, {"text": "In this paper, we present a series of experiments that use machine learning to identify correlation between intonation and features produced by a robust language generation tool, the FUF/SURGE system (Elhadad, 1993; Robin, 1994).", "labels": [], "entities": [{"text": "FUF/SURGE system (Elhadad, 1993; Robin, 1994)", "start_pos": 183, "end_pos": 228, "type": "DATASET", "confidence": 0.8299391177984384}]}, {"text": "The ultimate goal of this study is to provide a spoken language generation tool based on FUF/SURGE, extended with an intonation generation component to facilitate the development of new CTS applications.", "labels": [], "entities": [{"text": "spoken language generation", "start_pos": 48, "end_pos": 74, "type": "TASK", "confidence": 0.7175965905189514}, {"text": "FUF", "start_pos": 89, "end_pos": 92, "type": "METRIC", "confidence": 0.6423891186714172}]}], "introductionContent": [], "datasetContent": [{"text": "Our first set of experiments was designed as an initial test of how the features from FUF/SURGE contribute to intonation.", "labels": [], "entities": [{"text": "FUF/SURGE", "start_pos": 86, "end_pos": 95, "type": "DATASET", "confidence": 0.7717896302541097}]}, {"text": "We focused on how the newly available semantic features affect intonation.", "labels": [], "entities": []}, {"text": "We were also interested in finding out whether the 13 selected features are redundant in making intonation decisions.", "labels": [], "entities": []}, {"text": "We started from a simple model which includes only 3 factors, the type of semantic constituent boundary before (BB) and after (BA) the word, and part of speech (POS).", "labels": [], "entities": []}, {"text": "The semantic constituent boundary can take on 6 different values; for example, it can be a clause boundary, a boundary associated with a primary semantic role (e.g., a participant), with a secondary semantic role (e.g., a type of modifier), among others.", "labels": [], "entities": []}, {"text": "Our purpose in this experiment was to test how well the model can do with a limited number of parameters.", "labels": [], "entities": []}, {"text": "Applying RIPPER to the simple model yielded rules that significantly improved performance over the baseline models.", "labels": [], "entities": []}, {"text": "For example, the accuracy of the rules learned for break index increases to 87.37% from 67.09%; the average improvement on all 4 intonational features is 19.33%.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 17, "end_pos": 25, "type": "METRIC", "confidence": 0.999619722366333}, {"text": "break index", "start_pos": 51, "end_pos": 62, "type": "METRIC", "confidence": 0.6756061017513275}]}, {"text": "Next, we ran two additional tests, one with additional syntactic features and another with additional semantic features.", "labels": [], "entities": []}, {"text": "The results show that the two new models behave similarly on all intonational features; they both achieve some participant boundaries or circumstance boundaries etc.", "labels": [], "entities": []}, {"text": "The semantic feature of \"did\" in \"I did know him.\" is \"insistence\".", "labels": [], "entities": []}, {"text": "The SP of \"teacher\" in \"John is the teacher\" is \"identifier\".", "labels": [], "entities": [{"text": "SP", "start_pos": 4, "end_pos": 6, "type": "METRIC", "confidence": 0.9846342206001282}]}, {"text": "The GSP of \"teacher\" in \"John is the teacher\" is \"participant\" common noun, proper noun etc.", "labels": [], "entities": [{"text": "GSP", "start_pos": 4, "end_pos": 7, "type": "METRIC", "confidence": 0.9112182855606079}]}, {"text": "noun is the corresponding GPOS of both common noun and proper noun.", "labels": [], "entities": []}, {"text": "The SYNFUN of \"teacher\" in \"the teacher\" is \"head\".", "labels": [], "entities": [{"text": "SYNFUN", "start_pos": 4, "end_pos": 10, "type": "METRIC", "confidence": 0.9335924983024597}]}, {"text": "The SPPOS of \"teacher\" is \"common noun\".", "labels": [], "entities": [{"text": "SPPOS", "start_pos": 4, "end_pos": 9, "type": "METRIC", "confidence": 0.4869811534881592}]}, {"text": "I The SPGPOS of \"teacher\" in \"the teacher\" is \"noun phrase\".", "labels": [], "entities": []}, {"text": "] The SPSYNFUN of \"teacher\" in \"John is I the teacher\" is \"subject complement.", "labels": [], "entities": [{"text": "SPSYNFUN", "start_pos": 6, "end_pos": 14, "type": "METRIC", "confidence": 0.9512074589729309}]}], "tableCaptions": []}