{"title": [{"text": "A Connectionist Architecture for Learning to Parse", "labels": [], "entities": [{"text": "Parse", "start_pos": 45, "end_pos": 50, "type": "TASK", "confidence": 0.655316174030304}]}], "abstractContent": [{"text": "We present a connectionist architecture and demonstrate that it can learn syntactic parsing from a corpus of parsed text.", "labels": [], "entities": [{"text": "syntactic parsing from a corpus of parsed text", "start_pos": 74, "end_pos": 120, "type": "TASK", "confidence": 0.7628509849309921}]}, {"text": "The architecture can represent syntactic constituents, and can learn generalizations over syntactic constituents, thereby addressing the sparse data problems of previous connectionist ar-chitectures.", "labels": [], "entities": []}, {"text": "We apply these Simple Synchrony Networks to mapping sequences of word tags to parse trees.", "labels": [], "entities": []}, {"text": "After training on parsed samples of the Brown Corpus, the networks achieve precision and recall on constituents that approaches that of statistical methods for this task.", "labels": [], "entities": [{"text": "Brown Corpus", "start_pos": 40, "end_pos": 52, "type": "DATASET", "confidence": 0.9328005611896515}, {"text": "precision", "start_pos": 75, "end_pos": 84, "type": "METRIC", "confidence": 0.9989225268363953}, {"text": "recall", "start_pos": 89, "end_pos": 95, "type": "METRIC", "confidence": 0.9992095232009888}]}], "introductionContent": [{"text": "Connectionist networks are popular for many of the same reasons as statistical techniques.", "labels": [], "entities": []}, {"text": "They are robust and have effective learning algorithms.", "labels": [], "entities": []}, {"text": "They also have the advantage of learning their own internal representations, so they are less constrained by the way the system designer formulates the problem.", "labels": [], "entities": []}, {"text": "These properties and their prevalence in cognitive modeling has generated significant interest in the application of connectionist networks to natural language processing.", "labels": [], "entities": [{"text": "natural language processing", "start_pos": 143, "end_pos": 170, "type": "TASK", "confidence": 0.6337977250417074}]}, {"text": "However the results have been disappointing, being limited to artificial domains and oversimplified subproblems (e.g.).", "labels": [], "entities": []}, {"text": "Many have argued that these kinds of connectionist networks are simply not computationally adequate for learning the complexities of real natural language (e.g. (,).", "labels": [], "entities": []}, {"text": "Work on extending connectionist architectures for application to complex domains such as natural language syntax has developed a theoretically motivated technique called Temporal Synchrony Variable Binding (.", "labels": [], "entities": [{"text": "Temporal Synchrony Variable Binding", "start_pos": 170, "end_pos": 205, "type": "TASK", "confidence": 0.7777253538370132}]}, {"text": "TSVB allows syntactic constituency to be represented, but to date there has been no empirical demonstration of how a learning algorithm can be effectively applied to such a network.", "labels": [], "entities": [{"text": "TSVB", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.7455763816833496}]}, {"text": "In this paper we propose an architecture for TSVB networks and empirically demonstrate its ability to learn syntactic parsing, producing results approaching current statistical techniques.", "labels": [], "entities": [{"text": "syntactic parsing", "start_pos": 108, "end_pos": 125, "type": "TASK", "confidence": 0.6530240327119827}]}, {"text": "In the next section of this paper we present the proposed connectionist architecture, Simple Synchrony Networks (SSNs).", "labels": [], "entities": [{"text": "Simple Synchrony Networks (SSNs)", "start_pos": 86, "end_pos": 118, "type": "TASK", "confidence": 0.6235243231058121}]}, {"text": "SSNs area natural extension of Simple Kecurrent Networks (SRNs) (Elman, I99I), which are in turn a natural extension of Multi-Layered Perceptrons (MLPs).", "labels": [], "entities": [{"text": "Elman, I99I)", "start_pos": 65, "end_pos": 77, "type": "DATASET", "confidence": 0.9203318357467651}]}, {"text": "SRNs are an improvement over MLPs because they generalize what they have learned over words in different sentence positions.", "labels": [], "entities": []}, {"text": "SSNs are an improvement over SKNs because the use of TSVB gives them the additional ability to generalize over constituents in different structural positions.", "labels": [], "entities": []}, {"text": "The combination of these generalization abilities is what makes SSNs adequate for syntactic parsing.", "labels": [], "entities": [{"text": "SSNs", "start_pos": 64, "end_pos": 68, "type": "TASK", "confidence": 0.9814706444740295}, {"text": "syntactic parsing", "start_pos": 82, "end_pos": 99, "type": "TASK", "confidence": 0.7734988033771515}]}, {"text": "Section 3 presents experiments demonstrating SSNs' ability to learn syntactic parsing.", "labels": [], "entities": [{"text": "SSNs'", "start_pos": 45, "end_pos": 50, "type": "TASK", "confidence": 0.9872042536735535}, {"text": "syntactic parsing", "start_pos": 68, "end_pos": 85, "type": "TASK", "confidence": 0.6210544407367706}]}, {"text": "The task is to map a sentence's sequence of part of speech tags to either an unlabeled or labeled parse tree, as given in a preparsed sample of the Brown Corpus.", "labels": [], "entities": [{"text": "Brown Corpus", "start_pos": 148, "end_pos": 160, "type": "DATASET", "confidence": 0.9367076754570007}]}, {"text": "A network input-output format is developed for this task, along with some linguistic assumptions that were used to simplify these initial experiments.", "labels": [], "entities": []}, {"text": "Although only a small training set was used, an SSN achieved 63% precision and 69% recall on unlabeled constituents for previously unseen sentences.", "labels": [], "entities": [{"text": "SSN", "start_pos": 48, "end_pos": 51, "type": "TASK", "confidence": 0.9254780411720276}, {"text": "precision", "start_pos": 65, "end_pos": 74, "type": "METRIC", "confidence": 0.9995895028114319}, {"text": "recall", "start_pos": 83, "end_pos": 89, "type": "METRIC", "confidence": 0.9995337724685669}]}, {"text": "This is approaching the 75% precision and recall achieved on a similar task by Probabilistic Context Free Parsers (Charniak, forthcoming), which is the best current method for parsing based on part of speech tags alone.", "labels": [], "entities": [{"text": "precision", "start_pos": 28, "end_pos": 37, "type": "METRIC", "confidence": 0.9995777010917664}, {"text": "recall", "start_pos": 42, "end_pos": 48, "type": "METRIC", "confidence": 0.9995939135551453}]}, {"text": "Given that these are the very first results produced with this method, future developments are likely to improve on them, making the future for this method very promising.", "labels": [], "entities": []}], "datasetContent": [{"text": "The experiments in this paper use one of the Susanne genres (genre A, press reportage) for the selection of training, cross-validation and test data.", "labels": [], "entities": []}, {"text": "We describe three sets of experiments, training SSNs with the input-output format described in section 3.1.", "labels": [], "entities": []}, {"text": "In each experiment, a variety of networks was trained, varying the number of units in the hidden and combination layers.", "labels": [], "entities": []}, {"text": "Each network is trained using an extension of Backpropagation Through Time until the sum-squared error reaches a minimum.", "labels": [], "entities": []}, {"text": "A crossvalidation data set is used to choose the best networks, which are then given the test data, and precision/recall figures obtained.", "labels": [], "entities": [{"text": "precision", "start_pos": 104, "end_pos": 113, "type": "METRIC", "confidence": 0.999097466468811}, {"text": "recall", "start_pos": 114, "end_pos": 120, "type": "METRIC", "confidence": 0.8292372822761536}]}, {"text": "For experiments and, the first twelve files in Susanne genre A were used as a source for the training data, the next two for the cross-validation set (4700 words in 219 sentences, average length 21.56), and the final two for testing (4602 words in 176 sentences, average length 26.15).", "labels": [], "entities": []}, {"text": "For experiment (1), only sentences of length less than twenty words were used for training, resulting in a training set of 4683 words in 334 sentences.", "labels": [], "entities": []}, {"text": "The precision and recall results for the best network can be seen in the first row of table 1.", "labels": [], "entities": [{"text": "precision", "start_pos": 4, "end_pos": 13, "type": "METRIC", "confidence": 0.9995606541633606}, {"text": "recall", "start_pos": 18, "end_pos": 24, "type": "METRIC", "confidence": 0.9993891716003418}]}, {"text": "For experiment (2), a larger training set was used, containing sentences of length less than thirty words, resulting in a training set of 13,523 words in 696 sentences.", "labels": [], "entities": []}, {"text": "We averaged the performance of the best two networks to obtain the figures in the second row of table 1.", "labels": [], "entities": []}, {"text": "For experiment (3), labeled parse trees were used as a target output, i.e. for each word we also output the label of its parent constituent.", "labels": [], "entities": []}, {"text": "The output for the constituent labels uses one output unit for each of the 15 possible labels.", "labels": [], "entities": []}, {"text": "For calculating the precision and recall results, the network must also output the correct label with the head of a constituent in order to count that constituent as correct.", "labels": [], "entities": [{"text": "precision", "start_pos": 20, "end_pos": 29, "type": "METRIC", "confidence": 0.999535083770752}, {"text": "recall", "start_pos": 34, "end_pos": 40, "type": "METRIC", "confidence": 0.9983641505241394}]}, {"text": "Further, this experiment uses data sets selected at random from the total set, rather than taking blocks from the corpus.", "labels": [], "entities": []}, {"text": "Therefore, the cross-validation set in this case consists of 4551 words in 186 sentences, average length 24.47 words.", "labels": [], "entities": []}, {"text": "The test set consists of 4485 words in 181 sentences, average length 24.78 words.", "labels": [], "entities": []}, {"text": "As in experiment (2), we used a training set of sentences with less than 30 words, producing a set of 1079 sentences, 27,559 words.", "labels": [], "entities": []}, {"text": "For this experiment none of the networks we tried converged to nontrivial solutions on the training set, but one network achieved reasonable performance before it collapsed to a trivial solution.", "labels": [], "entities": []}, {"text": "The results for this network are shown in the third row of table 1.", "labels": [], "entities": []}, {"text": "From current corpus based statistical work on parsing, we know that sequences of part of speech tags contain enough information to achieve around 75% precision and recall on constituents (Charniak, forthcoming).", "labels": [], "entities": [{"text": "parsing", "start_pos": 46, "end_pos": 53, "type": "TASK", "confidence": 0.975734293460846}, {"text": "precision", "start_pos": 150, "end_pos": 159, "type": "METRIC", "confidence": 0.9990979433059692}, {"text": "recall", "start_pos": 164, "end_pos": 170, "type": "METRIC", "confidence": 0.9988654851913452}]}, {"text": "On the other extreme, the simplistic parsing strategy of producing a purely right branching structure only achieves 34% precision and 61% recall on our test set.", "labels": [], "entities": [{"text": "precision", "start_pos": 120, "end_pos": 129, "type": "METRIC", "confidence": 0.9995619654655457}, {"text": "recall", "start_pos": 138, "end_pos": 144, "type": "METRIC", "confidence": 0.9996746778488159}]}, {"text": "The fact that SSNs can achieve 63% precision and 69% recall using much smaller training sets than (Charniak, forthcoming) demonstrates that SSNs can be effective at learning the required generalizations from the data.", "labels": [], "entities": [{"text": "precision", "start_pos": 35, "end_pos": 44, "type": "METRIC", "confidence": 0.9991163611412048}, {"text": "recall", "start_pos": 53, "end_pos": 59, "type": "METRIC", "confidence": 0.9994459748268127}]}, {"text": "While there is still room for improvement, we conclude that SSNs can learn to parse real natural language.", "labels": [], "entities": [{"text": "SSNs", "start_pos": 60, "end_pos": 64, "type": "TASK", "confidence": 0.9594184160232544}, {"text": "parse real natural language", "start_pos": 78, "end_pos": 105, "type": "TASK", "confidence": 0.8441196084022522}]}], "tableCaptions": [{"text": " Table 1: Results of experiments on Susanne corpus.", "labels": [], "entities": [{"text": "Susanne corpus", "start_pos": 36, "end_pos": 50, "type": "DATASET", "confidence": 0.8414556086063385}]}]}