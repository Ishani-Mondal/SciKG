{"title": [{"text": "Flow Network Models for Word Alignment and Terminology Extraction from Bilingual Corpora", "labels": [], "entities": [{"text": "Word Alignment and Terminology Extraction", "start_pos": 24, "end_pos": 65, "type": "TASK", "confidence": 0.69924978017807}]}], "abstractContent": [{"text": "This paper presents anew model for word alignments between parallel sentences, which allows one to accurately estimate different parameters, in a computationally efficient way.", "labels": [], "entities": [{"text": "word alignments between parallel sentences", "start_pos": 35, "end_pos": 77, "type": "TASK", "confidence": 0.8530073404312134}]}, {"text": "An application of this model to bilingual terminology extraction , where terms are identified in one language and guessed, through the alignment process , in the other one, is also described.", "labels": [], "entities": [{"text": "bilingual terminology extraction", "start_pos": 32, "end_pos": 64, "type": "TASK", "confidence": 0.659827878077825}]}, {"text": "An experiment conducted on a small English-French parallel corpus gave results with high precision, demonstrating the validity of the model.", "labels": [], "entities": [{"text": "precision", "start_pos": 89, "end_pos": 98, "type": "METRIC", "confidence": 0.9958154559135437}]}], "introductionContent": [{"text": "Early works, (, and to a certain extent, presented methods to ex-~.:'~.ct bi'_.'i~gua!", "labels": [], "entities": []}, {"text": "le~cons of words from a parallel COl'p~s, relying on the distribution of the words in the set of parallel sentences (or other units).", "labels": [], "entities": []}, {"text": "() then extended their method and established a sound probabilistic model series, relying on different parameters describing how words within parallel sentences are aligned to each other.", "labels": [], "entities": []}, {"text": "On the other hand, ( proposed an algorithm, borrowed to the field of dynamic programming and based on the output of their previous work, to find the best alignment, subject to certain constraints, between words in parallel sentences.", "labels": [], "entities": []}, {"text": "A similar algorithm was used by.", "labels": [], "entities": []}, {"text": "Investigating alignments at the sentence level allows to clean and to refine the le~cons otherwise extracted from a parallel corpus as a whole, pruning what calls \"indirect associations\".", "labels": [], "entities": []}, {"text": "Now, what differentiates the models and algorithms proposed are the sets of parameters and constraints they rely on, their ability to find an appropriate solution under the constraints defined and their ability to nicely integrate new parameters.", "labels": [], "entities": []}, {"text": "We want to present here a model of the possible alignments in the form of flow networks.", "labels": [], "entities": []}, {"text": "This representation allows to define different kinds of alignments and to find the most probable or an approximation of this most probable alignment, under certain constraints.", "labels": [], "entities": []}, {"text": "Our procedure presents the advantage of an accurate modelling of the possible alignments, and can be used on small corpora.", "labels": [], "entities": []}, {"text": "We will introduce this model in the next section.", "labels": [], "entities": []}, {"text": "Section 3 describes a particular use of this model to find term translations, and presents the results we obtained for this task on a small corpus.", "labels": [], "entities": [{"text": "term translations", "start_pos": 59, "end_pos": 76, "type": "TASK", "confidence": 0.7279735207557678}]}, {"text": "Finally, the main features of our work and the research directions we envisage are summarized in the conlcusion.", "labels": [], "entities": []}], "datasetContent": [{"text": "In order to test the previous model, we selected a small bilingual corpus consisting of 1000 aligned sentences, from a corpus on satellite telecommunications.", "labels": [], "entities": []}, {"text": "We then ran the following algorithm, based on the previous model: 1.", "labels": [], "entities": []}, {"text": "tag and lemmatise the English and French texts, mark all the English candidate terms using morpho-syntactic rules encoded in regular expressions, 2.", "labels": [], "entities": []}, {"text": "build a first set of association probabilities, using the likelihood ratio test defined in, costs of edges linking English vertices to French ones as the opposite of the logarithm of the normalised sum of probabilities of all possible word associations defined by the edge (for the edge between multiple (el) access (e2) to the French unit acc~s (fl) mulitple (f2) it is \u00bc (~i,jp(ei, fj))), all the other edges receive an arbitrary cost value, compute the solved alignment, and increment the count of the associations obtained by overall value of the solved alignnlent,.", "labels": [], "entities": []}, {"text": "select the fisrt i00 unit associations according to their count, and consider them as valid.", "labels": [], "entities": []}, {"text": "Go back to step 2, excluding from the search space the associations selected, till all associations have been extracted.", "labels": [], "entities": []}], "tableCaptions": []}