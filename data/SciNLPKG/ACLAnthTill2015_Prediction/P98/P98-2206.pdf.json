{"title": [{"text": "Chinese Word Segmentation without Using Lexicon and Hand-crafted Training Data", "labels": [], "entities": [{"text": "Chinese Word Segmentation", "start_pos": 0, "end_pos": 25, "type": "TASK", "confidence": 0.6011777222156525}]}], "abstractContent": [{"text": "Chinese word segmentation is the first step in any Chinese NLP system.", "labels": [], "entities": [{"text": "Chinese word segmentation", "start_pos": 0, "end_pos": 25, "type": "TASK", "confidence": 0.5486105978488922}]}, {"text": "This paper presents anew algorithm for segmenting Chinese texts without making use of any lexicon and hand-crafted linguistic resource.", "labels": [], "entities": [{"text": "segmenting Chinese texts", "start_pos": 39, "end_pos": 63, "type": "TASK", "confidence": 0.873198131720225}]}, {"text": "The statistical data required by the algorithm, that is, mutual information and the difference of t-score between characters, is derived automatically from raw Chinese corpora.", "labels": [], "entities": []}, {"text": "The preliminary experiment shows that the segmentation accuracy of our algorithm is acceptable.", "labels": [], "entities": [{"text": "segmentation", "start_pos": 42, "end_pos": 54, "type": "TASK", "confidence": 0.9627159237861633}, {"text": "accuracy", "start_pos": 55, "end_pos": 63, "type": "METRIC", "confidence": 0.8687165975570679}]}, {"text": "We hope the gaining of this approach will be beneficial to improving the perfomaance(especially in ability to cope with unknown words and ability to adapt to various domains) of the existing segmenters, though the algorithm itself can also be utilized as a stand-alone segmenter in some NLP applications.", "labels": [], "entities": []}], "introductionContent": [{"text": "Any Chinese word is composed of either single or multiple characters.", "labels": [], "entities": []}, {"text": "Chinese texts are explicitly concatenations of characters, words are not delimited by spaces as that in English.", "labels": [], "entities": []}, {"text": "Chinese word segmentation is therefore the first step for any Chinese information processing system.", "labels": [], "entities": [{"text": "Chinese word segmentation", "start_pos": 0, "end_pos": 25, "type": "TASK", "confidence": 0.5649760067462921}]}, {"text": "Almost all methods for Chinese word segmentation developed so far, both statistical and rule-based, exploited two kinds of important resources, i.e., lexicon and hand-crafted linguistic resources(manually segmented and tagged corpus, knowledge for unknown words, and linguistic This work was supported in part by the National Natural Science Foundation of China under grant No. 69433010. rules).", "labels": [], "entities": [{"text": "Chinese word segmentation", "start_pos": 23, "end_pos": 48, "type": "TASK", "confidence": 0.6205686827500662}]}, {"text": "Lexicon is usually used as the means for finding segmentation candidates for input sentences, while linguistic resources for solving segnaentation ambiguities.", "labels": [], "entities": []}, {"text": "Preparation of these resources (well-defined lexicon, widely accepted tag set, consistent annotated corpus etc.) is very hard due to particularity of Chinese, and time consuming.", "labels": [], "entities": []}, {"text": "Furthermore, even the lexicon is large enough, and the corpus annotated is balanced and huge in size, the word segmenter will still face the problem of data incompleteness, sparseness and bias as it is utilized in different domains.", "labels": [], "entities": [{"text": "word segmenter", "start_pos": 106, "end_pos": 120, "type": "TASK", "confidence": 0.7110225111246109}]}, {"text": "An important issue in designing Chinese segmenters is thus how to reduce the effort of human supervision as much as possible.", "labels": [], "entities": []}, {"text": "conducted a Chinese segrnenter which merely made use of a manually segmented corpus(without referring to any lexicon).", "labels": [], "entities": []}, {"text": "A transformation-based algorithm was then explored to learn segmentation rules automatically from the segmented corpus.", "labels": [], "entities": []}, {"text": "further proposed a method using neither lexicon nor segmented corpus: for input texts, simply grouping character pairs with high value of mutual information into words.", "labels": [], "entities": []}, {"text": "Although this strategy is very simple and has many limitations(e.g., it can only treat bi-character words), the characteristic of it is that it is fully automatic --the nmtual information between characters can be trained from raw Chinese corpus directly.", "labels": [], "entities": []}, {"text": "Following the line of Sproat and Shih, here we present anew algorithm for segmenting Chinese texts which depends upon neither lexicon nor any hand-crafted resource.", "labels": [], "entities": [{"text": "segmenting Chinese texts", "start_pos": 74, "end_pos": 98, "type": "TASK", "confidence": 0.8825916846593221}]}, {"text": "All data necessary for our system is derived from the raw corpus.", "labels": [], "entities": []}, {"text": "The system maybe viewed as a stand-alone segmenter in some applications (preliminary experiments show that its accuracy is acceptable); nevertheless, our main purpose is to study how and how well the work can be done by machine at the extreme conditions, say, without any assistance of human.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 111, "end_pos": 119, "type": "METRIC", "confidence": 0.9990941286087036}]}, {"text": "We believe the performance of the existing Chinese segmenters, that is, the ability to deal with segmentation ambiguities and unknown words as well as the ability to adapt to new domains, will be improved in some degree if the gaining of this approach is incorporated into systems properly.", "labels": [], "entities": []}], "datasetContent": [{"text": "We select 100 Chinese sentences, consisting of 1588 characters(or 1587 locations between character pairs) randomly as testing texts.", "labels": [], "entities": []}, {"text": "The statistical data required by calculating mi and dts, in fact it is character bigram, is automatically derived from a news corpus of about 20M Chinese characters.", "labels": [], "entities": []}, {"text": "The testing texts and training corpus are mutually excluded.", "labels": [], "entities": []}, {"text": "Out of 1587 locations in the testing texts, 1456 are correctly marked by our algorithm.", "labels": [], "entities": [{"text": "correctly", "start_pos": 53, "end_pos": 62, "type": "METRIC", "confidence": 0.9543505907058716}]}, {"text": "We define the accuracy of segmentation as: # of locations being correctly marked", "labels": [], "entities": [{"text": "accuracy", "start_pos": 14, "end_pos": 22, "type": "METRIC", "confidence": 0.9995549321174622}, {"text": "segmentation", "start_pos": 26, "end_pos": 38, "type": "TASK", "confidence": 0.973189115524292}]}], "tableCaptions": []}