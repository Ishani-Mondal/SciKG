{"title": [{"text": "A Multi-Neuro Tagger Using Variable Lengths of Contexts", "labels": [], "entities": [{"text": "Multi-Neuro Tagger", "start_pos": 2, "end_pos": 20, "type": "TASK", "confidence": 0.732305571436882}]}], "abstractContent": [{"text": "This paper presents a multi-neuro tagger that uses variable lengths of contexts and weighted inputs (with information gains) for part of speech tagging.", "labels": [], "entities": [{"text": "multi-neuro tagger", "start_pos": 22, "end_pos": 40, "type": "TASK", "confidence": 0.7482078671455383}, {"text": "speech tagging", "start_pos": 137, "end_pos": 151, "type": "TASK", "confidence": 0.7522925436496735}]}, {"text": "Computer experiments show that it has a correct rate of over 94% for tagging ambiguous words when a small Thai corpus with 22,311 ambiguous words is used for training.", "labels": [], "entities": [{"text": "correct rate", "start_pos": 40, "end_pos": 52, "type": "METRIC", "confidence": 0.9808176457881927}, {"text": "tagging ambiguous words", "start_pos": 69, "end_pos": 92, "type": "TASK", "confidence": 0.8830371101697286}]}, {"text": "This result is better than any of the results obtained using the single-neuro taggers with fixed but different lengths of contexts, which indicates that the multi-neuro tagger can dynamically find a suitable length of contexts in tagging.", "labels": [], "entities": []}], "introductionContent": [{"text": "Words are often ambiguous in terms of their part of speech (POS).", "labels": [], "entities": []}, {"text": "POS tagging disambiguates them, i.e., it assigns to each word the correct POS in the context of the sentence.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 0, "end_pos": 11, "type": "TASK", "confidence": 0.793647050857544}, {"text": "POS", "start_pos": 74, "end_pos": 77, "type": "METRIC", "confidence": 0.9509240388870239}]}, {"text": "Several kinds of POS taggers using rule-based (e.g.,, statistical (e.g.,, memory-based (e.g.,, and neural network (e.g.,) models have been proposed for some languages.", "labels": [], "entities": [{"text": "POS taggers", "start_pos": 17, "end_pos": 28, "type": "TASK", "confidence": 0.8536489605903625}]}, {"text": "The correct rate of tagging of these models has reached 95%, in part by using a very large amount of training data (e.g., 1,000,000 words in.", "labels": [], "entities": [{"text": "correct rate", "start_pos": 4, "end_pos": 16, "type": "METRIC", "confidence": 0.9624705910682678}]}, {"text": "For many other languages (e.g., Thai, which we deal within this paper), however, the corpora have not been prepared and there is not a large amount of training data available.", "labels": [], "entities": []}, {"text": "It is therefore important to construct a practical tagger using as few training data as possible.", "labels": [], "entities": []}, {"text": "In most of the statistical and neural network models proposed so far, the length of the contexts used for tagging is fixed and has to be selected empirically.", "labels": [], "entities": []}, {"text": "In addition, all words in the input are regarded to have the same relevance in tagging.", "labels": [], "entities": []}, {"text": "An ideal model would be one in which the length of the contexts can be automatically selected as needed in tagging and the words used in tagging can be given different relevances.", "labels": [], "entities": []}, {"text": "A simple but effective solution is to introduce a multi-module tagger composed of multiple modules (basic taggers) with fixed but different lengths of contexts in the input and a selector (a selecting rule) to obtain the final answer.", "labels": [], "entities": []}, {"text": "The tagger should also have a set of weights reflecting the different relevances of the input elements.", "labels": [], "entities": []}, {"text": "If we construct such a multimodule tagger with statistical methods (e.g., ngram models), however, the size of the n-gram table would be extremely large, as mentioned in Sec.", "labels": [], "entities": []}, {"text": "On the other hand, in memory-based models such as IGtree, the number of features used in tagging is actually variable, within the maximum length (i.e., the number of features spanning the tree), and the different relevances of the different features are taken into account in tagging.", "labels": [], "entities": [{"text": "IGtree", "start_pos": 50, "end_pos": 56, "type": "DATASET", "confidence": 0.8888364434242249}]}, {"text": "Tagging by this approach, however, maybe computationally expensive if the maximum length is large.", "labels": [], "entities": [{"text": "Tagging", "start_pos": 0, "end_pos": 7, "type": "TASK", "confidence": 0.9714473485946655}]}, {"text": "Actually, the maximum length was set at 4 in Daelemans's model, which can therefore be regarded as one using fixed length of contexts.", "labels": [], "entities": []}, {"text": "This paper presents a multi-neuro tagger that is constructed using multiple neural networks, all of which can be regarded as singleneuro taggers with fixed but different lengths of contexts in inputs.", "labels": [], "entities": [{"text": "multi-neuro tagger", "start_pos": 22, "end_pos": 40, "type": "TASK", "confidence": 0.7303036153316498}]}, {"text": "The tagger performs POS tagging in different lengths of contexts based on longest context priority.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 20, "end_pos": 31, "type": "TASK", "confidence": 0.8597521185874939}]}, {"text": "Given that the target word is more relevant than any of the words in its context and that the words in context may have different relevances in tagging, each element of the input is weighted with information gains, i.e., numbers expressing the average amount of reduction of training set information entropy when the POSs of the element are known.", "labels": [], "entities": []}, {"text": "By using the trained results (weights) of the single-neuro taggers with short inputs as initial weights of those with long inputs, the training time for the latter ones can be greatly reduced and the cost to train a multineuro tagger is almost the same as that to train a single-neuro tagger.", "labels": [], "entities": []}], "datasetContent": [{"text": "The Thai corpus used in the computer experiments contains 10,452 sentences that are randomly divided into two sets: one with 8,322 sentences for training and another with 2,130 sentences for testing.", "labels": [], "entities": [{"text": "Thai corpus", "start_pos": 4, "end_pos": 15, "type": "DATASET", "confidence": 0.9038211107254028}]}, {"text": "The training and testing sets contain, respectively, 22,311 and 6,717 ambiguous words that serve as more than one POS and were used for training and testing.", "labels": [], "entities": []}, {"text": "Because there are 47 types of POSs in Thai (, n in,, and (14) was set at 47.", "labels": [], "entities": [{"text": "Thai", "start_pos": 38, "end_pos": 42, "type": "DATASET", "confidence": 0.8995782732963562}]}, {"text": "The single neuro-taggers are 3-layer neural networks whose input length, l(IPT) (=l+ l+r), is set to 3-7 and whose size is p x 2 ax n, where p = n x I(IPT).", "labels": [], "entities": []}, {"text": "The multineuro tagger is constructed by five (i.e., rn = .5) single-neuro taggers, SNTi (i = 1,...,.5), in which l(IPTi) = 2 + i. shows that no matter whether the information gain (IG) was used or not, the multi-neuro tagger has a correct rate of over 94%, which is higher than that of any of the single-neuro taggers.", "labels": [], "entities": [{"text": "information gain (IG)", "start_pos": 163, "end_pos": 184, "type": "METRIC", "confidence": 0.6176438391208648}, {"text": "correct rate", "start_pos": 231, "end_pos": 243, "type": "METRIC", "confidence": 0.9887733161449432}]}, {"text": "This indicates that by using the multi-neuro tagger the length of the context need not be chosen empirically; it can be selected dynamically instead.", "labels": [], "entities": []}, {"text": "If we focus on the single-neuro taggers with inputs greater than four, we can see that the taggers with information gain are superior to those without information gain.", "labels": [], "entities": []}, {"text": "Note that the correct rates shown in the table were obtained when only counting the ambiguous words in the testing set.", "labels": [], "entities": []}, {"text": "The correct rate of the multi-neuro tagger is 98.9% if all the words in the testing set (the ratio of ambiguous words was 0.19) are counted.", "labels": [], "entities": [{"text": "correct rate", "start_pos": 4, "end_pos": 16, "type": "METRIC", "confidence": 0.972284585237503}, {"text": "multi-neuro tagger", "start_pos": 24, "end_pos": 42, "type": "TASK", "confidence": 0.7264906466007233}]}, {"text": "Moreover, although the overall performance is not improved much by adopting the information gains, the training can be greatly speeded up.", "labels": [], "entities": []}, {"text": "It takes 1024 steps to train the first tagger, SNT1, when the information gains are not used and only 664 steps to train the same tagger when the information gains are used.", "labels": [], "entities": []}, {"text": "shows learning (training) curves in different cases for the single-neuro tagger with six input elements.", "labels": [], "entities": []}, {"text": "Thick line shows the casein which the tagger is trained by using trained weights of the tagger with five input elements as initial values.", "labels": [], "entities": []}, {"text": "The thin line shows the casein which the tagger is trained independently.", "labels": [], "entities": []}, {"text": "The dashed line shows the casein which the tagger is trained independently and does not use the information gain.", "labels": [], "entities": []}, {"text": "From this figure, we know that the training time can be greatly reduced by using the previous result and the information gain.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1. Results of POS Tagging for Testing Data  Taggers  \"single-neuro\"  \"multi-neuro\"", "labels": [], "entities": [{"text": "POS Tagging", "start_pos": 21, "end_pos": 32, "type": "TASK", "confidence": 0.7874064743518829}, {"text": "Testing Data  Taggers", "start_pos": 37, "end_pos": 58, "type": "TASK", "confidence": 0.6882307132085165}]}]}