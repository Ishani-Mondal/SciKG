{"title": [{"text": "Discourse Cues for Broadcast News Segmentation", "labels": [], "entities": [{"text": "Broadcast News Segmentation", "start_pos": 19, "end_pos": 46, "type": "TASK", "confidence": 0.5746271312236786}]}], "abstractContent": [{"text": "This paper describes the design and application of time-enhanced, finite state models of discourse cues to the automated segmentation of broadcast news.", "labels": [], "entities": [{"text": "automated segmentation of broadcast news", "start_pos": 111, "end_pos": 151, "type": "TASK", "confidence": 0.8213055610656739}]}, {"text": "We describe our analysis of a broadcast news corpus, the design of a discourse cue based story segmentor that builds upon information extraction techniques, and finally its computational implementation and evaluation in the Broadcast News Navigator (BNN) to support video news browsing, retrieval, and summarization.", "labels": [], "entities": [{"text": "information extraction", "start_pos": 122, "end_pos": 144, "type": "TASK", "confidence": 0.7400962114334106}, {"text": "summarization", "start_pos": 302, "end_pos": 315, "type": "TASK", "confidence": 0.9909731149673462}]}], "introductionContent": [{"text": "Large video collections require content-based information browsing, retrieval, extraction, and summarization to ensure their value for tasks such as real-time profiling and retrospective search.", "labels": [], "entities": [{"text": "summarization", "start_pos": 95, "end_pos": 108, "type": "TASK", "confidence": 0.9830418825149536}, {"text": "retrospective search", "start_pos": 173, "end_pos": 193, "type": "TASK", "confidence": 0.9317062497138977}]}, {"text": "Whereas image processing for video indexing currently provides low level indec~s such as visual transitions and shot classification (, some research has investigated the use of linguistic streams (e.g., closed captions, transcripts) to provide keyword-based indexes to video.", "labels": [], "entities": [{"text": "video indexing", "start_pos": 29, "end_pos": 43, "type": "TASK", "confidence": 0.7009714841842651}, {"text": "shot classification", "start_pos": 112, "end_pos": 131, "type": "TASK", "confidence": 0.7483373284339905}]}, {"text": "For example, traditional text tiling approaches often undersegment broadcast news because of rapid topic shifts (.", "labels": [], "entities": [{"text": "text tiling", "start_pos": 25, "end_pos": 36, "type": "TASK", "confidence": 0.7198103815317154}]}, {"text": "This paper takes a corpus-based approach to this problem, building linguistic models based on an analysis of a digital collection of broadcast news, exploiting the regularity utilized by humans in signaling topic shifts to detect story segments.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluated segmentor performance by measuring both the precision and recall of segment boundaries compared to manual annotation of story boundaries where: 1 presents average precision and recall results for multiple programs after applying generalized cue patterns developed first for ABC as described in Section 2.2.", "labels": [], "entities": [{"text": "precision", "start_pos": 57, "end_pos": 66, "type": "METRIC", "confidence": 0.9987012147903442}, {"text": "recall", "start_pos": 71, "end_pos": 77, "type": "METRIC", "confidence": 0.9867952466011047}, {"text": "precision", "start_pos": 176, "end_pos": 185, "type": "METRIC", "confidence": 0.9917071461677551}, {"text": "recall", "start_pos": 190, "end_pos": 196, "type": "METRIC", "confidence": 0.9736933708190918}]}, {"text": "Recall degrades when porting these same algorithms to different news programs (e.g., CNN, Jim Lehrer) given the genre differences as described in Section 2.1.", "labels": [], "entities": []}, {"text": "Errors in story boundary detection include erroneously splitting a single story segment into two story segments, and merging two contiguous story segments into a single story segment.", "labels": [], "entities": [{"text": "story boundary detection", "start_pos": 10, "end_pos": 34, "type": "TASK", "confidence": 0.8809998631477356}]}, {"text": "Furthermore, given our error-driven transformation based proper name taggers operate at approximately 80% precision and recall, this can adversely impact discourse cue detections.", "labels": [], "entities": [{"text": "proper name taggers", "start_pos": 57, "end_pos": 76, "type": "TASK", "confidence": 0.6973047455151876}, {"text": "precision", "start_pos": 106, "end_pos": 115, "type": "METRIC", "confidence": 0.9992154836654663}, {"text": "recall", "start_pos": 120, "end_pos": 126, "type": "METRIC", "confidence": 0.9984188079833984}, {"text": "discourse cue detections", "start_pos": 154, "end_pos": 178, "type": "TASK", "confidence": 0.7463740507761637}]}, {"text": "Also, our preliminary evaluation of speech transcription results in word error rates of approximately 50%, which suggest non captioned text is not yet feasible for this class of segmentation.", "labels": [], "entities": [{"text": "speech transcription", "start_pos": 36, "end_pos": 56, "type": "TASK", "confidence": 0.7018028646707535}, {"text": "word error rates", "start_pos": 68, "end_pos": 84, "type": "METRIC", "confidence": 0.7035450140635172}]}, {"text": "We have just completed an empirical study (Merlino and Maybury, forthcoming) with BNN users that explores the optimal mixture of media elements show in (e.g., keyframes, named entities, topics) in terms of speed and accuracy of story identification and comprehension tasks.", "labels": [], "entities": [{"text": "speed", "start_pos": 206, "end_pos": 211, "type": "METRIC", "confidence": 0.9878295660018921}, {"text": "accuracy", "start_pos": 216, "end_pos": 224, "type": "METRIC", "confidence": 0.9980319142341614}, {"text": "story identification", "start_pos": 228, "end_pos": 248, "type": "TASK", "confidence": 0.7278415262699127}]}, {"text": "Key findings include that users perform better and prefer mixed media presentations over just one media (e.g., named entities or topic lists), and they are quicker and more accurate working from extracts and summaries than from the source transcript or video.", "labels": [], "entities": []}], "tableCaptions": []}