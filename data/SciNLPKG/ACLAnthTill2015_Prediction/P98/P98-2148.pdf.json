{"title": [{"text": "A Stochastic Language Model using Dependency and Its Improvement by Word Clustering", "labels": [], "entities": []}], "abstractContent": [{"text": "In this paper, we present a stochastic language model for Japanese using dependency.", "labels": [], "entities": []}, {"text": "The prediction unit in this model is all attribute of \"bunsetsu\".", "labels": [], "entities": []}, {"text": "This is represented by the product of the head of content words and that of function words.", "labels": [], "entities": []}, {"text": "The relation between the attributes of \"bunsetsu\" is ruled by a context-free grammar.", "labels": [], "entities": []}, {"text": "The word sequences axe predicted from the attribute using word n-gram model.", "labels": [], "entities": []}, {"text": "The spell of Unknow word is predicted using character n-grain model.", "labels": [], "entities": []}, {"text": "This model is robust in that it can compute the probability of an arbitrary string and is complete in that it models from unknown word to dependency at the same time.", "labels": [], "entities": []}, {"text": "1 Introduction An effectiveness of stochastic language modeling as a methodology of natural language processing has been attested by various applications to the recognition system such as speech recognition and to the analysis system such as paxt-of-speech (POS) tagger.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 188, "end_pos": 206, "type": "TASK", "confidence": 0.7790187299251556}, {"text": "paxt-of-speech (POS) tagger", "start_pos": 242, "end_pos": 269, "type": "TASK", "confidence": 0.5572405099868775}]}, {"text": "In this methodology a stochastic language model with some parameters is built and they axe estimated in order to maximize its prediction power (minimize the cross entropy) on an unknown input.", "labels": [], "entities": []}, {"text": "Considering a single application, it might be better to estimate the parameters taking account of expected accuracy of recognition or analysis.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 107, "end_pos": 115, "type": "METRIC", "confidence": 0.9982807636260986}]}, {"text": "This method is, however, heavily dependent on the problem and of_ fers no systematic solution, as fax as we know.", "labels": [], "entities": []}, {"text": "The methodology of stochastic language modeling, however , allows us to separate, from various frameworks of natural language processing, the language description model common to them and enables us a systematic improvement of each application.", "labels": [], "entities": [{"text": "stochastic language modeling", "start_pos": 19, "end_pos": 47, "type": "TASK", "confidence": 0.7220220764478048}]}, {"text": "In this framework a description on a language is represented as a map from a sequence of alphabetic characters to a probability value.", "labels": [], "entities": []}, {"text": "The first model is C.", "labels": [], "entities": []}, {"text": "E. Shannon's n-gram model (Shannon, 1951).", "labels": [], "entities": []}, {"text": "The parameters of the model are estimated from the frequency of n character sequences of the alphabet (n-gram) on a corpus containing a large number of sentences of a language.", "labels": [], "entities": []}, {"text": "This is the same model as 0 This work is done when the auther was at Kyoto Univ. used in almost all of the recent practicM applications in that it describes only relations between sequential elements.", "labels": [], "entities": [{"text": "Kyoto Univ.", "start_pos": 69, "end_pos": 80, "type": "DATASET", "confidence": 0.970434695482254}]}, {"text": "Some linguistic phenomena, however, axe better described by assuming relations between sep-axated elements.", "labels": [], "entities": []}, {"text": "And modeling this kind of phenomena , the accuracies of various application axe generally augmented.", "labels": [], "entities": []}, {"text": "As for English, there have been researches in which a stochastic context-free grammar (SCFG) (Fujisaki et ~1., 1989) is used for model description.", "labels": [], "entities": [{"text": "model description", "start_pos": 129, "end_pos": 146, "type": "TASK", "confidence": 0.6861993372440338}]}, {"text": "Recently some researchers have pointed out the importance of the lexicon and proposed lexicalized models (Jelinek et al., 1994; Collins, 1997).", "labels": [], "entities": []}, {"text": "In these models, every headword is propagated up through the derivation tree such that every parent receives a headword from the head-child.", "labels": [], "entities": []}, {"text": "This kind of specialization may, however, be excessive if the criterion is predictive power of the model.", "labels": [], "entities": []}, {"text": "Research ~med at estimating the best specialization level for 2-gram model (Mori et aL, 1997) shows a class-based model is more predictive than a word-based 2-gram model, a completely lexicalized model, comparing cross en-tropy of a POS-based 2-graxa model, a word-based 2-gram model and a class-based 2-graxa model, estimated from information theoretical point of view.", "labels": [], "entities": []}, {"text": "As fora parser based on a class-based SCFG, Chax-niak (1997) reports better accuracy than the above lexicalized models, but the clustering method is not clear enough and, in addition, there is no report on predictive power (cross entropy or perplexity).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 76, "end_pos": 84, "type": "METRIC", "confidence": 0.9992433786392212}]}, {"text": "Hogenhout and Matsumoto (1997) propose a word-clustering method based on syntactic behavior, but no language model is discussed.", "labels": [], "entities": []}, {"text": "As the experiments in the present paper attest, word-class relation is dependent on language model.", "labels": [], "entities": []}, {"text": "In this paper, taking Japanese as the object language , we propose two complete stochastic language models using dependency between bugsetsu, a sequence of one or more content words followed by zero, one or more function words, and evaluate their predictive power by cross entropy.", "labels": [], "entities": []}, {"text": "Since the number of sorts of bunsetsu is enormous, considering it as a symbol to be predicted would surely invoke the data-sparseness problem.", "labels": [], "entities": []}, {"text": "To cope with this problem we 898", "labels": [], "entities": []}], "introductionContent": [], "datasetContent": [{"text": "We constructed the POS-based dependency model and the class-based dependency model to evaluate their predictive power.", "labels": [], "entities": []}, {"text": "In addition, we implemented parsers based on them which calculate the best syntactic tree from a given sequence of bun~etsu to observe their accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 141, "end_pos": 149, "type": "METRIC", "confidence": 0.9983626008033752}]}, {"text": "In this section, we present the experimental results and discuss them.", "labels": [], "entities": []}, {"text": "As a syntactically annotated corpus we used EDR corpus (Jap, 1993).", "labels": [], "entities": [{"text": "EDR corpus (Jap, 1993)", "start_pos": 44, "end_pos": 66, "type": "DATASET", "confidence": 0.9241512332643781}]}, {"text": "The corpus was divided into ten parts and the models estimated from nine of them were tested on the rest in terms of cross entropy (see).", "labels": [], "entities": []}, {"text": "The number of characters in the Japanese writing system is set to 6,879.", "labels": [], "entities": []}, {"text": "Two parameters which have not been determined yet in the explanation of the models (dmaz and v,naz) axe both set to 1.", "labels": [], "entities": []}, {"text": "Although the best value for each of them can also be estimated using the average cross entropy, they are fixed through the experiments.", "labels": [], "entities": []}, {"text": "For the purpose of evaluating the predictive power of the models, we calculated their cross entropy on the test corpus.", "labels": [], "entities": []}, {"text": "In this process the annotated tree is used as the structure of the sentences in the test corpus.", "labels": [], "entities": []}, {"text": "Therefore the probability of each sentence in the test corpus is not the summation overall its possible derivations.", "labels": [], "entities": []}, {"text": "In order to compare the POSbased dependency model and the class-based dependency model, we constructed these models from the same learning corpus and calculated their cross entropy on the same test corpus.", "labels": [], "entities": []}, {"text": "They are both interpolated with the SCFG with uniform distribution.", "labels": [], "entities": [{"text": "SCFG", "start_pos": 36, "end_pos": 40, "type": "DATASET", "confidence": 0.8501957654953003}]}, {"text": "The processes for their construction are as follows: \u2022 POS-based dependency model 1.", "labels": [], "entities": []}, {"text": "estimate the interpolation coefficients in Formula (4) by the deleted interpolation method 2.", "labels": [], "entities": []}, {"text": "count the frequency of each rewriting rule on the whole learning corpus \u2022 class-based dependency model 1.", "labels": [], "entities": []}, {"text": "estimate the interpolation coefficients in Formula (4) by the deleted interpolation method 2.", "labels": [], "entities": []}, {"text": "calculate an optimal word-class relation by the method proposed in Section 3. 3. count the frequency of each rewriting rule on the whole learning corpus The word-based 2-gram model for bunsetsu generation and the character-based 2-gram model as an unknown word model) are common to the POS-based model and class-based model.", "labels": [], "entities": [{"text": "bunsetsu generation", "start_pos": 185, "end_pos": 204, "type": "TASK", "confidence": 0.6841006428003311}]}, {"text": "Their contribution to the cross entropy is constant on the condition that the dependency models contain the prediction of the last word of the content word sequence and that of the function word sequence.", "labels": [], "entities": []}, {"text": "shows the cross entropy of each model on the test corpus.", "labels": [], "entities": []}, {"text": "The cross entropy of the classbased dependency model is lower than that of the POS-based dependency model.", "labels": [], "entities": []}, {"text": "This result attests experimentally that the class-based model estimated by our clustering method is more predictive than the POS-based model and that our word clustering method is efficient at improvement of a dependency model.", "labels": [], "entities": [{"text": "word clustering", "start_pos": 154, "end_pos": 169, "type": "TASK", "confidence": 0.6976582705974579}]}, {"text": "We also calculated the cross entropy of the classbased model which we estimated with a word 2-gram model as the model Min the Formula (5).", "labels": [], "entities": []}, {"text": "The number of terminals and non-terminals is 1,148,916 and the cross entropy is 6.3358, which is much higher than that of the POS-base model.", "labels": [], "entities": []}, {"text": "This result indicates that the best word-class relation for the dependency model is quite different from the best wordclass relation for the n-gram model.", "labels": [], "entities": []}, {"text": "Comparing the number of the terminals and non-terminals, the best word-class relation for n-gram model is exceedingly specialized fora dependency model.", "labels": [], "entities": []}, {"text": "We can conclude that word-class relation depends on the language model.", "labels": [], "entities": []}, {"text": "SVe implemented a parser based on the dependency models.", "labels": [], "entities": [{"text": "SVe", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.8681336641311646}]}, {"text": "Since our models, equipped with a wordbased 2-graan model for bunsetsu generation and the character-based 2-gram as an unknown word model, can return the probability for amy input, we can build a parser, based on our model, receiving a character sequence as input.", "labels": [], "entities": [{"text": "bunsetsu generation", "start_pos": 62, "end_pos": 81, "type": "TASK", "confidence": 0.6363044828176498}]}, {"text": "Its evaluation is not easy, however, because errors may occur in bunsetsu generation or in POS estimation of unknown words.", "labels": [], "entities": [{"text": "bunsetsu generation", "start_pos": 65, "end_pos": 84, "type": "TASK", "confidence": 0.6672966331243515}, {"text": "POS estimation of unknown words", "start_pos": 91, "end_pos": 122, "type": "TASK", "confidence": 0.8299709796905518}]}, {"text": "For this reason, in the following description, we assume a bunsetsu sequence as the input.", "labels": [], "entities": []}, {"text": "The criterion we adopted is the accuracy of dependency relation, but the last bunsetsu, which has no bunsetsu to depend on, and the second-to-last bunsetsu, which depends always on the last bunsetsu, are excluded from consideration.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 32, "end_pos": 40, "type": "METRIC", "confidence": 0.9993964433670044}]}, {"text": "shows cross entropy and parsing accuracy of the POS-based dependency model and the classbased dependency model.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 32, "end_pos": 40, "type": "METRIC", "confidence": 0.977590799331665}]}, {"text": "This result tells us our word clustering method increases parsing accuracy considerably.", "labels": [], "entities": [{"text": "word clustering", "start_pos": 25, "end_pos": 40, "type": "TASK", "confidence": 0.7560327053070068}, {"text": "parsing", "start_pos": 58, "end_pos": 65, "type": "TASK", "confidence": 0.9689351916313171}, {"text": "accuracy", "start_pos": 66, "end_pos": 74, "type": "METRIC", "confidence": 0.9614322781562805}]}, {"text": "This is quite natural in the light of the decrease of cross entropy.", "labels": [], "entities": []}, {"text": "The relation between the learning corpus size and cross entropy or parsing accuracy is shown in.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 75, "end_pos": 83, "type": "METRIC", "confidence": 0.9037148952484131}]}, {"text": "The lower bound of cross entropy is the entropy of Japanese, which is estimated to be 4.3033 bit.", "labels": [], "entities": [{"text": "entropy", "start_pos": 40, "end_pos": 47, "type": "METRIC", "confidence": 0.9835889339447021}, {"text": "Japanese", "start_pos": 51, "end_pos": 59, "type": "DATASET", "confidence": 0.8685017228126526}]}, {"text": "Taking this fact into consideration, the cross entropy of both of the models has stronger tendency to decrease.", "labels": [], "entities": []}, {"text": "As for ac- curacy, there also is a tendency to get more accurate as the learning corpus size increases, but it is a strong tendency for the class-based model than for the POS-based model.", "labels": [], "entities": [{"text": "accurate", "start_pos": 56, "end_pos": 64, "type": "METRIC", "confidence": 0.9622517824172974}]}, {"text": "It follows that the class-based model profits more greatly from an increase of the learning corpus size.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: Accuracy of each model.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9987660646438599}]}]}