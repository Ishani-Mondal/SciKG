{"title": [{"text": "Learning Optimal Dialogue Strategies: A Case Study of a Spoken Dialogue Agent for Email", "labels": [], "entities": [{"text": "Learning Optimal Dialogue Strategies", "start_pos": 0, "end_pos": 36, "type": "TASK", "confidence": 0.6706674620509148}, {"text": "Email", "start_pos": 82, "end_pos": 87, "type": "TASK", "confidence": 0.6506653428077698}]}], "abstractContent": [{"text": "This paper describes a novel method by which a dialogue agent can learn to choose an optimal dialogue strategy.", "labels": [], "entities": []}, {"text": "While it is widely agreed that dialogue strategies should be formulated in terms of communicative intentions, there has been little work on automatically optimizing an agent's choices when there are multiple ways to realize a communicative intention.", "labels": [], "entities": []}, {"text": "Our method is based on a combination of learning algorithms and empirical evaluation techniques.", "labels": [], "entities": []}, {"text": "The learning component of our method is based on algorithms for reinforcement learning, such as dynamic programming and Q-learning.", "labels": [], "entities": []}, {"text": "The empirical component uses the PARADISE evaluation framework (Walker et al., 1997) to identify the important performance factors and to provide the performance function needed by the learning algorithm.", "labels": [], "entities": [{"text": "PARADISE", "start_pos": 33, "end_pos": 41, "type": "METRIC", "confidence": 0.9309543371200562}]}, {"text": "We illustrate our method with a dialogue agent named ELVIS (EmaiL Voice Interactive System), that supports access to email over the phone.", "labels": [], "entities": [{"text": "ELVIS", "start_pos": 53, "end_pos": 58, "type": "METRIC", "confidence": 0.8710283041000366}]}, {"text": "We show how ELVIS can learn to choose among alternate strategies for agent initiative, for reading messages, and for summarizing email folders.", "labels": [], "entities": [{"text": "summarizing email folders", "start_pos": 117, "end_pos": 142, "type": "TASK", "confidence": 0.9197782278060913}]}], "introductionContent": [{"text": "This paper describes a novel method by which a dialogue agent can learn to choose an optimal dialogue strategy.", "labels": [], "entities": []}, {"text": "The main problem for dialogue agents is deciding what information to communicate to a hearer and how and when to communicate it.", "labels": [], "entities": []}, {"text": "For example, consider one of the strategy choices faced by a spoken dialogue agent that accesses email by phone.", "labels": [], "entities": []}, {"text": "When multiple messages match the user's query, e.g. Read my messages from Kim, an email agent must choose among multiple response strategies.", "labels": [], "entities": []}, {"text": "The agent might choose the Read-First strategy in DI: D1 involves summarizing all the messages from Kim, and then taking the initiative to read the first one.", "labels": [], "entities": []}, {"text": "Alternate strategies are the Read-SummaryOnly strategy in D2, where the agent provides information that allows users to refine their selection criteria, and the Read-Choice-Prompt strategy in D3, where the agent explicitly tells the user what to say in order to refine the selection: (D2) A: In the messages from Kim, there's 1 message about \"Interviewing Antonio\" and 1 message about \"Meeting Today:' (D3) A: In the messages from Kim, there's 1 message about \"Interviewing Antonio\" and 1 message about \"Meeting Today:' To hear the messages, say, \"Interviewing Antonio\" or \"Meeting.\"", "labels": [], "entities": []}, {"text": "Decision theoretic planning can be applied to the problem of choosing among strategies, by associating a utility U with each strategy (action) choice and by positing that agents should adhere to the Maximum Expected Utility Principle (,", "labels": [], "entities": [{"text": "Decision theoretic planning", "start_pos": 0, "end_pos": 27, "type": "TASK", "confidence": 0.7880441943804423}]}], "datasetContent": [{"text": "Experimental dialogues were collected via two experiments in which users (AT&T summer interns and MIT graduate students) interacted with ELVIS to complete three representative application tasks that required them to access email messages in three different email inboxes.", "labels": [], "entities": []}, {"text": "In the second experiment, users participated in a tutorial dialogue before doing the three tasks.", "labels": [], "entities": []}, {"text": "The first experiment varied initiative strategies and the second experiment varied the presentation strategies for reading messages and summarizing folders.", "labels": [], "entities": [{"text": "summarizing folders", "start_pos": 136, "end_pos": 155, "type": "TASK", "confidence": 0.8962473571300507}]}, {"text": "In order to have adequate data for learning, the agent must explore the space of strategy combinations and collect enough samples of each combination.", "labels": [], "entities": []}, {"text": "In the second experiment, we parameterized the agent so that each user interacted with three different versions of ELVIS, one for each task.", "labels": [], "entities": []}, {"text": "These experiments resulted in a corpus of 108 dialogues testing the initiative strategies, and a corpus of 124 dialogues testing the presentation strategies.", "labels": [], "entities": []}, {"text": "Each of the three tasks were performed in sequence, and each task consisted of two scenarios.", "labels": [], "entities": []}, {"text": "Following PARADISE, the agent and the user had to exchange information about criteria for selecting messages and information within the message body in each scenario.", "labels": [], "entities": [{"text": "PARADISE", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9118943214416504}]}, {"text": "\u2022 1.1: You are working at home in the morning and plan to go directly to a meeting when you go into work.", "labels": [], "entities": []}, {"text": "Kim said she would send you a message telling you where and when the meeting is.", "labels": [], "entities": []}, {"text": "Find out the Meeting Time and the Meeting Place.", "labels": [], "entities": []}, {"text": "Scenario 1.1 is represented in terms of the attribute value matrix (AVM) in.", "labels": [], "entities": [{"text": "attribute value matrix (AVM)", "start_pos": 44, "end_pos": 72, "type": "METRIC", "confidence": 0.7603838791449865}]}, {"text": "Successful completion of a scenario requires that all attributevalues must be exchanged (  The basic idea is to apply the performance function to the measures logged for each dialogue Di, thereby replacing a range of measures with a single performance value 19i.", "labels": [], "entities": []}, {"text": "Given the performance values Pi, any of a number of automatic learning algorithms can be used to determine which sequence of action choices (dialogue strategies) maximize utility, by using/~ as the utility for the final state of the dialogue Di.", "labels": [], "entities": []}, {"text": "Possible algorithms include Genetic Algorithms, Q-learning, TD-Leaming, and Adaptive Dynamic Programming (.", "labels": [], "entities": []}, {"text": "Here we use Q-learning to illustrate the method (. See) for experiments using alternative algorithms.", "labels": [], "entities": []}, {"text": "The utility of doing action a instate Si, U(a, Si) (its Q-value), can be calculated terms of the utility of a successor state Si, by obeying .the following recursive equation: where R(Si) is a reward associated with being instate Si, a is a strategy from a finite set of strategies A that are admissable instate Si, and M~j is the probability of reaching state Sj if strategy a is selected instate Si.", "labels": [], "entities": []}, {"text": "In the experiments reported here, the reward associated with each state, R(SI), is zero.", "labels": [], "entities": []}, {"text": "3 In addition, since reliable a priori prediction of a user action in a particular state is not possible (for example the user may say Help or the speech recognizer may fail to understand the user), the state transition model M/~ is estimated from the logged state-strategy history for the dialogue.", "labels": [], "entities": []}, {"text": "The utility values can be estimated to within a desired threshold using Value Iteration, which updates the estimate of U(a, Si), based on updated utility estimates for neighboring states, so that the equation above becomes: where Un(a, Si) is the utility estimate for doing a instate Si after n iterations.", "labels": [], "entities": []}, {"text": "Value Iteration stops when the difference between Un(a, Si) and Un+l (a, Si) is below a threshold, and utility values have been associated with states where strategy selections were made.", "labels": [], "entities": []}, {"text": "After experimenting with various threshholds, we used a threshold of 5% of the performance range of the dialogues.", "labels": [], "entities": []}, {"text": "The result of applying Q-learning to ELVIS data for the initiative strategies is illustrated in.", "labels": [], "entities": [{"text": "ELVIS data", "start_pos": 37, "end_pos": 47, "type": "DATASET", "confidence": 0.894390732049942}]}, {"text": "The figure plots utility estimates for SI and MI overtime.", "labels": [], "entities": [{"text": "SI", "start_pos": 39, "end_pos": 41, "type": "TASK", "confidence": 0.9116026759147644}, {"text": "MI", "start_pos": 46, "end_pos": 48, "type": "METRIC", "confidence": 0.5411342978477478}]}, {"text": "It is clear that the SI strategy is better because it has a higher utility: at the end of 108 training sessions (dialogues), the utility of SI is estimated at .249 and the utility of MI is estimated at -0.174.", "labels": [], "entities": [{"text": "SI", "start_pos": 21, "end_pos": 23, "type": "TASK", "confidence": 0.9680707454681396}, {"text": "MI", "start_pos": 183, "end_pos": 185, "type": "METRIC", "confidence": 0.9974120259284973}]}, {"text": "The SI and MI strategies affect the whole dialogue; the presentation strategies apply locally and a See) for experiments in which local rewards are nonzero.", "labels": [], "entities": []}, {"text": "can be actived in different states of the dialogue.", "labels": [], "entities": []}, {"text": "We examined the variation in a strategy' s utility at each phase of the task, by representing the task as having three phases: no scenarios completed, one scenario completed and both scenarios completed.", "labels": [], "entities": []}, {"text": "reports utilities for the use of a strategy after one scenario was completed.", "labels": [], "entities": []}, {"text": "The policy implied by the utilities at other phases of the task are the same.", "labels": [], "entities": []}, {"text": "See) for more detail.", "labels": [], "entities": []}, {"text": "The Read-First strategy in D1 has the best performance of the read strategies.", "labels": [], "entities": []}, {"text": "This strategy takes the initiative to read a message, which might result in messages being read that the user wasn't interested in.", "labels": [], "entities": []}, {"text": "However since the user can barge-in on system utterances, perhaps little is lost by taking the initiative to start reading a message.", "labels": [], "entities": []}, {"text": "After 124 training sessions, the best summarize strategy is Summarize-System, which automatically selects which attributes to summarize by, and so does not incur the cost of asking the user to specify these attributes.", "labels": [], "entities": [{"text": "summarize", "start_pos": 38, "end_pos": 47, "type": "TASK", "confidence": 0.9844112396240234}]}, {"text": "However, the utilities for the SummarizeChoice strategy have not completely converged after 124 trials.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Performance measure means per dialogue  for Initiative Strategies", "labels": [], "entities": []}, {"text": " Table 3: Utilities for Presentation Strategy Choices  after 124 Training Sessions", "labels": [], "entities": [{"text": "Presentation Strategy Choices", "start_pos": 24, "end_pos": 53, "type": "TASK", "confidence": 0.9318646589914957}]}]}