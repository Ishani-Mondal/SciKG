{"title": [{"text": "Learning Correlations between Linguistic Indicators and Semantic Constraints: Reuse of Context-Dependent Descriptions of Entities", "labels": [], "entities": [{"text": "Reuse of Context-Dependent Descriptions of Entities", "start_pos": 78, "end_pos": 129, "type": "TASK", "confidence": 0.7484969695409139}]}], "abstractContent": [{"text": "This paper presents the results of a study on the semantic constraints imposed on lexical choice by certain contextual indicators.", "labels": [], "entities": []}, {"text": "We show how such indicators are computed and how correlations between them and the choice of a noun phrase description of a named entity can be automatically established using supervised learning.", "labels": [], "entities": []}, {"text": "Based on this correlation, we have developed a technique for automatic lexical choice of descriptions of entities in text generation.", "labels": [], "entities": [{"text": "automatic lexical choice of descriptions of entities in text generation", "start_pos": 61, "end_pos": 132, "type": "TASK", "confidence": 0.6828154236078262}]}, {"text": "We discuss the underlying relationship between the pragmatics of choosing an appropriate description that serves a specific purpose in the automatically generated text and the semantics of the description itself.", "labels": [], "entities": []}, {"text": "We present our work in the framework of the more general concept of reuse of linguistic structures that are automatically extracted from large corpora.", "labels": [], "entities": []}, {"text": "We present a formal evaluation of our approach and we conclude with some thoughts on potential applications of our method.", "labels": [], "entities": []}], "introductionContent": [{"text": "Human writers constantly make deliberate decisions about picking a particular way of expressing a certain concept.", "labels": [], "entities": []}, {"text": "These decisions are made based on the topic of the text and the effect that the writer wants to achieve.", "labels": [], "entities": []}, {"text": "Such contextual and pragmatic constraints are obvious to experienced writers who produce context-specific text without much effort.", "labels": [], "entities": []}, {"text": "However, in order fora computer to produce text in a similar way, either these constraints have to be added manually by an expert or the system must be able to acquire them in an automatic way.", "labels": [], "entities": []}, {"text": "An example related to the lexical choice of an appropriate nominal description of a person should make the above clear.", "labels": [], "entities": []}, {"text": "Even though it seems intuitive that Bill Clinton should always be described with the NP \"U.", "labels": [], "entities": []}, {"text": "S. president\" or a variation thereof, it turns out that many other descriptions appear in on-line news stories that characterize him in light of the topic of the article.", "labels": [], "entities": []}, {"text": "For example, an article from 1996 on elections uses \"Bill Clinton, the democratic presidential candidate\", while a 1997 article on a false bomb alert in Little Rock, Ark.", "labels": [], "entities": []}, {"text": "uses \"Bill Clinton, an Arkansas native\".", "labels": [], "entities": []}, {"text": "This paper presents the results of a study of the correlation between named entities (people, places, or organizations) and noun phrases used to describe them in a corpus.", "labels": [], "entities": []}, {"text": "Intuitively, the use of a description is based on a deliberate decision on the part of the author of apiece of text.", "labels": [], "entities": []}, {"text": "A writer is likely to select a description that puts the entity in the context of the rest of the article.", "labels": [], "entities": []}, {"text": "It is known that the distribution of words in a document is related to its topic (.", "labels": [], "entities": []}, {"text": "We have developed related techniques for approximating pragmatic constraints using words that appear in the immediate context of the entity.", "labels": [], "entities": []}, {"text": "We will show that context influences the choice of a description, as do several other linguistic indicators.", "labels": [], "entities": []}, {"text": "Each of the indicators by itself doesn't provide enough empirical data that distinguishes among all descriptions that are related to a an entity.", "labels": [], "entities": []}, {"text": "However, a carefully selected combination of such indicators provides enough information in order pick an appropriate description with more than 80% accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 149, "end_pos": 157, "type": "METRIC", "confidence": 0.9977142810821533}]}, {"text": "Section 2 describes how we can automatically obtain enough constraints on the usage of descriptions.", "labels": [], "entities": []}, {"text": "In Section 3, we show how such constructions are related to language reuse.", "labels": [], "entities": []}, {"text": "In Section 4 we describe our experimental setup and the algorithms that we have designed.", "labels": [], "entities": []}, {"text": "Section 5 includes a description of our results.", "labels": [], "entities": []}, {"text": "In Section 6 we discuss some possible extensions to our study and we provide some thoughts about possible uses of our framework.", "labels": [], "entities": []}], "datasetContent": [{"text": "In our experiments, we have used two widely available tools -WordNet and Ripper.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 61, "end_pos": 68, "type": "DATASET", "confidence": 0.9777516722679138}]}, {"text": "WordNet () is an on-line hierarchical lexical database which contains semantic information about English words (including hypernymy relations which we use in our system).", "labels": [], "entities": [{"text": "WordNet", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.9368101954460144}]}, {"text": "We use chains of hypernyms when we need to approximate the usage of a particular word in a description using its ancestor and sibling nodes in WordNet.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 143, "end_pos": 150, "type": "DATASET", "confidence": 0.9672834277153015}]}, {"text": "Particularly useful for our application are the synset offsets of the words in a description.", "labels": [], "entities": []}, {"text": "The synset offset is a number that uniquely identifies a concept node (synset) in the WordNet hierarchy.", "labels": [], "entities": []}, {"text": "shows that the synset offset for the concept \"administrator, decision maker\" is \"(07063507}', 2We haven't included relative clauses in our study.", "labels": [], "entities": []}, {"text": "while its hypernym, \"head, chief, top dog\" has a synset offset of \"~07311393} \". is an algorithm that learns rules from example tuples in a relation.", "labels": [], "entities": []}, {"text": "Attributes in the tuples can be integers (e.g., length of an article, in words), sets (e.g., semantic features), or bags (e.g., words that appear in a sentence or document).", "labels": [], "entities": []}, {"text": "We use Ripper to learn rules that correlate context and other linguistic indicators with the semantics of the description being extracted and subsequently reused.", "labels": [], "entities": []}, {"text": "It is important to notice that Ripper is designed to learn rules that classify data into atomic classes (e.g., \"good\", \"average\", and \"bad\").", "labels": [], "entities": []}, {"text": "We had to modify its algorithm in order to classify data into sets of atoms.", "labels": [], "entities": []}, {"text": "For example, a rule can have the form \"if CONDITION then [( 07063762} { 02864326} { 0001795~}] \"3 . This rule states that if a certain \"CONDITION\" (which is a function of the indicators related to the description) is met, then the description is likely to contain words that are semantically related to the three WordNet nodes [{07063762} {02864326} {00017954}].", "labels": [], "entities": []}, {"text": "The stages of our experiments are described in detail in the remainder of this section.", "labels": [], "entities": []}, {"text": "We have performed a standard evaluation of the precision and recall that our system achieves in selecting a description.", "labels": [], "entities": [{"text": "precision", "start_pos": 47, "end_pos": 56, "type": "METRIC", "confidence": 0.9993500113487244}, {"text": "recall", "start_pos": 61, "end_pos": 67, "type": "METRIC", "confidence": 0.9918779134750366}]}, {"text": "shows our results under two sets of parameters.", "labels": [], "entities": []}, {"text": "Precision and recall are based on how well the system predicts a set of semantic constraints.", "labels": [], "entities": [{"text": "Precision", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.9327325820922852}, {"text": "recall", "start_pos": 14, "end_pos": 20, "type": "METRIC", "confidence": 0.9976812601089478}]}, {"text": "Precision (or P) is defined to be the number of matches divided by the number of elements in the predicted set.", "labels": [], "entities": [{"text": "Precision (or P)", "start_pos": 0, "end_pos": 16, "type": "METRIC", "confidence": 0.9035359859466553}]}, {"text": "Recall (or R) is the number of matches divided by the number of elements in the correct set.", "labels": [], "entities": [{"text": "Recall (or R)", "start_pos": 0, "end_pos": 13, "type": "METRIC", "confidence": 0.9137350201606751}]}, {"text": "If, for example, the system predicts [A] [C], but the set of constraints on the actual description is [D], we would compute that P = 33.3% and R ---50.0%.", "labels": [], "entities": []}, {"text": "reports the average values of P and R for all training examples 4.", "labels": [], "entities": []}, {"text": "Selecting appropriate descriptions based on our algorithm is feasible even though the values of precision and recall obtained may seem only moderately high.", "labels": [], "entities": [{"text": "precision", "start_pos": 96, "end_pos": 105, "type": "METRIC", "confidence": 0.9994543194770813}, {"text": "recall", "start_pos": 110, "end_pos": 116, "type": "METRIC", "confidence": 0.9988136291503906}]}, {"text": "The reason for this is that the problem that we axe trying to solve is underspecified.", "labels": [], "entities": []}, {"text": "That is, in the same context, more than one description can be potentially used.", "labels": [], "entities": []}, {"text": "Mutually interchangeable descriptions include synonyms and near synonyms (\"leader\": Values for precision and recall using word nodes only (left) and both word and parent nodes (right).", "labels": [], "entities": [{"text": "precision", "start_pos": 95, "end_pos": 104, "type": "METRIC", "confidence": 0.9979298114776611}, {"text": "recall", "start_pos": 109, "end_pos": 115, "type": "METRIC", "confidence": 0.9269402623176575}]}, {"text": "type of evaluation requires the availability of human judges.", "labels": [], "entities": []}, {"text": "There are two parts to the evaluation: how well does the system performs in selecting semantic features (WordNet nodes) and how well it works in constraining the choice of a description.", "labels": [], "entities": []}, {"text": "To select a description, our system does a lookup in the profile fora possible description that satisfies most semantic constraints (e.g., we select a row in based on constraints on the columns).", "labels": [], "entities": []}, {"text": "Our system depends crucially on the multiple components that we use.", "labels": [], "entities": []}, {"text": "For example, the shallow CREP grammar that is used in extracting entities and descriptions often fails to extract good descriptions, mostly due to incorrect PP attachment.", "labels": [], "entities": []}, {"text": "We have also had problems from the part-of-speech tagger and, as a result, we occasionally incorrectly extract word sequences that do not represent descriptions.", "labels": [], "entities": [{"text": "part-of-speech tagger", "start_pos": 35, "end_pos": 56, "type": "TASK", "confidence": 0.736118733882904}]}], "tableCaptions": [{"text": " Table 1: Profile of Ung Huot", "labels": [], "entities": []}, {"text": " Table 2: Number of distinct descriptions per entity (DDPE).", "labels": [], "entities": []}, {"text": " Table 4: Values for precision and recall using word nodes only (left) and both word and parent  nodes (right).", "labels": [], "entities": [{"text": "precision", "start_pos": 21, "end_pos": 30, "type": "METRIC", "confidence": 0.9988514184951782}, {"text": "recall", "start_pos": 35, "end_pos": 41, "type": "METRIC", "confidence": 0.9422373175621033}]}]}