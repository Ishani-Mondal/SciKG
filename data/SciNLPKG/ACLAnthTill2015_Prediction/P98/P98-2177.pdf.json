{"title": [{"text": "Statistical Models for Unsupervised Prepositional Phrase Attachment", "labels": [], "entities": [{"text": "Prepositional Phrase Attachment", "start_pos": 36, "end_pos": 67, "type": "TASK", "confidence": 0.7260927160580953}]}], "abstractContent": [{"text": "We present several unsupervised statistical models for the prepositional phrase attachment task that approach the accuracy of the best supervised methods for this task.", "labels": [], "entities": [{"text": "prepositional phrase attachment task", "start_pos": 59, "end_pos": 95, "type": "TASK", "confidence": 0.714085154235363}, {"text": "accuracy", "start_pos": 114, "end_pos": 122, "type": "METRIC", "confidence": 0.9993174076080322}]}, {"text": "Our unsuper-vised approach uses a heuristic based on attachment proximity and trains from raw text that is annotated with only part-of-speech tags and morphological base forms, as opposed to attachment information.", "labels": [], "entities": []}, {"text": "It is therefore less resource-intensive and more portable than previous corpus-based algorithm proposed for this task.", "labels": [], "entities": []}, {"text": "We present results for prepositional phrase attachment in both English and Span-ish.", "labels": [], "entities": [{"text": "prepositional phrase attachment", "start_pos": 23, "end_pos": 54, "type": "TASK", "confidence": 0.6239482363065084}]}], "introductionContent": [{"text": "Prepositional phrase attachment is the task of deciding, fora given preposition in a sentence, the attachment site that corresponds to the interpretation of the sentence.", "labels": [], "entities": [{"text": "Prepositional phrase attachment", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.7046529849370321}]}, {"text": "For example, the task in the following examples is to decide whether the preposition with modifies the preceding noun phrase (with headword shirt) or the preceding verb phrase (with headword bought or washed).", "labels": [], "entities": []}, {"text": "1. I bought the shirt with pockets.", "labels": [], "entities": []}, {"text": "2. I washed the shirt with soap.", "labels": [], "entities": []}, {"text": "In sentence 1, with modifies the noun shirt, since with pockets describes the shirt.", "labels": [], "entities": []}, {"text": "However in sentence 2, with modifies the verb washed since with soap describes how the shirt is washed.", "labels": [], "entities": []}, {"text": "While this form of attachment ambiguity is usually easy for people to resolve, a computer requires detailed knowledge about words (e.g., washed vs. bought) in order to successfully resolve such ambiguities and predict the correct interpretation.", "labels": [], "entities": []}], "datasetContent": [{"text": "Approximately 970K unannotated sentences from the 1988 Wall St. Journal were processed in a manner identical to the example sentence in.", "labels": [], "entities": [{"text": "1988 Wall St. Journal", "start_pos": 50, "end_pos": 71, "type": "DATASET", "confidence": 0.802147775888443}]}, {"text": "The result was approximately 910,000 headword tuples of the form (v,p, n2) or (n,p, n2).", "labels": [], "entities": []}, {"text": "Note that while the headword tuples represent correct attachments only 69% of the time, their quantity is about 45 times greater than the quantity of data used in previous supervised approaches.", "labels": [], "entities": []}, {"text": "The extracted data was used as training material for the three classifters Clbase , Clinterp, and Clbigram.", "labels": [], "entities": [{"text": "Clinterp", "start_pos": 84, "end_pos": 92, "type": "DATASET", "confidence": 0.5024343729019165}]}, {"text": "Each classifier is constructed as follows: Clbase This is the \"baseline\" classifier that predicts N of p = of, and V otherwise.", "labels": [], "entities": []}, {"text": "Clinterp: This classifier has the form of equation (1), uses the method in section 4.1 to generate \u00a2, and the method in section 4.2.2 to generate p. clbigram: This classifier has the form of equation (1), uses the method in section 4.1 to generate \u00a2, and the method in section 4.2.1 to generate p. shows accuracies of the classifiers on the test set of (, which is derived from the manually annotated attachments in the Penn Treebank Wall St. Journal data.", "labels": [], "entities": [{"text": "Penn Treebank Wall St. Journal data", "start_pos": 420, "end_pos": 455, "type": "DATASET", "confidence": 0.9789393444856008}]}, {"text": "The Penn Treebank is drawn from the 1989 Wall St. Journal data, so there is no possibility of overlap with our training data.", "labels": [], "entities": [{"text": "Penn Treebank", "start_pos": 4, "end_pos": 17, "type": "DATASET", "confidence": 0.9942817091941833}, {"text": "1989 Wall St. Journal data", "start_pos": 36, "end_pos": 62, "type": "DATASET", "confidence": 0.9005034327507019}]}, {"text": "Furthermore, the extraction heuristic was developed and tuned on a \"development set\", i.e., a set of annotated examples that did not overlap with either the test set or the training set. and Pr(n) are not needed.)", "labels": [], "entities": [{"text": "Pr", "start_pos": 191, "end_pos": 193, "type": "METRIC", "confidence": 0.9744798541069031}]}, {"text": "While the tuple (num, to, num) is more frequent than (rise, to, num), the conditional probabilities prefer a = V, which is the choice that maximizes Pr(v, n,p, a).", "labels": [], "entities": [{"text": "Pr", "start_pos": 149, "end_pos": 151, "type": "METRIC", "confidence": 0.9696589708328247}]}, {"text": "Both classifiers Clinter p and dbigram clearly outperform the.", "labels": [], "entities": []}, {"text": "baseline, but the classifier dinterp does not outperform dbigram, even though it interpolates between the less specific evidence (the preposition counts) and more specific evidence (the bigram counts).", "labels": [], "entities": []}, {"text": "This maybe due to the errors in our extracted training data; supervised classifiers that train from clean data typically benefit greatly by combining less specific evidence with more specific evidence.", "labels": [], "entities": []}, {"text": "Despite the errors in the training data, the performance of the unsupervised classifiers (81.9%) begins to approach the best performance of the comparable supervised classifiers (84.5%).", "labels": [], "entities": []}, {"text": "(Our goal is to replicate the supervision of a treebank, but not a semantic dictionary, so we do not compare against.)", "labels": [], "entities": []}, {"text": "Furthermore, we do not use the second noun n2, whereas the best supervised methods use this information.", "labels": [], "entities": []}, {"text": "Our result shows that the information in imperfect but abundant data from unambiguous attachments, as shown in, is sufficient to resolve ambiguous prepositional phrase attachments at accuracies just under the supervised state-of-the-art accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 237, "end_pos": 245, "type": "METRIC", "confidence": 0.9916411638259888}]}, {"text": "We claim that our approach is portable to languages with similar word order, and we support this claim by demonstrating our approach on the Spanish language.", "labels": [], "entities": []}, {"text": "We used the Spanish tagger and morphological analyzer developed at the Xerox Research Centre Europe 4 and we modified the extraction heuristic to account for the new tagset, and to account for the Spanish equivalents of the words of (i.e., de or del) and to be (i.e., set).", "labels": [], "entities": [{"text": "Xerox Research Centre Europe 4", "start_pos": 71, "end_pos": 101, "type": "DATASET", "confidence": 0.9259584426879883}]}, {"text": "Chunking was not performed on the Spanish data.", "labels": [], "entities": [{"text": "Spanish data", "start_pos": 34, "end_pos": 46, "type": "DATASET", "confidence": 0.9135304093360901}]}, {"text": "We used 450k sentences of raw text from the Linguistic Data Consortium's Spanish News Text Collection to extract a training set, and we used a non-overlapping set of 50k sentences from the collection to create test sets.", "labels": [], "entities": [{"text": "Linguistic Data Consortium's Spanish News Text Collection", "start_pos": 44, "end_pos": 101, "type": "DATASET", "confidence": 0.8788808956742287}]}, {"text": "Three native Spanish speakers were asked to extract and annotate ambiguous instances of Spanish prepositional phrase attachments.", "labels": [], "entities": [{"text": "extract and annotate ambiguous instances of Spanish prepositional phrase attachments", "start_pos": 44, "end_pos": 128, "type": "TASK", "confidence": 0.6738957941532135}]}, {"text": "They annotated two sets (using the full sentence context); one set consisted of all ambiguous prepositional phrase attachments of the form (v,n,p, n2), and the other set consisted of cases where p = con.", "labels": [], "entities": []}, {"text": "For testing our classifier, we used only those judgments on which all three annotators agreed.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Most frequent (v,p, n2) tuples", "labels": [], "entities": []}, {"text": " Table 3: Most frequent (n,p, n2) tuples", "labels": [], "entities": []}, {"text": " Table 4: Accuracy of mostly unsupervised classifiers on English Wall St. Journal data", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9726584553718567}, {"text": "English Wall St. Journal data", "start_pos": 57, "end_pos": 86, "type": "DATASET", "confidence": 0.9219964385032654}]}, {"text": " Table 5: The key probabilities for the ambigu-", "labels": [], "entities": [{"text": "ambigu", "start_pos": 40, "end_pos": 46, "type": "METRIC", "confidence": 0.9339020252227783}]}, {"text": " Table 6: Accuracy of mostly unsupervised classifiers on Spanish News Data", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9877752065658569}, {"text": "Spanish News Data", "start_pos": 57, "end_pos": 74, "type": "DATASET", "confidence": 0.9250108003616333}]}]}