{"title": [{"text": "General-to-Specific Model Selection for Subcategorization Preference*", "labels": [], "entities": [{"text": "Subcategorization Preference", "start_pos": 40, "end_pos": 68, "type": "TASK", "confidence": 0.8938592374324799}]}], "abstractContent": [{"text": "This paper proposes a novel method for learning probability models of subcategorization preference of verbs.", "labels": [], "entities": []}, {"text": "We consider the issues of case dependencies and noun class generalization in a uniform way by employing the maximum entropy modeling method.", "labels": [], "entities": [{"text": "noun class generalization", "start_pos": 48, "end_pos": 73, "type": "TASK", "confidence": 0.6696696976820627}]}, {"text": "We also propose anew model selection algorithm which starts from the most general model and gradually examines more specific models.", "labels": [], "entities": [{"text": "model selection", "start_pos": 21, "end_pos": 36, "type": "TASK", "confidence": 0.7360725700855255}]}, {"text": "In the experimental evaluation, it is shown that both of the case dependencies and specific sense restriction selected by the proposed method contribute to improving the performance in subcategorization preference resolution.", "labels": [], "entities": [{"text": "subcategorization preference resolution", "start_pos": 185, "end_pos": 224, "type": "TASK", "confidence": 0.7320223649342855}]}], "introductionContent": [{"text": "In empirical approaches to parsing, lexical/semantic collocation extracted from corpus has been proved to be quite useful for ranking parses in syntactic analysis.", "labels": [], "entities": [{"text": "parsing", "start_pos": 27, "end_pos": 34, "type": "TASK", "confidence": 0.972263753414154}, {"text": "syntactic analysis", "start_pos": 144, "end_pos": 162, "type": "TASK", "confidence": 0.7644566595554352}]}, {"text": "For example,, proposed statistical parsing models which incorporated lexical/semantic information.", "labels": [], "entities": [{"text": "statistical parsing", "start_pos": 23, "end_pos": 42, "type": "TASK", "confidence": 0.7283807098865509}]}, {"text": "In their models, syntactic and lexical/semantic features are dependent on each other and are combined together.", "labels": [], "entities": []}, {"text": "This paper also proposes a method of utilizing lexical/semantic features for the purpose of applying them to ranking parses in syntactic analysis.", "labels": [], "entities": [{"text": "syntactic analysis", "start_pos": 127, "end_pos": 145, "type": "TASK", "confidence": 0.7676495611667633}]}, {"text": "However, unlike the models of,, and, we assume that syntactic and lexical/semantic features are independent.", "labels": [], "entities": []}, {"text": "Then, we focus on extracting lexical/semantic collocational knowledge of verbs which is useful in syntactic analysis.", "labels": [], "entities": [{"text": "extracting lexical/semantic collocational knowledge of verbs", "start_pos": 18, "end_pos": 78, "type": "TASK", "confidence": 0.7837969362735748}, {"text": "syntactic analysis", "start_pos": 98, "end_pos": 116, "type": "TASK", "confidence": 0.7894023656845093}]}, {"text": "More specifically, we propose a novel method for learning a probability model of subcategorization preference of verbs.", "labels": [], "entities": []}, {"text": "In general, when learning lexical/semantic collocational knowledge of verbs from corpus, it is necessary to consider the two issues of 1) case dependencies, and 2) noun class generalization.", "labels": [], "entities": [{"text": "noun class generalization", "start_pos": 164, "end_pos": 189, "type": "TASK", "confidence": 0.6249554256598154}]}, {"text": "When considering 1), we have to decide which cases are dependent on each other and which cases are optional and in-* This research was partially supported by the Ministry of Education, Science, Sports and Culture, Japan, Grantin-Aid for Encouragement of.", "labels": [], "entities": []}, {"text": "An extended version of this paper is available from the above URL.", "labels": [], "entities": []}, {"text": "When considering 2), we have to decide which superordinate class generates each observed leaf class in the verb-noun collocation.", "labels": [], "entities": []}, {"text": "So far, there exist several works which worked on these two issues in learning collocational knowledge of verbs and also evaluated the results in terms of syntactic disambiguation. and studied how to find an optimal abstraction level of an argument noun in a tree-structured thesaurus.", "labels": [], "entities": []}, {"text": "Their works are limited to only one argument.", "labels": [], "entities": []}, {"text": "also studied a method for learning dependencies between case slots and reported that dependencies were discovered only at the slotlevel and not at the class-level.", "labels": [], "entities": []}, {"text": "Compared with these previous works, this paper proposes to consider the above two issues in a uniform way.", "labels": [], "entities": []}, {"text": "First, we introduce a model of generating a collocation of a verb and argument/adjunct nouns (section 2) and then view the model as a probability model (section 3).", "labels": [], "entities": []}, {"text": "As a model learning method, we adopt the maximum entropy model learning method (Della.", "labels": [], "entities": []}, {"text": "Case dependencies and noun class generalization are represented as features in the maximum entropy approach.", "labels": [], "entities": [{"text": "noun class generalization", "start_pos": 22, "end_pos": 47, "type": "TASK", "confidence": 0.6372941931088766}]}, {"text": "Features are allowed to have overlap and this is quite advantageous when we consider case dependencies and noun class generalization in parameter estimation.", "labels": [], "entities": [{"text": "noun class generalization", "start_pos": 107, "end_pos": 132, "type": "TASK", "confidence": 0.6149447560310364}]}, {"text": "An optimal model is selected by searching for an optimal set of features, i.e, optimal case dependencies and optimal noun class generalization levels.", "labels": [], "entities": []}, {"text": "As the feature selection process, this paper proposes anew feature selection algorithm which starts from the most general model and gradually examines more specific models (section 4).", "labels": [], "entities": [{"text": "feature selection", "start_pos": 7, "end_pos": 24, "type": "TASK", "confidence": 0.7884970605373383}]}, {"text": "As the model evaluation criterion during the model search from general to specific ones, we employ the description length of the model and guide the search process so as to minimize the description length.", "labels": [], "entities": []}, {"text": "Then, after obtaining a sequence of subcategorization preference models which are totally ordered from general to specific, we select an approximately optimal subcategorization preference model according to the accuracy of subcategorization preference test.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 211, "end_pos": 219, "type": "METRIC", "confidence": 0.9990629553794861}]}, {"text": "In the experimental evaluation of performance of subcatego-rization preference, it is shown that both of the case dependencies and specific sense restriction selected by the proposed method contribute to improving the performance in subcategorization preference resolution (section 5).", "labels": [], "entities": [{"text": "subcategorization preference resolution", "start_pos": 233, "end_pos": 272, "type": "TASK", "confidence": 0.7077122330665588}]}], "datasetContent": [{"text": "As the training and test corpus, we used the EDR Japanese bracketed corpus, which contains about 210,000 sentences collected from newspaper and magazine articles.", "labels": [], "entities": [{"text": "EDR Japanese bracketed corpus", "start_pos": 45, "end_pos": 74, "type": "DATASET", "confidence": 0.8840453028678894}]}, {"text": "We used 'Bunrui Goi Hyou'(BGH) as the Japanese thesaurus.", "labels": [], "entities": [{"text": "Bunrui Goi Hyou'(BGH)", "start_pos": 9, "end_pos": 30, "type": "METRIC", "confidence": 0.7103898227214813}]}, {"text": "BGH has a sevenlayered abstraction hierarchy and more than 60,000 words are assigned at the leaves and its nominal part contains about 45,000 words.", "labels": [], "entities": [{"text": "BGH", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.9138710498809814}]}], "tableCaptions": [{"text": " Table 1: Comparison of Coverage and Accuracy  of Optimal and Other Models (%)", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 37, "end_pos": 45, "type": "METRIC", "confidence": 0.9859070777893066}]}, {"text": " Table 1. Especially, it outperforms the \"Op- timal\" model of \"One-by-one Feature Adding\"  method both in coverage and accuracy. As for  the size of the optimal model, the average num- ber of the active feature set is 126 for \"General- to-Specific\" method and 800 for \"One-by-one  Feature Adding\" method. Therefore, general-to- specific feature selection algorithm achieves sig- nificant improvements over the one-by-one fea- ture adding algorithm with much smaller num- ber of active features. The \"Optimal\" model of  \"General-to-Specific\" method outperforms both  the \"Independent Cases\" and \"General Classes\"  models, and thus both of the case dependencies  and specific sense restriction selected by the pro- posed method have much contribution to improv- ing the performance in subcategorization prefer-ence test. The \"MDL\" model performs worse  than the \"Optimal\" model, because the features  of the \"MDL\" model have much more specific  sense restriction than those of the \"Optimal\"  model, and the coverage of the \"MDL\" model  is much lower than that of the \"Optimal\" model.", "labels": [], "entities": [{"text": "coverage", "start_pos": 106, "end_pos": 114, "type": "METRIC", "confidence": 0.9639537334442139}, {"text": "accuracy", "start_pos": 119, "end_pos": 127, "type": "METRIC", "confidence": 0.997498095035553}]}]}