{"title": [{"text": "Text Segmentation Using Reiteration and Collocation", "labels": [], "entities": [{"text": "Text Segmentation", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.6858169287443161}]}], "abstractContent": [{"text": "A method is presented for segmenting text into subtopic areas.", "labels": [], "entities": []}, {"text": "The proportion of related pairwise words is calculated between adjacent windows of text to determine their lexical similarity.", "labels": [], "entities": []}, {"text": "The lexical cohesion relations of reiteration and collocation are used to identify related words.", "labels": [], "entities": []}, {"text": "These relations are automatically located using a combination of three linguistic features: word repetition, collocation and relation weights.", "labels": [], "entities": [{"text": "word repetition", "start_pos": 92, "end_pos": 107, "type": "TASK", "confidence": 0.6503065526485443}]}, {"text": "This method is shown to successfully detect known subject changes in text and corresponds well to the segmentations placed by test subjects.", "labels": [], "entities": []}], "introductionContent": [{"text": "Many examples of heterogeneous data can be found in daily life.", "labels": [], "entities": []}, {"text": "The Wall Street Journal archives, for example, consist of a series of articles about different subject areas.", "labels": [], "entities": [{"text": "Wall Street Journal archives", "start_pos": 4, "end_pos": 32, "type": "DATASET", "confidence": 0.9829803705215454}]}, {"text": "Segmenting such data into distinct topics is useful for information retrieval, where only those segments relevant to a user's query can be retrieved.", "labels": [], "entities": [{"text": "information retrieval", "start_pos": 56, "end_pos": 77, "type": "TASK", "confidence": 0.7954208850860596}]}, {"text": "Text segmentation could also be used as a pre-processing step in automatic summarisation.", "labels": [], "entities": [{"text": "Text segmentation", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.6689430773258209}, {"text": "summarisation", "start_pos": 75, "end_pos": 88, "type": "TASK", "confidence": 0.7840563654899597}]}, {"text": "Each segment could be summarised individually and then combined to provide an abstract fora document.", "labels": [], "entities": []}, {"text": "Previous work on text segmentation has used term matching to identify clusters of related text. and later, extracted related text portions by matching high frequency terms.", "labels": [], "entities": [{"text": "text segmentation", "start_pos": 17, "end_pos": 34, "type": "TASK", "confidence": 0.7314302772283554}, {"text": "term matching", "start_pos": 44, "end_pos": 57, "type": "TASK", "confidence": 0.718880221247673}]}, {"text": "segmented text into a hierarchical structure, identifying sub-segments of larger segments.", "labels": [], "entities": []}, {"text": "used word co-occurrences to expand the number of terms for matching.", "labels": [], "entities": []}, {"text": "compared all words across a text rather than the more usual nearest neighbours.", "labels": [], "entities": []}, {"text": "A problem with using word repetition is that inappropriate matches can be made because of the lack of contextual information (.", "labels": [], "entities": [{"text": "word repetition", "start_pos": 21, "end_pos": 36, "type": "TASK", "confidence": 0.7248277068138123}]}, {"text": "Another approach to text segmentation is the detection of semantically related words.", "labels": [], "entities": [{"text": "text segmentation", "start_pos": 20, "end_pos": 37, "type": "TASK", "confidence": 0.7605615258216858}, {"text": "detection of semantically related words", "start_pos": 45, "end_pos": 84, "type": "TASK", "confidence": 0.7884674072265625}]}, {"text": "Hearst (1993) incorporated semantic information derived from WordNet but in later work reported that this information actually degraded word repetition results.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 61, "end_pos": 68, "type": "DATASET", "confidence": 0.9445810317993164}, {"text": "word repetition", "start_pos": 136, "end_pos": 151, "type": "TASK", "confidence": 0.7072750777006149}]}, {"text": "Related words have been located using spreading activation on a semantic network, although only one text was segmented.", "labels": [], "entities": []}, {"text": "Another approach extracted semantic information from Roget's Thesaurus (RT).", "labels": [], "entities": [{"text": "Roget's Thesaurus (RT)", "start_pos": 53, "end_pos": 75, "type": "DATASET", "confidence": 0.7485228478908539}]}, {"text": "Lexical cohesion relations between words were identified in RT and used to construct lexical chains of related words in five texts.", "labels": [], "entities": [{"text": "RT", "start_pos": 60, "end_pos": 62, "type": "TASK", "confidence": 0.9382666945457458}]}, {"text": "It was reported that the lexical chains closely correlated to the intentional structure ( of the texts, where the start and end of chains coincided with the intention ranges.", "labels": [], "entities": []}, {"text": "However, RT does not capture all types of lexical cohesion relations.", "labels": [], "entities": [{"text": "RT", "start_pos": 9, "end_pos": 11, "type": "TASK", "confidence": 0.9410118460655212}]}, {"text": "In previous work, it was found that collocation (a lexical cohesion relation) was under-represented in the thesaurus.", "labels": [], "entities": []}, {"text": "Furthermore, this process was not automated and relied on subjective decision making.", "labels": [], "entities": []}, {"text": "Following Morris and Hirst's work, a segmentation algorithm was developed based on identifying lexical cohesion relations across a text.", "labels": [], "entities": []}, {"text": "The proposed algorithm is fully automated, and a quantitative measure of the association between words is calculated.", "labels": [], "entities": []}, {"text": "This algorithm utilises linguistic features additional to those captured in the thesaurus to identify the other types of lexical cohesion relations that can exist in text.", "labels": [], "entities": []}], "datasetContent": [{"text": "An investigation was conducted to determine whether the segmentation algorithm could reliably locate subject change in text.", "labels": [], "entities": []}, {"text": "The objective of the current investigation was to determine whether all troughs coincide with a subject change.", "labels": [], "entities": []}, {"text": "The troughs placed by the algorithm were compared to the segmentations identified by test subjects for the same texts.", "labels": [], "entities": []}, {"text": "Method: Twenty texts were randomly selected for test data each consisting of approximately 500 words.", "labels": [], "entities": []}, {"text": "These texts were presented to seven test subjects who were instructed to identify the sentences at which anew subject area commenced.", "labels": [], "entities": []}, {"text": "No restriction was placed on the number of subject changes that could be identified.", "labels": [], "entities": []}, {"text": "Segmentation points, indicating a change of subject, were determined by the agreement of three or more test subjects (Litman and Passonneau, 1996).", "labels": [], "entities": [{"text": "Segmentation points", "start_pos": 0, "end_pos": 19, "type": "METRIC", "confidence": 0.9424631893634796}]}, {"text": "Adjacent segmentation points were treated as one point because it is likely that they refer to the same subject change.", "labels": [], "entities": [{"text": "Adjacent segmentation", "start_pos": 0, "end_pos": 21, "type": "TASK", "confidence": 0.7896657288074493}]}, {"text": "The troughs placed by the segmentation algorithm were compared to the segmentation points identified by the test subjects.", "labels": [], "entities": [{"text": "segmentation", "start_pos": 26, "end_pos": 38, "type": "TASK", "confidence": 0.9660229682922363}]}, {"text": "In Experiment 1, the top five approaches investigated identified at least 40 out of 42 known subject change points.", "labels": [], "entities": []}, {"text": "Due to that success, these five approaches were applied in this experiment.", "labels": [], "entities": []}, {"text": "To evaluate the results, the information retrieval metrics precision and recall were used.", "labels": [], "entities": [{"text": "precision", "start_pos": 59, "end_pos": 68, "type": "METRIC", "confidence": 0.9930418729782104}, {"text": "recall", "start_pos": 73, "end_pos": 79, "type": "METRIC", "confidence": 0.999617338180542}]}, {"text": "These metrics have tended to be adopted for the assessment of text segmentation algorithms, but they do not provide a scale of correctness (.", "labels": [], "entities": [{"text": "text segmentation algorithms", "start_pos": 62, "end_pos": 90, "type": "TASK", "confidence": 0.813572625319163}]}, {"text": "The degree to which a segmentation point was 'missed' by a trough, for instance, is not considered.", "labels": [], "entities": []}, {"text": "Allowing an error margin provides some degree of flexibility.", "labels": [], "entities": [{"text": "error margin", "start_pos": 12, "end_pos": 24, "type": "METRIC", "confidence": 0.9490484595298767}]}, {"text": "An error margin of two sentences either side of a segmentation point was used by and allowed three sentences.", "labels": [], "entities": [{"text": "error margin", "start_pos": 3, "end_pos": 15, "type": "METRIC", "confidence": 0.974819004535675}]}, {"text": "In this investigation, an error margin of two sentences was considered.", "labels": [], "entities": [{"text": "error margin", "start_pos": 26, "end_pos": 38, "type": "METRIC", "confidence": 0.9851581156253815}]}, {"text": "Results: gives the mean values for the comparison of troughs placed by the segmentation algorithm to the segmentation points identified by the test subjects for all the texts.", "labels": [], "entities": []}, {"text": "Discussion: The segmentation algorithm using word repetition and relation weights in combination achieved mean precision and recall rates of 0.80 and 0.69, respectively.", "labels": [], "entities": [{"text": "segmentation", "start_pos": 16, "end_pos": 28, "type": "TASK", "confidence": 0.9652848243713379}, {"text": "precision", "start_pos": 111, "end_pos": 120, "type": "METRIC", "confidence": 0.9167207479476929}, {"text": "recall", "start_pos": 125, "end_pos": 131, "type": "METRIC", "confidence": 0.997087299823761}]}, {"text": "For 9 out of the 20 texts segmented, all troughs were relevant.", "labels": [], "entities": []}, {"text": "Therefore, many of the troughs placed by the segmentation algorithm represented valid subject  In this investigation, recall rates tended to be lower than precision rates because the algorithm identified fewer segments (4.1 per text) than the test subjects (4.5).", "labels": [], "entities": [{"text": "recall", "start_pos": 118, "end_pos": 124, "type": "METRIC", "confidence": 0.9993472695350647}, {"text": "precision", "start_pos": 155, "end_pos": 164, "type": "METRIC", "confidence": 0.9988769888877869}]}, {"text": "Each text was only 500 words in length and was related to a specific subject area.", "labels": [], "entities": []}, {"text": "These factors limited the degree of subject change that occurred.", "labels": [], "entities": []}, {"text": "Consequently, the test subjects tended to identify subject changes that were more subtle than the algorithm could detect.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1. Comparison of segmentation algorithm", "labels": [], "entities": []}, {"text": " Table 2. Comparison of troughs to segmentation  points placed by the test subjects.", "labels": [], "entities": []}]}