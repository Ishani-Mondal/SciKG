{"title": [{"text": "Towards a single proposal in spelling correction", "labels": [], "entities": [{"text": "spelling", "start_pos": 29, "end_pos": 37, "type": "TASK", "confidence": 0.9596090912818909}]}], "abstractContent": [{"text": "The study presented here relies on the integrated use of different kinds of knowledge in order to improve first-guess accuracy in non-word context-sensitive correction for general unrestricted texts.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 118, "end_pos": 126, "type": "METRIC", "confidence": 0.9382282495498657}]}, {"text": "State of the art spelling correction systems, e.g. ispell, apart from detecting spelling errors, also assist the user by offering a set of candidate corrections that are close to the misspelled word.", "labels": [], "entities": [{"text": "spelling correction", "start_pos": 17, "end_pos": 36, "type": "TASK", "confidence": 0.7271567732095718}]}, {"text": "Based on the correction proposals of ispell, we built several guessers, which were combined in different ways.", "labels": [], "entities": []}, {"text": "Firstly, we evaluated all possibilities and selected the best ones in a corpus with artificially generated typing errors.", "labels": [], "entities": []}, {"text": "Secondly, the best combinations were tested on texts with genuine spelling errors.", "labels": [], "entities": []}, {"text": "The results for the latter suggest that we can expect automatic non-word correction for all the errors in a free running text with 80% precision and a single proposal 98% of the times (1.02 proposals on average).", "labels": [], "entities": [{"text": "precision", "start_pos": 135, "end_pos": 144, "type": "METRIC", "confidence": 0.9988718628883362}]}], "introductionContent": [{"text": "The problem of devising algorithms and techniques for automatically correcting words in text remains a research challenge.", "labels": [], "entities": [{"text": "automatically correcting words in text", "start_pos": 54, "end_pos": 92, "type": "TASK", "confidence": 0.7839186310768127}]}, {"text": "Existing spelling correction techniques are limited in their scope and accuracy.", "labels": [], "entities": [{"text": "spelling correction", "start_pos": 9, "end_pos": 28, "type": "TASK", "confidence": 0.944355309009552}, {"text": "accuracy", "start_pos": 71, "end_pos": 79, "type": "METRIC", "confidence": 0.9969815611839294}]}, {"text": "Apart from detecting spelling errors, many programs assist users by offering a set of candidate corrections that are close to the misspelled word.", "labels": [], "entities": [{"text": "detecting spelling errors", "start_pos": 11, "end_pos": 36, "type": "TASK", "confidence": 0.7719078660011292}]}, {"text": "This is true for most commercial word-processors as well as the Unixbased spelling-corrector ispelP.", "labels": [], "entities": []}, {"text": "These programs tolerate lower first guess accuracy by returning multiple guesses, allowing the user to make the final choice of the intended word.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 42, "end_pos": 50, "type": "METRIC", "confidence": 0.9491138458251953}]}, {"text": "In i lspell  contrast, some applications will require fully automatic correction for general-purpose texts.", "labels": [], "entities": []}, {"text": "It is clear that context-sensitive spelling correction offers better results than isolated-word error correction.", "labels": [], "entities": [{"text": "context-sensitive spelling correction", "start_pos": 17, "end_pos": 54, "type": "TASK", "confidence": 0.61609947681427}]}, {"text": "The underlying task is to determine the relative degree of well formedness among alternative sentences ().", "labels": [], "entities": []}, {"text": "The question is what kind of knowledge (lexical, syntactic, semantic ....", "labels": [], "entities": []}, {"text": ") should be represented, utilised and combined to aid in this determination.", "labels": [], "entities": [{"text": "determination", "start_pos": 62, "end_pos": 75, "type": "TASK", "confidence": 0.964621365070343}]}, {"text": "This study relies on the integrated use of three kinds of knowledge (syntagmatic, paradigmatic and statistical) in order to improve first guess accuracy in non-word context-sensitive correction for general unrestricted texts.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 144, "end_pos": 152, "type": "METRIC", "confidence": 0.9230133891105652}]}, {"text": "Our techniques were applied to the corrections posed by ispell.", "labels": [], "entities": []}, {"text": "Constraint Grammar () was chosen to represent syntagmatic knowledge.", "labels": [], "entities": [{"text": "Constraint Grammar", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.6982758492231369}]}, {"text": "Its use as apart of speech tagger for English has been highly successful.", "labels": [], "entities": [{"text": "apart of speech tagger", "start_pos": 11, "end_pos": 33, "type": "TASK", "confidence": 0.6374776810407639}]}, {"text": "Conceptual Density (Agirre and Rigau 1996) is the paradigmatic component chosen to discriminate semantically among potential noun corrections.", "labels": [], "entities": [{"text": "Conceptual Density", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.7102234959602356}, {"text": "noun corrections", "start_pos": 125, "end_pos": 141, "type": "TASK", "confidence": 0.7065689265727997}]}, {"text": "This technique measures \"affinity distance\" between nouns using Wordnet).", "labels": [], "entities": [{"text": "Wordnet", "start_pos": 64, "end_pos": 71, "type": "DATASET", "confidence": 0.9761176705360413}]}, {"text": "Finally, general and document word-occurrence frequency-rates complete the set of knowledge sources combined.", "labels": [], "entities": []}, {"text": "We knowingly did not use any model of common misspellings, the main reason being that we did not want to use knowledge about the error source.", "labels": [], "entities": []}, {"text": "This work focuses on language models, not error models (typing errors, common misspellings, OCR mistakes, speech recognition mistakes, etc.).", "labels": [], "entities": []}, {"text": "The system was evaluated against two sets of texts: artificially generated errors from the Brown corpus and genuine spelling errors from the Bank of EnglishL The remainder of this paper is organised as 2 http://titania.cobuild.collins.co.uk/boe_info.html follows.", "labels": [], "entities": [{"text": "Brown corpus", "start_pos": 91, "end_pos": 103, "type": "DATASET", "confidence": 0.9860355257987976}, {"text": "Bank of EnglishL", "start_pos": 141, "end_pos": 157, "type": "DATASET", "confidence": 0.8844306270281473}]}, {"text": "Firstly, we present the techniques that will be evaluated and the way to combine them.", "labels": [], "entities": []}, {"text": "Section 2 describes the experiments and shows the results, which are evaluated in section 3.", "labels": [], "entities": []}, {"text": "Section 4 compares other relevant work in context sensitive correction.", "labels": [], "entities": [{"text": "context sensitive correction", "start_pos": 42, "end_pos": 70, "type": "TASK", "confidence": 0.7885967691739401}]}], "datasetContent": [{"text": "Based on each kind of knowledge, we built simple guessers and combined them in different ways.", "labels": [], "entities": []}, {"text": "In the first phase, we evaluated all the possibilities and selected the best ones on part of the corpus with artificially generated errors.", "labels": [], "entities": []}, {"text": "Finally, the best combinations were tested against the texts with genuine spelling errors.", "labels": [], "entities": []}, {"text": "This section reviews the results obtained.", "labels": [], "entities": []}, {"text": "The results for the \"real\" corpus are evaluated first, and the comparison with the other corpora comes later.", "labels": [], "entities": []}, {"text": "Concerning the application of each of the simple techniques separately6: \u2022 Any of the guessers performs much better than random.", "labels": [], "entities": []}, {"text": "\u2022 DF has a high precision (75%) at the cost of a low coverage (12%).", "labels": [], "entities": [{"text": "precision", "start_pos": 16, "end_pos": 25, "type": "METRIC", "confidence": 0.9991831183433533}, {"text": "coverage", "start_pos": 53, "end_pos": 61, "type": "METRIC", "confidence": 0.9921982884407043}]}, {"text": "The difference in coverage compared to the artificial error corpora (84%) is mainly due to the smaller size of the documents in the real error corpus (around 50 words per document).", "labels": [], "entities": [{"text": "coverage", "start_pos": 18, "end_pos": 26, "type": "METRIC", "confidence": 0.9773786067962646}]}, {"text": "For mediumsized documents we expect a coverage similar to that of the artificial error corpora.", "labels": [], "entities": [{"text": "coverage", "start_pos": 38, "end_pos": 46, "type": "METRIC", "confidence": 0.9707403182983398}]}, {"text": "\u2022 BF offers lower precision (54%) with the gains of abroad coverage (96%).", "labels": [], "entities": [{"text": "BF", "start_pos": 2, "end_pos": 4, "type": "METRIC", "confidence": 0.8458368182182312}, {"text": "precision", "start_pos": 18, "end_pos": 27, "type": "METRIC", "confidence": 0.9996981620788574}, {"text": "coverage", "start_pos": 59, "end_pos": 67, "type": "METRIC", "confidence": 0.6058706045150757}]}, {"text": "\u2022 CG presents 62% precision with nearly 100% coverage, but at the cost of leaving many proposals (2.45) \u2022 The use of CD works only with a small fraction of the errors giving modest results.", "labels": [], "entities": [{"text": "precision", "start_pos": 18, "end_pos": 27, "type": "METRIC", "confidence": 0.9992045760154724}, {"text": "coverage", "start_pos": 45, "end_pos": 53, "type": "METRIC", "confidence": 0.9795464873313904}]}, {"text": "The fact that it was only applied a few times prevents us from making further conclusions.", "labels": [], "entities": []}, {"text": "Combining the techniques, the results improve: \u2022 The CGI+DF2 combination offers the best results in coverage (100%) and precision (70%) for all tests.", "labels": [], "entities": [{"text": "coverage", "start_pos": 100, "end_pos": 108, "type": "METRIC", "confidence": 0.9995218515396118}, {"text": "precision", "start_pos": 120, "end_pos": 129, "type": "METRIC", "confidence": 0.9996519088745117}]}, {"text": "As can be seen, CG raises the coverage of the DF method, at the cost of also increasing the number of proposals (1.9) per erroneous word.", "labels": [], "entities": [{"text": "CG", "start_pos": 16, "end_pos": 18, "type": "DATASET", "confidence": 0.5647718906402588}, {"text": "coverage", "start_pos": 30, "end_pos": 38, "type": "METRIC", "confidence": 0.9940283298492432}]}, {"text": "Had the coverage of DF increased, so would also the number of proposals decrease for this combination, for instance, close to that of the artificial error corpora (1.28).", "labels": [], "entities": [{"text": "coverage", "start_pos": 8, "end_pos": 16, "type": "METRIC", "confidence": 0.9542019963264465}]}, {"text": "\u2022 The CGI+DFI+BF1 combination provides the same coverage with nearly one interpretation per word, but decreasing precision to a 55%.", "labels": [], "entities": [{"text": "precision", "start_pos": 113, "end_pos": 122, "type": "METRIC", "confidence": 0.9992707371711731}]}, {"text": "\u2022 If full coverage is not necessary, the use of the H2 heuristic raises the precision at least 4% for all combinations.", "labels": [], "entities": [{"text": "coverage", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.948983907699585}, {"text": "precision", "start_pos": 76, "end_pos": 85, "type": "METRIC", "confidence": 0.9996795654296875}]}, {"text": "When comparing these results with those of the artificial errors, the precisions in tables 2, 4 and 6 can be misleading.", "labels": [], "entities": [{"text": "precisions", "start_pos": 70, "end_pos": 80, "type": "METRIC", "confidence": 0.9984118938446045}]}, {"text": "The reason is that the coverage of some techniques varies and the precision varies accordingly.", "labels": [], "entities": [{"text": "coverage", "start_pos": 23, "end_pos": 31, "type": "METRIC", "confidence": 0.9784854054450989}, {"text": "precision", "start_pos": 66, "end_pos": 75, "type": "METRIC", "confidence": 0.9996122717857361}]}, {"text": "For instance, coverage of DF is around 70% for real errors and 90% for artificial errors, while precisions are 93% and 89% respectively (cf. tables 6 and 2).", "labels": [], "entities": [{"text": "coverage", "start_pos": 14, "end_pos": 22, "type": "METRIC", "confidence": 0.9964723587036133}, {"text": "precisions", "start_pos": 96, "end_pos": 106, "type": "METRIC", "confidence": 0.9990550875663757}]}, {"text": "This increase in precision is not due to the better performance of DF 7, but can be explained because the lower the coverage, the higher the proportion of errors with a single proposal, and therefore the higher the precision.", "labels": [], "entities": [{"text": "precision", "start_pos": 17, "end_pos": 26, "type": "METRIC", "confidence": 0.9995997548103333}, {"text": "DF 7", "start_pos": 67, "end_pos": 71, "type": "DATASET", "confidence": 0.8461749255657196}, {"text": "coverage", "start_pos": 116, "end_pos": 124, "type": "METRIC", "confidence": 0.9930760860443115}, {"text": "precision", "start_pos": 215, "end_pos": 224, "type": "METRIC", "confidence": 0.9988973140716553}]}, {"text": "The comparison between tables 3 and 7 is more clarifying.", "labels": [], "entities": []}, {"text": "The performance of all techniques drops in table 7.", "labels": [], "entities": []}, {"text": "Precision of CG and BF drops 15 and 20 points.", "labels": [], "entities": [{"text": "Precision", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.9882110953330994}, {"text": "BF", "start_pos": 20, "end_pos": 22, "type": "METRIC", "confidence": 0.9967824220657349}]}, {"text": "DF goes down 20 points in precision and 50 points in coverage.", "labels": [], "entities": [{"text": "DF", "start_pos": 0, "end_pos": 2, "type": "DATASET", "confidence": 0.44682446122169495}, {"text": "precision", "start_pos": 26, "end_pos": 35, "type": "METRIC", "confidence": 0.9997239708900452}, {"text": "coverage", "start_pos": 53, "end_pos": 61, "type": "METRIC", "confidence": 0.9973471164703369}]}, {"text": "This latter degradation is not surprising, as the length of the documents in this corpus is only of 50 words on average.", "labels": [], "entities": []}, {"text": "Had we had access to medium sized documents, we would expect a coverage similar to that of the artificial error corpora.", "labels": [], "entities": [{"text": "coverage", "start_pos": 63, "end_pos": 71, "type": "METRIC", "confidence": 0.9811858534812927}]}, {"text": "The best combinations hold for the \"real\" texts, as before.", "labels": [], "entities": []}, {"text": "The highest precision is for CGI+DF2 (with and without H2).", "labels": [], "entities": [{"text": "precision", "start_pos": 12, "end_pos": 21, "type": "METRIC", "confidence": 0.9990342855453491}]}, {"text": "The number of proposals left is higher in the \"real\" texts than in the artificial ones (1.99 to 1.28).", "labels": [], "entities": []}, {"text": "It can be explained because DF does not manage to coverall errors, and that leaves many CG proposals untouched.", "labels": [], "entities": [{"text": "DF", "start_pos": 28, "end_pos": 30, "type": "DATASET", "confidence": 0.6307063102722168}]}, {"text": "We think that the drop in performance for the \"real\" texts was caused by different factors.", "labels": [], "entities": []}, {"text": "First of all, we already mentioned that the size of the documents strongly affected DF.", "labels": [], "entities": [{"text": "DF", "start_pos": 84, "end_pos": 86, "type": "DATASET", "confidence": 0.4966528117656708}]}, {"text": "Secondly, the nature of the errors changes: the algorithm to 7 In fact the contrary is deduced from tables 3 and 7.", "labels": [], "entities": []}, {"text": "produce spelling errors was biased in favour of frequent words, mostly short ones.", "labels": [], "entities": []}, {"text": "We will have to analyse this question further, specially regarding the origin of the natural errors.", "labels": [], "entities": []}, {"text": "Lastly, BF was trained on the Brown corpus on American English, while the \"real\" texts come from the Bank of English.", "labels": [], "entities": [{"text": "BF", "start_pos": 8, "end_pos": 10, "type": "METRIC", "confidence": 0.5847710967063904}, {"text": "Brown corpus on American English", "start_pos": 30, "end_pos": 62, "type": "DATASET", "confidence": 0.8868955135345459}, {"text": "Bank of English", "start_pos": 101, "end_pos": 116, "type": "DATASET", "confidence": 0.9493797818819681}]}, {"text": "Presumably, this could have also affected negatively the performance of these algorithms.", "labels": [], "entities": []}, {"text": "Back to table 6, the figures reveal which would be the output of the correction system.", "labels": [], "entities": []}, {"text": "Either we get a single proposal 98% of the times (1.02 proposals left on average) with 80% precision for all nonword errors in the text (CGI+DFI+BF1) or we can get a higher precision of 90% with 89% coverage and an average of 1.43 proposals (CGI+DF2+H2).", "labels": [], "entities": [{"text": "precision", "start_pos": 91, "end_pos": 100, "type": "METRIC", "confidence": 0.9987950325012207}, {"text": "BF1", "start_pos": 145, "end_pos": 148, "type": "METRIC", "confidence": 0.8402314782142639}, {"text": "precision", "start_pos": 173, "end_pos": 182, "type": "METRIC", "confidence": 0.9922349452972412}]}], "tableCaptions": [{"text": " Table 1 shows how, if the  misspellings are generated at random, 23.5% of  them are real words, and fall out of the scope of  this work. Although we did not make a similar  counting in the real texts, we observed that a  similar percentage can be expected.", "labels": [], "entities": []}, {"text": " Table 2. Results for several combinations (I\"", "labels": [], "entities": []}, {"text": " Table 4. Validation of the best combinations  (2 *J half)", "labels": [], "entities": [{"text": "Validation", "start_pos": 10, "end_pos": 20, "type": "TASK", "confidence": 0.8433207273483276}]}, {"text": " Table 5. Results on errors with multiple  proposals (2 \"d half)", "labels": [], "entities": []}, {"text": " Table 7. Results on errors with multiple  proposals (\"real\" corpus)", "labels": [], "entities": []}]}