{"title": [{"text": "Dialogue Act Tagging with Transformation-Based Learning", "labels": [], "entities": [{"text": "Dialogue Act Tagging", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.5789892375469208}]}], "abstractContent": [{"text": "For the task of recognizing dialogue acts, we are applying the Transformation-Based Learning (TBL) machine learning algorithm.", "labels": [], "entities": [{"text": "recognizing dialogue acts", "start_pos": 16, "end_pos": 41, "type": "TASK", "confidence": 0.8642632166544596}]}, {"text": "To circumvent a sparse data problem, we extract values of well-motivated features of utterances, such as speaker direction, punctuation marks, and anew feature, called dialogue act cues, which we find to be more effective than cue phrases and word n-grams in practice.", "labels": [], "entities": []}, {"text": "We present strategies for constructing a set of dialogue act cues automatically by minimizing the entropy of the distribution of dialogue acts in a training corpus , filtering out irrelevant dialogue act cues, and clustering semantically-related words.", "labels": [], "entities": []}, {"text": "In addition, to address limitations of TBL, we introduce a Monte Carlo strategy for training efficiently and a committee method for computing confidence measures.", "labels": [], "entities": [{"text": "TBL", "start_pos": 39, "end_pos": 42, "type": "TASK", "confidence": 0.6518141627311707}]}, {"text": "These ideas are combined in our working implementation, which labels held-out data as accurately as any other reported system for the dialogue act tagging task.", "labels": [], "entities": [{"text": "dialogue act tagging task", "start_pos": 134, "end_pos": 159, "type": "TASK", "confidence": 0.8016037344932556}]}], "introductionContent": [{"text": "Although machine learning approaches have achieved success in many areas of Natural Language Processing, researchers have only recently begun to investigate applying machine learning methods to discourse-level problems;.", "labels": [], "entities": [{"text": "Natural Language Processing", "start_pos": 76, "end_pos": 103, "type": "TASK", "confidence": 0.6538362602392832}]}, {"text": "An important task in discourse understanding is to interpret an utterance's dialogue act, which is a concise abstraction of a speaker's intention, such as SUGGEST and AC-CEPT.", "labels": [], "entities": [{"text": "discourse understanding", "start_pos": 21, "end_pos": 44, "type": "TASK", "confidence": 0.7049201130867004}]}, {"text": "Recognizing dialogue acts is critical for discourse-level understanding and can also be useful for other applications, such as resolving ambiguity in speech recognition.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 150, "end_pos": 168, "type": "TASK", "confidence": 0.6976258307695389}]}, {"text": "However, computing dialogue acts is a challenging task, because often a dialogue act cannot be directly inferred from a literal interpretation of an utterance.", "labels": [], "entities": []}, {"text": "We have investigated applying Transformation-Based Learning (TBL) to the task of computing dialogue acts.", "labels": [], "entities": []}, {"text": "This method, which has not been used previously in discourse, has a number of attractive characteristics for our task.", "labels": [], "entities": []}, {"text": "However, it also has some limitations, which we address with a Monte Carlo strategy that significantly improves the training time efficiency without compromising accuracy and a committee method that enables TBL to compute confidence measures for the dialogue acts assigned to utterances.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 162, "end_pos": 170, "type": "METRIC", "confidence": 0.997426450252533}]}, {"text": "Our machine learning algorithm makes use of abstract features extracted from utterances.", "labels": [], "entities": []}, {"text": "In addition, we utilize an entropy-minimization approach to automatically identify dialogue act cues, which are words and short phrases that serve as signals for dialogue acts.", "labels": [], "entities": []}, {"text": "Our experiments demonstrate that dialogue act cues tend to be more effective than cue phrases and word n-grams, and this strategy can be further improved by adding a filtering mechanism and a semantic-clustering method.", "labels": [], "entities": []}, {"text": "Although we still plan to implement more modifications, our system has already achieved success rates comparable to the best reported results for computing dialogue acts.", "labels": [], "entities": []}], "datasetContent": [{"text": "A survey of the other research projects that have applied machine learning methods to the dialogue act tagging task is presented in.", "labels": [], "entities": [{"text": "dialogue act tagging task", "start_pos": 90, "end_pos": 115, "type": "TASK", "confidence": 0.8324524611234665}]}, {"text": "The highest success rate was reported by, whose system could correctly label 74.7% of the utterances in a test corpus.", "labels": [], "entities": []}, {"text": "Their work utilized an N-Grams approach, in which an utterance's dialogue act was based on substrings of words as well as the dialogue acts and speaker information from the preceding two utterances.", "labels": [], "entities": []}, {"text": "Various probabilities were estimated from a training corpus by counting the frequencies of specific events, such as the number of times that each pair of consecutive words co-occurred with each dialogue act.", "labels": [], "entities": []}, {"text": "As a direct comparison, we applied our system to Reithinger and Klesen's training set (143 dialogues, 2701 utterances) and disjoint testing set (20 dialogues, 328 utterances), which consist of utterances labeled with 18 different dialogue acts.", "labels": [], "entities": []}, {"text": "Using semantic clustering, (9 --1 (the improvement score threshold), R = 14 (the Monte Carlo sample size), a set of dialogue act cues, change of speaker, the dialogue act on the preceding utterance, and other features, our system achieved an average accuracy score over five 9 runs of 75.12% (a=1.34%), including a high score of 77.44%.", "labels": [], "entities": [{"text": "R", "start_pos": 69, "end_pos": 70, "type": "METRIC", "confidence": 0.9415367841720581}, {"text": "Monte Carlo sample size)", "start_pos": 81, "end_pos": 105, "type": "METRIC", "confidence": 0.6832938134670258}, {"text": "accuracy", "start_pos": 250, "end_pos": 258, "type": "METRIC", "confidence": 0.9994142055511475}]}, {"text": "We have also run direct comparisons between our system and Decision Trees, determining that our system's performance is also comparable to this popular machine learning method (.", "labels": [], "entities": []}, {"text": "presents a series of experiments which vary the set of word substrings utilized by the system, l\u00b0 Each experiment was run ten times, and the results were compared using a two-tailed t test to determine that all of the accuracy differences were significant at the 0.05 level, except for the differences between rows 3 & 4, rows 4 &: 5, rows 9This is to factor out the random aspect of the Monte Carlo method.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 218, "end_pos": 226, "type": "METRIC", "confidence": 0.9982147216796875}]}, {"text": "1\u00b0Note that these results cannot be compared with the results presented above, since several parameter values differ between the two sets of experiments.", "labels": [], "entities": []}, {"text": "11There are only 478 different cue phrases in the set, but for our system, it was necessary to manipulate the: Tagging accuracy on held-out data, using different sets of word substrings in training As the figure shows, when the system was restricted from using any word substrings, its accuracy on unseen data was only 41.16%.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 119, "end_pos": 127, "type": "METRIC", "confidence": 0.9807650446891785}, {"text": "accuracy", "start_pos": 286, "end_pos": 294, "type": "METRIC", "confidence": 0.9986530542373657}]}, {"text": "When given access to all of the cue phrases proposed in previous work, 12 the accuracy rises significantly (p < 0.001) to 61.74%.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 78, "end_pos": 86, "type": "METRIC", "confidence": 0.9997574687004089}]}, {"text": "But this result is significantly lower (p < 0.001) than the 69.21% accuracy produced by using all substrings of one, two, or three words (word n-grams) in the training data, as did.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 67, "end_pos": 75, "type": "METRIC", "confidence": 0.9986911416053772}]}, {"text": "And the entropy-minimization approach with the filtering and clustering techniques produce dialogue act cues that cause the accuracy to rise significantly further (p = 0.003) to 71.22%.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 124, "end_pos": 132, "type": "METRIC", "confidence": 0.9994943141937256}]}, {"text": "Our experimental results show that the cue phrases identified in the literature do not capture all of the word substrings that signal dialogue acts.", "labels": [], "entities": []}, {"text": "On the other hand, the complete set of word n-grams causes the performance of TBL to suffer.", "labels": [], "entities": [{"text": "TBL", "start_pos": 78, "end_pos": 81, "type": "TASK", "confidence": 0.3538450598716736}]}, {"text": "Our dialogue act cues generate the highest accuracy scores, using significantly fewer word substrings than the word n-grams approach.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 43, "end_pos": 51, "type": "METRIC", "confidence": 0.9988724589347839}]}], "tableCaptions": []}