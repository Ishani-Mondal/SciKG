{"title": [], "abstractContent": [{"text": "We present the design of a practical context-sensitive glosser, incorporating current techniques for lightweight linguistic analysis based on large-scale lexical resources.", "labels": [], "entities": []}, {"text": "We outline a general model for ranking the possible translations of the words and expressions that makeup a text.", "labels": [], "entities": []}, {"text": "This information can be used by a simple resource-bounded algorithm, of complexity O(n log n) in sentence length, that determines a consistent gloss of best translations.", "labels": [], "entities": []}, {"text": "We then describe how the results of the general ranking model maybe approximated using a simple heuristic prioritisation scheme.", "labels": [], "entities": []}, {"text": "Finally we present a preliminary evaluation of the glosser's performance.", "labels": [], "entities": []}], "introductionContent": [{"text": "Ina lexicalist MT framework such as Shakeand-Bake, translation \u2022 equivalence is defined between collections of (suitably constrained) lexical material in the two languages.", "labels": [], "entities": [{"text": "MT", "start_pos": 15, "end_pos": 17, "type": "TASK", "confidence": 0.9260197281837463}]}, {"text": "Such an approach has been shown to be effective in the description of many types of complex bilingual equivalence.", "labels": [], "entities": []}, {"text": "However, the complexity of the associated parsing and generation phases leaves a system of this type someway from commercial exploitation.", "labels": [], "entities": [{"text": "parsing", "start_pos": 42, "end_pos": 49, "type": "TASK", "confidence": 0.9685875177383423}]}, {"text": "The parsing phase that is needed to establish adequate constraints on the words is of cubic complexity, while the most general generation algorithm, needed to order the words in the target text, is O(n 4) (.", "labels": [], "entities": []}, {"text": "In this paper, we show how a novel application domain, glossing, can be explored within such a framework, by omitting generation entirely and replacing syntactic parsing by a simple combination of morphological analysis and tagging.", "labels": [], "entities": []}, {"text": "The poverty of constraints established in this way, and the consequent inaccuracy in translation, is mitigated by providing a menu of alternatives for each gloss.", "labels": [], "entities": []}, {"text": "The gloss is automatically updated in the light of user choices.", "labels": [], "entities": []}, {"text": "While the availability of alternatives is generally desirable in automatic translation, it is the limitation to glossing which makes it feasible to manage the consistency maintenance required.", "labels": [], "entities": []}, {"text": "Glossing as a technique for elucidating the grammar and lexis of a second language text is well-known from the linguistics literature.", "labels": [], "entities": []}, {"text": "Each morpheme in the object language is provided with its meta-language equivalent aligned beneath it.", "labels": [], "entities": []}, {"text": "Such a glosser maybe used as a tool for second-language improvement, and thus provide an educational alternative to the passive consumption of a (usually low quality) translation.", "labels": [], "entities": []}, {"text": "We envisage the glosser's primary use as a tool for cross-language information gathering, and thus think it best not to display grammatical information.", "labels": [], "entities": [{"text": "cross-language information gathering", "start_pos": 52, "end_pos": 88, "type": "TASK", "confidence": 0.7881877223650614}]}, {"text": "Our glosser improves on the use of printed or even on-line dictionaries in several ways: \u2022 The system performs lemmatisation for the user.", "labels": [], "entities": []}, {"text": "\u2022 Lightweight analysis resolves part-ofspeech ambiguities in context.", "labels": [], "entities": [{"text": "Lightweight analysis", "start_pos": 2, "end_pos": 22, "type": "TASK", "confidence": 0.7242906987667084}]}, {"text": "\u2022 Multi-word expressions, including discontinuous and variable ones, are detected.", "labels": [], "entities": []}, {"text": "\u2022 A degree of consistency between system and user choices is maintained.", "labels": [], "entities": [{"text": "consistency", "start_pos": 14, "end_pos": 25, "type": "METRIC", "confidence": 0.9808157086372375}]}, {"text": "The glosser attempts to find all plausible equivalents for the words and multi-word expressions that constitute a text, displaying the most appropriate consistent subset as its first choice and the remainder within menus.", "labels": [], "entities": []}, {"text": "Consistency is maintained by treating source language lexical material as resources that are consumed by the matching of equivalences, so that the latter partially tile the text 1.", "labels": [], "entities": []}, {"text": "Our model has much in common with that of Alshawi (1996), though our linguistic representations are relatively impoverished.", "labels": [], "entities": []}, {"text": "Our aim is not true translation but the use of large existing bilingual lexicons for very wide-coverage glossing.", "labels": [], "entities": [{"text": "translation", "start_pos": 20, "end_pos": 31, "type": "TASK", "confidence": 0.9634072780609131}]}, {"text": "We have discovered that the effect of tiling with a large ordered set of detailed equivalences is to provide a close approximation to richer schemes for syntactic analysis.", "labels": [], "entities": [{"text": "syntactic analysis", "start_pos": 153, "end_pos": 171, "type": "TASK", "confidence": 0.7837616205215454}]}, {"text": "An example English-Japanese gloss as produced by our system is shown in.", "labels": [], "entities": []}, {"text": "Multi-word 1 Equivalences are not only consumers of source language resources but also producers of target language ones.", "labels": [], "entities": []}, {"text": "In glossing, the production of target language resources need not be complete -every word needs a translation, but not every word needs a gloss.", "labels": [], "entities": []}, {"text": "Tiling thus need only be partial.", "labels": [], "entities": [{"text": "Tiling", "start_pos": 0, "end_pos": 6, "type": "TASK", "confidence": 0.9494314789772034}]}, {"text": "collocations are underlined and discontinuous ones are also given a number (and colour) to facilitate identification.", "labels": [], "entities": []}, {"text": "Note how stemmed ... from is a discontinuous collocation surrounding the continuous collocation in part.", "labels": [], "entities": []}, {"text": "The pop-up menu shows the alternatives for fruit, by sense at the top-level with run-offs to synonyms, and at the bottom an option to access the machinereadable version of 'Genius', a published English Japanese dictionary.", "labels": [], "entities": [{"text": "Genius', a published English Japanese dictionary", "start_pos": 173, "end_pos": 221, "type": "DATASET", "confidence": 0.5786915384232998}]}, {"text": "The structure of this paper is as follows.", "labels": [], "entities": []}, {"text": "In 2.1 we outline the basic operation of the system, introducing our representation of natural language collocations as key descriptors, and give a probabilistic interpretation for these in 2.2.", "labels": [], "entities": []}, {"text": "Section 3 describes the algorithm for tiling a sentence using key descriptors, and goes onto describe a series of heuristics which approximate the full probabilistic model.", "labels": [], "entities": []}, {"text": "Section 4 presents the results of a preliminary evaluation of the glosser' s performance.", "labels": [], "entities": []}, {"text": "Finally in section 5 we give our conclusions and make some suggestions for future improvements to the system.", "labels": [], "entities": []}], "datasetContent": [{"text": "The above algorithm is implemented in the SID system for glossing English into Japanese a.", "labels": [], "entities": []}, {"text": "A large dictionary from an existing MT system was used as the basis for our dictionary, which comprises about 200k distinct key descriptors keying about 400k translations.", "labels": [], "entities": []}, {"text": "SID reaches a peak glossing speed of about 12,000 words per minute on a 200 MHz Pentium Pro.", "labels": [], "entities": []}, {"text": "To evaluate SID we compared its output with a 1 million word dependency-parsed corpus (based on the Penn TreeB ank) and rated as correct any collocation which corresponded to a connected piece of dependency structure with matching tags.", "labels": [], "entities": [{"text": "Penn TreeB ank", "start_pos": 100, "end_pos": 114, "type": "DATASET", "confidence": 0.9934859077135721}]}, {"text": "We added other correctness criteria to cope with those cases where a collocate is not dependency-connected in our corpus, such as a subject-main verb collocate separated by an auxiliary (a rally was held), or a discontinuous adjective phrase (an interesting man to know).", "labels": [], "entities": []}, {"text": "Correctness is somewhat over-estimated in that a dependent preposition, for example, may not have the intended collocational meaning (it marks an adjunct rather than an argument), but this appears to be more than offset by tag mismatch cases which might be significant but are not in many particular cases -e.g. Grand Jury where Grand maybe tagged ADJ by SID but NP in Penn, or passed the bill onto the House, whereon maybe tagged ADV by SID but IN (= preposition) in Penn.", "labels": [], "entities": [{"text": "Penn", "start_pos": 369, "end_pos": 373, "type": "DATASET", "confidence": 0.9636344909667969}, {"text": "IN", "start_pos": 446, "end_pos": 448, "type": "METRIC", "confidence": 0.9892007112503052}, {"text": "Penn", "start_pos": 468, "end_pos": 472, "type": "DATASET", "confidence": 0.98011714220047}]}, {"text": "To obtain a baseline recall figure we ran SID over the corpus with a much lower tag probability threshold and much higher search radius 4, and counted the total number of correct collocations detected anywhere amongst the alternatives.", "labels": [], "entities": [{"text": "recall", "start_pos": 21, "end_pos": 27, "type": "METRIC", "confidence": 0.9898091554641724}, {"text": "tag probability threshold", "start_pos": 80, "end_pos": 105, "type": "METRIC", "confidence": 0.8072681228319804}]}, {"text": "SID detected a total of c.", "labels": [], "entities": [{"text": "SID", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.8361393213272095}]}, {"text": "150k collocations with its parameters set to their values in the released version 5, of which we judged 110k correct for an overall precision of 72%, which rises to 82% for fringe elements.", "labels": [], "entities": [{"text": "precision", "start_pos": 132, "end_pos": 141, "type": "METRIC", "confidence": 0.9959533214569092}]}, {"text": "Overall recall was 98% (75% for the fringe).", "labels": [], "entities": [{"text": "recall", "start_pos": 8, "end_pos": 14, "type": "METRIC", "confidence": 0.999708354473114}]}, {"text": "These figures indicate that the user would have to consult the alternatives for nearly a fifth of collocations (more if we consider sense ambiguities), but would fail to find the right translation in only 2% of cases.", "labels": [], "entities": []}, {"text": "Preliminary inspection of the evaluation results on a collocation by collocation basis reveals large numbers of incorrect key descriptors which could be eliminated, adjusted or further constrained to improve precision with little loss of recall.", "labels": [], "entities": [{"text": "precision", "start_pos": 208, "end_pos": 217, "type": "METRIC", "confidence": 0.9981098175048828}, {"text": "recall", "start_pos": 238, "end_pos": 244, "type": "METRIC", "confidence": 0.9973090887069702}]}, {"text": "This leads us to believe that a fringe precision figure of 90% or so might represent the achievable limit of accuracy using our current technology.", "labels": [], "entities": [{"text": "precision", "start_pos": 39, "end_pos": 48, "type": "METRIC", "confidence": 0.9129214882850647}, {"text": "accuracy", "start_pos": 109, "end_pos": 117, "type": "METRIC", "confidence": 0.9988285899162292}]}], "tableCaptions": []}