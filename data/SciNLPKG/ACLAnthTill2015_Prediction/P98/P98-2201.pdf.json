{"title": [{"text": "A Connectionist Approach to Prepositional Phrase Attachment for Real World Texts", "labels": [], "entities": [{"text": "Prepositional Phrase Attachment", "start_pos": 28, "end_pos": 59, "type": "TASK", "confidence": 0.8075429201126099}]}], "abstractContent": [{"text": "Ill this paper we describe a neural network-based approach to prepositional phrase attachment disam-biguation for real world texts.", "labels": [], "entities": [{"text": "prepositional phrase attachment disam-biguation", "start_pos": 62, "end_pos": 109, "type": "TASK", "confidence": 0.6910369917750359}]}, {"text": "Although the use of semantic classes in this task seems intuitively to be adequate, methods employed to date have not used them very effectively.", "labels": [], "entities": []}, {"text": "Causes of their poor results are discussed.", "labels": [], "entities": []}, {"text": "Our model, which uses only classes, scores appreciably better than the other class-based methods which have been tested on the Wall Street Journal corpus.", "labels": [], "entities": [{"text": "Wall Street Journal corpus", "start_pos": 127, "end_pos": 153, "type": "DATASET", "confidence": 0.9734160900115967}]}, {"text": "To date, the best result obtained using only classes was a score of 79.1%; we obtained an accuracy score of 86.8%.", "labels": [], "entities": [{"text": "accuracy score", "start_pos": 90, "end_pos": 104, "type": "METRIC", "confidence": 0.9791826009750366}]}, {"text": "This score is among the best reported in the literature using this corpus.", "labels": [], "entities": []}, {"text": "1 Introduction Structural ambiguity is one of the most serious problems faced by Natural Language Processing (NLP) systems.", "labels": [], "entities": []}, {"text": "It occurs when the syntactic information does not suffice to make an assignment decision.", "labels": [], "entities": []}, {"text": "Prepositional phrase (PP) attachment is, perhaps, the canonical case of structural ambiguity.", "labels": [], "entities": [{"text": "Prepositional phrase (PP) attachment", "start_pos": 0, "end_pos": 36, "type": "TASK", "confidence": 0.7113344271977743}]}, {"text": "What kind of information should we use in order to solve this ambiguity?", "labels": [], "entities": []}, {"text": "In most cases, the information needed comes from a local context, and the attach-lnent decision is based essentially on the relationships existing between predicates and arguments, what Katz y Fodor (1963) called selectional restrictions.", "labels": [], "entities": []}, {"text": "For example, in the expression: (V accommodate) (gP Johnson's election) (PP as a director), the PP is attached to the NP.", "labels": [], "entities": []}, {"text": "However, in the expression: (V taking) (NP that news) (PP as a sign to be cautions), the PP is attached to the verb.", "labels": [], "entities": []}, {"text": "In both expressions, the attachment site is decided on tile basis of verb and noun seleetional restrictions.", "labels": [], "entities": []}, {"text": "In other eases, the information determining the PP attachment comes from a global context.", "labels": [], "entities": [{"text": "PP attachment", "start_pos": 48, "end_pos": 61, "type": "TASK", "confidence": 0.8711742758750916}]}, {"text": "In this paper we will focus on the disambiguation mechanism based on selectional restrictions.", "labels": [], "entities": []}, {"text": "Previous work has shown that it is extremely difficult to build handmade rule-based systems able to deal with this kind of problem.", "labels": [], "entities": []}, {"text": "Since such handmade systems proved unsuccessful, in recent years two main methods have appeared capable of auto-1233 matic learning from tagged corpora: automatic rule based methods and statistical methods.", "labels": [], "entities": []}, {"text": "In this paper we will show that, providing that the problem is correctly approached, an NN can obtain better results than any of the methods used to date for PP attachment disambiguation.", "labels": [], "entities": [{"text": "PP attachment disambiguation", "start_pos": 158, "end_pos": 186, "type": "TASK", "confidence": 0.8796785473823547}]}, {"text": "Statistical methods consider how a local context can disambiguate PP attachment estimating the probability from a corpus: p(verb attachlv NP1 prep NP2) Since an NP can be arbitrarily complex, the problem can be simplified by considering that only the heads of the respective phrases are relevant when deciding PP attachment.", "labels": [], "entities": [{"text": "PP attachment", "start_pos": 66, "end_pos": 79, "type": "TASK", "confidence": 0.8484984934329987}, {"text": "PP attachment", "start_pos": 310, "end_pos": 323, "type": "TASK", "confidence": 0.7987545132637024}]}, {"text": "Therefore, ambiguity is resolved by means of a model that takes into account only phrasal heads: p(verb attachlverb nl prep n2).", "labels": [], "entities": []}, {"text": "There are two distinct methods for establishing the relationships between the verb and its arguments: methods using words (lexical preferences) and methods using semantic classes (selectional restrictions).", "labels": [], "entities": []}, {"text": "2 Using Words The attachment probability p(verb attach]verb nl prep n2) should be computed.", "labels": [], "entities": []}, {"text": "Due to the use of word co-occurrence, this approach comes up against the serious problem of data sparseness: the same 4-tuple (v nl prep n2) is hardly ever repeated across the corpus even when the corpus is very large.", "labels": [], "entities": []}, {"text": "Collins and Brooks (1995) showed how serious this problem can be: almost 95% of the 3097 4-tuples of their test set do not appear in their 20801 training set 4-tuples.", "labels": [], "entities": []}, {"text": "In order to reduce data sparseness, Hindle and Rooth (1993) simplified the context, by considering only verb-preposition (p(prep]verb)), and nl-preposition (p(prep]nl)) co-occurrences, n2 was ignored in spite of the fact that it may play an important role.", "labels": [], "entities": []}, {"text": "In the test, attachment to verb was decided if p(preplverb) > p(prep]noun); otherwise attachment to nl is decided.", "labels": [], "entities": []}, {"text": "Despite these limitations , 80% of PP were correctly assigned.", "labels": [], "entities": []}, {"text": "Another method for reducing data sparseness has been introduced recently by Collins and Brooks", "labels": [], "entities": []}], "introductionContent": [{"text": "Structural ambiguity is one of the most serious problems faced by Natural Language Processing (NLP) systems.", "labels": [], "entities": [{"text": "Structural ambiguity", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.8976834714412689}]}, {"text": "It occurs when the syntactic information does not suffice to make an assignment decision.", "labels": [], "entities": []}, {"text": "Prepositional phrase (PP) attachment is, perhaps, the canonical case of structural ambiguity.", "labels": [], "entities": [{"text": "Prepositional phrase (PP) attachment", "start_pos": 0, "end_pos": 36, "type": "TASK", "confidence": 0.7113344271977743}]}, {"text": "What kind of information should we use in order to solve this ambiguity?", "labels": [], "entities": []}, {"text": "In most cases, the information needed comes from a local context, and the attachlnent decision is based essentially on the relationships existing between predicates and arguments, what Katz y Fodor (1963) called selectional restrictions.", "labels": [], "entities": []}, {"text": "For example, in the expression: (V accommodate) (gP Johnson's election) (PP as a director), the PP is attached to the NP.", "labels": [], "entities": []}, {"text": "However, in the expression: (V taking) (NP that news) (PP as a sign to be cautions), the PP is attached to the verb.", "labels": [], "entities": []}, {"text": "In both expressions, the attachment site is decided on tile basis of verb and noun seleetional restrictions.", "labels": [], "entities": []}, {"text": "In other eases, the information determining the PP attachment comes from a global context.", "labels": [], "entities": [{"text": "PP attachment", "start_pos": 48, "end_pos": 61, "type": "TASK", "confidence": 0.8711742758750916}]}, {"text": "In this paper we will focus on the disambiguation mechanism based on selectional restrictions.", "labels": [], "entities": []}, {"text": "Previous work has shown that it is extremely difficult to build hand-made rule-based systems able to deal with this kind of problem.", "labels": [], "entities": []}, {"text": "Since such handmade systems proved unsuccessful, in recent years two main methods have appeared capable of auto-1233 matic learning from tagged corpora: automatic rule based methods and statistical methods.", "labels": [], "entities": []}, {"text": "In this paper we will show that, providing that the problem is correctly approached, an NN can obtain better results than any of the methods used to date for PP attachment disambiguation.", "labels": [], "entities": [{"text": "PP attachment disambiguation", "start_pos": 158, "end_pos": 186, "type": "TASK", "confidence": 0.8796785473823547}]}, {"text": "Statistical methods consider how a local context can disambiguate PP attachment estimating the probability from a corpus: Since an NP can be arbitrarily complex, the problem can be simplified by considering that only the heads of the respective phrases are relevant when deciding PP attachment.", "labels": [], "entities": [{"text": "PP attachment", "start_pos": 66, "end_pos": 79, "type": "TASK", "confidence": 0.865860253572464}, {"text": "PP attachment", "start_pos": 280, "end_pos": 293, "type": "TASK", "confidence": 0.7741650342941284}]}, {"text": "Therefore, ambiguity is resolved by means of a model that takes into account only phrasal heads: p(verb attachlverb nl prep n2).", "labels": [], "entities": []}, {"text": "There are two distinct methods for establishing the relationships between the verb and its arguments: methods using words (lexical preferences) and methods using semantic classes (selectional restrictions).", "labels": [], "entities": []}], "datasetContent": [{"text": "21418 examples of structures of the kind 'VB N1 PREP N2' were extracted from the Penn-TreeBank Wall Street Journal ().", "labels": [], "entities": [{"text": "Penn-TreeBank Wall Street Journal", "start_pos": 81, "end_pos": 114, "type": "DATASET", "confidence": 0.9331390708684921}]}, {"text": "WordNet did not cover 100% of this material.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.9835342764854431}]}, {"text": "Proper names of people were substituted by the WordNet class someone, company names by the class business_organization, and prefixed nouns for their stem (co-chairman ---* chairman).", "labels": [], "entities": []}, {"text": "788 4-tuples were discarded because of some of their words were not in WordNet and could not be substituted.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 71, "end_pos": 78, "type": "DATASET", "confidence": 0.9774290323257446}]}, {"text": "20630 codified patterns were finally obtained: 12016 (58.25%) with the PP attached to N1, and 8614 (41.75%) to VB.", "labels": [], "entities": [{"text": "VB", "start_pos": 111, "end_pos": 113, "type": "DATASET", "confidence": 0.9329326748847961}]}, {"text": "We used the cross-validation method as a measure of a correct generalization.", "labels": [], "entities": []}, {"text": "After encoding, the 20630 patterns were divided into three subsets: training set (18630 patterns), set A (1000 patterns), and set B (1000 patterns).", "labels": [], "entities": []}, {"text": "This method evaluated performance (the number of attachment errors) on a 1235 pattern set (validation set) after each complete pass through the training data (epoch).", "labels": [], "entities": [{"text": "1235 pattern set (validation set", "start_pos": 73, "end_pos": 105, "type": "DATASET", "confidence": 0.738335132598877}]}, {"text": "Series of three runs were performed that systematically varied the random starting weights.", "labels": [], "entities": []}, {"text": "In each run the networks were trained for 40 epochs.", "labels": [], "entities": []}, {"text": "In each run the weights of the epoch having the smallest error with respect to the validation set were stored.", "labels": [], "entities": []}, {"text": "The weights corresponding to the best result obtained on the validation test in the three runs were selected and used to evaluate the performance in the test set.", "labels": [], "entities": []}, {"text": "First, we used set A as validation set and set B as test, and afterwards we used set B as validation and set A as test.", "labels": [], "entities": []}, {"text": "This experiment was replicated with two new partitions of the pattern set: two new training sets (18630 patterns) and 4 new validation/test sets of 1000 patterns each.", "labels": [], "entities": []}, {"text": "Results showed in table 3 are the average accuracy over the six test sets (1000 patterns each) used.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 42, "end_pos": 50, "type": "METRIC", "confidence": 0.9993600249290466}]}, {"text": "We performed three series of runs that varied the input encoding.", "labels": [], "entities": []}, {"text": "In all these encodings, three tree cut thresholds were used: 10~o, 6~ and 2~o.", "labels": [], "entities": []}, {"text": "The number of semantic classes in the input encoding ranged from 139 (10% cut) to 475 (2%) In the first encoding, the 4-tuple without extra information was used.", "labels": [], "entities": []}, {"text": "The results for this case are shown in the 4-tuple column entry of table 3.", "labels": [], "entities": []}, {"text": "In the second encoding, we added the prepositions the verbs select for their internal arguments, since English verbs with semantic similarity could select different prepositions (for example, accuse and blame).", "labels": [], "entities": []}, {"text": "Verbs can be classified on the basis of the kind of prepositions they select.", "labels": [], "entities": []}, {"text": "Adding this classification to the WordNet I classes in the input encoding improved the results.", "labels": [], "entities": [{"text": "WordNet I classes", "start_pos": 34, "end_pos": 51, "type": "DATASET", "confidence": 0.9037577311197916}]}, {"text": "The 2% cut results were significantly better (p < 0.02) than those of the 6% cut for 4-tuple and 4-tuple + encodings.", "labels": [], "entities": []}, {"text": "Also, the results for the 4-tuple + condition were significanly better (p < 0.01).", "labels": [], "entities": []}, {"text": "For all simulations the momentum was 0.8, initial weight range 0.1.", "labels": [], "entities": [{"text": "momentum", "start_pos": 24, "end_pos": 32, "type": "METRIC", "confidence": 0.9712514877319336}]}, {"text": "No exhaustive parameter exploration was carried out, so the results can still be improved.", "labels": [], "entities": []}, {"text": "Some of the errors committed by the network can be attributed to an inadequate class assignment by WordNet.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 99, "end_pos": 106, "type": "DATASET", "confidence": 0.9738644361495972}]}, {"text": "For instance, names of countries have only one sense, that of location.", "labels": [], "entities": []}, {"text": "This sense is not appropriate in sentences like: Italy increased its sales to Spain; locations do not sell or buy anything, and the correct sense is social_group.", "labels": [], "entities": []}, {"text": "Other mistakes come from what are known as reporting and aspectual verbs.", "labels": [], "entities": []}, {"text": "For example in expressions like reported injuries to employees or iniliated lalks with the Soviets the nl has an argumental structure, and it is the element that imposes selectional restrictions on the PP.", "labels": [], "entities": []}, {"text": "There is no good classification for these kinds of verbs in WordNet.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 60, "end_pos": 67, "type": "DATASET", "confidence": 0.9686592221260071}]}, {"text": "Finally, collocations or idioms, which are very frequent, (e.g. lake a look, pay atlention), are not considered lexical units in the WSJ corpus.", "labels": [], "entities": [{"text": "WSJ corpus", "start_pos": 133, "end_pos": 143, "type": "DATASET", "confidence": 0.9230513572692871}]}, {"text": "Their idiosyncratic behaviour introduces noise in the selectional restrictions acquisition process.", "labels": [], "entities": [{"text": "selectional restrictions acquisition", "start_pos": 54, "end_pos": 90, "type": "TASK", "confidence": 0.7746175130208334}]}, {"text": "Word-based models offer a clear advantage over class-based methods in these cases.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: WordNet information for the noun 'bank'.  Sense 1  Sense 2  Sense 3", "labels": [], "entities": []}, {"text": " Table 2: Test size and accuracy results reported in previous works. 'W' denotes words only, 'C' class only and  'W+C' words+classes.  Author  [ W [ C [ W+C [ Classes  Test size  Hindle and Rooth (93)  80  Resnik and Hearst(93)  81.6 79.3  83.9  Resnik and Hearst (93)  75 a  Ratnaparkhi et al. (94) 81.2 79.1  81.6  Brill and Resnik (94)  80.8  81.8  Collins and Brooks (95) 84.5  Li and Abe (95)  85.8 \u00b0  84.9", "labels": [], "entities": [{"text": "accuracy", "start_pos": 24, "end_pos": 32, "type": "METRIC", "confidence": 0.9990886449813843}, {"text": "Collins and Brooks (95) 84.5  Li and Abe (95)  85.8", "start_pos": 352, "end_pos": 403, "type": "DATASET", "confidence": 0.8539006326879773}, {"text": "\u00b0  84.9", "start_pos": 404, "end_pos": 411, "type": "METRIC", "confidence": 0.8078180849552155}]}]}