{"title": [{"text": "Maximum Entropy Model Learning of the Translation Rules", "labels": [], "entities": []}], "abstractContent": [{"text": "This paper proposes a learning method of translation rules from parallel corpora.", "labels": [], "entities": [{"text": "translation rules from parallel corpora", "start_pos": 41, "end_pos": 80, "type": "TASK", "confidence": 0.829654061794281}]}, {"text": "This method applies the maximum entropy principle to a probabilistic model of translation rules.", "labels": [], "entities": []}, {"text": "First, we define feature functions which express statistical properties of this model.", "labels": [], "entities": []}, {"text": "Next, in order to optimize the model, the system iterates following steps: (1) selects a feature function which maximizes log-likelihood, and (2) adds this function to the model incrementally.", "labels": [], "entities": []}, {"text": "As computational cost associated with this model is too expensive, we propose several methods to suppress the overhead in order to realize the system.", "labels": [], "entities": []}, {"text": "The result shows that it attained 69.54% recall rate.", "labels": [], "entities": [{"text": "recall rate", "start_pos": 41, "end_pos": 52, "type": "METRIC", "confidence": 0.9717262983322144}]}], "introductionContent": [{"text": "A statistical natural language modeling can be viewed as estimating a combinational distribution X x Y -+ [0, 1] using training data (xl, yl>,..., <XT, YT> 6 X. x Y observed in corpora.", "labels": [], "entities": []}, {"text": "For this topic, proposed EM algorithm, which was basis of Forward-Backward algorithm for the hidden Markov model (HMM) and Inside-Outside algorithm for the pr0babilis-tic context free grammar (PCFG).", "labels": [], "entities": []}, {"text": "However, these methods have problems such as increasing optimization costs which is due to a lot of parameters.", "labels": [], "entities": []}, {"text": "Therefore, estimating a natural language model based on the maximum entropy (ME) method () has been highlighted recently.", "labels": [], "entities": []}, {"text": "On the other hand, dictionaries for multilingual natural language processing such as the machine translation has been made by human hand usually.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 89, "end_pos": 108, "type": "TASK", "confidence": 0.7384389042854309}]}, {"text": "However, since this work requires a great deal of labor and it is difficult to keep description of dictionaries consistent, the researches of automatical dictionaries making for machine translation (translation rules) from corpora become active recently.", "labels": [], "entities": [{"text": "machine translation (translation rules)", "start_pos": 178, "end_pos": 217, "type": "TASK", "confidence": 0.8368439873059591}]}, {"text": "In this paper, we notice that estimating a language model based on ME method is suitable for learning the translation rules, and propose several methods to resolve problems in adapting ME method to learning the translation rules.", "labels": [], "entities": []}], "datasetContent": [{"text": "As the training corpora, we used 6,057 pairs of sentences included in Kodansya JapaneseEnglish Dictionary, a machine-readable dictionary made by the Electrotechnical Laboratory.", "labels": [], "entities": [{"text": "Kodansya JapaneseEnglish Dictionary", "start_pos": 70, "end_pos": 105, "type": "DATASET", "confidence": 0.87995578845342}]}, {"text": "By applying morphological analysis for the corpora, each word was transformed to the infinitive form.", "labels": [], "entities": []}, {"text": "We excluded words which appeared below 3 times or over 1,000 times from the target of learning.", "labels": [], "entities": []}, {"text": "Consequently, our target for the experiments included 1,375 English words and 1,195 Japanese words, and we prepared 1,375 feature functions for model 1 and 2,744 for model 2 (56 part-of-speech for English and 49 part-of-speech for Japanese).", "labels": [], "entities": []}, {"text": "We tried to learn the translation rules from English to Japanese.", "labels": [], "entities": []}, {"text": "We had two experiments: one of model 1 as the set of feature functions, and one of model 1 + 2.", "labels": [], "entities": []}, {"text": "For each experiment, 500 feature functions were selected according to the feature selection algorithm described in section 3.4, and we calculated p(yI x) in equation, that is, the probability that English word x is translated into Japanese wordy.", "labels": [], "entities": []}, {"text": "For each English word, all Japanese word were ordered by estimated probability p(yix), and we evaluated the recall rates by comparing the dictionary.", "labels": [], "entities": [{"text": "recall", "start_pos": 108, "end_pos": 114, "type": "METRIC", "confidence": 0.9992138147354126}]}, {"text": "recall rates when the empirical probability defined by equation (1) was used instead of the estimated probability.", "labels": [], "entities": [{"text": "recall", "start_pos": 0, "end_pos": 6, "type": "METRIC", "confidence": 0.9813736081123352}]}, {"text": "It is showed that the model 1 + 2 attains higher recall rates than the model 1 and ~(x, y).", "labels": [], "entities": [{"text": "recall rates", "start_pos": 49, "end_pos": 61, "type": "METRIC", "confidence": 0.9879757165908813}]}, {"text": "shows the log-likelihood for each model plotted by the number of feature functions in the feature selection algorithm.", "labels": [], "entities": []}, {"text": "Notice that the log-likelihood for the model 1+2 is always higher than the model 1.", "labels": [], "entities": []}, {"text": "Thus, the model 1 + 2 is more'effective than the model 1 for learning the translation rules.", "labels": [], "entities": [{"text": "translation rules", "start_pos": 74, "end_pos": 91, "type": "TASK", "confidence": 0.823724776506424}]}, {"text": "However, the result shows that the recall Ihe nun~ od ~t~ll: log-likelihood rates of the '1st' for all models are not favorable.", "labels": [], "entities": [{"text": "recall", "start_pos": 35, "end_pos": 41, "type": "METRIC", "confidence": 0.9959127306938171}]}, {"text": "We consider that it is the reason for this to assume word-to-word translation rules implicitly.", "labels": [], "entities": [{"text": "word-to-word translation", "start_pos": 53, "end_pos": 77, "type": "TASK", "confidence": 0.6690956354141235}]}], "tableCaptions": []}