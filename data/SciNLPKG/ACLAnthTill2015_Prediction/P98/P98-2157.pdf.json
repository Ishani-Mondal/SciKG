{"title": [{"text": "Prefix Probabilities from Stochastic Tree Adjoining Grammars*", "labels": [], "entities": []}], "abstractContent": [{"text": "Language models for speech recognition typically use a probability model of the form", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 20, "end_pos": 38, "type": "TASK", "confidence": 0.7916257083415985}]}], "introductionContent": [{"text": "Given some word sequence al'.'an-1, speech recognition language models are used to hypothesize the next word an, which could be any word from the vocabulary F~.", "labels": [], "entities": []}, {"text": "This is typically done using a probability model.", "labels": [], "entities": []}, {"text": "Based on the assumption that modelling the hidden structure of nat-* Part of this research was done while the first and the third authors were visiting the Institute for Research in Cognitive Science, University of Pennsylvania.", "labels": [], "entities": []}, {"text": "The first author was supported by the German Federal Ministry of Education, Science, Research and Technology (BMBF) in the framework of the VERBMOBIL Project under Grant 01 IV 701 V0, and by the Priority Programme Language and Speech Technology, which is sponsored by NWO (Dutch Organization for Scientific Research).", "labels": [], "entities": []}, {"text": "The second and third authors were partially supported by NSF grant SBR8920230 and ARO grant DAAH0404-94-G-0426.", "labels": [], "entities": [{"text": "NSF grant SBR8920230", "start_pos": 57, "end_pos": 77, "type": "DATASET", "confidence": 0.6996751427650452}, {"text": "ARO grant DAAH0404-94-G-0426", "start_pos": 82, "end_pos": 110, "type": "METRIC", "confidence": 0.6968529423077902}]}, {"text": "The authors wish to thank Aravind Joshi for his support in this research.", "labels": [], "entities": []}, {"text": "ural language would improve performance of such language models, some researchers tried to use stochastic context-free grammars (CFGs) to produce language models).", "labels": [], "entities": []}, {"text": "The probability model used fora stochastic grammar was ~we~*.", "labels": [], "entities": []}, {"text": "However, language models that are based on trigram probability models out-perform stochastic CFGs.", "labels": [], "entities": []}, {"text": "The common wisdom about this failure of CFGs is that trigram models are lexicalized models while CFGs are not.", "labels": [], "entities": []}, {"text": "Tree Adjoining Grammars (TAGs) are important in this respect since they are easily lexicalized while capturing the constituent structure of language.", "labels": [], "entities": [{"text": "Tree Adjoining Grammars (TAGs)", "start_pos": 0, "end_pos": 30, "type": "TASK", "confidence": 0.6665929555892944}]}, {"text": "More importantly, TAGs allow greater linguistic expressiveness.", "labels": [], "entities": []}, {"text": "The trees associated with words can be used to encode argument and adjunct relations in various syntactic environments.", "labels": [], "entities": []}, {"text": "This paper assumes some familiarity with the TAG formalism. and are good introductions to the formalism and its linguistic relevance.", "labels": [], "entities": [{"text": "TAG formalism.", "start_pos": 45, "end_pos": 59, "type": "TASK", "confidence": 0.47258588671684265}]}, {"text": "TAGs have been shown to have relations with both phrase-structure grammars and dependency grammars, which is relevant because recent work on structured language models () have used dependency grammars to exploit their lexicalization.", "labels": [], "entities": []}, {"text": "We use stochastic TAGs as such a structured language model in contrast with earlier work where TAGs have been exploited in a class-based n-gram language model.", "labels": [], "entities": []}, {"text": "This paper derives an algorithm to compute prefix probabilities ~we~* Pr(al... anw).", "labels": [], "entities": []}, {"text": "The algorithm assumes as input a stochastic TAG G and a string which is a prefix of some string in L(G), the language generated by G. This algorithm enables existing corpus-based estimation techniques in stochastic TAGs to be used for language modelling.", "labels": [], "entities": [{"text": "language modelling", "start_pos": 235, "end_pos": 253, "type": "TASK", "confidence": 0.7458214461803436}]}], "datasetContent": [], "tableCaptions": []}