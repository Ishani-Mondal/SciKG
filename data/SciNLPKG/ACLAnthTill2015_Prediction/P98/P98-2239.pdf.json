{"title": [{"text": "Word Association and MI-Trigger-based Language Modeling", "labels": [], "entities": [{"text": "MI-Trigger-based Language Modeling", "start_pos": 21, "end_pos": 55, "type": "TASK", "confidence": 0.5890214840571085}]}], "abstractContent": [{"text": "There exists strong word association in natural language.", "labels": [], "entities": []}, {"text": "Based on mutual information, this paper proposes anew MI-Trigger-based modeling approach to capture the preferred relationships between words over a short or long distance.", "labels": [], "entities": []}, {"text": "Both the distance-independent(DI) and distance-dependent(DD) MI-Trigger-based models are constructed within a window.", "labels": [], "entities": []}, {"text": "It is found that proper MI-Trigger modeling is superior to word bigram model and the DD MI-Trigger models have better performance than the DI MI-Trigger models for the same window size.", "labels": [], "entities": []}, {"text": "It is also found that the number of the trigger pairs in an MI-Trigger model can be kept to a reasonable size without losing too much of its modeling power.", "labels": [], "entities": []}, {"text": "Finally, it is concluded that the preferred relationships between words are useful to language disambiguation and can be modeled efficiently by the MI-Trigger-based modeling approach.", "labels": [], "entities": [{"text": "language disambiguation", "start_pos": 86, "end_pos": 109, "type": "TASK", "confidence": 0.7092798054218292}]}], "introductionContent": [{"text": "In natural language there always exist many preferred relationships between words.", "labels": [], "entities": []}, {"text": "Lexicographers always use the concepts of collocation, co-occurrence and lexis to describe them.", "labels": [], "entities": []}, {"text": "Psychologists also have a similar concept: word association.", "labels": [], "entities": [{"text": "word association", "start_pos": 43, "end_pos": 59, "type": "TASK", "confidence": 0.6951009780168533}]}, {"text": "Two highly associated word pairs are \"not only/but also\" and \"doctor/nurse\".", "labels": [], "entities": []}, {"text": "Psychological experiments in indicated that the human's reaction to a highly associated word pair was stronger and faster than that to a poorly associated word pair.", "labels": [], "entities": []}, {"text": "The strength of word association can be measured by mutual information.", "labels": [], "entities": []}, {"text": "By computing mutual information of a word pair, we can get many useful preference information from the corpus, such as the semantic preference between noun and noun(e.g.\"doctor/nurse\"), the particular preference between adjective and noun(e.g.\"strong/currency'), and solid structure (e.g.\"pay/attention\").", "labels": [], "entities": []}, {"text": "These information are useful for automatic sentence disambiguation.", "labels": [], "entities": [{"text": "automatic sentence disambiguation", "start_pos": 33, "end_pos": 66, "type": "TASK", "confidence": 0.6427669823169708}]}, {"text": "Similar research includes,, Magerman+90],,, and.", "labels": [], "entities": [{"text": "Magerman+90", "start_pos": 28, "end_pos": 39, "type": "DATASET", "confidence": 0.591984232266744}]}, {"text": "In Chinese, a word is made up of one or more characters.", "labels": [], "entities": []}, {"text": "Hence, there also exists preferred relationships between Chinese characters.", "labels": [], "entities": []}, {"text": "[Sproat+90] employed a statistical method to group neighboring Chinese characters in a sentence into two-character words by making use of a measure of character association based on mutual information.", "labels": [], "entities": [{"text": "Sproat+90]", "start_pos": 1, "end_pos": 11, "type": "DATASET", "confidence": 0.8565900325775146}]}, {"text": "Here, we will focus instead on the preferred relationships between words.", "labels": [], "entities": []}, {"text": "The preference relationships between words can expand from a short to long distance.", "labels": [], "entities": []}, {"text": "While N-gram models are simple in language modeling and have been successfully used in many tasks, they have obvious deficiencies.", "labels": [], "entities": [{"text": "language modeling", "start_pos": 34, "end_pos": 51, "type": "TASK", "confidence": 0.7166797816753387}]}, {"text": "For instance, Ngram models can only capture the short-distance dependency within an N-word window where currently the largest practical N for natural language is three and many kinds of dependencies in natural language occur beyond a three-word window.", "labels": [], "entities": []}, {"text": "While we can use conventional N-gram models to capture the short-distance dependency, the long-distance dependency should also be exploited properly.", "labels": [], "entities": []}, {"text": "The purpose of this paper is to study the preferred relationships between words over a short or long distance and propose anew modeling approach to capture such phenomena in the Chinese language.", "labels": [], "entities": []}, {"text": "This paper is organized as follows: Section 1 defines the concept of trigger pair.", "labels": [], "entities": []}, {"text": "The criteria of selecting a trigger pair are described in Section 2 while Section 3 describes how to measure the strength of a trigger pair.", "labels": [], "entities": []}, {"text": "Section 4 describes trigger-based language modeling.", "labels": [], "entities": [{"text": "trigger-based language modeling", "start_pos": 20, "end_pos": 51, "type": "TASK", "confidence": 0.6777035693327585}]}, {"text": "Section 5 gives one of its applications: PINYIN-to-Character Conversion.", "labels": [], "entities": [{"text": "PINYIN-to-Character Conversion", "start_pos": 41, "end_pos": 71, "type": "TASK", "confidence": 0.8360885977745056}]}, {"text": "Finally, a conclusion is given.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Conditional perplexities of the long- distance WB models for different distances", "labels": [], "entities": []}, {"text": " Table 2: The occurrence frequency of word  pairs as a function of distance  To compare the effects of the above two  factors, 20 MI-trigger models(in which DI and", "labels": [], "entities": []}, {"text": " Table 3: The conditional perplexities of the 20  different MI-Trigger models", "labels": [], "entities": []}, {"text": " Table 4: The PYCC recognition  MI-Trigger models  No. of the MI- Trigger Pairs  0  100,000  200,000  400,000", "labels": [], "entities": [{"text": "PYCC recognition  MI-Trigger", "start_pos": 14, "end_pos": 42, "type": "DATASET", "confidence": 0.6460757454236349}]}, {"text": " Table 5: The effect of different numbers of the  trigger pairs on the PYCC recognition rates", "labels": [], "entities": [{"text": "PYCC recognition", "start_pos": 71, "end_pos": 87, "type": "TASK", "confidence": 0.7188098728656769}]}, {"text": " Table 6: Comparison of word umgram, bigram  and MI-Trigger model  In order to evaluate the efficiency of MI- Trigger-based language modeling, we compare it  with word unigram and bigram models. Both  word unigram and word bigram models are  trained on the XinHua corpus of 29M words. The  result is shown in", "labels": [], "entities": [{"text": "XinHua corpus of 29M words", "start_pos": 257, "end_pos": 283, "type": "DATASET", "confidence": 0.8904583334922791}]}, {"text": " Table 7: The PYCC recognition rates of word  bigram and MI-Trigger merging  Through the experiments, it has been proven  that the merged model has better results over both  word bigram and Ml-Trigger models. Compared  to the pure word bigram model, the merged model  also captures the long-distance dependency of  word pairs using the concept of mutual  information. Compared to the MI-trigger model  which only captures highly correlated word pairs,  the merged model also captures poorly correlated  word pairs within a short distance by using the  word bigram model.", "labels": [], "entities": [{"text": "PYCC recognition", "start_pos": 14, "end_pos": 30, "type": "TASK", "confidence": 0.6535048931837082}]}]}