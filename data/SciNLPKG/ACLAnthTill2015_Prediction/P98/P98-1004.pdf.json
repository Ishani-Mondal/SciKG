{"title": [{"text": "A Simple Hybrid Aligner for Generating Lexical Correspondences in Parallel Texts", "labels": [], "entities": []}], "abstractContent": [{"text": "We present an algorithm for bilingual word alignment that extends previous work by treating multi-word candidates on a par with single words, and combining some simple assumptions about the translation process to capture alignments for low frequency words.", "labels": [], "entities": [{"text": "bilingual word alignment", "start_pos": 28, "end_pos": 52, "type": "TASK", "confidence": 0.6334043641885122}]}, {"text": "As most other alignment algorithms it uses co-occurrence statistics as a basis, but differs in the assumptions it makes about the translation process.", "labels": [], "entities": []}, {"text": "The algorithm has been implemented in a modular system that allows the user to experiment with different combinations and variants of these assumptions.", "labels": [], "entities": []}, {"text": "We give performance results from two evaluations, which compare well with results reported in the literature.", "labels": [], "entities": []}], "introductionContent": [{"text": "In recent years much progress have been made in the area of bilingual alignment for the support of tasks such as machine translation, machine-aided translation, bilingual lexicography and terminology.", "labels": [], "entities": [{"text": "bilingual alignment", "start_pos": 60, "end_pos": 79, "type": "TASK", "confidence": 0.752388596534729}, {"text": "machine translation", "start_pos": 113, "end_pos": 132, "type": "TASK", "confidence": 0.8012970983982086}, {"text": "machine-aided translation", "start_pos": 134, "end_pos": 159, "type": "TASK", "confidence": 0.6865425407886505}]}, {"text": "For instance, reports that his word-to-word model for translational equivalence produced lexicon entries with 99% precision and 46% recall when trained on 13 million words of the Hansard corpus, where recall was measured as the fraction of words from the bitext that were assigned some translation.", "labels": [], "entities": [{"text": "translational equivalence", "start_pos": 54, "end_pos": 79, "type": "TASK", "confidence": 0.9254790246486664}, {"text": "precision", "start_pos": 114, "end_pos": 123, "type": "METRIC", "confidence": 0.9974639415740967}, {"text": "recall", "start_pos": 132, "end_pos": 138, "type": "METRIC", "confidence": 0.9989303946495056}, {"text": "Hansard corpus", "start_pos": 179, "end_pos": 193, "type": "DATASET", "confidence": 0.9813242852687836}, {"text": "recall", "start_pos": 201, "end_pos": 207, "type": "METRIC", "confidence": 0.9971022009849548}]}, {"text": "Using the same model but less data, a French/English software manual of 400,000 words, reported 94% precision with 30% recall.", "labels": [], "entities": [{"text": "French/English software manual", "start_pos": 38, "end_pos": 68, "type": "DATASET", "confidence": 0.8008863210678101}, {"text": "precision", "start_pos": 100, "end_pos": 109, "type": "METRIC", "confidence": 0.9995800852775574}, {"text": "recall", "start_pos": 119, "end_pos": 125, "type": "METRIC", "confidence": 0.9986036419868469}]}, {"text": "While these figures are indeed impressive, more telling figures can only be obtained by measuring the effect of the alignment system on some specific task.", "labels": [], "entities": []}, {"text": "reports that their Termight system helped double the speed at which terminology lists could be compiled at the AT&T Business Translation Services.", "labels": [], "entities": [{"text": "AT&T Business Translation", "start_pos": 111, "end_pos": 136, "type": "TASK", "confidence": 0.8075078725814819}]}, {"text": "It is also clear that the usability of bilingual concordances would be greatly improved if the system could indicate both items of a translation pair and if phrases could be looked up with the same ease and precision as single words.", "labels": [], "entities": [{"text": "ease", "start_pos": 198, "end_pos": 202, "type": "METRIC", "confidence": 0.9711648225784302}, {"text": "precision", "start_pos": 207, "end_pos": 216, "type": "METRIC", "confidence": 0.8848771452903748}]}, {"text": "For the language pairs that are of particular interest to us, English vs. other Germanic languages, the ability to handle multi-word units adequately is crucial (cf..", "labels": [], "entities": []}, {"text": "In English a large number of technical terms are multi-word compounds, while the corresponding terms in other Germanic languages are often single-word compounds.", "labels": [], "entities": []}, {"text": "We illustrate with a few examples from an English/Swedish computer manual: Also, many common adverbials and prepositions are multi-word units, which mayor may not be translated as such.", "labels": [], "entities": []}], "datasetContent": [{"text": "The algorithm was tested on two different texts; one novel (66,693 source words) and one computer program manual (169.779 source words) which both were translated from English into Swedish.", "labels": [], "entities": []}, {"text": "The tests were run on a Sun UltraSparcl Workstation with 320 MB RAM and took 55 minutes for the novel and 4 and a half hour for the program manual.", "labels": [], "entities": []}, {"text": "The tests were run with three different configurations on each text: (i) the baseline (B) configuration which is the t-score measure, (ii) all modules except the weights module (AM-W), but a linkdistance constraint was used and set to 10; and (iii) all modules (AM) including morphology, weights and phrases.", "labels": [], "entities": []}, {"text": "The t-score threshold used was 1.65 for B and AM-W, and 2.7 for AM, the minimum frequency of source expression was set to 3.", "labels": [], "entities": []}, {"text": "Closed-class expressions were linked in all configurations.", "labels": [], "entities": []}, {"text": "In the baseline configuration no distinction was made between closed-class and open-class expressions.", "labels": [], "entities": []}, {"text": "In the AM-W and AM tests the closed-class expressions were divided into different subcategories and at the end of each iteration the linking direction was reversed at the end of each of the six iterations which improves the chances of linking low frequency source expressions.", "labels": [], "entities": [{"text": "AM", "start_pos": 16, "end_pos": 18, "type": "DATASET", "confidence": 0.7926291823387146}]}, {"text": "The characteristics of the source texts used are shown in..", "labels": [], "entities": []}, {"text": "Results from two bitexts, using T-score W), and all modules (AM a high number of low frequency words whereas the program manual contains a higher proportion of words that the algorithm acturally tested as the frequency threshold was set to 3.", "labels": [], "entities": []}, {"text": "The results from the tests are shown in.", "labels": [], "entities": []}, {"text": "The evaluation was done on an extract from the automatically produced dictionary.", "labels": [], "entities": []}, {"text": "All expressions starting with the letters N, O and P were evaluated for all three configurations of each text.", "labels": [], "entities": []}, {"text": "The results from the novel show that recall is almost tripled in the sample, from 234 in the B configuration to 709 linked source expressions with the AM configuration.", "labels": [], "entities": [{"text": "recall", "start_pos": 37, "end_pos": 43, "type": "METRIC", "confidence": 0.999041736125946}]}, {"text": "Precision values for the novel lie in the range from 90.13 to 92.50 percent when partial links are judged as errors and slightly higher if they are not.", "labels": [], "entities": [{"text": "Precision", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.9953859448432922}]}, {"text": "The use of weights seems to make precision somewhat lower for the novel which perhaps could be explained by the fact that the novel is a much more varied text type.", "labels": [], "entities": [{"text": "precision", "start_pos": 33, "end_pos": 42, "type": "METRIC", "confidence": 0.9994053840637207}]}, {"text": "For the program manual the recall results are as good as for the novel (three times as many linked source types for the AM configuration compared to baseline).", "labels": [], "entities": [{"text": "recall", "start_pos": 27, "end_pos": 33, "type": "METRIC", "confidence": 0.9996054768562317}]}, {"text": "Precision is increased, but perhaps not to the level we anticipated at first.", "labels": [], "entities": [{"text": "Precision", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.9898738861083984}]}, {"text": "Multi-word expressions are linked with a relatively high recall (above 70%), but the precision of these links are not as high as for single words.", "labels": [], "entities": [{"text": "recall", "start_pos": 57, "end_pos": 63, "type": "METRIC", "confidence": 0.9993658661842346}, {"text": "precision", "start_pos": 85, "end_pos": 94, "type": "METRIC", "confidence": 0.9995130300521851}]}, {"text": "Our evaluations of the links show that one major problem lies in the quality of the multi-word expressions that are fed into the alignment program.", "labels": [], "entities": []}, {"text": "As the program works iteratively and in the current version starts with the multi-word expressions, any errors at this stage will have consequences in later iterations.", "labels": [], "entities": []}, {"text": "We have run each module separately and observed that the addition of each module improves the baseline configuration by itself.", "labels": [], "entities": []}, {"text": "To compare our results to those from other approaches is difficult.", "labels": [], "entities": []}, {"text": "Not only are we dealing with different language pairs but also with different texts and text types.", "labels": [], "entities": []}, {"text": "There is also the issue of different evaluation criteria.", "labels": [], "entities": []}, {"text": "A pure wordto-word alignment cannot be compared to an approach where lexical units (both single word expressions and multi-word expressions) are linked.", "labels": [], "entities": [{"text": "wordto-word alignment", "start_pos": 7, "end_pos": 28, "type": "TASK", "confidence": 0.7124041318893433}]}, {"text": "Neither can the combined approach be compared to a pure phrase alignment program because the aims of the alignment are different.", "labels": [], "entities": [{"text": "phrase alignment", "start_pos": 56, "end_pos": 72, "type": "TASK", "confidence": 0.7156387567520142}]}, {"text": "However, as far as we can judge given these difficulties, the results presented in this paper are on par with previous work for precision and possibly an improvement on recall because of how we handle low-frequency variants in the morphology module and by using the single-wordline strategy.", "labels": [], "entities": [{"text": "precision", "start_pos": 128, "end_pos": 137, "type": "METRIC", "confidence": 0.9991338849067688}, {"text": "recall", "start_pos": 169, "end_pos": 175, "type": "METRIC", "confidence": 0.9981024861335754}]}, {"text": "The handling of closed-class expressions have also been improved due to the division of these expressions into subcategories which limits the search space considerably.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 4. Results from two bitexts, using T-score  W), and all modules (AM", "labels": [], "entities": []}, {"text": " Table 3. Characteristics for the two source texts", "labels": [], "entities": []}]}