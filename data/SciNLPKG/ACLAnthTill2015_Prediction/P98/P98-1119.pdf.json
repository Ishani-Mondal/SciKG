{"title": [{"text": "Automatic Acquisition of Language Model based on Head-Dependent Relation between Words", "labels": [], "entities": [{"text": "Head-Dependent Relation between Words", "start_pos": 49, "end_pos": 86, "type": "TASK", "confidence": 0.7997644692659378}]}], "abstractContent": [{"text": "Language modeling is to associate a sequence of words with a priori probability, which is a key part of many natural language applications such as speech recognition and statistical machine translation.", "labels": [], "entities": [{"text": "Language modeling", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.7309046983718872}, {"text": "speech recognition", "start_pos": 147, "end_pos": 165, "type": "TASK", "confidence": 0.756776750087738}, {"text": "statistical machine translation", "start_pos": 170, "end_pos": 201, "type": "TASK", "confidence": 0.6905596951643626}]}, {"text": "In this paper, we present a language modeling based on a kind of simple dependency grammar.", "labels": [], "entities": []}, {"text": "The grammar consists of head-dependent relations between words and can be learned automatically from a raw corpus using the reestimation algorithm which is also introduced in this paper.", "labels": [], "entities": []}, {"text": "Our experiments show that the proposed model performs better than n-gram models at 11% to 11.5~ reductions in test corpus entropy.", "labels": [], "entities": []}], "introductionContent": [{"text": "Language modeling is to associate a priori probability to a sentence.", "labels": [], "entities": [{"text": "Language modeling", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.6781911551952362}]}, {"text": "It is a key part of many natural language applications such as speech recognition and statistical machine translation.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 63, "end_pos": 81, "type": "TASK", "confidence": 0.8364953398704529}, {"text": "statistical machine translation", "start_pos": 86, "end_pos": 117, "type": "TASK", "confidence": 0.744626541932424}]}, {"text": "Previous works for language modeling can be broadly divided into two approaches; one is ngram-based and the other is grammar-based.", "labels": [], "entities": [{"text": "language modeling", "start_pos": 19, "end_pos": 36, "type": "TASK", "confidence": 0.7306229621171951}]}, {"text": "N-gram model estimates the probability of a sentence as the product of the probability of each word in the sentence.", "labels": [], "entities": []}, {"text": "It assumes that probability of the nth word is dependent on the previous n-1 words.", "labels": [], "entities": []}, {"text": "The n-gram probabilities are estimated by simply counting the n-gram frequencies in a training corpus.", "labels": [], "entities": []}, {"text": "In some cases, class (or part of speech) n-grams are used instead of word n-grams(.", "labels": [], "entities": []}, {"text": "N-gram model has been widely used so far, but it has always been clear that n-gram cannot represent long distance dependencies.", "labels": [], "entities": []}, {"text": "In contrast with n-gram model, grammarbased approach assigns syntactic structures to a sentence and computes the probability of the sentence using the probabilities of the structures.", "labels": [], "entities": []}, {"text": "Long distance dependencies can be represented well by means of the structures.", "labels": [], "entities": []}, {"text": "The approach usually makes use of phrase structure grammars such as probabilistic context-free grammar and recursive transition network.", "labels": [], "entities": []}, {"text": "In the approach, however, a sentence which is not accepted by the grammar is assigned zero probability.", "labels": [], "entities": []}, {"text": "Thus, the grammar must have broadcoverage so that any sentence will get non-zero probability.", "labels": [], "entities": []}, {"text": "But acquisition of such a robust grammar has been known to be very difficult.", "labels": [], "entities": []}, {"text": "Due to the difficulty, some works try to use an integrated model of grammar and n-gram compensating each other.", "labels": [], "entities": []}, {"text": "Given a robust grammar, grammar-based language modeling is expected to be more powerful and compact in model size than n-gram-based one.", "labels": [], "entities": []}, {"text": "In this paper we present a language modeling based on a kind of simple dependency grammar.", "labels": [], "entities": []}, {"text": "The grammar consists of head-dependent relations between words and can be learned automatically from a raw corpus using the reestimation algorithm which is also introduced in this paper.", "labels": [], "entities": []}, {"text": "Based on the dependencies, a sentence is analyzed and assigned syntactic structures by which long distance dependences are represented.", "labels": [], "entities": []}, {"text": "Because the model can bethought of as a linguistic bi-gram model, the smoothing functions of n-gram models can be applied to it.", "labels": [], "entities": []}, {"text": "Thus, the model can be robust, adapt easily to new domains, and be effective.", "labels": [], "entities": []}, {"text": "The paper is organized as follows.", "labels": [], "entities": []}, {"text": "We introduce some definitions and notations for the dependency grammar and the reestimation algorithm in section 2, and explain the algorithm in section 3.", "labels": [], "entities": []}, {"text": "In section 4, we show the experimental results for the suggested model compared to n-gram models.", "labels": [], "entities": []}, {"text": "Finally, section 5 concludes this paper.", "labels": [], "entities": []}], "datasetContent": [{"text": "We have experimented with three language models, tri-gram model (TRI), bi-gram model, and the proposed model (DEP) on a raw corpus extracted from KAIST corpus 3.", "labels": [], "entities": [{"text": "KAIST corpus 3", "start_pos": 146, "end_pos": 160, "type": "DATASET", "confidence": 0.9308948318163554}]}, {"text": "The raw corpus consists of 1,589 sentences with 13,139 words, describing animal life in nature.", "labels": [], "entities": []}, {"text": "We randomly divided the corpus into two parts: a training set of 1,445 sentences and a test set of 144 sentences.", "labels": [], "entities": []}, {"text": "And we made 15 partial training sets which include the first s sentences in the whole training set, for s ranging from 100 to 1,445 sentences.", "labels": [], "entities": []}, {"text": "We trained the three language models for each partial training set, and tested the training and the test corpus entropies.", "labels": [], "entities": []}, {"text": "TRI and BI was trained by counting the occurrence of tri-grams and bi-grams respectively.", "labels": [], "entities": [{"text": "TRI", "start_pos": 0, "end_pos": 3, "type": "METRIC", "confidence": 0.7240407466888428}, {"text": "BI", "start_pos": 8, "end_pos": 10, "type": "METRIC", "confidence": 0.9970911741256714}]}, {"text": "DEP was trained by running the reestimation algorithm iteratively until it converges to an optimal dependency grammar.", "labels": [], "entities": [{"text": "DEP", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.6612794399261475}]}, {"text": "On the average, 26 iterations were done for the training sets.", "labels": [], "entities": []}, {"text": "Smoothing is needed for language modeling due to the sparse data problem.", "labels": [], "entities": [{"text": "language modeling", "start_pos": 24, "end_pos": 41, "type": "TASK", "confidence": 0.7945536077022552}]}, {"text": "It is to compensate for the overestimated and the underestimated probabilities.", "labels": [], "entities": []}, {"text": "Smoothing method itself is an important factor.", "labels": [], "entities": []}, {"text": "But our goal is not to find out a better smoothing method.", "labels": [], "entities": [{"text": "smoothing", "start_pos": 41, "end_pos": 50, "type": "TASK", "confidence": 0.9826942086219788}]}, {"text": "So we fixed on an interpolation method and applied it for the three models.", "labels": [], "entities": []}, {"text": "It can be represented as ..., w,-x) = ,\\P,(wilw,-,+l, ..., wi_l) The Ks is the global smoothing factor.", "labels": [], "entities": []}, {"text": "The bigger the Ks, the larger the degree of smoothing.", "labels": [], "entities": []}, {"text": "For the experiments we used 2 for Ks.", "labels": [], "entities": []}, {"text": "We take the performance of a language model to be its cross-entropy on test corpus, 3KAIST (Korean Advanced Institute of Science and Technology) corpus has been under construction since 1994.", "labels": [], "entities": [{"text": "3KAIST (Korean Advanced Institute of Science and Technology) corpus", "start_pos": 84, "end_pos": 151, "type": "DATASET", "confidence": 0.8553846099159934}]}, {"text": "It consists of raw text collection(45,000,000 words), POS-tagged collection(6,750,000 words), and tree-tagged collection(30,000 sentences) at present.", "labels": [], "entities": []}, {"text": "where the test corpus contains a total of IV] words and is composed of S sentences.: Training corpus entropies shows the training corpus entropies of the three models.", "labels": [], "entities": []}, {"text": "It is not surprising that DEP performs better than BI.", "labels": [], "entities": [{"text": "DEP", "start_pos": 26, "end_pos": 29, "type": "TASK", "confidence": 0.7872480154037476}, {"text": "BI", "start_pos": 51, "end_pos": 53, "type": "METRIC", "confidence": 0.8765134215354919}]}, {"text": "DEP can bethought of as a kind of linguistic bi-gram model in which long distance dependencies can be represented through the head-dependent relations between words.", "labels": [], "entities": []}, {"text": "TRI shows better performance than both BI and DEP.", "labels": [], "entities": [{"text": "TRI", "start_pos": 0, "end_pos": 3, "type": "METRIC", "confidence": 0.8311163783073425}, {"text": "BI", "start_pos": 39, "end_pos": 41, "type": "METRIC", "confidence": 0.9901767373085022}]}, {"text": "We think it is because TRI overfits the training corpus, judging from the experimental results for the test corpus.", "labels": [], "entities": [{"text": "TRI", "start_pos": 23, "end_pos": 26, "type": "METRIC", "confidence": 0.716724693775177}]}, {"text": "For the test corpus, BI shows slightly better performance than TRI as depicted in.", "labels": [], "entities": [{"text": "BI", "start_pos": 21, "end_pos": 23, "type": "METRIC", "confidence": 0.9981361627578735}, {"text": "TRI", "start_pos": 63, "end_pos": 66, "type": "METRIC", "confidence": 0.9739744067192078}]}, {"text": "Increase in the order of n-gram from two to three shows no gains in entropy reduction.", "labels": [], "entities": []}, {"text": "DEP, however, Shows still better performance than the n-gram models.", "labels": [], "entities": [{"text": "DEP", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.8496891260147095}]}, {"text": "It shows about 11.5% entropy reduction to BI and about 11% entropy reduction to TRI.", "labels": [], "entities": [{"text": "BI", "start_pos": 42, "end_pos": 44, "type": "METRIC", "confidence": 0.9954696893692017}, {"text": "TRI", "start_pos": 80, "end_pos": 83, "type": "METRIC", "confidence": 0.9612113833427429}]}, {"text": "shows the entropies for the mixed corpus of training and test sets.", "labels": [], "entities": []}, {"text": "From the results, we can see that head-dependent relations between words are more useful information than the naive ngram sequences, for language modeling.", "labels": [], "entities": [{"text": "language modeling", "start_pos": 137, "end_pos": 154, "type": "TASK", "confidence": 0.7057041823863983}]}, {"text": "We can see also that the reestimation algorithm can find out properly the hidden head-dependent relations between words, from a raw corpus.", "labels": [], "entities": []}, {"text": "No. of training sentences Related to the size of model, however, DEP has much more parameters than TRI and BI as depicted in.", "labels": [], "entities": [{"text": "TRI", "start_pos": 99, "end_pos": 102, "type": "METRIC", "confidence": 0.9892724752426147}, {"text": "BI", "start_pos": 107, "end_pos": 109, "type": "METRIC", "confidence": 0.9935827851295471}]}, {"text": "This can be a serious problem when we create a language model from a large body of text.", "labels": [], "entities": []}, {"text": "In the experiments, however, DEP used the grammar acquired automatically as it is.", "labels": [], "entities": []}, {"text": "In the grammar, many inter-word dependencies have probabilities near 0.", "labels": [], "entities": []}, {"text": "If we exclude such dependencies as was experimented for n-grams by, we may get much more compact DEP model with very slight increase in entropy.", "labels": [], "entities": []}], "tableCaptions": []}