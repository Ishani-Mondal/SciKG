{"title": [{"text": "Error-Driven Pruning of Treebank Grammars for Base Noun Phrase Identification", "labels": [], "entities": [{"text": "Base Noun Phrase Identification", "start_pos": 46, "end_pos": 77, "type": "TASK", "confidence": 0.7031788229942322}]}], "abstractContent": [{"text": "Finding simple, non-recursive, base noun phrases is an important subtask for many natural language processing applications.", "labels": [], "entities": []}, {"text": "While previous empirical methods for base NP identification have been rather complex, this paper instead proposes a very simple algorithm that is tailored to the relative simplicity of the task.", "labels": [], "entities": [{"text": "NP identification", "start_pos": 42, "end_pos": 59, "type": "TASK", "confidence": 0.7072469890117645}]}, {"text": "In particular, we present a corpus-based approach for finding base NPs by matching part-of-speech tag sequences.", "labels": [], "entities": []}, {"text": "The training phase of the algorithm is based on two successful techniques: first the base NP grammar is read from a \"treebank\" corpus ; then the grammar is improved by selecting rules with high \"benefit\" scores.", "labels": [], "entities": []}, {"text": "Using this simple algorithm with a naive heuristic for matching rules, we achieve surprising accuracy in an evaluation on the Penn Treebank Wall Street Journal.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 93, "end_pos": 101, "type": "METRIC", "confidence": 0.9993124008178711}, {"text": "Penn Treebank Wall Street Journal", "start_pos": 126, "end_pos": 159, "type": "DATASET", "confidence": 0.9703446626663208}]}], "introductionContent": [{"text": "Finding base noun phrases is a sensible first step for many natural language processing (NLP) tasks: Accurate identification of base noun phrases is arguably the most critical component of any partial parser; in addition, information retrieval systems rely on base noun phrases as the main source of multi-word indexing terms; furthermore, the psycholinguistic studies of indicate that text chunks like base noun phrases play an important role inhuman language processing.", "labels": [], "entities": [{"text": "Accurate identification of base noun phrases", "start_pos": 101, "end_pos": 145, "type": "TASK", "confidence": 0.8574536045392355}]}, {"text": "In this work we define base NPs to be simple, nonrecursive noun phrases --noun phrases that do not contain other noun phrase descendants.", "labels": [], "entities": []}, {"text": "The bracketed portions of, for example, show the base NPs in one sentence from the Penn Treebank Wall Street Journal (WSJ) corpus).", "labels": [], "entities": [{"text": "Penn Treebank Wall Street Journal (WSJ) corpus", "start_pos": 83, "end_pos": 129, "type": "DATASET", "confidence": 0.9761006500985887}]}, {"text": "Thus, the string the sunny confines of resort towns like Boca Raton and Hot Springs is too complex to be abase NP; instead, it contains four simpler noun phrases, each of which is considered abase NP: the sunny confines, resort towns, Boca Raton, and Hot Springs.", "labels": [], "entities": []}, {"text": "Previous empirical research has addressed the problem of base NP identification.", "labels": [], "entities": [{"text": "NP identification", "start_pos": 62, "end_pos": 79, "type": "TASK", "confidence": 0.7155534476041794}]}, {"text": "Several algorithms identify \"terminological phrases\" --certain  Voutilainen's NPTool (1993) uses a handcrafted lexicon and constraint grammar to find terminological noun phrases that include phrase-final prepositional phrases., on the other hand, uses a probabilistic model automatically trained on the Brown corpus to locate core noun phrases as well as to assign parts of speech.", "labels": [], "entities": [{"text": "Brown corpus", "start_pos": 303, "end_pos": 315, "type": "DATASET", "confidence": 0.8204894661903381}]}, {"text": "More recently,) apply transformation-based learning to the problem.", "labels": [], "entities": []}, {"text": "Unfortunately, it is difficult to directly compare approaches.", "labels": [], "entities": []}, {"text": "Each method uses a slightly different definition of base NP.", "labels": [], "entities": []}, {"text": "Each is evaluated on a different corpus.", "labels": [], "entities": []}, {"text": "Most approaches have been evaluated by hand on a small test set rather than by automatic comparison to a large test corpus annotated by an impartial third party.", "labels": [], "entities": []}, {"text": "A notable exception is the Ramshaw & Marcus work, which evaluates their transformation-based learning approach on abase NP corpus derived from the Penn Treebank WSJ, and achieves precision and recall levels of approximately 93%.", "labels": [], "entities": [{"text": "Penn Treebank WSJ", "start_pos": 147, "end_pos": 164, "type": "DATASET", "confidence": 0.9342796603838602}, {"text": "precision", "start_pos": 179, "end_pos": 188, "type": "METRIC", "confidence": 0.9994056224822998}, {"text": "recall", "start_pos": 193, "end_pos": 199, "type": "METRIC", "confidence": 0.9970318078994751}]}, {"text": "This paper presents anew algorithm for identifying base NPs in an arbitrary text.", "labels": [], "entities": []}, {"text": "Like some of the earlier work on base NP identification, ours is a trainable, corpus-based algorithm.", "labels": [], "entities": [{"text": "NP identification", "start_pos": 38, "end_pos": 55, "type": "TASK", "confidence": 0.6755839139223099}]}, {"text": "In contrast to other corpus-based approaches, however, we hypothesized that the relatively simple nature of base NPs would permit their accurate identification using correspondingly simple methods.", "labels": [], "entities": []}, {"text": "Assume, for example, that we use the annotated text of as our training corpus.", "labels": [], "entities": []}, {"text": "To identify base NPs in an unseen text, we could simply search for all occurrences of the base NPs seen during training --it, time, their biannual powwow, ..., Hot Springs --and mark them as base NPs in the new text.", "labels": [], "entities": []}, {"text": "However, this method would certainly suffer from data sparseness.", "labels": [], "entities": []}, {"text": "Instead, we use a similar approach, but back off from lexical items to parts of speech: we identify as abase NP any string having the same part-of-speech tag sequence as abase NP from the training corpus.", "labels": [], "entities": []}, {"text": "The training phase of the algorithm employs two previously successful techniques: like Charniak's (1996) statistical parser, our initial base NP grammar is read from a \"treebank\" corpus; then the grammar is improved by selecting rules with high \"benefit\" scores.", "labels": [], "entities": []}, {"text": "Our benefit measure is identical to that used in transformation-based learning to select an ordered set of useful transformations.", "labels": [], "entities": []}, {"text": "Using this simple algorithm with a naive heuristic for matching rules, we achieve surprising accuracy in an evaluation on two base NP corpora of varying complexity, both derived from the Penn Treebank WSJ.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 93, "end_pos": 101, "type": "METRIC", "confidence": 0.9985847473144531}, {"text": "Penn Treebank WSJ", "start_pos": 187, "end_pos": 204, "type": "DATASET", "confidence": 0.9885265827178955}]}, {"text": "The first base NP corpus is that used in the Ramshaw & Marcus work.", "labels": [], "entities": [{"text": "Ramshaw & Marcus work", "start_pos": 45, "end_pos": 66, "type": "DATASET", "confidence": 0.8680304288864136}]}, {"text": "The second espouses a slightly simpler definition of base NP that conforms to the base NPs used in our Empire sentence analyzer.", "labels": [], "entities": [{"text": "Empire sentence analyzer", "start_pos": 103, "end_pos": 127, "type": "TASK", "confidence": 0.7508792877197266}]}, {"text": "These simpler phrases appear to be a good starting point for partial parsers that purposely delay all complex attachment decisions to later phases of processing.", "labels": [], "entities": []}, {"text": "Overall results for the approach are promising.", "labels": [], "entities": []}, {"text": "For the Empire corpus, our base NP finder achieves 94% precision and recall; for the Ramshaw & Marcus corpus, it obtains 91% precision and recall, which is 2% less than the best published results.", "labels": [], "entities": [{"text": "Empire corpus", "start_pos": 8, "end_pos": 21, "type": "DATASET", "confidence": 0.9808860421180725}, {"text": "precision", "start_pos": 55, "end_pos": 64, "type": "METRIC", "confidence": 0.9993259906768799}, {"text": "recall", "start_pos": 69, "end_pos": 75, "type": "METRIC", "confidence": 0.9983751773834229}, {"text": "Ramshaw & Marcus corpus", "start_pos": 85, "end_pos": 108, "type": "DATASET", "confidence": 0.8924000114202499}, {"text": "precision", "start_pos": 125, "end_pos": 134, "type": "METRIC", "confidence": 0.9994138479232788}, {"text": "recall", "start_pos": 139, "end_pos": 145, "type": "METRIC", "confidence": 0.9992493987083435}]}, {"text": "Ramshaw & Marcus, however, provide the learning algorithm with word-level information in addition to the partof-speech information used in our base NP finder.", "labels": [], "entities": [{"text": "NP finder", "start_pos": 148, "end_pos": 157, "type": "TASK", "confidence": 0.6946630477905273}]}, {"text": "By controlling for this disparity in available knowledge sources, we find that our base NP algorithm performs comparably, achieving slightly worse precision (-1.1%) and slightly better recall (+0.2%) than the Ramshaw & Marcus approach.", "labels": [], "entities": [{"text": "precision", "start_pos": 147, "end_pos": 156, "type": "METRIC", "confidence": 0.9991328120231628}, {"text": "recall", "start_pos": 185, "end_pos": 191, "type": "METRIC", "confidence": 0.9993351101875305}]}, {"text": "Moreover, our approach offers many important advantages that make it appropriate for many NLP tasks: * Training is exceedingly simple.", "labels": [], "entities": []}, {"text": "The base NP bracketer is very fast, operating in time linear in the length of the text.", "labels": [], "entities": [{"text": "NP bracketer", "start_pos": 9, "end_pos": 21, "type": "TASK", "confidence": 0.6236705482006073}]}, {"text": "The accuracy of the treebank approach is good for applications that require or prefer fairly simple base NPs.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.9996269941329956}]}, {"text": "The learned grammar is easily modified for use with corpora that differ from the training texts.", "labels": [], "entities": []}, {"text": "Rules can be selectively added to or deleted from the grammar without worrying about ordering effects.", "labels": [], "entities": []}, {"text": "* Finally, our benefit-based training phase offers a simple, general approach for extracting grammars other than noun phrase grammars from annotated text.", "labels": [], "entities": [{"text": "extracting grammars other than noun phrase grammars from annotated text", "start_pos": 82, "end_pos": 153, "type": "TASK", "confidence": 0.755997383594513}]}, {"text": "Note also that the treebank approach to base NP identification obtains good results in spite of a very simple algorithm for \"parsing\" base NPs.", "labels": [], "entities": [{"text": "NP identification", "start_pos": 45, "end_pos": 62, "type": "TASK", "confidence": 0.6745801120996475}, {"text": "parsing\" base NPs", "start_pos": 125, "end_pos": 142, "type": "TASK", "confidence": 0.8320781588554382}]}, {"text": "This is extremely encouraging, and our evaluation suggests at least two areas for immediate improvement.", "labels": [], "entities": []}, {"text": "First, by replacing the naive match heuristic with a probabilistic base NP parser that incorporates lexical preferences, we would expect a nontrivial increase in recall and precision.", "labels": [], "entities": [{"text": "recall", "start_pos": 162, "end_pos": 168, "type": "METRIC", "confidence": 0.9992712140083313}, {"text": "precision", "start_pos": 173, "end_pos": 182, "type": "METRIC", "confidence": 0.991732120513916}]}, {"text": "Second, many of the remaining base NP errors tend to follow simple patterns; these might be corrected using localized, learnable repair rules.", "labels": [], "entities": []}, {"text": "The remainder of the paper describes the specifics of the approach and its evaluation.", "labels": [], "entities": []}, {"text": "The next section presents the training and application phases of the treebank approach to base NP identification in more detail.", "labels": [], "entities": [{"text": "NP identification", "start_pos": 95, "end_pos": 112, "type": "TASK", "confidence": 0.7964206635951996}]}, {"text": "Section 3 describes our general approach for pruning the base NP grammar as well as two instantiations of that approach.", "labels": [], "entities": []}, {"text": "The evaluation and a discussion of the results appear in Section 4, along with techniques for reducing training time and an initial investigation into the use of local repair heuristics.", "labels": [], "entities": []}], "datasetContent": [{"text": "To evaluate the treebank approach to base NP identification, we created two base NP corpora.", "labels": [], "entities": [{"text": "NP identification", "start_pos": 42, "end_pos": 59, "type": "TASK", "confidence": 0.6604779213666916}]}, {"text": "Each is derived from the Penn Treebank WSJ.", "labels": [], "entities": [{"text": "Penn Treebank WSJ", "start_pos": 25, "end_pos": 42, "type": "DATASET", "confidence": 0.9913183053334554}]}, {"text": "The first corpus attempts to duplicate the base NPs used the Ramshaw & Marcus (R&M) study.", "labels": [], "entities": [{"text": "Ramshaw & Marcus (R&M) study", "start_pos": 61, "end_pos": 89, "type": "DATASET", "confidence": 0.9063100020090739}]}, {"text": "The second corpus contains slightly less complicated base NPs --base NPs that are better suited for use with our sentence analyzer, Empire.", "labels": [], "entities": []}, {"text": "2 By evaluating on both corpora, we can measure the effect of noun phrase complexity on the treebank approach to base NP identification.", "labels": [], "entities": [{"text": "NP identification", "start_pos": 118, "end_pos": 135, "type": "TASK", "confidence": 0.7342800348997116}]}, {"text": "In particular, we hypothesize that the treebank approach will be most appropriate when the base NPs are sufficiently simple.", "labels": [], "entities": []}, {"text": "For all experiments, we derived the training, pruning, and testing sets from the 25 sections of Wall Street Journal distributed with the Penn Treebank II.", "labels": [], "entities": [{"text": "Wall Street Journal distributed with the Penn Treebank II", "start_pos": 96, "end_pos": 153, "type": "DATASET", "confidence": 0.9016260041130914}]}, {"text": "All experiments employ 5-fold cross validation.", "labels": [], "entities": []}, {"text": "More specifically, in each of five runs, a different fold is used for testing the final, pruned rule set; three of the remaining folds comprise the training corpus (to create the initial rule set); and the final partition is the pruning corpus (to prune bad rules from the initial rule set).", "labels": [], "entities": []}, {"text": "All results are averages across the five folds.", "labels": [], "entities": []}, {"text": "Performance is measured in terms of precision and recall.", "labels": [], "entities": [{"text": "precision", "start_pos": 36, "end_pos": 45, "type": "METRIC", "confidence": 0.9996860027313232}, {"text": "recall", "start_pos": 50, "end_pos": 56, "type": "METRIC", "confidence": 0.9991111159324646}]}, {"text": "Precision was described earlier --it is a standard measure of accuracy.", "labels": [], "entities": [{"text": "Precision", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.9777942299842834}, {"text": "accuracy", "start_pos": 62, "end_pos": 70, "type": "METRIC", "confidence": 0.9983314871788025}]}, {"text": "Recall, on the other hand, is an attempt to measure coverage: # of correct proposed NPs P = # of proposed NPs # of correct proposed NPs R = # of NPs in the annotated text summarizes the performance of the treebank approach to base NP identification on the R&M and Empire corpora using the initial and pruned rule sets.", "labels": [], "entities": [{"text": "coverage", "start_pos": 52, "end_pos": 60, "type": "METRIC", "confidence": 0.9764959216117859}, {"text": "NP identification", "start_pos": 231, "end_pos": 248, "type": "TASK", "confidence": 0.8443844020366669}, {"text": "R&M and Empire corpora", "start_pos": 256, "end_pos": 278, "type": "DATASET", "confidence": 0.8392777740955353}]}, {"text": "The first column of results shows the performance of the initial, unpruned base NP grammar.", "labels": [], "entities": []}, {"text": "The next two columns show the performance of the automatically pruned rule sets.", "labels": [], "entities": []}, {"text": "The final column indicates the performance of rule sets that had been pruned using the handcrafted pruning heuristics.", "labels": [], "entities": []}, {"text": "As expected, the initial rule set performs quite poorly.", "labels": [], "entities": []}, {"text": "Both automated approaches provide significant increases in both recall and precision.", "labels": [], "entities": [{"text": "recall", "start_pos": 64, "end_pos": 70, "type": "METRIC", "confidence": 0.9994246959686279}, {"text": "precision", "start_pos": 75, "end_pos": 84, "type": "METRIC", "confidence": 0.9982460737228394}]}, {"text": "In addition, they outperform the rule set pruned using handcrafted pruning heuristics.", "labels": [], "entities": []}, {"text": "2Very briefly, the Empire sentence analyzer relies on partial parsing to find simple constituents like base NPs and verb groups.", "labels": [], "entities": [{"text": "Empire sentence analyzer", "start_pos": 19, "end_pos": 43, "type": "TASK", "confidence": 0.7517107129096985}]}, {"text": "Machine learning algorithms then operate on the output of the partial parser to perform all attachment decisions.", "labels": [], "entities": []}, {"text": "The ultimate output of the parser is a semantic case frame representation of the functional structure of the input sentence.", "labels": [], "entities": []}, {"text": "Throughout the table, we seethe effects of base NP complexity --the base NPs of the R&M corpus are substantially more difficult for our approach to identify than the simpler NPs of the Empire corpus.", "labels": [], "entities": [{"text": "R&M corpus", "start_pos": 84, "end_pos": 94, "type": "DATASET", "confidence": 0.7216610386967659}, {"text": "Empire corpus", "start_pos": 185, "end_pos": 198, "type": "DATASET", "confidence": 0.8989666700363159}]}, {"text": "For the R&M corpus, we lag the best published results (93.1P/93.5R) by approximately 3%.", "labels": [], "entities": [{"text": "R&M corpus", "start_pos": 8, "end_pos": 18, "type": "DATASET", "confidence": 0.740822359919548}]}, {"text": "This straightforward comparison, however, is not entirely appropriate.", "labels": [], "entities": []}, {"text": "Ramshaw & Marcus allow their learning algorithm to access word-level information in addition to part-of-speech tags.", "labels": [], "entities": []}, {"text": "The treebank approach, on the other hand, makes use only of part-ofspeech tags.", "labels": [], "entities": []}, {"text": "compares) results with and without lexical knowledge.", "labels": [], "entities": []}, {"text": "The first column reports their performance when using lexical templates; the second when lexical templates are not used; the third again shows the treebank approach using incremental pruning.", "labels": [], "entities": []}, {"text": "The treebank approach and the R&M approach without lecial templates are shown to perform comparably (-1.1P/+0.2R).", "labels": [], "entities": []}, {"text": "Lexicalization of our base NP finder will be addressed in Section 4.1.", "labels": [], "entities": [{"text": "NP finder", "start_pos": 27, "end_pos": 36, "type": "TASK", "confidence": 0.6541670262813568}]}], "tableCaptions": []}