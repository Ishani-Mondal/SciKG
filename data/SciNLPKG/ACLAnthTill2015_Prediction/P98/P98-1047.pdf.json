{"title": [{"text": "Learning a syntagmatic and paradigmatic structure from language data with a bi-multigram model", "labels": [], "entities": []}], "abstractContent": [{"text": "In this paper, we present a stochastic language mod-eling tool which aims at retrieving variable-length phrases (multigrams), assuming bigram dependencies between them.", "labels": [], "entities": []}, {"text": "The phrase retrieval can be in-termixed with a phrase clustering procedure, so that the language data are iteratively structured at both a paradigmatic and a syntagmatic level in a fully integrated way.", "labels": [], "entities": [{"text": "phrase retrieval", "start_pos": 4, "end_pos": 20, "type": "TASK", "confidence": 0.7727587819099426}, {"text": "phrase clustering", "start_pos": 47, "end_pos": 64, "type": "TASK", "confidence": 0.703797921538353}]}, {"text": "Perplexity results on ATR travel arrangement data with a bi-multigram model (assum-ing bigram correlations between the phrases) come very close to the trigram scores with a reduced number of entries in the language model.", "labels": [], "entities": []}, {"text": "Also the ability of the class version of the model to merge semantically related phrases into a common class is illustrated.", "labels": [], "entities": []}, {"text": "1 Introduction There is currently an increasing interest in statistical language models, which in one way or another aim at exploiting word-dependencies spanning over a variable number of words.", "labels": [], "entities": []}, {"text": "Though all these models commonly relax the assumption of fixed-length dependency of the conventional ngram model, they cover a wide variety of modeling assumptions and of parameter estimation frameworks.", "labels": [], "entities": []}, {"text": "In this paper, we focus on a phrase-based approach, as opposed to a gram-based approach: sentences are structured into phrases and probabilities are assigned to phrases instead of words.", "labels": [], "entities": []}, {"text": "Regardless of whether they are gram or phrase-based, models can be either determinis-tic or stochastic.", "labels": [], "entities": []}, {"text": "In the phrase-based framework, non determinism is introduced via an ambiguity on the parse of the sentence into phrases.", "labels": [], "entities": []}], "introductionContent": [{"text": "There is currently an increasing interest in statistical language models, which in one way or another aim at exploiting word-dependencies spanning over a variable number of words.", "labels": [], "entities": []}, {"text": "Though all these models commonly relax the assumption of fixed-length dependency of the conventional ngram model, they cover a wide variety of modeling assumptions and of parameter estimation frameworks.", "labels": [], "entities": []}, {"text": "In this paper, we focus on a phrase-based approach, as opposed to a gram-based approach: sentences are structured into phrases and probabilities are assigned to phrases instead of words.", "labels": [], "entities": []}, {"text": "Regardless of whether they are gram or phrase-based, models can be either deterministic or stochastic.", "labels": [], "entities": []}, {"text": "In the phrase-based framework, non determinism is introduced via an ambiguity on the parse of the sentence into phrases.", "labels": [], "entities": []}, {"text": "In practice, it means that even if phrase abe is registered as a phrase, the possibility of parsing the string as, for instance, lab] [c] still remains.", "labels": [], "entities": []}, {"text": "By contrast, in a deterministic approach, all co-occurences of a, band c would be systematically interpreted as an occurence of phrase [abel.", "labels": [], "entities": []}, {"text": "Various criteria have been proposed to derive phrases in a purely statistical way 1; data likeli-I i.e. without using grammar rules like in Stochastic Context Free hood, leaving-one-out likelihood (, mutual information, and entropy (.", "labels": [], "entities": []}, {"text": "The use of the likelihood criterion in a stochastic framework allows EM principled optimization procedures, but it is prone to overlearning.", "labels": [], "entities": []}, {"text": "The other criteria tend to reduce the risk of overlearning, but their optimization relies on heuristic procedures (e.g. word grouping via a greedy algorithm) for which convergence and optimality are not theoretically guaranteed.", "labels": [], "entities": [{"text": "word grouping", "start_pos": 120, "end_pos": 133, "type": "TASK", "confidence": 0.7116879224777222}]}, {"text": "The work reported in this paper is based on the multigram model, which is a stochastic phrase-based model, the parameters of which are estimated according to a likelihood criterion using an EM procedure.", "labels": [], "entities": []}, {"text": "The multigram approach was introduced in , and in it was used to derive variable-length phrases under the assumption of independence of the phrases.", "labels": [], "entities": []}, {"text": "Various ways of theoretically releasing this assumption were given in.", "labels": [], "entities": []}, {"text": "More recently, experiments with 2-word multigrams embedded in a deterministic variable ngram scheme were reported in.", "labels": [], "entities": []}, {"text": "In section 2 of this paper, we further formulate a model with bigram (more generally ~-gram) dependencies between the phrases, by including a paradigmatic aspect which enables the clustering of variable-length phrases.", "labels": [], "entities": []}, {"text": "It results in a stochastic class-phrase model, which can be interpolated with the stochastic phrase model, in a similar way to deterministic approaches.", "labels": [], "entities": []}, {"text": "In section 3 and 4, the phrase and class-phrase models are evaluated in terms of perplexity values and model size.", "labels": [], "entities": []}, {"text": "2 Theoretical formulation of the multigrams 2.1 Variable-length phrase distribution In the multigram framework, the assumption is made that sentences result from the concatenation of variable-length phrases, called multigrams.", "labels": [], "entities": []}, {"text": "The likelihood of a sentence is computed by summing the likelihood values of all possible segmentations of the sentence into phrases.", "labels": [], "entities": [{"text": "likelihood", "start_pos": 4, "end_pos": 14, "type": "METRIC", "confidence": 0.9454630017280579}]}, {"text": "The likelihood computa-tion for any particular segmentation into phrases depends on the model assumed to describe the dependencies between the phrases.", "labels": [], "entities": []}, {"text": "We call bi-multigram model the model where bigram dependencies are assumed between the phrases.", "labels": [], "entities": []}, {"text": "For instance, by limiting to 3 words the maximal length of a phrase, the bi-multigram likelihood of the string \"a b c d\" is: To )resent the general formalism of the model in this section, we assume ~-gram correlations between the phrases, and we note n the maximal length of a phrase (in the above example, ~=2 and n=3).", "labels": [], "entities": []}, {"text": "Let W denote a string of words, and {S} the set of possible segmentations on W.", "labels": [], "entities": []}, {"text": "The likelihood of W is: and the likelihood of a segmentation S of W is: c (w,s) = I-I P(S(,) I s(,_~-+~)...s(,_~))", "labels": [], "entities": []}], "datasetContent": [{"text": "4.1 Protocol and database Evaluation protocol In section 4.2, we compare class versions and interpolated versions of the bigram, trigram and bi-multigram models, in terms of perplexity values and of model size.", "labels": [], "entities": []}, {"text": "For bigrams (resp. trigrams) of classes, the size of the model is the number of distinct 2-uplets (resp.", "labels": [], "entities": []}, {"text": "3-uplets) of word-classes observed, plus the size of the vocabulary.", "labels": [], "entities": []}, {"text": "For the class version of the bi-multigrams, the size of the model is the number of distinct 2-uplets of phrase-classes, plus the number of distinct phrases maintained.", "labels": [], "entities": []}, {"text": "In section 4.3, we show samples from classes of up to 5-word phrases, to illustrate the potential benefit of clustering relatively long and variable-length phrases for issues related to language understanding.", "labels": [], "entities": [{"text": "language understanding", "start_pos": 186, "end_pos": 208, "type": "TASK", "confidence": 0.6932776868343353}]}, {"text": "Training protocol All non-class models are the same as in section 3.", "labels": [], "entities": []}, {"text": "The class-phrase models are trained with 5 iterations of the algorithm described in section 2.2: each iteration consists in clustering the phrases into 300 phrase-classes (step 1), and in reestimating the phrase distribution (step 2) with Eq.", "labels": [], "entities": []}, {"text": "Database The training and test data used to train and evaluate the models are the same as the ones described in.", "labels": [], "entities": []}, {"text": "We use an additional set of 7350 sentences and 55000 word tokens to estimate the interpolation weights of the interpolated models.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: ATR Travel Arrangement Data", "labels": [], "entities": [{"text": "ATR Travel Arrangement", "start_pos": 10, "end_pos": 32, "type": "TASK", "confidence": 0.6791561643282572}]}, {"text": " Table 2: Ambiguity on a parse.", "labels": [], "entities": [{"text": "Ambiguity", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.8794620633125305}]}, {"text": " Table 3: Influence of the estimation procedure:  forward-backward (F.-B.) or Viterbi.", "labels": [], "entities": [{"text": "estimation", "start_pos": 27, "end_pos": 37, "type": "TASK", "confidence": 0.9245606660842896}]}, {"text": " Table 4: Comparison with n-grams: Test perplexity  values and model size.", "labels": [], "entities": []}, {"text": " Table 5: Comparison of class-phrase bi-multigrams  and of class-word bigrams and trigrams: Test per- plexity values and model size.", "labels": [], "entities": []}]}