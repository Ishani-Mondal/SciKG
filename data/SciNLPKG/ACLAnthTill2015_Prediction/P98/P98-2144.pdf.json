{"title": [{"text": "HPSG-Style Underspecified Japanese Grammar with Wide Coverage", "labels": [], "entities": [{"text": "HPSG-Style Underspecified Japanese Grammar", "start_pos": 0, "end_pos": 42, "type": "DATASET", "confidence": 0.6628510057926178}]}], "abstractContent": [{"text": "This paper describes a wide-coverage Japanese grammar based on HPSG.", "labels": [], "entities": [{"text": "HPSG", "start_pos": 63, "end_pos": 67, "type": "DATASET", "confidence": 0.978570282459259}]}, {"text": "The aim of this work is to seethe coverage and accuracy attainable using an underspecified grammar.", "labels": [], "entities": [{"text": "coverage", "start_pos": 34, "end_pos": 42, "type": "METRIC", "confidence": 0.9734635353088379}, {"text": "accuracy", "start_pos": 47, "end_pos": 55, "type": "METRIC", "confidence": 0.9957839846611023}]}, {"text": "Under-specification, allowed in a typed feature structure formalism, enables us to write down a wide-coverage grammar concisely.", "labels": [], "entities": []}, {"text": "The grammar we have implemented consists of only 6 ID schemata, 68 lexical entries (assigned to functional words), and 63 lexical entry templates (assigned to parts of speech (BOSs)).", "labels": [], "entities": []}, {"text": "word-specific constraints such as subcate-gorization of verbs are not fixed in the grammar.", "labels": [], "entities": []}, {"text": "this granllnar call generate parse trees for 87% of the 10000 sentences in the Japanese EDR corpus.", "labels": [], "entities": [{"text": "Japanese EDR corpus", "start_pos": 79, "end_pos": 98, "type": "DATASET", "confidence": 0.803317924340566}]}, {"text": "The dependency accuracy is 78% when a parser uses the heuristic that every bunsetsu 1 is attached to the nearest possible one.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 15, "end_pos": 23, "type": "METRIC", "confidence": 0.9697469472885132}]}], "introductionContent": [{"text": "Our purpose is to design a practical Japanese grammar based on HPSG (Head-driven Phrase Structure Grammar), with wide coverage and reasonable accuracy for syntactic structures of real-world texts.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 142, "end_pos": 150, "type": "METRIC", "confidence": 0.9985682964324951}]}, {"text": "In this paper, \"coverage\" refers to the percentage of input sentences for which the grammar returns at least one parse tree, and \"accuracy\" refers to the percentage of bunsetsus which are attached correctly.", "labels": [], "entities": [{"text": "coverage", "start_pos": 16, "end_pos": 24, "type": "METRIC", "confidence": 0.9782376289367676}, {"text": "accuracy", "start_pos": 130, "end_pos": 138, "type": "METRIC", "confidence": 0.9991801381111145}]}, {"text": "To realize wide coverage and reasonable accuracy, the following steps had been taken: A) At first we prepared a linguistically valid but coarse grammar with wide coverage.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 40, "end_pos": 48, "type": "METRIC", "confidence": 0.9989023208618164}]}, {"text": "B) We then refined the grammar in regard to accuracy, using practical heuristics which are not linguistically motivated.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 44, "end_pos": 52, "type": "METRIC", "confidence": 0.9992967844009399}]}, {"text": "As for A), the first grammar we have constructed actually consists of only 68 lexical en-1A bunsetsu is a common unit when syntactic structures in Japanese are discussed.", "labels": [], "entities": []}, {"text": "tries (LEs) for some functional words 2, 63 lexical entry templates (LETs) for POSs 3, and 6 ID schemata.", "labels": [], "entities": []}, {"text": "Nevertheless, the coverage of our grammar was 92% for the Japanese corpus in the EDR Electronic Dictionary, mainly due to underspecification, which is allowed in HPSG and does not always require detailed grammar descriptions.", "labels": [], "entities": [{"text": "coverage", "start_pos": 18, "end_pos": 26, "type": "METRIC", "confidence": 0.9700196981430054}, {"text": "Japanese corpus", "start_pos": 58, "end_pos": 73, "type": "DATASET", "confidence": 0.7740958034992218}, {"text": "EDR Electronic Dictionary", "start_pos": 81, "end_pos": 106, "type": "DATASET", "confidence": 0.8733202219009399}, {"text": "HPSG", "start_pos": 162, "end_pos": 166, "type": "DATASET", "confidence": 0.9558998346328735}]}, {"text": "As for B), in order to improve accuracy, the grammar should restrict ambiguity as much as possible.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 31, "end_pos": 39, "type": "METRIC", "confidence": 0.9989809393882751}]}, {"text": "For this purpose, the grammar needs more constraints in itself.", "labels": [], "entities": []}, {"text": "To reduce ambiguity, we added additional feature structures which may not be linguistically valid but be empirically correct, as constraints to i) the original LFs and LETs, and ii) the ID schemata.", "labels": [], "entities": []}, {"text": "The rest of this paper describes the architecture of our Japanese grammar (Section 2).", "labels": [], "entities": []}, {"text": "refinement of our grammar (Section 3), experimental results (Section 4). and discussion regarding errors (Section 5).", "labels": [], "entities": []}, {"text": "2 Architecture of Japanese Grammar In this section we describe the architecture of the HPSG-style Japanese grammar we have developed.", "labels": [], "entities": [{"text": "HPSG-style Japanese grammar", "start_pos": 87, "end_pos": 114, "type": "DATASET", "confidence": 0.9035685857137045}]}, {"text": "In the HPSG framework, a grammar consists of (i) immediate dominance schemata (ID schemata), (ii) principles, and (iii) lexical entries (LEs).", "labels": [], "entities": []}, {"text": "All of them are represented by typed feature structures (TFSs), the fundamental data structures of HPSG.", "labels": [], "entities": [{"text": "HPSG", "start_pos": 99, "end_pos": 103, "type": "DATASET", "confidence": 0.9572254419326782}]}, {"text": "ID schemata, corresponding to rewriting rules in CFG, are significant for constructing syntactic structures.", "labels": [], "entities": [{"text": "CFG", "start_pos": 49, "end_pos": 52, "type": "DATASET", "confidence": 0.8795778751373291}]}, {"text": "The details of our ID schemata are discussed in Section 2.1.", "labels": [], "entities": []}, {"text": "Principles are constraints between mother and daughter feature structures.", "labels": [], "entities": []}, {"text": "4 LEs, which compose the lexicon, are detailed constraints on each word.", "labels": [], "entities": [{"text": "LEs", "start_pos": 2, "end_pos": 5, "type": "METRIC", "confidence": 0.9380561113357544}]}, {"text": "In our grammar, we do not always assign LEs to each word.", "labels": [], "entities": [{"text": "LEs", "start_pos": 40, "end_pos": 43, "type": "METRIC", "confidence": 0.8385420441627502}]}, {"text": "Instead, we assign lexical entry 2A functional word is assigned one or more LEs.", "labels": [], "entities": [{"text": "LEs", "start_pos": 76, "end_pos": 79, "type": "METRIC", "confidence": 0.9625535607337952}]}, {"text": "SA POS is also assigned one or more LETs.", "labels": [], "entities": [{"text": "SA POS", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.6649330854415894}, {"text": "LETs", "start_pos": 36, "end_pos": 40, "type": "METRIC", "confidence": 0.9427075982093811}]}, {"text": "4We omit further explanation about principles here due to limited space.", "labels": [], "entities": []}], "datasetContent": [{"text": "We implemented our parser and grammar in LiLFeS () s, a featurestructure description language developed by our group.", "labels": [], "entities": []}, {"text": "We tested randomly selected 10000 sentences fi'om the Japanese EDR corpus.", "labels": [], "entities": [{"text": "Japanese EDR corpus", "start_pos": 54, "end_pos": 73, "type": "DATASET", "confidence": 0.8934283455212911}]}, {"text": "Tile EDR Corpus is a Japanese version of treebank with morphological, structural, and semantic information.", "labels": [], "entities": [{"text": "Tile EDR Corpus", "start_pos": 0, "end_pos": 15, "type": "DATASET", "confidence": 0.7447043061256409}]}, {"text": "In our experiments, we used only the structural information, that is, parse trees.", "labels": [], "entities": []}, {"text": "Both the parse trees in our parser and the parse trees in the EDR Corpus are first converted into bunsetsu dependencies, and they are compared when calculating accuracy.", "labels": [], "entities": [{"text": "EDR Corpus", "start_pos": 62, "end_pos": 72, "type": "DATASET", "confidence": 0.9707549214363098}, {"text": "accuracy", "start_pos": 160, "end_pos": 168, "type": "METRIC", "confidence": 0.9930526614189148}]}, {"text": "Note that the internal structures of bunsetsus, e.~.", "labels": [], "entities": []}, {"text": "structures of compound nouns, are not considered in our evaluations.", "labels": [], "entities": []}, {"text": "~re evaluated the following grammars: (a) the original underspecified grammar, (b) (a) + constraint for wa-marked PPs, (c) (a) + constraint for relative clauses with a comma, (d) (a) + constraint for nominal time suffixes with a comma, and (e) (a) + all the three constraints.", "labels": [], "entities": []}, {"text": "We evaluated those grammars by the following three measurements: Coverage The percentage of the sentences that generate at least one parse tree.", "labels": [], "entities": []}, {"text": "Partial Accuracy The percentage of the correct dependencies between bunsetsus (excepting the last obvious dependency) for the parsable sentences.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 8, "end_pos": 16, "type": "METRIC", "confidence": 0.8253618478775024}]}, {"text": "Total Accuracy The percentage of the correct dependencies between bunsetsus (excepting the last dependency) overall sentences.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 6, "end_pos": 14, "type": "METRIC", "confidence": 0.5068620443344116}]}, {"text": "When calculating total accuracy, the dependencies for unparsable sentences are predicted so that every bunsetsu is attached to the nearest bunsetsu.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 23, "end_pos": 31, "type": "METRIC", "confidence": 0.9909093976020813}]}, {"text": "In other words, total accuracy can be regarded as a weighted average of partial accuracy and baseline accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 22, "end_pos": 30, "type": "METRIC", "confidence": 0.9422809481620789}, {"text": "accuracy", "start_pos": 80, "end_pos": 88, "type": "METRIC", "confidence": 0.9091101288795471}, {"text": "accuracy", "start_pos": 102, "end_pos": 110, "type": "METRIC", "confidence": 0.6754412651062012}]}, {"text": "lists the results of our experiments.", "labels": [], "entities": []}, {"text": "Comparison of the results between (a) and shows that all the three constraints improve partial accuracy and total accuracy with little coverage loss.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 95, "end_pos": 103, "type": "METRIC", "confidence": 0.9528923034667969}, {"text": "accuracy", "start_pos": 114, "end_pos": 122, "type": "METRIC", "confidence": 0.9545062780380249}, {"text": "coverage", "start_pos": 135, "end_pos": 143, "type": "METRIC", "confidence": 0.9461871981620789}]}, {"text": "And grammar (e) using the combination of the three constraints still works with no side effect.", "labels": [], "entities": []}, {"text": "We also measured average parsing time per sentence for the original grammar (a) and the fully augmented grammar (e).", "labels": [], "entities": []}, {"text": "The parser we adopted is a naive CKY-style parser.", "labels": [], "entities": []}, {"text": "gives the average parsing time per sentence for those 2 grammars.", "labels": [], "entities": []}, {"text": "Pseudo-principles and further constraints on LEs/LETs also make parsing more time-efficient.", "labels": [], "entities": [{"text": "parsing", "start_pos": 64, "end_pos": 71, "type": "TASK", "confidence": 0.9623491764068604}]}, {"text": "Even though they are sometimes considered to be slow in practical application because of their heavy feature structures, actually we found them to improve speed.", "labels": [], "entities": [{"text": "speed", "start_pos": 155, "end_pos": 160, "type": "METRIC", "confidence": 0.9650208950042725}]}, {"text": "In (, an efficient HPSG parser is proposed, and our preliminary experiments show that the parsing time of the effident parser is about three times shorter than that of the naive one.", "labels": [], "entities": [{"text": "HPSG parser", "start_pos": 19, "end_pos": 30, "type": "TASK", "confidence": 0.6232775449752808}]}, {"text": "Thus, the average parsing time per sentence will be about 300 msec., and we believe our grammar will achive a practical speed.", "labels": [], "entities": []}, {"text": "Other techniques to speed-up the parser are proposed in ().", "labels": [], "entities": []}], "tableCaptions": []}