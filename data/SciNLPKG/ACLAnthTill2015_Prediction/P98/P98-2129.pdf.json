{"title": [{"text": "Evaluating Response Strategies in a Web-Based Spoken Dialogue Agent", "labels": [], "entities": [{"text": "Evaluating Response Strategies", "start_pos": 0, "end_pos": 30, "type": "TASK", "confidence": 0.8672447204589844}]}], "abstractContent": [{"text": "While the notion of a cooperative response has been the focus of considerable research in natural language dialogue systems, there has been little empirical work demonstrating how such responses lead to more efficient, natural, or successful dialogues.", "labels": [], "entities": []}, {"text": "This paper presents an experimental evaluation of two alternative response strategies in TOOT, a spoken dialogue agent that allows users to access train schedules stored on the web via a telephone conversation.", "labels": [], "entities": [{"text": "TOOT", "start_pos": 89, "end_pos": 93, "type": "DATASET", "confidence": 0.765035092830658}]}, {"text": "We compare the performance of two versions of TOOT (literal and cooperative), by having users carryout a set of tasks with each version.", "labels": [], "entities": [{"text": "TOOT", "start_pos": 46, "end_pos": 50, "type": "DATASET", "confidence": 0.8196980357170105}]}, {"text": "By using hypothesis testing methods, we show that a combination of response strategy, application task, and task/strategy interactions account for various types of performance differences.", "labels": [], "entities": []}, {"text": "By using the PARADISE evaluation framework to estimate an overall performance function, we identify inter-dependencies that exist between speech recognition and response strategy.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 138, "end_pos": 156, "type": "TASK", "confidence": 0.7409235537052155}]}, {"text": "Our results elaborate the conditions under which TOOT' s cooperative rather than literal strategy contributes to greater performance.", "labels": [], "entities": []}], "introductionContent": [{"text": "The notion of a cooperative response has been the focus of considerable research in natural language and spoken dialogue systems).", "labels": [], "entities": []}, {"text": "However, despite the existence of many algorithms for generating cooperative responses, there has been little empirical work addressing the evaluation of such algorithms in the context of real-time natural language dialogue systems with human users.", "labels": [], "entities": []}, {"text": "Thus it is unclear under what conditions cooperative responses result in more efficient or efficacious dialogues.", "labels": [], "entities": []}, {"text": "This paper presents an empirical evaluation of two alternative algorithms for responding to database queries in TOOT, a spoken dialogue agent for accessing online train schedules via a telephone conversation.", "labels": [], "entities": [{"text": "TOOT", "start_pos": 112, "end_pos": 116, "type": "DATASET", "confidence": 0.790357232093811}]}, {"text": "We conduct an experiment in which 12 users carryout 4 tasks of varying difficulty with one of two versions of TOOT (literal and cooperative TOOT), resulting in a corpus of 48 dialogues.", "labels": [], "entities": [{"text": "TOOT", "start_pos": 110, "end_pos": 114, "type": "DATASET", "confidence": 0.8284257054328918}]}, {"text": "The values fora wide range of evaluation measures are then extracted from this corpus.", "labels": [], "entities": []}, {"text": "We analyze our data using both traditional hypothesis testing methods and the PARADISE () methodology for estimating a performance function.", "labels": [], "entities": [{"text": "PARADISE", "start_pos": 78, "end_pos": 86, "type": "METRIC", "confidence": 0.9534019231796265}]}, {"text": "Hypothesis testing shows that while differences among some evaluation measures depend on the response strategy (literal or cooperative), other differences area function of application task and task/strategy interactions.", "labels": [], "entities": []}, {"text": "A PAR-ADISE assessment of the contribution of each evaluation measure to overall performance shows that strategy-dependent dialogue phenomena as well as phenomena associated with speech recognition significantly predict performance.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 179, "end_pos": 197, "type": "TASK", "confidence": 0.7051085829734802}]}, {"text": "Our results identify the conditions under which TOOT' s cooperative response strategy leads to greater agent performance.", "labels": [], "entities": []}], "datasetContent": [{"text": "The experimental instructions were given on a web page, which consisted of a description of TOOT's functionality, hints for talking to TOOT, and links to 4 task pages.", "labels": [], "entities": []}, {"text": "Each task page contained a task scenario, the hints, instructions for calling TOOT, anal a web survey designed to ascertain the depart and travel times obtained by the user and to measure user perceptions of task success and agent usability.", "labels": [], "entities": [{"text": "TOOT", "start_pos": 78, "end_pos": 82, "type": "DATASET", "confidence": 0.7222721576690674}]}, {"text": "Users were 12 researchers not involved with the design or implementation of TOOT; 6 users were randomly assigned to LT and 6 to CT.", "labels": [], "entities": [{"text": "TOOT", "start_pos": 76, "end_pos": 80, "type": "DATASET", "confidence": 0.7597599029541016}, {"text": "LT", "start_pos": 116, "end_pos": 118, "type": "DATASET", "confidence": 0.6878739595413208}]}, {"text": "Users read the instructions in their office and then called TOOT from their phone.", "labels": [], "entities": [{"text": "TOOT", "start_pos": 60, "end_pos": 64, "type": "DATASET", "confidence": 0.8563364148139954}]}, {"text": "Our experiment yielded a corpus of 48 dialogues (1344 total tums; 214 minutes of speech).", "labels": [], "entities": []}, {"text": "Users were provided with task scenarios for two reasons.", "labels": [], "entities": []}, {"text": "First, our hypothesis was that performance depended not only on response strategy, but also on task difficulty.", "labels": [], "entities": []}, {"text": "To include the task as a factor in our experiment, we needed to ensure that users executed the same tasks and that they varied in difficulty.", "labels": [], "entities": []}, {"text": "shows the task scenarios used in our experiment.", "labels": [], "entities": []}, {"text": "Our hypotheses about agent performance are summarized in.", "labels": [], "entities": []}, {"text": "We predicted that optimal performance would occur whenever the correct task solution was included in TOOT' s initial re- sponse to a web query (i.e., when the task was easy).", "labels": [], "entities": [{"text": "TOOT' s initial re- sponse to a web query", "start_pos": 101, "end_pos": 142, "type": "TASK", "confidence": 0.5486751280047677}]}, {"text": "Task 1 (dialogue fragment (4) above) produced a query that resulted in 2 matching trains, one of which was the train requested in the scenario.", "labels": [], "entities": []}, {"text": "Since the response strategies of LT and CT were identical under this condition, we predicted identical LT and CT performance, as shown in Tasks 2 (dialogue fragments and) and 3 led to queries that yielded no matching trains.", "labels": [], "entities": []}, {"text": "In Task 2 users were told to find the closest train.", "labels": [], "entities": []}, {"text": "Since only CT included this extra information in its response, we predicted that it would perform better than LT.", "labels": [], "entities": []}, {"text": "In Task 3 users were told to find the shortest train within anew departure interval.", "labels": [], "entities": []}, {"text": "Since neither LT nor CT provided this information initially, we hypothesized comparable LT and CT performance.", "labels": [], "entities": []}, {"text": "However, since CT allowed users to change just their departure time while LT required users to construct a whole new query, we also thought it possible that CT might perform slightly better than LT.", "labels": [], "entities": []}, {"text": "Task 4 and dialogue fragment  A second reason for having task scenarios was that it allowed us to objectively determine whether users achieved their tasks.", "labels": [], "entities": []}, {"text": "Following PAR-ADISE (), we defined a \"key\" for each scenario using an attribute value matrix (AVM) task representation, as in.", "labels": [], "entities": []}, {"text": "The key indicates the attribute values that must be exchanged between the agent and user by the end of the dialogue.", "labels": [], "entities": []}, {"text": "If the task is successfully completed in a scenario execution (as in), the AVM representing the dialogue is identical to the key.", "labels": [], "entities": [{"text": "AVM", "start_pos": 75, "end_pos": 78, "type": "METRIC", "confidence": 0.9502045512199402}]}], "tableCaptions": []}