{"title": [{"text": "Part of Speech Tagging Using a Network of Linear Separators", "labels": [], "entities": [{"text": "Speech Tagging", "start_pos": 8, "end_pos": 22, "type": "TASK", "confidence": 0.7914866507053375}]}], "abstractContent": [{"text": "We present an architecture and an on-line learning algorithm and apply it to the problem of part-of-speech tagging.", "labels": [], "entities": [{"text": "part-of-speech tagging", "start_pos": 92, "end_pos": 114, "type": "TASK", "confidence": 0.7791593074798584}]}, {"text": "The architecture presented, SNOW, is a network of linear separators in the feature space, utilizing the Winnow update algorithm.", "labels": [], "entities": []}, {"text": "Multiplicative weight-update algorithms such as Winnow have been shown to have exceptionally good behavior when applied to very high dimensional problems, and especially when the target concepts depend on only a small subset of the features in the feature space.", "labels": [], "entities": []}, {"text": "In this paper we describe an architecture that utilizes this mistake-driven algorithm for multi-class prediction-selecting the part of speech of a word.", "labels": [], "entities": [{"text": "multi-class prediction-selecting the part of speech of a word", "start_pos": 90, "end_pos": 151, "type": "TASK", "confidence": 0.8682901263237}]}, {"text": "The experimental analysis presented here provides more evidence to that these algorithms are suitable for natural language problems.", "labels": [], "entities": []}, {"text": "The algorithm used is an on-line algorithm: every example is used by the algorithm only once, and is then discarded.", "labels": [], "entities": []}, {"text": "This has significance in terms of efficiency , as well as quick adaptation to new contexts.", "labels": [], "entities": []}, {"text": "We present an extensive experimental study of our algorithm under various conditions; in particular, it is shown that the algorithm performs comparably to the best known algorithms for POS.", "labels": [], "entities": []}], "introductionContent": [{"text": "Learning problems in the natural language domain often map the text to a space whose dimensions are the measured features of the text, e.g., its words.", "labels": [], "entities": []}, {"text": "Two characteristic properties of this domain are that its dimensionality is very high and that both the learned concepts and the instances reside very sparsely in the feature space.", "labels": [], "entities": []}, {"text": "In this paper we present a learning algorithm and an architecture with properties suitable for this domain.", "labels": [], "entities": []}, {"text": "The SNOW algorithm presented here builds on recently introduced theories of multiplicative weight-updating learning algorithms for linear functions.", "labels": [], "entities": []}, {"text": "Multiplicative weight-updating algorithms such as Winnow and Weighted Majority ( have been studied extensively in the COLT literature.", "labels": [], "entities": [{"text": "COLT literature", "start_pos": 118, "end_pos": 133, "type": "DATASET", "confidence": 0.7504048943519592}]}, {"text": "Theoretical analysis has shown that they have exceptionally good behavior in the presence of irrelevant attributes, noise, and even a target function changing in time.", "labels": [], "entities": []}, {"text": "Only recently have people started to test these claimed abilities in applications.", "labels": [], "entities": []}, {"text": "We address these claims empirically by applying SNOW to one of the fundamental disambiguation problems in natural language: part-of speech tagging.", "labels": [], "entities": [{"text": "part-of speech tagging", "start_pos": 124, "end_pos": 146, "type": "TASK", "confidence": 0.6709549327691396}]}, {"text": "Part of Speech tagging (POS) is the problem of assigning each word in a sentence the part of speech that it assumes in that sentence.", "labels": [], "entities": [{"text": "Part of Speech tagging (POS)", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.7769566689218793}]}, {"text": "The importance of the problem stems from the fact that POS is one of the first stages in the process performed by various natural language related processes such as speech, information extraction and others.", "labels": [], "entities": [{"text": "information extraction", "start_pos": 173, "end_pos": 195, "type": "TASK", "confidence": 0.7557248175144196}]}, {"text": "The architecture presented here, SNOW, is a Sparse Network Of Linear separators which utilizes the Winnow learning algorithm.", "labels": [], "entities": []}, {"text": "A target node in the network corresponds to a candidate in the disambiguation task; all subnetworks learn autonomously from the same data in an online fashion, and at run time, they compete for assigning the correct meaning.", "labels": [], "entities": []}, {"text": "A similar architecture which includes an additional layer is described in.", "labels": [], "entities": []}, {"text": "The POS problem suggests a special challenge to this approach.", "labels": [], "entities": []}, {"text": "First, the problem is a multiclass prediction problem.", "labels": [], "entities": [{"text": "multiclass prediction", "start_pos": 24, "end_pos": 45, "type": "TASK", "confidence": 0.6973787397146225}]}, {"text": "Second, determining the POS of a word in a sentence may depend on the POS of its neighbors in the sentence, but these are not known with any certainty.", "labels": [], "entities": [{"text": "POS", "start_pos": 24, "end_pos": 27, "type": "METRIC", "confidence": 0.9516013860702515}, {"text": "POS", "start_pos": 70, "end_pos": 73, "type": "METRIC", "confidence": 0.989096999168396}]}, {"text": "In the SNOW architecture, we address these problems by learning at the same time and from the same input, a network of many classifiers.", "labels": [], "entities": []}, {"text": "Each sub-network is devoted to a single POS tag and learns to separate its POS tag from all others.", "labels": [], "entities": []}, {"text": "At run time, all classifiers are applied simultaneously and compete for deciding the POS of this word.", "labels": [], "entities": [{"text": "POS", "start_pos": 85, "end_pos": 88, "type": "METRIC", "confidence": 0.9893219470977783}]}, {"text": "We present an extensive set of experiments in which we study some of the properties that SNOWexhibits on this problem, as well as compare it to other algorithms.", "labels": [], "entities": []}, {"text": "In our first experiment, for example, we study the quality of the learned classifiers by, artificially, supplying each classifier with the correct POS tags of its neighbors.", "labels": [], "entities": []}, {"text": "We show that under these conditions our classifier is almost perfect.", "labels": [], "entities": []}, {"text": "This observation motivates an improvement in the algorithm which aims at trying to gradually improve the input supplied to the classifier.", "labels": [], "entities": []}, {"text": "We then perform a preliminary study of learning the POS tagger in an unsupervised fashion.", "labels": [], "entities": [{"text": "POS tagger", "start_pos": 52, "end_pos": 62, "type": "TASK", "confidence": 0.6577062010765076}]}, {"text": "We show that we can reduce the requirements from the training corpus to some degree, but do not get good results, so far, when it is trained in a completely unsupervised fashion.", "labels": [], "entities": []}, {"text": "Unlike most of the algorithms tried on this and other disambiguation tasks, SNOW is an online learning algorithm.", "labels": [], "entities": []}, {"text": "That is, during training, every example is used once to update the learned hypothesis, and is then discarded.", "labels": [], "entities": []}, {"text": "While on-line learning algorithms maybe at disadvantage because they see each example only once, the algorithms are able to adapt to testing examples by receiving feedback after prediction.", "labels": [], "entities": []}, {"text": "We evaluate this claim for the POS task, and discover that indeed, allowing feedback while testing, significantly improves the performance of SNOWon this task.", "labels": [], "entities": [{"text": "POS task", "start_pos": 31, "end_pos": 39, "type": "TASK", "confidence": 0.5966077446937561}]}, {"text": "Finally, we compare our approach to a stateof-the-art tagger, based on Brill's transformation based approach; we show that SNOWbased taggers already achieve results that are comparable to it, and outperform it, when we allow online update.", "labels": [], "entities": [{"text": "SNOWbased taggers", "start_pos": 123, "end_pos": 140, "type": "TASK", "confidence": 0.796420156955719}]}, {"text": "Our work also raises a few methodological questions with regard to the way we measure the performance of algorithms for solving this problem, and improvements that can be made by better defining the goals of the tagger.", "labels": [], "entities": []}, {"text": "The paper is organized as follows.", "labels": [], "entities": []}, {"text": "We start by presenting the SNOW approach.", "labels": [], "entities": [{"text": "SNOW", "start_pos": 27, "end_pos": 31, "type": "TASK", "confidence": 0.8861994743347168}]}, {"text": "We then describe our test task, POS tagging, and the way we model it, and in Section 5 we describe our experimental studies.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 32, "end_pos": 43, "type": "TASK", "confidence": 0.8191713988780975}]}, {"text": "We conclude by discussing the significance of the approach to future research on natural language inferences.", "labels": [], "entities": [{"text": "natural language inferences", "start_pos": 81, "end_pos": 108, "type": "TASK", "confidence": 0.6577861706415812}]}, {"text": "In the discussion below, sis an input example, zi's denote the features of the example, and c, t refer to parts of speech from a set C of possible POS tags.", "labels": [], "entities": []}], "datasetContent": [{"text": "The data for all the experiments was extracted from the Penn Treebank WSJ corpus.", "labels": [], "entities": [{"text": "Penn Treebank WSJ corpus", "start_pos": 56, "end_pos": 80, "type": "DATASET", "confidence": 0.9766929745674133}]}, {"text": "The training and test corpus consist of 600000 and 150000, respectively.", "labels": [], "entities": []}, {"text": "The first set of experiment uses only the SNOW system and evaluate its performance under various conditions.", "labels": [], "entities": []}, {"text": "In the second set SNOW is compared with a naive Bayes algorithm and with Brill's TBL, all trained and tested on the same data.", "labels": [], "entities": [{"text": "SNOW", "start_pos": 18, "end_pos": 22, "type": "TASK", "confidence": 0.8970534801483154}]}, {"text": "We also compare with Baseline which simply assigns each word in the test corpus its most common POS in the lexicon.", "labels": [], "entities": [{"text": "POS", "start_pos": 96, "end_pos": 99, "type": "METRIC", "confidence": 0.9855133891105652}]}, {"text": "Baseline performance on our test corpus is 94.1%.", "labels": [], "entities": [{"text": "Baseline", "start_pos": 0, "end_pos": 8, "type": "METRIC", "confidence": 0.9877644181251526}]}, {"text": "A lexicon is computed from both the training and the test corpus.", "labels": [], "entities": []}, {"text": "The lexicon has 81227 distinct words, with an average of 2.2 possible POS tags per word in the lexicon.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Quality of classifier\" The SNOW", "labels": [], "entities": [{"text": "Quality", "start_pos": 10, "end_pos": 17, "type": "METRIC", "confidence": 0.983258068561554}, {"text": "SNOW", "start_pos": 37, "end_pos": 41, "type": "TASK", "confidence": 0.6020094156265259}]}, {"text": " Table 3: Effect of supervision. Performance", "labels": [], "entities": []}, {"text": " Table 4: Comparison of tagging perfor- mance,", "labels": [], "entities": []}]}