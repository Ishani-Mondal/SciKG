{"title": [{"text": "Automated Scoring Using A Hybrid Feature Identification Technique", "labels": [], "entities": [{"text": "Automated Scoring", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.6055213809013367}]}], "abstractContent": [{"text": "This study exploits statistical redundancy inherent in natural language to automatically predict scores for essays.", "labels": [], "entities": []}, {"text": "We use a hybrid feature identification method, including syntactic structure analysis, rhetorical structure analysis, and topical analysis, to score essay responses from test-takers of the Graduate Management Admissions Test (GMAT) and the Test of Written English (TWE).", "labels": [], "entities": [{"text": "rhetorical structure analysis", "start_pos": 87, "end_pos": 116, "type": "TASK", "confidence": 0.779429574807485}]}, {"text": "For each essay question, a stepwise linear regression analysis is run on a training set (sample of human scored essay responses) to extract a weighted set of predictive features for each test question.", "labels": [], "entities": []}, {"text": "Score prediction for cross-validation sets is calculated from the set of predictive features.", "labels": [], "entities": [{"text": "Score prediction", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.7573106586933136}]}, {"text": "Exact or adjacent agreement between the Electronic Essay Rater (e-rater) score predictions and human rater scores ranged from 87% to 94% across the 15 test questions.", "labels": [], "entities": []}], "introductionContent": [{"text": "This paper describes the development and evaluation of a prototype system designed for the purpose of automatically scoring essay responses.", "labels": [], "entities": []}, {"text": "The paper reports on evaluation results from scoring 13 sets of essay data from the Analytical Writing Assessments of the Graduate Management Admissions Test (GMAT) (see the GMAT Web site at http://www.gmat.org/ for sample questions) and 2 sets of essay data from the Test of Written English (TWE) (see http://www.toefl.org/ tstprpmt.html for sample TWE questions). was designed to automatically analyze essay features based on writing characteristics specified at each of six score points in the scoring guide used by human raters for manual scoring (also available at http://www.gmat.orff).", "labels": [], "entities": []}, {"text": "The scoring guide indicates that an essay that stays on the topic of the question has a strong, coherent and wellorganized argument structure, and displays a variety of word use and syntactic structure will receive a score at the higher end of the six-point scale (5 or 6).", "labels": [], "entities": []}, {"text": "Lower scores are assigned to essays as these characteristics diminish.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Mean Percentage and Standard  Deviation for E-rater (E) and Human Rater  (H) Agreement & Human Interrater  Agreement For 15 Cross-Validation Tests", "labels": [], "entities": [{"text": "Mean Percentage", "start_pos": 10, "end_pos": 25, "type": "METRIC", "confidence": 0.9354601204395294}, {"text": "Standard", "start_pos": 30, "end_pos": 38, "type": "METRIC", "confidence": 0.9587199687957764}]}]}