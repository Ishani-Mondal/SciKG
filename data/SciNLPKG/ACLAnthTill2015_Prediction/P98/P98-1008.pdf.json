{"title": [], "abstractContent": [{"text": "Word graphs are able to represent a large number of different utterance hypotheses in a very compact manner.", "labels": [], "entities": []}, {"text": "However, usually they contain a huge amount of redundancy in terms of word hypotheses that cover almost identical intervals in time.", "labels": [], "entities": []}, {"text": "We address this problem by introducing hypergraphs for speech processing.", "labels": [], "entities": [{"text": "speech processing", "start_pos": 55, "end_pos": 72, "type": "TASK", "confidence": 0.7134296596050262}]}, {"text": "Hypergraphs can be classified as an extension to word graphs and charts, their edges possibly having several start and end vertices.", "labels": [], "entities": []}, {"text": "By converting ordinary word graphs to hyper-graphs one can reduce the number of edges considerably.", "labels": [], "entities": []}, {"text": "We define hypergraphs formally, present an algorithm to convert word graphs into hypergraphs and state consistency properties for edges and their combination.", "labels": [], "entities": []}, {"text": "Finally, we present some empirical results concerning graph size and parsing efficiency.", "labels": [], "entities": [{"text": "parsing", "start_pos": 69, "end_pos": 76, "type": "TASK", "confidence": 0.9738816022872925}]}, {"text": "1 Introduction The interface between a word recognizer and language processing modules is a crucial issue with modern speech processing systems.", "labels": [], "entities": [{"text": "word recognizer", "start_pos": 39, "end_pos": 54, "type": "TASK", "confidence": 0.7074222713708878}]}, {"text": "Given a sufficiently high word recognition rate, it suffices to transmit the most probable word sequence from the recognizer to a subsequent module (e.g. a parser).", "labels": [], "entities": [{"text": "word recognition", "start_pos": 26, "end_pos": 42, "type": "TASK", "confidence": 0.7351970076560974}]}, {"text": "A slight extension over this best chain mode would be to deliver n-best chains to improve language processing results.", "labels": [], "entities": []}, {"text": "However, it is usually not enough to deliver just the best 10 or 20 utterances, at least not for reasonable sized applications given todays speech recognition technology.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 140, "end_pos": 158, "type": "TASK", "confidence": 0.703782469034195}]}, {"text": "To overcome this problem, inmost current systems word graphs are used as speech-language interface.", "labels": [], "entities": []}, {"text": "Word graphs offer a simple and efficient means to represent a very high number of utterance hypotheses in a extremely compact way (Oerder and Ney, 1993; Aubert and Ney, 1995).", "labels": [], "entities": []}], "introductionContent": [{"text": "The interface between a word recognizer and language processing modules is a crucial issue with modern speech processing systems.", "labels": [], "entities": [{"text": "word recognizer", "start_pos": 24, "end_pos": 39, "type": "TASK", "confidence": 0.702914223074913}]}, {"text": "Given a sufficiently high word recognition rate, it suffices to transmit the most probable word sequence from the recognizer to a subsequent module (e.g. a parser).", "labels": [], "entities": [{"text": "word recognition", "start_pos": 26, "end_pos": 42, "type": "TASK", "confidence": 0.7351970672607422}]}, {"text": "A slight extension over this best chain mode would be to deliver n-best chains to improve language processing results.", "labels": [], "entities": []}, {"text": "However, it is usually not enough to deliver just the best 10 or 20 utterances, at least not for reasonable sized applications given todays speech recognition technology.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 140, "end_pos": 158, "type": "TASK", "confidence": 0.703782469034195}]}, {"text": "To overcome this problem, inmost current systems word graphs are used as speech-language interface.", "labels": [], "entities": []}, {"text": "Word graphs offer a simple and efficient means to represent a very high number of utterance hypotheses in a extremely compact way.", "labels": [], "entities": []}, {"text": "dann (then) Figure 1: Two families of edges in a word graph Although they are compact, the use of word graphs leads to problems by itself.", "labels": [], "entities": []}, {"text": "One of them is the current lack of a reasonable measure for word graph size and evaluation of their contents ( ).", "labels": [], "entities": [{"text": "word graph size", "start_pos": 60, "end_pos": 75, "type": "TASK", "confidence": 0.6460558176040649}]}, {"text": "The problem we want to address in this paper is the presence of a large number of almost identical word hypotheses.", "labels": [], "entities": []}, {"text": "By almost identical we mean that the start and end vertices of edges differ only slightly.", "labels": [], "entities": []}, {"text": "Consider as an example section of a word graph.", "labels": [], "entities": []}, {"text": "There are several word hypotheses representing the words und (and) and dann (then).", "labels": [], "entities": []}, {"text": "The start and end points of them differ by small numbers of frames, each of them 10ms long.", "labels": [], "entities": []}, {"text": "The reasons for the existence of these families of edges are at least twofold: * Standard HMM-based word recognizers try to start (and finish) word models at each individual frame.", "labels": [], "entities": [{"text": "HMM-based word recognizers", "start_pos": 90, "end_pos": 116, "type": "TASK", "confidence": 0.6321199735005697}]}, {"text": "Since the resolution is quite high (10ms, in many cases shorter than the word onset), a word model may have boundaries at several points in time.", "labels": [], "entities": [{"text": "resolution", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9893659353256226}]}, {"text": "\u2022 Natural speech (and in particular spontaneously produced speech) tends to blur word boundaries.", "labels": [], "entities": []}, {"text": "This effect is in part responsible for the dramatic decrease in word recognition rate, given fluent speech as input in contrast to isolated words as input.", "labels": [], "entities": [{"text": "word recognition", "start_pos": 64, "end_pos": 80, "type": "TASK", "confidence": 0.7831540703773499}]}, {"text": "demonstrates the inaccuracy of word boundaries by containing several meeting points between und and dann, emphasized by the fact that both words end resp.", "labels": [], "entities": []}, {"text": "start with the same consonant.", "labels": [], "entities": []}, {"text": "Thus, for most words, there is a whole set of word hypotheses in a word graph which results in several meets between two sets of hypotheses.", "labels": [], "entities": []}, {"text": "Both facts are disadvantageous for speech processing: Many word edges result in a high number of lexical lookups and basic operations (e.g. bottom-up proposals of syntactic categories); many meeting points between edges result in a high number of possibly complex operations (like unifications in a parser).", "labels": [], "entities": []}, {"text": "The most obvious way to reduce the number of neighboring, identically labeled edges is to reduce the time resolution provided by a word recognizer.", "labels": [], "entities": [{"text": "time resolution", "start_pos": 101, "end_pos": 116, "type": "METRIC", "confidence": 0.8735995590686798}, {"text": "word recognizer", "start_pos": 131, "end_pos": 146, "type": "TASK", "confidence": 0.6821995675563812}]}, {"text": "If a word edge is to be processed, the start and end vertices are mapped to the more coarse grained points in time used by linguistic modules and a redundancy check is carried out in order to prevent multiple copies of edges.", "labels": [], "entities": []}, {"text": "This can be easily done, but one has to face the drawback on introducing many more paths through the graph due to artificially constructed overlaps.", "labels": [], "entities": []}, {"text": "Furthermore, it is not simple to choose a correct resolution, as the intervals effectively appearing with word onsets and offsets change considerably with words spoken.", "labels": [], "entities": []}, {"text": "Also, the introduction of cycles has to be avoided.", "labels": [], "entities": []}, {"text": "A more sophisticated schema would use interval graphs to encode word graphs.", "labels": [], "entities": []}, {"text": "Edges of interval graphs do not have individual start and end vertices, but instead use intervals to denote the range of applicability of an edge.", "labels": [], "entities": []}, {"text": "The major problem with interval graphs lies with the complexity of edge access methods.", "labels": [], "entities": []}, {"text": "However, many formal statements shown below will use interval arithmetics, as the argument will be easier to follow.", "labels": [], "entities": []}, {"text": "The approach we take in this paper is to use hypergraphs as representation medium for word graphs.", "labels": [], "entities": []}, {"text": "What one wants is to carryout operations only once and record the fact that there are several start and end points of words.", "labels": [], "entities": []}, {"text": "Hypergraphs ( are generalizations of ordinary graphs that allow multiple start and end vertices of edges.", "labels": [], "entities": []}, {"text": "We extend the approach of H.", "labels": [], "entities": []}, {"text": "Weber (Weber, 1995) for time mapping.", "labels": [], "entities": [{"text": "Weber (Weber, 1995)", "start_pos": 0, "end_pos": 19, "type": "DATASET", "confidence": 0.8539778689543406}, {"text": "time mapping", "start_pos": 24, "end_pos": 36, "type": "TASK", "confidence": 0.8057612180709839}]}, {"text": "Weber considered sets of edges with identical start vertices but slightly different end vertices, for which the notion family was introduced.", "labels": [], "entities": []}, {"text": "We use full hypergraphs as representation and thus additionally allow several start vertices, which results in a further decrease of 6% in terms of resulting chart edges while parsing (cf. section 3).", "labels": [], "entities": []}, {"text": "shows the example section using hyperedges for the two families of edges.", "labels": [], "entities": []}, {"text": "We adopt the way of dealing with different acoustical scores of word hypotheses from Weber.", "labels": [], "entities": []}, {"text": "To associate vertices with points in time, we use a function t : 1) > N that returns the frame number fora given vertex.", "labels": [], "entities": []}, {"text": "\u2022 A nonempty set of weighted, labeled, directed edges g = {el,...,em} C_ V x ~2 x 14) \u00d7 E.", "labels": [], "entities": []}, {"text": "To access the components of an edge e = (v, v', w, l), we use functions a, ~3, wand l, which return the start vertex (~(e) = v), the end vertex (/~(e) = v'), the weight (w(e) = w) and the label (l(e) = l) of an edge, respectively.", "labels": [], "entities": []}, {"text": "\u2022 A nonempty set of edge weights ~ --{wi,...,wp}.", "labels": [], "entities": []}, {"text": "Edge weights normally represent a the acoustic score assigned to the word hypothesis by a HMM based word recognizer.", "labels": [], "entities": []}, {"text": "\u2022 A nonempty set of Labels \u00a3 = {tl,...", "labels": [], "entities": []}, {"text": ",lo}, which represents information attached to an edge, usually words.", "labels": [], "entities": []}, {"text": "We define the relation of teachability for vertices (--r) as Vv, w E V : v --+ w ~ 3e E $ : The transitive hull of the reachability relation ---r is denoted by -~.", "labels": [], "entities": []}, {"text": "We already stated that a word graph is acyclic and distinctly rooted and ended.", "labels": [], "entities": []}], "datasetContent": [{"text": "The method of converting word graphs to hypergraphs has been used in two experiments so far.", "labels": [], "entities": []}, {"text": "One of them is devoted to the study of connectionist unification in speech applications (Weber, forthcoming).", "labels": [], "entities": [{"text": "connectionist unification", "start_pos": 39, "end_pos": 64, "type": "TASK", "confidence": 0.916386604309082}]}, {"text": "The other one, from which the performance figures in this section are drawn, is an experimental speech translation system focusing on incremental operation and uniform representation (Amtrup, 1997).", "labels": [], "entities": [{"text": "speech translation", "start_pos": 96, "end_pos": 114, "type": "TASK", "confidence": 0.7481845617294312}, {"text": "Amtrup, 1997)", "start_pos": 184, "end_pos": 197, "type": "DATASET", "confidence": 0.8301750719547272}]}], "tableCaptions": []}