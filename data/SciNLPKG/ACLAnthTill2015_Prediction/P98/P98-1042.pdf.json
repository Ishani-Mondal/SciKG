{"title": [{"text": "An Experiment in Hybrid Dictionary and Statistical Sentence Alignment", "labels": [], "entities": [{"text": "Statistical Sentence Alignment", "start_pos": 39, "end_pos": 69, "type": "TASK", "confidence": 0.8396097222963969}]}], "abstractContent": [{"text": "The task of aligning sentences in parallel corpora of two languages has been well studied using pure statistical or linguistic models.", "labels": [], "entities": []}, {"text": "We developed a linguistic method based on lexical matching with a bilingual dictionary and two statistical methods based on sentence length ratios and sentence offset probabilities.", "labels": [], "entities": []}, {"text": "This paper seeks to further our knowledge of the alignment task by comparing the performance of the alignment models when used separately and together, i.e. as a hybrid system.", "labels": [], "entities": [{"text": "alignment task", "start_pos": 49, "end_pos": 63, "type": "TASK", "confidence": 0.8969511091709137}]}, {"text": "Our results show that for our English-Japanese corpus of newspaper articles, the hybrid system using lexical matching and sentence length ratios outperforms the pure methods.", "labels": [], "entities": []}], "introductionContent": [{"text": "There have been many approaches proposed to solve the problem of aligning corresponding sentences in parallel corpora.", "labels": [], "entities": []}, {"text": "With a few notable exceptions however, much of this work has focussed on either corpora containing European language pairs or clean-parallel corpora where there is little reformatting.", "labels": [], "entities": []}, {"text": "In our work we have focussed on developing a method for robust matching of English-Japanese sentences, based primarily on lexical matching.", "labels": [], "entities": []}, {"text": "The method combines statistical information from byte length ratios.", "labels": [], "entities": []}, {"text": "We show in this paper that this hybrid model is more effective than its constituent parts used separately.", "labels": [], "entities": []}, {"text": "The task of sentence alignment is a critical first step in many automatic applications involving the analysis of bilingual texts such as extraction of bilingum vocabulary, extraction of translation templates, word sense disambiguation, word and phrase alignment, and extraction of parameters for statistical translation models.", "labels": [], "entities": [{"text": "sentence alignment", "start_pos": 12, "end_pos": 30, "type": "TASK", "confidence": 0.7277200669050217}, {"text": "extraction of translation templates", "start_pos": 172, "end_pos": 207, "type": "TASK", "confidence": 0.7953455001115799}, {"text": "word sense disambiguation", "start_pos": 209, "end_pos": 234, "type": "TASK", "confidence": 0.6742126146952311}, {"text": "word and phrase alignment", "start_pos": 236, "end_pos": 261, "type": "TASK", "confidence": 0.6516944468021393}]}, {"text": "Many software products which aid human translators now contain sentence alignment tools as an aid to speeding up editing and terminology searching.", "labels": [], "entities": [{"text": "sentence alignment", "start_pos": 63, "end_pos": 81, "type": "TASK", "confidence": 0.7229196727275848}, {"text": "terminology searching", "start_pos": 125, "end_pos": 146, "type": "TASK", "confidence": 0.9179542660713196}]}, {"text": "Various methods have been developed for sentence alignment which we can categorise as either lexical such as, based on a large-scale bilingual lexicon; statistical such as ( (Church, 1993)(), based on distributional regularities of words or byte-length ratios and possibly inducing a bilingual lexicon as a by-product, or hybrid such as (), based on some combination of the other two.", "labels": [], "entities": [{"text": "sentence alignment", "start_pos": 40, "end_pos": 58, "type": "TASK", "confidence": 0.7589090764522552}]}, {"text": "Neither of the pure approaches is entirely satisfactory for the following reasons: \u2022 Text volume limits the usefulness of statistical approaches.", "labels": [], "entities": []}, {"text": "We would often like to be able to align small amounts of text, or texts from various domains which do not share the same statistical properties.", "labels": [], "entities": []}, {"text": "\u2022 Bilingual dictionary coverage limitations mean that we will often encounter problems establishing a correspondence in non-general domains.", "labels": [], "entities": [{"text": "Bilingual dictionary coverage", "start_pos": 2, "end_pos": 31, "type": "TASK", "confidence": 0.7363696694374084}]}, {"text": "\u2022 Dictionary-based approaches are founded on an assumption of lexicul correspondence between language pairs.", "labels": [], "entities": []}, {"text": "We cannot always rely on this for non-cognate language pairs, such as English and Japanese.", "labels": [], "entities": []}, {"text": "\u2022 Texts are often heavily reformatted in translation, so we cannot assume that the corpus will be clean, i.e. contain many one-to-one sentence mappings.", "labels": [], "entities": []}, {"text": "In this case statistical methods which rely on structure correspondence such as byte-length ratios may not perform well.", "labels": [], "entities": []}, {"text": "These factors suggest that some hybrid method may give us the best combination of coverage and accuracy when we have a variety of text domains, text sizes and language pairs.", "labels": [], "entities": [{"text": "coverage", "start_pos": 82, "end_pos": 90, "type": "METRIC", "confidence": 0.9626433253288269}, {"text": "accuracy", "start_pos": 95, "end_pos": 103, "type": "METRIC", "confidence": 0.9982407093048096}]}, {"text": "In this paper we seek to fill a gap in our understanding and to show how the various components of the hybrid method influence the quality of sentence alignment for Japanese and English newspaper articles.", "labels": [], "entities": [{"text": "sentence alignment for Japanese and English newspaper articles", "start_pos": 142, "end_pos": 204, "type": "TASK", "confidence": 0.8473886772990227}]}], "datasetContent": [{"text": "In this section we present the results of using different combinations of the three basic methods.", "labels": [], "entities": []}, {"text": "We combined the basic methods to make hybrid models simply by taking the product of the scores for the models given above.", "labels": [], "entities": []}, {"text": "Although this is simplistic we felt that in the first stage of our investigation it was better to give equal weight to each method.", "labels": [], "entities": []}, {"text": "The seven methods we tested are coded as follows: The results are shown in.", "labels": [], "entities": []}, {"text": "We see that the baseline method using lexical matching with a bilingual lexicon, DICE, performs better than either of the two statistical methods LEN or OFFSET used separately.", "labels": [], "entities": []}, {"text": "Offset probabilities in particular performed poorly showing tltat we cannot expect the correctly matching sentence to appear constantly in the same highest probability position.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1. We see that the  baseline method using lexical matching with a bilin- gual lexicon, DICE, performs better than either of  the two statistical methods LEN or OFFSET used  separately. Offset probabilities in particular per- formed poorly showing tltat we cannot expect the  correctly matching sentence to appear constantly in", "labels": [], "entities": [{"text": "DICE", "start_pos": 94, "end_pos": 98, "type": "METRIC", "confidence": 0.8899040222167969}, {"text": "OFFSET", "start_pos": 167, "end_pos": 173, "type": "METRIC", "confidence": 0.6474881768226624}]}, {"text": " Table 1: Sentence alignment results as recall and  precision.", "labels": [], "entities": [{"text": "Sentence alignment", "start_pos": 10, "end_pos": 28, "type": "TASK", "confidence": 0.907947301864624}, {"text": "recall", "start_pos": 40, "end_pos": 46, "type": "METRIC", "confidence": 0.9995474219322205}, {"text": "precision", "start_pos": 52, "end_pos": 61, "type": "METRIC", "confidence": 0.9995676875114441}]}]}