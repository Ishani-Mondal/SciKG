{"title": [{"text": "Japanese OCR Error Correction using Character Shape Similarity and Statistical Language Model", "labels": [], "entities": [{"text": "Character Shape Similarity", "start_pos": 36, "end_pos": 62, "type": "TASK", "confidence": 0.7422285278638204}]}], "abstractContent": [{"text": "We present a novel OCR error correction method for languages without word delimiters that have a large character set, such as Japanese and Chinese.", "labels": [], "entities": [{"text": "OCR error correction", "start_pos": 19, "end_pos": 39, "type": "TASK", "confidence": 0.8510847290356954}]}, {"text": "It consists of a statistical OCR model, an approximate word matching method using character shape similarity, and a word segmentation algorithm using a statistical language model.", "labels": [], "entities": [{"text": "word matching", "start_pos": 55, "end_pos": 68, "type": "TASK", "confidence": 0.6941627860069275}, {"text": "word segmentation", "start_pos": 116, "end_pos": 133, "type": "TASK", "confidence": 0.7314291149377823}]}, {"text": "By using a statistical OCR model and character shape similarity, the proposed error corrector outperforms the previously published method.", "labels": [], "entities": [{"text": "error corrector", "start_pos": 78, "end_pos": 93, "type": "METRIC", "confidence": 0.9057825803756714}]}, {"text": "When the baseline character recognition accuracy is 90%, it achieves 97.4% character recognition accuracy.", "labels": [], "entities": [{"text": "character recognition", "start_pos": 18, "end_pos": 39, "type": "TASK", "confidence": 0.8872464597225189}, {"text": "accuracy", "start_pos": 40, "end_pos": 48, "type": "METRIC", "confidence": 0.9376661777496338}, {"text": "character recognition", "start_pos": 75, "end_pos": 96, "type": "TASK", "confidence": 0.8992360830307007}, {"text": "accuracy", "start_pos": 97, "end_pos": 105, "type": "METRIC", "confidence": 0.9567293524742126}]}], "introductionContent": [{"text": "As our society is becoming more computerized, people are getting enthusiastic about entering everything into computers.", "labels": [], "entities": []}, {"text": "So the need for OCR in areas such as office automation and information retrieval is becoming larger, contrary to our expectation.", "labels": [], "entities": [{"text": "OCR", "start_pos": 16, "end_pos": 19, "type": "TASK", "confidence": 0.9577820897102356}, {"text": "information retrieval", "start_pos": 59, "end_pos": 80, "type": "TASK", "confidence": 0.7961485981941223}]}, {"text": "In Japanese, although the accuracy of printed character OCR is about 98%, sources such as old books, poor quality photocopies, and faxes are still difficult to process and cause many errors.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 26, "end_pos": 34, "type": "METRIC", "confidence": 0.9994809031486511}]}, {"text": "The accuracy of handwritten OCR is still about 90%, and it worsens dramatically when the input quality is poor.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.9993826150894165}]}, {"text": "If NLP techniques could be used to boost the accuracy of handwriting and poor quality documents, we could enjoy a very large market for OCR related applications.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 45, "end_pos": 53, "type": "METRIC", "confidence": 0.9966979026794434}]}, {"text": "OCR error correction can bethought of a spelling correction problem.", "labels": [], "entities": [{"text": "OCR error correction", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.6573547720909119}, {"text": "spelling correction", "start_pos": 40, "end_pos": 59, "type": "TASK", "confidence": 0.8893211483955383}]}, {"text": "Although spelling correction has been studied for several decades, the traditional techniques are implicitly based on English and cannot be used for Asian languages such as Japanese and Chinese.", "labels": [], "entities": [{"text": "spelling correction", "start_pos": 9, "end_pos": 28, "type": "TASK", "confidence": 0.9695354700088501}]}, {"text": "The traditional strategy for English spelling correction is called isolated word error correction: Word boundaries are placed by white spaces.", "labels": [], "entities": [{"text": "English spelling correction", "start_pos": 29, "end_pos": 56, "type": "TASK", "confidence": 0.722854495048523}, {"text": "isolated word error correction", "start_pos": 67, "end_pos": 97, "type": "TASK", "confidence": 0.5732183530926704}]}, {"text": "If the tokenized string is not in the dictionary, it is a nonword.", "labels": [], "entities": []}, {"text": "For a non-word, correction candidates are retrieved from the dictionary by approximate string match techniques using context-independent word distance measures such as edit distance and ngram distance (.", "labels": [], "entities": [{"text": "ngram distance", "start_pos": 186, "end_pos": 200, "type": "METRIC", "confidence": 0.9094692766666412}]}, {"text": "Recently, statistical language models and featurebased method have been used for context-sensitive spelling correction, where errors are corrected considering the context in which the error occurs.", "labels": [], "entities": [{"text": "context-sensitive spelling correction", "start_pos": 81, "end_pos": 118, "type": "TASK", "confidence": 0.6361806492010752}]}, {"text": "Similar techniques are used for correcting the output of English OCRs and English speech recognizers.", "labels": [], "entities": [{"text": "English speech recognizers", "start_pos": 74, "end_pos": 100, "type": "TASK", "confidence": 0.5272213518619537}]}, {"text": "There are two problems in Japanese (and Chinese) spelling correction.", "labels": [], "entities": [{"text": "spelling correction", "start_pos": 49, "end_pos": 68, "type": "TASK", "confidence": 0.8205170929431915}]}, {"text": "The first is the word boundary problem.", "labels": [], "entities": [{"text": "word boundary problem", "start_pos": 17, "end_pos": 38, "type": "TASK", "confidence": 0.7888135115305582}]}, {"text": "It is impossible to use isolated word error correction techniques because there are no delimiters between words.", "labels": [], "entities": [{"text": "word error correction", "start_pos": 33, "end_pos": 54, "type": "TASK", "confidence": 0.6270838479200999}]}, {"text": "The second is the short word problem.", "labels": [], "entities": []}, {"text": "Word distance measures are useless because the average word length is short, and the character set is large (> 3000).", "labels": [], "entities": []}, {"text": "There area much larger number of one edit distance neighbors fora word, compared with English.", "labels": [], "entities": []}, {"text": "Recently, the first problem was solved by selecting the most likely word sequence from all combinations of exactly and approximately matched words using a Viterbi-like word segmentation algorithm and a statistical language model considering unknown words and non-words.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 168, "end_pos": 185, "type": "TASK", "confidence": 0.7495717704296112}]}, {"text": "However, the second problem is not solved yet, at least elegantly.", "labels": [], "entities": []}, {"text": "The solution presented in which sorts a list of one edit distance words considering the context in which it will be placed is inaccurate because the context itself might include some errors.", "labels": [], "entities": []}, {"text": "In this paper, we present a context-independent approximate word match method using character shape similarity.", "labels": [], "entities": [{"text": "word match", "start_pos": 60, "end_pos": 70, "type": "TASK", "confidence": 0.6476996541023254}]}, {"text": "This is suitable for languages with large character sets, such as Japanese and Chinese.", "labels": [], "entities": []}, {"text": "We also present a method to build a statistical OCR model by smoothing the character confusion probability using character shape similarity.", "labels": [], "entities": [{"text": "OCR", "start_pos": 48, "end_pos": 51, "type": "TASK", "confidence": 0.894960880279541}, {"text": "character confusion", "start_pos": 75, "end_pos": 94, "type": "TASK", "confidence": 0.8264009952545166}]}, {"text": "It seems previous NLP researchers are reluctant to use resources such as the character confusion matrix and feature vectors of the characters, and try to solve the problem by using only linguistic devices.", "labels": [], "entities": []}, {"text": "We found that, by using character shape similarity, the resulting OCR error corrector is robust and accurate enough to correct unrestricted texts with a wide range of recognition accuracies.", "labels": [], "entities": [{"text": "OCR error corrector", "start_pos": 66, "end_pos": 85, "type": "TASK", "confidence": 0.5355580449104309}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: The amount of the training data and the  test data for handwritten OCR", "labels": [], "entities": [{"text": "OCR", "start_pos": 77, "end_pos": 80, "type": "TASK", "confidence": 0.7086512446403503}]}, {"text": " Table 2: The document type and the image resolu- tion of the test data for the printed character OCR", "labels": [], "entities": [{"text": "OCR", "start_pos": 98, "end_pos": 101, "type": "TASK", "confidence": 0.3536326289176941}]}, {"text": " Table 3: OCR score and the number of right and  wrong corrections by the error corrector", "labels": [], "entities": [{"text": "OCR score", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9660220444202423}, {"text": "error corrector", "start_pos": 74, "end_pos": 89, "type": "METRIC", "confidence": 0.8524699807167053}]}]}