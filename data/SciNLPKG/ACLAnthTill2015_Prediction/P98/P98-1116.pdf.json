{"title": [{"text": "Generation that Exploits Corpus-Based Statistical Knowledge", "labels": [], "entities": []}], "abstractContent": [{"text": "We describe novel aspects of anew natural language generator called Nitrogen.", "labels": [], "entities": []}, {"text": "This generator has a highly flexible input representation that allows a spectrum of input from syntactic to semantic depth, and shifts' the burden of many linguistic decisions to the statistical post-processor.", "labels": [], "entities": []}, {"text": "The generation algorithm is compositional, making it efficient, yet it also handles non-compositional aspects of language.", "labels": [], "entities": []}, {"text": "Nitrogen's design makes it robust and scalable, operating with lexicons and knowledge bases of one hundred thousand entities.", "labels": [], "entities": []}], "introductionContent": [{"text": "Language generation is an important subtask of applications like machine translation, humancomputer dialogue, explanation, and summarization.", "labels": [], "entities": [{"text": "Language generation", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.6845908910036087}, {"text": "machine translation", "start_pos": 65, "end_pos": 84, "type": "TASK", "confidence": 0.8114050626754761}, {"text": "summarization", "start_pos": 127, "end_pos": 140, "type": "TASK", "confidence": 0.9845661520957947}]}, {"text": "The recurring need for generation suggests the usefulness of a general-purpose, domain-independent natural language generator (NLG).", "labels": [], "entities": []}, {"text": "However, \"plugin\" generators available today, such as FUF/SURGE (), MUMBLE, KPML, and CoGenTex's RealPro, require inputs with a daunting amount of linguistic detail.", "labels": [], "entities": []}, {"text": "As a result, many client applications resort instead to simpler template-based methods.", "labels": [], "entities": []}, {"text": "An important advantage of templates is that they sidestep linguistic decision-making, and avoid the need for large complex knowledge resources and processing.", "labels": [], "entities": []}, {"text": "For example, the following structure could be atypical result from a database query on the type of food avenue serves: ((:obj-type venue)(:obj-name Top_of_the_Mark) (:attribute food-type)(:attrib-value American)) By using a template like <obj-name> 's <attribute> is <attrib-value>.", "labels": [], "entities": []}, {"text": "the structure could produce the sentence, \"Top of the Mark's food type is American.\"", "labels": [], "entities": [{"text": "Top of the Mark's food type", "start_pos": 43, "end_pos": 70, "type": "DATASET", "confidence": 0.760611035994121}]}, {"text": ".Templates avoid the need for detailed linguistic information about lexical items, part-of-speech tags, number, gender, definiteness, tense, sentence organization, sub-categorization structure, semantic rela-\u2022 tions, etc., that more general NLG methods need to have specified in the input (or supply defaults for).", "labels": [], "entities": []}, {"text": "Such information is usually not readily inferrable from an application's database, nor is it always readily available from other sources, with the breadth of coverage or level of detail that is needed.", "labels": [], "entities": []}, {"text": "Thus, using a general-purpose generator can be formidable.", "labels": [], "entities": []}, {"text": "However, templates only work in very controlled or limited situations.", "labels": [], "entities": []}, {"text": "They cannot provide the expressiveness, flexibility or scalability that many real domains need.", "labels": [], "entities": []}, {"text": "A desirable solution is a generator that abstracts away from templates enough to provide the needed flexibility and scalability, and yet still requires only minimal semantic input (and maintains reasonable efficiency).", "labels": [], "entities": []}, {"text": "This generator would take on the responsibility of finding an appropriate linguistic realization for an underspecified semantic input.", "labels": [], "entities": []}, {"text": "This solution is especially important in the context of machine translation, where the surface syntactic organization of the source text is usually different from that of the target language, and the deep semantics are often difficult to obtain or represent completely as well.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 56, "end_pos": 75, "type": "TASK", "confidence": 0.7982997894287109}]}, {"text": "In Japanese to English translation, for example, it is often hard to determine from a Japanese text the number or gender of a noun phrase, the English equivalent of a verb tense, or the deep semantic meaning of sentential arguments.", "labels": [], "entities": []}, {"text": "There are many other obvious syntactic divergences as well.", "labels": [], "entities": []}, {"text": "Thus, shifting such linguistic decisions to the generator is significantly helpful for client applications.", "labels": [], "entities": []}, {"text": "However, at the same time, it imposes enormous needs for knowledge on the generator program.", "labels": [], "entities": []}, {"text": "Traditional large-scale NLG already requires immense amounts of knowledge, as does any large-scale AI enterprise.", "labels": [], "entities": []}, {"text": "NLG operating on a scale of 200,000 entities (concepts, relations, and words) requires large and sophisticated lexicons, grammars, ontologies, collocation lists, and morphological tables.", "labels": [], "entities": []}, {"text": "Acquiring and applying accurate, detailed knowledge of this breadth poses difficult problems.", "labels": [], "entities": []}, {"text": "(  suggested  . overcoming this knowledge acquisition bottleneck in NLG by tapping the vast knowledge inherent in English text corpora.", "labels": [], "entities": []}, {"text": "Experiments showed that corpusbased knowledge greatly reduced the need for deep, hand-crafted knowledge.", "labels": [], "entities": []}, {"text": "This knowledge, in the form of n-gram (word-pair) frequencies, could be applied to a set of semantically related sentences to help sort good ones from bad ones.", "labels": [], "entities": []}, {"text": "A corpus-based statistical ranker takes a set of sentences packed efficiently into a word lattice, (a state transition diagram with links labeled by English words), and extracts the best path from the lattice as output, preferring fluent sentences over contorted ones.", "labels": [], "entities": []}, {"text": "A generator can take advantage of this by producing a lattice that encodes various alternative possibilities when the information needed to make a linguistic decision is not available.", "labels": [], "entities": []}, {"text": "Such a system organization shown in, is robust against underspecified and even ambiguous input meaning structures.", "labels": [], "entities": []}, {"text": "Traditionally, underspecification is handled with rigid defaults (e.g., assume present tense, use the alphabetically-first synonyms, use nominal arguments, etc.).", "labels": [], "entities": []}, {"text": "However, the word lattice structure permits all the different possibilities to be encoded as different phrasings, and the corpus-based statistical extractor can select a good sentence from these possibilities.", "labels": [], "entities": []}, {"text": "The questions that still remain are: What kind of input representation is minimally necessary?", "labels": [], "entities": []}, {"text": "What kinds of linguistic decisions can the statistics reliably make, and which instead need to be made symbolically?", "labels": [], "entities": []}, {"text": "How should symbolic knowledge be applied to the input to efficiently produce word lattices from the input?", "labels": [], "entities": []}, {"text": "This paper describes Nitrogen, a generation system that computes word lattices from a meaning representation to take advantage of corpus-based statistical knowledge.", "labels": [], "entities": []}, {"text": "Nitrogen performs sentence realization and some components of sentence planning--namely, mapping domain concepts to content words, and to some extent, mapping semantic relations to grammatical ones.", "labels": [], "entities": [{"text": "sentence realization", "start_pos": 18, "end_pos": 38, "type": "TASK", "confidence": 0.7405655086040497}, {"text": "sentence planning", "start_pos": 62, "end_pos": 79, "type": "TASK", "confidence": 0.744305819272995}]}, {"text": "It contributes: \u2022 A flexible input representation based on conceptual meanings and the relations between them.", "labels": [], "entities": []}, {"text": "\u2022 A new grammar formalism for defining the mapping of meanings onto word lattices.", "labels": [], "entities": []}, {"text": "\u2022 A new efficient algorithm to do this mapping.", "labels": [], "entities": []}, {"text": "\u2022 A large grammar, lexicon, and morphology of English, addressing linguistic phenomena such as knowledge acquisition bottlenecks and underspecified/ambiguous input.", "labels": [], "entities": []}, {"text": "This paper is organized as follows.", "labels": [], "entities": []}, {"text": "First, we describe our Abstract Meaning Representation language (AMR).", "labels": [], "entities": [{"text": "Abstract Meaning Representation language (AMR)", "start_pos": 23, "end_pos": 69, "type": "TASK", "confidence": 0.7411744466849736}]}, {"text": "Then we outline the generation algorithm and describe how various knowledge sources apply to render an AMR into English, including lexical, morphological, and grammatical knowledge bases.", "labels": [], "entities": []}, {"text": "We describe the structure of these knowledge bases and give examples.", "labels": [], "entities": []}, {"text": "We also present a technique that adds powerful flexibility to the grammar formalism.", "labels": [], "entities": []}, {"text": "We finish with a discussion of the strengths and weaknesses of our generation system.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": []}