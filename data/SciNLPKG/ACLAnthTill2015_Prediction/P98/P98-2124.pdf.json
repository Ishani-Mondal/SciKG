{"title": [{"text": "Word Clustering and Disambiguation Based on Co-occurrence Data", "labels": [], "entities": []}], "abstractContent": [{"text": "We address the problem of clustering words (or constructing a thesaurus) based on co-occurrence data, and using the acquired word classes to improve the accuracy of syntactic disambiguation.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 153, "end_pos": 161, "type": "METRIC", "confidence": 0.997957706451416}, {"text": "syntactic disambiguation", "start_pos": 165, "end_pos": 189, "type": "TASK", "confidence": 0.7141833305358887}]}, {"text": "We view this problem as that of estimating a joint probability distribution specifying the joint probabilities of word pairs, such as noun verb pairs.", "labels": [], "entities": []}, {"text": "We propose an efficient algorithm based on the Minimum Description Length (MDL) principle for estimating such a probability distribution.", "labels": [], "entities": [{"text": "Minimum Description Length (MDL)", "start_pos": 47, "end_pos": 79, "type": "METRIC", "confidence": 0.6795984208583832}]}, {"text": "Our method is a natural extension of those proposed in (Brown et al., 1992) and (Li and Abe, 1996), and overcomes their drawbacks while retaining their advantages.", "labels": [], "entities": []}, {"text": "We then combined this clustering method with the disam-biguation method of (Li and Abe, 1995) to derive a disambiguation method that makes use of both automatically constructed thesauruses and a handmade thesaurus.", "labels": [], "entities": []}, {"text": "The overall disambiguation accuracy achieved by our method is 85.2%, which compares favorably against the accuracy (82.4%) obtained by the state-of-the-art disambiguation method of (Brill and Resnik, 1994).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 27, "end_pos": 35, "type": "METRIC", "confidence": 0.944732666015625}, {"text": "accuracy", "start_pos": 106, "end_pos": 114, "type": "METRIC", "confidence": 0.9991565942764282}]}], "introductionContent": [{"text": "We address the problem of clustering words, or that of constructing a thesaurus, based on co-occurrence data.", "labels": [], "entities": []}, {"text": "We view this problem as that of estimating a joint probability distribution over word pairs, specifying the joint probabilities of word pairs, such as noun verb pairs.", "labels": [], "entities": []}, {"text": "In this paper, we assume that the joint distribution can be expressed in the following manner, which is stated for noun verb pairs for the sake of readability: The joint probability of a noun and a verb is expressed as the product of the joint probability of the noun class and the verb class which the noun and the verb respectively belong to, and the conditional probabilities of the noun and the verb given their respective classes.", "labels": [], "entities": []}, {"text": "As a method for estimating such a probability distribution, we propose an algorithm based on the Minimum Description Length (MDL) principle.", "labels": [], "entities": [{"text": "Minimum Description Length (MDL)", "start_pos": 97, "end_pos": 129, "type": "METRIC", "confidence": 0.6588508834441503}]}, {"text": "Our clustering algorithm iteratively merges noun classes and verb classes in turn, in a bottom up fashion.", "labels": [], "entities": []}, {"text": "For each merge it performs, it calculates the increase in data description length resulting from merging any noun (or verb) class pair, and performs the merge having the least increase in data description length, provided that the increase in data description length is less than the reduction in model description length.", "labels": [], "entities": []}, {"text": "There have been a number of methods proposed in the literature to address the word clustering problem (e.g.,).", "labels": [], "entities": [{"text": "word clustering problem", "start_pos": 78, "end_pos": 101, "type": "TASK", "confidence": 0.8222564061482748}]}, {"text": "The method proposed in this paper is a natural extension of both Li & Abe's and Brown et al's methods, and is an attempt to overcome their drawbacks while retaining their advantages.", "labels": [], "entities": []}, {"text": "The method of Brown et al, which is based on the Maximum Likelihood Estimation (MLE), performs a merge which would result in the least reduction in (average) mutual information.", "labels": [], "entities": [{"text": "Maximum Likelihood Estimation (MLE)", "start_pos": 49, "end_pos": 84, "type": "METRIC", "confidence": 0.7495700816313425}]}, {"text": "Our method turns out to be equivalent to performing the merge with the least reduction in mutual information, provided that the reduction is below a certain threshold which depends on the size of the co-occurrence data and the number of classes in the current situation.", "labels": [], "entities": []}, {"text": "This method, based on the MDL principle, takes into account both the fit to data and the simplicity of a model, and thus can help cope with the over-fitting problem that the MLE-based method of Brown et al faces.", "labels": [], "entities": []}, {"text": "The model employed in () is based on the assumption that the word distribution within a class is a uniform distribution, i.e. every word in a same class is generated with an equal probability.", "labels": [], "entities": []}, {"text": "Employing such a model has the undesirable tendency of classifying into different classes those words that have similar co-occurrence patterns but have different absolute frequencies.", "labels": [], "entities": []}, {"text": "The proposed method, in contrast, employs a model in which different words within a same class can have different conditional generation probabilities, and thus can classify words in away that is not affected by words' absolute frequencies and resolve the problem faced by the method of (.", "labels": [], "entities": []}, {"text": "We evaluate our clustering method by using the word classes and the joint probabilities obtained by it in syntactic disambiguation experiments.", "labels": [], "entities": []}, {"text": "Our experimental results indicate that using the word classes constructed by our method gives better disambiguation results than when using Li & Abe or Brown et al's methods.", "labels": [], "entities": []}, {"text": "By combining thesauruses automatically constructed by our method and an existing hand-made thesaurus (WordNet), we were able to achieve the overall accuracy of 85.2% for ppattachment disambiguation, which compares favorably against the accuracy (82.4%) obtained using the state-of-the-art method of.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 102, "end_pos": 109, "type": "DATASET", "confidence": 0.9544981718063354}, {"text": "accuracy", "start_pos": 148, "end_pos": 156, "type": "METRIC", "confidence": 0.9993236064910889}, {"text": "ppattachment disambiguation", "start_pos": 170, "end_pos": 197, "type": "TASK", "confidence": 0.7270981967449188}, {"text": "accuracy", "start_pos": 236, "end_pos": 244, "type": "METRIC", "confidence": 0.9990239143371582}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Compound noun disambiguation results", "labels": [], "entities": [{"text": "Compound noun disambiguation", "start_pos": 10, "end_pos": 38, "type": "TASK", "confidence": 0.7224137584368387}]}, {"text": " Table 2: PP-attachment disambiguation results", "labels": [], "entities": [{"text": "PP-attachment disambiguation", "start_pos": 10, "end_pos": 38, "type": "TASK", "confidence": 0.8267723023891449}]}, {"text": " Table 3: PP-attachment disambiguation results", "labels": [], "entities": [{"text": "PP-attachment disambiguation", "start_pos": 10, "end_pos": 38, "type": "TASK", "confidence": 0.829272985458374}]}]}