{"title": [{"text": "An Empirical Evaluation of Probabilistic Lexicalized Tree Insertion Grammars *", "labels": [], "entities": [{"text": "Probabilistic Lexicalized Tree Insertion Grammars", "start_pos": 27, "end_pos": 76, "type": "TASK", "confidence": 0.612895280122757}]}], "abstractContent": [{"text": "We present an empirical study of the applicability of Probabilistic Lexicalized Tree Insertion Grammars (PLTIG), a lexicalized counterpart to Probabilistic Context-Free Grammars (PCFG), to problems in stochastic natural-language processing.", "labels": [], "entities": []}, {"text": "Comparing the performance of PLTIGs with non-hierarchical N-gram models and PCFGs, we show that PLTIG combines the best aspects of both, with language modeling capability comparable to N-grams, and improved parsing performance over its non-lexicalized counterpart.", "labels": [], "entities": []}, {"text": "Furthermore, training of PLTIGs displays faster convergence than PCFGs.", "labels": [], "entities": [{"text": "convergence", "start_pos": 48, "end_pos": 59, "type": "METRIC", "confidence": 0.9709632396697998}]}], "introductionContent": [], "datasetContent": [{"text": "In the following experiments we show that PLTIGs of varying sizes and configurations can be induced by processing a large training corpus, and that the trained PLTIGs can provide parses on unseen test data of comparable quality to the parses produced by PCFGs.", "labels": [], "entities": []}, {"text": "Moreover, we show that PLTIGs have significantly lower entropy values than PCFGs, suggesting that they make better language models.", "labels": [], "entities": []}, {"text": "We describe the induction process of the PLTIGs in Section 3.1.", "labels": [], "entities": [{"text": "PLTIGs", "start_pos": 41, "end_pos": 47, "type": "DATASET", "confidence": 0.7088654637336731}]}, {"text": "Two corpora of very different nature are used for training and testing.", "labels": [], "entities": []}, {"text": "The first set of experiments uses the Air Travel Information System (ATIS) corpus.", "labels": [], "entities": [{"text": "Air Travel Information System (ATIS) corpus", "start_pos": 38, "end_pos": 81, "type": "DATASET", "confidence": 0.863532543182373}]}, {"text": "Section 3.2 presents the complete results of this set of experiments.", "labels": [], "entities": []}, {"text": "To determine if PLTIGs can scale up well, we have also begun another study that uses a larger and more complex corpus, the Wall Street Journal TreeBank corpus.", "labels": [], "entities": [{"text": "Wall Street Journal TreeBank corpus", "start_pos": 123, "end_pos": 158, "type": "DATASET", "confidence": 0.9742389798164368}]}, {"text": "The initial results are discussed in Section 3.3.", "labels": [], "entities": []}, {"text": "To reduce the effect of the data sparsity problem, we back off from lexical words to using the part of speech tags as the anchoring lexical items in all the experiments.", "labels": [], "entities": []}, {"text": "Moreover, we use the deletedinterpolation smoothing technique for the Ngram models and PLTIGs.", "labels": [], "entities": [{"text": "PLTIGs", "start_pos": 87, "end_pos": 93, "type": "DATASET", "confidence": 0.7703022956848145}]}, {"text": "PCFGs do not require smoothing in these experiments.", "labels": [], "entities": [{"text": "PCFGs", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.8587847948074341}]}], "tableCaptions": [{"text": " Table 1: Summary results for ATIS. The machine used to measure real-time is an HP 9000/859.", "labels": [], "entities": [{"text": "ATIS", "start_pos": 30, "end_pos": 34, "type": "DATASET", "confidence": 0.7457668781280518}]}, {"text": " Table 3: Summary results of the training phase for WSJ", "labels": [], "entities": [{"text": "Summary", "start_pos": 10, "end_pos": 17, "type": "METRIC", "confidence": 0.7423970699310303}, {"text": "WSJ", "start_pos": 52, "end_pos": 55, "type": "TASK", "confidence": 0.7009100317955017}]}]}