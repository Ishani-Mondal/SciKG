{"title": [], "abstractContent": [{"text": "Treebanks, such as the Penn Treebank (PTB), offer a simple approach to obtaining abroad coverage grammar: one can simply read the grammar off the parse trees in the treebank.", "labels": [], "entities": [{"text": "Penn Treebank (PTB)", "start_pos": 23, "end_pos": 42, "type": "DATASET", "confidence": 0.975693428516388}]}, {"text": "While such a grammar is easy to obtain, a square-root rate of growth of the rule set with corpus size suggests that the derived grammar is far from complete and that much more tree-banked text would be required to obtain a complete grammar, if one exists at some limit.", "labels": [], "entities": []}, {"text": "However, we offer an alternative explanation in terms of the underspecification of structures within the treebank.", "labels": [], "entities": []}, {"text": "This hypothesis is explored by applying an algorithm to compact the derived grammar by eliminating redundant rules-rules whose right hand sides can be parsed by other rules.", "labels": [], "entities": []}, {"text": "The size of the resulting compacted grammar, which is significantly less than that of the full treebank grammar, is shown to approach a limit.", "labels": [], "entities": []}, {"text": "However, such a compacted grammar does not yield very good performance figures.", "labels": [], "entities": []}, {"text": "A version of the compac-tion algorithm taking rule probabilities into account is proposed, which is argued to be more linguistically motivated.", "labels": [], "entities": []}, {"text": "Combined with simple thresholding, this method can be used to give a 58% reduction in grammar size without significant change in parsing performance, and can produce a 69% reduction with some gain in recall , but a loss in precision.", "labels": [], "entities": [{"text": "parsing", "start_pos": 129, "end_pos": 136, "type": "TASK", "confidence": 0.9655107259750366}, {"text": "recall", "start_pos": 200, "end_pos": 206, "type": "METRIC", "confidence": 0.9993175268173218}, {"text": "precision", "start_pos": 223, "end_pos": 232, "type": "METRIC", "confidence": 0.9981729984283447}]}], "introductionContent": [{"text": "The Penn Treebank (PTB) has been used fora rather simple approach to deriving large grammars automatically: one where the grammar rules are simply 'read off' the parse trees in the corpus, with each local subtree providing the left and right hand sides of a rule.", "labels": [], "entities": [{"text": "Penn Treebank (PTB)", "start_pos": 4, "end_pos": 23, "type": "DATASET", "confidence": 0.9798784494400025}]}, {"text": "Charniak) reports precision and recall figures of around 80% fora parser employing such a grammar.", "labels": [], "entities": [{"text": "precision", "start_pos": 18, "end_pos": 27, "type": "METRIC", "confidence": 0.9995917677879333}, {"text": "recall", "start_pos": 32, "end_pos": 38, "type": "METRIC", "confidence": 0.9974898099899292}]}, {"text": "In this paper we show that the huge size of such a treebank grammar (see below) can be reduced in size without appreciable loss in performance, and, in fact, an improvement in recall can be achieved.", "labels": [], "entities": [{"text": "recall", "start_pos": 176, "end_pos": 182, "type": "METRIC", "confidence": 0.9992252588272095}]}, {"text": "Our approach can be generalised in terms of Data-Oriented Parsing (DOP) methods (see () with the tree depth of 1.", "labels": [], "entities": []}, {"text": "However, the number of trees produced with a general DOP method is so large that Bonnema () has to resort to restricting the tree depth, using a very domain-specific corpus such as ATIS or OVIS, and parsing very short sentences of average length 4.74 words.", "labels": [], "entities": [{"text": "Bonnema", "start_pos": 81, "end_pos": 88, "type": "DATASET", "confidence": 0.71274334192276}]}, {"text": "Our compaction algorithm can be easily extended for the use within the DOP framework but, because of the huge size of the derived grammar (see below), we chose to use the simplest PCFG framework for our experiments.", "labels": [], "entities": []}, {"text": "We are concerned with the nature of the rule set extracted, and how it can be improved, with regard both to linguistic criteria and processing efficiency.", "labels": [], "entities": []}, {"text": "Inwhat follows, we report the worrying observation that the growth of the rule set continues at a square root rate throughout processing of the entire treebank (suggesting, perhaps that the rule set is far from complete).", "labels": [], "entities": []}, {"text": "Our results are similar to those reported in (.", "labels": [], "entities": []}, {"text": "1 We discuss an alternative possible source of this rule growth phenomenon, partial bracketting, and suggest that it can be alleviated by compaction, where rules that are redundant (in a sense to be defined) are eliminated from the grammar.", "labels": [], "entities": [{"text": "partial bracketting", "start_pos": 76, "end_pos": 95, "type": "TASK", "confidence": 0.6551303267478943}]}, {"text": "Our experiments on compacting a PTB tree- one, that the grammar can be compacted to about 7% of its original size, and the rule number growth of the compacted grammar stops at some point.", "labels": [], "entities": [{"text": "PTB tree", "start_pos": 32, "end_pos": 40, "type": "DATASET", "confidence": 0.6950938254594803}]}, {"text": "The other is that a 58% reduction can be achieved with no loss in parsing performance, whereas a 69% reduction yields again in recall, but a loss in precision.", "labels": [], "entities": [{"text": "parsing", "start_pos": 66, "end_pos": 73, "type": "TASK", "confidence": 0.9656775593757629}, {"text": "recall", "start_pos": 127, "end_pos": 133, "type": "METRIC", "confidence": 0.9987207055091858}, {"text": "precision", "start_pos": 149, "end_pos": 158, "type": "METRIC", "confidence": 0.9982467889785767}]}, {"text": "This, we believe, gives further support to the utility of treebank grammars and to the compaction method.", "labels": [], "entities": []}, {"text": "For example, compaction methods can be applied within the DOP framework to reduce the number of trees.", "labels": [], "entities": []}, {"text": "Also, by partially lexicalising the rule extraction process (i.e., by using some more frequent words as well as the part-of-speech tags), we maybe able to achieve parsing performance similar to the best results in the field obtained in.", "labels": [], "entities": [{"text": "rule extraction", "start_pos": 36, "end_pos": 51, "type": "TASK", "confidence": 0.7835953831672668}, {"text": "parsing", "start_pos": 163, "end_pos": 170, "type": "TASK", "confidence": 0.9637328386306763}]}], "datasetContent": [{"text": "We conducted a number of compaction experiments: 5 first, the complete grammar was parsed as described in Section 4.", "labels": [], "entities": []}, {"text": "Results exceeded our expectations: the set of 17,529 rules reduced to only 1,667 rules, a better than 90% reduction.", "labels": [], "entities": []}, {"text": "To investigate in more detail how the compacted grammar grows, we conducted a third experiment involving a staged compaction of the grammar.", "labels": [], "entities": []}, {"text": "Firstly, the corpus was split into 10% chunks (by number of files) and the rule sets extracted from each.", "labels": [], "entities": []}, {"text": "The staged compaction proceeded as follows: the rule set of the first 10% was compacted, and then the rules for the 4See for discussion.", "labels": [], "entities": [{"text": "4See", "start_pos": 116, "end_pos": 120, "type": "DATASET", "confidence": 0.8711605668067932}]}, {"text": "SFor these experiments, we used two parsers: Stolcke's BOOGIE and Sekine's Apple Pie Parser (..", "labels": [], "entities": [{"text": "BOOGIE", "start_pos": 55, "end_pos": 61, "type": "METRIC", "confidence": 0.986132025718689}]}, {"text": "At 50% of the corpus processed the compacted grammar size actually exceeds the level it reaches at 100%, and then the overall grammar size starts to go down as well as up.", "labels": [], "entities": []}, {"text": "This reflects the fact that new rules are either redundant, or make \"old\" rules redundant, so that the compacted grammar size seems to approach a limit.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Preliminary results of evaluating the grammar compaction method", "labels": [], "entities": []}]}