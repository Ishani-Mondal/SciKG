{"title": [], "abstractContent": [{"text": "We report in this paper the observation of one tokenization per source.", "labels": [], "entities": []}, {"text": "That is, the same critical fragment in different sentences from the same source almost always realize one and the same of its many possible tokenizations.", "labels": [], "entities": []}, {"text": "This observation is demonstrated very helpful in sentence tokenization practice, and is argued to be with far-reaching implications in natural language processing.", "labels": [], "entities": [{"text": "sentence tokenization", "start_pos": 49, "end_pos": 70, "type": "TASK", "confidence": 0.712754875421524}, {"text": "natural language processing", "start_pos": 135, "end_pos": 162, "type": "TASK", "confidence": 0.6496095458666483}]}], "introductionContent": [{"text": "This paper sets to establish the hypothesis of one tokenization per source.", "labels": [], "entities": []}, {"text": "That is, if an ambiguous fragment appears two or more times in different sentences from the same source, it is extremely likely that they will all share the same tokenization.", "labels": [], "entities": []}, {"text": "Sentence tokenization is the task of mapping sentences from character strings into streams of tokens.", "labels": [], "entities": [{"text": "Sentence tokenization", "start_pos": 0, "end_pos": 21, "type": "TASK", "confidence": 0.8817378580570221}]}, {"text": "This is a long-standing problem in Chinese Language Processing, since, in Chinese, there is an apparent lack of such explicit word delimiters as white-spaces in English.", "labels": [], "entities": [{"text": "Chinese Language Processing", "start_pos": 35, "end_pos": 62, "type": "TASK", "confidence": 0.5990134576956431}]}, {"text": "And researchers have gradually been turning to model the task as a general lexicalization or bracketing problem in Computational Linguistics, with the hope that the research might also benefit the study of similar problems in multiple languages.", "labels": [], "entities": []}, {"text": "For instance, in Machine Translation, it is widely agreed that many multiple-word expressions, such as idioms, compounds and some collocations, while not explicitly delimited in sentences, are ideally to be treated as single lexicalized units.", "labels": [], "entities": [{"text": "Machine Translation", "start_pos": 17, "end_pos": 36, "type": "TASK", "confidence": 0.840128481388092}]}, {"text": "The primary obstacle in sentence tokenization is in the existence of uncertainties both in the notion of words/tokens and in the recognition of words/tokens in context.", "labels": [], "entities": [{"text": "sentence tokenization", "start_pos": 24, "end_pos": 45, "type": "TASK", "confidence": 0.6866331100463867}]}, {"text": "The same fragment-in different contexts would have to be tokenized differently.", "labels": [], "entities": []}, {"text": "For instance, the character string todayissunday would normally be tokenized as \"'today is sunday\" but can also reasonably be \"'today is sun day\".", "labels": [], "entities": []}, {"text": "In terms of possibility, it has been argued that no lexically possible tokenization cannot be grammatically and meaningfully realized in at least some special contexts, as every token can be assigned to bear any meaning without any orthographic means.", "labels": [], "entities": []}, {"text": "Consequently, the mainstream research in the literature has been focused on the modeling and utilization of local and sentential contexts, either linguistically in a rule-based framework or statistically in a searching and optimization set-up (Gan, Palmer and Lua 1996; Sproat, Shih, Gale and Chang 1996;.", "labels": [], "entities": []}, {"text": "Hence, it was really a surprise when we first observed the regularity of one tokenization per source.", "labels": [], "entities": []}, {"text": "Nevertheless, the regularity turns out to be very helpful in sentence tokenization practice, and to be with far-reaching implications in natural language processing.", "labels": [], "entities": [{"text": "sentence tokenization", "start_pos": 61, "end_pos": 82, "type": "TASK", "confidence": 0.7192347943782806}, {"text": "natural language processing", "start_pos": 137, "end_pos": 164, "type": "TASK", "confidence": 0.6442940930525461}]}, {"text": "Retrospectively, we now understand that it is by no means an isolated special phenomenon but another display of the postulated general law of one realization per expression.", "labels": [], "entities": []}, {"text": "In the rest of the paper, we will first present a concrete corpus verification (Section 2), clarify its meaning and scope (Section 3), display its striking utility value in tokenization (Section 4), and then disclose its implication for the notion of words/tokens (Section 5), and associate the hypothesis with the general law of one realization per expression through examination of related works in the literature (Section 6).", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 2: Statistics for dictionary-entry fragments.  (0)  (1)  (2) (3)=(2)/(1)  Fragment  All  Multiple Percentage  Occurrences 46635  87  0.19  Forms  6700  14  0.21  Errors 46635  16  0.03  In short, 0.19% of all the different dictionary-entry  fragments, taking 0.21% of all the occurrences,  have realized different tokenizations, and 0.03%  tokenization errors would be introduced if forced  to take one tokenization per fragment.", "labels": [], "entities": [{"text": "46635  87  0.19  Forms  6700  14  0.21  Errors 46635", "start_pos": 129, "end_pos": 181, "type": "DATASET", "confidence": 0.8520188397831387}]}, {"text": " Table 4: Statistics for non-dictionary entry fragments.  (0)  Fragment  Forms", "labels": [], "entities": []}, {"text": " Table 5: Occurrence distribution of non-dictionary- entry critical fragments in the PH corpus.", "labels": [], "entities": [{"text": "PH corpus", "start_pos": 85, "end_pos": 94, "type": "DATASET", "confidence": 0.8807931244373322}]}]}