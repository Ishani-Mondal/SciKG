{"title": [{"text": "Trainable, Scalable Summarization Using Robust NLP and Machine Learning*", "labels": [], "entities": []}], "abstractContent": [{"text": "We describe a trainable and scalable sum-marization system which utilizes features derived from information retrieval, information extraction, and NLP techniques and on-line resources.", "labels": [], "entities": [{"text": "information extraction", "start_pos": 119, "end_pos": 141, "type": "TASK", "confidence": 0.7662249505519867}]}, {"text": "The system combines these features using a trainable feature combiner learned from summary examples through a machine learning algorithm.", "labels": [], "entities": []}, {"text": "We demonstrate system scalability by reporting results on the best combination of summarization features for different document sources.", "labels": [], "entities": []}, {"text": "We also present preliminary results from a task-based evaluation on summarization output usability.", "labels": [], "entities": [{"text": "summarization output usability", "start_pos": 68, "end_pos": 98, "type": "TASK", "confidence": 0.8941189845403036}]}, {"text": "1 Introduction Frequency-based (Edmundson, 1969; Kupiec, Ped-ersen, and Chen, 1995; Brandow, Mitze, and Rau, 1995), knowledge-based (Reimer and Hahn, 1988; McKeown and l:Ladev, 1995), and discourse-based (Johnson et al., 1993; Miike et al., 1994; Jones, 1995) approaches to automated summarization correspond to a continuum of increasing understanding of the text and increasing complexity in text processing.", "labels": [], "entities": []}, {"text": "Given the goal of machine-generated summaries , these approaches attempt to answer three central questions: \u2022 How does the system count words to calculate worthiness for summarization?", "labels": [], "entities": []}, {"text": "\u2022 How does the system incorporate the knowledge of the domain represented in the text?", "labels": [], "entities": []}, {"text": "\u2022 How does the system create a coherent and cohesive summary?", "labels": [], "entities": []}, {"text": "Our work leverages off of research in these three approaches and attempts to remedy some of the difficulties encountered in each by applying a combination of information retrieval, information extraction, \"We would like to thank Jamie Callan for his help with the INQUERY experiments. and NLP techniques and on-line resources with machine learning to generate summaries.", "labels": [], "entities": [{"text": "information retrieval", "start_pos": 158, "end_pos": 179, "type": "TASK", "confidence": 0.7632442116737366}, {"text": "information extraction", "start_pos": 181, "end_pos": 203, "type": "TASK", "confidence": 0.8335407972335815}]}, {"text": "Our DimSum system follows a common paradigm of sentence extraction , but automates acquiring candidate knowledge and learns what knowledge is necessary to summarize.", "labels": [], "entities": [{"text": "sentence extraction", "start_pos": 47, "end_pos": 66, "type": "TASK", "confidence": 0.7482747435569763}]}, {"text": "We present how we automatically acquire candidate features in Section 2.", "labels": [], "entities": []}, {"text": "Section 3 describes our training methodology for combining features to generate summaries, and discusses evaluation results of both batch and machine learning methods.", "labels": [], "entities": []}, {"text": "Section 4 reports our task-based evaluation.", "labels": [], "entities": []}, {"text": "2 Extracting Features In this section, we describe how the system counts linguistically-motivated, automatically-derived words and multi-words in calculating wor-thiness for summarization.", "labels": [], "entities": [{"text": "summarization", "start_pos": 174, "end_pos": 187, "type": "TASK", "confidence": 0.9796820878982544}]}, {"text": "We show how the system uses an external corpus to incorporate domain knowledge in contrast to text-only statistics.", "labels": [], "entities": []}, {"text": "Finally , we explain how we attempt to increase the co-hesiveness of our summaries by using name aliasing, WordNet synonyms, and morphological variants.", "labels": [], "entities": []}, {"text": "2.1 Defining Single and Multi-word Terms Frequency-based summarization systems typically use a single word string as the unit for counting frequency.", "labels": [], "entities": [{"text": "Defining Single and Multi-word Terms Frequency-based summarization", "start_pos": 4, "end_pos": 70, "type": "TASK", "confidence": 0.6308699548244476}]}, {"text": "Though robust, such a method ignores the semantic content of words and their potential membership in multi-word phrases and may introduce noise in frequency counting by treating the same strings uniformly regardless of context.", "labels": [], "entities": []}, {"text": "Our approach, similar to (Tzoukerman, Klavans, and Jacquemin, 1997), is to apply NLP tools to extract multi-word phrases automatically with high accuracy and use them as the basic unit in the sum-marization process, including frequency calculation.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 145, "end_pos": 153, "type": "METRIC", "confidence": 0.9784042239189148}, {"text": "frequency calculation", "start_pos": 226, "end_pos": 247, "type": "TASK", "confidence": 0.6178333610296249}]}, {"text": "Our system uses both text statistics (term frequency, or t]) and corpus statistics (inverse docmnent frequency , or id]) (Salton and McGill, 1983) to derive signature words as one of the summarization features.", "labels": [], "entities": []}, {"text": "If single words were the sole basis of counting for our summarization application, noise would be 62", "labels": [], "entities": [{"text": "summarization", "start_pos": 56, "end_pos": 69, "type": "TASK", "confidence": 0.9882660508155823}]}], "introductionContent": [{"text": "Frequency-based, knowledge-based (, and discoursebased ( approaches to automated summarization correspond to a continuum of increasing understanding of the text and increasing complexity in text processing.", "labels": [], "entities": []}, {"text": "Given the goal of machine-generated summaries, these approaches attempt to answer three central questions: \u2022 How does the system count words to calculate worthiness for summarization?", "labels": [], "entities": []}, {"text": "\u2022 How does the system incorporate the knowledge of the domain represented in the text?", "labels": [], "entities": []}, {"text": "\u2022 How does the system create a coherent and cohesive summary?", "labels": [], "entities": []}, {"text": "Our work leverages off of research in these three approaches and attempts to remedy some of the difficulties encountered in each by applying a combination of information retrieval, information extraction, \"We would like to thank Jamie Callan for his help with the INQUERY experiments. and NLP techniques and on-line resources with machine learning to generate summaries.", "labels": [], "entities": [{"text": "information retrieval", "start_pos": 158, "end_pos": 179, "type": "TASK", "confidence": 0.7632442116737366}, {"text": "information extraction", "start_pos": 181, "end_pos": 203, "type": "TASK", "confidence": 0.8335407972335815}]}, {"text": "Our DimSum system follows a common paradigm of sentence extraction, but automates acquiring candidate knowledge and learns what knowledge is necessary to summarize.", "labels": [], "entities": [{"text": "sentence extraction", "start_pos": 47, "end_pos": 66, "type": "TASK", "confidence": 0.7482747435569763}]}, {"text": "We present how we automatically acquire candidate features in Section 2.", "labels": [], "entities": []}, {"text": "Section 3 describes our training methodology for combining features to generate summaries, and discusses evaluation results of both batch and machine learning methods.", "labels": [], "entities": []}, {"text": "Section 4 reports our task-based evaluation.", "labels": [], "entities": []}], "datasetContent": [{"text": "We performed two different rounds of experiments, the first with newspaper sets and the second with a broader set from the TREC-5 collection).", "labels": [], "entities": [{"text": "TREC-5 collection", "start_pos": 123, "end_pos": 140, "type": "DATASET", "confidence": 0.9426221251487732}]}, {"text": "In both rounds we experimented with * different feature sets \u2022 different data sources \u2022 the effects of training.", "labels": [], "entities": []}, {"text": "In the first round, we trained our system on 70 texts from the L.A.", "labels": [], "entities": []}, {"text": "Times/Washington Post (latwpdevl) and then tested it against 50 new texts from the L.A.", "labels": [], "entities": [{"text": "Times/Washington Post (latwpdevl)", "start_pos": 0, "end_pos": 33, "type": "DATASET", "confidence": 0.9244190539632525}]}, {"text": "Times/Washington Post (latwp-testl) and 50 texts from the Philadelphia Inquirer (pi-testl).", "labels": [], "entities": [{"text": "Times/Washington Post", "start_pos": 0, "end_pos": 21, "type": "DATASET", "confidence": 0.6902963072061539}]}, {"text": "The results are shown in.", "labels": [], "entities": []}, {"text": "In both cases, we found that the effects of training increased system scores by as much as 10% F-Measure or greater.", "labels": [], "entities": [{"text": "F-Measure", "start_pos": 95, "end_pos": 104, "type": "METRIC", "confidence": 0.9990936517715454}]}, {"text": "Our results are similar to those of Mitra), but our system with the trainable combiner was able to outperform the lead sentence summaries.", "labels": [], "entities": []}, {"text": "summarizes the results of using different training features on the 70 texts from L.A.", "labels": [], "entities": []}, {"text": "Times/Washington Post (latwp-devl).", "labels": [], "entities": [{"text": "Times/Washington Post (latwp-devl).", "start_pos": 0, "end_pos": 35, "type": "DATASET", "confidence": 0.9405630826950073}]}, {"text": "It is evident that positional information is the most valuable.", "labels": [], "entities": []}, {"text": "while the sentence length feature introduces the most noise.", "labels": [], "entities": []}, {"text": "High scoring signature word sentences contribute, especially in conjunction with the positional information and the paragraph feature.", "labels": [], "entities": []}, {"text": "High Score refers to using ant]* idfmetric with WordNet synonyms and name aliases enabled, person names suppressed, but all other name types active.", "labels": [], "entities": [{"text": "High Score", "start_pos": 0, "end_pos": 10, "type": "METRIC", "confidence": 0.7318308651447296}, {"text": "WordNet", "start_pos": 48, "end_pos": 55, "type": "DATASET", "confidence": 0.9252161383628845}]}, {"text": "The second round of experiments were conducted using 100 training and 100 test texts for each of six sources from the the TREC 5 corpora (i.e., Associated Press, Congressional Records, Federal Registry, Financial Times, Wall Street Journal, and Ziff).", "labels": [], "entities": [{"text": "TREC 5 corpora", "start_pos": 122, "end_pos": 136, "type": "DATASET", "confidence": 0.842741588751475}, {"text": "Financial Times, Wall Street Journal", "start_pos": 203, "end_pos": 239, "type": "DATASET", "confidence": 0.8067741890748342}]}, {"text": "Each corpus was trained and tested on a large baseline database created by using multiple text sources.", "labels": [], "entities": []}, {"text": "Results on the test sets are shown in.", "labels": [], "entities": []}, {"text": "The discrepancy in results among data sources suggests that summarization may not be equally viable for all data types.", "labels": [], "entities": [{"text": "summarization", "start_pos": 60, "end_pos": 73, "type": "TASK", "confidence": 0.9802618026733398}]}, {"text": "This squares with results reported in  The goal of our task-based evaluation was to determine whether it was possible to retrieve automatically generated summaries with similar precision to that of retrieving the full texts.", "labels": [], "entities": [{"text": "precision", "start_pos": 177, "end_pos": 186, "type": "METRIC", "confidence": 0.9962408542633057}]}, {"text": "Underpinning this was the intention to examine whether a generic summary could substitute fora full-text document given that a common application for summarization is assumed to be browsing/scanning summarized versions of retrieved documents.", "labels": [], "entities": []}, {"text": "The assumption is that summaries help to accelerate the browsing/scanning without information loss.", "labels": [], "entities": []}, {"text": "described preliminary experiments comparing browsing of original full texts with browsing of dynamically generated abstracts and reported that abstract browsing was about 80% of the original browsing function with precision and recall about the same.", "labels": [], "entities": [{"text": "precision", "start_pos": 214, "end_pos": 223, "type": "METRIC", "confidence": 0.9996222257614136}, {"text": "recall", "start_pos": 228, "end_pos": 234, "type": "METRIC", "confidence": 0.9991288781166077}]}, {"text": "There is also an assumption that summaries, as encapsulated views of texts, may actually improve retrieval effectiveness.", "labels": [], "entities": []}, {"text": "reported that using programmatically generated sulnmaries improved precision significantly, but with a dramatic loss in recall.", "labels": [], "entities": [{"text": "precision", "start_pos": 67, "end_pos": 76, "type": "METRIC", "confidence": 0.9994127750396729}, {"text": "recall", "start_pos": 120, "end_pos": 126, "type": "METRIC", "confidence": 0.9988148212432861}]}, {"text": "We identified 30 TREC-5 topics, classified by the easy/hard retrieval schema of), five as hard, five as easy, and the remaining twenty were randomly selected.", "labels": [], "entities": []}, {"text": "In our evaluation,) retrieved and ranked 50 documents for these 30 TREC-5 topics.", "labels": [], "entities": []}, {"text": "Our summary system summarized these 1500 texts at 10%.reduction, 20%, 30%, and at what our system considers the BEST reduction.", "labels": [], "entities": [{"text": "reduction", "start_pos": 54, "end_pos": 63, "type": "METRIC", "confidence": 0.995628833770752}, {"text": "BEST reduction", "start_pos": 112, "end_pos": 126, "type": "METRIC", "confidence": 0.9829376935958862}]}, {"text": "For each level of reduction, anew index database was built for IN-QUERY, replacing the full texts with summaries.", "labels": [], "entities": [{"text": "IN-QUERY", "start_pos": 63, "end_pos": 71, "type": "DATASET", "confidence": 0.7824554443359375}]}, {"text": "The 30 queries were run against the new database, retrieving 10,000 documents per query.", "labels": [], "entities": []}, {"text": "At this point, some of the summarized versions were dropped as these documents no longer ranked in the 10,000 per topic, as shown in.", "labels": [], "entities": []}, {"text": "For each query, all results except for the documents summarized were thrown away.", "labels": [], "entities": []}, {"text": "New rankings were computed with the remaining summarized documents.", "labels": [], "entities": []}, {"text": "Precision for the INQUERY baseline (INQ.base) was then compared against each level of the reduction.", "labels": [], "entities": [{"text": "Precision", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.9941151142120361}, {"text": "INQUERY baseline (INQ.base)", "start_pos": 18, "end_pos": 45, "type": "METRIC", "confidence": 0.8072675943374634}]}, {"text": "shows that at each level of reduction the overall precision dropped for the summarized versions.", "labels": [], "entities": [{"text": "precision", "start_pos": 50, "end_pos": 59, "type": "METRIC", "confidence": 0.9994648098945618}]}, {"text": "With more reduction, the drop was more dra-: Precision for 5 High Recall Queries matic.", "labels": [], "entities": [{"text": "Precision", "start_pos": 45, "end_pos": 54, "type": "METRIC", "confidence": 0.9970480799674988}]}, {"text": "However, the BEST summary version performed better than the percentage methods.", "labels": [], "entities": [{"text": "BEST", "start_pos": 13, "end_pos": 17, "type": "METRIC", "confidence": 0.9546825885772705}]}, {"text": "We examined in more detail document-level averages for five \"easy\" topics for which the INQUERY system had retrieved a high number of texts.", "labels": [], "entities": [{"text": "INQUERY system", "start_pos": 88, "end_pos": 102, "type": "DATASET", "confidence": 0.8637972474098206}]}, {"text": "Table 7 reveals that for topics with a high INQUERY retrieval rate the precision is comparable.", "labels": [], "entities": [{"text": "INQUERY retrieval rate", "start_pos": 44, "end_pos": 66, "type": "METRIC", "confidence": 0.9316577514012655}, {"text": "precision", "start_pos": 71, "end_pos": 80, "type": "METRIC", "confidence": 0.9994567036628723}]}, {"text": "We posit that when queries have a high number of relevant documents retrieved, the summary system is more likely to reduce information rather than lose information.", "labels": [], "entities": []}, {"text": "Query topics with a high retrieval rate are likely to have documents on the subject matter and therefore the summary just reduces the information, possibly alleviating the browsing/scanning load.", "labels": [], "entities": []}, {"text": "We are currently examining documents lost in the re-ranking process and are cautious in interpreting results because of the difficulty of closely correlating the term selection and ranking algorithms of automatic IR systems with human performance.", "labels": [], "entities": [{"text": "IR", "start_pos": 213, "end_pos": 215, "type": "TASK", "confidence": 0.9359784126281738}]}, {"text": "Our experimental results do indicate, however, that generic summarization is more useful when there are many documents of interest to the user and the user wants to scan summaries and weed out less relevant document quickly.", "labels": [], "entities": [{"text": "generic summarization", "start_pos": 52, "end_pos": 73, "type": "TASK", "confidence": 0.7220586538314819}]}], "tableCaptions": [{"text": " Table 2: Results on Different Test Sets with or with- out Training", "labels": [], "entities": []}, {"text": " Table 3: Effects of Different Training Features", "labels": [], "entities": []}, {"text": " Table 4: Results of Summaries for Different Corpora", "labels": [], "entities": []}, {"text": " Table 7: Precision for 5 High Recall Queries", "labels": [], "entities": [{"text": "Precision", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.8512706160545349}]}]}