{"title": [{"text": "Discovery of Term Variation in Japanese Web Search Queries", "labels": [], "entities": [{"text": "Discovery of Term Variation", "start_pos": 0, "end_pos": 27, "type": "TASK", "confidence": 0.8008523881435394}]}], "abstractContent": [{"text": "In this paper we address the problem of identifying abroad range of term variations in Japa-nese web search queries, where these variations pose a particularly thorny problem due to the multiple character types employed in its writing system.", "labels": [], "entities": [{"text": "Japa-nese web search queries", "start_pos": 87, "end_pos": 115, "type": "DATASET", "confidence": 0.8281717449426651}]}, {"text": "Our method extends the techniques proposed for English spelling correction of web queries to handle a wider range of term variants including spelling mistakes, valid alternative spellings using multiple character types, transliterations and abbreviations.", "labels": [], "entities": [{"text": "English spelling correction of web queries", "start_pos": 47, "end_pos": 89, "type": "TASK", "confidence": 0.8158048788706461}]}, {"text": "The core of our method is a statistical model built on the MART algorithm (Friedman, 2001).", "labels": [], "entities": []}, {"text": "We show that both string and semantic similarity features contribute to identifying term variation in web search queries; specifically , the semantic similarity features used in our system are learned by mining user session and click-through logs, and are useful not only as model features but also in generating term variation candidates efficiently.", "labels": [], "entities": []}, {"text": "The proposed method achieves 70% precision on the term variation identification task with the recall slightly higher than 60%, reducing the error rate of a na\u00efve baseline by 38%.", "labels": [], "entities": [{"text": "precision", "start_pos": 33, "end_pos": 42, "type": "METRIC", "confidence": 0.9994263648986816}, {"text": "term variation identification task", "start_pos": 50, "end_pos": 84, "type": "TASK", "confidence": 0.8490468710660934}, {"text": "recall", "start_pos": 94, "end_pos": 100, "type": "METRIC", "confidence": 0.9992245435714722}, {"text": "error rate", "start_pos": 140, "end_pos": 150, "type": "METRIC", "confidence": 0.9880618155002594}]}], "introductionContent": [{"text": "Identification of term variations is fundamental to many NLP applications: words (or more generally, terms) are the building blocks of NLP applications, and any robust application must be able to handle variations in the surface representation of terms, be it a spelling mistake, valid spelling variation, or abbreviation.", "labels": [], "entities": [{"text": "Identification of term variations", "start_pos": 0, "end_pos": 33, "type": "TASK", "confidence": 0.896597683429718}]}, {"text": "In search applications, term variations can be used for query expansion, which generates additional query terms for better matching with the terms in the document set.", "labels": [], "entities": [{"text": "query expansion", "start_pos": 56, "end_pos": 71, "type": "TASK", "confidence": 0.8523752987384796}]}, {"text": "Identifying term variations is also useful in other scenarios where semantic equivalence of terms is sought, as it represents a very special case of paraphrase.", "labels": [], "entities": [{"text": "Identifying term variations", "start_pos": 0, "end_pos": 27, "type": "TASK", "confidence": 0.813230832417806}]}, {"text": "This paper addresses the problem of identifying term variations in Japanese, specifically for the purpose of query expansion in web search, which appends additional terms to the original query string for better retrieval quality.", "labels": [], "entities": [{"text": "query expansion", "start_pos": 109, "end_pos": 124, "type": "TASK", "confidence": 0.7766017317771912}]}, {"text": "Query expansion has been shown to be effective in improving web search results in English, where different methods of generating the expansion terms have been attempted, including relevance feedback (e.g.,), correction of spelling errors (e.g.,), stemming or lemmatization (e.g.,, use of manually-(e.g., or automatically-(e.g., Rasmussen 1992) constructed thesauri, and Latent Semantic Indexing (e.g.,).", "labels": [], "entities": [{"text": "Query expansion", "start_pos": 0, "end_pos": 15, "type": "TASK", "confidence": 0.7779824435710907}]}, {"text": "Though many of these methods can be applied to Japanese query expansion, there are unique problems posed by Japanese search queries, the most challenging of which is that valid alternative spellings of a word are extremely common due to the multiple script types employed in the language.", "labels": [], "entities": [{"text": "Japanese query expansion", "start_pos": 47, "end_pos": 71, "type": "TASK", "confidence": 0.629673421382904}]}, {"text": "For example, the word for 'protein' can be spelled as \u305f\u3093\u3071\u304f\u3057\u3064, \u30bf\u30f3\u30d1\u30af\u8cea, \u86cb\u767d\u8cea, \u305f\u3093\u767d\u8cea and soon, all pronounced tanpakushitsu but using combinations of different script types.", "labels": [], "entities": []}, {"text": "We give a detailed description of the problem posed by the Japanese writing system in Section 2.", "labels": [], "entities": []}, {"text": "Though there has been previous work on addressing specific subsets of spelling alterations within and across character types in Japanese, there has not been any comprehensive solution for the purpose of query expansion.", "labels": [], "entities": [{"text": "query expansion", "start_pos": 203, "end_pos": 218, "type": "TASK", "confidence": 0.8121402859687805}]}, {"text": "Our approach to Japanese query expansion is unique in that we address the problem comprehensively: our method works independently of the character types used, and targets a wide range of term variations that are both orthographically and semantically similar, including spelling errors, valid alternative spellings, transliterations and abbreviations.", "labels": [], "entities": [{"text": "Japanese query expansion", "start_pos": 16, "end_pos": 40, "type": "TASK", "confidence": 0.7159883777300516}]}, {"text": "As described in Section 4, we define the problem of term variation identifica-tion as a binary classification task, and build two types of classifiers according to the maximum entropy model) and the MART algorithm, where all term similarity metrics are incorporated as features and are jointly optimized.", "labels": [], "entities": [{"text": "MART", "start_pos": 199, "end_pos": 203, "type": "METRIC", "confidence": 0.764319658279419}]}, {"text": "Another important contribution of our approach is that we derive our semantic similarity models by mining user query logs, which has been explored for the purposes of collecting related words (e.g.,), improving search results ranking (e.g., and learning query intention (e.g.,, but not for the task of collecting term variations.", "labels": [], "entities": []}, {"text": "We show that our semantic similarity models are not only effective in the term variation identification task, but also for generating candidates of term variations much more efficiently than the standard method whose candidate generation is based on edit distance metrics.", "labels": [], "entities": [{"text": "term variation identification task", "start_pos": 74, "end_pos": 108, "type": "TASK", "confidence": 0.7307612001895905}]}], "datasetContent": [{"text": "In order to generate the training data for the binary classification task, we randomly sampled the query session (5,712 samples) and clickthrough data (6,228 samples), and manually labeled each pair as positive or negative: the positive label was assigned when the term pair fell into Categories 1 through 5 in; otherwise it was assigned a negative label.", "labels": [], "entities": [{"text": "binary classification task", "start_pos": 47, "end_pos": 73, "type": "TASK", "confidence": 0.7787860035896301}]}, {"text": "Only 364 (6.4%) and 244 (3.9%) of the samples were positive examples for the query session and click-through data respectively, which makes the baseline perString similarity features (16 real-valued features) 1.", "labels": [], "entities": []}, {"text": "Lev distance on surface form 2.", "labels": [], "entities": [{"text": "Lev distance", "start_pos": 0, "end_pos": 12, "type": "METRIC", "confidence": 0.9608440399169922}]}, {"text": "Lev distance on surface form normalized by q1 length 3.", "labels": [], "entities": [{"text": "Lev distance", "start_pos": 0, "end_pos": 12, "type": "METRIC", "confidence": 0.8544594943523407}]}, {"text": "Lev distance on surface form using character equivalence table 4.", "labels": [], "entities": []}, {"text": "Lev distance on surface form normalized by q1 length using character equivalence  formance of the classifier quite high (always predict the negative label -the accuracy will be 95%).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 160, "end_pos": 168, "type": "METRIC", "confidence": 0.999388575553894}]}, {"text": "Note, however, that these data sets include term variation candidates much more efficiently than a candidate set generated by the standard method that uses an edit distance function with a threshold.", "labels": [], "entities": []}, {"text": "For example, there is a querycandidate pair q=\u5bb6\u98a8\u60c5\u5831 kafuujouhou 'housestyle information' c= \u82b1\u7c89\u60c5\u5831 kafunjouhou 'pollen information') in the session data extract, the first one of which is likely to be a misspelling of the second.", "labels": [], "entities": []}, {"text": "If we try to find candidates for the query \u5bb6\u98a8\u60c5\u5831 using an edit distance function naively with a threshold of 2 from the queries in the log, we end up collecting a large amount of completely irrelevant set of candidates such as \u53f0\u98a8\u60c5\u5831 taifuujouhou 'typhoon information', \u682a\u60c5\u5831 kabu jouhou 'stock information', \u964d\u96e8\u60c5\u5831 kouu jouhou 'rainfall information' and soon -as many as 372 candidates were found in the top one million most frequent queries in the query log from the same period; for rarer queries these numbers will only be worse.", "labels": [], "entities": []}, {"text": "Computing the edit distance based on the pronunciation will not help here: the examples above are within the edit distance of 2 even in terms of Romanized strings.", "labels": [], "entities": []}, {"text": "Another advantage of generating the annotated data using the result of query log data mining is that the annotation process is less prone to subjectivity than creating the annotation from scratch.", "labels": [], "entities": []}, {"text": "As point out, the process of manually creating a spelling correction candidate is seriously flawed as the intention of the original query is completely lost: for the query gogle, it is not clear out of context if the user meant goggle, google, or gogle.", "labels": [], "entities": [{"text": "spelling correction candidate", "start_pos": 49, "end_pos": 78, "type": "TASK", "confidence": 0.8222118417421976}]}, {"text": "Using data mined from query logs solves this problem: an annotator can safely assume that if goglegoggle appears in the candidate set, it is very likely to be a valid term variation intended by the user.", "labels": [], "entities": []}, {"text": "This makes the annotation more robust and efficient: the inter-annotator agreement rate for 2,000 query pairs by two annotators was 95.7% on our data set, each annotator spending only about two hours to annotate 2,000 pairs.", "labels": [], "entities": []}], "tableCaptions": []}