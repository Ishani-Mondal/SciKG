{"title": [{"text": "Better Synchronous Binarization for Machine Translation", "labels": [], "entities": [{"text": "Machine Translation", "start_pos": 36, "end_pos": 55, "type": "TASK", "confidence": 0.7926082909107208}]}], "abstractContent": [{"text": "Binarization of Synchronous Context Free Grammars (SCFG) is essential for achieving polynomial time complexity of decoding for SCFG parsing based machine translation systems.", "labels": [], "entities": [{"text": "Binarization of Synchronous Context Free Grammars (SCFG)", "start_pos": 0, "end_pos": 56, "type": "TASK", "confidence": 0.6474414633380042}, {"text": "SCFG parsing based machine translation", "start_pos": 127, "end_pos": 165, "type": "TASK", "confidence": 0.7972406029701233}]}, {"text": "In this paper, we first investigate the excess edge competition issue caused by a left-heavy binary SCFG derived with the method of Zhang et al.", "labels": [], "entities": []}, {"text": "Then we propose anew binarization method to mitigate the problem by exploring other alternative equivalent binary SCFGs.", "labels": [], "entities": []}, {"text": "We present an algorithm that iteratively improves the resulting binary SCFG, and empirically show that our method can improve a string-to-tree statistical machine translations system based on the synchronous bina-rization method in Zhang et al.", "labels": [], "entities": [{"text": "statistical machine translations", "start_pos": 143, "end_pos": 175, "type": "TASK", "confidence": 0.6295235455036163}]}, {"text": "(2006) on the NIST machine translation evaluation tasks.", "labels": [], "entities": [{"text": "NIST machine translation evaluation tasks", "start_pos": 14, "end_pos": 55, "type": "TASK", "confidence": 0.7714400589466095}]}], "introductionContent": [{"text": "Recently Statistical Machine Translation (SMT) systems based on Synchronous Context Free Grammar (SCFG) have been extensively investigated) and have achieved state-of-the-art performance.", "labels": [], "entities": [{"text": "Statistical Machine Translation (SMT)", "start_pos": 9, "end_pos": 46, "type": "TASK", "confidence": 0.852212150891622}]}, {"text": "In these systems, machine translation decoding is cast as asynchronous parsing task.", "labels": [], "entities": [{"text": "machine translation decoding", "start_pos": 18, "end_pos": 46, "type": "TASK", "confidence": 0.8362703522046407}]}, {"text": "Because general SCFG parsing is an NPhard problem), practical SMT decoders based on SCFG parsing requires an equivalent binary SCFG that is directly learned from training data to achieve polynomial time complexity using the CKY algorithm borrowed from CFG parsing techniques.", "labels": [], "entities": [{"text": "SCFG parsing", "start_pos": 16, "end_pos": 28, "type": "TASK", "confidence": 0.7816773355007172}, {"text": "SMT decoders", "start_pos": 62, "end_pos": 74, "type": "TASK", "confidence": 0.950692355632782}, {"text": "CFG parsing", "start_pos": 252, "end_pos": 263, "type": "TASK", "confidence": 0.7256806194782257}]}, {"text": "proposed synchronous binarization, a principled method to binarize an SCFG in such away that both the source-side and target-side virtual non-terminals have contiguous spans.", "labels": [], "entities": []}, {"text": "This property of synchronous binarization guarantees the polynomial time complexity of SCFG parsers even when an n-gram language model is integrated, which has been proved to be one of the keys to the success of a string-to-tree syntax-based SMT system.", "labels": [], "entities": [{"text": "SCFG parsers", "start_pos": 87, "end_pos": 99, "type": "TASK", "confidence": 0.8107223510742188}, {"text": "SMT", "start_pos": 242, "end_pos": 245, "type": "TASK", "confidence": 0.8745181560516357}]}, {"text": "However, as shown by, SCFGbased decoding with an integrated n-gram language model still has a time complexity of \u00ed \u00b5\u00ed\u00bb\u00a9(\u00ed \u00b5\u00ed\u00b1\u009a 3 \u00ed \u00b5\u00ed\u00b1\u0087 4(\u00ed \u00b5\u00ed\u00b1\u009b\u22121) ), where m is the source sentence length, and \u00ed \u00b5\u00ed\u00b1\u0087 is the vocabulary size of the language model.", "labels": [], "entities": []}, {"text": "Although it is not exponential in theory, the actual complexity can still be very high in practice.", "labels": [], "entities": [{"text": "complexity", "start_pos": 53, "end_pos": 63, "type": "METRIC", "confidence": 0.9541147351264954}]}, {"text": "Here is an example extracted from real data.", "labels": [], "entities": []}, {"text": "Given the following SCFG rule: VP \u2192 VB NP \u4f1a JJR , VB NP will be JJR we can obtain a set of equivalent binary rules using the synchronous binarization method () as follows: This binarization is shown with the solid lines as binarization (a) in.", "labels": [], "entities": []}, {"text": "We can see that binarization (a) requires that \"NP \u4f1a\" should be reduced at first.", "labels": [], "entities": []}, {"text": "Data analysis shows that \"NP \u4f1a\" is a frequent pattern in the training corpus, and there are 874 binary rules of which the source language sides are \"NP \u4f1a\".", "labels": [], "entities": []}, {"text": "Consequently these binary rules generate a large number of competing edges in the chart when \"NP \u4f1a\" is matched in decoding.", "labels": [], "entities": []}, {"text": "To reduce the number of edges pro-posed in decoding, hypothesis re-combination is used to combine the equivalent edges in terms of dynamic programming.", "labels": [], "entities": []}, {"text": "Generally, two edges can be re-combined if they satisfy the following two constraints: 1) the LHS (left-hand side) nonterminals are identical and the sub-alignments are the same (); and 2) the boundary words 1 on both sides of the partial translations are equal between the two edges.", "labels": [], "entities": []}, {"text": "However, as shown in, the decoder still generates 801 edges after the hypothesis re-combination.", "labels": [], "entities": []}, {"text": "As a result, aggressive pruning with beam search has to be employed to reduce the search space to make the decoding practical.", "labels": [], "entities": []}, {"text": "Usually in beam search only a very small number of edges are kept in the beam of each chart cell (e.g. less than 100).", "labels": [], "entities": [{"text": "beam search", "start_pos": 11, "end_pos": 22, "type": "TASK", "confidence": 0.9105484485626221}]}, {"text": "These edges have to compete with each other to survive from the pruning.", "labels": [], "entities": []}, {"text": "Obviously, more competing edges proposed during decoding can lead to a higher risk of making search errors.", "labels": [], "entities": []}, {"text": "The edge competition problem for SMT decoding is not addressed in previous work () in which each SCFG rule is binarized in a fixed way.", "labels": [], "entities": [{"text": "SMT decoding", "start_pos": 33, "end_pos": 45, "type": "TASK", "confidence": 0.9578319489955902}]}, {"text": "Actually the results of synchronous binarization may not be the only solution.", "labels": [], "entities": []}, {"text": "As illustrated in, the rule 1 For the case of n-gram language model integration, 2 \u00d7 (\u00ed \u00b5\u00ed\u00b1\u009b \u2212 1) boundary words needs to be examined.", "labels": [], "entities": [{"text": "n-gram language model integration", "start_pos": 46, "end_pos": 79, "type": "TASK", "confidence": 0.6934542804956436}]}, {"text": "can also be binarized as binarization (b) which is shown with the dashed lines.", "labels": [], "entities": []}, {"text": "We think that this problem can be alleviated by choosing better binarizations for SMT decoders, since there is generally more than one binarization fora SCFG rule.", "labels": [], "entities": [{"text": "SMT decoders", "start_pos": 82, "end_pos": 94, "type": "TASK", "confidence": 0.9387357831001282}]}, {"text": "In our investigation, about 96% rules that need to be binarized have more than one binarization under the contiguous constraint.", "labels": [], "entities": []}, {"text": "As shown in binarization (b)), \"\u4f1a JJR\" is reduced first.", "labels": [], "entities": []}, {"text": "In the decoder, the number of binary rules with the source-side \"\u4f1a JJR\" is 62, and the corresponding number of edges is 57).", "labels": [], "entities": []}, {"text": "The two numbers are both much smaller than those of \"NP \u4f1a\" in (a).", "labels": [], "entities": []}, {"text": "This is an informative clue that the binarization (b) could be better than the binarization (a) based on the following: the probability of pruning the rule in (a) is higher than that in (b) as the rule in (b) has fewer competitors and has more chances to survive during pruning.", "labels": [], "entities": []}, {"text": "In this paper we propose a novel binarization method, aiming to find better binarizations to improve an SCFG-based machine translation system.", "labels": [], "entities": [{"text": "SCFG-based machine translation", "start_pos": 104, "end_pos": 134, "type": "TASK", "confidence": 0.6398844718933105}]}, {"text": "We formulate the binarization optimization as a cost reduction process, where the cost is defined as the number of rules sharing a common source-side derivation in an SCFG.", "labels": [], "entities": []}, {"text": "We present an algorithm, iterative cost reduction algorithm, to obtain better binarization for the SCFG learnt automatically from the training corpus.", "labels": [], "entities": []}, {"text": "It can work with an efficient CKY-style binarizer to search for the lowest-cost binarization.", "labels": [], "entities": []}, {"text": "We apply our method into a state-of-the-art string-to-tree SMT system.", "labels": [], "entities": [{"text": "SMT", "start_pos": 59, "end_pos": 62, "type": "TASK", "confidence": 0.9517099857330322}]}, {"text": "The experimental results show that our method outperforms the synchronous binarization method () with over 0.8 BLEU scores on both Chinese-to-English evaluation data sets.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 111, "end_pos": 115, "type": "METRIC", "confidence": 0.9989375472068787}, {"text": "Chinese-to-English evaluation data sets", "start_pos": 131, "end_pos": 170, "type": "DATASET", "confidence": 0.7027778476476669}]}], "datasetContent": [{"text": "The experiments are conducted on Chinese-toEnglish translation in a state-of-the-art string-totree SMT system.", "labels": [], "entities": [{"text": "Chinese-toEnglish translation", "start_pos": 33, "end_pos": 62, "type": "TASK", "confidence": 0.4300435334444046}, {"text": "SMT", "start_pos": 99, "end_pos": 102, "type": "TASK", "confidence": 0.9059910178184509}]}, {"text": "All the results are reported in terms of case-insensitive BLEU4(%).", "labels": [], "entities": [{"text": "BLEU4", "start_pos": 58, "end_pos": 63, "type": "METRIC", "confidence": 0.9845218062400818}]}, {"text": "Our bilingual training corpus consists of about 350K bilingual sentences (9M Chinese words + 10M English words) 2 . Giza++ is employed to perform word alignment on the bilingual sentences.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 146, "end_pos": 160, "type": "TASK", "confidence": 0.7262719869613647}]}, {"text": "The parse trees on the English side are generated using the Berkeley Parser 3 . A 5-gram language model is trained on the English part of LDC bilingual training data and the Xinhua part of Gigaword corpus.", "labels": [], "entities": [{"text": "Berkeley Parser 3", "start_pos": 60, "end_pos": 77, "type": "DATASET", "confidence": 0.8718432784080505}, {"text": "LDC bilingual training data", "start_pos": 138, "end_pos": 165, "type": "DATASET", "confidence": 0.8789456784725189}, {"text": "Gigaword corpus", "start_pos": 189, "end_pos": 204, "type": "DATASET", "confidence": 0.8663971126079559}]}, {"text": "Our development data set comes from NIST2003 evaluation data in which the sentences of more than 20 words are excluded to speedup the Minimum Error Rate Training (MERT).", "labels": [], "entities": [{"text": "NIST2003 evaluation data", "start_pos": 36, "end_pos": 60, "type": "DATASET", "confidence": 0.9011443257331848}, {"text": "Minimum Error Rate Training (MERT)", "start_pos": 134, "end_pos": 168, "type": "METRIC", "confidence": 0.8432025143078395}]}, {"text": "The test data sets are the NIST evaluation sets of 2005 and 2008.", "labels": [], "entities": [{"text": "NIST evaluation sets of 2005", "start_pos": 27, "end_pos": 55, "type": "DATASET", "confidence": 0.9494342565536499}]}, {"text": "Our string-to-tree SMT system is built based on the work of (), where both the minimal GHKM and SPMT rules are extracted from the training corpus, and the composed rules are generated by combining two or three minimal GHKM and SPMT rules.", "labels": [], "entities": [{"text": "SMT", "start_pos": 19, "end_pos": 22, "type": "TASK", "confidence": 0.9288358092308044}]}, {"text": "Before the rule extraction, we also binarize the parse trees on the English side using \"s method to increase the coverage of GHKM and SPMT rules.", "labels": [], "entities": [{"text": "rule extraction", "start_pos": 11, "end_pos": 26, "type": "TASK", "confidence": 0.7695775628089905}, {"text": "GHKM", "start_pos": 125, "end_pos": 129, "type": "DATASET", "confidence": 0.8962844014167786}]}, {"text": "There are totally 4.26M rules after the low frequency rules are filtered out.", "labels": [], "entities": []}, {"text": "The pruning strategy is similar to the cube pruning described in.", "labels": [], "entities": []}, {"text": "To achieve acceptable translation speed, the beam size is set to 50 by default.", "labels": [], "entities": [{"text": "translation", "start_pos": 22, "end_pos": 33, "type": "TASK", "confidence": 0.9579468369483948}]}, {"text": "The baseline system is based on the synchronous binarization ().", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Sub-sequences and corresponding costs", "labels": [], "entities": []}, {"text": " Table 3: Performance (BLUE4(%)) of different  binarization methods. * = significantly better than  baseline (p < 0.05).", "labels": [], "entities": [{"text": "BLUE4", "start_pos": 23, "end_pos": 28, "type": "METRIC", "confidence": 0.9984601736068726}]}, {"text": " Table 2. A possible reason  that the random synchronous binarization me- thod can outperform the baseline method lies in  that compared with binarizing SCFG in a fixed  way, the random synchronous binarization tends  to give a more even distribution of rules among  buckets, which alleviates the problem of edge  competition. However, since the high-frequency  source sub-sequences still have high probabilities  to be generated in the binarization and lead to the excess competing edges, it just achieves a very  small improvement.", "labels": [], "entities": []}]}