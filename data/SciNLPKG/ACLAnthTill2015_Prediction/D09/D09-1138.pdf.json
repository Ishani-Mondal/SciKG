{"title": [{"text": "Supervised Learning of a Probabilistic Lexicon of Verb Semantic Classes", "labels": [], "entities": []}], "abstractContent": [{"text": "The work presented in this paper explores a supervised method for learning a prob-abilistic model of a lexicon of VerbNet classes.", "labels": [], "entities": []}, {"text": "We intend for the probabilis-tic model to provide a probability distribution of verb-class associations, over known and unknown verbs, including pol-ysemous words.", "labels": [], "entities": []}, {"text": "In our approach, training instances are obtained from an existing lexicon and/or from an annotated corpus, while the features, which represent syntactic frames, semantic similarity, and selectional preferences, are extracted from unannotated corpora.", "labels": [], "entities": []}, {"text": "Our model is evaluated in type-level verb classification tasks: we measure the prediction accuracy of VerbNet classes for unknown verbs, and also measure the dissimilarity between the learned and observed probability distributions.", "labels": [], "entities": [{"text": "type-level verb classification", "start_pos": 26, "end_pos": 56, "type": "TASK", "confidence": 0.606487363576889}, {"text": "accuracy", "start_pos": 90, "end_pos": 98, "type": "METRIC", "confidence": 0.9124181270599365}]}, {"text": "We empirically compare several settings for model learning, while we vary the use of features, source corpora for feature extraction, and disam-biguated corpora.", "labels": [], "entities": [{"text": "feature extraction", "start_pos": 114, "end_pos": 132, "type": "TASK", "confidence": 0.7112971991300583}]}, {"text": "In the task of verb classification into all VerbNet classes, our best model achieved a 10.69% error reduction in the classification accuracy, over the previously proposed model.", "labels": [], "entities": [{"text": "verb classification", "start_pos": 15, "end_pos": 34, "type": "TASK", "confidence": 0.7518714666366577}, {"text": "error", "start_pos": 94, "end_pos": 99, "type": "METRIC", "confidence": 0.9783686399459839}, {"text": "accuracy", "start_pos": 132, "end_pos": 140, "type": "METRIC", "confidence": 0.9755462408065796}]}], "introductionContent": [{"text": "Lexicons are invaluable resources for semantic processing.", "labels": [], "entities": [{"text": "semantic processing", "start_pos": 38, "end_pos": 57, "type": "TASK", "confidence": 0.8687141239643097}]}, {"text": "In many cases, lexicons are necessary to restrict a set of semantic classes to be assigned to a word.", "labels": [], "entities": []}, {"text": "In fact, a considerable number of works on semantic processing implicitly or explicitly presupposes the availability of a lexicon, such as in word sense disambiguation (WSD)), and in token-level verb class disambiguation (.", "labels": [], "entities": [{"text": "word sense disambiguation (WSD))", "start_pos": 142, "end_pos": 174, "type": "TASK", "confidence": 0.7702474892139435}, {"text": "token-level verb class disambiguation", "start_pos": 183, "end_pos": 220, "type": "TASK", "confidence": 0.6044931635260582}]}, {"text": "In other words, those methods are heavily dependent on the availability of a semantic lexicon.", "labels": [], "entities": []}, {"text": "Therefore, recent research efforts have invested in developing semantic resources, such as WordNet), FrameNet (, and VerbNet (), which greatly advanced research in semantic processing.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 91, "end_pos": 98, "type": "DATASET", "confidence": 0.954496443271637}]}, {"text": "However, the construction of such resources is expensive, and it is unrealistic to presuppose the availability of full-coverage lexicons; this is the case because unknown words always appear in real texts, and word-semantics associations may vary (.", "labels": [], "entities": []}, {"text": "This paper explores a method for the supervised learning of a probabilistic model for the VerbNet lexicon.", "labels": [], "entities": [{"text": "VerbNet lexicon", "start_pos": 90, "end_pos": 105, "type": "DATASET", "confidence": 0.9403887093067169}]}, {"text": "We target the automatic classification of arbitrary verbs, including polysemous verbs, into all VerbNet classes; further, we target the estimation of a probabilistic model, which represents the saliences of verb-class associations for polysemous verbs.", "labels": [], "entities": []}, {"text": "In our approach, an existing lexicon and/or an annotated corpus are used as the training data.", "labels": [], "entities": []}, {"text": "Since VerbNet classes are designed to represent the distinctions in the syntactic frames that verbs can take, features, representing the statistics of syntactic frames, are extracted from the unannotated corpora.", "labels": [], "entities": []}, {"text": "Additionally, as the classes represent semantic commonalities, semantically inspired features, like distributionally similar words, are used.", "labels": [], "entities": []}, {"text": "These features can be considered as a generalized representation of verbs, and we expect that the obtained probabilistic model predicts VerbNet classes of the unknown words.", "labels": [], "entities": []}, {"text": "Our model is evaluated in two tasks of typelevel verb classification: one is the classification of monosemous verbs into a small subset of the classes, which was studied in some previous works.", "labels": [], "entities": [{"text": "typelevel verb classification", "start_pos": 39, "end_pos": 68, "type": "TASK", "confidence": 0.7827564080556234}]}, {"text": "The other task is the classification of all verbs into the full set of VerbNet classes, which has not yet been attempted.", "labels": [], "entities": []}, {"text": "In the experiments, training instances are obtained from VerbNet and/or SemLink (, while features are extracted from the British National Corpus or from Wall Street Journal.", "labels": [], "entities": [{"text": "VerbNet", "start_pos": 57, "end_pos": 64, "type": "DATASET", "confidence": 0.9493156671524048}, {"text": "British National Corpus", "start_pos": 121, "end_pos": 144, "type": "DATASET", "confidence": 0.9499731262524923}, {"text": "Wall Street Journal", "start_pos": 153, "end_pos": 172, "type": "DATASET", "confidence": 0.9020961125691732}]}, {"text": "We empirically compare several settings for model learning by varying the set of features, the source domain and the size of a corpus for feature extraction, and the use of the token-level statistics obtained from a manually disambiguated corpus.", "labels": [], "entities": [{"text": "feature extraction", "start_pos": 138, "end_pos": 156, "type": "TASK", "confidence": 0.739641085267067}]}, {"text": "We also provide the analysis of the remaining errors, which will lead us to further improve the supervised learning of a probabilistic semantic lexicon.", "labels": [], "entities": []}, {"text": "Supervised methods for automatic verb classification have been extensively investigated).", "labels": [], "entities": [{"text": "automatic verb classification", "start_pos": 23, "end_pos": 52, "type": "TASK", "confidence": 0.6160186926523844}]}, {"text": "However, their focus has been limited to a small subset of verb classes, and a limited number of monosemous verbs.", "labels": [], "entities": []}, {"text": "The main contributions of the present work are: i) to provide empirical results for the automatic classification of all verbs, including polysemous ones, into all VerbNet classes, and ii) to empirically explore the effective settings for the supervised learning of a probabilistic lexicon of verb semantic classes.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this work, we empirically compare several settings for the learning of the above probabilistic model, in the two tasks of automatic verb classification.", "labels": [], "entities": [{"text": "automatic verb classification", "start_pos": 125, "end_pos": 154, "type": "TASK", "confidence": 0.6375280916690826}]}, {"text": "In what follows, we explain the training/test data, corpora for extracting features, and the design of the features and evaluation tasks.", "labels": [], "entities": []}, {"text": "The measures for evaluation are also introduced.", "labels": [], "entities": []}, {"text": "For the 14-class task, we simply measure the classification accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 60, "end_pos": 68, "type": "METRIC", "confidence": 0.9377890825271606}]}, {"text": "However, the evaluation in the all-class task is not trivial, because verbs maybe assigned multiple classes.", "labels": [], "entities": []}, {"text": "Since our purpose is to obtain a probabilistic model rather than to classify monosemous verbs, the evaluation criterion should be sensitive to the probabilistic distribution on the test data.", "labels": [], "entities": []}, {"text": "In this paper, we adopt two evaluation measures.", "labels": [], "entities": []}, {"text": "One is the top-N weighted accuracy; we count the number of correct pairs c, v in the N -best outputs from the model (where N is the number of gold classes for each lemma), where each count is weighted by the relative frequency (i.e., the counts in SemLink) of the pair in the test set.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 26, "end_pos": 34, "type": "METRIC", "confidence": 0.9756912589073181}]}, {"text": "For example, in the case for \"blare\" in, if the model states that Sound Emission has the largest probability, we get 0.7 points.", "labels": [], "entities": []}, {"text": "If Manner Speaking has the largest probability, we instead obtain 0.3 points.", "labels": [], "entities": [{"text": "Manner Speaking", "start_pos": 3, "end_pos": 18, "type": "DATASET", "confidence": 0.81842041015625}]}, {"text": "Intuitively, the score is higher when the model presents larger probabilities to classes with higher relative frequencies.", "labels": [], "entities": []}, {"text": "This measure is similar to the top-N precision in information retrieval; it evaluates the ranked output by the model.", "labels": [], "entities": [{"text": "information retrieval", "start_pos": 50, "end_pos": 71, "type": "TASK", "confidence": 0.7567400634288788}]}, {"text": "It is intuitively interpretable, but is insufficient for evaluating the quality of probability distributions.", "labels": [], "entities": []}, {"text": "The other measure is KL-divergence, which is popularly used for measuring the dissimilarity between two probability distributions.", "labels": [], "entities": [{"text": "KL-divergence", "start_pos": 21, "end_pos": 34, "type": "METRIC", "confidence": 0.7570253610610962}]}, {"text": "This is defined as follows: In the experiments, this measure is applied, with the assumption that p is the relative frequency of c, v in the test set, and that q is the estimated probability distribution.", "labels": [], "entities": []}, {"text": "Although the KLdivergence is not a true distance metric, it is sufficient for measuring the fitting of the estimated model to the true distribution.", "labels": [], "entities": []}, {"text": "We report the KL-divergence averaged overall verbs in the test set.", "labels": [], "entities": []}, {"text": "Since this measure indicates a dissimilarity, a smaller value is better.", "labels": [], "entities": []}, {"text": "When p and q are equivalent, KL(p||q) = 0.", "labels": [], "entities": [{"text": "KL", "start_pos": 29, "end_pos": 31, "type": "METRIC", "confidence": 0.9345188736915588}]}, {"text": "shows the accuracy obtained for the 14-class task.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9997046589851379}]}, {"text": "The first column denotes the incorporated features (\"Joanis et al.'s features\" or \"All features\"), and the sources of the features (\"WSJ\" or \"BNC\").", "labels": [], "entities": [{"text": "Joanis et al.'s features", "start_pos": 53, "end_pos": 77, "type": "DATASET", "confidence": 0.8376610974470774}, {"text": "WSJ", "start_pos": 133, "end_pos": 136, "type": "DATASET", "confidence": 0.8066185116767883}]}, {"text": "The two baseline results are also given: \"Baseline (random)\" indicates that classes are randomly output, and \"Baseline (majority)\" indicates: Accuracy and KL-divergence for the allclass task (the VerbNet+SemLink setting) that the majority class (i.e., the class that has the largest number of member verbs) is output to every lemma.", "labels": [], "entities": [{"text": "KL-divergence", "start_pos": 155, "end_pos": 168, "type": "METRIC", "confidence": 0.9852928519248962}]}, {"text": "While these figures cannot be compared directly to the previous works due to the difference in the preprocessing, Joanis et al. achieved 58.4% accuracy for the 14-class task. and 4 present the results for the all-class task.", "labels": [], "entities": [{"text": "preprocessing", "start_pos": 99, "end_pos": 112, "type": "METRIC", "confidence": 0.9615234136581421}, {"text": "accuracy", "start_pos": 143, "end_pos": 151, "type": "METRIC", "confidence": 0.9994407296180725}]}, {"text": "gives the accuracy and KL-divergence achieved by the model trained with the VerbNet+SemLink training instances, while presents the same measures by the training instances created from VerbNet only.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9995952248573303}, {"text": "KL-divergence", "start_pos": 23, "end_pos": 36, "type": "METRIC", "confidence": 0.9944165945053101}]}, {"text": "Our models performed substantially better on both tasks than the baseline models.", "labels": [], "entities": []}, {"text": "The results also proved that the features we proposed in this paper contributed to the further improvement of the model from.", "labels": [], "entities": []}, {"text": "In the all-class task with the VerbNet+SemLink setting, our features achieved 10.69% error reduction in the accuracy over's features.", "labels": [], "entities": [{"text": "VerbNet+SemLink setting", "start_pos": 31, "end_pos": 54, "type": "DATASET", "confidence": 0.8758161962032318}, {"text": "error reduction", "start_pos": 85, "end_pos": 100, "type": "METRIC", "confidence": 0.9825834929943085}, {"text": "accuracy", "start_pos": 108, "end_pos": 116, "type": "METRIC", "confidence": 0.9977796673774719}]}, {"text": "Another interesting fact is that the model with BNC consistently outperformed the model with WSJ.", "labels": [], "entities": [{"text": "BNC", "start_pos": 48, "end_pos": 51, "type": "METRIC", "confidence": 0.7002051472663879}, {"text": "WSJ", "start_pos": 93, "end_pos": 96, "type": "DATASET", "confidence": 0.9198530316352844}]}, {"text": "This outcome is somewhat surprising, provided that the relative frequencies in the training/test sets are created from the WSJ portion of SemLink.", "labels": [], "entities": [{"text": "WSJ", "start_pos": 123, "end_pos": 126, "type": "DATASET", "confidence": 0.9145177602767944}, {"text": "SemLink", "start_pos": 138, "end_pos": 145, "type": "DATASET", "confidence": 0.5058766603469849}]}, {"text": "The reason for this is independent of the corpus size, as will be shown below.", "labels": [], "entities": []}, {"text": "When comparing and 4, we can see that using SemLink statistics resulted in a slightly better model.", "labels": [], "entities": []}, {"text": "This result is predictable, because the evaluation measures are sensitive to the relative frequencies estimated from SemLink.", "labels": [], "entities": [{"text": "SemLink", "start_pos": 117, "end_pos": 124, "type": "DATASET", "confidence": 0.8488066792488098}]}, {"text": "However, the difference remained small.", "labels": [], "entities": []}, {"text": "In both of the tasks and the evaluation measures, the best model was achieved when we use   all the features extracted from BNC, and create training instances from VerbNet+SemLink. and 7 plot the accuracy and KLdivergence against the size of the unannotated corpus used for feature extraction.", "labels": [], "entities": [{"text": "BNC", "start_pos": 124, "end_pos": 127, "type": "DATASET", "confidence": 0.8694184422492981}, {"text": "accuracy", "start_pos": 196, "end_pos": 204, "type": "METRIC", "confidence": 0.9994914531707764}, {"text": "KLdivergence", "start_pos": 209, "end_pos": 221, "type": "METRIC", "confidence": 0.9687647819519043}, {"text": "feature extraction", "start_pos": 274, "end_pos": 292, "type": "TASK", "confidence": 0.7371086776256561}]}, {"text": "The result clearly indicates that the learning curve still grows at the corpus size with 100 million words (especially for the all features + BNC setting), which indicates that better models are obtained by increasing the size of the unannotated corpora.", "labels": [], "entities": [{"text": "BNC", "start_pos": 142, "end_pos": 145, "type": "METRIC", "confidence": 0.8892827033996582}]}, {"text": "Therefore, we can claim that the differences between the domains and the size of the unannotated corpora are more influential than the availability of the annotated corpora.", "labels": [], "entities": []}, {"text": "This indicates that learning only from a lexicon would be a viable solution, when a token-disambiguated corpus like SemLink is unavailable.", "labels": [], "entities": []}, {"text": "shows the contribution of each feature group.", "labels": [], "entities": []}, {"text": "BNC is used for feature extraction, and VerbNet+SemLink is used for the creation of training instances.", "labels": [], "entities": [{"text": "BNC", "start_pos": 0, "end_pos": 3, "type": "METRIC", "confidence": 0.46259015798568726}, {"text": "feature extraction", "start_pos": 16, "end_pos": 34, "type": "TASK", "confidence": 0.768494576215744}]}, {"text": "The results demonstrated the effectiveness of the slot POS features, and in particular, for the all-class task, most likely because VerbNet covers verbs that take non-nominal arguments.", "labels": [], "entities": []}, {"text": "Additionally, the similar word features contributed equally or more in both of the tasks.", "labels": [], "entities": []}, {"text": "This result suggests that we were reasonable in hypothesizing that distributionally similar words tend to be clas-: Contribution of features sified into the same class.", "labels": [], "entities": []}, {"text": "Slot classes also contributed to a slight improvement, indicating that selectional preferences are effective clues for predicting VerbNet classes.", "labels": [], "entities": [{"text": "predicting VerbNet classes", "start_pos": 119, "end_pos": 145, "type": "TASK", "confidence": 0.6566967964172363}]}, {"text": "The result of the \"All features\" model for the all-class task attests that these features worked collaboratively, and using them all resulted in a considerably better model.", "labels": [], "entities": []}, {"text": "From the analysis of the confusion matrix for the outputs by our best model, we identified several reasons for the remaining misclassification errors.", "labels": [], "entities": []}, {"text": "A major portion of the errors were caused by confusing the classes that take the same prepositions.", "labels": [], "entities": []}, {"text": "Examples of these errors include: \u2022 Other Change of State verbs were misclassified into the Butter class: \"embalm,\" \"laminate.\"", "labels": [], "entities": [{"text": "Butter class", "start_pos": 92, "end_pos": 104, "type": "DATASET", "confidence": 0.9002628922462463}]}, {"text": "(they take \"with\" phrases) \u2022 Judgement verbs were misclassified into the Characterize class: \"acclaim,\" \"hail.\"", "labels": [], "entities": [{"text": "Judgement verbs", "start_pos": 29, "end_pos": 44, "type": "TASK", "confidence": 0.9259781241416931}]}, {"text": "(they take \"as\" phrases) Since prepositions are strong features for automatic verb classification (, the classes that take the same prepositions remained confusing.", "labels": [], "entities": [{"text": "automatic verb classification", "start_pos": 68, "end_pos": 97, "type": "TASK", "confidence": 0.636022279659907}]}, {"text": "The discovery of the features to discriminate among these classes would be crucial for further improvement.", "labels": [], "entities": []}, {"text": "Another major error is in classifying verbs into Other Change of State.", "labels": [], "entities": []}, {"text": "Examples include: \u2022 Amuse verbs: \"impair,\" \"recharge.\"", "labels": [], "entities": []}, {"text": "\u2022 Herd verbs: \"aggregate,\" \"mass.\"", "labels": [], "entities": []}, {"text": "Because Other Change of State is one of the biggest classes, supervised learning tends to place a high probability to this class.", "labels": [], "entities": []}, {"text": "Therefore, when strong clues do not exist, verbs tend to be misclassified into this class.", "labels": [], "entities": []}, {"text": "In addition, this class is not syntactically/semantically homogeneous, and is likely to introduce noise in the machine learning classifier.", "labels": [], "entities": []}, {"text": "A possible solution to this problem would be to exclude this class from the classification, and to process the class separately.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: 14 classes used in Joanis et al. (2008) and  their corresponding Levin class numbers", "labels": [], "entities": []}, {"text": " Table 2: Accuracy for the 14-class task", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.99893718957901}]}, {"text": " Table 3: Accuracy and KL-divergence for the all- class task (the VerbNet+SemLink setting)", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9982181191444397}, {"text": "KL-divergence", "start_pos": 23, "end_pos": 36, "type": "METRIC", "confidence": 0.9755278825759888}, {"text": "VerbNet+SemLink setting", "start_pos": 66, "end_pos": 89, "type": "DATASET", "confidence": 0.8162602782249451}]}, {"text": " Table 4: Accuracy and KL-divergence for the all- class task (the VerbNet only setting)", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.998813271522522}, {"text": "KL-divergence", "start_pos": 23, "end_pos": 36, "type": "METRIC", "confidence": 0.9878457188606262}]}, {"text": " Table 5: Contribution of features", "labels": [], "entities": []}]}