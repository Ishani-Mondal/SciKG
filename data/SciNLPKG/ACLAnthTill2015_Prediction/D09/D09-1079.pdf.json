{"title": [{"text": "Stream-based Randomised Language Models for SMT", "labels": [], "entities": [{"text": "SMT", "start_pos": 44, "end_pos": 47, "type": "TASK", "confidence": 0.9912168979644775}]}], "abstractContent": [{"text": "Randomised techniques allow very big language models to be represented succinctly.", "labels": [], "entities": []}, {"text": "However, being batch-based they are unsuitable for modelling an unbounded stream of language whilst maintaining a constant error rate.", "labels": [], "entities": []}, {"text": "We present a novel randomised language model which uses an online perfect hash function to efficiently deal with unbounded text streams.", "labels": [], "entities": []}, {"text": "Translation experiments over a text stream show that our online ran-domised model matches the performance of batch-based LMs without incurring the computational overhead associated with full retraining.", "labels": [], "entities": []}, {"text": "This opens up the possibility of randomised language models which continuously adapt to the massive volumes of texts published on the Web each day.", "labels": [], "entities": []}], "introductionContent": [{"text": "Language models (LM) are an integral feature of statistical machine translation (SMT) systems.", "labels": [], "entities": [{"text": "statistical machine translation (SMT)", "start_pos": 48, "end_pos": 85, "type": "TASK", "confidence": 0.7867898643016815}]}, {"text": "They assign probabilities to generated hypotheses in the target language informing lexical selection.", "labels": [], "entities": []}, {"text": "The most common form of LMs in SMT systems are smoothed n-gram models which predict a word based on a contextual history of n \u2212 1 words.", "labels": [], "entities": [{"text": "SMT", "start_pos": 31, "end_pos": 34, "type": "TASK", "confidence": 0.9853958487510681}]}, {"text": "For some languages (such as English) trillions of words are available for training purposes.", "labels": [], "entities": []}, {"text": "This fact, along with the observation that machine translation quality improves as the amount of monolingual training material increases, has lead to the introduction of randomised techniques for representing large LMs in small space.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 43, "end_pos": 62, "type": "TASK", "confidence": 0.8002945184707642}]}, {"text": "Randomised LMs (RLMs) solve the problem of representing large, static LMs but they are batch oriented and cannot incorporate new data without fully retraining from scratch.", "labels": [], "entities": [{"text": "Randomised LMs (RLMs", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.6900746002793312}]}, {"text": "This property makes current RLMs ill-suited for modelling the massive volume of textual material published daily on the Web.", "labels": [], "entities": [{"text": "RLMs", "start_pos": 28, "end_pos": 32, "type": "TASK", "confidence": 0.9432870149612427}]}, {"text": "We present a novel RLM which is capable of incremental (re)training.", "labels": [], "entities": [{"text": "RLM", "start_pos": 19, "end_pos": 22, "type": "TASK", "confidence": 0.8570005297660828}]}, {"text": "We use random hash functions coupled with an online perfect hashing algorithm to represent n-grams in small space.", "labels": [], "entities": []}, {"text": "This makes it well-suited for dealing with an unbounded stream of training material.", "labels": [], "entities": []}, {"text": "To our knowledge this is the first stream-based RLM reported in the machine translation literature.", "labels": [], "entities": [{"text": "RLM", "start_pos": 48, "end_pos": 51, "type": "TASK", "confidence": 0.9226529002189636}, {"text": "machine translation", "start_pos": 68, "end_pos": 87, "type": "TASK", "confidence": 0.7195459604263306}]}, {"text": "As well as introducing the basic stream-based RLM, we also consider adaptation strategies.", "labels": [], "entities": [{"text": "RLM", "start_pos": 46, "end_pos": 49, "type": "TASK", "confidence": 0.9134020805358887}]}, {"text": "Perplexity and translation results show that populating the language model with material chronologically close to test points yields good results.", "labels": [], "entities": [{"text": "translation", "start_pos": 15, "end_pos": 26, "type": "TASK", "confidence": 0.9469060897827148}]}, {"text": "As with previous randomised language models, our experiments focus on machine translation but we also expect that our findings are general and should help inform the design of other stream-based models.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 70, "end_pos": 89, "type": "TASK", "confidence": 0.7771182656288147}]}, {"text": "Section 2 introduces the incrementally retrainable randomised LM and section 3 considers related work; Section 4 then considers the question of how unbounded text streams should be modelled.", "labels": [], "entities": []}, {"text": "Sections 5 and 6 show stream-based translation results and properties of our novel datastructure.", "labels": [], "entities": []}, {"text": "Section 7 concludes the paper.", "labels": [], "entities": []}], "datasetContent": [{"text": "We used publicly available resources for all our tests: for decoding we used Moses () and our parallel data was taken from the Spanish-English section of Europarl.", "labels": [], "entities": [{"text": "Spanish-English section of Europarl", "start_pos": 127, "end_pos": 162, "type": "DATASET", "confidence": 0.649579644203186}]}, {"text": "For test material, we translated 63 documents (800 sentences) from three randomly selected dates spaced throughout the RCV1 year (January 2nd,.", "labels": [], "entities": [{"text": "RCV1 year", "start_pos": 119, "end_pos": 128, "type": "DATASET", "confidence": 0.9650792181491852}]}, {"text": "1 This effectively divided the stream into three epochs between the test dates ( table 2).", "labels": [], "entities": []}, {"text": "We held out 300 sentences for minimum error rate training (MERT) and optimised the parameters of the feature functions of the decoder for each experimental run.", "labels": [], "entities": [{"text": "minimum error rate training (MERT", "start_pos": 30, "end_pos": 63, "type": "METRIC", "confidence": 0.8211329480012258}]}, {"text": "The RCV1 is not a large corpus when compared to the entire web but it is multilingual, chronological, and large enough to enable us to test the effect of recency in a translation setting.", "labels": [], "entities": [{"text": "RCV1", "start_pos": 4, "end_pos": 8, "type": "DATASET", "confidence": 0.940526008605957}]}], "tableCaptions": [{"text": " Table 1: Example false postive rates and corre- sponding memory usage for all randomised LMs.", "labels": [], "entities": []}, {"text": " Table 2: The stream timeline is divided into win- dowed epochs for our recency experiments.", "labels": [], "entities": []}, {"text": " Table 3: Distinct n-grams (in millions) encoun- tered in the full stream and example epochs.", "labels": [], "entities": []}, {"text": " Table 4: Baseline translation results in BLEU us- ing data from the first stream epoch with a lossless  LM (4.5GB RAM), the TB-LM and the O-RLM  (300MB RAM). All LMs are static.", "labels": [], "entities": [{"text": "Baseline translation", "start_pos": 10, "end_pos": 30, "type": "TASK", "confidence": 0.7976506948471069}, {"text": "BLEU", "start_pos": 42, "end_pos": 46, "type": "METRIC", "confidence": 0.9934379458427429}]}, {"text": " Table 5: Translation results for stream-based LMs in BLEU. Performance degrades with time using the  Naive approach. The batch retrained TB-LM and stream-based O-RLM use constant error rates of 1", "labels": [], "entities": [{"text": "Translation", "start_pos": 10, "end_pos": 21, "type": "TASK", "confidence": 0.9649447202682495}, {"text": "BLEU", "start_pos": 54, "end_pos": 58, "type": "METRIC", "confidence": 0.7234880924224854}]}]}