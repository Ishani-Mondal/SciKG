{"title": [{"text": "Fast Translation Rule Matching for Syntax-based Statistical Machine Translation", "labels": [], "entities": [{"text": "Fast Translation Rule Matching", "start_pos": 0, "end_pos": 30, "type": "TASK", "confidence": 0.6743417903780937}, {"text": "Syntax-based Statistical Machine Translation", "start_pos": 35, "end_pos": 79, "type": "TASK", "confidence": 0.7102616280317307}]}], "abstractContent": [{"text": "Ina linguistically-motivated syntax-based translation system, the entire translation process is normally carried out in two steps, translation rule matching and target sentence decoding using the matched rules.", "labels": [], "entities": [{"text": "translation rule matching", "start_pos": 131, "end_pos": 156, "type": "TASK", "confidence": 0.7413447101910909}]}, {"text": "Both steps are very time-consuming due to the tremendous number of translation rules, the exhaustive search in translation rule matching and the complex nature of the translation task itself.", "labels": [], "entities": [{"text": "translation rule matching", "start_pos": 111, "end_pos": 136, "type": "TASK", "confidence": 0.8004459142684937}]}, {"text": "In this paper, we propose a hyper-tree-based fast algorithm for translation rule matching.", "labels": [], "entities": [{"text": "translation rule matching", "start_pos": 64, "end_pos": 89, "type": "TASK", "confidence": 0.9383980433146158}]}, {"text": "Experimental results on the NIST MT-2003 Chinese-English translation task show that our algorithm is at least 19 times faster in rule matching and is able to help to save 57% of overall translation time over previous methods when using large fragment translation rules.", "labels": [], "entities": [{"text": "NIST MT-2003 Chinese-English translation task", "start_pos": 28, "end_pos": 73, "type": "TASK", "confidence": 0.7908946752548218}, {"text": "rule matching", "start_pos": 129, "end_pos": 142, "type": "TASK", "confidence": 0.7581118047237396}]}], "introductionContent": [{"text": "Recently linguistically-motivated syntax-based translation method has achieved great success in statistical machine translation (SMT) (.", "labels": [], "entities": [{"text": "linguistically-motivated syntax-based translation", "start_pos": 9, "end_pos": 58, "type": "TASK", "confidence": 0.603789895772934}, {"text": "statistical machine translation (SMT)", "start_pos": 96, "end_pos": 133, "type": "TASK", "confidence": 0.7855987697839737}]}, {"text": "It translates a source sentence to its target one in two steps by using structured translation rules.", "labels": [], "entities": []}, {"text": "In the first step, which is called translation rule matching step, all the applicable 1 translation rules are extracted from the entire rule set by matching the source parse tree/forest.", "labels": [], "entities": [{"text": "translation rule matching", "start_pos": 35, "end_pos": 60, "type": "TASK", "confidence": 0.8418841361999512}]}, {"text": "The second step is to decode the source sentence into its target one using the extracted translation rules.", "labels": [], "entities": []}, {"text": "Both of the two steps are very time-consuming due to the exponential number of translation rules and the complex nature of machine translation as an NP-hard search problem).", "labels": [], "entities": [{"text": "machine translation", "start_pos": 123, "end_pos": 142, "type": "TASK", "confidence": 0.7419719994068146}]}, {"text": "In the SMT research community, the second step has been well studied and many methods have been proposed to speedup the decoding process, such as node-based or span-based beam search with different pruning strategies () and cube pruning.", "labels": [], "entities": [{"text": "SMT", "start_pos": 7, "end_pos": 10, "type": "TASK", "confidence": 0.9942519664764404}]}, {"text": "However, the first step attracts less attention.", "labels": [], "entities": []}, {"text": "The previous solution to this problem is to do exhaustive searching with heuristics on each tree/forest node or on each source span.", "labels": [], "entities": []}, {"text": "This solution becomes computationally infeasible when it is applied to packed forests with loose pruning threshold or rule sets with large tree fragments of large rule height and width.", "labels": [], "entities": []}, {"text": "This not only overloads the translation process but also compromises the translation performance since as shown in our experiments the large tree fragment rules are also very useful.", "labels": [], "entities": [{"text": "translation", "start_pos": 28, "end_pos": 39, "type": "TASK", "confidence": 0.9657256603240967}]}, {"text": "To solve the above issue, in this paper, we propose a hyper-tree-based fast algorithm for translation rule matching.", "labels": [], "entities": [{"text": "translation rule matching", "start_pos": 90, "end_pos": 115, "type": "TASK", "confidence": 0.9396058519681295}]}, {"text": "Our solution includes two steps.", "labels": [], "entities": []}, {"text": "In the first step, all the translation rules are re-organized using our proposed hyper-tree structure, which is a compact representation of the entire translation rule set, in order to make the common parts of translation rules shared as much as possible.", "labels": [], "entities": []}, {"text": "This enables the common parts of different translation rules to be visited only once in rule matching.", "labels": [], "entities": [{"text": "rule matching", "start_pos": 88, "end_pos": 101, "type": "TASK", "confidence": 0.750283271074295}]}, {"text": "Please note that the first step can be easily done off-line very fast.", "labels": [], "entities": []}, {"text": "As a result, it does not consume real translation time.", "labels": [], "entities": []}, {"text": "In the second step, we design a recursive algorithm to traverse the hyper-tree structure and the input source forest in a top-down manner to do the rule matching between them.", "labels": [], "entities": [{"text": "rule matching", "start_pos": 148, "end_pos": 161, "type": "TASK", "confidence": 0.6819960922002792}]}, {"text": "As we will show later, the hyper-tree structure and the recursive algorithm significantly improve the speed of the rule matching and the entire translation process compared with previous methods.", "labels": [], "entities": [{"text": "rule matching", "start_pos": 115, "end_pos": 128, "type": "TASK", "confidence": 0.7833796441555023}]}, {"text": "With the proposed algorithm, we are able to carryout experiments with very loose pruning thresholds and larger tree fragment rules efficiently.", "labels": [], "entities": []}, {"text": "Experimental results on the NIST MT-2003 Chinese-English translation task shows that our algorithm is 19 times faster in rule matching and is able to save 57% of overall translation time over previous methods when using large fragment translation rules with height up to 5.", "labels": [], "entities": [{"text": "NIST MT-2003 Chinese-English translation task", "start_pos": 28, "end_pos": 73, "type": "TASK", "confidence": 0.7967806339263916}, {"text": "rule matching", "start_pos": 121, "end_pos": 134, "type": "TASK", "confidence": 0.7628546059131622}]}, {"text": "It also shows that the larger rules with height of up to 5 significantly outperforms the rules with height of up to 3 by around 1 BLEU score.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 130, "end_pos": 134, "type": "METRIC", "confidence": 0.9994001388549805}]}, {"text": "The rest of this paper is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 introduces the syntax-based translation system that we are working on.", "labels": [], "entities": [{"text": "syntax-based translation", "start_pos": 25, "end_pos": 49, "type": "TASK", "confidence": 0.5759829580783844}]}, {"text": "Section 3 reviews the previous work.", "labels": [], "entities": []}, {"text": "Section 4 explains our solution while section 5 reports the experimental results.", "labels": [], "entities": []}, {"text": "Section 6 concludes the paper.", "labels": [], "entities": []}], "datasetContent": [{"text": "We carryout experiment on Chinese-English NIST evaluation tasks.", "labels": [], "entities": [{"text": "NIST evaluation tasks", "start_pos": 42, "end_pos": 63, "type": "TASK", "confidence": 0.5946078697840372}]}, {"text": "We use FBIS corpus (250K sentence pairs) as training data with the source side parsed by a modified Charniak parser (Charniak 2000) which can output a packed forest.", "labels": [], "entities": [{"text": "FBIS corpus", "start_pos": 7, "end_pos": 18, "type": "DATASET", "confidence": 0.7529831230640411}]}, {"text": "The Charniak Parser is trained on CTB5, tuned on 301-325 portion, with F1 score of 80.85% on 271-300 portion.", "labels": [], "entities": [{"text": "Charniak Parser", "start_pos": 4, "end_pos": 19, "type": "DATASET", "confidence": 0.8340190351009369}, {"text": "CTB5", "start_pos": 34, "end_pos": 38, "type": "DATASET", "confidence": 0.9646187424659729}, {"text": "F1 score", "start_pos": 71, "end_pos": 79, "type": "METRIC", "confidence": 0.9743028283119202}]}, {"text": "We use GIZA++ ( to do m-to-n word-alignment and adopt heuristic \"grow-diag-final-and\" to do refinement.", "labels": [], "entities": []}, {"text": "A 4-gram language model is trained on Gigaword 3 Xinhua portion by SRILM toolkit) with Kneser-Ney smoothing.", "labels": [], "entities": [{"text": "Gigaword 3 Xinhua", "start_pos": 38, "end_pos": 55, "type": "DATASET", "confidence": 0.8393186330795288}, {"text": "SRILM toolkit", "start_pos": 67, "end_pos": 80, "type": "DATASET", "confidence": 0.8904727697372437}]}, {"text": "We use NIST 2002 as development set and NIST 2003 as test set.", "labels": [], "entities": [{"text": "NIST 2002", "start_pos": 7, "end_pos": 16, "type": "DATASET", "confidence": 0.9480186998844147}, {"text": "NIST 2003", "start_pos": 40, "end_pos": 49, "type": "DATASET", "confidence": 0.9290493428707123}]}, {"text": "The feature weights are tuned by the modified Koehn's MER trainer.", "labels": [], "entities": [{"text": "Koehn's MER trainer", "start_pos": 46, "end_pos": 65, "type": "DATASET", "confidence": 0.8461547493934631}]}, {"text": "We use case-sensitive BLEU-4 () to measure the quality of translation result.'s implementation is used to do significant test.", "labels": [], "entities": [{"text": "BLEU-4", "start_pos": 22, "end_pos": 28, "type": "METRIC", "confidence": 0.984708309173584}]}, {"text": "Following , we use viterbi algorithm to prune the forest.", "labels": [], "entities": []}, {"text": "Instead of using a static pruning threshold , we set the threshold as the distance of the probabilities of then th best tree and the 1 st best tree.", "labels": [], "entities": []}, {"text": "It means the pruned forest is able to at least keep all the top n best trees.", "labels": [], "entities": []}, {"text": "However, because of the sharing nature of the packed forest, it may still contain a large number of additional trees.", "labels": [], "entities": []}, {"text": "Our statistic shows that when we set the threshold as the 100 th best tree, the average number of all possible trees in the forest is 1.2*10 5 after pruning.", "labels": [], "entities": []}, {"text": "In our experiments, we compare our algorithm with the two traditional algorithms as discussed in section 3.", "labels": [], "entities": []}, {"text": "For the \"Exhaustive search by tree\" algorithm, we use a bottom-up dynamic programming algorithm to generate all the candidate tree fragments rooted at each node.", "labels": [], "entities": []}, {"text": "For the \"Exhaustive search by rule\" algorithm, we group all rules with the same left hand side in order to remove the duplicated matching for the same left hand side rules.", "labels": [], "entities": []}, {"text": "All these settings aim for fair comparison.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1. Speed in seconds per sentence vs. rule  height; \"H\" is rule height, \"D\" represents the de- coding time after rule matching", "labels": [], "entities": [{"text": "Speed", "start_pos": 10, "end_pos": 15, "type": "METRIC", "confidence": 0.9881250858306885}]}, {"text": " Table 2. BLEU vs. rule height", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9993071556091309}]}, {"text": " Table 3. statistics of rules used in the 1-best trans- lation output, \"F\" means full-lexicalized, \"P\"  means partial-lexicalized, \"U\" means unlexiclaizd.", "labels": [], "entities": []}, {"text": " Table 4. Speed in seconds per sentence vs. for- est pruning threshold", "labels": [], "entities": [{"text": "Speed", "start_pos": 10, "end_pos": 15, "type": "METRIC", "confidence": 0.9967599511146545}]}, {"text": " Table 6. Statistics of rule set and hyper-tree. \"H\"  is rule height, \"n_rules\" is the number of rules,  \"n_LHS\" is the number of unique left hand side,  \"n_nodes\" is the number of hyper-nodes in hyper- tree and \"c_rate\" is the compression rate.", "labels": [], "entities": []}]}