{"title": [{"text": "Fast, Cheap, and Creative: Evaluating Translation Quality Using Amazon's Mechanical Turk", "labels": [], "entities": [{"text": "Translation Quality", "start_pos": 38, "end_pos": 57, "type": "TASK", "confidence": 0.7792308926582336}]}], "abstractContent": [{"text": "Manual evaluation of translation quality is generally thought to be excessively time consuming and expensive.", "labels": [], "entities": [{"text": "translation", "start_pos": 21, "end_pos": 32, "type": "TASK", "confidence": 0.9236619472503662}]}, {"text": "We explore a fast and inexpensive way of doing it using Amazon's Mechanical Turk to pay small sums to a large number of non-expert an-notators.", "labels": [], "entities": [{"text": "Amazon's Mechanical Turk", "start_pos": 56, "end_pos": 80, "type": "DATASET", "confidence": 0.9358320385217667}]}, {"text": "For $10 we redundantly recreate judgments from a WMT08 translation task.", "labels": [], "entities": [{"text": "WMT08 translation task", "start_pos": 49, "end_pos": 71, "type": "TASK", "confidence": 0.7838608821233114}]}, {"text": "We find that when combined non-expert judgments have a high-level of agreement with the existing gold-standard judgments of machine translation quality, and correlate more strongly with expert judgments than Bleu does.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 124, "end_pos": 143, "type": "TASK", "confidence": 0.7618432939052582}]}, {"text": "We goon to show that Mechanical Turk can be used to calculate human-mediated translation edit rate (HTER), to conduct reading comprehension experiments with machine translation , and to create high quality reference translations.", "labels": [], "entities": [{"text": "calculate human-mediated translation edit rate (HTER)", "start_pos": 52, "end_pos": 105, "type": "METRIC", "confidence": 0.7278362810611725}, {"text": "machine translation", "start_pos": 157, "end_pos": 176, "type": "TASK", "confidence": 0.7109962552785873}]}], "introductionContent": [{"text": "Conventional wisdom holds that manual evaluation of machine translation is too time-consuming and expensive to conduct.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 52, "end_pos": 71, "type": "TASK", "confidence": 0.7442541122436523}]}, {"text": "Instead, researchers routinely use automatic metrics like Bleu () as the sole evidence of improvement to translation quality.", "labels": [], "entities": [{"text": "Bleu", "start_pos": 58, "end_pos": 62, "type": "METRIC", "confidence": 0.9729704856872559}]}, {"text": "Automatic metrics have been criticized fora variety of reasons (, and it is clear that they only loosely approximate human judgments.", "labels": [], "entities": []}, {"text": "Therefore, having people evaluate translation output would be preferable, if it were more practical.", "labels": [], "entities": [{"text": "translation output", "start_pos": 34, "end_pos": 52, "type": "TASK", "confidence": 0.8324066698551178}]}, {"text": "In this paper we demonstrate that the manual evaluation of translation quality is not as expensive or as time consuming as generally thought.", "labels": [], "entities": [{"text": "translation", "start_pos": 59, "end_pos": 70, "type": "TASK", "confidence": 0.9321349859237671}]}, {"text": "We use Amazon's Mechanical Turk, an online labor market that is designed to pay people small sums of money to complete human intelligence teststasks that are difficult for computers but easy for people.", "labels": [], "entities": []}, {"text": "We show that: \u2022 Non-expert annotators produce judgments that are very similar to experts and that have a stronger correlation than Bleu.", "labels": [], "entities": []}, {"text": "\u2022 Mechanical Turk can be used for complex tasks like human-mediated translation edit rate (HTER) and creating multiple reference translations.", "labels": [], "entities": [{"text": "human-mediated translation edit rate (HTER)", "start_pos": 53, "end_pos": 96, "type": "TASK", "confidence": 0.6254062354564667}]}, {"text": "\u2022 Evaluating translation quality through reading comprehension, which is rarely done, can be easily accomplished through creative use of Mechanical Turk.", "labels": [], "entities": [{"text": "translation", "start_pos": 13, "end_pos": 24, "type": "TASK", "confidence": 0.7554694414138794}]}, {"text": "examined the accuracy of labels created using Mechanical Turk fora variety of natural language processing tasks.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 13, "end_pos": 21, "type": "METRIC", "confidence": 0.998456597328186}]}, {"text": "These tasks included word sense disambiguation, word similarity, textual entailment, and temporal ordering of events, but not machine translation.", "labels": [], "entities": [{"text": "word sense disambiguation", "start_pos": 21, "end_pos": 46, "type": "TASK", "confidence": 0.769376814365387}, {"text": "word similarity", "start_pos": 48, "end_pos": 63, "type": "TASK", "confidence": 0.7656556963920593}, {"text": "textual entailment", "start_pos": 65, "end_pos": 83, "type": "TASK", "confidence": 0.6798478215932846}, {"text": "machine translation", "start_pos": 126, "end_pos": 145, "type": "TASK", "confidence": 0.7594809234142303}]}, {"text": "Snow et al. measured the quality of non-expert annotations by comparing them against labels that had been previously created by expert annotators.", "labels": [], "entities": []}, {"text": "They report inter-annotator agreement between expert and non-expert annotators, and show that the average of many non-experts converges on performance of a single expert for many of their tasks.", "labels": [], "entities": []}, {"text": "Although it is not common for manual evaluation results to be reported in conference papers, several large-scale manual evaluations of machine translation quality take place annually.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 135, "end_pos": 154, "type": "TASK", "confidence": 0.801379919052124}]}, {"text": "These include public forums like the NIST MT Evaluation Workshop, IWSLT and WMT, as well as the project-specific Go/No Go evaluations for the DARPA GALE program.", "labels": [], "entities": [{"text": "NIST MT Evaluation Workshop", "start_pos": 37, "end_pos": 64, "type": "DATASET", "confidence": 0.7332565486431122}, {"text": "IWSLT", "start_pos": 66, "end_pos": 71, "type": "DATASET", "confidence": 0.761824369430542}, {"text": "WMT", "start_pos": 76, "end_pos": 79, "type": "DATASET", "confidence": 0.7334991693496704}, {"text": "DARPA GALE", "start_pos": 142, "end_pos": 152, "type": "TASK", "confidence": 0.42773860692977905}]}, {"text": "Various types of human judgments are used.", "labels": [], "entities": []}, {"text": "NIST collects 5-point fluency and adequacy scores), IWSLT and WMT collect relative rankings), and DARPA evaluates using HTER).", "labels": [], "entities": [{"text": "NIST", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.9558706283569336}, {"text": "IWSLT", "start_pos": 52, "end_pos": 57, "type": "METRIC", "confidence": 0.543319582939148}, {"text": "WMT", "start_pos": 62, "end_pos": 65, "type": "DATASET", "confidence": 0.6453787088394165}]}, {"text": "The details of these are provided later in the paper.", "labels": [], "entities": []}, {"text": "Public evaluation campaigns provide a ready source of goldstandard data that non-expert annotations can be compared to.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section we report on a number of creative uses of Mechanical Turk to do more sophisticated tasks.", "labels": [], "entities": []}, {"text": "We give evidence that Turkers can create high quality translations for some languages, which would make creating multiple reference translations for Bleu less costly than using professional translators.", "labels": [], "entities": [{"text": "Bleu", "start_pos": 149, "end_pos": 153, "type": "DATASET", "confidence": 0.973704993724823}]}, {"text": "We report on experiments evaluating translation quality with HTER and with reading comprehension tests.", "labels": [], "entities": [{"text": "translation", "start_pos": 36, "end_pos": 47, "type": "TASK", "confidence": 0.9773533344268799}]}], "tableCaptions": [{"text": " Table 2: HTER scores for five MT systems. The  edit rate decreases as the number of editors in- creases from zero (where HTER is simply the TER  score between the MT output and the reference  translation) and five.", "labels": [], "entities": [{"text": "MT", "start_pos": 31, "end_pos": 33, "type": "TASK", "confidence": 0.9752457141876221}, {"text": "edit rate", "start_pos": 48, "end_pos": 57, "type": "METRIC", "confidence": 0.9349136650562286}, {"text": "HTER", "start_pos": 122, "end_pos": 126, "type": "METRIC", "confidence": 0.8930971622467041}, {"text": "TER  score", "start_pos": 141, "end_pos": 151, "type": "METRIC", "confidence": 0.9758471846580505}]}]}