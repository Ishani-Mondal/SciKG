{"title": [{"text": "Recognizing Implicit Discourse Relations in the Penn Discourse Treebank", "labels": [], "entities": [{"text": "Recognizing Implicit Discourse Relations", "start_pos": 0, "end_pos": 40, "type": "TASK", "confidence": 0.8938169330358505}, {"text": "Penn Discourse Treebank", "start_pos": 48, "end_pos": 71, "type": "DATASET", "confidence": 0.9814256230990092}]}], "abstractContent": [{"text": "We present an implicit discourse relation classifier in the Penn Discourse Treebank (PDTB).", "labels": [], "entities": [{"text": "Penn Discourse Treebank (PDTB)", "start_pos": 60, "end_pos": 90, "type": "DATASET", "confidence": 0.9624620874722799}]}, {"text": "Our classifier considers the context of the two arguments, word pair information , as well as the arguments' internal constituent and dependency parses.", "labels": [], "entities": []}, {"text": "Our results on the PDTB yields a significant 14.1% improvement over the baseline.", "labels": [], "entities": [{"text": "PDTB", "start_pos": 19, "end_pos": 23, "type": "DATASET", "confidence": 0.8599272966384888}]}, {"text": "In our error analysis, we discuss four challenges in recognizing implicit relations in the PDTB.", "labels": [], "entities": []}], "introductionContent": [{"text": "In the field of discourse modeling, it is widely agreed that text is not understood in isolation, but in relation to its context.", "labels": [], "entities": [{"text": "discourse modeling", "start_pos": 16, "end_pos": 34, "type": "TASK", "confidence": 0.726592019200325}]}, {"text": "One focus in the study of discourse is to identify and label the relations between textual units (clauses, sentences, or paragraphs).", "labels": [], "entities": []}, {"text": "Such research can enable downstream natural language processing (NLP) such as summarization, question answering, and textual entailment.", "labels": [], "entities": [{"text": "downstream natural language processing (NLP)", "start_pos": 25, "end_pos": 69, "type": "TASK", "confidence": 0.7673200113432748}, {"text": "summarization", "start_pos": 78, "end_pos": 91, "type": "TASK", "confidence": 0.9829246401786804}, {"text": "question answering", "start_pos": 93, "end_pos": 111, "type": "TASK", "confidence": 0.8690997958183289}, {"text": "textual entailment", "start_pos": 117, "end_pos": 135, "type": "TASK", "confidence": 0.7008802592754364}]}, {"text": "For example, recognizing causal relations can assist in answering why questions.", "labels": [], "entities": []}, {"text": "Detecting contrast and restatements is useful for paraphrasing and summarization systems.", "labels": [], "entities": [{"text": "Detecting contrast", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.8995736241340637}, {"text": "summarization", "start_pos": 67, "end_pos": 80, "type": "TASK", "confidence": 0.9773913621902466}]}, {"text": "While different discourse frameworks have been proposed from different perspectives (), most admit these basic types of discourse relationships between textual units.", "labels": [], "entities": []}, {"text": "When there is a discourse connective (e.g., because) between two text spans, it is often easy to recognize the relation between the spans, as most connectives are unambiguous).", "labels": [], "entities": []}, {"text": "On the other hand, it is difficult to recognize the discourse relations when there are no explicit textual cues.", "labels": [], "entities": []}, {"text": "We term these cases explicit and implicit relations, respectively.", "labels": [], "entities": []}, {"text": "While the recognition of discourse structure has been studied in the context of explicit relations) in the past, little published work has yet attempted to recognize implicit discourse relations between text spans.", "labels": [], "entities": [{"text": "recognition of discourse structure", "start_pos": 10, "end_pos": 44, "type": "TASK", "confidence": 0.8009219914674759}]}, {"text": "Detecting implicit relations is a critical step in forming a discourse understanding of text, as many text spans do not mark their discourse relations with explicit cues.", "labels": [], "entities": []}, {"text": "Recently, the Penn Discourse Treebank (PDTB) has been released, which features discourse level annotation on both explicit and implicit relations.", "labels": [], "entities": [{"text": "Penn Discourse Treebank (PDTB)", "start_pos": 14, "end_pos": 44, "type": "DATASET", "confidence": 0.9440879126389822}]}, {"text": "It provides a valuable linguistic resource towards understanding discourse relations and a common platform for researchers to develop discourse-centric systems.", "labels": [], "entities": []}, {"text": "With the recent release of the second version of this corpus (, which provides a cleaner and more thorough implicit relation annotation, there is an opportunity to address this area of work.", "labels": [], "entities": []}, {"text": "In this paper, we provide classification of implicit discourse relations on the second version of the PDTB.", "labels": [], "entities": [{"text": "PDTB", "start_pos": 102, "end_pos": 106, "type": "DATASET", "confidence": 0.8972717523574829}]}, {"text": "The features we used include contextual modeling of relation dependencies, features extracted from constituent parse trees and dependency parse trees, and word pair features.", "labels": [], "entities": []}, {"text": "We show an accuracy of 40.2%, which is a significant improvement of 14.1% over the majority baseline.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 11, "end_pos": 19, "type": "METRIC", "confidence": 0.9998323917388916}]}, {"text": "After reviewing related work, we first give an overview of the Penn Discourse Treebank.", "labels": [], "entities": [{"text": "Penn Discourse Treebank", "start_pos": 63, "end_pos": 86, "type": "DATASET", "confidence": 0.992740273475647}]}, {"text": "We then describe our classification methodology, followed by experimental results.", "labels": [], "entities": []}, {"text": "We give a detailed discussion on the difficulties of implicit relation classification in the PDTB, and then conclude the paper.", "labels": [], "entities": [{"text": "relation classification", "start_pos": 62, "end_pos": 85, "type": "TASK", "confidence": 0.7038239240646362}, {"text": "PDTB", "start_pos": 93, "end_pos": 97, "type": "DATASET", "confidence": 0.8545920252799988}]}], "datasetContent": [{"text": "We experimented with a maximum entropy classifier from the OpenNLP MaxEnt package using various combinations of features to assess their efficacy.", "labels": [], "entities": [{"text": "OpenNLP MaxEnt package", "start_pos": 59, "end_pos": 81, "type": "DATASET", "confidence": 0.9273674885431925}]}, {"text": "We used PDTB Sections 2 -21 as our training set and Section 23 as the test set, and only used the implicit discourse relations.", "labels": [], "entities": [{"text": "PDTB Sections 2", "start_pos": 8, "end_pos": 23, "type": "DATASET", "confidence": 0.8434188365936279}]}, {"text": "In the PDTB, about 2.2% of the implicit relations are annotated with two types, as shown in Example 7 in Section 6.", "labels": [], "entities": []}, {"text": "During training, a relation that is annotated with two types is considered as two training instances, each with one of the types.", "labels": [], "entities": []}, {"text": "During testing, such a relation is considered one test instance, and if the classifier assigns either of the two types, we consider it as correct.", "labels": [], "entities": []}, {"text": "Thus, the test accuracy is calculated as the number of correctly classified test instances divided by the total number of test instances.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 15, "end_pos": 23, "type": "METRIC", "confidence": 0.9710317254066467}]}, {"text": "In our work, we use the majority class as the baseline, where all instances are classified as Cause.", "labels": [], "entities": []}, {"text": "This yields an accuracy of 26.1% on the test set.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 15, "end_pos": 23, "type": "METRIC", "confidence": 0.9997031092643738}]}, {"text": "A random baseline yields an even lower accuracy of 9.1% on the test set.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 39, "end_pos": 47, "type": "METRIC", "confidence": 0.999055802822113}]}], "tableCaptions": [{"text": " Table 1: Distribution of Level 2 relation types of  implicit relations from the training sections (Sec.  2 -21). The last two columns show the initial  distribution and the distribution after removing the  five types that have only a few training instances.", "labels": [], "entities": []}, {"text": " Table 3: Classification accuracy with all features  from each feature class. Rows 1 to 4: individual  feature class; Row 5: all feature classes.", "labels": [], "entities": [{"text": "Classification", "start_pos": 10, "end_pos": 24, "type": "TASK", "confidence": 0.9434986710548401}, {"text": "accuracy", "start_pos": 25, "end_pos": 33, "type": "METRIC", "confidence": 0.9817306995391846}]}, {"text": " Table 4:  Classification accuracy with top  rules/word pairs for each feature class. Rows 1  to 4: individual feature class; Row 5: all feature  classes.", "labels": [], "entities": [{"text": "Classification", "start_pos": 11, "end_pos": 25, "type": "TASK", "confidence": 0.9546555280685425}, {"text": "accuracy", "start_pos": 26, "end_pos": 34, "type": "METRIC", "confidence": 0.9774353504180908}]}, {"text": " Table 5. These results  confirm that each additional feature class indeed  contributes a marginal performance improvement,  (although it is not significant) and that all feature  classes are needed for optimal performance.", "labels": [], "entities": []}, {"text": " Table 5: Accuracy with feature classes gradually  added in the order of their predictiveness.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9944888353347778}]}, {"text": " Table 3 Row 3 with ours from Table 5  Row 4, which shows that our system significantly  (p < 0.01) outperforms theirs.", "labels": [], "entities": []}, {"text": " Table 6: Recall, precision, F 1 , and counts for 11  Level 2 relation types. \"-\" indicates 0.00.", "labels": [], "entities": [{"text": "Recall", "start_pos": 10, "end_pos": 16, "type": "METRIC", "confidence": 0.9948422312736511}, {"text": "precision", "start_pos": 18, "end_pos": 27, "type": "METRIC", "confidence": 0.9994021654129028}, {"text": "F 1", "start_pos": 29, "end_pos": 32, "type": "METRIC", "confidence": 0.9953915476799011}, {"text": "counts", "start_pos": 39, "end_pos": 45, "type": "METRIC", "confidence": 0.9922405481338501}]}]}