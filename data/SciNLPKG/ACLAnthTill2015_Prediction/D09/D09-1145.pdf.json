{"title": [{"text": "Detecting Speculations and their Scopes in Scientific Text", "labels": [], "entities": [{"text": "Detecting Speculations and their Scopes", "start_pos": 0, "end_pos": 39, "type": "TASK", "confidence": 0.9218477249145508}]}], "abstractContent": [{"text": "Distinguishing speculative statements from factual ones is important for most biomedical text mining applications.", "labels": [], "entities": [{"text": "Distinguishing speculative statements", "start_pos": 0, "end_pos": 37, "type": "TASK", "confidence": 0.8825016617774963}, {"text": "biomedical text mining", "start_pos": 78, "end_pos": 100, "type": "TASK", "confidence": 0.6173377335071564}]}, {"text": "We introduce an approach which is based on solving two sub-problems to identify speculative sentence fragments.", "labels": [], "entities": []}, {"text": "The first sub-problem is identifying the speculation keywords in the sentences and the second one is resolving their linguistic scopes.", "labels": [], "entities": []}, {"text": "We formulate the first sub-problem as a supervised classification task, where we classify the potential keywords as real speculation keywords or not by using a diverse set of linguistic features that represent the contexts of the keywords.", "labels": [], "entities": []}, {"text": "After detecting the actual speculation keywords, we use the syntactic structures of the sentences to determine their scopes.", "labels": [], "entities": []}], "introductionContent": [{"text": "Speculation, also known as hedging, is a frequently used language phenomenon in scientific articles, especially in experimental studies, which are common in the biomedical domain.", "labels": [], "entities": []}, {"text": "When researchers are not completely certain about the inferred conclusions, they use speculative language to convey this uncertainty.", "labels": [], "entities": []}, {"text": "Consider the following example sentences from abstracts of articles in the biomedical domain.", "labels": [], "entities": []}, {"text": "The abstracts are available at the U.S. National Library of Medicine PubMed web page . The PubMed Identifier (PMID) of the corresponding article is given in parenthesis.", "labels": [], "entities": [{"text": "National Library of Medicine PubMed web", "start_pos": 40, "end_pos": 79, "type": "DATASET", "confidence": 0.9126084446907043}, {"text": "PubMed Identifier (PMID)", "start_pos": 91, "end_pos": 115, "type": "METRIC", "confidence": 0.7925870358943939}]}, {"text": "1. We showed that the Roaz protein bound specifically to O/E-1 by using the yeast two-hybrid system.", "labels": [], "entities": []}, {"text": "(PMID: 9151733) 2.", "labels": [], "entities": [{"text": "PMID: 9151733)", "start_pos": 1, "end_pos": 15, "type": "METRIC", "confidence": 0.8092479556798935}]}, {"text": "These data suggest that p56lck is physically associated with Fc gamma RIIIA (CD16) and functions to mediate The first sentence is definite, whereas the second one contains speculative information, which is conveyed by the use of the word \"suggest\".", "labels": [], "entities": [{"text": "Fc gamma RIIIA (CD16", "start_pos": 61, "end_pos": 81, "type": "DATASET", "confidence": 0.5689442038536072}]}, {"text": "While speculative information might still be useful for biomedical scientists, it is important that it is distinguished from the factual information.", "labels": [], "entities": []}, {"text": "Recognizing speculations in scientific text has gained interest in the recent years.", "labels": [], "entities": [{"text": "Recognizing speculations in scientific text", "start_pos": 0, "end_pos": 43, "type": "TASK", "confidence": 0.8679286122322083}]}, {"text": "Previous studies focus on identifying speculative sentences (.", "labels": [], "entities": []}, {"text": "However, in many cases, not the entire sentence, but fragments of a sentence are speculative.", "labels": [], "entities": []}, {"text": "Consider the following example sentences.", "labels": [], "entities": []}, {"text": "1. The mature mitochondrial forms of the erythroid and housekeeping ALAS isozymes are predicted to have molecular weights of 59.5 kd and 64.6 kd, respectively.", "labels": [], "entities": []}, {"text": "(PMID: 2050125) 2.", "labels": [], "entities": [{"text": "PMID: 2050125)", "start_pos": 1, "end_pos": 15, "type": "METRIC", "confidence": 0.823929637670517}]}, {"text": "Like RAD9, RAD9B associates with HUS1, RAD1, and RAD17, suggesting that it is a RAD9 paralog that engages in similar biochemical reactions.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluated our approach on two different types of scientific text from the biomedical domain, namely the scientific abstracts sub-corpus and the full text articles sub-corpus of the BioScope corpus (see Section 3).", "labels": [], "entities": [{"text": "BioScope corpus", "start_pos": 184, "end_pos": 199, "type": "DATASET", "confidence": 0.8493851721286774}]}, {"text": "We used stratified 10-fold cross-validation to evaluate the performance on the abstracts.", "labels": [], "entities": []}, {"text": "In each fold, 90% of the abstracts are used for training and 10% are used to test.", "labels": [], "entities": []}, {"text": "To facilitate comparison with future studies the PubMed Identifiers of the abstracts that we used as a test set in each fold are provided . The full text papers sub-corpus consists of nine articles.", "labels": [], "entities": []}, {"text": "We used leave-one-out cross-validation to evaluate the per-3 http://belobog.si.umich.edu/clair/bioscope/ formance on the full text papers.", "labels": [], "entities": []}, {"text": "In each iteration eight articles are used for training and one article is used to test.", "labels": [], "entities": []}, {"text": "We report the average results over the runs for each data set.", "labels": [], "entities": []}, {"text": "To classify whether the occurrence of a keyword is in speculative context or not, we built linear SVM models by using various combinations of the features introduced in Section 4.1.", "labels": [], "entities": []}, {"text": "summarize the results obtained for the abstracts and the full text papers, respectively.", "labels": [], "entities": []}, {"text": "BOW N is the bag-of-words features obtained from the words surrounding the keyword (see Section 4.1.3).", "labels": [], "entities": [{"text": "BOW N", "start_pos": 0, "end_pos": 5, "type": "METRIC", "confidence": 0.9363245666027069}]}, {"text": "N is the window size.", "labels": [], "entities": []}, {"text": "We experimented both with the stemmed and non-stemmed versions of this feature type.", "labels": [], "entities": []}, {"text": "The non-stemmed versions performed slightly better than the stemmed versions.", "labels": [], "entities": []}, {"text": "The reason might be due to the different likelihoods of being used in a speculative context of different inflected forms of words.", "labels": [], "entities": []}, {"text": "For example, consider the words \"appears\" and \"appearance\".", "labels": [], "entities": []}, {"text": "They have the same stems, but \"appearance\" is less likely to be areal speculation keyword than \"appears\".", "labels": [], "entities": []}, {"text": "Another observation is that, decreasing the window size led to improvement in performance.", "labels": [], "entities": []}, {"text": "This suggests that the words right before and right after the candidate speculation keyword are more effective in distinguishing its speculative vs. non-speculative context compared to a wider local context.", "labels": [], "entities": []}, {"text": "Wider local context might create sparse data and degrade performance.", "labels": [], "entities": []}, {"text": "Consider the example, \"it appears that TP53 interacts with AR\".", "labels": [], "entities": []}, {"text": "The keyword \"appears\", and BOW1 (\"it\" and \"that\") are more relevant for the speculative context of the keyword than \"TP53\", \"interacts\", and \"with\".", "labels": [], "entities": [{"text": "BOW1", "start_pos": 27, "end_pos": 31, "type": "METRIC", "confidence": 0.9985223412513733}]}, {"text": "Therefore, for the rest of the experiments we used the BOW 1 version, i.e., the non-stemmed surrounding bagof-words with window size of 1.", "labels": [], "entities": [{"text": "BOW 1 version", "start_pos": 55, "end_pos": 68, "type": "DATASET", "confidence": 0.7143888672192892}]}, {"text": "KW stands for the keyword specific features, i.e., the keyword, its stem, and its part-of-speech (discussed in Section 4.1.1).", "labels": [], "entities": []}, {"text": "DEP stands for the dependency relation features (discussed in Section 4.1.2).", "labels": [], "entities": []}, {"text": "POS stands for the positional features (discussed in Section 4.1.4) and CO-KW stands for the co-occurring keywords feature (discussed in Section 4.1.5).", "labels": [], "entities": [{"text": "POS", "start_pos": 0, "end_pos": 3, "type": "METRIC", "confidence": 0.8479686379432678}]}, {"text": "Our results are not directly comparable with the prior studies about identifying speculative sentences (see Section 2), since we attempted to solve a different problem, which is identifying speculative parts of sentences.", "labels": [], "entities": []}, {"text": "Only the substring matching approach that was introduced in () could be adapted as a keyword classification task, since the substrings are keywords themselves and we used this approach as a baseline in the keyword classification sub-problem.", "labels": [], "entities": [{"text": "keyword classification task", "start_pos": 85, "end_pos": 112, "type": "TASK", "confidence": 0.8097918828328451}, {"text": "keyword classification", "start_pos": 206, "end_pos": 228, "type": "TASK", "confidence": 0.8235430717468262}]}, {"text": "We compare the performances of our models with two baseline methods, which are based on the substring matching approach.", "labels": [], "entities": []}, {"text": "have shown that the substring matching method with a predefined set of 14 strings performs slightly better than an SVM model with bag-of-words features in classifying sentences as speculative vs. non-speculative (see Section 2).", "labels": [], "entities": [{"text": "substring matching", "start_pos": 20, "end_pos": 38, "type": "TASK", "confidence": 0.6955020427703857}]}, {"text": "In baseline 1, we use the 14 strings identified in () and classify all the keywords in the test set that match any of them as real speculation keywords.", "labels": [], "entities": []}, {"text": "Baseline 2 is similar to baseline 1, with the difference that rather than using the set of strings in (), we extract the set of keywords from the training set and classify all the words (or phrases) in the test set that match any of the keywords in the list as real speculation keywords.", "labels": [], "entities": []}, {"text": "Baseline 1 achieves high precision, but low recall.", "labels": [], "entities": [{"text": "precision", "start_pos": 25, "end_pos": 34, "type": "METRIC", "confidence": 0.999354898929596}, {"text": "recall", "start_pos": 44, "end_pos": 50, "type": "METRIC", "confidence": 0.9994097948074341}]}, {"text": "Whereas, baseline 2 achieves high recall in the expense of low precision.", "labels": [], "entities": [{"text": "recall", "start_pos": 34, "end_pos": 40, "type": "METRIC", "confidence": 0.9996579885482788}, {"text": "precision", "start_pos": 63, "end_pos": 72, "type": "METRIC", "confidence": 0.9990212917327881}]}, {"text": "All the SVM models in achieve more balanced precision and recall values, with F-measure values significantly higher than the baseline methods.", "labels": [], "entities": [{"text": "precision", "start_pos": 44, "end_pos": 53, "type": "METRIC", "confidence": 0.9975258708000183}, {"text": "recall", "start_pos": 58, "end_pos": 64, "type": "METRIC", "confidence": 0.9990770816802979}, {"text": "F-measure", "start_pos": 78, "end_pos": 87, "type": "METRIC", "confidence": 0.9963170289993286}]}, {"text": "We start with a model that uses only the keywordspecific features (KW).", "labels": [], "entities": []}, {"text": "This type of feature alone achieved a significantly better performance than the baseline methods (90.61% F-measure for the abstracts and 80.57% F-measure for the full text papers), suggesting that the keyword-specific features are important in determining its speculative context.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 105, "end_pos": 114, "type": "METRIC", "confidence": 0.9975067973136902}, {"text": "F-measure", "start_pos": 144, "end_pos": 153, "type": "METRIC", "confidence": 0.9957131147384644}]}, {"text": "We extended the feature set by including the dependency relation (DEP), surrounding words (BOW 1), positional (POS), and cooccurring keywords (CO-KW) features.", "labels": [], "entities": [{"text": "BOW 1)", "start_pos": 91, "end_pos": 97, "type": "METRIC", "confidence": 0.9391198952992758}]}, {"text": "Each new type of included feature improved the performance of the model for the abstracts.", "labels": [], "entities": []}, {"text": "The best F-measure (91.69%) is achieved by using all the proposed types of features.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 9, "end_pos": 18, "type": "METRIC", "confidence": 0.9986904263496399}]}, {"text": "This performance is close to the upper bound, which is the human inter-annotator agreement F-measure of 92.05%.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 91, "end_pos": 100, "type": "METRIC", "confidence": 0.6844473481178284}]}, {"text": "Including the co-occurring keywords to the feature set for full text articles slightly improved precision, but deceased recall, which led to lower Fmeasure.", "labels": [], "entities": [{"text": "precision", "start_pos": 96, "end_pos": 105, "type": "METRIC", "confidence": 0.9995107650756836}, {"text": "recall", "start_pos": 120, "end_pos": 126, "type": "METRIC", "confidence": 0.9807900190353394}, {"text": "Fmeasure", "start_pos": 147, "end_pos": 155, "type": "METRIC", "confidence": 0.9982325434684753}]}, {"text": "The best F-measure (82.82%) for the full text articles is achieved by using all the feature types except the co-occurring keywords.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 9, "end_pos": 18, "type": "METRIC", "confidence": 0.9993150234222412}]}, {"text": "The achieved performance is significantly higher than the baseline methods, but lower than the human inter-annotator agreement F-measure of 90.81%.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 127, "end_pos": 136, "type": "METRIC", "confidence": 0.6980621814727783}]}, {"text": "The lower performance for the full text papers might be due to the small size of the data set (9 full text papers compared to 1273 abstracts).", "labels": [], "entities": []}, {"text": "We compared the proposed rule-based approach for scope resolution with two baseline methods.", "labels": [], "entities": [{"text": "scope resolution", "start_pos": 49, "end_pos": 65, "type": "TASK", "confidence": 0.9446780681610107}]}, {"text": "Previous studies classify sentences as speculative or not, therefore implicitly assigning the scope of a speculation to the whole sentence (.", "labels": [], "entities": []}, {"text": "Baseline 1 follows this approach and assigns the scope of a speculation keyword to the whole sentence.", "labels": [], "entities": []}, {"text": "Szarvas (2008) suggest assigning the scope of a keyword from its occurrence to the end of the sentence.", "labels": [], "entities": []}, {"text": "They state that this approach works accurately for clinical free texts, but no any results are reported.", "labels": [], "entities": []}, {"text": "Baseline 2 follows the approach proposed in and assigns the scope of a keyword to the fragment of the sentence that starts with the keyword and ends at the end of the sentence.", "labels": [], "entities": []}, {"text": "summarizes the accuracy results obtained for the abstracts and the full text papers.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 15, "end_pos": 23, "type": "METRIC", "confidence": 0.9996218681335449}]}, {"text": "The poor performance of baseline 1, emphasizes the importance of detecting the portions of sentences that are speculative, since less than 5% of the sentences that contain speculation keywords are entirely speculative.", "labels": [], "entities": []}, {"text": "Classifying the entire sentences as speculative or not leads to loss in information for more than 95% of the sentences.", "labels": [], "entities": []}, {"text": "The rule-based method significantly outperformed the two baseline methods, indicating that the part-ofspeech of the keywords and the syntactic parses of the sentences are effective in characterizing the speculation scopes.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Summary of the biomedical scientific articles sub-", "labels": [], "entities": [{"text": "Summary", "start_pos": 10, "end_pos": 17, "type": "TASK", "confidence": 0.8634443283081055}]}, {"text": " Table 2: Results for the Scientific Abstracts", "labels": [], "entities": []}, {"text": " Table 3: Results for the Scientific Full Text Papers", "labels": [], "entities": [{"text": "Scientific Full Text Papers", "start_pos": 26, "end_pos": 53, "type": "DATASET", "confidence": 0.7914833277463913}]}, {"text": " Table 4: Scope resolution results", "labels": [], "entities": [{"text": "Scope resolution", "start_pos": 10, "end_pos": 26, "type": "TASK", "confidence": 0.9669125080108643}]}]}