{"title": [{"text": "Semi-supervised Semantic Role Labeling Using the Latent Words Language Model", "labels": [], "entities": [{"text": "Semantic Role Labeling", "start_pos": 16, "end_pos": 38, "type": "TASK", "confidence": 0.7215278546015421}]}], "abstractContent": [{"text": "Semantic Role Labeling (SRL) has proved to be a valuable tool for performing automatic analysis of natural language texts.", "labels": [], "entities": [{"text": "Semantic Role Labeling (SRL)", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.807684083779653}, {"text": "automatic analysis of natural language texts", "start_pos": 77, "end_pos": 121, "type": "TASK", "confidence": 0.7464283555746078}]}, {"text": "Currently however, most systems rely on a large training set, which is manually annotated , an effort that needs to be repeated whenever different languages or a different set of semantic roles is used in a certain application.", "labels": [], "entities": []}, {"text": "A possible solution for this problem is semi-supervised learning, where a small set of training examples is automatically expanded using unlabeled texts.", "labels": [], "entities": []}, {"text": "We present the Latent Words Language Model, which is a language model that learns word similarities from unla-beled texts.", "labels": [], "entities": []}, {"text": "We use these similarities for different semi-supervised SRL methods as additional features or to automatically expand a small training set.", "labels": [], "entities": [{"text": "SRL", "start_pos": 56, "end_pos": 59, "type": "TASK", "confidence": 0.9559041857719421}]}, {"text": "We evaluate the methods on the PropBank dataset and find that for small training sizes our best performing system achieves an error reduction of 33.27% F1-measure compared to a state-of-the-art supervised baseline.", "labels": [], "entities": [{"text": "PropBank dataset", "start_pos": 31, "end_pos": 47, "type": "DATASET", "confidence": 0.9781919121742249}, {"text": "error reduction", "start_pos": 126, "end_pos": 141, "type": "METRIC", "confidence": 0.9833278059959412}, {"text": "F1-measure", "start_pos": 152, "end_pos": 162, "type": "METRIC", "confidence": 0.9995183944702148}]}], "introductionContent": [{"text": "Automatic analysis of natural language is still a very hard task to perform fora computer.", "labels": [], "entities": [{"text": "Automatic analysis of natural language", "start_pos": 0, "end_pos": 38, "type": "TASK", "confidence": 0.8398322820663452}]}, {"text": "Although some successful applications have been developed (see for instance), implementing an automatic text analysis system is still a labour and time intensive task.", "labels": [], "entities": [{"text": "text analysis", "start_pos": 104, "end_pos": 117, "type": "TASK", "confidence": 0.7699392735958099}]}, {"text": "Many applications would benefit from an intermediate representation of texts, where an automatic analysis is already performed which is sufficiently general to be useful in a wide range of applications.", "labels": [], "entities": []}, {"text": "Syntactic analysis of texts (such as Part-OfSpeech tagging and syntactic parsing) is an example of such a generic analysis, and has proved useful in applications ranging from machine translation () to text mining in the bio-medical domain.", "labels": [], "entities": [{"text": "Syntactic analysis of texts", "start_pos": 0, "end_pos": 27, "type": "TASK", "confidence": 0.8588900566101074}, {"text": "Part-OfSpeech tagging", "start_pos": 37, "end_pos": 58, "type": "TASK", "confidence": 0.6315697133541107}, {"text": "syntactic parsing", "start_pos": 63, "end_pos": 80, "type": "TASK", "confidence": 0.7175513207912445}, {"text": "machine translation", "start_pos": 175, "end_pos": 194, "type": "TASK", "confidence": 0.7673579454421997}, {"text": "text mining", "start_pos": 201, "end_pos": 212, "type": "TASK", "confidence": 0.7997006773948669}]}, {"text": "A syntactic parse is however a representation that is very closely tied with the surface-form of natural language, in contrast to Semantic Role Labeling (SRL) which adds a layer of predicate-argument information that generalizes across different syntactic alternations).", "labels": [], "entities": [{"text": "syntactic parse", "start_pos": 2, "end_pos": 17, "type": "TASK", "confidence": 0.778539389371872}, {"text": "Semantic Role Labeling (SRL)", "start_pos": 130, "end_pos": 158, "type": "TASK", "confidence": 0.7852927843729655}]}, {"text": "SRL has received a lot of attention in the research community, and many systems have been developed (see section 2).", "labels": [], "entities": [{"text": "SRL", "start_pos": 0, "end_pos": 3, "type": "TASK", "confidence": 0.7532438039779663}]}, {"text": "Most of these systems rely on a large dataset for training that is manually annotated.", "labels": [], "entities": []}, {"text": "In this paper we investigate whether we can develop a system that achieves state-of-the-art semantic role labeling without relying on a large number of labeled examples.", "labels": [], "entities": [{"text": "semantic role labeling", "start_pos": 92, "end_pos": 114, "type": "TASK", "confidence": 0.6645587285359701}]}, {"text": "We aim to do so by employing the Latent Words Language Model that learns latent words from a large unlabeled corpus.", "labels": [], "entities": []}, {"text": "Latent words are words that (unlike observed words) did not occur at a particular position in a text, but given semantic and syntactic constraints from the context could have occurred at that particular position.", "labels": [], "entities": []}, {"text": "In section 2 we revise existing work on SRL and on semi-supervised learning.", "labels": [], "entities": [{"text": "SRL", "start_pos": 40, "end_pos": 43, "type": "TASK", "confidence": 0.9883297681808472}]}, {"text": "Section 3 outlines our supervised classifier for SRL and section 4 discusses the Latent Words Language Model.", "labels": [], "entities": [{"text": "SRL", "start_pos": 49, "end_pos": 52, "type": "TASK", "confidence": 0.9760042428970337}]}, {"text": "In section 5 we will combine the two models for semisupervised role labeling.", "labels": [], "entities": [{"text": "semisupervised role labeling", "start_pos": 48, "end_pos": 76, "type": "TASK", "confidence": 0.5775206387042999}]}, {"text": "We will test the model on the standard PropBank dataset and compare it with state-of-the-art semi-supervised SRL systems in section 6 and finally in section 7 we draw conclusions and outline future work. were the first to describe a statistical system trained on the data from the FrameNet project to automatically assign semantic roles.", "labels": [], "entities": [{"text": "PropBank dataset", "start_pos": 39, "end_pos": 55, "type": "DATASET", "confidence": 0.9660850465297699}]}, {"text": "This approach was soon followed by other researchers (), focus-ing on improved sets of features, improved machine learning methods or both, and SRL became a shared task at the and 2008 conferences 1 . The best system) in CoNLL 2008 achieved an F1-measure of 81.65% on the workshop's evaluation corpus.", "labels": [], "entities": [{"text": "SRL", "start_pos": 144, "end_pos": 147, "type": "TASK", "confidence": 0.6944313049316406}, {"text": "F1-measure", "start_pos": 244, "end_pos": 254, "type": "METRIC", "confidence": 0.9994112253189087}]}], "datasetContent": [{"text": "A first evaluation of the quality of the automatically learned latent words is by translation of this model into a sequential language model and by measuring its perplexity on previously unseen texts.", "labels": [], "entities": []}, {"text": "In we perform a number of experiments, comparing different corpora (news texts from Reuters and from Associated Press, and articles from Wikipedia) and n-gram sizes (3-gram and 4-gram).", "labels": [], "entities": []}, {"text": "We also compared the proposed model with two state-ofthe-art language models, Interpolated Kneser-Ney smoothing and fullibmpredict, and found that LWLM outperformed both models on all corpora, with a perplexity reduction ranging between 12.40% and 5.87%.", "labels": [], "entities": []}, {"text": "These results show that the estimated distributions over latent words are of a high quality and lead us to believe they could be used to improve automatic text analysis, like SRL.", "labels": [], "entities": [{"text": "automatic text analysis", "start_pos": 145, "end_pos": 168, "type": "TASK", "confidence": 0.591105451186498}, {"text": "SRL", "start_pos": 175, "end_pos": 178, "type": "TASK", "confidence": 0.9260855317115784}]}, {"text": "We perform a number of experiments where we compare the fully supervised model with the semisupervised models proposed in the previous section.", "labels": [], "entities": []}, {"text": "We first train the LWLM model on an unlabeled 5 million word Reuters corpus 8 . We perform different experiments for the supervised and the four different semi-supervised methods (see previous section).", "labels": [], "entities": [{"text": "Reuters corpus 8", "start_pos": 61, "end_pos": 77, "type": "DATASET", "confidence": 0.8987541993459066}]}, {"text": "shows the results of the different methods on the test set of the CoNLL 2008 shared task.", "labels": [], "entities": [{"text": "CoNLL 2008 shared task", "start_pos": 66, "end_pos": 88, "type": "DATASET", "confidence": 0.9130924940109253}]}, {"text": "We experimented with different sizes for the training set, ranging from 5% to 100%.", "labels": [], "entities": []}, {"text": "When using a subset of the full training set, we run 10 different experiments with random subsets and average the results.", "labels": [], "entities": []}, {"text": "We see that the LWFeatures method performs better than the other methods across all training sizes.", "labels": [], "entities": []}, {"text": "Furthermore, these improvements are See http://www.daviddlewis.com/resources larger for smaller training sets, showing that the approach can be applied successfully in a setting where only a small number of training examples is available.", "labels": [], "entities": []}, {"text": "When comparing the LWFeatures method with the ClusterFeatures method we see that, although the ClusterFeatures method has a similar performance for small training sizes, this performance drops for larger training sizes.", "labels": [], "entities": []}, {"text": "A possible explanation for this result is the use of the clusters employed in the ClusterFeatures method.", "labels": [], "entities": []}, {"text": "By definition the clusters merge many words into one cluster, which might lead to good generalization (more important for small training sizes) but can potentially hurt precision (more important for larger training sizes).", "labels": [], "entities": [{"text": "precision", "start_pos": 169, "end_pos": 178, "type": "METRIC", "confidence": 0.9988808035850525}]}, {"text": "A third observation that can be made from table 1 is that, although both automatic expansion methods outperform the supervised method for the smallest training size, for other sizes of the training set they perform relatively poorly.", "labels": [], "entities": []}, {"text": "An informal inspection showed that for some examples in the training set, little or no correct similar occurrences were found in the unlabeled text.", "labels": [], "entities": []}, {"text": "The algorithm described in section 5 adds the most similar k occurrences to the training set for every annotated example, also for these examples where little or no similar occurrences were found.", "labels": [], "entities": []}, {"text": "Often the automatic alignment fails to generate correct labels for these occurrences and introduces errors in the training set.", "labels": [], "entities": []}, {"text": "In the future we would like to perform experiments that determine dynamically (for instance based on the similarity measure between occurrences) for every annotated example how many training examples to add.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Results (in F1-measure) on the CoNLL 2008 test set for the different methods, comparing  the supervised method (Supervised) with the semi-supervised methods LWFeatures, ClusterFeatures,  AutomaticExpansionCOS and AutomaticExpansionLW. See section 5 for details on the different methods.  Best results are in bold.", "labels": [], "entities": [{"text": "F1-measure", "start_pos": 22, "end_pos": 32, "type": "METRIC", "confidence": 0.9950879216194153}, {"text": "CoNLL 2008 test set", "start_pos": 41, "end_pos": 60, "type": "DATASET", "confidence": 0.9814789593219757}]}]}