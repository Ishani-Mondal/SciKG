{"title": [{"text": "Multilingual Spectral Clustering Using Document Similarity Propagation", "labels": [], "entities": [{"text": "Multilingual Spectral Clustering", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.758043646812439}]}], "abstractContent": [{"text": "We present a novel approach for multilingual document clustering using only comparable corpora to achieve cross-lingual semantic interoperability.", "labels": [], "entities": [{"text": "multilingual document clustering", "start_pos": 32, "end_pos": 64, "type": "TASK", "confidence": 0.6248652140299479}]}, {"text": "The method models document collections as weighted graph, and supervisory information is given as sets of must-linked constraints for documents in different languages.", "labels": [], "entities": []}, {"text": "Recur-sive k-nearest neighbor similarity propagation is used to exploit the prior knowledge and merge two language spaces.", "labels": [], "entities": [{"text": "Recur-sive k-nearest neighbor similarity propagation", "start_pos": 0, "end_pos": 52, "type": "TASK", "confidence": 0.601533305644989}]}, {"text": "Spectral method is applied to find the best cuts of the graph.", "labels": [], "entities": []}, {"text": "Experimental results show that using limited supervisory information , our method achieves promising clustering results.", "labels": [], "entities": []}, {"text": "Furthermore, since the method does not need any language dependent information in the process, our algorithm can be applied to languages in various alphabetical systems.", "labels": [], "entities": []}], "introductionContent": [{"text": "Document clustering is unsupervised classification of text collections into distinct groups of similar documents.", "labels": [], "entities": [{"text": "Document clustering", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.9179105758666992}]}, {"text": "It has been used in many information retrieval tasks, including data organization (), language modeling (), and improving performances of text categorization system).", "labels": [], "entities": [{"text": "information retrieval", "start_pos": 25, "end_pos": 46, "type": "TASK", "confidence": 0.7665635943412781}, {"text": "data organization", "start_pos": 64, "end_pos": 81, "type": "TASK", "confidence": 0.7380907833576202}, {"text": "language modeling", "start_pos": 86, "end_pos": 103, "type": "TASK", "confidence": 0.7830025553703308}]}, {"text": "Advance in internet technology has made the task of managing multilingual documents an intriguing research area.", "labels": [], "entities": []}, {"text": "The growth of internet leads to the necessity of organizing documents in various languages.", "labels": [], "entities": []}, {"text": "There exist thousands of languages, not to mention countless minor ones.", "labels": [], "entities": []}, {"text": "Creating document clustering model for each language is simply unfeasible.", "labels": [], "entities": [{"text": "document clustering", "start_pos": 9, "end_pos": 28, "type": "TASK", "confidence": 0.6966512650251389}]}, {"text": "We need methods to deal with text collections in diverse languages simultaneously.", "labels": [], "entities": []}, {"text": "Multilingual document clustering (MLDC) involves partitioning documents, written in more than one languages, into sets of clusters.", "labels": [], "entities": [{"text": "Multilingual document clustering (MLDC)", "start_pos": 0, "end_pos": 39, "type": "TASK", "confidence": 0.8071188579003016}]}, {"text": "Similar documents, even if they are written in different languages, should be grouped together into one cluster.", "labels": [], "entities": []}, {"text": "The major challenge of MLDC is achieving cross-lingual semantic interoperability.", "labels": [], "entities": [{"text": "MLDC", "start_pos": 23, "end_pos": 27, "type": "TASK", "confidence": 0.9509516358375549}]}, {"text": "Most monolingual techniques will notwork since documents in different languages are mapped into different spaces.", "labels": [], "entities": []}, {"text": "Spectral method such as Latent Semantic Analysis has been commonly applied for MLDC task.", "labels": [], "entities": [{"text": "Latent Semantic Analysis", "start_pos": 24, "end_pos": 48, "type": "TASK", "confidence": 0.5888264278570811}, {"text": "MLDC task", "start_pos": 79, "end_pos": 88, "type": "TASK", "confidence": 0.9242812097072601}]}, {"text": "However, current techniques strongly rely on the presence of common words between different languages.", "labels": [], "entities": []}, {"text": "This method would only work if the languages are highly related, i.e., languages that share the same root.", "labels": [], "entities": []}, {"text": "Therefore, we need another method to improve the robustness of MLDC model.", "labels": [], "entities": [{"text": "MLDC", "start_pos": 63, "end_pos": 67, "type": "TASK", "confidence": 0.719201385974884}]}, {"text": "In this paper, we focus on the problem of bridging multilingual space for document clustering.", "labels": [], "entities": [{"text": "document clustering", "start_pos": 74, "end_pos": 93, "type": "TASK", "confidence": 0.7143815904855728}]}, {"text": "We are given text documents in different languages and asked to group them into clusters such that documents that belong to the same topic are grouped together.", "labels": [], "entities": []}, {"text": "Traditional monolingual approach is impracticable since it is unable to predict how similar two multilingual documents are.", "labels": [], "entities": []}, {"text": "They have two different spaces which make conventional cosine similarity irrelevant.", "labels": [], "entities": []}, {"text": "We try to solve this problem utilizing prior knowledge in the form of must-linked constraints, gathered from comparable corpora.", "labels": [], "entities": []}, {"text": "Propagation method is used to guide the language-space merging process.", "labels": [], "entities": [{"text": "language-space merging", "start_pos": 40, "end_pos": 62, "type": "TASK", "confidence": 0.7095501124858856}]}, {"text": "Experimental results show that the approach gives encouraging clustering results.", "labels": [], "entities": []}, {"text": "This paper is organized as follows.", "labels": [], "entities": []}, {"text": "In section 2, we review related work.", "labels": [], "entities": []}, {"text": "In section 3, we propose our algorithm for multilingual document clustering.", "labels": [], "entities": [{"text": "multilingual document clustering", "start_pos": 43, "end_pos": 75, "type": "TASK", "confidence": 0.6157710254192352}]}, {"text": "The experimental results are shown in section 4.", "labels": [], "entities": []}, {"text": "Section 5 concludes with a summary.", "labels": [], "entities": []}], "datasetContent": [{"text": "The goals of empirical evaluation include (1) testing whether the propagation method can merge multilingual space and produce acceptable clustering results; (2) comparing the performance to spectral clustering method without propagation.", "labels": [], "entities": []}, {"text": "For our experiment, we used Rand Index (RI) which is a common evaluation technique for clustering task where the true class of unlabeled data Rand Index penalizes false positive and false negative decisions during clustering.", "labels": [], "entities": [{"text": "Rand Index (RI)", "start_pos": 28, "end_pos": 43, "type": "METRIC", "confidence": 0.9562638163566589}, {"text": "clustering task", "start_pos": 87, "end_pos": 102, "type": "TASK", "confidence": 0.887212723493576}]}, {"text": "It takes into account decision that assign two similar documents to one cluster (TP), two dissimilar documents to different clusters (TN), two similar documents to different clusters (FN), and two dissimilar documents to one cluster (FP).", "labels": [], "entities": []}, {"text": "We do not include links created by supervisory information when calculating true positive decisions and only consider the number of free decisions made.", "labels": [], "entities": []}, {"text": "We also used F \u03b1 -measure, the weighted harmonic mean of precision (P) and recall (R).", "labels": [], "entities": [{"text": "F \u03b1 -measure", "start_pos": 13, "end_pos": 25, "type": "METRIC", "confidence": 0.9693258255720139}, {"text": "weighted harmonic mean of precision (P)", "start_pos": 31, "end_pos": 70, "type": "METRIC", "confidence": 0.7440990842878819}, {"text": "recall (R)", "start_pos": 75, "end_pos": 85, "type": "METRIC", "confidence": 0.9650185257196426}]}, {"text": "F \u03b1 -measure is defined as: Last, we used purity to evaluate the accuracy of assignments.", "labels": [], "entities": [{"text": "F \u03b1 -measure", "start_pos": 0, "end_pos": 12, "type": "METRIC", "confidence": 0.8358163088560104}, {"text": "purity", "start_pos": 42, "end_pos": 48, "type": "METRIC", "confidence": 0.9825981259346008}, {"text": "accuracy", "start_pos": 65, "end_pos": 73, "type": "METRIC", "confidence": 0.9988409876823425}]}, {"text": "Purity is defined as: where N is the number of documents, t is the number of clusters, j is the number of classes, \u03c9 t and c j are sets of documents in cluster t and class j respectively.", "labels": [], "entities": []}, {"text": "To prove the effectiveness of our clustering algorithm, we performed the following experiments on our data set.", "labels": [], "entities": []}, {"text": "We first tested our algorithm on four topics, science, sports, religion, and economy.", "labels": [], "entities": []}, {"text": "We then tested our algorithm using all six topics to get an understanding of the performance of our model in larger collections with more topics.", "labels": [], "entities": []}, {"text": "We used subset of our data as supervisory information and built must-linked constraints from it.", "labels": [], "entities": []}, {"text": "The proportion of supervisory information provided to the system is given in x-axis -3).", "labels": [], "entities": []}, {"text": "0.2 here means 20% of documents in each language are taken to be used as prior knowledge.", "labels": [], "entities": []}, {"text": "Since the number of documents in each language for our experiment is the same, we have the same numbers of documents in subset of English collection, subset of French collection, and subset of Spanish collection.", "labels": [], "entities": []}, {"text": "We also ensure there are same numbers of documents fora particular topic in all three languages.", "labels": [], "entities": []}, {"text": "We can build must-linked constraints as follows.", "labels": [], "entities": []}, {"text": "For each document in the subset of English collection, we create must-linked constraints with one randomly selected document from the subset of French collection and one randomly selected document from the subset of Spanish collection that belong to the same topic with it.", "labels": [], "entities": []}, {"text": "We then create must-linked constraint between the respective French and Spanish documents.", "labels": [], "entities": []}, {"text": "The constraints given to the algorithm are chosen so that there are several links that connect every topic in every language.", "labels": [], "entities": []}, {"text": "Note that the class label in- formation is only used to build must-linked constraints between documents, and we do not assign the documents to a particular cluster.", "labels": [], "entities": []}, {"text": "shows the Rand Index as proportion of supervisory information increases. and.3 give purity and F 2 -measure for the algorithm respectively.", "labels": [], "entities": [{"text": "Rand Index", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9843095541000366}, {"text": "purity", "start_pos": 84, "end_pos": 90, "type": "METRIC", "confidence": 0.9993376135826111}, {"text": "F 2 -measure", "start_pos": 95, "end_pos": 107, "type": "METRIC", "confidence": 0.9858250170946121}]}, {"text": "To show the importance of the propagation in multilingual space, we give comparison with spectral clustering model without propagation.", "labels": [], "entities": []}, {"text": "Three lines in to.3 indicate: (1) results with propagation (solid line); (2) results without propagation (longdashed line); and (3) results using Latent Semantic Analysis(LSA)-based method by exploiting common words between languages (shortdashed line).", "labels": [], "entities": []}, {"text": "For each figure, 6 plots are taken starting from 0 in 0.2-point-increments.", "labels": [], "entities": []}, {"text": "We conducted the experiments three times for each proportion of supervisory information and use the average values.", "labels": [], "entities": []}, {"text": "As we can see from, and.3, the propagation method can significantly improve the performance of spectral clustering algorithm.", "labels": [], "entities": [{"text": "spectral clustering", "start_pos": 95, "end_pos": 114, "type": "TASK", "confidence": 0.7137318849563599}]}, {"text": "For 1800 documents in 6 topics, we manage to achieve RI = 0.91, purity = 0.84, and F 2 -measure = 0.76 with only 20% of documents (360 documents) used as supervisory information.", "labels": [], "entities": [{"text": "RI", "start_pos": 53, "end_pos": 55, "type": "METRIC", "confidence": 0.998538613319397}, {"text": "purity", "start_pos": 64, "end_pos": 70, "type": "METRIC", "confidence": 0.9993124008178711}, {"text": "F 2 -measure", "start_pos": 83, "end_pos": 95, "type": "METRIC", "confidence": 0.9848566204309464}]}, {"text": "Spectral clustering algorithm without propagation can only achieve 0.69, 0.30, 0.28 for RI, purity, and F 2 -measure respectively.", "labels": [], "entities": [{"text": "RI", "start_pos": 88, "end_pos": 90, "type": "METRIC", "confidence": 0.9958255290985107}, {"text": "purity", "start_pos": 92, "end_pos": 98, "type": "METRIC", "confidence": 0.9624258875846863}, {"text": "F 2 -measure", "start_pos": 104, "end_pos": 116, "type": "METRIC", "confidence": 0.9768635928630829}]}, {"text": "The propagation method is highly effective when only small amount of supervisory information given to the algorithm.", "labels": [], "entities": []}, {"text": "Obviously, the more supervisory information given, the better the performance is.", "labels": [], "entities": []}, {"text": "As the number of supervisory information increases, the difference of the model performance with and without propagation becomes smaller.", "labels": [], "entities": []}, {"text": "This is because there are already enough links between multilingual documents, so we do not necessarily build more links through similarity propagation anymore.", "labels": [], "entities": [{"text": "similarity propagation", "start_pos": 129, "end_pos": 151, "type": "TASK", "confidence": 0.7277871519327164}]}, {"text": "However, even when there are already many links, our model with propagation still outperforms the model without propagation.", "labels": [], "entities": []}, {"text": "We compare the performance of our algorithm to LSA-based multilingual document clustering model.", "labels": [], "entities": [{"text": "LSA-based multilingual document clustering", "start_pos": 47, "end_pos": 89, "type": "TASK", "confidence": 0.5203573629260063}]}, {"text": "We performed LSA to the multilingual term by document matrix.", "labels": [], "entities": []}, {"text": "We do not use parallel texts and only rely on common words across languages as well as must-linked constraints to build multilingual space.", "labels": [], "entities": []}, {"text": "The results show that exploiting common words between languages alone is not enough to build a good multilingual semantic space, justifying the usage of supervisory information in multilingual document clustering task.", "labels": [], "entities": [{"text": "multilingual document clustering", "start_pos": 180, "end_pos": 212, "type": "TASK", "confidence": 0.609900712966919}]}, {"text": "When supervisory information is introduced, our method achieves better results than LSA-based method.", "labels": [], "entities": []}, {"text": "In general, the LSA-based method performs better than the model without propagation.", "labels": [], "entities": []}, {"text": "We assess the sensitivity of our algorithm to parameter \u03b2, the penalty for similarity propagation.", "labels": [], "entities": [{"text": "similarity propagation", "start_pos": 75, "end_pos": 97, "type": "TASK", "confidence": 0.738427460193634}]}, {"text": "We assess the sensitivity of our algorithm to parameter \u03b2, the penalty for similarity propagation.", "labels": [], "entities": [{"text": "similarity propagation", "start_pos": 75, "end_pos": 97, "type": "TASK", "confidence": 0.738427460193634}]}, {"text": "We tested our algorithm using various \u03b2, starting from 0 to 1 in 0.2-point-increments, while other parameters being held constant.", "labels": [], "entities": []}, {"text": "shows that changing \u03b2 to some extent affects the performance of the algorithm.", "labels": [], "entities": []}, {"text": "However, after some value of reasonable \u03b2 is found, increasing \u03b2 does not have significant impact on the per- formance of the algorithm.", "labels": [], "entities": []}, {"text": "We also tested our algorithm using various k, starting from 0 to 100 in 20-point-increments.(b) reveals that the performances of the model with different k are comparable, as long ask is not too small.", "labels": [], "entities": []}, {"text": "However, using too large k will slightly decrease the performance of the model.", "labels": [], "entities": []}, {"text": "Too many propagations make several dissimilar documents receive high similarity value that cannot be nullified by the post-processing step.", "labels": [], "entities": []}, {"text": "Last, we experimented using various t ranging from 2 to 20.(c) shows that the method performs best when t = 10, and for reasonable value oft the method achieves comparable performance.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Average number of words of documents  in the corpus. Each language consists of 600 doc- uments, and each topic consists of 100 documents  (per language).", "labels": [], "entities": []}]}