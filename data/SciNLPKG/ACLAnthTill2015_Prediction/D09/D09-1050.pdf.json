{"title": [{"text": "Acquiring Translation Equivalences of Multiword Expressions by Normalized Correlation Frequencies", "labels": [], "entities": [{"text": "Acquiring Translation Equivalences of Multiword Expressions", "start_pos": 0, "end_pos": 59, "type": "TASK", "confidence": 0.8548596799373627}]}], "abstractContent": [{"text": "In this paper, we present an algorithm for extracting translations of any given multiword expression from parallel corpora.", "labels": [], "entities": [{"text": "extracting translations of any given multiword expression from parallel corpora", "start_pos": 43, "end_pos": 122, "type": "TASK", "confidence": 0.8181159973144532}]}, {"text": "Given a multiword expression to be translated, the method involves extracting a shortlist of target candidate words from parallel corpora based on scores of normalized frequency, generating possible translations and filtering out common subsequences, and selecting the top-n possible translations using the Dice coefficient.", "labels": [], "entities": []}, {"text": "Experiments show that our approach outperforms the word alignment-based and other naive association-based methods.", "labels": [], "entities": []}, {"text": "We also demonstrate that adopting the extracted translations can significantly improve the performance of the Moses machine translation system.", "labels": [], "entities": [{"text": "Moses machine translation", "start_pos": 110, "end_pos": 135, "type": "TASK", "confidence": 0.6206367115179697}]}], "introductionContent": [{"text": "Translation of multiword expressions (MWEs), such as compound words, phrases, collocations and idioms, is important for many NLP tasks, including the techniques are helpful for dictionary compilation, cross language information retrieval, second language learning, and machine translation.", "labels": [], "entities": [{"text": "Translation of multiword expressions (MWEs)", "start_pos": 0, "end_pos": 43, "type": "TASK", "confidence": 0.8704293199947902}, {"text": "dictionary compilation", "start_pos": 177, "end_pos": 199, "type": "TASK", "confidence": 0.8082380890846252}, {"text": "cross language information retrieval", "start_pos": 201, "end_pos": 237, "type": "TASK", "confidence": 0.7179269939661026}, {"text": "second language learning", "start_pos": 239, "end_pos": 263, "type": "TASK", "confidence": 0.6921473940213522}, {"text": "machine translation", "start_pos": 269, "end_pos": 288, "type": "TASK", "confidence": 0.820202112197876}]}, {"text": "(;. However, extracting exact translations of MWEs is still an open problem, possibly because the senses of many MWEs are not compositional), i.e., their translations are not compositions of the translations of individual words.", "labels": [], "entities": [{"text": "extracting exact translations of MWEs", "start_pos": 13, "end_pos": 50, "type": "TASK", "confidence": 0.8133334279060364}]}, {"text": "For example, the Chinese idiom \u5750\u8996\u4e0d\u7406 should be translated as \"turn a blind eye,\" which has no direct relation with respect to the translation of each constituent (i.e., \"to sit\", \"to see\" and \"to ignore\") at the word level.", "labels": [], "entities": []}, {"text": "Previous SMT systems (e.g.,) used a word-based translation model which assumes that a sentence can be translated into other languages by translating each word into one or more words in the target language.", "labels": [], "entities": [{"text": "SMT", "start_pos": 9, "end_pos": 12, "type": "TASK", "confidence": 0.989876925945282}, {"text": "word-based translation", "start_pos": 36, "end_pos": 58, "type": "TASK", "confidence": 0.6824108958244324}]}, {"text": "Since many concepts are expressed by idiomatic multiword expressions instead of single words, and different languages may realize the same concept using different numbers of words (, word alignment based methods, which are highly dependent on the probability information at the lexical level, are not well suited for this type of translation.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 183, "end_pos": 197, "type": "TASK", "confidence": 0.7194839715957642}]}, {"text": "To address the above problem, some methods have been proposed for extending word alignments to phrase alignments.", "labels": [], "entities": [{"text": "word alignments", "start_pos": 76, "end_pos": 91, "type": "TASK", "confidence": 0.7353168874979019}, {"text": "phrase alignments", "start_pos": 95, "end_pos": 112, "type": "TASK", "confidence": 0.7017875760793686}]}, {"text": "For example, proposed the so-called grow-diagfinal heuristic method for extending word alignments to phrase alignments.", "labels": [], "entities": [{"text": "word alignments", "start_pos": 82, "end_pos": 97, "type": "TASK", "confidence": 0.7164553999900818}, {"text": "phrase alignments", "start_pos": 101, "end_pos": 118, "type": "TASK", "confidence": 0.7086947858333588}]}, {"text": "The method is widely used and has achieved good results for phrase-based statistical machine translation.", "labels": [], "entities": [{"text": "phrase-based statistical machine translation", "start_pos": 60, "end_pos": 104, "type": "TASK", "confidence": 0.7056101486086845}]}, {"text": "Instead of using heuristic rules, showed that syntactic information, e.g., phrase or dependency structures, is useful in extending the word-level alignment.", "labels": [], "entities": [{"text": "word-level alignment", "start_pos": 135, "end_pos": 155, "type": "TASK", "confidence": 0.7159301936626434}]}, {"text": "However, the above methods still depend on word-based alignment models, so they are not well suited to extracting the translation equivalences of semantically opaque MWEs due to the lack of word level relations between the translational correspondences.", "labels": [], "entities": [{"text": "extracting the translation equivalences of semantically opaque MWEs", "start_pos": 103, "end_pos": 170, "type": "TASK", "confidence": 0.5988505519926548}]}, {"text": "Moreover, the aligned phrases are not precise enough to be used in many NLP applications like dictionary compilation, which require high quality translations.", "labels": [], "entities": [{"text": "dictionary compilation", "start_pos": 94, "end_pos": 116, "type": "TASK", "confidence": 0.8013879358768463}]}, {"text": "Association-based methods, e.g., the Dice coefficient, are widely used to extract translations of MWEs.).", "labels": [], "entities": [{"text": "Dice coefficient", "start_pos": 37, "end_pos": 53, "type": "METRIC", "confidence": 0.7973246574401855}, {"text": "translations of MWEs.", "start_pos": 82, "end_pos": 103, "type": "TASK", "confidence": 0.7223523855209351}]}, {"text": "The advantage of such methods is that association relations are established at the phrase level instead of the lexical level, so they have the potential to resolve the above-mentioned translation problem.", "labels": [], "entities": []}, {"text": "However, when applying association-based methods, we have to consider the following complications.", "labels": [], "entities": []}, {"text": "The first complication, which we call the contextual effect, causes the extracted translation to contain noisy words.", "labels": [], "entities": []}, {"text": "For example, translations of the Chinese idiom \u5169\u5168 \u5176\u7f8e (best of both worlds) extracted by a naive association-based method may contain noisy collocation words like difficult, try and cannot, which are not part of the translation of the idiom.", "labels": [], "entities": []}, {"text": "They are actually translations of its collocation context, such as \u96e3\u4ee5(difficult), \u5617\u8a66(try), and \u4e0d\u80fd(cannot).", "labels": [], "entities": []}, {"text": "This problem arises because naive association methods do not deal with the effect of strongly collocated contexts carefully.", "labels": [], "entities": []}, {"text": "If we can incorporate lexical-level information to discount the noisy collocation words, the contextual effect could be resolved.", "labels": [], "entities": []}, {"text": "The second complication, which we call the common subsequence problem, is that the Dice coefficient tends to select the common subsequences of a set of similar translations instead of the full translations.", "labels": [], "entities": []}, {"text": "Consider the translations of \u65b7\u7ae0\u53d6\u7fa9 (quote out of context) shown in the first three rows of.", "labels": [], "entities": []}, {"text": "The Dice coefficient of each translation is smaller than that of the common subsequence \"out of context\" in the last row.", "labels": [], "entities": [{"text": "Dice coefficient", "start_pos": 4, "end_pos": 20, "type": "METRIC", "confidence": 0.9873737096786499}]}, {"text": "If we can tell common subsequence apart from correct translations, the common subsequence problem could be resolved.", "labels": [], "entities": []}, {"text": "In this paper, we propose an improved precision method for extracting MWE translations from parallel corpora.", "labels": [], "entities": [{"text": "precision", "start_pos": 38, "end_pos": 47, "type": "METRIC", "confidence": 0.9962816834449768}, {"text": "extracting MWE translations from parallel corpora", "start_pos": 59, "end_pos": 108, "type": "TASK", "confidence": 0.8717343707879385}]}, {"text": "Our method is similar to that of, except that we incorporate lexical-level information into the association-based method.", "labels": [], "entities": []}, {"text": "The algorithm works effectively for various types of MWEs, such as phrases, single words, rigid word sequences (i.e., no gaps) and gapped word sequences.", "labels": [], "entities": []}, {"text": "Our experiment results show that the proposed translation extraction method outperforms word alignmentbased methods and association-based methods.", "labels": [], "entities": [{"text": "translation extraction", "start_pos": 46, "end_pos": 68, "type": "TASK", "confidence": 0.9758477807044983}]}, {"text": "We also demonstrate that precise translations derived by our method significantly improve the performance of the Moses machine translation system.", "labels": [], "entities": [{"text": "Moses machine translation", "start_pos": 113, "end_pos": 138, "type": "TASK", "confidence": 0.5686522920926412}]}, {"text": "The remainder of this paper is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 describes the methodology for extracting translation equivalences of MWEs.", "labels": [], "entities": [{"text": "extracting translation equivalences of MWEs", "start_pos": 40, "end_pos": 83, "type": "TASK", "confidence": 0.8180087089538575}]}, {"text": "Section 3 describes the experiment and presents the results.", "labels": [], "entities": []}, {"text": "In Section 4, we consider the application of our results to machine translation.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 60, "end_pos": 79, "type": "TASK", "confidence": 0.7466742992401123}]}, {"text": "Section 5 contains some concluding remarks.", "labels": [], "entities": []}], "datasetContent": [{"text": "In our experiments, we use the Hong Kong Hansard and the Hong Kong News parallel corpora as training data.", "labels": [], "entities": [{"text": "Hong Kong Hansard", "start_pos": 31, "end_pos": 48, "type": "DATASET", "confidence": 0.748932401339213}, {"text": "Hong Kong News parallel corpora", "start_pos": 57, "end_pos": 88, "type": "DATASET", "confidence": 0.9022507786750793}]}, {"text": "The training data was preprocessed by Chinese word segmentation to identify words and parsed by Chinese parser to extract MWEs.", "labels": [], "entities": [{"text": "Chinese word segmentation", "start_pos": 38, "end_pos": 63, "type": "TASK", "confidence": 0.6538443664709727}]}, {"text": "To evaluate the proposed approach, we randomly extract 309 Chinese MWEs from training data, including dependent word pairs and rigid idioms.", "labels": [], "entities": []}, {"text": "We then randomly select 103 of those MWEs as the development set and use the other 206 as the test set.", "labels": [], "entities": []}, {"text": "The reference translations of each Chinese MWE are manually extracted from the parallel corpora.", "labels": [], "entities": [{"text": "Chinese MWE", "start_pos": 35, "end_pos": 46, "type": "DATASET", "confidence": 0.8152501285076141}]}, {"text": "To evaluate the method for selecting candidate words, we use the coverage rate, which is defined as follows: where n is the number of MWEs in the test set, A w denotes the word set of the reference translations of w, and C w denotes a candidate word list extracted by the system.", "labels": [], "entities": [{"text": "coverage rate", "start_pos": 65, "end_pos": 78, "type": "METRIC", "confidence": 0.9713631570339203}]}, {"text": "shows the coverage of our method, NCF, compared with the coverage of the IBM model 1 and the association-based methods MI, Chi-square, and Dice.", "labels": [], "entities": [{"text": "IBM model 1", "start_pos": 73, "end_pos": 84, "type": "DATASET", "confidence": 0.8751545747121176}, {"text": "MI", "start_pos": 119, "end_pos": 121, "type": "METRIC", "confidence": 0.8676684498786926}]}, {"text": "As we can see, the top-10 candidate words of NCF cover almost 90% of the words in the reference translations.", "labels": [], "entities": [{"text": "NCF", "start_pos": 45, "end_pos": 48, "type": "DATASET", "confidence": 0.954626739025116}]}, {"text": "Whereas, the coverage of the association-based methods and IBM model 1 is much lower than 90%.", "labels": [], "entities": [{"text": "coverage", "start_pos": 13, "end_pos": 21, "type": "METRIC", "confidence": 0.9817950129508972}, {"text": "IBM model 1", "start_pos": 59, "end_pos": 70, "type": "DATASET", "confidence": 0.9097140232721964}]}, {"text": "The result implies that the candidate extraction method can extract a more precise candidate set than other methods.", "labels": [], "entities": [{"text": "candidate extraction", "start_pos": 28, "end_pos": 48, "type": "TASK", "confidence": 0.8247580826282501}]}, {"text": "We trained a model using Moses toolkit () on the training data as our baseline system.", "labels": [], "entities": []}, {"text": "shows the influence of adding the MWE translations to the test data.", "labels": [], "entities": []}, {"text": "In the first row (NIST06-sub), we only consider sentences containing MWE translations for BLEU score evaluation (726 sentences).", "labels": [], "entities": [{"text": "NIST06-sub", "start_pos": 18, "end_pos": 28, "type": "DATASET", "confidence": 0.9503083229064941}, {"text": "BLEU score", "start_pos": 90, "end_pos": 100, "type": "METRIC", "confidence": 0.948680579662323}]}, {"text": "In the second row, we took the whole NIST 2006 evaluation set into consideration (1,664 sentences).", "labels": [], "entities": [{"text": "NIST 2006 evaluation set", "start_pos": 37, "end_pos": 61, "type": "DATASET", "confidence": 0.9720857292413712}]}, {"text": "The Chinese words covered by the MWEs in NIST06-sub and NIST06 were 9.9% and 5.3% respectively.", "labels": [], "entities": [{"text": "NIST06-sub", "start_pos": 41, "end_pos": 51, "type": "DATASET", "confidence": 0.9647447466850281}, {"text": "NIST06", "start_pos": 56, "end_pos": 62, "type": "DATASET", "confidence": 0.9645149111747742}]}, {"text": "Adding MWE translations to the test data statistically significantly lead to better results than those of the baseline.", "labels": [], "entities": [{"text": "MWE translations", "start_pos": 7, "end_pos": 23, "type": "TASK", "confidence": 0.7828004956245422}]}, {"text": "Significance was tested using a paired bootstrap) with 1000 samples (p<0.02).", "labels": [], "entities": [{"text": "Significance", "start_pos": 0, "end_pos": 12, "type": "METRIC", "confidence": 0.9358991384506226}]}, {"text": "Although the improvement in BLEU score seems small, it is actually reasonably good given that the MWEs account for only 5% of the NIST06 test set.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 28, "end_pos": 38, "type": "METRIC", "confidence": 0.9757265448570251}, {"text": "NIST06 test set", "start_pos": 130, "end_pos": 145, "type": "DATASET", "confidence": 0.988744835058848}]}, {"text": "Examples of improved translations are shown in.", "labels": [], "entities": []}, {"text": "There is still room for improvement of the proposed MWE extraction method in order to provide more MWE translation pairs or design a feasible way to incorporate discontinuous bilingual MWEs to the decoder.", "labels": [], "entities": [{"text": "MWE extraction", "start_pos": 52, "end_pos": 66, "type": "TASK", "confidence": 0.9739448726177216}]}], "tableCaptions": [{"text": " Table 1. The Dice coefficient tends to select a com- mon subsequence of translations. (The frequency of", "labels": [], "entities": []}, {"text": " Table 2. Candidate words for the Chinese term", "labels": [], "entities": []}, {"text": " Table 3. Candidate words for the Chinese term \u65b7\u7ae0  \u53d6\u7fa9 sorted by their Dice coefficient values.", "labels": [], "entities": []}, {"text": " Table 5. Part of the candidate translation list for the", "labels": [], "entities": []}, {"text": " Table 6. The coverage rates of the candidate words  extracted by the compared methods", "labels": [], "entities": [{"text": "coverage", "start_pos": 14, "end_pos": 22, "type": "METRIC", "confidence": 0.9615985751152039}]}, {"text": " Table 7. Translation error rates of the systems.", "labels": [], "entities": [{"text": "Translation error rates", "start_pos": 10, "end_pos": 33, "type": "METRIC", "confidence": 0.7388073205947876}]}, {"text": " Table 8. Translation accuracy rates of the systems.  (%)", "labels": [], "entities": [{"text": "Translation", "start_pos": 10, "end_pos": 21, "type": "TASK", "confidence": 0.9487180709838867}, {"text": "accuracy", "start_pos": 22, "end_pos": 30, "type": "METRIC", "confidence": 0.9617294073104858}]}, {"text": " Table 9. BLEU scores of the translation results.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.999245285987854}, {"text": "translation", "start_pos": 29, "end_pos": 40, "type": "TASK", "confidence": 0.9605107307434082}]}]}