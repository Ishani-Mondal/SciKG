{"title": [{"text": "Sinuhe -Statistical Machine Translation using a Globally Trained Conditional Exponential Family Translation Model", "labels": [], "entities": [{"text": "Statistical Machine Translation", "start_pos": 8, "end_pos": 39, "type": "TASK", "confidence": 0.5196793476740519}, {"text": "Globally Trained Conditional Exponential Family Translation", "start_pos": 48, "end_pos": 107, "type": "TASK", "confidence": 0.5753313402334849}]}], "abstractContent": [{"text": "We present anew phrase-based conditional exponential family translation model for statistical machine translation.", "labels": [], "entities": [{"text": "phrase-based conditional exponential family translation", "start_pos": 16, "end_pos": 71, "type": "TASK", "confidence": 0.5686380445957184}, {"text": "statistical machine translation", "start_pos": 82, "end_pos": 113, "type": "TASK", "confidence": 0.6907215615113577}]}, {"text": "The model operates on a feature representation in which sentence level translations are represented by enumerating all the known phrase level translations that occur inside them.", "labels": [], "entities": []}, {"text": "This makes the model a good match with the commonly used phrase extraction heuristics.", "labels": [], "entities": [{"text": "phrase extraction heuristics", "start_pos": 57, "end_pos": 85, "type": "TASK", "confidence": 0.8242116371790568}]}, {"text": "The model's predictions are properly normalized probabilities.", "labels": [], "entities": []}, {"text": "In addition, the model automatically takes into account information provided by phrase overlaps, and does not suffer from reference translation reacha-bility problems.", "labels": [], "entities": [{"text": "reference translation reacha-bility", "start_pos": 122, "end_pos": 157, "type": "TASK", "confidence": 0.7619079152743021}]}, {"text": "We have implemented an open source translation system Sinuhe based on the proposed translation model.", "labels": [], "entities": []}, {"text": "Our experiments on Europarl and GigaFrEn corpora demonstrate that finding the unique MAP parameters for the model on large scale data is feasible with simple stochastic gradient methods.", "labels": [], "entities": [{"text": "Europarl", "start_pos": 19, "end_pos": 27, "type": "DATASET", "confidence": 0.9929220676422119}, {"text": "GigaFrEn corpora", "start_pos": 32, "end_pos": 48, "type": "DATASET", "confidence": 0.7909852266311646}]}, {"text": "Sinuhe is fast and memory efficient, and the BLEU scores obtained by it are only slightly inferior to those of Moses.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 45, "end_pos": 49, "type": "METRIC", "confidence": 0.999491810798645}]}], "introductionContent": [{"text": "In current phrase-based statistical machine translation systems such as Moses 1 (), the translation model is defined in terms of phrase pairs (biphrases) extracted from a bilingual corpus as follows.", "labels": [], "entities": [{"text": "phrase-based statistical machine translation", "start_pos": 11, "end_pos": 55, "type": "TASK", "confidence": 0.5690708830952644}]}, {"text": "The corpus is first word-aligned using a word alignment heuristic).", "labels": [], "entities": []}, {"text": "The phrase extraction heuristic then extracts all the biphrases that are compatible with the word alignment ().", "labels": [], "entities": [{"text": "phrase extraction", "start_pos": 4, "end_pos": 21, "type": "TASK", "confidence": 0.7416436970233917}]}, {"text": "This way, each sentence pair may generate any number of potentially overlapping biphrases.", "labels": [], "entities": []}, {"text": "However, when defining the phrase-based sentence level translation model, phrase overlaps are explicitly disallowed: The source sentence is segmented into disjoint phrases, which are translated independently using conditional phrase-level translation models that have been estimated from extracted biphrase counts.", "labels": [], "entities": [{"text": "phrase-based sentence level translation", "start_pos": 27, "end_pos": 66, "type": "TASK", "confidence": 0.6003647595643997}]}, {"text": "The disparity between the phrase extraction heuristic and the use of the extracted biphrases can be addressed in at least three ways.", "labels": [], "entities": [{"text": "phrase extraction heuristic", "start_pos": 26, "end_pos": 53, "type": "TASK", "confidence": 0.7626567085584005}]}, {"text": "One approach is to simply ignore the disparity as is done, e.g., in Moses.", "labels": [], "entities": []}, {"text": "While empirically succesful, this approach is hard to justify theoretically, and begs the question of whether more principled methods might lead to better translation results.", "labels": [], "entities": []}, {"text": "The other extensively studied approach is to replace the phrase extraction heuristic with a method that better matches the use of the extracted phrases (see, e.g.,) and the references therein).", "labels": [], "entities": [{"text": "phrase extraction heuristic", "start_pos": 57, "end_pos": 84, "type": "TASK", "confidence": 0.7961457272370657}]}, {"text": "While theoretically sound, this approach is computationally challenging both in practice (  and in theory , may suffer from reference reachability problems), and in the end may lead to inferior translation quality (.", "labels": [], "entities": []}, {"text": "In this paper, we study a third alternative.", "labels": [], "entities": []}, {"text": "We propose anew translation model that is compatible with the phrase extraction heuristic.", "labels": [], "entities": [{"text": "phrase extraction heuristic", "start_pos": 62, "end_pos": 89, "type": "TASK", "confidence": 0.8460068305333456}]}, {"text": "The proposed machine learning inspired translation model takes the form of a conditional exponential family probability distribution over a feature representation for word-aligned sentence pairs.", "labels": [], "entities": []}, {"text": "The feature representation represents a word-aligned sentence pair by essentially enumerating the (multi)set of biphrases that would have been extracted from it, together with the source positions at which the biphrases occur.", "labels": [], "entities": []}, {"text": "The model's predictions are conditional probabilities for such sets of biphrases given the source sentence.", "labels": [], "entities": []}, {"text": "The chosen feature representation has many advantages.", "labels": [], "entities": []}, {"text": "Since all word-aligned sentence pairs can be represented, reference reachability problems are automatically circumvented.", "labels": [], "entities": [{"text": "reference reachability", "start_pos": 58, "end_pos": 80, "type": "TASK", "confidence": 0.6973420977592468}]}, {"text": "For example, if the translation of a sentence consisted solely of words that do not occur in the phrase table, then the feature vector for the translation would be the all zero vector.", "labels": [], "entities": []}, {"text": "As the training data receives nonzero probability, maximum likelihood or maximum a posteriori (MAP) parameters for the model can be estimated in a principled way without resorting to pseudo-references.", "labels": [], "entities": [{"text": "maximum likelihood or maximum a posteriori (MAP)", "start_pos": 51, "end_pos": 99, "type": "METRIC", "confidence": 0.783983180920283}]}, {"text": "The fact that the model is not restricted to using disjoint biphrases means that the information in biphrase overlaps is automatically taken into account.", "labels": [], "entities": []}, {"text": "This may help in smoothing the model's predictions on long and rare phrases, and in enhancing fluency at places that otherwise would be phrase boundaries.", "labels": [], "entities": []}, {"text": "Also, the model can be extended in a principled way by introducing additional features (e.g., translations from a dictionary, biphrases with gaps, biphrases over POS tags,.", "labels": [], "entities": []}, {"text": "). The proposed model has one parameter per biphrase feature, so the total number of parameters is easily millions or more.", "labels": [], "entities": []}, {"text": "Still, the model structure is designed so that feature expectations and related quantities can be computed efficiently by dynamic programming.", "labels": [], "entities": []}, {"text": "It is thus feasible to compute the gradient of the MAP objective, and simple gradient ascent can be used to efficiently find the globally optimal model parameters (with respect to a suitably scaled Gaussian prior used for regularization).", "labels": [], "entities": []}, {"text": "Exact inference is also possible by dynamic programming when translations are predicted by the translation model alone.", "labels": [], "entities": []}, {"text": "When other features like a language model are included, one needs to resort to beam search type approximate dynamic programming for decoding.", "labels": [], "entities": []}, {"text": "We have implemented a translation system called Sinuhe based on the proposed translation model.", "labels": [], "entities": []}, {"text": "The system has been released under the GPLv3 open source license.", "labels": [], "entities": []}, {"text": "Our experiments on Europarl and GigaFrEn corpora demonstrate that the proposed translation model scales well to large data, and offers translation quality that is only slightly worse than that of the baseline system Moses.", "labels": [], "entities": [{"text": "Europarl", "start_pos": 19, "end_pos": 27, "type": "DATASET", "confidence": 0.9887840747833252}]}, {"text": "In terms of translation speed, Sinuhe is already clearly better.", "labels": [], "entities": [{"text": "translation", "start_pos": 12, "end_pos": 23, "type": "TASK", "confidence": 0.9509360194206238}]}, {"text": "The rest of this paper is organized as follows.", "labels": [], "entities": []}, {"text": "After briefly reviewing related work in Section 2, we describe the proposed translation model in Section 3.", "labels": [], "entities": [{"text": "translation", "start_pos": 76, "end_pos": 87, "type": "TASK", "confidence": 0.9769590497016907}]}, {"text": "Finally, experimental results are presented in Section 4, and conclusions in Section 5.", "labels": [], "entities": []}], "datasetContent": [{"text": "Our experiments are on the Europarl translation tasks following the setup used in the shared translation task of the ACL 2008 Third Workshop on Statistical Machine Translation, and on the French-to-English translation task of the EACL 2009 Fourth Workshop on Statistical Machine Translation).", "labels": [], "entities": [{"text": "Europarl translation", "start_pos": 27, "end_pos": 47, "type": "TASK", "confidence": 0.6975970417261124}, {"text": "shared translation task of the ACL 2008 Third Workshop on Statistical Machine Translation", "start_pos": 86, "end_pos": 175, "type": "TASK", "confidence": 0.6906287968158722}, {"text": "French-to-English translation task of the EACL 2009 Fourth Workshop on Statistical Machine Translation", "start_pos": 188, "end_pos": 290, "type": "TASK", "confidence": 0.85303428539863}]}, {"text": "The size of the Europarl training corpora is about 1M sentence pairs per language pair, while the larger GigaFrEn corpus contains about 22M sentence pairs.", "labels": [], "entities": [{"text": "Europarl training corpora", "start_pos": 16, "end_pos": 41, "type": "DATASET", "confidence": 0.9437533815701803}, {"text": "GigaFrEn corpus", "start_pos": 105, "end_pos": 120, "type": "DATASET", "confidence": 0.9285574853420258}]}, {"text": "The corpora were used for biphrase extraction and translation model training.", "labels": [], "entities": [{"text": "biphrase extraction", "start_pos": 26, "end_pos": 45, "type": "TASK", "confidence": 0.8816004693508148}, {"text": "translation model training", "start_pos": 50, "end_pos": 76, "type": "TASK", "confidence": 0.9511861403783163}]}, {"text": "Decoder feature weights were tuned on the provided development sets.", "labels": [], "entities": []}, {"text": "In case of Europarl, language models were trained on the target sides of es-en en-es fr-en en-fr de-en en-de the bilingual corpora.", "labels": [], "entities": [{"text": "Europarl", "start_pos": 11, "end_pos": 19, "type": "DATASET", "confidence": 0.9592351317405701}]}, {"text": "In the GigaFrEn experiments we used the provided monolingual news domain data.", "labels": [], "entities": []}, {"text": "All data was tokenized and lowercased using the tools in the Moses distribution.", "labels": [], "entities": []}, {"text": "We experimented with four translation systems: Sinuhe trans , Sinuhe, Moses trans , and Moses.", "labels": [], "entities": []}, {"text": "Sinuhe trans uses only the translation model in producing translations (see Section 3.5), while the full system Sinuhe uses also a language model and some additional features (see Section 3.6).", "labels": [], "entities": []}, {"text": "As a baseline, we used the Moses translation system, which is known to be very competitive on the Europarl translation tasks as evidenced by the University of Edinburgh entries in the translation challenge.", "labels": [], "entities": [{"text": "Europarl translation tasks", "start_pos": 98, "end_pos": 124, "type": "TASK", "confidence": 0.6813306212425232}, {"text": "translation challenge", "start_pos": 184, "end_pos": 205, "type": "TASK", "confidence": 0.9221009612083435}]}, {"text": "The other comparison point Moses trans was obtained from Moses by disabling distortions and setting setting the weights of all features except the forward translation model to 0.", "labels": [], "entities": []}, {"text": "By comparing Sinuhe trans and Moses trans , we hope to indirectly compare the performance of the underlying translation models.", "labels": [], "entities": []}, {"text": "A more direct comparison was not possible as it is not feasible to normalize the \"probabilities\" predicted by the Moses translation model.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Left: The translation quality of the SMT systems as measured by the BLEU score. Translations  were detokenized but not recased before evaluating their quality against lowercased reference translations  by the mteval-v11b.pl script. Right: Average total translation time in seconds.", "labels": [], "entities": [{"text": "SMT", "start_pos": 47, "end_pos": 50, "type": "TASK", "confidence": 0.9889074563980103}, {"text": "BLEU", "start_pos": 78, "end_pos": 82, "type": "METRIC", "confidence": 0.9989694356918335}]}]}