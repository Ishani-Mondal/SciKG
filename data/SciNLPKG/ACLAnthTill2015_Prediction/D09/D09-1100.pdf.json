{"title": [{"text": "Reading to Learn: Constructing Features from Semantic Abstracts", "labels": [], "entities": []}], "abstractContent": [{"text": "Machine learning offers a range of tools for training systems from data, but these methods are only as good as the underlying representation.", "labels": [], "entities": []}, {"text": "This paper proposes to acquire representations for machine learning by reading text written to accommodate human learning.", "labels": [], "entities": []}, {"text": "We propose a novel form of semantic analysis called reading to learn, where the goal is to obtain a high-level semantic abstract of multiple documents in a representation that facilitates learning.", "labels": [], "entities": [{"text": "semantic analysis", "start_pos": 27, "end_pos": 44, "type": "TASK", "confidence": 0.7704132795333862}]}, {"text": "We obtain this abstract through a generative model that requires no labeled data, instead leveraging repetition across multiple documents.", "labels": [], "entities": []}, {"text": "The semantic abstract is converted into a transformed feature space for learning, resulting in improved generalization on a rela-tional learning task.", "labels": [], "entities": []}], "introductionContent": [{"text": "Machine learning offers a range of powerful tools for training systems to act in complex environments, but these methods depend on a well-chosen representation for features.", "labels": [], "entities": []}, {"text": "For learning to succeed the representation often must be crafted with knowledge about the application domain.", "labels": [], "entities": []}, {"text": "This poses a bottleneck, requiring expertise in both machine learning and the application domain.", "labels": [], "entities": []}, {"text": "However, domain experts often express their knowledge through text; one direct expression is through text designed to aid human learning.", "labels": [], "entities": []}, {"text": "In this paper we exploit text written by domain experts in order to build a more expressive representation for learning.", "labels": [], "entities": []}, {"text": "We term this approach reading to learn.", "labels": [], "entities": []}, {"text": "The following scenario demonstrates the motivation for reading to learn.", "labels": [], "entities": []}, {"text": "Imagine an agent given a task within its world/environment.", "labels": [], "entities": []}, {"text": "The agent has no prior knowledge of the task but can perceive the world through low-level sensors.", "labels": [], "entities": []}, {"text": "Learning directly from the sensors maybe difficult, as interesting tasks typically require a complex combination of sensors.", "labels": [], "entities": []}, {"text": "Our goal is to acquire domain knowledge through the semantic analysis of text, so as to produce higher-level relations through combinations of sensors.", "labels": [], "entities": []}, {"text": "As a concrete example consider the problem of learning how to make legal moves in Freecell solitaire.", "labels": [], "entities": [{"text": "Freecell solitaire", "start_pos": 82, "end_pos": 100, "type": "DATASET", "confidence": 0.9670464098453522}]}, {"text": "Relevant sensors may indicate if an object is a card or a freecell, whether a card is a certain value, and whether two values are in sequence.", "labels": [], "entities": []}, {"text": "Although it is possible to express the rules with a combination of sensors, learning this combination is difficult.", "labels": [], "entities": []}, {"text": "Text can facilitate learning by providing relations at the appropriate level of generalization.", "labels": [], "entities": []}, {"text": "For example, the sentence: \"You can place a card on an empty freecell,\" suggests not only which sensors are useful together but also how these sensors should be linked.", "labels": [], "entities": []}, {"text": "Assuming the sensors are represented as predicates, one possible relation this sentence suggests is: r(x, y) = card(x) \u2227 freecell(y) \u2227 empty(y).", "labels": [], "entities": []}, {"text": "Armed with this new relation the agent's learning task maybe simpler.", "labels": [], "entities": []}, {"text": "Throughout the paper we refer to low-level sensory input as sensor or predicate, and to a higher level concept as a logical formula or relation.", "labels": [], "entities": []}, {"text": "Our approach to semantic analysis does not require a complete semantic representation of the text.", "labels": [], "entities": [{"text": "semantic analysis", "start_pos": 16, "end_pos": 33, "type": "TASK", "confidence": 0.9296142160892487}]}, {"text": "We merely wish to acquire a semantic abstract of a document or document collection, and use the discovered relations to facilitate datadriven learning.", "labels": [], "entities": []}, {"text": "This will allow us to directly evaluate the contribution of the extracted relations for learning.", "labels": [], "entities": []}, {"text": "We develop an approach to recover semantic abstracts that uses minimal supervision: we assume only a very small set of lexical glosses, which map from words to sensors.", "labels": [], "entities": []}, {"text": "This marks a substantial departure from previous work on semantic parsing, which requires either annotations of the meanings of each individual sentence, or alignments of sentences to grounded representations of the world.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 57, "end_pos": 73, "type": "TASK", "confidence": 0.7251901030540466}]}, {"text": "For the purpose of learning, this approach maybe inapplicable, as such text is often written at a high level of abstraction that permits no grounded representation.", "labels": [], "entities": []}, {"text": "There are two properties of our setting that make unsupervised learning feasible.", "labels": [], "entities": []}, {"text": "First, it is not necessary to extract a semantic representation of each individual sentence, but rather a summary of the semantics of the document collection.", "labels": [], "entities": []}, {"text": "Errors in the semantic abstract are not fatal, as long it guides the learning component towards a more useful representation.", "labels": [], "entities": []}, {"text": "Second, we can exploit repetition across documents, which should generally express the same underlying meaning.", "labels": [], "entities": []}, {"text": "Logical formulae that are well-supported by multiple documents are especially likely to be useful.", "labels": [], "entities": []}, {"text": "The rest of this paper describes our approach for recovering semantic abstracts and outlines how we apply and evaluate this approach on the Freecell domain.", "labels": [], "entities": [{"text": "Freecell domain", "start_pos": 140, "end_pos": 155, "type": "DATASET", "confidence": 0.8795311450958252}]}, {"text": "The paper contributes the following key ideas: (1) Interpreting abstract \"instructional\" text, written at a level that does not correspond to concrete sensory inputs in the world, so that no grounded representation is possible, (2) reading to learn, anew setting in which extracted semantic representations are evaluated by whether they facilitate learning; (3) abstractive semantic summarization, aimed at capturing broad semantic properties of a multi-document dataset, rather than a semantic parse of individual sentences; (4) a novel, minimally-supervised generative model for semantic analysis which leverages both lexical and syntactic properties of text.", "labels": [], "entities": [{"text": "Interpreting abstract \"instructional\" text", "start_pos": 51, "end_pos": 93, "type": "TASK", "confidence": 0.843257486820221}, {"text": "abstractive semantic summarization", "start_pos": 362, "end_pos": 396, "type": "TASK", "confidence": 0.6200343867142996}, {"text": "semantic analysis", "start_pos": 581, "end_pos": 598, "type": "TASK", "confidence": 0.7999342381954193}]}], "datasetContent": [{"text": "Our experimental setup is designed to evaluate the quality of the semantic abstraction performed by our model.", "labels": [], "entities": []}, {"text": "The logical formulae obtained by our system are applied as features for relational learning of the rules of the game of Freecell solitaire.", "labels": [], "entities": [{"text": "Freecell solitaire", "start_pos": 120, "end_pos": 138, "type": "DATASET", "confidence": 0.9144023358821869}]}, {"text": "We investigate whether these features enable better generalization given varying number of training examples of Freecell game states.", "labels": [], "entities": [{"text": "generalization", "start_pos": 52, "end_pos": 66, "type": "TASK", "confidence": 0.9693137407302856}]}, {"text": "We also quantify the specific role of syntax, lexical choice, and feature expressivity in learning performance.", "labels": [], "entities": []}, {"text": "This section describes the details of this evaluation.", "labels": [], "entities": []}, {"text": "We compare four different feature sets, which will be provided to the ALEPH ILP learner.", "labels": [], "entities": [{"text": "ALEPH ILP learner", "start_pos": 70, "end_pos": 87, "type": "DATASET", "confidence": 0.899341901143392}]}, {"text": "All feature sets include the sensor-level predicates shown in.", "labels": [], "entities": []}, {"text": "The FULL-MODEL feature set also includes the top logical formulae obtained in our model's semantic abstract (see Sec-tion 4.3).", "labels": [], "entities": [{"text": "FULL-MODEL", "start_pos": 4, "end_pos": 14, "type": "METRIC", "confidence": 0.8908629417419434}]}, {"text": "The NO-SYNTAX feature set is obtained from a variant of our model in which the influence of syntax is removed by setting parameters \u03b1, \u03b2 = 0.", "labels": [], "entities": [{"text": "NO-SYNTAX feature set", "start_pos": 4, "end_pos": 25, "type": "DATASET", "confidence": 0.7966947158177694}]}, {"text": "The SENSORS-ONLY feature set uses only the sensor-level predicates.", "labels": [], "entities": [{"text": "SENSORS-ONLY feature set", "start_pos": 4, "end_pos": 28, "type": "DATASET", "confidence": 0.6840415199597677}]}, {"text": "Finally, the RELATIONAL-RANDOM feature set is constructed by replacing each feature in the FULL-MODEL set with a randomly generated relational feature of identical expressivity (each predicate is replaced by a randomly chosen alternative with identical arity; terms are also assigned randomly).", "labels": [], "entities": [{"text": "RELATIONAL-RANDOM", "start_pos": 13, "end_pos": 30, "type": "METRIC", "confidence": 0.8880882263183594}, {"text": "FULL-MODEL", "start_pos": 91, "end_pos": 101, "type": "METRIC", "confidence": 0.8226557970046997}]}, {"text": "This ensures that any performance gains obtained by our model were not due merely to the greater expressivity of its relational features.", "labels": [], "entities": []}, {"text": "The number of features included in each scenario is tuned on a development set of test examples.", "labels": [], "entities": []}, {"text": "The performance metric assesses the ability of the ILP learner to classify proposed Freecell moves as legal or illegal.", "labels": [], "entities": [{"text": "ILP learner", "start_pos": 51, "end_pos": 62, "type": "DATASET", "confidence": 0.8758817315101624}]}, {"text": "As the evaluation set contains an equal number of positive and negative examples, accuracy is the appropriate metric.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 82, "end_pos": 90, "type": "METRIC", "confidence": 0.9994553923606873}]}, {"text": "The training scenarios are randomly generated; we repeat each run 50 times and average our results.", "labels": [], "entities": []}, {"text": "For the RELATIONAL-RANDOM feature set -in which predicates and terms are chosen randomly -we also regenerate the formulae per run.", "labels": [], "entities": [{"text": "RELATIONAL-RANDOM", "start_pos": 8, "end_pos": 25, "type": "METRIC", "confidence": 0.9411606192588806}]}, {"text": "shows a comparison of the results using the setup described above.", "labels": [], "entities": []}, {"text": "Our FULL-MODEL achieves the best performance at every training set size, consistently outperforming the SENSORS-ONLY representation by an absolute difference of three to four percent.", "labels": [], "entities": [{"text": "FULL-MODEL", "start_pos": 4, "end_pos": 14, "type": "METRIC", "confidence": 0.9704571962356567}, {"text": "SENSORS-ONLY", "start_pos": 104, "end_pos": 116, "type": "METRIC", "confidence": 0.9687291979789734}]}, {"text": "This demonstrates the semantic abstract obtained by our model does indeed facilitate machine learning in this domain.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Results as number of training examples  varied. Each value represents the accuracy of the  induced rules obtained with the given feature set.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 84, "end_pos": 92, "type": "METRIC", "confidence": 0.999180018901825}]}]}