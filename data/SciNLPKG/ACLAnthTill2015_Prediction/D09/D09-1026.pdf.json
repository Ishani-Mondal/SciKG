{"title": [{"text": "Labeled LDA: A supervised topic model for credit attribution in multi-labeled corpora", "labels": [], "entities": []}], "abstractContent": [{"text": "A significant portion of the world's text is tagged by readers on social bookmark-ing websites.", "labels": [], "entities": []}, {"text": "Credit attribution is an inherent problem in these corpora because most pages have multiple tags, but the tags do not always apply with equal specificity across the whole document.", "labels": [], "entities": [{"text": "Credit attribution", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.6400586366653442}]}, {"text": "Solving the credit attribution problem requires associating each word in a document with the most appropriate tags and vice versa.", "labels": [], "entities": [{"text": "credit attribution", "start_pos": 12, "end_pos": 30, "type": "TASK", "confidence": 0.6795664429664612}]}, {"text": "This paper introduces Labeled LDA, a topic model that constrains Latent Dirichlet Allocation by defining a one-to-one correspondence between LDA's latent topics and user tags.", "labels": [], "entities": []}, {"text": "This allows Labeled LDA to directly learn word-tag correspondences.", "labels": [], "entities": []}, {"text": "We demonstrate Labeled LDA's improved expressiveness over traditional LDA with visualizations of a corpus of tagged web pages from del.icio.us.", "labels": [], "entities": []}, {"text": "Labeled LDA out-performs SVMs by more than 3 to 1 when extracting tag-specific document snippets.", "labels": [], "entities": []}, {"text": "As a multi-label text classifier, our model is competitive with a discriminative base-line on a variety of datasets.", "labels": [], "entities": []}], "introductionContent": [{"text": "From news sources such as Reuters to modern community web portals like del.icio.us, a significant proportion of the world's textual data is labeled with multiple human-provided tags.", "labels": [], "entities": []}, {"text": "These collections reflect the fact that documents are often about more than one thing-for example, a news story about a highway transportation bill might naturally be filed under both transportation and politics, with neither category acting as a clear subset of the other.", "labels": [], "entities": []}, {"text": "Similarly, a single web page in del.icio.us might well be annotated with tags as diverse as arts, physics, alaska, and beauty.", "labels": [], "entities": []}, {"text": "However, not all tags apply with equal specificity across the whole document, opening up new opportunities for information retrieval and corpus analysis on tagged corpora.", "labels": [], "entities": [{"text": "information retrieval", "start_pos": 111, "end_pos": 132, "type": "TASK", "confidence": 0.7882743775844574}, {"text": "corpus analysis", "start_pos": 137, "end_pos": 152, "type": "TASK", "confidence": 0.697513610124588}]}, {"text": "For instance, users who browse for documents with a particular tag might prefer to see summaries that focus on the portion of the document most relevant to the tag, a task we call tag-specific snippet extraction.", "labels": [], "entities": [{"text": "tag-specific snippet extraction", "start_pos": 180, "end_pos": 211, "type": "TASK", "confidence": 0.6766239702701569}]}, {"text": "And when a user browses to a particular document, a tag-augmented user interface might provide overview visualization cues highlighting which portions of the document are more or less relevant to the tag, helping the user quickly access the information they seek.", "labels": [], "entities": []}, {"text": "One simple approach to these challenges can be found in models that explicitly address the credit attribution problem by associating individual words in a document with their most appropriate labels.", "labels": [], "entities": []}, {"text": "For instance, in our news story about the transportation bill, if the model knew that the word \"highway\" went with transportation and that the word \"politicians\" went with politics, more relevant passages could be extracted for either label.", "labels": [], "entities": []}, {"text": "We seek an approach that can automatically learn the posterior distribution of each word in a document conditioned on the document's label set.", "labels": [], "entities": []}, {"text": "One promising approach to the credit attribution problem lies in the machinery of Latent Dirichlet Allocation (LDA) (), a recent model that has gained popularity among theoreticians and practitioners alike as a tool for automatic corpus summarization and visualization.", "labels": [], "entities": [{"text": "credit attribution problem", "start_pos": 30, "end_pos": 56, "type": "TASK", "confidence": 0.7943059404691061}, {"text": "Latent Dirichlet Allocation (LDA)", "start_pos": 82, "end_pos": 115, "type": "METRIC", "confidence": 0.8957952857017517}, {"text": "corpus summarization", "start_pos": 230, "end_pos": 250, "type": "TASK", "confidence": 0.5263463407754898}]}, {"text": "LDA is a completely unsupervised algorithm that models each document as a mixture of topics.", "labels": [], "entities": []}, {"text": "The model generates automatic summaries of topics in terms of a discrete probability distribution over words for each topic, and further infers per-document discrete distributions over topics.", "labels": [], "entities": []}, {"text": "Most importantly, LDA makes the explicit assumption that each word is generated from one underlying topic.", "labels": [], "entities": []}, {"text": "Although LDA is expressive enough to model multiple topics per document, it is not appropriate for multi-labeled corpora because, as an unsupervised model, it offers no obvious way of incorporating a supervised label set into its learning procedure.", "labels": [], "entities": []}, {"text": "In particular, LDA often learns some topics that are hard to interpret, and the model provides no tools for tuning the generated topics to suit an end-use application, even when time and resources exist to provide some document labels.", "labels": [], "entities": []}, {"text": "Several modifications of LDA to incorporate supervision have been proposed in the literature.", "labels": [], "entities": []}, {"text": "Two such models, Supervised LDA and DiscLDA ( are inappropriate for multiply labeled corpora because they limit a document to being associated with only a single label.", "labels": [], "entities": []}, {"text": "Supervised LDA posits that a label is generated from each document's empirical topic mixture distribution.", "labels": [], "entities": []}, {"text": "DiscLDA associates a single categorical label variable with each document and associates a topic mixture with each label.", "labels": [], "entities": []}, {"text": "A third model, MM-LDA, is not constrained to one label per document because it models each document as a bag of words with a bag of labels, with topics for each observation drawn from a shared topic distribution.", "labels": [], "entities": []}, {"text": "But, like the other models, MM-LDA's learned topics do not correspond directly with the label set.", "labels": [], "entities": []}, {"text": "Consequently, these models fall short as a solution to the credit attribution problem.", "labels": [], "entities": [{"text": "credit attribution", "start_pos": 59, "end_pos": 77, "type": "TASK", "confidence": 0.7258141040802002}]}, {"text": "Because labels have meaning to the people that assigned them, a simple solution to the credit attribution problem is to assign a document's words to its labels rather than to a latent and possibly less interpretable semantic space.", "labels": [], "entities": []}, {"text": "This paper presents Labeled LDA (L-LDA), a generative model for multiply labeled corpora that marries the multi-label supervision common to modern text datasets with the word-assignment ambiguity resolution of the LDA family of models.", "labels": [], "entities": []}, {"text": "In contrast to standard LDA and its existing supervised variants, our model associates each label with one topic indirect correspondence.", "labels": [], "entities": []}, {"text": "In the following section, L-LDA is shown to be a natural extension of both LDA (by incorporating supervision) and Multinomial Naive Bayes (by incorporating a mixture model).", "labels": [], "entities": []}, {"text": "We demonstrate that L-LDA can go along way toward solving the credit attribution problem in multiply labeled documents with improved interpretability over LDA (Section 4).", "labels": [], "entities": []}, {"text": "We show that L-LDA's credit attribution ability enables it to greatly outperform sup- port vector machines on a tag-driven snippet extraction task on web pages from del.icio.us (Section 6).", "labels": [], "entities": [{"text": "tag-driven snippet extraction task on web pages", "start_pos": 112, "end_pos": 159, "type": "TASK", "confidence": 0.796588829585484}]}, {"text": "And despite its generative semantics, we show that Labeled LDA is competitive with a strong baseline discriminative classifier on two multi-label text classification tasks (Section 7).", "labels": [], "entities": [{"text": "generative semantics", "start_pos": 16, "end_pos": 36, "type": "TASK", "confidence": 0.9283218383789062}, {"text": "multi-label text classification", "start_pos": 134, "end_pos": 165, "type": "TASK", "confidence": 0.6748481591542562}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 2: Human judgments of tag-specific snippet  quality as extracted by L-LDA and SVM. The cen- ter column is the number of document-tag pairs for  which a system's snippet was judged superior. The  right column is the number of snippets for which  all three annotators were in complete agreement  (numerator) in the subset of document scored by  all three annotators (denominator).", "labels": [], "entities": []}]}