{"title": [{"text": "Matching Reviews to Objects using a Language Model", "labels": [], "entities": [{"text": "Matching", "start_pos": 0, "end_pos": 8, "type": "TASK", "confidence": 0.9742105007171631}]}], "abstractContent": [{"text": "We develop a general method to match un-structured text reviews to a structured list of objects.", "labels": [], "entities": []}, {"text": "For this, we propose a language model for generating reviews that incorporates a description of objects and a generic review language model.", "labels": [], "entities": []}, {"text": "This mixture model gives us a principled method to find, given a review, the object most likely to be the topic of the review.", "labels": [], "entities": []}, {"text": "Extensive experiments and analysis on reviews from Yelp show that our language model-based method vastly outperforms traditional tf-idf-based methods.", "labels": [], "entities": [{"text": "Yelp", "start_pos": 51, "end_pos": 55, "type": "DATASET", "confidence": 0.9297109246253967}]}], "introductionContent": [{"text": "Consider a user searching for reviews of \"Casablanca Moroccan Restaurant.\"", "labels": [], "entities": [{"text": "Casablanca Moroccan Restaurant", "start_pos": 42, "end_pos": 72, "type": "DATASET", "confidence": 0.9294323126475016}]}, {"text": "The search engine would like to obtain as many reviews of this restaurant as possible, both to offer a highquality result set for even obscure restaurants, and to enable advanced applications such as aggregation/summarization/categorization of reviews and recommendation of alternate restaurants.", "labels": [], "entities": [{"text": "summarization/categorization of reviews", "start_pos": 212, "end_pos": 251, "type": "TASK", "confidence": 0.7286852598190308}]}, {"text": "To solve this problem, it faces two high-level challenges: first, identify the restaurant review pages on the Web; and second, given a review, identify the restaurant that is being reviewed.", "labels": [], "entities": []}, {"text": "There has been previous work addressing the first challenge (Section 2).", "labels": [], "entities": []}, {"text": "We focus in this paper on the second.", "labels": [], "entities": []}, {"text": "The Web is replete with restaurant reviews available on top restaurant verticals such as Yelp and CitySearch, as well as newspaper articles, newsgroup discussions, blog posts, small local review aggregators and so forth.", "labels": [], "entities": [{"text": "Yelp", "start_pos": 89, "end_pos": 93, "type": "DATASET", "confidence": 0.9522163271903992}]}, {"text": "Ideally, the search engine would like to obtain reviews from all possible sources.", "labels": [], "entities": []}, {"text": "While identifying the subject matter of a given review on the large sites maybe amenable to structured extraction through wrapper induction or related techniques, it is typically not cost-effective to apply such techniques to smaller \"tail\" sites, and purely unstructured sources require alternate approaches altogether.", "labels": [], "entities": [{"text": "structured extraction", "start_pos": 92, "end_pos": 113, "type": "TASK", "confidence": 0.7235000431537628}]}, {"text": "In this paper, we explore the setting of matching reviews to objects using only their textual content.", "labels": [], "entities": []}, {"text": "Note that matching reviews to objects is a pervasive problem beyond the restaurant domain.", "labels": [], "entities": []}, {"text": "Shopping verticals like to aggregate camera reviews, entertainment verticals wish to collect movie reviews, and soon.", "labels": [], "entities": []}, {"text": "We use restaurant reviews as a running example, but the techniques are general.", "labels": [], "entities": []}, {"text": "More specifically, the problem we consider in this paper is the following.", "labels": [], "entities": []}, {"text": "Given a list of structured objects (restaurants/cameras/movies) and a text review, identify the object from the list that is the topic of the review.", "labels": [], "entities": []}, {"text": "Our focus on textual content allows us to expand the universe of sources from which we can extract reviews to include sources that are purely textual, such as forum posts, blog posts, newsgroup postings, and the like.", "labels": [], "entities": []}, {"text": "In fact, even among collections of \"structured\" sources like review aggregators, there are no highly accurate unsupervised techniques to match a known review page to an object.", "labels": [], "entities": []}, {"text": "Structured (e.g., HTML) cues provide valuable leverage in attacking this problem, but the types of textual cues we focus on are also a key part of the puzzle; in such a context, our techniques can still contribute to the overall matching problem.", "labels": [], "entities": []}, {"text": "It is important to contrast our problem against two settings of related flavor -entity matching, whose goal is to find the correspondence between two structured objects and information retrieval (IR), whose goal is to match unstructured short text (query) against unstructured text (document).", "labels": [], "entities": []}, {"text": "Our problem is considerably harder than entity matching for the following reasons.", "labels": [], "entities": [{"text": "entity matching", "start_pos": 40, "end_pos": 55, "type": "TASK", "confidence": 0.7889639139175415}]}, {"text": "In matching two structured objects there is often a natural correspondence between their attributes, whereas no such correspondence exists between an object and its review.", "labels": [], "entities": []}, {"text": "For instance, while trying to match a review to a restaurant object, it is unclear if a specific portion of the review refers to the name of the restaurant, or to its location, or is a statement not concerning specifics of the restaurant.", "labels": [], "entities": []}, {"text": "Moreover, even if we wish to use entity matching, we must first recognize the entities from a review.", "labels": [], "entities": [{"text": "entity matching", "start_pos": 33, "end_pos": 48, "type": "TASK", "confidence": 0.7223532795906067}]}, {"text": "There are two methods to do this, namely, wrapper induction and information extraction.", "labels": [], "entities": [{"text": "wrapper induction", "start_pos": 42, "end_pos": 59, "type": "TASK", "confidence": 0.9200721085071564}, {"text": "information extraction", "start_pos": 64, "end_pos": 86, "type": "TASK", "confidence": 0.8971753418445587}]}, {"text": "Wrapper induction methods have serious limitations: they are applicable only to highly-structured websites and involve human labeling effort that is expensive and error-prone and entails constant maintenance to keep wrappers up-to-date.", "labels": [], "entities": [{"text": "Wrapper induction", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.9125634133815765}]}, {"text": "Information extraction methods, on the other hand, often have limited accuracy.", "labels": [], "entities": [{"text": "Information extraction", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.8489263653755188}, {"text": "accuracy", "start_pos": 70, "end_pos": 78, "type": "METRIC", "confidence": 0.9970494508743286}]}, {"text": "Our problem is also not amenable to classical IR methods such as tf-idf.", "labels": [], "entities": []}, {"text": "For example, suppose we want to find the relevant restaurant fora given review.", "labels": [], "entities": []}, {"text": "The standard tf-idf will treat the review as the query, the set of restaurant as documents and compute the tf-idf scores.", "labels": [], "entities": []}, {"text": "Now consider a restaurant called \"Food.\"", "labels": [], "entities": [{"text": "Food", "start_pos": 34, "end_pos": 38, "type": "DATASET", "confidence": 0.8062027096748352}]}, {"text": "1 Since the term \"food\" is rare as a restaurant name, it will get a very high idf score and hence will likely be the top match for all reviews containing the word \"food.\"", "labels": [], "entities": []}, {"text": "In fact, unlike in traditional IR, a \"query\" (i.e., review) is long and a \"document\" (i.e., restaurant) is short -this demands adapting established IR concepts such as inverse document frequency and document length normalization to our setting.", "labels": [], "entities": []}, {"text": "If we take the opposite view by considering reviews as documents and restaurants as queries, we still deviate from the IR setting, since now we need to rank and find the best \"query\" fora given \"document.\"", "labels": [], "entities": []}, {"text": "In Section 3.4, we illustrate the shortcomings of both these approaches.", "labels": [], "entities": []}, {"text": "In fact, the nature of the object database we consider provides several unique opportunities over traditional IR.", "labels": [], "entities": []}, {"text": "First the \"document\", i.e., the object to be matched, has more semantics, since each document is associated with one or more semantic attribute, such as the name/location of the restaurant.", "labels": [], "entities": []}, {"text": "Second, the \"query\", i.e., the text we are matching is known to be a review of the object, and hence is rendered in a language that is \"review-like\" -this can be modeled by a generative process that produces reviews from objects.", "labels": [], "entities": []}, {"text": "Third, the set of objects we are interested in is  given a priori, and we only seek to match reviews with one of these objects; this makes our problem more tractable than open-ended entity recognition.", "labels": [], "entities": [{"text": "open-ended entity recognition", "start_pos": 171, "end_pos": 200, "type": "TASK", "confidence": 0.6472099224726359}]}, {"text": "We propose a general method to match reviews to objects.", "labels": [], "entities": []}, {"text": "To this end, we postulate a language model for generating reviews.", "labels": [], "entities": []}, {"text": "The intuition behind our model is simple and natural: when a review is written about an object, each word in the review is drawn either from a description of the objector from a generic review language that is independent of the object.", "labels": [], "entities": []}, {"text": "This mixture model leads to a method to find, given a review, the object most likely to be the topic of the review.", "labels": [], "entities": []}, {"text": "Our method is light-weight and scalable and can be viewed as obviating the need for highlyexpensive information extraction.", "labels": [], "entities": [{"text": "information extraction", "start_pos": 100, "end_pos": 122, "type": "TASK", "confidence": 0.7416845411062241}]}, {"text": "Since the method is text-based and does not rely on any HTML structural clues, it is especially applicable to reviews present in blogs and the so-called tail web sites -web sites for which it is not feasible to maintain wrappers to automatically extract the object of a review.", "labels": [], "entities": []}, {"text": "We then report results on over 11K restaurant reviews from Yelp.", "labels": [], "entities": [{"text": "Yelp", "start_pos": 59, "end_pos": 63, "type": "DATASET", "confidence": 0.9252620339393616}]}, {"text": "The experiments and our extensive analysis show that our language modelbased method significantly outperforms traditional tf-idf based methods, which fail to take full advantage of the properties that are specific to our setting.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section we evaluate our proposed reviewlanguage based matching algorithm RLM.", "labels": [], "entities": []}, {"text": "We use the TFIDF and TFIDF + algorithms described in Section 3.4 as baseline algorithms.", "labels": [], "entities": []}, {"text": "Since we are comparing objects that can have varying lengths, we tried the standard cosine-normalization techniques for document length normalization.", "labels": [], "entities": [{"text": "document length normalization", "start_pos": 120, "end_pos": 149, "type": "TASK", "confidence": 0.650408019622167}]}, {"text": "For reasons described in Section 3.4, however, the normalization significantly lowered the accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 91, "end_pos": 99, "type": "METRIC", "confidence": 0.999188244342804}]}, {"text": "All the numbers reported here are using tf-idf scores without normalization.", "labels": [], "entities": []}, {"text": "For both RLM and the baseline algorithms, it is impractical to compute the similarity of a review with each object in the database.", "labels": [], "entities": [{"text": "RLM", "start_pos": 9, "end_pos": 12, "type": "TASK", "confidence": 0.945965051651001}, {"text": "similarity", "start_pos": 75, "end_pos": 85, "type": "METRIC", "confidence": 0.9722540378570557}]}, {"text": "Since all objects that do not intersect with the review have a zero score, we built an inverted index to retrieve all objects containing a given word.", "labels": [], "entities": []}, {"text": "Even a simple inverted index can be very inefficient since for each review, words such as \"Restaurant\" or \"Cafe\" retrieve a substantial fraction of the whole database.", "labels": [], "entities": []}, {"text": "Hence, we further optimized the index by looking at the document frequencies of the words and considering word bigrams in object descriptions.", "labels": [], "entities": []}, {"text": "The index only retrieves objects that have a non-trivial overlap with the review; e.g., an overlap of \"Casablanca\" is considered non-trivial while an overlap of \"Restaurant\" is considered trivial.", "labels": [], "entities": [{"text": "Casablanca", "start_pos": 103, "end_pos": 113, "type": "DATASET", "confidence": 0.8814864158630371}]}, {"text": "Once these candidates are retrieved, our scoring function takes into account all overlapping tokens.", "labels": [], "entities": []}, {"text": "For the YELP dataset, the index returns an average of 200 restaurants for each review.", "labels": [], "entities": [{"text": "YELP dataset", "start_pos": 8, "end_pos": 20, "type": "DATASET", "confidence": 0.9110023975372314}]}, {"text": "This points to the general difficulty of review matching over a large corpus of objects, since a simple dictionary-based named-entity recognition will hit at least 200 objects for many reviews.", "labels": [], "entities": [{"text": "review matching", "start_pos": 41, "end_pos": 56, "type": "TASK", "confidence": 0.6922280639410019}]}, {"text": "For RLM, we conducted initial experiments and performed parameter estimation on the development data.", "labels": [], "entities": [{"text": "RLM", "start_pos": 4, "end_pos": 7, "type": "TASK", "confidence": 0.9796223640441895}]}, {"text": "The experimental settings we used for RLM are as follows: we set g(w) = log(1/f w ) for P e , where f w is estimated on the review collection.", "labels": [], "entities": [{"text": "RLM", "start_pos": 38, "end_pos": 41, "type": "TASK", "confidence": 0.9908007979393005}]}, {"text": "P (w) is estimated on all reviews in R , where for each review, all tokens of its corresponding text(e), if present, are removed, in order to approximate the generic review language independent of e, as required by our generative model.", "labels": [], "entities": [{"text": "P (w)", "start_pos": 0, "end_pos": 5, "type": "METRIC", "confidence": 0.9178772121667862}]}, {"text": "We estimate \u03b1 to be 0.002, tuned on the development set; in our experiments, we observe that the performance is not very sensitive to \u03b1.", "labels": [], "entities": []}, {"text": "We now examine the experimental choices we made for different components of RLM by defining the following variations of RLM.", "labels": [], "entities": []}, {"text": "RLM-UNIFORM: rather than setting g(w) = log(1/f w ) for P e , we use the uniform distribution P e (w) = 1/|text(e)|.", "labels": [], "entities": []}, {"text": "From the third line of (b), there is a slight accuracy drop of \u223c 1.3%.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 46, "end_pos": 54, "type": "METRIC", "confidence": 0.9996702671051025}]}, {"text": "RLM-UNCUT: suppose we only have access to a review corpus with no alignment to text(e), and thus have to approximate P (w) by estimating it on the set of original \"un-cut\" reviews, how much does that affect our performance?", "labels": [], "entities": [{"text": "RLM-UNCUT", "start_pos": 0, "end_pos": 9, "type": "DATASET", "confidence": 0.46432748436927795}]}, {"text": "As indicated in the fourth row of Table 1 (b), this reduces accuracy by about 2% on our test data.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 60, "end_pos": 68, "type": "METRIC", "confidence": 0.9996457099914551}]}, {"text": "RLM-DECAP: as an alternative way to deal with lack of aligned data, we consider a variation of the above algorithm by removing all the capitalized words from un-annotated reviews.", "labels": [], "entities": []}, {"text": "Clearly, this can result in both \"over-cutting\" and \"undercutting\" of true restaurant name mentions.", "labels": [], "entities": []}, {"text": "However, as indicated in the fourth row of Table 1 (b), this is very close to the best accuracy achieved.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 87, "end_pos": 95, "type": "METRIC", "confidence": 0.9995039701461792}]}, {"text": "Thus, an effective model can be learned even without aligned data.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Average accuracy of the top-1 prediction  for various techniques. Micro-average computed  over 11,217 reviews in R test ; macro-average com- puted over 2,810 unique restaurants in R test .", "labels": [], "entities": [{"text": "accuracy", "start_pos": 18, "end_pos": 26, "type": "METRIC", "confidence": 0.9816772937774658}, {"text": "Micro-average", "start_pos": 76, "end_pos": 89, "type": "METRIC", "confidence": 0.9697737693786621}]}]}