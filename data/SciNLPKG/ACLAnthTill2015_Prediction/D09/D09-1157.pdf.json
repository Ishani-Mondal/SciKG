{"title": [{"text": "Classifying Relations for Biomedical Named Entity Disambiguation", "labels": [], "entities": [{"text": "Biomedical Named Entity Disambiguation", "start_pos": 26, "end_pos": 64, "type": "TASK", "confidence": 0.6502527371048927}]}], "abstractContent": [{"text": "Named entity disambiguation concerns linking a potentially ambiguous mention of named entity in text to an unambigu-ous identifier in a standard database.", "labels": [], "entities": [{"text": "Named entity disambiguation", "start_pos": 0, "end_pos": 27, "type": "TASK", "confidence": 0.6242022415002187}]}, {"text": "One approach to this task is supervised classification.", "labels": [], "entities": [{"text": "supervised classification", "start_pos": 29, "end_pos": 54, "type": "TASK", "confidence": 0.7069427967071533}]}, {"text": "However, the availability of training data is often limited, and the available data sets tend to be imbalanced and, in some cases, heterogeneous.", "labels": [], "entities": []}, {"text": "We propose anew method that distinguishes a named entity by finding the informative keywords in its surrounding context, and then trains a model to predict whether each keyword indicates the semantic class of the entity.", "labels": [], "entities": []}, {"text": "While maintaining a comparable performance to supervised classification , this method avoids using expensive manually annotated data for each new domain , and thus achieves better portability.", "labels": [], "entities": []}], "introductionContent": [{"text": "While technology on named entity recognition (NER) matures, many researchers in the field of information extraction (IE) gradually shifted their focus to more complex tasks such as named entity disambiguation and relation extraction.", "labels": [], "entities": [{"text": "named entity recognition (NER)", "start_pos": 20, "end_pos": 50, "type": "TASK", "confidence": 0.8145217895507812}, {"text": "information extraction (IE)", "start_pos": 93, "end_pos": 120, "type": "TASK", "confidence": 0.88633633852005}, {"text": "named entity disambiguation", "start_pos": 181, "end_pos": 208, "type": "TASK", "confidence": 0.6518010497093201}, {"text": "relation extraction", "start_pos": 213, "end_pos": 232, "type": "TASK", "confidence": 0.8356577754020691}]}, {"text": "Both tasks are particularly important for biomedical text mining, which concerns automatically extracting facts from the exponentially growing biomedical literature).", "labels": [], "entities": [{"text": "biomedical text mining", "start_pos": 42, "end_pos": 64, "type": "TASK", "confidence": 0.7134350339571635}]}, {"text": "One type of facts is relations between biomedical named entities, such as disease-drug relation, gene-disease relation, protein-protein interaction (PPI), etc.", "labels": [], "entities": []}, {"text": "To automatically extract these facts, advanced natural language processing techniques such as parsing have been adopted to analyse the syntactic and semantic structure of text.", "labels": [], "entities": []}, {"text": "The idea is that linguistic structures between the interacting biological entities may have common characteristics that can be exploited by similarity measures or machine learning algorithms.", "labels": [], "entities": []}, {"text": "For example, used the shortest path between two genes according to edit distance in a dependency tree to define a kernel function for extracting gene interactions.", "labels": [], "entities": []}, {"text": "comparably evaluated a number of kernels for incorporating syntactic features, including the bag-of-word kernel, the subset tree kernel) and the graph kernel (), and they concluded that combining all kernels achieved better results than using any individual one.", "labels": [], "entities": []}, {"text": "used syntactic paths as one of the features to train a support vector machines (SVM) model for PPIs and also discussed how different parsers and output representations affected the end results.", "labels": [], "entities": []}, {"text": "Another crucial IE task is named entity disambiguation, which concerns grounding mentions of named entities in text to unambiguous concepts as defined in some standard dictionary or database.", "labels": [], "entities": [{"text": "IE", "start_pos": 16, "end_pos": 18, "type": "TASK", "confidence": 0.9914843440055847}, {"text": "named entity disambiguation", "start_pos": 27, "end_pos": 54, "type": "TASK", "confidence": 0.6021233002344767}]}, {"text": "For instance, given a search term Python, users may like to seethe results grouped into the following categories: a type of snake, a programming language, or a film ().", "labels": [], "entities": []}, {"text": "One approach to such lexical disambiguation tasks is supervised classification.", "labels": [], "entities": [{"text": "supervised classification", "start_pos": 53, "end_pos": 78, "type": "TASK", "confidence": 0.7135057598352432}]}, {"text": "However, such techniques suffer from the knowledge acquisition bottleneck, meaning that manually annotating training data is costly and can never satisfy the need by the machine learning algorithms.", "labels": [], "entities": [{"text": "knowledge acquisition", "start_pos": 41, "end_pos": 62, "type": "TASK", "confidence": 0.7788600325584412}]}, {"text": "In addition, supervised techniques may not yield reliable results when the distributions of the semantic classes are different in the training and test datasets ().", "labels": [], "entities": []}, {"text": "For example, on the task of word sense disambiguation, a model trained on a dataset where the predominant sense of the word star is \"heavenly body\", may notwork well on text mainly composed of entertainment news.", "labels": [], "entities": [{"text": "word sense disambiguation", "start_pos": 28, "end_pos": 53, "type": "TASK", "confidence": 0.7494358420372009}]}, {"text": "Such problems are also major concerns when developing a system to disambiguate biomedical named entities (e.g., protein, gene, and disease), for which some researchers rely on hand-crafted rules in addition to a small amount of training data.", "labels": [], "entities": []}, {"text": "This paper proposes anew disambiguation method that, instead of classifying each individual occurrence of an entity, it classifies pair-wise relations between the entity mention in question and the \"cue words\" in its adjacent context, where each cue word is assumed to bear a semantic class.", "labels": [], "entities": []}, {"text": "We then select the cue word that has a positive relation with the entity, and pass its semantic tag to it.", "labels": [], "entities": []}, {"text": "While an individual entity mention may belong to a large number of semantic classes, a relation can only take one of two values: positive or negative, hence transforming a complex multi-classification problem into a less complicated binary classification task.", "labels": [], "entities": []}, {"text": "The remainder of the paper is organised as follows: Section 2 proposes the disambiguation method and Section 3 introduces the task of disambiguating the model organisms of biomedical named entities.", "labels": [], "entities": []}, {"text": "Section 4 describes in detail our proposed method and also a number of baseline systems for comparison purposes.", "labels": [], "entities": []}, {"text": "Section 5 shows the evaluation results and discusses the advantages and drawback of our system, and we finally conclude in Section 6.", "labels": [], "entities": []}], "datasetContent": [{"text": "The evaluation was carried out on the DEVTEST dataset, and the systems are compared using av-  eraged precision, recall and F1 scores overall species.", "labels": [], "entities": [{"text": "DEVTEST dataset", "start_pos": 38, "end_pos": 53, "type": "DATASET", "confidence": 0.9907801151275635}, {"text": "av-  eraged", "start_pos": 90, "end_pos": 101, "type": "METRIC", "confidence": 0.7777294317881266}, {"text": "precision", "start_pos": 102, "end_pos": 111, "type": "METRIC", "confidence": 0.5608688592910767}, {"text": "recall", "start_pos": 113, "end_pos": 119, "type": "METRIC", "confidence": 0.9993723034858704}, {"text": "F1", "start_pos": 124, "end_pos": 126, "type": "METRIC", "confidence": 0.9995096921920776}]}, {"text": "In more detail, for each model organism that appears in the DEVTEST dataset, we collect two lists of entity mentions of that species: one from the gold-standard DEVTEST dataset, and the other from the output of a disambiguation system.", "labels": [], "entities": [{"text": "DEVTEST dataset", "start_pos": 60, "end_pos": 75, "type": "DATASET", "confidence": 0.9832535982131958}, {"text": "DEVTEST dataset", "start_pos": 161, "end_pos": 176, "type": "DATASET", "confidence": 0.965549886226654}]}, {"text": "Then the list of system output is compared against the gold-standard list to obtain precision, recall and F1 score.", "labels": [], "entities": [{"text": "precision", "start_pos": 84, "end_pos": 93, "type": "METRIC", "confidence": 0.9998247027397156}, {"text": "recall", "start_pos": 95, "end_pos": 101, "type": "METRIC", "confidence": 0.9996658563613892}, {"text": "F1 score", "start_pos": 106, "end_pos": 114, "type": "METRIC", "confidence": 0.9814946353435516}]}, {"text": "For each system, the scores obtained from all species are averaged using microaverage and macro-average.", "labels": [], "entities": []}, {"text": "The micro-average is the mean of the summation of contingency metrics for all model organisms, so that scores of the more frequent species influence the mean more than those of less frequent ones.", "labels": [], "entities": []}, {"text": "The macro-average is the mean of precision, recall, or F1 overall labels, thus attributing equal weights to each species, and measuring a system's adaptability across different model organisms.", "labels": [], "entities": [{"text": "precision", "start_pos": 33, "end_pos": 42, "type": "METRIC", "confidence": 0.9995102882385254}, {"text": "recall", "start_pos": 44, "end_pos": 50, "type": "METRIC", "confidence": 0.9982030391693115}, {"text": "F1 overall labels", "start_pos": 55, "end_pos": 72, "type": "METRIC", "confidence": 0.9315343300501505}]}, {"text": "First of all, shows the results of the classification methods described in Section 4.2.", "labels": [], "entities": []}, {"text": "The multi-classification system using a maximum entropy model (Maxent) yielded the highest overall micro-averaged F1.", "labels": [], "entities": [{"text": "F1", "start_pos": 114, "end_pos": 116, "type": "METRIC", "confidence": 0.9213568568229675}]}, {"text": "Among the SVM-based systems, the one using IG feature selection achieved better performance.", "labels": [], "entities": []}, {"text": "In particular, it outperformed the Maxent model in term of macro-averages.", "labels": [], "entities": []}, {"text": "The performance of the SVM model with BNS feature selection is disappointing, perhaps because the occurrences of a feature in each instance are not normally distributed.", "labels": [], "entities": [{"text": "BNS feature selection", "start_pos": 38, "end_pos": 59, "type": "TASK", "confidence": 0.5782139301300049}]}, {"text": "As the Maxent system obtained better results, it was used to compare with other disambiguation systems.", "labels": [], "entities": []}, {"text": "shows the results of a number of methods described in the previous sections.", "labels": [], "entities": []}, {"text": "The methods are categorised into 4 groups: rule-based baseline systems, a Maxent classification model, relation-classification methods, and a hybrid system.", "labels": [], "entities": []}, {"text": "The difference between the relation classification systems is the features adopted.", "labels": [], "entities": [{"text": "relation classification", "start_pos": 27, "end_pos": 50, "type": "TASK", "confidence": 0.8442883193492889}]}, {"text": "Rel-Context was trained on only bag-of-word and distance features, whereas each other system also used syntactic features provided by a specific parser.", "labels": [], "entities": [{"text": "Rel-Context", "start_pos": 0, "end_pos": 11, "type": "DATASET", "confidence": 0.856808602809906}]}, {"text": "For example, the Rel-RASP system identifies an entity's species by finding positive relations between the entity and its neighbouring species words, using features including bag-of-word, distance, and dependency paths generated by RASP.", "labels": [], "entities": []}, {"text": "The hybrid system (Hbrd) ran the Rel-ENJU-Genia system on top of the outcome of Maxent.", "labels": [], "entities": [{"text": "Hbrd", "start_pos": 19, "end_pos": 23, "type": "DATASET", "confidence": 0.9157454371452332}, {"text": "Rel-ENJU-Genia", "start_pos": 33, "end_pos": 47, "type": "DATASET", "confidence": 0.7311433553695679}, {"text": "Maxent", "start_pos": 80, "end_pos": 86, "type": "DATASET", "confidence": 0.7929501533508301}]}, {"text": "When a conflict occurs, the species ID is chosen by Rel-ENJU-Genia.", "labels": [], "entities": [{"text": "Rel-ENJU-Genia", "start_pos": 52, "end_pos": 66, "type": "METRIC", "confidence": 0.6381563544273376}]}, {"text": "The idea is that the relation classification system is more accurate than Maxent when it is applicable, and hence would improve precision on disambiguating the species with few or no training instances.", "labels": [], "entities": [{"text": "relation classification", "start_pos": 21, "end_pos": 44, "type": "TASK", "confidence": 0.9397311210632324}, {"text": "precision", "start_pos": 128, "end_pos": 137, "type": "METRIC", "confidence": 0.9990180730819702}]}, {"text": "Without spreading (shown in the \"NO SPRD\" columns of), most of the rule-based and relation classification systems only work on a subset of DEVTEST, resulting in low recall: Rule-Sp works on the small proportion of entities (5.68%) with a preceding species word, while the other systems only work on the collection of sentences containing at least one species word and one entity, which covers 4.60% sentences and 22.16% entity mentions.", "labels": [], "entities": [{"text": "relation classification", "start_pos": 82, "end_pos": 105, "type": "TASK", "confidence": 0.7138390690088272}, {"text": "DEVTEST", "start_pos": 139, "end_pos": 146, "type": "DATASET", "confidence": 0.8131737112998962}, {"text": "recall", "start_pos": 165, "end_pos": 171, "type": "METRIC", "confidence": 0.9987695813179016}]}, {"text": "Rule-Majority, Maxent, and Hbrd, on the other hand, apply to all entity mentions, and therefore they are only compared against the others when spreading was applied.", "labels": [], "entities": [{"text": "Maxent", "start_pos": 15, "end_pos": 21, "type": "METRIC", "confidence": 0.9567471742630005}, {"text": "Hbrd", "start_pos": 27, "end_pos": 31, "type": "METRIC", "confidence": 0.8624618053436279}]}, {"text": "The results shown in the \"NO SPRD\" columns can be viewed as a comparative evaluation of the usefulness of the syntactic features supplied by the parsers on this particular task.", "labels": [], "entities": [{"text": "NO SPRD\"", "start_pos": 26, "end_pos": 34, "type": "METRIC", "confidence": 0.6897038022677103}]}, {"text": "The rulebased systems set high baselines: Rule-Sp produced good precision and Rule-SpSent achieved the highest micro-averaged F1, thanks to its high coverage, which is also an upperbound of recall for the relation classification systems.", "labels": [], "entities": [{"text": "precision", "start_pos": 64, "end_pos": 73, "type": "METRIC", "confidence": 0.9994800686836243}, {"text": "F1", "start_pos": 126, "end_pos": 128, "type": "METRIC", "confidence": 0.8948034644126892}, {"text": "coverage", "start_pos": 149, "end_pos": 157, "type": "METRIC", "confidence": 0.9755170941352844}, {"text": "recall", "start_pos": 190, "end_pos": 196, "type": "METRIC", "confidence": 0.9995138645172119}, {"text": "relation classification", "start_pos": 205, "end_pos": 228, "type": "TASK", "confidence": 0.8398203253746033}]}, {"text": "Nevertheless, it is encouraging that the relation classification systems obtained higher precision than RuleSpSent, which is important, considering the decisions will be transfered to the untagged entity mentions across the document.", "labels": [], "entities": [{"text": "relation classification", "start_pos": 41, "end_pos": 64, "type": "TASK", "confidence": 0.7812641263008118}, {"text": "precision", "start_pos": 89, "end_pos": 98, "type": "METRIC", "confidence": 0.998963475227356}]}, {"text": "Indeed, as shown in the SPRD columns in, most relation classification systems outperformed the Rule-SpSent baseline when spreading was used.", "labels": [], "entities": [{"text": "relation classification", "start_pos": 46, "end_pos": 69, "type": "TASK", "confidence": 0.8439587950706482}, {"text": "spreading", "start_pos": 121, "end_pos": 130, "type": "TASK", "confidence": 0.9634970426559448}]}, {"text": "The scores of the systems using different parser outputs only vary slightly.", "labels": [], "entities": []}, {"text": "Rel-Context, on the other hand, surpassed others in terms of micro-averaged precision, while sacrificing micro-averaged recall and macro-averaged scores.", "labels": [], "entities": [{"text": "Rel-Context", "start_pos": 0, "end_pos": 11, "type": "DATASET", "confidence": 0.8651099801063538}, {"text": "precision", "start_pos": 76, "end_pos": 85, "type": "METRIC", "confidence": 0.9176585674285889}, {"text": "recall", "start_pos": 120, "end_pos": 126, "type": "METRIC", "confidence": 0.9782721996307373}]}, {"text": "Next, the SPRD columns in show the results when the spreading rules were applied, which METHOD NO SPRD (micro-avg) NO SPRD (macro-avg)   effectively improved recall (see Section 5.2.3 for discussion on statistical significance tests on the results).", "labels": [], "entities": [{"text": "METHOD", "start_pos": 88, "end_pos": 94, "type": "METRIC", "confidence": 0.998408854007721}, {"text": "NO SPRD", "start_pos": 95, "end_pos": 102, "type": "METRIC", "confidence": 0.7195005714893341}, {"text": "NO SPRD", "start_pos": 115, "end_pos": 122, "type": "METRIC", "confidence": 0.7443803250789642}, {"text": "recall", "start_pos": 158, "end_pos": 164, "type": "METRIC", "confidence": 0.9994157552719116}]}, {"text": "The Maxent system achieved very good micro-averaged precision, but low macroaveraged scores.", "labels": [], "entities": [{"text": "precision", "start_pos": 52, "end_pos": 61, "type": "METRIC", "confidence": 0.9661471247673035}]}, {"text": "In fact, as shown in, Maxent can only disambiguate 7 species (out of a total of 54) that have relatively large amount of training instances, and failed completely on other species.", "labels": [], "entities": []}, {"text": "This suggests that Maxent may not be able to generate good micro-averaged scores when applied to a dataset where the dominant species are different from those in the training set.", "labels": [], "entities": []}, {"text": "On the other hand, the relation-classification approaches have a clear advantage over Maxent as measured by macro-averaged scores.", "labels": [], "entities": []}, {"text": "As shown in, Rel-ENJU-Genia worked well on most of the species, displaying its good adaptability, while achieving comparable micro-averaged F1 to Maxent.", "labels": [], "entities": [{"text": "F1", "start_pos": 140, "end_pos": 142, "type": "METRIC", "confidence": 0.9814292788505554}]}, {"text": "Overall, Hbrd, which combines the strengths of relation classification and the Maxent classification model, obtained the highest points as measured by every metric.", "labels": [], "entities": [{"text": "Hbrd", "start_pos": 9, "end_pos": 13, "type": "DATASET", "confidence": 0.8210395574569702}, {"text": "relation classification", "start_pos": 47, "end_pos": 70, "type": "TASK", "confidence": 0.8888312876224518}]}], "tableCaptions": [{"text": " Table 2: Evaluation results of the classification systems on", "labels": [], "entities": []}, {"text": " Table 3: Evaluation results of the species disambiguation systems on DEVTEST (precision/recall/F1-score, in %)", "labels": [], "entities": [{"text": "DEVTEST", "start_pos": 70, "end_pos": 77, "type": "DATASET", "confidence": 0.8480033874511719}, {"text": "precision", "start_pos": 79, "end_pos": 88, "type": "METRIC", "confidence": 0.9992048144340515}, {"text": "recall", "start_pos": 89, "end_pos": 95, "type": "METRIC", "confidence": 0.8282283544540405}, {"text": "F1-score", "start_pos": 96, "end_pos": 104, "type": "METRIC", "confidence": 0.953402578830719}]}, {"text": " Table 4: The micro-averaged F1 scores (%) of Maxent", "labels": [], "entities": [{"text": "F1 scores", "start_pos": 29, "end_pos": 38, "type": "METRIC", "confidence": 0.9464205205440521}, {"text": "Maxent", "start_pos": 46, "end_pos": 52, "type": "TASK", "confidence": 0.3975224494934082}]}]}