{"title": [{"text": "Deriving lexical and syntactic expectation-based measures for psycholinguistic modeling via incremental top-down parsing", "labels": [], "entities": [{"text": "parsing", "start_pos": 113, "end_pos": 120, "type": "TASK", "confidence": 0.41934654116630554}]}], "abstractContent": [{"text": "A number of recent publications have made use of the incremental output of stochastic parsers to derive measures of high utility for psycholinguistic modeling, following the work of Hale (2001; 2003; 2006).", "labels": [], "entities": []}, {"text": "In this paper, we present novel methods for calculating separate lexical and syntactic surprisal measures from a single incremental parser using a lexical-ized PCFG.", "labels": [], "entities": []}, {"text": "We also present an approximation to entropy measures that would otherwise be intractable to calculate fora grammar of that size.", "labels": [], "entities": []}, {"text": "Empirical results demonstrate the utility of our methods in predicting human reading times.", "labels": [], "entities": [{"text": "predicting human reading", "start_pos": 60, "end_pos": 84, "type": "TASK", "confidence": 0.8679604927698771}]}], "introductionContent": [{"text": "Assessment of linguistic complexity has played an important role in psycholinguistics and neurolinguistics fora longtime, from the use of mean length of utterance and related scores in child language development, to complexity scores related to reading difficulty inhuman sentence processing studies.", "labels": [], "entities": []}, {"text": "Operationally, such linguistic complexity scores are derived via deterministic manual (human) annotation and scoring algorithms of language samples.", "labels": [], "entities": []}, {"text": "Natural language processing has been employed to automate the extraction of such measures (, which can have high utility in terms of reduction of time required to annotate and score samples.", "labels": [], "entities": []}, {"text": "More interestingly, however, novel data driven methods are being increasingly employed in this sphere, yielding language sample characterizations that require NLP in their derivation.", "labels": [], "entities": []}, {"text": "For example, scores derived from variously estimated language models have been used to evaluate and classify language samples associated with neurodevelopmental or neurodegenerative disorders, as well as within general studies of human sentence processing).", "labels": [], "entities": []}, {"text": "These scores cannot feasibly be derived by hand, but rather rely on large-scale statistical models and structured inference algorithms to be derived.", "labels": [], "entities": []}, {"text": "This is quickly becoming an important application of NLP, making possible new methods in the study of human language processing in both typical and impaired populations.", "labels": [], "entities": [{"text": "human language processing", "start_pos": 102, "end_pos": 127, "type": "TASK", "confidence": 0.6388099392255148}]}, {"text": "The use of broad-coverage parsing for psycholinguistic modeling has become very popular recently.", "labels": [], "entities": [{"text": "broad-coverage parsing", "start_pos": 11, "end_pos": 33, "type": "TASK", "confidence": 0.6848961859941483}]}, {"text": "suggested a measure (surprisal) derived from an parser using a probabilistic context-free grammar (PCFG) for psycholinguistic modeling; and in later work) he suggested an alternate parser-derived measure (entropy reduction) that may also account for some human sentence processing performance.", "labels": [], "entities": []}, {"text": "Recent work continues to advocate surprisal in particular as a very useful measure for predicting processing difficulty (, and the measure has been derived using a variety of incremental (left-to-right) parsing strategies, including an Earley parser (), the Roark (2001) incremental top-down parser, and an n-best version of the incremental dependency parser ().", "labels": [], "entities": []}, {"text": "Deriving such measures by hand, even fora relatively limited set of stimuli, is not feasible, hence parsing plays a critical role in this developing psycholinguistic enterprise.", "labels": [], "entities": []}, {"text": "There is no single measure that can account for all of the factors influencing human sentence processing performance, and some of the most recent work on using parser-derived measures for psycholinguistic modeling has looked to try to derive multiple, complementary measures.", "labels": [], "entities": []}, {"text": "One of the key distinctions being looked at is syntactic versus lexical expectations).", "labels": [], "entities": []}, {"text": "For example, in, trials were run deriving surprisal from the Roark (2001) parser under two different conditions: fully lexicalized parsing, and fully unlexicalized parsing (to pre-terminal part-of-speech tags).", "labels": [], "entities": []}, {"text": "capture a similar distinction by making use of an unlexicalized PCFG within an Earley parser and a fully lexicalized unlabeled dependency parser.", "labels": [], "entities": []}, {"text": "As point out, fully unlexicalized grammars ignore important lexico-syntactic information when deriving the \"syntactic\" expectations, such as subcategorization preferences of particular verbs, which are generally accepted to impact syntactic expectations inhuman sentence processing ().", "labels": [], "entities": []}, {"text": "Demberg and Keller argue, based on their results, for unlexicalized surprisal instead of lexicalized surprisal.", "labels": [], "entities": []}, {"text": "Here we present a novel method for deriving separate syntactic and lexical surprisal measures from a fully lexicalized incremental parser, to allow for rich probabilistic grammars to be used to derive either measure, and demonstrate the utility of this method versus that of Demberg and Keller in empirical trials.", "labels": [], "entities": []}, {"text": "The use of large-scale lexicalized grammars presents a problem for using an Earley parser to derive surprisal or for the calculation of entropy as) defines it, because both methods require matrix inversion of a matrix with dimensionality the size of the non-terminal set.", "labels": [], "entities": []}, {"text": "With very large lexicalized PCFGs, the size of the nonterminal set is too large for tractable matrix inversion.", "labels": [], "entities": []}, {"text": "The use of an incremental, beam-search parser provides a tractable approximation to both measures.", "labels": [], "entities": []}, {"text": "Incremental top-down and left-corner parsers have been shown to effectively (and efficiently) make use of non-local features from the left-context to yield very high accuracy syntactic parses), and we will use such rich models to derive our scores.", "labels": [], "entities": []}, {"text": "In addition to teasing apart syntactic and lexical surprisal (defined explicitly in \u00a73), we present an approximation to the full entropy that) used to define the entropy reduction hypothesis.", "labels": [], "entities": [{"text": "entropy reduction", "start_pos": 162, "end_pos": 179, "type": "TASK", "confidence": 0.8835539817810059}]}, {"text": "Such an entropy measure is derived via a predictive step, advancing the parses independently of the input, as described in \u00a73.3.", "labels": [], "entities": []}, {"text": "We also present syntactic and lexical alternatives for this measure, and demonstrate the utility of making such a distinction for entropy as well as surprisal.", "labels": [], "entities": []}, {"text": "The purpose of this paper is threefold.", "labels": [], "entities": []}, {"text": "First, to present a careful and well-motivated decomposition of lexical and syntactic expectation-based measures from a given lexicalized PCFG.", "labels": [], "entities": []}, {"text": "Second, to explicitly document methods for calculating these and other measures from a specific incremental parser.", "labels": [], "entities": []}, {"text": "And finally, to present some empirical validation of the novel measures from real reading time trials.", "labels": [], "entities": []}, {"text": "We modified the parser to calculate the discussed measures 1 , and the empirical results in \u00a74 show several things, including: 1) using a fully lexicalized parser to calculate syntactic surprisal and entropy provides higher predictive utility for reading times than these measures calculated via unlexicalized parsing (as in Demberg and Keller); and 2) syntactic entropy is a useful predictor of reading time.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Correlations between (mean-centered) predictors. Note that unigram frequencies were represented as logs, other", "labels": [], "entities": []}, {"text": " Table 2: Estimated effects from mixed effects models on", "labels": [], "entities": [{"text": "Estimated", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9711540341377258}]}]}