{"title": [{"text": "Feature-Rich Translation by Quasi-Synchronous Lattice Parsing", "labels": [], "entities": []}], "abstractContent": [{"text": "We present a machine translation framework that can incorporate arbitrary features of both input and output sentences.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 13, "end_pos": 32, "type": "TASK", "confidence": 0.7126465886831284}]}, {"text": "The core of the approach is a novel de-coder based on lattice parsing with quasi-synchronous grammar (Smith and Eis-ner, 2006), a syntactic formalism that does not require source and target trees to be isomorphic.", "labels": [], "entities": []}, {"text": "Using generic approximate dynamic programming techniques, this decoder can handle \"non-local\" features.", "labels": [], "entities": []}, {"text": "Similar approximate inference techniques support efficient parameter estimation with hidden variables.", "labels": [], "entities": []}, {"text": "We use the decoder to conduct controlled experiments on a German-to-English translation task, to compare lexical phrase, syntax , and combined models, and to measure effects of various restrictions on non-isomorphism.", "labels": [], "entities": [{"text": "German-to-English translation task", "start_pos": 58, "end_pos": 92, "type": "TASK", "confidence": 0.741382877031962}]}], "introductionContent": [{"text": "We have seen rapid recent progress in machine translation through the use of rich features and the development of improved decoding algorithms, often based on grammatical formalisms.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 38, "end_pos": 57, "type": "TASK", "confidence": 0.8032367527484894}]}, {"text": "If we view MT as a machine learning problem, features and formalisms imply structural independence assumptions, which are in turn exploited by efficient inference algorithms, including decoders (.", "labels": [], "entities": [{"text": "MT as a machine learning problem", "start_pos": 11, "end_pos": 43, "type": "TASK", "confidence": 0.8045027852058411}]}, {"text": "Hence a tension is visible in the many recent research efforts aiming to decode with \"non-local\" features.", "labels": [], "entities": []}, {"text": "recently argued fora separation between features/formalisms (and the indepen-dence assumptions they imply) from inference algorithms in MT; this separation is widely appreciated in machine learning.", "labels": [], "entities": [{"text": "MT", "start_pos": 136, "end_pos": 138, "type": "TASK", "confidence": 0.9463906288146973}]}, {"text": "Here we take first steps toward such a \"universal\" decoder, making the following contributions: Arbitrary feature model ( \u00a72): We define a single, direct log-linear translation model () that encodes most popular MT features and can be used to encode any features on source and target sentences, dependency trees, and alignments.", "labels": [], "entities": [{"text": "Arbitrary", "start_pos": 96, "end_pos": 105, "type": "METRIC", "confidence": 0.9624744653701782}, {"text": "MT", "start_pos": 212, "end_pos": 214, "type": "TASK", "confidence": 0.9712060689926147}]}, {"text": "The trees are optional and can be easily removed, allowing simulation of \"string-to-tree,\" \"tree-to-string,\" \"treeto-tree,\" and \"phrase-based\" models, among many others.", "labels": [], "entities": []}, {"text": "We follow the widespread use of log-linear modeling for direct translation modeling; the novelty is in the use of richer feature sets than have been previously used in a single model.", "labels": [], "entities": [{"text": "direct translation modeling", "start_pos": 56, "end_pos": 83, "type": "TASK", "confidence": 0.7100887894630432}]}, {"text": "Decoding as QG parsing ( \u00a73-4): We present a novel decoder based on lattice parsing with quasisynchronous grammar (QG;).", "labels": [], "entities": [{"text": "QG parsing", "start_pos": 12, "end_pos": 22, "type": "TASK", "confidence": 0.6386996209621429}]}, {"text": "Further, we exploit generic approximate inference techniques to incorporate arbitrary \"nonlocal\" features in the dynamic programming algorithm.", "labels": [], "entities": []}, {"text": "Parameter estimation ( \u00a75): We exploit similar approximate inference methods in regularized pseudolikelihood estimation with hidden variables to discriminatively and efficiently train our model.", "labels": [], "entities": [{"text": "Parameter estimation", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.6999333649873734}]}, {"text": "Because we start with inference (the key subroutine in training), many other learning algorithms are possible.", "labels": [], "entities": []}, {"text": "Experimental platform ( \u00a76): The flexibility of our model/decoder permits carefully controlled experiments.", "labels": [], "entities": []}, {"text": "We compare lexical phrase and dependency syntax features, as well as a novel com-\u03a3, T source and target language vocabularies, respectively Trans : \u03a3 \u222a {NULL} \u2192 2 T function mapping each source word to target words to which it may translate s = s0, . .", "labels": [], "entities": []}, {"text": ", sn \u2208 \u03a3 n source language sentence (s0 is the NULL word) t = t1, . .", "labels": [], "entities": []}, {"text": ", tm \u2208 Tm target language sentence, translation of s \u03c4s : {1, . .", "labels": [], "entities": []}, {"text": ", n} \u2192 {0, . .", "labels": [], "entities": []}, {"text": ", n} dependency tree of s, where \u03c4s(i) is the index of the parent of si (0 is the root, $) \u03c4t : {1, . .", "labels": [], "entities": []}, {"text": ", m} \u2192 {0, . .", "labels": [], "entities": []}, {"text": ", m} dependency tree oft, where \u03c4t(i) is the index of the parent of ti (0 is the root, $) a : {1, . .", "labels": [], "entities": []}, {"text": ", m} \u2192 2 {1,...,n} alignments from words int to words in s; \u2205 denotes alignment to NULL \u03b8 parameters of the model g trans (s, a, t) lexical translation features ( \u00a72.1): word-to-word translation features for translating s as t f phr (s j i , t k ) phrase-to-phrase translation features for translating s j i as t kg lm (t) language model features ( \u00a72.2): target syntactic features ( \u00a72.3): f att (t, j, t , k) syntactic features for attaching target word tat position k to target word tat position j f val (t, j, I) syntactic valence features with word tat position j having children I \u2286 {1, . .", "labels": [], "entities": [{"text": "phrase-to-phrase translation", "start_pos": 248, "end_pos": 276, "type": "TASK", "confidence": 0.7421034872531891}]}, {"text": ", m} g reor (s, \u03c4s, a, t, \u03c4t) reordering features ( \u00a72.4): distortion features fora source word at position i aligned to a target word at position jg tree 2 (\u03c4s, a, \u03c4t) tree-to-tree syntactic features ( \u00a73): configuration features for source pair si/s i being aligned to target pair counters for \"covering\" each sword each time, the zth time, and leaving it \"uncovered\" bination of the two.", "labels": [], "entities": []}, {"text": "We quantify the effects of our approximate inference.", "labels": [], "entities": []}, {"text": "We explore the effects of various ways of restricting syntactic non-isomorphism between source and target trees through the QG.", "labels": [], "entities": []}, {"text": "We do not report state-of-the-art performance, but these experiments reveal interesting trends that will inform continued research.", "labels": [], "entities": []}, {"text": "Given a sentence sand its parse tree \u03c4 s , we formulate the translation problem as finding the target sentence t * (along with its parse tree \u03c4 * t and alignment a * to the source tree) such that 3", "labels": [], "entities": []}], "datasetContent": [{"text": "Our decoding framework allows us to perform many experiments with the same feature representation and inference algorithms, including combining and comparing phrase-based and syntax-based features and examining how isomorphism constraints of synchronous formalisms affect translation output.", "labels": [], "entities": []}, {"text": "We use the German-English portion of the Basic Travel Expression Corpus (BTEC).", "labels": [], "entities": [{"text": "Basic Travel Expression Corpus (BTEC)", "start_pos": 41, "end_pos": 78, "type": "DATASET", "confidence": 0.7102099997656686}]}, {"text": "The corpus has approximately 100K sentence pairs.", "labels": [], "entities": []}, {"text": "We filter sentences of length more than 15 words, which only removes 6% of the data.", "labels": [], "entities": []}, {"text": "We end up with a training set of 82,299 sentences, a development set of 934 sentences, and a test set of 500 sentences.", "labels": [], "entities": []}, {"text": "We evaluate translation output using case-insensitive BLEU (), as provided by NIST, and METEOR (), version 0.6, with Porter stemming and WordNet synonym matching.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 54, "end_pos": 58, "type": "METRIC", "confidence": 0.9862624406814575}, {"text": "NIST", "start_pos": 78, "end_pos": 82, "type": "DATASET", "confidence": 0.9636099338531494}, {"text": "METEOR", "start_pos": 88, "end_pos": 94, "type": "METRIC", "confidence": 0.9612541198730469}, {"text": "WordNet synonym matching", "start_pos": 137, "end_pos": 161, "type": "TASK", "confidence": 0.7552522222201029}]}, {"text": "Our model permits training the system on the full set of parallel data, but we instead use the parallel data to estimate feature functions and learn \u03b8 on the development set.", "labels": [], "entities": []}, {"text": "We trained using three iterations of SGA over the development data with a batch size of 1 and a fixed step size of 0.01.", "labels": [], "entities": []}, {"text": "We used 2 regularization with a fixed, untuned coefficient of 0.1.", "labels": [], "entities": []}, {"text": "Cube summing used a 10-best list for training and a 7-best list for decoding unless otherwise specified.", "labels": [], "entities": [{"text": "Cube summing", "start_pos": 0, "end_pos": 12, "type": "TASK", "confidence": 0.7620872259140015}]}, {"text": "To obtain the translation lexicon (Trans) we first included the top three target words t for each s using p(s | t) \u00d7 p(t | s) to score target words.", "labels": [], "entities": []}, {"text": "For any training sentence s, t and t j for which t j \u2208 n i=1 Trans(s i ), we added t j to Trans(s i ) for i = argmax i \u2208I p(s i |t j ) \u00d7 p(t j |s i ), where I = {i : 0 \u2264 i \u2264 n \u2227 |Trans(s i )| < q i }.", "labels": [], "entities": []}, {"text": "We used q 0 = 10 and q >0 = 5, restricting |Trans(NULL)| \u2264 10 and |Trans(s)| \u2264 5 for any s \u2208 \u03a3.", "labels": [], "entities": []}, {"text": "This made 191 of the development sentences unreachable by the model, leaving 743 sentences for learning \u03b8.", "labels": [], "entities": []}, {"text": "During decoding, we generated lattices with all t \u2208 Trans(s i ) for 0 \u2264 i \u2264 n, for every position.", "labels": [], "entities": []}, {"text": "We used \u03c1 = 0.9, causing states within 90% of the source sentence length to be final states.", "labels": [], "entities": []}, {"text": "Between each pair of consecutive states, we pruned edges that fell outside abeam of 70% of the sum of edge weights (see \u00a74.1; edge weights use flex , f dist , and f scov ) of all edges between those two states.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 4: Feature set comparison (BLEU).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 34, "end_pos": 38, "type": "METRIC", "confidence": 0.7199125289916992}]}, {"text": " Table 5: QG configuration comparison. The name of each  configuration, following", "labels": [], "entities": [{"text": "QG configuration comparison", "start_pos": 10, "end_pos": 37, "type": "TASK", "confidence": 0.5898405114809672}]}]}