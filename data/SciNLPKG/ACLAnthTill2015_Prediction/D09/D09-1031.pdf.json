{"title": [{"text": "How well does active learning actually work? Time-based evaluation of cost-reduction strategies for language documentation", "labels": [], "entities": [{"text": "language documentation", "start_pos": 100, "end_pos": 122, "type": "TASK", "confidence": 0.7262279391288757}]}], "abstractContent": [{"text": "Machine involvement has the potential to speedup language documentation.", "labels": [], "entities": []}, {"text": "We assess this potential with timed annotation experiments that consider annotator expertise , example selection methods, and suggestions from a machine classifier.", "labels": [], "entities": []}, {"text": "We find that better example selection and label suggestions improve efficiency, but effectiveness depends strongly on annota-tor expertise.", "labels": [], "entities": []}, {"text": "Our expert performed best with uncertainty selection, but gained little from suggestions.", "labels": [], "entities": [{"text": "uncertainty selection", "start_pos": 31, "end_pos": 52, "type": "TASK", "confidence": 0.77604079246521}]}, {"text": "Our non-expert performed best with random selection and suggestions.", "labels": [], "entities": []}, {"text": "The results underscore the importance both of measuring annotation cost reductions with respect to time and of the need for cost-sensitive learning methods that adapt to annotators.", "labels": [], "entities": []}], "introductionContent": [{"text": "Data annotated with linguistically interesting labels is used in a wide variety of contexts.", "labels": [], "entities": []}, {"text": "Computational linguists generally use annotated data as training and evaluation material for natural language processing systems; corpus linguists use it to test hypotheses about language; documentary linguists create interlinear glossed texts to preserve examples of endangered languages and hypotheses about the grammars of those languages.", "labels": [], "entities": []}, {"text": "Regardless of the context, creating annotated data is costly in terms of time and/or money.", "labels": [], "entities": []}, {"text": "Since both time and money are undeniably in limited supply, there is a widely shared desire to reduce this cost.", "labels": [], "entities": []}, {"text": "Reducing cost involves strategies that do more with fewer human-annotated labels and/or reduce the per-label cost.", "labels": [], "entities": []}, {"text": "An example of the former is active learning, which focuses annotation effort on data points selected by the learner(s) for their expected utility in developing a more accurate model.", "labels": [], "entities": []}, {"text": "Examples of the latter include providing suggestions from a machine labeler and using extremely cheap human labelers, e.g. with the Amazon Mechanical Turk (.", "labels": [], "entities": [{"text": "Amazon Mechanical Turk", "start_pos": 132, "end_pos": 154, "type": "DATASET", "confidence": 0.897928794225057}]}, {"text": "Different techniques maybe more or less applicable depending on the language being annotated, the kind of labels which are desired (tags, syntactic structures, etc.), and the desired use of the annotated data (e.g., for training models, testing linguistic hypotheses, or preserving a language).", "labels": [], "entities": []}, {"text": "This paper discusses experiments that measure the effectiveness of machine-aided annotation for language documentation using both active learning simulation experiments and annotation experiments which involve actual documentary linguists interacting with machine example selection and label suggestion.", "labels": [], "entities": [{"text": "language documentation", "start_pos": 96, "end_pos": 118, "type": "TASK", "confidence": 0.7106905281543732}]}, {"text": "Specifically, we deal with the task of labeling morphemes of the Mayan language Uspanteko with fine-grained parts-ofspeech.", "labels": [], "entities": []}, {"text": "We also run active learning simulation experiments for part-of-speech tagging for Danish, Dutch, English, Swedish, and Uspanteko to show the validity of our models and methods in a standard setting.", "labels": [], "entities": [{"text": "part-of-speech tagging", "start_pos": 55, "end_pos": 77, "type": "TASK", "confidence": 0.7567905485630035}, {"text": "Uspanteko", "start_pos": 119, "end_pos": 128, "type": "DATASET", "confidence": 0.9379550218582153}]}, {"text": "For Uspanteko, we provide results from annotation experiments in which annotation cost is measured in terms of the actual annotation time required while varying three factors: (1) example selection, (2) machine label suggestions, and (3) annotator expertise.", "labels": [], "entities": [{"text": "example selection", "start_pos": 180, "end_pos": 197, "type": "TASK", "confidence": 0.7503358423709869}]}, {"text": "Our findings indicate that there is considerable promise for reducing the cost of producing IGT, but they also demonstrate considerable variation due to the interaction of these factors.", "labels": [], "entities": []}, {"text": "This suggests different prescriptions for appropriate strategies in different contexts.", "labels": [], "entities": []}, {"text": "Most clearly, the worst performing strategy-by far-is that used in nearly all documentary work: sequential annotation without automation.", "labels": [], "entities": []}, {"text": "Also, our expert annotator did best with examples picked by uncertainty selection, while our non-expert did best with random selection aided by machine label sug-Language #words-tr #words-dev #tags #sents-tr #sents-dev Avg.sent Avg.tr.sent Avg.dev.: Corpora: number of words and sentences, number of possible tags, and average sentence length. gestions.", "labels": [], "entities": []}, {"text": "This difference confirms the importance of cost-sensitive active learning strategies that are not just learner-guided, but also take into account modeling of the annotators ().", "labels": [], "entities": []}, {"text": "Finally, we confirm the importance of using actual annotation time to measure annotation cost: a unit-cost assumption-even at a finegrained level-can dramatically misrepresent the actual effectiveness of different strategies.", "labels": [], "entities": []}], "datasetContent": [{"text": "We verify that our tagger and dataset behave as expected in standard active learning experiments by running simulations on the Uspanteko data set, and on POS-tagging for Danish, Dutch, English, and Swedish.", "labels": [], "entities": [{"text": "Uspanteko data set", "start_pos": 127, "end_pos": 145, "type": "DATASET", "confidence": 0.9834967652956644}]}, {"text": "Here, we vary only the selection method: sequential, random, or uncertainty.", "labels": [], "entities": []}, {"text": "For each language, we randomly select a seed set of 10 labeled sentences.", "labels": [], "entities": []}, {"text": "The number of examples selected to be labeled in each round begins at 10 and doubles after every 20 rounds.", "labels": [], "entities": []}, {"text": "For rand and unc, each batch of examples is selected from a pool (size of 1000) that is itself randomly selected from the entire set of remaining unlabeled examples.", "labels": [], "entities": []}, {"text": "rand and unc experiments for each language are replicated 5 times; splines and regressions are computed overall runs for each condition.", "labels": [], "entities": []}, {"text": "gives learning curves for the Uspanteko simulations, with cost measured in terms of (a) clauses and (b) morphemes.", "labels": [], "entities": []}, {"text": "Both graphs show the usual behavior found in active learning experiments.", "labels": [], "entities": []}, {"text": "rand and unc both rise more quickly than seq, and unc is well above rand.", "labels": [], "entities": [{"text": "rand", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.8122975826263428}]}, {"text": "The relationship between the methods is the same regardless   of the cost metric, but the relative differences in cost-savings are not, which we see when we look at OPER values.", "labels": [], "entities": []}, {"text": "The dashed vertical lines in the two graphs correspond to the 20% mark used to calculate OPER values, which are given in.", "labels": [], "entities": [{"text": "OPER", "start_pos": 89, "end_pos": 93, "type": "METRIC", "confidence": 0.8689979314804077}]}, {"text": "Most importantly, note the much larger OPER for unc over rand with clause cost (7.86 vs 4.55).", "labels": [], "entities": [{"text": "OPER", "start_pos": 39, "end_pos": 43, "type": "METRIC", "confidence": 0.9656931161880493}]}, {"text": "Also note that OPER(rand,seq) is lower with clause costthis indicates that the beginning portions of the corpus contain longer sentences with more morphemes, an accident which overstates how well seq would likely work in general.", "labels": [], "entities": [{"text": "OPER", "start_pos": 15, "end_pos": 19, "type": "METRIC", "confidence": 0.9962030053138733}]}, {"text": "Since rand is unbiased with respect to picking longer sentences, the large increase of OPER(unc,rand) from 4.55 to 7.86 is a clear indication of the well-known-but not always attended to-tendency of uncertainty sampling to select longer sentences.", "labels": [], "entities": [{"text": "OPER", "start_pos": 87, "end_pos": 91, "type": "METRIC", "confidence": 0.9985916018486023}]}, {"text": "Consequently, one should at least use sub-sentence cost in order not to overstate the gains from active learning.", "labels": [], "entities": []}, {"text": "The annotation experiments in the next section take this word  of caution one step further: even sub-sentence cost (morpheme cost, in our setting) can overestimate gains since the morphemes selected are actually harder to annotate and thus take more time.", "labels": [], "entities": []}, {"text": "gives overall percentage error reductions (OPER) between different selection methods based on word/morpheme cost, for each language.", "labels": [], "entities": [{"text": "percentage error reductions (OPER)", "start_pos": 14, "end_pos": 48, "type": "METRIC", "confidence": 0.8105210761229197}]}, {"text": "For all languages, rand and unc are better than seq.", "labels": [], "entities": []}, {"text": "Only in the case of Swedish is there no benefit from unc over rand.", "labels": [], "entities": []}, {"text": "For Dutch, the large gains over seq for both rand and unc accurately reflect the heterogeneity of the underlying Alpino corpus.", "labels": [], "entities": [{"text": "Alpino corpus", "start_pos": 113, "end_pos": 126, "type": "DATASET", "confidence": 0.9531672596931458}]}, {"text": "3 Most importantly, for Uspanteko, there are large reductions from unc to rand to seq, mirroring the clear trends in.", "labels": [], "entities": []}, {"text": "These simulations have an unrealistic \"perfect\" annotator, the corpus.", "labels": [], "entities": []}, {"text": "Next, we discuss results with real annotators-who maybe fallible or may (reasonably) beg to differ with the corpus analysis.", "labels": [], "entities": []}, {"text": "With two annotators (expert, non-expert), three selection methods (seq, rand, unc), and two machine labeling settings (ns, ds), we obtain 12 different experiments.", "labels": [], "entities": []}, {"text": "Each experiment measures accuracy in terms of all words and unknown words and cost in terms of clauses, morphemes and time; this produces six views on every experiment.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 25, "end_pos": 33, "type": "METRIC", "confidence": 0.9990647435188293}]}, {"text": "In this paper we focus on one view: accuracy overall words with time-based evaluation of cost.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 36, "end_pos": 44, "type": "METRIC", "confidence": 0.9979633092880249}]}, {"text": "As with the simulations, clause cost in the annotation experiments overestimates the cost reductions.", "labels": [], "entities": []}, {"text": "For morpheme cost, the annotation experiments show that (a) it also overstates cost reductions compared to time, and (b) it can mis-state relative effectiveness when comparing annotators.", "labels": [], "entities": []}, {"text": "shows curves for four experiments: seq-ns for both annotators and the most effective overall condition for each annotator.", "labels": [], "entities": []}, {"text": "uses morpheme cost evaluation; on that metric, both annotators appear to be about equally effective with seq-ns and much more effective with machine involvement (unc or ds) than without.", "labels": [], "entities": []}, {"text": "Additionally, the non-expert's rand-ds appears to beat the expert's unc-ns.", "labels": [], "entities": []}, {"text": "However, the time cost evaluation in tells a dramatically different story.", "labels": [], "entities": []}, {"text": "Each annotator's machine- Recall that sequential annotation is the default mode for producing IGT, so this strategy is of particular interest.", "labels": [], "entities": []}, {"text": "involved experiment is much better than their seqns, but now the expert's best is clearly better than the non-expert's.", "labels": [], "entities": []}, {"text": "We see this as clear evidence for the need for cost-sensitive learning over vanilla active learning (as we do here).", "labels": [], "entities": []}, {"text": "The non-expert with rand-ds caught up to and surpassed the unaided expert in about six hours total annotation time, and he caught up to her unc-ns curve after 35 hours.", "labels": [], "entities": []}, {"text": "This is encouraging since often language documentation projects have participants with a wide range of expertise levels, and these results suggest that assistance from machine learning, if done properly, may increase the effectiveness of participants with less language-specific expertise.", "labels": [], "entities": [{"text": "language documentation", "start_pos": 32, "end_pos": 54, "type": "TASK", "confidence": 0.7012598365545273}]}, {"text": "We are also encouraged, with respect to the effectiveness of active learning, that the expert's best performance is obtained with uncertainty-based selection.", "labels": [], "entities": []}, {"text": "shows both actual measurements and the fitted nonlinear regression curves used to compute OPER., the expert without suggestions, exhibits typical active learning behavior similar to that seen in the simulation experiments., q q q q q q q q q q q q qq qq qq qq q q q qq q q q q q q q q q q q q q q q q q q q qq q q q q q q q q q qq. the non-expert with suggestions, shows that in the ds conditions the non-expert was less effective with unc.", "labels": [], "entities": []}, {"text": "This is not unexpected: uncertainty selects harder examples that will either take longer to annotate or are easier to get wrong, especially if the annotator trusts the classifier and especially on examples the classifier is uncertain about.", "labels": [], "entities": []}, {"text": "Nonetheless, in all ds cases, the non-expert performs better than with seq-ns.", "labels": [], "entities": []}, {"text": "provides OPER values from time 0 to 12,500 seconds (about 35 hours), the minimum amount of annotation time logged in anyone of the twelve experiments.", "labels": [], "entities": [{"text": "OPER", "start_pos": 9, "end_pos": 13, "type": "METRIC", "confidence": 0.995229959487915}]}, {"text": "The table mixes three types of comparison: (1) the boxed values on the diagonal give OPER for the expert versus the non-expert given the same selection and suggestion conditions; (2) the upper (right) triangle gives OPER for the expert versus herself for different conditions; and (3) the lower (left) triangle is the non-expert versus himself.", "labels": [], "entities": [{"text": "OPER", "start_pos": 85, "end_pos": 89, "type": "METRIC", "confidence": 0.9993693232536316}, {"text": "OPER", "start_pos": 216, "end_pos": 220, "type": "METRIC", "confidence": 0.9979546070098877}]}, {"text": "For example: (1) the expert obtained an 11.52 OPER versus the non-expert when both used rand-ns; (2) the expert obtained a 10.52 OPER by using rand-ds rather than seq-ns; and (3) the non-expert obtained a 5.93 OPER over rand-ns by using rand-ds.", "labels": [], "entities": [{"text": "OPER", "start_pos": 46, "end_pos": 50, "type": "METRIC", "confidence": 0.9650502800941467}, {"text": "OPER", "start_pos": 129, "end_pos": 133, "type": "METRIC", "confidence": 0.9455394148826599}, {"text": "OPER", "start_pos": 210, "end_pos": 214, "type": "METRIC", "confidence": 0.9750676155090332}]}, {"text": "A number of patterns emerge.", "labels": [], "entities": []}, {"text": "Quite unsurpris-6 Stopping at 12,500 seconds ensures a fair comparison, for example, between the expert and the non-expert because it requires no extrapolation of the expert's performance.", "labels": [], "entities": [{"text": "Stopping", "start_pos": 18, "end_pos": 26, "type": "METRIC", "confidence": 0.9812760353088379}]}, {"text": "ingly, the values on the diagonal show that the expert is more effective than the non-expert in all conditions.", "labels": [], "entities": []}, {"text": "Also, every other condition is more effective than seq-ns for both annotators (first row for the expert, first column for the non-expert).", "labels": [], "entities": []}, {"text": "unc-ns and rand-ds are particularly effective for the non-expert, giving OPERs of 19.20 and 18.59 over seq-ns, respectively.", "labels": [], "entities": [{"text": "OPERs", "start_pos": 73, "end_pos": 78, "type": "METRIC", "confidence": 0.9995478987693787}]}, {"text": "These reductions, bigger than the expert's reductions of 14.17 and 10.52 for the same conditions, considerably reduce the large gap in seq-ns effectiveness between the two annotators (see).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Corpora: number of words and sentences, number of possible tags, and average sentence length.", "labels": [], "entities": []}, {"text": " Table 3: OPER values for morpheme cost for sim- ulations. A  B indicates we compute OPER(A,B).", "labels": [], "entities": [{"text": "OPER", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9756563305854797}, {"text": "OPER", "start_pos": 85, "end_pos": 89, "type": "METRIC", "confidence": 0.9818413257598877}]}, {"text": " Table 4: Overall percentage error reduction  (OPER) comparisons, with timing cost. See ex- planation of table in the OPER subsection.", "labels": [], "entities": [{"text": "Overall percentage error reduction  (OPER)", "start_pos": 10, "end_pos": 52, "type": "METRIC", "confidence": 0.8254760163170951}, {"text": "timing", "start_pos": 71, "end_pos": 77, "type": "METRIC", "confidence": 0.9808447360992432}]}]}