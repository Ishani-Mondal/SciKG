{"title": [{"text": "Language Models Based on Semantic Composition", "labels": [], "entities": [{"text": "Semantic Composition", "start_pos": 25, "end_pos": 45, "type": "TASK", "confidence": 0.7479308247566223}]}], "abstractContent": [{"text": "In this paper we propose a novel statistical language model to capture long-range semantic dependencies.", "labels": [], "entities": []}, {"text": "Specifically, we apply the concept of semantic composition to the problem of constructing predictive history representations for upcoming words.", "labels": [], "entities": [{"text": "semantic composition", "start_pos": 38, "end_pos": 58, "type": "TASK", "confidence": 0.749282717704773}]}, {"text": "We also examine the influence of the underlying semantic space on the composition task by comparing spatial semantic representations against topic-based ones.", "labels": [], "entities": []}, {"text": "The composition models yield reductions in perplexity when combined with a standard n-gram language model over the n-gram model alone.", "labels": [], "entities": []}, {"text": "We also obtain per-plexity reductions when integrating our models with a structured language model.", "labels": [], "entities": []}], "introductionContent": [{"text": "Statistical language modeling plays an important role in many areas of natural language processing including speech recognition, machine translation, and information retrieval.", "labels": [], "entities": [{"text": "Statistical language modeling", "start_pos": 0, "end_pos": 29, "type": "TASK", "confidence": 0.819022536277771}, {"text": "natural language processing", "start_pos": 71, "end_pos": 98, "type": "TASK", "confidence": 0.6921376585960388}, {"text": "speech recognition", "start_pos": 109, "end_pos": 127, "type": "TASK", "confidence": 0.7288282215595245}, {"text": "machine translation", "start_pos": 129, "end_pos": 148, "type": "TASK", "confidence": 0.7840186059474945}, {"text": "information retrieval", "start_pos": 154, "end_pos": 175, "type": "TASK", "confidence": 0.8069848418235779}]}, {"text": "The prototypical use of language models is to assign probabilities to sequences of words.", "labels": [], "entities": []}, {"text": "By invoking the chain rule, these probabilities are generally estimated as the product of conditional probabilities P(w i |h i ) of a word w i given the history of preceding words hi \u2261 w i\u22121 1 . In theory, the history could span any number of words up tow i such as sentences or even a paragraphs.", "labels": [], "entities": []}, {"text": "In practice, however, it has proven challenging to deal with the combinatorial growth in the number of possible histories which in turn impacts reliable parameter estimation.", "labels": [], "entities": []}, {"text": "A simple and effective strategy is to truncate the chain rule to include only the n-1 preceding words (n is often set within the range of 3-5).", "labels": [], "entities": []}, {"text": "The simplification reduces the number of free parameters.", "labels": [], "entities": []}, {"text": "However, low values of n impose an artificially local horizon to the language model, and compromise its ability to capture long-range dependencies, such as syntactic relationships, semantic or thematic constraints.", "labels": [], "entities": []}, {"text": "The literature offers many examples of how to overcome this limitation, essentially by allowing the modulation of probabilities by dependencies which extend to words beyond the n-gram horizon.", "labels": [], "entities": []}, {"text": "Cache language models increase the probability of words observed in the history, e.g., by some factor which decays exponentially with distance.", "labels": [], "entities": []}, {"text": "Trigger models) go a step further by allowing arbitrary word pairs to be incorporated into the cache.", "labels": [], "entities": []}, {"text": "Structured language models (e.g.,) go beyond the representation of history as a linear sequence of words to capture the syntactic constructions in which these words are embedded.", "labels": [], "entities": []}, {"text": "It is also possible to build representations of history which are semantic rather than syntactic ().", "labels": [], "entities": []}, {"text": "In this approach, estimates for the probabilities of upcoming words are derived from a comparison of their semantic content with the content of the history so far.", "labels": [], "entities": []}, {"text": "The semantic representations, in this case, are vectors derived from the distributional properties of words in a corpus, based on the insight that words which are semantically similar will be found in similar contexts.", "labels": [], "entities": []}, {"text": "Although the the construction of a semantic representation for the history is crucial to this approach, the underlying vector-based models are primarily designed to represent isolated words rather than word sequences.", "labels": [], "entities": []}, {"text": "Ideally, we would like to compose the meaning of the history out of its constituent parts.", "labels": [], "entities": []}, {"text": "This is by no means anew idea.", "labels": [], "entities": []}, {"text": "Much work in linguistic theory has been devoted to compositionality, the process of determining the meaning of complex expressions from simpler ones.", "labels": [], "entities": []}, {"text": "Previous work either ignores this issue (e.g.,) or simply com-putes the centroid of the vectors representing the history (e.g.,).", "labels": [], "entities": []}, {"text": "This is motivated primarily by mathematical convenience rather than by empirical evidence.", "labels": [], "entities": []}, {"text": "In our earlier work (Mitchell and Lapata, 2008) we formulated composition as a function of two vectors and introduced a variety of models based on addition and multiplication.", "labels": [], "entities": []}, {"text": "In this paper we apply vector composition to the problem of constructing predictive history representations for language modeling.", "labels": [], "entities": []}, {"text": "Besides integrating composition with language modeling, a task which is novel to our knowledge, our approach also serves as a valuable testbed of our earlier framework which we originally evaluated on a small scale verb-subject similarity task.", "labels": [], "entities": [{"text": "language modeling", "start_pos": 37, "end_pos": 54, "type": "TASK", "confidence": 0.7263164520263672}]}, {"text": "We also investigate how the choice of the underlying semantic representation interacts with the choice of composition function by comparing a spatial model that represents words as vectors in a high-dimensional space against a probabilistic model that represents words as topic distributions.", "labels": [], "entities": []}, {"text": "Our results show that the proposed composition models yield reductions in perplexity when combined with a standard n-gram model over the n-gram model alone.", "labels": [], "entities": []}, {"text": "We also show that with an appropriate composition function spatial models outperform the more sophisticated topic models.", "labels": [], "entities": []}, {"text": "Finally, we obtain further perplexity reductions when our models are integrated with a structured language model, indicating that the two approaches to language modeling are complementary.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section we discuss our experimental design for assessing the performance of the models presented above.", "labels": [], "entities": []}, {"text": "We give details on our training procedure and parameter estimation, and present the methods used for comparison with our approach.", "labels": [], "entities": []}, {"text": "Method Following previous work (e.g., Bellegarda) we integrated our compositional language models with a standard n-gram model (see equation).", "labels": [], "entities": []}, {"text": "We experimented with additive and multiplicative composition functions, and two semantic representations (LDA and the simpler semantic space model), resulting in four compositional models.", "labels": [], "entities": []}, {"text": "In addition, we compared our models against a state of the art structured language model in order to assess the extent to which the information provided by the semantic representation is complementary to syntactic structure.", "labels": [], "entities": []}, {"text": "Our experiments used grammarbased language model.", "labels": [], "entities": []}, {"text": "Similarly to standard language models, it computes the probability of the next word based upon the previous words of the sentence.", "labels": [], "entities": []}, {"text": "This is done by computing a subset of all possible grammatical relations for the prior words and then estimating the probability of the next grammatical structure and the probability of seeing the next word given each of the prior grammatical relations.", "labels": [], "entities": []}, {"text": "When estimating the probability of the next word, the model conditions on the two prior heads of constituents, thereby using information about word triples (like a trigram model).", "labels": [], "entities": []}, {"text": "All our models were evaluated by computing perplexity on the test set.", "labels": [], "entities": []}, {"text": "Roughly, this quantifies the degree of unpredictability in a probability distribution, such that a fair k-sided dice would have a perplexity of k.", "labels": [], "entities": []}, {"text": "More precisely, perplexity is the reciprocal of the geometric average of the word probabilities and a lower score indicates better predictions.", "labels": [], "entities": []}, {"text": "Parameter Estimation The compositional language models were trained on the BLLIP corpus, a collection of texts from the Wall Street Journal (years 1987-89).", "labels": [], "entities": [{"text": "Parameter Estimation", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.7560815215110779}, {"text": "BLLIP corpus, a collection of texts from the Wall Street Journal (years 1987-89)", "start_pos": 75, "end_pos": 155, "type": "DATASET", "confidence": 0.9281005673110485}]}, {"text": "The training corpus consisted of 38,521,346 words.", "labels": [], "entities": []}, {"text": "We used a development corpus of 50,006 words and a test corpus of similar size.", "labels": [], "entities": []}, {"text": "All words were converted to lowercase and numbers were replaced with the symbol num.", "labels": [], "entities": []}, {"text": "A vocabulary of 20,000 words was chosen and the remaining tokens were replaced with unk.", "labels": [], "entities": []}, {"text": "Following Mitchell and Lapata (2008), we constructed a simple semantic space based on cooccurrence statistics from the BLLIP training set.", "labels": [], "entities": [{"text": "BLLIP training set", "start_pos": 119, "end_pos": 137, "type": "DATASET", "confidence": 0.8780831893285116}]}, {"text": "We used the 2,000 most frequent word types as contexts and asymmetric five word window.", "labels": [], "entities": []}, {"text": "Vector components were defined as in equation.", "labels": [], "entities": []}, {"text": "Contrary to our earlier work, we did not lemmatize the corpus before constructing the vectors as in the context of language modeling this was not appropriate.", "labels": [], "entities": []}, {"text": "We also trained the LDA model on BLLIP, using implementation.", "labels": [], "entities": [{"text": "BLLIP", "start_pos": 33, "end_pos": 38, "type": "METRIC", "confidence": 0.5376699566841125}]}, {"text": "We experimented with different numbers of topics on the development set (from 10 to 200) and report results on the test set with 100 topics.", "labels": [], "entities": []}, {"text": "In our experiments, the hyperparameter \u03b1 was initialized to 0.5, and the \u03b2 word probabilities were initialized randomly.", "labels": [], "entities": []}, {"text": "We integrated our compositional models with a trigram model which we also trained on BLLIP.", "labels": [], "entities": [{"text": "BLLIP", "start_pos": 85, "end_pos": 90, "type": "DATASET", "confidence": 0.7537596821784973}]}, {"text": "The model was built using the SRILM toolkit) with backoff and Good-Turing smoothing.", "labels": [], "entities": [{"text": "SRILM toolkit", "start_pos": 30, "end_pos": 43, "type": "DATASET", "confidence": 0.8179936707019806}]}, {"text": "Ideally, we would have liked to train Roark's (2001) parser on the same data as that used for the semantic models.", "labels": [], "entities": []}, {"text": "However, this would require a gold standard treebank several times larger than those currently available.", "labels": [], "entities": []}, {"text": "Following previous work on structured language modeling, we therefore trained the parser on sections 2-21 of the Penn Treebank containing 936,017 words.", "labels": [], "entities": [{"text": "structured language modeling", "start_pos": 27, "end_pos": 55, "type": "TASK", "confidence": 0.6263866523901621}, {"text": "Penn Treebank containing 936,017 words", "start_pos": 113, "end_pos": 151, "type": "DATASET", "confidence": 0.9629256248474121}]}, {"text": "Note that Roark's (2001) parser produces prefix probabilities for each word of a sentence which we converted to conditional probabilities by dividing each current probability by the previous one.", "labels": [], "entities": []}, {"text": "shows perplexity results when the compositional models are combined with an n-gram model.", "labels": [], "entities": []}, {"text": "With regard to the simple semantic space model (SSM) we observe that both additive and multiplicative approaches to constructing history are successful in reducing perplexity over the n-gram baseline, with the multiplicative model outperforming the additive one.", "labels": [], "entities": []}, {"text": "This confirms the hypothesis that for this type of semantic space the multiplicative vector combination function produces representations which have a sounder probabilistic basis.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Perplexities for n-gram, composition and  structured language models, and their combina- tions; subscripts SSM and LSA refer to the semantic  space and LDA models, respectively.", "labels": [], "entities": []}]}