{"title": [{"text": "Using the Web for Language Independent Spellchecking and Autocorrection", "labels": [], "entities": [{"text": "Language Independent Spellchecking", "start_pos": 18, "end_pos": 52, "type": "TASK", "confidence": 0.6794498860836029}]}], "abstractContent": [{"text": "We have designed, implemented and evaluated an end-to-end system spellcheck-ing and autocorrection system that does not require any manually annotated training data.", "labels": [], "entities": []}, {"text": "The World Wide Web is used as a large noisy corpus from which we infer knowledge about misspellings and word usage.", "labels": [], "entities": []}, {"text": "This is used to build an error model and an n-gram language model.", "labels": [], "entities": []}, {"text": "A small secondary set of news texts with artificially inserted misspellings are used to tune confidence classifiers.", "labels": [], "entities": []}, {"text": "Because no manual annotation is required, our system can easily be instantiated for new languages.", "labels": [], "entities": []}, {"text": "When evaluated on human typed data with real misspellings in English and German, our web-based systems outper-form baselines which use candidate corrections based on hand-curated dictionaries.", "labels": [], "entities": []}, {"text": "Our system achieves 3.8% total error rate in English.", "labels": [], "entities": [{"text": "total error rate", "start_pos": 25, "end_pos": 41, "type": "METRIC", "confidence": 0.7790127595265707}]}, {"text": "We show similar improvements in preliminary results on artificial data for Russian and Arabic.", "labels": [], "entities": []}], "introductionContent": [{"text": "Spellchecking is the task of predicting which words in a document are misspelled.", "labels": [], "entities": [{"text": "predicting which words in a document are misspelled", "start_pos": 29, "end_pos": 80, "type": "TASK", "confidence": 0.624800018966198}]}, {"text": "These predictions might be presented to a user by underlining the misspelled words.", "labels": [], "entities": []}, {"text": "Correction is the task of substituting the well-spelled hypotheses for misspellings.", "labels": [], "entities": []}, {"text": "Spellchecking and autocorrection are widely applicable for tasks such as wordprocessing and postprocessing Optical Character Recognition.", "labels": [], "entities": [{"text": "Optical Character Recognition", "start_pos": 107, "end_pos": 136, "type": "TASK", "confidence": 0.8084977070490519}]}, {"text": "We have designed, implemented and evaluated an end-to-end system that performs spellchecking and autocorrection.", "labels": [], "entities": []}, {"text": "The key novelty of our work is that the system was developed entirely without the use of manually annotated resources or any explicitly compiled dictionaries of well-spelled words.", "labels": [], "entities": []}, {"text": "Our multi-stage system integrates knowledge from statistical error models and language models (LMs) with a statistical machine learning classifier.", "labels": [], "entities": []}, {"text": "At each stage, data are required for training models and determining weights on the classifiers.", "labels": [], "entities": []}, {"text": "The models and classifiers are all automatically trained from frequency counts derived from the Web and from news data.", "labels": [], "entities": []}, {"text": "System performance has been validated on a set of human typed data.", "labels": [], "entities": []}, {"text": "We have also shown that the system can be rapidly ported across languages with very little manual effort.", "labels": [], "entities": []}, {"text": "Most spelling systems today require some handcrafted language-specific resources, such as lexica, lists of misspellings, or rule bases.", "labels": [], "entities": []}, {"text": "Systems using statistical models require large annotated corpora of spelling errors for training.", "labels": [], "entities": []}, {"text": "Our statistical models require no annotated data.", "labels": [], "entities": []}, {"text": "Instead, we rely on the Web as a large noisy corpus in the following ways.", "labels": [], "entities": []}, {"text": "1) We infer information about misspellings from term usage observed on the Web, and use this to build an error model.", "labels": [], "entities": []}, {"text": "2) The most frequently observed terms are taken as a noisy list of potential candidate corrections.", "labels": [], "entities": []}, {"text": "3) Token n-grams are used to build an LM, which we use to make context-appropriate corrections.", "labels": [], "entities": []}, {"text": "Because our error model is based on scoring substrings, there is no fixed lexicon of well-spelled words to determine misspellings.", "labels": [], "entities": []}, {"text": "Hence, both novel misspelled or well-spelled words are allowable.", "labels": [], "entities": []}, {"text": "Moreover, in combination with an n-gram LM component, our system can detect and correct real-word substitutions, ie, word usage and grammatical errors.", "labels": [], "entities": []}, {"text": "Confidence classifiers determine the thresholds for spelling error detection and autocorrection, given error and LM scores.", "labels": [], "entities": [{"text": "spelling error detection", "start_pos": 52, "end_pos": 76, "type": "TASK", "confidence": 0.7393255829811096}, {"text": "autocorrection", "start_pos": 81, "end_pos": 95, "type": "METRIC", "confidence": 0.9799474477767944}, {"text": "error and LM scores", "start_pos": 103, "end_pos": 122, "type": "METRIC", "confidence": 0.8019238412380219}]}, {"text": "In order to train these classifiers, we require some textual content with some misspellings and corresponding well-spelled words.", "labels": [], "entities": []}, {"text": "A small subset of the Web data from news pages are used because we assume they contain relatively few misspellings.", "labels": [], "entities": [{"text": "Web data from news pages", "start_pos": 22, "end_pos": 46, "type": "DATASET", "confidence": 0.8443825125694275}]}, {"text": "We show that confidence classifiers can be adequately trained and tuned without real-world spelling errors, but rather with clean news data injected with artificial misspellings.", "labels": [], "entities": []}, {"text": "This paper will proceed as follows.", "labels": [], "entities": []}, {"text": "In Section 2, we survey related prior research.", "labels": [], "entities": []}, {"text": "Section 3 describes our approach, and how we use data at each stage of the spelling system.", "labels": [], "entities": []}, {"text": "In experiments (Section 4), we first verify our system on data with artificial misspellings.", "labels": [], "entities": []}, {"text": "Then we report performance on data with real typing errors in English and German.", "labels": [], "entities": []}, {"text": "We also show preliminary results from porting our system to Russian and Arabic.", "labels": [], "entities": []}], "datasetContent": [{"text": "In our evaluation, we aimed to select metrics that we hypothesize would correlate well with real performance in a word-processing application.", "labels": [], "entities": []}, {"text": "In our intended system, misspelled words are autocorrected when confidence is high and misspelled words are flagged when a highly confident suggestion is absent.", "labels": [], "entities": []}, {"text": "This could be cast as a simple classification or retrieval task, where traditional measures of precision, recall and F metrics are used.", "labels": [], "entities": [{"text": "classification or retrieval task", "start_pos": 31, "end_pos": 63, "type": "TASK", "confidence": 0.7921659201383591}, {"text": "precision", "start_pos": 95, "end_pos": 104, "type": "METRIC", "confidence": 0.9993139505386353}, {"text": "recall", "start_pos": 106, "end_pos": 112, "type": "METRIC", "confidence": 0.9983153343200684}, {"text": "F metrics", "start_pos": 117, "end_pos": 126, "type": "METRIC", "confidence": 0.9742420017719269}]}, {"text": "However we wanted to focus on metrics that reflect the quality of end-toend behavior, that account for the combined effects of flagging and automatic correction.", "labels": [], "entities": []}, {"text": "Essentially, there are three states: a word could be unchanged, flagged or corrected to a suggested word.", "labels": [], "entities": []}, {"text": "Hence, we report on error rates that measure the errors that a user would encounter if the spellchecking/autocorrection were deployed in a word-processor.", "labels": [], "entities": []}, {"text": "We have identified 5 types of errors that a system could produce: 1.", "labels": [], "entities": []}, {"text": "E 1 : A misspelled word is wrongly corrected.", "labels": [], "entities": []}, {"text": "2. E 2 : A misspelled word is not corrected but is flagged.", "labels": [], "entities": []}, {"text": "3. E 3 : A misspelled word is not corrected or flagged.", "labels": [], "entities": []}, {"text": "4. E 4 : A well spelled word is wrongly corrected.", "labels": [], "entities": []}, {"text": "5. E 5 : A well spelled word is wrongly flagged.", "labels": [], "entities": []}, {"text": "It can be argued that these errors have varying impact on user experience.", "labels": [], "entities": []}, {"text": "For instance, a well spelled word that is wrongly corrected is more frustrating than a misspelled word that is not corrected but is flagged.", "labels": [], "entities": []}, {"text": "However, in this paper, we treat each error equally.", "labels": [], "entities": []}, {"text": "E 1 , E 2 , E 3 and E 4 pertain to the correction task.", "labels": [], "entities": [{"text": "correction task", "start_pos": 39, "end_pos": 54, "type": "TASK", "confidence": 0.8394792079925537}]}, {"text": "Hence we can define Correction Error Rate (CER): T where T is the total number of tokens.", "labels": [], "entities": [{"text": "Correction Error Rate (CER)", "start_pos": 20, "end_pos": 47, "type": "METRIC", "confidence": 0.9091830849647522}]}, {"text": "E 3 and E 5 pertain to the nature of flagging.", "labels": [], "entities": []}, {"text": "We define Flagging Error Rate (FER) and Total Error Rate (TER): T For each system, we computed a No Good Suggestion Rate (NGS) which represents the proportion of misspelled words for which the suggestions list did not contain the correct word.", "labels": [], "entities": [{"text": "Flagging Error Rate (FER)", "start_pos": 10, "end_pos": 35, "type": "METRIC", "confidence": 0.8770654400189718}, {"text": "Total Error Rate (TER)", "start_pos": 40, "end_pos": 62, "type": "METRIC", "confidence": 0.9804025590419769}, {"text": "No Good Suggestion Rate (NGS)", "start_pos": 97, "end_pos": 126, "type": "METRIC", "confidence": 0.726354820387704}]}, {"text": "System  Results on English news data with artificial spelling errors are displayed in.", "labels": [], "entities": [{"text": "English news data", "start_pos": 19, "end_pos": 36, "type": "DATASET", "confidence": 0.7283557256062826}]}, {"text": "The systems which do not employ the LM scores perform substantially poorer that the ones with LM scores.", "labels": [], "entities": []}, {"text": "The Aspell system yields a total error rate of 17.65% and our system with Web-based suggestions yields TER of 9.06%.", "labels": [], "entities": [{"text": "error rate", "start_pos": 33, "end_pos": 43, "type": "METRIC", "confidence": 0.9890198409557343}, {"text": "TER", "start_pos": 103, "end_pos": 106, "type": "METRIC", "confidence": 0.9996250867843628}]}, {"text": "When comparing the simple scorer with the logistic regression classifier, the Aspell Systems 2 and 3 generate similar performances while the confidence classifier afforded some gains in our Web-based suggestions system, with total error reduced from 2.62% to 2.55%.", "labels": [], "entities": [{"text": "error", "start_pos": 231, "end_pos": 236, "type": "METRIC", "confidence": 0.7153923511505127}]}, {"text": "The ability to tune each phase during development has so far proven more useful than the specific features or classifier used.", "labels": [], "entities": []}, {"text": "Blacklisting is crucial as seen by our results for Systems 4 and 8.", "labels": [], "entities": []}, {"text": "When the blacklisting mechanism is not used, performance steeply declines.", "labels": [], "entities": []}, {"text": "When comparing overall performance for the data between the Aspell systems and the Webbased suggestions systems, our Web-based suggestions fare better across the board for the news data with artificial misspellings.", "labels": [], "entities": []}, {"text": "Performance gains are evident for each error metric that was examined.", "labels": [], "entities": []}, {"text": "Total error rate for our best system (System 7) reduces the error of the best Aspell system (System 3) by 45.7% (from 4.83% to 2.62%).", "labels": [], "entities": [{"text": "Total error rate", "start_pos": 0, "end_pos": 16, "type": "METRIC", "confidence": 0.7714512348175049}, {"text": "error", "start_pos": 60, "end_pos": 65, "type": "METRIC", "confidence": 0.9911044836044312}]}, {"text": "In addition, our no good suggestion rate is only 10% compared to 18% in the Aspell system.", "labels": [], "entities": [{"text": "no good suggestion rate", "start_pos": 17, "end_pos": 40, "type": "METRIC", "confidence": 0.7021651566028595}, {"text": "Aspell system", "start_pos": 76, "end_pos": 89, "type": "DATASET", "confidence": 0.9090037047863007}]}, {"text": "Even where no LM scores are used, our Web-based suggestions system outperforms the Aspell system.", "labels": [], "entities": []}, {"text": "The above results suggest that the Web-based suggestions system performs at least as well as the Aspell system.", "labels": [], "entities": []}, {"text": "However, it must be highlighted that results on the test set with artificial errors does not guarantee similar performance on real user data.", "labels": [], "entities": []}, {"text": "The artificial errors were generated at a systematically uniform rate, and are not modeled after real human errors made in real wordprocessing applications.", "labels": [], "entities": []}, {"text": "We attempt to consider the impact of real human errors on our systems in the next section.", "labels": [], "entities": []}, {"text": "System  Results for our system evaluated on data with real misspellings in English and in German are shown in.", "labels": [], "entities": []}, {"text": "We used the systems that performed best on the artificial data (System 3 for Aspell, and System 7 for Web suggestions).", "labels": [], "entities": []}, {"text": "The misspelling error rates of the test sets were 10.8% and 12.5% respectively, higher than those of the artificial data which were used during development.", "labels": [], "entities": [{"text": "misspelling error rates", "start_pos": 4, "end_pos": 27, "type": "METRIC", "confidence": 0.9016683300336202}]}, {"text": "For English, the Web-based suggestions resulted in a 17% improvement (from 4.58% to 3.80%) in total error rate, but the correction error rate was slightly (2.4%) higher.", "labels": [], "entities": [{"text": "total error rate", "start_pos": 94, "end_pos": 110, "type": "METRIC", "confidence": 0.7770819862683614}, {"text": "correction error rate", "start_pos": 120, "end_pos": 141, "type": "METRIC", "confidence": 0.977405329545339}]}, {"text": "By contrast, in German our system improved total error by 30%, from 14.09% to 9.80%.", "labels": [], "entities": [{"text": "total", "start_pos": 43, "end_pos": 48, "type": "METRIC", "confidence": 0.9726272821426392}, {"text": "error", "start_pos": 49, "end_pos": 54, "type": "METRIC", "confidence": 0.5656902194023132}]}, {"text": "Correction error rate was also much lower in our German system, comparing 7.89% with 10.23% for the Aspell system.", "labels": [], "entities": [{"text": "Correction error rate", "start_pos": 0, "end_pos": 21, "type": "METRIC", "confidence": 0.9063216845194498}, {"text": "Aspell system", "start_pos": 100, "end_pos": 113, "type": "DATASET", "confidence": 0.8948324620723724}]}, {"text": "The no good suggestion rates for the real misspelling data are also higher than that of the news data.", "labels": [], "entities": []}, {"text": "Our suggestions are limited to an edit distance of 2 with the original, and it was found that in real human errors, the average edit distance of misspelled words is 1.38 but for our small data, the maximum edit distance is 4 in English and 7 in German.", "labels": [], "entities": []}, {"text": "Nonetheless, our no good suggestion rates (17.2% and 32.3%) are much lower than those of the Aspell system (23% and 44%), highlighting the advantage of not using a hand-crafted lexicon.", "labels": [], "entities": [{"text": "Aspell system", "start_pos": 93, "end_pos": 106, "type": "DATASET", "confidence": 0.8422409892082214}]}, {"text": "Our results on real typed data were slightly worse than those for the news data.", "labels": [], "entities": []}, {"text": "Several factors may account for this.", "labels": [], "entities": []}, {"text": "(1) While the news data test set does not overlap with the classifier training set, the nature of the content is similar to the train and dev sets in that they are all news articles from a one week period.", "labels": [], "entities": [{"text": "news data test set", "start_pos": 14, "end_pos": 32, "type": "DATASET", "confidence": 0.8218845725059509}]}, {"text": "This differs substantially from Wikipedia article topics that were generally about the history and sights a city.", "labels": [], "entities": []}, {"text": "(2) Second, the method for inserting character errors (random generation) was the same for the news data sets while the real typed test set differed from the artificial errors in the training set.", "labels": [], "entities": [{"text": "news data sets", "start_pos": 95, "end_pos": 109, "type": "DATASET", "confidence": 0.8170937895774841}]}, {"text": "Typed errors are less consistent and error rates differed across subjects.", "labels": [], "entities": [{"text": "error rates", "start_pos": 37, "end_pos": 48, "type": "METRIC", "confidence": 0.9712876081466675}]}, {"text": "More in depth study is needed to understand the nature of real typed errors.", "labels": [], "entities": []}, {"text": "System: Results for German, Russian, Arabic news data.", "labels": [], "entities": []}, {"text": "Our system can be trained on many languages with almost no manual effort.", "labels": [], "entities": []}, {"text": "Results for German, Arabic and Russian news data are shown in Table 4.", "labels": [], "entities": [{"text": "German, Arabic and Russian news data", "start_pos": 12, "end_pos": 48, "type": "DATASET", "confidence": 0.5876643317086356}]}, {"text": "Performance improvements by the Web suggester over Aspell are greater for these languages than for English.", "labels": [], "entities": []}, {"text": "Relative performance improvements in total error rates are 47% in German, 60% in Arabic and 79% in Russian.", "labels": [], "entities": [{"text": "total error rates", "start_pos": 37, "end_pos": 54, "type": "METRIC", "confidence": 0.75262979666392}]}, {"text": "Differences in no good suggestion rates are also very pronounced between Aspell and the Web suggester.", "labels": [], "entities": []}, {"text": "It cannot be assumed that the Arabic and Russian systems would perform as well on real data.", "labels": [], "entities": []}, {"text": "However the correlation between data sets reported in Section 5.4 lead us to hypothesize that a comparison between the Web suggester and Aspell on real data would be favourable.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Results for English news data on an in- dependent test set with artificial spelling errors.  Numbers are given in percentages. LM: Language  Model, Sim: Simple, LR: Logistic Regression,  WS: Web-based suggestions. NGS: No good sug- gestion rate.", "labels": [], "entities": [{"text": "NGS", "start_pos": 224, "end_pos": 227, "type": "DATASET", "confidence": 0.6989040970802307}, {"text": "sug- gestion rate", "start_pos": 237, "end_pos": 254, "type": "METRIC", "confidence": 0.6207478046417236}]}, {"text": " Table 3: Results for Data with Real Errors in En- glish and German.", "labels": [], "entities": []}, {"text": " Table 4: Results for German, Russian, Arabic  news data.", "labels": [], "entities": []}]}