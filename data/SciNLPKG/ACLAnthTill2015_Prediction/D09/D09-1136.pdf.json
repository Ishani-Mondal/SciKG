{"title": [{"text": "Bayesian Learning of Phrasal Tree-to-String Templates", "labels": [], "entities": []}], "abstractContent": [{"text": "We examine the problem of overcoming noisy word-level alignments when learning tree-to-string translation rules.", "labels": [], "entities": []}, {"text": "Our approach introduces new rules, and re-estimates rule probabilities using EM.", "labels": [], "entities": []}, {"text": "The major obstacles to this approach are the very reasons that word-alignments are used for rule extraction: the huge space of possible rules, as well as controlling overfitting.", "labels": [], "entities": [{"text": "rule extraction", "start_pos": 92, "end_pos": 107, "type": "TASK", "confidence": 0.8278800547122955}]}, {"text": "By carefully controlling which portions of the original alignments are re-analyzed, and by using Bayesian inference during re-analysis, we show significant improvement over the baseline rules extracted from word-level alignments.", "labels": [], "entities": []}], "introductionContent": [{"text": "Non-parametric Bayesian methods have been successfully applied to directly learn phrase pairs from a bilingual corpus with little or no dependence on word alignments.", "labels": [], "entities": []}, {"text": "Because such approaches directly learn a generative model over phrase pairs, they are theoretically preferable to the standard heuristics for extracting the phrase pairs from the many-to-one word-level alignments produced by the IBM series models ( or the Hidden Markov Model (HMM) (.", "labels": [], "entities": []}, {"text": "We wish to apply this direct, Bayesian approach to learn better translation rules for syntaxbased statistical MT (SSMT), by which we specifically refer to MT systems using Tree-to-String (TTS) translation templates derived from syntax trees (;, as opposed to formally syntactic systems such as Hiero.", "labels": [], "entities": [{"text": "syntaxbased statistical MT (SSMT)", "start_pos": 86, "end_pos": 119, "type": "TASK", "confidence": 0.7038247386614481}]}, {"text": "The stumbling block preventing us from taking this approach is the extremely large space of possible TTS templates when no word alignments are given.", "labels": [], "entities": [{"text": "TTS templates", "start_pos": 101, "end_pos": 114, "type": "TASK", "confidence": 0.8482522666454315}, {"text": "word alignments", "start_pos": 123, "end_pos": 138, "type": "TASK", "confidence": 0.7133697420358658}]}, {"text": "Given a sentence pair and syntax tree over one side, there are an exponential number of potential TTS templates and a polynomial number of phrase pairs.", "labels": [], "entities": []}, {"text": "In this paper, we explore methods for restricting the space of possible TTS templates under consideration, while still allowing good templates to emerge directly from the data as much as possible.", "labels": [], "entities": []}, {"text": "We find an improvement in translation accuracy through, first, using constraints to limit the number of new templates, second, using Bayesian methods to limit which of these new templates are favored when re-analyzing the training data with EM, and, third, experimenting with different renormalization techniques for the EM re-analysis.", "labels": [], "entities": [{"text": "translation", "start_pos": 26, "end_pos": 37, "type": "TASK", "confidence": 0.9607177376747131}, {"text": "accuracy", "start_pos": 38, "end_pos": 46, "type": "METRIC", "confidence": 0.9151782393455505}]}, {"text": "We introduce two constraints to limit the number of TTS templates that we extract directly from tree/string pairs without using word alignments.", "labels": [], "entities": [{"text": "TTS templates", "start_pos": 52, "end_pos": 65, "type": "TASK", "confidence": 0.8612080812454224}]}, {"text": "The first constraint is to limit direct TTS template extraction to the part of the corpus where word alignment tools such as GIZA++ do poorly.", "labels": [], "entities": [{"text": "TTS template extraction", "start_pos": 40, "end_pos": 63, "type": "TASK", "confidence": 0.9453619917233785}, {"text": "word alignment", "start_pos": 96, "end_pos": 110, "type": "TASK", "confidence": 0.7534044981002808}]}, {"text": "There is no reason not to re-use the good alignments from GIZA++, which holds a very competitive baseline performance.", "labels": [], "entities": [{"text": "GIZA++", "start_pos": 58, "end_pos": 64, "type": "DATASET", "confidence": 0.8633798062801361}]}, {"text": "As already mentioned, the noisy alignments from GIZA++ are likely to cross the boundaries of the tree constituents, which leads to comparatively big TTS templates.", "labels": [], "entities": []}, {"text": "We use this fact as a heuristic to roughly distinguish noisy from good word alignments.", "labels": [], "entities": []}, {"text": "Here we define big templates as those with more than 8 symbols in their right hand sides (RHSs).", "labels": [], "entities": []}, {"text": "The word alignments in big templates are considered to be noisy and will be recomposed by extracting smaller TTS templates.", "labels": [], "entities": [{"text": "word alignments", "start_pos": 4, "end_pos": 19, "type": "TASK", "confidence": 0.6931414157152176}]}, {"text": "Another reason to do extraction on big templates is that the applicability of big templates to new sentences is very limited due to their size, and the portion of the training data from which they are extracted is effectively wasted.", "labels": [], "entities": []}, {"text": "The second constraint, after choosing the extraction site, is to extract the TTS templates all the way down to the leaves of the hosting templates.", "labels": [], "entities": []}, {"text": "This constraint limits the number of possible left hand sides (LHSs) to be equal to the number of tree nodes in the hosting templates.", "labels": [], "entities": []}, {"text": "The entire extraction process can be summarized in 3 steps: 1.", "labels": [], "entities": []}, {"text": "Compute word alignments using GIZA++, and generate the basic TTS templates.", "labels": [], "entities": [{"text": "word alignments", "start_pos": 8, "end_pos": 23, "type": "TASK", "confidence": 0.7150552272796631}]}, {"text": "2. Select big templates from the basic TTS templates in step 1, and extract smaller TTS templates all the way down to the bottom from big templates, without considering the precomputed word alignments.", "labels": [], "entities": []}, {"text": "3. Combine TTS templates from step 1 and step 2 and estimate their probabilities using Variational Bayes with a Dirichlet Process prior.", "labels": [], "entities": []}, {"text": "In step 2, since there are no constraints from the pre-computed word alignments, we have complete freedom in generating all possible TTS templates to overcome noisy word alignments.", "labels": [], "entities": []}, {"text": "We use variational EM to approximate the inference of our Bayesian model and explore different normalization methods for the TTS templates.", "labels": [], "entities": []}, {"text": "A two-stage normalization is proposed by combining LHSbased normalization with normalization based on the root of the LHS, and is shown to be the best model when used with variational EM.", "labels": [], "entities": []}, {"text": "recompose the TTS templates by inserting unaligned target words and combining small templates into bigger ones.", "labels": [], "entities": [{"text": "TTS templates", "start_pos": 14, "end_pos": 27, "type": "TASK", "confidence": 0.732084184885025}]}, {"text": "The recomposed templates are then re-estimated using the EM algorithm described in.", "labels": [], "entities": []}, {"text": "This approach also generates TTS templates beyond the precomputed word alignments, but the freedom is only granted over unaligned target words, and most of the pre-computed word alignments remain unchanged.", "labels": [], "entities": [{"text": "TTS templates", "start_pos": 29, "end_pos": 42, "type": "TASK", "confidence": 0.8672585487365723}]}, {"text": "Other prior approaches towards improving TTS templates focus on improving the word alignment performance over the classic models such as IBM series models and Hidden Markov Model (HMM), which do not consider the syntactic structure of the aligning languages and produce syntax-violating alignments.", "labels": [], "entities": [{"text": "TTS templates", "start_pos": 41, "end_pos": 54, "type": "TASK", "confidence": 0.9386078417301178}, {"text": "word alignment", "start_pos": 78, "end_pos": 92, "type": "TASK", "confidence": 0.7356045246124268}]}, {"text": "use a syntaxbased distance in an HMM word alignment model to favor syntax-friendly alignments.", "labels": [], "entities": [{"text": "HMM word alignment", "start_pos": 33, "end_pos": 51, "type": "TASK", "confidence": 0.8039969205856323}]}, {"text": "start from the GIZA++ alignment and incrementally delete bad links based on a discrim-", "labels": [], "entities": [{"text": "GIZA++ alignment", "start_pos": 15, "end_pos": 31, "type": "DATASET", "confidence": 0.8619180123011271}]}], "datasetContent": [{"text": "We train an English-to-Chinese translation system using the FBIS corpus, where 73,597 sentence pairs are selected as the training data, and 500 sentence pairs with no more than 25 words on the Chinese side are selected for both the development and test data.: BLEU-4 scores (test set) of the union alignment, using TTS templates up to a certain size, in terms of the number of leaves in their LHSs", "labels": [], "entities": [{"text": "FBIS corpus", "start_pos": 60, "end_pos": 71, "type": "DATASET", "confidence": 0.959709882736206}, {"text": "BLEU-4", "start_pos": 260, "end_pos": 266, "type": "METRIC", "confidence": 0.9984686970710754}]}], "tableCaptions": [{"text": " Table 1: Percentage of corpus used to generate big templates,  based on different word alignments", "labels": [], "entities": []}, {"text": " Table 3: BLEU-4 scores (test set) of systems based on  GIZA++ word alignments", "labels": [], "entities": [{"text": "BLEU-4", "start_pos": 10, "end_pos": 16, "type": "METRIC", "confidence": 0.9980459213256836}, {"text": "GIZA++ word alignments", "start_pos": 56, "end_pos": 78, "type": "TASK", "confidence": 0.6258819401264191}]}, {"text": " Table 4: BLEU-4 scores (test set) of the union alignment, us- ing TTS templates up to a certain size, in terms of the number  of leaves in their LHSs", "labels": [], "entities": [{"text": "BLEU-4", "start_pos": 10, "end_pos": 16, "type": "METRIC", "confidence": 0.9989606142044067}]}, {"text": " Table 5: BLEU-4 scores (test set) of different systems.", "labels": [], "entities": [{"text": "BLEU-4 scores (test set)", "start_pos": 10, "end_pos": 34, "type": "METRIC", "confidence": 0.8750179708003998}]}, {"text": " Table 6: The total number of templates and the percentage of  NEW-PHR, in the beginning and end of annealing VB", "labels": [], "entities": [{"text": "VB", "start_pos": 110, "end_pos": 112, "type": "TASK", "confidence": 0.4145139753818512}]}]}