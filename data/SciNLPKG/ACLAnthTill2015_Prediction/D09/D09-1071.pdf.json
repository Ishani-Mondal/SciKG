{"title": [{"text": "The infinite HMM for unsupervised PoS tagging", "labels": [], "entities": [{"text": "PoS tagging", "start_pos": 34, "end_pos": 45, "type": "TASK", "confidence": 0.819083034992218}]}], "abstractContent": [{"text": "We extend previous work on fully unsu-pervised part-of-speech tagging.", "labels": [], "entities": [{"text": "part-of-speech tagging", "start_pos": 47, "end_pos": 69, "type": "TASK", "confidence": 0.705718994140625}]}, {"text": "Using a non-parametric version of the HMM, called the infinite HMM (iHMM), we address the problem of choosing the number of hidden states in unsupervised Markov models for PoS tagging.", "labels": [], "entities": [{"text": "PoS tagging", "start_pos": 172, "end_pos": 183, "type": "TASK", "confidence": 0.8483968675136566}]}, {"text": "We experiment with two non-parametric priors, the Dirichlet and Pitman-Yor processes, on the Wall Street Journal dataset using a paral-lelized implementation of an iHMM inference algorithm.", "labels": [], "entities": [{"text": "Wall Street Journal dataset", "start_pos": 93, "end_pos": 120, "type": "DATASET", "confidence": 0.9782281666994095}]}, {"text": "We evaluate the results with a variety of clustering evaluation metrics and achieve equivalent or better performances than previously reported.", "labels": [], "entities": []}, {"text": "Building on this promising result we evaluate the output of the unsupervised PoS tagger as a direct replacement for the output of a fully supervised PoS tagger for the task of shallow parsing and compare the two evaluations.", "labels": [], "entities": [{"text": "PoS tagger", "start_pos": 77, "end_pos": 87, "type": "TASK", "confidence": 0.5757709294557571}]}], "introductionContent": [{"text": "Many Natural Language Processing (NLP) tasks are commonly tackled using supervised learning approaches.", "labels": [], "entities": [{"text": "Natural Language Processing (NLP) tasks", "start_pos": 5, "end_pos": 44, "type": "TASK", "confidence": 0.7300923892429897}]}, {"text": "These learning methods rely on the availability of labeled datasets which are usually produced by expensive manual annotation.", "labels": [], "entities": []}, {"text": "For some tasks, we have the choice to use unsupervised learning approaches.", "labels": [], "entities": []}, {"text": "While they do not necessarily achieve the same level of performance, they are appealing as unlabeled data is usually abundant.", "labels": [], "entities": []}, {"text": "In particular, for the purpose of exploring new domains and languages, obtainining labeled material can be prohibitively expensive and unsupervised learning methods area very attractive choice.", "labels": [], "entities": []}, {"text": "Recent work explored the task of part-of-speech tagging (PoS) using unsupervised Hidden Markov Models (HMMs) with encouraging results.", "labels": [], "entities": [{"text": "part-of-speech tagging (PoS)", "start_pos": 33, "end_pos": 61, "type": "TASK", "confidence": 0.8421247363090515}]}, {"text": "PoS tagging is a standard component in many linguistic processing pipelines, so any improvement on its performance is likely to impact a wide range of tasks.", "labels": [], "entities": [{"text": "PoS tagging", "start_pos": 0, "end_pos": 11, "type": "TASK", "confidence": 0.6888878494501114}]}, {"text": "It is important to point out that a completely unsupervised learning method will discover the statistics of a dataset according to a particular model choice but these statistics might not correspond exactly to our intuition about PoS tags. and assume that words are generated by a hidden Markov model and find that the resulting states strongly correlate with POS tags.", "labels": [], "entities": []}, {"text": "Nonetheless, identifying the HMM states with appropriate POS tags is hard.", "labels": [], "entities": []}, {"text": "Because many evaluation methods often require POS tags (rather than HMM states) this identification problem makes unsupervised systems difficult to evaluate.", "labels": [], "entities": []}, {"text": "One potential solution is to add a small amount of supervision as in who assume a dictionary of frequent words associated with possible PoS tags extracted from a labeled corpus.", "labels": [], "entities": []}, {"text": "Although this technique improves performance, in this paper we explore the completely unsupervised approach.", "labels": [], "entities": []}, {"text": "The reason for this is that better unsupervised approaches provide us with better starting points from which to explore how and whereto incorporate supervision.", "labels": [], "entities": []}, {"text": "In previous work on unsupervised PoS tagging a main question was how to set the number of hidden states appropriately.", "labels": [], "entities": [{"text": "PoS tagging", "start_pos": 33, "end_pos": 44, "type": "TASK", "confidence": 0.9141623377799988}]}, {"text": "reports results for different numbers of hidden states but it is unclear how to make this choice a priori, while leave this question as future work.", "labels": [], "entities": []}, {"text": "It is not uncommon in statistical machine learning to distinguish between parameters of a model and the capacity of a model.", "labels": [], "entities": [{"text": "statistical machine learning", "start_pos": 22, "end_pos": 50, "type": "TASK", "confidence": 0.6501623690128326}]}, {"text": "E.g. in a clustering context, the choice for the number of clusters (capacity) and the parameters of each cluster are often treated differently: the latter are estimated using algorithms like EM, MCMC or Variational Bayes while the former is chosen using commonsense, heuristics or in a Bayesian framework maybe using evidence maximization.", "labels": [], "entities": []}, {"text": "Non-parametric Bayesian methods area class of probability distributions which explicitly treat the capacity of a model as \"just another parameter\".", "labels": [], "entities": []}, {"text": "Potential advantages are \u2022 the model capacity can automatically adjust to the amount of data: e.g. when clustering a very small dataset, it is unlikely that many fine grained clusters can be distinguished, \u2022 inference can be more efficient: e.g. instead of running full inference for different model capacities and then choosing the best capacity (according to some choice of \"best\"), inference in non-parametric Bayesian methods integrates the capacity search in one algorithm.", "labels": [], "entities": []}, {"text": "This is particularly advantageous when parameters other than capacity need to be explored, since it reduces signifcantly the number of experiments needed.", "labels": [], "entities": []}, {"text": "None of these potential advantages are guaranteed and in this paper we investigate these two aspects for the task of unsupervised PoS tagging.", "labels": [], "entities": [{"text": "PoS tagging", "start_pos": 130, "end_pos": 141, "type": "TASK", "confidence": 0.8892607986927032}]}, {"text": "The contributions in this paper extend previous work on unsupervised PoS tagging in five ways.", "labels": [], "entities": [{"text": "PoS tagging", "start_pos": 69, "end_pos": 80, "type": "TASK", "confidence": 0.8872776329517365}]}, {"text": "First, we introduce the use of a non-parametric version of the HMM, namely the infinite HMM (iHMM) () for unsupervised PoS tagging.", "labels": [], "entities": [{"text": "PoS tagging", "start_pos": 119, "end_pos": 130, "type": "TASK", "confidence": 0.7201917171478271}]}, {"text": "This answers an open problem from.", "labels": [], "entities": []}, {"text": "Second, we carefully implemented a parallelized version of the inference algorithms for the iHMM so we could use it on the Wall Street Journal Penn Treebank dataset.", "labels": [], "entities": [{"text": "iHMM", "start_pos": 92, "end_pos": 96, "type": "DATASET", "confidence": 0.9426677227020264}, {"text": "Wall Street Journal Penn Treebank dataset", "start_pos": 123, "end_pos": 164, "type": "DATASET", "confidence": 0.9649979869524637}]}, {"text": "Third, we introduce anew variant of the iHMM that builds on the Pitman-Yor process.", "labels": [], "entities": []}, {"text": "Fourth, we evaluate the results with a variety of clustering evaluation methods and achieve equivalent or better performances than previously reported.", "labels": [], "entities": []}, {"text": "Finally, building on this promising result we use the output of the unsupervised PoS tagger as a direct replacement for the output of a fully supervised PoS tagger for the task of shallow parsing.", "labels": [], "entities": [{"text": "PoS tagger", "start_pos": 81, "end_pos": 91, "type": "TASK", "confidence": 0.609127014875412}, {"text": "shallow parsing", "start_pos": 180, "end_pos": 195, "type": "TASK", "confidence": 0.5911417901515961}]}, {"text": "This evaluation enables us to assess the applicability of an unsupervised PoS tagging method and provides us with means of comparing its performance against a supervised PoS tagger.", "labels": [], "entities": [{"text": "PoS tagging", "start_pos": 74, "end_pos": 85, "type": "TASK", "confidence": 0.8153873085975647}, {"text": "PoS tagger", "start_pos": 170, "end_pos": 180, "type": "TASK", "confidence": 0.6328214108943939}]}, {"text": "The rest of the paper is structured as follows: in section 2 we introduce the iHMM as a nonparametric version of the Bayesian HMM used in previous work on unsupervised PoS tagging.", "labels": [], "entities": [{"text": "PoS tagging", "start_pos": 168, "end_pos": 179, "type": "TASK", "confidence": 0.7385708391666412}]}, {"text": "Then, in section 3 we describe some details of our implementation of the iHMM.", "labels": [], "entities": [{"text": "iHMM", "start_pos": 73, "end_pos": 77, "type": "DATASET", "confidence": 0.9416036009788513}]}, {"text": "In section 4 we present a variety of evaluation metrics to compare our results with previous work.", "labels": [], "entities": []}, {"text": "Finally, in section 5 we report our experimental results.", "labels": [], "entities": []}, {"text": "We conclude this paper with a discussion of ongoing work and experiments.", "labels": [], "entities": []}], "datasetContent": [{"text": "Evaluating unsupervised PoS tagging is rather difficult mainly due to the fact that the output of such systems are not actual PoS tags but state identifiers.", "labels": [], "entities": [{"text": "PoS tagging", "start_pos": 24, "end_pos": 35, "type": "TASK", "confidence": 0.8280887305736542}]}, {"text": "Therefore it is impossible to evaluate performance against a manually annotated gold standard using accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 100, "end_pos": 108, "type": "METRIC", "confidence": 0.9982060194015503}]}, {"text": "Recent work on this task explored a variety of methodologies to address this issue.", "labels": [], "entities": []}, {"text": "The most common approach followed in previous work is to evaluate unsupervised PoS tagging as clustering against a gold standard using the Variation of Information (VI).", "labels": [], "entities": [{"text": "PoS tagging", "start_pos": 79, "end_pos": 90, "type": "TASK", "confidence": 0.8769441545009613}]}, {"text": "VI assesses homogeneity and completeness using the quantities H(C|K) (the conditional entropy of the class distribution in the gold standard given the clustering) and H(K|C) (the conditional entropy of clustering given the class distribution in the gold standard).", "labels": [], "entities": []}, {"text": "However, as point out, VI is biased towards clusterings with a small number of clusters.", "labels": [], "entities": [{"text": "VI", "start_pos": 23, "end_pos": 25, "type": "TASK", "confidence": 0.9677147269248962}]}, {"text": "A different evaluation measure that uses the same quantities but weighs them differently is the V-measure, which is defined in Equation 2 by setting the parameter \u03b2 to 1.", "labels": [], "entities": []}, {"text": "Vlachos et al. noted that V-measure favors clusterings with a large number of clusters.", "labels": [], "entities": []}, {"text": "Both of these biases become crucial in our experiments, since the number of clusters (states of the iHMM) is not fixed in advance.", "labels": [], "entities": []}, {"text": "Vlachos et al. proposed a variation of the V-measure, V-beta, that adjusts the balance between homogeneity and completeness using the parameter \u03b2 in Eq.", "labels": [], "entities": []}, {"text": "2. It is worth mentioning that, unlike V-measure and V-beta, VI scores are not normalized and therefore they are difficult to interpret.", "labels": [], "entities": [{"text": "VI", "start_pos": 61, "end_pos": 63, "type": "METRIC", "confidence": 0.9743050932884216}]}, {"text": "Meil\u02d8 a (2007) presented two normalizations, acknowledging the potential disadvantages they have.", "labels": [], "entities": [{"text": "Meil\u02d8 a (2007)", "start_pos": 0, "end_pos": 14, "type": "DATASET", "confidence": 0.8800651729106903}]}, {"text": "The first one normalizes VI by 2 log(max(|K|, |C|)), which is inappropriate when the number of clusters discovered |K| changes between experiments.", "labels": [], "entities": []}, {"text": "The second normalization involves the quantity log N which is appropriate when comparing different algorithms on the same dataset (N is the number of instances).", "labels": [], "entities": []}, {"text": "However, this quantity depends exclusively on the size of the dataset and hence if the dataset is very large it can result in normalized VI scores misleadingly close to 100%.", "labels": [], "entities": [{"text": "normalized VI scores", "start_pos": 126, "end_pos": 146, "type": "METRIC", "confidence": 0.7529739936192831}]}, {"text": "This does not affect rankings, i.e. a better VI score will also be translated into a better normalized VI score.", "labels": [], "entities": [{"text": "VI score", "start_pos": 45, "end_pos": 53, "type": "METRIC", "confidence": 0.9794207811355591}, {"text": "VI score", "start_pos": 103, "end_pos": 111, "type": "METRIC", "confidence": 0.9057348072528839}]}, {"text": "In our experiments, we report results only with the un-normalized VI scores, V-measure and V-beta.", "labels": [], "entities": [{"text": "VI", "start_pos": 66, "end_pos": 68, "type": "METRIC", "confidence": 0.9181286096572876}]}, {"text": "All the evaluation measures mentioned so far evaluate PoS tagging as a clustering task against a manually annotated gold standard.", "labels": [], "entities": [{"text": "PoS tagging", "start_pos": 54, "end_pos": 65, "type": "TASK", "confidence": 0.9252443611621857}]}, {"text": "While this is reasonable, it still does not provide means of assessing the performance in away that would allow comparisons with supervised methods that output actual PoS tags.", "labels": [], "entities": []}, {"text": "Even for the normalized measures V-measure and V-beta, it is unclear how their values relate to accuracy levels.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 96, "end_pos": 104, "type": "METRIC", "confidence": 0.9987320303916931}]}, {"text": "partially addressed this issue by mapping states to PoS tags following two different strategies, cross-validation accuracy, and greedy 1-to-1 mapping, which both have shortcomings.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 114, "end_pos": 122, "type": "METRIC", "confidence": 0.9595680236816406}]}, {"text": "We argue that since an unsupervised PoS tagger is trained without taking any gold standard into account, it is not appropriate to evaluate against a particular gold standard, or at least this should not be the sole criterion.", "labels": [], "entities": [{"text": "PoS tagger", "start_pos": 36, "end_pos": 46, "type": "TASK", "confidence": 0.8796436488628387}]}, {"text": "The fact that different authors use different versions of the same gold standard to evaluate similar experiments (e.g.) supports this claim.", "labels": [], "entities": []}, {"text": "Furthermore, PoS tagging is seldomly a goal in itself, but it is a component in a linguistic pipeline.", "labels": [], "entities": [{"text": "PoS tagging", "start_pos": 13, "end_pos": 24, "type": "TASK", "confidence": 0.8745419085025787}]}, {"text": "In order to address these issues, we perform an extrinsic evaluation using a well-explored task that involves PoS tags.", "labels": [], "entities": []}, {"text": "While PoS tagging is considered a pre-processing step in many natural language processing pipelines, the choice of task is restricted by the lack of real PoS tags in the output of our system.", "labels": [], "entities": [{"text": "PoS tagging", "start_pos": 6, "end_pos": 17, "type": "TASK", "confidence": 0.865447998046875}]}, {"text": "For our purposes we need a task that relies on discriminating between PoS tags rather than the PoS tag semantics themselves, in other words, a task in which knowing whether a word is tagged as noun instead of a verb is equivalent to knowing it is tagged as state 1 instead of state 2.", "labels": [], "entities": []}, {"text": "Taking these considerations into account, in Section 5 we experiment with shallow parsing in the context of the CoNLL-2000 shared task) in which very good performances were achieved using only the words with their PoS tags.", "labels": [], "entities": [{"text": "CoNLL-2000 shared task", "start_pos": 112, "end_pos": 134, "type": "DATASET", "confidence": 0.8162373503049215}]}, {"text": "Our intuition is that if the iHMM (or any unsupervised PoS tagging method) has a reasonable level of performance, it should improve on the performance of a system that does not use PoS tags.", "labels": [], "entities": [{"text": "PoS tagging", "start_pos": 55, "end_pos": 66, "type": "TASK", "confidence": 0.7733193933963776}]}, {"text": "Moreover, if the performance is very good indeed, it should get close to the performance of a system that uses real PoS tags, provided either by human annotation or by a good supervised system.", "labels": [], "entities": []}, {"text": "Similar extrinsic evaluation was performed by.", "labels": [], "entities": []}, {"text": "It is of interest to compare the results between the clustering evaluation and the extrinsic one.", "labels": [], "entities": []}, {"text": "A different approach in evaluating nonparametric Bayesian models for NLP is statesplitting (.", "labels": [], "entities": []}, {"text": "In this setting, the model is used in order to refine existing annotation of the dataset.", "labels": [], "entities": []}, {"text": "While this approach can provide us with some insights and interpretable results, the use of existing annotation influences the output of the model.", "labels": [], "entities": []}, {"text": "In this work, we want to verify whether the output of the iHMM (without any supervision) can be used instead of that of a supervised system.", "labels": [], "entities": [{"text": "iHMM", "start_pos": 58, "end_pos": 62, "type": "DATASET", "confidence": 0.9075023531913757}]}, {"text": "In all our experiments, the Wall Street Journal (WSJ) part of the Penn Treebank was used.", "labels": [], "entities": [{"text": "Wall Street Journal (WSJ) part of the Penn Treebank", "start_pos": 28, "end_pos": 79, "type": "DATASET", "confidence": 0.9440882910381664}]}, {"text": "As explained in Section 4, we evaluate the output of the iHMM in two ways, as clustering with respect to a gold standard and as direct replacement of the PoS tags in the task of shallow parsing.", "labels": [], "entities": [{"text": "iHMM", "start_pos": 57, "end_pos": 61, "type": "DATASET", "confidence": 0.9327332973480225}]}, {"text": "In each experiment, we obtain a sample from the iHMM overall the sections of WSJ.", "labels": [], "entities": [{"text": "iHMM", "start_pos": 48, "end_pos": 52, "type": "DATASET", "confidence": 0.939415454864502}, {"text": "WSJ", "start_pos": 77, "end_pos": 80, "type": "DATASET", "confidence": 0.8819100856781006}]}, {"text": "The states for sections 15-18 and 20 of the WSJ (training and testing sets respectvely in the CoNLL shared task) are used for the evaluation based on shallow parsing, while the remaining sections are used for evaluation against the WSJ gold standard PoS tags using clustering evaluation measures.", "labels": [], "entities": [{"text": "WSJ", "start_pos": 44, "end_pos": 47, "type": "DATASET", "confidence": 0.9509140253067017}, {"text": "WSJ gold standard PoS tags", "start_pos": 232, "end_pos": 258, "type": "DATASET", "confidence": 0.9380980253219604}]}, {"text": "As described in Section 2 we performed three runs with the iHMM: one run with DP prior and fixed \u03b3, \u03b1, one with PY prior and fixed d, \u03b3, \u03b1 and one with DP prior but where we learn the hyperparameters \u03b3, \u03b1 from the data.", "labels": [], "entities": [{"text": "iHMM", "start_pos": 59, "end_pos": 63, "type": "DATASET", "confidence": 0.9615417122840881}]}, {"text": "Our inference algorithm uses 1000 burn-in iterations after which we collect a sample every 1000 iterations.", "labels": [], "entities": []}, {"text": "Our inference procedure is annealed during the first 1000 burnin and 2400 iterations by powering the likelihood of the output distribution with a number that smoothly increases from 0.4 to 1.0 over the 3400 first iterations.", "labels": [], "entities": []}, {"text": "The numbers of iterations reported in the remainder of the section refer to the iterations after burn-in.", "labels": [], "entities": []}, {"text": "We initialized the sampler by: a) sampling the hyperparameters from the prior where applicable, b) uniformly assign each word one out of 20 iHMM states.", "labels": [], "entities": []}, {"text": "For the DP run with fixed parameters, we chose \u03b1 = 0.8 to encourage some sparsity in the transition matrix and \u03b3 = 5.0 to allow for enough hidden states.", "labels": [], "entities": []}, {"text": "For the PY run with fixed parameters, we chose \u03b1 = 0.8 for similar reasons and d = 0.1 and \u03b3 = 1.0.", "labels": [], "entities": []}, {"text": "We point out that one weakness of MCMC methods is that they are hard to test for convergence.", "labels": [], "entities": []}, {"text": "We chose to run the simulations until they became prohibitively expensive to obtain anew sample.", "labels": [], "entities": []}, {"text": "First, we present results using clustering evaluation measures which appear in the figures of.", "labels": [], "entities": []}, {"text": "The three runs exhibit different behavior.", "labels": [], "entities": []}, {"text": "The number of states reached by the iHMM with fixed parameters using the DP prior stabilizes close to 50 states, while for the experiment with learnt hyperparameters the number of states grows more rapidly, reaching 194 states after 8,000 iterations.", "labels": [], "entities": [{"text": "iHMM", "start_pos": 36, "end_pos": 40, "type": "DATASET", "confidence": 0.9356667399406433}, {"text": "DP prior", "start_pos": 73, "end_pos": 81, "type": "DATASET", "confidence": 0.6773994565010071}]}, {"text": "With the PY prior, the number of states reached grows less rapidly reaching 90 states.", "labels": [], "entities": [{"text": "PY prior", "start_pos": 9, "end_pos": 17, "type": "DATASET", "confidence": 0.8941364288330078}]}, {"text": "All runs achieve better performances with respect to all the measures used as the number of iterations grows.", "labels": [], "entities": []}, {"text": "An exception is that VI scores tend to increase (lower VI scores are better) when the number of states grows larger than the gold standard.", "labels": [], "entities": [{"text": "VI scores", "start_pos": 21, "end_pos": 30, "type": "METRIC", "confidence": 0.94218510389328}, {"text": "VI scores", "start_pos": 55, "end_pos": 64, "type": "METRIC", "confidence": 0.9668701887130737}]}, {"text": "It is interesting to notice how the measures exhibit different biases, in particular that VI penalizes the larger numbers of states discovered in the DP run with learnt parameters as well as the run with the PY prior, compared to the more lenient scores provided by V-measure and V-beta.", "labels": [], "entities": [{"text": "VI", "start_pos": 90, "end_pos": 92, "type": "METRIC", "confidence": 0.9591233730316162}]}, {"text": "The latter though assigns lower scores to the DP run with learnt parameters because it takes into account that the high homogeneity is achieved using even more states.", "labels": [], "entities": []}, {"text": "Finally, the interpretability of these scores presents some interest.", "labels": [], "entities": [{"text": "interpretability", "start_pos": 13, "end_pos": 29, "type": "TASK", "confidence": 0.9545825719833374}]}, {"text": "For example, in the run with fixed parameters using the DP prior, after burn-in VI was 4.6, which corresponds to 76.65% normalized VI score, while V-measure and V-beta were 12.7% and 9% respectively.", "labels": [], "entities": [{"text": "VI", "start_pos": 80, "end_pos": 82, "type": "METRIC", "confidence": 0.8640950322151184}, {"text": "normalized VI score", "start_pos": 120, "end_pos": 139, "type": "METRIC", "confidence": 0.7817314664522806}]}, {"text": "In 8,000 iterations after burnin, VI was 3.94 (80.3% when normalized), while V-measure and V-beta were 53.3%, since the number of states was almost the same as the number of unique PoS tags in the gold standard.", "labels": [], "entities": [{"text": "VI", "start_pos": 34, "end_pos": 36, "type": "METRIC", "confidence": 0.9985257983207703}]}, {"text": "The closest experiment to ours is the one by who run their Bayesian HMM over the whole WSJ and evaluated against the full gold standard, the only difference being is that we exclude the CoNLL shared task sec-    tions from our evaluation, which leaves us with 19 sections instead of 24.", "labels": [], "entities": [{"text": "WSJ", "start_pos": 87, "end_pos": 90, "type": "DATASET", "confidence": 0.9366831183433533}]}, {"text": "Their best VI score was 4.03886 which they achieved using the collapsed, sentence-blocked Gibbs sampler with the number of states fixed to 50.", "labels": [], "entities": [{"text": "VI score", "start_pos": 11, "end_pos": 19, "type": "METRIC", "confidence": 0.9757912755012512}]}, {"text": "The VI score achieved by the iHMM with fixed parameters using the PY prior reaches 3.73, while using the DP prior VI reaches 4.32 with learnt parameters and 3.93 with fixed parameters.", "labels": [], "entities": [{"text": "VI score", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.9810589551925659}, {"text": "iHMM", "start_pos": 29, "end_pos": 33, "type": "DATASET", "confidence": 0.9467670321464539}, {"text": "PY prior", "start_pos": 66, "end_pos": 74, "type": "DATASET", "confidence": 0.7777202129364014}, {"text": "DP prior VI", "start_pos": 105, "end_pos": 116, "type": "DATASET", "confidence": 0.7911787033081055}]}, {"text": "These results, even if they are not directly comparable, are on par with the state-ofthe-art, which encouraged us to proceed with the extrinsic evaluation.", "labels": [], "entities": []}, {"text": "For the experiments with shallow parsing we used the CRF++ toolkit 3 which has an efficient implementation of the model introduced by for this task.", "labels": [], "entities": [{"text": "CRF++ toolkit", "start_pos": 53, "end_pos": 66, "type": "DATASET", "confidence": 0.9057102799415588}]}, {"text": "First we ran an experiment using the words and the PoS tags provided in the shared task data and the performances obtained were 96.07% accuracy and 93.81% F-measure.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 135, "end_pos": 143, "type": "METRIC", "confidence": 0.9986542463302612}, {"text": "F-measure", "start_pos": 155, "end_pos": 164, "type": "METRIC", "confidence": 0.9983664155006409}]}, {"text": "The PoS tags were produced using the Brill tagger which employs tranformationbased learning and was trained using the WSJ corpus.", "labels": [], "entities": [{"text": "WSJ corpus", "start_pos": 118, "end_pos": 128, "type": "DATASET", "confidence": 0.9784336388111115}]}, {"text": "Then we ran an experiment removing the PoS tags altogether, and the performances were 93.25% accuracy and 88.58% F-measure respectively.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 93, "end_pos": 101, "type": "METRIC", "confidence": 0.9996638298034668}, {"text": "F-measure", "start_pos": 113, "end_pos": 122, "type": "METRIC", "confidence": 0.9987529516220093}]}, {"text": "This gave us some indication as to what the contribution of the PoS tags is in the context of the shallow parsing task at hand.", "labels": [], "entities": [{"text": "PoS tags", "start_pos": 64, "end_pos": 72, "type": "DATASET", "confidence": 0.762629896402359}, {"text": "parsing task", "start_pos": 106, "end_pos": 118, "type": "TASK", "confidence": 0.7514035403728485}]}, {"text": "The experiments using the output of the iHMM as PoS tags for shallow parsing are presented in.", "labels": [], "entities": []}, {"text": "The best performance achieved was 94.48% and 90.98% inaccuracy and F-measure, which is 1.23% and 2.4% better respectively than just using words, but worse by 1.57% and 2.83% compared to using the supervised PoS tagger output.", "labels": [], "entities": [{"text": "inaccuracy", "start_pos": 52, "end_pos": 62, "type": "METRIC", "confidence": 0.9675275683403015}, {"text": "F-measure", "start_pos": 67, "end_pos": 76, "type": "METRIC", "confidence": 0.9933023452758789}, {"text": "PoS tagger", "start_pos": 207, "end_pos": 217, "type": "TASK", "confidence": 0.7455180287361145}]}, {"text": "Given that the latter is trained on WSJ we believe that this is a good result.", "labels": [], "entities": [{"text": "WSJ", "start_pos": 36, "end_pos": 39, "type": "DATASET", "confidence": 0.892293393611908}]}, {"text": "Interestingly, this was obtained by using the last sample from the iHMM run using the DP prior with learnt parameters which has worse overall clustering evaluation scores, especially in terms of VI.", "labels": [], "entities": [{"text": "iHMM run", "start_pos": 67, "end_pos": 75, "type": "DATASET", "confidence": 0.9596313238143921}, {"text": "VI", "start_pos": 195, "end_pos": 197, "type": "METRIC", "confidence": 0.9872666001319885}]}, {"text": "This sample though has the best homogeneity score (69.39%).", "labels": [], "entities": []}, {"text": "We believe that homogeneity is more important than the overall clustering score due to the fact that, in the application considered, it is probably worse to assign tokens that belong to different PoS tags to the same state, e.g. verb and adverbs, rather than generate more than one state for the same PoS.", "labels": [], "entities": []}, {"text": "This is likely to be the casein tasks where we are interested in distinguishing between PoS tags rather than the actual tag itself.", "labels": [], "entities": []}, {"text": "Also, clustering evaluation measures tend to score leniently consistent mixing of members of different classes in the same cluster.", "labels": [], "entities": []}, {"text": "However, such mixing results in consistent noise when the clustering output becomes input to a machine learning method, which is harder to deal with.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Performance of the three iHMM runs according to clustering evaluation measures against num- ber of iteretions (in thousands).", "labels": [], "entities": []}, {"text": " Table 2: Performance of the output of the three iHMM runs when used in shallow parsing against number  of iteretions (in thousands).", "labels": [], "entities": []}]}