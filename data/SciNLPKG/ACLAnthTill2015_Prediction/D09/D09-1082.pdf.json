{"title": [{"text": "Recognizing Textual Relatedness with Predicate-Argument Structures", "labels": [], "entities": [{"text": "Recognizing Textual Relatedness", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.896248459815979}]}], "abstractContent": [{"text": "In this paper, we first compare several strategies to handle the newly proposed three-way Recognizing Textual Entailment (RTE) task.", "labels": [], "entities": [{"text": "Recognizing Textual Entailment (RTE) task", "start_pos": 90, "end_pos": 131, "type": "TASK", "confidence": 0.7880923407418388}]}, {"text": "Then we define anew measurement fora pair of texts, called Textual Relatedness, which is a weaker concept than semantic similarity or paraphrase.", "labels": [], "entities": [{"text": "Textual Relatedness", "start_pos": 59, "end_pos": 78, "type": "TASK", "confidence": 0.6205597817897797}]}, {"text": "We show that an alignment model based on the predicate-argument structures using this measurement can help an RTE system to recognize the Unknown cases at the first stage, and contribute to the improvement of the overall performance in the RTE task.", "labels": [], "entities": [{"text": "RTE task", "start_pos": 240, "end_pos": 248, "type": "TASK", "confidence": 0.9143820405006409}]}, {"text": "In addition, several heterogeneous lexical resources are tested, and different contributions from them are observed.", "labels": [], "entities": []}], "introductionContent": [{"text": "Recognizing Textual Entailment (RTE) () is a task to detect whether one Hypothesis (H) can be inferred (or entailed) by a Text (T).", "labels": [], "entities": [{"text": "Recognizing Textual Entailment (RTE)", "start_pos": 0, "end_pos": 36, "type": "TASK", "confidence": 0.7907547205686569}]}, {"text": "Being a challenging task, it has been shown that it is helpful to applications like question answering ().", "labels": [], "entities": [{"text": "question answering", "start_pos": 84, "end_pos": 102, "type": "TASK", "confidence": 0.9180057048797607}]}, {"text": "The recent research on RTE extends the two-way annotation into three-way 1 2 , making it even more difficult, but more linguistic-motivated.", "labels": [], "entities": [{"text": "RTE", "start_pos": 23, "end_pos": 26, "type": "TASK", "confidence": 0.8664494156837463}]}, {"text": "The straightforward strategy is to treat it as a three-way classification task, but the performance suffers a significant drop even when using the same classifier and the same feature model.", "labels": [], "entities": []}, {"text": "In fact, it can also be dealt with as an extension to the traditional two-way classification, e.g., by identi-fying the Entailment (E) cases first and then further label the Contradiction (C) and Unknown (U) T-H pairs.", "labels": [], "entities": []}, {"text": "Some other researchers also work on detecting negative cases, i.e. contradiction, instead of entailment (.", "labels": [], "entities": []}, {"text": "However, according to our best knowledge, the detailed comparison between these strategies has not been fully explored, let alone the impact of the linguistic motivation behind the strategy selection.", "labels": [], "entities": []}, {"text": "This paper will address this issue.", "labels": [], "entities": []}, {"text": "Take the following example from the RTE-4 test set ( as an example, T: At least five people have been killed in a head-on train collision in north-eastern France, while others are still trapped in the wreckage.", "labels": [], "entities": [{"text": "RTE-4 test set", "start_pos": 36, "end_pos": 50, "type": "DATASET", "confidence": 0.9623109698295593}, {"text": "T", "start_pos": 68, "end_pos": 69, "type": "METRIC", "confidence": 0.986065149307251}]}, {"text": "All the victims are adults.", "labels": [], "entities": []}], "datasetContent": [{"text": "In order to evaluate our method, we setup several experiments.", "labels": [], "entities": []}, {"text": "The baseline system here is a simple Naive Bayes classifier with a feature set containing the Bag-of-Words (BoW) overlapping ratio between T and H, and also the syntactic dependency overlapping ratio.", "labels": [], "entities": [{"text": "Bag-of-Words (BoW) overlapping ratio", "start_pos": 94, "end_pos": 130, "type": "METRIC", "confidence": 0.9366873204708099}]}, {"text": "The feature model combines two baseline systems proposed by previous work, which gives out quite competitive performance.", "labels": [], "entities": []}, {"text": "Since the main goal of this paper is to show the impact of the PAS-based alignment module, we will not compare our results with other RTE systems (In fact, the baseline system already outperforms the average accuracy score of the RTE-4 challenge).", "labels": [], "entities": [{"text": "PAS-based alignment", "start_pos": 63, "end_pos": 82, "type": "TASK", "confidence": 0.7340750396251678}, {"text": "accuracy", "start_pos": 208, "end_pos": 216, "type": "METRIC", "confidence": 0.9971479773521423}]}, {"text": "The main data set used for testing here is the RTE-4 data set with three-way annotations (500 entailment T-H pairs (E), 150 contradiction pairs (C), and 350 unknown pairs (U)).", "labels": [], "entities": [{"text": "RTE-4 data set", "start_pos": 47, "end_pos": 61, "type": "DATASET", "confidence": 0.9610646963119507}]}, {"text": "The results on RTE-3 data set (combination of the development set and test set, in all, 822 E pairs, 161 C pairs, and 617 U pairs) is also shown, although the original annotation is two-way and the three-way annotation was done by different researchers after the challenge . We will first show the performance of the baseline systems, followed by the results of our PASbased alignment module and its impact on the whole task.", "labels": [], "entities": [{"text": "RTE-3 data set", "start_pos": 15, "end_pos": 29, "type": "DATASET", "confidence": 0.970898965994517}, {"text": "PASbased alignment", "start_pos": 366, "end_pos": 384, "type": "TASK", "confidence": 0.5944259613752365}]}, {"text": "After that, we will also give more detailed analysis of our alignment module, according to different lexical relatedness measurements.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Results on the Whole Datasets", "labels": [], "entities": [{"text": "Whole Datasets", "start_pos": 25, "end_pos": 39, "type": "DATASET", "confidence": 0.7108191251754761}]}]}