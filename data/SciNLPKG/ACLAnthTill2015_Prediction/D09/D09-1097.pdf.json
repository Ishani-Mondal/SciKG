{"title": [{"text": "Hypernym Discovery Based on Distributional Similarity and Hierarchical Structures", "labels": [], "entities": [{"text": "Hypernym Discovery", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.8115599453449249}]}], "abstractContent": [{"text": "This paper presents anew method of developing a large-scale hyponymy relation database by combining Wikipedia and other Web documents.", "labels": [], "entities": []}, {"text": "We attach new words to the hy-ponymy database extracted from Wikipedia by using distributional similarity calculated from documents on the Web.", "labels": [], "entities": []}, {"text": "For a given target word, our algorithm first finds k similar words from the Wikipedia database.", "labels": [], "entities": [{"text": "Wikipedia database", "start_pos": 76, "end_pos": 94, "type": "DATASET", "confidence": 0.9480897784233093}]}, {"text": "Then, the hypernyms of these k similar words are assigned scores by considering the distribu-tional similarities and hierarchical distances in the Wikipedia database.", "labels": [], "entities": []}, {"text": "Finally, new hy-ponymy relations are output according to the scores.", "labels": [], "entities": []}, {"text": "In this paper, we tested two distribu-tional similarities.", "labels": [], "entities": []}, {"text": "One is based on raw verb-noun dependencies (which we call \"RVD\"), and the other is based on a large-scale clustering of verb-noun dependencies (called \"CVD\").", "labels": [], "entities": []}, {"text": "Our method achieved an attachment accuracy of 91.0% for the top 10,000 relations , and an attachment accuracy of 74.5% for the top 100,000 relations when using CVD.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 34, "end_pos": 42, "type": "METRIC", "confidence": 0.685196042060852}, {"text": "attachment accuracy", "start_pos": 90, "end_pos": 109, "type": "METRIC", "confidence": 0.7975779473781586}]}, {"text": "This was afar better outcome compared to the other baseline approaches.", "labels": [], "entities": []}, {"text": "Excluding the region that had very high scores, CVD was found to be more effective than RVD.", "labels": [], "entities": [{"text": "CVD", "start_pos": 48, "end_pos": 51, "type": "METRIC", "confidence": 0.6222235560417175}, {"text": "RVD", "start_pos": 88, "end_pos": 91, "type": "METRIC", "confidence": 0.564563512802124}]}, {"text": "We also confirmed that most relations extracted by our method cannot be extracted merely by applying the well-known lexico-syntactic patterns to Web documents.", "labels": [], "entities": []}], "introductionContent": [{"text": "Large-scale taxonomies such as WordNet play an important role in information extraction and question answering.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 31, "end_pos": 38, "type": "DATASET", "confidence": 0.9620558023452759}, {"text": "information extraction", "start_pos": 65, "end_pos": 87, "type": "TASK", "confidence": 0.8682920038700104}, {"text": "question answering", "start_pos": 92, "end_pos": 110, "type": "TASK", "confidence": 0.9225284457206726}]}, {"text": "However, extremely high costs are borne to manually enlarge and maintain such taxonomies.", "labels": [], "entities": []}, {"text": "Thus, applications using these taxonomies tend to face the drawback of data sparseness.", "labels": [], "entities": []}, {"text": "This paper presents anew method for discovering a large set of hyponymy relations.", "labels": [], "entities": []}, {"text": "Here, a word 1 X is regarded as a hypernym of a word Y if Y is a kind of X or Y is an instance of X.", "labels": [], "entities": []}, {"text": "We are able to generate large-scale hyponymy relations by attaching new words to the hyponymy database extracted from Wikipedia (referred to as \"Wikipedia relation database\") by using distributional similarity calculated from Web documents.", "labels": [], "entities": []}, {"text": "Relations extracted from Wikipedia are relatively clean.", "labels": [], "entities": [{"text": "Wikipedia", "start_pos": 25, "end_pos": 34, "type": "DATASET", "confidence": 0.9210638403892517}]}, {"text": "On the other hand, reliable distributional similarity can be calculated using a large number of documents on the Web.", "labels": [], "entities": []}, {"text": "In this paper, we combine the advantages of these two resources.", "labels": [], "entities": []}, {"text": "Using distributional similarity, our algorithm first computes k similar words fora target word.", "labels": [], "entities": []}, {"text": "Then, each k similar word assigns a score to its ancestors in the hierarchical structures of the Wikipedia relation database.", "labels": [], "entities": [{"text": "Wikipedia relation database", "start_pos": 97, "end_pos": 124, "type": "DATASET", "confidence": 0.805375337600708}]}, {"text": "The hypernym that has the highest score for the target word is selected as the hypernym of the target word. is an overview of the proposed approach.", "labels": [], "entities": []}, {"text": "In the experiment, we extracted hypernyms for approximately 670,000 target words that are not included in the Wikipedia relation database but are found on the Web.", "labels": [], "entities": [{"text": "Wikipedia relation database", "start_pos": 110, "end_pos": 137, "type": "DATASET", "confidence": 0.8366851011912028}]}, {"text": "We tested two distributional similarities: one based on raw verb-noun dependencies (RVD) and the other based on a large-scale clustering of verb-noun dependencies (CVD).", "labels": [], "entities": []}, {"text": "The experimental results showed that the proposed methods were more effective than the other baseline approaches.", "labels": [], "entities": []}, {"text": "In addition, we confirmed that most of the relations extracted by our method could not be extracted using the lexicosyntactic pattern-based method.", "labels": [], "entities": []}, {"text": "In the remainder of this paper, we first intro-duce some related works in Section 2.", "labels": [], "entities": []}, {"text": "Section 3 describes the Wikipedia relation database.", "labels": [], "entities": [{"text": "Wikipedia relation database", "start_pos": 24, "end_pos": 51, "type": "DATASET", "confidence": 0.9084800680478414}]}, {"text": "Section 4 describes the distributional similarity calculated by the two methods.", "labels": [], "entities": [{"text": "distributional similarity", "start_pos": 24, "end_pos": 49, "type": "METRIC", "confidence": 0.6621842086315155}]}, {"text": "In Section 5, we describe a method to discover an appropriate hypernym for each target word.", "labels": [], "entities": []}, {"text": "The experimental results are presented in Section 6 before concluding the paper in Section 7.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluated our proposed methods by using it in experiments to discover hypernyms from the Wikipedia relation database for the target words extracted from about 670,000 noun phrases.", "labels": [], "entities": [{"text": "Wikipedia relation database", "start_pos": 92, "end_pos": 119, "type": "DATASET", "confidence": 0.8542252580324808}]}, {"text": "In the proposed methods, there are several parameters.", "labels": [], "entities": []}, {"text": "We performed parameter optimization by randomly selecting 694 words as development data in our preliminary experiments.", "labels": [], "entities": [{"text": "parameter optimization", "start_pos": 13, "end_pos": 35, "type": "TASK", "confidence": 0.7433372437953949}]}, {"text": "The hypernyms of these words were determined manually.", "labels": [], "entities": []}, {"text": "We adjusted the parameters so that each method achieved the best performance for this development data.", "labels": [], "entities": []}, {"text": "The parameters in the scoring method with k similar words were adjusted as follows 3 : (RVD) Number of similar words: k = 100.", "labels": [], "entities": [{"text": "RVD) Number", "start_pos": 88, "end_pos": 99, "type": "METRIC", "confidence": 0.8778698841730753}]}, {"text": "Similarity threshold: S min = 0.05.", "labels": [], "entities": [{"text": "Similarity", "start_pos": 0, "end_pos": 10, "type": "METRIC", "confidence": 0.9823911786079407}, {"text": "S min", "start_pos": 22, "end_pos": 27, "type": "METRIC", "confidence": 0.9831947088241577}]}, {"text": "Penalty value for ancestors: d = 0.6.", "labels": [], "entities": [{"text": "Penalty value", "start_pos": 0, "end_pos": 13, "type": "METRIC", "confidence": 0.9661242663860321}, {"text": "ancestors", "start_pos": 18, "end_pos": 27, "type": "METRIC", "confidence": 0.8826942443847656}]}, {"text": "We tested the parameter values k = {100, 200, 300, 400, 500, 600, 700, 800, 900, 1000}, S min ={0, 0.05, 0.", "labels": [], "entities": []}, {"text": "The parameter in baseline approach 3 was adjusted as follows: Threshold for the number of children: 20.", "labels": [], "entities": [{"text": "Threshold", "start_pos": 62, "end_pos": 71, "type": "METRIC", "confidence": 0.9994646906852722}]}, {"text": "Using the adjusted parameters, we conducted experiments to extract the hypernym of each target word with the help of the scoring method based on k similar words.", "labels": [], "entities": []}, {"text": "In these experiments, two kinds of distributional similarity mentioned in Section 4 were exploited individually.", "labels": [], "entities": []}, {"text": "The words that were used in the development data were excluded.", "labels": [], "entities": []}, {"text": "We also conducted a comparative experiment in which the parameter value for the penalty of the hierarchal difference, d, was set to 0 to clarify the ability of using hierarchal structures in the k similar words method.", "labels": [], "entities": []}, {"text": "This means each k similar word votes only to their parent.", "labels": [], "entities": []}, {"text": "We then judged the quality of each acquired hypernym.", "labels": [], "entities": []}, {"text": "The evaluation data sets were sampled from the top 1,000, 10,000, 100,000, and 670,000 results that were ranked according to the score of each method.", "labels": [], "entities": []}, {"text": "Then, against 200 samples that were randomly sampled from each set, one of the authors judged whether the hypernym extracted by each method for the target word was corrector not.", "labels": [], "entities": []}, {"text": "In this evaluation, if the sentence \"The target word is a kind of the hypernym\" or \"The target word is an instance of the hypernym\" was consistent, the extracted hyponymy was judged as correct.", "labels": [], "entities": []}, {"text": "It should be noted that the outputs of the compared methods are combined and shuffled to enable fair comparison.", "labels": [], "entities": []}, {"text": "In addition, baseline approach 1 extracted several hypernyms for the target word.", "labels": [], "entities": []}, {"text": "In this case, we judged the hypernym as correct when the case where one of the hypernyms was correct.", "labels": [], "entities": []}, {"text": "The precision of each result is shown in.", "labels": [], "entities": [{"text": "precision", "start_pos": 4, "end_pos": 13, "type": "METRIC", "confidence": 0.999643087387085}]}, {"text": "The results of the k similar words method are far better than those of the other baseline methods.", "labels": [], "entities": []}, {"text": "In particular, the k similar words method with CVD outperformed the methods of the k similar words where the parameter value d was set to 0 and the method using RVD except for the top 1,000 results.", "labels": [], "entities": []}, {"text": "This means that the use of hierarchal structures and the clustering process for calculating distributional similarity are effective for this task.", "labels": [], "entities": []}, {"text": "We confirmed the significant differences of the proposed method (CVD) as compared with all the baseline approaches at the 1% significant level by the Fisher's exact test.", "labels": [], "entities": []}, {"text": "The precision of baseline approach 2 that selected the most similar hypernym was the worst among all the methods.", "labels": [], "entities": [{"text": "precision", "start_pos": 4, "end_pos": 13, "type": "METRIC", "confidence": 0.9990955591201782}]}, {"text": "There were words that were similar to the target word among the hypernyms extracted incorrectly.", "labels": [], "entities": []}, {"text": "For example, the word semento-kojo (cement factory) was extracted for the hypernym of the word kuriningukojo (dry cleaning plant).", "labels": [], "entities": [{"text": "semento-kojo (cement factory)", "start_pos": 22, "end_pos": 51, "type": "TASK", "confidence": 0.7988747596740723}]}, {"text": "It is difficult to judge whether the word is a hypernym or just a similar word by using only the similarity measure.", "labels": [], "entities": []}, {"text": "As for the results of baseline approach 1 using the most similar hyponym and baseline approach 3 using the similarity of the set of hypernym's children, the noise on the Wikipedia relation database decreased the precision.", "labels": [], "entities": [{"text": "Wikipedia relation database", "start_pos": 170, "end_pos": 197, "type": "DATASET", "confidence": 0.8073385953903198}, {"text": "precision", "start_pos": 212, "end_pos": 221, "type": "METRIC", "confidence": 0.9985408782958984}]}, {"text": "Moreover, overspecified hypernyms were extracted incorrectly by these methods.", "labels": [], "entities": []}, {"text": "In contrast, the method of scoring based on the use of k similar words was robust against noise because it uses the voting approach for the similarities.", "labels": [], "entities": []}, {"text": "Further, this method can extract hypernyms that are not overspecific because it uses all descendants for scoring.", "labels": [], "entities": []}, {"text": "shows some examples of relations extracted by the k similar words method using CVD.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Precision of each approach based on the score ranking. CVD represents the method that uses the dis- tributional similarity based on large-scale of clustering of verb-noun dependencies. RVD represents the  one based on raw verb-noun dependencies.", "labels": [], "entities": [{"text": "Precision", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9894911050796509}, {"text": "RVD", "start_pos": 195, "end_pos": 198, "type": "METRIC", "confidence": 0.7541965842247009}]}]}