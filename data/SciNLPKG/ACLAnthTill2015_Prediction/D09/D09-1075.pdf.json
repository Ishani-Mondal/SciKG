{"title": [], "abstractContent": [{"text": "Training a statistical machine translation starts with tokenizing a parallel corpus.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 11, "end_pos": 42, "type": "TASK", "confidence": 0.6358963251113892}]}, {"text": "Some languages such as Chinese do not incorporate spacing in their writing system, which creates a challenge for tokenization.", "labels": [], "entities": []}, {"text": "Moreover, morphologically rich languages such as Korean present an even bigger challenge, since optimal token boundaries for machine translation in these languages are often unclear.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 125, "end_pos": 144, "type": "TASK", "confidence": 0.7265966832637787}]}, {"text": "Both rule-based solutions and statistical solutions are currently used.", "labels": [], "entities": []}, {"text": "In this paper, we present unsuper-vised methods to solve tokenization problem.", "labels": [], "entities": [{"text": "tokenization", "start_pos": 57, "end_pos": 69, "type": "TASK", "confidence": 0.9778814315795898}]}, {"text": "Our methods incorporate information available from parallel corpus to determine a good tokenization for machine translation.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 104, "end_pos": 123, "type": "TASK", "confidence": 0.7635473310947418}]}], "introductionContent": [{"text": "Tokenizing a parallel corpus is usually the first step of training a statistical machine translation system.", "labels": [], "entities": [{"text": "Tokenizing a parallel corpus", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.8564601987600327}, {"text": "statistical machine translation", "start_pos": 69, "end_pos": 100, "type": "TASK", "confidence": 0.6234821677207947}]}, {"text": "With languages such as Chinese, which has no spaces in its writing system, the main challenge is to segment sentences into appropriate tokens.", "labels": [], "entities": []}, {"text": "With languages such as Korean and Hungarian, although the writing systems of both languages incorporate spaces between \"words\", the granularity is too coarse compared with languages such as English.", "labels": [], "entities": []}, {"text": "A single word in these languages is composed of several morphemes, which often correspond to separate words in English.", "labels": [], "entities": []}, {"text": "These languages also form compound nouns more freely.", "labels": [], "entities": []}, {"text": "Ideally, we want to find segmentations for source and target languages that create a one-toone mapping of words.", "labels": [], "entities": []}, {"text": "However, this is not always straightforward for two major reasons.", "labels": [], "entities": []}, {"text": "First, what the optimal tokenization for machine translation should be is not always clear. and show that getting the tokenization of one of the languages in the corpus close to a gold standard does not necessarily help with building better machine translation systems.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 41, "end_pos": 60, "type": "TASK", "confidence": 0.8211741149425507}, {"text": "machine translation", "start_pos": 241, "end_pos": 260, "type": "TASK", "confidence": 0.7350477576255798}]}, {"text": "Second, even statistical methods require hand-annotated training data, which means that in resource-poor languages, good tokenization is hard to achieve.", "labels": [], "entities": []}, {"text": "In this paper, we explore unsupervised methods for tokenization, with the goal of automatically finding an appropriate tokenization for machine translation.", "labels": [], "entities": [{"text": "tokenization", "start_pos": 51, "end_pos": 63, "type": "TASK", "confidence": 0.9681182503700256}, {"text": "machine translation", "start_pos": 136, "end_pos": 155, "type": "TASK", "confidence": 0.7490378022193909}]}, {"text": "We compare methods that have access to parallel corpora to methods that are trained solely using data from the source language.", "labels": [], "entities": []}, {"text": "Unsupervised monolingual segmentation has been studied as a model of language acquisition), and as model of learning morphology in European languages.", "labels": [], "entities": []}, {"text": "Unsupervised segmentation using bilingual data has been attempted for finding new translation pairs (, and for finding good segmentation for Chinese in machine translation using Gibbs sampling (.", "labels": [], "entities": []}, {"text": "In this paper, further investigate the use of bilingual information to find tokenizations tailored for machine translation.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 103, "end_pos": 122, "type": "TASK", "confidence": 0.747379720211029}]}, {"text": "We find a benefit not only for segmentation of languages with no space in the writing system (such as Chinese), but also for the smaller-scale tokenization problem of normalizing between languages that include more or less information in a \"word\" as defined by the writing system, using Korean-English for our experiments.", "labels": [], "entities": []}, {"text": "Here too, we find a benefit from using bilingual information, with unsupervised segmentation rivaling and in some cases surpassing supervised segmentation.", "labels": [], "entities": []}, {"text": "On the modeling side, we use dynamic programming-based variational Bayes, making Gibbs sampling unnecessary.", "labels": [], "entities": []}, {"text": "We also develop and compare various factors in the model to control the length of the tokens learned, and find a benefit from adjusting these parameters directly to optimize the end-to-end translation quality.", "labels": [], "entities": []}], "datasetContent": [{"text": "We used Moses () to train machine translation systems.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 26, "end_pos": 45, "type": "TASK", "confidence": 0.7578317523002625}]}, {"text": "Default parameters were used for all experiments except for the number of iterations for GIZA++ (.", "labels": [], "entities": []}, {"text": "GIZA++ was run until the perplexity on development set stopped decreasing.", "labels": [], "entities": [{"text": "GIZA++", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.9131766259670258}]}, {"text": "For practical reasons, the maximum size of a token was set at three for Chinese, and four for Korean.", "labels": [], "entities": []}, {"text": "Minimum error rate training was run on each system afterwards and BLEU score () was calculated on the test sets.", "labels": [], "entities": [{"text": "Minimum error rate", "start_pos": 0, "end_pos": 18, "type": "METRIC", "confidence": 0.7887225151062012}, {"text": "BLEU score", "start_pos": 66, "end_pos": 76, "type": "METRIC", "confidence": 0.9850145876407623}]}, {"text": "For the monolingual model, we tested two versions with the length factor \u03c6 1 , and \u03c6 2 . We picked \u03bb and P (s) so that the number of tokens on source side (Chinese, and Korean) will be about the same as the number of tokens in the target side.", "labels": [], "entities": []}, {"text": "For the bilingual model, as explained in the model section, we are learning P (f | e), but only P (f ) is available for tokenizing any new data.", "labels": [], "entities": []}, {"text": "We compared two conditions: using only the source data to tokenize the source language training data according to P (f ) (which is consistent with the conditions attest time), and using both the source and English data to tokenize the source language training data (which might produce better tokenization by using more information).", "labels": [], "entities": []}, {"text": "For the first length factor \u03c6 1 , we ran an experiment where the model learns P (s) as described in the model section, and we also had experiments where P (s) was pre-set at 0.9, 0.7, 0.5, and 0.3 for comparison.", "labels": [], "entities": []}, {"text": "We also ran an experiment with the second length factor \u03c6 2 where \u03bb was picked as the same manner as the monolingual model.", "labels": [], "entities": []}, {"text": "We varied tokenization of development set and test set to match the training data for each experiment.", "labels": [], "entities": []}, {"text": "However, as we have implied in the previous paragraph, in the one experiment where P (f | e) was used to segment training data, directly incorporating information from target corpus, tokenization for test and development set is not exactly consistent with tokenization of training corpus.", "labels": [], "entities": []}, {"text": "Since we assume only source corpus is available at the test time, the test and the development set was tokenized only using information from P (f ).", "labels": [], "entities": []}, {"text": "We also trained MT systems using supervised tokenizations and tokenization requiring a minimal effort for the each language pair.", "labels": [], "entities": [{"text": "MT", "start_pos": 16, "end_pos": 18, "type": "TASK", "confidence": 0.97552490234375}]}, {"text": "For ChineseEnglish, the minimal effort tokenization is maximal tokenization where every Chinese character is segmented.", "labels": [], "entities": []}, {"text": "Since a number of Chinese tokenizers are available, we have tried four different tokenizations for the supervised tokenizations.", "labels": [], "entities": []}, {"text": "The first one is the LDC Chinese tokenizer available at the LDC website 3 , which is compiled by Zhibiao Wu.", "labels": [], "entities": [{"text": "LDC Chinese tokenizer", "start_pos": 21, "end_pos": 42, "type": "DATASET", "confidence": 0.7160933017730713}, {"text": "LDC website 3", "start_pos": 60, "end_pos": 73, "type": "DATASET", "confidence": 0.9206626812616984}]}, {"text": "The second tokenizer is a maxent-based tokenizer described by.", "labels": [], "entities": []}, {"text": "The third and fourth tokenizations come from the CRF-based Stanford Chinese segmenter described by.", "labels": [], "entities": [{"text": "CRF-based Stanford Chinese segmenter", "start_pos": 49, "end_pos": 85, "type": "DATASET", "confidence": 0.9209245890378952}]}, {"text": "The difference between third and fourth tokenization comes from the different gold standard, the third one is based on Beijing University's segmentation (pku) and the fourth one is based on Chinese Treebank (ctb 20.32 0.75 Bilingual P (f | e) with \u03c6 1 P (s) = learned 19. 6.93 Bilingual P (f ) with \u03c6 1 P (s) = learned 20.04 0.80 7.06 Bilingual P (f ) with \u03c6 1 P (s) = 0.9 20.75 0.87 7.46 Bilingual P (f ) with \u03c6 1 P (s) = 0.7 20.59 0.81 7.31 Bilingual P (f ) with \u03c6 1 P (s) = 0.5 19.68 0.80 7.18 Bilingual P (f ) with \u03c6 1 P (s) = 0.3 20.02 0.79 7.38 Bilingual P (f ) with \u03c6 2 22.31 0.88 7.35 Monolingual P (f ) with \u03c6 1 20.93 0.83 6.76 Monolingual P (f ) with \u03c6 2 20.72 0.85 7.02: BLEU score results for Chinese-English and Korean-English experiments and F-score of segmentation compared against Chinese Treebank standard.", "labels": [], "entities": [{"text": "Chinese Treebank", "start_pos": 190, "end_pos": 206, "type": "DATASET", "confidence": 0.9512656033039093}, {"text": "BLEU", "start_pos": 682, "end_pos": 686, "type": "METRIC", "confidence": 0.995250940322876}, {"text": "F-score", "start_pos": 756, "end_pos": 763, "type": "METRIC", "confidence": 0.9978220462799072}, {"text": "Chinese Treebank standard", "start_pos": 797, "end_pos": 822, "type": "DATASET", "confidence": 0.9079001347223917}]}, {"text": "The highest unsupervised score is highlighted.", "labels": [], "entities": []}, {"text": "English, the minimal effort tokenization splitting off punctuation and otherwise respecting the spacing in the Korean writing system.", "labels": [], "entities": []}, {"text": "A Korean morphological analysis tool 4 was used to create the supervised tokenization.", "labels": [], "entities": []}, {"text": "For Chinese-English, since a gold standard for Chinese segmentation is available, we ran an additional evaluation of tokenization from each methods we have tested.", "labels": [], "entities": [{"text": "Chinese segmentation", "start_pos": 47, "end_pos": 67, "type": "TASK", "confidence": 0.674508661031723}]}, {"text": "We tokenized the raw text of Chinese Treebank () using all of the methods (supervised/unsupervised) we have described in this section except for the bilingual tokenization using P (f | e) because the English translation of the Chinese Treebank data was not available.", "labels": [], "entities": [{"text": "Chinese Treebank", "start_pos": 29, "end_pos": 45, "type": "DATASET", "confidence": 0.875254213809967}, {"text": "Chinese Treebank data", "start_pos": 227, "end_pos": 248, "type": "DATASET", "confidence": 0.9356812040011088}]}, {"text": "We compared the result against the gold standard segmentation and calculated the F-score.", "labels": [], "entities": [{"text": "gold standard segmentation", "start_pos": 35, "end_pos": 61, "type": "DATASET", "confidence": 0.7454014420509338}, {"text": "F-score", "start_pos": 81, "end_pos": 88, "type": "METRIC", "confidence": 0.9977547526359558}]}], "tableCaptions": []}