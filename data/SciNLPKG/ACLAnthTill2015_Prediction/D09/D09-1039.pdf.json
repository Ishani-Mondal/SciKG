{"title": [{"text": "Accuracy-Based Scoring for DOT: Towards Direct Error Minimization for Data-Oriented Translation", "labels": [], "entities": [{"text": "Accuracy-Based", "start_pos": 0, "end_pos": 14, "type": "METRIC", "confidence": 0.9127164483070374}, {"text": "Data-Oriented Translation", "start_pos": 70, "end_pos": 95, "type": "TASK", "confidence": 0.7086038887500763}]}], "abstractContent": [{"text": "In this work we present a novel technique to rescore fragments in the Data-Oriented Translation model based on their contribution to translation accuracy.", "labels": [], "entities": [{"text": "Data-Oriented Translation", "start_pos": 70, "end_pos": 95, "type": "TASK", "confidence": 0.6711922287940979}, {"text": "accuracy", "start_pos": 145, "end_pos": 153, "type": "METRIC", "confidence": 0.6868135333061218}]}, {"text": "We describe three new rescoring methods, and present the initial results of a pilot experiment on a small subset of the Europarl corpus.", "labels": [], "entities": [{"text": "Europarl corpus", "start_pos": 120, "end_pos": 135, "type": "DATASET", "confidence": 0.9909341931343079}]}, {"text": "This work is a proof-of-concept, and is the first step in directly optimizing translation decisions solely on the hypothesized accuracy of potential translations resulting from those decisions.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 127, "end_pos": 135, "type": "METRIC", "confidence": 0.9804835915565491}]}], "introductionContent": [{"text": "The Data-Oriented Translation (DOT)) model is a tree-structured translation model, in which linked subtree fragments extracted from a parsed bitext are composed to cover a sourcelanguage sentence to be translated.", "labels": [], "entities": [{"text": "Data-Oriented Translation (DOT))", "start_pos": 4, "end_pos": 36, "type": "TASK", "confidence": 0.8504193067550659}]}, {"text": "Each linked fragment pair consists of a source-language side and a target-language side, similar to (.", "labels": [], "entities": []}, {"text": "Translating anew sentence involves composing the linked fragments into derivations so that anew source-language sentence is covered by the source tree fragments of the linked pairs, where the yields of the target-side derivations are the candidate translations.", "labels": [], "entities": []}, {"text": "Derivations are scored according to their likelihood, and the translation is selected from the derivation pair with the highest score.", "labels": [], "entities": []}, {"text": "However, we have no reason to believe that maximizing likelihood is the best way to maximize translation accuracy -likelihood and accuracy do not necessarily correlate well.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 105, "end_pos": 113, "type": "METRIC", "confidence": 0.8477945327758789}, {"text": "accuracy", "start_pos": 130, "end_pos": 138, "type": "METRIC", "confidence": 0.9965171813964844}]}, {"text": "We can frame the problem as a search problem, where we are searching a space of derivations for the one that yields the highest scoring translation.", "labels": [], "entities": []}, {"text": "By putting weights on the derivations in the search space, we wish to point the decoder in the direction of the optimal translation.", "labels": [], "entities": []}, {"text": "Since we want the decoder to find the translation with the highest evaluation score, we would want to score the derivations with weights that correlate well with the particular evaluation measure in mind.", "labels": [], "entities": []}, {"text": "Much of the work in the MT literature has focused on the scoring of translation decisions made.) follow) in using the noisy channel model, by decomposing the translation decisions modeled by the translation model into different types, and inducing probability distributions via maximum likelihood estimation over each decision type.", "labels": [], "entities": [{"text": "MT", "start_pos": 24, "end_pos": 26, "type": "TASK", "confidence": 0.9894474744796753}]}, {"text": "This model is then decoded as described in).", "labels": [], "entities": []}, {"text": "This type of approach is also followed in ().", "labels": [], "entities": []}, {"text": "There has been some previous work on accuracy-driven training techniques for SMT, such as MERT and the Simplex Armijo Downhill method (, which tune the parameters in a linear combination of various phrase scores according to a held-out tuning set.", "labels": [], "entities": [{"text": "accuracy-driven", "start_pos": 37, "end_pos": 52, "type": "METRIC", "confidence": 0.9698331952095032}, {"text": "SMT", "start_pos": 77, "end_pos": 80, "type": "TASK", "confidence": 0.9959498643875122}, {"text": "MERT", "start_pos": 90, "end_pos": 94, "type": "METRIC", "confidence": 0.8242841362953186}]}, {"text": "While this does tune the relative weights of the scores to maximize the accuracy of candidates in the tuning set, the scores themselves in the linear combination are not necessarily correlated with the accuracy of the translation.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 72, "end_pos": 80, "type": "METRIC", "confidence": 0.9956865310668945}, {"text": "accuracy", "start_pos": 202, "end_pos": 210, "type": "METRIC", "confidence": 0.9973560571670532}]}, {"text": "present a procedure to directly optimize the global scoring function used by a phrasebased decoder on the accuracy of the translations.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 106, "end_pos": 114, "type": "METRIC", "confidence": 0.9986512064933777}]}, {"text": "Similarly to MERT, Tillmann and Zhang estimate the parameters of a weight vector on a linear combination of (binary) features using a global objective function correlated with BLEU (.", "labels": [], "entities": [{"text": "MERT", "start_pos": 13, "end_pos": 17, "type": "TASK", "confidence": 0.6909008026123047}, {"text": "BLEU", "start_pos": 176, "end_pos": 180, "type": "METRIC", "confidence": 0.9984925985336304}]}, {"text": "In this work, we prototype some methods for moving directly towards incorporating a measure of the translation quality of each fragment used, bringing DOT more into the mainstream of current SMT research.", "labels": [], "entities": [{"text": "SMT", "start_pos": 191, "end_pos": 194, "type": "TASK", "confidence": 0.9953666925430298}]}, {"text": "In Section 2 we describe probability-based DOT fragment scoring.", "labels": [], "entities": [{"text": "DOT fragment scoring", "start_pos": 43, "end_pos": 63, "type": "TASK", "confidence": 0.8028734723726908}]}, {"text": "In Section 3 we describe our rescoring setup and the three rescoring methods.", "labels": [], "entities": []}, {"text": "In Section 4, we describe our experiments.", "labels": [], "entities": []}, {"text": "In Section 5 we compare the results of rescoring the fragments with the three methods.", "labels": [], "entities": []}, {"text": "In Section 6 we discuss some of the decisions that are affected by our rescoring methods.", "labels": [], "entities": []}, {"text": "Finally, we discuss the next steps in training the DOT system by optimizing over a translation accuracy-based objective function in Section 7.", "labels": [], "entities": [{"text": "accuracy-based", "start_pos": 95, "end_pos": 109, "type": "METRIC", "confidence": 0.7606928944587708}]}], "datasetContent": [{"text": "For our pilot experiments, we tested all the rescoring methods in the previous section on Spanish-toEnglish translation against the relative-frequency baseline.", "labels": [], "entities": [{"text": "Spanish-toEnglish translation", "start_pos": 90, "end_pos": 119, "type": "TASK", "confidence": 0.6043616980314255}]}, {"text": "We randomly selected 10,000 sentences from the Europarl corpus (, and parsed and aligned the bitext as described in.", "labels": [], "entities": [{"text": "Europarl corpus", "start_pos": 47, "end_pos": 62, "type": "DATASET", "confidence": 0.9930325150489807}]}, {"text": "From the parallel treebank, we extracted a Goodman reduction DOT grammar, as described in), although on an order of magnitude greater amount of training data.", "labels": [], "entities": []}, {"text": "Unlike, we did not use the unsupervised version of DOT, and did not attempt to scale up our amount of training data to his levels, although in ongoing work we are optimizing our system to be able to handle that amount of training data.", "labels": [], "entities": [{"text": "DOT", "start_pos": 51, "end_pos": 54, "type": "DATASET", "confidence": 0.8977941870689392}]}, {"text": "To perform the rescoring, we randomly chose an additional 30K sentence pairs from the Spanish-toEnglish bitext.", "labels": [], "entities": [{"text": "rescoring", "start_pos": 15, "end_pos": 24, "type": "TASK", "confidence": 0.965092122554779}]}, {"text": "We rescored the grammar by translating the source side of the 10K training sentence pairs and 10K of the additional sentences, and using the methods in Section 3 to score the fragments derived in the translation process.", "labels": [], "entities": []}, {"text": "We then performed the same experiment translating the full 40K-sentence set.", "labels": [], "entities": []}, {"text": "Rules in the grammar that were not used during tuning were rescored using a default score defined to be the median of all scores observed.", "labels": [], "entities": []}, {"text": "Our system performs translation by first obtaining the n-best parses for the source sentences and: Results on test set.", "labels": [], "entities": [{"text": "translation", "start_pos": 20, "end_pos": 31, "type": "TASK", "confidence": 0.9829597473144531}]}, {"text": "Underlined are statistically significantly better than the baseline at p = 0.01.", "labels": [], "entities": []}, {"text": "then computing the k-best bilingual derivations for each source parse.", "labels": [], "entities": []}, {"text": "In our experiments we used beams of n = 10, 000 and k = 5.", "labels": [], "entities": []}, {"text": "We also experimented with different values of \u03b1 0 and \u03b1 1 in Equation.", "labels": [], "entities": [{"text": "Equation", "start_pos": 61, "end_pos": 69, "type": "METRIC", "confidence": 0.8997371196746826}]}, {"text": "We set these parameters manually, although in future work we will automatically tune them, perhaps using a MERT-like algorithm.", "labels": [], "entities": []}, {"text": "We tested our rescored grammars on a set of 2,000 randomly chosen Europarl sentences, and used a set of 200 randomly chosen sentences as a development test set.", "labels": [], "entities": [{"text": "Europarl", "start_pos": 66, "end_pos": 74, "type": "DATASET", "confidence": 0.9719794988632202}]}], "tableCaptions": [{"text": " Table 2: Results on test set. Rescoring on 20K sentences.  SFR stands for Structured Fragment Rescoring, NSFR for  Normalized SFR and FSR for Fragment Surface Rescoring.  system-i-j represents the corresponding system with \u03b10 = i  and \u03b11 = j. Underlined results are statistically significantly  better than the baseline at p = 0.01.", "labels": [], "entities": [{"text": "FSR", "start_pos": 135, "end_pos": 138, "type": "METRIC", "confidence": 0.9962562322616577}, {"text": "Fragment Surface Rescoring", "start_pos": 143, "end_pos": 169, "type": "TASK", "confidence": 0.6503054400285085}]}, {"text": " Table 3: Results on test set. Rescoring on 40K sentences. Un- derlined are statistically significantly better than the baseline  at p = 0.01.", "labels": [], "entities": [{"text": "Rescoring", "start_pos": 31, "end_pos": 40, "type": "METRIC", "confidence": 0.918957531452179}]}, {"text": " Table 4: Results on development test set. Rescoring on 40K  sentences.", "labels": [], "entities": [{"text": "Rescoring", "start_pos": 43, "end_pos": 52, "type": "METRIC", "confidence": 0.967088520526886}]}]}