{"title": [{"text": "Accurate Semantic Class Classifier for Coreference Resolution", "labels": [], "entities": [{"text": "Accurate", "start_pos": 0, "end_pos": 8, "type": "METRIC", "confidence": 0.9516586065292358}, {"text": "Coreference Resolution", "start_pos": 39, "end_pos": 61, "type": "TASK", "confidence": 0.954105943441391}]}], "abstractContent": [{"text": "There have been considerable attempts to incorporate semantic knowledge into coreference resolution systems: different knowledge sources such as WordNet and Wikipedia have been used to boost the performance.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 77, "end_pos": 99, "type": "TASK", "confidence": 0.9433826506137848}, {"text": "WordNet", "start_pos": 145, "end_pos": 152, "type": "DATASET", "confidence": 0.9452804923057556}]}, {"text": "In this paper, we propose new ways to extract WordNet feature.", "labels": [], "entities": [{"text": "WordNet feature", "start_pos": 46, "end_pos": 61, "type": "DATASET", "confidence": 0.8733551800251007}]}, {"text": "This feature, along with other features such as named entity feature, can be used to build an accurate semantic class (SC) classifier.", "labels": [], "entities": []}, {"text": "In addition, we analyze the SC classification errors and propose to use relaxed SC agreement features.", "labels": [], "entities": [{"text": "SC classification", "start_pos": 28, "end_pos": 45, "type": "TASK", "confidence": 0.8623450696468353}]}, {"text": "The proposed accurate SC classifier and the relaxation of SC agreement features on ACE2 coreference evaluation can boost our baseline system by 10.4% and 9.7% using MUC score and anaphor accuracy respectively.", "labels": [], "entities": [{"text": "accurate", "start_pos": 13, "end_pos": 21, "type": "METRIC", "confidence": 0.9665274620056152}, {"text": "ACE2 coreference evaluation", "start_pos": 83, "end_pos": 110, "type": "TASK", "confidence": 0.6115305920441946}, {"text": "MUC score", "start_pos": 165, "end_pos": 174, "type": "METRIC", "confidence": 0.8461167216300964}, {"text": "anaphor accuracy", "start_pos": 179, "end_pos": 195, "type": "METRIC", "confidence": 0.705042690038681}]}], "introductionContent": [{"text": "Coreference resolution is used to determine which noun phrases (including pronouns, proper names, and common nouns) refer to the same entities in documents.", "labels": [], "entities": [{"text": "Coreference resolution", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.8817424476146698}]}, {"text": "Much work on coreference resolution is based on (), which built a decision tree classifier to label pairs of mentions as coreferent or not.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 13, "end_pos": 35, "type": "TASK", "confidence": 0.9731124341487885}]}, {"text": "Recent work aims to improve the performance from two aspects: new models and new features.", "labels": [], "entities": []}, {"text": "The former cast the pairwise mention classifications into various forms such as the best path in a Bell tree (), the best graph cut (), integer linear programming and graph partition based conditional model ().", "labels": [], "entities": []}, {"text": "The latter develop and investigate new linguistic features for the problem.", "labels": [], "entities": []}, {"text": "For instance, WordNet (), Wikipedia (), semantic neighbor words, and pattern based features) have been extensively studied.", "labels": [], "entities": []}, {"text": "Deeper linguistic knowledge is required to enable the coreference resolution to reach a higher level of performance ().", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 54, "end_pos": 76, "type": "TASK", "confidence": 0.9338262975215912}]}, {"text": "An important type of semantic knowledge that has been employed in coreference resolution system is the semantic class (SC) of an NP, which can be used to filter out the coreference between semantically incompatible NPs.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 66, "end_pos": 88, "type": "TASK", "confidence": 0.9564087092876434}]}, {"text": "However, the difficulty is to accurately compute the semantic class features.", "labels": [], "entities": []}, {"text": "In this paper, we show that the WordNet may not be efficiently employed in the traditional way such as () to compute the semantic class features.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 32, "end_pos": 39, "type": "DATASET", "confidence": 0.9541265368461609}]}, {"text": "We introduce new ways to use the WordNet and the experiments show its effectiveness in determining the semantic classes for noun phrases.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 33, "end_pos": 40, "type": "DATASET", "confidence": 0.9506120681762695}]}, {"text": "In addition, we analyze the classification errors of the SC classifier and propose to use relaxed SC agreement features.", "labels": [], "entities": []}, {"text": "With these proposed features and other standard syntactic features (which are commonly employed in existing coreference systems), our coreference resolution system can obtain an increase of 10.4% for MUC score and 9.7% for anaphor accuracy from the baseline in ACE2 evaluation.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 134, "end_pos": 156, "type": "TASK", "confidence": 0.9001106023788452}, {"text": "MUC score", "start_pos": 200, "end_pos": 209, "type": "METRIC", "confidence": 0.9257859587669373}, {"text": "accuracy", "start_pos": 231, "end_pos": 239, "type": "METRIC", "confidence": 0.8025655746459961}]}], "datasetContent": [{"text": "We design three experiments to test the accuracy of our classifiers.), and variants of HW+WN HYP are the work proposed in this paper.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 40, "end_pos": 48, "type": "METRIC", "confidence": 0.9985914826393127}, {"text": "HW+WN HYP", "start_pos": 87, "end_pos": 96, "type": "DATASET", "confidence": 0.6648351699113846}]}, {"text": "We combine headword and the semantic features due to the fact that WordNet features are dependent on head words and they could be treated as units.", "labels": [], "entities": []}, {"text": "In the second experiment, features are fed into the ME model incrementally until all features have been used.", "labels": [], "entities": []}, {"text": "Finally, we perform the feature ablation experiments.", "labels": [], "entities": []}, {"text": "That is, we remove one feature at a time from the entire feature set and test the accuracy loss.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 82, "end_pos": 90, "type": "METRIC", "confidence": 0.9992014765739441}]}, {"text": "The SC classification performance is measured by accuracy, i.e., the proportion of the correctly classified instances among all test instances.", "labels": [], "entities": [{"text": "SC classification", "start_pos": 4, "end_pos": 21, "type": "TASK", "confidence": 0.8559730350971222}, {"text": "accuracy", "start_pos": 49, "end_pos": 57, "type": "METRIC", "confidence": 0.999434769153595}]}, {"text": "Individual feature contribution shows the SC classification accuracy of all NPs (all) and non-pronoun NPs (non-PN) on the development and test datasets using individual feature The optimal of HW+WN HYP configuration is used. sets.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 60, "end_pos": 68, "type": "METRIC", "confidence": 0.840644896030426}]}, {"text": "Among all the lexical features, unigram fea- ture performs the best (81.3%) for all NPs over the development dataset.", "labels": [], "entities": []}, {"text": "The bigram feature performs poorly due to the sparsity problem: NPs usually consist of one to three words.", "labels": [], "entities": []}, {"text": "The first-last word feature effectively models the prefix words (such as a and the) and the head words and thus obtains a reasonably high accuracy of 80.1%.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 138, "end_pos": 146, "type": "METRIC", "confidence": 0.9989798665046692}]}, {"text": "As mentioned before, the headword feature may suffer from the sparsity and it results in the accuracy of 78.2%.", "labels": [], "entities": [{"text": "sparsity", "start_pos": 62, "end_pos": 70, "type": "METRIC", "confidence": 0.9762814044952393}, {"text": "accuracy", "start_pos": 93, "end_pos": 101, "type": "METRIC", "confidence": 0.9996298551559448}]}, {"text": "We also list the accuracies for non-pronoun NP SC classification, which are slightly different compared to all NP SC classification except for bigram, in which the accuracy has increased 3.9%.", "labels": [], "entities": [{"text": "accuracies", "start_pos": 17, "end_pos": 27, "type": "METRIC", "confidence": 0.995246946811676}, {"text": "NP SC classification", "start_pos": 44, "end_pos": 64, "type": "TASK", "confidence": 0.6089219252268473}, {"text": "accuracy", "start_pos": 164, "end_pos": 172, "type": "METRIC", "confidence": 0.9994667172431946}]}, {"text": "Although Stanford NER performs well on named entity recognition task, it results inaccuracy of 74.0% for all NP SC classification, due to its inability to deal with pronouns such as he and common nouns such as the government.", "labels": [], "entities": [{"text": "Stanford NER", "start_pos": 9, "end_pos": 21, "type": "DATASET", "confidence": 0.8129312098026276}, {"text": "named entity recognition", "start_pos": 39, "end_pos": 63, "type": "TASK", "confidence": 0.6895523170630137}, {"text": "NP SC classification", "start_pos": 109, "end_pos": 129, "type": "TASK", "confidence": 0.6533650755882263}]}, {"text": "The removal of pronouns significantly boosts its accuracy to 82.8%.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 49, "end_pos": 57, "type": "METRIC", "confidence": 0.9990469813346863}]}, {"text": "The introduction of semantic feature HW+WN CLASS boosts the performance to 79.5% compared to the headword alone of 78.2%.", "labels": [], "entities": [{"text": "HW+WN CLASS", "start_pos": 37, "end_pos": 48, "type": "DATASET", "confidence": 0.7119475901126862}]}, {"text": "This conforms to) that only small gain can be achieved using WN CLASS feature.", "labels": [], "entities": []}, {"text": "The HW+WN SIM feature outperforms HW+WN CLASS and the accuracy reaches 81.2%.", "labels": [], "entities": [{"text": "HW+WN CLASS", "start_pos": 34, "end_pos": 45, "type": "DATASET", "confidence": 0.8055669665336609}, {"text": "accuracy", "start_pos": 54, "end_pos": 62, "type": "METRIC", "confidence": 0.9996813535690308}]}, {"text": "For the variants of HW+WN HYP, we first search the optimal depth.", "labels": [], "entities": [{"text": "HW+WN HYP", "start_pos": 20, "end_pos": 29, "type": "DATASET", "confidence": 0.7779747098684311}]}, {"text": "This is performed by using all synsets for NP headword, encoding the feature using synset id (rather than synset word), and no hypernym depth is encoded in the features.", "labels": [], "entities": []}, {"text": "We try various depths of 1, 3, 6, 9 and \u221e, with \u221e signifies that no depth constraint is imposed.", "labels": [], "entities": []}, {"text": "The optimal depth of 6 is obtained with the accuracy of 83.1% over the development dataset.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 44, "end_pos": 52, "type": "METRIC", "confidence": 0.9996675252914429}]}, {"text": "We then fix the depth of 6 to try using synset word as features, using synset id with depth encoded as features, and using first synset only.", "labels": [], "entities": []}, {"text": "The results show that the optimum is to encode the features using hypernym synset id without hypernym depth information and all synsets are considered for hypernym extraction.", "labels": [], "entities": [{"text": "hypernym extraction", "start_pos": 155, "end_pos": 174, "type": "TASK", "confidence": 0.7022995203733444}]}, {"text": "This is slightly different from the previous finding) that a coreference resolution system employing only the first WordNet synset performs slightly better than that employing more than one synset.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 61, "end_pos": 83, "type": "TASK", "confidence": 0.9235011637210846}]}, {"text": "The best result reaches the accuracy of 83.1%.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 28, "end_pos": 36, "type": "METRIC", "confidence": 0.9996529817581177}]}, {"text": "Although the best semantic feature only outperforms the best lexical feature by 1.8% on the development dataset, its gain in the test dataset is more significant (3.2%, from 72.4% to 75.6%).", "labels": [], "entities": []}, {"text": "Incremental feature contribution Once we use the training and development datasets to find the optimal configuration of HW+WN HYP semantic feature, we use all lexical features and the optimal HW+WN HYP feature incrementally to train an ME model over the combination of training and development datasets.", "labels": [], "entities": []}, {"text": "shows the SC classification accuracy of all NPs (all) and non-pronoun NPs (non-PN) on the training+development (we refer it as training hereafter) and test datasets.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 28, "end_pos": 36, "type": "METRIC", "confidence": 0.9079142212867737}]}, {"text": "Note that the significant higher accuracies in training compared to test are due to the overfitting problem.", "labels": [], "entities": [{"text": "accuracies", "start_pos": 33, "end_pos": 43, "type": "METRIC", "confidence": 0.994103729724884}]}, {"text": "The interesting evaluation thus remains on the test data.", "labels": [], "entities": []}, {"text": "As can be seen, the inclusion of more features results in higher performance.", "labels": [], "entities": []}, {"text": "This is more obvious in the test dataset than in the training dataset.", "labels": [], "entities": [{"text": "training dataset", "start_pos": 53, "end_pos": 69, "type": "DATASET", "confidence": 0.6254466027021408}]}, {"text": "The inclusion of the optimized WN HYP feature (ie, using all synsets' hypernyms up to 6 depth and with synset id encoding) results in 7.1% increase for all NP SC classification over test data.", "labels": [], "entities": [{"text": "WN HYP", "start_pos": 31, "end_pos": 37, "type": "TASK", "confidence": 0.6685057282447815}, {"text": "NP SC classification", "start_pos": 156, "end_pos": 176, "type": "TASK", "confidence": 0.6205989023049673}]}, {"text": "This shows the effectiveness of the WN HYP features to overcome the sparsity of headword feature.", "labels": [], "entities": [{"text": "WN HYP", "start_pos": 36, "end_pos": 42, "type": "TASK", "confidence": 0.532083198428154}]}, {"text": "The unigram, bigram and first-last word features offer reasonable accuracy gain, and the final inclusion of NE boosts the overall performance to 83.1% for all NP and 84.4% for non pronoun NPs over test data.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 66, "end_pos": 74, "type": "METRIC", "confidence": 0.9988700747489929}, {"text": "NE", "start_pos": 108, "end_pos": 110, "type": "METRIC", "confidence": 0.8972302675247192}]}, {"text": "This result can be directly compared to the SC classification accuracy as reported in, in which the highest accuracy is 83.3% for non pronoun NPs.", "labels": [], "entities": [{"text": "SC classification", "start_pos": 44, "end_pos": 61, "type": "TASK", "confidence": 0.676889955997467}, {"text": "accuracy", "start_pos": 62, "end_pos": 70, "type": "METRIC", "confidence": 0.5980711579322815}, {"text": "accuracy", "start_pos": 108, "end_pos": 116, "type": "METRIC", "confidence": 0.9978183507919312}]}, {"text": "The large difference between the highest training accuracies is due to that our classifier is trained directly on the ACE2 training dataset, while their SC classifier was trained on BBN Entity Type Corpus (, which is five times larger than the ACE2 corpus used by us.", "labels": [], "entities": [{"text": "ACE2 training dataset", "start_pos": 118, "end_pos": 139, "type": "DATASET", "confidence": 0.9367319941520691}, {"text": "BBN Entity Type Corpus", "start_pos": 182, "end_pos": 204, "type": "DATASET", "confidence": 0.8691885620355606}, {"text": "ACE2 corpus", "start_pos": 244, "end_pos": 255, "type": "DATASET", "confidence": 0.9185753166675568}]}, {"text": "In addition to WordNet, they have adopted multiple knowledge sources which include BBN's IdentiFinder (this is equivalent to the Stanford NER in our work), BLLIP corpus and Reuters Corpus, and dependency based thesaurus.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 15, "end_pos": 22, "type": "DATASET", "confidence": 0.9401493072509766}, {"text": "BBN's IdentiFinder", "start_pos": 83, "end_pos": 101, "type": "DATASET", "confidence": 0.8544125556945801}, {"text": "BLLIP corpus", "start_pos": 156, "end_pos": 168, "type": "DATASET", "confidence": 0.8540912866592407}, {"text": "Reuters Corpus", "start_pos": 173, "end_pos": 187, "type": "DATASET", "confidence": 0.9199649393558502}]}, {"text": "It is remarkable that our SC classifier can achieve even higher accuracy only using WordNet hypernym and NE features.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 64, "end_pos": 72, "type": "METRIC", "confidence": 0.9983682036399841}, {"text": "WordNet", "start_pos": 84, "end_pos": 91, "type": "DATASET", "confidence": 0.9416615962982178}]}, {"text": "It is worth noting that the small accuracy gain is indeed hard to achieve considering that the test data size is large (15360).", "labels": [], "entities": [{"text": "accuracy gain", "start_pos": 34, "end_pos": 47, "type": "METRIC", "confidence": 0.9852624237537384}]}, {"text": "Feature ablation experiment We now perform the feature ablation experiments to further determine the importance of individual features.", "labels": [], "entities": []}, {"text": "We remove one feature at a time from the entire feature set.", "labels": [], "entities": []}, {"text": "shows the SC classification accuracy of all NPs (all) and non-pronoun NPs (non-PN) on the training and test datasets respectively.", "labels": [], "entities": [{"text": "SC classification", "start_pos": 10, "end_pos": 27, "type": "TASK", "confidence": 0.6305670142173767}, {"text": "accuracy", "start_pos": 28, "end_pos": 36, "type": "METRIC", "confidence": 0.8698556423187256}]}, {"text": "Again, the significant higher accuracies in training compared to test are due to overfitting.", "labels": [], "entities": [{"text": "accuracies", "start_pos": 30, "end_pos": 40, "type": "METRIC", "confidence": 0.9925077557563782}]}, {"text": "The re-7 All NP accuracy was not reported as they excluded the pronouns in creating their training and test data.", "labels": [], "entities": [{"text": "re-7 All NP accuracy", "start_pos": 4, "end_pos": 24, "type": "METRIC", "confidence": 0.8250791132450104}]}, {"text": "8 They use these corpus to extract patterns to induce SC of common nouns.", "labels": [], "entities": [{"text": "SC of common nouns", "start_pos": 54, "end_pos": 72, "type": "TASK", "confidence": 0.8046746402978897}]}, {"text": "moval of NE feature results in the largest accuracy loss of 4.3% (from 83.1% to 78.8%) for all nouns on test data.", "labels": [], "entities": [{"text": "accuracy loss", "start_pos": 43, "end_pos": 56, "type": "METRIC", "confidence": 0.9939949810504913}]}, {"text": "It follows WN HYP (0.5% loss) and the bigram (0.4%).", "labels": [], "entities": [{"text": "WN HYP", "start_pos": 11, "end_pos": 17, "type": "DATASET", "confidence": 0.7804960012435913}, {"text": "bigram", "start_pos": 38, "end_pos": 44, "type": "DATASET", "confidence": 0.822225034236908}]}, {"text": "If we treat HW+WN HYP as one feature, the removal of it results inaccuracy loss of 0.8% for all nouns on test data.", "labels": [], "entities": []}, {"text": "The unigram, first-last word and headword each results in the loss of 0.2%.", "labels": [], "entities": []}, {"text": "The reason that the removal of NE results in a much significant loss is due to the fact that the NE feature is quite different from other features.", "labels": [], "entities": []}, {"text": "Its strength is to distinguish SCs for proper names, while other features are more similar (their targets are common nouns).", "labels": [], "entities": []}, {"text": "The proposed use of HW+WN HYP can bring 0.8% gain on top of other features, higher than other informative lexical features including unigram and firstlast word.", "labels": [], "entities": [{"text": "HW+WN HYP", "start_pos": 20, "end_pos": 29, "type": "DATASET", "confidence": 0.7278315722942352}]}, {"text": "We use the ACE-2 (version 1.0) coreference corpus.", "labels": [], "entities": [{"text": "ACE-2 (version 1.0) coreference corpus", "start_pos": 11, "end_pos": 49, "type": "DATASET", "confidence": 0.8190663967813764}]}, {"text": "Each raw text in this corpus was preprocessed automatically by a pipeline of NLP components, including sentence boundary detection, POS-tagging and text chunking.", "labels": [], "entities": [{"text": "sentence boundary detection", "start_pos": 103, "end_pos": 130, "type": "TASK", "confidence": 0.712598443031311}, {"text": "text chunking", "start_pos": 148, "end_pos": 161, "type": "TASK", "confidence": 0.7287785112857819}]}, {"text": "The statistics of corpus and mention extraction are shown in, where g-mention is the automatically extracted mentions which contain the annotated (gold) mentions.", "labels": [], "entities": [{"text": "mention extraction", "start_pos": 29, "end_pos": 47, "type": "TASK", "confidence": 0.7229577451944351}]}, {"text": "The recalls of gold mentions are 95.88% and 95.93% for training and test data respectively.", "labels": [], "entities": [{"text": "recalls", "start_pos": 4, "end_pos": 11, "type": "METRIC", "confidence": 0.998450756072998}]}, {"text": "Our coreference system uses Maximum Entropy model to determine whether two NPs are coreferent.", "labels": [], "entities": []}, {"text": "As in (), we generate training instances as follows: a positive instance is created for each anaphoric NP, NP j , and its closest antecedent, NP i ; and a negative instance is created for NP j paired with each of the intervening NPs, NP i+1 , NP i+2 , ..., NP j\u22121 . Each instance is represented by syntactic or semantic features described as follows.", "labels": [], "entities": []}, {"text": "All training data are used to train a maximum entropy model.", "labels": [], "entities": []}, {"text": "In the test stage,we select the closest preceding NP that is classified as coreferent with NP j as the antecedent of NP j . If no such NP exists, no antecedent is selected for NP j . Unlike other natural language processing tasks such as information extraction which have de facto evaluation metrics, it is an open question which evaluation is the most suitable one.", "labels": [], "entities": [{"text": "information extraction", "start_pos": 238, "end_pos": 260, "type": "TASK", "confidence": 0.7885175049304962}]}, {"text": "The evaluation becomes more complicated when automatically extracted mentions (in contrast to the gold mentions) are used.", "labels": [], "entities": []}, {"text": "To facilitate the comparison with previous work, we report performance using two different scoring metrics: the commonlyused MUC scorer ( and the accuracy of the anaphoric references ().", "labels": [], "entities": [{"text": "MUC scorer", "start_pos": 125, "end_pos": 135, "type": "DATASET", "confidence": 0.6248866319656372}, {"text": "accuracy", "start_pos": 146, "end_pos": 154, "type": "METRIC", "confidence": 0.9990928173065186}]}, {"text": "An anaphoric reference is correctly resolved if it and its closest antecedent are in the same coreference chain in the resulting partition.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Distributions of SCs in ACE2 corpus.", "labels": [], "entities": [{"text": "ACE2 corpus", "start_pos": 34, "end_pos": 45, "type": "DATASET", "confidence": 0.9744705259799957}]}, {"text": " Table 3: SC classification accuracy of ME using  individual feature sets for development and test  ACE2 datasets.", "labels": [], "entities": [{"text": "SC classification", "start_pos": 10, "end_pos": 27, "type": "TASK", "confidence": 0.8910320103168488}, {"text": "accuracy", "start_pos": 28, "end_pos": 36, "type": "METRIC", "confidence": 0.9079345464706421}, {"text": "ME", "start_pos": 40, "end_pos": 42, "type": "TASK", "confidence": 0.6955310702323914}, {"text": "ACE2 datasets", "start_pos": 100, "end_pos": 113, "type": "DATASET", "confidence": 0.9721953272819519}]}, {"text": " Table 4: SC classification accuracy of ME using  incremental feature sets for training and test ACE2  datasets.", "labels": [], "entities": [{"text": "SC classification", "start_pos": 10, "end_pos": 27, "type": "TASK", "confidence": 0.8750943839550018}, {"text": "accuracy", "start_pos": 28, "end_pos": 36, "type": "METRIC", "confidence": 0.9393947124481201}, {"text": "ME", "start_pos": 40, "end_pos": 42, "type": "TASK", "confidence": 0.5871429443359375}, {"text": "ACE2  datasets", "start_pos": 97, "end_pos": 111, "type": "DATASET", "confidence": 0.9690848588943481}]}, {"text": " Table 5: SC classification accuracy of ME by re- moving one feature at a time for training and test  ACE2 datasets.", "labels": [], "entities": [{"text": "SC classification", "start_pos": 10, "end_pos": 27, "type": "TASK", "confidence": 0.8882957100868225}, {"text": "accuracy", "start_pos": 28, "end_pos": 36, "type": "METRIC", "confidence": 0.9628599286079407}, {"text": "ME", "start_pos": 40, "end_pos": 42, "type": "TASK", "confidence": 0.5438230633735657}, {"text": "ACE2 datasets", "start_pos": 102, "end_pos": 115, "type": "DATASET", "confidence": 0.9610756039619446}]}, {"text": " Table 6: Statistics for corpus and extracted men- tions.", "labels": [], "entities": []}]}