{"title": [{"text": "Unbounded Dependency Recovery for Parser Evaluation", "labels": [], "entities": []}], "abstractContent": [{"text": "This paper introduces anew parser evaluation corpus containing around 700 sentences annotated with unbounded dependencies , from seven different grammatical constructions.", "labels": [], "entities": []}, {"text": "We run a series of off-the-shelf parsers on the corpus to evaluate how well state-of-the-art parsing technology is able to recover such dependencies.", "labels": [], "entities": []}, {"text": "The overall results range from 25% accuracy to 59%.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 35, "end_pos": 43, "type": "METRIC", "confidence": 0.9996520280838013}]}, {"text": "These low scores call into question the validity of using Parseval scores as a general measure of parsing capability.", "labels": [], "entities": [{"text": "parsing", "start_pos": 98, "end_pos": 105, "type": "TASK", "confidence": 0.9710695743560791}]}, {"text": "We discuss the importance of parsers being able to recover unbounded dependencies , given their relatively low frequency in corpora.", "labels": [], "entities": []}, {"text": "We also analyse the various errors made on these constructions by one of the more successful parsers.", "labels": [], "entities": []}], "introductionContent": [{"text": "Statistical parsers are now obtaining Parseval scores of over 90% on the WSJ section of the Penn Treebank.", "labels": [], "entities": [{"text": "Parseval scores", "start_pos": 38, "end_pos": 53, "type": "METRIC", "confidence": 0.9772751927375793}, {"text": "WSJ section of the Penn Treebank", "start_pos": 73, "end_pos": 105, "type": "DATASET", "confidence": 0.8655504882335663}]}, {"text": "report an F-score of 92.1% using selftraining applied to the reranker of.", "labels": [], "entities": [{"text": "F-score", "start_pos": 10, "end_pos": 17, "type": "METRIC", "confidence": 0.9996548891067505}]}, {"text": "Such scores, in isolation, may suggest that statistical parsing is close to becoming a solved problem, and that further incremental improvements will lead to parsers becoming as accurate as POS taggers.", "labels": [], "entities": [{"text": "statistical parsing", "start_pos": 44, "end_pos": 63, "type": "TASK", "confidence": 0.6070646643638611}, {"text": "POS taggers", "start_pos": 190, "end_pos": 201, "type": "TASK", "confidence": 0.6707816570997238}]}, {"text": "A single score in isolation can be misleading, however, fora number of reasons.", "labels": [], "entities": []}, {"text": "First, the single score is an aggregate over a highly skewed distribution of all constituent types; evaluations which look at individual constituent or dependency types show that the accuracies on some, semantically important, constructions, such as coordination and PP-attachment, are much lower).", "labels": [], "entities": []}, {"text": "Second, it is well known that the accuracy of parsers trained on the Penn Treebank degrades when they are applied to different genres and domains).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 34, "end_pos": 42, "type": "METRIC", "confidence": 0.9994626641273499}, {"text": "Penn Treebank", "start_pos": 69, "end_pos": 82, "type": "DATASET", "confidence": 0.9909648299217224}]}, {"text": "Finally, some researchers have argued that the Parseval metrics ( are too forgiving with respect to certain errors and that an evaluation based on syntactic dependencies, for which scores are typically lower, is a better test of parser performance.", "labels": [], "entities": []}, {"text": "In this paper we focus on the first issue, that the performance of parsers on some constructions is much lower than the overall score.", "labels": [], "entities": []}, {"text": "The constructions that we focus on are various unbounded dependency constructions.", "labels": [], "entities": []}, {"text": "These are interesting for parser evaluation for the following reasons: one, they provide a strong test of the parser's knowledge of the grammar of the language, since many instances of unbounded dependencies are difficult to recover using shallow techniques in which the grammar is only superficially represented; and two, recovering these dependencies is necessary to completely represent the underlying predicateargument structure of a sentence, useful for applications such as Question Answering and Information Extraction.", "labels": [], "entities": [{"text": "parser evaluation", "start_pos": 26, "end_pos": 43, "type": "TASK", "confidence": 0.8202707171440125}, {"text": "Question Answering", "start_pos": 480, "end_pos": 498, "type": "TASK", "confidence": 0.8316540420055389}, {"text": "Information Extraction", "start_pos": 503, "end_pos": 525, "type": "TASK", "confidence": 0.7696011662483215}]}, {"text": "To give an example of the sorts of constructions we are considering, and the (in)ability of parsers to recover the corresponding unbounded dependencies, none of the parsers that we have tested were able to recover the dependencies shown in bold from the following sentences: We have also developed techniques for recognizing and locating underground nuclear tests through the waves in the ground which they generate.", "labels": [], "entities": [{"text": "recognizing and locating underground nuclear tests", "start_pos": 313, "end_pos": 363, "type": "TASK", "confidence": 0.7217906067768732}]}, {"text": "By Monday , they hope to have a sheaf of documents both sides can trust.", "labels": [], "entities": []}, {"text": "By means of charts showing wave-travel times and depths in the ocean at various locations , it is possible to estimate the rate of approach and probable time of arrival at Hawaii of a tsunami getting underway at any spot in the Pacific . The contributions of this paper are as follows.", "labels": [], "entities": []}, {"text": "First, we present the first set of results for the recovery of a variety of unbounded dependencies, fora range of existing parsers.", "labels": [], "entities": []}, {"text": "Second, we describe the creation of a publicly available unbounded dependency test suite, and give statistics summarising properties of these dependencies in naturally occurring text.", "labels": [], "entities": []}, {"text": "Third, we demonstrate that performing the evaluation is surprisingly difficult, because of different conventions across the parsers as to how the underlying grammar is represented.", "labels": [], "entities": []}, {"text": "Fourth, we show that current parsing technology is very poor at representing some important elements of the argument structure of sentences, and argue fora more focused constructionbased parser evaluation as a complement to existing grammatical relation-based evaluations.", "labels": [], "entities": []}, {"text": "We also perform an error-analysis for one of the more successful parsers.", "labels": [], "entities": []}, {"text": "There has been some prior work on evaluating parsers on long-range dependencies, but no work we are aware of that has the scope and focus of this paper.", "labels": [], "entities": []}, {"text": "evaluated a CCG parser on a small corpus of object extraction cases.", "labels": [], "entities": [{"text": "object extraction", "start_pos": 44, "end_pos": 61, "type": "TASK", "confidence": 0.7197564840316772}]}, {"text": "began the body of work on inserting traces into the output of Penn Treebank (PTB) parsers, followed by, among others.", "labels": [], "entities": [{"text": "Penn Treebank (PTB) parsers", "start_pos": 62, "end_pos": 89, "type": "DATASET", "confidence": 0.9651171465714773}]}, {"text": "This PTB work focused heavily on the representation in the Treebank, evaluating against patterns in the trace annotation.", "labels": [], "entities": []}, {"text": "In this paper we have tried to be more \"formalismindependent\" and construction focused.", "labels": [], "entities": []}], "datasetContent": [{"text": "The five parsers described in Section 3.2 were used to parse the test sentences in the corpus, and the percentage of dependencies in the test set recovered by each parser for each construction was calculated.", "labels": [], "entities": []}, {"text": "The details of how the parsers were run and how the parser output was matched against the gold standard are given in Section 3.3.", "labels": [], "entities": []}, {"text": "This  is essentially a recall evaluation, and so is open to abuse; for example, a program which returns all the possible word pairs in a sentence, together with all possible labels, would score 100%.", "labels": [], "entities": [{"text": "recall", "start_pos": 23, "end_pos": 29, "type": "METRIC", "confidence": 0.9901789426803589}]}, {"text": "However, this is easily guarded against: we simply assume that each parser is being run in a \"standard\" mode, and that each parser has already been evaluated on a full corpus of GRs in order to measure precision and recall across all dependency types.", "labels": [], "entities": [{"text": "precision", "start_pos": 202, "end_pos": 211, "type": "METRIC", "confidence": 0.9985604882240295}, {"text": "recall", "start_pos": 216, "end_pos": 222, "type": "METRIC", "confidence": 0.9961040019989014}]}, {"text": "(Calculating precision for the unbounded dependency evaluation would be difficult since that would require us to know how many incorrect unbounded dependencies were returned by each parser.) shows the percentage of sentences in the PTB, from those sections that were examined, which contain an example of each type of unbounded dependency.", "labels": [], "entities": [{"text": "precision", "start_pos": 13, "end_pos": 22, "type": "METRIC", "confidence": 0.9986055493354797}, {"text": "PTB", "start_pos": 232, "end_pos": 235, "type": "DATASET", "confidence": 0.8675658702850342}]}, {"text": "Perhaps not surprisingly, root subject extractions from relative clauses are by far the most common, with the remaining constructions occurring in roughly between 1 and 2% of sentences.", "labels": [], "entities": [{"text": "root subject extractions from relative clauses", "start_pos": 26, "end_pos": 72, "type": "TASK", "confidence": 0.8462504148483276}]}, {"text": "Note that, although examples of each individual construction are relatively rare, the combined total is over 10% (assuming that each construction occurs independently).", "labels": [], "entities": []}, {"text": "Section 6 contains a discussion regarding the frequency of occurrence of these events and the consequences of this for parser performance.", "labels": [], "entities": []}, {"text": "shows the average and maximum distance between head and dependent for each construction, as measured by the difference between word indices.", "labels": [], "entities": []}, {"text": "This is a fairly crude measure of distance but gives some indication of how \"longrange\" the dependencies are for each construction.", "labels": [], "entities": []}, {"text": "The cases of object extraction from a relative clause and subject extraction from an embedded clause provide the longest dependencies, on average.", "labels": [], "entities": [{"text": "object extraction from a relative clause", "start_pos": 13, "end_pos": 53, "type": "TASK", "confidence": 0.8246244142452875}]}, {"text": "The following sentence gives an example of how far apart the head and dependent can be in a: Distance between head and dependent.", "labels": [], "entities": []}, {"text": "subject embedded construction: the same stump which had impaled the car of many a guest in the past thirty years and which he refused to have removed.", "labels": [], "entities": []}, {"text": "The parsers were run essentially out-of-the-box when parsing the test sentences.", "labels": [], "entities": []}, {"text": "The one exception was C&C, which required some minor adjusting of parameters, as described in the parser documentation, to obtain close to full coverage on the data.", "labels": [], "entities": []}, {"text": "In addition, the C&C parser comes with a  specially designed question model, and so we applied both this and the standard model to the object wh-question cases.", "labels": [], "entities": []}, {"text": "The parser output was evaluated against each dependency in the corpus.", "labels": [], "entities": []}, {"text": "Due to the various GR schemes used by the parsers, an exact match on the dependency label could not always be expected.", "labels": [], "entities": []}, {"text": "We considered a correctly recovered dependency to be one where the gold-standard head and dependent were correctly identified, and the label was an \"acceptable match\" to the gold-standard label.", "labels": [], "entities": []}, {"text": "To bean acceptable match, the label had to indicate the grammatical function of the extracted element at least to the level of distinguishing active subjects, passive subjects, objects, and adjuncts.", "labels": [], "entities": []}, {"text": "For example, we allowed an obj (object) relation as a close enough match for dobj (direct object) in the corpus, even though obj does not distinguish different kinds of objects, but we did not allow generic \"relative pronoun\" relations that are underspecified for the grammatical role of the extracted element.", "labels": [], "entities": []}, {"text": "The differences in GR schemes were such that we ended up performing a time-consuming largely manual evaluation.", "labels": [], "entities": []}, {"text": "We list here some of the key differences that made the evaluation difficult.", "labels": [], "entities": []}, {"text": "In some cases, the parser's set of labels was less fine-grained than the gold standard.", "labels": [], "entities": []}, {"text": "For example, RASP represents the direct objects of both verbs and prepositions as dobj (direct object), whereas the gold-standard uses pobj for the preposition case.", "labels": [], "entities": [{"text": "RASP", "start_pos": 13, "end_pos": 17, "type": "TASK", "confidence": 0.7175698280334473}]}, {"text": "We counted the RASP output as correctly matching the gold standard.", "labels": [], "entities": [{"text": "RASP", "start_pos": 15, "end_pos": 19, "type": "TASK", "confidence": 0.6416473984718323}]}, {"text": "In other cases, the label on the dependency containing the gold-standard head and dependent was too underspecified to be acceptable by itself.", "labels": [], "entities": []}, {"text": "For example, where the gold-standard relation was dobj(placed,buckets), DCU produced relmod(buckets,placed) with a generic \"relative modifier\" label.", "labels": [], "entities": [{"text": "DCU", "start_pos": 72, "end_pos": 75, "type": "DATASET", "confidence": 0.895302951335907}]}, {"text": "However, the correct label could be recovered from elsewhere in the parser output, specifically a combination of relpro(buckets,which) and obj(placed,which).", "labels": [], "entities": []}, {"text": "In this case we counted the DCU output as correctly matching the gold standard.", "labels": [], "entities": []}, {"text": "In some constructions the Stanford scheme, upon which the gold-standard was based, makes different choices about heads than other schemes.", "labels": [], "entities": []}, {"text": "For example, in the the phrase Honolulu, which is the center of the warning system, the corpus contains a subject dependency with center as the head: nsubj(center,Honolulu).", "labels": [], "entities": []}, {"text": "Other schemes, however, treat the auxiliary verb is as the head of the dependency, rather than the predicate nominal center.", "labels": [], "entities": []}, {"text": "As long as the difference in head selection was due solely to the idiosyncracies of the GR schemes involved, we counted the relation as correct.", "labels": [], "entities": []}, {"text": "Finally, the different GR schemes treat coordination differently.", "labels": [], "entities": []}, {"text": "In the corpus, coordinated elements are always represented with two dependencies.", "labels": [], "entities": []}, {"text": "Thus the phrase they may half see and half imagine the old splendor has two gold-standard dependencies: dobj(see,splendor) and dobj(imagine,splendor).", "labels": [], "entities": []}, {"text": "If a parser produced only the former dependency, but appeared to have the coordination correct, then we awarded two marks, even though the second dependency was not explicitly represented.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Frequency of constructions in the PTB  (percentage of sentences).", "labels": [], "entities": [{"text": "Frequency", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9783627986907959}, {"text": "PTB", "start_pos": 44, "end_pos": 47, "type": "DATASET", "confidence": 0.6517376899719238}]}, {"text": " Table 3: Distance between head and dependent.", "labels": [], "entities": []}, {"text": " Table 4: Parser accuracy on the unbounded dependency corpus; the highest score for each construction  is in bold; the figures in brackets for C&C derive from the use of a separate question model.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 17, "end_pos": 25, "type": "METRIC", "confidence": 0.9661421179771423}]}, {"text": " Table 5: Error analysis for C&C. Errs is the to- tal number of errors for a construction, Tot is the  number of dependencies of that type in the devel- opment data.", "labels": [], "entities": [{"text": "Errs", "start_pos": 34, "end_pos": 38, "type": "METRIC", "confidence": 0.9963904023170471}, {"text": "Tot", "start_pos": 91, "end_pos": 94, "type": "METRIC", "confidence": 0.9884805679321289}]}]}