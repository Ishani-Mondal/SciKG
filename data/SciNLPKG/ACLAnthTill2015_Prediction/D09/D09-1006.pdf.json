{"title": [{"text": "Feasibility of Human-in-the-loop Minimum Error Rate Training", "labels": [], "entities": [{"text": "Minimum Error Rate", "start_pos": 33, "end_pos": 51, "type": "METRIC", "confidence": 0.7815845807393392}]}], "abstractContent": [{"text": "Minimum error rate training (MERT) involves choosing parameter values fora machine translation (MT) system that maximize performance on a tuning set as measured by an automatic evaluation metric , such as BLEU.", "labels": [], "entities": [{"text": "Minimum error rate training (MERT)", "start_pos": 0, "end_pos": 34, "type": "TASK", "confidence": 0.6806635941777911}, {"text": "machine translation (MT)", "start_pos": 75, "end_pos": 99, "type": "TASK", "confidence": 0.8403475522994995}, {"text": "BLEU", "start_pos": 205, "end_pos": 209, "type": "METRIC", "confidence": 0.9857775568962097}]}, {"text": "The method is best when the system will eventually be evaluated using the same metric, but in reality, most MT evaluations have a human-based component.", "labels": [], "entities": [{"text": "MT evaluations", "start_pos": 108, "end_pos": 122, "type": "TASK", "confidence": 0.9228811264038086}]}, {"text": "Although performing MERT with a human-based metric seems like a daunting task, we describe anew metric, RYPT, which takes human judgments into account, but only requires human input to build a database that can be reused over and over again, hence eliminating the need for human input at tuning time.", "labels": [], "entities": [{"text": "MERT", "start_pos": 20, "end_pos": 24, "type": "TASK", "confidence": 0.9385882616043091}, {"text": "RYPT", "start_pos": 104, "end_pos": 108, "type": "METRIC", "confidence": 0.9391444325447083}]}, {"text": "In this investigative study, we analyze the diversity (or lack thereof) of the candidates produced during MERT, we describe how this redundancy can be used to our advantage, and show that RYPT is a better predictor of translation quality than BLEU.", "labels": [], "entities": [{"text": "MERT", "start_pos": 106, "end_pos": 110, "type": "TASK", "confidence": 0.6821684241294861}, {"text": "RYPT", "start_pos": 188, "end_pos": 192, "type": "METRIC", "confidence": 0.9272335767745972}, {"text": "translation", "start_pos": 218, "end_pos": 229, "type": "TASK", "confidence": 0.9603623747825623}, {"text": "BLEU", "start_pos": 243, "end_pos": 247, "type": "METRIC", "confidence": 0.9944215416908264}]}], "introductionContent": [{"text": "Many state-of-the-art machine translation (MT) systems over the past few years () rely on several models to evaluate the \"goodness\" of a given candidate translation in the target language.", "labels": [], "entities": [{"text": "machine translation (MT)", "start_pos": 22, "end_pos": 46, "type": "TASK", "confidence": 0.8837316751480102}]}, {"text": "The MT system proceeds by searching for the highest-scoring candidate translation, as scored by the different model components, and returns that candidate as the hypothesis translation.", "labels": [], "entities": [{"text": "MT", "start_pos": 4, "end_pos": 6, "type": "TASK", "confidence": 0.9875811338424683}]}, {"text": "Each of these models need not be a probabilistic model, and instead corresponds to a feature that is a function of a (candidate translation,foreign sentence) pair.", "labels": [], "entities": []}, {"text": "Treated as a log-linear model, we need to assign a weight for each of the features.", "labels": [], "entities": []}, {"text": "shows that setting those weights should take into account the evaluation metric by which the MT system will eventually be judged.", "labels": [], "entities": [{"text": "MT", "start_pos": 93, "end_pos": 95, "type": "TASK", "confidence": 0.951742947101593}]}, {"text": "This is achieved by choosing the weights so as to maximize the performance of the MT system on a development set, as measured by that evaluation metric.", "labels": [], "entities": [{"text": "MT", "start_pos": 82, "end_pos": 84, "type": "TASK", "confidence": 0.980647623538971}]}, {"text": "The other insight of Och's work is that there exists an efficient algorithm to find such weights.", "labels": [], "entities": []}, {"text": "This process has come to be known as the MERT phase (for Minimum Error Rate Training) in training pipelines of MT systems.", "labels": [], "entities": [{"text": "MERT", "start_pos": 41, "end_pos": 45, "type": "METRIC", "confidence": 0.9105324745178223}, {"text": "MT", "start_pos": 111, "end_pos": 113, "type": "TASK", "confidence": 0.9835729002952576}]}, {"text": "A problem arises if the performance of the system is not judged by an automatic evaluation metric such as BLEU or TER, but instead through an evaluation process involving a human.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 106, "end_pos": 110, "type": "METRIC", "confidence": 0.9985198378562927}, {"text": "TER", "start_pos": 114, "end_pos": 117, "type": "METRIC", "confidence": 0.964796245098114}]}, {"text": "The GALE evaluation, for instance, judges the quality of systems as measured by human-targeted TER (HTER), which computes the edit distance between the system's output and aversion of the output post-edited by a human.", "labels": [], "entities": [{"text": "GALE", "start_pos": 4, "end_pos": 8, "type": "METRIC", "confidence": 0.562950849533081}, {"text": "TER (HTER)", "start_pos": 95, "end_pos": 105, "type": "METRIC", "confidence": 0.8990310430526733}]}, {"text": "The IWSLT and WMT workshops also have a manual evaluation component, as does the NIST Evaluation, in the form of adequacy and fluency.", "labels": [], "entities": [{"text": "IWSLT", "start_pos": 4, "end_pos": 9, "type": "DATASET", "confidence": 0.870197057723999}, {"text": "WMT", "start_pos": 14, "end_pos": 17, "type": "DATASET", "confidence": 0.4549386203289032}, {"text": "NIST Evaluation", "start_pos": 81, "end_pos": 96, "type": "DATASET", "confidence": 0.8568243086338043}]}, {"text": "In theory, one could imagine trying to optimize a metric like HTER during the MERT phase, but that would require the availability of an HTER automatic scorer, which, by definition, does not exist.", "labels": [], "entities": [{"text": "HTER", "start_pos": 62, "end_pos": 66, "type": "METRIC", "confidence": 0.7998767495155334}, {"text": "MERT phase", "start_pos": 78, "end_pos": 88, "type": "TASK", "confidence": 0.7493696808815002}, {"text": "HTER automatic scorer", "start_pos": 136, "end_pos": 157, "type": "METRIC", "confidence": 0.6763911247253418}]}, {"text": "If done manually, the scoring of thousands of candidates produced during MERT would literally take weeks, and cost a large sum of money.", "labels": [], "entities": [{"text": "MERT", "start_pos": 73, "end_pos": 77, "type": "TASK", "confidence": 0.5537489056587219}]}, {"text": "For these reasons, researchers resort to optimizing an automatic metric (almost always BLEU) as a proxy for human judgment.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 87, "end_pos": 91, "type": "METRIC", "confidence": 0.9983546137809753}]}, {"text": "As daunting as such a task seems for any human-based metric, we describe anew metric, RYPT, that takes human judgment into accout when scoring candidates, but takes advantage of the redundancy in the candidates produced during MERT.", "labels": [], "entities": [{"text": "RYPT", "start_pos": 86, "end_pos": 90, "type": "METRIC", "confidence": 0.9792308211326599}]}, {"text": "In this investigative study, we describe how this redundancy can be used to our advantage to eliminate the need to involve a human at anytime except when building a database of reusable judgments, and furthermore show that RYPT is a better predictor of translation quality than BLEU, making it an excellent candidate for MERT tuning.", "labels": [], "entities": [{"text": "RYPT", "start_pos": 223, "end_pos": 227, "type": "METRIC", "confidence": 0.887327253818512}, {"text": "translation", "start_pos": 253, "end_pos": 264, "type": "TASK", "confidence": 0.9483127593994141}, {"text": "BLEU", "start_pos": 278, "end_pos": 282, "type": "METRIC", "confidence": 0.997097373008728}, {"text": "MERT tuning", "start_pos": 321, "end_pos": 332, "type": "TASK", "confidence": 0.9391435980796814}]}, {"text": "The paper is organized as follows.", "labels": [], "entities": []}, {"text": "We start by describing the core idea of MERT before introducing our new metric, RYPT, and describing the data collection effort we undertook to collect the needed human judgments.", "labels": [], "entities": [{"text": "MERT", "start_pos": 40, "end_pos": 44, "type": "TASK", "confidence": 0.6881667971611023}, {"text": "RYPT", "start_pos": 80, "end_pos": 84, "type": "METRIC", "confidence": 0.9108818769454956}]}, {"text": "We analyze a MERT run optimizing BLEU to quantify the level of redundancy in the candidate set, and also provide an extensive analysis of the collected judgments, before describing a set of experiments showing RYPT is a better predictor of translation quality than BLEU.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 33, "end_pos": 37, "type": "METRIC", "confidence": 0.9913552403450012}, {"text": "translation", "start_pos": 240, "end_pos": 251, "type": "TASK", "confidence": 0.9505758881568909}, {"text": "BLEU", "start_pos": 265, "end_pos": 269, "type": "METRIC", "confidence": 0.9917837381362915}]}, {"text": "Following a discussion of our findings, we briefly review related work, before pointing out future directions and summarizing.", "labels": [], "entities": []}], "datasetContent": [{"text": "By limiting our queries to source segments corresponding to frontier nodes with maxLen = 4, we obtain a total of 3601 subtrees across the 250 sentences, for an average of 14.4 per sentence.", "labels": [], "entities": []}, {"text": "On average, each subtree has 3.65 alternative translations.", "labels": [], "entities": []}, {"text": "Only about 4.8% of the judgments were returned as NOT SURE (or, occasionally, blank), with the rest split into 35.1% YES judgments and 60.1% NO judgments.", "labels": [], "entities": [{"text": "NOT", "start_pos": 50, "end_pos": 53, "type": "METRIC", "confidence": 0.9729248881340027}, {"text": "SURE", "start_pos": 54, "end_pos": 58, "type": "METRIC", "confidence": 0.7584192752838135}, {"text": "YES", "start_pos": 117, "end_pos": 120, "type": "METRIC", "confidence": 0.9895337224006653}, {"text": "NO", "start_pos": 141, "end_pos": 143, "type": "METRIC", "confidence": 0.9960260391235352}]}, {"text": "The coverage we get before percolating labels up and down the trees is 39.4% of the nodes, increasing to a coverage of 72.9% after percolation.", "labels": [], "entities": []}, {"text": "This is quite good, considering we only do a single data collection pass, and considering that about 10% of the subtrees do not align to candidate substrings to begin with (e.g. single source words that lack a word alignment into the candidate string).", "labels": [], "entities": []}, {"text": "The main question, of course, is whether or not those labels allow us to calculate a RYPT score that is reliably correlated with human judgment.", "labels": [], "entities": [{"text": "RYPT score", "start_pos": 85, "end_pos": 95, "type": "METRIC", "confidence": 0.8707650601863861}]}, {"text": "We designed an experiment to compare the predictive power of RYPT vs. BLEU.", "labels": [], "entities": [{"text": "RYPT", "start_pos": 61, "end_pos": 65, "type": "METRIC", "confidence": 0.811949610710144}, {"text": "BLEU", "start_pos": 70, "end_pos": 74, "type": "METRIC", "confidence": 0.996437668800354}]}, {"text": "Given the candidate set of a source sentence, we rerank the candidate set according to RYPT and extract the top-1 candidate, and we rerank the candidate set according to BLEU, and extract the top-1 candidate.", "labels": [], "entities": [{"text": "RYPT", "start_pos": 87, "end_pos": 91, "type": "METRIC", "confidence": 0.7550092935562134}, {"text": "BLEU", "start_pos": 170, "end_pos": 174, "type": "METRIC", "confidence": 0.9975857734680176}]}, {"text": "We then present the two candidates to human judges, and ask them to choose the one that is a more adequate translation.", "labels": [], "entities": []}, {"text": "For reliability, we collect 3 judgments per sentence pair comparison, instead of just 1.", "labels": [], "entities": [{"text": "reliability", "start_pos": 4, "end_pos": 15, "type": "METRIC", "confidence": 0.8786391019821167}]}, {"text": "The results show that RYPT significantly outperforms BLEU when it comes to predicting human preference, with its choice prevailing in 46.1% of judgments vs. 36.0% for BLEU, with 17.9% judged to be of equal quality (left half of Table 1).", "labels": [], "entities": [{"text": "RYPT", "start_pos": 22, "end_pos": 26, "type": "METRIC", "confidence": 0.9497584104537964}, {"text": "BLEU", "start_pos": 53, "end_pos": 57, "type": "METRIC", "confidence": 0.9944213032722473}, {"text": "predicting human preference", "start_pos": 75, "end_pos": 102, "type": "TASK", "confidence": 0.8927743434906006}, {"text": "BLEU", "start_pos": 167, "end_pos": 171, "type": "METRIC", "confidence": 0.9914022088050842}]}, {"text": "This advantage is especially true when the judgments are grouped by sentence, and we examine cases of strong agreement among the three annotators: whereas BLEU's candidate is strongly preferred in 32 of the candidate pairs (bottom 2 rows), RYPT's candidate is strongly preferred in about double that number: 60 candidate pairs (top 2 rows).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 155, "end_pos": 159, "type": "METRIC", "confidence": 0.9792490601539612}]}, {"text": "This is quite a remarkable result, given that BLEU, by definition, selects a candidate that has significant overlap with the reference shown to the annotators to aid in their decision-making.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 46, "end_pos": 50, "type": "METRIC", "confidence": 0.9800356030464172}]}, {"text": "This means that BLEU has an inherent advantage in comparisons where both candidates are more or less of equal quality, since annotators are encouraged (in the instructions) to make a choice even if the two candidates seem of be of equal quality at first glance.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 16, "end_pos": 20, "type": "METRIC", "confidence": 0.9941561222076416}]}, {"text": "Pressed to make such a choice, the annotator is likely to select the candidate that superficially 'looks' more like the reference to be the 'better' of the two candidates.", "labels": [], "entities": []}, {"text": "That candidate will most likely be the BLEU-selected one.", "labels": [], "entities": [{"text": "BLEU-selected", "start_pos": 39, "end_pos": 52, "type": "METRIC", "confidence": 0.9909821152687073}]}, {"text": "To test this hypothesis, we repeated the experiment without showing the annotators the reference translations, and limited data collection to workers living in Germany, making judgments based only on the source sentences.", "labels": [], "entities": []}, {"text": "(We only collected one judgment per source sentence, since German workers on AMT are in short supply.)", "labels": [], "entities": [{"text": "AMT", "start_pos": 77, "end_pos": 80, "type": "DATASET", "confidence": 0.7420647144317627}]}, {"text": "As expected, the difference is even more pronounced: human judges prefer the RYPT-selected candidate 45.2% of the time, while BLEU's candidate is preferred only 29.2% of the time, with 25.6% judged to be of equal quality (right half of).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 126, "end_pos": 130, "type": "METRIC", "confidence": 0.9878024458885193}]}, {"text": "Our hypothesis is further supported by the fact that most of the gain of the \"equalquality\" category comes from BLEU, which loses 6.8 percentage points, whereas RYPT's share remains largely intact, losing less than a single percentage point.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 112, "end_pos": 116, "type": "METRIC", "confidence": 0.9969107508659363}, {"text": "RYPT's share", "start_pos": 161, "end_pos": 173, "type": "METRIC", "confidence": 0.6622260212898254}]}], "tableCaptions": [{"text": " Table 1: Ranking comparison results. The left half corresponds to the experiment (open to all workers)  where the English reference was shown, whereas the right half corresponds to the experiment (open only  to workers living in Germany) where the English reference was not shown.", "labels": [], "entities": []}, {"text": " Table 2: Ranking comparison results, grouped by sentence. This table corresponds to the left half of", "labels": [], "entities": []}, {"text": " Table 1. 3 judgments were collected for each comparison, with the \"aggregate\" for a comparison calcu- lated from these 3 judgments. For instance, an aggregate of \"RYPT +3\" means all 3 judgments favored  RYPT's choice, and \"RYPT +1\" means one more judgment favored RYPT than did BLEU.", "labels": [], "entities": [{"text": "RYPT +1\"", "start_pos": 224, "end_pos": 232, "type": "METRIC", "confidence": 0.9292531758546829}, {"text": "BLEU", "start_pos": 279, "end_pos": 283, "type": "METRIC", "confidence": 0.9889435768127441}]}]}