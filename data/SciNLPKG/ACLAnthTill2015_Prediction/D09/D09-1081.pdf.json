{"title": [{"text": "Estimating Semantic Distance Using Soft Semantic Constraints in Knowledge-Source-Corpus Hybrid Models", "labels": [], "entities": []}], "abstractContent": [{"text": "Strictly corpus-based measures of semantic distance conflate co-occurrence information pertaining to the many possible senses of target words.", "labels": [], "entities": []}, {"text": "We propose a corpus-thesaurus hybrid method that uses soft constraints to generate word-sense-aware distributional profiles (DPs) from coarser \"concept DPs\" (derived from a Roget-like thesaurus) and sense-unaware traditional word DPs (derived from raw text).", "labels": [], "entities": []}, {"text": "Although it uses a knowledge source, the method is not vocabulary-limited: if the target word is not in the thesaurus, the method falls back gracefully on the word's co-occurrence information.", "labels": [], "entities": []}, {"text": "This allows the method to access valuable information encoded in a lexical resource, such as a thesaurus, while still being able to effectively handle domain-specific terms and named entities.", "labels": [], "entities": []}, {"text": "Experiments on word-pair ranking by semantic distance show the new hybrid method to be superior to others.", "labels": [], "entities": []}], "introductionContent": [{"text": "Semantic distance is a measure of the closeness in meaning of two concepts.", "labels": [], "entities": []}, {"text": "People are consistent judges of semantic distance.", "labels": [], "entities": [{"text": "semantic distance", "start_pos": 32, "end_pos": 49, "type": "TASK", "confidence": 0.7699548900127411}]}, {"text": "For example, we can easily tell that the concepts of \"exercise\" and \"jog\" are closer in meaning than \"exercise\" and \"theater\".", "labels": [], "entities": []}, {"text": "Studies asking native speakers of a language to rank word pairs in order of semantic distance confirm this-average inter-annotator correlation on ranking word pairs in order of semantic distance has been repeatedly shown to be around 0.9 (.", "labels": [], "entities": []}, {"text": "A number of natural language tasks such as machine translation and word sense disambiguation (), can be framed as semantic distance problems.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 43, "end_pos": 62, "type": "TASK", "confidence": 0.8182645738124847}, {"text": "word sense disambiguation", "start_pos": 67, "end_pos": 92, "type": "TASK", "confidence": 0.6649153828620911}]}, {"text": "Thus, developing automatic measures that are in-line with human notions of semantic distance has received much attention.", "labels": [], "entities": []}, {"text": "These automatic approaches to semantic distance rely on manually created lexical resources such as WordNet, large amounts of text corpora, or both.", "labels": [], "entities": [{"text": "semantic distance", "start_pos": 30, "end_pos": 47, "type": "TASK", "confidence": 0.8450150191783905}, {"text": "WordNet", "start_pos": 99, "end_pos": 106, "type": "DATASET", "confidence": 0.9599282145500183}]}, {"text": "WordNet-based information content measures have been successful), but there are significant limitations on their applicability.", "labels": [], "entities": [{"text": "WordNet-based", "start_pos": 0, "end_pos": 13, "type": "DATASET", "confidence": 0.9262075424194336}]}, {"text": "They can be applied only if a WordNet exists for the language of interest (which is not the case for the \"low-density\" languages); and even if there is a WordNet, a number of domainspecific terms may not be encoded in it.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 154, "end_pos": 161, "type": "DATASET", "confidence": 0.931725800037384}]}, {"text": "On the other hand, corpus-based distributional measures of semantic distance, such as cosine and \u03b1-skew divergence (), rely on raw text alone (.", "labels": [], "entities": []}, {"text": "However, when used to rank word pairs in order of semantic distance or correct real-word spelling errors, they have been shown to perform poorly (). and argued that word sense ambiguity is a key reason for the poor performance of traditional distributional measures, and they proposed hybrid approaches that are distributional in nature, but also make use of information in lexical resources such as published thesauri and WordNet.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 423, "end_pos": 430, "type": "DATASET", "confidence": 0.9756020903587341}]}, {"text": "However, both these approaches can be applied to estimate the semantic distance between two terms only if both terms exist in the lexical resource they rely on.", "labels": [], "entities": []}, {"text": "We know lexical resources tend to have limited vocabulary and a large number of domain-specific terms are usually not included.", "labels": [], "entities": []}, {"text": "It should also be noted that similarity values from different distance measures are not comparable (even after normalization to the same scale), that is, a similarity score of .75 as per one distance measure does not correspond to the same semantic distance as a similarity score of .75 from another distance measure.", "labels": [], "entities": []}, {"text": "1 Thus if one uses two independent distance measures, in this case: one resource-reliant and one only corpus-dependent, then these two measures are not comparable (and hence cannot be used in tandem), even if both rely-partially or entirely-on distributional corpus statistics.", "labels": [], "entities": []}, {"text": "We propose a hybrid semantic distance method that inherently combines the elements of a resource-reliant measure and a strictly corpusdependent measure by imposing resource-reliant soft constraints on the corpus-dependent model.", "labels": [], "entities": []}, {"text": "We choose the method as the resource-reliant method and not one of the WordNet-based measures because, unlike the WordNet-based measures, the Mohammad and Hirst method is distributional in nature and so lends itself immediately for combination with traditional distributional similarity measures.", "labels": [], "entities": [{"text": "WordNet-based", "start_pos": 71, "end_pos": 84, "type": "DATASET", "confidence": 0.9283784031867981}]}, {"text": "Our new hybrid method combines concept-word co-occurrence information (the Mohammad and Hirst distributional profiles of thesaurus concepts (DPC)) with word-word co-occurrence information, to generate word-sense-biased distributional profiles.", "labels": [], "entities": []}, {"text": "The \"pure\" corpus-based distributional profile (a.k.a. co-occurrence vector, or word association vector), for some target word u, is biased with soft constraints towards each of the concepts c that list u in the thesaurus, to create a distributional profile that is specific to u in the sense that is most related to the other words listed under c.", "labels": [], "entities": []}, {"text": "Thus, this method can make more finegrained distinctions than the Mohammad and Hirst method, and yet uses word sense information.", "labels": [], "entities": []}, {"text": "Our proposed method falls back gracefully to rely only on word-word co-occurrence information if any of the target terms is not listed in the lexical resource.", "labels": [], "entities": []}, {"text": "Experiments on the word-pair ranking task on three different datasets show that the our proposed hybrid measure outperforms all other comparable distance measures.", "labels": [], "entities": [{"text": "word-pair ranking task", "start_pos": 19, "end_pos": 41, "type": "TASK", "confidence": 0.787454883257548}]}, {"text": "show that their method can be used to compute semantic distance in a resource poor language L 1 by combining its text with a thesaurus in a resource-rich language L 2 using an L 1 -L 2 bilingual lexicon to create cross-lingual distributional profiles of concepts, that is, L 2 word co-occurrence profiles of L 1 thesaurus concepts.", "labels": [], "entities": []}, {"text": "Since our method makes use of the Mohammad and Hirst DPCs, it can just as well make use of their cross-lingual DPCs, to compute semantic distance in a resource-poor language, just as they did.", "labels": [], "entities": []}, {"text": "We leave that for future work.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluated various methods on the task of ranking word pairs in order of semantic distance.", "labels": [], "entities": []}, {"text": "These methods included our sense-biased methods as well as several baselines: the Mohammad and Hirst (2006) DPC-based methods, the traditional word-based distributional similarity methods, and several Latent Semantic Analysis (LSA)-based methods.", "labels": [], "entities": []}, {"text": "We used three testsets and their corresponding human judgment gold standards: (1) the set of 65 noun pairs-denoted RG-65; (2) the WordSimilarity-353 () set of 353 noun pairs (which include the RG-65 pairs) of which we discarded of one repeating pair-denoted WS-353; and (3) the Resnik and Diab (2000) set of 27 verb pairs-denoted RD-00.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Spearman rank correlation on RG-65,  WS-353, and RD-00 testsets, trained on BNC.  '*' indicates the use of a smaller bootstrapped  concept-word co-occurrence matrix. 'n.p.' indi- cates that the experiment was not pursued.", "labels": [], "entities": [{"text": "Spearman rank correlation", "start_pos": 10, "end_pos": 35, "type": "METRIC", "confidence": 0.6777075131734213}, {"text": "RG-65", "start_pos": 39, "end_pos": 44, "type": "DATASET", "confidence": 0.8686302304267883}, {"text": "WS-353", "start_pos": 47, "end_pos": 53, "type": "DATASET", "confidence": 0.8426357507705688}, {"text": "RD-00 testsets", "start_pos": 59, "end_pos": 73, "type": "DATASET", "confidence": 0.8206136226654053}, {"text": "BNC", "start_pos": 86, "end_pos": 89, "type": "DATASET", "confidence": 0.9313985109329224}]}]}