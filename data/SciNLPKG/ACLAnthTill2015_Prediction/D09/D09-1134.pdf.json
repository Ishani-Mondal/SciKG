{"title": [{"text": "On the Use of Virtual Evidence in Conditional Random Fields", "labels": [], "entities": []}], "abstractContent": [{"text": "Virtual evidence (VE), first introduced by (Pearl, 1988), provides a convenient way of incorporating prior knowledge into Bayesian networks.", "labels": [], "entities": [{"text": "Virtual evidence (VE)", "start_pos": 0, "end_pos": 21, "type": "TASK", "confidence": 0.6849241495132447}]}, {"text": "This work generalizes the use of VE to undirected graph-ical models and, in particular, to conditional random fields (CRFs).", "labels": [], "entities": [{"text": "VE", "start_pos": 33, "end_pos": 35, "type": "TASK", "confidence": 0.8641749620437622}]}, {"text": "We show that VE can be naturally encoded into a CRF model as potential functions.", "labels": [], "entities": []}, {"text": "More importantly, we propose a novel semi-supervised machine learning objective for estimating a CRF model integrated with VE.", "labels": [], "entities": [{"text": "VE", "start_pos": 123, "end_pos": 125, "type": "DATASET", "confidence": 0.8105848431587219}]}, {"text": "The objective can be optimized using the Expectation-Maximization algorithm while maintaining the discriminative nature of CRFs.", "labels": [], "entities": [{"text": "Expectation-Maximization", "start_pos": 41, "end_pos": 65, "type": "METRIC", "confidence": 0.8706212639808655}]}, {"text": "When evaluated on the CLASSIFIEDS data, our approach significantly outperforms the best known solutions reported on this task.", "labels": [], "entities": [{"text": "CLASSIFIEDS data", "start_pos": 22, "end_pos": 38, "type": "DATASET", "confidence": 0.9582649171352386}]}], "introductionContent": [{"text": "Statistical approaches to sequential labeling problems rely on necessary training data to model the uncertainty of a sequence of events.", "labels": [], "entities": [{"text": "sequential labeling", "start_pos": 26, "end_pos": 45, "type": "TASK", "confidence": 0.8690341413021088}]}, {"text": "Human's prior knowledge about the task, on the other hand, often requires minimum cognitive load to specify, and yet can provide information often complementary to that offered by a limited amount of training data.", "labels": [], "entities": []}, {"text": "Whenever prior knowledge becomes available, it is desired that such information is integrated to a probabilistic model to improve learning.", "labels": [], "entities": []}, {"text": "Virtual evidence (VE), first introduced by, offers a principled and convenient way of incorporating external knowledge into Bayesian networks.", "labels": [], "entities": [{"text": "Virtual evidence (VE)", "start_pos": 0, "end_pos": 21, "type": "TASK", "confidence": 0.6608268737792968}]}, {"text": "In contrast to standard evidence (also known as observed variables), VE expresses a prior belief over values of random variables.", "labels": [], "entities": [{"text": "VE", "start_pos": 69, "end_pos": 71, "type": "METRIC", "confidence": 0.939050018787384}]}, {"text": "It has been shown that VE can significantly extend the modeling power of Bayesian networks without complicating the fundamental inference methodology.", "labels": [], "entities": [{"text": "VE", "start_pos": 23, "end_pos": 25, "type": "TASK", "confidence": 0.7565083503723145}]}, {"text": "This work extends the use of VE to undirected graphical models and, in particular, to conditional random fields (CRFs).", "labels": [], "entities": [{"text": "VE", "start_pos": 29, "end_pos": 31, "type": "TASK", "confidence": 0.8528080582618713}]}, {"text": "We show that VE can be naturally encoded into an undirected graphical model as potential functions.", "labels": [], "entities": []}, {"text": "More importantly, we discuss a semi-supervised machine learning setting for estimating CRFs with the presence of VE.", "labels": [], "entities": [{"text": "estimating CRFs", "start_pos": 76, "end_pos": 91, "type": "TASK", "confidence": 0.7437743246555328}, {"text": "VE", "start_pos": 113, "end_pos": 115, "type": "METRIC", "confidence": 0.6579754948616028}]}, {"text": "As the conditional likelihood objective of CRFs is not directly maximizable with respect to unlabeled data, we propose a novel semisupervised learning objective that can be optimized using the Expectation-Maximization (EM) algorithm while maintaining the discriminative nature of CRFs.", "labels": [], "entities": []}, {"text": "We apply our model to the CLASSIFIEDS data ().", "labels": [], "entities": [{"text": "CLASSIFIEDS data", "start_pos": 26, "end_pos": 42, "type": "DATASET", "confidence": 0.8985266089439392}]}, {"text": "Specifically, we use VE to incorporate into a CRF model two types of prior knowledge specified in previous works.", "labels": [], "entities": []}, {"text": "The first is defined based on the notion of prototypes, i.e., example words fora given label; and the other assumes that adjacent tokens tend to have the same label.", "labels": [], "entities": []}, {"text": "When unlabeled data becomes available, we further extend the sparse prototype information to other words based on distributional similarity.", "labels": [], "entities": []}, {"text": "This results in so-called collocation lists, each consisting of a relatively large number of noisy \"prototypes\" fora label.", "labels": [], "entities": []}, {"text": "Given the fact that these noisy prototypes are often located close to each other in an input sequence, we create anew type of VE based on word collocation to reduce ambiguity.", "labels": [], "entities": []}, {"text": "We compare our CRF model integrated with VE with two state-of-the-art models, i.e., constraintdriven learning and generalized expectation criteria (.", "labels": [], "entities": [{"text": "VE", "start_pos": 41, "end_pos": 43, "type": "DATASET", "confidence": 0.8584899306297302}]}, {"text": "Experiments show that our approach leads to sequential labeling accuracies superior to the best results reported on this task in both supervised and semi-supervised learning.", "labels": [], "entities": []}], "datasetContent": [{"text": "We use the CLASSIFIEDS data provided by and compare with results reported by CRR07 () and MM08 () for both supervised and semi-supervised learning.", "labels": [], "entities": [{"text": "CLASSIFIEDS data", "start_pos": 11, "end_pos": 27, "type": "DATASET", "confidence": 0.7657240033149719}, {"text": "CRR07", "start_pos": 77, "end_pos": 82, "type": "DATASET", "confidence": 0.9572292566299438}, {"text": "MM08", "start_pos": 90, "end_pos": 94, "type": "DATASET", "confidence": 0.914405882358551}]}, {"text": "Following all previous works conducted on this task, we tokenized both words and punctuations, and created a number of regular expression tokens for phone numbers, email addresses, URLs, dates, money amounts and soon.", "labels": [], "entities": []}, {"text": "However, we did not tokenize newline breaks, as CRR07 did, which might be useful in determining sentence boundaries.", "labels": [], "entities": [{"text": "tokenize newline breaks", "start_pos": 20, "end_pos": 43, "type": "TASK", "confidence": 0.8229196667671204}, {"text": "CRR07", "start_pos": 48, "end_pos": 53, "type": "DATASET", "confidence": 0.9615382552146912}, {"text": "determining sentence boundaries", "start_pos": 84, "end_pos": 115, "type": "TASK", "confidence": 0.756589670976003}]}, {"text": "Based on such tokenization, we extract n-grams, n = 1, 2, 3, from the corpus as features for CRFs.", "labels": [], "entities": []}, {"text": "As described in Section 3, we integrate the prior knowledge K1 and K2 in our CRF model.", "labels": [], "entities": []}, {"text": "The prototypes that represent K1 are given in.", "labels": [], "entities": []}, {"text": "CRR07 used the same two kinds of prior knowledge in the form of constraints, and they implemented another constraint on the minimum number of words in afield chunk.", "labels": [], "entities": [{"text": "CRR07", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.979415774345398}]}, {"text": "MM08 used almost the same set of prototypes as labeled features, but they exploited two sets of 33 additional features for some experiments.", "labels": [], "entities": [{"text": "MM08", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.9607555270195007}]}, {"text": "In this regard, the comparison between CRR07, MM08 and the method presented here cannot be exact.", "labels": [], "entities": [{"text": "CRR07", "start_pos": 39, "end_pos": 44, "type": "DATASET", "confidence": 0.8676857352256775}, {"text": "MM08", "start_pos": 46, "end_pos": 50, "type": "DATASET", "confidence": 0.8952166438102722}]}, {"text": "However, we show that while our prior knowledge is no more than that used in previous works, our approach is able: Token-level accuracy of supervised learning methods; \"+ VE\" refers to the cases where both kinds of prior knowledge, K1 and K2, are incorporated as VE in the CRF model.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 127, "end_pos": 135, "type": "METRIC", "confidence": 0.974127471446991}, {"text": "VE", "start_pos": 171, "end_pos": 173, "type": "METRIC", "confidence": 0.9763175249099731}]}, {"text": "to achieve the state-of-art performance.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Token-level accuracy of supervised learn- ing methods; \"+ VE\" refers to the cases where  both kinds of prior knowledge, K1 and K2, are in- corporated as VE in the CRF model.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 22, "end_pos": 30, "type": "METRIC", "confidence": 0.9803411960601807}, {"text": "VE", "start_pos": 68, "end_pos": 70, "type": "METRIC", "confidence": 0.9823309183120728}]}, {"text": " Table 3: Token-level accuracy of semi-supervised  learning methods. \"+ Col-VE\" refers to cases  where collocation-based VE is integrated in the  CRF model in addition to the VE representing K1  and K2.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 22, "end_pos": 30, "type": "METRIC", "confidence": 0.985612154006958}]}]}