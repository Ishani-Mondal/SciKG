{"title": [], "abstractContent": [{"text": "The recently introduced online confidence-weighted (CW) learning algorithm for binary classification performs well on many binary NLP tasks.", "labels": [], "entities": [{"text": "binary classification", "start_pos": 79, "end_pos": 100, "type": "TASK", "confidence": 0.7477690577507019}]}, {"text": "However, for multi-class problems CW learning updates and inference cannot be computed analytically or solved as convex optimization problems as they are in the binary case.", "labels": [], "entities": [{"text": "CW learning updates", "start_pos": 34, "end_pos": 53, "type": "TASK", "confidence": 0.7366889516512553}]}, {"text": "We derive learning algorithms for the multi-class CW setting and provide extensive evaluation using nine NLP datasets, including three derived from the recently released New York Times corpus.", "labels": [], "entities": [{"text": "NLP datasets", "start_pos": 105, "end_pos": 117, "type": "DATASET", "confidence": 0.7790849506855011}, {"text": "New York Times corpus", "start_pos": 170, "end_pos": 191, "type": "DATASET", "confidence": 0.716722846031189}]}, {"text": "Our best algorithm out-performs state-of-the-art online and batch methods on eight of the nine tasks.", "labels": [], "entities": []}, {"text": "We also show that the confidence information maintained during learning yields useful probabilistic information attest time.", "labels": [], "entities": []}], "introductionContent": [{"text": "Online learning algorithms such as the Perceptron process one example at a time, yielding simple and fast updates.", "labels": [], "entities": []}, {"text": "They generally make few statistical assumptions about the data and are often used for natural language problems, where high dimensional feature representations, e.g., bags-of-words, demand efficiency.", "labels": [], "entities": []}, {"text": "Most online algorithms, however, do not take into account the unique properties of such data, where many features are extremely rare and a few are very frequent.", "labels": [], "entities": []}, {"text": "Dredze, Crammer and Pereira () recently introduced confidence weighted (CW) online learning for binary prediction problems.", "labels": [], "entities": [{"text": "binary prediction", "start_pos": 96, "end_pos": 113, "type": "TASK", "confidence": 0.6677860915660858}]}, {"text": "CW learning explicitly models classifier weight uncertainty using a multivariate Gaussian distribution overweight vectors.", "labels": [], "entities": [{"text": "CW learning", "start_pos": 0, "end_pos": 11, "type": "TASK", "confidence": 0.8280735909938812}]}, {"text": "The learner makes online updates based on its confidence in the current parameters, making larger changes in the weights of infrequently observed features.", "labels": [], "entities": []}, {"text": "Empirical evaluation has demonstrated the advantages of this approach fora number of binary natural language processing (NLP) problems.", "labels": [], "entities": []}, {"text": "In this work, we develop and test multi-class confidence weighted online learning algorithms.", "labels": [], "entities": []}, {"text": "For binary problems, the update rule is a simple convex optimization problem and inference is analytically computable.", "labels": [], "entities": []}, {"text": "However, neither is true in the multi-class setting.", "labels": [], "entities": []}, {"text": "We discuss several efficient online learning updates.", "labels": [], "entities": []}, {"text": "These update rules can involve one, some, or all of the competing (incorrect) labels.", "labels": [], "entities": []}, {"text": "We then perform an extensive evaluation of our algorithms using nine multi-class NLP classification problems, including three derived from the recently released New York Times corpus.", "labels": [], "entities": [{"text": "NLP classification", "start_pos": 81, "end_pos": 99, "type": "TASK", "confidence": 0.7242371588945389}, {"text": "New York Times corpus", "start_pos": 161, "end_pos": 182, "type": "DATASET", "confidence": 0.679044134914875}]}, {"text": "To the best of our knowledge, this is the first learning evaluation on these data.", "labels": [], "entities": []}, {"text": "Our best algorithm outperforms state-of-the-art online algorithms and batch algorithms on eight of the nine datasets.", "labels": [], "entities": []}, {"text": "Surprisingly, we find that a simple algorithm in which updates consider only a single competing label often performs as well as or better than multiconstraint variants if it makes multiple passes over the data.", "labels": [], "entities": []}, {"text": "This is especially promising for large datasets, where the efficiency of the update can be important.", "labels": [], "entities": []}, {"text": "In the true online setting, where only one iteration is possible, multi-constraint algorithms yield better performance.", "labels": [], "entities": []}, {"text": "Finally, we demonstrate that the label distributions induced by the Gaussian parameter distributions resulting from our methods have interesting properties, such as higher entropy, compared to those from maximum entropy models.", "labels": [], "entities": []}, {"text": "Improved label distributions maybe useful in a variety of learning settings.", "labels": [], "entities": []}], "datasetContent": [{"text": "Following the approach of, we evaluate using five natural language classification tasks over nine datasets that vary in difficulty, size, and label/feature counts.", "labels": [], "entities": [{"text": "natural language classification", "start_pos": 50, "end_pos": 81, "type": "TASK", "confidence": 0.7284085353215536}]}, {"text": "See  Enron Automatic sorting of emails into folders.", "labels": [], "entities": [{"text": "sorting of emails into folders", "start_pos": 21, "end_pos": 51, "type": "TASK", "confidence": 0.7635775208473206}]}, {"text": "We selected two users with many email folders and messages: farmer-d (Enron A) and kaminski-v (Enron B).", "labels": [], "entities": []}, {"text": "We used the ten largest folders for each user, excluding non-archival email folders such as \"inbox,\" \"deleted items,\" and \"discussion threads.\"", "labels": [], "entities": []}, {"text": "Emails were represented as binary bags-of-words with stop-words removed.", "labels": [], "entities": []}, {"text": "NY Times To the best of our knowledge we are the first to evaluate machine learning methods on the New York Times corpus.", "labels": [], "entities": [{"text": "NY Times", "start_pos": 0, "end_pos": 8, "type": "DATASET", "confidence": 0.9772394299507141}, {"text": "New York Times corpus", "start_pos": 99, "end_pos": 120, "type": "DATASET", "confidence": 0.8933424055576324}]}, {"text": "The corpus contains 1.8 million articles that appeared from 1987 to 2007.", "labels": [], "entities": []}, {"text": "In addition to being one of the largest collections of raw news text, it is possibly the largest collection of publicly released annotated news text, and therefore an ideal corpus for large scale NLP tasks.", "labels": [], "entities": []}, {"text": "Among other annotations, each article is labeled with the desk that produced the story (Financial, Sports, etc.)", "labels": [], "entities": []}, {"text": "(NYTD), the online section to which the article was: A comparison of k = \u221e updates.", "labels": [], "entities": [{"text": "NYTD)", "start_pos": 1, "end_pos": 6, "type": "DATASET", "confidence": 0.9546214640140533}]}, {"text": "While the two approximations (sequential and parallel) are roughly the same, the exact solution over-fits.", "labels": [], "entities": []}, {"text": "posted (NYTO), and the section in which the article was printed (NYTS).", "labels": [], "entities": [{"text": "NYTO", "start_pos": 8, "end_pos": 12, "type": "DATASET", "confidence": 0.750160276889801}, {"text": "NYTS", "start_pos": 65, "end_pos": 69, "type": "DATASET", "confidence": 0.7881690859794617}]}, {"text": "Articles were represented as bags-of-words with feature counts (stop-words removed).", "labels": [], "entities": []}, {"text": "Reuters Over 800,000 manually categorized newswire stories (RCV1-v2/ LYRL2004).", "labels": [], "entities": [{"text": "Reuters", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.9691404104232788}, {"text": "RCV1-v2/ LYRL2004", "start_pos": 60, "end_pos": 77, "type": "DATASET", "confidence": 0.7668272852897644}]}, {"text": "Each article contains one or more labels describing its general topic, industry, and region.", "labels": [], "entities": []}, {"text": "We performed topic classification with the four general topics: corporate, economic, government, and markets.", "labels": [], "entities": [{"text": "topic classification", "start_pos": 13, "end_pos": 33, "type": "TASK", "confidence": 0.718194767832756}]}, {"text": "Details on document preparation and feature extraction are given by.", "labels": [], "entities": [{"text": "document preparation", "start_pos": 11, "end_pos": 31, "type": "TASK", "confidence": 0.7546752393245697}, {"text": "feature extraction", "start_pos": 36, "end_pos": 54, "type": "TASK", "confidence": 0.7018261253833771}]}, {"text": "We first set out to compare the three update approaches proposed in Sec.", "labels": [], "entities": []}, {"text": "5.2: an exact solution and two approximations (sequential and parallel).", "labels": [], "entities": []}, {"text": "Results show that the two approximations perform similarly.", "labels": [], "entities": []}, {"text": "For every experiment the CW parameter \u03b7 and the number of iterations (up to 10) were optimized using a single randomized iteration.", "labels": [], "entities": []}, {"text": "However, sequential converges faster, needing an average of 4.33 iterations compared to 7.56 for parallel across all datasets.", "labels": [], "entities": []}, {"text": "Therefore, we select sequential for our subsequent experiments.", "labels": [], "entities": []}, {"text": "The exact method performs poorly, displaying the lowest performance on almost every dataset.", "labels": [], "entities": []}, {"text": "This is unsurprising given similar results for binary CW learning, where exact updates were shown to over-fit but converged after a single iteration of training.", "labels": [], "entities": [{"text": "CW learning", "start_pos": 54, "end_pos": 65, "type": "TASK", "confidence": 0.7443362772464752}]}, {"text": "Similarly, our exact implementation converges after an average of 1.25 iterations, much faster than either of the approximations.", "labels": [], "entities": []}, {"text": "However, this rapid convergence appears to come at the expense of accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 66, "end_pos": 74, "type": "METRIC", "confidence": 0.998388409614563}]}, {"text": "shows the accuracy on Amazon 7 test data after each training iteration.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9997451901435852}, {"text": "Amazon 7 test data", "start_pos": 22, "end_pos": 40, "type": "DATASET", "confidence": 0.9767438769340515}]}, {"text": "While both sequential and parallel improve with several iterations, exact de- grades after the first iteration, suggesting that it may over-fit to the training data.", "labels": [], "entities": []}, {"text": "The approximations appear to smooth learning and produce better performance in the long run.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: A summary of the nine datasets, includ- ing the number of instances, features, and labels,  and whether the numbers of examples in each class  are balanced.", "labels": [], "entities": []}, {"text": " Table 2: A comparison of k = \u221e updates. While  the two approximations (sequential and parallel)  are roughly the same, the exact solution over-fits.", "labels": [], "entities": []}]}