{"title": [{"text": "Quantifier Scope Disambiguation Using Extracted Pragmatic Knowledge: Preliminary Results", "labels": [], "entities": [{"text": "Quantifier Scope Disambiguation", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.8857076366742452}]}], "abstractContent": [{"text": "It is well known that pragmatic knowledge is useful and necessary in many difficult language processing tasks, but because this knowledge is difficult to acquire and process automatically, it is rarely used.", "labels": [], "entities": []}, {"text": "We present an open information extraction technique for automatically extracting a particular kind of pragmatic knowledge from text, and we show how to integrate the knowledge into a Markov Logic Network model for quantifier scope disam-biguation.", "labels": [], "entities": [{"text": "quantifier scope disam-biguation", "start_pos": 214, "end_pos": 246, "type": "TASK", "confidence": 0.6366903781890869}]}, {"text": "Our model improves quantifier scope judgments in experiments.", "labels": [], "entities": [{"text": "quantifier scope judgments", "start_pos": 19, "end_pos": 45, "type": "TASK", "confidence": 0.8189347187678019}]}], "introductionContent": [{"text": "It has long been a goal of the natural language processing (NLP) community to be able to interpret language utterances into logical representations of their meaning.", "labels": [], "entities": [{"text": "interpret language utterances into logical representations of their meaning", "start_pos": 89, "end_pos": 164, "type": "TASK", "confidence": 0.7477119829919603}]}, {"text": "Quantifier scope ambiguity has been recognized as one particularly challenging aspect of this problem.", "labels": [], "entities": [{"text": "Quantifier scope ambiguity", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.8652534683545431}]}, {"text": "For example, the following sentence has two possible readings, depending on the scope of its quantifiers: Every boy wants a dog.", "labels": [], "entities": []}, {"text": "One reading of this sentence is that there exists a single dog in the world which all boys want.", "labels": [], "entities": []}, {"text": "The second, and usually preferred, reading is that the sentence is describing a separate \"wanting\" relation for each boy, and that the dog in question is a function of the boy who wants it.", "labels": [], "entities": []}, {"text": "In this reading, there maybe as many different dogs as boys, although it leaves open the possibility that several of the boys want the same dog.", "labels": [], "entities": []}, {"text": "In logic, these two readings can be represented as follows: The readings differ only in the order of the quantifiers.", "labels": [], "entities": []}, {"text": "The quantifier that comes first in each expression is said to have wide scope; the second quantifier has narrow scope.", "labels": [], "entities": []}, {"text": "Linguists and NLP researchers have come up with several theories and mechanisms for automatically determining the scope of quantified linguistic expressions.", "labels": [], "entities": []}, {"text": "Despite along history of proposed solutions, however, researchers have for the most part abandoned this task as hopeless because of \"overwhelming evidence suggesting that quantifier scope is a phenomenon that must be treated at the pragmatic level\").", "labels": [], "entities": []}, {"text": "For example, in active voice clauses, the quantifier for the subject noun is usually preferred for wide scope over the quantifier of the predicate noun (.", "labels": [], "entities": []}, {"text": "But such preferences can easily be overruled by world knowledge: A doctor lives in every city.", "labels": [], "entities": []}, {"text": "1. \u2203 d\u2208Docs \u2200 c\u2208Cities lives in(d, c) (A single doctor lives in all cities.)", "labels": [], "entities": []}, {"text": "2. \u2200 c\u2208Cities \u2203 d\u2208Docs lives in (Each city has a different doctor living there.)", "labels": [], "entities": []}, {"text": "Syntactic preferences would normally indicate that reading 1 is better, but in this particular case common-sense knowledge of the world overrules that preference and makes reading 2 far more probable.", "labels": [], "entities": []}, {"text": "Open-domain pragmatic knowledge is usually not available to language processing systems, but that is beginning to change.", "labels": [], "entities": []}, {"text": "Recent research in open information extraction ( has shown that we can extract large amounts of relational data from open-domain text with high accuracy.", "labels": [], "entities": [{"text": "open information extraction", "start_pos": 19, "end_pos": 46, "type": "TASK", "confidence": 0.6725576122601827}, {"text": "accuracy", "start_pos": 144, "end_pos": 152, "type": "METRIC", "confidence": 0.99367356300354}]}, {"text": "Here, we show how we can connect the two fields, by extracting a targeted form of pragmatic knowledge for use in quantifier scope disambiguation.", "labels": [], "entities": [{"text": "quantifier scope disambiguation", "start_pos": 113, "end_pos": 144, "type": "TASK", "confidence": 0.7286178867022196}]}, {"text": "Our contributions are: 1) We build an extraction mechanism for extracting pragmatic knowledge about relations.", "labels": [], "entities": []}, {"text": "In par-ticular, we extract knowledge about the expected sizes of the sets of objects that participate in the relations.", "labels": [], "entities": []}, {"text": "The task of identifying functional relationships is a subtask of our extraction problem that has received recent attention in the literature ().", "labels": [], "entities": []}, {"text": "2) We devise a novel probabilistic model in the Markov Logic Network framework for reasoning over possible readings of sentences that involve quantifier scope ambiguities.", "labels": [], "entities": []}, {"text": "The model is able to assign a probability that a particular reading is plausible, given the pragmatic knowledge we extract.", "labels": [], "entities": []}, {"text": "3) We provide an empirical demonstration that our system is able to resolve quantifier scope ambiguities in cases where the syntactic and lexical features used by previous systems are of no help.", "labels": [], "entities": []}, {"text": "The remainder of this paper is organized as follows.", "labels": [], "entities": []}, {"text": "The next section describes previous work.", "labels": [], "entities": []}, {"text": "Section 3 shows how the problem can be formulated as a task of assigning probabilities to possible worlds, and that the crucial difference between them has to do with the number of objects participating in individual relationships.", "labels": [], "entities": []}, {"text": "Section 4 discusses our techniques for extracting the pragmatic knowledge that allows us to make judgments about quantifier scope.", "labels": [], "entities": []}, {"text": "Section 5 presents our probabilistic model for resolving scope ambiguities.", "labels": [], "entities": [{"text": "resolving scope ambiguities", "start_pos": 47, "end_pos": 74, "type": "TASK", "confidence": 0.8422521750132242}]}, {"text": "We present an empirical study in section 6, and section 7 concludes and suggests items for future work.", "labels": [], "entities": []}], "datasetContent": [{"text": "We report on two sets of experiments.", "labels": [], "entities": []}, {"text": "The first tests our extraction technique on its own, and the second tests the accuracy of our complete QSD system, including the extraction mechanisms and the prediction model, on a quantifier scope disambiguation task.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 78, "end_pos": 86, "type": "METRIC", "confidence": 0.9992417097091675}, {"text": "quantifier scope disambiguation task", "start_pos": 182, "end_pos": 218, "type": "TASK", "confidence": 0.6684845462441444}]}, {"text": "Function detection is an important task in its own right, and has been used in several previous applications (.", "labels": [], "entities": [{"text": "Function detection", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.911509096622467}]}, {"text": "To turn our extraction system into a classifier for functions vs. nonfunctions, we simply checked whether there were  any extractions for R with F n2 > 1.", "labels": [], "entities": []}, {"text": "If so, we predicted that the R was nonfunctional, and otherwise we predicted it was functional.", "labels": [], "entities": []}, {"text": "We used the Web1Tgram Corpus of n-grams provided by Google, Inc to extract classes, relations, and counts.", "labels": [], "entities": [{"text": "Web1Tgram Corpus of n-grams", "start_pos": 12, "end_pos": 39, "type": "DATASET", "confidence": 0.9240262806415558}]}, {"text": "This corpus contains counts for 2-through 5-grams that appear on the Web pages indexed by Google.", "labels": [], "entities": []}, {"text": "Counts are included in this data set for all n-grams that appeared at least 40 times in their text.", "labels": [], "entities": []}, {"text": "We ran our extraction techniques on the 3-, 4-and 5-grams.", "labels": [], "entities": []}, {"text": "To create a test set, we sampled a set of 200 relations from our extractions, removed any relations that consisted of punctuations, stopwords, or other non-relational items.", "labels": [], "entities": []}, {"text": "We then manually labeled the remainder as functions or non-functions.", "labels": [], "entities": []}, {"text": "A baseline system that simply predicts the majority class (nonfunctions) on this data set would achieve an accuracy of 56%, well below the 81% accuracy of our classifier.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 107, "end_pos": 115, "type": "METRIC", "confidence": 0.9994279742240906}, {"text": "accuracy", "start_pos": 143, "end_pos": 151, "type": "METRIC", "confidence": 0.9644594788551331}]}, {"text": "Many of the relations in our test set, like built(Person, House) and is riding(Person,Animal), do not ordinarily have named-entity extractions for both arguments, and would therefore not be amenable to previous function detection approaches.", "labels": [], "entities": [{"text": "function detection", "start_pos": 211, "end_pos": 229, "type": "TASK", "confidence": 0.7697193622589111}]}, {"text": "Some of our technique's errors highlight interesting difficulties with function detection.", "labels": [], "entities": [{"text": "function detection", "start_pos": 71, "end_pos": 89, "type": "TASK", "confidence": 0.8477981984615326}]}, {"text": "For instance, while we labeled the is capital of relation as a function, our technique predicted that it was not.", "labels": [], "entities": []}, {"text": "It turns out that the country of Bolivia has two capitals, and the South Asian region of Jammu and Kashmir also has two capitals.", "labels": [], "entities": []}, {"text": "Both of these facts are prominent enough on the Web to cause our system to detect a small probability for P Right capital of (2).", "labels": [], "entities": [{"text": "P Right capital", "start_pos": 106, "end_pos": 121, "type": "METRIC", "confidence": 0.9259854753812155}]}, {"text": "Thus any label for this relation is somewhat unsatisfying: it is almost entirely functional, but not strictly so.", "labels": [], "entities": []}, {"text": "By generalizing the problem to one of determining a distribution for the size of the argument, we can handle these border cases in a useful way for QSD, as discussed below.", "labels": [], "entities": []}, {"text": "We test our complete QSD system on two important tasks.", "labels": [], "entities": [{"text": "QSD", "start_pos": 21, "end_pos": 24, "type": "TASK", "confidence": 0.8168225884437561}]}, {"text": "In the first, the system is presented with a series of QSD problems Q in which the first quantifier Q q1 is always \"a,\" and the second (Q q2 ) is always \"every.\"", "labels": [], "entities": []}, {"text": "Each example is manually labeled to indicate whether a director indirect reading of the sentence is preferred, and the system is charged with predicting the preferred reading.", "labels": [], "entities": []}, {"text": "In the second task, each Q has \"every\" as the first quantifier, and \"a\" as the second quantifier.", "labels": [], "entities": []}, {"text": "Since indirect readings are very rarely preferred for active-voice sentences of this form,we charge the system with making a different type of prediction: determine whether the indirect reading is plausible or not.", "labels": [], "entities": []}, {"text": "The system assumes that every sentence has a plausible direct reading, but by determining whether the indirect reading is plausible, it can determine whether the sentence is ambiguous between the two readings.", "labels": [], "entities": []}, {"text": "We created data sets for these tasks by sampling our 5grams for examples containing the relations in our function experiment.", "labels": [], "entities": []}, {"text": "From this set, we selected phrases that involved named classes for the arguments to the relation.", "labels": [], "entities": []}, {"text": "When a class was missing, we either manually supplied one, or discarded the example.", "labels": [], "entities": []}, {"text": "We then constructed two examples from each combination of relation and argument classes: one example in which the first argument is constrained by the quantifier \"a\" and the second by \"every,\" and a second example in which the quantifiers are reversed.", "labels": [], "entities": []}, {"text": "Finally, we manually labeled every example with a preference for director indirect reading (in the case of \"a/every\" examples) or with a plausibility judgment for the indirect reading (in the case of \"every/a\" examples).", "labels": [], "entities": []}, {"text": "Our final test sets included 46 labeled examples for each task.", "labels": [], "entities": []}, {"text": "Further experiments involving multiple annotators, as in the experiments of, are of course desirable, but note that even their experiments included just 32 labeled examples.", "labels": [], "entities": []}, {"text": "shows our results for the first QSD task, and shows our results for the second one.", "labels": [], "entities": [{"text": "QSD task", "start_pos": 32, "end_pos": 40, "type": "TASK", "confidence": 0.7968298494815826}]}, {"text": "In each case, we compare our supervised Corrected MLN model against an Uncorrected MLN model that uses no supervised data, and simply takes its weights straight from our extracted distributions.", "labels": [], "entities": []}, {"text": "The supervised model uses a training corpus of 10 manually labeled examples for each task, five from each class.", "labels": [], "entities": []}, {"text": "We also compare against a majority class baseline.", "labels": [], "entities": []}, {"text": "Note that the Corrected   Our results demonstrate the utility of our extracted distributions for these difficult tasks.", "labels": [], "entities": []}, {"text": "Although the extracted data prevents us from determining that is capital of should be classified as a function, since almost all of the probability mass in P Right is still on n \u2208 {0, 1}.", "labels": [], "entities": []}, {"text": "Thus, the probability for the direct reading of a sentence like \"Some city is the capital of every country\" is still very low.", "labels": [], "entities": []}, {"text": "Likewise, even though our system (correctly) determines that the relation is a parent of is non-functional, it does not therefore group it with other nonfunctional relations like visited.", "labels": [], "entities": []}, {"text": "The distribution P Right is parent of (n) is skewed to much smaller numbers for n than is the distribution for visited, and thus the indirect reading for \"A person is the parent of every child\" is much more likely than the indirect reading of \"A person visited every country.\"", "labels": [], "entities": []}, {"text": "The biggest hurdle for better performance is noise in our extraction technique.", "labels": [], "entities": []}, {"text": "Polysemous relations sometimes have large counts for large argument sizes in one sense, but not another.", "labels": [], "entities": []}, {"text": "Using argument classes to disambiguate relations can help, but extractions for relations in combination with argument classes are much more sparse.", "labels": [], "entities": []}, {"text": "Improved extraction techniques could directly impact performance on the QSD task.", "labels": [], "entities": [{"text": "QSD task", "start_pos": 72, "end_pos": 80, "type": "TASK", "confidence": 0.8095250725746155}]}], "tableCaptions": [{"text": " Table 1: Numeric phrases used in our extraction pat-", "labels": [], "entities": []}, {"text": " Table 3: Precision and recall for detecting functions us-", "labels": [], "entities": [{"text": "Precision", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9973594546318054}, {"text": "recall", "start_pos": 24, "end_pos": 30, "type": "METRIC", "confidence": 0.9988248944282532}]}, {"text": " Table 4: Our trained MLN outperforms two other sys-", "labels": [], "entities": []}, {"text": " Table 5: Our trained MLN outperforms two other sys-", "labels": [], "entities": []}]}