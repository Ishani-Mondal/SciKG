{"title": [{"text": "Discriminative Corpus Weight Estimation for Machine Translation", "labels": [], "entities": [{"text": "Discriminative Corpus Weight Estimation", "start_pos": 0, "end_pos": 39, "type": "TASK", "confidence": 0.5687931329011917}, {"text": "Machine Translation", "start_pos": 44, "end_pos": 63, "type": "TASK", "confidence": 0.797084242105484}]}], "abstractContent": [{"text": "Current statistical machine translation (SMT) systems are trained on sentence-aligned and word-aligned parallel text collected from various sources.", "labels": [], "entities": [{"text": "statistical machine translation (SMT)", "start_pos": 8, "end_pos": 45, "type": "TASK", "confidence": 0.7793400237957636}]}, {"text": "Translation model parameters are estimated from the word alignments, and the quality of the translations on a given test set depends on the parameter estimates.", "labels": [], "entities": []}, {"text": "There are at least two factors affecting the parameter estimation: domain match and training data quality.", "labels": [], "entities": []}, {"text": "This paper describes a novel approach for automatically detecting and down-weighing certain parts of the training corpus by assigning a weight to each sentence in the training bitext so as to optimize a discriminative objective function on a designated tuning set.", "labels": [], "entities": []}, {"text": "This way, the proposed method can limit the negative effects of low quality training data, and can adapt the translation model to the domain of interest.", "labels": [], "entities": []}, {"text": "It is shown that such discrim-inative corpus weights can provide significant improvements in Arabic-English translation on various conditions, using a state-of-the-art SMT system.", "labels": [], "entities": [{"text": "SMT", "start_pos": 168, "end_pos": 171, "type": "TASK", "confidence": 0.9890118837356567}]}], "introductionContent": [{"text": "Statistical machine translation (SMT) systems rely on a training corpus consisting of sentences in the source language and their respective reference translations to the target language.", "labels": [], "entities": [{"text": "Statistical machine translation (SMT)", "start_pos": 0, "end_pos": 37, "type": "TASK", "confidence": 0.8181317200263342}]}, {"text": "These parallel sentences are used to perform automatic word alignment, and extract translation rules with associated probabilities.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 55, "end_pos": 69, "type": "TASK", "confidence": 0.6956068575382233}]}, {"text": "Typically, a parallel training corpus is comprised of collections of varying quality and relevance to the translation problem of interest.", "labels": [], "entities": [{"text": "translation problem", "start_pos": 106, "end_pos": 125, "type": "TASK", "confidence": 0.9226506650447845}]}, {"text": "For example, an SMT system applied to broadcast conversational data maybe trained on a corpus consisting mostly of United Nations and newswire data, with only a very small amount of in-domain broadcast news/conversational data.", "labels": [], "entities": [{"text": "SMT", "start_pos": 16, "end_pos": 19, "type": "TASK", "confidence": 0.9935611486434937}, {"text": "United Nations and newswire data", "start_pos": 115, "end_pos": 147, "type": "DATASET", "confidence": 0.7126905083656311}]}, {"text": "In this case, it would be desirable to down-weigh the out-of-domain data relative to the in-domain data during the rule extraction and probability estimation.", "labels": [], "entities": [{"text": "rule extraction", "start_pos": 115, "end_pos": 130, "type": "TASK", "confidence": 0.8400731384754181}, {"text": "probability estimation", "start_pos": 135, "end_pos": 157, "type": "TASK", "confidence": 0.6565503180027008}]}, {"text": "Similarly, it would be good to assign a lower weight to data of low quality (e.g., poorly aligned or incorrectly translated sentences) relative to data of high quality.", "labels": [], "entities": []}, {"text": "In this paper, we describe a novel discriminative training method that can be used to estimate a weight for each sentence in the training bitext so as to optimize an objective function -expected translation edit rate (TER)) -on a held-out development set.", "labels": [], "entities": [{"text": "expected translation edit rate (TER))", "start_pos": 186, "end_pos": 223, "type": "METRIC", "confidence": 0.804794362613133}]}, {"text": "The training bitext typically consists of millions of (parallel) sentences, so in order to ensure robust estimation we express each sentence weight as a function of sentencelevel features, and estimate the parameters of this mapping function instead.", "labels": [], "entities": []}, {"text": "Sentence-level features may include the identifier of the collection or genre that the sentence belongs to, the number of tokens in the source or target side, alignment information, etc.", "labels": [], "entities": []}, {"text": "The mapping from features to weights can be implemented via any differentiable function, but in our experiments we used a simple perceptron.", "labels": [], "entities": []}, {"text": "Sentence weights estimated in this fashion are applied directly to the phrase and lexical counts unlike any previously published method to the author's knowledge.", "labels": [], "entities": []}, {"text": "The tuning framework is developed for phrase-based SMT models, but the tuned weights are also applicable to the training of a hierarchical model.", "labels": [], "entities": [{"text": "SMT", "start_pos": 51, "end_pos": 54, "type": "TASK", "confidence": 0.8834936618804932}]}, {"text": "In cases where the tuning set used for corpus weight estimation is a close match to the test set, this method yields significant gains in TER, BLEU), and ME-TEOR () scores over a state-of-the-art hierarchical baseline.", "labels": [], "entities": [{"text": "corpus weight estimation", "start_pos": 39, "end_pos": 63, "type": "TASK", "confidence": 0.6338249742984772}, {"text": "TER", "start_pos": 138, "end_pos": 141, "type": "METRIC", "confidence": 0.9991156458854675}, {"text": "BLEU", "start_pos": 143, "end_pos": 147, "type": "METRIC", "confidence": 0.9533215165138245}, {"text": "ME-TEOR", "start_pos": 154, "end_pos": 161, "type": "METRIC", "confidence": 0.991969883441925}]}, {"text": "The paper is organized as follows.", "labels": [], "entities": []}, {"text": "Related work on data selection, data weighting, and model adaptation is presented in Section 2.", "labels": [], "entities": [{"text": "data selection", "start_pos": 16, "end_pos": 30, "type": "TASK", "confidence": 0.7600285112857819}, {"text": "data weighting", "start_pos": 32, "end_pos": 46, "type": "TASK", "confidence": 0.8205297887325287}, {"text": "model adaptation", "start_pos": 52, "end_pos": 68, "type": "TASK", "confidence": 0.7428772747516632}]}, {"text": "The corpus weight approach and estimation algorithm are described in Section 3.", "labels": [], "entities": [{"text": "estimation", "start_pos": 31, "end_pos": 41, "type": "TASK", "confidence": 0.9116254448890686}]}, {"text": "Experimental evaluation of the approach is presented in Sections 4 and 5.", "labels": [], "entities": []}, {"text": "Section 6 concludes the paper with a few directions for future work.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section we describe the setup that was used for all experiments reported in this paper.", "labels": [], "entities": []}, {"text": "Specifically, we provide details about the training data, development sets, and MT systems (phrase-based and hierarchical).", "labels": [], "entities": [{"text": "MT", "start_pos": 80, "end_pos": 82, "type": "TASK", "confidence": 0.9779194593429565}]}], "tableCaptions": [{"text": " Table 1. The \"Sakhr\" newswire  collection is a set of Arabic-to-English and  English-to-Arabic data provided by Sakhr Soft- ware, totaling about 30.8 million tokens, and  is only available to research teams participat- ing in the Defense Advanced Research Projects  Agency (DARPA) Global Autonomous Language  Exploitation (GALE) program. The \"LDC Giga- word (ISI)\" collection was produced by automati- cally detecting and extracting portions of parallel  text from the monolingual LDC Arabic and En- glish Gigaword collections, using a method devel- oped at the Information Sciences Institute (ISI) of  the University of Southern California.", "labels": [], "entities": [{"text": "Defense Advanced Research Projects  Agency (DARPA) Global Autonomous Language  Exploitation (GALE)", "start_pos": 231, "end_pos": 329, "type": "TASK", "confidence": 0.7282742301623026}]}, {"text": " Table 1: Composition of the Arabic-English par- allel corpus used for MT training.", "labels": [], "entities": [{"text": "MT training", "start_pos": 71, "end_pos": 82, "type": "TASK", "confidence": 0.9431005716323853}]}, {"text": " Table 2: Characteristics of the tuning (Tune) and validation (Test) sets used for development on Arabic  newswire, web, and audio. The audio sets include material from both broadcast news and broadcast  conversations.", "labels": [], "entities": [{"text": "validation (Test)", "start_pos": 51, "end_pos": 68, "type": "METRIC", "confidence": 0.7341412901878357}]}, {"text": " Table 3: Phrase-based trigram decoding results on the Arabic text and audio development sets. Decoding  weights were optimized on the Tune set in order to directly minimize TER. Corpus weights were also  optimized on Tune set, but based on expected TER.", "labels": [], "entities": [{"text": "Phrase-based trigram decoding", "start_pos": 10, "end_pos": 39, "type": "TASK", "confidence": 0.7855440974235535}, {"text": "Tune set", "start_pos": 135, "end_pos": 143, "type": "DATASET", "confidence": 0.932495504617691}, {"text": "TER", "start_pos": 174, "end_pos": 177, "type": "METRIC", "confidence": 0.9953272342681885}, {"text": "Tune set", "start_pos": 218, "end_pos": 226, "type": "DATASET", "confidence": 0.9323297441005707}, {"text": "TER", "start_pos": 250, "end_pos": 253, "type": "METRIC", "confidence": 0.981652021408081}]}, {"text": " Table 4: Hierarchical 5-gram rescoring results on the Arabic text and audio development sets. Decod- ing/rescoring weights were optimized on the Tune set in order to directly maximize BLEU (for newswire)  or minimize TERBLEU (for web and audio). Corpus weights were the same as the ones used in the cor- responding phrase-based decodings.", "labels": [], "entities": [{"text": "Tune set", "start_pos": 146, "end_pos": 154, "type": "DATASET", "confidence": 0.9367879927158356}, {"text": "BLEU", "start_pos": 185, "end_pos": 189, "type": "METRIC", "confidence": 0.9977553486824036}, {"text": "TERBLEU", "start_pos": 218, "end_pos": 225, "type": "METRIC", "confidence": 0.9954702854156494}]}]}