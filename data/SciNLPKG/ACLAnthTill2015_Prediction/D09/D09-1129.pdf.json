{"title": [{"text": "Real-Word Spelling Correction using Google Web 1T 3-grams", "labels": [], "entities": [{"text": "Google Web 1T 3-grams", "start_pos": 36, "end_pos": 57, "type": "DATASET", "confidence": 0.8354697376489639}]}], "abstractContent": [{"text": "We present a method for detecting and correcting multiple real-word spelling errors using the Google Web 1T 3-gram data set and a normalized and modified version of the Longest Common Subsequence (LCS) string matching algorithm.", "labels": [], "entities": [{"text": "detecting and correcting multiple real-word spelling errors", "start_pos": 24, "end_pos": 83, "type": "TASK", "confidence": 0.7804941364697048}, {"text": "Google Web 1T 3-gram data set", "start_pos": 94, "end_pos": 123, "type": "DATASET", "confidence": 0.8933981160322825}, {"text": "Longest Common Subsequence (LCS) string matching", "start_pos": 169, "end_pos": 217, "type": "TASK", "confidence": 0.6467660255730152}]}, {"text": "Our method is focused mainly on how to improve the detection recall (the fraction of errors correctly detected) and the correction recall (the fraction of errors correctly amended), while keeping the respective precisions (the fraction of detections or amendments that are correct) as high as possible.", "labels": [], "entities": [{"text": "recall", "start_pos": 61, "end_pos": 67, "type": "METRIC", "confidence": 0.49523964524269104}, {"text": "correction recall", "start_pos": 120, "end_pos": 137, "type": "METRIC", "confidence": 0.9513663053512573}, {"text": "precisions", "start_pos": 211, "end_pos": 221, "type": "METRIC", "confidence": 0.986850917339325}]}, {"text": "Evaluation results on a standard data set show that our method outperforms two other methods on the same task.", "labels": [], "entities": []}], "introductionContent": [{"text": "Real-word spelling errors are words in a text that occur when a user mistakenly types a correctly spelled word when another was intended.", "labels": [], "entities": [{"text": "Real-word spelling errors are words in a text that occur when a user mistakenly types a correctly spelled word when another was intended", "start_pos": 0, "end_pos": 136, "type": "Description", "confidence": 0.7597527555797411}]}, {"text": "Errors of this type maybe caused by the writer's ignorance of the correct spelling of the intended word or by typing mistakes.", "labels": [], "entities": []}, {"text": "Such errors generally go unnoticed by most spellcheckers as they deal with words in isolation, accepting them as correct if they are found in the dictionary, and flagging them as errors if they are not.", "labels": [], "entities": []}, {"text": "This approach would be sufficient to detect the non-word error myss in \"It doesn't know what the myss is all about.\" but not the real-word error muss in \"It doesn't know what the muss is all about.\"", "labels": [], "entities": []}, {"text": "To detect the latter, the spell-checker needs to make use of the surrounding context such as, in this case, to recognise that fuss is more likely to occur than muss in the context of all about.", "labels": [], "entities": []}, {"text": "Ironically, errors of this type may even be caused by spelling checkers in the correction of non-word spelling errors when the auto-correct feature in some word-processing software sometimes silently change a non-word to the wrong real word, and sometimes when correcting a flagged error, the user accidentally make a wrong selection from the choices offered).", "labels": [], "entities": []}, {"text": "An extensive review of real-word spelling correction is given in) and the problem of spelling correction more generally is reviewed in.", "labels": [], "entities": [{"text": "real-word spelling correction", "start_pos": 23, "end_pos": 52, "type": "TASK", "confidence": 0.6525630255540212}, {"text": "spelling correction", "start_pos": 85, "end_pos": 104, "type": "TASK", "confidence": 0.8415030241012573}]}, {"text": "The Google Web 1T data set (), contributed by Google Inc., contains English word n-grams (from unigrams to 5-grams) and their observed frequency counts calculated over 1 trillion words from web page text collected by Google in January 2006.", "labels": [], "entities": [{"text": "Google Web 1T data set", "start_pos": 4, "end_pos": 26, "type": "DATASET", "confidence": 0.8451440811157227}]}, {"text": "The text was tokenised following the Penn Treebank tokenisation, except that hyphenated words, dates, email addresses and URLs are kept as single tokens.", "labels": [], "entities": [{"text": "Penn Treebank tokenisation", "start_pos": 37, "end_pos": 63, "type": "DATASET", "confidence": 0.9807651042938232}]}, {"text": "The sentence boundaries are marked with two special tokens <S> and </S>.", "labels": [], "entities": []}, {"text": "Words that occurred fewer than 200 times were replaced with the special token <UNK>.", "labels": [], "entities": []}, {"text": "shows the data sizes of the Web 1T corpus.", "labels": [], "entities": [{"text": "Web 1T corpus", "start_pos": 28, "end_pos": 41, "type": "DATASET", "confidence": 0.8638325333595276}]}, {"text": "The n-grams themselves must appear at least 40 times to be included in the Web 1T corpus . It is expected that this data will be useful for statistical language modeling, e.g., for machine translation or speech recognition, as well as for other uses.", "labels": [], "entities": [{"text": "Web 1T corpus", "start_pos": 75, "end_pos": 88, "type": "DATASET", "confidence": 0.86445019642512}, {"text": "statistical language modeling", "start_pos": 140, "end_pos": 169, "type": "TASK", "confidence": 0.7324440081914266}, {"text": "machine translation or speech recognition", "start_pos": 181, "end_pos": 222, "type": "TASK", "confidence": 0.6696515858173371}]}, {"text": "In this paper, we present a method for detecting and correcting multiple real-word spelling errors using the Google Web 1T 3-gram data set, and a normalized and modified version of the Longest Common Subsequence (LCS) string matching algorithm (details are in section 3.1).", "labels": [], "entities": [{"text": "detecting and correcting multiple real-word spelling errors", "start_pos": 39, "end_pos": 98, "type": "TASK", "confidence": 0.7700033613613674}, {"text": "Google Web 1T 3-gram data set", "start_pos": 109, "end_pos": 138, "type": "DATASET", "confidence": 0.8930992980798086}, {"text": "Longest Common Subsequence (LCS) string matching", "start_pos": 185, "end_pos": 233, "type": "TASK", "confidence": 0.6285191066563129}]}, {"text": "By multiple errors, we mean that if we haven words in the input sentence, then we try to detect and correct at most n-1 errors.", "labels": [], "entities": [{"text": "correct", "start_pos": 100, "end_pos": 107, "type": "METRIC", "confidence": 0.966661810874939}]}, {"text": "We do not try to detect and correct an error, if any, in the first word as it is not computationally feasible to search in the Google Web 1T 3-grams while keeping the first word in the 3-gram as a variable.", "labels": [], "entities": [{"text": "correct", "start_pos": 28, "end_pos": 35, "type": "METRIC", "confidence": 0.9613367319107056}]}, {"text": "Our intention is to focus on how to improve the detection recall (the fraction of errors correctly detected) or correction recall (the fraction of errors correctly amended) while maintaining the respective precisions (the fraction of detections or amendments that are correct) as high as possible.", "labels": [], "entities": [{"text": "recall", "start_pos": 58, "end_pos": 64, "type": "METRIC", "confidence": 0.5100371241569519}, {"text": "correction recall", "start_pos": 112, "end_pos": 129, "type": "METRIC", "confidence": 0.943051815032959}, {"text": "precisions", "start_pos": 206, "end_pos": 216, "type": "METRIC", "confidence": 0.9908464550971985}]}, {"text": "The reason behind this intention is that if the recall for any method is around 0.5, this means that the method fails to detector correct around 50 percent of the errors.", "labels": [], "entities": [{"text": "recall", "start_pos": 48, "end_pos": 54, "type": "METRIC", "confidence": 0.9990598559379578}]}, {"text": "As a result, we cannot completely rely on these type of methods, for that we need some type of human interventions or suggestions to detector correct the rest of the undetected or uncorrected errors.", "labels": [], "entities": []}, {"text": "Thus, if we have a method that can detector correct almost 80 percent of the errors, even generating some extra candidates that are incorrect is more helpful to the human.", "labels": [], "entities": []}, {"text": "This paper is organized as follow: Section 2 presents a brief overview of the related work.", "labels": [], "entities": []}, {"text": "Our proposed method is described in Section 3.", "labels": [], "entities": []}, {"text": "Evaluation and experimental results are discussed in Section 4.", "labels": [], "entities": []}, {"text": "We conclude in Section 5.", "labels": [], "entities": []}], "datasetContent": [{"text": "We used as test data the same data that Wilcox-O' used in their evaluation of method, which in turn was a replication of the data used by and to evaluate their methods.", "labels": [], "entities": []}, {"text": "The data consisted of 500 articles (approximately 300,000 words) from the 1987\u221289 Wall Street Journal corpus, with all headings, identifiers, and soon removed; that is, just along stream of text.", "labels": [], "entities": [{"text": "Wall Street Journal corpus", "start_pos": 82, "end_pos": 108, "type": "DATASET", "confidence": 0.9179320186376572}]}, {"text": "It is assumed that this data contains no errors; that is, the Wall Street Journal contains no malapropisms or other typos.", "labels": [], "entities": [{"text": "Wall Street Journal", "start_pos": 62, "end_pos": 81, "type": "DATASET", "confidence": 0.9607195258140564}]}, {"text": "In fact, a few typos (both non-word and real-word) were noticed during the evaluation, but they were small in number compared to the size of the text.", "labels": [], "entities": []}, {"text": "Malapropisms were randomly induced into this text at a frequency of approximately one word in 200.", "labels": [], "entities": []}, {"text": "Specifically, any word whose base form was listed as a noun in WordNet (but regardless of whether it was used as a noun in the text; there was no syntactic analysis) was potentially replaced by any spelling variation found in the lexicon of the ispell spelling checker 7 . A spelling variation was defined as any word with an edit distance of 1 from the original word; that is, any single-character insertion, deletion, or substitution, or the transposition of two characters, that results in another real word.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 63, "end_pos": 70, "type": "DATASET", "confidence": 0.9687424302101135}]}, {"text": "Thus, none of the induced malapropisms were derived from closed-class words, and none were formed by the insertion or deletion of an apostrophe or by splitting a word.", "labels": [], "entities": []}, {"text": "The data contained 1402 inserted malapropisms.", "labels": [], "entities": []}, {"text": "Because it had earlier been used for evaluating's trigram method, which operates at the sentence level, the data set had been divided into three parts, without regard for article boundaries or text coherence: sentences into which no malapropism had been induced; the original versions of the sentences that received malapropisms; and the malapropized sentences.", "labels": [], "entities": []}, {"text": "In addition, all instances of numbers of various kinds had been replaced by tags such as <INTEGER>, <DOLLAR VALUE>, Ispell is a fast screen-oriented spelling checker that shows you your errors in the context of the original file, and suggests possible corrections when it can figure them out.", "labels": [], "entities": [{"text": "DOLLAR VALUE", "start_pos": 101, "end_pos": 113, "type": "METRIC", "confidence": 0.7185935974121094}]}, {"text": "The original was written in PDP-10 assembly in 1971, by R. E. Gorin.", "labels": [], "entities": []}, {"text": "The C version was written by Pace Willisson of MIT.", "labels": [], "entities": []}, {"text": "Geoff Kuenning added the international support and created the current release. and <PERCENTAGE VALUE>.", "labels": [], "entities": [{"text": "PERCENTAGE", "start_pos": 85, "end_pos": 95, "type": "METRIC", "confidence": 0.9668552279472351}, {"text": "VALUE", "start_pos": 96, "end_pos": 101, "type": "METRIC", "confidence": 0.7470298409461975}]}, {"text": "Actual (random) numbers or values were restored for these tags.", "labels": [], "entities": []}, {"text": "Some spacing anomalies around punctuation marks were corrected.", "labels": [], "entities": []}, {"text": "A detailed description of this data can be found in).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Google Web 1T Data Sizes  Number of  Number Size on disk  (in KB)  Tokens  1,024,908,267,229  N/A  Sentences  95,119,665,584  N/A  Unigrams  13,588,391  185,569  Bigrams  314,843,401  5,213,440  Trigrams  977,069,902 19,978,540  4-grams  1,313,818,354 32,040,884  5-grams  1,176,470,663 33,678,504", "labels": [], "entities": []}, {"text": " Table 3: A comparison of recall, precision, and F 1  score for three methods of malapropism detection  and correction on the same data set.", "labels": [], "entities": [{"text": "recall", "start_pos": 26, "end_pos": 32, "type": "METRIC", "confidence": 0.9996036887168884}, {"text": "precision", "start_pos": 34, "end_pos": 43, "type": "METRIC", "confidence": 0.9993498921394348}, {"text": "F 1  score", "start_pos": 49, "end_pos": 59, "type": "METRIC", "confidence": 0.9913168350855509}, {"text": "malapropism detection  and correction", "start_pos": 81, "end_pos": 118, "type": "TASK", "confidence": 0.7317461743950844}]}]}