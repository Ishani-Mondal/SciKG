{"title": [{"text": "Less is More: Significance-Based N-gram Selection for Smaller, Better Language Models", "labels": [], "entities": []}], "abstractContent": [{"text": "The recent availability of large corpora for training N-gram language models has shown the utility of models of higher order than just trigrams.", "labels": [], "entities": []}, {"text": "In this paper, we investigate methods to control the increase in model size resulting from applying standard methods at higher orders.", "labels": [], "entities": []}, {"text": "We introduce significance-based N-gram selection , which not only reduces model size, but also improves perplexity for several smoothing methods, including Katz back-off and absolute discounting.", "labels": [], "entities": []}, {"text": "We also show that, when combined with anew smoothing method and a novel variant of weighted-difference pruning, our selection method performs better in the trade-off between model size and perplexity than the best pruning method we found for modified Kneser-Ney smoothing.", "labels": [], "entities": []}], "introductionContent": [{"text": "Statistical language models are potentially useful for any language technology task that produces natural-language text as a final (or intermediate) output.", "labels": [], "entities": []}, {"text": "In particular, they are extensively used in speech recognition and machine translation.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 44, "end_pos": 62, "type": "TASK", "confidence": 0.8418120443820953}, {"text": "machine translation", "start_pos": 67, "end_pos": 86, "type": "TASK", "confidence": 0.7955729067325592}]}, {"text": "Despite the criticism that they ignore the structure of natural language, simple N-gram models, which estimate the probability of each word in a text string based on the N \u22121 preceding words, remain the most widely-used type of model.", "labels": [], "entities": []}, {"text": "Until the late 1990s, N-gram language models of order higher than trigrams were seldom used.", "labels": [], "entities": []}, {"text": "This was due, at least in part, to the fact the amounts of training data available did not produce significantly better results from higher-order models.", "labels": [], "entities": []}, {"text": "Since that time, however, increasingly large amounts of language model training data have become available ranging from approximately one billion words (the Gigaword corpora from the Linguistic Data Consortium) to trillions of words (.", "labels": [], "entities": [{"text": "Gigaword corpora from the Linguistic Data Consortium", "start_pos": 157, "end_pos": 209, "type": "DATASET", "confidence": 0.9245987279074532}]}, {"text": "With these larger resources, the use of language models based on 5-grams to 7-grams is becoming increasingly common.", "labels": [], "entities": []}, {"text": "The problem we address here is that, even when relatively modest amounts of training data are used, high-order N-gram language models estimated by standard techniques can be impractically large.", "labels": [], "entities": []}, {"text": "Hence, we investigate ways of building high-order N-gram language models without dramatically increasing model size.", "labels": [], "entities": []}, {"text": "This is, of course, the same goal behind much previous work on language model pruning, including that of,, and Goodman and.", "labels": [], "entities": []}, {"text": "We take a novel approach, however, which we refer to as significance-based N-gram selection.", "labels": [], "entities": [{"text": "significance-based N-gram selection", "start_pos": 56, "end_pos": 91, "type": "TASK", "confidence": 0.5548292597134908}]}, {"text": "We reject a higher-order estimate of the probability of a particular word in a particular context whenever the distribution of observations for the higher-order estimate provides no evidence that the higher-order estimate is better than our backoff estimate.", "labels": [], "entities": []}, {"text": "Perhaps our most surprising result is that significance-based N-gram selection not only reduces language model size, but it also improves perplexity when applied to a number of widelyused smoothing methods, including Katz backoff and several variants of absolute discounting.", "labels": [], "entities": []}, {"text": "In contrast, experiments applying previous pruning methods to Katz backoff) and absolute discounting) always found the lowest perplexity model to be the unpruned model.", "labels": [], "entities": []}, {"text": "We tested significance-based selection on only one smoothing method without obtaining improved perplexity: modified Kneser-Ney smoothing.", "labels": [], "entities": []}, {"text": "This is unfortunate, because modified KN smoothing generally seems to have the lowest perplexity of any known smoothing method for N-gram language models; in our tests it had a lower perplexity than any of the other models, with or without significance-based N-gram selection.", "labels": [], "entities": [{"text": "KN smoothing", "start_pos": 38, "end_pos": 50, "type": "TASK", "confidence": 0.847628265619278}]}, {"text": "However, when we compared modified KN smoothing to our best results applying N-gram selection to other smoothing methods for multiple N-gram orders, two of our models outperformed modified KN in terms of perplexity fora given model size.", "labels": [], "entities": []}, {"text": "Of course, the trade-off between perplexity and model size for modified KN can also be improved by pruning.", "labels": [], "entities": []}, {"text": "So, in a final set of experiments we found the best combinations we could for pruned modified KN models, and we did the same for our best model using significancebased selection.", "labels": [], "entities": []}, {"text": "The best pruning method for the latter turned out to be a novel modification of weighted-difference pruning) that was especially convenient to compute given our method for performing significance-based N-gram selection.", "labels": [], "entities": []}, {"text": "The final result is that our best model using significance-based selection and modified weighted difference pruning always had a better size/perplexity trade-off than pruned modified KN, with up to about 8% perplexity reduction fora given model size.", "labels": [], "entities": []}], "datasetContent": [{"text": "We carried out three sets of evaluations to test the new techniques described above.", "labels": [], "entities": []}, {"text": "First we compared the perplexity of full models and models reduced by significance-based N-gram selection for seven language model smoothing methods.", "labels": [], "entities": []}, {"text": "For the best three results in that comparison, we looked at the trade-off between perplexity and model size over a range of N-gram orders.", "labels": [], "entities": []}, {"text": "Finally, we tried various pruning methods to further reduce model size, and then compared the best result we obtained using previous techniques with the best result we obtained using our new techniques.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Perplexity results for N-gram selection", "labels": [], "entities": [{"text": "Perplexity", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9627627730369568}]}]}