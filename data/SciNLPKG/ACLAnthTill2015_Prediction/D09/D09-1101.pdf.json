{"title": [], "abstractContent": [{"text": "Traditional learning-based coreference re-solvers operate by training a mention-pair classifier for determining whether two mentions are coreferent or not.", "labels": [], "entities": [{"text": "coreference re-solvers", "start_pos": 27, "end_pos": 49, "type": "TASK", "confidence": 0.8818999826908112}]}, {"text": "Two independent lines of recent research have attempted to improve these mention-pair classifiers, one by learning a mention-ranking model to rank preceding mentions fora given anaphor, and the other by training an entity-mention classifier to determine whether a preceding cluster is coreferent with a given mention.", "labels": [], "entities": []}, {"text": "We propose a cluster-ranking approach to coreference resolution that combines the strengths of mention rankers and entity-mention models.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 41, "end_pos": 63, "type": "TASK", "confidence": 0.979307234287262}]}, {"text": "We additionally show how our cluster-ranking framework naturally allows discourse-new entity detection to be learned jointly with coreference resolution.", "labels": [], "entities": [{"text": "discourse-new entity detection", "start_pos": 72, "end_pos": 102, "type": "TASK", "confidence": 0.6768397688865662}, {"text": "coreference resolution", "start_pos": 130, "end_pos": 152, "type": "TASK", "confidence": 0.9486067295074463}]}, {"text": "Experimental results on the ACE data sets demonstrate its superior performance to competing approaches.", "labels": [], "entities": [{"text": "ACE data sets", "start_pos": 28, "end_pos": 41, "type": "DATASET", "confidence": 0.9749646782875061}]}], "introductionContent": [{"text": "Noun phrase (NP) coreference resolution is the task of identifying which NPs (or mentions) refer to the same real-world entity or concept.", "labels": [], "entities": [{"text": "Noun phrase (NP) coreference resolution", "start_pos": 0, "end_pos": 39, "type": "TASK", "confidence": 0.6334420953478131}]}, {"text": "Traditional learning-based coreference resolvers operate by training a model for classifying whether two mentions are co-referring or not (e.g.,,,,).", "labels": [], "entities": [{"text": "coreference resolvers", "start_pos": 27, "end_pos": 48, "type": "TASK", "confidence": 0.8794130980968475}]}, {"text": "Despite their initial successes, these mention-pair models have at least two major weaknesses.", "labels": [], "entities": []}, {"text": "First, since each candidate antecedent fora mention to be resolved (henceforth an active mention) is considered independently of the others, these models only determine how good a candidate antecedent is relative to the active mention, but not how good a candidate antecedent is relative to other candidates.", "labels": [], "entities": []}, {"text": "In other words, they fail to answer the critical question of which candidate antecedent is most probable.", "labels": [], "entities": []}, {"text": "Second, they have limitations in their expressiveness: the information extracted from the two mentions alone may not be sufficient for making an informed coreference decision, especially if the candidate antecedent is a pronoun (which is semantically empty) or a mention that lacks descriptive information such as gender (e.g., Clinton).", "labels": [], "entities": []}, {"text": "To address the first weakness, researchers have attempted to train a mention-ranking model for determining which candidate antecedent is most probable given an active mention (e.g.,).", "labels": [], "entities": []}, {"text": "Ranking is arguably a more natural reformulation of coreference resolution than classification, as a ranker allows all candidate antecedents to be considered simultaneously and therefore directly captures the competition among them.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 52, "end_pos": 74, "type": "TASK", "confidence": 0.9618381857872009}]}, {"text": "Another desirable consequence is that there exists a natural resolution strategy fora ranking approach: a mention is resolved to the candidate antecedent that has the highest rank.", "labels": [], "entities": []}, {"text": "This contrasts with classification-based approaches, where many clustering algorithms have been employed to co-ordinate the pairwise coreference decisions (because it is unclear which one is the best).", "labels": [], "entities": []}, {"text": "To address the second weakness, researchers have investigated the acquisition of entity-mention coreference models (e.g., ,).", "labels": [], "entities": []}, {"text": "Unlike mention-pair models, these entity-mention models are trained to determine whether an active mention belongs to a preceding, possibly partially-formed, coreference cluster.", "labels": [], "entities": []}, {"text": "Hence, they can employ cluster-level features (i.e., features that are defined over any subset of mentions in a preceding cluster), which makes them more expressive than mention-pair models.", "labels": [], "entities": []}, {"text": "Motivated in part by these recently developed models, we propose in this paper a clusterranking approach to coreference resolution that combines the strengths of mention-ranking mod-els and entity-mention models.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 108, "end_pos": 130, "type": "TASK", "confidence": 0.9706457257270813}]}, {"text": "Specifically, we recast coreference as the problem of determining which of a set of preceding coreference clusters is the best to link to an active mention using a learned cluster ranker.", "labels": [], "entities": []}, {"text": "In addition, we show how discourse-new detection (i.e., the task of determining whether a mention introduces anew entity in a discourse) can be learned jointly with coreference resolution in our cluster-ranking framework.", "labels": [], "entities": [{"text": "discourse-new detection", "start_pos": 25, "end_pos": 48, "type": "TASK", "confidence": 0.7230678349733353}, {"text": "coreference resolution", "start_pos": 165, "end_pos": 187, "type": "TASK", "confidence": 0.8923273384571075}]}, {"text": "It is worth noting that researchers typically adopt a pipeline coreference architecture, performing discourse-new detection prior to coreference resolution and using the resulting information to prevent a coreference system from resolving mentions that are determined to be discourse-new (see for an overview).", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 133, "end_pos": 155, "type": "TASK", "confidence": 0.7548889517784119}]}, {"text": "As a result, errors in discourse-new detection could be propagated to the resolver, possibly leading to a deterioration of coreference performance (see).", "labels": [], "entities": []}, {"text": "Jointly learning discoursenew detection and coreference resolution can potentially address this error-propagation problem.", "labels": [], "entities": [{"text": "learning discoursenew detection", "start_pos": 8, "end_pos": 39, "type": "TASK", "confidence": 0.6107738415400187}, {"text": "coreference resolution", "start_pos": 44, "end_pos": 66, "type": "TASK", "confidence": 0.9524639844894409}]}, {"text": "In sum, we believe our work makes three main contributions to coreference resolution: Proposing a simple, yet effective coreference model.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 62, "end_pos": 84, "type": "TASK", "confidence": 0.9824835062026978}]}, {"text": "Our work advances the state-of-the-art in coreference resolution by bringing learningbased coreference systems to the next level of performance.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 42, "end_pos": 64, "type": "TASK", "confidence": 0.9658661782741547}]}, {"text": "When evaluated on the ACE 2005 coreference data sets, cluster rankers outperform three competing models -mention-pair, entitymention, and mention-ranking models -by a large margin.", "labels": [], "entities": [{"text": "ACE 2005 coreference data sets", "start_pos": 22, "end_pos": 52, "type": "DATASET", "confidence": 0.9689039587974548}]}, {"text": "Also, our joint-learning approach to discourse-new detection and coreference resolution consistently yields cluster rankers that outperform those adopting the pipeline architecture.", "labels": [], "entities": [{"text": "discourse-new detection", "start_pos": 37, "end_pos": 60, "type": "TASK", "confidence": 0.680268794298172}, {"text": "coreference resolution", "start_pos": 65, "end_pos": 87, "type": "TASK", "confidence": 0.925743043422699}]}, {"text": "Equally importantly, cluster rankers are conceptually simple and easy to implement and do not rely on sophisticated training and inference procedures to make coreference decisions in dependent relation to each other, unlike relational coreference models (see).", "labels": [], "entities": []}, {"text": "Bridging the gap between machine-learning approaches and linguistically-motivated approaches to coreference resolution.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 96, "end_pos": 118, "type": "TASK", "confidence": 0.9560829102993011}]}, {"text": "While machine learning approaches to coreference resolution have received a lot of attention since the mid90s, popular learning-based coreference frameworks such as the mention-pair model are arguably rather unsatisfactory from a linguistic point of view.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 37, "end_pos": 59, "type": "TASK", "confidence": 0.9546611309051514}]}, {"text": "In particular, they have not leveraged advances in discourse-based anaphora resolution research in the 70s and 80s.", "labels": [], "entities": [{"text": "discourse-based anaphora resolution", "start_pos": 51, "end_pos": 86, "type": "TASK", "confidence": 0.5936125119527181}]}, {"text": "Our work bridges this gap by realizing in anew machine learning framework ideas rooted in heuristic-based pronoun resolver, which in turn was motivated by classic salience-based approaches to anaphora resolution.", "labels": [], "entities": [{"text": "heuristic-based pronoun resolver", "start_pos": 90, "end_pos": 122, "type": "TASK", "confidence": 0.7438316543896993}, {"text": "anaphora resolution", "start_pos": 192, "end_pos": 211, "type": "TASK", "confidence": 0.7202275991439819}]}, {"text": "Revealing the importance of adopting the right model.", "labels": [], "entities": []}, {"text": "While entity-mention models have previously been shown to be worse or at best marginally better than their mention-pair counterparts ( , our cluster-ranking models, which area natural extension of entity-mention models, significantly outperformed all competing approaches.", "labels": [], "entities": []}, {"text": "This suggests that the use of an appropriate learning framework can bring us along way towards highperformance coreference resolution.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 111, "end_pos": 133, "type": "TASK", "confidence": 0.7834716737270355}]}, {"text": "The rest of the paper is structured as follows.", "labels": [], "entities": []}, {"text": "Section 2 discusses related work.", "labels": [], "entities": []}, {"text": "Section 3 describes our baseline coreference models: mentionpair, entity-mention, and mention-ranking.", "labels": [], "entities": []}, {"text": "We discuss our cluster-ranking approach in Section 4, evaluate it in Section 5, and conclude in Section 6.", "labels": [], "entities": []}], "datasetContent": [{"text": "We use the ACE 2005 coreference corpus as released by the LDC, which consists of the 599 training documents used in the official ACE evaluation.", "labels": [], "entities": [{"text": "ACE 2005 coreference corpus", "start_pos": 11, "end_pos": 38, "type": "DATASET", "confidence": 0.9684430211782455}, {"text": "LDC", "start_pos": 58, "end_pos": 61, "type": "DATASET", "confidence": 0.5211564898490906}, {"text": "ACE evaluation", "start_pos": 129, "end_pos": 143, "type": "DATASET", "confidence": 0.8864891529083252}]}, {"text": "To ensure diversity, the corpus was created by selecting documents from six different sources: Broadcast News (bn), Broadcast Conversations (bc), Newswire (nw), Webblog (wb), Usenet (un), and conversational telephone speech (cts).", "labels": [], "entities": []}, {"text": "The number of documents belonging to each source is shown in.", "labels": [], "entities": []}, {"text": "For evaluation, we partition the 599 documents into a training set and a test set following a 80/20 ratio, ensuring that the two sets have the same proportion of documents from the six sources.", "labels": [], "entities": []}, {"text": "We evaluate each coreference model using both true mentions (i.e., gold standard mentions 4 ) and system mentions (i.e., au-, we recast mention extraction as a sequence labeling task, where we assign to each token in a test text a label that indicates whether it begins a mention, is inside a mention, or is outside a mention.", "labels": [], "entities": [{"text": "mention extraction", "start_pos": 136, "end_pos": 154, "type": "TASK", "confidence": 0.7715900242328644}]}, {"text": "Hence, to learn the extractor, we create one training instance for each token in a training text and derive its class value (one of b, i, and o) from the annotated data.", "labels": [], "entities": []}, {"text": "Each instance represents w i , the token under consideration, and consists of 29 linguistic features, many of which are modeled after the systems of and, as described below.", "labels": [], "entities": []}, {"text": "Lexical (7): Tokens in a window of 7:", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Statistics for the ACE 2005 corpus", "labels": [], "entities": [{"text": "the ACE 2005 corpus", "start_pos": 25, "end_pos": 44, "type": "DATASET", "confidence": 0.8905128091573715}]}, {"text": " Table 3: MUC, CEAF, and B 3 coreference results using true mentions.", "labels": [], "entities": [{"text": "MUC", "start_pos": 10, "end_pos": 13, "type": "DATASET", "confidence": 0.7225273251533508}, {"text": "CEAF", "start_pos": 15, "end_pos": 19, "type": "DATASET", "confidence": 0.50847989320755}, {"text": "coreference", "start_pos": 29, "end_pos": 40, "type": "TASK", "confidence": 0.8307003974914551}]}, {"text": " Table 4: MUC, CEAF, and B 3 coreference results using system mentions.", "labels": [], "entities": [{"text": "MUC", "start_pos": 10, "end_pos": 13, "type": "DATASET", "confidence": 0.7830167412757874}, {"text": "CEAF", "start_pos": 15, "end_pos": 19, "type": "DATASET", "confidence": 0.5973272919654846}, {"text": "coreference", "start_pos": 29, "end_pos": 40, "type": "TASK", "confidence": 0.8385985493659973}]}]}