{"title": [{"text": "Integrating sentence-and word-level error identification for disfluency correction", "labels": [], "entities": [{"text": "Integrating sentence-and word-level error identification", "start_pos": 0, "end_pos": 56, "type": "TASK", "confidence": 0.7024006068706512}, {"text": "disfluency correction", "start_pos": 61, "end_pos": 82, "type": "TASK", "confidence": 0.7081775963306427}]}], "abstractContent": [{"text": "While speaking spontaneously, speakers often make errors such as self-correction or false starts which interfere with the successful application of natural language processing techniques like summarization and machine translation to this data.", "labels": [], "entities": [{"text": "summarization", "start_pos": 192, "end_pos": 205, "type": "TASK", "confidence": 0.9803007245063782}, {"text": "machine translation", "start_pos": 210, "end_pos": 229, "type": "TASK", "confidence": 0.6746570765972137}]}, {"text": "There is active work on reconstructing this error-ful data into a clean and fluent transcript by identifying and removing these simple errors.", "labels": [], "entities": []}, {"text": "Previous research has approximated the potential benefit of conducting word-level reconstruction of simple errors only on those sentences known to have errors.", "labels": [], "entities": [{"text": "word-level reconstruction of simple errors", "start_pos": 71, "end_pos": 113, "type": "TASK", "confidence": 0.7879251539707184}]}, {"text": "In this work, we explore new approaches for automatically identifying speaker construction errors on the utterance level, and quantify the impact that this initial step has on word-and sentence-level reconstruction accuracy.", "labels": [], "entities": [{"text": "automatically identifying speaker construction errors", "start_pos": 44, "end_pos": 97, "type": "TASK", "confidence": 0.6473437786102295}, {"text": "word-and sentence-level reconstruction", "start_pos": 176, "end_pos": 214, "type": "TASK", "confidence": 0.6050351957480112}, {"text": "accuracy", "start_pos": 215, "end_pos": 223, "type": "METRIC", "confidence": 0.7395254373550415}]}], "introductionContent": [{"text": "A system would accomplish reconstruction of its spontaneous speech input if its output were to represent, in flawless, fluent, and content-preserving text, the message that the speaker intended to convey.", "labels": [], "entities": []}, {"text": "While full speech reconstruction would likely require a range of string transformations and potentially deep syntactic and semantic analysis of the errorful text, in this work we will attempt only to resolve less complex errors, correctable by deletion alone, in a given manuallytranscribed utterance.", "labels": [], "entities": [{"text": "full speech reconstruction", "start_pos": 6, "end_pos": 32, "type": "TASK", "confidence": 0.6498874326546987}]}, {"text": "The benefit of conducting word-level reconstruction of simple errors only on those sentences known to have errors was approximated in ( ).", "labels": [], "entities": [{"text": "word-level reconstruction of simple errors", "start_pos": 26, "end_pos": 68, "type": "TASK", "confidence": 0.7894594430923462}]}, {"text": "In the current work, we explore approaches for automatically identifying speaker-generated errors on the utterance level, and calculate the gain inaccuracy that this initial step has on word-and sentence-level accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 210, "end_pos": 218, "type": "METRIC", "confidence": 0.9148476123809814}]}], "datasetContent": [{"text": "We first evaluate edit detection accuracy on those test SUs predicted to be errorful on a per-word basis.", "labels": [], "entities": [{"text": "edit detection", "start_pos": 18, "end_pos": 32, "type": "TASK", "confidence": 0.7892768979072571}, {"text": "accuracy", "start_pos": 33, "end_pos": 41, "type": "METRIC", "confidence": 0.9059678316116333}]}, {"text": "To evaluate our progress identifying word-level error classes, we calculate precision, recall and F-scores for each labeled class c in each experimental scenario.", "labels": [], "entities": [{"text": "precision", "start_pos": 76, "end_pos": 85, "type": "METRIC", "confidence": 0.9996651411056519}, {"text": "recall", "start_pos": 87, "end_pos": 93, "type": "METRIC", "confidence": 0.9982494115829468}, {"text": "F-scores", "start_pos": 98, "end_pos": 106, "type": "METRIC", "confidence": 0.9943607449531555}]}, {"text": "As usual, these metrics are calculated as ratios of correct, false, and missed predictions.", "labels": [], "entities": []}, {"text": "However, to take advantage of the double reconstruction annotations provided in SSR (and more importantly, in recognition of the occasional ambiguities of reconstruction) we modified these calculations slightly to account for all references.", "labels": [], "entities": [{"text": "SSR", "start_pos": 80, "end_pos": 83, "type": "DATASET", "confidence": 0.884016752243042}]}, {"text": "Analysis of word-level label evaluation, post SU filtering.", "labels": [], "entities": [{"text": "word-level label evaluation", "start_pos": 12, "end_pos": 39, "type": "TASK", "confidence": 0.6829819877942404}, {"text": "SU filtering", "start_pos": 46, "end_pos": 58, "type": "TASK", "confidence": 0.9224076867103577}]}, {"text": "Word-level F 1 -score results for error region identification are shown in.", "labels": [], "entities": [{"text": "Word-level F 1 -score", "start_pos": 0, "end_pos": 21, "type": "METRIC", "confidence": 0.8360365152359008}, {"text": "error region identification", "start_pos": 34, "end_pos": 61, "type": "TASK", "confidence": 0.6156610647837321}]}, {"text": "By first automatically selecting testing as described in Section 3 (with a sentence-level F-score of 83.3,(b)), we see in some gain in F-score for all three error classes, though much potential improvement remains based on the oracle gain (rows indicated as having \"Gold errors\" testing data).", "labels": [], "entities": [{"text": "F-score", "start_pos": 90, "end_pos": 97, "type": "METRIC", "confidence": 0.8572252988815308}, {"text": "F-score", "start_pos": 135, "end_pos": 142, "type": "METRIC", "confidence": 0.9956710338592529}]}, {"text": "Note that there are no results from training only on errorful data but testing on all data, as this was shown to yield dramatically worse results due to data mismatch issues.", "labels": [], "entities": []}, {"text": "Unlike in the experiments where all data was used for testing and training, the best NC and RC detection performance given the automatically selected testing data was achieved when training a CRF model to detect each class separately (RC or NC alone) and not in conjunction with filler word detection FL.", "labels": [], "entities": [{"text": "filler word detection", "start_pos": 279, "end_pos": 300, "type": "TASK", "confidence": 0.596425841252009}]}, {"text": "As in FHJ, training RC and NC models separately instead of in a joint FL+RC+NC model yielded higher accuracy.", "labels": [], "entities": [{"text": "FHJ", "start_pos": 6, "end_pos": 9, "type": "DATASET", "confidence": 0.928652286529541}, {"text": "accuracy", "start_pos": 100, "end_pos": 108, "type": "METRIC", "confidence": 0.9983687996864319}]}, {"text": "We notice also that the F-score for RC identification is lower when automatically filtering the test data.", "labels": [], "entities": [{"text": "F-score", "start_pos": 24, "end_pos": 31, "type": "METRIC", "confidence": 0.9994187355041504}, {"text": "RC identification", "start_pos": 36, "end_pos": 53, "type": "TASK", "confidence": 0.9437207281589508}]}, {"text": "There are two likely causes.", "labels": [], "entities": []}, {"text": "The most likely issue is that the automatic SU-error classifier filtered out some SUs with true RC errors which had previously been correctly identified, reducing the overall precision ratio as well as recall (i.e., we no longer receive accuracy credit for some easier errors once caught).", "labels": [], "entities": [{"text": "SU-error classifier", "start_pos": 44, "end_pos": 63, "type": "TASK", "confidence": 0.8323222994804382}, {"text": "precision ratio", "start_pos": 175, "end_pos": 190, "type": "METRIC", "confidence": 0.9899075925350189}, {"text": "recall", "start_pos": 202, "end_pos": 208, "type": "METRIC", "confidence": 0.9995284080505371}, {"text": "accuracy", "start_pos": 237, "end_pos": 245, "type": "METRIC", "confidence": 0.9973069429397583}]}, {"text": "A second, related possibility is that the errorful SUs identified by the Section 3 method had a higher density of errors that the current CRF word-level classification model is unable to identify (i.e. the more difficult errors are now a higher relative percentage of the errors we need to catch).", "labels": [], "entities": [{"text": "CRF word-level classification", "start_pos": 138, "end_pos": 167, "type": "TASK", "confidence": 0.6452647050221761}]}, {"text": "While the former possibility seems more likely, both causes should be investigated in future work.", "labels": [], "entities": []}, {"text": "The F-score gain in NC identification from 42.5 to 54.6 came primarily from again in precision (in the original model, many non-errorful SUs were mistakenly determined to include errors).", "labels": [], "entities": [{"text": "F-score gain", "start_pos": 4, "end_pos": 16, "type": "METRIC", "confidence": 0.9827301502227783}, {"text": "NC identification", "start_pos": 20, "end_pos": 37, "type": "TASK", "confidence": 0.7454018294811249}, {"text": "precision", "start_pos": 85, "end_pos": 94, "type": "METRIC", "confidence": 0.9994390606880188}]}, {"text": "Though capturing approximately 55% of the non-copy NC errors (for SUs likely to have errors) is an improvement, this remains a challenging and unsolved task which should be investigated further in the future.", "labels": [], "entities": []}, {"text": "Depending on the downstream task of speech reconstruction, it maybe imperative not only to identify many of the errors in a given spoken utterance, but indeed to identify all errors (and only those errors), yielding the exact cleaned sentence that a human annotator might provide.", "labels": [], "entities": [{"text": "speech reconstruction", "start_pos": 36, "end_pos": 57, "type": "TASK", "confidence": 0.7153173834085464}]}, {"text": "In these experiments we apply simple cleanup (as described in Section 1.1) to both JC04 output and the predicted output for each experimental setup, deleting words when their error class is a filler, rough copy or non-copy.", "labels": [], "entities": []}, {"text": "Taking advantage of the dual annotations provided for each sentence in the SSR corpus, we can report double-reference evaluation.", "labels": [], "entities": [{"text": "SSR corpus", "start_pos": 75, "end_pos": 85, "type": "DATASET", "confidence": 0.8827190697193146}]}, {"text": "Thus, we judge that if a hypothesized cleaned sentence exactly matches either reference sentence cleaned in the same manner we count the cleaned utterance as correct, and otherwise we assign no credit.", "labels": [], "entities": []}, {"text": "We report double-reference exact match evaluation between a given SU sand references r \u2208 R, as defined below.", "labels": [], "entities": []}, {"text": "Analysis of sentence level evaluation, post SU identification.", "labels": [], "entities": [{"text": "sentence level evaluation", "start_pos": 12, "end_pos": 37, "type": "TASK", "confidence": 0.694044311841329}, {"text": "SU identification", "start_pos": 44, "end_pos": 61, "type": "TASK", "confidence": 0.9516567587852478}]}, {"text": "Results from this second evaluation of rough copy and non-copy error reconstruction can be seen in.", "labels": [], "entities": []}, {"text": "As seen in word-level identification results (Table 2), automatically selecting a subset of testing data upon which to apply simple cleanup reconstruction does not perform at the accuracy shown to be possible given an oracle filtering.", "labels": [], "entities": [{"text": "word-level identification", "start_pos": 11, "end_pos": 36, "type": "TASK", "confidence": 0.7740412056446075}, {"text": "accuracy", "start_pos": 179, "end_pos": 187, "type": "METRIC", "confidence": 0.990872323513031}]}, {"text": "While measuring improvement is difficult (here, nonfiltered data is incomparable to filtered test data results since a majority of these sentences require no major deletions at all), we note again that our methods (MaxEnt/FHJ-x) outperform the baseline of deleting nothing but filled pauses like \"eh\" and \"um\", as well as the state-of-the-art baseline JC04.", "labels": [], "entities": [{"text": "FHJ-x", "start_pos": 222, "end_pos": 227, "type": "METRIC", "confidence": 0.7246255874633789}, {"text": "JC04", "start_pos": 352, "end_pos": 356, "type": "DATASET", "confidence": 0.9509129524230957}]}, {"text": "For the baseline, we delete only filled pause filler words like \"eh\" and \"um\".", "labels": [], "entities": []}, {"text": "For JC04 output, we deleted any word assigned the class E or FL.", "labels": [], "entities": [{"text": "FL", "start_pos": 61, "end_pos": 63, "type": "METRIC", "confidence": 0.9505209922790527}]}, {"text": "Finally, for the MaxEnt/FHJ models, we used the jointly trained FL+RC+NC CRF model and deleted all words assigned any of the three classes.", "labels": [], "entities": [{"text": "FHJ", "start_pos": 24, "end_pos": 27, "type": "DATASET", "confidence": 0.5207076072692871}, {"text": "FL+RC+NC CRF", "start_pos": 64, "end_pos": 76, "type": "METRIC", "confidence": 0.8490965267022451}]}], "tableCaptions": [{"text": " Table 2: Error predictions, post-SU identification: F 1 -score results. Automatically identified \"SUs for  testing\" were determined via the maximum entropy classification model described earlier in this paper,  and feature set #7 from Table 1. Filler (FL), rough copy error (RC) and non-copy error (NC) results are  given in terms of word-level F 1 -score. Bold numbers indicate the highest performance post-automatic  filter for each of the three classes. Italicized values indicate experiments where no filtering outperformed  automatic filtering (for RC errors).", "labels": [], "entities": [{"text": "F 1 -score", "start_pos": 53, "end_pos": 63, "type": "METRIC", "confidence": 0.9768951684236526}, {"text": "Filler (FL)", "start_pos": 245, "end_pos": 256, "type": "METRIC", "confidence": 0.6432236582040787}, {"text": "rough copy error (RC)", "start_pos": 258, "end_pos": 279, "type": "METRIC", "confidence": 0.7430628885825475}, {"text": "non-copy error (NC)", "start_pos": 284, "end_pos": 303, "type": "METRIC", "confidence": 0.7562707304954529}, {"text": "word-level F 1 -score", "start_pos": 335, "end_pos": 356, "type": "METRIC", "confidence": 0.8145725727081299}]}, {"text": " Table 3: Error predictions, post-SU identification: Exact Sentence Match Results.", "labels": [], "entities": [{"text": "Error", "start_pos": 10, "end_pos": 15, "type": "METRIC", "confidence": 0.9611728191375732}]}]}