{"title": [], "abstractContent": [{"text": "Many named entities contain other named entities inside them.", "labels": [], "entities": []}, {"text": "Despite this fact, the field of named entity recognition has almost entirely ignored nested named entity recognition, but due to technological, rather than ideological reasons.", "labels": [], "entities": [{"text": "named entity recognition", "start_pos": 32, "end_pos": 56, "type": "TASK", "confidence": 0.6290598511695862}, {"text": "named entity recognition", "start_pos": 92, "end_pos": 116, "type": "TASK", "confidence": 0.7275878588358561}]}, {"text": "In this paper , we present anew technique for recognizing nested named entities, by using a discriminative constituency parser.", "labels": [], "entities": []}, {"text": "To train the model, we transform each sentence into a tree, with constituents for each named entity (and no other syntactic structure).", "labels": [], "entities": []}, {"text": "We present results on both newspaper and biomedical corpora which contain nested named entities.", "labels": [], "entities": []}, {"text": "In three out of four sets of experiments, our model outperforms a standard semi-CRF on the more traditional top-level entities.", "labels": [], "entities": []}, {"text": "At the same time, we improve the overall F-score by up to 30% over the flat model, which is unable to recover any nested entities.", "labels": [], "entities": [{"text": "F-score", "start_pos": 41, "end_pos": 48, "type": "METRIC", "confidence": 0.998561441898346}]}], "introductionContent": [{"text": "Named entity recognition is the task of finding entities, such as people and organizations, in text.", "labels": [], "entities": [{"text": "Named entity recognition", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.6428189078966776}]}, {"text": "Frequently, entities are nested within each other, such as Bank of China and University of Washington, both organizations with nested locations.", "labels": [], "entities": []}, {"text": "Nested entities are also common in biomedical data, where different biological entities of interest are often composed of one another.", "labels": [], "entities": []}, {"text": "In the GENIA corpus ( ), which is labeled with entity types such as protein and DNA, roughly 17% of entities are embedded within another entity.", "labels": [], "entities": [{"text": "GENIA corpus", "start_pos": 7, "end_pos": 19, "type": "DATASET", "confidence": 0.9433274269104004}]}, {"text": "In the AnCora corpus of Spanish and Catalan newspaper text, nearly half of the entities are embedded.", "labels": [], "entities": [{"text": "AnCora corpus of Spanish and Catalan newspaper text", "start_pos": 7, "end_pos": 58, "type": "DATASET", "confidence": 0.949997790157795}]}, {"text": "However, work on named entity recognition (NER) has almost entirely ignored nested entities and instead chosen to focus on the outermost entities.", "labels": [], "entities": [{"text": "named entity recognition (NER)", "start_pos": 17, "end_pos": 47, "type": "TASK", "confidence": 0.8056043485800425}]}, {"text": "We believe this has largely been for practical, not ideological, reasons.", "labels": [], "entities": []}, {"text": "Most corpus designers have chosen to skirt the issue entirely, and have annotated only the topmost entities.", "labels": [], "entities": []}, {"text": "The widely used CoNLL (), MUC-6, and MUC-7 NER corpora, composed of American and British newswire, are all flatly annotated.", "labels": [], "entities": [{"text": "MUC-6", "start_pos": 26, "end_pos": 31, "type": "DATASET", "confidence": 0.92960524559021}, {"text": "MUC-7 NER corpora", "start_pos": 37, "end_pos": 54, "type": "DATASET", "confidence": 0.9032689730326334}]}, {"text": "The GENIA corpus contains nested entities, but the JNLPBA 2004 shared task), which utilized the corpus, removed all embedded entities for the evaluation.", "labels": [], "entities": [{"text": "GENIA corpus", "start_pos": 4, "end_pos": 16, "type": "DATASET", "confidence": 0.9401450753211975}, {"text": "JNLPBA 2004 shared task", "start_pos": 51, "end_pos": 74, "type": "DATASET", "confidence": 0.9111916422843933}]}, {"text": "To our knowledge, the only shared task which has included nested entities is the, which used a subset of the AnCora corpus.", "labels": [], "entities": [{"text": "AnCora corpus", "start_pos": 109, "end_pos": 122, "type": "DATASET", "confidence": 0.9426500499248505}]}, {"text": "However, in that task all entities corresponded to particular parts of speech or noun phrases in the provided syntactic structure, and no participant directly addressed the nested nature of the data.", "labels": [], "entities": []}, {"text": "Another reason for the lack of focus on nested NER is technological.", "labels": [], "entities": [{"text": "NER", "start_pos": 47, "end_pos": 50, "type": "TASK", "confidence": 0.8747012615203857}]}, {"text": "The NER task arose in the context of the MUC workshops, as small chunks which could be identified by finite state models or gazetteers.", "labels": [], "entities": [{"text": "NER task", "start_pos": 4, "end_pos": 12, "type": "TASK", "confidence": 0.920990914106369}, {"text": "MUC workshops", "start_pos": 41, "end_pos": 54, "type": "DATASET", "confidence": 0.8850583136081696}]}, {"text": "This then led to the widespread use of sequence models, first hidden Markov models, then conditional Markov models, and, more recently, linear chain conditional random fields (CRFs) ().", "labels": [], "entities": []}, {"text": "All of these models suffer from an inability to model nested entities.", "labels": [], "entities": []}, {"text": "In this paper we present a novel solution to the problem of nested named entity recognition.", "labels": [], "entities": [{"text": "nested named entity recognition", "start_pos": 60, "end_pos": 91, "type": "TASK", "confidence": 0.6400645822286606}]}, {"text": "Our model explicitly represents the nested structure, allowing entities to be influenced not just by the labels of the words surrounding them, as in a CRF, but also by the entities contained in them, and in which they are contained.", "labels": [], "entities": []}, {"text": "We represent each sentence as a parse tree, with the words as leaves, and with phrases corresponding to each entity (and anode which joins the entire sentence).", "labels": [], "entities": []}, {"text": "Our trees look just like syntactic constituency trees, such as those in the Penn TreeBank (, but they tend to be much flatter.", "labels": [], "entities": [{"text": "Penn TreeBank", "start_pos": 76, "end_pos": 89, "type": "DATASET", "confidence": 0.9924054145812988}]}, {"text": "This model allows us to include parts of speech in the tree, and therefore to jointly model the named entities and the part of speech tags.", "labels": [], "entities": []}, {"text": "Once we have converted our sentences into parse trees, we train a discriminative constituency parser similar to that of (.", "labels": [], "entities": []}, {"text": "We found that on top-level entities, our model does just as well as more conventional methods.", "labels": [], "entities": []}, {"text": "When evaluating on all entities our model does well, with F-scores ranging from slightly worse than performance on top-level only, to substantially better than top-level only.", "labels": [], "entities": [{"text": "F-scores", "start_pos": 58, "end_pos": 66, "type": "METRIC", "confidence": 0.9980034232139587}]}], "datasetContent": [{"text": "We performed two sets of experiments, the first set over biomedical data, and the second over Spanish and Catalan newspaper text.", "labels": [], "entities": []}, {"text": "We designed our experiments to show that our model works just as well on outermost entities, the typical NER task, and also works well on nested entities.", "labels": [], "entities": [{"text": "NER task", "start_pos": 105, "end_pos": 113, "type": "TASK", "confidence": 0.8306802809238434}]}, {"text": "The parts of speech provided in the data include detailed morphological information, using a similar annotation scheme to the Prague TreeBank ().", "labels": [], "entities": [{"text": "Prague TreeBank", "start_pos": 126, "end_pos": 141, "type": "DATASET", "confidence": 0.9860833883285522}]}, {"text": "There are around 250 possible tags, and experiments on the development data with the full tagset where unsuccessful.", "labels": [], "entities": []}, {"text": "We removed all but the first two characters of each POS tag, resulting in a set of 57 tags which more closely resembles that of the Penn TreeBank.", "labels": [], "entities": [{"text": "Penn TreeBank", "start_pos": 132, "end_pos": 145, "type": "DATASET", "confidence": 0.9936233460903168}]}, {"text": "All reported results use our modified version of the POS tag set.", "labels": [], "entities": [{"text": "POS tag set", "start_pos": 53, "end_pos": 64, "type": "DATASET", "confidence": 0.9040237466494242}]}, {"text": "We took only the words as input, none of the extra annotations.", "labels": [], "entities": []}, {"text": "For both languages we trained a 200 cluster distributional similarity model over the words in the corpus.", "labels": [], "entities": []}, {"text": "We performed the same set of experiments on AnCora as we did on GENIA.", "labels": [], "entities": [{"text": "AnCora", "start_pos": 44, "end_pos": 50, "type": "DATASET", "confidence": 0.9043498635292053}, {"text": "GENIA", "start_pos": 64, "end_pos": 69, "type": "DATASET", "confidence": 0.9641373157501221}]}, {"text": "Because we could not compare our results on the NER portion of the GENIA corpus with any other work, we also evaluated on the JNLPBA corpus.", "labels": [], "entities": [{"text": "NER portion of the GENIA corpus", "start_pos": 48, "end_pos": 79, "type": "DATASET", "confidence": 0.6768918037414551}, {"text": "JNLPBA corpus", "start_pos": 126, "end_pos": 139, "type": "DATASET", "confidence": 0.9748629331588745}]}, {"text": "This corpus was used in a shared task for the BioNLP workshop at Coling in 2004).", "labels": [], "entities": [{"text": "BioNLP workshop at Coling in 2004", "start_pos": 46, "end_pos": 79, "type": "DATASET", "confidence": 0.7658041218916575}]}, {"text": "They used the entire GENIA corpus for training, and modified the label set as discussed in Section 5.1.1.", "labels": [], "entities": [{"text": "GENIA corpus", "start_pos": 21, "end_pos": 33, "type": "DATASET", "confidence": 0.9429446458816528}]}, {"text": "They also removed all embedded entities, and kept only the top-level ones.", "labels": [], "entities": []}, {"text": "They then annotated new data for the test set.", "labels": [], "entities": []}, {"text": "This dataset has no nested entities, but because the training data is GENIA we can still train our model on the data annotated with nested entities, and then evaluate on their test data by ignoring all embedded entities found by our named entity recognizer.", "labels": [], "entities": [{"text": "GENIA", "start_pos": 70, "end_pos": 75, "type": "DATASET", "confidence": 0.8875780701637268}]}, {"text": "This experiment allows us to show that our named entity recognizer works well on top-level entities, by comparing it with prior work.", "labels": [], "entities": []}, {"text": "Our model also produces part of speech tags, but the test data is not annotated with POS tags, so we cannot show POS tagging results on this dataset.", "labels": [], "entities": []}, {"text": "One difficulty we had with the JNLPBA experiments was with tokenization.", "labels": [], "entities": [{"text": "JNLPBA", "start_pos": 31, "end_pos": 37, "type": "DATASET", "confidence": 0.7973745465278625}, {"text": "tokenization", "start_pos": 59, "end_pos": 71, "type": "TASK", "confidence": 0.9802098870277405}]}, {"text": "The version of GE-NIA distributed for the shared task is tokenized differently from the original GENIA corpus, but we needed to train on the original corpus as it is the only version with nested entities.", "labels": [], "entities": [{"text": "GENIA corpus", "start_pos": 97, "end_pos": 109, "type": "DATASET", "confidence": 0.9430493414402008}]}, {"text": "We tried our best to retokenize the original corpus to match the distributed data, but did not have complete success.", "labels": [], "entities": []}, {"text": "It is worth noting that the data is actually tokenized in a manner which allows a small amount of \"cheating.\"", "labels": [], "entities": [{"text": "cheating", "start_pos": 99, "end_pos": 107, "type": "METRIC", "confidence": 0.9521066546440125}]}, {"text": "Normally, hyphenated words, such as LPS-induced, are tokenized as one word.", "labels": [], "entities": []}, {"text": "However, if the portion of the word before the hyphen is in an entity, and the part after is not, such as BCR-induced, then the word is split into two tokens: BCR and -induced.", "labels": [], "entities": [{"text": "BCR", "start_pos": 159, "end_pos": 162, "type": "METRIC", "confidence": 0.6647827625274658}]}, {"text": "Therefore, when a word starts with a hyphen it is a strong indicator that the prior word and it span the right boundary of an entity.", "labels": [], "entities": []}, {"text": "Because the train and test data for the shared task do not contain nested entities, fewer words are split in this manner than in the original data.", "labels": [], "entities": []}, {"text": "We did not intentionally exploit this fact in our feature design, but it is probable that some of our orthographic features \"learned\" this fact anyway.", "labels": [], "entities": []}, {"text": "This probably harmed our results overall, because some hyphenated words, which straddled boundaries in nested entities and would have been split in the original corpus (and were split in our training data), were not split in the test data, prohibiting our model from properly identifying them.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Named entity results on GENIA, evaluating on all entities.", "labels": [], "entities": [{"text": "GENIA", "start_pos": 34, "end_pos": 39, "type": "DATASET", "confidence": 0.8758717179298401}]}, {"text": " Table 3: Named entity results on GENIA, evaluating on only top-level entities.", "labels": [], "entities": [{"text": "GENIA", "start_pos": 34, "end_pos": 39, "type": "DATASET", "confidence": 0.9018348455429077}]}, {"text": " Table 4: Named entity results on the JNLPBA 2004 shared task data.", "labels": [], "entities": [{"text": "JNLPBA 2004 shared task data", "start_pos": 38, "end_pos": 66, "type": "DATASET", "confidence": 0.8940639019012451}]}, {"text": " Table 5: Named entity results on the Spanish portion of AnCora, evaluating on all entities.", "labels": [], "entities": [{"text": "Spanish portion of AnCora", "start_pos": 38, "end_pos": 63, "type": "DATASET", "confidence": 0.826553225517273}]}, {"text": " Table 6: Named entity results on the Spanish portion of AnCora, evaluating on only top-level entities.", "labels": [], "entities": [{"text": "Spanish portion of AnCora", "start_pos": 38, "end_pos": 63, "type": "DATASET", "confidence": 0.7938978374004364}]}, {"text": " Table 7: Named entity results on the Catalan portion of AnCora, evaluating on all entities.", "labels": [], "entities": [{"text": "Catalan portion of AnCora", "start_pos": 38, "end_pos": 63, "type": "DATASET", "confidence": 0.7851482480764389}]}, {"text": " Table 8: Named entity results on the Catalan portion of AnCora, evaluating on only top-level entities.", "labels": [], "entities": [{"text": "Catalan portion of AnCora", "start_pos": 38, "end_pos": 63, "type": "DATASET", "confidence": 0.7836868986487389}]}]}