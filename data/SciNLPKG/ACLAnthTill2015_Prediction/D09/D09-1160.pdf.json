{"title": [{"text": "Polynomial to Linear: Efficient Classification with Conjunctive Features", "labels": [], "entities": []}], "abstractContent": [{"text": "This paper proposes a method that speeds up a classifier trained with many con-junctive features: combinations of (prim-itive) features.", "labels": [], "entities": []}, {"text": "The key idea is to pre-compute as partial results the weights of primitive feature vectors that appear frequently in the target NLP task.", "labels": [], "entities": []}, {"text": "A trie compactly stores the primitive feature vectors with their weights, and it enables the classifier to find fora given feature vector its longest prefix feature vector whose weight has already been computed.", "labels": [], "entities": []}, {"text": "Experimental results fora Japanese dependency parsing task show that our method speeded up the SVM and LLM classifiers of the parsers, which achieved accuracy of 90.84/90.71%, by a factor of 10.7/11.6.", "labels": [], "entities": [{"text": "Japanese dependency parsing task", "start_pos": 26, "end_pos": 58, "type": "TASK", "confidence": 0.6652900502085686}, {"text": "accuracy", "start_pos": 150, "end_pos": 158, "type": "METRIC", "confidence": 0.999438464641571}]}], "introductionContent": [{"text": "Deep and accurate text analysis based on discriminative models is not yet efficient enough as a component of real-time applications, and it is inadequate to process Web-scale corpora for knowledge acquisition) or semi-supervised learning.", "labels": [], "entities": [{"text": "text analysis", "start_pos": 18, "end_pos": 31, "type": "TASK", "confidence": 0.7126045227050781}, {"text": "knowledge acquisition", "start_pos": 187, "end_pos": 208, "type": "TASK", "confidence": 0.7285936623811722}]}, {"text": "One of the main reasons for this inefficiency is attributed to the inefficiency of core classifiers trained with many feature combinations (e.g., word n-grams).", "labels": [], "entities": []}, {"text": "Hereafter, we refer to features that explicitly represent combinations of features as conjunctive features and the other atomic features as primitive features.", "labels": [], "entities": []}, {"text": "The feature combinations play an essential role in obtaining a classifier with state-of-the-art accuracy for several NLP tasks; recent examples include dependency parsing (, parse re-ranking (), pronoun resolution, and semantic role labeling (.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 96, "end_pos": 104, "type": "METRIC", "confidence": 0.9947935938835144}, {"text": "dependency parsing", "start_pos": 152, "end_pos": 170, "type": "TASK", "confidence": 0.8656924962997437}, {"text": "pronoun resolution", "start_pos": 195, "end_pos": 213, "type": "TASK", "confidence": 0.8031810224056244}, {"text": "semantic role labeling", "start_pos": 219, "end_pos": 241, "type": "TASK", "confidence": 0.6712134679158529}]}, {"text": "However, 'explicit' feature combinations significantly increase the feature space, which slows down not only training but also testing of the classifier.", "labels": [], "entities": []}, {"text": "Kernel-based methods such as support vector machines (SVMs) consider feature combinations space-efficiently by using a polynomial kernel function.", "labels": [], "entities": []}, {"text": "The kernelbased classification is, however, known to be very slow in NLP tasks, so efficient classifiers should sum up the weights of the explicit conjunctive features (.", "labels": [], "entities": []}, {"text": "1 -regularized log-linear models ( 1 -LLMs), on the other hand, provide sparse solutions, in which weights of irrelevant features are exactly zero, by assuming a Laplacian prior on the weights.", "labels": [], "entities": []}, {"text": "However, as have reported in a text categorization task and we later confirm in a dependency parsing task, when most features regarded as irrelevant during training 1 -LLMs appear rarely in the task, we cannot greatly reduce the number of active features in each classification.", "labels": [], "entities": [{"text": "dependency parsing task", "start_pos": 82, "end_pos": 105, "type": "TASK", "confidence": 0.8462758660316467}]}, {"text": "In the end, when efficiency is a major concern, we must use exhaustive feature selection () or even restrict the order of conjunctive features at the expense of accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 161, "end_pos": 169, "type": "METRIC", "confidence": 0.9971539974212646}]}, {"text": "In this study, we provide a simple, but effective solution to the inefficiency of classifiers trained with higher-order conjunctive features (or polynomial kernel), by exploiting the Zipfian nature of language data.", "labels": [], "entities": []}, {"text": "The key idea is to precompute the weights of primitive feature vectors and use them as partial results to compute the weight of a given feature vector.", "labels": [], "entities": []}, {"text": "We use a trie called the feature sequence trie to efficiently find fora given feature vector its longest prefix feature vector whose weight has been computed.", "labels": [], "entities": []}, {"text": "The trie is built from feature vectors generated by applying the classifier to actual data in the classification task.", "labels": [], "entities": []}, {"text": "The time complexity of the classifier approaches time that is linear with respect to the number of primitive features when the retrieved feature vector covers most of the features in the input feature vector.", "labels": [], "entities": []}, {"text": "We implemented our algorithm for SVM and LLM classifiers and evaluated the performance of the resulting classifiers in a Japanese dependency parsing task.", "labels": [], "entities": [{"text": "Japanese dependency parsing task", "start_pos": 121, "end_pos": 153, "type": "TASK", "confidence": 0.647916167974472}]}, {"text": "Experimental results show that it successfully speeded up classifiers trained with higher-order conjunctive features by a factor of 10.", "labels": [], "entities": []}, {"text": "The rest of this paper is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 introduces LLMs and SVMs.", "labels": [], "entities": [{"text": "LLMs", "start_pos": 21, "end_pos": 25, "type": "TASK", "confidence": 0.9429099559783936}]}, {"text": "Section 3 proposes our classification algorithm.", "labels": [], "entities": [{"text": "classification", "start_pos": 23, "end_pos": 37, "type": "TASK", "confidence": 0.9682924747467041}]}, {"text": "Section 4 presents experimental results.", "labels": [], "entities": []}, {"text": "Section 5 concludes with a summary and addresses future directions.", "labels": [], "entities": []}], "datasetContent": [{"text": "We applied our algorithm to SVM-KE, SVM-HKE, and 1 -LLM classifiers and evaluated the resulting classifiers in a Japanese dependency parsing task.", "labels": [], "entities": [{"text": "Japanese dependency parsing task", "start_pos": 113, "end_pos": 145, "type": "TASK", "confidence": 0.6148104295134544}]}, {"text": "To the best of our knowledge, there are no previous reports of an exact weight calculation faster than linear summation (Eqs. 1 and 5).", "labels": [], "entities": []}, {"text": "We also compared our SVM classifier with a classifier called polynomial kernel inverted (PKI:), which uses the polynomial kernel (Eq. 4) and inverted indexing to support vectors.", "labels": [], "entities": []}, {"text": "A Japanese dependency parser inputs bunsetsusegmented sentences and outputs the correct head (bunsetsu) for each bunsetsu; here, a bunsetsu is a grammatical unit in Japanese consisting of one or more content words followed by zero or more function words.", "labels": [], "entities": []}, {"text": "A parser generates a feature vec-Modifier, modifiee bunsetsu headword (surface-form, POS, POS-subcategory, inflection form), functional word (surface-form, POS, POS-subcategory, inflection form), brackets, quotation marks, punctuation marks, position in sentence (beginning, end) Between bunsetsus distance (1, 2-5, 6-), case-particles, brackets, quotation marks, punctuation marks: Feature set used for experiments.", "labels": [], "entities": []}, {"text": "tor fora particular pair of bunsetsus (modifier and modifiee candidates) by exploiting the head-final and projective nature of dependency relations in Japanese.", "labels": [], "entities": []}, {"text": "The classifier then outputs label y = '+1' (dependent) or '\u22121' (independent).", "labels": [], "entities": []}, {"text": "Since our classifier is independent of individual parsing algorithms, we targeted speeding up (a classifier in) the shift-reduce parser proposed by, which has been reported to be the most efficient for this task, with almost stateof-the-art accuracy (.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 241, "end_pos": 249, "type": "METRIC", "confidence": 0.9988580942153931}]}, {"text": "This parser decreases the number of classification steps by using the fact that a bunsetsu is likely to modify a bunsetsu close to itself.", "labels": [], "entities": []}, {"text": "Due to space limitations, we omit the details of the parsing algorithm.", "labels": [], "entities": [{"text": "parsing", "start_pos": 53, "end_pos": 60, "type": "TASK", "confidence": 0.970359206199646}]}, {"text": "We used the standard feature set tailored for this task () ().", "labels": [], "entities": []}, {"text": "Note that features listed in the 'Between bunsetsus' row represent contexts between the target pair of bunsetsus and appear independently from other features, which will become an obstacle to finding the longest prefix vector.", "labels": [], "entities": []}, {"text": "This task is therefore a better measure of our method than simple sequential labeling such as POS tagging or named-entity recognition.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 94, "end_pos": 105, "type": "TASK", "confidence": 0.8026488721370697}, {"text": "named-entity recognition", "start_pos": 109, "end_pos": 133, "type": "TASK", "confidence": 0.7163641154766083}]}, {"text": "For evaluation, we used Kyoto Text Corpus Version 4.0 (, Mainichi news articles in 1995 that have been manually annotated with dependency relations.", "labels": [], "entities": [{"text": "Kyoto Text Corpus Version 4.0", "start_pos": 24, "end_pos": 53, "type": "DATASET", "confidence": 0.9674623250961304}, {"text": "Mainichi news articles in 1995", "start_pos": 57, "end_pos": 87, "type": "DATASET", "confidence": 0.9747750401496887}]}, {"text": "The training, development, and test sets included, and 89,874 bunsetsus, respectively.", "labels": [], "entities": []}, {"text": "The training samples generated from the training set included 150,064 positive and 146,712 negative samples.", "labels": [], "entities": []}, {"text": "The following experiments were performed on a server with an Intel R Xeon TM 3.20-GHz CPU.", "labels": [], "entities": []}, {"text": "We used TinySVM and a simple C++ library for maximum entropy classification   a double-array trie, as a compact trie implementation.", "labels": [], "entities": [{"text": "TinySVM", "start_pos": 8, "end_pos": 15, "type": "DATASET", "confidence": 0.9379675984382629}]}, {"text": "All these libraries and algorithms are implemented in C++.", "labels": [], "entities": []}, {"text": "The code for building fstries occupies 100 lines, while the code for the classifier occupies 20 lines (except those for kernel expansion).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Feature set used for experiments.", "labels": [], "entities": []}, {"text": " Table 3: Parsing results for test corpus: SVM-KE classifiers with dense feature space.", "labels": [], "entities": []}, {"text": " Table 4: Parsing results for test corpus: SVM-HKE and 1 -LLM classifiers with sparse feature space.", "labels": [], "entities": []}]}