{"title": [{"text": "Tree Kernel-based SVM with Structured Syntactic Know- ledge for BTG-based Phrase Reordering", "labels": [], "entities": [{"text": "Phrase Reordering", "start_pos": 74, "end_pos": 91, "type": "TASK", "confidence": 0.6572151035070419}]}], "abstractContent": [{"text": "Structured syntactic knowledge is important for phrase reordering.", "labels": [], "entities": [{"text": "phrase reordering", "start_pos": 48, "end_pos": 65, "type": "TASK", "confidence": 0.820218414068222}]}, {"text": "This paper proposes using convolution tree kernel over source parse tree to model structured syntactic knowledge for BTG-based phrase reordering in the context of statistical machine translation.", "labels": [], "entities": [{"text": "BTG-based phrase reordering", "start_pos": 117, "end_pos": 144, "type": "TASK", "confidence": 0.6911057631174723}, {"text": "statistical machine translation", "start_pos": 163, "end_pos": 194, "type": "TASK", "confidence": 0.658672034740448}]}, {"text": "Our study reveals that the structured syntactic features over the source phrases are very effective for BTG constraint-based phrase reordering and those features can be well captured by the tree kernel.", "labels": [], "entities": [{"text": "BTG constraint-based phrase reordering", "start_pos": 104, "end_pos": 142, "type": "TASK", "confidence": 0.7249549329280853}]}, {"text": "We further combine the structured features and other commonly-used linear features into a composite kernel.", "labels": [], "entities": []}, {"text": "Experimental results on the NIST MT-2005 Chinese-English translation tasks show that our proposed phrase reordering model statistically significantly outperforms the baseline methods.", "labels": [], "entities": [{"text": "NIST MT-2005 Chinese-English translation tasks", "start_pos": 28, "end_pos": 74, "type": "TASK", "confidence": 0.7599422216415406}, {"text": "phrase reordering", "start_pos": 98, "end_pos": 115, "type": "TASK", "confidence": 0.8487086892127991}]}], "introductionContent": [{"text": "Phrase-based method ( and syntaxbased method;;) represent the state-of-the-art technologies in statistical machine translation (SMT).", "labels": [], "entities": [{"text": "statistical machine translation (SMT)", "start_pos": 95, "end_pos": 132, "type": "TASK", "confidence": 0.8227629661560059}]}, {"text": "As the two technologies are complementary in many ways, an interesting research topic is how to combine the strengths of the two methods.", "labels": [], "entities": []}, {"text": "Many research efforts have been made to address this issue, which can be summarized into two ideas.", "labels": [], "entities": []}, {"text": "One is to add syntax into phrase-based model while another one is to enhance syntaxbased model to handle non-syntactic phrases.", "labels": [], "entities": []}, {"text": "In this paper, we bring forward the first idea by studying the issue of how to utilize structured syntactic features for phrase reordering in a phrase-based SMT system with BTG (Bracketing Transduction Grammar) constraints.", "labels": [], "entities": [{"text": "phrase reordering", "start_pos": 121, "end_pos": 138, "type": "TASK", "confidence": 0.7549312710762024}, {"text": "SMT", "start_pos": 157, "end_pos": 160, "type": "TASK", "confidence": 0.876144528388977}]}, {"text": "Word and phrase reordering is a crucial component in a SMT system.", "labels": [], "entities": [{"text": "Word and phrase reordering", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.5387838333845139}, {"text": "SMT", "start_pos": 55, "end_pos": 58, "type": "TASK", "confidence": 0.9968164563179016}]}, {"text": "In syntax-based method, word reordering is implicitly addressed by translation rules, thus the performance is subject to parsing errors to a large extent) and the impact of syntax on reordering is difficult to single out ( . In phrasebased method, local word reordering 1 can be effectively captured by phrase pairs directly while local phrase reordering is explicitly modeled by phrase reordering model and distortion model.", "labels": [], "entities": []}, {"text": "Recently, many phrase reordering methods have been proposed, ranging from simple distancebased distortion model (), flat reordering model), lexicalized reordering model), to hierarchical phrase-based model and classifier-based reordering model with linear features ().", "labels": [], "entities": [{"text": "phrase reordering", "start_pos": 15, "end_pos": 32, "type": "TASK", "confidence": 0.7999667525291443}]}, {"text": "However, one of the major limitations of these advances is the structured syntactic knowledge, which is important to global reordering (, has not been well exploited.", "labels": [], "entities": []}, {"text": "This makes the phrasebased method particularly weak in handling global phrase reordering.", "labels": [], "entities": [{"text": "global phrase reordering", "start_pos": 64, "end_pos": 88, "type": "TASK", "confidence": 0.5958843231201172}]}, {"text": "From machine learning viewpoint, it is computationally infeasible to explicitly generate features involving structured information in many NLP applica-tions.", "labels": [], "entities": []}, {"text": "For example, one cannot enumerate efficiently all the sub-tree features fora full parse tree.", "labels": [], "entities": []}, {"text": "This would be the reason why structured features are not fully utilized in previous statistical feature-based phrase reordering model.", "labels": [], "entities": [{"text": "statistical feature-based phrase reordering", "start_pos": 84, "end_pos": 127, "type": "TASK", "confidence": 0.5963708609342575}]}, {"text": "Thanks to the nice property of kernel-based machine learning method that can implicitly explore (structured) features in a high dimensional feature space, in this paper we propose using convolution tree kernel) to explore the structured syntactic knowledge for phrase reordering and further combine the tree kernel with other diverse linear features into a composite kernel to strengthen the model's predictive ability.", "labels": [], "entities": [{"text": "phrase reordering", "start_pos": 261, "end_pos": 278, "type": "TASK", "confidence": 0.7791413366794586}]}, {"text": "Indeed, using tree kernel methods to mine structured knowledge has shown success in some NLP applications like parsing), semantic role labeling), relation extraction (), pronoun resolution () and question classification.", "labels": [], "entities": [{"text": "parsing", "start_pos": 111, "end_pos": 118, "type": "TASK", "confidence": 0.9669169783592224}, {"text": "semantic role labeling", "start_pos": 121, "end_pos": 143, "type": "TASK", "confidence": 0.669503927230835}, {"text": "relation extraction", "start_pos": 146, "end_pos": 165, "type": "TASK", "confidence": 0.8895075023174286}, {"text": "pronoun resolution", "start_pos": 170, "end_pos": 188, "type": "TASK", "confidence": 0.7849146127700806}, {"text": "question classification", "start_pos": 196, "end_pos": 219, "type": "TASK", "confidence": 0.8843521773815155}]}, {"text": "However, to our knowledge, such technique still remains unexplored for phrase reordering.", "labels": [], "entities": [{"text": "phrase reordering", "start_pos": 71, "end_pos": 88, "type": "TASK", "confidence": 0.8583924174308777}]}, {"text": "In this paper, we look into the phrase reordering problem in two aspects: 1) how to model and optimize structured features, and 2) how to combine the structured features with other linear features and further integrate them into the loglinear model-based translation framework.", "labels": [], "entities": [{"text": "phrase reordering", "start_pos": 32, "end_pos": 49, "type": "TASK", "confidence": 0.7327852249145508}]}, {"text": "Our study shows that: 1) the structured syntactic features are very useful and 2) our kernel-based model can well explore diverse knowledge, including previously-used linear features and the structured syntactic features, for phrase reordering.", "labels": [], "entities": [{"text": "phrase reordering", "start_pos": 226, "end_pos": 243, "type": "TASK", "confidence": 0.8275968432426453}]}, {"text": "Our model displays one advantage over the previous work that it is able to utilize the structured syntactic features without the need for extensive feature engineering in decoding a parse tree into a set of linear syntactic features.", "labels": [], "entities": []}, {"text": "To have a more insightful evaluation, we design three experiments with three different evaluation metrics.", "labels": [], "entities": []}, {"text": "Experimental results on the NIST MT-2005 Chinese-English translation tasks show that our method statistically significantly outperforms the baseline methods in term of the three different evaluation metrics.", "labels": [], "entities": [{"text": "NIST MT-2005 Chinese-English translation tasks", "start_pos": 28, "end_pos": 74, "type": "TASK", "confidence": 0.7480204701423645}]}, {"text": "The rest of the paper is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 introduces the baseline method of BTG-based phrase translation method while section 3 discusses the proposed method in detail.", "labels": [], "entities": [{"text": "BTG-based phrase translation", "start_pos": 44, "end_pos": 72, "type": "TASK", "confidence": 0.6296870013078054}]}, {"text": "The experimental results are reported and discussed in section 4.", "labels": [], "entities": []}, {"text": "Finally, we conclude the paper in section 5.", "labels": [], "entities": []}], "datasetContent": [{"text": "Basic Settings: we evaluate our method on Chinese-English translation task.", "labels": [], "entities": [{"text": "Chinese-English translation task", "start_pos": 42, "end_pos": 74, "type": "TASK", "confidence": 0.7173029681046804}]}, {"text": "We use the FBIS corpus as training set, the NIST MT-2002 test set as development (dev) set and the NIST MT-2005 test set as test set.", "labels": [], "entities": [{"text": "FBIS corpus", "start_pos": 11, "end_pos": 22, "type": "DATASET", "confidence": 0.9259953200817108}, {"text": "NIST MT-2002 test set", "start_pos": 44, "end_pos": 65, "type": "DATASET", "confidence": 0.898906871676445}, {"text": "NIST MT-2005 test set", "start_pos": 99, "end_pos": 120, "type": "DATASET", "confidence": 0.9342682808637619}]}, {"text": "The Stanford parser) is used to parse Chinese sentences on the training, dev and test sets.", "labels": [], "entities": []}, {"text": "GIZA++ () and the heuristics \"growdiag-final-and\" are used to generate m-to-n word alignments.", "labels": [], "entities": []}, {"text": "The translation model is trained on the FBIS corpus and a 4-gram language model is trained on the Xinhua portion of the English Gigaword corpus using the SRILM Toolkits) with modified Kneser-Ney smoothing.", "labels": [], "entities": [{"text": "FBIS corpus", "start_pos": 40, "end_pos": 51, "type": "DATASET", "confidence": 0.9612185060977936}, {"text": "English Gigaword corpus", "start_pos": 120, "end_pos": 143, "type": "DATASET", "confidence": 0.7930061022440592}]}, {"text": "For the MER training, we modify Koehn's MER trainer) to train our system.", "labels": [], "entities": [{"text": "MER", "start_pos": 8, "end_pos": 11, "type": "TASK", "confidence": 0.950204610824585}]}, {"text": "For significance test, we use Zhang et al's implementation ().", "labels": [], "entities": []}, {"text": "Baseline Systems: we set three baseline systems: B1) Moses ( ) that uses lexicalized unigram reordering model to predict three orientations: monotone, swap and discontinuous; B2) MaxEnt-based reordering model with lexical boundary word features only (); B3) Linguistically annotated reordering model for BTG-based (LABTG) SMT ().", "labels": [], "entities": [{"text": "BTG-based (LABTG) SMT", "start_pos": 304, "end_pos": 325, "type": "TASK", "confidence": 0.6739800035953522}]}, {"text": "For Moses, we used the default settings.", "labels": [], "entities": []}, {"text": "We build a CKY-style decoder and integrate the corresponding reordering modelling methods into the decoder to implement the 2 nd and the 3 rd baseline systems and our system.", "labels": [], "entities": []}, {"text": "Except reordering models, all the four systems use the same features in translation model, language model and distortion model as Moses in the loglinear framework.", "labels": [], "entities": []}, {"text": "We tune the four systems using the strategies as discussed previously in this section.", "labels": [], "entities": []}, {"text": "Reordering Model Training: we extract all reordering instances from the m-to-n wordaligned training corpus.", "labels": [], "entities": []}, {"text": "The reordering instances include the two source phrases, two target phrases, order label and its corresponding parse tree.", "labels": [], "entities": []}, {"text": "We generate the boundary word features from the extracted reordering instances in the same way as discussed in and use Zhang's MaxEnt Tools 2 to train a reordering model for the 2 nd baseline system.", "labels": [], "entities": []}, {"text": "Similarly, we use the algorithm 1 in to extract features and use the same MaxEnt Tools to train a reordering model for the 3 rd baseline system.", "labels": [], "entities": []}, {"text": "Based on the extracted reordering instances, we generate the four structured features and the linear features, and then use the Tree Kernel Tools) to train our kernel-based reordering model (linear, tree and composite).", "labels": [], "entities": []}, {"text": "Experimental Design and Evaluation Metrics: we design three experiments and evaluate them using three metrics.", "labels": [], "entities": []}, {"text": "Classification-based: in the first experiment, we extract all reordering instances and their features from the dev and test sets, and then use the reordering models trained on the training set to classify (label) those instances extracted from the dev and test sets.", "labels": [], "entities": []}, {"text": "In this way, we can isolate the reordering problem from the influence of others, such as translation model, pruning and decoding strategies, to better examine the reordering models' ability and to give analytical insights into the features.", "labels": [], "entities": [{"text": "translation model", "start_pos": 89, "end_pos": 106, "type": "TASK", "confidence": 0.9055584669113159}]}, {"text": "Classification Accuracy (CAcc), the percentage of the correctly labeled instances overall trials, is used as the evaluation metric.", "labels": [], "entities": [{"text": "Classification Accuracy (CAcc)", "start_pos": 0, "end_pos": 30, "type": "METRIC", "confidence": 0.7258962750434875}]}, {"text": "Forced decoding 3 -based and normal decodingbased: the two experiments evaluate the reordering models through areal SMT system.", "labels": [], "entities": [{"text": "SMT", "start_pos": 116, "end_pos": 119, "type": "TASK", "confidence": 0.9251648783683777}]}, {"text": "The reordering model and the language model are the same in the two experiments.", "labels": [], "entities": []}, {"text": "However, in forced decoding, we train two translation models, one using training data only while another using both training, dev and test data.", "labels": [], "entities": []}, {"text": "By forced decoding, we aim to isolate the reordering problem from those of OOV and lexical selections resulting from imperfect translation model in the context of areal SMT task.", "labels": [], "entities": [{"text": "SMT task", "start_pos": 169, "end_pos": 177, "type": "TASK", "confidence": 0.8938038647174835}]}, {"text": "Besides the the case-sensitive BLEU-4 () used in the two experiments, we design another evaluation metrics Reordering Accuracy (RAcc) for forced decoding evaluation.", "labels": [], "entities": [{"text": "BLEU-4", "start_pos": 31, "end_pos": 37, "type": "METRIC", "confidence": 0.9880161881446838}, {"text": "Reordering Accuracy (RAcc)", "start_pos": 107, "end_pos": 133, "type": "METRIC", "confidence": 0.9355465292930603}]}, {"text": "RAcc is the percentage of the adjacent word pairs with correct word order 4 overall words in one-best translation results.", "labels": [], "entities": [{"text": "RAcc", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.9645206928253174}]}, {"text": "Similar to BLEU score, we also use the similar Brevity Penalty BP () to penalize the short translations in computing RAcc.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 11, "end_pos": 21, "type": "METRIC", "confidence": 0.9842671155929565}, {"text": "Brevity Penalty BP", "start_pos": 47, "end_pos": 65, "type": "METRIC", "confidence": 0.9695833524068197}]}, {"text": "Finally, please note for the three evaluation metrics, the higher values represent better performance.", "labels": [], "entities": []}, {"text": "78.92 78.67: Performance of our methods on the dev and test sets with different feature combinations  Classification of Instances: reports the performance of our defined four structured features, linear feature and the composite kernel.", "labels": [], "entities": []}, {"text": "The results are summarized as follows.", "labels": [], "entities": []}, {"text": "The last row reports the performance without using any reordering features.", "labels": [], "entities": []}, {"text": "We just suppose that all the translations are monotonic, no reordering happens.", "labels": [], "entities": []}, {"text": "The CAccs of 78.92% and 78.67% serve as the bottom line in our study.", "labels": [], "entities": [{"text": "CAccs", "start_pos": 4, "end_pos": 9, "type": "METRIC", "confidence": 0.9737994074821472}]}, {"text": "Compared with the bottom line, the tree kernels over the 4 structured features are very effective for phrase reordering since only structured information is used in the tree kernel . The CTs performs the worst among the 4 structured features.", "labels": [], "entities": [{"text": "phrase reordering", "start_pos": 102, "end_pos": 119, "type": "TASK", "confidence": 0.8600359261035919}]}, {"text": "This suggests that the middle and high-level structures beyond base phrases are very useful for phrase reordering.", "labels": [], "entities": [{"text": "phrase reordering", "start_pos": 96, "end_pos": 113, "type": "TASK", "confidence": 0.8331164121627808}]}, {"text": "The MSSs show lower performance than the CMSSs and the MSTs achieve the best performance.", "labels": [], "entities": []}, {"text": "This clearly indicates that the structured context information is useful for phrase reordering.", "labels": [], "entities": [{"text": "phrase reordering", "start_pos": 77, "end_pos": 94, "type": "TASK", "confidence": 0.843941867351532}]}, {"text": "For this reason, the subsequent discussions are focused on the MSTs, unless otherwise specified.", "labels": [], "entities": [{"text": "MSTs", "start_pos": 63, "end_pos": 67, "type": "TASK", "confidence": 0.8323038816452026}]}, {"text": "The MSSs without using the 5 function tags perform much worse than the original ones.", "labels": [], "entities": []}, {"text": "This suggests that the partitions of the structured feature spaces are very helpful, which can effectively avoid the undesired matching between partitions of different functionalities.", "labels": [], "entities": []}, {"text": "Comparison of K land K l-LM shows the LM plays an important role in phrase reordering.", "labels": [], "entities": [{"text": "phrase reordering", "start_pos": 68, "end_pos": 85, "type": "TASK", "confidence": 0.7798389792442322}]}, {"text": "The composite kernel (K c ) performs much better than the two individual kernels.", "labels": [], "entities": []}, {"text": "This suggests that the structured and linear features are complementary and the composite kernel can well integrate them for phrase reordering.", "labels": [], "entities": [{"text": "phrase reordering", "start_pos": 125, "end_pos": 142, "type": "TASK", "confidence": 0.7768088281154633}]}, {"text": "compares the performance of the baseline methods with ours.", "labels": [], "entities": []}, {"text": "Comparison between B3_1 and MST clearly demonstrates that the structured syntactic features are much more effective than the linear syntactic features that are manually extracted via heuristics.", "labels": [], "entities": [{"text": "MST", "start_pos": 28, "end_pos": 31, "type": "DATASET", "confidence": 0.8104621171951294}]}, {"text": "It also suggests that the tree kernel can well capture the structured features implicitly.", "labels": [], "entities": []}, {"text": "This is mainly due to the contribution of LM features.", "labels": [], "entities": []}, {"text": "B2 (MaxEnt-based) significantly outperforms K l-LM in (SVM-based).", "labels": [], "entities": [{"text": "B2", "start_pos": 0, "end_pos": 2, "type": "METRIC", "confidence": 0.8719968795776367}]}, {"text": "This suggests that phrase reordering may not be a good linearly binary-separable task if only boundary word features are used.", "labels": [], "entities": [{"text": "phrase reordering", "start_pos": 19, "end_pos": 36, "type": "TASK", "confidence": 0.7836733162403107}]}, {"text": "Our composite kernel (K c ) significantly outperforms LABTG (B3).", "labels": [], "entities": [{"text": "LABTG", "start_pos": 54, "end_pos": 59, "type": "DATASET", "confidence": 0.7249393463134766}]}, {"text": "This mainly attributes to the contributions of structured syntactic features, LM and the tree kernel.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Performance of our methods on the  dev and test sets with different feature combi- nations", "labels": [], "entities": []}, {"text": " Table 2: Performance comparison of different me- thods", "labels": [], "entities": []}, {"text": " Table 3: Performance comparison of forced de- coding", "labels": [], "entities": [{"text": "forced de- coding", "start_pos": 36, "end_pos": 53, "type": "TASK", "confidence": 0.6540466696023941}]}]}