{"title": [{"text": "Graphical Models over Multiple Strings *", "labels": [], "entities": []}], "abstractContent": [{"text": "We study graphical modeling in the case of string-valued random variables.", "labels": [], "entities": []}, {"text": "Whereas a weighted finite-state transducer can model the probabilis-tic relationship between two strings, we are interested in building up joint models of three or more strings.", "labels": [], "entities": []}, {"text": "This is needed for inflectional paradigms in morphology, cognate modeling or language reconstruction , and multiple-string alignment.", "labels": [], "entities": [{"text": "language reconstruction", "start_pos": 77, "end_pos": 100, "type": "TASK", "confidence": 0.7303313314914703}, {"text": "multiple-string alignment", "start_pos": 107, "end_pos": 132, "type": "TASK", "confidence": 0.7611951231956482}]}, {"text": "We propose a Markov Random Field in which each factor (potential function) is a weighted finite-state machine, typically a transducer that evaluates the relationship between just two of the strings.", "labels": [], "entities": []}, {"text": "The full joint distribution is then a product of these factors.", "labels": [], "entities": []}, {"text": "Though decoding is actually undecidable in general, we can still do efficient joint inference using approximate belief propagation; the necessary computations and messages are all finite-state.", "labels": [], "entities": [{"text": "approximate belief propagation", "start_pos": 100, "end_pos": 130, "type": "TASK", "confidence": 0.7388287583986918}]}, {"text": "We demonstrate the methods by jointly predicting morphological forms.", "labels": [], "entities": []}, {"text": "1 Overview This paper considers what happens if a graphical model's variables can range over strings of unbounded length, rather than over the typical finite domains such as booleans, words, or tags.", "labels": [], "entities": []}, {"text": "Variables that are connected in the graphical model are related by some weighted finite-state transduction.", "labels": [], "entities": []}, {"text": "Graphical models have become popular in machine learning as a principled way to work with collections of interrelated random variables.", "labels": [], "entities": []}, {"text": "Most often they are used as follows: 1.", "labels": [], "entities": []}, {"text": "Build: Manually specify then variables of interest; their domains; and the possible direct interactions among them.", "labels": [], "entities": []}, {"text": "2. Train: Train this model's parameters \u03b8 to obtain a specific joint probability distribution p(V 1 ,. .. , V n) over then variables.", "labels": [], "entities": []}, {"text": "3. Infer: Use this joint distribution to predict the values of various unobserved variables from observed ones.", "labels": [], "entities": []}, {"text": "requires intuitions about the domain; 2.", "labels": [], "entities": []}, {"text": "requires some choice of training procedure; and 3.", "labels": [], "entities": []}, {"text": "requires a choice of exact or approximate inference algorithm.", "labels": [], "entities": []}, {"text": "Our graphical models over strings are natural objects to investigate.", "labels": [], "entities": []}, {"text": "We motivate them with some natural applications in computational linguistics (section 2).", "labels": [], "entities": [{"text": "computational linguistics", "start_pos": 51, "end_pos": 76, "type": "TASK", "confidence": 0.7322591543197632}]}, {"text": "We then give our formalism: a Markov Random Field whose potential functions are rational weighted languages and relations (sec-tion 3).", "labels": [], "entities": []}, {"text": "Next, we point out that inference is in general undecidable, and explain how to do approximate inference using message-passing algorithms such as belief propagation (section 4).", "labels": [], "entities": [{"text": "belief propagation", "start_pos": 146, "end_pos": 164, "type": "TASK", "confidence": 0.7214415222406387}]}, {"text": "The messages are represented as weighted finite-state machines.", "labels": [], "entities": []}, {"text": "Finally, we report on some initial experiments using these methods (section 7).", "labels": [], "entities": []}, {"text": "We use incomplete data to train a joint model of morphological paradigms, then use the trained model to complete the data by predicting unseen forms.", "labels": [], "entities": []}, {"text": "2 Motivation The problem of mapping between different forms and representations of strings is ubiquitous in natural language processing and computational linguistics.", "labels": [], "entities": []}, {"text": "This is typically done between string pairs, where a pronunciation is mapped to its spelling, an inflected form to its lemma, a spelling variant to its canonical spelling, or a name is transliterated from one alphabet into another.", "labels": [], "entities": []}, {"text": "However, many problems involve more than just two strings: \u2022 in morphology, the inflected forms of a (possi-bly irregular) verb are naturally considered together as a whole morphological paradigm in which different forms reinforce one another; \u2022 mapping an English word to its foreign translit-eration maybe easier when one considers the orthographic and phonological forms of both words; \u2022 similar cognates in multiple languages are naturally described together, in orthographic or phonological representations, or both; 101", "labels": [], "entities": []}], "introductionContent": [], "datasetContent": [{"text": "To study our approach, we conducted initial experiments that reconstruct missing word forms in morphological paradigms.", "labels": [], "entities": []}, {"text": "In inflectional morphology, each uninflected verb form (lemma) is associated with a vector of forms that are inflected for tense, person, number, etc.", "labels": [], "entities": []}, {"text": "Some inflected forms maybe observed frequently in natural text, others rarely.", "labels": [], "entities": []}, {"text": "Two variables that are usually predictable from each other mayor may not keep this relationship in the case of an irregular verb.: Statistics of our training data.", "labels": [], "entities": []}, {"text": "Our task is to reconstruct (generate) specific unobserved morphological forms in a paradigm by learning from observed ones.", "labels": [], "entities": []}, {"text": "This is a particularly interesting semisupervised scenario, because different subsets of the variables are observed on different examples.", "labels": [], "entities": []}, {"text": "We used orthographic rather than phonological forms.", "labels": [], "entities": []}, {"text": "We extracted morphological paradigms for all 9393 German verbs in the CELEX morphological database.", "labels": [], "entities": [{"text": "CELEX morphological database", "start_pos": 70, "end_pos": 98, "type": "DATASET", "confidence": 0.9376310308774313}]}, {"text": "Each paradigm lists 5 present-tense and 4 past-tense indicative forms, as well as the verb's lemma, fora total of 10 string-valued variables.", "labels": [], "entities": []}, {"text": "In each paradigm, we removed, or hid, verb forms that occur only rarely in natural text, i.e, verb forms with a small frequency figure provided by CELEX.", "labels": [], "entities": [{"text": "CELEX", "start_pos": 147, "end_pos": 152, "type": "DATASET", "confidence": 0.9789890646934509}]}, {"text": "All paradigms other than sein ('to be') were now incompletely observed.", "labels": [], "entities": []}, {"text": "We trained \u03b8 on the incompletely observed paradigms.", "labels": [], "entities": []}, {"text": "As suggested in section 5, we used a variant of piecewise pseudolikelihood training.", "labels": [], "entities": []}, {"text": "Suppose there is a binary factor F attached to forms U and V . For any value of \u03b8, we can define p UV (U | V ) from the tiny MRF consisting only of U , V , and F . We can therefore compute the goodness summed overall observed (U, V ) pairs in training data.", "labels": [], "entities": []}, {"text": "We attempted to tune \u03b8 to maximize the total L UV overall U, V pairs, 21 regularized by subtracting ||\u03b8|| 2 . Note that different factors thus enjoyed different amounts of observed training data, but training was fully supervised (except for the unobserved alignments between u i and vi ).", "labels": [], "entities": []}, {"text": "At test time, we are given each lemma (e.g. brechen) and all its observed (frequent) inflected forms (e.g., brachen, bricht,.", "labels": [], "entities": []}, {"text": "), and are asked to predict the remaining (rarer) forms (e.g., breche, brichst, . .", "labels": [], "entities": []}, {"text": "). We run approximate joint inference using belief propagation.", "labels": [], "entities": [{"text": "belief propagation", "start_pos": 44, "end_pos": 62, "type": "TASK", "confidence": 0.7103948593139648}]}, {"text": "We extract our output from the final beliefs: for each unseen variable V , we preseemed to hurt in our current training setup.", "labels": [], "entities": []}, {"text": "We followed in slightly pruning the space of possible alignments.", "labels": [], "entities": []}, {"text": "We compensated by replacing their WFST, F , with the union F \u222a 10 \u221212 (0.999\u03a3 \u00d7 \u03a3) * . This ensured that the factor could still map any string to any other string (though perhaps with very low weight), guaranteeing that the intersection at the end of section 4.3 would be non-empty.", "labels": [], "entities": []}, {"text": "The second term is omitted if V is the lemma.", "labels": [], "entities": []}, {"text": "We do not train the model to predict the lemma since it is always observed in test data.", "labels": [], "entities": []}, {"text": "Unfortunately, just before press time we discovered that this was not quite what we had done.", "labels": [], "entities": []}, {"text": "A shortcut in our implementation trained pUV (U | V ) and pV U (V | U ) separately.", "labels": [], "entities": []}, {"text": "This let them make different use of the (unobserved) alignments-so that even if each individually liked the pair (u, v), they might not have been able to agree on the same accepting path for it attest time.", "labels": [], "entities": []}, {"text": "This could have slightly harmed our joint inference results, though not our baselines.dict its value to be argmax v \u02dc p V (v).", "labels": [], "entities": [{"text": "argmax", "start_pos": 107, "end_pos": 113, "type": "METRIC", "confidence": 0.9676614999771118}]}, {"text": "This prediction considers the values of all other unseen variables but sums over their possibilities.", "labels": [], "entities": []}, {"text": "This is the Bayes-optimal decoder for our scoring function, since that function reports the fraction of individual forms that were predicted perfectly.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Whole-word accuracies of the different models in reconstructing the missing forms in morphological paradigms, here  on 100 verbs (development data). The names refer to the graphs in", "labels": [], "entities": []}, {"text": " Table 4: Accuracy on test data, reported separately for  paradigms in which 1-3, 4-6, or 7-9 forms are missing.  Missing words have CELEX frequency count < 10; these are  the ones to predict. (The numbers in col. 2 add up to 9256,  not 9293, since some paradigms are incomplete in CELEX to  begin with, with no forms to be removed or evaluated.)", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9926422238349915}, {"text": "CELEX frequency count", "start_pos": 133, "end_pos": 154, "type": "METRIC", "confidence": 0.8680193026860555}, {"text": "CELEX", "start_pos": 282, "end_pos": 287, "type": "DATASET", "confidence": 0.9304669499397278}]}]}