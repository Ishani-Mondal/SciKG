{"title": [{"text": "Multi-Document Summarisation Using Generic Relation Extraction", "labels": [], "entities": [{"text": "Multi-Document Summarisation", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.6688964813947678}, {"text": "Generic Relation Extraction", "start_pos": 35, "end_pos": 62, "type": "TASK", "confidence": 0.7239156564076742}]}], "abstractContent": [{"text": "Experiments are reported that investigate the effect of various source document representations on the accuracy of the sentence extraction phase of a multi-document summarisation task.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 103, "end_pos": 111, "type": "METRIC", "confidence": 0.9987905621528625}, {"text": "sentence extraction", "start_pos": 119, "end_pos": 138, "type": "TASK", "confidence": 0.7082793116569519}, {"text": "multi-document summarisation task", "start_pos": 150, "end_pos": 183, "type": "TASK", "confidence": 0.6589572230974833}]}, {"text": "A novel representation is introduced based on generic relation extraction (GRE), which aims to build systems for relation identification and characterisation that can be transferred across domains and tasks without modification of model parameters.", "labels": [], "entities": [{"text": "generic relation extraction (GRE)", "start_pos": 46, "end_pos": 79, "type": "TASK", "confidence": 0.7889481683572134}, {"text": "relation identification and characterisation", "start_pos": 113, "end_pos": 157, "type": "TASK", "confidence": 0.6846113204956055}]}, {"text": "Results demonstrate performance that is significantly higher than a non-trivial base-line that uses tf*idf-weighted words and at least as good as a comparable but less general approach from the literature.", "labels": [], "entities": []}, {"text": "Analysis shows that the representations compared are complementary, suggesting that extraction performance could be further improved through system combination.", "labels": [], "entities": []}], "introductionContent": [{"text": "The goal of summarisation is to take an information source, extract content from it, and present the most important content in a condensed form).", "labels": [], "entities": [{"text": "summarisation", "start_pos": 12, "end_pos": 25, "type": "TASK", "confidence": 0.9917639493942261}]}, {"text": "The field of automatic summarisation aims to create tools that address various summarisation tasks with minimal human intervention.", "labels": [], "entities": [{"text": "summarisation", "start_pos": 23, "end_pos": 36, "type": "TASK", "confidence": 0.8167945146560669}, {"text": "summarisation tasks", "start_pos": 79, "end_pos": 98, "type": "TASK", "confidence": 0.9041234850883484}]}, {"text": "Extractive approaches to automatic summarisation create representations of the source document that are generally based on an easily identified text sub-unit such as sentences or paragraphs.", "labels": [], "entities": [{"text": "summarisation", "start_pos": 35, "end_pos": 48, "type": "TASK", "confidence": 0.7871952652931213}]}, {"text": "These representations are then used to identify representative or otherwise important snippets of text to place in the summary.", "labels": [], "entities": []}, {"text": "Following Sp\u00e4rck Jones (2007), summarisation systems can be characterised with respect to their approach to three main sub-tasks: 1) interpretation, 2) transformation and 3) generation.", "labels": [], "entities": [{"text": "summarisation", "start_pos": 31, "end_pos": 44, "type": "TASK", "confidence": 0.9793423414230347}]}, {"text": "The input consists of the source document (or a collection of source documents in the case of multidocument summarisation).", "labels": [], "entities": []}, {"text": "The first step (interpretation) creates a representation of the source document by performing some level of interpretation.", "labels": [], "entities": []}, {"text": "A simple approach here represents sentences by their tokens (i.e., as an unordered bag-of-words).", "labels": [], "entities": []}, {"text": "The next step (transformation) is the compaction step where the source representation is converted into the summary representation, e.g. by identifying sentences whose words are most representative of the full text.", "labels": [], "entities": []}, {"text": "Finally, in the last step (generation), the output summary is created.", "labels": [], "entities": []}, {"text": "In the case of sentence extraction, this includes various operations to maximise coherence such as ensuring that entity references are comprehensible and arranging the sentences in a sensible order.", "labels": [], "entities": [{"text": "sentence extraction", "start_pos": 15, "end_pos": 34, "type": "TASK", "confidence": 0.7440018951892853}]}, {"text": "The current work investigates several representations of source documents.", "labels": [], "entities": []}, {"text": "In particular, an approach from the literature based on atomic events () is compared to a novel approach based on generic relation extraction (GRE), which aims to build systems for relation identification and characterisation that can be transferred across domains and tasks without modification of model parameters.", "labels": [], "entities": [{"text": "generic relation extraction (GRE)", "start_pos": 114, "end_pos": 147, "type": "TASK", "confidence": 0.7384614497423172}, {"text": "relation identification", "start_pos": 181, "end_pos": 204, "type": "TASK", "confidence": 0.7325728088617325}]}, {"text": "The various representations are substituted in the interpretation phase of a multi-document summarisation task and used as the basis for extracting sentences to be placed in the summary.", "labels": [], "entities": []}, {"text": "System summaries are compared by calculating term overlap with reference summaries created by human analysts.", "labels": [], "entities": [{"text": "System summaries", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.5778063833713531}]}], "datasetContent": [{"text": "The evaluation uses Rouge 9 to determine which representation selects content that overlaps most with human summaries.", "labels": [], "entities": []}, {"text": "Rouge estimates the coverage of appropriate concepts) in a summary by comparing it to several humancreated reference summaries.", "labels": [], "entities": []}, {"text": "Rouge-1 does so by computing recall based on macro-averaged unigram overlap.", "labels": [], "entities": [{"text": "recall", "start_pos": 29, "end_pos": 35, "type": "METRIC", "confidence": 0.9986945986747742}]}, {"text": "Rouge-SU4 does so by calculating skip-bigram overlap where bigrams are allowed to  be composed of non-contiguous words (with as many as four words intervening).", "labels": [], "entities": []}, {"text": "Rouge-SU4 also includes unigrams to decrease the chances of zero scores where there is no skip-bigram overlap.", "labels": [], "entities": []}, {"text": "The configuration is based on comparisons between Rouge and human judgements of content coverage, which suggest that Rouge-1 and Rouge-SU4 with stemming and removal of stop words are good measures for evaluating multi-document summarisation tasks, consistently achieving Pearson's correlation scores above 0.72 and as high as 0.9 for longer summaries.", "labels": [], "entities": [{"text": "multi-document summarisation tasks", "start_pos": 212, "end_pos": 246, "type": "TASK", "confidence": 0.6493054628372192}, {"text": "Pearson's correlation", "start_pos": 271, "end_pos": 292, "type": "METRIC", "confidence": 0.9580633838971456}]}, {"text": "Paired Wilcoxon signed ranks tests across document sets are used to check for significant differences between systems.", "labels": [], "entities": []}, {"text": "The paired Wilcoxon signed ranks testis a non-parametric analogue of the paired t test.", "labels": [], "entities": []}, {"text": "The null hypothesis is that the two populations from which the scores are sampled are identical.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Comparison of Rouge scores for the tf*idf  (TF), event (EV) and relation (RL).", "labels": [], "entities": [{"text": "event (EV) and relation (RL)", "start_pos": 59, "end_pos": 87, "type": "METRIC", "confidence": 0.6914021571477255}]}, {"text": " Table 3: Comparison of Rouge scores for entity  pairs based on relations (ER) and events (EE).", "labels": [], "entities": []}]}