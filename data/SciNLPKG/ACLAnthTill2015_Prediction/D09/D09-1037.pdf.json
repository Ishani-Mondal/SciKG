{"title": [{"text": "A Bayesian Model of Syntax-Directed Tree to String Grammar Induction", "labels": [], "entities": [{"text": "Syntax-Directed Tree to String Grammar Induction", "start_pos": 20, "end_pos": 68, "type": "TASK", "confidence": 0.609356164932251}]}], "abstractContent": [{"text": "Tree based translation models area compelling means of integrating linguistic information into machine translation.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 95, "end_pos": 114, "type": "TASK", "confidence": 0.7352940440177917}]}, {"text": "Syntax can inform lexical selection and reordering choices and thereby improve translation quality.", "labels": [], "entities": []}, {"text": "Research to date has focussed primarily on decoding with such models, but lesson the difficult problem of inducing the bilingual grammar from data.", "labels": [], "entities": []}, {"text": "We propose a generative Bayesian model of tree-to-string translation which induces grammars that are both smaller and produce better translations than the previous heuristic two-stage approach which employs a separate word alignment step.", "labels": [], "entities": [{"text": "tree-to-string translation", "start_pos": 42, "end_pos": 68, "type": "TASK", "confidence": 0.7908411920070648}, {"text": "word alignment", "start_pos": 218, "end_pos": 232, "type": "TASK", "confidence": 0.7101430743932724}]}], "introductionContent": [{"text": "Many recent advances in statistical machine translation (SMT) area result of the incorporation of syntactic knowledge into the translation process ( ;).", "labels": [], "entities": [{"text": "statistical machine translation (SMT)", "start_pos": 24, "end_pos": 61, "type": "TASK", "confidence": 0.8028605331977209}]}, {"text": "This has been facilitated by the use of synchronous grammars to model translation as a generative process over pairs of strings in two languages.", "labels": [], "entities": []}, {"text": "Such models are particularly attractive for translating between languages with divergent word orders, such as Chinese and English, where syntax-inspired translation rules can succinctly describe the requisite reordering operations.", "labels": [], "entities": []}, {"text": "In contrast, standard phrase-based models () assume a mostly monotone mapping between source and target, and therefore cannot adequately model these phenomena.", "labels": [], "entities": []}, {"text": "Currently the most successful paradigm for the use of synchronous grammars in translation is that of stringto-tree transduction ().", "labels": [], "entities": [{"text": "stringto-tree transduction", "start_pos": 101, "end_pos": 127, "type": "TASK", "confidence": 0.734061986207962}]}, {"text": "In this case a grammar is extracted from a parallel corpus, with strings on its source side and syntax trees on its target side, which is then used to translate novel sentences by performing inference over the space of target syntax trees licensed by the grammar.", "labels": [], "entities": []}, {"text": "To date grammar-based translation models have relied on heuristics to extract a grammar from a word-aligned parallel corpus.", "labels": [], "entities": []}, {"text": "These heuristics are extensions of those developed for phrase-based models (, and involve symmetrising two directional word alignments followed by a projection step which uses the alignments to find a mapping between source words and nodes in the target parse trees ().", "labels": [], "entities": []}, {"text": "However, such approaches leave much to be desired.", "labels": [], "entities": []}, {"text": "Word-alignments rarely factorise cleanly with parse trees (i.e., alignment points cross constituent structures), resulting in large and implausible translation rules which generalise poorly to unseen data ().", "labels": [], "entities": []}, {"text": "The principal reason for employing a grammar based formalism is to induce rules which capture long-range reorderings between source and target.", "labels": [], "entities": []}, {"text": "However if the grammar itself is extracted using wordalignments induced with models that are unable to capture such reorderings, it is unlikely that the grammar will live up to expectations.", "labels": [], "entities": []}, {"text": "In this work we draw on recent advances in Bayesian modelling of grammar induction) to propose a non-parametric model of synchronous tree substitution grammar (STSG), continuing a recent trend in SMT to seek principled probabilistic formulations for heuristic translation models (.", "labels": [], "entities": [{"text": "synchronous tree substitution grammar (STSG)", "start_pos": 121, "end_pos": 165, "type": "TASK", "confidence": 0.7684269121715}, {"text": "SMT", "start_pos": 196, "end_pos": 199, "type": "TASK", "confidence": 0.9939204454421997}]}, {"text": "This model leverages a hierarchical Bayesian prior to induce a compact translation grammar directly from a parsed parallel corpus, unconstrained by word-alignments.", "labels": [], "entities": []}, {"text": "We show that the induced grammars are more plausible and improve translation output.", "labels": [], "entities": []}, {"text": "This paper is structured as follows: In Section 2 we introduce the STSG formalism and describe current heuristic approaches to grammar induction.", "labels": [], "entities": [{"text": "grammar induction", "start_pos": 127, "end_pos": 144, "type": "TASK", "confidence": 0.798733115196228}]}, {"text": "We define a principled Bayesian model of string-to-tree translation in Section 3, and describe an inference technique using Gibbs sampling in Section 4.", "labels": [], "entities": [{"text": "string-to-tree translation", "start_pos": 41, "end_pos": 67, "type": "TASK", "confidence": 0.7626520395278931}]}, {"text": "In Section 5 we analyse an induced grammar on a corpus of Chinese\u2192English translation, comparing them with a heuristic grammar in terms of grammar size and translation quality.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluate our non-parametric model of grammar induction on a subset of the NIST ChineseEnglish translation evaluation, representing a realistic SMT experiment with millions of words and long sentences.", "labels": [], "entities": [{"text": "grammar induction", "start_pos": 40, "end_pos": 57, "type": "TASK", "confidence": 0.7269699573516846}, {"text": "NIST ChineseEnglish translation evaluation", "start_pos": 77, "end_pos": 119, "type": "DATASET", "confidence": 0.8420908749103546}, {"text": "SMT", "start_pos": 146, "end_pos": 149, "type": "TASK", "confidence": 0.9920951724052429}]}, {"text": "The Chinese-English training data consists of the FBIS corpus (LDC2003E14) and the first 100k sentence pairs from the Sinorama corpus (LDC2005E47).", "labels": [], "entities": [{"text": "FBIS corpus (LDC2003E14", "start_pos": 50, "end_pos": 73, "type": "DATASET", "confidence": 0.8551511168479919}, {"text": "Sinorama corpus", "start_pos": 118, "end_pos": 133, "type": "DATASET", "confidence": 0.9201444089412689}]}, {"text": "The Chinese text was segmented with a CRF-based Chinese segmenter optimized for MT (, and the English text was parsed using the Stanford parser (.", "labels": [], "entities": [{"text": "MT", "start_pos": 80, "end_pos": 82, "type": "TASK", "confidence": 0.7275608777999878}]}, {"text": "As a baseline we implemented the heuristic grammar extraction technique of (henceforth GHKM).", "labels": [], "entities": [{"text": "heuristic grammar extraction", "start_pos": 33, "end_pos": 61, "type": "TASK", "confidence": 0.6142370700836182}, {"text": "GHKM", "start_pos": 87, "end_pos": 91, "type": "DATASET", "confidence": 0.8569718599319458}]}, {"text": "This method finds the minimum sized translation rules which are consistent with a word-aligned sentence pair, as described in section 2.1.", "labels": [], "entities": []}, {"text": "The rules are then treated as events in a relative frequency estimate.", "labels": [], "entities": []}, {"text": "We used Giza++ Model 4 to obtain word alignments, using the grow-diag-final-and heuristic to symmetrise the two directional predictions (.", "labels": [], "entities": [{"text": "word alignments", "start_pos": 33, "end_pos": 48, "type": "TASK", "confidence": 0.7546151280403137}]}, {"text": "The model was sampled for 300 iterations to 'burn-in', wherein each iteration we applied both sampling operators to all nodes (or node pairs) of all training instances.", "labels": [], "entities": []}, {"text": "We initialised the sampler using the GHKM derivation of the training data (the baseline system).", "labels": [], "entities": []}, {"text": "The final state of the sampler was used to extract the grammar.", "labels": [], "entities": []}, {"text": "The hyperparameters were set by hand to \u03b1 = 10 6 , p child = 0.5, p expand = 0.5, and p term = 0.5. 5 Overall the model took on average 2,218s per full iteration of Gibbs sampling and 1 week in total to train, using a single core of a 2.3Ghz AMD Opteron machine.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Grammar rules specified by the derivation in Fig- ure 1. Each rule is shown as a tuple comprising a tar- get elementary tree and a source string. Boxed numbers  show the alignment between string variables and frontier non- terminals.", "labels": [], "entities": [{"text": "Fig- ure 1", "start_pos": 55, "end_pos": 65, "type": "DATASET", "confidence": 0.9080332666635513}]}, {"text": " Table 2:  NIST Chinese-English corpora statistics  (LDC2003E14, LDC2005E47).", "labels": [], "entities": [{"text": "NIST Chinese-English corpora statistics", "start_pos": 11, "end_pos": 50, "type": "DATASET", "confidence": 0.9101930558681488}]}, {"text": " Table 5: Top five rules which include the possessive particle  and at least one variable.", "labels": [], "entities": []}]}