{"title": [{"text": "Multi-Word Expression Identification Using Sentence Surface Features", "labels": [], "entities": [{"text": "Multi-Word Expression Identification", "start_pos": 0, "end_pos": 36, "type": "TASK", "confidence": 0.8098610639572144}]}], "abstractContent": [{"text": "Much NLP research on Multi-Word Expressions (MWEs) focuses on the discovery of new expressions, as opposed to the identification in texts of known expressions.", "labels": [], "entities": [{"text": "Multi-Word Expressions (MWEs)", "start_pos": 21, "end_pos": 50, "type": "TASK", "confidence": 0.7318464040756225}, {"text": "identification in texts of known expressions", "start_pos": 114, "end_pos": 158, "type": "TASK", "confidence": 0.7666794856389364}]}, {"text": "However, MWE identification is not trivial because many expressions allow variation inform and differ in the range of variations they allow.", "labels": [], "entities": [{"text": "MWE identification", "start_pos": 9, "end_pos": 27, "type": "TASK", "confidence": 0.9887038171291351}]}, {"text": "We show that simple rule-based baselines do not perform identification satisfactorily, and present a supervised learning method for identification that uses sentence surface features based on expressions' canonical form.", "labels": [], "entities": []}, {"text": "To evaluate the method, we have annotated 3350 sentences from the British National Corpus, containing potential uses of 24 verbal MWEs.", "labels": [], "entities": [{"text": "British National Corpus", "start_pos": 66, "end_pos": 89, "type": "DATASET", "confidence": 0.9472585717837015}]}, {"text": "The method achieves an F-score of 94.86%, compared with 80.70% for the leading rule-based base-line.", "labels": [], "entities": [{"text": "F-score", "start_pos": 23, "end_pos": 30, "type": "METRIC", "confidence": 0.9998049139976501}]}, {"text": "Our method is easily applicable to any expression type.", "labels": [], "entities": []}, {"text": "Experiments in previous research have been limited to the compositional/non-compositional distinction , while we also test on sentences in which the words comprising the MWE appear but not as an expression.", "labels": [], "entities": []}], "introductionContent": [{"text": "Multi-Word Expressions (MWEs) such as 'pull strings', 'make a face' and 'get on one's nerves' are very common in language.", "labels": [], "entities": [{"text": "Multi-Word Expressions (MWEs)", "start_pos": 0, "end_pos": 29, "type": "TASK", "confidence": 0.6989697337150573}]}, {"text": "Such MWEs can be characterized as being non-compositional: the meaning of the expression does not transparently follow from the meaning of the words that comprise it.", "labels": [], "entities": []}, {"text": "Much of the work on MWEs in NLP has been in MWE extraction -the discovery of new MWEs from a corpus, using statistical and other methods.", "labels": [], "entities": [{"text": "MWE extraction", "start_pos": 44, "end_pos": 58, "type": "TASK", "confidence": 0.9774379134178162}]}, {"text": "Identification of known MWEs in text has received less attention, but is necessary for many NLP applications, for example in machine translation.", "labels": [], "entities": [{"text": "Identification of known MWEs in text", "start_pos": 0, "end_pos": 36, "type": "TASK", "confidence": 0.882511039574941}, {"text": "machine translation", "start_pos": 125, "end_pos": 144, "type": "TASK", "confidence": 0.8038675487041473}]}, {"text": "The current work deals with the MWE identification task: deciding if a sentence contains a use of a known expression.", "labels": [], "entities": [{"text": "MWE identification task", "start_pos": 32, "end_pos": 55, "type": "TASK", "confidence": 0.9639370242754618}]}, {"text": "MWE identification is not as simple as may initially appear, as will be shown by the performance of two rule-based baselines in our experiments.", "labels": [], "entities": [{"text": "MWE identification", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.9638268053531647}]}, {"text": "One source of difficulty is variations in expressions' usage in text.", "labels": [], "entities": []}, {"text": "Although MWEs generally show less variation than single words, they show enough that it cannot be ignored.", "labels": [], "entities": []}, {"text": "Ina study on V+NP idioms, found that the idioms' canonical form accounted for 75% of their appearances in a corpus.", "labels": [], "entities": []}, {"text": "Additionally, expressions differ considerably in the types of variations they allow, which include passivization, nominalization and addition of modifying words.", "labels": [], "entities": []}, {"text": "A second source of difficulty is that expressions consisting of very frequent words will often cooccur in sentences in a non-MWE usage and in similar but distinct expressions.", "labels": [], "entities": []}, {"text": "MWE identification can be modeled as a two step process.", "labels": [], "entities": [{"text": "MWE identification", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.9731356203556061}]}, {"text": "Given a sentence and a known expression, step (1) is to decide if the sentence contains a potential use of the expression.", "labels": [], "entities": []}, {"text": "This is a relatively simple step based on the appearance in the sentence of the words comprising the MWE.", "labels": [], "entities": []}, {"text": "Step (2) is to decide if the potential use is indeed non-compositional.", "labels": [], "entities": []}, {"text": "Consider the following sentences with regard to the expression hit the road, meaning 'to leave on a journey': (a) 'At the time, the road was long and difficult with few travelers daring to take it.'", "labels": [], "entities": []}, {"text": "(b) 'The headlights of the taxi-van behind us 468 flashed as it hit bumps in the road.'", "labels": [], "entities": []}, {"text": "(c) 'The bullets were hitting the road and I could see them coming towards me a lot faster than I was able to reverse.'", "labels": [], "entities": []}, {"text": "(d) 'Lorry trailers which would have been hitting the road tomorrow now stand idle.'", "labels": [], "entities": []}, {"text": "Sentence (a) does not contain a potential use of the expression due to the missing component 'hit'.", "labels": [], "entities": []}, {"text": "Each of (b)-(d) does contain a potential use of the expression.", "labels": [], "entities": []}, {"text": "In (b) all of the expression components are present, but they do not form an expression.", "labels": [], "entities": []}, {"text": "In (c), the words form an expression, but with a compositional (literal) meaning.", "labels": [], "entities": []}, {"text": "Only (d) contains a non-compositional use of hit the road.", "labels": [], "entities": []}, {"text": "The task we address in this paper is to identify whether or not we are in case (d), fora given expression in a given sentence.", "labels": [], "entities": []}, {"text": "To date, most work in MWE identification has focused on manually encoding rules that identify expressions in text.", "labels": [], "entities": [{"text": "MWE identification", "start_pos": 22, "end_pos": 40, "type": "TASK", "confidence": 0.9906483292579651}]}, {"text": "The encodings, usually consisting of regular expressions and syntactic structures, are intended to contain all the necessary information for processing the MWE in text.", "labels": [], "entities": []}, {"text": "Being manual, this is time-consuming work and requires expert knowledge of individual expressions.", "labels": [], "entities": []}, {"text": "In terms of the above model, such encodings handle both MWE identification steps.", "labels": [], "entities": [{"text": "MWE identification", "start_pos": 56, "end_pos": 74, "type": "TASK", "confidence": 0.9716015160083771}]}, {"text": "A second approach is to use machine learning methods to learn an expression's behavior from a corpus.", "labels": [], "entities": []}, {"text": "Studies taking this approach have focused on distinguishing between compositional and noncompositional uses of an expression (cases (c) and (d) above).", "labels": [], "entities": []}, {"text": "As will be detailed in Section 2, existing methods are tailored to an expression's type, and experiment with a single MWE pattern.", "labels": [], "entities": []}, {"text": "In addition, the training and test sets they used did not contain non-expression uses as in case (b), which can be quite common in practice.", "labels": [], "entities": []}, {"text": "Our approach is more general.", "labels": [], "entities": []}, {"text": "Given a set of sentences with potential MWE uses, we use sentence surface features to create a Support Vector Machine (SVM) classifier for each expression.", "labels": [], "entities": []}, {"text": "The classifier is binary and differentiates between non-compositional uses of the expression ((d) above) on the one hand, and compositional and non-expression uses ((b) and (c)) on the other.", "labels": [], "entities": []}, {"text": "The experiments and results presented below focus on verbal MWEs, since verbal MWEs are quite common in language use and have also been investigated in related MWE research (e.g.,).", "labels": [], "entities": []}, {"text": "However, the developed features are not specific to a particular type of expression.", "labels": [], "entities": []}, {"text": "The supervised method is compared with two simple rule-based baselines in order to test whether a simple approach is sufficient.", "labels": [], "entities": []}, {"text": "In addition, the use of surface features is compared with the use of syntactic features (based on dependency parse trees of the sentences).", "labels": [], "entities": []}, {"text": "Averaged over expressions in an independent test set, the supervised classifiers outperform the rule-based baselines, with F-scores of 94.86% (surface features) and 87.77% (syntactic features), compared with 80.70% for the best baseline.", "labels": [], "entities": [{"text": "F-scores", "start_pos": 123, "end_pos": 131, "type": "METRIC", "confidence": 0.9978978633880615}]}, {"text": "Section 2 reviews previous work.", "labels": [], "entities": []}, {"text": "Section 3 discusses the features used for the supervised classifier.", "labels": [], "entities": []}, {"text": "Section 4 explains the experimental setting.", "labels": [], "entities": []}, {"text": "The results and a discussion are given in sections 5 and 6.", "labels": [], "entities": []}], "datasetContent": [{"text": "As described, an expression's canonical form (CF) is used in many of the learning algorithm's features.", "labels": [], "entities": []}, {"text": "The CF is taken from Collins COBUILD Advanced Learner's English Dictionary (2003) which is also used as our source for MWEs.", "labels": [], "entities": [{"text": "Collins COBUILD Advanced Learner's English Dictionary (2003)", "start_pos": 21, "end_pos": 81, "type": "DATASET", "confidence": 0.9413133263587952}]}, {"text": "COBUILD is an English-English dictionary based on the Bank of English (BOE) corpus (over 520 million words) with approximately 34,000 entries.", "labels": [], "entities": [{"text": "COBUILD", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.9296608567237854}, {"text": "Bank of English (BOE) corpus", "start_pos": 54, "end_pos": 82, "type": "DATASET", "confidence": 0.8893499289240155}]}, {"text": "Traditional single-word dictionaries area good source for expressions because they usually list, as part of single-word entries, expressions in which the word is a component.", "labels": [], "entities": []}, {"text": "The CF is not explicitly given in COBUILD, so an approximation is the form which appears in the expression's definition.", "labels": [], "entities": [{"text": "CF", "start_pos": 4, "end_pos": 6, "type": "METRIC", "confidence": 0.9193242192268372}, {"text": "COBUILD", "start_pos": 34, "end_pos": 41, "type": "DATASET", "confidence": 0.9205471277236938}]}, {"text": "This is a reasonable approximation since the COBUILD authors claim to have selected typical uses of the expressions in their definitions.", "labels": [], "entities": [{"text": "COBUILD", "start_pos": 45, "end_pos": 52, "type": "DATASET", "confidence": 0.8797163963317871}]}, {"text": "Each CF also has a matching part-of-speech (POS) pattern, which is a list of the partsof-speech of the components in the CF.", "labels": [], "entities": []}, {"text": "For example, 'walking on air' has the pattern (V erb, P reposition, N oun).", "labels": [], "entities": [{"text": "V erb", "start_pos": 47, "end_pos": 52, "type": "METRIC", "confidence": 0.8033436834812164}]}, {"text": "COBUILD does not include part-of-speech information for expressions so this information was determined using the British National Corpus (BNC)), a (mostly) automatically POS tagged corpus (using the CLAWS tagger).", "labels": [], "entities": [{"text": "COBUILD", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.9558745622634888}, {"text": "British National Corpus (BNC))", "start_pos": 113, "end_pos": 143, "type": "DATASET", "confidence": 0.9712340931097666}]}, {"text": "For each MWE, the POS patterns of all instances of the CF in the corpus were counted.", "labels": [], "entities": [{"text": "POS", "start_pos": 18, "end_pos": 21, "type": "METRIC", "confidence": 0.8552996516227722}]}, {"text": "The most frequent pattern is the expression's POS pattern.", "labels": [], "entities": [{"text": "POS", "start_pos": 46, "end_pos": 49, "type": "METRIC", "confidence": 0.9675776362419128}]}, {"text": "A set of 17 verbal MWEs, the development set, was used for development of the surface and syntactic features described above.", "labels": [], "entities": []}, {"text": "All of the development set MWEs had the POS pattern (V erb, Determiner, N oun).", "labels": [], "entities": [{"text": "POS", "start_pos": 40, "end_pos": 43, "type": "METRIC", "confidence": 0.9624859690666199}, {"text": "Determiner", "start_pos": 60, "end_pos": 70, "type": "METRIC", "confidence": 0.8763513565063477}]}, {"text": "Another set of 24 verbal MWEs, the training/test set 4 , was then used to test the method.", "labels": [], "entities": []}, {"text": "Because the method is not specific to the (V erb, Determiner, N oun) pattern, new POS patterns are included in the training/test set.", "labels": [], "entities": [{"text": "Determiner, N oun) pattern", "start_pos": 50, "end_pos": 76, "type": "METRIC", "confidence": 0.8587901989618937}]}, {"text": "The training/test set consists of 8 MWEs of the POS pattern (V erb, Determiner, N oun), 7 (V erb, P reposition, N oun) MWEs and and 9 (V erb, N oun, P reposition) MWEs.", "labels": [], "entities": []}, {"text": "The list of MWEs was selected randomly from the corresponding POS pattern types.", "labels": [], "entities": []}, {"text": "MWEs with a positive or negative percentage of under 5% in their data set were discarded . The MWEs, in their canonical form, are: Development set: (V erb, Determiner, N oun): break the ice, calls the shots, catch a cold, clear the air, face the consequences, fits the bill, hit the road, make a face, make a distinction, makes an impression, raise the alarm, set an example, sound the alarm, stay the course, take a chance, take the initiative, tie the knot.", "labels": [], "entities": [{"text": "V erb, Determiner, N oun)", "start_pos": 149, "end_pos": 174, "type": "METRIC", "confidence": 0.8612192273139954}]}, {"text": "Training/test set: (V erb, Determiner, N oun): changes the subject, get a grip, get the picture, lead the way, makes the grade, sets the scene, take a seat, take the plunge; (V erb, P reposition, N oun): fall into place, goes to extremes, brought to justice, take to heart, gets on nerves, keep up appearances, comes to light; (V erb, N oun, P reposition): take aim at, make allowances for, takes advantage of, keep hands off, lay claim to, take care of, make contact with, gives rise to, wash hands of.", "labels": [], "entities": []}, {"text": "As mentioned, the first step of MWE identification is to identify if the sentence contains a potential non-compositional use of the expression.", "labels": [], "entities": [{"text": "MWE identification", "start_pos": 32, "end_pos": 50, "type": "TASK", "confidence": 0.9890298545360565}]}, {"text": "In order to test our method, which targets step (2), a set of such sentences (for each expression) was collected from the BNC corpus and then labeled for use as training/test sentences . The collection method was intended to allow a wide range of variations in expression use.", "labels": [], "entities": [{"text": "BNC corpus", "start_pos": 122, "end_pos": 132, "type": "DATASET", "confidence": 0.9677787125110626}]}, {"text": "In practice, for each expression sentences containing all of the expression's CF components, in any of their inflections, were collected, but excluding common auxiliary words.", "labels": [], "entities": []}, {"text": "So for example, when targeting the MWE 'make an impression' we allowed inflections of 'make' and 'impression' and did not require 'an', to allow for variations such as 'make no impression' and 'make some impression'.", "labels": [], "entities": []}, {"text": "For some expressions, sentences were limited to those with a distance of up to 8 words between each expression component.", "labels": [], "entities": []}, {"text": "Very long sentences (above 80 words) were discarded.", "labels": [], "entities": []}, {"text": "The final set of sentences was then randomly selected.", "labels": [], "entities": []}, {"text": "Given this method, training/test sentences allow non-lexical variations: inflection, word order, part-of-speech, syntactic structure and other non-syntactic transformations.", "labels": [], "entities": []}, {"text": "Lexical variations which involve a change in one of the expression's components are not allowed, except for common auxiliary words.", "labels": [], "entities": []}, {"text": "For the development set an average of 97 (40-137) sentences were collected per MWE, giving a total of 1663 sentences, with a micro average of 49% positive labels.", "labels": [], "entities": []}, {"text": "For the training/test set there were 139 (73-150) sentences per MWE on average, totaling 3350, with a 40% average positive ratio.", "labels": [], "entities": []}, {"text": "The sentences were manually labeled as positive if they contained a non-compositional use of the MWE and negative if they contained a compositional or non-expression usage.", "labels": [], "entities": []}, {"text": "Judgment was based on a single sentence, without wider context.", "labels": [], "entities": [{"text": "Judgment", "start_pos": 0, "end_pos": 8, "type": "TASK", "confidence": 0.8127856850624084}]}, {"text": "Two baseline methods are used to test the intuitive notion that simple rulebased methods are sufficient for MWE identification as well as for comparison with the supervised learning methods.", "labels": [], "entities": [{"text": "MWE identification", "start_pos": 108, "end_pos": 126, "type": "TASK", "confidence": 0.9779912233352661}]}, {"text": "The first method, CanonicalForm (CF), accepts a sentence use as a non-compositional MWE use if and only if the MWE is in canonical form (there are no intervening words between the MWE components, their order matches canonical-form order, and there is an inflection in at most one component word).", "labels": [], "entities": []}, {"text": "The second method, DistanceOrder (DO), ac- cepts a sentence use if and only if the number of words between the leftmost and rightmost MWE components is less than or equal to 2 (not counting the middle MWE component), and if the order matches the canonical form order.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Development set: Average performance over all", "labels": [], "entities": [{"text": "Average", "start_pos": 27, "end_pos": 34, "type": "METRIC", "confidence": 0.972236156463623}]}, {"text": " Table 2: Test set: Average performance over all MWEs and", "labels": [], "entities": [{"text": "Average", "start_pos": 20, "end_pos": 27, "type": "METRIC", "confidence": 0.9637013673782349}]}]}