{"title": [{"text": "A Unified Model of Phrasal and Sentential Evidence for Information Extraction", "labels": [], "entities": [{"text": "Information Extraction", "start_pos": 55, "end_pos": 77, "type": "TASK", "confidence": 0.7064679116010666}]}], "abstractContent": [{"text": "Information Extraction (IE) systems that extract role fillers for events typically look at the local context surrounding a phrase when deciding whether to extract it.", "labels": [], "entities": [{"text": "Information Extraction (IE)", "start_pos": 0, "end_pos": 27, "type": "TASK", "confidence": 0.8343026280403137}]}, {"text": "Often , however, role fillers occur in clauses that are not directly linked to an event word.", "labels": [], "entities": [{"text": "role fillers", "start_pos": 17, "end_pos": 29, "type": "TASK", "confidence": 0.7961866557598114}]}, {"text": "We present anew model for event extraction that jointly considers both the local context around a phrase along with the wider sentential context in a proba-bilistic framework.", "labels": [], "entities": [{"text": "event extraction", "start_pos": 26, "end_pos": 42, "type": "TASK", "confidence": 0.7535671889781952}]}, {"text": "Our approach uses a sentential event recognizer and a plausible role-filler recognizer that is conditioned on event sentences.", "labels": [], "entities": [{"text": "sentential event recognizer", "start_pos": 20, "end_pos": 47, "type": "TASK", "confidence": 0.6317048172156016}]}, {"text": "We evaluate our system on two IE data sets and show that our model performs well in comparison to existing IE systems that rely on local phrasal context.", "labels": [], "entities": [{"text": "IE data sets", "start_pos": 30, "end_pos": 42, "type": "DATASET", "confidence": 0.9028695027033488}]}], "introductionContent": [{"text": "Information Extraction (IE) systems typically use extraction patterns (e.g.,,,,) or classifiers (e.g.,,,,) to extract role fillers for events.", "labels": [], "entities": [{"text": "Information Extraction (IE)", "start_pos": 0, "end_pos": 27, "type": "TASK", "confidence": 0.8375760018825531}]}, {"text": "Most IE systems consider only the immediate context surrounding a phrase when deciding whether to extract it.", "labels": [], "entities": [{"text": "IE", "start_pos": 5, "end_pos": 7, "type": "TASK", "confidence": 0.9775625467300415}]}, {"text": "For tasks such as named entity recognition, immediate context is usually sufficient.", "labels": [], "entities": [{"text": "named entity recognition", "start_pos": 18, "end_pos": 42, "type": "TASK", "confidence": 0.606999526421229}]}, {"text": "But for more complex tasks, such as event extraction, a larger field of view is often needed to understand how facts tie together.", "labels": [], "entities": [{"text": "event extraction", "start_pos": 36, "end_pos": 52, "type": "TASK", "confidence": 0.8483377695083618}]}, {"text": "Most IE systems are designed to identify role fillers that appear as arguments to event verbs or nouns, either explicitly via syntactic relations or implicitly via proximity (e.g., John murdered Tom or the murder of Tom by John).", "labels": [], "entities": [{"text": "IE", "start_pos": 5, "end_pos": 7, "type": "TASK", "confidence": 0.9641724824905396}]}, {"text": "But many facts are presented in clauses that do not contain event words, requiring discourse relations or deep structural analysis to associate the facts with event roles.", "labels": [], "entities": []}, {"text": "For example, consider the sentences below: Seven people have died . .", "labels": [], "entities": []}, {"text": ". and 30 were injured in India after terrorists launched an attack on the Taj Hotel.", "labels": [], "entities": []}, {"text": "in Mexico City and its surrounding suburbs in a Swine Flu outbreak.", "labels": [], "entities": [{"text": "Swine Flu outbreak", "start_pos": 48, "end_pos": 66, "type": "TASK", "confidence": 0.5499137043952942}]}], "datasetContent": [{"text": "We now evaluate the performance of our unified IE model, GLACIER, which allows a plausible rolefiller recognizer and a sentential event recognizer to make joint decisions about phrase extractions.", "labels": [], "entities": [{"text": "GLACIER", "start_pos": 57, "end_pos": 64, "type": "METRIC", "confidence": 0.9746668934822083}, {"text": "phrase extractions", "start_pos": 177, "end_pos": 195, "type": "TASK", "confidence": 0.6958984285593033}]}, {"text": "As with our baseline system, we obtain good results using a threshold of 0.90 for our NB/NB model (i.e., only NPs with probability \u2265 0.90 are extracted).", "labels": [], "entities": []}, {"text": "For our NB/SVM models, we evaluated using the default threshold (0.50) but observed that recall was sometimes low.", "labels": [], "entities": [{"text": "recall", "start_pos": 89, "end_pos": 95, "type": "METRIC", "confidence": 0.9995181560516357}]}, {"text": "So we also use a threshold of 0.40, which produces superior results.", "labels": [], "entities": []}, {"text": "Here too, we used the Weka) implementation of the Na\u00a8\u0131veNa\u00a8\u0131ve Bayes model and the SVMLight (Joachims, 1998) implementation of the SVM.", "labels": [], "entities": [{"text": "SVMLight (Joachims, 1998)", "start_pos": 83, "end_pos": 108, "type": "DATASET", "confidence": 0.8343216876188914}]}, {"text": "For the MUC-4 data, our unified IE model using the SVM (0.40) outperforms all 3 baselines on three roles (PerpInd, Victim, Weapon) and outperforms 2 of the 3 baselines on the Target role.", "labels": [], "entities": [{"text": "MUC-4 data", "start_pos": 8, "end_pos": 18, "type": "DATASET", "confidence": 0.8954761624336243}, {"text": "IE", "start_pos": 32, "end_pos": 34, "type": "TASK", "confidence": 0.858104944229126}]}, {"text": "When GLACIER outperforms the other systems it is often by a wide margin: the F-score for PerpInd jumped from 0.43 for the best baseline (Sem Affinity) to 0.54 for GLACIER, and the F-scores for Victim and Weapon each improved by 5% over the best baseline.", "labels": [], "entities": [{"text": "F-score", "start_pos": 77, "end_pos": 84, "type": "METRIC", "confidence": 0.9987375140190125}, {"text": "F-scores", "start_pos": 180, "end_pos": 188, "type": "METRIC", "confidence": 0.9923328161239624}]}, {"text": "These gains came from both increased recall and increased precision, demonstrating that GLACIER extracts some information that was missed by the other systems and is also less prone to false hits.", "labels": [], "entities": [{"text": "recall", "start_pos": 37, "end_pos": 43, "type": "METRIC", "confidence": 0.9996528625488281}, {"text": "precision", "start_pos": 58, "end_pos": 67, "type": "METRIC", "confidence": 0.9994857311248779}, {"text": "GLACIER", "start_pos": 88, "end_pos": 95, "type": "METRIC", "confidence": 0.8794748783111572}]}, {"text": "Only the PerpOrg role shows inferior performance.", "labels": [], "entities": [{"text": "PerpOrg", "start_pos": 9, "end_pos": 16, "type": "DATASET", "confidence": 0.7941643595695496}]}, {"text": "Organizations perpetrating a terrorist event are often discussed later in a document, far removed from the main event description.", "labels": [], "entities": []}, {"text": "For example, a statement that Al Qaeda is believed to be responsible for an attack would typically appear after the event description.", "labels": [], "entities": []}, {"text": "As a result, the sentential event recognizer tends to generate low probabilities for such sentences.", "labels": [], "entities": [{"text": "sentential event recognizer", "start_pos": 17, "end_pos": 44, "type": "TASK", "confidence": 0.7344056566556295}]}, {"text": "We believe that addressing this issue would require the use of discourse relations or the use of even larger context sizes.", "labels": [], "entities": []}, {"text": "We intend to explore these avenues of research in future work.", "labels": [], "entities": []}, {"text": "On the ProMed data, GLACIER produces results that are similar to the baselines for the Victim role, but it outperforms the baselines for the Disease role.", "labels": [], "entities": [{"text": "ProMed data", "start_pos": 7, "end_pos": 18, "type": "DATASET", "confidence": 0.9853634834289551}, {"text": "GLACIER", "start_pos": 20, "end_pos": 27, "type": "METRIC", "confidence": 0.9918251037597656}]}, {"text": "We find that for this domain, the unified IE model with the Na\u00a8\u0131veNa\u00a8\u0131ve Bayes sentential event recognizer is superior to the unified IE model with the SVM classifier.", "labels": [], "entities": []}, {"text": "For the Disease role, the Fscore jumped 6%, from 0.43 for the best baseline systems (AutoSlog-TS and the NB baseline) to 0.49 for GLACIER NB/NB . In contrast to the MUC-4 data, this improvement was mostly due to an increase in precision (up to 0.41), indicating that our unified IE model was effective at eliminating many false hits.", "labels": [], "entities": [{"text": "Fscore", "start_pos": 26, "end_pos": 32, "type": "METRIC", "confidence": 0.9884886145591736}, {"text": "GLACIER", "start_pos": 130, "end_pos": 137, "type": "METRIC", "confidence": 0.9095858335494995}, {"text": "MUC-4 data", "start_pos": 165, "end_pos": 175, "type": "DATASET", "confidence": 0.8780302405357361}, {"text": "precision", "start_pos": 227, "end_pos": 236, "type": "METRIC", "confidence": 0.9993025064468384}]}, {"text": "For the Victim role, the performance of the unified model is comparable to the baselines.", "labels": [], "entities": []}, {"text": "On this event role, the F-score of GLACIER NB/NB (0.44) matches that of the best baseline system (Sem Affinity, with 0.44).", "labels": [], "entities": [{"text": "F-score", "start_pos": 24, "end_pos": 31, "type": "METRIC", "confidence": 0.9992187023162842}, {"text": "GLACIER NB/NB", "start_pos": 35, "end_pos": 48, "type": "METRIC", "confidence": 0.8496718406677246}]}, {"text": "However, note that GLACIER NB/NB can achieve a 5% gain in recall over this baseline, at the cost of a 3% precision loss.", "labels": [], "entities": [{"text": "GLACIER", "start_pos": 19, "end_pos": 26, "type": "METRIC", "confidence": 0.9551383852958679}, {"text": "recall", "start_pos": 58, "end_pos": 64, "type": "METRIC", "confidence": 0.999573290348053}, {"text": "precision", "start_pos": 105, "end_pos": 114, "type": "METRIC", "confidence": 0.999178946018219}]}], "tableCaptions": [{"text": " Table 1: Baseline Results on MUC-4", "labels": [], "entities": [{"text": "MUC-4", "start_pos": 30, "end_pos": 35, "type": "TASK", "confidence": 0.6763392686843872}]}, {"text": " Table 3: Sentential Event Recognizers Results (5-fold Cross-Validation)", "labels": [], "entities": [{"text": "Sentential Event Recognizers", "start_pos": 10, "end_pos": 38, "type": "TASK", "confidence": 0.9289857745170593}]}, {"text": " Table 5: Unified IE Model on MUC-4", "labels": [], "entities": [{"text": "IE", "start_pos": 18, "end_pos": 20, "type": "TASK", "confidence": 0.7331434488296509}, {"text": "MUC-4", "start_pos": 30, "end_pos": 35, "type": "TASK", "confidence": 0.5334517955780029}]}]}