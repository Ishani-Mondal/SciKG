{"title": [{"text": "A Structural Support Vector Method for Extracting Contexts and Answers of Questions from Online Forums", "labels": [], "entities": [{"text": "Extracting Contexts and Answers of Questions from Online Forums", "start_pos": 39, "end_pos": 102, "type": "TASK", "confidence": 0.8903024991353353}]}], "abstractContent": [{"text": "This paper addresses the issue of extracting contexts and answers of questions from post discussion of online forums.", "labels": [], "entities": [{"text": "extracting contexts and answers of questions from post discussion of online forums", "start_pos": 34, "end_pos": 116, "type": "TASK", "confidence": 0.8056419293085734}]}, {"text": "We propose a novel and unified model by customizing the structural Support Vector Machine method.", "labels": [], "entities": []}, {"text": "Our customization has several attractive properties: (1) it gives a comprehensive graphical representation of thread discussion.", "labels": [], "entities": []}, {"text": "(2) It designs special inference algorithms instead of general-purpose ones.", "labels": [], "entities": []}, {"text": "(3) It can be readily extended to different task preferences by varying loss functions.", "labels": [], "entities": []}, {"text": "Experimental results on areal data set show that our methods are both promising and flexible.", "labels": [], "entities": []}], "introductionContent": [{"text": "Recently, extracting questions, contexts and answers from post discussions of online forums incurs increasing academic attention ().", "labels": [], "entities": [{"text": "extracting questions, contexts and answers from post discussions of online forums", "start_pos": 10, "end_pos": 91, "type": "TASK", "confidence": 0.805318241318067}]}, {"text": "The extracted knowledge can be used either to enrich the knowledge base of community question answering (QA) services such as Yahoo!", "labels": [], "entities": [{"text": "community question answering (QA)", "start_pos": 75, "end_pos": 108, "type": "TASK", "confidence": 0.8476003607114156}]}, {"text": "Answers or to augment the knowledge base of chatbot (.", "labels": [], "entities": []}, {"text": "gives an example of a forum thread with questions, contexts and answers annotated.", "labels": [], "entities": []}, {"text": "This thread contains three posts and ten sentences, among which three questions are discussed.", "labels": [], "entities": []}, {"text": "The three questions are proposed in three sentences, S3, S5 and S6.", "labels": [], "entities": []}, {"text": "The context sentences S1 and S2 provide contextual information for question sentence S3.", "labels": [], "entities": []}, {"text": "Similarly, the context sentence S4 provides contextual information for question sentence S5 and S6.", "labels": [], "entities": []}, {"text": "There are three question-contextanswer triples in this example, (S3) \u2212 (S1, S2) \u2212 (S8, S9), (S5) \u2212 (S4) \u2212 (S10) and (S6) \u2212 (S4) \u2212 Post1: <context id=1> S1: Hi I am looking fora pet friendly hotel in Hong Kong because all of my family is going therefor vacation.", "labels": [], "entities": []}, {"text": "S2: my family has 2 sons and a dog.", "labels": [], "entities": []}, {"text": "</context> <question id=1> S3: Is there any recommended hotel near Sheung Wan or Tsing Sha Tsui?", "labels": [], "entities": []}, {"text": "</question> <context id=2, 3> S4: We also plan to go shopping in Causeway Bay.", "labels": [], "entities": [{"text": "Causeway Bay", "start_pos": 65, "end_pos": 77, "type": "DATASET", "confidence": 0.9455941915512085}]}, {"text": "</context> <question id=2> S5: What's the traffic situation around those commercial areas?", "labels": [], "entities": []}, {"text": "</question> <question id=3> S6: Is it necessary to take a taxi?", "labels": [], "entities": []}, {"text": "</question> S7: Any information would be appreciated.", "labels": [], "entities": []}, {"text": "Post2: <answer id=1> S8: The Comfort Lodge near Kowloon Park allows pet as I know, and usually fits well within normal budgets.", "labels": [], "entities": [{"text": "Comfort Lodge near Kowloon Park", "start_pos": 29, "end_pos": 60, "type": "DATASET", "confidence": 0.7632988393306732}]}, {"text": "S9: It is also conveniently located, nearby the Kowloon railway station and subway.", "labels": [], "entities": []}, {"text": "</answer> Post3: <answer id=2, 3> S10: It's very crowd in those areas, so I recommend MTR in Causeway Bay because it is cheap to take you around.", "labels": [], "entities": [{"text": "Post3", "start_pos": 10, "end_pos": 15, "type": "DATASET", "confidence": 0.8925363421440125}, {"text": "MTR in Causeway Bay", "start_pos": 86, "end_pos": 105, "type": "DATASET", "confidence": 0.7206645905971527}]}, {"text": "</answer>: An example thread with three posts and ten sentences (S10).", "labels": [], "entities": []}, {"text": "As shown in the example, a forum question usually requires contextual information to complement its expression.", "labels": [], "entities": []}, {"text": "For example, the question sentence S3 would be of incomplete meaning without the contexts S1 and S2, since the important keyword pet friendly would be lost.", "labels": [], "entities": []}, {"text": "The problem of extracting questions, contexts, and answers can be solved in two steps: (1) identify questions and then (2) extract contexts and answers for them.", "labels": [], "entities": []}, {"text": "Since identifying questions from forum discussions is already well solved in ( , in this paper, we are focused on step (2) while assuming questions already identified.", "labels": [], "entities": []}, {"text": "Previously, employ generalpurpose graphical models without any customizations to the specific extraction problem (step 2).", "labels": [], "entities": []}, {"text": "In this paper, we improve the existing models in three aspects: graphical representation, inference algorithm and loss function.", "labels": [], "entities": []}, {"text": "We propose a more comprehensive and unified graphical representation to model the thread for relational learning.", "labels": [], "entities": [{"text": "relational learning", "start_pos": 93, "end_pos": 112, "type": "TASK", "confidence": 0.8245525658130646}]}, {"text": "Our graphical representation has two advantages over previous work: unifying sentence relations and incorporating question interactions.", "labels": [], "entities": []}, {"text": "Three types of relation should be considered for context and answer extraction: (a) relations between successive sentences (e.g., context sentence S2 occurs immediately before question sentence S3); (b) relations between context sentences and answer sentences (e.g., context S4 presents the phrase Causeway Bay linking to answer which is absent from question S6); and (c) relations between multiple labels for one sentence (e.g., one question sentence is unlikely to be the answer to another question although one sentence can serve as contexts for more than one questions).", "labels": [], "entities": [{"text": "answer extraction", "start_pos": 61, "end_pos": 78, "type": "TASK", "confidence": 0.6935452669858932}, {"text": "Causeway Bay", "start_pos": 298, "end_pos": 310, "type": "DATASET", "confidence": 0.901912271976471}]}, {"text": "Our proposed graphical representation improves the modeling of the three types of sentence relation (Section 2.2).", "labels": [], "entities": []}, {"text": "Certain interactions exist among questions.", "labels": [], "entities": []}, {"text": "For example, question sentences S5 and S6 interact by sharing context sentence S4.", "labels": [], "entities": []}, {"text": "Our proposed graphical representation can naturally model the interactions.", "labels": [], "entities": []}, {"text": "Previous work () performs the extraction of contexts and answers in multiple passes of the thread (with each pass corresponding to one question), which cannot address the interactions well.", "labels": [], "entities": []}, {"text": "In comparison, our model performs the extraction in one pass of the thread.", "labels": [], "entities": []}, {"text": "Inference is usually a time-consuming process for structured prediction.", "labels": [], "entities": [{"text": "Inference", "start_pos": 0, "end_pos": 9, "type": "TASK", "confidence": 0.9594701528549194}, {"text": "structured prediction", "start_pos": 50, "end_pos": 71, "type": "TASK", "confidence": 0.6991219222545624}]}, {"text": "We design special inference algorithms, instead of general-purpose inference algorithms used in previous works, by taking advantage of special properties of our task.", "labels": [], "entities": []}, {"text": "Specifically, we utilize two special properties of thread structure to reduce the inference (time) cost.", "labels": [], "entities": []}, {"text": "First, context sentences and question sentences usually occur in the same post while answer sentences can only occur in the following posts.", "labels": [], "entities": []}, {"text": "With this properties, we can greatly reduce context (or answer) candidate sets of a question, which results in a significant decrease in inference cost (Section 3).", "labels": [], "entities": []}, {"text": "Second, context candidate set is usually much smaller than the number of sentences in a thread.", "labels": [], "entities": []}, {"text": "This property enables our proposal to have an exact and efficient inference (Section 4.1).", "labels": [], "entities": []}, {"text": "Moreover, an approximate inference algorithm is also given (Section 4.2).", "labels": [], "entities": []}, {"text": "In practice, different application settings usually imply different requirements for system performance.", "labels": [], "entities": []}, {"text": "For example, we expect a higher recall for the purpose of archiving questions but a higher precision for the purpose of retrieving questions.", "labels": [], "entities": [{"text": "recall", "start_pos": 32, "end_pos": 38, "type": "METRIC", "confidence": 0.9993276596069336}, {"text": "precision", "start_pos": 91, "end_pos": 100, "type": "METRIC", "confidence": 0.9989192485809326}]}, {"text": "A flexible framework should be able to cope with various requirements.", "labels": [], "entities": []}, {"text": "We employ structural Support Vector Machine (SVM) model that could naturally incorporate different loss functions (Section 5).", "labels": [], "entities": []}, {"text": "We use areal data set to evaluate our approach to extracting contexts and answers of questions.", "labels": [], "entities": [{"text": "extracting contexts and answers of questions", "start_pos": 50, "end_pos": 94, "type": "TASK", "confidence": 0.8007411658763885}]}, {"text": "The experimental results show both the effectiveness and the flexibility of our approach.", "labels": [], "entities": []}, {"text": "In the next section, we formalize the problem of context and answer extraction and introduce the structural model.", "labels": [], "entities": [{"text": "context and answer extraction", "start_pos": 49, "end_pos": 78, "type": "TASK", "confidence": 0.6055641025304794}]}, {"text": "In Sections 3, 4 and 5 we give the details of customizing structural model for our task.", "labels": [], "entities": []}, {"text": "In Section 6, we evaluate our methods.", "labels": [], "entities": []}, {"text": "In Section 7, we discuss the related work.", "labels": [], "entities": []}, {"text": "Finally, we conclude this paper in Section 8.", "labels": [], "entities": []}], "datasetContent": [{"text": "We made use of the same data set as introduced in.", "labels": [], "entities": []}, {"text": "Specifically, the data set includes about 591 threads from the forum TripAdvisor 2 . Each sentence in the threads is tagged with the labels 'question', 'context', 'answer', or 'plain' by two annotators.", "labels": [], "entities": []}, {"text": "We removed 76 threads that have no question sentences or more than 40 sentences and 6 questions.", "labels": [], "entities": []}, {"text": "The remaining 515 forum threads form our data set.", "labels": [], "entities": []}, {"text": "gives the statistics on the data set.", "labels": [], "entities": []}, {"text": "On average, each thread contains 3.95 posts and 2.73 questions, and each question has 1.39 context sentences and 3.31 answer sentences.", "labels": [], "entities": []}, {"text": "Note that the number of annotations is much larger than the number of sentences because one sentence can be annotated with multiple labels.", "labels": [], "entities": []}, {"text": "In all the experiments, we made use of linear models for the sake of computational efficiency.", "labels": [], "entities": []}, {"text": "As a preprocessing step, we normalized the value of each feature value into the interval [0, 1] and then followed the heuristic used in SVM-light) to set C to 1/||x|| 2 , where ||x|| is the average length of input samples (in our case, sentences).", "labels": [], "entities": []}, {"text": "The tolerance parameter was set to 0.1 (the value also used in) in all the runs of the experiments.", "labels": [], "entities": []}, {"text": "We calculated the standard precision (P), recall (R) and F 1 -score (F 1 ) for both tasks (context extraction and answer extraction).", "labels": [], "entities": [{"text": "standard precision (P)", "start_pos": 18, "end_pos": 40, "type": "METRIC", "confidence": 0.8756221175193787}, {"text": "recall (R)", "start_pos": 42, "end_pos": 52, "type": "METRIC", "confidence": 0.9589870870113373}, {"text": "F 1 -score (F 1 )", "start_pos": 57, "end_pos": 74, "type": "METRIC", "confidence": 0.9691881835460663}, {"text": "context extraction", "start_pos": 91, "end_pos": 109, "type": "TASK", "confidence": 0.7122383117675781}, {"text": "answer extraction", "start_pos": 114, "end_pos": 131, "type": "TASK", "confidence": 0.6889057010412216}]}, {"text": "All the experimental results were obtained through 5-fold cross validation.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 4: The use of different loss functions", "labels": [], "entities": []}]}