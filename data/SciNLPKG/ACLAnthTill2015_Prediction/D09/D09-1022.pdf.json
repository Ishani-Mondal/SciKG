{"title": [{"text": "Extending Statistical Machine Translation with Discriminative and Trigger-Based Lexicon Models", "labels": [], "entities": [{"text": "Statistical Machine Translation", "start_pos": 10, "end_pos": 41, "type": "TASK", "confidence": 0.7002422114213308}]}], "abstractContent": [{"text": "In this work, we propose two extensions of standard word lexicons in statistical machine translation: A discriminative word lexicon that uses sentence-level source information to predict the target words and a trigger-based lexicon model that extends IBM model 1 with a second trigger, allowing fora more fine-grained lexical choice of target words.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 69, "end_pos": 100, "type": "TASK", "confidence": 0.6486156185468038}]}, {"text": "The models capture dependencies that go beyond the scope of conventional SMT models such as phrase-and language models.", "labels": [], "entities": [{"text": "SMT", "start_pos": 73, "end_pos": 76, "type": "TASK", "confidence": 0.9903954863548279}]}, {"text": "We show that the models improve translation quality by 1% in BLEU over a competitive baseline on a large-scale task.", "labels": [], "entities": [{"text": "translation", "start_pos": 32, "end_pos": 43, "type": "TASK", "confidence": 0.9649562835693359}, {"text": "BLEU", "start_pos": 61, "end_pos": 65, "type": "METRIC", "confidence": 0.9964950680732727}]}], "introductionContent": [{"text": "Lexical dependencies modeled in standard phrasebased SMT are rather local.", "labels": [], "entities": [{"text": "SMT", "start_pos": 53, "end_pos": 56, "type": "TASK", "confidence": 0.6727815866470337}]}, {"text": "Even though the decision about the best translation is made on sentence level, phrase models and word lexicons usually do not take context beyond the phrase boundaries into account.", "labels": [], "entities": []}, {"text": "This is especially problematic since the average source phrase length used during decoding is small.", "labels": [], "entities": []}, {"text": "When translating Chinese to English, e.g., it is typically close to only two words.", "labels": [], "entities": []}, {"text": "The target language model is the only model that uses lexical context across phrase boundaries.", "labels": [], "entities": []}, {"text": "It is a very important feature in the log-linear setup of today's phrase-based decoders.", "labels": [], "entities": []}, {"text": "However, its context is typically limited to three to six words and it is not informed about the source sentence.", "labels": [], "entities": []}, {"text": "In the presented models, we explicitly take advantage of sentence-level dependencies including the source side and make non-local predictions for the target words.", "labels": [], "entities": []}, {"text": "This is an important aspect when translating from languages like German and Chinese where long-distance dependencies are common.", "labels": [], "entities": []}, {"text": "In Chinese, for example, tenses are often encoded by indicator words and particles whose position is relatively free in the sentence.", "labels": [], "entities": []}, {"text": "In German, prefixes of verbs can be moved overlong distances towards the end of the sentence.", "labels": [], "entities": []}, {"text": "In this work, we propose two models that can be categorized as extensions of standard word lexicons: A discriminative word lexicon that uses global, i.e. sentence-level source information to predict the target words using a statistical classifier and a trigger-based lexicon model that extends the well-known IBM model 1 () with a second trigger, allowing fora more finegrained lexical choice of target words.", "labels": [], "entities": []}, {"text": "The loglinear framework of the discriminative word lexicon offers a high degree of flexibility in the selection of features.", "labels": [], "entities": []}, {"text": "Other sources of information such as syntax or morphology can be easily integrated.", "labels": [], "entities": []}, {"text": "The trigger-based lexicon model, or simply triplet model since it is based on word triplets, is not trained discriminatively but uses the classical maximum likelihood approach (MLE) instead.", "labels": [], "entities": []}, {"text": "We train the triplets iteratively on a training corpus using the Expectation-Maximization (EM) algorithm.", "labels": [], "entities": []}, {"text": "We will present how both models allow fora representation of topic-related sentencelevel information which puts them close to word sense disambiguation (WSD) approaches.", "labels": [], "entities": [{"text": "word sense disambiguation (WSD)", "start_pos": 126, "end_pos": 157, "type": "TASK", "confidence": 0.7868502289056778}]}, {"text": "As will be shown later, the experiments indicate that these models help to ensure translation of content words that are often omitted by the baseline system.", "labels": [], "entities": [{"text": "translation of content words", "start_pos": 82, "end_pos": 110, "type": "TASK", "confidence": 0.8482875525951385}]}, {"text": "This is a common problem in Chinese-English translation.", "labels": [], "entities": []}, {"text": "Furthermore, the models are often capable to produce a better lexical choice of content words.", "labels": [], "entities": []}, {"text": "The structure of the paper is as follows: In Section 2, we will address related work and briefly pin down how our models differentiate from previous work.", "labels": [], "entities": []}, {"text": "Section 3 will describe the discriminative lexical selection model and the triplet model in more detail, explain the training procedures and show how the models are integrated into the decoder.", "labels": [], "entities": []}, {"text": "The experimental setup and results will be given in Section 4.", "labels": [], "entities": []}, {"text": "A more detailed discussion will be presented in Section 5.", "labels": [], "entities": []}, {"text": "In the end, we conclude our findings and give an outlook for further research in Section 6.", "labels": [], "entities": [{"text": "Section 6", "start_pos": 81, "end_pos": 90, "type": "DATASET", "confidence": 0.9230124056339264}]}], "datasetContent": [{"text": "In this section we evaluate our lexicon models on the GALE Chinese-English task for newswire and web text translation and additionally on the official NIST 2008 task for both Chinese-English and Arabic-English.", "labels": [], "entities": [{"text": "GALE Chinese-English task", "start_pos": 54, "end_pos": 79, "type": "TASK", "confidence": 0.5556728839874268}, {"text": "web text translation", "start_pos": 97, "end_pos": 117, "type": "TASK", "confidence": 0.7118799885114034}, {"text": "NIST 2008 task", "start_pos": 151, "end_pos": 165, "type": "DATASET", "confidence": 0.9216084480285645}]}, {"text": "The baseline system was built using a state-of-the art phrase-based MT system (.", "labels": [], "entities": []}, {"text": "We use the standard set of models with phrase translation probabilities for source-to-target and target-to-source direction, smoothing with lexical weights, a word and phrase penalty, distance-based and lexicalized reordering and a 5-gram (GALE) or 6-gram (NIST) target language model.", "labels": [], "entities": [{"text": "phrase translation", "start_pos": 39, "end_pos": 57, "type": "TASK", "confidence": 0.7334817051887512}]}, {"text": "We used training data provided by the Linguistic Data Consortium (LDC) consisting of 9.1M parallel Chinese-English sentence pairs of various domains for GALE (cf.) and smaller amounts of data for the NIST systems (cf. Table 2).", "labels": [], "entities": [{"text": "GALE", "start_pos": 153, "end_pos": 157, "type": "TASK", "confidence": 0.5539178848266602}, {"text": "NIST", "start_pos": 200, "end_pos": 204, "type": "DATASET", "confidence": 0.8863294124603271}]}, {"text": "The DWL and Triplet models were integrated into the decoder as presented in Section 3.", "labels": [], "entities": [{"text": "DWL", "start_pos": 4, "end_pos": 7, "type": "DATASET", "confidence": 0.8996551632881165}]}, {"text": "For the GALE development and test set, we separated the newswire and web text parts and did separate parameter tuning for each genre using the corresponding development set which consists of 485 sentences for newswire texts and 533 sentences of web text.: Results on the GALE Chinese-English test set for the newswire and web text setting (case-insensitive evaluation).", "labels": [], "entities": [{"text": "GALE development and test set", "start_pos": 8, "end_pos": 37, "type": "DATASET", "confidence": 0.8079103946685791}, {"text": "GALE Chinese-English test set", "start_pos": 271, "end_pos": 300, "type": "DATASET", "confidence": 0.8996508419513702}]}], "tableCaptions": [{"text": " Table 1: GALE Chinese-English corpus statistics  including two test sets: newswire and web text.", "labels": [], "entities": [{"text": "GALE Chinese-English corpus statistics", "start_pos": 10, "end_pos": 48, "type": "DATASET", "confidence": 0.8581776916980743}]}, {"text": " Table 2: NIST Chinese-English and Arabic- English corpus statistics including the official  2008 test sets.", "labels": [], "entities": [{"text": "NIST Chinese-English and Arabic- English corpus statistics", "start_pos": 10, "end_pos": 68, "type": "DATASET", "confidence": 0.8932056948542595}, {"text": "2008 test sets", "start_pos": 93, "end_pos": 107, "type": "DATASET", "confidence": 0.7110685706138611}]}, {"text": " Table 3: Results on the GALE Chinese-English  test set for the newswire and web text setting  (case-insensitive evaluation).", "labels": [], "entities": [{"text": "GALE Chinese-English  test set", "start_pos": 25, "end_pos": 55, "type": "DATASET", "confidence": 0.8253021389245987}]}, {"text": " Table 4: Results on the test sets for the NIST 2008", "labels": [], "entities": [{"text": "NIST 2008", "start_pos": 43, "end_pos": 52, "type": "DATASET", "confidence": 0.9576642215251923}]}, {"text": " Table 6: The top 10 content words predicted by  each model for the GALE newswire example sen- tence. Original ranks for the related IBM model 1  are given as subscripts for the triplet model.", "labels": [], "entities": [{"text": "GALE newswire example sen- tence", "start_pos": 68, "end_pos": 100, "type": "DATASET", "confidence": 0.8974325557549795}]}]}