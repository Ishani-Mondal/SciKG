{"title": [{"text": "Unsupervised morphological segmentation and clustering with document boundaries", "labels": [], "entities": [{"text": "Unsupervised morphological segmentation", "start_pos": 0, "end_pos": 39, "type": "TASK", "confidence": 0.5978883703549703}]}], "abstractContent": [{"text": "Many approaches to unsupervised morphology acquisition incorporate the frequency of character sequences with respect to each other to identify word stems and affixes.", "labels": [], "entities": [{"text": "unsupervised morphology acquisition", "start_pos": 19, "end_pos": 54, "type": "TASK", "confidence": 0.6712385714054108}]}, {"text": "This typically involves heuris-tic search procedures and calibrating multiple arbitrary thresholds.", "labels": [], "entities": []}, {"text": "We present a simple approach that uses no thresholds other than those involved in standard application of \u03c7 2 significance testing.", "labels": [], "entities": []}, {"text": "A key part of our approach is using document boundaries to constrain generation of candidate stems and affixes and clustering morphological variants of a given word stem.", "labels": [], "entities": []}, {"text": "We evaluate our model on English and the Mayan language Uspanteko; it compares favorably to two benchmark systems which use considerably more complex strategies and rely more on experimentally chosen threshold values.", "labels": [], "entities": []}], "introductionContent": [{"text": "Unsupervised morphology acquisition attempts to learn from raw corpora one or more of the following about the written morphology of a language: (1) the segmentation of the set of word types in a corpus, (2) the clustering of word types in a corpus based on some notion of morphological relatedness (Schone and Jurafsky,), (3) the generation of out-of-vocabulary items which are morphologically related to other word types in the corpus ().", "labels": [], "entities": [{"text": "Unsupervised morphology acquisition", "start_pos": 0, "end_pos": 35, "type": "TASK", "confidence": 0.7404192884763082}]}, {"text": "We take a novel approach to segmenting words and clustering morphologically related words.", "labels": [], "entities": [{"text": "clustering morphologically related words", "start_pos": 49, "end_pos": 89, "type": "TASK", "confidence": 0.8652261346578598}]}, {"text": "The approach uses no parameters that need to be tuned on data.", "labels": [], "entities": []}, {"text": "The two main ideas of the approach are (a) the filtering of affixes by significant co-occurrence, and (b) the integration of knowledge of document boundaries when generating candidate stems and affixes and when clustering morphologically related words.", "labels": [], "entities": [{"text": "clustering morphologically related words", "start_pos": 211, "end_pos": 251, "type": "TASK", "confidence": 0.8165499418973923}]}, {"text": "The main application that we envision for our approach is to produce interlinearized glossed texts for underresourced/endangered languages).", "labels": [], "entities": []}, {"text": "Thus, we strive to eliminate hand-tuned parameters to enable documentary linguists to use our model as a preprocessing step for their manual analysis of stems and affixes.", "labels": [], "entities": []}, {"text": "To require a documentary linguist-who is likely to have little to no knowledge of NLP methods-to tune parameters is unfeasible.", "labels": [], "entities": []}, {"text": "Additionally, data-driven exploration of parameter settings is unlikely to be reliable in language documentation since datasets typically are quite small.", "labels": [], "entities": []}, {"text": "To be relevant in this context, a model needs to produce useful results out of the box.", "labels": [], "entities": []}, {"text": "Constraining learning by using document boundaries has been used quite effectively in unsupervised word sense disambiguation.", "labels": [], "entities": [{"text": "word sense disambiguation", "start_pos": 99, "end_pos": 124, "type": "TASK", "confidence": 0.6844889024893442}]}, {"text": "Many applications in information retrieval are built on the statistical correlation between documents and terms.", "labels": [], "entities": [{"text": "information retrieval", "start_pos": 21, "end_pos": 42, "type": "TASK", "confidence": 0.7664680182933807}]}, {"text": "However, we are unaware of cases where knowledge of document boundaries has been used for unsupervised learning for morphology.", "labels": [], "entities": []}, {"text": "The intuition behind our approach is very simple: if two words in a single document are very similar in terms of orthography, then the two words are likely to be related morphologically.", "labels": [], "entities": []}, {"text": "We measure how integrating these assumptions into our model at different stages affects performance.", "labels": [], "entities": []}, {"text": "We define a simple pipeline model.", "labels": [], "entities": []}, {"text": "After generating candidate stems and affixes (possibly constrained by document boundaries), a \u03c7 2 test based on global corpus counts filters out unlikely affixes.", "labels": [], "entities": []}, {"text": "Mutually consistent affix pairs are then clustered to form affix groups.", "labels": [], "entities": []}, {"text": "These in turn are used to build morphologically related word clusters, possibly constrained by evidence from co-occurence of word forms in documents.", "labels": [], "entities": []}, {"text": "Following, clusters are evaluated for whether they capture inflectional paradigms using CELEX (.", "labels": [], "entities": [{"text": "CELEX", "start_pos": 88, "end_pos": 93, "type": "DATASET", "confidence": 0.8616609573364258}]}, {"text": "We are unaware of other work on morphology using \u03c7 2 tests despite its wide application across many disciplines.", "labels": [], "entities": []}, {"text": "1 This maybe due to the large degree of noise found in the candidate affix sets induced through other candidate generation methods.", "labels": [], "entities": []}, {"text": "The \u03c7 2 test has two standard thresholds-a significance threshold and a lower bound on observed counts.", "labels": [], "entities": [{"text": "significance threshold", "start_pos": 43, "end_pos": 65, "type": "METRIC", "confidence": 0.917342871427536}]}, {"text": "These are the only manually set parameters we require-and we in fact use the widely accepted standard values for these thresholds without varying them in our experiments.", "labels": [], "entities": []}, {"text": "This is a significant improvement over other approaches that typically require a number of arbitrary thresholds and parameters yet provide little intuitive justification for them.", "labels": [], "entities": []}, {"text": "(We give examples of these in \u00a73.)", "labels": [], "entities": []}, {"text": "We evaluate our approach on two languages, English and Uspanteko, and compare its performance to two benchmark systems, Morfessor and Linguistica).", "labels": [], "entities": []}, {"text": "English is commonly used in other studies and permits the use of CELEX as a gold standard for evaluation.", "labels": [], "entities": [{"text": "CELEX", "start_pos": 65, "end_pos": 70, "type": "DATASET", "confidence": 0.5048240423202515}]}, {"text": "Uspanteko is an endangered Mayan language for which we have a set of interlinearized glossed texts (IGT) (.", "labels": [], "entities": []}, {"text": "IGT provides wordby-word morpheme segmenation, which we use to create a synthetic gold standard.", "labels": [], "entities": [{"text": "IGT", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.9706515073776245}]}, {"text": "In addition to evaluation against this standard, Telma Kaan Pixabaj-a Mayan linguist who helped create the annotated corpus-reviewed by hand 100 word clusters produced by our system, Morfessor and Linguistica.", "labels": [], "entities": [{"text": "Morfessor", "start_pos": 183, "end_pos": 192, "type": "DATASET", "confidence": 0.915287971496582}]}, {"text": "Note that because English is suffixal and Uspanteko is both prefixal and suffixal, we use a slightly modified model for Uspanteko.", "labels": [], "entities": []}, {"text": "The approach introduced in this paper compares favorably to Linguistica and Morfessor, two models that employ much more complex strategies and rely on experimentally-tuned language/corpusspecific parameters.", "labels": [], "entities": []}, {"text": "In our evaluation, document boundary awareness greatly benefits precision for small datasets, blocking acquisition of spurious affixes.", "labels": [], "entities": [{"text": "document boundary awareness", "start_pos": 19, "end_pos": 46, "type": "TASK", "confidence": 0.7634132305781046}, {"text": "precision", "start_pos": 64, "end_pos": 73, "type": "METRIC", "confidence": 0.9988204836845398}]}, {"text": "For large datasets, global candidate generation outperforms document-aware candidate generation at the task of filtering out spurious stems, but document-aware clustering improves precision.", "labels": [], "entities": [{"text": "precision", "start_pos": 180, "end_pos": 189, "type": "METRIC", "confidence": 0.99628084897995}]}, {"text": "These findings are promising for the application of this approach to under-resourced languages suggests, but does not actually use, \u03c7 2 . like Uspanteko.", "labels": [], "entities": []}], "datasetContent": [{"text": "We outline our evaluation methodology, baselines, benchmarks and results, and discuss the results.", "labels": [], "entities": []}, {"text": "Schone and Jurafsky give definitions for correct (C), inserted (I), and deleted (D) words in model-derived conflation sets in relation to a gold standard.", "labels": [], "entities": []}, {"text": "Their formulation does not allow for multiple cluster membership of words.", "labels": [], "entities": []}, {"text": "We extend the definition to incorporate this fact about the data.", "labels": [], "entities": []}, {"text": "Let w be a word form.", "labels": [], "entities": []}, {"text": "We write X w for the clusters induced by the model that contain w, and Y w for gold standard clusters containing w.", "labels": [], "entities": []}, {"text": "X wand Y w only count words which occurred in both model and gold standard clusters.", "labels": [], "entities": []}, {"text": "Then Based on these definitions, we formulate precision (P ), recall (R), and the f -score (F ) as: P = C/(C +I), R = C/(C +D), F = (2P R)/(P +R).", "labels": [], "entities": [{"text": "precision (P )", "start_pos": 46, "end_pos": 60, "type": "METRIC", "confidence": 0.9171097576618195}, {"text": "recall (R)", "start_pos": 62, "end_pos": 72, "type": "METRIC", "confidence": 0.8922283947467804}, {"text": "f -score (F )", "start_pos": 82, "end_pos": 95, "type": "METRIC", "confidence": 0.9707383116086324}]}, {"text": "USP evaluation We use two different means to evaluate the performance on USP.", "labels": [], "entities": [{"text": "USP", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.9198751449584961}, {"text": "USP", "start_pos": 73, "end_pos": 76, "type": "DATASET", "confidence": 0.936347484588623}]}, {"text": "One is the f -score derived from the above section with respect to a standard that was automatically generated from the morpheme segment tiers of the OKMA IGT.", "labels": [], "entities": [{"text": "f -score", "start_pos": 11, "end_pos": 19, "type": "METRIC", "confidence": 0.9544389049212137}, {"text": "OKMA IGT", "start_pos": 150, "end_pos": 158, "type": "DATASET", "confidence": 0.9338307678699493}]}, {"text": "We generated the standard by taking non-hyphenated segments as the stem and clustering words with shared stems.", "labels": [], "entities": []}, {"text": "We also had an expert in Uspanteko manually evaluate a random subset (N = 100) of the model output to compensate for any failings in the standard.", "labels": [], "entities": []}, {"text": "The evaluator determined a dominant stem fora cluster and identified words which were not related to that stem.", "labels": [], "entities": []}, {"text": "We measured accuracy and 5 CELEX does have a second file listing words and their breakup into constituent morphemes for both derivation and inflection, but its use would have required additional processing that could introduce errors.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 12, "end_pos": 20, "type": "METRIC", "confidence": 0.9997105002403259}, {"text": "CELEX", "start_pos": 27, "end_pos": 32, "type": "DATASET", "confidence": 0.7216189503669739}]}, {"text": "full cluster accuracy 6 for the expert evaluations.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 13, "end_pos": 21, "type": "METRIC", "confidence": 0.6769067645072937}]}, {"text": "We experimented on Uspanteko with three different assumptions: (1) it is only prefixal; (2) it is only suffixal; (3) it is both prefixal and suffixal.", "labels": [], "entities": []}, {"text": "We applied the assumptions of only prefixal or only suffixal to LINGUISTICA as well.", "labels": [], "entities": [{"text": "LINGUISTICA", "start_pos": 64, "end_pos": 75, "type": "METRIC", "confidence": 0.9213547706604004}]}, {"text": "The relevant results are given row headers in tables with a corresponding +P(prefix) or +S(suffix).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Affix counts in contingency tables for the valid pair ed/ing and spurious pair le/s according to  CandGen-D in (a) and (b) and according to CandGen-G in (c) and (d). \u03c7 2 test values are given under  each table. Data is from NYT. Total affix token counts induced through CandGen-D and CandGen-G  are N =4178578 and N =156299, respectively. A total of 2054 and 3739 affix types were induced for", "labels": [], "entities": [{"text": "Affix", "start_pos": 10, "end_pos": 15, "type": "METRIC", "confidence": 0.9830594062805176}, {"text": "NYT", "start_pos": 234, "end_pos": 237, "type": "DATASET", "confidence": 0.9580021500587463}]}, {"text": " Table 2: Results on English for all models in precision(P), recall(R), f -score(F) for each data set.", "labels": [], "entities": [{"text": "precision(P)", "start_pos": 47, "end_pos": 59, "type": "METRIC", "confidence": 0.9385248124599457}, {"text": "recall(R)", "start_pos": 61, "end_pos": 70, "type": "METRIC", "confidence": 0.9636452794075012}, {"text": "f -score(F)", "start_pos": 72, "end_pos": 83, "type": "METRIC", "confidence": 0.9757847587267557}]}, {"text": " Table 3: Performance of models on automatically  generated USP evaluation set. P: Prefix only, S:  Suffix only. If there is no indication of S or P, it  means model attempted to learn both", "labels": [], "entities": [{"text": "USP evaluation set", "start_pos": 60, "end_pos": 78, "type": "DATASET", "confidence": 0.823318342367808}, {"text": "Prefix", "start_pos": 83, "end_pos": 89, "type": "METRIC", "confidence": 0.9532018303871155}]}, {"text": " Table 4: Human expert evaluated accuracy (Acc.)  and full cluster accuracy (FAcc.) of models on  USP and average cluster size in words (Avg. Sz.)", "labels": [], "entities": [{"text": "accuracy", "start_pos": 33, "end_pos": 41, "type": "METRIC", "confidence": 0.9966926574707031}, {"text": "Acc.", "start_pos": 43, "end_pos": 47, "type": "METRIC", "confidence": 0.7092525959014893}, {"text": "cluster accuracy (FAcc.)", "start_pos": 59, "end_pos": 83, "type": "METRIC", "confidence": 0.7676468968391419}, {"text": "USP", "start_pos": 98, "end_pos": 101, "type": "DATASET", "confidence": 0.7238720655441284}]}]}