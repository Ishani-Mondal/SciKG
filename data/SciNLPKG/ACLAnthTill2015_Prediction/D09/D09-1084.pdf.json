{"title": [{"text": "A Relational Model of Semantic Similarity between Words using Automatically Extracted Lexical Pattern Clusters from the Web", "labels": [], "entities": []}], "abstractContent": [{"text": "Semantic similarity is a central concept that extends across numerous fields such as artificial intelligence, natural language processing, cognitive science and psychology.", "labels": [], "entities": [{"text": "Semantic similarity", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.8204426765441895}]}, {"text": "Accurate measurement of semantic similarity between words is essential for various tasks such as, document clustering , information retrieval, and synonym extraction.", "labels": [], "entities": [{"text": "document clustering", "start_pos": 98, "end_pos": 117, "type": "TASK", "confidence": 0.7645532488822937}, {"text": "information retrieval", "start_pos": 120, "end_pos": 141, "type": "TASK", "confidence": 0.8272584676742554}, {"text": "synonym extraction", "start_pos": 147, "end_pos": 165, "type": "TASK", "confidence": 0.9496376812458038}]}, {"text": "We propose a novel model of semantic similarity using the semantic relations that exist among words.", "labels": [], "entities": []}, {"text": "Given two words, first, we represent the semantic relations that hold between those words using automatically extracted lexical pattern clusters.", "labels": [], "entities": []}, {"text": "Next, the semantic similarity between the two words is computed using a Mahalanobis distance measure.", "labels": [], "entities": [{"text": "Mahalanobis distance measure", "start_pos": 72, "end_pos": 100, "type": "METRIC", "confidence": 0.5715245803197225}]}, {"text": "We compare the proposed similarity measure against previously proposed semantic similarity measures on Miller-Charles benchmark dataset and WordSimilarity-353 collection.", "labels": [], "entities": [{"text": "Miller-Charles benchmark dataset", "start_pos": 103, "end_pos": 135, "type": "DATASET", "confidence": 0.8312368988990784}, {"text": "WordSimilarity-353 collection", "start_pos": 140, "end_pos": 169, "type": "DATASET", "confidence": 0.9391229450702667}]}, {"text": "The proposed method out-performs all existing web-based semantic similarity measures, achieving a Pear-son correlation coefficient of 0.867 on the Millet-Charles dataset.", "labels": [], "entities": [{"text": "Pear-son correlation coefficient", "start_pos": 98, "end_pos": 130, "type": "METRIC", "confidence": 0.9134880900382996}, {"text": "Millet-Charles dataset", "start_pos": 147, "end_pos": 169, "type": "DATASET", "confidence": 0.8817180097103119}]}], "introductionContent": [{"text": "Similarity is a fundamental concept in theories of knowledge and behavior.", "labels": [], "entities": []}, {"text": "Psychological experiments have shown that similarity acts as an organizing principle by which individuals classify objects, and make generalizations.", "labels": [], "entities": []}, {"text": "For example, a biologist would classify a newly found animal specimen based upon the properties that it shares with existing categories of animals.", "labels": [], "entities": [{"text": "classify a newly found animal specimen", "start_pos": 31, "end_pos": 69, "type": "TASK", "confidence": 0.7554860214392344}]}, {"text": "We can then make additional inferences on the new specimen using the properties * Research Fellow of the Japan Society for the Promotion of Science known for the existing category.", "labels": [], "entities": []}, {"text": "As the similarity between two objects X and Y increases, so does the probability of correctly inferring that Y has the property T upon knowing that X has T.", "labels": [], "entities": []}, {"text": "Accurate measurement of semantic similarity between lexical units such as words or phrases is important for numerous tasks in natural language processing such as word sense disambiguation, synonym extraction, and automatic thesauri generation).", "labels": [], "entities": [{"text": "word sense disambiguation", "start_pos": 162, "end_pos": 187, "type": "TASK", "confidence": 0.668056309223175}, {"text": "synonym extraction", "start_pos": 189, "end_pos": 207, "type": "TASK", "confidence": 0.8883670568466187}, {"text": "automatic thesauri generation", "start_pos": 213, "end_pos": 242, "type": "TASK", "confidence": 0.6599585513273875}]}, {"text": "In information retrieval, similar or related words are used to expand user queries to improve recall ().", "labels": [], "entities": [{"text": "information retrieval", "start_pos": 3, "end_pos": 24, "type": "TASK", "confidence": 0.7855647802352905}, {"text": "recall", "start_pos": 94, "end_pos": 100, "type": "METRIC", "confidence": 0.9971350431442261}]}, {"text": "Semantic similarity is a context dependent and dynamic phenomenon.", "labels": [], "entities": [{"text": "Semantic similarity", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.8572166264057159}]}, {"text": "New words are constantly being created and existing words are assigned with new senses on the Web.", "labels": [], "entities": []}, {"text": "To decide whether two words are semantically similar, it is important to know the semantic relations that hold between the words.", "labels": [], "entities": []}, {"text": "For example, the words horse and cow can be considered semantically similar because both horses and cows are useful animals in agriculture.", "labels": [], "entities": []}, {"text": "Similarly, a horse and a car can be considered semantically similar because cars, and historically horses, are used for transportation.", "labels": [], "entities": []}, {"text": "Semantic relations such as X and Y are used in agriculture, or X and Y are used for transportation, exist between two words X and Y in these examples.", "labels": [], "entities": []}, {"text": "We use bold-italics, X, to denote the slot of a word X in a lexical pattern.", "labels": [], "entities": []}, {"text": "We propose a relational model to compute the semantic similarity between two words.", "labels": [], "entities": []}, {"text": "First, using snippets retrieved from a web search engine, we present an automatic lexical pattern extraction algorithm to represent the semantic relations that exist between two words.", "labels": [], "entities": [{"text": "lexical pattern extraction", "start_pos": 82, "end_pos": 108, "type": "TASK", "confidence": 0.7804340720176697}]}, {"text": "For example, given two words ostrich and bird, we extract X is a Y, X is a large Y, and X is a flightless Y from the Web.", "labels": [], "entities": []}, {"text": "Using a set of semantically related words as training data, we evaluate the confidence of a lexical pattern as an indicator of semantic similarity.", "labels": [], "entities": []}, {"text": "For example, the pattern X is a Y is a better indicator of semantic similarity between X and Y than the pattern X and Y.", "labels": [], "entities": []}, {"text": "Consequently, we would like to emphasize the former pattern by assigning it a higher confidence score.", "labels": [], "entities": [{"text": "confidence score", "start_pos": 85, "end_pos": 101, "type": "METRIC", "confidence": 0.9551136493682861}]}, {"text": "It is noteworthy that all lexical patterns are not independent -multiple lexical patterns can express the same semantic relation.", "labels": [], "entities": []}, {"text": "For example, the pattern X is a large Y subsumes the more general pattern X is a Y and they both indicate a hypernymic relationship between X and Y.", "labels": [], "entities": []}, {"text": "By clustering the semantically related patterns into groups, we can both overcome the data sparseness problem, and reduce the number of parameters during training.", "labels": [], "entities": []}, {"text": "To identify semantically related patterns, we use a sequential pattern clustering algorithm that is based on the distributional hypothesis.", "labels": [], "entities": [{"text": "sequential pattern clustering", "start_pos": 52, "end_pos": 81, "type": "TASK", "confidence": 0.6563573479652405}]}, {"text": "We represent two words by a feature vector defined over the clusters of patterns.", "labels": [], "entities": []}, {"text": "Finally, the semantic similarity is computed as the Mahalanobis distance between points corresponding to the feature vectors.", "labels": [], "entities": []}, {"text": "By using Mahalanobis distance instead of Euclidean distance, we can account for the inter-dependence between semantic relations.", "labels": [], "entities": []}], "datasetContent": [{"text": "Evaluating a semantic similarity measure is difficult because the notion of semantic similarity is subjective.", "labels": [], "entities": []}, {"text": "Miller-Charles (1998) dataset has been frequently used to benchmark semantic similarity measures.", "labels": [], "entities": [{"text": "Miller-Charles (1998) dataset", "start_pos": 0, "end_pos": 29, "type": "DATASET", "confidence": 0.6083189964294433}, {"text": "benchmark semantic similarity", "start_pos": 58, "end_pos": 87, "type": "TASK", "confidence": 0.747399906317393}]}, {"text": "Miller-Charles dataset contains 30 word pairs rated by a group of 38 human subjects.", "labels": [], "entities": [{"text": "Miller-Charles dataset", "start_pos": 0, "end_pos": 22, "type": "DATASET", "confidence": 0.9157246351242065}]}, {"text": "The word pairs are rated on a scale from 0 (no similarity) to 4 (perfect synonymy).", "labels": [], "entities": []}, {"text": "Because of the omission of two word pairs in earlier versions of WordNet, most researchers had used only 28 pairs for evaluations.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 65, "end_pos": 72, "type": "DATASET", "confidence": 0.967415988445282}]}, {"text": "The degree of correlation between the human ratings in the benchmark dataset and the similarity scores produced by an automatic semantic similarity measure, can be considered as a Algorithm 1 Sequential pattern clustering algorithm.", "labels": [], "entities": [{"text": "Sequential pattern clustering", "start_pos": 192, "end_pos": 221, "type": "TASK", "confidence": 0.6261631945768992}]}, {"text": "Input: patterns P = {p 1 , . .", "labels": [], "entities": []}, {"text": ", p n }, threshold \u03b8 Output: clusters C 1: SORT(P ) 2: C \u2190 {} 3: for pattern pi \u2208 P do end if 18: end for 19: return C measurement of how well the semantic similarity measure captures the notion of semantic similarity held by humans.", "labels": [], "entities": [{"text": "Output", "start_pos": 21, "end_pos": 27, "type": "METRIC", "confidence": 0.8886984586715698}]}, {"text": "In addition to Miller-Charles dataset we also evaluate on the WordSimilarity-353 () dataset.", "labels": [], "entities": [{"text": "Miller-Charles dataset", "start_pos": 15, "end_pos": 37, "type": "DATASET", "confidence": 0.7849112451076508}, {"text": "WordSimilarity-353 () dataset", "start_pos": 62, "end_pos": 91, "type": "DATASET", "confidence": 0.8897857864697775}]}, {"text": "In contrast to Miller-Charles dataset which has only 30 word pairs, WordSimilarity-353 dataset contains 353 word pairs.", "labels": [], "entities": [{"text": "Miller-Charles dataset", "start_pos": 15, "end_pos": 37, "type": "DATASET", "confidence": 0.7598094344139099}, {"text": "WordSimilarity-353 dataset", "start_pos": 68, "end_pos": 94, "type": "DATASET", "confidence": 0.9686285853385925}]}, {"text": "Each pair has 13-16 human judgments, which were averaged for each pair to produce a single relatedness score.", "labels": [], "entities": []}, {"text": "Following the previous work, we use both Miller-Charles dataset and WordSimilarity-353 dataset to evaluate the proposed semantic similarity measure.", "labels": [], "entities": [{"text": "Miller-Charles dataset", "start_pos": 41, "end_pos": 63, "type": "DATASET", "confidence": 0.7169650942087173}, {"text": "WordSimilarity-353 dataset", "start_pos": 68, "end_pos": 94, "type": "DATASET", "confidence": 0.9700258672237396}]}, {"text": "We evaluate the proposed method using the WordSimilarity-353 dataset.", "labels": [], "entities": [{"text": "WordSimilarity-353 dataset", "start_pos": 42, "end_pos": 68, "type": "DATASET", "confidence": 0.9877725541591644}]}, {"text": "Experimental results are presented in.", "labels": [], "entities": []}, {"text": "Following previous work, we use Spearman rank correlation coefficient, which does not require ratings to be linearly dependent, for the evaluations on this dataset.", "labels": [], "entities": [{"text": "Spearman rank correlation coefficient", "start_pos": 32, "end_pos": 69, "type": "METRIC", "confidence": 0.7557050138711929}]}, {"text": "Likewise with the Miller-Charles ratings, we measure the correlation between the similarity scores produced by the proposed method for word pairs in the WordSimilarity-353 dataset and the human ratings.", "labels": [], "entities": [{"text": "WordSimilarity-353 dataset", "start_pos": 153, "end_pos": 179, "type": "DATASET", "confidence": 0.986402153968811}]}, {"text": "A higher Spearman correlation coefficient (value=0.504, confidence interval [0.422, 0.578]) indicates a better agreement with the human notion of semantic similarity.", "labels": [], "entities": [{"text": "Spearman correlation coefficient", "start_pos": 9, "end_pos": 41, "type": "METRIC", "confidence": 0.7475897669792175}]}, {"text": "From Table 3 we can see that the proposed method outperforms a wide variety of semantic similarity measures developed using numerous resources including lexical resources such as WordNet and knowledge sources such as Wikipedia (i.e. WikiRelate!).", "labels": [], "entities": [{"text": "WordNet", "start_pos": 179, "end_pos": 186, "type": "DATASET", "confidence": 0.9517526626586914}]}, {"text": "In contrast to the Miller-Charles dataset which only contains common English words selected from the WordNet, the WordSimilarity-353 dataset contains word pairs where one or both words are named entities (e.g. (Maradona, foot- 0.848 0.822 0.745 0.891 ball) and (Jerusalem, Israel)).", "labels": [], "entities": [{"text": "WordNet", "start_pos": 101, "end_pos": 108, "type": "DATASET", "confidence": 0.9588713049888611}, {"text": "WordSimilarity-353 dataset", "start_pos": 114, "end_pos": 140, "type": "DATASET", "confidence": 0.9741479754447937}]}, {"text": "Because the proposed method use snippets retrieved from a web search engine, it is capable of extracting expressive lexical patterns that can explicitly state the relationship between two entities.", "labels": [], "entities": []}, {"text": "If we must compare n objects using a feature model of similarity, then we only need to define features for each of those n objects.", "labels": [], "entities": []}, {"text": "However, in the proposed relational model we must define relations between all pairs of objects.", "labels": [], "entities": []}, {"text": "In the case where all n objects are different, this requires us to define relations for n(n\u22121)/2 object pairs.", "labels": [], "entities": []}, {"text": "Defining relations for all pairs can be computationally costly for large n values.", "labels": [], "entities": []}, {"text": "Efficiently comparing n objects using a relational model is an interesting future research direction of the current work.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Semantic similarity scores on Miller-Charles dataset", "labels": [], "entities": [{"text": "Miller-Charles dataset", "start_pos": 40, "end_pos": 62, "type": "DATASET", "confidence": 0.8264121413230896}]}]}