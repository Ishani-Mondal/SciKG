{"title": [{"text": "Generalized Expectation Criteria for Bootstrapping Extractors using Record-Text Alignment", "labels": [], "entities": [{"text": "Record-Text Alignment", "start_pos": 68, "end_pos": 89, "type": "TASK", "confidence": 0.6820397675037384}]}], "abstractContent": [{"text": "Traditionally, machine learning approaches for information extraction require human annotated data that can be costly and time-consuming to produce.", "labels": [], "entities": [{"text": "information extraction", "start_pos": 47, "end_pos": 69, "type": "TASK", "confidence": 0.8069000840187073}]}, {"text": "However, in many cases, there already exists a database (DB) with schema related to the desired output, and records related to the expected input text.", "labels": [], "entities": []}, {"text": "We present a conditional random field (CRF) that aligns tokens of a given DB record and its realization in text.", "labels": [], "entities": []}, {"text": "The CRF model is trained using only the available DB and unlabeled text with generalized expectation criteria.", "labels": [], "entities": []}, {"text": "An annotation of the text induced from inferred alignments is used to train an information extractor.", "labels": [], "entities": [{"text": "information extractor", "start_pos": 79, "end_pos": 100, "type": "TASK", "confidence": 0.7134285420179367}]}, {"text": "We evaluate our method on a citation extraction task in which alignments between DBLP database records and citation texts are used to train an extractor.", "labels": [], "entities": [{"text": "citation extraction task", "start_pos": 28, "end_pos": 52, "type": "TASK", "confidence": 0.9032160441080729}, {"text": "DBLP database records", "start_pos": 81, "end_pos": 102, "type": "DATASET", "confidence": 0.90863964955012}]}, {"text": "Experimental results demonstrate an error reduction of 35% over a previous state-of-the-art method that uses heuristic alignments.", "labels": [], "entities": [{"text": "error", "start_pos": 36, "end_pos": 41, "type": "METRIC", "confidence": 0.9813650250434875}]}], "introductionContent": [{"text": "A substantial portion of information on the Web consists of unstructured and semi-structured text.", "labels": [], "entities": []}, {"text": "Information extraction (IE) systems segment and label such text to populate a structured database that can then be queried and mined efficiently.", "labels": [], "entities": [{"text": "Information extraction (IE)", "start_pos": 0, "end_pos": 27, "type": "TASK", "confidence": 0.8406952917575836}]}, {"text": "In this paper, we mainly deal with information extraction from text fragments that closely resemble structured records.", "labels": [], "entities": [{"text": "information extraction from text fragments", "start_pos": 35, "end_pos": 77, "type": "TASK", "confidence": 0.8361965954303742}]}, {"text": "Examples of such texts include citation strings in research papers, contact addresses on person homepages and apartment listings in classified ads.", "labels": [], "entities": []}, {"text": "Pattern matching and rule-based approaches for IE) that only use specific patterns, and delimiter and font-based cues for segmentation are prone to failure on such data because these cues are generally not broadly reliable.", "labels": [], "entities": [{"text": "Pattern matching", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.7990332841873169}, {"text": "IE)", "start_pos": 47, "end_pos": 50, "type": "TASK", "confidence": 0.9715026617050171}, {"text": "segmentation", "start_pos": 122, "end_pos": 134, "type": "TASK", "confidence": 0.9672873616218567}]}, {"text": "Statistical machine learning methods such as hidden Markov models (HMMs)) and conditional random fields (CRFs) () have become popular approaches to address the text extraction problem.", "labels": [], "entities": [{"text": "text extraction", "start_pos": 160, "end_pos": 175, "type": "TASK", "confidence": 0.812187135219574}]}, {"text": "However, these methods require labeled training data, such as annotated text, which is often scarce and expensive to produce.", "labels": [], "entities": []}, {"text": "In many cases, however, there already exists a database with schema related to the desired output, and records that are imperfectly rendered in the available unlabeled text.", "labels": [], "entities": []}, {"text": "This database can serve as a source of significant supervised guidance to machine learning methods.", "labels": [], "entities": []}, {"text": "Previous work on using databases to train information extractors has taken one of three simpler approaches.", "labels": [], "entities": [{"text": "information extractors", "start_pos": 42, "end_pos": 64, "type": "TASK", "confidence": 0.7376895844936371}]}, {"text": "In the first, a separate language model is trained on each column of the database and these models are then used to segment and label a given text sequence (.", "labels": [], "entities": []}, {"text": "However, this approach does not model context, errors or different formats of fields in text, and requires large number of database entries to learn an accurate language model.", "labels": [], "entities": []}, {"text": "The second approach () uses database or dictionary lookups in combination with similarity measures to add features to the text sequence.", "labels": [], "entities": []}, {"text": "Although these features are very informative, learning algorithms still require annotated data to make use of them.", "labels": [], "entities": []}, {"text": "The final approach heuristically labels texts using matching records and learns extractors from these annotations (.", "labels": [], "entities": []}, {"text": "Heuris-tic labeling decisions, however, are made independently without regard for the Markov dependencies among labels in text and are sensitive to subtle changes in text.", "labels": [], "entities": []}, {"text": "Here we propose a method that automatically induces a labeling of an input text sequence using a word alignment with a matching database record.", "labels": [], "entities": []}, {"text": "This induced labeling is then used to train a text extractor.", "labels": [], "entities": [{"text": "text extractor", "start_pos": 46, "end_pos": 60, "type": "TASK", "confidence": 0.7650063931941986}]}, {"text": "Our approach has several advantages over previous methods.", "labels": [], "entities": []}, {"text": "First, we are able to model field ordering and context around fields by learning an extractor from annotations of the text itself.", "labels": [], "entities": []}, {"text": "Second, a probabilistic model for word alignment can exploit dependencies among alignments, and is also robust to errors, formatting differences, and missing fields in text and the record.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 34, "end_pos": 48, "type": "TASK", "confidence": 0.7814114689826965}]}, {"text": "Our word alignment model is a conditional random field (CRF) () that generates alignments between tokens of a text sequence and a matching database record.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 4, "end_pos": 18, "type": "TASK", "confidence": 0.7317824363708496}]}, {"text": "The structure of the graphical model resembles IBM Model 1 () in which each target (record) word is assigned one or more source (text) words.", "labels": [], "entities": []}, {"text": "The alignment is generated conditioned on both the record and text sequence, and therefore supports large sets of rich and nonindependent features of the sequence pairs.", "labels": [], "entities": []}, {"text": "Our model is trained without the need for labeled word alignments by using generalized expectation (GE) criteria) that penalize the divergence of specific model expectations from target expectations.", "labels": [], "entities": [{"text": "generalized expectation (GE) criteria", "start_pos": 75, "end_pos": 112, "type": "METRIC", "confidence": 0.8529031376043955}]}, {"text": "Model parameters are estimated by minimizing this divergence.", "labels": [], "entities": []}, {"text": "To limit over-fitting we include a L 2 -regularization term in the objective.", "labels": [], "entities": []}, {"text": "The model expectations in GE criteria are taken with respect to a set of alignment latent variables that are either specific to each sequence pair (local) or summarizing the entire data set (global).", "labels": [], "entities": []}, {"text": "This set is constructed by including all alignment variables a that satisfy a certain binary feature (e.g., f (a, x 1 , y 1 , x 2 ) = 1, for labeled record (x 1 , y 1 ), and text sequence x 2 ).", "labels": [], "entities": []}, {"text": "One example global criterion is that \"an alignment exists between two orthographically similar 1 words 95% of the time.\"", "labels": [], "entities": []}, {"text": "Here the criterion has a target expectation of 95% and is defined over alignments Another criterion for extraction can be \"the word 'EMNLP' is always aligned with the record label booktitle\".", "labels": [], "entities": []}, {"text": "1 Two words are orthographically similar if they have low edit distance.", "labels": [], "entities": []}, {"text": "This criterion has a target of 100% and defined One-to-one correspondence between words in the sequence pair can be specified as collection of local expectation constraints.", "labels": [], "entities": []}, {"text": "Since we directly encode prior knowledge of how alignments behave in our criteria, we obtain sufficiently accurate alignments with little supervision.", "labels": [], "entities": []}, {"text": "We apply our method to the task of citation extraction.", "labels": [], "entities": [{"text": "citation extraction", "start_pos": 35, "end_pos": 54, "type": "TASK", "confidence": 0.9701789319515228}]}, {"text": "The input to our training algorithm is a set of matching DBLP 2 -record/citation-text pairs and global GE criteria 3 of the following two types: (1) alignment criteria that consider features of mapping between record and text words, and, (2) extraction criteria that consider features of the schema label assigned to a text word.", "labels": [], "entities": [{"text": "GE", "start_pos": 103, "end_pos": 105, "type": "METRIC", "confidence": 0.7828654050827026}]}, {"text": "In our experiments, the parallel record-text pairs are collected manually but this process can be automated using systems that match text sequences to records in the DB (.", "labels": [], "entities": []}, {"text": "Such systems achieve very high accuracy close to 90% F1 on semi-structured domains similar to ours.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 31, "end_pos": 39, "type": "METRIC", "confidence": 0.9984961748123169}, {"text": "F1", "start_pos": 53, "end_pos": 55, "type": "METRIC", "confidence": 0.9997091889381409}]}, {"text": "Our trained alignment model can be used to directly align new record-text pairs to create a labeling of the texts.", "labels": [], "entities": []}, {"text": "Empirical results demonstrate a 20.6% error reduction in token labeling accuracy compared to a strong baseline method that employs a set of high-precision alignments.", "labels": [], "entities": [{"text": "token labeling", "start_pos": 57, "end_pos": 71, "type": "TASK", "confidence": 0.8575199544429779}, {"text": "accuracy", "start_pos": 72, "end_pos": 80, "type": "METRIC", "confidence": 0.898162305355072}]}, {"text": "Furthermore, we provide a 63.8% error reduction compared to IBM Model 4 (.", "labels": [], "entities": [{"text": "error reduction", "start_pos": 32, "end_pos": 47, "type": "METRIC", "confidence": 0.9819632768630981}]}, {"text": "Alignments learned by our model are used to train a linear-chain CRF extractor.", "labels": [], "entities": [{"text": "CRF extractor", "start_pos": 65, "end_pos": 78, "type": "TASK", "confidence": 0.7771854102611542}]}, {"text": "We obtain an error reduction of 35.1% over a previous state-of-the-art extraction method that uses heuristically generated alignments.", "labels": [], "entities": [{"text": "error reduction", "start_pos": 13, "end_pos": 28, "type": "METRIC", "confidence": 0.9741294980049133}]}], "datasetContent": [{"text": "In this section, we present details about the application of our method to citation extraction task.", "labels": [], "entities": [{"text": "citation extraction task", "start_pos": 75, "end_pos": 99, "type": "TASK", "confidence": 0.9470012784004211}]}, {"text": "We collected a set of 260 random records from the DBLP bibliographic database.", "labels": [], "entities": [{"text": "DBLP bibliographic database", "start_pos": 50, "end_pos": 77, "type": "DATASET", "confidence": 0.9775934815406799}]}, {"text": "The schema of DBLP has the following labels {author, editor, address, title, booktitle, pages, year, journal, volume, number, month, url, ee, cdrom, school, publisher, note, isbn, chapter, series}.", "labels": [], "entities": [{"text": "DBLP", "start_pos": 14, "end_pos": 18, "type": "DATASET", "confidence": 0.8380297422409058}]}, {"text": "The complexity of our alignment model depends on the number of schema labels and number of tokens in the DB record.", "labels": [], "entities": [{"text": "DB record", "start_pos": 105, "end_pos": 114, "type": "DATASET", "confidence": 0.7932607233524323}]}, {"text": "We reduced the number of schema labels by: (1) mapping the labels address, booktitle, journal and school to venue, (2) mapping month and year to date, and (3) dropping the fields url, ee, cdrom, note, isbn and chapter, since they never appeared in citation texts.", "labels": [], "entities": []}, {"text": "We also added the other label O for fields in text that are not represented in the database.", "labels": [], "entities": [{"text": "O", "start_pos": 30, "end_pos": 31, "type": "METRIC", "confidence": 0.9687845706939697}]}, {"text": "Therefore, our final DB schema is {author, title, date, venue, volume, number, pages, editor, publisher, series, O}.", "labels": [], "entities": [{"text": "O", "start_pos": 113, "end_pos": 114, "type": "METRIC", "confidence": 0.9828557372093201}]}, {"text": "For each DBLP record we searched on the web for matching citation texts using the first author's last name and words in the title.", "labels": [], "entities": [{"text": "DBLP record", "start_pos": 9, "end_pos": 20, "type": "DATASET", "confidence": 0.8968910872936249}]}, {"text": "Each citation text found is manually labeled for evaluation purposes.", "labels": [], "entities": []}, {"text": "An example of a matching DBLP record-citation text pair is shown in.", "labels": [], "entities": []}, {"text": "Our data set 6 contains 522 record-text pairs for 260 DBLP entries.", "labels": [], "entities": []}, {"text": "We use a variety of rich, non-independent features in our models to optimize system performance.", "labels": [], "entities": []}, {"text": "The input features in our models are of the following two types: (a) Extraction features in the AlignCRF model (f (a t , y 1 , x 2 , t)) and ExtrCRF model (g(y t\u22121 , y t , x, t)) are conjunctions of assigned labels and observational tests on text sequence at time step t.", "labels": [], "entities": []}, {"text": "The following observational tests are used: (1) regular expressions to detect tokens containing all characters (ALLCHAR), all digits (ALLDIGITS) or both digits and characters (AL-PHADIGITS), (2) number of characters or digits in the token (NUMCHAR=3, NUMDIGITS=1), (3) domain-specific patterns for date and pages, (4) token identity, suffixes, prefixes and character ngrams, (5) presence of a token in lexicons such as \"last names,\" \"publisher names,\" \"cities,\" (6) lexicon features within a window of 10, (7) regular The: Example of matching record-text pair found on the web.", "labels": [], "entities": []}, {"text": "expression features within a window of 10, and token identity features within a window of 3.", "labels": [], "entities": []}, {"text": "(b) Alignment features in the AlignCRF model (f (a t , x 1 , x 2 , t)) that operate on the aligned source token x 1 [a t ] and target token x 2.", "labels": [], "entities": []}, {"text": "Again the observational tests used for alignment are: (1) exact token match tests whether the source-target tokens are string identical, (2) approximate token match produces a binary feature after binning the Jaro-Winkler edit distance () between the tokens, (3) substring token match tests whether one token is a substring of the other, (4) prefix token match returns true if the prefixes match for lengths {1, 2, 3, 4}, (5) suffix token match returns true if the prefixes match for lengths {1, 2, 3, 4}, and (6) exact and approximate token matches at offsets {\u22121, \u22121} and {+1, +1} around the alignment.", "labels": [], "entities": [{"text": "alignment", "start_pos": 39, "end_pos": 48, "type": "TASK", "confidence": 0.9709939360618591}]}, {"text": "Thus, a conditional model lets us use these arbitrary helpful features that cannot be exploited tractably in a generative model.", "labels": [], "entities": []}, {"text": "As is common practice (), we simulate user-specified expectation criteria through statistics on manually labeled citation texts.", "labels": [], "entities": []}, {"text": "For extraction criteria, we select for each label, the top N extraction features ordered by mutual information (MI) with that label.", "labels": [], "entities": [{"text": "mutual information (MI)", "start_pos": 92, "end_pos": 115, "type": "METRIC", "confidence": 0.6789390623569489}]}, {"text": "Also, we aggregate the alignment features of record tokens whose alignment with a target text token results in a correct label assignment.", "labels": [], "entities": []}, {"text": "The top N alignment features that have maximum MI with this correct labeling are selected as alignment criteria.", "labels": [], "entities": [{"text": "MI", "start_pos": 47, "end_pos": 49, "type": "METRIC", "confidence": 0.9946020841598511}]}, {"text": "We bin target expectations of these criteria into 11 bins as [0.05, 0.1, 0.2, 0.3, . .", "labels": [], "entities": []}, {"text": ", 0.9, 0.95].", "labels": [], "entities": []}, {"text": "In our experiments, we set N = 10 and use a fixed weight w = 10.0 for all expectation criteria (no tuning of parameters was performed).", "labels": [], "entities": []}, {"text": "Our experiments use a 3:1 split of the data for training and testing.", "labels": [], "entities": []}, {"text": "We repeat the experiment 20 times with different random splits of the data.", "labels": [], "entities": []}, {"text": "We train the AlignCRF model using the training data and the automatically created expectation criteria (Section 3.2).", "labels": [], "entities": []}, {"text": "We evaluate our alignment model indirectly in terms of token labeling accuracy (i.e., percentage of correctly labeled tokens in test citation data) since we do not have annotated alignments.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 70, "end_pos": 78, "type": "METRIC", "confidence": 0.7819709181785583}]}, {"text": "The alignment model is then used to train a ExtrCRF model as described in Section 3.3.", "labels": [], "entities": []}, {"text": "Again, we use token labeling accuracy for evaluation.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 29, "end_pos": 37, "type": "METRIC", "confidence": 0.8956038355827332}]}, {"text": "We also measure F1 performance as the harmonic mean of precision and recall for each label.", "labels": [], "entities": [{"text": "F1 performance", "start_pos": 16, "end_pos": 30, "type": "METRIC", "confidence": 0.9515021443367004}, {"text": "precision", "start_pos": 55, "end_pos": 64, "type": "METRIC", "confidence": 0.9995096921920776}, {"text": "recall", "start_pos": 69, "end_pos": 75, "type": "METRIC", "confidence": 0.9994809031486511}]}], "tableCaptions": [{"text": " Table 3: Example of matching record-text pair found on the web.", "labels": [], "entities": []}, {"text": " Table 5: Token-labeling accuracy and per-label F1  for different alignment methods. These methods  all use matching DB records at test time. Bold- faced numbers indicate the best performing model.  HMM, Model4: generative alignment models  from GIZA++, AlignCRF: alignment model from  this paper.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 25, "end_pos": 33, "type": "METRIC", "confidence": 0.955995500087738}, {"text": "F1", "start_pos": 48, "end_pos": 50, "type": "METRIC", "confidence": 0.8446424603462219}]}, {"text": " Table 6: Token-labeling accuracy and per-label F1 for different extraction methods. Except M+R-CRF  \u2020 ,  all other approaches do not use any records at test time. Bold-faced numbers indicate the best performing  model. DB-CRF: CRF trained on DB fields. M+R-CRF, M-CRF: CRFs trained from heuristic align- ments. ExtrCRF: Extraction model presented in this paper. GS-CRF: CRF trained on human annotated  citation texts.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 25, "end_pos": 33, "type": "METRIC", "confidence": 0.9869018197059631}, {"text": "F1", "start_pos": 48, "end_pos": 50, "type": "METRIC", "confidence": 0.7912095189094543}]}]}