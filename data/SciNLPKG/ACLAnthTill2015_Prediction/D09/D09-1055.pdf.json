{"title": [{"text": "Mining Search Engine Clickthrough Log for Matching N-gram Features", "labels": [], "entities": []}], "abstractContent": [{"text": "User clicks on a URL in response to a query are extremely useful predictors of the URL's relevance to that query.", "labels": [], "entities": []}, {"text": "Exact match click features tend to suffer from severe data sparsity issues in web ranking.", "labels": [], "entities": []}, {"text": "Such sparsity is particularly pronounced for new URLs or long queries where each distinct query-url pair will rarely occur.", "labels": [], "entities": []}, {"text": "To remedy this, we present a set of straightforward yet informative query-url n-gram features that allows for generalization of limited user click data to large amounts of unseen query-url pairs.", "labels": [], "entities": []}, {"text": "The method is motivated by techniques leveraged in the NLP community for dealing with unseen words.", "labels": [], "entities": []}, {"text": "We find that there are interesting regularities across queries and their preferred destination URLs; for example, queries containing \"form\" tend to lead to clicks on URLs containing \"pdf\".", "labels": [], "entities": []}, {"text": "We evaluate our set of new query-url features on a web search ranking task and obtain improvements that are statistically significant at a p-value < 0.0001 level over a strong baseline with exact match clickthrough features.", "labels": [], "entities": []}], "introductionContent": [{"text": "Clickthrough logs record user click behaviors, which area critical source for improving search relevance (;).", "labels": [], "entities": []}, {"text": "Previous work ( ) demonstrated that clickthrough features (e.g., can lead to substantial improvements in relevance.", "labels": [], "entities": []}, {"text": "Such features summarize query-specific user interactions on a search engine.", "labels": [], "entities": []}, {"text": "One commonly used clickthrough feature is generated based on the following observation: if a URL receives a large number of first and last clicks across many user sessions, then it indicates that this URL might be a strongly preferred destination of a query.", "labels": [], "entities": []}, {"text": "For example, when a user searches for \"yahoo\", they tend to only click on the URL www.yahoo.com rather than other alternatives.", "labels": [], "entities": []}, {"text": "This results in www.yahoo.com being the first and last clicked URL for the query.", "labels": [], "entities": []}, {"text": "We refer to such behavior as being navigational clicks (NavClicks).", "labels": [], "entities": []}, {"text": "Features that use exact query and URL string matches (e.g., NavClick, IsNextClicked and IsPreviousClicked) are referred to as exact match features (ExactM) for the remainder of this paper.", "labels": [], "entities": [{"text": "NavClick", "start_pos": 60, "end_pos": 68, "type": "DATASET", "confidence": 0.9326805472373962}]}, {"text": "The coverage of ExactM features is sparse, especially for long queries and new URLs.", "labels": [], "entities": []}, {"text": "Many long queries are either unique or very low frequency.", "labels": [], "entities": []}, {"text": "Hence, the improvements from ExactM features are limited to the more popular queries.", "labels": [], "entities": []}, {"text": "In addition, ExactM features tend to be weighted heavily in the ranking of results when they are available.", "labels": [], "entities": []}, {"text": "This introduces a bias where the ranking models tend to strongly favor older URLs over new URLs even when the latter otherwise appear to be more relevant.", "labels": [], "entities": []}, {"text": "By inspecting the clickthrough logs, we observed that unseen query-url pairs are often composed of informative previously observed subsequences.", "labels": [], "entities": []}, {"text": "Specifically, we saw that query n-grams can be correlated with sequences of URL n-grams.", "labels": [], "entities": []}, {"text": "For example, we find that there are interesting regularities across queries and URLs, such as queries containing \"form\" tending to lead to clicks on URLs containing \"pdf\".", "labels": [], "entities": []}, {"text": "This strongly motivates the adoption of an approach similar to the Natural Language Processing (NLP) technique of using n-grams to deal with unseen words.", "labels": [], "entities": []}, {"text": "For example, part-of-speech tagging) and parsing () both require dealing with unknown words.", "labels": [], "entities": [{"text": "part-of-speech tagging", "start_pos": 13, "end_pos": 35, "type": "TASK", "confidence": 0.726078063249588}, {"text": "parsing", "start_pos": 41, "end_pos": 48, "type": "TASK", "confidence": 0.9798992872238159}]}, {"text": "By using n-gram substrings, novel items can be dealt with using any informative substrings they contain that were actually observed in the training data.", "labels": [], "entities": []}, {"text": "The remainder of the paper is organized as follows.", "labels": [], "entities": []}, {"text": "In Section 2, we introduce our overall methodology.", "labels": [], "entities": []}, {"text": "Section 2.1 presents a data mining method for building a query-url n-gram dictionary, Section 2.2 describes the new ranking features in detail.", "labels": [], "entities": []}, {"text": "In section 3, we present our experimental results.", "labels": [], "entities": []}, {"text": "Section 4 discusses related work, and Section 5 summarizes the contribution of this work.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluate the performance of query-url ngrams features on a ranking application and analyze the results from several different perspectives.", "labels": [], "entities": []}, {"text": "For all experiments, our training and test data are query-url pairs annotated with human judgments.", "labels": [], "entities": []}, {"text": "In our data, we use five grades to evaluate relevance of a query and URL pair.", "labels": [], "entities": []}, {"text": "The data includes 94K queries for training and 3.4K queries for evaluation, and each query is associated with the top ranked URLs returned from a search engine.", "labels": [], "entities": []}, {"text": "Totally, there are 916K query-url pairs for training and 42K pairs for testing.", "labels": [], "entities": []}, {"text": "The queries are general and uniformly and randomly sampled with replacement, resulting in more frequent queries also appearing more frequently in our training and test sets.", "labels": [], "entities": []}, {"text": "We use Discounted Cumulative Gain () to evaluate our ranking accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 61, "end_pos": 69, "type": "METRIC", "confidence": 0.9787063002586365}]}, {"text": "Discounted Cumulative Gain (DCG) has been widely used in evaluating the quality of search engine rankings and is defined as: G i represents the editorial judgment of the i-th document.", "labels": [], "entities": []}, {"text": "In this paper, we only report normalized DCG , which is an absolute DCG 5 normalized by a baseline, and relative DCG 5 improvement, which is an improvement normalized by the baseline.", "labels": [], "entities": [{"text": "DCG", "start_pos": 41, "end_pos": 44, "type": "METRIC", "confidence": 0.47529280185699463}, {"text": "DCG 5 improvement", "start_pos": 113, "end_pos": 130, "type": "METRIC", "confidence": 0.7885810931523641}]}, {"text": "Note normalized DCG 5 is different than NDCG (Normalized Discounted Cumulative Gain defined in).", "labels": [], "entities": []}, {"text": "We use Wilcoxon signed test to evaluate the significance for model comparison.", "labels": [], "entities": []}, {"text": "We compare the query-URL N-gram feature set (I) with the base feature set in Section 3.5.1, and contrast the NavClick features and the query-URL N-gram features (II) using the Core Feature Set in Section 3.5.2.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 5: Example of MI Scores  Query n-gram URL n-gram MI score  \"kanji\" ja (japanese)  11.3862  \"kanji\" zh (chinese)  6.2567  \"kanji\" en (english)  4.2110", "labels": [], "entities": []}, {"text": " Table 6: Example of MI Score  Query n-gram URL n-gram MI score  \"form\"  pdf  4.9067  \"form\"  htm  1.0916  \"video\"  watch  5.7192  \"video\"  htm -1.9079", "labels": [], "entities": []}, {"text": " Table 8: Example of Selecting Query Segmenta- tion  MI(q,u)  irs.gov  \"irs\"  11.2174  \"1040\"  11.6175  \"forms\"  7.5049", "labels": [], "entities": []}, {"text": " Table 11: Relative DCG 5 Improvement of  NavClick, Query-URL N-gram (II), and Query- URL N-gram Features (I) vs Core Feature Set  Length  NavClick  vs Core  (%)", "labels": [], "entities": [{"text": "Relative", "start_pos": 11, "end_pos": 19, "type": "METRIC", "confidence": 0.9708974361419678}]}, {"text": " Table 12: Relative DCG 5 Improvement of  NavClick, Query-URL N-gram Features (II) and  Query-URL N-gram Features (I) vs Core Feature  Set  NavClick vs  Core (%)", "labels": [], "entities": [{"text": "Relative", "start_pos": 11, "end_pos": 19, "type": "METRIC", "confidence": 0.9583878517150879}]}]}