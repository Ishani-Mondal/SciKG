{"title": [{"text": "A Comparison of Model Free versus Model Intensive Approaches to Sentence Compression", "labels": [], "entities": [{"text": "Sentence Compression", "start_pos": 64, "end_pos": 84, "type": "TASK", "confidence": 0.8471391499042511}]}], "abstractContent": [{"text": "This work introduces a model free approach to sentence compression, which grew out of ideas from Nomoto (2008), and examines how it compares to a state-of-art model intensive approach known as Tree-to-Tree Transducer, or T3 (Cohn and Lapata, 2008).", "labels": [], "entities": [{"text": "sentence compression", "start_pos": 46, "end_pos": 66, "type": "TASK", "confidence": 0.8122275471687317}]}, {"text": "It is found that a model free approach significantly outperforms T3 on the particular data we created from the Internet.", "labels": [], "entities": []}, {"text": "We also discuss what might have caused T3's poor performance .", "labels": [], "entities": []}], "introductionContent": [{"text": "While there area few notable exceptions (), it would be safe to say that much of prior research on sentence compression has been focusing on what we might call 'model-intensive approaches,' where the goal is to mimic human created compressions as faithfully as possible, using probabilistic and/or machine learning techniques ().", "labels": [], "entities": [{"text": "sentence compression", "start_pos": 99, "end_pos": 119, "type": "TASK", "confidence": 0.7608201205730438}]}, {"text": "Because of this, the question has never been raised as to whether a model free approach \u2212 where the goal is not to model what humans would produce as compression, but to provide compressions just as useful as those created by human \u2212 will offer a viable alternative to model intensive approaches.", "labels": [], "entities": []}, {"text": "This is the question we take on in this paper.", "labels": [], "entities": []}, {"text": "An immediate benefit of the model-free approach is that we could free ourselves from the drudgery of collecting gold standard data from humans, which is costly and time-consuming.", "labels": [], "entities": []}, {"text": "Another benefit is intellectual; it opens up an alternative avenue to addressing the problem of sentence compression hitherto under-explored.", "labels": [], "entities": [{"text": "sentence compression", "start_pos": 96, "end_pos": 116, "type": "TASK", "confidence": 0.7422244399785995}]}, {"text": "Also breaking from the tradition of previous research on sentence compression, we explore the use of naturally occurring data from the Internet as the gold standard.", "labels": [], "entities": [{"text": "sentence compression", "start_pos": 57, "end_pos": 77, "type": "TASK", "confidence": 0.7958663105964661}]}, {"text": "The present work builds on and takes further an approach called 'Generic Sentence Trimmer' (GST), demonstrating along the way that it could be adapted for English with relative ease.", "labels": [], "entities": [{"text": "Generic Sentence Trimmer' (GST)", "start_pos": 65, "end_pos": 96, "type": "TASK", "confidence": 0.7037670995507922}]}, {"text": "(GST was originally intended for Japanese.)", "labels": [], "entities": [{"text": "GST", "start_pos": 1, "end_pos": 4, "type": "DATASET", "confidence": 0.42221274971961975}]}, {"text": "In addition, to get a perspective on where we stand with this approach, we will look at how it fares against a state-of-the-art model intensive approach known as 'Tree-to-Tree Transducer' (T3), on the corpus we created.", "labels": [], "entities": []}], "datasetContent": [{"text": "We ran the Stanford Parser on NYT-RSS to extract dependency structures for sentences involved, to be used with GST/g (de;.", "labels": [], "entities": [{"text": "NYT-RSS", "start_pos": 30, "end_pos": 37, "type": "DATASET", "confidence": 0.9429681897163391}]}, {"text": "We manually developed 28 DMN rules out of NYT-RSS(A), some of which are presented in.", "labels": [], "entities": [{"text": "NYT-RSS", "start_pos": 42, "end_pos": 49, "type": "DATASET", "confidence": 0.8785887360572815}]}, {"text": "An alignment between the source sentence and its corresponding gold standard compression was made by SWA or a standard sequence alignment algorithm by.", "labels": [], "entities": [{"text": "SWA", "start_pos": 101, "end_pos": 104, "type": "DATASET", "confidence": 0.8553304672241211}]}, {"text": "Importantly, we setup GST/g and T3 in such away that they rely on the same set of dependency analyses and alignments when they are put into operation.", "labels": [], "entities": []}, {"text": "We trained T3 on NYT-RSS(A) with default settings except for \"-epsilon\" and \"-delete\" options which we turned off, as preliminary runs indicated that their use led to a degraded performance.", "labels": [], "entities": [{"text": "NYT-RSS", "start_pos": 17, "end_pos": 24, "type": "DATASET", "confidence": 0.6478312611579895}]}, {"text": "We also set the loss function as was given in the default settings.", "labels": [], "entities": []}, {"text": "We trained both GST/r, and T3 on NYT-RSS(A).", "labels": [], "entities": [{"text": "GST/r", "start_pos": 16, "end_pos": 21, "type": "DATASET", "confidence": 0.7647889057795206}, {"text": "NYT-RSS(A)", "start_pos": 33, "end_pos": 43, "type": "DATASET", "confidence": 0.8945862501859665}]}, {"text": "We ran GST/g and GST/g+r, i.e., GST/r pipelined with GST/g, varying the compression rate from 0.4 to 0.7.", "labels": [], "entities": []}, {"text": "This involved letting GST/g rank candidate compressions by S(p) and then choosing the first candidate to satisfy a given compression rate, whereas GST/g+r was made to output the highest ranking candidate as measured by p(y | x; \u03b8), which meets a particular compression rate.", "labels": [], "entities": []}, {"text": "It should be emphasized, however, that in T3, varying compression rate is not something the user has control over; so we accepted whatever output T3 generated fora given sentence.", "labels": [], "entities": []}, {"text": "shows how GST/g, GST/g+r, and T3 performed on NYT-RSS, along with the gold standard, on a scale of 1 to 5.", "labels": [], "entities": [{"text": "NYT-RSS", "start_pos": 46, "end_pos": 53, "type": "DATASET", "confidence": 0.9617035984992981}]}, {"text": "Ratings were solicited from 4 native speakers of English.", "labels": [], "entities": []}, {"text": "'CompR' indicates compression rate.", "labels": [], "entities": [{"text": "CompR", "start_pos": 1, "end_pos": 6, "type": "METRIC", "confidence": 0.9927883744239807}, {"text": "compression rate", "start_pos": 18, "end_pos": 34, "type": "METRIC", "confidence": 0.9447227716445923}]}, {"text": "'Intelligibility' means how well the compression reads; 'representativeness' how well the compression represents its source sentence.", "labels": [], "entities": []}, {"text": "presents a guideline for rating, describing what each rating should mean, which was also presented to human judges to facilitate evaluation.", "labels": [], "entities": []}, {"text": "The results in indicate a clear superiority of GST/g and GST/g+r over T3, while differences in intelligibility between GST/g and GST/g+r were found not statistically significant.", "labels": [], "entities": []}, {"text": "What is intriguing, though, is that GST/g produced performance statistically different in representativeness from GST/g+r at 5% level as marked by the asterisk.", "labels": [], "entities": [{"text": "GST/g", "start_pos": 36, "end_pos": 41, "type": "DATASET", "confidence": 0.5853719214598337}]}, {"text": "Shown in are examples of compression created by GST/g+r, GST/g and T3, together with gold standard compressions and relevant source sentences.", "labels": [], "entities": [{"text": "GST/g", "start_pos": 57, "end_pos": 62, "type": "DATASET", "confidence": 0.9156426191329956}]}, {"text": "One thing worth noting about the examples is that T3 keeps inserting out-of-the-source information into compression, which obviously has done more harm than good to compression.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 5: Results on NYT-RSS. '*'-marked figures  mean that performance of GST/g is different from  that of GST/g+r (on the comparable CompR) at  5% significance level according to t-test. The fig- ures indicate average ratings.", "labels": [], "entities": [{"text": "NYT-RSS", "start_pos": 21, "end_pos": 28, "type": "DATASET", "confidence": 0.8314283490180969}]}]}