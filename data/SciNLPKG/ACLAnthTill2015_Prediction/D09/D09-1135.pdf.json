{"title": [{"text": "Refining Grammars for Parsing with Hierarchical Semantic Knowledge", "labels": [], "entities": [{"text": "Refining Grammars", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.9219770431518555}]}], "abstractContent": [{"text": "This paper proposes a novel method to refine the grammars in parsing by utilizing semantic knowledge from HowNet.", "labels": [], "entities": [{"text": "HowNet", "start_pos": 106, "end_pos": 112, "type": "DATASET", "confidence": 0.9717339277267456}]}, {"text": "Based on the hierarchical state-split approach , which can refine grammars automatically in a data-driven manner, this study introduces semantic knowledge into the splitting process at two steps.", "labels": [], "entities": []}, {"text": "Firstly, each part-of-speech node will be annotated with a semantic tag of its terminal word.", "labels": [], "entities": []}, {"text": "These new tags generated in this step are semantic-related, which can provide a good start for splitting.", "labels": [], "entities": [{"text": "splitting", "start_pos": 95, "end_pos": 104, "type": "TASK", "confidence": 0.975735604763031}]}, {"text": "Secondly , a knowledge-based criterion is used to supervise the hierarchical splitting of these semantic-related tags, which can alleviate overfitting.", "labels": [], "entities": []}, {"text": "The experiments are carried out on both Chinese and English Penn Treebank show that the refined grammars with semantic knowledge can improve parsing performance significantly.", "labels": [], "entities": [{"text": "Chinese and English Penn Treebank", "start_pos": 40, "end_pos": 73, "type": "DATASET", "confidence": 0.7025853991508484}, {"text": "parsing", "start_pos": 141, "end_pos": 148, "type": "TASK", "confidence": 0.9675306677818298}]}, {"text": "Especially with respect to Chinese, our parser achieves an F 1 score of 87.5%, which is the best published result we are aware of.", "labels": [], "entities": [{"text": "F 1 score", "start_pos": 59, "end_pos": 68, "type": "METRIC", "confidence": 0.9925620953241984}]}], "introductionContent": [{"text": "At present, most high-performance parsers are based on probabilistic context-free grammars (PCFGs) in one way or another).", "labels": [], "entities": []}, {"text": "However, restricted by the strong contextfree assumptions, the original PCFG model which simply takes the grammars and probabilities off a treebank, does not perform well.", "labels": [], "entities": [{"text": "PCFG", "start_pos": 72, "end_pos": 76, "type": "DATASET", "confidence": 0.878596842288971}]}, {"text": "Therefore, a variety of techniques have been developed to enrich and generalize the original grammar, ranging from lexicalization to symbol annotation.", "labels": [], "entities": [{"text": "symbol annotation", "start_pos": 133, "end_pos": 150, "type": "TASK", "confidence": 0.751983106136322}]}, {"text": "* Corresponding author: Xihong Wu.", "labels": [], "entities": []}, {"text": "Lexicalized PCFGs use the structural features on the lexical head of phrasal node in a tree, and get significant improvements for parsing).", "labels": [], "entities": [{"text": "parsing", "start_pos": 130, "end_pos": 137, "type": "TASK", "confidence": 0.9623810648918152}]}, {"text": "However, they suffer from the problem of fundamental sparseness of the lexical dependency information.", "labels": [], "entities": []}, {"text": "(. In order to deal with this limitation, a variety of unlexicalized parsing techniques have been proposed.", "labels": [], "entities": []}, {"text": "annotates each node by its parent category in a tree, and gets significant improvements compared with the original PCFGs on the Penn Treebank.", "labels": [], "entities": [{"text": "Penn Treebank", "start_pos": 128, "end_pos": 141, "type": "DATASET", "confidence": 0.9774240553379059}]}, {"text": "Then, some manual and automatic symbol splitting methods are presented, which get comparable performance with lexicalized parsers ().", "labels": [], "entities": [{"text": "symbol splitting", "start_pos": 32, "end_pos": 48, "type": "TASK", "confidence": 0.7487381100654602}]}, {"text": "Recently, introduces an automatic hierarchical state-split approach to refine the grammars, which can alternately split and merge the basic nonterminals by the Expectation-Maximization (EM) algorithm.", "labels": [], "entities": []}, {"text": "In this method, the nonterminals are split to different degrees, as appropriate to the actual complexity in the data.", "labels": [], "entities": []}, {"text": "The grammars refined in this way are proved to be much more accurate and compact than previous work on automatic annotation.", "labels": [], "entities": []}, {"text": "This data-driven method still suffers from the overfitting problem, which maybe improved by integrating other external information.", "labels": [], "entities": []}, {"text": "In this paper, we propose a novel method that combines the strengths of both data-driven and knowledge-driven strategies to refine grammars.", "labels": [], "entities": []}, {"text": "Based on the work proposed by, we use the semantic knowledge from HowNet () to supervise the hierarchical state-split process at the part-ofspeech(POS) level.", "labels": [], "entities": [{"text": "HowNet", "start_pos": 66, "end_pos": 72, "type": "DATASET", "confidence": 0.9279136061668396}]}, {"text": "At first, we define the most general hypernym in HowNet as the semantic class of a word, and then use this semantic class to initialize the tag of each POS node.", "labels": [], "entities": [{"text": "HowNet", "start_pos": 49, "end_pos": 55, "type": "DATASET", "confidence": 0.9444171786308289}]}, {"text": "In this way, anew set of semantic-related tags is generated, and a good starting annotation is provided to reduce the search space for the EM algorithm in the splitting process.", "labels": [], "entities": []}, {"text": "Then, in order to mitigate the overfitting risk, the hierarchical hypernym-hyponym relation between hypernyms in HowNet is utilized to supervise the splitting of these new semanticrelated tags.", "labels": [], "entities": [{"text": "HowNet", "start_pos": 113, "end_pos": 119, "type": "DATASET", "confidence": 0.9620494246482849}]}, {"text": "By introducing a knowledge-based criterion, these new tags are decided whether or not to split into subcategories from a semantic perspective.", "labels": [], "entities": []}, {"text": "To investigate the effectiveness of the presented approach, several experiments are conduced on both Chinese and English.", "labels": [], "entities": []}, {"text": "They reveal that the semantic knowledge is potentially useful to parsing.", "labels": [], "entities": [{"text": "parsing", "start_pos": 65, "end_pos": 72, "type": "TASK", "confidence": 0.9706206321716309}]}, {"text": "The remainder of this paper is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 reviews some closely related works, including the lexical semantic related parsing and the hierarchical state-split unlexicalized parsing.", "labels": [], "entities": [{"text": "lexical semantic related parsing", "start_pos": 60, "end_pos": 92, "type": "TASK", "confidence": 0.6434163898229599}, {"text": "hierarchical state-split unlexicalized parsing", "start_pos": 101, "end_pos": 147, "type": "TASK", "confidence": 0.5755714848637581}]}, {"text": "In section 3, the presented method for grammar refining is described in detail, and several experiments are carried out for evaluation in Section 4.", "labels": [], "entities": [{"text": "grammar refining", "start_pos": 39, "end_pos": 55, "type": "TASK", "confidence": 0.8287573158740997}]}, {"text": "Conclusions are drawn in Section 5.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we designed several experiments to investigate the validity of refining grammars with semantic knowledge.", "labels": [], "entities": []}, {"text": "We did experiments on Chinese and English.", "labels": [], "entities": []}, {"text": "In order to make a fair comparison with previous works, we split the standard corpora as shown in.", "labels": [], "entities": []}, {"text": "Our parsers were evaluated by the EVALB parseval reference implementation 1 . The Berkeley parser 2 was used to train the models with the original automatic hierarchical state-split process.", "labels": [], "entities": []}, {"text": "The semantic resource we used to improve parsing was HowNet, which has been introduced in Subsection 3.1.", "labels": [], "entities": [{"text": "parsing", "start_pos": 41, "end_pos": 48, "type": "TASK", "confidence": 0.969477117061615}, {"text": "HowNet", "start_pos": 53, "end_pos": 59, "type": "DATASET", "confidence": 0.9324800968170166}]}, {"text": "Statistical significance was checked using Dan Bikels randomized parsing evaluation comparator with the default setting of 10,000 iterations 3 .  First of all, we ran experiments with different semantic representation methods on Chinese.", "labels": [], "entities": [{"text": "significance", "start_pos": 12, "end_pos": 24, "type": "METRIC", "confidence": 0.8012335896492004}]}, {"text": "The polysemous words in the training set were annotated with the WSD strategy of first sense option, which was proved to be useful in.", "labels": [], "entities": [{"text": "WSD", "start_pos": 65, "end_pos": 68, "type": "TASK", "confidence": 0.5724838972091675}]}, {"text": "As mentioned in Subsection 3.1, the semantic information of each word can be represented as a hierarchical relation among its hypernyms from specialty to generalization in HowNet.", "labels": [], "entities": [{"text": "HowNet", "start_pos": 172, "end_pos": 178, "type": "DATASET", "confidence": 0.9645277857780457}]}, {"text": "In order to choose the appropriate level of granularity to represent words, we annotated the training set with different levels of granularity as semantic representation.", "labels": [], "entities": []}, {"text": "In our experiments, the automatic hierarchical state-split process is used to train models on these training sets with different level of semantic representation.", "labels": [], "entities": []}, {"text": "We tried two kinds of semantic representations, one is using the most general hypernym, and the other is using the most special hypernym.", "labels": [], "entities": []}, {"text": "Results in proved the effectiveness of our method in Subsection 3.2.", "labels": [], "entities": []}, {"text": "When we annotated the tag of each POS node with the most general hypernym of its terminal word, the parser performs much better than both the baseline and the one annotated with the most special hypernym.", "labels": [], "entities": []}, {"text": "Moreover, the F 1 score starts dropping after 3 training iterations on the training set annotated with the most special hypernyms, while it is still improving with the most general one, indicating overfitting.", "labels": [], "entities": [{"text": "F 1 score", "start_pos": 14, "end_pos": 23, "type": "METRIC", "confidence": 0.9897990822792053}]}, {"text": "When the training set was annotated with the most general hypernyms, there were only 57 new semantic-related tags such as \"NN-Entity\", \"NNAttribute\" and soon.", "labels": [], "entities": []}, {"text": "However, when the training set was annotated with the most special hypernyms, 4313 new tags would be introduced.", "labels": [], "entities": []}, {"text": "Ob-viously, it introduces too many tags at once and is difficult to refine appropriate grammars in the subsequent step starting with this over-splitting training set.", "labels": [], "entities": []}, {"text": "Several experiments were carried out on Chinese and English to verify the effectiveness of refining grammars with semantic knowledge.", "labels": [], "entities": []}, {"text": "We took the most general hypernym as the semantic representation, and the polysemous words in the training set were annotated with the WSD strategy of first sense option.", "labels": [], "entities": []}, {"text": "In our experiments, three kinds of method were compared.", "labels": [], "entities": []}, {"text": "The baseline was trained on the raw training set with the automatic hierarchical statesplit approach.", "labels": [], "entities": []}, {"text": "Then, we improved it with the semantic annotation, which annotated the raw training set with the most general hypernyms as semantic representations, while keeping the training approach used in the baseline unchanged.", "labels": [], "entities": []}, {"text": "Further, our knowledge-based criterion was introduced to supervise the automatic hierarchical state-split process with semantic knowledge.", "labels": [], "entities": []}, {"text": "In this section, since most of the parsers (including the baseline parser and our advanced parsers) had the same behavior on development set that the accuracy continued increasing in the five beginning iterations and then dropped at the sixth iteration, we chose the results at the fifth iteration as our final test set parsing performance.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 150, "end_pos": 158, "type": "METRIC", "confidence": 0.9988147020339966}]}, {"text": "shows that refining grammars with semantic knowledge can help improve parsing performance significantly on Chinese (sentences of length 40 or less).", "labels": [], "entities": []}, {"text": "Benefitting from the good starting annotations, our parser achieved significant improvements compared with the baseline (86.8% vs. 86.1%, p<.08).", "labels": [], "entities": []}, {"text": "It proved that the good starting annotations with semantic knowledge were effective in the splitting process.", "labels": [], "entities": [{"text": "splitting", "start_pos": 91, "end_pos": 100, "type": "TASK", "confidence": 0.9792962074279785}]}, {"text": "Further, we supervised the splitting of the new semantic-related tags from the semantic annotations, and achieved the best results at the fifth iteration.", "labels": [], "entities": []}, {"text": "The best F 1 score reached 87.5%, with an error rate reduction of 10.1%, relative to the baseline (p<.004).", "labels": [], "entities": [{"text": "F 1 score", "start_pos": 9, "end_pos": 18, "type": "METRIC", "confidence": 0.9827986558278402}, {"text": "error rate", "start_pos": 42, "end_pos": 52, "type": "METRIC", "confidence": 0.9932501316070557}]}, {"text": "compared our methods with the best previous works on Chinese.", "labels": [], "entities": []}, {"text": "The result showed that refining grammars integrated with semantic knowledge could resolve syntactic ambiguities re-   markably and achieved the state-of-the-art performance on Chinese.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: Our final parsing performance compared  with the best previous works on Chinese.", "labels": [], "entities": []}]}