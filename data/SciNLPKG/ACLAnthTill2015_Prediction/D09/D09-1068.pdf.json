{"title": [{"text": "Improving Web Search Relevance with Semantic Features", "labels": [], "entities": [{"text": "Improving Web Search Relevance", "start_pos": 0, "end_pos": 30, "type": "TASK", "confidence": 0.9090468883514404}]}], "abstractContent": [{"text": "Most existing information retrieval (IR) systems do not take much advantage of natural language processing (NLP) techniques due to the complexity and limited observed effectiveness of applying NLP to IR.", "labels": [], "entities": [{"text": "information retrieval (IR)", "start_pos": 14, "end_pos": 40, "type": "TASK", "confidence": 0.8565910458564758}]}, {"text": "In this paper, we demonstrate that substantial gains can be obtained over a strong baseline using NLP techniques, if properly handled.", "labels": [], "entities": []}, {"text": "We propose a framework for deriving semantic text matching features from named entities identified in Web queries; we then utilize these features in a supervised machine-learned ranking approach, applying a set of emerging machine learning techniques.", "labels": [], "entities": [{"text": "semantic text matching features from named entities identified in Web queries", "start_pos": 36, "end_pos": 113, "type": "TASK", "confidence": 0.8157678842544556}]}, {"text": "Our approach is especially useful for queries that contain multiple types of concepts.", "labels": [], "entities": []}, {"text": "Comparing to a major commercial Web search engine, we observe a substantial 4% DCG5 gain over the affected queries.", "labels": [], "entities": []}], "introductionContent": [{"text": "Most existing IR models score documents primarily based on various term statistics.", "labels": [], "entities": [{"text": "IR", "start_pos": 14, "end_pos": 16, "type": "TASK", "confidence": 0.9511787295341492}]}, {"text": "In traditional models-from classic probabilistic models, through vector space models (), to well studied statistical language models)-these term statistics have been captured directly in the ranking formula.", "labels": [], "entities": []}, {"text": "More recently, learning to rank approaches to IR) have become prominent; in these frameworks, that aim at learning a ranking function from data, term statistics are often modeled as term matching features in a machine learning process.", "labels": [], "entities": [{"text": "IR", "start_pos": 46, "end_pos": 48, "type": "TASK", "confidence": 0.9847054481506348}]}, {"text": "Traditional text matching features are mainly based on frequencies of n-grams of the user's query in a variety of document sections, such as the document title, body text, anchor text, and soon.", "labels": [], "entities": [{"text": "text matching", "start_pos": 12, "end_pos": 25, "type": "TASK", "confidence": 0.7115503996610641}]}, {"text": "Global information such as frequency of term or term group in the corpus may also be used, as well as its combination with local statistics -producing relative scores such as tf \u00b7 idf or BM25 scores (.", "labels": [], "entities": [{"text": "BM25 scores", "start_pos": 187, "end_pos": 198, "type": "METRIC", "confidence": 0.733377993106842}]}, {"text": "Matching maybe restricted to certain window sizes to enforce proximity, or maybe more lenient, allowing unordered sequences and nonconsecutive sequences fora higher recall.", "labels": [], "entities": [{"text": "recall", "start_pos": 165, "end_pos": 171, "type": "METRIC", "confidence": 0.9988162517547607}]}, {"text": "Even before machine learning was applied to IR, NLP techniques such as Named Entity Recognition (NER), Part-of-Speech (POS) tagging, and parsing have been applied to both query modeling and document indexing (Smeaton and van).", "labels": [], "entities": [{"text": "IR", "start_pos": 44, "end_pos": 46, "type": "TASK", "confidence": 0.983314037322998}, {"text": "Named Entity Recognition (NER)", "start_pos": 71, "end_pos": 101, "type": "TASK", "confidence": 0.7248718291521072}, {"text": "Part-of-Speech (POS) tagging", "start_pos": 103, "end_pos": 131, "type": "TASK", "confidence": 0.6688716769218445}, {"text": "parsing", "start_pos": 137, "end_pos": 144, "type": "TASK", "confidence": 0.9715689420700073}, {"text": "query modeling", "start_pos": 171, "end_pos": 185, "type": "TASK", "confidence": 0.7838620245456696}, {"text": "document indexing", "start_pos": 190, "end_pos": 207, "type": "TASK", "confidence": 0.6657277643680573}]}, {"text": "For example, statistical concept language models generalize classic n-gram models to concept n-gram model by enforcing query term proximity within each concept.", "labels": [], "entities": []}, {"text": "However, researchers have often reported limited gains or even decreased performance when applying NLP to IR.", "labels": [], "entities": [{"text": "IR", "start_pos": 106, "end_pos": 108, "type": "TASK", "confidence": 0.9486653208732605}]}, {"text": "Typically, concepts detected through NLP techniques either in the query or in documents are used as proximity constraints for text matching), ignoring the actual concept type.", "labels": [], "entities": [{"text": "text matching", "start_pos": 126, "end_pos": 139, "type": "TASK", "confidence": 0.7039771676063538}]}, {"text": "The machine learned approach to document ranking provides us with an opportunity to revisit the manner in which NLP information is used for ranking.", "labels": [], "entities": []}, {"text": "Using knowledge gained from NLP application as features rather than heuristically allows us much greater flexibility in the amount and variability of information used -e.g., incorporating knowledge about the actual entity types.", "labels": [], "entities": []}, {"text": "This has several benefits: first, entity types appearing in queries are an indicator of the user's intent.", "labels": [], "entities": []}, {"text": "A query consisting of a business category and a location (e.g., hotels Palo Alto) appears to be informational, and perhaps is best answered with a page containing a list of hotels in Palo Alto.", "labels": [], "entities": []}, {"text": "Queries containing a business name and a location (e.g., Fuki Sushi Palo Alto) are more navigational in nature -for many users, the intent is finding the homepage of a specific business.", "labels": [], "entities": [{"text": "Fuki Sushi Palo Alto", "start_pos": 57, "end_pos": 77, "type": "DATASET", "confidence": 0.9387524127960205}]}, {"text": "Similarly, entity types appearing in documents are an indicator of the document type.", "labels": [], "entities": []}, {"text": "For example, if \"Palo Alto\" appears ten times in document's body text, it is more likely to be a local listing page than a homepage.", "labels": [], "entities": [{"text": "Palo Alto\"", "start_pos": 17, "end_pos": 27, "type": "DATASET", "confidence": 0.8499062259991964}]}, {"text": "For the query hotels Palo Alto, a local listing page maybe a good page, while for the query Fuki Sushi Palo Alto a listing page is not a good page.", "labels": [], "entities": [{"text": "Fuki Sushi Palo Alto", "start_pos": 92, "end_pos": 112, "type": "DATASET", "confidence": 0.8418947160243988}]}, {"text": "In addition, knowledge of the particular entities in queries allows us to incorporate external knowledge about these entities, such as entity-specific stopwords (\"inc.\" as in Yahoo Inc. or \"services\" as in kaiser medical service), and soon.", "labels": [], "entities": []}, {"text": "Finally, even when using named entities only for deriving proximity-related features, we can benefit from applying different levels of proximity for different entities.", "labels": [], "entities": []}, {"text": "For example, for entities like cities (e.g., \"River Side\"), the proximity requirement is fairly strict: we should not allow extra words between the original terms, and preserve their order.", "labels": [], "entities": []}, {"text": "For other entities the proximity constraint can be relaxed-for example, for person names, due to the middle name convention: Hillary Clinton vs. Hillary R. Clinton.", "labels": [], "entities": []}, {"text": "In this paper, we propose a systematic approach to modeling semantic features, incorporating concept types extracted from query analysis.", "labels": [], "entities": []}, {"text": "Vertical attributes, such as city-state relationships, metropolitan definition, or idf scores from a domain specific corpus, are extracted for each concept type from vertical database.", "labels": [], "entities": []}, {"text": "The vertical attributes, together with the concept attributes, are used to compose a set of semantic features for machine learning based IR models.", "labels": [], "entities": []}, {"text": "A few machine learning techniques are discussed to further improve relevance for subclass of difficult queries such as queries containing multiple types of concepts.", "labels": [], "entities": []}, {"text": "shows an overview of our approach; after discussing related work in Section 2, we spend Sections 3 to 5 of the paper describing the components of our system.", "labels": [], "entities": []}, {"text": "We then evaluate the effectiveness of our approach both using general queries and with a set of \"difficult\" queries; our results show that the techniques are robust, and particularly effective for this type of queries.", "labels": [], "entities": []}, {"text": "We conclude in Section 7.", "labels": [], "entities": []}], "datasetContent": [{"text": "We now measure the effectiveness of our proposal, and answer related questions, through extensive experimental evaluation.", "labels": [], "entities": []}, {"text": "We begin by examining the effectiveness of features as well as the modeling approaches introduced in Section 5 on a particular class of queries-those with a local intent.", "labels": [], "entities": []}, {"text": "We proceed by evaluating whether if the type associated with each entity really matters by comparing results with type dependent semantic features and segment features.", "labels": [], "entities": []}, {"text": "Finally, we examine the robustness of our features by measuring the change in the accuracy of our resulting ranking function when the query analysis is wrong; we do this by introducing simulated noise into the query analysis results.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 82, "end_pos": 90, "type": "METRIC", "confidence": 0.9986729621887207}]}, {"text": "Our training, validation and test sets are humanlabeled query-document pairs.", "labels": [], "entities": []}, {"text": "Each item in the sets consists of a feature vector xi representing the query and the document, and a judgment score y i assigned by a human.", "labels": [], "entities": []}, {"text": "There are around 600 features in each vector, including both the newly introduced semantic features and existing features; features are either query-dependent ones, document-dependent ones, or query-anddocument-dependent features.", "labels": [], "entities": []}, {"text": "The training set is based on uniformly sampled Web queries from our query log, and top ranked documents returned by commercial search engines for these queries; this set consists of 1.24M querydocument pairs.", "labels": [], "entities": []}, {"text": "We use two additional sets for validation and testing.", "labels": [], "entities": [{"text": "validation", "start_pos": 31, "end_pos": 41, "type": "TASK", "confidence": 0.9595938324928284}]}, {"text": "One set is based on uniformly sampled Web queries, and contains 42790 validation samples and 70320 test samples.", "labels": [], "entities": []}, {"text": "The second set is based on uniformly sampled local queries.", "labels": [], "entities": []}, {"text": "By local queries, we mean queries that contain at least two types of semantic tags: a location tag (such as street, city or state name) and a business tag (a business name or business category).", "labels": [], "entities": []}, {"text": "We refer to this class of queries \"local queries,\" as users often type this kind of queries in local vertical search.", "labels": [], "entities": []}, {"text": "The local query set consists of 11040 validation samples and 39169 test samples.", "labels": [], "entities": []}, {"text": "In the training set we described above, there are 56299 training samples out of the 1.24M total number of training samples that satisfy the definition of local queries.", "labels": [], "entities": []}, {"text": "We call this set local training subset.", "labels": [], "entities": []}, {"text": "To evaluate the effectiveness of our semantic features we use Discounted Cumulative Gain (DCG)), a widelyused metric for measuring Web search relevance.).", "labels": [], "entities": []}, {"text": "Given a query and a ranked list of K documents (K = 5 in our experiments), the DCG for this query is defined as where y i \u2208 [0, 10] is a relevance score for the document at position i, typically assigned by a human, where 10 is assigned to the most relevant documents and 0 to the least relevant ones.", "labels": [], "entities": []}, {"text": "To measure statistical significance, we use the Wilcoxon test; when the p-value is below 0.01 we consider a difference to be statistically significant and mark it with a bold font in the result table.", "labels": [], "entities": []}, {"text": "We use Stochastic Gradient Boosting Trees (SGBT)), a robust none linear regression algorithm, for training ranking functions and, as mentioned earlier,) for model adaptation.", "labels": [], "entities": [{"text": "model adaptation", "start_pos": 157, "end_pos": 173, "type": "TASK", "confidence": 0.7956528961658478}]}, {"text": "Training parameters are selected to optimize the relevance on a separated validation set.", "labels": [], "entities": []}, {"text": "The best resulting is evaluated against the test set; all results presented here use the test set for evaluation.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Evaluation of Ranking Models Trained  Against Over-weighted Local Queries with Se- mantic Features on the Local Query Test Set", "labels": [], "entities": [{"text": "Local Query Test Set", "start_pos": 116, "end_pos": 136, "type": "DATASET", "confidence": 0.7238185256719589}]}, {"text": " Table 2: Trada Algorithms with Semantic Features  on Local Query Test Set", "labels": [], "entities": []}, {"text": " Table 3: Type-dependent Semantic Features vs.  Segment Features", "labels": [], "entities": []}, {"text": " Table 4: Relevance with simulated error on local  query test set", "labels": [], "entities": [{"text": "Relevance", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.8976214528083801}]}, {"text": " Table 5: Search relevance with simulated error for  semantic features on general test set", "labels": [], "entities": []}]}