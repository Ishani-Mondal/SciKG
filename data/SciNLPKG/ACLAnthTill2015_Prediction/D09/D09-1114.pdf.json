{"title": [{"text": "The Feature Subspace Method for SMT System Combination", "labels": [], "entities": [{"text": "SMT System Combination", "start_pos": 32, "end_pos": 54, "type": "TASK", "confidence": 0.919502854347229}]}], "abstractContent": [{"text": "Recently system combination has been shown to bean effective way to improve translation quality over single machine translation systems.", "labels": [], "entities": [{"text": "translation", "start_pos": 76, "end_pos": 87, "type": "TASK", "confidence": 0.9687398672103882}]}, {"text": "In this paper, we present a simple and effective method to systematically derive an ensemble of SMT systems from one baseline linear SMT model for use in system combination.", "labels": [], "entities": [{"text": "SMT", "start_pos": 96, "end_pos": 99, "type": "TASK", "confidence": 0.9836073517799377}, {"text": "SMT", "start_pos": 133, "end_pos": 136, "type": "TASK", "confidence": 0.9585107564926147}]}, {"text": "Each system in the resulting ensemble is based on a feature set derived from the features of the baseline model (typically a subset of it).", "labels": [], "entities": []}, {"text": "We will discuss the principles to determine the feature sets for derived systems, and present in detail the system combination model used in our work.", "labels": [], "entities": []}, {"text": "Evaluation is performed on the data sets for NIST 2004 and NIST 2005 Chinese-to-English machine translation tasks.", "labels": [], "entities": [{"text": "NIST 2004", "start_pos": 45, "end_pos": 54, "type": "DATASET", "confidence": 0.9198571741580963}, {"text": "NIST 2005 Chinese-to-English machine translation tasks", "start_pos": 59, "end_pos": 113, "type": "TASK", "confidence": 0.810145229101181}]}, {"text": "Experimental results show that our method can bring significant improvements to baseline systems with state-of-the-art performance.", "labels": [], "entities": []}], "introductionContent": [{"text": "Research on Statistical Machine Translation (SMT) has shown substantial progress in recent years.", "labels": [], "entities": [{"text": "Statistical Machine Translation (SMT)", "start_pos": 12, "end_pos": 49, "type": "TASK", "confidence": 0.8875254690647125}]}, {"text": "Since the success of phrase-based methods (, models based on formal syntax) or linguistic syntax () have also achieved state-of-the-art performance.", "labels": [], "entities": []}, {"text": "As a result of the increasing numbers of available machine translation systems, studies on system combination have been drawing more and more attention in SMT research.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 51, "end_pos": 70, "type": "TASK", "confidence": 0.7162479907274246}, {"text": "system combination", "start_pos": 91, "end_pos": 109, "type": "TASK", "confidence": 0.7111986577510834}, {"text": "SMT", "start_pos": 155, "end_pos": 158, "type": "TASK", "confidence": 0.9976921081542969}]}, {"text": "There have been many successful attempts to combine outputs from multiple machine translation systems to further improve translation quality.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 74, "end_pos": 93, "type": "TASK", "confidence": 0.7525518834590912}]}, {"text": "A system combination model usually takes nbest translations of single systems as input, and depending on the combination strategy, different methods can be used.", "labels": [], "entities": []}, {"text": "Sentence-level combination methods directly select hypotheses from original outputs of single SMT systems (, while phrase-level or word-level combination methods are more complicated and could produce new translations different from any translations in the input (.", "labels": [], "entities": [{"text": "SMT", "start_pos": 94, "end_pos": 97, "type": "TASK", "confidence": 0.9785112738609314}]}, {"text": "Among all the factors contributing to the success of system combination, there is no doubt that the availability of multiple machine translation systems is an indispensable premise.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 125, "end_pos": 144, "type": "TASK", "confidence": 0.7630689144134521}]}, {"text": "Although various approaches to SMT system combination have been explored, including enhanced combination model structure ( , better word alignment between translations () and improved confusion network construction (, most previous work simply used the ensemble of SMT systems based on different models and paradigms at hand and did not tackle the issue of how to obtain the ensemble in a principled way.", "labels": [], "entities": [{"text": "SMT system combination", "start_pos": 31, "end_pos": 53, "type": "TASK", "confidence": 0.9375076691309611}, {"text": "word alignment between translations", "start_pos": 132, "end_pos": 167, "type": "TASK", "confidence": 0.8002444207668304}, {"text": "confusion network construction", "start_pos": 184, "end_pos": 214, "type": "TASK", "confidence": 0.6539871990680695}, {"text": "SMT", "start_pos": 265, "end_pos": 268, "type": "TASK", "confidence": 0.9492414593696594}]}, {"text": "To our knowledge the only work discussed this problem is, in which they experimented with building different SMT systems by varying one or more sub-models (i.e. translation model or distortion model) of an existing SMT system, and observed that changes in early-stage model training introduced most diversities in translation outputs.", "labels": [], "entities": [{"text": "SMT", "start_pos": 109, "end_pos": 112, "type": "TASK", "confidence": 0.992450475692749}]}, {"text": "In this paper, we address the problem of building an ensemble of diversified machine translation systems from a single translation engine for system combination.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 77, "end_pos": 96, "type": "TASK", "confidence": 0.7251683622598648}]}, {"text": "In particular, we propose a novel Feature Subspace method for the ensemble construction based on any baseline SMT model which can be formulated as a standard linear function.", "labels": [], "entities": [{"text": "SMT", "start_pos": 110, "end_pos": 113, "type": "TASK", "confidence": 0.9832595586776733}]}, {"text": "Each system within the ensemble is based on a group of features directly derived from the baseline model with minimal efforts (which is typically a subset of the features used in the baseline model), and the resulting system is optimized in the derived feature space accordingly.", "labels": [], "entities": []}, {"text": "We evaluated our method on the test sets for NIST 2004 and NIST 2005 Chinese-to-English machine translation tasks using two baseline SMT systems with state-of-the-art performance.", "labels": [], "entities": [{"text": "NIST 2004", "start_pos": 45, "end_pos": 54, "type": "DATASET", "confidence": 0.8851307928562164}, {"text": "NIST 2005 Chinese-to-English machine translation tasks", "start_pos": 59, "end_pos": 113, "type": "TASK", "confidence": 0.8141481280326843}, {"text": "SMT", "start_pos": 133, "end_pos": 136, "type": "TASK", "confidence": 0.9808268547058105}]}, {"text": "Experimental results show that the feature subspace method can bring significant improvements to both baseline systems.", "labels": [], "entities": []}, {"text": "The rest of the paper is organized as follows.", "labels": [], "entities": []}, {"text": "The motivation of our work is described on Section 2.", "labels": [], "entities": []}, {"text": "In Section 3, we first give a detailed description about feature subspace method, including the principle to select subspaces from all possible options, and then an n-gram consensusbased sentence-level system combination method is presented.", "labels": [], "entities": []}, {"text": "Experimental results are given in Section 4.", "labels": [], "entities": []}, {"text": "Section 5 discusses some related issues and concludes the paper.", "labels": [], "entities": []}], "datasetContent": [{"text": "We first evaluate the oracle performance on the n-best lists of baseline systems and on the combined n-best lists of sub-systems generated from each baseline system.", "labels": [], "entities": []}, {"text": "The oracle translations are obtained by using the metric of sentence-level BLEU score (.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 75, "end_pos": 85, "type": "METRIC", "confidence": 0.9712514877319336}]}, {"text": "shows the evaluation results, in which Baseline stands for baseline system with a 5-gram LM feature, and FS stands for 11 sub-systems derived from the baseline system.", "labels": [], "entities": [{"text": "FS", "start_pos": 105, "end_pos": 107, "type": "METRIC", "confidence": 0.9984219074249268}]}, {"text": "For both SYS1 and SYS2, feature subspace method achieves higher oracle BLEU and lower TER scores on both MT04 and MT05 test sets, which gives the feature subspace method more potential to achieve higher performance than the baseline systems.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 71, "end_pos": 75, "type": "METRIC", "confidence": 0.9967526197433472}, {"text": "TER", "start_pos": 86, "end_pos": 89, "type": "METRIC", "confidence": 0.9988743662834167}, {"text": "MT04", "start_pos": 105, "end_pos": 109, "type": "DATASET", "confidence": 0.9363508224487305}, {"text": "MT05 test sets", "start_pos": 114, "end_pos": 128, "type": "DATASET", "confidence": 0.8830333153406779}]}, {"text": "We then investigate the ratio of translation candidates in the combined n-best lists of nonbaseline sub-systems that are not included in the baseline's n-best list.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: An example of translations generated  from the same decoder but with different feature  settings.", "labels": [], "entities": []}, {"text": " Table 2: Parameters of related phrases for exam- ples in Table 1.", "labels": [], "entities": []}, {"text": " Table 3: Data set statistics.", "labels": [], "entities": []}, {"text": " Table 4: Oracle BLEU and TER scores on base- line systems and their generated sub-systems.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 17, "end_pos": 21, "type": "METRIC", "confidence": 0.9778362512588501}, {"text": "TER", "start_pos": 26, "end_pos": 29, "type": "METRIC", "confidence": 0.9853814840316772}]}, {"text": " Table 6: Translation results of Baseline, Base- line+mLM and FS-Comb (+: significant better  than baseline system with \u00ed \u00b5\u00ed\u00b1\u009d < 0.05; ++: signifi- cant better than baseline system with \u00ed \u00b5\u00ed\u00b1\u009d < 0.01; *:  no significant improvement).", "labels": [], "entities": [{"text": "FS-Comb", "start_pos": 62, "end_pos": 69, "type": "METRIC", "confidence": 0.9801718592643738}]}, {"text": " Table 8: Performances of sentence-level combi- nation on multiple SMT systems.", "labels": [], "entities": [{"text": "SMT", "start_pos": 67, "end_pos": 70, "type": "TASK", "confidence": 0.9783188700675964}]}]}