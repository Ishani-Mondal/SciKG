{"title": [{"text": "Learning Linear Ordering Problems for Better Translation *", "labels": [], "entities": []}], "abstractContent": [{"text": "We apply machine learning to the Linear Ordering Problem in order to learn sentence-specific reordering models for machine translation.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 115, "end_pos": 134, "type": "TASK", "confidence": 0.7555304765701294}]}, {"text": "We demonstrate that even when these models are used as a mere preprocessing step for German-English translation, they significantly outperform Moses' integrated lexicalized reordering model.", "labels": [], "entities": [{"text": "German-English translation", "start_pos": 85, "end_pos": 111, "type": "TASK", "confidence": 0.552743136882782}]}, {"text": "Our models are trained on automatically aligned bitext.", "labels": [], "entities": []}, {"text": "Their form is simple but novel.", "labels": [], "entities": []}, {"text": "They assess, based on features of the input sentence, how strongly each pair of input word tokens w i , w j would like to reverse their relative order.", "labels": [], "entities": []}, {"text": "Combining all these pairwise preferences to find the best global reordering is NP-hard.", "labels": [], "entities": []}, {"text": "However , we present a non-trivial O(n 3) algorithm , based on chart parsing, that at least finds the best reordering within a certain exponentially large neighborhood.", "labels": [], "entities": [{"text": "chart parsing", "start_pos": 63, "end_pos": 76, "type": "TASK", "confidence": 0.780208170413971}]}, {"text": "We show how to iterate this reordering process within a local search algorithm, which we use in training.", "labels": [], "entities": []}], "introductionContent": [{"text": "Machine translation is an important but difficult problem.", "labels": [], "entities": [{"text": "Machine translation", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.8488403260707855}]}, {"text": "One of the properties that makes it difficult is the fact that different languages express the same concepts in different orders.", "labels": [], "entities": []}, {"text": "A machine translation system must therefore rearrange the source language concepts to produce a fluent translation in the target language.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 2, "end_pos": 21, "type": "TASK", "confidence": 0.7608586251735687}]}, {"text": "Phrase-based translation systems rely heavily on the target language model to ensure a fluent output order.", "labels": [], "entities": [{"text": "Phrase-based translation", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.7372883260250092}]}, {"text": "However, a target n-gram language model alone is known to be inadequate.", "labels": [], "entities": []}, {"text": "Thus, translation systems should also look at how the source sentence prefers to reorder.", "labels": [], "entities": []}, {"text": "Yet past systems have traditionally used rather weak models of the reordering process.", "labels": [], "entities": []}, {"text": "They may look only at the distance between neighboring phrases, or depend only on phrase unigrams.", "labels": [], "entities": []}, {"text": "The decoders also rely on search error, in the form of limited reordering windows, for both efficiency and translation quality.", "labels": [], "entities": []}, {"text": "Demonstrating the inadequacy of such approaches, Al-Onaizan and showed that even given the words in the reference translation, and their alignment to the source words, a decoder of this sort charged with merely rearranging them into the correct target-language order could achieve a BLEU score () of at best 69%-and that only when restricted to keep most words very close to their source positions.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 283, "end_pos": 293, "type": "METRIC", "confidence": 0.98896324634552}]}, {"text": "This paper introduces a more sophisticated model of reordering based on the Linear Ordering Problem (LOP), itself an NP-hard permutation problem.", "labels": [], "entities": []}, {"text": "We apply machine learning, in the form of a modified perceptron algorithm, to learn parameters of a linear model that constructs a matrix of weights from each source language sentence.", "labels": [], "entities": []}, {"text": "We train the parameters on orderings derived from automatic word alignments of parallel sentences.", "labels": [], "entities": [{"text": "word alignments of parallel sentences", "start_pos": 60, "end_pos": 97, "type": "TASK", "confidence": 0.8059797883033752}]}, {"text": "The LOP model of reordering is a complete ordering model, capable of assigning a different score to every possible permutation of the sourcelanguage sentence.", "labels": [], "entities": []}, {"text": "Unlike the target language model, it uses information about the relative positions of the words in the source language, as well as the source words themselves and their parts of speech and contexts.", "labels": [], "entities": []}, {"text": "It is therefore a language-pair specific model.", "labels": [], "entities": []}, {"text": "We apply the learned LOP model as a preprocessing step before both training and evaluation of a phrase-based translation system, namely Moses.", "labels": [], "entities": [{"text": "phrase-based translation", "start_pos": 96, "end_pos": 120, "type": "TASK", "confidence": 0.6368441879749298}]}, {"text": "Our methods for finding a good reordering under the NP-hard LOP are themselves of interest, adapting algorithms from natural language parsing and developing novel dynamic programs.", "labels": [], "entities": [{"text": "NP-hard LOP", "start_pos": 52, "end_pos": 63, "type": "DATASET", "confidence": 0.7842311561107635}]}, {"text": "Our results demonstrate a significant improvement over translation using unreordered German.", "labels": [], "entities": [{"text": "translation", "start_pos": 55, "end_pos": 66, "type": "TASK", "confidence": 0.9663600325584412}]}, {"text": "Using Moses with only distance-based reordering and a distortion limit of 6, our preprocessing improves BLEU from 25.27 to 26.40.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 104, "end_pos": 108, "type": "METRIC", "confidence": 0.999618411064148}]}, {"text": "Furthermore, that improvement is significantly greater than the improvement Moses achieves with its lexicalized reordering model, 25.55.", "labels": [], "entities": []}, {"text": "improved German-English translation using a statistical parser and several hand-written rules for preprocessing the German sentences.", "labels": [], "entities": [{"text": "German-English translation", "start_pos": 9, "end_pos": 35, "type": "TASK", "confidence": 0.45037975907325745}]}, {"text": "This paper presents a similar improvement using fully automatic methods.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 2: Monolingual BLEU score on develop- ment data, measured against the \"true\" German  ordering that was derived from automatic align- ments to known English translations. The table  evaluates three candidate orderings: the original  German, German reordered using the log-odds  initialized model, and German reordered using  the perceptron-learned model. In addition to the  BLEU score, the table shows bigram, trigram, and  4-gram precisions. The unigram precisions are al- ways 100%, because the correct words are given.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 22, "end_pos": 26, "type": "METRIC", "confidence": 0.9344584345817566}, {"text": "BLEU", "start_pos": 381, "end_pos": 385, "type": "METRIC", "confidence": 0.9982380867004395}]}, {"text": " Table 3: Machine translation performance of several systems, measured against a single English refer- ence translation. The results vary both the preprocessing-either none, or reordered using the learned  Linear Ordering Problems-and the reordering model used in Moses. Performance is measured using  BLEU, METEOR (Lavie et al., 2004), and TER (Snover et al., 2006). (For TER, smaller values are  better.)", "labels": [], "entities": [{"text": "Machine translation", "start_pos": 10, "end_pos": 29, "type": "TASK", "confidence": 0.7612347900867462}, {"text": "BLEU", "start_pos": 302, "end_pos": 306, "type": "METRIC", "confidence": 0.9989137649536133}, {"text": "METEOR", "start_pos": 308, "end_pos": 314, "type": "METRIC", "confidence": 0.9827829599380493}, {"text": "TER", "start_pos": 341, "end_pos": 344, "type": "METRIC", "confidence": 0.9944151639938354}, {"text": "TER", "start_pos": 373, "end_pos": 376, "type": "METRIC", "confidence": 0.9626733064651489}]}]}