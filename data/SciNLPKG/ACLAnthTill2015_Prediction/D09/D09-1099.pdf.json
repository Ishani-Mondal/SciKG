{"title": [{"text": "Toward Completeness in Concept Extraction and Classification", "labels": [], "entities": [{"text": "Concept Extraction and Classification", "start_pos": 23, "end_pos": 60, "type": "TASK", "confidence": 0.7915671914815903}]}], "abstractContent": [{"text": "Many algorithms extract terms from text together with some kind of taxonomic classification (is-a) link.", "labels": [], "entities": []}, {"text": "However, the general approaches used today, and specifically the methods of evaluating results, exhibit serious shortcomings.", "labels": [], "entities": []}, {"text": "Harvesting without focusing on a specific conceptual area may deliver large numbers of terms, but they are scattered over an immense concept space, making Recall judgments impossible.", "labels": [], "entities": [{"text": "Recall judgments", "start_pos": 155, "end_pos": 171, "type": "TASK", "confidence": 0.8954557776451111}]}, {"text": "Regarding Precision, simply judging the correctness of terms and their individual classification links may provide high scores, but this doesn't help with the eventual assembly of terms into a single coherent taxonomy.", "labels": [], "entities": [{"text": "Precision", "start_pos": 10, "end_pos": 19, "type": "TASK", "confidence": 0.8933958411216736}]}, {"text": "Furthermore, since there is no correct and complete gold standard to measure against, most work invents some ad hoc evaluation measure.", "labels": [], "entities": []}, {"text": "We present an algorithm that is more precise and complete than previous ones for identifying from web text just those concepts 'below' a given seed term.", "labels": [], "entities": []}, {"text": "Comparing the results to WordNet, we find that the algorithm misses terms, but also that it learns many new terms not in WordNet, and that it classifies them in ways acceptable to humans but different from WordNet.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 25, "end_pos": 32, "type": "DATASET", "confidence": 0.9643445014953613}, {"text": "WordNet", "start_pos": 121, "end_pos": 128, "type": "DATASET", "confidence": 0.9563115239143372}, {"text": "WordNet", "start_pos": 206, "end_pos": 213, "type": "DATASET", "confidence": 0.9690203666687012}]}], "introductionContent": [], "datasetContent": [{"text": "The root concepts discussed in this paper are Animals and People, because they head large taxonomic structures that are well-represented in WordNet.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 140, "end_pos": 147, "type": "DATASET", "confidence": 0.9397483468055725}]}, {"text": "Throughout these experiments, we used as the initial SeedTerm2 lions for Animals and Madonna for People (by specifically choosing a proper name for People we force harvesting down to the level of individual instances).", "labels": [], "entities": [{"text": "SeedTerm2 lions", "start_pos": 53, "end_pos": 68, "type": "DATASET", "confidence": 0.8163156807422638}]}, {"text": "To collect data, we submitted the DAP patterns as web queries to Google, retrieved the top 1000 web snippets per query, and kept only the unique ones.", "labels": [], "entities": []}, {"text": "In total, we collected 1.1 GB of snippets for Animals and 1.5 GB for People.", "labels": [], "entities": []}, {"text": "The algorithm was allowed to run for 10 iterations.", "labels": [], "entities": []}, {"text": "The algorithm learns a staggering variety of terms that is much more diverse than we had anticipated.", "labels": [], "entities": []}, {"text": "In addition to many basic-level concepts or instances, such as dog and Madonna respectively, and many intermediate concepts, such as mammals, pets, and predators, it also harvested categories that clearly seemed useful, such as laboratory animals, forest dwellers, and endangered species.", "labels": [], "entities": []}, {"text": "Many other harvested terms were more difficult to judge, including bait, allergens, seafood, vectors, protein, and pests.", "labels": [], "entities": []}, {"text": "While these terms have an obvious relationship to Animals, we have to determine whether they are legitimate and valuable subconcepts of Animals.", "labels": [], "entities": []}, {"text": "A second issue involves relative terms that are hard to define in an absolute sense, such as native animals and large mammals.", "labels": [], "entities": []}, {"text": "A complete evaluation should answer the following three questions: \u2022 Precision: What is the correctness of the harvested concepts?", "labels": [], "entities": [{"text": "Precision", "start_pos": 69, "end_pos": 78, "type": "METRIC", "confidence": 0.9897741079330444}]}, {"text": "(How many of them are simply wrong, given the root concept?)", "labels": [], "entities": []}, {"text": "\u2022 Recall: What is the coverage of the harvested concepts?", "labels": [], "entities": [{"text": "Recall", "start_pos": 2, "end_pos": 8, "type": "METRIC", "confidence": 0.8335400819778442}]}, {"text": "(How many are missing, below a given root concept?)", "labels": [], "entities": []}, {"text": "\u2022 How correct is the taxonomic structure learned?", "labels": [], "entities": []}, {"text": "Given the number and variety of terms obtained, we initially decided that an automatic evaluation against existing resources (such as WordNet or something similar) would be inadequate because they do not contain many of our harvested terms, even though many of these terms are clearly sensible and potentially valuable.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 134, "end_pos": 141, "type": "DATASET", "confidence": 0.9451090097427368}]}, {"text": "Indeed, the whole point of our work is to learn concepts and taxonomies that go above and beyond what is currently available.", "labels": [], "entities": []}, {"text": "However, it is necessary to compare with something, and it is important not to skirt the issue by conducting evaluations that measure subsets of results, or that perhaps may mislead.", "labels": [], "entities": []}, {"text": "We therefore decided to compare our results against WordNet and to have human annotators judge as many results as we could afford (to obtain a measure of Precision and the legitimate extensions beyond WordNet).", "labels": [], "entities": [{"text": "WordNet", "start_pos": 52, "end_pos": 59, "type": "DATASET", "confidence": 0.9725610017776489}, {"text": "Precision", "start_pos": 154, "end_pos": 163, "type": "METRIC", "confidence": 0.9518132209777832}, {"text": "WordNet", "start_pos": 201, "end_pos": 208, "type": "DATASET", "confidence": 0.9595695734024048}]}, {"text": "Unfortunately, it proved impossible to measure Recall against WordNet, because this requires ascertaining the number of synsets in WordNet between the root and its basic-level categories.", "labels": [], "entities": [{"text": "Recall", "start_pos": 47, "end_pos": 53, "type": "METRIC", "confidence": 0.5246965885162354}, {"text": "WordNet", "start_pos": 62, "end_pos": 69, "type": "DATASET", "confidence": 0.9573893547058105}, {"text": "WordNet", "start_pos": 131, "end_pos": 138, "type": "DATASET", "confidence": 0.9438212513923645}]}, {"text": "This requires human judgment, which we could not afford.", "labels": [], "entities": []}, {"text": "We plan to address this question in future work.", "labels": [], "entities": []}, {"text": "Also, assessing the correctness of the learned taxonomy structure requires the manual assessment of each classification link proposed by the system that is not already in WordNet, a task also beyond our budget to complete in full.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 171, "end_pos": 178, "type": "DATASET", "confidence": 0.9677878618240356}]}, {"text": "Some results-for just basic-level terms and intermediate concepts, but not among intermediate-level concepts-are shown in Section 4.3.", "labels": [], "entities": []}, {"text": "We provide Precision scores using the following measures, where terms refers to the harvested terms: We conducted three sets of experiments.", "labels": [], "entities": [{"text": "Precision scores", "start_pos": 11, "end_pos": 27, "type": "METRIC", "confidence": 0.9088641405105591}]}, {"text": "Experiment 1 evaluates the results of using DAP to learn basic-level concepts for Animals and instances for People.", "labels": [], "entities": []}, {"text": "Experiment 2 evaluates the results of using DAP \u22121 to harvest intermediate concepts between each root concept and its basic-level concepts or instances.", "labels": [], "entities": []}, {"text": "Experiment 3 evaluates the taxonomy structure that is produced via the links between the instances and intermediate concepts.", "labels": [], "entities": []}, {"text": "In this section we discuss the results of harvesting the basic-level Animal concepts and People instances.", "labels": [], "entities": []}, {"text": "The bootstrapping algorithm ranks the harvested terms by their Out-Degree score and considers as correct only those with Out-Degree > 0.", "labels": [], "entities": [{"text": "Out-Degree score", "start_pos": 63, "end_pos": 79, "type": "METRIC", "confidence": 0.9739401638507843}]}, {"text": "In ten iterations, the bootstrapping algorithm produced 913 Animal basic-level concepts and 1, 344 People instances that passed this Out-Degree criterion.", "labels": [], "entities": []}, {"text": "We hired 4 annotators (undergraduates at a different institution) to judge the correctness of the intermediate concepts.", "labels": [], "entities": []}, {"text": "We created detailed annotation guidelines that define 14 annotation labels for each of the Animal and People classes, as shown in  types: Correct, Borderline, BasicConcept, and NotConcept.", "labels": [], "entities": []}, {"text": "The details of our annotation guidelines, the reasons for the intermediate labels, and the annotation study can be found in ().", "labels": [], "entities": []}, {"text": "we chose a random sample of intermediate concepts with frequency over 1, which was given to four human judges for annotation.", "labels": [], "entities": []}, {"text": "summarizes the labels assigned by the four annotators (A 1 -A 4 ).", "labels": [], "entities": []}, {"text": "The top portion of Without the CPT, accuracies range from 53-66% for Animals and 75-85% for People.", "labels": [], "entities": [{"text": "accuracies", "start_pos": 36, "end_pos": 46, "type": "METRIC", "confidence": 0.9971569776535034}]}, {"text": "After applying the CPT, the accuracies increase to 71-84% for animals and 82-94% for people.", "labels": [], "entities": [{"text": "CPT", "start_pos": 19, "end_pos": 22, "type": "METRIC", "confidence": 0.4220643639564514}, {"text": "accuracies", "start_pos": 28, "end_pos": 38, "type": "METRIC", "confidence": 0.9986883997917175}]}, {"text": "These results confirm that the Concept Positioning Test is effective at removing many of the undesirable terms.", "labels": [], "entities": [{"text": "Concept Positioning", "start_pos": 31, "end_pos": 50, "type": "TASK", "confidence": 0.81739541888237}]}, {"text": "Overall, these results demonstrate that our algorithm produced many high-quality intermediate concepts, with good precision.", "labels": [], "entities": [{"text": "precision", "start_pos": 114, "end_pos": 123, "type": "METRIC", "confidence": 0.9977005124092102}]}, {"text": "For example, the Acc1 curves for animals show that nearly 90% of the top 100 intermediate concepts were correct after applying the CPT, whereas only 70% of the top 100 intermediate concepts were correct before.", "labels": [], "entities": [{"text": "Acc1", "start_pos": 17, "end_pos": 21, "type": "METRIC", "confidence": 0.9936936497688293}, {"text": "CPT", "start_pos": 131, "end_pos": 134, "type": "DATASET", "confidence": 0.7172741889953613}]}, {"text": "However, the CPT also eliminates many desirable terms.", "labels": [], "entities": [{"text": "CPT", "start_pos": 13, "end_pos": 16, "type": "DATASET", "confidence": 0.7398751378059387}]}, {"text": "For People, the accuracies are still relatively high even without the CPT, and a much larger set of intermediate concepts is learned.", "labels": [], "entities": [{"text": "accuracies", "start_pos": 16, "end_pos": 26, "type": "METRIC", "confidence": 0.9965529441833496}, {"text": "CPT", "start_pos": 70, "end_pos": 73, "type": "DATASET", "confidence": 0.5327309370040894}]}, {"text": "We also compared the intermediate concepts harvested by the algorithm to the contents of WordNet.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 89, "end_pos": 96, "type": "DATASET", "confidence": 0.9742214679718018}]}, {"text": "The results are shown in.", "labels": [], "entities": []}, {"text": "WordNet contains 20% of the Animal concepts and 51% of the People concepts learned by our algorithm, which confirms that many of these concepts were considered to be valuable taxonomic terms by the WordNet developers.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.9690564274787903}]}, {"text": "However, our human annotators judged 57% of the Animal and 84% of the People concepts to be correct, which suggests that our algorithm generates a substantial number of additional concepts that could be used to enrich taxonomic structure in WordNet.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 241, "end_pos": 248, "type": "DATASET", "confidence": 0.9266413450241089}]}, {"text": "To assess how well our algorithm compares with previous semantic class learning methods, we compared our results to those of (.", "labels": [], "entities": []}, {"text": "Our work was inspired by that approach-in fact, we use that previous algorithm as the first step of our bootstrapping process.", "labels": [], "entities": []}, {"text": "The novelty of our approach is the insertion of an additional bootstrapping stage that iteratively learns new intermediate concepts using DAP \u22121 and the Concept Positioning Test, followed by the subsequent use of the newly learned intermediate concepts in DAP to expand the search space beyond the original root concept.", "labels": [], "entities": []}, {"text": "This leads to the discovery of additional basic-level terms or instances, which are then recycled in turn to discover new intermediate concepts, and soon.", "labels": [], "entities": []}, {"text": "Consequently, we can compare the results produced by the first iteration of our algorithm (before intermediate concepts are learned) to those of ( for the Animal and People categories, and then compare again after 10 bootstrapping iterations of intermediate concept learning.", "labels": [], "entities": []}, {"text": "shows the number of harvested concepts for Animals and People after each bootstrapping iteration.", "labels": [], "entities": []}, {"text": "Bootstrapping with intermediate concepts produces nearly 5 times as many basic-level concepts and instances than ( obtain, while maintaining similar levels of precision.", "labels": [], "entities": [{"text": "precision", "start_pos": 159, "end_pos": 168, "type": "METRIC", "confidence": 0.9974811673164368}]}, {"text": "The intermediate concepts help so much because they steer the learning process into new (yet still correct) regions of the search space after each iteration.", "labels": [], "entities": []}, {"text": "For instance, in the first iteration, the pattern \"animals such as lions and *\" harvests about 350 basiclevel concepts, but only animals that are mentioned in conjunction with lions are learned.", "labels": [], "entities": []}, {"text": "Of these, animals typically quite different from lions, such as grass-eating kudu, are often not discovered.", "labels": [], "entities": []}, {"text": "However, in the second iteration, the intermediate concept Herbivore is chosen for expansion.", "labels": [], "entities": []}, {"text": "The pattern \"herbivore such as antelope and *\" discovers many additional animals, including kudu, that cooccur with antelope but do not co-occur with lions.", "labels": [], "entities": []}, {"text": "shows examples of the 10 top-ranked basic-level concepts and instances that were learned for 3 randomly-selected intermediate Animal and People concepts (IConcepts) that were acquired during bootstrapping.", "labels": [], "entities": []}, {"text": "In the next section, we present an evaluation of the intermediate concept terms.", "labels": [], "entities": []}, {"text": "In this section we discuss the results of harvesting the intermediate-level concepts.", "labels": [], "entities": []}, {"text": "Given the variety of the harvested results, manual judgment of correctness required an in-depth human annotation study.", "labels": [], "entities": []}, {"text": "We also compare our harvested results against the concept terms in WordNet.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 67, "end_pos": 74, "type": "DATASET", "confidence": 0.9762527346611023}]}, {"text": "In this section we evaluate the classification (taxonomy) that is learned by evaluating the links between the intermediate concepts and the basic-level concept/instance terms.", "labels": [], "entities": []}, {"text": "That is, when our algorithm claims that isa(X,Y), how often is X truly a subconcept of Y?", "labels": [], "entities": []}, {"text": "For example, isa(goat, herbivore) would be correct, but isa(goat, bird) would not.", "labels": [], "entities": []}, {"text": "Again, since WordNet does not contain all the harvested concepts, we conduct both a manual evaluation and a comparison against WordNet.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 13, "end_pos": 20, "type": "DATASET", "confidence": 0.9724360704421997}, {"text": "WordNet", "start_pos": 127, "end_pos": 134, "type": "DATASET", "confidence": 0.9811094999313354}]}, {"text": "Creating and evaluating the full taxonomic structure between the root and the basic-level or instance terms is future work.", "labels": [], "entities": []}, {"text": "Here we evaluate simply the accuracy of the taxonomic links between basic-level concepts/instances and intermediate concepts as harvested, but not between intermediate concepts.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 28, "end_pos": 36, "type": "METRIC", "confidence": 0.9992731213569641}]}, {"text": "For each pair, we extracted all harvested links and determined whether the same links appear in WordNet.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 96, "end_pos": 103, "type": "DATASET", "confidence": 0.9885170459747314}]}, {"text": "The links were also given to human judges.", "labels": [], "entities": []}, {"text": "The results show that WordNet lacks nearly half of the taxonomic relations that were generated by the algorithm: 804 Animal and 539 People links.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 22, "end_pos": 29, "type": "DATASET", "confidence": 0.9335053563117981}]}], "tableCaptions": [{"text": " Table 4: Human Intermediate Concept Evaluation.", "labels": [], "entities": [{"text": "Human Intermediate Concept Evaluation", "start_pos": 10, "end_pos": 47, "type": "TASK", "confidence": 0.7979656532406807}]}]}