{"title": [{"text": "On the Role of Lexical Features in Sequence Labeling", "labels": [], "entities": [{"text": "Sequence Labeling", "start_pos": 35, "end_pos": 52, "type": "TASK", "confidence": 0.9375573098659515}]}], "abstractContent": [{"text": "We use the technique of SVM anchoring to demonstrate that lexical features extracted from a training corpus are not necessary to obtain state of the art results on tasks such as Named Entity Recognition and Chunk-ing.", "labels": [], "entities": [{"text": "SVM anchoring", "start_pos": 24, "end_pos": 37, "type": "TASK", "confidence": 0.9533153176307678}, {"text": "Named Entity Recognition", "start_pos": 178, "end_pos": 202, "type": "TASK", "confidence": 0.6373360256354014}]}, {"text": "While standard models require as many as 100K distinct features, we derive models with as little as 1K features that perform as well or better on different domains.", "labels": [], "entities": []}, {"text": "These robust reduced models indicate that the way rare lexical features contribute to classification in NLP is not fully understood.", "labels": [], "entities": []}, {"text": "Contrastive error analysis (with and without lexical features) indicates that lexical features do contribute to resolving some semantic and complex syntactic ambiguities-but we find this contribution does not generalize outside the training corpus.", "labels": [], "entities": []}, {"text": "As a general strategy , we believe lexical features should not be directly derived from a training corpus but instead, carefully inferred and selected from other sources.", "labels": [], "entities": []}], "introductionContent": [{"text": "Common NLP tasks, such as Named Entity Recognition and Chunking, involve the identification of spans of words belonging to the same phrase.", "labels": [], "entities": [{"text": "Named Entity Recognition", "start_pos": 26, "end_pos": 50, "type": "TASK", "confidence": 0.6249065399169922}, {"text": "identification of spans of words belonging to the same phrase", "start_pos": 77, "end_pos": 138, "type": "TASK", "confidence": 0.8026029288768768}]}, {"text": "These tasks are traditionally reduced to a tagging task, in which each word is to be classified as either Beginning a span, Inside a span, or Outside of a span.", "labels": [], "entities": []}, {"text": "The decision is based on the word to be classified and its neighbors.", "labels": [], "entities": []}, {"text": "Features supporting the classification usually include the word forms themselves and properties derived from the word forms, such as prefixes, suffixes, capitalization information, and parts-of-speech.", "labels": [], "entities": []}, {"text": "While early approaches to the NP-chunking task) relied on part-of-speech information alone, it is widely accepted that lexical information (word forms) is crucial for building accurate systems for these tasks.", "labels": [], "entities": []}, {"text": "Indeed, all the better-performing systems in the CoNLL shared tasks competitions for Chunking) and Named Entity Recognition) make extensive use of such lexical information.", "labels": [], "entities": [{"text": "Named Entity Recognition", "start_pos": 99, "end_pos": 123, "type": "TASK", "confidence": 0.5943429370721182}]}, {"text": "In this paper, we show that the influence of lexical features on such sequence labeling tasks is more complex than is generally assumed.", "labels": [], "entities": [{"text": "sequence labeling tasks", "start_pos": 70, "end_pos": 93, "type": "TASK", "confidence": 0.6962428092956543}]}, {"text": "We find that exact word forms aren't necessary for accurate classification.", "labels": [], "entities": [{"text": "accurate classification", "start_pos": 51, "end_pos": 74, "type": "TASK", "confidence": 0.5312843024730682}]}, {"text": "This observation is important because relying on the exact word forms that appear in a training corpus leads to over-fitting, as well as to larger models.", "labels": [], "entities": []}, {"text": "In this work, we focus on learning with Support Vector Machines (SVMs).", "labels": [], "entities": []}, {"text": "SVM classifiers can handle very large feature spaces, and produce state-of-the-art results for NLP applications (see e.g. ().", "labels": [], "entities": [{"text": "SVM classifiers", "start_pos": 0, "end_pos": 15, "type": "TASK", "confidence": 0.8644157946109772}]}, {"text": "Alas, when trained on pruned feature sets, in which rare lexical items are removed, SVM models suffer a loss in classification accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 127, "end_pos": 135, "type": "METRIC", "confidence": 0.9139936566352844}]}, {"text": "It would seem that rare lexical items are indeed crucial for SVM classification performance.", "labels": [], "entities": [{"text": "SVM classification", "start_pos": 61, "end_pos": 79, "type": "TASK", "confidence": 0.9674550294876099}]}, {"text": "However, in, we suggested that the SVM learner is using the rare lexical features for singling out hard cases rather than for learning meaningful generalizations.", "labels": [], "entities": []}, {"text": "We provide further evidence to support this claim in this paper.", "labels": [], "entities": []}, {"text": "We show that by using a variant of SVMAnchored SVM) with a polynomial kernel, one can learn accurate models for English NP-chunking, base-phrase chunking, and Dutch Named Entity Recognition), on a heavily pruned feature space.", "labels": [], "entities": [{"text": "Dutch Named Entity Recognition", "start_pos": 159, "end_pos": 189, "type": "TASK", "confidence": 0.5364266037940979}]}, {"text": "Our models make use of only a fraction of the lexical features available in the training set (less than 1%), and yet provide highly-competitive accuracies.", "labels": [], "entities": []}, {"text": "For the Chunking and NP-Chunking tasks, the most heavily pruned experiments, in which we consider only features appearing at least 100 times in the training corpus, do show a small but significant drop inaccuracy on the testing corpus compared to the non-pruned models exposed to all available features in the training data.", "labels": [], "entities": []}, {"text": "We provide detailed error analysis of a development set in Section 6, revealing the causes for these differences.", "labels": [], "entities": [{"text": "Section 6", "start_pos": 59, "end_pos": 68, "type": "DATASET", "confidence": 0.9014250338077545}]}, {"text": "We suggest one additional binary feature in order to account for some of the performance gap.", "labels": [], "entities": []}, {"text": "Moreover, we show that the differences inaccuracy vanish when the lexicalized and unlexicalized models are tested on text from slightly different sources than the training corpus.", "labels": [], "entities": []}, {"text": "This goes to show that with an appropriate learning method, orthographic and structural (in the form of POS tag sequences) information is sufficient for achieving state-of-the-art performance on these kind of sequence labeling tasks.", "labels": [], "entities": [{"text": "sequence labeling tasks", "start_pos": 209, "end_pos": 232, "type": "TASK", "confidence": 0.7321740388870239}]}, {"text": "This does not mean semantic information is not needed for these tasks.", "labels": [], "entities": []}, {"text": "It does mean that current models capture only a tiny amount of such semantic information through rare lexical features, and in a manner that does not generalize well.", "labels": [], "entities": []}, {"text": "We believe this data motivates a different strategy to incorporate lexical features into classification models: instead of collecting the raw lexical forms appearing in a training corpus, we should attempt to actively construct a feature space including lexical features derived from external sources.", "labels": [], "entities": []}, {"text": "The feature representation of (Collobert and Weston, 2008) could be a step in that direction.", "labels": [], "entities": []}, {"text": "We also believe that hard cases for sequence labeling (POS ambiguity, coordination, long syntactic constructs) could be directly approached with specialized classifiers.", "labels": [], "entities": [{"text": "sequence labeling", "start_pos": 36, "end_pos": 53, "type": "TASK", "confidence": 0.6223822981119156}]}], "datasetContent": [{"text": "How important are the rare lexical features for learning accurate NLP models?", "labels": [], "entities": []}, {"text": "To investigate this question, we experiment with 3 different NLP sequence-labeling tasks.", "labels": [], "entities": []}, {"text": "For each task, we train a sequence of polynomial kernel (d=2) SVM classifiers, using both soft-margin (C=1) and anchored SVM.", "labels": [], "entities": []}, {"text": "Each classifier is trained on a pruned feature set, in which only features appearing at least k times in the training data are kept.", "labels": [], "entities": []}, {"text": "We vary the pruning parameter k.", "labels": [], "entities": []}, {"text": "Pruning is performed overall the features in the model, but lexical features are most affected by it.", "labels": [], "entities": [{"text": "Pruning", "start_pos": 0, "end_pos": 7, "type": "METRIC", "confidence": 0.9032593965530396}]}, {"text": "For all the models, we use the B-I-O representation, and perform multiclass classification using pairwise-voting.", "labels": [], "entities": [{"text": "multiclass classification", "start_pos": 65, "end_pos": 90, "type": "TASK", "confidence": 0.7483750879764557}]}, {"text": "For our features, we consider properties of tokens in a 5-token window centered around the token to be classified, as well as the two previous classifier predictions.", "labels": [], "entities": []}, {"text": "Results are reported as F-measure over labeled identified spans.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 24, "end_pos": 33, "type": "METRIC", "confidence": 0.9980545043945312}]}, {"text": "Polynomial vs. Linear models The polynomial kernel of degree 2 allows us to efficiently and implicitly include in our models all feature pairs.", "labels": [], "entities": []}, {"text": "Syntactic structure information as captured by pairs of POS-tags and Word-POS pairs is certainly important for such syntactic tasks as Chunking and NER, as demonstrated by the many systems described in).", "labels": [], "entities": []}, {"text": "By using the polynomial kernel, we can easily make use of this information without intensive feature-tuning for the most successful feature pairs.", "labels": [], "entities": []}, {"text": "L1-SVM, L2-SVM and the choice of the C parameter Throughout our experiments, we use the \"standard\" variant of SVM, L1-penalty soft margin SVM, as implemented by the TinySVM 1 software package, with the default C value of 1.", "labels": [], "entities": []}, {"text": "This setting is shown to produce good results for sequence labeling tasks in previous work, and is what most end-users of SVM classifiers are likely to use.", "labels": [], "entities": [{"text": "sequence labeling tasks", "start_pos": 50, "end_pos": 73, "type": "TASK", "confidence": 0.7236021359761556}]}, {"text": "As we show in Sect.5.4, fine-tuning the C parameter reaches better accuracy than L1-SVM with C=1.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 67, "end_pos": 75, "type": "METRIC", "confidence": 0.9991769194602966}]}, {"text": "However, as this fine-tuning is computationally expensive, we first report the comparison L1-SVM/C=1 vs. anchored-SVM, which consistently reached the best results, and was the quickest to train.", "labels": [], "entities": []}, {"text": "Feature Pruning vs. Feature Selection Our aim in this set of experiments is not to find the optimal set of lexical features, but rather to demonstrate that most lexical items are not needed for accurate classification in sequence labeling tasks.", "labels": [], "entities": [{"text": "Feature Selection", "start_pos": 20, "end_pos": 37, "type": "TASK", "confidence": 0.6687432080507278}, {"text": "sequence labeling tasks", "start_pos": 221, "end_pos": 244, "type": "TASK", "confidence": 0.6510687669118246}]}, {"text": "To this end, we perform very crude frequency based feature pruning.", "labels": [], "entities": []}, {"text": "We believe better motivated feature selection technique taking into account linguistic (e.g. prune only open-class words) or statistic information could result in slightly more accurate models with even fewer lexical items.", "labels": [], "entities": []}, {"text": "Based on the observations from the error analysis, we performed another pruned-chunking experiment, with the following features: \u2022 Word and POS fora -2,+2 window around the current token, and 2-and-3-letter suffixes of the token to be classified (same as Experiment 2 in Section 5.2 above).", "labels": [], "entities": []}, {"text": "\u2022 Features of words appearing as a preposition (IN) anywhere in the training set are not pruned (this result in a model with 310 unique lexical items after k = 100 pruning).", "labels": [], "entities": []}, {"text": "\u2022 An additional binary feature indicating for each token whether it can function as a PP.", "labels": [], "entities": []}, {"text": "The list of possible-PP forms is generated by considering all tokens seen inside a PP in the training corpus.", "labels": [], "entities": []}, {"text": "It can be easily extended if additional lexicographic resources are available, without retraining the model.", "labels": [], "entities": []}, {"text": "This last proposed feature incorporates important lexical knowledge without relying on features for specific lexical forms, and is more generalizable.", "labels": [], "entities": []}, {"text": "The accuracy of this new model on the development and test set with various pruning thresholds is presented in.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.999161958694458}]}, {"text": "The addition of the CanBePrep feature improves the fully-lexicalized model accuracy on the development set (93.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 75, "end_pos": 83, "type": "METRIC", "confidence": 0.9955209493637085}]}], "tableCaptions": [{"text": " Table 1: Named Entity Identification results (F- score) on dev set, with various pruning thresholds.", "labels": [], "entities": [{"text": "F- score)", "start_pos": 47, "end_pos": 56, "type": "METRIC", "confidence": 0.9777455925941467}]}, {"text": " Table 2: NP-Chunking results (F-score), with var- ious pruning thresholds.", "labels": [], "entities": [{"text": "F-score", "start_pos": 31, "end_pos": 38, "type": "METRIC", "confidence": 0.9970659613609314}]}, {"text": " Table 3: Chunking results (F), with various prun- ing thresholds. Experiment 1. Features: POS,  Word.", "labels": [], "entities": [{"text": "Chunking results (F)", "start_pos": 10, "end_pos": 30, "type": "METRIC", "confidence": 0.5783941984176636}, {"text": "prun- ing thresholds", "start_pos": 45, "end_pos": 65, "type": "METRIC", "confidence": 0.8282166123390198}]}, {"text": " Table 4: Chunking results (F), with various prun- ing thresholds. Experiment 2. Features: POS,  Word, {Suff2, Suff3} of main Word.", "labels": [], "entities": []}, {"text": " Table 5: Chunking results (F), with various prun- ing thresholds. Experiment 3. Features: POS ,  Suff2, Suff3 .", "labels": [], "entities": [{"text": "Chunking results (F)", "start_pos": 10, "end_pos": 30, "type": "METRIC", "confidence": 0.5938900351524353}, {"text": "prun- ing thresholds", "start_pos": 45, "end_pos": 65, "type": "METRIC", "confidence": 0.8540518283843994}]}, {"text": " Table 6: NP-Chunking results (F), with various  pruning thresholds K, for L1 and L2 SVMs with  tuned C values", "labels": [], "entities": [{"text": "F)", "start_pos": 31, "end_pos": 33, "type": "METRIC", "confidence": 0.9599021077156067}]}, {"text": " Table 8: Corpus Variation Text Sources", "labels": [], "entities": []}, {"text": " Table 9: Comparison of Models' performance on  different text genres", "labels": [], "entities": []}]}