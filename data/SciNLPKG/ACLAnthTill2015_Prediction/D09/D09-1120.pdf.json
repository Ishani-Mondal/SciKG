{"title": [{"text": "Simple Coreference Resolution with Rich Syntactic and Semantic Features", "labels": [], "entities": [{"text": "Coreference Resolution", "start_pos": 7, "end_pos": 29, "type": "TASK", "confidence": 0.9230121970176697}]}], "abstractContent": [{"text": "Coreference systems are driven by syntactic, semantic , and discourse constraints.", "labels": [], "entities": []}, {"text": "We present a simple approach which completely modularizes these three aspects.", "labels": [], "entities": []}, {"text": "In contrast to much current work, which focuses on learning and on the discourse component, our system is deterministic and is driven entirely by syntactic and semantic compatibility as learned from a large, unlabeled corpus.", "labels": [], "entities": []}, {"text": "Despite its simplicity and discourse naivete, our system substantially outperforms all unsupervised systems and most supervised ones.", "labels": [], "entities": []}, {"text": "Primary contributions include (1) the presentation of a simple-to-reproduce, high-performing baseline and (2) the demonstration that most remaining errors can be attributed to syntactic and semantic factors external to the coreference phenomenon (and perhaps best addressed by non-coreference systems).", "labels": [], "entities": []}], "introductionContent": [{"text": "The resolution of entity reference is influenced by a variety of constraints.", "labels": [], "entities": [{"text": "resolution of entity reference", "start_pos": 4, "end_pos": 34, "type": "TASK", "confidence": 0.8376569896936417}]}, {"text": "Syntactic constraints like the binding theory, the i-within-i filter, and appositive constructions restrict reference by configuration.", "labels": [], "entities": []}, {"text": "Semantic constraints like selectional compatibility (e.g. a spokesperson can announce things) and subsumption (e.g. Microsoft is a company) rule out many possible referents.", "labels": [], "entities": []}, {"text": "Finally, discourse phenomena such as salience and centering theory are assumed to heavily influence reference preferences.", "labels": [], "entities": []}, {"text": "As these varied factors have given rise to a multitude of weak features, recent work has focused on how best to learn to combine them using models over reference structures.", "labels": [], "entities": []}, {"text": "In this work, we break from the standard view.", "labels": [], "entities": []}, {"text": "Instead, we consider a vastly more modular system in which coreference is predicted from a deterministic function of a few rich features.", "labels": [], "entities": [{"text": "coreference", "start_pos": 59, "end_pos": 70, "type": "TASK", "confidence": 0.9713779091835022}]}, {"text": "In particular, we assume a three-step process.", "labels": [], "entities": []}, {"text": "First, a selfcontained syntactic module carefully represents syntactic structures using an augmented parser and extracts syntactic paths from mentions to potential antecedents.", "labels": [], "entities": []}, {"text": "Some of these paths can be ruled in or out by deterministic but conservative syntactic constraints.", "labels": [], "entities": []}, {"text": "Importantly, the bulk of the work in the syntactic module is in making sure the parses are correctly constructed and used, and this module's most important training data is a treebank.", "labels": [], "entities": []}, {"text": "Second, a self-contained semantic module evaluates the semantic compatibility of headwords and individual names.", "labels": [], "entities": []}, {"text": "These decisions are made from compatibility lists extracted from unlabeled data sources such as newswire and web data.", "labels": [], "entities": []}, {"text": "Finally, of the antecedents which remain after rich syntactic and semantic filtering, reference is chosen to minimize tree distance.", "labels": [], "entities": []}, {"text": "This procedure is trivial where most systems are rich, and so does not need any supervised coreference data.", "labels": [], "entities": []}, {"text": "However, it is rich in important ways which we argue are marginalized in recent coreference work.", "labels": [], "entities": []}, {"text": "Interestingly, error analysis from our final system shows that its failures are far more often due to syntactic failures (e.g. parsing mistakes) and semantic failures (e.g. missing knowledge) than failure to model discourse phenomena or appropriately weigh conflicting evidence.", "labels": [], "entities": []}, {"text": "One contribution of this paper is the exploration of strong modularity, including the result that our system beats all unsupervised systems and approaches the state of the art in supervised ones.", "labels": [], "entities": []}, {"text": "Another contribution is the error analysis result that, even with substantial syntactic and semantic richness, the path to greatest improvement appears to be to further improve the syntactic and semantic modules.", "labels": [], "entities": []}, {"text": "Finally, we offer our approach as a very strong, yet easy to implement, baseline.", "labels": [], "entities": []}, {"text": "We make no claim that learning to reconcile disparate features in a joint model offers no benefit, only that it must not be pursued to the exclusion of rich, nonreference analysis.", "labels": [], "entities": []}], "datasetContent": [{"text": "We will present evaluations on multiple coreference resolution metrics, as no single one is clearly superior: \u2022 Pairwise F1: precision, recall, and F1 overall pairs of mentions in the same entity cluster.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 40, "end_pos": 62, "type": "TASK", "confidence": 0.8860730528831482}, {"text": "F1", "start_pos": 121, "end_pos": 123, "type": "METRIC", "confidence": 0.8109631538391113}, {"text": "precision", "start_pos": 125, "end_pos": 134, "type": "METRIC", "confidence": 0.9988033771514893}, {"text": "recall", "start_pos": 136, "end_pos": 142, "type": "METRIC", "confidence": 0.9980931878089905}, {"text": "F1", "start_pos": 148, "end_pos": 150, "type": "METRIC", "confidence": 0.9972789883613586}]}, {"text": "Note that this over-penalizes the merger or separation of clusters quadratically in the size of the cluster.", "labels": [], "entities": []}, {"text": "\u2022 b 3 (Amit and Baldwin, 1998): For each mention, form the intersection between the predicted cluster and the true cluster for that mention.", "labels": [], "entities": []}, {"text": "The precision is the ratio of the intersection and the true cluster sizes and recall the ratio of the intersection to the predicted sizes; F1 is given by the harmonic mean over precision and recall from all mentions.", "labels": [], "entities": [{"text": "precision", "start_pos": 4, "end_pos": 13, "type": "METRIC", "confidence": 0.9993391633033752}, {"text": "recall", "start_pos": 78, "end_pos": 84, "type": "METRIC", "confidence": 0.9943482279777527}, {"text": "F1", "start_pos": 139, "end_pos": 141, "type": "METRIC", "confidence": 0.9990493655204773}, {"text": "precision", "start_pos": 177, "end_pos": 186, "type": "METRIC", "confidence": 0.9979150891304016}, {"text": "recall", "start_pos": 191, "end_pos": 197, "type": "METRIC", "confidence": 0.9987762570381165}]}, {"text": "\u2022 MUC (: For each true cluster, compute the number of predicted clusters which need to be merged to cover the true cluster.", "labels": [], "entities": [{"text": "MUC", "start_pos": 2, "end_pos": 5, "type": "METRIC", "confidence": 0.9584668278694153}]}, {"text": "Divide this quantity by true cluster size minus one.", "labels": [], "entities": []}, {"text": "Recall is given by the same procedure with predicated and true clusters reversed.", "labels": [], "entities": [{"text": "Recall", "start_pos": 0, "end_pos": 6, "type": "METRIC", "confidence": 0.9182741641998291}]}, {"text": "4 \u2022 CEAF (): For a similarity function between predicted and true clusters, CEAF scores the best match between true and predicted clusters using this function.", "labels": [], "entities": [{"text": "CEAF", "start_pos": 4, "end_pos": 8, "type": "METRIC", "confidence": 0.7243768572807312}, {"text": "CEAF", "start_pos": 76, "end_pos": 80, "type": "METRIC", "confidence": 0.8522217869758606}]}, {"text": "We use the \u03c6 3 similarity function from.", "labels": [], "entities": [{"text": "\u03c6 3 similarity function", "start_pos": 11, "end_pos": 34, "type": "METRIC", "confidence": 0.9062065035104752}]}, {"text": "We present formal experimental results here (see).", "labels": [], "entities": []}, {"text": "We first evaluate our model on the ACE2004-CULOTTA-TEST dataset used in the state-of-the-art systems from and.", "labels": [], "entities": [{"text": "ACE2004-CULOTTA-TEST dataset", "start_pos": 35, "end_pos": 63, "type": "DATASET", "confidence": 0.9767785370349884}]}, {"text": "Both of these systems were supervised systems discriminatively trained to maximize b 3 and used features from many different structured resources including WordNet, as well as domain-specific features ().", "labels": [], "entities": [{"text": "WordNet", "start_pos": 156, "end_pos": 163, "type": "DATASET", "confidence": 0.951123833656311}]}, {"text": "Our best b 3 result of 79.0 is broadly in the range of these results.", "labels": [], "entities": [{"text": "b 3", "start_pos": 9, "end_pos": 12, "type": "METRIC", "confidence": 0.8548649549484253}]}, {"text": "We should note that in our work we use neither the gold mention types (we do not model pre-nominals separately) nor do we use the gold NER tags which does.", "labels": [], "entities": []}, {"text": "Across metrics, the syntactic constraints and semantic compatibility components contribute most to the overall final result.", "labels": [], "entities": []}, {"text": "On the MUC6-TEST dataset, our system outper-: Errors for each type of antecedent decision made by the system.", "labels": [], "entities": [{"text": "MUC6-TEST dataset", "start_pos": 7, "end_pos": 24, "type": "DATASET", "confidence": 0.9605569839477539}, {"text": "Errors", "start_pos": 46, "end_pos": 52, "type": "METRIC", "confidence": 0.9890185594558716}]}, {"text": "Each row is a mention type and the column the predicted mention type antecedent.", "labels": [], "entities": []}, {"text": "The majority of errors are made in the NOMINAL category.", "labels": [], "entities": [{"text": "errors", "start_pos": 16, "end_pos": 22, "type": "METRIC", "confidence": 0.9569861888885498}, {"text": "NOMINAL", "start_pos": 39, "end_pos": 46, "type": "TASK", "confidence": 0.612362265586853}]}, {"text": "forms both (an unsupervised Markov Logic Network system which uses explicit constraints) and Finkel and Manning (2008) (a supervised system which uses ILP inference to reconcile the predictions of a pairwise classifier) on all comparable measures.", "labels": [], "entities": []}, {"text": "11 Similarly, on the ACE2004-NWIRE dataset, we also outperform the state-of-the-art unsupervised system of.", "labels": [], "entities": [{"text": "ACE2004-NWIRE dataset", "start_pos": 21, "end_pos": 42, "type": "DATASET", "confidence": 0.984659343957901}]}, {"text": "Overall, we conclude that our system outperforms state-of-the-art unsupervised systems 12 and is in the range of the state-of-the art systems of and.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Experimental Results (See Section 4): When comparisons between systems are presented, the  largest result is bolded. The CEAF measure has equal values for precision, recall, and F1.", "labels": [], "entities": [{"text": "CEAF measure", "start_pos": 131, "end_pos": 143, "type": "METRIC", "confidence": 0.7820490598678589}, {"text": "precision", "start_pos": 165, "end_pos": 174, "type": "METRIC", "confidence": 0.9997219443321228}, {"text": "recall", "start_pos": 176, "end_pos": 182, "type": "METRIC", "confidence": 0.9996805191040039}, {"text": "F1", "start_pos": 188, "end_pos": 190, "type": "METRIC", "confidence": 0.9995899796485901}]}, {"text": " Table 3: Errors for each type of antecedent deci- sion made by the system. Each row is a mention  type and the column the predicted mention type  antecedent. The majority of errors are made in the  NOMINAL category.", "labels": [], "entities": [{"text": "Errors", "start_pos": 10, "end_pos": 16, "type": "METRIC", "confidence": 0.9858533143997192}, {"text": "NOMINAL category", "start_pos": 199, "end_pos": 215, "type": "DATASET", "confidence": 0.6403968930244446}]}, {"text": " Table 4: Error analysis on ACE2004-CULOTTA-TEST data by mention type. The dominant errors are in  either semantic or syntactic compatibility of mentions rather than discourse phenomena. See Section 5.", "labels": [], "entities": [{"text": "ACE2004-CULOTTA-TEST data", "start_pos": 28, "end_pos": 53, "type": "DATASET", "confidence": 0.9508620798587799}]}]}