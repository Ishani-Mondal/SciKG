{"title": [{"text": "Domain adaptive bootstrapping for named entity recognition", "labels": [], "entities": [{"text": "named entity recognition", "start_pos": 34, "end_pos": 58, "type": "TASK", "confidence": 0.6896381179491679}]}], "abstractContent": [{"text": "Bootstrapping is the process of improving the performance of a trained classifier by iteratively adding data that is labeled by the classifier itself to the training set, and retraining the classifier.", "labels": [], "entities": []}, {"text": "It is often used in situations where labeled training data is scarce but unlabeled data is abundant.", "labels": [], "entities": []}, {"text": "In this paper, we consider the problem of domain adaptation: the situation where training data may not be scarce, but belongs to a different domain from the target application domain.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 42, "end_pos": 59, "type": "TASK", "confidence": 0.7404798418283463}]}, {"text": "As the distribution of un-labeled data is different from the training data, standard bootstrapping often has difficulty selecting informative data to add to the training set.", "labels": [], "entities": []}, {"text": "We propose an effective domain adaptive bootstrapping algorithm that selects unlabeled target domain data that are informative about the target domain and easy to automatically label correctly.", "labels": [], "entities": []}, {"text": "We call these instances bridges, as they are used to bridge the source domain to the target domain.", "labels": [], "entities": []}, {"text": "We show that the method outperforms supervised, transduc-tive and bootstrapping algorithms on the named entity recognition task.", "labels": [], "entities": [{"text": "named entity recognition task", "start_pos": 98, "end_pos": 127, "type": "TASK", "confidence": 0.696959063410759}]}], "introductionContent": [{"text": "Most recent researches on natural language processing (NLP) problems are based on machine learning algorithms.", "labels": [], "entities": [{"text": "natural language processing (NLP)", "start_pos": 26, "end_pos": 59, "type": "TASK", "confidence": 0.7981335520744324}]}, {"text": "High performance can often be achieved if the system is trained and tested on data from the same domain.", "labels": [], "entities": []}, {"text": "However, the performance of NLP systems often degrades badly when the test data is drawn from a source that is different from the labeled data used to train the system.", "labels": [], "entities": []}, {"text": "For named entity recognition (NER), for example, reported that a system trained on a labeled Reuters corpus achieved an F-measure of 91% on a Reuters test set, but only 64% on a Wall Street Journal test set.", "labels": [], "entities": [{"text": "named entity recognition (NER)", "start_pos": 4, "end_pos": 34, "type": "TASK", "confidence": 0.7874685029188792}, {"text": "Reuters corpus", "start_pos": 93, "end_pos": 107, "type": "DATASET", "confidence": 0.8241202235221863}, {"text": "F-measure", "start_pos": 120, "end_pos": 129, "type": "METRIC", "confidence": 0.9993923902511597}, {"text": "Reuters test set", "start_pos": 142, "end_pos": 158, "type": "DATASET", "confidence": 0.9223324259122213}, {"text": "Wall Street Journal test set", "start_pos": 178, "end_pos": 206, "type": "DATASET", "confidence": 0.9608940958976746}]}, {"text": "The task of adapting a system trained on one domain (called the source domain) to anew domain (called the target domain) is called domain adaptation.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 131, "end_pos": 148, "type": "TASK", "confidence": 0.722698450088501}]}, {"text": "In domain adaptation, it is generally assumed that we have labeled data in the source domain while labeled data mayor may not be available in the target domain.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 3, "end_pos": 20, "type": "TASK", "confidence": 0.7573022544384003}]}, {"text": "Previous work in domain adaptation can be classified into two categories:, where a small, labeled target domain data is available, e.g. (, or, where no labeled target domain data is available, e.g. (. In both cases, and especially for, domain adaptation can leverage on large amounts of unlabeled data in the target domain.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 17, "end_pos": 34, "type": "TASK", "confidence": 0.7711232006549835}, {"text": "domain adaptation", "start_pos": 236, "end_pos": 253, "type": "TASK", "confidence": 0.8300226330757141}]}, {"text": "In practice, it is often unreasonable to expect labeled data for every new domain that we come across, such as blogs, emails, a different newspaper agency, or simply articles from a different topic or period in time.", "labels": [], "entities": []}, {"text": "Thus although is easier to handle, [S+T-] is of higher practical importance.", "labels": [], "entities": []}, {"text": "In this paper, we propose a domain adaptive bootstrapping (DAB) approach to tackle the domain adaptation problem under the setting.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 87, "end_pos": 104, "type": "TASK", "confidence": 0.706736370921135}]}, {"text": "Bootstrapping is an iterative process that uses a trained classifier to label and select unlabeled instances to add to the training set for retraining the classifier.", "labels": [], "entities": []}, {"text": "It is often used when labeled training data is scarce but unlabeled data is abundant.", "labels": [], "entities": []}, {"text": "In contrast, for domain adaptation problems, we may have a lot of training data but the target application domain has a different data distribution.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 17, "end_pos": 34, "type": "TASK", "confidence": 0.7460801303386688}]}, {"text": "Standard bootstrapping usually selects instances that are most confidently labeled from the unlabeled data.", "labels": [], "entities": []}, {"text": "In domain adaptation situations, usually the most confidently labeled instances are the ones that are most similar to the source domain in-stances -these instances tend to contain very little information about the target domain.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 3, "end_pos": 20, "type": "TASK", "confidence": 0.714196041226387}]}, {"text": "For domain adaptive bootstrapping, we propose a selection criterion that selects instances that are informative and easy to automatically label correctly.", "labels": [], "entities": []}, {"text": "In addition, we propose a criterion for stopping the process of bootstrapping before it adds uninformative and incorrectly labeled instances that can reduce performance.", "labels": [], "entities": []}, {"text": "Our approach leverages on instances in the target domain called bridges.", "labels": [], "entities": []}, {"text": "These instances contain domain-independent features, as well as features specific to the target domain.", "labels": [], "entities": []}, {"text": "As they contain domain-independent features, they can be classified correctly by classifiers trained on the source domain labeled data.", "labels": [], "entities": []}, {"text": "We argue that these instances act as abridge between the source and the target domain.", "labels": [], "entities": []}, {"text": "We show that, on the NER task, DAB outperforms supervised, transductive and standard bootstrapping algorithms, as well as a bootstrapping variant, called balanced bootstrapping (, that has recently been proposed for domain adaptation.", "labels": [], "entities": [{"text": "NER task", "start_pos": 21, "end_pos": 29, "type": "TASK", "confidence": 0.8609282076358795}, {"text": "domain adaptation", "start_pos": 216, "end_pos": 233, "type": "TASK", "confidence": 0.7241211831569672}]}], "datasetContent": [{"text": "In this paper, we use the annotated data provided by the Automatic Content Extraction (ACE) program.", "labels": [], "entities": [{"text": "Automatic Content Extraction (ACE)", "start_pos": 57, "end_pos": 91, "type": "TASK", "confidence": 0.6859678626060486}]}, {"text": "The ACE data set is annotated for an Entity Detection task, and the annotation consists of the labeling of entity names (e.g. Powell) and mentions for each entity (e.g. pronouns such as he).", "labels": [], "entities": [{"text": "ACE data set", "start_pos": 4, "end_pos": 16, "type": "DATASET", "confidence": 0.959774931271871}, {"text": "Entity Detection task", "start_pos": 37, "end_pos": 58, "type": "TASK", "confidence": 0.8356213370958964}]}, {"text": "In this paper, we are interested in the problem of recognition of the proper names (the named entity recognition task), and hence use only entities labeled with the type NAM (LDC, 2005).", "labels": [], "entities": [{"text": "recognition of the proper names", "start_pos": 51, "end_pos": 82, "type": "TASK", "confidence": 0.8084935426712037}, {"text": "named entity recognition task", "start_pos": 88, "end_pos": 117, "type": "TASK", "confidence": 0.7154055833816528}]}, {"text": "Entities are classified into seven types: Person entities are humans mentioned in a document; Organization entities are limited to established associations of people; Geo-political entities are geographical areas defined by political and/or social groups; Location entities are geographical items like landmasses and bodies of water; Facility entities refer to buildings and real estate improvements; Vehicle entities are devices used for transportation; and Weapon entities are devices used for harming or destruction.", "labels": [], "entities": []}, {"text": "We compare performances of a few algorithms: MaxEnt classifier (MaxEnt); MaxEnt classifier with standard bootstrapping (MaxEnt-SB); balanced bootstrapping based on MaxEnt classifier (MaxEnt-BB); MaxEnt with DAB (MaxEnt-DAB); SVM classifier (SVM); transductive SVM classifier (SVM-Trans); and DAB based on SVM classifier (SVM-DAB).", "labels": [], "entities": []}, {"text": "No regularization is used for MaxEnt classifiers.", "labels": [], "entities": []}, {"text": "SVM classifiers use a value of 10 for parameter C (trade-off between training error and margin).", "labels": [], "entities": [{"text": "SVM classifiers", "start_pos": 0, "end_pos": 15, "type": "TASK", "confidence": 0.8126083314418793}, {"text": "training error", "start_pos": 69, "end_pos": 83, "type": "METRIC", "confidence": 0.9348960816860199}, {"text": "margin", "start_pos": 88, "end_pos": 94, "type": "METRIC", "confidence": 0.9102994799613953}]}, {"text": "Bootstrapping based algorithms are run for 30 iterations and 100 instances are selected in every iteration.", "labels": [], "entities": []}, {"text": "The evaluation measure used is the F-measure.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 35, "end_pos": 44, "type": "METRIC", "confidence": 0.9900417327880859}]}, {"text": "F-measure is the harmonic mean of precision and recall, and is commonly used to evaluate NER systems.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.9757071137428284}, {"text": "precision", "start_pos": 34, "end_pos": 43, "type": "METRIC", "confidence": 0.9992559552192688}, {"text": "recall", "start_pos": 48, "end_pos": 54, "type": "METRIC", "confidence": 0.997298538684845}, {"text": "NER", "start_pos": 89, "end_pos": 92, "type": "TASK", "confidence": 0.9760867953300476}]}, {"text": "We use the scorer for CONLL 2003 shared task where the F-measure is computed by averaging F-measures for name-classes, weighted by the number of oc- currences.", "labels": [], "entities": [{"text": "CONLL 2003 shared task", "start_pos": 22, "end_pos": 44, "type": "DATASET", "confidence": 0.9023478776216507}, {"text": "F-measure", "start_pos": 55, "end_pos": 64, "type": "METRIC", "confidence": 0.9716020226478577}]}], "tableCaptions": [{"text": " Table 2: The topics, their descriptions, and the  number of training and test documents in each  topic.", "labels": [], "entities": []}, {"text": " Table 3: F-measure of supervised learning on the  cross-source target domains.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9969620108604431}]}, {"text": " Table 5: F-measure of the cross-source transfer.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9976869821548462}]}, {"text": " Table 6: F-measure of the cross-topic transfer.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9979658126831055}, {"text": "cross-topic transfer", "start_pos": 27, "end_pos": 47, "type": "TASK", "confidence": 0.744177907705307}]}]}