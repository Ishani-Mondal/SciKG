{"title": [{"text": "Large-Scale Verb Entailment Acquisition from the Web", "labels": [], "entities": [{"text": "Large-Scale Verb Entailment Acquisition", "start_pos": 0, "end_pos": 39, "type": "TASK", "confidence": 0.6725806519389153}]}], "abstractContent": [{"text": "Textual entailment recognition plays a fundamental role in tasks that require in-depth natural language understanding.", "labels": [], "entities": [{"text": "Textual entailment recognition", "start_pos": 0, "end_pos": 30, "type": "TASK", "confidence": 0.8322180509567261}]}, {"text": "In order to use entailment recognition technologies for real-world applications, a large-scale entailment knowledge base is indispensable.", "labels": [], "entities": [{"text": "entailment recognition", "start_pos": 16, "end_pos": 38, "type": "TASK", "confidence": 0.7230915874242783}]}, {"text": "This paper proposes a conditional probability based directional similarity measure to acquire verb entailment pairs on a large scale.", "labels": [], "entities": []}, {"text": "We targeted 52,562 verb types that were derived from 10 8 Japanese Web documents, without regard for whether they were used in daily life or only in specific fields.", "labels": [], "entities": []}, {"text": "In an evaluation of the top 20,000 verb entailment pairs acquired by previous methods and ours, we found that our similarity measure outper-formed the previous ones.", "labels": [], "entities": []}, {"text": "Our method also worked well for the top 100,000 results .", "labels": [], "entities": []}], "introductionContent": [{"text": "We all know that if you snored, you must have been sleeping, that if you are divorced, you must have been married, and that if you won a lawsuit, you must have sued somebody.", "labels": [], "entities": []}, {"text": "These relationships between events where one is the logical consequence of the other are called entailment.", "labels": [], "entities": []}, {"text": "Such knowledge plays a fundamental role in tasks that require in-depth natural language understanding, e.g., answering questions and using natural language interfaces.", "labels": [], "entities": []}, {"text": "This paper proposes a novel method for verb entailment acquisition.", "labels": [], "entities": [{"text": "verb entailment acquisition", "start_pos": 39, "end_pos": 66, "type": "TASK", "confidence": 0.8502951860427856}]}, {"text": "Using a Japanese Web corpus () derived from 10 8 Japanese Web documents, we automatically acquired such verb pairs as snore \u2192 sleep and divorce \u2192 marry, where entailment holds between the verbs in the pair.", "labels": [], "entities": [{"text": "Japanese Web corpus", "start_pos": 8, "end_pos": 27, "type": "DATASET", "confidence": 0.6972107191880544}]}, {"text": "Our definition of \"entailment\" is the same as that in WordNet3.0; v 1 entails v 2 if v 1 cannot be done unless v 2 is, or has been, done.", "labels": [], "entities": [{"text": "WordNet3.0", "start_pos": 54, "end_pos": 64, "type": "DATASET", "confidence": 0.9451274275779724}]}, {"text": "Our method follows the distributional similarity hypothesis, i.e., words that occur in the same context tend to have similar meanings.", "labels": [], "entities": []}, {"text": "Just as in the methods of and, we regard the arguments of verbs as the context in the hypothesis.", "labels": [], "entities": []}, {"text": "However, unlike the previous methods, ours is based on conditional probability and is augmented with a simple trick that improves the accuracy of verb entailment acquisition.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 134, "end_pos": 142, "type": "METRIC", "confidence": 0.9985265731811523}, {"text": "verb entailment acquisition", "start_pos": 146, "end_pos": 173, "type": "TASK", "confidence": 0.7264019151528677}]}, {"text": "In an evaluation of the top 20,000 verb entailment pairs acquired by the previous methods and ours, we found that our similarity measure outperformed the previous ones.", "labels": [], "entities": []}, {"text": "Our method also worked well for the top 100,000 results, Since the scope of Natural Language Processing (NLP) has advanced from a formal writing style to a colloquial style and from restricted to open domains, it is necessary for the language resources for NLP, including verb entailment knowledge bases, to cover abroad range of expressions, regardless of whether they are used in daily life or only in specific fields that are highly technical.", "labels": [], "entities": [{"text": "Natural Language Processing (NLP)", "start_pos": 76, "end_pos": 109, "type": "TASK", "confidence": 0.680959035952886}]}, {"text": "As we will discuss later, our method can acquire, with reasonable accuracy, verb entailment pairs that deal not only with common and familiar verbs but also with technical and unfamiliar ones like podcast \u2192 download and jibe \u2192 sail.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 66, "end_pos": 74, "type": "METRIC", "confidence": 0.9918864369392395}]}, {"text": "Note that previous researches on entailment acquisition focused on templates with variables or word-lattices (; Shinyama 1 Verb entailment pairs are described as v1 \u2192 v2 (v1 is the entailing verb and v2 is the entailed one) henceforth.", "labels": [], "entities": [{"text": "entailment acquisition", "start_pos": 33, "end_pos": 55, "type": "TASK", "confidence": 0.8820291459560394}, {"text": "Shinyama 1 Verb entailment", "start_pos": 112, "end_pos": 138, "type": "DATASET", "confidence": 0.8963529765605927}]}, {"text": "WordNet3.0 provides entailment relationships between synsets like divorce, split up \u2192 marry, get married, wed, conjoin, hookup with, get hitched).", "labels": [], "entities": [{"text": "WordNet3.0", "start_pos": 0, "end_pos": 10, "type": "DATASET", "confidence": 0.968616783618927}]}, {"text": "Certainly these templates or word lattices are more useful in such NLP applications as Q&A than simple entailment relations between verbs.", "labels": [], "entities": []}, {"text": "However, our contention is that entailment certainly holds for some verb pairs (like snore \u2192 sleep) by themselves, and that such pairs constitute the core of a future entailment rule database.", "labels": [], "entities": []}, {"text": "Although we focused on verb entailment, our method can also acquire template-level entailment pairs with a reasonable accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 118, "end_pos": 126, "type": "METRIC", "confidence": 0.995010495185852}]}, {"text": "The rest of this paper is organized as follows.", "labels": [], "entities": []}, {"text": "In \u00a72, related works are described.", "labels": [], "entities": []}, {"text": "\u00a73 presents our proposed method.", "labels": [], "entities": []}, {"text": "After this, an evaluation of our method and the existing methods is presented in Section 4.", "labels": [], "entities": []}, {"text": "Finally, we conclude the paper in \u00a75.", "labels": [], "entities": []}], "datasetContent": [{"text": "We compare the accuracy of our method with that of the alternative methods in \u00a74.1.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 15, "end_pos": 23, "type": "METRIC", "confidence": 0.9996955394744873}]}, {"text": "\u00a74.2 shows the effectiveness of the trick.", "labels": [], "entities": []}, {"text": "We examine the entailment acquisition accuracy for frequent verbs in \u00a74.3, and evaluate the performance of our method when applied to template-level entailment acquisition in \u00a74.4.", "labels": [], "entities": [{"text": "entailment acquisition", "start_pos": 15, "end_pos": 37, "type": "TASK", "confidence": 0.7486212849617004}, {"text": "accuracy", "start_pos": 38, "end_pos": 46, "type": "METRIC", "confidence": 0.6741156578063965}, {"text": "template-level entailment acquisition", "start_pos": 134, "end_pos": 171, "type": "TASK", "confidence": 0.6751864949862162}]}, {"text": "Finally, by showing the accuracy for verb pairs obtained from the top 100,000 results, we claim that our method provides a good starting point from which a large-scale verb entailment resource can be constructed in \u00a74.5.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 24, "end_pos": 32, "type": "METRIC", "confidence": 0.9994006156921387}]}, {"text": "For the evaluation, three human annotators (not the authors) checked whether each acquired entailment pair was correct.", "labels": [], "entities": []}, {"text": "The average of the three Kappa values for each annotator pair was 0.579 for verb entailment pairs and 0.568 for template entailment pairs, both of which indicate the middling stability of this evaluation annotation.", "labels": [], "entities": []}, {"text": "We applied Score, BInc, Lin, and Precision to the template-feature tuples ( \u00a73.3), obtained template entailment pairs, and finally obtained verb entailment pairs by removing the postpositions from the templates as described in \u00a73.", "labels": [], "entities": [{"text": "Score", "start_pos": 11, "end_pos": 16, "type": "METRIC", "confidence": 0.988321840763092}, {"text": "BInc", "start_pos": 18, "end_pos": 22, "type": "METRIC", "confidence": 0.9955631494522095}, {"text": "Lin", "start_pos": 24, "end_pos": 27, "type": "METRIC", "confidence": 0.9023001790046692}, {"text": "Precision", "start_pos": 33, "end_pos": 42, "type": "METRIC", "confidence": 0.9827647805213928}]}, {"text": "As a baseline, we created pairs from randomly chosen verbs.", "labels": [], "entities": []}, {"text": "Since we targeted all of the verbs that appeared on the Web (under the condition of F req(\ud97b\udf59p, v\ud97b\udf59) \u2265 20), the annotators were confronted with technical terms and slang that they did not know.", "labels": [], "entities": [{"text": "F req(\ud97b\udf59p, v\ud97b\udf59)", "start_pos": 84, "end_pos": 97, "type": "METRIC", "confidence": 0.921552164213998}]}, {"text": "In such cases, they consulted dictionaries (either printed or machine readable ones) and the Web.", "labels": [], "entities": []}, {"text": "If they still could not find the meaning of a verb, they labeled the pair containing the unknown verb as incorrect.", "labels": [], "entities": []}, {"text": "We used the accuracy = # of correct pairs # of acquired pairs as an evaluation measure.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 12, "end_pos": 20, "type": "METRIC", "confidence": 0.9996213912963867}]}, {"text": "We regarded a pair as correct if it was judged correct by one (Accuracy-1), two (Accuracy-2), or three (Accuracy-3) annotators.", "labels": [], "entities": [{"text": "Accuracy-1", "start_pos": 63, "end_pos": 73, "type": "METRIC", "confidence": 0.9384355545043945}, {"text": "Accuracy-2", "start_pos": 81, "end_pos": 91, "type": "METRIC", "confidence": 0.950613260269165}]}, {"text": "We evaluated 200 entailment pairs sampled from the top 20,000 for each method (# of acquired pairs = 200).", "labels": [], "entities": []}, {"text": "For fairness, the evaluation samples for each method were shuffled and placed in one file from which the annotators worked.", "labels": [], "entities": []}, {"text": "In this way, they were unable to know which entailment pair came from which method.", "labels": [], "entities": []}, {"text": "Note that the verb entailment pairs produced by Lin do not provide the directionality of entailment.", "labels": [], "entities": []}, {"text": "Thus, the annotators decided the directionality of these entailment pairs as follows: i) Copy 200 original samples and reverse the order of v 1 and v 2 . ii) Shuffle the 400 Lin samples (the original and reversed samples) with the other ones.", "labels": [], "entities": []}, {"text": "iii) Evaluate all of the shuffled pairs.", "labels": [], "entities": []}, {"text": "Each Lin pair was regarded as correct if either direction was judged correct.", "labels": [], "entities": []}, {"text": "In other words, we evaluated the upper bound performance of the LEDIR algorithm.", "labels": [], "entities": []}, {"text": "shows the accuracy of the acquired verb entailment pairs for each method.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9994181394577026}]}, {"text": "shows the accuracy figures for the N-best entailment pairs for each method, with N being 1,000, 2,000, . .", "labels": [], "entities": [{"text": "accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9997336268424988}]}, {"text": "We observed the following points from the results.", "labels": [], "entities": []}, {"text": "First, Score outperformed all the other methods.", "labels": [], "entities": [{"text": "Score", "start_pos": 7, "end_pos": 12, "type": "METRIC", "confidence": 0.9921668767929077}]}, {"text": "Second, Score and Precision, which are directional similarity measures, worked well, while Lin, which is asymmetric one, performed poorly even though the directionality of its output was determined manually.", "labels": [], "entities": [{"text": "Score", "start_pos": 8, "end_pos": 13, "type": "METRIC", "confidence": 0.9982812404632568}, {"text": "Precision", "start_pos": 18, "end_pos": 27, "type": "METRIC", "confidence": 0.9947499632835388}]}, {"text": "Looking at the evaluated samples, Score successfully acquired pairs in which the entailed verbs generalized entailing verbs that were technical terms.", "labels": [], "entities": []}, {"text": "shows examples of Score's outputs.", "labels": [], "entities": [{"text": "Score's outputs", "start_pos": 18, "end_pos": 33, "type": "DATASET", "confidence": 0.9181737303733826}]}, {"text": "(5) suisou-siiku-suru tank-raising-do \"raise (fish) in a tank\" \u2192 siken-houryuu-suru test-discharge-do \"stock (with fish) experimentally\" These terms are related in some sense, but they are not entailment pairs.", "labels": [], "entities": []}, {"text": "Next, we investigated the effectiveness of the trick described in \u00a73.", "labels": [], "entities": []}, {"text": "We evaluated Score, Score trick , and Score base . shows the accuracy figures for each method.", "labels": [], "entities": [{"text": "Score", "start_pos": 13, "end_pos": 18, "type": "METRIC", "confidence": 0.996178150177002}, {"text": "Score trick", "start_pos": 20, "end_pos": 31, "type": "METRIC", "confidence": 0.9355205297470093}, {"text": "Score base", "start_pos": 38, "end_pos": 48, "type": "METRIC", "confidence": 0.9790183007717133}, {"text": "accuracy", "start_pos": 61, "end_pos": 69, "type": "METRIC", "confidence": 0.9996340274810791}]}, {"text": "results illustrate that introducing the trick significantly improved the performance of Score base , and so did multiplying Score trick and Score base , which is our proposal Score.", "labels": [], "entities": [{"text": "multiplying Score trick", "start_pos": 112, "end_pos": 135, "type": "METRIC", "confidence": 0.7414626081784567}]}, {"text": "(6) shows an example of Score base 's errors.", "labels": [], "entities": [{"text": "Score base", "start_pos": 24, "end_pos": 34, "type": "DATASET", "confidence": 0.8476905524730682}]}, {"text": "(6) gazou-sakusei-suru \u2192 henkou-suru image-making-do change-do \"make an image\" \"change\" This pair has only two shared nouns (f \u2208 F l \u2229F r ), and more than 99.99% of the pair's similarity reflects only one of the two.", "labels": [], "entities": []}, {"text": "Clearly, the trick would have prevented the pair from being highly ranked.", "labels": [], "entities": []}, {"text": "We found that the errors made by Lin and BInc in Experiment 1 were mostly pairs of infrequent verbs such as technical terms.", "labels": [], "entities": [{"text": "BInc", "start_pos": 41, "end_pos": 45, "type": "METRIC", "confidence": 0.6620469093322754}]}, {"text": "Thus, we conducted the acquisition of entailment pairs targeting more frequent verbs to see how their performance changed.", "labels": [], "entities": []}, {"text": "The experimental conditions were the same as in Experiment 1, except that the templates (\ud97b\udf59p, v\ud97b\udf59) used were all F req(\ud97b\udf59p, v\ud97b\udf59) \u2265 200.", "labels": [], "entities": [{"text": "F req", "start_pos": 111, "end_pos": 116, "type": "METRIC", "confidence": 0.9518656730651855}]}, {"text": "shows the accuracy figures for each method with the changes inaccuracy from those of the original methods in parentheses.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.999675989151001}]}, {"text": "The re-  sults show that the accuracies of Score and Precision (the two best methods in Experiment 1) degraded, while the other two improved a little.", "labels": [], "entities": [{"text": "re-  sults", "start_pos": 4, "end_pos": 14, "type": "METRIC", "confidence": 0.9121863842010498}, {"text": "accuracies", "start_pos": 29, "end_pos": 39, "type": "METRIC", "confidence": 0.9942122101783752}, {"text": "Score", "start_pos": 43, "end_pos": 48, "type": "METRIC", "confidence": 0.9977546334266663}, {"text": "Precision", "start_pos": 53, "end_pos": 62, "type": "METRIC", "confidence": 0.9909829497337341}]}, {"text": "We suspect that the performance difference between these methods would get smaller if we further restricted the target verbs to more frequent ones.", "labels": [], "entities": []}, {"text": "However, we believe that dealing with verbs comprehensively, including infrequent ones, is important, since, in the era of information explosion, the impact on applications is determined not only by frequent verbs but also infrequent ones that constitute the long tail of a verb-frequency graph.", "labels": [], "entities": []}, {"text": "Thus, this tendency does not matter for our purpose.", "labels": [], "entities": []}, {"text": "This section presents the entailment acquisition accuracy for template pairs to show that our method can also perform the entailment acquisition of unary templates.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 49, "end_pos": 57, "type": "METRIC", "confidence": 0.8383505940437317}]}, {"text": "We presented pairs of unary templates, obtained by the procedure in \u00a73.4, to the annotators.", "labels": [], "entities": []}, {"text": "In doing so, we restricted the correct entailment pairs to those for which entailment always held regardless of what argument filled the two unary slots, and the two slots had to be filled with the same argument, as exemplified in (2).", "labels": [], "entities": []}, {"text": "We evaluated Score and Precision.", "labels": [], "entities": [{"text": "Score", "start_pos": 13, "end_pos": 18, "type": "METRIC", "confidence": 0.999071478843689}, {"text": "Precision", "start_pos": 23, "end_pos": 32, "type": "METRIC", "confidence": 0.9979521632194519}]}, {"text": "acquisition, the accuracy of both methods dropped by about 10%.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 17, "end_pos": 25, "type": "METRIC", "confidence": 0.999758780002594}]}, {"text": "This was mainly due to the evaluation restriction exemplified in (2) which was not introduced in the previous experiments; the annotators ignored the argument correspondence between the verb pairs in Experiment 1.", "labels": [], "entities": []}, {"text": "Also note that Score outperformed Precision in this experiment, too.", "labels": [], "entities": [{"text": "Score", "start_pos": 15, "end_pos": 20, "type": "METRIC", "confidence": 0.9605254530906677}, {"text": "Precision", "start_pos": 34, "end_pos": 43, "type": "METRIC", "confidence": 0.9385393261909485}]}, {"text": "(7) and (8) are examples of the Scoring Slots template entailment pairs and (9) is that of the Nom Slots acquired by our method.", "labels": [], "entities": []}, {"text": "Finally, we examined the accuracy of the top 100,000 verb pairs acquired by Score and Precision.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 25, "end_pos": 33, "type": "METRIC", "confidence": 0.999281108379364}]}, {"text": "As shows, Score outperformed PreMethod Acc-1 Acc-2 Acc-3 Score 0.610 0.480 0.300 Precision 0.470 0.295 0.190: Accuracy of the top 100,000 verb pairs. cision.", "labels": [], "entities": [{"text": "PreMethod Acc-1 Acc-2 Acc-3 Score", "start_pos": 29, "end_pos": 62, "type": "METRIC", "confidence": 0.6544906735420227}, {"text": "Accuracy", "start_pos": 110, "end_pos": 118, "type": "METRIC", "confidence": 0.9972220659255981}]}, {"text": "Note also that Score kept a reasonable accuracy for the top 100,000 results (Acc-2: 48%).", "labels": [], "entities": [{"text": "Score", "start_pos": 15, "end_pos": 20, "type": "METRIC", "confidence": 0.8438736796379089}, {"text": "accuracy", "start_pos": 39, "end_pos": 47, "type": "METRIC", "confidence": 0.999045193195343}, {"text": "Acc-2", "start_pos": 77, "end_pos": 82, "type": "METRIC", "confidence": 0.9979643821716309}]}, {"text": "The accuracy is encouraging enough to consider human annotation for the top 100,000 results to produce a language resource for verb entailment, which we actually plan to do.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.9991933703422546}, {"text": "verb entailment", "start_pos": 127, "end_pos": 142, "type": "TASK", "confidence": 0.7651164829730988}]}, {"text": "Below are correct verb entailment examples from the top 100,000 results of our method.", "labels": [], "entities": []}, {"text": "not.take.part-do take.part-do \"not take part\" \"take part\"", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Accuracy of verb entailment pairs.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9884709119796753}]}, {"text": " Table 2: Effectiveness of the trick.", "labels": [], "entities": [{"text": "Effectiveness", "start_pos": 10, "end_pos": 23, "type": "METRIC", "confidence": 0.9895426034927368}]}, {"text": " Table 3: Accuracy of frequent verb pairs.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9937818646430969}]}, {"text": " Table 4: Accuracy of entailment pairs of templates  whose slots were used for scoring.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9889792203903198}]}]}