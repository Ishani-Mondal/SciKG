{"title": [{"text": "Human-competitive tagging using automatic keyphrase extraction", "labels": [], "entities": [{"text": "Human-competitive tagging", "start_pos": 0, "end_pos": 25, "type": "TASK", "confidence": 0.6772717982530594}]}], "abstractContent": [{"text": "This paper connects two research areas: automatic tagging on the web and statistical key-phrase extraction.", "labels": [], "entities": [{"text": "automatic tagging on the web", "start_pos": 40, "end_pos": 68, "type": "TASK", "confidence": 0.8096832871437073}, {"text": "statistical key-phrase extraction", "start_pos": 73, "end_pos": 106, "type": "TASK", "confidence": 0.7709798216819763}]}, {"text": "First, we analyze the quality of tags in a collaboratively created folksonomy using traditional evaluation techniques.", "labels": [], "entities": []}, {"text": "Next, we demonstrate how documents can be tagged automatically with a state-of-the-art keyphrase extraction algorithm, and further improve performance in this new domain using anew algorithm , \"Maui\", that utilizes semantic information extracted from Wikipedia.", "labels": [], "entities": [{"text": "keyphrase extraction", "start_pos": 87, "end_pos": 107, "type": "TASK", "confidence": 0.7624308168888092}]}, {"text": "Maui out-performs existing approaches and extracts tags that are competitive with those assigned by the best performing human taggers.", "labels": [], "entities": []}], "introductionContent": [{"text": "Tagging is the process of labeling web resources based on their content.", "labels": [], "entities": []}, {"text": "Each label, or tag, corresponds to a topic in a given document.", "labels": [], "entities": []}, {"text": "Unlike metadata assigned by authors, or by professional indexers in libraries, tags are assigned by endusers for organizing and sharing information that is of interest to them.", "labels": [], "entities": []}, {"text": "The organic system of tags assigned by all users of a given web platform is called a folksonomy.", "labels": [], "entities": []}, {"text": "In contrast to traditional taxonomies painstakingly constructed by experts, a user can add any tags to a folksonomy.", "labels": [], "entities": []}, {"text": "This leads to the greatest downside of tagging, inconsistency, which originates in the synonymy and polysemy of human language, as well as in the varying degrees of specificity used by taggers).", "labels": [], "entities": [{"text": "tagging", "start_pos": 39, "end_pos": 46, "type": "TASK", "confidence": 0.9663711786270142}]}, {"text": "In traditional libraries, consistency is the primary evaluation criterion of indexing.", "labels": [], "entities": [{"text": "consistency", "start_pos": 26, "end_pos": 37, "type": "METRIC", "confidence": 0.9891989827156067}]}, {"text": "Much work has been done on describing the statistical properties of folksonomies, such as tag distribution and co-occurrences, but to our knowledge there has been none on assessing the actual quality of tags.", "labels": [], "entities": []}, {"text": "How well do human taggers perform?", "labels": [], "entities": [{"text": "human taggers", "start_pos": 12, "end_pos": 25, "type": "TASK", "confidence": 0.795268326997757}]}, {"text": "How consistent are they with each other?", "labels": [], "entities": []}, {"text": "One potential solution to inconsistency in folksonomies is to use suggestion tools that automatically compute tags for new documents (e.g..", "labels": [], "entities": []}, {"text": "Interestingly, the blooming research on automatic tagging has so far not been connected to work on keyphrase extraction (e.g., which can be used as a tool for the same task (note: we use tag and keyphrase as synonyms).", "labels": [], "entities": [{"text": "keyphrase extraction", "start_pos": 99, "end_pos": 119, "type": "TASK", "confidence": 0.7414775937795639}]}, {"text": "Instead of simple heuristics based on term frequencies and co-occurrence of tags, keyphrase extraction methods apply machine learning to determine typical distributions of properties common to manually assigned phrases, and can include analysis of semantic relations between candidate tags.", "labels": [], "entities": [{"text": "keyphrase extraction", "start_pos": 82, "end_pos": 102, "type": "TASK", "confidence": 0.8743701577186584}]}, {"text": "How well do state-of-the-art keyphrase extraction systems perform compared to simple tagging techniques?", "labels": [], "entities": [{"text": "keyphrase extraction", "start_pos": 29, "end_pos": 49, "type": "TASK", "confidence": 0.8570336401462555}]}, {"text": "How consistent are they with human taggers?", "labels": [], "entities": []}, {"text": "These are questions we address in this paper.", "labels": [], "entities": []}, {"text": "Until now, keyphrase extraction methods have primarily been evaluated using a single set of keyphrases for each document, thereby largely ignoring the subjective nature of the task.", "labels": [], "entities": [{"text": "keyphrase extraction", "start_pos": 11, "end_pos": 31, "type": "TASK", "confidence": 0.9033160507678986}]}, {"text": "Collaboratively tagged documents, on the other hand, offer multiple tag assignments by independent users, a unique basis for evaluation that we capitalize upon in this paper.", "labels": [], "entities": []}, {"text": "The experiments reported in this paper fill these gaps in the research on automatic tagging and keyphrase extraction.", "labels": [], "entities": [{"text": "automatic tagging", "start_pos": 74, "end_pos": 91, "type": "TASK", "confidence": 0.5885596573352814}, {"text": "keyphrase extraction", "start_pos": 96, "end_pos": 116, "type": "TASK", "confidence": 0.8455439209938049}]}, {"text": "First, we analyze tagging consistency on the CiteULike.org platform for organizing academic citations.", "labels": [], "entities": [{"text": "organizing academic citations", "start_pos": 72, "end_pos": 101, "type": "TASK", "confidence": 0.8504612644513448}]}, {"text": "Methods traditionally used for the evaluation of professional indexing will provide insight into the quality of this folksonomy.", "labels": [], "entities": []}, {"text": "Next, we extract a high quality corpus from CiteULike, containing documents that have been tagged consistently by the best human taggers.", "labels": [], "entities": []}, {"text": "Following that, our goal is to build a system that matches the performance of these taggers.", "labels": [], "entities": []}, {"text": "We first apply an existing approach proposed by and compare it to the keyphrase extraction algorithm).", "labels": [], "entities": [{"text": "keyphrase extraction", "start_pos": 70, "end_pos": 90, "type": "TASK", "confidence": 0.8123361766338348}]}, {"text": "Next we create anew algorithm, called Maui, that enhances Kea's successful machine learning framework with semantic knowledge retrieved from Wikipedia, new features, and anew classification model.", "labels": [], "entities": [{"text": "Maui", "start_pos": 38, "end_pos": 42, "type": "DATASET", "confidence": 0.8626635670661926}]}, {"text": "We evaluate Maui using tag sets assigned to the same documents by several users and show that it is as consistent with CiteULike users as they are with each other.", "labels": [], "entities": []}, {"text": "Most of the computation required for automatic tagging with this method can be performed offline.", "labels": [], "entities": [{"text": "automatic tagging", "start_pos": 37, "end_pos": 54, "type": "TASK", "confidence": 0.5003663599491119}]}, {"text": "In practice, it can be used as a tag suggestion tool that provides users with tags describing the main topics of newly added documents, which can then be corrected or enhanced by personal tags if required.", "labels": [], "entities": []}, {"text": "This will improve consistency in the folksonomy without compromising its flexibility.", "labels": [], "entities": [{"text": "consistency", "start_pos": 18, "end_pos": 29, "type": "METRIC", "confidence": 0.9879305362701416}]}], "datasetContent": [{"text": "Here we describe the data used in the experiments and the results obtained, addressing the following questions: 1.", "labels": [], "entities": []}, {"text": "How does a state-of-the-art keyphrase extraction method perform on collaboratively tagged data, compared to a baseline automatic tagging method?", "labels": [], "entities": [{"text": "keyphrase extraction", "start_pos": 28, "end_pos": 48, "type": "TASK", "confidence": 0.8239277899265289}]}, {"text": "2. What is the performance of Maui with old and new features?", "labels": [], "entities": []}, {"text": "3. How consistent are Maui's tags compared to those assigned by human taggers?", "labels": [], "entities": []}, {"text": "The evaluation was performed using a set of 180 documents, described in Section 3.1, each tagged with at least three tags on which two users have agreed.", "labels": [], "entities": []}, {"text": "In the following, unless explicitly stated otherwise, these are the only tags we use.", "labels": [], "entities": []}, {"text": "We consider them to be ground truth.", "labels": [], "entities": []}, {"text": "There are on average five such tags per document, and our goal is to extract tag sets that contain them all.", "labels": [], "entities": []}, {"text": "We regard a predicted tag as \"correct\" if it matches one of the ground truth tags after using the Porter stemmer.", "labels": [], "entities": [{"text": "Porter stemmer", "start_pos": 98, "end_pos": 112, "type": "DATASET", "confidence": 0.7740223705768585}]}, {"text": "We measure performance by computing Precision (the percentage of correct extracted tags out of all extracted), Recall (the percentage of correct extracted tags out of all correct) and F-Measure (the harmonic mean of the two).", "labels": [], "entities": [{"text": "Precision", "start_pos": 36, "end_pos": 45, "type": "METRIC", "confidence": 0.995298445224762}, {"text": "Recall", "start_pos": 111, "end_pos": 117, "type": "METRIC", "confidence": 0.9984793066978455}, {"text": "F-Measure", "start_pos": 184, "end_pos": 193, "type": "METRIC", "confidence": 0.9972225427627563}]}, {"text": "Given the set {yeast (4), network (3), regulation (2), metabolic (2)} of ground truth tags, where the numbers in parenthesis show how many users have assigned each one, and the set {network, metabolic, regulatory, ChIP-chip, transcription} of predicted tags, three out of five predicted terms are correct, yielding a precision of 60%, and three out of four ground-truth terms are extracted, a recall of 75%.", "labels": [], "entities": [{"text": "precision", "start_pos": 317, "end_pos": 326, "type": "METRIC", "confidence": 0.9974228143692017}, {"text": "recall", "start_pos": 393, "end_pos": 399, "type": "METRIC", "confidence": 0.9990710020065308}]}, {"text": "The F-measure combining the two values is 67%.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 4, "end_pos": 13, "type": "METRIC", "confidence": 0.9992092847824097}]}, {"text": "The reported precision and recall values are averaged overall test documents.", "labels": [], "entities": [{"text": "precision", "start_pos": 13, "end_pos": 22, "type": "METRIC", "confidence": 0.9928908348083496}, {"text": "recall", "start_pos": 27, "end_pos": 33, "type": "METRIC", "confidence": 0.9989392161369324}]}, {"text": "We use 10-fold cross-validation for evaluation, which allows us to use all 180 documents as test documents without introducing optimistic bias in the performance measures obtained.", "labels": [], "entities": []}, {"text": "The results obtained in Sections 4.2 and 4.3 using this evaluation provide answers to the first two questions above.", "labels": [], "entities": []}, {"text": "To answer the third we compare the indexing consistency of Maui to that of CiteULike users in Section 4.4.", "labels": [], "entities": []}, {"text": "Here, we consider the assigned tag sets individually and compute the consistency of Maui with each tagger as described in Section 3.2.", "labels": [], "entities": [{"text": "consistency", "start_pos": 69, "end_pos": 80, "type": "METRIC", "confidence": 0.9978941082954407}, {"text": "Maui", "start_pos": 84, "end_pos": 88, "type": "DATASET", "confidence": 0.9242503643035889}]}, {"text": "We compare Maui both to all 332 users who tagged these documents, and to the 36 best taggers identified in Section 3.3.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1. Consistency of the most prolific and  most consistent taggers", "labels": [], "entities": [{"text": "taggers", "start_pos": 64, "end_pos": 71, "type": "TASK", "confidence": 0.6249011158943176}]}, {"text": " Table 3. Rows 1 to 3 evaluate the stan- dard features used by", "labels": [], "entities": []}, {"text": " Table 2. Baseline auto-tagging approach vs. Kea", "labels": [], "entities": [{"text": "Kea", "start_pos": 45, "end_pos": 48, "type": "DATASET", "confidence": 0.5788189768791199}]}, {"text": " Table 3. Evaluation of individual features", "labels": [], "entities": []}, {"text": " Table 4. Combining all features in Maui", "labels": [], "entities": [{"text": "Maui", "start_pos": 36, "end_pos": 40, "type": "DATASET", "confidence": 0.5930837392807007}]}, {"text": " Table 5. Evaluation using feature elimination", "labels": [], "entities": [{"text": "feature elimination", "start_pos": 27, "end_pos": 46, "type": "TASK", "confidence": 0.7031318843364716}]}]}