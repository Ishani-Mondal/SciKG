{"title": [{"text": "Investigation of Question Classifier in Question Answering", "labels": [], "entities": [{"text": "Question Classifier in Question Answering", "start_pos": 17, "end_pos": 58, "type": "TASK", "confidence": 0.6176301717758179}]}], "abstractContent": [{"text": "In this paper, we investigate how an accurate question classifier contributes to a question answering system.", "labels": [], "entities": [{"text": "question answering", "start_pos": 83, "end_pos": 101, "type": "TASK", "confidence": 0.8161501884460449}]}, {"text": "We first present a Maximum Entropy (ME) based question classifier which makes use of headword features and their WordNet hy-pernyms.", "labels": [], "entities": []}, {"text": "We show that our question clas-sifier can achieve the state of the art performance in the standard UIUC question dataset.", "labels": [], "entities": [{"text": "UIUC question dataset", "start_pos": 99, "end_pos": 120, "type": "DATASET", "confidence": 0.9631354212760925}]}, {"text": "We then investigate quantitatively the contribution of this question classifier to a feature driven question answering system.", "labels": [], "entities": [{"text": "question answering", "start_pos": 100, "end_pos": 118, "type": "TASK", "confidence": 0.7025466859340668}]}, {"text": "With our accurate question classifier and some standard question answer features , our question answering system performs close to the state of the art using TREC corpus.", "labels": [], "entities": [{"text": "question answering", "start_pos": 87, "end_pos": 105, "type": "TASK", "confidence": 0.8586358428001404}, {"text": "TREC corpus", "start_pos": 158, "end_pos": 169, "type": "DATASET", "confidence": 0.760844349861145}]}], "introductionContent": [{"text": "Question answering has drawn significant attention from the last decade).", "labels": [], "entities": [{"text": "Question answering", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.9185211956501007}]}, {"text": "It attempts to answer the question posed in natural language by providing the answer phrase rather than the whole documents.", "labels": [], "entities": []}, {"text": "An important step in question answering (QA) is to classify the question to the anticipated type of the answer.", "labels": [], "entities": [{"text": "question answering (QA)", "start_pos": 21, "end_pos": 44, "type": "TASK", "confidence": 0.9007524847984314}]}, {"text": "For example, the question of Who discovered x-rays should be classified into the type of human.", "labels": [], "entities": []}, {"text": "This information would narrow down the search space to identify the correct answer string.", "labels": [], "entities": []}, {"text": "In addition, this information can suggest different strategies to search and verify a candidate answer.", "labels": [], "entities": []}, {"text": "In fact, the combination of question classification and the named entity recognition is a key approach in modern question answering systems.", "labels": [], "entities": [{"text": "question classification", "start_pos": 28, "end_pos": 51, "type": "TASK", "confidence": 0.8622036874294281}, {"text": "named entity recognition", "start_pos": 60, "end_pos": 84, "type": "TASK", "confidence": 0.7445935408274332}, {"text": "question answering", "start_pos": 113, "end_pos": 131, "type": "TASK", "confidence": 0.8898414373397827}]}, {"text": "The question classification is by no means trivial: Simply using question wh-words cannot achieve satisfactory results.", "labels": [], "entities": [{"text": "question classification", "start_pos": 4, "end_pos": 27, "type": "TASK", "confidence": 0.7707362174987793}]}, {"text": "The difficulty lies in classifying the what and which type questions.", "labels": [], "entities": []}, {"text": "Considering the example What is the capital of Yugoslavia, it is of location (city) type, while What is the pH scale is of definition type.", "labels": [], "entities": []}, {"text": "As with the previous work of (;, we propose a feature driven statistical question classifier (.", "labels": [], "entities": [{"text": "statistical question classifier", "start_pos": 61, "end_pos": 92, "type": "TASK", "confidence": 0.5918361445267996}]}, {"text": "In particular, we propose headword feature and augment semantic features of such head words using WordNet.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 98, "end_pos": 105, "type": "DATASET", "confidence": 0.9832268357276917}]}, {"text": "In addition, Lesk's word sense disambiguation (WSD) algorithm is adapted and the depth of hypernym feature is optimized.", "labels": [], "entities": [{"text": "word sense disambiguation (WSD)", "start_pos": 20, "end_pos": 51, "type": "TASK", "confidence": 0.7123326361179352}]}, {"text": "With further augment of other standard features such as unigrams, we can obtain accuracy of 89.0% using ME model for 50 fine classes over UIUC dataset.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 80, "end_pos": 88, "type": "METRIC", "confidence": 0.9996435642242432}, {"text": "ME", "start_pos": 104, "end_pos": 106, "type": "METRIC", "confidence": 0.9111824631690979}, {"text": "UIUC dataset", "start_pos": 138, "end_pos": 150, "type": "DATASET", "confidence": 0.9807818233966827}]}, {"text": "In addition to building an accurate question classifier, we investigate the contribution of this question classifier to a feature driven question answering rank model.", "labels": [], "entities": []}, {"text": "It is worth noting that, most of the features we used in question answering rank model, depend on the question type information.", "labels": [], "entities": [{"text": "question answering", "start_pos": 57, "end_pos": 75, "type": "TASK", "confidence": 0.799252837896347}]}, {"text": "For instance, if a question is classified as a type of sport, we then only care about whether there are sport entities existing in the candidate sentences.", "labels": [], "entities": []}, {"text": "It is expected that a fine grained named entity recognizer (NER) should make good use of the accurate question type information.", "labels": [], "entities": [{"text": "named entity recognizer (NER", "start_pos": 35, "end_pos": 63, "type": "TASK", "confidence": 0.7692827701568603}]}, {"text": "However, due to the lack of a fine grained NER tool at hand, we employ the Stanford NER package () which identifies only four types of named entities.", "labels": [], "entities": [{"text": "Stanford NER package", "start_pos": 75, "end_pos": 95, "type": "DATASET", "confidence": 0.8746011654535929}]}, {"text": "Even with such a coarse named entity recognizer, the experiments show that the question classifier plays an important role in determining the performance of a question answering system.", "labels": [], "entities": [{"text": "question answering system", "start_pos": 159, "end_pos": 184, "type": "TASK", "confidence": 0.7922929326693217}]}, {"text": "The rest of the paper is organized as following.", "labels": [], "entities": []}, {"text": "Section 2 reviews the maximum entropy model which are used in both question classification and question answering ranking.", "labels": [], "entities": [{"text": "question classification", "start_pos": 67, "end_pos": 90, "type": "TASK", "confidence": 0.8688479065895081}, {"text": "question answering ranking", "start_pos": 95, "end_pos": 121, "type": "TASK", "confidence": 0.913175086180369}]}, {"text": "Section 3 presents the features used in question classification.", "labels": [], "entities": [{"text": "question classification", "start_pos": 40, "end_pos": 63, "type": "TASK", "confidence": 0.8505747616291046}]}, {"text": "Section 4 presents the question classification accuracy over UIUC question dataset.", "labels": [], "entities": [{"text": "question classification", "start_pos": 23, "end_pos": 46, "type": "TASK", "confidence": 0.8161944448947906}, {"text": "accuracy", "start_pos": 47, "end_pos": 55, "type": "METRIC", "confidence": 0.9443593621253967}, {"text": "UIUC question dataset", "start_pos": 61, "end_pos": 82, "type": "DATASET", "confidence": 0.9484323461850485}]}, {"text": "Section 5 presents the question answer features.", "labels": [], "entities": []}, {"text": "Section 6 illustrates the results based on TREC question answer dataset.", "labels": [], "entities": [{"text": "TREC question answer dataset", "start_pos": 43, "end_pos": 71, "type": "DATASET", "confidence": 0.7879456430673599}]}, {"text": "And Section 7 draws the conclusion.", "labels": [], "entities": []}], "datasetContent": [{"text": "We train a Maximum Entropy model using the UIUC 5500 training questions and test over the 500 test questions.", "labels": [], "entities": [{"text": "UIUC 5500 training questions", "start_pos": 43, "end_pos": 71, "type": "DATASET", "confidence": 0.9621706604957581}]}, {"text": "Tables 2 shows the accuracy of 6 coarse class and 50 fine grained class, with features being fed incrementally.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 19, "end_pos": 27, "type": "METRIC", "confidence": 0.9996519088745117}]}, {"text": "The question classification performance is measured by accuracy, i.e., the proportion of the correctly classified questions among all test questions.", "labels": [], "entities": [{"text": "question classification", "start_pos": 4, "end_pos": 27, "type": "TASK", "confidence": 0.7651665508747101}, {"text": "accuracy", "start_pos": 55, "end_pos": 63, "type": "METRIC", "confidence": 0.9994307160377502}]}, {"text": "The baseline using the wh-head word results in 46.0% and 46.8% respectively for 6 coarse and 50 fine class classification.", "labels": [], "entities": []}, {"text": "The incremental use of headword boosts the accuracy significantly to 92.2% and 82.0% for 6 and 50 classes.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 43, "end_pos": 51, "type": "METRIC", "confidence": 0.9997226595878601}]}, {"text": "This reflects the informativeness of such feature.", "labels": [], "entities": []}, {"text": "The inclusion of hypernym feature within 6 depths boosts 3.6% for 50 classes, while resulting in slight loss for 6 coarse classes.", "labels": [], "entities": []}, {"text": "The further use of unigram feature leads to 2.8% gain in 50 classes.", "labels": [], "entities": []}, {"text": "Finally, the use of word shape leads to 0.6% accuracy increase for 50 classes.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 45, "end_pos": 53, "type": "METRIC", "confidence": 0.9993533492088318}]}, {"text": "The best accuracies achieved are 93.6% and 89.0% for 6 and 50 classes respectively.", "labels": [], "entities": [{"text": "accuracies", "start_pos": 9, "end_pos": 19, "type": "METRIC", "confidence": 0.9991735816001892}]}, {"text": "The individual feature contributions were discussed in greater detail in.", "labels": [], "entities": []}, {"text": "Also, The SVM (rathern than ME model) was employed using the same feature set and the results were very close (93.4% for 6 class and 89.2% for 50 class).", "labels": [], "entities": [{"text": "ME", "start_pos": 28, "end_pos": 30, "type": "METRIC", "confidence": 0.9046494364738464}]}, {"text": "shows the feature ablation experiment 4 which is missing in that paper.", "labels": [], "entities": []}, {"text": "The experiment shows that the proposed headword and its hypernym features play an essential role in building an accurate question classifier.", "labels": [], "entities": []}, {"text": "Our best result feature space only consists of 13'697 binary features and each question has 10 to 30 active features.", "labels": [], "entities": []}, {"text": "Compared to the over feature size of 200'000 in, our feature space is much more compact, yet turned out to be more informative as suggested by the experiments.", "labels": [], "entities": []}, {"text": "shows the summary of the classification accuracy of all question classifiers which were applied to UIUC dataset.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 40, "end_pos": 48, "type": "METRIC", "confidence": 0.9306882619857788}, {"text": "UIUC dataset", "start_pos": 99, "end_pos": 111, "type": "DATASET", "confidence": 0.9794307351112366}]}, {"text": "Our results are summarized in the last row.", "labels": [], "entities": []}, {"text": "In addition, we have performed the 10 cross validation experiment over the 5500 UIUC training corpus using our best model.", "labels": [], "entities": [{"text": "UIUC training corpus", "start_pos": 80, "end_pos": 100, "type": "DATASET", "confidence": 0.9022221763928732}]}, {"text": "The result is 89.05\u00b11.25 and 83.73\u00b11.61 for 6 and 50 classes, which outperforms the best result of 86.1 \u00b1 1.1 for 6 classes as reported in ().", "labels": [], "entities": []}, {"text": "Recall that most of the question answer features depend on the question classifier.", "labels": [], "entities": []}, {"text": "For instance, the NE feature checks the presence or absence of CoNLL style named entities subject to the classified question type.", "labels": [], "entities": []}, {"text": "In this section, we evaluate how the quality of question classifiers affects the question answering performance.", "labels": [], "entities": [{"text": "question answering", "start_pos": 81, "end_pos": 99, "type": "TASK", "confidence": 0.6836546063423157}]}, {"text": "We use TREC99-03 factoid questions for training and TREC04 factoid questions for testing.", "labels": [], "entities": []}, {"text": "To facilitate the comparison to others work (), we first retrieve all relevant documents which are compiled by Ken Litkowski to create training and test datasets.", "labels": [], "entities": []}, {"text": "We then apply keyword search for each question and retrieve the top 20 relevant sentences.", "labels": [], "entities": []}, {"text": "We create a feature represented data point using each pair of question and candidate sentence and label it either true or false depending on whether the sentence can answer the given question or not.", "labels": [], "entities": []}, {"text": "The labeling is conducted by matching the gold factoid answer pattern against the candidate sentence.", "labels": [], "entities": []}, {"text": "There are two extra steps performed for training set but not for test data.", "labels": [], "entities": []}, {"text": "In order to construct a high quality training set, we manually check the correctness of the training data points and remove the false positive ones which cannot support the question although there is a match to gold answer.", "labels": [], "entities": []}, {"text": "In addition, in order to keep the training data well balanced, we keep maximum four false data points (question answer pair) for each question but no limit over the true label data points.", "labels": [], "entities": []}, {"text": "In doing so, we use 1458 questions to compile 8712 training data points and among them 1752 have true labels.", "labels": [], "entities": []}, {"text": "Similarly, we use 202 questions to compile 4008 test data points and among them 617 have true labels.", "labels": [], "entities": []}, {"text": "We use the training data to train a maximum entropy model and use such model to rank test data set.", "labels": [], "entities": []}, {"text": "Compared with a classification task (such as the question classifier), the ranking process requires one extra step: For data points which share the same question, the probabilities of being predicted as true label are used to rank the data points.", "labels": [], "entities": []}, {"text": "In align with the previous work, performance is evaluated using mean reciprocal rank (MRR), top 1 prediction accuracy (top1) and top 5 prediction accuracy (top5).", "labels": [], "entities": [{"text": "mean reciprocal rank (MRR)", "start_pos": 64, "end_pos": 90, "type": "METRIC", "confidence": 0.9129213988780975}, {"text": "prediction accuracy", "start_pos": 98, "end_pos": 117, "type": "METRIC", "confidence": 0.8542651236057281}, {"text": "prediction accuracy", "start_pos": 135, "end_pos": 154, "type": "METRIC", "confidence": 0.8586553931236267}]}, {"text": "For the test data set, 157 among the 202 questions have correct answers found in retrieved sentences.", "labels": [], "entities": []}, {"text": "This leads to the upper bound of MRR score being 77.8%.", "labels": [], "entities": [{"text": "MRR score", "start_pos": 33, "end_pos": 42, "type": "METRIC", "confidence": 0.9279837310314178}]}, {"text": "To evaluate how the quality of question classifiers affects the question answering, we have created three question classifiers: QC1, QC2 and QC3.", "labels": [], "entities": [{"text": "question answering", "start_pos": 64, "end_pos": 82, "type": "TASK", "confidence": 0.7355527281761169}]}, {"text": "The features which are used to train these question classifiers and their performance are shown in.", "labels": [], "entities": []}, {"text": "Note that QC3 is the best question classifier we obtained in Section 4.", "labels": [], "entities": []}, {"text": "The first experiment is to evaluate the individual contribution of various features derived using three question classifiers.", "labels": [], "entities": []}, {"text": "shows the baseline result and results using DIC, NE, NE-4, REG, SPE, and DEP features.", "labels": [], "entities": [{"text": "REG", "start_pos": 59, "end_pos": 62, "type": "METRIC", "confidence": 0.881584882736206}]}, {"text": "The baseline is the keyword search without the use of maximum entropy model.", "labels": [], "entities": []}, {"text": "As can be seen, the question classifiers do not affect the DIC feature at all, as DIC feature does not depend on question classifiers.", "labels": [], "entities": []}, {"text": "Better question classifier boosts considerable gain for NE, NE-4 and REG in their contribution to question answering.", "labels": [], "entities": [{"text": "NE", "start_pos": 56, "end_pos": 58, "type": "DATASET", "confidence": 0.6165060997009277}, {"text": "NE-4", "start_pos": 60, "end_pos": 64, "type": "DATASET", "confidence": 0.7928757667541504}, {"text": "REG", "start_pos": 69, "end_pos": 72, "type": "METRIC", "confidence": 0.9424263834953308}, {"text": "question answering", "start_pos": 98, "end_pos": 116, "type": "TASK", "confidence": 0.9004645943641663}]}, {"text": "For example, the best question classifier QC3 outperforms the worst one (QC1) by 1.5%, 2.0%, and 2.0% MRR scores for NE, NE-4 and REG respectively.", "labels": [], "entities": [{"text": "MRR", "start_pos": 102, "end_pos": 105, "type": "METRIC", "confidence": 0.998267412185669}]}, {"text": "However, it is surprising that the MRR and top5 contribution of NE and NE-4 decreases if QC1 is replaced by QC2, although the top1 score results in performance gain slightly.", "labels": [], "entities": [{"text": "MRR", "start_pos": 35, "end_pos": 38, "type": "METRIC", "confidence": 0.9657853841781616}]}, {"text": "This unexpected results can be partially explained as follows.", "labels": [], "entities": []}, {"text": "For some questions, even QC2 produces correct predictions, the errors of NE and NE-4 features may cause over-confident scores for certain candidate sentences.", "labels": [], "entities": []}, {"text": "As SPE and DEP are not directly dependent on question classifier, their individual contribution only changes slightly or remains the same for different question classifiers.", "labels": [], "entities": []}, {"text": "If the best question classifier is used, the most important features are SPE and REG, which can individually boost the MRR score over 54%, while the others result in less significant gains.", "labels": [], "entities": [{"text": "SPE", "start_pos": 73, "end_pos": 76, "type": "METRIC", "confidence": 0.9973342418670654}, {"text": "REG", "start_pos": 81, "end_pos": 84, "type": "METRIC", "confidence": 0.9963082671165466}, {"text": "MRR score", "start_pos": 119, "end_pos": 128, "type": "METRIC", "confidence": 0.8552888929843903}]}, {"text": "We now incrementally use various features and the results are show in as well.", "labels": [], "entities": []}, {"text": "As can be seen, the more features and the better question classifier are used, the higher performance the ME model has.", "labels": [], "entities": []}, {"text": "The inclusion of REG and SPE results in significant boost for the performance.", "labels": [], "entities": [{"text": "REG", "start_pos": 17, "end_pos": 20, "type": "METRIC", "confidence": 0.9875907897949219}, {"text": "SPE", "start_pos": 25, "end_pos": 28, "type": "METRIC", "confidence": 0.9068986773490906}]}, {"text": "For example, if the best question classifier QC3 is used, the REG results in 6.9% and 8% gain for MRR and top1 scores respectively.", "labels": [], "entities": [{"text": "REG", "start_pos": 62, "end_pos": 65, "type": "METRIC", "confidence": 0.9961885809898376}, {"text": "MRR", "start_pos": 98, "end_pos": 101, "type": "METRIC", "confidence": 0.9090030193328857}]}, {"text": "This is due to a large portion of NUM type questions in test dataset.", "labels": [], "entities": []}, {"text": "The SPE feature contributes significantly to the performance due to its high precision in answering birth/death time/location questions.", "labels": [], "entities": [{"text": "SPE", "start_pos": 4, "end_pos": 7, "type": "TASK", "confidence": 0.5001940131187439}, {"text": "precision", "start_pos": 77, "end_pos": 86, "type": "METRIC", "confidence": 0.9970793724060059}, {"text": "answering birth/death time/location questions", "start_pos": 90, "end_pos": 135, "type": "TASK", "confidence": 0.5680123046040535}]}, {"text": "NE and NE-4 result in reasonable gains while DEP feature contributes little.", "labels": [], "entities": [{"text": "NE", "start_pos": 0, "end_pos": 2, "type": "DATASET", "confidence": 0.8926018476486206}, {"text": "DEP", "start_pos": 45, "end_pos": 48, "type": "METRIC", "confidence": 0.6694466471672058}]}, {"text": "However, this does not mean that DEP is not important, as once the model reaches a high MRR score, it becomes hard to improve.", "labels": [], "entities": [{"text": "DEP", "start_pos": 33, "end_pos": 36, "type": "METRIC", "confidence": 0.6672320365905762}, {"text": "MRR score", "start_pos": 88, "end_pos": 97, "type": "METRIC", "confidence": 0.8600496053695679}]}, {"text": "clearly shows that the question type classifier plays an essential role in a high perfor- for MRR, top1 and top5 scores respectively.", "labels": [], "entities": [{"text": "MRR", "start_pos": 94, "end_pos": 97, "type": "METRIC", "confidence": 0.8119035959243774}]}, {"text": "Even compared to a good question classifier QC2, the gain of using QC3 is still 3.4%, 4.0% and 2.0% for MRR, top1 and top5 scores respectively.", "labels": [], "entities": [{"text": "MRR", "start_pos": 104, "end_pos": 107, "type": "METRIC", "confidence": 0.8154861330986023}]}, {"text": "One can imagine that if a fine grained NER is available (rather than the current four type coarse NER), the potential gain is much significant.", "labels": [], "entities": []}, {"text": "The reason that the question classifier affects the question answering performance is straightforward.", "labels": [], "entities": [{"text": "question answering", "start_pos": 52, "end_pos": 70, "type": "TASK", "confidence": 0.688700720667839}]}, {"text": "As a upstream source, the incorrect classification of question type would confuse the downstream answer search process.", "labels": [], "entities": []}, {"text": "For example, for question What is Rohm and Haas's annual revenue, our best question classifier is able to classify it into the correct type of NUM:money and thus would put $ 4 billion as a candidate answer.", "labels": [], "entities": []}, {"text": "However, the inferior question classifiers misclassify it into HUM:ind type and thereby could not return a correct answer.", "labels": [], "entities": []}, {"text": "shows the individual MRR scores for the 42 questions (among the 202 test questions) which have different predicted question types using QC3 and QC2.", "labels": [], "entities": [{"text": "MRR", "start_pos": 21, "end_pos": 24, "type": "METRIC", "confidence": 0.6786083579063416}, {"text": "QC3", "start_pos": 136, "end_pos": 139, "type": "DATASET", "confidence": 0.7796761393547058}, {"text": "QC2", "start_pos": 144, "end_pos": 147, "type": "DATASET", "confidence": 0.8891136050224304}]}, {"text": "For almost all test questions, the accurate question classifier QC3 achieves higher MRR scores compared to QC2.", "labels": [], "entities": [{"text": "MRR", "start_pos": 84, "end_pos": 87, "type": "METRIC", "confidence": 0.9964905381202698}]}, {"text": "shows performance of various question answer systems including () and this work which were applied to the same training and test datasets.", "labels": [], "entities": []}, {"text": "Among all the systems, our model can achieve the best MRR score of 66.3%, which is close to the state of the art of 67.0%.", "labels": [], "entities": [{"text": "MRR score", "start_pos": 54, "end_pos": 63, "type": "METRIC", "confidence": 0.8929595053195953}]}, {"text": "Considering the question answer features used in this paper are quite standard, the boost is mainly due to our accurate question classifier.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Question classification accuracy using in- cremental feature sets for 6 and 50 classes over  UIUC split.  6 class 50 class  wh-word  46.0  46.8  + head word  92.2  82.0  + hypernym  91.8  85.6  + unigram  93.0  88.4  + word shape  93.6  89.0", "labels": [], "entities": [{"text": "Question classification", "start_pos": 10, "end_pos": 33, "type": "TASK", "confidence": 0.788453221321106}, {"text": "accuracy", "start_pos": 34, "end_pos": 42, "type": "METRIC", "confidence": 0.9524253010749817}, {"text": "UIUC split", "start_pos": 103, "end_pos": 113, "type": "DATASET", "confidence": 0.9711757004261017}]}, {"text": " Table 3: Question classification accuracy by re- moving one feature at a time for 6 and 50 classes  over UIUC split.  6 class 50 class  overall  93.6  89.0  -wh-word  93.6  89.0  -head word  92.8  88.2  -hypernym  90.8  84.2  -unigram  93.6  86.8  -word shape  93.0  88.4", "labels": [], "entities": [{"text": "Question classification", "start_pos": 10, "end_pos": 33, "type": "TASK", "confidence": 0.840803325176239}, {"text": "accuracy", "start_pos": 34, "end_pos": 42, "type": "METRIC", "confidence": 0.9745332598686218}, {"text": "UIUC split", "start_pos": 106, "end_pos": 116, "type": "DATASET", "confidence": 0.9744513034820557}]}, {"text": " Table 4: Accuracy of all question classifiers which  were applied to UIUC dataset.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.993867814540863}, {"text": "UIUC dataset", "start_pos": 70, "end_pos": 82, "type": "DATASET", "confidence": 0.9700652062892914}]}, {"text": " Table 5. Note that QC3 is the best  question classifier we obtained in Section 4.", "labels": [], "entities": []}, {"text": " Table 5: Features used to train and the perfor- mance of three question classifiers.  Name features  6 class 50 class  QC1  wh-word  46.0  46.8  QC2  wh-word+ head  92.2  82.0  QC3  All  93.6  89.0", "labels": [], "entities": [{"text": "QC3  All  93.6  89.0", "start_pos": 178, "end_pos": 198, "type": "DATASET", "confidence": 0.7269322574138641}]}, {"text": " Table 6: Performance of individual and incremental feature sets for three question classifiers.  Individual  Feature  MRR  Top1  Top5  QC1 QC2 QC3 QC1 QC2 QC3 QC1 QC2 QC3  Baseline 49.9 49.9 49.9 40.1 40.1 40.1 59.4 59.4 59.4  DIC  49.5 49.5 49.5 42.6 42.6 42.6 60.4 60.4 60.4  NE  48.5 47.5 50.0 40.6 40.6 42.6 61.9 60.9 63.4  NE-4  49.5 48.5 51.5 41.6 42.1 44.6 62.4 61.9 64.4  REG  52.0 54.0 54.0 44.1 47.0 47.5 64.4 65.3 65.3  SPE  55.0 55.0 55.0 48.5 48.5 48.5 64.4 64.4 64.4  DEP  51.0 51.5 52.0 43.6 44.1 44.6 65.3 65.8 65.8  Incremental  Baseline 49.9 49.9 49.9 40.1 40.1 40.1 59.4 59.4 59.4  +DIC  49.5 49.5 49.5 42.6 42.6 42.6 60.4 60.4 60.4  +NE  50.0 48.5 51.0 43.1 42.1 44.6 62.9 61.4 64.4  +NE-4  51.5 50.0 53.0 44.1 43.6 46.0 63.4 62.9 65.8  +REG  55.0 56.9 59.9 48.0 51.0 54.0 68.3 68.8 71.8  +SPE  60.4 62.4 65.3 55.4 58.4 61.4 70.8 70.8 73.8  +DEP  61.4 62.9 66.3 55.9 58.4 62.4 71.8 71.8 73.8", "labels": [], "entities": [{"text": "MRR  Top1  Top5  QC1 QC2 QC3 QC1 QC2 QC3 QC1 QC2 QC3  Baseline 49.9", "start_pos": 119, "end_pos": 186, "type": "TASK", "confidence": 0.4769706279039383}]}, {"text": " Table 7: Various system performance comparison.", "labels": [], "entities": []}]}