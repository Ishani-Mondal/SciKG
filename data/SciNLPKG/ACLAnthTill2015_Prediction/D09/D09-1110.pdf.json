{"title": [{"text": "A Compact Forest for Scalable Inference over Entailment and Paraphrase Rules", "labels": [], "entities": []}], "abstractContent": [{"text": "A large body of recent research has been investigating the acquisition and application of applied inference knowledge.", "labels": [], "entities": []}, {"text": "Such knowledge maybe typically captured as entailment rules, applied over syntactic representations.", "labels": [], "entities": []}, {"text": "Efficient inference with such knowledge then becomes a fundamental problem.", "labels": [], "entities": []}, {"text": "Starting out from a formalism for entailment-rule application we present a novel packed data-structure and a corresponding algorithm for its scalable implementation.", "labels": [], "entities": []}, {"text": "We proved the validity of the new algorithm and established its efficiency analytically and empirically.", "labels": [], "entities": []}], "introductionContent": [{"text": "Applied semantic inference is concerned with deriving target meanings from texts.", "labels": [], "entities": [{"text": "Applied semantic inference", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.6847681899865469}]}, {"text": "In the textual entailment framework, this is reduced to inferring a textual statement (the hypothesis h) from a source text (t).", "labels": [], "entities": []}, {"text": "Traditional formal semantics approaches perform such inferences over logical forms derived from the text.", "labels": [], "entities": []}, {"text": "By contrast, most practical NLP applications operate over shallower representations such as parse trees, possibly supplemented with limited semantic information about named entities, semantic roles etc.", "labels": [], "entities": []}, {"text": "Most commonly, inference over such representations is made by applying some kind of transformations or substitutions to the tree or graph representing the text.", "labels": [], "entities": []}, {"text": "Such transformations maybe generally viewed as entailment (inference) rules, which capture semantic knowledge about paraphrases, lexical relations such as synonyms and hyponyms, syntactic variations etc.", "labels": [], "entities": []}, {"text": "Such knowledge is either composed manually, e.g. WordNet, or learned automatically.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 49, "end_pos": 56, "type": "DATASET", "confidence": 0.9627724885940552}]}, {"text": "A large body of work has been dedicated to learning paraphrases and entailment rules, e.g. (, identifying appropriate contexts for their application ( and utilizing them for inference (.", "labels": [], "entities": []}, {"text": "Although current available rule bases are still quite noisy and incomplete, the progress made in recent years suggests that they may become increasingly valuable for text understanding applications.", "labels": [], "entities": [{"text": "text understanding", "start_pos": 166, "end_pos": 184, "type": "TASK", "confidence": 0.9137170910835266}]}, {"text": "Overall, applied knowledge-based inference is a prominent line of research gaining much interest, with recent examples including the series of workshops on Knowledge and Reasoning for Answering Questions (KRAQ) 1 and the planned evaluation of knowledge resources in the forthcoming 5 th Recognizing Textual Entailment challenge (RTE-5) . While many applied systems utilize semantic knowledge via such inference rules, their use is typically limited, application-specific, and quite heuristic.", "labels": [], "entities": [{"text": "Knowledge and Reasoning for Answering Questions (KRAQ)", "start_pos": 156, "end_pos": 210, "type": "TASK", "confidence": 0.7047110729747348}, {"text": "Recognizing Textual Entailment challenge (RTE-5)", "start_pos": 287, "end_pos": 335, "type": "TASK", "confidence": 0.6344685682228633}]}, {"text": "Formalizing these practices seems important for applied semantic inference research, analogously to the role of well-formalized models in parsing and machine translation.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 150, "end_pos": 169, "type": "TASK", "confidence": 0.7072574645280838}]}, {"text": "made a step in this direction by introducing a generic formalism for semantic inference over parse trees.", "labels": [], "entities": []}, {"text": "Their formalism uses entailment rules as a unifying representation for various types of inference knowledge, allowing unified inference as well.", "labels": [], "entities": []}, {"text": "In this formalism, rule application has a clear, intuitive interpretation as generating anew sentence parse (a consequent), semantically entailed by the source sentence.", "labels": [], "entities": []}, {"text": "The inferred consequent maybe subject to further rule applications and soon.", "labels": [], "entities": []}, {"text": "In their implementation, each consequent was generated explicitly as a separate tree.", "labels": [], "entities": []}, {"text": "Following this line of work, our long-term research goal is to investigate effective utilization of entailment rules for inference.", "labels": [], "entities": []}, {"text": "While the formalism of Bar-Haim et al. provides a principled framework for modeling such inferences, its implementation using explicit generation of consequents raises severe efficiency issues, since the number of consequents may grow exponentially in the number of rule applications.", "labels": [], "entities": []}, {"text": "Consider, for example, the sentence \"Children are fond of candies.\", and the following entailment rules: 'children\u2192kids', 'candies\u2192sweets', and 'X is fond of Y\u2192X likes Y'.", "labels": [], "entities": []}, {"text": "The number of derivable sentences (including the source sentence) would be 2 3 as each rule can either be applied or not, independently.", "labels": [], "entities": []}, {"text": "Indeed, we found that this exponential explosion leads to poor scalability in practice.", "labels": [], "entities": []}, {"text": "Intuitively, we would like that each rule application would add just the entailed part (e.g. kids) to a packed sentence representation.", "labels": [], "entities": []}, {"text": "Yet, we still want the resulting structure to represent a set of entailed sentences, rather than some mixture of sentence fragments whose semantics is unclear.", "labels": [], "entities": []}, {"text": "As discussed in section 5, previous work proposed only partial solutions to this problem.", "labels": [], "entities": []}, {"text": "In this paper we present a novel data structure, termed compact forest, and a corresponding inference algorithm, which efficiently generate and represent all consequents while preserving the identity of each individual one (section 3).", "labels": [], "entities": []}, {"text": "Our work is inspired by previous work on packed representations in other fields, such as parsing, generation and machine translation (section 5).", "labels": [], "entities": [{"text": "parsing, generation", "start_pos": 89, "end_pos": 108, "type": "TASK", "confidence": 0.7138641774654388}, {"text": "machine translation", "start_pos": 113, "end_pos": 132, "type": "TASK", "confidence": 0.7765024900436401}]}, {"text": "As we follow a well-defined formalism, we could prove that all inference operations of Bar-Haim et al. are equivalently applied over the compact forest.", "labels": [], "entities": []}, {"text": "We compare inference cost over compact forests to explicit consequent generation both theoretically (section 3.4), illustrating an exponentialto-linear complexity ratio, and empirically (section 4), showing improvement by orders of magnitude.", "labels": [], "entities": []}, {"text": "These results suggest that our data-structure and algorithm are both valid and scalable, opening up the possibility to investigate large-scale entailment rule application within a well-formalized framework.", "labels": [], "entities": []}], "datasetContent": [{"text": "This section reports empirical evaluation of the efficiency of compact inference, tested in the recognizing textual entailment setting using the RTE-3 and RTE-4 datasets ().", "labels": [], "entities": [{"text": "RTE-3", "start_pos": 145, "end_pos": 150, "type": "DATASET", "confidence": 0.9072127342224121}, {"text": "RTE-4 datasets", "start_pos": 155, "end_pos": 169, "type": "DATASET", "confidence": 0.8991751372814178}]}, {"text": "These datasets consist of (text, hypothesis) pairs, which need to be classified as entailing/non entailing.", "labels": [], "entities": []}, {"text": "Our first experiment shows, using a small rule set, that compact inference outperforms explicit inference by orders of magnitude (Section 4.1).", "labels": [], "entities": []}, {"text": "The second experiment shows that compact inference scales well to a full-blown RTE setting with several large-scale rule bases, where up to hundreds of rules are applied fora text (Section 4.2).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Compact vs. explicit inference, us- ing generic rules. Results are averaged per text- hypothesis pair.", "labels": [], "entities": []}, {"text": " Table 2: Application of compact inference to the  RTE-3 Dev. and RTE-4 datasets, using all rule  types.", "labels": [], "entities": [{"text": "RTE-3 Dev.", "start_pos": 51, "end_pos": 61, "type": "DATASET", "confidence": 0.885686993598938}, {"text": "RTE-4 datasets", "start_pos": 66, "end_pos": 80, "type": "DATASET", "confidence": 0.8812839984893799}]}, {"text": " Table 3: Inference contribution to RTE perfor- mance. The system was trained on the RTE-3 de- velopment set. * indicates statistically significant  difference (at level p < 0.02, using McNemar's  test).", "labels": [], "entities": [{"text": "RTE", "start_pos": 36, "end_pos": 39, "type": "TASK", "confidence": 0.4696773886680603}, {"text": "RTE-3 de- velopment set", "start_pos": 85, "end_pos": 108, "type": "METRIC", "confidence": 0.5841629981994629}, {"text": "McNemar's  test", "start_pos": 186, "end_pos": 201, "type": "DATASET", "confidence": 0.8236641685167948}]}, {"text": " Table 4: Average number of rule applications per  (t, h) pair, for each rule base. App counts each rule  application, while Rules ignores multiple matches  of the same rule in the same iteration.", "labels": [], "entities": [{"text": "App", "start_pos": 84, "end_pos": 87, "type": "METRIC", "confidence": 0.9965590834617615}]}, {"text": " Table 5: Contribution of various rule bases. Re- sults show accuracy loss on RTE-4, obtained for  removing each rule base (ablation tests).", "labels": [], "entities": [{"text": "Re- sults", "start_pos": 46, "end_pos": 55, "type": "METRIC", "confidence": 0.9462481339772543}, {"text": "accuracy", "start_pos": 61, "end_pos": 69, "type": "METRIC", "confidence": 0.9983270764350891}, {"text": "RTE-4", "start_pos": 78, "end_pos": 83, "type": "METRIC", "confidence": 0.620849072933197}]}]}