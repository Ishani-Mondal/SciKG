{"title": [{"text": "A Joint Language Model With Fine-grain Syntactic Tags", "labels": [], "entities": []}], "abstractContent": [{"text": "We present a scalable joint language model designed to utilize fine-grain syntactic tags.", "labels": [], "entities": []}, {"text": "We discuss challenges such a design faces and describe our solutions that scale well to large tagsets and corpora.", "labels": [], "entities": []}, {"text": "We advocate the use of relatively simple tags that do not require deep linguistic knowledge of the language but provide more structural information than POS tags and can be derived from automatically generated parse trees-a combination of properties that allows easy adoption of this model for new languages.", "labels": [], "entities": []}, {"text": "We propose two fine-grain tagsets and evaluate our model using these tags, as well as POS tags and SuperARV tags in a speech recognition task and discuss future directions .", "labels": [], "entities": [{"text": "speech recognition task", "start_pos": 118, "end_pos": 141, "type": "TASK", "confidence": 0.8192087610562643}]}], "introductionContent": [{"text": "Ina number of language processing tasks, particularly automatic speech recognition (ASR) and machine translation (MT), there is the problem of selecting the best sequence of words from multiple hypotheses.", "labels": [], "entities": [{"text": "automatic speech recognition (ASR)", "start_pos": 54, "end_pos": 88, "type": "TASK", "confidence": 0.8082545797030131}, {"text": "machine translation (MT)", "start_pos": 93, "end_pos": 117, "type": "TASK", "confidence": 0.8358943283557891}]}, {"text": "This problem stems from the noisy channel approach to these applications.", "labels": [], "entities": []}, {"text": "The noisy channel model states that the observed data, e.g., the acoustic signal, is the result of some input translated by some unknown stochastic process.", "labels": [], "entities": []}, {"text": "Then the problem of finding the best sequence of words given the acoustic input, not approachable directly, is transformed into two separate models: (1) where A is the acoustic signal and w n 1 is a sequence of n words.", "labels": [], "entities": []}, {"text": "p(A|w n 1 ) is called an acoustic model and p(w n 1 ) is the language model . Typically, these applications use language models that compute the probability of a sequence in a generative way: Approximation is required to keep the parameter space tractable.", "labels": [], "entities": [{"text": "Approximation", "start_pos": 192, "end_pos": 205, "type": "METRIC", "confidence": 0.9781739115715027}]}, {"text": "Most commonly the context is reduced to just a few immediately preceding words.", "labels": [], "entities": []}, {"text": "This type of model is called an ngram model: Even with limited context, the parameter space can be quite sparse and requires sophisticated techniques for reliable probability estimation.", "labels": [], "entities": []}, {"text": "While the ngram models perform fairly well, they are only capable of capturing very shallow knowledge of the language.", "labels": [], "entities": []}, {"text": "There is extensive literature on a variety of methods that have been used to imbue models with syntactic and semantic information in different ways.", "labels": [], "entities": []}, {"text": "These methods can be broadly categorized into two types: \u2022 The first method uses surface words within its context, sometimes organizing them into deterministic classes.", "labels": [], "entities": []}, {"text": "Models of this type include:, which use semantic word clustering, and (, which uses variablelength context.", "labels": [], "entities": [{"text": "semantic word clustering", "start_pos": 40, "end_pos": 64, "type": "TASK", "confidence": 0.6525188485781351}]}, {"text": "\u2022 The other method adds stochastic variables to express the ambiguous nature of surface words 2 . To obtain the probability of the next Real applications use argmax w n 1 p(A|w n 1 )\u00b7p(w n 1 ) \u03b1 \u00b7n \u03b2 instead of Eq.", "labels": [], "entities": []}, {"text": "1, where \u03b1 and \u03b2 are set to optimize a heldout set.", "labels": [], "entities": []}, {"text": "These variables have to be predicted by the model.", "labels": [], "entities": []}, {"text": "word we need to sum overall assignments of the stochastic variables, as in Eq.", "labels": [], "entities": []}, {"text": "2. Models of this type, which we call joint models since they essentially predict joint events of words and some random variable(s), include) which used POS tags in combination with \"parser instructions\" for constructing a full parse tree in a left-to-right manner; () used SuperARVs (complex tuples of dependency information) without resolving the dependencies, thus called almost parsing;) utilize part of speech (POS) tags.", "labels": [], "entities": []}, {"text": "Note that some models reduce the context by making the following approximation: thus, transforming the problem into a standard HMM application.", "labels": [], "entities": []}, {"text": "However, these models perform poorly and have only been able to improve over the ngram model when interpolated with it.", "labels": [], "entities": []}, {"text": "Although joint models have the potential to better express variability in word usage through the introduction of additional latent variables, they do not necessarily perform better because the increased dimensionality of the context substantially increases the already complex problem of parameter estimation.", "labels": [], "entities": []}, {"text": "The complexity of the space also makes computation of the probability a challenge because of space and time constraints.", "labels": [], "entities": []}, {"text": "This makes the choice of the random variables a matter of utmost importance.", "labels": [], "entities": []}, {"text": "The model presented in this paper has some elements borrowed from prior work, notably), while others are novel.", "labels": [], "entities": []}], "datasetContent": [{"text": "To evaluate the impact of fine-grain tags on language modeling, we trained our model with five settings: In the first model, questions were restricted to be about overt factors only, thus making it a tree-based word model.", "labels": [], "entities": [{"text": "language modeling", "start_pos": 45, "end_pos": 62, "type": "TASK", "confidence": 0.7023301720619202}]}, {"text": "In the second model, we used POS tags.", "labels": [], "entities": []}, {"text": "To evaluate the effect of finegrain tags, we train two models: head and parent described in Section 2.3 and Section 2.4 respectively.", "labels": [], "entities": []}, {"text": "Since our joint model can be used with any kind of tags, we also trained it with Super-ARV tags ().", "labels": [], "entities": []}, {"text": "The SuperARVs were created from the same parse trees that were used to produce POS and fine-grain tags.", "labels": [], "entities": []}, {"text": "All our models, including SuperARV, use trigram context.", "labels": [], "entities": [{"text": "SuperARV", "start_pos": 26, "end_pos": 34, "type": "DATASET", "confidence": 0.9288777709007263}]}, {"text": "We include standard trigram, four-gram, and five-gram models for reference.", "labels": [], "entities": []}, {"text": "The ngram models were trained using SRILM toolkit with interpolated modified Kneser-Ney smoothing.", "labels": [], "entities": []}, {"text": "We evaluate our model with an nbest rescoring task using 100-best lists from the DARPA WSJ'93 and WSJ'92 20k open vocabulary data sets.", "labels": [], "entities": [{"text": "DARPA WSJ'93 and WSJ'92 20k open vocabulary data sets", "start_pos": 81, "end_pos": 134, "type": "DATASET", "confidence": 0.8539911111195883}]}, {"text": "The details on the acoustic model used to produce the nbest lists can be found in ().", "labels": [], "entities": []}, {"text": "All language models were trained on the NYT 1994-1995 section of the English Gigaword corpus (approximately 70M words).", "labels": [], "entities": [{"text": "NYT 1994-1995 section of the English Gigaword corpus", "start_pos": 40, "end_pos": 92, "type": "DATASET", "confidence": 0.8960939422249794}]}, {"text": "Since the New York Times covers a wider range of topics than the Wall Street Journal, we eliminated the most irrelevant stories based on their trigram coverage by sections 00-22 of WSJ.", "labels": [], "entities": [{"text": "Wall Street Journal", "start_pos": 65, "end_pos": 84, "type": "DATASET", "confidence": 0.9317135016123453}, {"text": "WSJ", "start_pos": 181, "end_pos": 184, "type": "DATASET", "confidence": 0.985240638256073}]}, {"text": "We also eliminated sentences over 120 words, because the parser's performance drops significantly on long sentences.", "labels": [], "entities": []}, {"text": "After parsing the corpus, we deleted sentences that were assigned a very low probability by the parser.", "labels": [], "entities": []}, {"text": "Overall we removed only a few percent of the data; however, we believe that such a rigorous approach to data cleaning is important for building discriminating models.", "labels": [], "entities": [{"text": "data cleaning", "start_pos": 104, "end_pos": 117, "type": "TASK", "confidence": 0.7308493852615356}]}, {"text": "Parse trees were produced by an extended version of the Berkeley parser.", "labels": [], "entities": []}, {"text": "We trained the parser on a combination of the BN and WSJ treebanks, preprocessed to make them more consistent with each other.", "labels": [], "entities": [{"text": "BN", "start_pos": 46, "end_pos": 48, "type": "DATASET", "confidence": 0.8892412185668945}, {"text": "WSJ treebanks", "start_pos": 53, "end_pos": 66, "type": "DATASET", "confidence": 0.8401018679141998}]}, {"text": "We also modified the trees for the speech recognition task by replacing numbers and abbreviations with their verbalized forms.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 35, "end_pos": 53, "type": "TASK", "confidence": 0.8377121090888977}]}, {"text": "We pre-processed the NYT corpus in the same way, and parsed it.", "labels": [], "entities": [{"text": "NYT corpus", "start_pos": 21, "end_pos": 31, "type": "DATASET", "confidence": 0.9062941074371338}]}, {"text": "After that, we removed punctuation and downcased words.", "labels": [], "entities": []}, {"text": "For the ngram model, we used text processed in the same way.", "labels": [], "entities": []}, {"text": "In head and parent models, tag vocabularies contain approximately 1,500 tags each, while the SuperARV model has approximately 1,400 distinct SuperARVs, most of which represent verbs.", "labels": [], "entities": []}, {"text": "In these experiments we did not use overt factors other than the surface word because they split We optimized the LM weight and computed WER with scripts in the SRILM and NIST SCTK toolkits.: WER results, optimized on 92et set, evaluated on combined 93et and 93dt set.", "labels": [], "entities": [{"text": "WER", "start_pos": 137, "end_pos": 140, "type": "METRIC", "confidence": 0.8136599659919739}, {"text": "SRILM", "start_pos": 161, "end_pos": 166, "type": "DATASET", "confidence": 0.8930375576019287}, {"text": "NIST SCTK toolkits.", "start_pos": 171, "end_pos": 190, "type": "DATASET", "confidence": 0.8523988525072733}]}, {"text": "The Oracle WER is 9.5%.", "labels": [], "entities": [{"text": "Oracle", "start_pos": 4, "end_pos": 10, "type": "DATASET", "confidence": 0.631996214389801}, {"text": "WER", "start_pos": 11, "end_pos": 14, "type": "METRIC", "confidence": 0.8271855711936951}]}, {"text": "<unk>, effectively changing the vocabulary thus making perplexity incomparable to models without these factors, without improving WER noticeably.", "labels": [], "entities": [{"text": "WER", "start_pos": 130, "end_pos": 133, "type": "METRIC", "confidence": 0.9434692859649658}]}, {"text": "However, we do plan to use more overt factors in Machine Translation experiments where a language model faces a wider range of OOV phenomena, such as abbreviations, foreign words, numbers, dates, time, etc.", "labels": [], "entities": [{"text": "Machine Translation", "start_pos": 49, "end_pos": 68, "type": "TASK", "confidence": 0.8505595624446869}]}, {"text": "summarizes performance of the LMs on the rescoring task.", "labels": [], "entities": []}, {"text": "The parent tags model outperforms the trigram baseline model by 0.8% WER.", "labels": [], "entities": [{"text": "WER", "start_pos": 69, "end_pos": 72, "type": "METRIC", "confidence": 0.9952749013900757}]}, {"text": "Note that four-and five-gram models fail to outperform the trigram baseline.", "labels": [], "entities": []}, {"text": "We believe this is due to the sparsity as well as relatively short sentences in the test set (16 words on average).", "labels": [], "entities": []}, {"text": "Interestingly, whereas the improvement of the POS model over the baseline is not statistically significant (p < 0.10) 9 , the fine-grain models outperform the baseline much more reliably: p < 0.03 (SuperARV) and p < 0.007 (parent).", "labels": [], "entities": []}, {"text": "We present perplexity evaluations in.", "labels": [], "entities": []}, {"text": "The perplexity was computed on Section 23 of WSJ PTB, preprocessed as the rest of the data we used.", "labels": [], "entities": [{"text": "Section 23 of WSJ PTB", "start_pos": 31, "end_pos": 52, "type": "DATASET", "confidence": 0.8792654037475586}]}, {"text": "The head model has the lowest perplexity outperforming the baseline by 9%.", "labels": [], "entities": []}, {"text": "Note, it even outperforms the five-gram model, although by a small 2% margin.", "labels": [], "entities": []}, {"text": "Although the improvements by the fine-grain tagsets over POS are not significant (due to the small size of the test set), the reductions in perplexity suggest that the improvements are not random.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: WER results, optimized on 92et set, eval- uated on combined 93et and 93dt set. The Oracle  WER is 9.5%.", "labels": [], "entities": [{"text": "WER", "start_pos": 101, "end_pos": 104, "type": "METRIC", "confidence": 0.5709354281425476}]}, {"text": " Table 2: Perplexity results on Section 23 WSJ  PTB", "labels": [], "entities": [{"text": "Perplexity", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9401007294654846}, {"text": "Section 23 WSJ  PTB", "start_pos": 32, "end_pos": 51, "type": "DATASET", "confidence": 0.8736490607261658}]}]}