{"title": [], "abstractContent": [{"text": "Cube pruning is a fast inexact method for generating the items of abeam decoder.", "labels": [], "entities": []}, {"text": "In this paper, we show that cube pruning is essentially equivalent to A* search on a specific search space with specific heuris-tics.", "labels": [], "entities": [{"text": "A", "start_pos": 70, "end_pos": 71, "type": "METRIC", "confidence": 0.9428259134292603}]}, {"text": "We use this insight to develop faster and exact variants of cube pruning.", "labels": [], "entities": []}], "introductionContent": [{"text": "In recent years, an intense research focus on machine translation (MT) has raised the quality of MT systems to the degree that they are now viable fora variety of real-world applications.", "labels": [], "entities": [{"text": "machine translation (MT)", "start_pos": 46, "end_pos": 70, "type": "TASK", "confidence": 0.8694698810577393}, {"text": "MT", "start_pos": 97, "end_pos": 99, "type": "TASK", "confidence": 0.9807249307632446}]}, {"text": "Because of this, the research community has turned its attention to a major drawback of such systems: they are still quite slow.", "labels": [], "entities": []}, {"text": "Recent years have seen a flurry of innovative techniques designed to tackle this problem.", "labels": [], "entities": []}, {"text": "These include cube pruning, cube growing (, early pruning (, closing spans, coarse-to-fine methods (, pervasive laziness, and many more.", "labels": [], "entities": [{"text": "cube growing", "start_pos": 28, "end_pos": 40, "type": "TASK", "confidence": 0.8316830694675446}]}, {"text": "This massive interest in speed is bringing rapid progress to the field, but it comes with a certain amount of baggage.", "labels": [], "entities": [{"text": "speed", "start_pos": 25, "end_pos": 30, "type": "METRIC", "confidence": 0.9446802139282227}]}, {"text": "Each technique brings its own terminology (from the cubes of to the lazy lists of) into the mix.", "labels": [], "entities": []}, {"text": "Often, it is not entirely clear why they work.", "labels": [], "entities": []}, {"text": "Many apply only to specialized MT situations.", "labels": [], "entities": [{"text": "MT", "start_pos": 31, "end_pos": 33, "type": "TASK", "confidence": 0.9934331178665161}]}, {"text": "Without a deeper understanding of these methods, it is difficult for the practitioner to combine them and adapt them to new use cases.", "labels": [], "entities": []}, {"text": "In this paper, we attempt to bring some clarity to the situation by taking a closer look atone of these existing methods.", "labels": [], "entities": []}, {"text": "Specifically, we cast the popular technique of cube pruning in the well-understood terms of heuristic search.", "labels": [], "entities": []}, {"text": "We show that cube pruning is essentially equivalent to A* search on a specific search space with specific heuristics.", "labels": [], "entities": []}, {"text": "This simple observation affords a deeper insight into how and why cube pruning works.", "labels": [], "entities": []}, {"text": "We show how this insight enables us to easily develop faster and exact variants of cube pruning for tree-to-string transducer-based MT ().", "labels": [], "entities": [{"text": "MT", "start_pos": 132, "end_pos": 134, "type": "TASK", "confidence": 0.9024298787117004}]}], "datasetContent": [{"text": "We evaluated all of the algorithms in this paper on a syntax-based Arabic-English translation system based on (), with rules extracted from 200 million words of parallel data from NIST 2008 and GALE data collections, and with a 4-gram language model trained on 1 billion words of monolingual English data from the LDC Gigaword corpus.", "labels": [], "entities": [{"text": "syntax-based Arabic-English translation", "start_pos": 54, "end_pos": 93, "type": "TASK", "confidence": 0.6494804422060648}, {"text": "NIST 2008 and GALE data collections", "start_pos": 180, "end_pos": 215, "type": "DATASET", "confidence": 0.8627051115036011}, {"text": "LDC Gigaword corpus", "start_pos": 314, "end_pos": 333, "type": "DATASET", "confidence": 0.9039797981580099}]}, {"text": "We evaluated the system's performance on the NIST 2008 test corpus, which consists of 1357 Arabic sentences from a mixture of newswire and web domains, with four English reference translations.", "labels": [], "entities": [{"text": "NIST 2008 test corpus", "start_pos": 45, "end_pos": 66, "type": "DATASET", "confidence": 0.9814176857471466}]}, {"text": "We report BLEU scores) on untokenized, recapitalized output.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9988013505935669}]}], "tableCaptions": [{"text": " Table 1: Results of standard and augmented cube  pruning. The number of (thousands of) search  nodes visited is given along with BLEU and av- erage time to decode one sentence, in seconds.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 130, "end_pos": 134, "type": "METRIC", "confidence": 0.9994361996650696}, {"text": "av- erage time", "start_pos": 139, "end_pos": 153, "type": "METRIC", "confidence": 0.8138139098882675}]}, {"text": " Table 2: Breakdown of visited search nodes by  type (for a fixed beam size).", "labels": [], "entities": [{"text": "Breakdown of visited search nodes", "start_pos": 10, "end_pos": 43, "type": "TASK", "confidence": 0.831310486793518}]}]}