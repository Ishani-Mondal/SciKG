{"title": [{"text": "Graph Alignment for Semi-Supervised Semantic Role Labeling", "labels": [], "entities": [{"text": "Graph Alignment", "start_pos": 0, "end_pos": 15, "type": "TASK", "confidence": 0.798347681760788}, {"text": "Semantic Role Labeling", "start_pos": 36, "end_pos": 58, "type": "TASK", "confidence": 0.6329922576745352}]}], "abstractContent": [{"text": "Unknown lexical items present a major obstacle to the development of broad-coverage semantic role labeling systems.", "labels": [], "entities": [{"text": "broad-coverage semantic role labeling", "start_pos": 69, "end_pos": 106, "type": "TASK", "confidence": 0.7225526571273804}]}, {"text": "We address this problem with a semi-supervised learning approach which acquires training instances for unseen verbs from an unlabeled corpus.", "labels": [], "entities": []}, {"text": "Our method relies on the hypothesis that unknown lexical items will be structurally and semantically similar to known items for which annotations are available.", "labels": [], "entities": []}, {"text": "Accordingly, we represent known and unknown sentences as graphs, formalize the search for the most similar verb as a graph alignment problem and solve the optimization using integer linear programming.", "labels": [], "entities": [{"text": "graph alignment", "start_pos": 117, "end_pos": 132, "type": "TASK", "confidence": 0.8032979071140289}]}, {"text": "Experimental results show that role labeling performance for unknown lexical items improves with training data produced automatically by our method.", "labels": [], "entities": [{"text": "role labeling", "start_pos": 31, "end_pos": 44, "type": "TASK", "confidence": 0.8218502402305603}]}], "introductionContent": [{"text": "Semantic role labeling, the task of automatically identifying the semantic roles conveyed by sentential constituents, has recently attracted much attention in the literature.", "labels": [], "entities": [{"text": "Semantic role labeling", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.7899865309397379}]}, {"text": "The ability to express the relations between predicates and their arguments while abstracting over surface syntactic configurations holds promise for many applications that require broad coverage semantic processing.", "labels": [], "entities": []}, {"text": "Examples include information extraction (, question answering (), machine translation (, and summarization ().", "labels": [], "entities": [{"text": "information extraction", "start_pos": 17, "end_pos": 39, "type": "TASK", "confidence": 0.8582479059696198}, {"text": "question answering", "start_pos": 43, "end_pos": 61, "type": "TASK", "confidence": 0.8487175703048706}, {"text": "machine translation", "start_pos": 66, "end_pos": 85, "type": "TASK", "confidence": 0.82247593998909}, {"text": "summarization", "start_pos": 93, "end_pos": 106, "type": "TASK", "confidence": 0.9694538116455078}]}, {"text": "Much progress in the area of semantic role labeling is due to the creation of resources like FrameNet (, which document the surface realization of semantic roles in real world corpora.", "labels": [], "entities": [{"text": "semantic role labeling", "start_pos": 29, "end_pos": 51, "type": "TASK", "confidence": 0.655995637178421}]}, {"text": "Such data is paramount for developing semantic role labelers which are usually based on supervised learning techniques and thus require training on role-annotated data.", "labels": [], "entities": []}, {"text": "Examples of the training instances provided in FrameNet are given below: ( Each verb in the example sentences evokes a frame which is situation-specific.", "labels": [], "entities": [{"text": "FrameNet", "start_pos": 47, "end_pos": 55, "type": "DATASET", "confidence": 0.8488919734954834}]}, {"text": "For instance, chance evokes the Daring frame, purchased the Commerce buy frame, and injured the Cause harm frame.", "labels": [], "entities": [{"text": "Daring frame", "start_pos": 32, "end_pos": 44, "type": "TASK", "confidence": 0.6185125112533569}, {"text": "Commerce buy frame", "start_pos": 60, "end_pos": 78, "type": "DATASET", "confidence": 0.9745497703552246}]}, {"text": "In addition, frames are associated with semantic roles corresponding to salient entities present in the situation evoked by the predicate.", "labels": [], "entities": []}, {"text": "The semantic roles for the frame Daring are Agent and Manner, whereas for Commerce buy these are Buyer and Goods.", "labels": [], "entities": [{"text": "Manner", "start_pos": 54, "end_pos": 60, "type": "METRIC", "confidence": 0.9915794134140015}]}, {"text": "A system trained on large amounts of such hand-annotated sentences typically learns to identify the boundaries of the arguments of the verb predicate (argument identification) and label them with semantic roles (argument classification).", "labels": [], "entities": [{"text": "argument identification)", "start_pos": 151, "end_pos": 175, "type": "TASK", "confidence": 0.799402912457784}, {"text": "argument classification", "start_pos": 212, "end_pos": 235, "type": "TASK", "confidence": 0.7553918063640594}]}, {"text": "A variety of methods have been developed for semantic role labeling with reasonably good performance (F 1 measures in the low 80s on standard test collections for English; we refer the interested reader to the proceedings of the SemEval-2007 shared task ( for an overview of the state-of-the-art).", "labels": [], "entities": [{"text": "semantic role labeling", "start_pos": 45, "end_pos": 67, "type": "TASK", "confidence": 0.8124127586682638}, {"text": "F 1", "start_pos": 102, "end_pos": 105, "type": "METRIC", "confidence": 0.989764928817749}]}, {"text": "Unfortunately, the reliance on training data, which is both difficult and highly expensive to produce, presents a major obstacle to the widespread application of semantic role labeling across different languages and text genres.", "labels": [], "entities": [{"text": "semantic role labeling", "start_pos": 162, "end_pos": 184, "type": "TASK", "confidence": 0.6256375710169474}]}, {"text": "The English FrameNet (version 1.3) is not a small resource -it contains 502 frames covering 5,866 lexical entries and 135,000 annotated sentences.", "labels": [], "entities": [{"text": "English FrameNet", "start_pos": 4, "end_pos": 20, "type": "DATASET", "confidence": 0.8811216652393341}]}, {"text": "Nevertheless, by virtue of being underdevelopment it is incomplete.", "labels": [], "entities": []}, {"text": "Lexical items (i.e., predicates evoking existing frames) are missing as well as frames and annotated sentences (their number varies greatly across lexical items).", "labels": [], "entities": []}, {"text": "Considering how the performance of supervised systems degrades on out-of-domain data (, not to mention unseen events, semisupervised or unsupervised methods seem to offer the primary near-term hope for broad coverage semantic role labeling.", "labels": [], "entities": [{"text": "broad coverage semantic role labeling", "start_pos": 202, "end_pos": 239, "type": "TASK", "confidence": 0.7172301530838012}]}, {"text": "In this work, we develop a semi-supervised method for enhancing FrameNet with additional annotations which could then be used for classifier training.", "labels": [], "entities": []}, {"text": "We assume that an initial set of labeled examples is available.", "labels": [], "entities": []}, {"text": "Then, faced with an unknown predicate, i.e., a predicate that does not evoke any frame according to the FrameNet database, we must decide (a) which frames it belongs to and (b) how to automatically annotate example sentences containing the predicate.", "labels": [], "entities": [{"text": "FrameNet database", "start_pos": 104, "end_pos": 121, "type": "DATASET", "confidence": 0.9251821339130402}]}, {"text": "We solve both problems jointly, using a graph alignment algorithm.", "labels": [], "entities": [{"text": "graph alignment", "start_pos": 40, "end_pos": 55, "type": "TASK", "confidence": 0.7446940541267395}]}, {"text": "Specifically, we view the task of inferring annotations for new verbs as an instance of a structural matching problem and follow a graph-based formulation for pairwise global network alignment.", "labels": [], "entities": [{"text": "pairwise global network alignment", "start_pos": 159, "end_pos": 192, "type": "TASK", "confidence": 0.6246950104832649}]}, {"text": "Labeled and unlabeled sentences are represented as dependencygraphs; we formulate the search for an optimal alignment as an integer linear program where different graph alignments are scored using a function based on semantic and structural similarity.", "labels": [], "entities": []}, {"text": "We evaluate our algorithm in two ways.", "labels": [], "entities": []}, {"text": "We assess how accurate it is in predicting the frame for an unknown verb and also evaluate whether the annotations we produce are useful for semantic role labeling.", "labels": [], "entities": [{"text": "semantic role labeling", "start_pos": 141, "end_pos": 163, "type": "TASK", "confidence": 0.6850108106931051}]}, {"text": "In the following section we provide an overview of related work.", "labels": [], "entities": []}, {"text": "Next, we describe our graphalignment model in more detail (Section 3) and present the resources and evaluation methodology used in our experiments (Section 4).", "labels": [], "entities": []}, {"text": "We conclude the paper by presenting and discussing our results.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section we present our experimental set-up for assessing the performance of our method.", "labels": [], "entities": []}, {"text": "We give details on the data sets we used, describe the baselines we adopted for comparison with our approach, and explain how our system output was evaluated.", "labels": [], "entities": []}, {"text": "Data Our experiments used annotated sentences from FrameNet as a seed corpus.", "labels": [], "entities": [{"text": "FrameNet", "start_pos": 51, "end_pos": 59, "type": "DATASET", "confidence": 0.9051418900489807}]}, {"text": "These were augmented with automatically labeled sentences from the BNC which we used as our expansion corpus.", "labels": [], "entities": [{"text": "BNC", "start_pos": 67, "end_pos": 70, "type": "DATASET", "confidence": 0.9376881718635559}]}, {"text": "FrameNet sentences were parsed with RASP ().", "labels": [], "entities": [{"text": "RASP", "start_pos": 36, "end_pos": 40, "type": "METRIC", "confidence": 0.6641339659690857}]}, {"text": "In addition to phrase structure trees, RASP delivers a dependency-based representation of the sentence which we used in our experiments.", "labels": [], "entities": [{"text": "RASP", "start_pos": 39, "end_pos": 43, "type": "TASK", "confidence": 0.9712226986885071}]}, {"text": "FrameNet role annotations were mapped onto those dependency graph nodes that corresponded most closely to the annotated substring (see fora detailed description of the mapping algorithm).", "labels": [], "entities": []}, {"text": "BNC sentences were also parsed with RASP.", "labels": [], "entities": [{"text": "BNC", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.8484008312225342}, {"text": "RASP", "start_pos": 36, "end_pos": 40, "type": "METRIC", "confidence": 0.6103255152702332}]}, {"text": "We randomly split the FrameNet corpus 1 into 80% training set, 10% test set, and 10% development set.", "labels": [], "entities": [{"text": "FrameNet corpus 1", "start_pos": 22, "end_pos": 39, "type": "DATASET", "confidence": 0.897036055723826}]}, {"text": "Next, all frame evoking verbs in the training set were ordered by their number of occurrence and split into two groups, seen and unseen.", "labels": [], "entities": []}, {"text": "Every other verb from the ordered list was considered unseen.", "labels": [], "entities": []}, {"text": "This quasi-random split covers abroad range of predicates with a varying number of annotations.", "labels": [], "entities": []}, {"text": "Accordingly, the FrameNet sentences in the training and test sets were divided into the sets train seen, train unseen, test seen, and test unseen.", "labels": [], "entities": []}, {"text": "As we explain below, this was necessary for evaluation purposes.", "labels": [], "entities": []}, {"text": "The train seen dataset consisted of 24,220 sentences, with 1,238 distinct frame evoking verbs, whereas train unseen contained 24,315 sentences with the same number of frame evoking verbs.", "labels": [], "entities": []}, {"text": "Analogously, test seen had 2,990 sentences and 817 unique frame evoking verbs; the number of sentences in test unseen was 3,064 (with 847 unique frame evoking verbs).", "labels": [], "entities": []}, {"text": "Model Parameters The alignment model presented in Section 3 crucially relies on the similar-ity function that scores potential alignments (see Equation).", "labels": [], "entities": []}, {"text": "This function has a free parameter, the weight \u03b1 for determining the relative contribution of semantic and syntactic similarity.", "labels": [], "entities": []}, {"text": "We tuned \u03b1 using leave-one-out cross-validation on the development set.", "labels": [], "entities": []}, {"text": "For each annotated sentence in this set we found its most similar other sentence and determined the best alignment between the two dependency graphs representing them.", "labels": [], "entities": []}, {"text": "Since the true annotations for each sentence were available, it was possible to evaluate the accuracy of our method for any \u03b1 value.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 93, "end_pos": 101, "type": "METRIC", "confidence": 0.9996127486228943}]}, {"text": "We did this by comparing the true annotation of a sentence to the annotation its nearest neighbor would have induced by projection.", "labels": [], "entities": []}, {"text": "Following this procedure, we obtained best results with \u03b1 = 0.2.", "labels": [], "entities": []}, {"text": "The semantic similarity measure relies on a semantic space model which we built on a lemmatized version of the BNC.", "labels": [], "entities": [{"text": "BNC", "start_pos": 111, "end_pos": 114, "type": "DATASET", "confidence": 0.9525940418243408}]}, {"text": "Our implementation followed closely the model presented in as it was used in a similar task and obtained good results.", "labels": [], "entities": []}, {"text": "Specifically, we used a context window of five words on either side of the target word, and 2,000 vector dimensions.", "labels": [], "entities": []}, {"text": "These were the common context words in the BNC.", "labels": [], "entities": [{"text": "BNC", "start_pos": 43, "end_pos": 46, "type": "DATASET", "confidence": 0.8460513949394226}]}, {"text": "Their values were set to the ratio of the probability of the context word given the target word to the probability of the context word overall.", "labels": [], "entities": []}, {"text": "Semantic similarity was measured using the cosine of the angle between the vectors representing any two words.", "labels": [], "entities": []}, {"text": "The same semantic space was used to create the distributional profile of a frame (which is the centroid of the vectors of its verbs).", "labels": [], "entities": []}, {"text": "For each unknown verb, we consider the k most similar frame candidates (again similarity is measured via cosine).", "labels": [], "entities": []}, {"text": "Our experiments explored different values of k ranging from 1 to 10.", "labels": [], "entities": []}, {"text": "Evaluation Our evaluation assessed the performance of a semantic frame and role labeler with and without the annotations produced by our method.", "labels": [], "entities": []}, {"text": "The labeler followed closely the implementation described in.", "labels": [], "entities": []}, {"text": "We extracted features from dependency parses corresponding to those routinely used in the semantic role labeling literature (see for an overview).", "labels": [], "entities": [{"text": "semantic role labeling", "start_pos": 90, "end_pos": 112, "type": "TASK", "confidence": 0.6268495122591654}]}, {"text": "SVM classifiers were trained 2 with the LIBLINEAR library) and learned to predict the frame name, role spans, and role labels.", "labels": [], "entities": [{"text": "SVM classifiers", "start_pos": 0, "end_pos": 15, "type": "TASK", "confidence": 0.8040004670619965}, {"text": "LIBLINEAR", "start_pos": 40, "end_pos": 49, "type": "METRIC", "confidence": 0.8653291463851929}]}, {"text": "We followed the one-versus-one strategy for multi-class classification.", "labels": [], "entities": [{"text": "multi-class classification", "start_pos": 44, "end_pos": 70, "type": "TASK", "confidence": 0.8539979755878448}]}, {"text": "Specifically, the labeler was trained on the train seen data set without any access to training instances representative of the \"unknown\" verbs in test unseen.", "labels": [], "entities": [{"text": "train seen data set", "start_pos": 45, "end_pos": 64, "type": "DATASET", "confidence": 0.8126204311847687}]}, {"text": "We then trained the labeler on a larger set containing train seen and new training examples obtained with our method.", "labels": [], "entities": []}, {"text": "To do this, we used train seen as the seed corpus and the BNC as the expansion corpus.", "labels": [], "entities": [{"text": "BNC", "start_pos": 58, "end_pos": 61, "type": "DATASET", "confidence": 0.9637866020202637}]}, {"text": "For each \"unknown\" verb in train unseen we obtained BNC sentences with annotations projected from their most similar seeds.", "labels": [], "entities": [{"text": "BNC", "start_pos": 52, "end_pos": 55, "type": "DATASET", "confidence": 0.8048811554908752}]}, {"text": "The quality of these sentences as training instances varies depending on their similarity to the seed.", "labels": [], "entities": []}, {"text": "In our experiments we added to the training set the 20 highest scoring BNC sentences per verb (adding lessor more instances led to worse performance).", "labels": [], "entities": []}, {"text": "The average number of frames which can be evoked by a verb token in the set test unseen was 1.96.", "labels": [], "entities": []}, {"text": "About half of them (1,522 instances) can evoke only one frame, 22% can evoke two frames, and 14 instances can evoke up to 11 different frames.", "labels": [], "entities": []}, {"text": "Finally, there are 120 instances (4%) in test unseen for which the correct frame is not annotated on any sentence in train seen.", "labels": [], "entities": []}], "tableCaptions": []}