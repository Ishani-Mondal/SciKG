{"title": [{"text": "Improved Statistical Machine Translation Using Monolingually-Derived Paraphrases", "labels": [], "entities": [{"text": "Improved Statistical Machine Translation", "start_pos": 0, "end_pos": 40, "type": "TASK", "confidence": 0.8600200116634369}]}], "abstractContent": [{"text": "Untranslated words still constitute a major problem for Statistical Machine Translation (SMT), and current SMT systems are limited by the quantity of parallel training texts.", "labels": [], "entities": [{"text": "Statistical Machine Translation (SMT)", "start_pos": 56, "end_pos": 93, "type": "TASK", "confidence": 0.8755544523398081}, {"text": "SMT", "start_pos": 107, "end_pos": 110, "type": "TASK", "confidence": 0.986005425453186}]}, {"text": "Augmenting the training data with paraphrases generated by pivoting through other languages alleviates this problem, especially for the so-called \"low density\" languages.", "labels": [], "entities": []}, {"text": "But pivoting requires additional parallel texts.", "labels": [], "entities": []}, {"text": "We address this problem by deriving paraphrases monolin-gually, using distributional semantic similarity measures, thus providing access to larger training resources, such as comparable and unrelated monolingual corpora.", "labels": [], "entities": []}, {"text": "We present what is to our knowledge the first successful integration of a colloca-tional approach to untranslated words with an end-to-end, state of the art SMT system demonstrating significant translation improvements in a low-resource setting.", "labels": [], "entities": [{"text": "SMT", "start_pos": 157, "end_pos": 160, "type": "TASK", "confidence": 0.9922713041305542}]}], "introductionContent": [{"text": "Phrase-based systems, flat and hierarchical alike (, have achieved a much better translation coverage than wordbased ones), but untranslated words remain a major problem in SMT.", "labels": [], "entities": [{"text": "SMT", "start_pos": 173, "end_pos": 176, "type": "TASK", "confidence": 0.9938313961029053}]}, {"text": "For example, according to, a SMT system with a training corpus of 10,000 words learned only 10% of the vocabulary; the same system learned about 30% with a training corpus of 100,000 words; and even with a large training corpus of nearly 10,000,000 words it only reached about 90% coverage of the source vocabulary.", "labels": [], "entities": [{"text": "SMT", "start_pos": 29, "end_pos": 32, "type": "TASK", "confidence": 0.9893017411231995}]}, {"text": "Coverage of higher order n-gram levels is even harder.", "labels": [], "entities": [{"text": "Coverage", "start_pos": 0, "end_pos": 8, "type": "METRIC", "confidence": 0.9048917293548584}]}, {"text": "This problem plays a major part in reducing machine translation quality, as reflected by both automatic measures such as BLEU () and human judgment tests.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 44, "end_pos": 63, "type": "TASK", "confidence": 0.7936086654663086}, {"text": "BLEU", "start_pos": 121, "end_pos": 125, "type": "METRIC", "confidence": 0.9986055493354797}]}, {"text": "Improving translation coverage accurately is therefore important for SMT systems.", "labels": [], "entities": [{"text": "translation coverage", "start_pos": 10, "end_pos": 30, "type": "TASK", "confidence": 0.8548352122306824}, {"text": "SMT", "start_pos": 69, "end_pos": 72, "type": "TASK", "confidence": 0.9961658716201782}]}, {"text": "The first solution that might come to mind is to use larger parallel training corpora.", "labels": [], "entities": []}, {"text": "However, current state-of-the-art SMT systems cannot learn from non-aligned corpora, while sentence-aligned parallel corpora (bitexts) area limited resource (See Section 2 for discussion of automaticallycompiled bitexts).", "labels": [], "entities": [{"text": "SMT", "start_pos": 34, "end_pos": 37, "type": "TASK", "confidence": 0.9918509125709534}]}, {"text": "Another direction might be to make use of non-parallel corpora for training.", "labels": [], "entities": []}, {"text": "However, this requires developing techniques to extract alignments or translations from them, and in a sufficiently fast, memory-efficient, and scalable manner.", "labels": [], "entities": [{"text": "alignments or translations", "start_pos": 56, "end_pos": 82, "type": "TASK", "confidence": 0.7348118821779887}]}, {"text": "One approach that can, in principle, better exploit both alignments from bitexts and make use of non-parallel corpora is the distributional collocational approach, e.g., as used by and.", "labels": [], "entities": []}, {"text": "However, the systems described there are not easily scalable, and require pre-computation of a very large collocation counts matrix.", "labels": [], "entities": []}, {"text": "Related attempts propose generating bitexts from comparable and \"quasicomparable\" bilingual texts by iteratively bootstrapping documents, sentences, and words (), or by using a maximum entropy classifier (.", "labels": [], "entities": []}, {"text": "Alignment accuracy remains a challenge for them.", "labels": [], "entities": [{"text": "Alignment", "start_pos": 0, "end_pos": 9, "type": "TASK", "confidence": 0.8991931676864624}, {"text": "accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.8973202109336853}]}, {"text": "Recent work has proposed augmenting the training data with paraphrases generated by pivoting through other languages.", "labels": [], "entities": []}, {"text": "This indeed alleviates the vocabulary coverage problem, especially for the so-called \"low density\" languages.", "labels": [], "entities": [{"text": "vocabulary coverage", "start_pos": 27, "end_pos": 46, "type": "TASK", "confidence": 0.7056409865617752}]}, {"text": "However, these approaches still require bitexts where one side contains the original source language.", "labels": [], "entities": []}, {"text": "The paradigm described in this paper involves constructing monolingual distributional profiles (DPs; a.k.a. word association profiles, or cooccurrence vectors) of out-of-vocabulary words and phrases in the source language; then, generating paraphrase candidates from phrases that cooccur in similar contexts, and assigning them similarity scores.", "labels": [], "entities": []}, {"text": "The highest ranking paraphrases are used to augment the translation phrase table.", "labels": [], "entities": []}, {"text": "The table augmentation idea is similar to), but our proposed paradigm does not require using a limited resource such as parallel texts in order to generate paraphrases.", "labels": [], "entities": []}, {"text": "Moreover, our proposed paradigm can, in principle, achieve large-scale acquisition of paraphrases with high semantic similarity.", "labels": [], "entities": []}, {"text": "However, using parallel training texts in pivoting techniques offers the potential advantage of implicit translational knowledge, in the form of sentence alignments, while our approach is unguided in this respect.", "labels": [], "entities": [{"text": "sentence alignments", "start_pos": 145, "end_pos": 164, "type": "TASK", "confidence": 0.7231253385543823}]}, {"text": "Therefore, we conducted experiments to find out how these relative advantages play out.", "labels": [], "entities": []}, {"text": "We present here, to our knowledge for the first time, positive results of integrating distributional monolingually-derived paraphrases in an end-to-end state-of-the-art SMT system.", "labels": [], "entities": [{"text": "SMT", "start_pos": 169, "end_pos": 172, "type": "TASK", "confidence": 0.9877227544784546}]}, {"text": "In the rest of this paper we discuss related work in Section 2, describe the distributional hypothesis and distributional profiles in Section 3, and present the monolingually-derived paraphrase generation system in Section 4.", "labels": [], "entities": []}, {"text": "We report our experiments and results in Section 5, and conclude by discussing the implications and future research directions in Section 6.", "labels": [], "entities": []}], "datasetContent": [{"text": "We examined the application of the system's paraphrases to handling unknown phrases when translating from English into Chinese (E2C) and from Spanish into English (S2E).", "labels": [], "entities": []}, {"text": "For all baselines we used the phrase-based statistical machine translation system Moses (, with the default model features, weighted in a log-linear framework).", "labels": [], "entities": [{"text": "phrase-based statistical machine translation", "start_pos": 30, "end_pos": 74, "type": "TASK", "confidence": 0.5421012789011002}]}, {"text": "Feature weights were set with minimum error rate training (Och, 2003) on a development set using BLEU () as the objective function.", "labels": [], "entities": [{"text": "minimum error rate training", "start_pos": 30, "end_pos": 57, "type": "METRIC", "confidence": 0.7953579127788544}, {"text": "BLEU", "start_pos": 97, "end_pos": 101, "type": "METRIC", "confidence": 0.9980181455612183}]}, {"text": "Test results were evaluated using BLEU and TER).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 34, "end_pos": 38, "type": "METRIC", "confidence": 0.9989582300186157}, {"text": "TER", "start_pos": 43, "end_pos": 46, "type": "METRIC", "confidence": 0.9971656203269958}]}, {"text": "The phrase translation probabilities were determined using maximum likelihood estimation over phrases induced from word-level alignments produced by performing Giza++ training) on both source and target sides of the parallel training sets.", "labels": [], "entities": [{"text": "phrase translation", "start_pos": 4, "end_pos": 22, "type": "TASK", "confidence": 0.7578736245632172}]}, {"text": "When the baseline system encountered unknown words in the test set, its behavior was simply to reproduce the foreign word in the translated output.", "labels": [], "entities": []}, {"text": "The paraphrase-augmented systems were identical to the corresponding baseline system, with the exception of additional (paraphrase-based) translation rules, and additional feature(s).", "labels": [], "entities": []}, {"text": "Similarly to, we added the following feature: is generated from (e, f ) using monolinguallyderived paraphrases.", "labels": [], "entities": []}, {"text": "1 Otherwise, Note that it is possible to construct anew translation rule from f toe via more than one pair of source-side phrase and its paraphrase; e.g., if f 1 is a paraphrase off , and so is f 2 , and both f 1 , f 2 translate to the same e, then both lead to the construction of the new rule translating f toe, but with potentially different feature scores.", "labels": [], "entities": []}, {"text": "In order to eliminate this duplicity and leverage over these alternate paths which can be used to increase our confidence level in the new rule, we did the following: For each paraphrase f of some source-side phrases f i , with respective similarity scores sim(f i , f ), we calculated an aggregate score asim with a \"quasi-onlineupdating\" method as follows: asim i = (1 \u2212 asim i\u22121 )sim(f i , f ), where asim 0 = 0.", "labels": [], "entities": []}, {"text": "The aggregate score asim is updated in an \"online\" fashion with each pair f i , f as they are processed, but only the final asim k score is used, after all k pairs have been processed.", "labels": [], "entities": []}, {"text": "Simple arithmetics can show that this method is insensitive to the order in which the paraphrases are processed.", "labels": [], "entities": []}, {"text": "We only augment the phrase table with a single rule from f toe, and in it are the feature values of the phrase f i for which the score sim(f i , f ) was the highest.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Training set sizes (million tokens).", "labels": [], "entities": []}, {"text": " Table 2. For the E2C sys- tems, for which we had four reference translations  for the test set, we used shortest reference length,  and used the NIST-provided script to split the out- put words to Chinese characters before evaluation.  Statistical significance for the BLEU results were  calculated using", "labels": [], "entities": [{"text": "E2C sys- tems", "start_pos": 18, "end_pos": 31, "type": "DATASET", "confidence": 0.8119592219591141}, {"text": "BLEU", "start_pos": 270, "end_pos": 274, "type": "METRIC", "confidence": 0.9967684745788574}]}, {"text": " Table 2: E2C Results: character-based BLEU and  TER scores. All models have one additional fea- ture over baseline, except for the \"1 + 2-6\" mod- els that have one feature for unigrams and an- other feature for bigrams to 6-grams. Paraphrases  with score < .3 were filtered out. *** = sig- nificance test over baseline with p < 0.0001,  using Koehn's (2004) pair-wise bootstrap resam- pling test for BLEU with 95% confidence interval.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 39, "end_pos": 43, "type": "METRIC", "confidence": 0.9421551823616028}, {"text": "TER", "start_pos": 49, "end_pos": 52, "type": "METRIC", "confidence": 0.9946604371070862}, {"text": "BLEU", "start_pos": 401, "end_pos": 405, "type": "METRIC", "confidence": 0.8353458046913147}]}, {"text": " Table 3: English paraphrases from E2C 29K- bitext systems.", "labels": [], "entities": [{"text": "E2C 29K- bitext", "start_pos": 35, "end_pos": 50, "type": "DATASET", "confidence": 0.8197202682495117}]}, {"text": " Table 4: S2E Results: Lowercase BLEU and TER. Paraphrases with score < minScore were filtered out.  *** = significance test over baseline with p < 0.0001, using Koehn's (2004) pair-wise bootstrap test for  BLEU with 95% confidence interval.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 33, "end_pos": 37, "type": "METRIC", "confidence": 0.991571307182312}, {"text": "TER", "start_pos": 42, "end_pos": 45, "type": "METRIC", "confidence": 0.9958970546722412}, {"text": "significance", "start_pos": 107, "end_pos": 119, "type": "METRIC", "confidence": 0.9658015966415405}, {"text": "BLEU", "start_pos": 207, "end_pos": 211, "type": "METRIC", "confidence": 0.9779154658317566}]}]}