{"title": [{"text": "First-and Second-Order Expectation Semirings with Applications to Minimum-Risk Training on Translation Forests *", "labels": [], "entities": []}], "abstractContent": [{"text": "Many statistical translation models can be regarded as weighted logical deduction.", "labels": [], "entities": [{"text": "statistical translation", "start_pos": 5, "end_pos": 28, "type": "TASK", "confidence": 0.6010266691446304}]}, {"text": "Under this paradigm, we use weights from the expectation semiring (Eisner, 2002), to compute first-order statistics (e.g., the expected hypothesis length or feature counts) over packed forests of translations (lat-tices or hypergraphs).", "labels": [], "entities": []}, {"text": "We then introduce a novel second-order expectation semir-ing, which computes second-order statistics (e.g., the variance of the hypothesis length or the gradient of entropy).", "labels": [], "entities": []}, {"text": "This second-order semiring is essential for many interesting training paradigms such as minimum risk, deterministic anneal-ing, active learning, and semi-supervised learning, where gradient descent optimization requires computing the gradient of en-tropy or risk.", "labels": [], "entities": [{"text": "gradient descent optimization", "start_pos": 181, "end_pos": 210, "type": "TASK", "confidence": 0.6835598746935526}]}, {"text": "We use these semirings in an open-source machine translation toolkit, Joshua, enabling minimum-risk training fora benefit of up to 1.0 BLEU point.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 41, "end_pos": 60, "type": "TASK", "confidence": 0.7024246603250504}, {"text": "BLEU", "start_pos": 135, "end_pos": 139, "type": "METRIC", "confidence": 0.9974356293678284}]}], "introductionContent": [{"text": "A hypergraph or \"packed forest\" () is a compact data structure that uses structure-sharing to represent exponentially many trees in polynomial space.", "labels": [], "entities": []}, {"text": "A weighted hypergraph also defines a probability or other weight for each tree, and can be used to represent the hypothesis space considered (for a given input) by a monolingual parser or a tree-based translation system, e.g., tree to string;), string to tree (), tree to tree, or string to string with latent tree structures.", "labels": [], "entities": []}, {"text": "Given a hypergraph, we are often interested in computing some quantities over it using dynamic programming algorithms.", "labels": [], "entities": []}, {"text": "For example, we may want to run the Viterbi algorithm to find the most probable derivation tree in the hypergraph, or the k most probable trees.", "labels": [], "entities": []}, {"text": "Semiring-weighted logic programming is a general framework to specify these algorithms (.", "labels": [], "entities": []}, {"text": "Goodman (1999) describes many useful semirings (e.g., Viterbi, inside, and Viterbin-best).", "labels": [], "entities": []}, {"text": "While most of these semirings are used in \"testing\" (i.e., decoding), we are mainly interested in the semirings that are useful for \"training\" (i.e., parameter estimation).", "labels": [], "entities": []}, {"text": "The expectation semiring, originally proposed for finite-state machines, is one such \"training\" semiring, and can be used to compute feature expectations for the Estep of the EM algorithm, or gradients of the likelihood function for gradient descent.", "labels": [], "entities": [{"text": "Estep", "start_pos": 162, "end_pos": 167, "type": "METRIC", "confidence": 0.9921556115150452}]}, {"text": "In this paper, we apply the expectation semiring) to a hypergraph (or packed forest) rather than just a lattice.", "labels": [], "entities": []}, {"text": "We then propose a novel second-order expectation semiring, nicknamed the \"variance semiring.\"", "labels": [], "entities": []}, {"text": "The original first-order expectation semiring allows us to efficiently compute a vector of firstorder statistics (expectations; first derivatives) on the set of paths in a lattice or the set of trees in a hypergraph.", "labels": [], "entities": []}, {"text": "The second-order expectation semiring additionally computes a matrix of secondorder statistics (expectations of products; second derivatives (Hessian); derivatives of expectations).", "labels": [], "entities": []}, {"text": "We present details on how to compute many interesting quantities over the hypergraph using the expectation and variance semirings.", "labels": [], "entities": []}, {"text": "These quantities include expected hypothesis length, feature expectation, entropy, cross-entropy, KullbackLeibler divergence, Bayes risk, variance of hypothesis length, gradient of entropy and Bayes risk, covariance and Hessian matrix, and soon.", "labels": [], "entities": []}, {"text": "The variance semiring is essential for many interesting training paradigms such as deterministic annealing, minimum risk (), active and semi-supervised learning ().", "labels": [], "entities": []}, {"text": "In these settings, we must compute the gradient of entropy or risk.", "labels": [], "entities": []}, {"text": "The semirings can also be used for second-order gradient optimization algorithms.", "labels": [], "entities": [{"text": "second-order gradient optimization", "start_pos": 35, "end_pos": 69, "type": "TASK", "confidence": 0.722254474957784}]}, {"text": "We implement the expectation and variance semirings in Joshua (, and demonstrate their practical benefit by using minimumrisk training to improve).", "labels": [], "entities": []}], "datasetContent": [{"text": "We built a translation model on a corpus for IWSLT 2005 Chinese-to-English translation task, which consists of 40k pairs of sentences.", "labels": [], "entities": [{"text": "IWSLT 2005 Chinese-to-English translation task", "start_pos": 45, "end_pos": 91, "type": "TASK", "confidence": 0.8558310151100159}]}, {"text": "We used a 5-gram language model with modified Kneser-Ney smoothing, trained on the bitext's English using SRILM).", "labels": [], "entities": [{"text": "SRILM", "start_pos": 106, "end_pos": 111, "type": "METRIC", "confidence": 0.4682128429412842}]}], "tableCaptions": [{"text": " Table 5: BLEU scores on the Dev and test sets under different  training scenarios. In the \"small\" model, five features (i.e.,  one for the language model, three for the translation model,  and one for word penalty) are tuned. In the \"large\" model,  21k additional unigram and bigram features are used.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9983850717544556}]}]}