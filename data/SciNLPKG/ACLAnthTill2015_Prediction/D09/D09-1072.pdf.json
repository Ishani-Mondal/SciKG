{"title": [{"text": "A Simple Unsupervised Learner for POS Disambiguation Rules Given Only a Minimal Lexicon", "labels": [], "entities": [{"text": "POS Disambiguation", "start_pos": 34, "end_pos": 52, "type": "TASK", "confidence": 0.8086532950401306}]}], "abstractContent": [{"text": "We propose anew model for unsupervised POS tagging based on linguistic distinctions between open and closed-class items.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 39, "end_pos": 50, "type": "TASK", "confidence": 0.8650357127189636}]}, {"text": "Exploiting notions from current linguistic theory, the system uses far less information than previous systems, far simpler computational methods, and far sparser descriptions in learning contexts.", "labels": [], "entities": []}, {"text": "By applying simple language acquisition techniques based on counting, the system is given the closed-class lexicon, acquires a large open-class lexicon and then acquires disambiguation rules for both.", "labels": [], "entities": []}, {"text": "This system achieves a 20% error reduction for POS tagging over state-of-the-art unsuper-vised systems tested under the same conditions , and achieves comparable accuracy when trained with much less prior information .", "labels": [], "entities": [{"text": "error reduction", "start_pos": 27, "end_pos": 42, "type": "METRIC", "confidence": 0.9762287139892578}, {"text": "POS tagging", "start_pos": 47, "end_pos": 58, "type": "TASK", "confidence": 0.84951251745224}, {"text": "accuracy", "start_pos": 162, "end_pos": 170, "type": "METRIC", "confidence": 0.999221920967102}]}], "introductionContent": [{"text": "All recent research on unsupervised tagging, as well as the majority of work on supervised taggers, views POS tagging as a sequential labeling problem and treats all POS tags, both closed-and open-class, as roughly equivalent.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 106, "end_pos": 117, "type": "TASK", "confidence": 0.7846381068229675}]}, {"text": "In this work we explore a different understanding of the tagging problem, viewing it as a process of first identifying functional syntactic contexts, which are flagged by closed-class items, and then using these functional contexts to determine the POS labels.", "labels": [], "entities": []}, {"text": "This disambiguation model differs from most previous work in three ways: 1) it uses different encodings over two distinct domains (roughly open-and closed-class words) with complementary distribution (and so decodes separately); 2) it is deterministic and 3) it is non-lexicalized.", "labels": [], "entities": []}, {"text": "By learning disambiguation models for open-and closed-classes separately, we found that the deterministic, rulebased model can be learned from unannotated data by a simple strategy of selecting a rule in each appropriate context with the highest count.", "labels": [], "entities": []}, {"text": "In contrast to this, most previous work on unsupervised tagging (especially for English) concentrates on improving the parameter estimation techniques for training statistical disambiguation models from unannotated data.", "labels": [], "entities": []}, {"text": "For example, proposes contrastive estimation (CE) for log-linear models (CRF), achieving the current state-of-the-art performance of 90.4%; applies a Bayesian approach to improve maximumlikelihood estimation (MLE) for training generative models (HMM).", "labels": [], "entities": [{"text": "contrastive estimation (CE)", "start_pos": 22, "end_pos": 49, "type": "TASK", "confidence": 0.6450017690658569}]}, {"text": "In the main experiments of both of these papers, the disambiguation model is learned, but the algorithms assume a complete knowledge of the lexicon with all possible tags for each word.", "labels": [], "entities": []}, {"text": "In this work, we propose making such a large lexicon unnecessary by learning the bulk of the lexicon along with learning a disambiguation model.", "labels": [], "entities": []}, {"text": "Little previous work has been done on this natural and simple idea because the clusters found by previous induction schemes are not inline with the lexical categories that we care about. is perhaps the first with the intention of generating \"a discrete set of clusters.\"", "labels": [], "entities": []}, {"text": "By applying similar techniques to, which we discuss later, we can generate clusters that closely approximate the central open-class lexical categories, a major advance, but we still require a closed-class lexicon specifying possible tags for these words.", "labels": [], "entities": []}, {"text": "This asymmetry in our lexicon acquisition model conforms with our understanding of natural language as structured data over two distinct domains with complementary distribution: open-class (lexical) and closed-class (functional).", "labels": [], "entities": []}, {"text": "Provided with only a closed-class lexicon of 288 words, about 0.6% of the full lexicon, the system acquires a large open-class lexicon and then acquires disambiguation rules for both closed-and open-class words, achieving a tagging accuracy of 90.6% fora 24k dataset, as high as the current state-of-the-art (90.4%) achieved with a complete dictionary.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 232, "end_pos": 240, "type": "METRIC", "confidence": 0.9597662091255188}]}, {"text": "In the test condition where both algorithms are provided with a full lexicon, and are trained and evaluated over the same 96k dataset, we reduce the tagging error by up to 20%.", "labels": [], "entities": []}, {"text": "In Section 2 we explain our understanding of the POS tagging problem in detail and define the notions of functional context and open-and closedclass elements.", "labels": [], "entities": [{"text": "POS tagging problem", "start_pos": 49, "end_pos": 68, "type": "TASK", "confidence": 0.8685136437416077}]}, {"text": "Then we will introduce our methods for acquiring the lexicon (Section 3) and learning disambiguation models (Section 4, 5 and 6) step by step.", "labels": [], "entities": []}, {"text": "Results are reported in Section 7 followed by Section 8 which discusses the linguistic motivation behind this work and the simplicity and efficiency of our model.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Comparison of tagsets", "labels": [], "entities": []}, {"text": " Table 2: N/V categories of 27 POS tags", "labels": [], "entities": []}, {"text": " Table 3: Tagging accuracy with partial dictionaries over", "labels": [], "entities": [{"text": "Tagging", "start_pos": 10, "end_pos": 17, "type": "TASK", "confidence": 0.9930838346481323}, {"text": "accuracy", "start_pos": 18, "end_pos": 26, "type": "METRIC", "confidence": 0.9581897258758545}]}, {"text": " Table 4: Tagging Accuracy of models trained over dataset", "labels": [], "entities": [{"text": "Tagging", "start_pos": 10, "end_pos": 17, "type": "TASK", "confidence": 0.9883316159248352}, {"text": "Accuracy", "start_pos": 18, "end_pos": 26, "type": "METRIC", "confidence": 0.8892025351524353}]}, {"text": " Table 5: The number of errors and percent ambiguous to-", "labels": [], "entities": [{"text": "errors", "start_pos": 24, "end_pos": 30, "type": "METRIC", "confidence": 0.8549988269805908}, {"text": "ambiguous", "start_pos": 43, "end_pos": 52, "type": "METRIC", "confidence": 0.9670197367668152}]}]}