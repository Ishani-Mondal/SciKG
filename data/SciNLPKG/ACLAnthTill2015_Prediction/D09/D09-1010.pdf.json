{"title": [{"text": "Efficient kernels for sentence pair classification", "labels": [], "entities": [{"text": "sentence pair classification", "start_pos": 22, "end_pos": 50, "type": "TASK", "confidence": 0.7643033464749655}]}], "abstractContent": [{"text": "In this paper, we propose a novel class of graphs, the tripartite directed acyclic graphs (tDAGs), to model first-order rule feature spaces for sentence pair classification.", "labels": [], "entities": [{"text": "sentence pair classification", "start_pos": 144, "end_pos": 172, "type": "TASK", "confidence": 0.7205763558546702}]}, {"text": "We introduce a novel algorithm for computing the similarity in first-order rewrite rule feature spaces.", "labels": [], "entities": [{"text": "similarity", "start_pos": 49, "end_pos": 59, "type": "METRIC", "confidence": 0.9601252675056458}]}, {"text": "Our algorithm is extremely efficient and, as it computes the similarity of instances that can be represented in explicit feature spaces, it is a valid kernel function.", "labels": [], "entities": []}], "introductionContent": [{"text": "Natural language processing models are generally positive combinations between linguistic models and automatically learnt classifiers.", "labels": [], "entities": []}, {"text": "As trees are extremely important in many linguistic theories, a large amount of works exploiting machine learning algorithms for NLP tasks has been developed for this class of data structures).", "labels": [], "entities": []}, {"text": "These works propose efficient algorithms for determining the similarity among two trees in tree fragment feature spaces.", "labels": [], "entities": []}, {"text": "Yet, some NLP tasks such as textual entailment recognition () and some linguistic theories such as HPSG ( require more general graphs and, then, more general algorithms for computing similarity among graphs.", "labels": [], "entities": [{"text": "textual entailment recognition", "start_pos": 28, "end_pos": 58, "type": "TASK", "confidence": 0.8242740035057068}, {"text": "HPSG", "start_pos": 99, "end_pos": 103, "type": "DATASET", "confidence": 0.8796404004096985}]}, {"text": "Unfortunately, algorithms for computing similarity among two general graphs in term of common subgraphs are still exponential.", "labels": [], "entities": []}, {"text": "In these cases, approximated algorithms have been proposed.", "labels": [], "entities": []}, {"text": "For example, the one proposed in counts the number of subpaths in common.", "labels": [], "entities": []}, {"text": "The same happens for the one proposed in () that is applicable to a particular class of graphs, i.e. the hierarchical directed acyclic graphs.", "labels": [], "entities": []}, {"text": "These algorithms do not compute the number of subgraphs in common between two graphs.", "labels": [], "entities": []}, {"text": "Then, these algorithms approximate the feature spaces we need in these NLP tasks.", "labels": [], "entities": []}, {"text": "For computing similarities in these feature spaces, we have to investigate if we can define a particular class of graphs for the class of tasks we want to solve.", "labels": [], "entities": []}, {"text": "Once we focused the class of graph, we can explore efficient similarity algorithms.", "labels": [], "entities": []}, {"text": "Avery important class of graphs can be defined for tasks involving sentence pairs.", "labels": [], "entities": []}, {"text": "In these cases, an important class of feature spaces is the one that represents first-order rewrite rules.", "labels": [], "entities": []}, {"text": "For example, in textual entailment recognition), we need to determine whether a text T implies a hypothesis H, e.g., whether or not \"Farmers feed cows animal extracts\" entails \"Cows eat animal extracts\" (T 1 , H 1 ).", "labels": [], "entities": [{"text": "textual entailment recognition", "start_pos": 16, "end_pos": 46, "type": "TASK", "confidence": 0.771435817082723}]}, {"text": "If we want to learn textual entailment classifiers, we need to exploit first-order rules hidden in training instances.", "labels": [], "entities": []}, {"text": "To positively exploit the training instance \"Pediatricians suggest women to feed newborns breast milk\" entails \"Pediatricians suggest that newborns eat breast milk\" (T 2 , H 2 ) for classifying the above example, learning algorithms should learn that the two instances hide the first-order rule \u03c1 = feed Y Z \u2192 Y eat Z . The first-order rule feature space, introduced by), gives high performances in term of accuracy for textual entailment recognition with respect to other features spaces.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 407, "end_pos": 415, "type": "METRIC", "confidence": 0.9982964396476746}, {"text": "textual entailment recognition", "start_pos": 420, "end_pos": 450, "type": "TASK", "confidence": 0.7407717208067576}]}, {"text": "In this paper, we propose a novel class of graphs, the tripartite directed acyclic graphs (tDAGs), that model first-order rule feature spaces and, using this class of graphs, we introduce a novel algorithm for computing the similarity in first-order rewrite rule feature spaces.", "labels": [], "entities": []}, {"text": "The possibility of explicitly representing the first-order feature space as subgraphs of tDAGs makes the derived similarity function a valid kernel.", "labels": [], "entities": []}, {"text": "With respect to the algorithm proposed in, our algorithm is more efficient and it is a valid kernel function.", "labels": [], "entities": []}, {"text": "The paper is organized as follows.", "labels": [], "entities": []}, {"text": "2, we firstly describe tripartite directed acyclic graphs (tDAGs) to model first-order feature (FOR) spaces.", "labels": [], "entities": []}, {"text": "3, we then present the related work.", "labels": [], "entities": []}, {"text": "4, we introduce the similarity function for these FOR spaces.", "labels": [], "entities": [{"text": "similarity", "start_pos": 20, "end_pos": 30, "type": "METRIC", "confidence": 0.9513214826583862}]}, {"text": "This can be used as kernel function in kernel-based machines (e.g., support vector machines).", "labels": [], "entities": []}, {"text": "We then introduce our efficient algorithm for computing the similarity among tDAGs.", "labels": [], "entities": []}, {"text": "5, we analyze the computational efficiency of our algorithm showing that it is extremely more efficient than the algorithm proposed in.", "labels": [], "entities": []}, {"text": "6, we draw conclusions and plan the future work.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section we want to empirically estimate the benefits on the computational cost of our novel algorithm with respect to the algorithm proposed by.", "labels": [], "entities": []}, {"text": "Our algorithm is in principle exponential with respect to the set of alternative constraints C.", "labels": [], "entities": []}, {"text": "Yet, due to what presented in Sec.", "labels": [], "entities": []}, {"text": "4.4 and as the set C * is usually very small, the average complexity is extremely low.", "labels": [], "entities": [{"text": "complexity", "start_pos": 58, "end_pos": 68, "type": "METRIC", "confidence": 0.9812723398208618}]}, {"text": "Following the theory on the average-cost computational complexity (, we estimated the behavior of the algorithms on a large distribution of cases.", "labels": [], "entities": []}, {"text": "We then compared the computing times of the two algorithms.", "labels": [], "entities": []}, {"text": "Finally, as K and K max compute slightly different kernels, we compare the accuracy of the two methods.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 75, "end_pos": 83, "type": "METRIC", "confidence": 0.9995881915092468}]}, {"text": "We implemented both algorithms K(G 1 , G 2 ) and K max (G 1 , G 2 ) in support vector machine classifier (Joachims, 1999) and we experimented with both implementations on the same machine.", "labels": [], "entities": []}, {"text": "We hereafter analyze the results in term of execution time (Sec. 5.1) and in term of accuracy (Sec. 5.2).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 85, "end_pos": 93, "type": "METRIC", "confidence": 0.9995182752609253}]}], "tableCaptions": [{"text": " Table 1: Comparative performances of Kmax and K", "labels": [], "entities": []}]}