{"title": [], "abstractContent": [{"text": "This paper shows that discriminative reranking with an averaged perceptron model yields substantial improvements in realization quality with CCG.", "labels": [], "entities": []}, {"text": "The paper confirms the utility of including language model log probabilities as features in the model, which prior work on discrimina-tive training with log linear models for HPSG realization had called into question.", "labels": [], "entities": [{"text": "HPSG realization", "start_pos": 175, "end_pos": 191, "type": "TASK", "confidence": 0.7100671827793121}]}, {"text": "The perceptron model allows the combination of multiple n-gram models to be optimized and then augmented with both syntactic features and discriminative n-gram features.", "labels": [], "entities": []}, {"text": "The full model yields a state-of-the-art BLEU score of 0.8506 on Section 23 of the CCGbank, to our knowledge the best score reported to date using a reversible , corpus-engineered grammar.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 41, "end_pos": 51, "type": "METRIC", "confidence": 0.9787462055683136}, {"text": "Section 23 of the CCGbank", "start_pos": 65, "end_pos": 90, "type": "DATASET", "confidence": 0.9350717186927795}]}], "introductionContent": [{"text": "In this paper, we show how discriminative training with averaged perceptron models can be used to substantially improve surface realization with Combinatory Categorial Grammar. and have shown that discriminative training with log-linear (maximum entropy) models is effective in realization ranking with Head-Driven Phrase Structure Grammar (.", "labels": [], "entities": [{"text": "surface realization", "start_pos": 120, "end_pos": 139, "type": "TASK", "confidence": 0.7286503911018372}, {"text": "realization ranking", "start_pos": 278, "end_pos": 297, "type": "TASK", "confidence": 0.91151562333107}]}, {"text": "Here we show that averaged perceptron models also perform well for realization ranking with CCG.", "labels": [], "entities": [{"text": "realization", "start_pos": 67, "end_pos": 78, "type": "TASK", "confidence": 0.9703146815299988}]}, {"text": "Averaged perceptron models are very simple, just requiring a decoder and a simple update function, yet despite their simplicity they have been shown to achieve state-of-the-art results in Treebank and CCG parsing) as well as on other NLP tasks.", "labels": [], "entities": [{"text": "CCG parsing", "start_pos": 201, "end_pos": 212, "type": "TASK", "confidence": 0.5948015749454498}]}, {"text": "Along the way, we address the question of whether it is beneficial to incorporate n-gram log probabilities as baseline features in a discriminatively trained realization ranking model.", "labels": [], "entities": []}, {"text": "On a limited domain corpus, Velldal & Oepen found that including the n-gram log probability of each candidate realization as a feature in their log-linear model yielded a substantial boost in ranking performance; on the Penn Treebank (PTB), however, Nakanishi et al. found that including an n-gram log prob feature in their model was of no benefit (with the use of bigrams instead of 4-grams suggested as a possible explanation).", "labels": [], "entities": [{"text": "Penn Treebank (PTB)", "start_pos": 220, "end_pos": 239, "type": "DATASET", "confidence": 0.9825100421905517}]}, {"text": "With these mixed results, the utility of n-gram baseline features for PTBscale discriminative realization ranking has been unclear.", "labels": [], "entities": [{"text": "PTBscale discriminative realization ranking", "start_pos": 70, "end_pos": 113, "type": "TASK", "confidence": 0.6610540747642517}]}, {"text": "In our particular setting, the question is: Do n-gram log prob features improve performance in broad coverage realization ranking with CCG, where factored language models over words, partof-speech tags and supertags have previously been employed ()?", "labels": [], "entities": [{"text": "broad coverage realization ranking", "start_pos": 95, "end_pos": 129, "type": "TASK", "confidence": 0.6645937189459801}]}, {"text": "We answer this question in the affirmative, confirming the results of Velldal & Oepen, despite the differences in corpus size and kind of language model.", "labels": [], "entities": []}, {"text": "We show that including n-gram log prob features in the perceptron model is highly beneficial, as the discriminative models we tested without these features performed worse than the generative baseline.", "labels": [], "entities": []}, {"text": "These findings are inline with  results with incremental parsing with perceptrons, where it is suggested that a generative baseline feature provides the perceptron algorithm with a much better starting point for learning.", "labels": [], "entities": []}, {"text": "We also show that discriminative training allows the combination of multiple n-gram models to be optimized, and that the best model augments the n-gram log prob features with both syntactic features and discriminative n-gram features.", "labels": [], "entities": []}, {"text": "The full model yields a stateof-the-art BLEU () score of 0.8506 on Section 23 of the CCGbank, which is to our knowledge the best score reported to date using a reversible, corpus-engineered grammar.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 40, "end_pos": 44, "type": "METRIC", "confidence": 0.9987452030181885}, {"text": "Section 23 of the CCGbank", "start_pos": 67, "end_pos": 92, "type": "DATASET", "confidence": 0.9136916637420655}]}, {"text": "The paper is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 reviews previous work on broad coverage realization with OpenCCG.", "labels": [], "entities": [{"text": "OpenCCG", "start_pos": 67, "end_pos": 74, "type": "DATASET", "confidence": 0.9288833141326904}]}, {"text": "Section 3 describes our approach to realization reranking with averaged perceptron models.", "labels": [], "entities": [{"text": "realization reranking", "start_pos": 36, "end_pos": 57, "type": "TASK", "confidence": 0.971089631319046}]}, {"text": "Section 4 presents our evaluation of the perceptron models, comparing the results of different feature sets.", "labels": [], "entities": []}, {"text": "Section 5 compares our results to those obtained by related systems and discusses the difficulties of cross-system comparisons.", "labels": [], "entities": []}, {"text": "Finally, Section 6 concludes with a summary and discussion of future directions for research.", "labels": [], "entities": []}], "datasetContent": [{"text": "For the experiments reported below, we used a lexico-grammar extracted from Sections 02-21 of our enhanced CCGbank, a hypertagging model incorporating named entity class features, and a trigram factored language model over words, named entity classes, part-of-speech tags and supertags, as described in the preceding section.", "labels": [], "entities": []}, {"text": "BLEU scores were calculated after removing the underscores between collapsed NEs.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.9710083603858948}]}, {"text": "Events were generated for each training section separately.", "labels": [], "entities": []}, {"text": "As already noted, the hypertagger and POS/supertag language model was trained on all the training sections, while separate word-based models were trained excluding each of the training sections in turn.", "labels": [], "entities": []}, {"text": "Event files for 26530 training sentences with complete realizations were generated in 7 hours and 16 minutes on a cluster using one commodity server per section, with an average n-best list size of 18.2.", "labels": [], "entities": []}, {"text": "Perceptron models were trained on single machines; details for three of the models appear in.", "labels": [], "entities": []}, {"text": "The complete set of models is listed in.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Perceptron Training Details-number of  features in the alphabet, number of features in the  model, training accuracy and training time (hours)  for 10 iterations on a single commodity server", "labels": [], "entities": [{"text": "Perceptron Training Details-number", "start_pos": 10, "end_pos": 44, "type": "TASK", "confidence": 0.6244605282942454}, {"text": "accuracy", "start_pos": 118, "end_pos": 126, "type": "METRIC", "confidence": 0.9891995191574097}]}, {"text": " Table 4: Section 00 Results (98.9% coverage)- percentage of exact match and grammatically  complete realizations, BLEU scores and average  times, in seconds", "labels": [], "entities": [{"text": "coverage", "start_pos": 36, "end_pos": 44, "type": "METRIC", "confidence": 0.9632964730262756}, {"text": "BLEU scores", "start_pos": 115, "end_pos": 126, "type": "METRIC", "confidence": 0.9758742451667786}]}, {"text": " Table 5: Section 23 Results (97.06% coverage)", "labels": [], "entities": [{"text": "coverage", "start_pos": 37, "end_pos": 45, "type": "METRIC", "confidence": 0.9645355343818665}]}, {"text": " Table 6: Examples of realized output", "labels": [], "entities": []}, {"text": " Table 7: PTB Section 23 BLEU scores and exact  match percentages in the NLG literature (Nakan- ishi et al.'s results are for sentences of length 20 or  less)", "labels": [], "entities": [{"text": "PTB Section 23", "start_pos": 10, "end_pos": 24, "type": "DATASET", "confidence": 0.7976531783739725}, {"text": "BLEU", "start_pos": 25, "end_pos": 29, "type": "METRIC", "confidence": 0.9834243655204773}, {"text": "exact  match percentages", "start_pos": 41, "end_pos": 65, "type": "METRIC", "confidence": 0.9162554939587911}, {"text": "NLG literature", "start_pos": 73, "end_pos": 87, "type": "DATASET", "confidence": 0.916050374507904}]}]}