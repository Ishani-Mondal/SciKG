{"title": [{"text": "A Syntactified Direct Translation Model with Linear-time Decoding", "labels": [], "entities": [{"text": "Syntactified Direct Translation", "start_pos": 2, "end_pos": 33, "type": "TASK", "confidence": 0.846809466679891}]}], "abstractContent": [{"text": "Recent syntactic extensions of statistical translation models work with asynchronous context-free or tree-substitution grammar extracted from an automatically parsed parallel corpus.", "labels": [], "entities": [{"text": "statistical translation", "start_pos": 31, "end_pos": 54, "type": "TASK", "confidence": 0.6261188685894012}]}, {"text": "The decoders accompanying these extensions typically exceed quadratic time complexity.", "labels": [], "entities": []}, {"text": "This paper extends the Direct Translation Model 2 (DTM2) with syntax while maintaining linear-time decoding.", "labels": [], "entities": [{"text": "Direct Translation Model 2 (DTM2)", "start_pos": 23, "end_pos": 56, "type": "TASK", "confidence": 0.684925377368927}]}, {"text": "We employ a linear-time parsing algorithm based on an eager, incremental interpretation of Combinatory Categorial Grammar (CCG).", "labels": [], "entities": []}, {"text": "As every input word is processed , the local parsing decisions resolve ambiguity eagerly, by selecting a single supertag-operator pair for extending the dependency parse incrementally.", "labels": [], "entities": []}, {"text": "Alongside translation features extracted from the derived parse tree, we explore syntactic features extracted from the incre-mental derivation process.", "labels": [], "entities": []}, {"text": "Our empirical experiments show that our model significantly outperforms the state-of-the art DTM2 system.", "labels": [], "entities": []}], "introductionContent": [{"text": "Syntactic structure is gradually showing itself to constitute a promising enrichment of state-of-theart Statistical Machine Translation (SMT) models.", "labels": [], "entities": [{"text": "Statistical Machine Translation (SMT)", "start_pos": 104, "end_pos": 141, "type": "TASK", "confidence": 0.8041900297005972}]}, {"text": "However, it would appear that the decoding algorithms are bearing the brunt of this improvement in terms of time and space complexity.", "labels": [], "entities": []}, {"text": "Most recent extensions work with asynchronous context-free or tree-substitution grammar extracted from an automatically parsed parallel corpus.", "labels": [], "entities": []}, {"text": "While attractive in many ways, the decoders that are needed for these types of grammars usually have time and space complexities that are far beyond linear.", "labels": [], "entities": []}, {"text": "Leaving pruning aside, there is a genuine question as to whether syntactic structure necessarily implies more complex decoding algorithms.", "labels": [], "entities": []}, {"text": "This paper shows that this need not necessarily be the case.", "labels": [], "entities": []}, {"text": "In this paper we extend the Direct Translation Model (DTM2) with target language syntax while maintaining linear-time decoding.", "labels": [], "entities": [{"text": "Direct Translation", "start_pos": 28, "end_pos": 46, "type": "TASK", "confidence": 0.6709636449813843}]}, {"text": "With this extension we make three novel contributions to SMT.", "labels": [], "entities": [{"text": "SMT", "start_pos": 57, "end_pos": 60, "type": "TASK", "confidence": 0.9957072734832764}]}, {"text": "Our first contribution is to define a linear-time syntactic parser that works as incrementally as standard SMT decoders (.", "labels": [], "entities": [{"text": "SMT decoders", "start_pos": 107, "end_pos": 119, "type": "TASK", "confidence": 0.9189594686031342}]}, {"text": "At every word position in the target language string, this parser spans at most a single parse-state to augment the translation states in the decoder.", "labels": [], "entities": []}, {"text": "The parse state summarizes previous parsing decisions and imposes constraints on the set of valid future extensions such that a wellformed sequence of parse states unambiguously defines a dependency structure.", "labels": [], "entities": [{"text": "parse state summarizes previous parsing", "start_pos": 4, "end_pos": 43, "type": "TASK", "confidence": 0.5637625753879547}]}, {"text": "This approach is based on an incremental interpretation of the mechanisms of Combinatory Categorial Grammar (CCG).", "labels": [], "entities": [{"text": "Combinatory Categorial Grammar (CCG)", "start_pos": 77, "end_pos": 113, "type": "TASK", "confidence": 0.7152840594450632}]}, {"text": "Our second contribution lies in extending the DMT2 model with a novel set of syntacticallyoriented feature functions.", "labels": [], "entities": []}, {"text": "Crucially, these feature functions concern the derived (partial) dependency structure as well as local aspects of the derivation process, including such information as the CCG lexical categories (supertag), the CCG operators and the intermediate parse states.", "labels": [], "entities": []}, {"text": "This accomplishment is interesting both from a linguistic and technical point of view.", "labels": [], "entities": []}, {"text": "Our third contribution is the extension of the standard phrase-based decoder with the syntactic structure and definition of new grammar-specific pruning techniques that control the size of the search space.", "labels": [], "entities": []}, {"text": "Interestingly, because it is eager, the incremental parser used in this work is hard pushed to perform at a parsing level close to state-of-the-art cubic-time parsers.", "labels": [], "entities": []}, {"text": "Nevertheless, the parsing information it provides allows for significant improvement in translation quality.", "labels": [], "entities": [{"text": "parsing", "start_pos": 18, "end_pos": 25, "type": "TASK", "confidence": 0.9641975164413452}, {"text": "translation", "start_pos": 88, "end_pos": 99, "type": "TASK", "confidence": 0.9669060111045837}]}, {"text": "We test the new model, called the Dependencybased Direct Translation Model (DDTM), on standard Arabic-English translation tasks used in the community, including LDC and GALE data.", "labels": [], "entities": [{"text": "Dependencybased Direct Translation", "start_pos": 34, "end_pos": 68, "type": "TASK", "confidence": 0.6769364873568217}, {"text": "GALE data", "start_pos": 169, "end_pos": 178, "type": "DATASET", "confidence": 0.7933813333511353}]}, {"text": "We show that our DDTM system provides significant improvements in BLEU () and TER () scores over the already extremely competitive DTM2 system.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 66, "end_pos": 70, "type": "METRIC", "confidence": 0.999441921710968}, {"text": "TER () scores", "start_pos": 78, "end_pos": 91, "type": "METRIC", "confidence": 0.9644402861595154}]}, {"text": "We also provide results of manual, qualitative analysis of the system output to provide insight into the quantitative results.", "labels": [], "entities": []}, {"text": "This paper is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 reviews the related work.", "labels": [], "entities": []}, {"text": "Section 3 discusses the DTM2 baseline model.", "labels": [], "entities": [{"text": "DTM2 baseline", "start_pos": 24, "end_pos": 37, "type": "DATASET", "confidence": 0.7799070179462433}]}, {"text": "Section 4 presents the general workings of the incremental CCG parser laying the foundations for integrating it into DTM2.", "labels": [], "entities": []}, {"text": "Section 5 details our own DDTM, the dependencybased extension of the DTM2 model.", "labels": [], "entities": []}, {"text": "Section 6 reports on extensive experiments and their results.", "labels": [], "entities": []}, {"text": "Section 7 provides translation output to shed further detailed insight into the characteristics of the systems.", "labels": [], "entities": [{"text": "translation", "start_pos": 19, "end_pos": 30, "type": "TASK", "confidence": 0.9642781019210815}]}, {"text": "Finally, Section 8 concludes, and discusses future work.", "labels": [], "entities": []}], "datasetContent": [{"text": "We conducted experiments on an Arabic-toEnglish translation task using LDC parallel data and GALE parallel data.", "labels": [], "entities": [{"text": "Arabic-toEnglish translation task", "start_pos": 31, "end_pos": 64, "type": "TASK", "confidence": 0.653683582941691}, {"text": "GALE parallel data", "start_pos": 93, "end_pos": 111, "type": "DATASET", "confidence": 0.6490579148133596}]}, {"text": "We used the UN parallel corpus and LDC news corpus together with the GALE parallel corpus, totaling 7.8M parallel sentences.", "labels": [], "entities": [{"text": "UN parallel corpus", "start_pos": 12, "end_pos": 30, "type": "DATASET", "confidence": 0.7890399495760599}, {"text": "LDC news corpus", "start_pos": 35, "end_pos": 50, "type": "DATASET", "confidence": 0.8730056484540304}, {"text": "GALE parallel corpus", "start_pos": 69, "end_pos": 89, "type": "DATASET", "confidence": 0.8913024266560873}]}, {"text": "The 5-gram Language Model was trained on the English Gigaword Corpus and the English part of the parallel corpus.", "labels": [], "entities": [{"text": "English Gigaword Corpus", "start_pos": 45, "end_pos": 68, "type": "DATASET", "confidence": 0.8413150111834208}]}, {"text": "Our baseline system is similar to the system described in.", "labels": [], "entities": []}, {"text": "We report results on NIST MT05 and NIST MT06 evaluations test sets using BLEU and TER as automatic evaluation metrics.", "labels": [], "entities": [{"text": "NIST MT05", "start_pos": 21, "end_pos": 30, "type": "DATASET", "confidence": 0.7841576039791107}, {"text": "NIST MT06 evaluations test sets", "start_pos": 35, "end_pos": 66, "type": "DATASET", "confidence": 0.9104056835174561}, {"text": "BLEU", "start_pos": 73, "end_pos": 77, "type": "METRIC", "confidence": 0.9984042048454285}, {"text": "TER", "start_pos": 82, "end_pos": 85, "type": "METRIC", "confidence": 0.9856428503990173}]}, {"text": "To train the DDTM model, we use the incremental parser introduced in () to parse the target side of the parallel training data.", "labels": [], "entities": []}, {"text": "Each sentence is associated with supertag, operator and parse-state sequences.", "labels": [], "entities": []}, {"text": "We then train models with different feature sets.", "labels": [], "entities": []}, {"text": "Results: We compared the baseline DTM2) with our DDTM system with the features listed above.", "labels": [], "entities": [{"text": "DDTM", "start_pos": 49, "end_pos": 53, "type": "DATASET", "confidence": 0.8839757442474365}]}, {"text": "We examine the effect of all features on system performance.", "labels": [], "entities": []}, {"text": "In this set of experiments we used LDC parallel data only which is composed of 3.7M sentences and the results are reported on MT05 test set.", "labels": [], "entities": [{"text": "MT05 test set", "start_pos": 126, "end_pos": 139, "type": "DATASET", "confidence": 0.983116070429484}]}, {"text": "Each of the examined systems deploys DTM2 features in addition to a number of newly added syntactic features.", "labels": [], "entities": []}, {"text": "The systems examined are: \u2022 DTM2: Direct Translation model 2 baseline.", "labels": [], "entities": [{"text": "Direct Translation", "start_pos": 34, "end_pos": 52, "type": "TASK", "confidence": 0.6843918263912201}]}, {"text": "\u2022 D-SW: DTM2 + Supertag-Word features.", "labels": [], "entities": []}, {"text": "\u2022 D-SLM: DTM2 + Supertag-Word and supertag n-gram features.", "labels": [], "entities": []}, {"text": "\u2022 D-SO: DTM2+ Supertag-Operator features.", "labels": [], "entities": []}, {"text": "\u2022 D-SS : DTM2 + supertags and states features with parse-state construction.", "labels": [], "entities": []}, {"text": "\u2022 D-WS : DTM2 + words and states features with parse-state construction.", "labels": [], "entities": []}, {"text": "\u2022 D-STLM: DTM2 + state n-gram features with parse-state construction.", "labels": [], "entities": []}, {"text": "\u2022 DDTM: fully fledged system with all features that proved useful above which are: Supertag-Word features, supertag n-gram As shown in, the DTM baseline system demonstrates a very high BLEU score, unsurprisingly given its top-ranked performance in two recent major MT evaluation campaigns.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 185, "end_pos": 195, "type": "METRIC", "confidence": 0.9851042628288269}, {"text": "MT evaluation", "start_pos": 265, "end_pos": 278, "type": "TASK", "confidence": 0.8950275778770447}]}, {"text": "Among the features we tried, supertags and n-gram supertags systems give slight yet statistically insignificant improvements.", "labels": [], "entities": []}, {"text": "On the other hand, the states n-gram sequence features give small yet statistically significant improvements (as calculated via bootstrap resampling).", "labels": [], "entities": []}, {"text": "The D-WS system shows a small degradation in performance, probably due to the fact that the states-words interactions are quite sparse and could not be estimated with good evidence.", "labels": [], "entities": []}, {"text": "Similarly, the D-SO system shows a small degradation in performance.", "labels": [], "entities": []}, {"text": "When we investigated the features types, we found out that all features that deploy the operators had bad effect on the model.", "labels": [], "entities": []}, {"text": "We think this is due to the fact that the operator set is a small set with high evidence in many training instances such that it has low discriminative power on it is own.", "labels": [], "entities": []}, {"text": "However, it implicitly helps in producing the state sequence which proved useful.", "labels": [], "entities": []}, {"text": "We examined a combination of the best features in our DDTM system on a larger training data comprising 7.8M sentences from both NIST and GALE parallel corpora.", "labels": [], "entities": [{"text": "NIST", "start_pos": 128, "end_pos": 132, "type": "DATASET", "confidence": 0.9186564683914185}, {"text": "GALE parallel corpora", "start_pos": 137, "end_pos": 158, "type": "DATASET", "confidence": 0.7375378012657166}]}, {"text": "shows the results on both MT05 and MT06 test sets.", "labels": [], "entities": [{"text": "MT05", "start_pos": 26, "end_pos": 30, "type": "DATASET", "confidence": 0.8844256401062012}, {"text": "MT06 test sets", "start_pos": 35, "end_pos": 49, "type": "DATASET", "confidence": 0.9436749815940857}]}, {"text": "As shown, DDTM significantly outperforms the stateof-the-art baseline system.", "labels": [], "entities": [{"text": "DDTM", "start_pos": 10, "end_pos": 14, "type": "DATASET", "confidence": 0.7755010724067688}]}, {"text": "It is worth noting that DDTM outperforms this baseline even when very large amounts of training data are used.", "labels": [], "entities": []}, {"text": "Despite the fact that the actual scores are not so different, we found that the baseline translation output and the DDTM translation outout are significantly different.", "labels": [], "entities": []}, {"text": "We measured this by calculating the TER between the baseline translation and the DDTM translation for the MT05 test set, and found this to be 25.9%.", "labels": [], "entities": [{"text": "TER", "start_pos": 36, "end_pos": 39, "type": "METRIC", "confidence": 0.9993413090705872}, {"text": "DDTM", "start_pos": 81, "end_pos": 85, "type": "DATASET", "confidence": 0.898938775062561}, {"text": "MT05 test set", "start_pos": 106, "end_pos": 119, "type": "DATASET", "confidence": 0.9469905098279318}]}, {"text": "This large difference has not been realized by the BLEU or TER scores in comparison to the baseline.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 51, "end_pos": 55, "type": "METRIC", "confidence": 0.9990948438644409}, {"text": "TER", "start_pos": 59, "end_pos": 62, "type": "METRIC", "confidence": 0.9904424548149109}]}, {"text": "We believe that this is due to the fact that most changes that match the syntactic constraints do not bring about the best match where the automatic evaluation metrics are concerned.", "labels": [], "entities": []}, {"text": "Accordingly, in the next section we describe the outcome of a detailed manual analysis of the output translations.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: DDTM Results with various features.", "labels": [], "entities": [{"text": "DDTM Results", "start_pos": 10, "end_pos": 22, "type": "DATASET", "confidence": 0.7302300184965134}]}, {"text": " Table 2: DDTM Results on MT05 and MT06.", "labels": [], "entities": [{"text": "MT05", "start_pos": 26, "end_pos": 30, "type": "DATASET", "confidence": 0.8833423256874084}, {"text": "MT06", "start_pos": 35, "end_pos": 39, "type": "DATASET", "confidence": 0.9230785369873047}]}]}