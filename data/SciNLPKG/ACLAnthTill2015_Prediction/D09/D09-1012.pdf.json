{"title": [{"text": "Reverse Engineering of Tree Kernel Feature Spaces", "labels": [], "entities": [{"text": "Reverse Engineering of Tree Kernel Feature Spaces", "start_pos": 0, "end_pos": 49, "type": "TASK", "confidence": 0.8965382831437247}]}], "abstractContent": [{"text": "We present a framework to extract the most important features (tree fragments) from a Tree Kernel (TK) space according to their importance in the target kernel-based machine, e.g. Support Vector Machines (SVMs).", "labels": [], "entities": []}, {"text": "In particular, our mining algorithm selects the most relevant features based on SVM estimated weights and uses this information to automatically infer an explicit representation of the input data.", "labels": [], "entities": []}, {"text": "The explicit features (a) improve our knowledge on the target problem domain and (b) make large-scale learning practical, improving training and test time, while yielding accuracy inline with traditional TK classifiers.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 171, "end_pos": 179, "type": "METRIC", "confidence": 0.9986847043037415}]}, {"text": "Experiments on semantic role labeling and question classification illustrate the above claims.", "labels": [], "entities": [{"text": "semantic role labeling", "start_pos": 15, "end_pos": 37, "type": "TASK", "confidence": 0.766903301080068}, {"text": "question classification", "start_pos": 42, "end_pos": 65, "type": "TASK", "confidence": 0.8345602452754974}]}], "introductionContent": [{"text": "The last decade has seen a massive use of Support Vector Machines (SVMs) for carrying out NLP tasks.", "labels": [], "entities": []}, {"text": "Indeed, their appealing properties such as 1) solid theoretical foundations, 2) robustness to irrelevant features and 3) outperforming accuracy have been exploited to design state-of-the-art language applications.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 135, "end_pos": 143, "type": "METRIC", "confidence": 0.9624510407447815}]}, {"text": "More recently, kernel functions, which implicitly represent data in some high dimensional space, have been employed to study and further improve many natural language systems, e.g. (), (,,,,),,, (), (), ( ,.", "labels": [], "entities": []}, {"text": "Unfortunately, the benefit to easily and effectively model the target linguistic phenomena is reduced by the the implicit nature of the kernel space, which prevents to directly observe the most relevant features.", "labels": [], "entities": []}, {"text": "As a consequence, even very accurate models generally fail in providing useful feedback for improving our understanding of the problems at study.", "labels": [], "entities": []}, {"text": "Moreover, the computational burden induced by high dimensional kernels makes the application of SVMs to large corpora still more problematic.", "labels": [], "entities": []}, {"text": "In, we proposed a feature extraction algorithm for Tree Kernel (TK) spaces, which selects the most relevant features (tree fragments) according to the gradient components (weight vector) of the hyperplane learnt by an SVM, inline with current research, e.g.).", "labels": [], "entities": [{"text": "feature extraction", "start_pos": 18, "end_pos": 36, "type": "TASK", "confidence": 0.7622766196727753}]}, {"text": "In particular, we provided algorithmic solutions to deal with the huge dimensionality and, consequently, high computational complexity of the fragment space.", "labels": [], "entities": []}, {"text": "Our experimental results showed that our approach reduces learning and classification processing time leaving the accuracy unchanged.", "labels": [], "entities": [{"text": "classification processing", "start_pos": 71, "end_pos": 96, "type": "TASK", "confidence": 0.936010092496872}, {"text": "accuracy", "start_pos": 114, "end_pos": 122, "type": "METRIC", "confidence": 0.9995139837265015}]}, {"text": "In this paper, we present anew version of such algorithm which, under the same parameterization, is almost three times as fast while producing the same results.", "labels": [], "entities": []}, {"text": "Most importantly, we explored tree fragment spaces for two interesting natural language tasks: Semantic Role Labeling (SRL) and Question Classification (QC).", "labels": [], "entities": [{"text": "Semantic Role Labeling (SRL)", "start_pos": 95, "end_pos": 123, "type": "TASK", "confidence": 0.7649212082227071}, {"text": "Question Classification (QC)", "start_pos": 128, "end_pos": 156, "type": "TASK", "confidence": 0.817136263847351}]}, {"text": "The results show that: (a) on large data sets, our approach can improve training and test time while yielding almost unaffected classification accuracy, and (b) our framework can effectively exploit the ability of TKs and SVMs to, respectively, generate and recognize relevant structured features.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 143, "end_pos": 151, "type": "METRIC", "confidence": 0.7972973585128784}]}, {"text": "In particular, we (i) study in more detail the relevant fragments identfied for the boundary classification task of SRL, (ii) closely observe the most relevant fragments for each QC class and (iii) look at the diverse syntactic patterns characterizing each ques-tion category.", "labels": [], "entities": [{"text": "boundary classification task of SRL", "start_pos": 84, "end_pos": 119, "type": "TASK", "confidence": 0.6603545844554901}]}, {"text": "The rest of the paper is structured as follows: Section 2 will briefly review SVMs and TK functions; Section 3 will detail our proposal for the linearization of a TK feature space; Section 4 will review previous work on related subjects; Section 5 will detail the outcome of our experiments, and Section 6 will discuss some relevant aspects of the evaluation; finally, in Section 7 we will draw our conclusions.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluated the capability of our model to extract relevant features on two data sets: the CoNLL 2005 shared task on Semantic Role Labeling (SRL), and the Question Classification (QC) task based on data from the TREC 10 QA competition).", "labels": [], "entities": [{"text": "CoNLL 2005 shared task on Semantic Role Labeling (SRL)", "start_pos": 92, "end_pos": 146, "type": "TASK", "confidence": 0.8269528258930553}, {"text": "Question Classification (QC) task", "start_pos": 156, "end_pos": 189, "type": "TASK", "confidence": 0.8132042785485586}, {"text": "TREC 10 QA competition", "start_pos": 213, "end_pos": 235, "type": "DATASET", "confidence": 0.7762457877397537}]}, {"text": "The next sections will detail the setup and outcome of the two sets of experiments.", "labels": [], "entities": []}, {"text": "All the experiments were run on a machine equipped with 4 Intel R Xeon R CPUs clocked at 1.6 GHz and 4 GB of RAM.", "labels": [], "entities": []}, {"text": "As a supervised learning framework we used SVM-Light-TK 1 , which extends the SVM-Light optimizer) with tree kernel support.", "labels": [], "entities": []}, {"text": "For each classification task, we compare the accuracy of a vanilla SST classifier against the corresponding linearized SST classifier (SST \u2113 ).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 45, "end_pos": 53, "type": "METRIC", "confidence": 0.999508261680603}]}, {"text": "For KSL and SST training we used the default decay factor \u03bb = 0.4.", "labels": [], "entities": [{"text": "KSL", "start_pos": 4, "end_pos": 7, "type": "TASK", "confidence": 0.6495034098625183}, {"text": "SST", "start_pos": 12, "end_pos": 15, "type": "TASK", "confidence": 0.9071803689002991}]}, {"text": "For ESL, we use a non-normalized, linear kernel.", "labels": [], "entities": [{"text": "ESL", "start_pos": 4, "end_pos": 7, "type": "TASK", "confidence": 0.7361292243003845}]}, {"text": "No further parametrization of the learning algorithms is carried out.", "labels": [], "entities": []}, {"text": "Indeed, our focus is on showing that, under the same conditions, our linearized tree kernel can be as accurate as the original kernel, and choosing of parameters may just bias such test.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Number of positive training (Tr + ) and test  (Te + ) examples in the SRL dataset. Accuracy of  the non-linearized (SST) and linearized (SST \u2113 ) bi- nary classifiers (i.e. BC, A0, . . . A5) is F 1 measure.  Accuracy of RM is the percentage of correct class  assignments.", "labels": [], "entities": [{"text": "SRL dataset", "start_pos": 80, "end_pos": 91, "type": "DATASET", "confidence": 0.8329988420009613}, {"text": "Accuracy", "start_pos": 93, "end_pos": 101, "type": "METRIC", "confidence": 0.995877742767334}, {"text": "F 1", "start_pos": 203, "end_pos": 206, "type": "METRIC", "confidence": 0.9794524908065796}, {"text": "Accuracy", "start_pos": 217, "end_pos": 225, "type": "METRIC", "confidence": 0.9985400438308716}, {"text": "RM", "start_pos": 229, "end_pos": 231, "type": "METRIC", "confidence": 0.7041518688201904}]}, {"text": " Table 2: Number of positive training (Tr + ) and test  (Te + ) examples in the QA dataset. Accuracy of  the non-linearized (SST) and linearized (SST \u2113 ) bi- nary classifiers is F 1 measure. Overall accuracy is  the percentage of correct class assignments.", "labels": [], "entities": [{"text": "QA dataset", "start_pos": 80, "end_pos": 90, "type": "DATASET", "confidence": 0.8546852767467499}, {"text": "Accuracy", "start_pos": 92, "end_pos": 100, "type": "METRIC", "confidence": 0.9970898032188416}, {"text": "F 1", "start_pos": 178, "end_pos": 181, "type": "METRIC", "confidence": 0.9735355377197266}, {"text": "accuracy", "start_pos": 199, "end_pos": 207, "type": "METRIC", "confidence": 0.9994012117385864}]}]}