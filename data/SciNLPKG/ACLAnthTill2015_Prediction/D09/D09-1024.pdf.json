{"title": [{"text": "Improved Word Alignment with Statistics and Linguistic Heuristics", "labels": [], "entities": [{"text": "Improved Word Alignment", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.8662169377009074}]}], "abstractContent": [{"text": "We present a method to align words in a bitext that combines elements of a traditional statistical approach with linguistic knowledge.", "labels": [], "entities": []}, {"text": "We demonstrate this approach for Arabic-English, using an alignment lexicon produced by a statistical word aligner, as well as linguistic resources ranging from an English parser to heuristic alignment rules for function words.", "labels": [], "entities": []}, {"text": "These linguistic heuristics have been generalized from a development corpus of 100 parallel sentences.", "labels": [], "entities": []}, {"text": "Our aligner, UALIGN, outperforms both the commonly used GIZA++ aligner and the state-of-the-art LEAF aligner on F-measure and produces superior scores in end-to-end statistical machine translation, +1.3 BLEU points over GIZA++, and +0.7 over LEAF.", "labels": [], "entities": [{"text": "UALIGN", "start_pos": 13, "end_pos": 19, "type": "METRIC", "confidence": 0.9807985424995422}, {"text": "GIZA++ aligner", "start_pos": 56, "end_pos": 70, "type": "DATASET", "confidence": 0.8606949249903361}, {"text": "statistical machine translation", "start_pos": 165, "end_pos": 196, "type": "TASK", "confidence": 0.5733363131682078}, {"text": "BLEU", "start_pos": 203, "end_pos": 207, "type": "METRIC", "confidence": 0.9933639764785767}]}], "introductionContent": [{"text": "Word alignment is a critical component in training statistical machine translation systems and has received a significant amount of research, for example,, including work leveraging syntactic parse trees, e.g.,.", "labels": [], "entities": [{"text": "Word alignment", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.733145147562027}, {"text": "statistical machine translation", "start_pos": 51, "end_pos": 82, "type": "TASK", "confidence": 0.5934264361858368}]}, {"text": "Word alignment is also a required first step in other algorithms such as for learning sub-sentential phrase pairs ( or the generation of parallel treebanks).", "labels": [], "entities": [{"text": "Word alignment", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.7419295310974121}, {"text": "learning sub-sentential phrase pairs", "start_pos": 77, "end_pos": 113, "type": "TASK", "confidence": 0.6447711139917374}]}, {"text": "Yet word alignment precision remains surprisingly low, under 80% for state-of-the-art aligners on not closely related language pairs.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 4, "end_pos": 18, "type": "TASK", "confidence": 0.7539516985416412}, {"text": "precision", "start_pos": 19, "end_pos": 28, "type": "METRIC", "confidence": 0.9378804564476013}]}, {"text": "Consider the following Arabic/English sentence pair with alignments built by the statistical word aligner LEAF: Gloss: Won(1) Thai Paradorn Srichaphan(1) on/to(2) Australian Jason(2) Stoltenberg(3) 6(4) -4(5) and(3) 6(4) -4(5), and Czech Ji\u0159\u00ed(7) Van\u011bk on/to German(6) Lars Burgsm\u00fcller 6(4) -4 and(3) 6(4) -4 Bitext English: Thailand 's(1) Baradorn Srichfan(1) beat(2) Australian Gayson(1) Stultenberg(3) 6(4) -6(4) 6(4) -4(5) , Czech player(1) Pierre(1) Vanic(7) beat(6) Germany(6) 's Lars Burgsmuller 6 -4 6 -4 In the example above, words with the same index in the gloss for Arabic and the English are aligned to each other, alignment errors are underlined, translation errors are in italics.", "labels": [], "entities": []}, {"text": "For example, the Arabic words for won and Srichaphan are aligned with the English words 's, Srichfan, Gayson, player and Pierre.", "labels": [], "entities": [{"text": "Pierre", "start_pos": 121, "end_pos": 127, "type": "METRIC", "confidence": 0.9806914329528809}]}, {"text": "As reflected in the example above, typical alignment problems include \ud97b\udf59 words that change sentence position between languages, such as verbs, which in Arabic are often sentence-initial (e.g. won/beat in the example above) \ud97b\udf59 function words without a clear and explicit equivalent in the other language (e.g. the Arabic \ud97b\udf59/and in the example above) \ud97b\udf59 lack of robustness with respect to poor translations (e.g. Gayson Stultenberg instead of Jason Stoltenberg) or bad sentence alignment.", "labels": [], "entities": [{"text": "sentence alignment", "start_pos": 463, "end_pos": 481, "type": "TASK", "confidence": 0.7373717129230499}]}, {"text": "We believe we can overcome such problems with the increased use of linguistically based heuristics.", "labels": [], "entities": []}, {"text": "We can model typical word order differences between English and Arabic using English parse trees and a few Arabic-specific phrase reordering heuristics.", "labels": [], "entities": []}, {"text": "We can narrow the space of possible alignment candidates for function words using English parse trees and a few heuristics for each type of function word.", "labels": [], "entities": []}, {"text": "These heuristics have been developed using a development corpus of 100 parallel sentences.", "labels": [], "entities": []}, {"text": "The heuristics are generalizations based on patterns of misaligned words, misaligned with respect to a Gold Standard alignment for that development corpus.", "labels": [], "entities": []}, {"text": "The following sections describe how our word aligner works, first how relatively reliable content words are aligned, and then how function words and any remaining content words are aligned, with a brief discussion of an interesting issue relating to the Gold Standard we used.", "labels": [], "entities": [{"text": "Gold Standard", "start_pos": 254, "end_pos": 267, "type": "DATASET", "confidence": 0.9784827530384064}]}, {"text": "Finally we present evaluations on word alignment accuracy as well as the impact on end-to-end machine translation quality.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 34, "end_pos": 48, "type": "TASK", "confidence": 0.8298168778419495}, {"text": "accuracy", "start_pos": 49, "end_pos": 57, "type": "METRIC", "confidence": 0.8250102996826172}, {"text": "machine translation", "start_pos": 94, "end_pos": 113, "type": "TASK", "confidence": 0.6383102983236313}]}], "datasetContent": [{"text": "We evaluated our word aligner in terms of both alignment accuracy and its impact on an end-toend machine translation system.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 57, "end_pos": 65, "type": "METRIC", "confidence": 0.9181663393974304}, {"text": "end-toend machine translation", "start_pos": 87, "end_pos": 116, "type": "TASK", "confidence": 0.6423772176106771}]}, {"text": "We evaluated our word aligner against a Gold Standard distributed by LDC.", "labels": [], "entities": [{"text": "Gold Standard distributed by LDC", "start_pos": 40, "end_pos": 72, "type": "DATASET", "confidence": 0.9227437496185302}]}, {"text": "The human alignments of the sentences in this Gold Standard are based on the 2006 GALE Guidelines for Arabic Word Alignment Annotation.", "labels": [], "entities": [{"text": "GALE Guidelines", "start_pos": 82, "end_pos": 97, "type": "DATASET", "confidence": 0.8803334534168243}, {"text": "Arabic Word Alignment Annotation", "start_pos": 102, "end_pos": 134, "type": "TASK", "confidence": 0.6576806753873825}]}, {"text": "Both the 100-sentence development set and the separate 837-sentence test set are Arabic newswire sentences from LDC2006E86.", "labels": [], "entities": [{"text": "LDC2006E86", "start_pos": 112, "end_pos": 122, "type": "DATASET", "confidence": 0.7359976172447205}]}, {"text": "The test set includes only sentences for which our English parser could produce a parse tree, which effectively excluded a few very long sentences.", "labels": [], "entities": []}, {"text": "In the first set of experiments, we compare two settings of our UALIGN system with other aligners, GIZA++ (Union) and LEAF (with 2 iterations) (.", "labels": [], "entities": [{"text": "GIZA", "start_pos": 99, "end_pos": 103, "type": "METRIC", "confidence": 0.9325807094573975}, {"text": "LEAF", "start_pos": 118, "end_pos": 122, "type": "METRIC", "confidence": 0.9931049942970276}]}, {"text": "The GIZA++ aligner is based on IBM Model 4 ().", "labels": [], "entities": []}, {"text": "We chose GIZA Union for our comparison, because it led to a higher BLEU score for our overall MT system than other GIZA variants such as GIZA Intersect and Grow-Diag.", "labels": [], "entities": [{"text": "GIZA Union", "start_pos": 9, "end_pos": 19, "type": "DATASET", "confidence": 0.8010734617710114}, {"text": "BLEU score", "start_pos": 67, "end_pos": 77, "type": "METRIC", "confidence": 0.9797492027282715}, {"text": "MT", "start_pos": 94, "end_pos": 96, "type": "TASK", "confidence": 0.9715334177017212}]}, {"text": "The two settings of our system vary in the style on how to align orphan prepositions.", "labels": [], "entities": []}, {"text": "Besides precision, recall and (balanced) F-measure, we also include an F-measure variant strongly biased towards recall (\ud97b\udf59=0.1), which found to be best to tune their LEAF aligner for maximum MT accuracy.", "labels": [], "entities": [{"text": "precision", "start_pos": 8, "end_pos": 17, "type": "METRIC", "confidence": 0.9994457364082336}, {"text": "recall", "start_pos": 19, "end_pos": 25, "type": "METRIC", "confidence": 0.9989892840385437}, {"text": "F-measure", "start_pos": 41, "end_pos": 50, "type": "METRIC", "confidence": 0.9879871010780334}, {"text": "F-measure", "start_pos": 71, "end_pos": 80, "type": "METRIC", "confidence": 0.9710568785667419}, {"text": "recall", "start_pos": 113, "end_pos": 119, "type": "METRIC", "confidence": 0.9984819293022156}, {"text": "LEAF", "start_pos": 166, "end_pos": 170, "type": "METRIC", "confidence": 0.9583655595779419}, {"text": "MT", "start_pos": 191, "end_pos": 193, "type": "TASK", "confidence": 0.824560284614563}, {"text": "accuracy", "start_pos": 194, "end_pos": 202, "type": "METRIC", "confidence": 0.7813518047332764}]}, {"text": "GIZA++ and LEAF alignments are based on a parallel training corpus of 6.6 million sentence pairs, incl.", "labels": [], "entities": []}, {"text": "the LDC2006E86 set mentioned above.", "labels": [], "entities": [{"text": "LDC2006E86 set", "start_pos": 4, "end_pos": 18, "type": "DATASET", "confidence": 0.9051697552204132}]}, {"text": "The ultimate test fora word aligner is to measure its impact on an end-to-end machine translation system.", "labels": [], "entities": []}, {"text": "For this we aligned 170,863 pairs of Arabic/English newswire sentences from LDC, trained a state-of-the-art syntax-based statistical machine translation system () on these sentences and alignments, and measured BLEU scores () on a separate set of 1298 newswire test sentences.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 121, "end_pos": 152, "type": "TASK", "confidence": 0.6687400937080383}, {"text": "BLEU scores", "start_pos": 211, "end_pos": 222, "type": "METRIC", "confidence": 0.978428989648819}]}, {"text": "Besides swapping in anew set of alignments for the same set of training sentences, and automatically retuning the parameters of the translation system for each set of alignments, no other changes or adjustments were made to the existing MT system.", "labels": [], "entities": [{"text": "MT", "start_pos": 237, "end_pos": 239, "type": "TASK", "confidence": 0.9641191363334656}]}, {"text": "In the first set of experiments, we compare two settings of our UALIGN system with other aligners, again GIZA++ (Union) and LEAF (with 2 iterations).", "labels": [], "entities": [{"text": "GIZA", "start_pos": 105, "end_pos": 109, "type": "METRIC", "confidence": 0.9591630697250366}, {"text": "LEAF", "start_pos": 124, "end_pos": 128, "type": "METRIC", "confidence": 0.9952378273010254}]}, {"text": "The two settings vary in the alignment lexicon that the UALIGN aligner uses as input.", "labels": [], "entities": []}, {"text": "With a BLEU score of 48.7, UALIGN using a LEAF alignment-lexicon is significantly better than both GIZA (+1.3) and LEAF (+0.7).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 7, "end_pos": 11, "type": "METRIC", "confidence": 0.9996685981750488}, {"text": "GIZA", "start_pos": 99, "end_pos": 103, "type": "METRIC", "confidence": 0.8459796905517578}]}, {"text": "This and other significance assertions in this paper are based on paired bootstrap resampling tests with 95% confidence.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Alignment precision, recall, F-measure  (\ud97b\udf59=0.5), F-measure(\ud97b\udf59=0.1) for different aligners;  with UALIGN using LEAF alignment lexicon.", "labels": [], "entities": [{"text": "Alignment", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.8600102663040161}, {"text": "precision", "start_pos": 20, "end_pos": 29, "type": "METRIC", "confidence": 0.6542237997055054}, {"text": "recall", "start_pos": 31, "end_pos": 37, "type": "METRIC", "confidence": 0.9993290901184082}, {"text": "F-measure", "start_pos": 39, "end_pos": 48, "type": "METRIC", "confidence": 0.9895386695861816}, {"text": "F-measure", "start_pos": 59, "end_pos": 68, "type": "METRIC", "confidence": 0.9936981797218323}]}, {"text": " Table 2: Alignment precision, recall, F-measure  (\ud97b\udf59=0.5), F-measure(\ud97b\udf59=0.1), all of UALIGN, for  different alignment styles, different input align- ment lexicons.", "labels": [], "entities": [{"text": "precision", "start_pos": 20, "end_pos": 29, "type": "METRIC", "confidence": 0.5671707391738892}, {"text": "recall", "start_pos": 31, "end_pos": 37, "type": "METRIC", "confidence": 0.9990721940994263}, {"text": "F-measure", "start_pos": 39, "end_pos": 48, "type": "METRIC", "confidence": 0.9849351644515991}, {"text": "F-measure", "start_pos": 59, "end_pos": 68, "type": "METRIC", "confidence": 0.9929516911506653}, {"text": "UALIGN", "start_pos": 84, "end_pos": 90, "type": "METRIC", "confidence": 0.9022694230079651}]}, {"text": " Table 3: Percentages of Arabic and English words  aligned", "labels": [], "entities": []}, {"text": " Table 4: Impact of sub-components on alignment  precision, recall, F-measure, with GS-style attach- ments, based on the LEAF alignment lexicon.", "labels": [], "entities": [{"text": "precision", "start_pos": 49, "end_pos": 58, "type": "METRIC", "confidence": 0.9500430226325989}, {"text": "recall", "start_pos": 60, "end_pos": 66, "type": "METRIC", "confidence": 0.9994100332260132}, {"text": "F-measure", "start_pos": 68, "end_pos": 77, "type": "METRIC", "confidence": 0.9970472455024719}, {"text": "GS-style attach- ments", "start_pos": 84, "end_pos": 106, "type": "METRIC", "confidence": 0.8839595317840576}, {"text": "LEAF alignment lexicon", "start_pos": 121, "end_pos": 143, "type": "DATASET", "confidence": 0.7467863957087199}]}, {"text": " Table 5: BLEU scores in end-to-end statistical MT  system based on different aligners. Both UALIGN  variants use MT-style alignments.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9989145994186401}, {"text": "MT", "start_pos": 48, "end_pos": 50, "type": "TASK", "confidence": 0.9567001461982727}]}, {"text": " Table 6: BLEU scores in end-to-end statistical MT  system based on different alignment styles for or- phan prepositions. Both UALIGN variants use a  LEAF alignment lexicon.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9989781379699707}, {"text": "MT", "start_pos": 48, "end_pos": 50, "type": "TASK", "confidence": 0.9431617856025696}]}]}