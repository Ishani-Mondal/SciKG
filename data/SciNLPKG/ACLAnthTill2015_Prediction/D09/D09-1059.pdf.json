{"title": [], "abstractContent": [{"text": "We present an inexact search algorithm for the problem of predicting a two-layered dependency graph.", "labels": [], "entities": []}, {"text": "The algorithm is based on a k-best version of the standard cubic-time search algorithm for projective dependency parsing, which is used as the backbone of abeam search procedure.", "labels": [], "entities": [{"text": "projective dependency parsing", "start_pos": 91, "end_pos": 120, "type": "TASK", "confidence": 0.6322842439015707}]}, {"text": "This allows us to handle the complex non-local feature dependencies occurring in bistratal parsing if we model the interde-pendency between the two layers.", "labels": [], "entities": [{"text": "bistratal parsing", "start_pos": 81, "end_pos": 98, "type": "TASK", "confidence": 0.7178215682506561}]}, {"text": "We apply the algorithm to the syntactic-semantic dependency parsing task of the CoNLL-2008 Shared Task, and we obtain a competitive result equal to the highest published fora system that jointly learns syntactic and semantic structure.", "labels": [], "entities": [{"text": "syntactic-semantic dependency parsing task", "start_pos": 30, "end_pos": 72, "type": "TASK", "confidence": 0.7350936383008957}, {"text": "CoNLL-2008 Shared Task", "start_pos": 80, "end_pos": 102, "type": "DATASET", "confidence": 0.8488752444585165}]}], "introductionContent": [{"text": "Numerous linguistic theories assume a multistratal model of linguistic structure, such as a layer of surface syntax, deep syntax, and shallow semantics.", "labels": [], "entities": []}, {"text": "Examples include Meaning-Text Theory, Discontinuous Grammar (Buch-), Extensible Dependency Grammar (), and the Functional Generative Description () which forms the theoretical foundation of the Prague Dependency Treebank.", "labels": [], "entities": [{"text": "Meaning-Text Theory", "start_pos": 17, "end_pos": 36, "type": "TASK", "confidence": 0.7891738712787628}, {"text": "Prague Dependency Treebank", "start_pos": 194, "end_pos": 220, "type": "DATASET", "confidence": 0.9727772275606791}]}, {"text": "In the statistical NLP community, the most widely used grammatical resource is the Penn Treebank (.", "labels": [], "entities": [{"text": "Penn Treebank", "start_pos": 83, "end_pos": 96, "type": "DATASET", "confidence": 0.9955645501613617}]}, {"text": "This is a purely syntactic resource, but we can also include this treebank in the category of multistratal resources since the PropBank () and NomBank () projects have annotated shallow semantic structures on top of it.", "labels": [], "entities": []}, {"text": "Dependency-converted versions of the Penn Treebank, PropBank and NomBank were used in the), in which the task of the participants was to produce a bistratal dependency structure consisting of surface syntax and shallow semantics.", "labels": [], "entities": [{"text": "Penn Treebank", "start_pos": 37, "end_pos": 50, "type": "DATASET", "confidence": 0.994767814874649}, {"text": "PropBank", "start_pos": 52, "end_pos": 60, "type": "DATASET", "confidence": 0.8237598538398743}]}, {"text": "Producing a consistent multistratal structure is a conceptually and computationally complex task, and most previous methods have employed a purely pipeline-based decomposition of the task.", "labels": [], "entities": []}, {"text": "This includes the majority of work on shallow semantic analysis (, inter alia).", "labels": [], "entities": [{"text": "shallow semantic analysis", "start_pos": 38, "end_pos": 63, "type": "TASK", "confidence": 0.8020379543304443}]}, {"text": "Nevertheless, since it is obvious that syntax and semantics are highly interdependent, it has repeatedly been suggested that the problems of syntactic and semantic analysis should be carried out simultaneously rather than in a pipeline, and that modeling the interdependency between syntax and semantics would improve the quality of all the substructures.", "labels": [], "entities": [{"text": "syntactic and semantic analysis", "start_pos": 141, "end_pos": 172, "type": "TASK", "confidence": 0.6570548191666603}]}, {"text": "The purpose of the CoNLL-2008 Shared Task was to study the feasibility of a joint analysis of syntax and semantics, and while most participating systems used a pipeline-based approach to the problem, there were a number of contributions that attempted to take the interdependence between syntax and semantics into account.", "labels": [], "entities": []}, {"text": "The top-performing system in the task) applied a very simple reranking scheme by means of a k-best syntactic output, similar to previous attempts) to improve semantic role labeling performance by using mul-tiple parses.", "labels": [], "entities": [{"text": "semantic role labeling", "start_pos": 158, "end_pos": 180, "type": "TASK", "confidence": 0.6337543924649557}]}, {"text": "The system by extended previous stack-based algorithms for dependency parsing by using two separate stacks to build the syntactic and semantic graphs.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 59, "end_pos": 77, "type": "TASK", "confidence": 0.8275030553340912}]}, {"text": "proposed a model that simultaneously predicts syntactic and semantic links, but since its search algorithm could not take the syntactic-semantic interdependencies into account, a pre-parsing step was still needed.", "labels": [], "entities": []}, {"text": "In addition, before the CoNLL-2008 shared task there have been a few attempts to jointly learn syntactic and semantic structure; for instance,  appended semantic role labels to the phrase tags in a constituent treebank and applied a conventional constituent parser to predict constituent structure and semantic roles.", "labels": [], "entities": []}, {"text": "In this paper, we propose anew approximate search method for bistratal dependency analysis.", "labels": [], "entities": [{"text": "bistratal dependency analysis", "start_pos": 61, "end_pos": 90, "type": "TASK", "confidence": 0.7632578611373901}]}, {"text": "The search method is based on abeam search procedure that extends a k-best version of the standard cubic-time search algorithm for projective dependency parsing.", "labels": [], "entities": [{"text": "projective dependency parsing", "start_pos": 131, "end_pos": 160, "type": "TASK", "confidence": 0.6309179464975992}]}, {"text": "This is similar to the search method for constituent parsing used by, who referred to it as cube pruning, inspired by an idea from machine translation decoding.", "labels": [], "entities": [{"text": "constituent parsing", "start_pos": 41, "end_pos": 60, "type": "TASK", "confidence": 0.6838236600160599}]}, {"text": "The cube pruning approach, which is normally used to solve the arg max problem, was also recently extended to summing problems, which is needed in some learning algorithms.", "labels": [], "entities": [{"text": "summing", "start_pos": 110, "end_pos": 117, "type": "TASK", "confidence": 0.98883455991745}]}, {"text": "We apply the algorithm on the CoNLL-2008 Shared Task data, and obtain the same evaluation score as the best previously published system that simultaneously learns syntactic and semantic structure ().", "labels": [], "entities": [{"text": "CoNLL-2008 Shared Task data", "start_pos": 30, "end_pos": 57, "type": "DATASET", "confidence": 0.9085981547832489}]}], "datasetContent": [{"text": "We evaluated the performance of our system on the test set from the CoNLL-2008 shared task, which consists of section 23 of the WSJ part of the Penn Treebank, as well as a small part of the Brown corpus.", "labels": [], "entities": [{"text": "CoNLL-2008 shared task", "start_pos": 68, "end_pos": 90, "type": "DATASET", "confidence": 0.8285772005716959}, {"text": "WSJ part of the Penn Treebank", "start_pos": 128, "end_pos": 157, "type": "DATASET", "confidence": 0.9285270174344381}, {"text": "Brown corpus", "start_pos": 190, "end_pos": 202, "type": "DATASET", "confidence": 0.9739027619361877}]}, {"text": "A beam width k of 4 was used in this experiment.", "labels": [], "entities": []}, {"text": "shows the results of the evaluation.", "labels": [], "entities": []}, {"text": "The table shows the three most important scores computed by the official evaluation script: labeled syntactic dependency accuracy (LAS), labeled semantic dependency F 1 -measure (Sem.", "labels": [], "entities": [{"text": "accuracy (LAS)", "start_pos": 121, "end_pos": 135, "type": "METRIC", "confidence": 0.9307986497879028}, {"text": "F 1 -measure", "start_pos": 165, "end_pos": 177, "type": "METRIC", "confidence": 0.8936288505792618}]}, {"text": "F1), and the macro-averaged F 1 -measure, a weighted combination of the syntactic and semantic scores (M. F1).", "labels": [], "entities": [{"text": "F 1 -measure", "start_pos": 28, "end_pos": 40, "type": "METRIC", "confidence": 0.9108350425958633}, {"text": "M. F1)", "start_pos": 103, "end_pos": 109, "type": "METRIC", "confidence": 0.6136135856310526}]}, {"text": "Our result is competitive; we obtain the same macro F1 as the newly published result by, which is the highest published figure fora joint syntactic-semantic parser so far.", "labels": [], "entities": [{"text": "F1", "start_pos": 52, "end_pos": 54, "type": "METRIC", "confidence": 0.7646862268447876}]}, {"text": "Importantly, our system clearly outperforms the system by , which is the most similar system in problem modeling, but which uses a different search strategy.", "labels": [], "entities": [{"text": "problem modeling", "start_pos": 96, "end_pos": 112, "type": "TASK", "confidence": 0.7761658728122711}]}, {"text": "Since the search procedure is inexact, it is important to quantify roughly how much of a detrimental impact the approximation has on the parsing quality.", "labels": [], "entities": []}, {"text": "We studied the influence of the beam width parameter k on the performance of the parser.", "labels": [], "entities": []}, {"text": "The results on the development set can be seen in.", "labels": [], "entities": []}, {"text": "As can be seen, a modest increase in performance can be obtained by increasing the beam width, at the cost of increased parsing time.", "labels": [], "entities": [{"text": "parsing", "start_pos": 120, "end_pos": 127, "type": "TASK", "confidence": 0.9498142004013062}]}, {"text": "In addition, to have a rough indication of the impact of search errors on the quality of the parses, we computed the fraction of sentences where the gold-standard parse had a higher score according to the model than the parse returned by the search . shows the results of this experiment.", "labels": [], "entities": []}, {"text": "This suggests that the search errors, although they clearly have an impact, are not the major source of errors, even with small beam widths.: Fraction of sentences in the development set where the gold-standard parse has a higher score than the parse returned by the search procedure.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Results of published joint syntactic- semantic parsers on the CoNLL-2008 test set.", "labels": [], "entities": [{"text": "CoNLL-2008 test set", "start_pos": 72, "end_pos": 91, "type": "DATASET", "confidence": 0.9674040079116821}]}, {"text": " Table 3. As can be seen, a modest increase  in performance can be obtained by increasing the  beam width, at the cost of increased parsing time.", "labels": [], "entities": [{"text": "parsing", "start_pos": 132, "end_pos": 139, "type": "TASK", "confidence": 0.9507867693901062}]}, {"text": " Table 3: Influence of beam width on parsing accu- racy.", "labels": [], "entities": [{"text": "parsing accu- racy", "start_pos": 37, "end_pos": 55, "type": "TASK", "confidence": 0.7716719210147858}]}, {"text": " Table 4: Fraction of sentences in the development  set where the gold-standard parse has a higher  score than the parse returned by the search pro- cedure.", "labels": [], "entities": []}, {"text": " Table 5: The three most significant bottlenecks  and their fraction of the total runtime.", "labels": [], "entities": []}]}