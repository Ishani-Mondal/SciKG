{"title": [{"text": "Joint Optimization for Machine Translation System Combination", "labels": [], "entities": [{"text": "Machine Translation System Combination", "start_pos": 23, "end_pos": 61, "type": "TASK", "confidence": 0.8606568574905396}]}], "abstractContent": [{"text": "System combination has emerged as a powerful method for machine translation (MT).", "labels": [], "entities": [{"text": "machine translation (MT)", "start_pos": 56, "end_pos": 80, "type": "TASK", "confidence": 0.8312165319919587}]}, {"text": "This paper pursues a joint optimization strategy for combining outputs from multiple MT systems, where word alignment, ordering, and lexical selection decisions are made jointly according to a set of feature functions combined in a single log-linear model.", "labels": [], "entities": [{"text": "MT", "start_pos": 85, "end_pos": 87, "type": "TASK", "confidence": 0.9713420271873474}, {"text": "word alignment", "start_pos": 103, "end_pos": 117, "type": "TASK", "confidence": 0.7608426511287689}]}, {"text": "The decoding algorithm is described in detail and a set of new features that support this joint decoding approach is proposed.", "labels": [], "entities": []}, {"text": "The approach is evaluated in comparison to state-of-the-art confusion-network-based system combination methods using equivalent features and shown to outperform them significantly.", "labels": [], "entities": []}], "introductionContent": [{"text": "System combination for machine translation (MT) has emerged as a powerful method of combining the strengths of multiple MT systems and achieving results which surpass those of each individual system (e.g.).", "labels": [], "entities": [{"text": "machine translation (MT)", "start_pos": 23, "end_pos": 47, "type": "TASK", "confidence": 0.8603510141372681}]}, {"text": "Most state-of-the-art system combination methods are based on constructing a confusion network (CN) from several input translation hypotheses, and choosing the best output from the CN based on several scoring functions (e.g.).", "labels": [], "entities": []}, {"text": "Confusion networks allow word-level system combination, which was shown to outperform sentence re-ranking methods and phrase-level combination).", "labels": [], "entities": [{"text": "word-level system combination", "start_pos": 25, "end_pos": 54, "type": "TASK", "confidence": 0.6461307605107626}, {"text": "phrase-level combination", "start_pos": 118, "end_pos": 142, "type": "TASK", "confidence": 0.7419620454311371}]}, {"text": "We will review confusion-network-based system combination with the help of the examples in Figures 1 and 2.", "labels": [], "entities": []}, {"text": "shows translation hypotheses from three Chinese-toEnglish MT systems.", "labels": [], "entities": [{"text": "MT", "start_pos": 58, "end_pos": 60, "type": "TASK", "confidence": 0.7663100361824036}]}, {"text": "The general idea is to combine hypotheses in a representation such as the ones in, where for each word position there is a set of possible words, shown in columns.", "labels": [], "entities": []}, {"text": "The final output is determined by choosing one word from each column, which can be areal word or the empty word \u03b5.", "labels": [], "entities": []}, {"text": "For example, the CN in) can generate eight distinct sequences of words, including e.g. \"she bought the Jeep\" and \"she bought the SUV Jeep\".", "labels": [], "entities": []}, {"text": "The choice is performed to maximize a scoring function using a set of features and a log-linear model).", "labels": [], "entities": []}, {"text": "We can view a confusion network as an ordered sequence of columns (correspondence sets).", "labels": [], "entities": []}, {"text": "Each word from each input hypothesis belongs to exactly one correspondence set.", "labels": [], "entities": []}, {"text": "Each correspondence set contains at most one word from each input hypothesis and contributes exactly one of its words (including the possible \u03b5) to the final output.", "labels": [], "entities": []}, {"text": "Final words are output in the order of correspondence sets.", "labels": [], "entities": []}, {"text": "In order to construct such a representation, we need to solve the following two sub-problems: arrange words from all input hypotheses into correspondence sets (alignment problem) and order correspondence sets (ordering problem).", "labels": [], "entities": []}, {"text": "After constructing the confusion network we need to solve a third sub-problem: decide which words to output from each correspondence set (lexical choice problem).", "labels": [], "entities": []}, {"text": "In current state-of-the-art approaches, the construction of the confusion network is performed as follows: first, a backbone hypothesis is selected.", "labels": [], "entities": []}, {"text": "The backbone hypothesis determines the order of words in the final system output, and guides word-level alignments for construction of columns of possible words at each position.", "labels": [], "entities": []}, {"text": "Let us assume that for our example in, the second hypothesis is selected as a backbone.", "labels": [], "entities": []}, {"text": "All other hypotheses are aligned to the backbone such that these alignments are one-to-one; empty words are inserted where necessary to make one-to-one alignment possible.", "labels": [], "entities": []}, {"text": "Words in all hypotheses are sorted by the position of the backbone word they align to and the confusion network is determined.", "labels": [], "entities": []}, {"text": "It is clear that the quality of selection of the backbone and alignments has a large impact on the performance, because the word order is determined by the backbone, and the set of possible words at each position is determined by alignment.", "labels": [], "entities": []}, {"text": "Since the space of possible alignments is extremely large, approximate and heuristic techniques have been employed to derive them.", "labels": [], "entities": []}, {"text": "In pair-wise alignment, each hypothesis is aligned to the backbone in turn, with separate processing to combine the multiple alignments.", "labels": [], "entities": []}, {"text": "Several models have been used for pair-wise alignment, starting with TER and proceeding with more sophisticated techniques such as HMM models, ITG, and IHMM.", "labels": [], "entities": [{"text": "pair-wise alignment", "start_pos": 34, "end_pos": 53, "type": "TASK", "confidence": 0.6171246767044067}, {"text": "TER", "start_pos": 69, "end_pos": 72, "type": "METRIC", "confidence": 0.8150766491889954}]}, {"text": "A major problem with such methods is that each hypothesis is aligned to the backbone independently, leading to suboptimal behavior.", "labels": [], "entities": []}, {"text": "For example, suppose that we use a state-of-the-art word alignment model for pairs of hypotheses, such as the IHMM.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 52, "end_pos": 66, "type": "TASK", "confidence": 0.7071104198694229}, {"text": "IHMM", "start_pos": 110, "end_pos": 114, "type": "DATASET", "confidence": 0.8884508609771729}]}, {"text": "shows likely alignment links between every pair of hypotheses.", "labels": [], "entities": []}, {"text": "If Hypothesis 1 is aligned to Hypothesis 2 (the backbone), Jeep is likely to align to SUV because they express similar Chinese content.", "labels": [], "entities": []}, {"text": "Hypothesis 3 is separately aligned to the backbone and since the alignment is constrained to be one-to-one, SUV is aligned to SUV and Jeep to an empty word which is inserted after SUV.", "labels": [], "entities": []}, {"text": "The network in) is the result of this process.", "labels": [], "entities": []}, {"text": "An undesirable property of this CN is that the two instances of Jeep are placed in separate columns and cannot vote to reinforce each other.", "labels": [], "entities": []}, {"text": "Incremental alignment methods have been proposed to relax the independence assumption of pair-wise alignment ().", "labels": [], "entities": []}, {"text": "Such methods align hypotheses to a partially constructed CN in some order.", "labels": [], "entities": []}, {"text": "For example, if in such method, Hypothesis 3 is first aligned to the backbone, followed by Hypothesis 1, we are likely to arrive at the CN in) in which the two instances of Jeep are aligned.", "labels": [], "entities": []}, {"text": "However, if Hypothesis 1 is aligned to the backbone first, we would still get the CN in).", "labels": [], "entities": []}, {"text": "Notice that the desirable output \"She bought the Jeep SUV\" cannot be generated from either of the confusion networks because a rereordering of columns would be required.", "labels": [], "entities": [{"text": "She bought the Jeep SUV", "start_pos": 34, "end_pos": 57, "type": "DATASET", "confidence": 0.7401473879814148}]}, {"text": "A common characteristic of CN-based approaches is that the order of and the alignment of words (correspondence sets) are decided as greedy steps independently of the lexical choice for the final output.", "labels": [], "entities": []}, {"text": "The backbone and alignment are optimized according to auxiliary scoring functions and heuristics which mayor may not be optimal with respect to producing CNs leading to good translations.", "labels": [], "entities": []}, {"text": "In some recent approaches, these assumptions are relaxed to allow each input hypothesis as a backbone.", "labels": [], "entities": []}, {"text": "Each backbone produces a separate CN and the decision of which CN to choose is taken at a later decoding stage, but this still restricts the possible orders and alignments greatly (.", "labels": [], "entities": []}, {"text": "In this paper, we present a joint optimization method for system combination.", "labels": [], "entities": []}, {"text": "In this method, the alignment, ordering and lexical selection subproblems are solved jointly in a single decoding framework based on a log-linear model.", "labels": [], "entities": []}], "datasetContent": [{"text": "For the joint decoding method, the threshold for alignment-score-based pruning is set to 0.25 and the maximum number of words that can align to the same word is limited to 3.", "labels": [], "entities": []}, {"text": "We call this the standard setting.", "labels": [], "entities": []}, {"text": "The joint decoding approach is evaluated on the Chinese-to-English (C2E) test set of the 2008 NIST Open MT Evaluation.", "labels": [], "entities": [{"text": "Chinese-to-English (C2E) test set of the 2008 NIST Open MT Evaluation", "start_pos": 48, "end_pos": 117, "type": "DATASET", "confidence": 0.7979792356491089}]}, {"text": "Results are reported in case insensitive BLEU score in percentages).", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 41, "end_pos": 51, "type": "METRIC", "confidence": 0.9609642326831818}]}, {"text": "The NIST MT08 C2E test set contains 691 and 666 sentences of data from two genres, newswire and web-data, respectively.", "labels": [], "entities": [{"text": "NIST MT08 C2E test set", "start_pos": 4, "end_pos": 26, "type": "DATASET", "confidence": 0.9178209185600281}]}, {"text": "Each test sentence has four references provided by human translators.", "labels": [], "entities": []}, {"text": "Individual systems in our experiments belong to the official submissions of the MT08 C2E constraint-training track.", "labels": [], "entities": [{"text": "MT08 C2E constraint-training track", "start_pos": 80, "end_pos": 114, "type": "DATASET", "confidence": 0.7287635952234268}]}, {"text": "Each submission provides 1-best translation of the whole test set.", "labels": [], "entities": []}, {"text": "In order to train feature weights, the original test set is divided into two parts, called the dev and test set, respectively.", "labels": [], "entities": []}, {"text": "The dev set consists of the first half of both newswire and web-data, and the test set consists of the second half of data of both genres.", "labels": [], "entities": []}, {"text": "There are 20 individual systems available.", "labels": [], "entities": []}, {"text": "We ranked them by their BLEU score results on the dev set and picked the top five systems, excluding systems ranked 5th and 6th since they are subsets of the first entry.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 24, "end_pos": 34, "type": "METRIC", "confidence": 0.9788287878036499}]}, {"text": "Performance of these systems on the dev and test sets is shown in.", "labels": [], "entities": []}, {"text": "The baselines include a pair-wise hypothesis alignment approach using the indirect HMM (IHMM) proposed by, and an incremental hypothesis alignment approach using the incremental HMM (IncHMM) proposed by.", "labels": [], "entities": []}, {"text": "The lexical translation model used to compute the semantic similarity is estimated from two million parallel sentencepairs selected from the training corpus of MT08.", "labels": [], "entities": [{"text": "MT08", "start_pos": 160, "end_pos": 164, "type": "DATASET", "confidence": 0.9056815505027771}]}, {"text": "The backbone for the IHMM-based approach is selected based on Minimum Bayes Risk (MBR) using a BLEU-based loss function.", "labels": [], "entities": [{"text": "Minimum Bayes Risk (MBR)", "start_pos": 62, "end_pos": 86, "type": "METRIC", "confidence": 0.8990490436553955}, {"text": "BLEU-based", "start_pos": 95, "end_pos": 105, "type": "METRIC", "confidence": 0.9975444674491882}]}, {"text": "The various parameters of the IHMM and the IncHMM are tuned on the dev set.", "labels": [], "entities": [{"text": "IHMM", "start_pos": 30, "end_pos": 34, "type": "DATASET", "confidence": 0.7979004979133606}, {"text": "IncHMM", "start_pos": 43, "end_pos": 49, "type": "DATASET", "confidence": 0.8671635389328003}]}, {"text": "The same IHMM is used to compute the alignment feature score for the joint decoding approach.", "labels": [], "entities": [{"text": "IHMM", "start_pos": 9, "end_pos": 13, "type": "METRIC", "confidence": 0.6339973211288452}]}, {"text": "The final combination output can be obtained by decoding the CN with a set of features.", "labels": [], "entities": []}, {"text": "The features used for the baseline systems are the same as the features used by the joint decoding approach.", "labels": [], "entities": []}, {"text": "Some of these features are constant across decoding hypotheses and can be ignored.", "labels": [], "entities": []}, {"text": "The non-constant features are word posterior, bigram voting, language model score, and word count.", "labels": [], "entities": []}, {"text": "They are computed in the same way as for the joint decoding approach.", "labels": [], "entities": []}, {"text": "System weights and feature weights are trained together using Powell's search for the IHMM-based approach.", "labels": [], "entities": []}, {"text": "Then the same system weights are applied to both IncHMM and Joint Decoding -based approaches, and the feature weights of them are trained using the max-BLEU training method proposed by and refined by. lists the BLEU scores achieved by the two baselines and the joint decoding approach.", "labels": [], "entities": [{"text": "IncHMM", "start_pos": 49, "end_pos": 55, "type": "DATASET", "confidence": 0.915913999080658}, {"text": "BLEU", "start_pos": 211, "end_pos": 215, "type": "METRIC", "confidence": 0.9988443851470947}]}, {"text": "Both baselines surpass the best individual system significantly.", "labels": [], "entities": []}, {"text": "However, the gain of incremental HMM over IHMM is smaller than that reported in.", "labels": [], "entities": []}, {"text": "One possible reason of such discrepancy could be that fewer hypotheses are used for combination in this experiment compared to that of, so the performance difference between them is narrowed accordingly.", "labels": [], "entities": []}, {"text": "Despite that, the proposed joint decoding method outperforms both IHMM and IncHMM baselines significantly.", "labels": [], "entities": [{"text": "IHMM", "start_pos": 66, "end_pos": 70, "type": "DATASET", "confidence": 0.723654568195343}, {"text": "IncHMM", "start_pos": 75, "end_pos": 81, "type": "DATASET", "confidence": 0.7207088470458984}]}], "tableCaptions": [{"text": " Table 1: Performance of individual systems on  the dev and test set", "labels": [], "entities": []}, {"text": " Table 2: Comparison between the joint decoding  approach and the two baselines", "labels": [], "entities": []}, {"text": " Table 3.  Compared to the standard setting, allowing only  links in the union of the bi-directional Viterbi  alignments  causes  slight  performance  degradation. On the other hand, it still  outperforms the IHMM baseline by a fair margin.  This is because the joint decoding approach is  effectively resolving the ambiguous 1-to-many  alignments and deciding proper places to insert  empty words during decoding.", "labels": [], "entities": [{"text": "IHMM baseline", "start_pos": 209, "end_pos": 222, "type": "DATASET", "confidence": 0.9044659733772278}]}, {"text": " Table 3: Comparison between different settings  of alignment pruning", "labels": [], "entities": []}, {"text": " Table 4: Effect of ordering constraints", "labels": [], "entities": []}]}