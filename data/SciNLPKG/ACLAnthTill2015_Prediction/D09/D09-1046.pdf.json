{"title": [], "abstractContent": [{"text": "Word sense disambiguation is typically phrased as the task of labeling a word in context with the best-fitting sense from a sense inventory such as WordNet.", "labels": [], "entities": [{"text": "Word sense disambiguation", "start_pos": 0, "end_pos": 25, "type": "TASK", "confidence": 0.66596919298172}, {"text": "WordNet", "start_pos": 148, "end_pos": 155, "type": "DATASET", "confidence": 0.9541411995887756}]}, {"text": "While questions have often been raised over the choice of sense inventory, computational linguists have readily accepted the best-fitting sense methodology despite the fact that the case for discrete sense boundaries is widely disputed by lexical semantics researchers.", "labels": [], "entities": []}, {"text": "This paper studies graded word sense assignment, based on a recent dataset of graded word sense annotation.", "labels": [], "entities": [{"text": "graded word sense assignment", "start_pos": 19, "end_pos": 47, "type": "TASK", "confidence": 0.6280861273407936}]}], "introductionContent": [{"text": "The task of automatically characterizing word meaning in text is typically modeled as word sense disambiguation (WSD): given a list of senses for target lemma w, the task is to pick the best-fitting sense fora given occurrence of w.", "labels": [], "entities": [{"text": "characterizing word meaning in text", "start_pos": 26, "end_pos": 61, "type": "TASK", "confidence": 0.8218668580055237}, {"text": "word sense disambiguation (WSD)", "start_pos": 86, "end_pos": 117, "type": "TASK", "confidence": 0.7559578269720078}]}, {"text": "The list of senses is usually taken from an online dictionary or thesaurus.", "labels": [], "entities": []}, {"text": "However, clear cut sense boundaries are sometimes hard to define, and the meaning of words depends strongly on the context in which they are used).", "labels": [], "entities": []}, {"text": "Some researchers in lexical semantics have suggested that word meanings lie on a continuum between i) clear cut cases of ambiguity and ii) vagueness where clear cut boundaries do not hold.", "labels": [], "entities": []}, {"text": "Certainly, it seems that a more complex representation of word sense is needed with a softer, graded representation of meaning rather than a fixed listing of senses.", "labels": [], "entities": []}, {"text": "A recent annotation study), hereafter GWS) marked a target word in context with graded ratings (on a scale of 1-5) on senses from WordNet.", "labels": [], "entities": [{"text": "GWS", "start_pos": 38, "end_pos": 41, "type": "DATASET", "confidence": 0.749047577381134}, {"text": "WordNet", "start_pos": 130, "end_pos": 137, "type": "DATASET", "confidence": 0.9507134556770325}]}, {"text": "shows an example of a sentence with the target word in bold, and with the annotator judgments given to each sense.", "labels": [], "entities": []}, {"text": "The study found that annotators made ample use of the intermediate ratings on the scale, and often gave high ratings to more than one WordNet sense for the same occurrence.", "labels": [], "entities": []}, {"text": "It was found that the annotator ratings could not easily be transformed to categorial judgments by making more coarse-grained senses.", "labels": [], "entities": []}, {"text": "If human word sense judgments are best viewed as graded, it makes sense to explore models of word sense that can predict graded sense assignments.", "labels": [], "entities": []}, {"text": "In this paper we look at the issue of graded applicability of word sense from the point of view of automatic graded word sense assignment, using the GWS graded word sense dataset.", "labels": [], "entities": [{"text": "graded applicability of word sense", "start_pos": 38, "end_pos": 72, "type": "TASK", "confidence": 0.5317162990570068}, {"text": "automatic graded word sense assignment", "start_pos": 99, "end_pos": 137, "type": "TASK", "confidence": 0.6225432515144348}, {"text": "GWS graded word sense dataset", "start_pos": 149, "end_pos": 178, "type": "DATASET", "confidence": 0.8873640656471252}]}, {"text": "We make three primary contributions.", "labels": [], "entities": []}, {"text": "Firstly, we propose evaluation metrics that can be used on graded word sense judgments.", "labels": [], "entities": [{"text": "graded word sense judgments", "start_pos": 59, "end_pos": 86, "type": "TASK", "confidence": 0.6779833883047104}]}, {"text": "Some of these metrics, like Spearman's \u03c1, have been used previously (), but we also introduce new metrics based on the traditional precision and recall.", "labels": [], "entities": [{"text": "Spearman's \u03c1", "start_pos": 28, "end_pos": 40, "type": "METRIC", "confidence": 0.4809140662352244}, {"text": "precision", "start_pos": 131, "end_pos": 140, "type": "METRIC", "confidence": 0.9992133378982544}, {"text": "recall", "start_pos": 145, "end_pos": 151, "type": "METRIC", "confidence": 0.9971359968185425}]}, {"text": "Secondly, we investigate how two classes of models perform on the task of graded word sense assignment: on the one hand classical WSD models, on the other hand prototype-based vector space models that can be viewed as simple one-class classifiers.", "labels": [], "entities": [{"text": "graded word sense assignment", "start_pos": 74, "end_pos": 102, "type": "TASK", "confidence": 0.6335246786475182}]}, {"text": "We study supervised models, training on traditional WSD data and evaluating against a graded scale.", "labels": [], "entities": [{"text": "WSD", "start_pos": 52, "end_pos": 55, "type": "TASK", "confidence": 0.9413509368896484}]}, {"text": "Thirdly, the evaluation metrics we use also provides a novel analysis of annotator performance on the GWS dataset.", "labels": [], "entities": [{"text": "GWS dataset", "start_pos": 102, "end_pos": 113, "type": "DATASET", "confidence": 0.9854991137981415}]}], "datasetContent": [{"text": "This section reports on experiments for the task of graded word sense assignment.", "labels": [], "entities": [{"text": "graded word sense assignment", "start_pos": 52, "end_pos": 80, "type": "TASK", "confidence": 0.6991396546363831}]}, {"text": "As data, we use the GWS dataset described in Sec.", "labels": [], "entities": [{"text": "GWS dataset", "start_pos": 20, "end_pos": 31, "type": "DATASET", "confidence": 0.9747998118400574}]}, {"text": "3. We test the models discussed in Sec.", "labels": [], "entities": []}, {"text": "5, evaluating with the methods described in Sec.", "labels": [], "entities": []}, {"text": "4. To put the models' performance into perspective, we first consider the human performance on the task, shown in.", "labels": [], "entities": []}, {"text": "The first three lines of the table show the performance of each annotator evaluated against the average of the other two.", "labels": [], "entities": []}, {"text": "The fourth line averages over the previous three lines to provide an average human ceiling for the task.", "labels": [], "entities": []}, {"text": "In the correlation of rankings by lemma, correlation is statistically significant for all lemmas at p \u2264 0.01.", "labels": [], "entities": [{"text": "correlation", "start_pos": 41, "end_pos": 52, "type": "METRIC", "confidence": 0.9823348522186279}]}, {"text": "For correlation by lemma+sense and by lemma+sentence, the percentage of pairs with significant correlation is lower: 73.6 of lemma/sense pairs and 29.0 of lemma/sentence pairs reach significance at p \u2264 0.05.", "labels": [], "entities": []}, {"text": "For p \u2264 0.01, the percentage is 58.3 and 12.2, respectively.", "labels": [], "entities": []}, {"text": "The higher \u03c1 but lower proportion of significant values for lemma+sentence pairs compared to lemma+sense is due to the fact that there are far fewer datapoints (sample size) for each calculation of \u03c1 (#senses for lemma+sentence vs 50 sentences for lemma+sense).", "labels": [], "entities": []}, {"text": "At 0.131, J/S for Annotator 1 is considerably lower than for Annotators 2 and 3.", "labels": [], "entities": [{"text": "J/S", "start_pos": 10, "end_pos": 13, "type": "METRIC", "confidence": 0.9581502079963684}, {"text": "Annotator", "start_pos": 18, "end_pos": 27, "type": "DATASET", "confidence": 0.7783521413803101}]}, {"text": "In terms of precision and recall, Annotator 1 again differs from the other two.", "labels": [], "entities": [{"text": "precision", "start_pos": 12, "end_pos": 21, "type": "METRIC", "confidence": 0.999529242515564}, {"text": "recall", "start_pos": 26, "end_pos": 32, "type": "METRIC", "confidence": 0.9994909763336182}]}, {"text": "At 87.5, her recall is higher than her precision (50.6), while the other annotators have considerably higher precision (75.5 and 82.4) than recall (62.4 and 52.3).", "labels": [], "entities": [{"text": "recall", "start_pos": 13, "end_pos": 19, "type": "METRIC", "confidence": 0.999671220779419}, {"text": "precision", "start_pos": 39, "end_pos": 48, "type": "METRIC", "confidence": 0.9994003772735596}, {"text": "precision", "start_pos": 109, "end_pos": 118, "type": "METRIC", "confidence": 0.9982470273971558}, {"text": "recall", "start_pos": 140, "end_pos": 146, "type": "METRIC", "confidence": 0.9989839196205139}]}, {"text": "This indicates that Annotator 1 tended to assign higher ratings throughout, an impression that is confirmed by Table 6.", "labels": [], "entities": []}, {"text": "The left two columns show average ratings for each annotator overall senses of all tokens (normalized to values between 0.0 and 1.0 as described in Sec. 3).", "labels": [], "entities": []}, {"text": "The three annotators differ widely in their average ratings, which range from 0.285 for Ann.3 to 0.540 for Ann..", "labels": [], "entities": [{"text": "Ann.3", "start_pos": 88, "end_pos": 93, "type": "DATASET", "confidence": 0.9242126941680908}, {"text": "Ann.", "start_pos": 107, "end_pos": 111, "type": "DATASET", "confidence": 0.9242794513702393}]}, {"text": "We tested the performance of the WSD/single model on a standard WSD task, using the same training and testing data as in our subsequent experiments, as described in section 3. 7 The model's accuracy when trained and tested on SemCor was A=77.0%, with a most frequent sense baseline of 63.5%.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 190, "end_pos": 198, "type": "METRIC", "confidence": 0.9992522597312927}, {"text": "A", "start_pos": 237, "end_pos": 238, "type": "METRIC", "confidence": 0.9980837106704712}, {"text": "frequent sense baseline", "start_pos": 258, "end_pos": 281, "type": "METRIC", "confidence": 0.7434469560782114}]}, {"text": "When trained and tested on SE-3, the model achieved A=53.0% against a baseline of 44.0%.", "labels": [], "entities": [{"text": "A", "start_pos": 52, "end_pos": 53, "type": "METRIC", "confidence": 0.999882698059082}]}, {"text": "When trained and tested on SemCor plus SE-3, the model reached an accuracy 58.2%, with a baseline of 56.0%.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 66, "end_pos": 74, "type": "METRIC", "confidence": 0.9997296929359436}]}, {"text": "So on the combined dataset, the baseline is the average of the baselines on the individual datasets, while the model's performance falls below the average performance on the individual datasets.", "labels": [], "entities": []}, {"text": "WSD models for graded sense assignment.", "labels": [], "entities": [{"text": "graded sense assignment", "start_pos": 15, "end_pos": 38, "type": "TASK", "confidence": 0.6366983155409495}]}, {"text": "shows the performance of different models in the task of graded word sense assignment.", "labels": [], "entities": [{"text": "graded word sense assignment", "start_pos": 57, "end_pos": 85, "type": "TASK", "confidence": 0.6427067965269089}]}, {"text": "The first line in lists results for the maximum entropy model when used to assign a single best sense.: Evaluation: computational models, and baseline.", "labels": [], "entities": []}, {"text": "* , * * : percentage significant at p \u2264 0.05, p \u2264 0.01 the same maximum entropy model when classifier confidence is used as predicted judgment.", "labels": [], "entities": []}, {"text": "The last line shows the baseline, an adaptation of the most frequent sense baseline to the graded case.", "labels": [], "entities": []}, {"text": "For this baseline, we computed the relative frequency of each sense in the training corpus and used this relative frequency as the prediction for each test sentence and sense combination.", "labels": [], "entities": []}, {"text": "The WSD/single model remains below the baseline in all evaluations except correlation by lemma+sense, where no rank-based correlation could be computed for the baseline because it always assigns the same judgment fora given sense.", "labels": [], "entities": []}, {"text": "WSD/conf shows a performance slightly above the baseline in all evaluation measures.", "labels": [], "entities": [{"text": "WSD", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.49772804975509644}]}, {"text": "lists average ratings, averaged overall lemmas, senses, and occurrences, for each model in the two right-hand columns.", "labels": [], "entities": []}, {"text": "Lines 3-6 in show results for Prototype variants.", "labels": [], "entities": []}, {"text": "While each Prototype and Prototype/2 model only sees positive data annotated fora single sense, the variants with /N (lines 5 and 6) make very limited use of information coming from all senses of a given lemma.", "labels": [], "entities": [{"text": "Prototype", "start_pos": 11, "end_pos": 20, "type": "DATASET", "confidence": 0.9280808568000793}]}, {"text": "They normalize judgments for each sentence, with assigned norm ,i,t = assigned ,i,t j\u2208S assigned ,j,t Line 3 evaluates the Prototype model with firstorder vectors.", "labels": [], "entities": []}, {"text": "Its correlation with the gold data is somewhat lower than that of WSD/conf in almost all cases.", "labels": [], "entities": []}, {"text": "The Prototype model deviates strongly The reason why the average \u03c1 for correlation by from both WSD/conf and baseline in having a very good recall, at 78.3, with lower precision at 58.4, for an overall F-score that is 16 points higher than that of WSD/conf . Both Prototype and Prototype/2 have average ratings) far above those of the WSD models and of the /N variants.", "labels": [], "entities": [{"text": "\u03c1", "start_pos": 65, "end_pos": 66, "type": "METRIC", "confidence": 0.9518270492553711}, {"text": "correlation", "start_pos": 71, "end_pos": 82, "type": "METRIC", "confidence": 0.9454983472824097}, {"text": "recall", "start_pos": 140, "end_pos": 146, "type": "METRIC", "confidence": 0.9992840886116028}, {"text": "precision", "start_pos": 168, "end_pos": 177, "type": "METRIC", "confidence": 0.9977554678916931}, {"text": "F-score", "start_pos": 202, "end_pos": 209, "type": "METRIC", "confidence": 0.9982591271400452}]}, {"text": "The second-order vector model Prototype/2 has relatively low correlation by lemma+sense, while correlation by lemma+sentence shows the best performance of all models (along with Prototype/2N).", "labels": [], "entities": []}, {"text": "Its correlation by lemma+sentence is similar to the lowest correlation by lemma+sentence achieved by a human annotator.", "labels": [], "entities": []}, {"text": "In terms of J/S, this model also shows the best performance along with WSD/conf and Prototype/2N.", "labels": [], "entities": []}, {"text": "Both /N variants achieve very high correlation by lemma.", "labels": [], "entities": [{"text": "correlation", "start_pos": 35, "end_pos": 46, "type": "METRIC", "confidence": 0.9684467315673828}]}, {"text": "Correlation by lemma+sense for the /N models is between those of Prototype and WSD/conf . The correlation by lemma+sentence is the same with or without normalization, as normalization does not change the ranking of senses of an individual sentence.", "labels": [], "entities": [{"text": "Prototype", "start_pos": 65, "end_pos": 74, "type": "DATASET", "confidence": 0.9193998575210571}]}, {"text": "While Prototype has higher recall than precision, normalization turns it into a model with even higher precision than WSD/conf but even lower recall.", "labels": [], "entities": [{"text": "Prototype", "start_pos": 6, "end_pos": 15, "type": "DATASET", "confidence": 0.9188697338104248}, {"text": "recall", "start_pos": 27, "end_pos": 33, "type": "METRIC", "confidence": 0.9991388320922852}, {"text": "precision", "start_pos": 39, "end_pos": 48, "type": "METRIC", "confidence": 0.998795747756958}, {"text": "precision", "start_pos": 103, "end_pos": 112, "type": "METRIC", "confidence": 0.9954531192779541}, {"text": "recall", "start_pos": 142, "end_pos": 148, "type": "METRIC", "confidence": 0.9979593753814697}]}], "tableCaptions": [{"text": " Table 1: A sample annotation in the GWS experiment. The senses are: 1 material from cellulose 2 report  3 publication 4 medium for writing 5 scientific 6 publishing firm 7 physical object", "labels": [], "entities": [{"text": "GWS experiment", "start_pos": 37, "end_pos": 51, "type": "DATASET", "confidence": 0.8983556926250458}]}, {"text": " Table 2: Lemmas used in this study", "labels": [], "entities": []}, {"text": " Table 4: Human ceiling: one annotator vs. average of the other two annotators.  * ,  *  * : percentage  significant at p \u2264 0.05, p \u2264 0.01. Avg: average annotator performance", "labels": [], "entities": [{"text": "Avg", "start_pos": 140, "end_pos": 143, "type": "METRIC", "confidence": 0.9980329871177673}]}, {"text": " Table 5: Evaluation: computational models, and baseline.  * ,  *  * : percentage significant at p \u2264 0.05,  p \u2264 0.01", "labels": [], "entities": []}, {"text": " Table 6: Average judgment for individual annota- tors (transformed) and average rating for models", "labels": [], "entities": [{"text": "Average judgment", "start_pos": 10, "end_pos": 26, "type": "METRIC", "confidence": 0.8719505965709686}]}]}