{"title": [], "abstractContent": [{"text": "Methods that learn from prior information about input features such as generalized expectation (GE) have been used to train accurate models with very little effort.", "labels": [], "entities": [{"text": "generalized expectation (GE)", "start_pos": 71, "end_pos": 99, "type": "METRIC", "confidence": 0.8335978388786316}]}, {"text": "In this paper, we propose an active learning approach in which the machine solicits \"labels\" on features rather than instances.", "labels": [], "entities": []}, {"text": "In both simulated and real user experiments on two sequence labeling tasks we show that our active learning method outperforms passive learning with features as well as traditional active learning with instances.", "labels": [], "entities": []}, {"text": "Preliminary experiments suggest that novel interfaces which intelligently solicit labels on multiple features facilitate more efficient annotation.", "labels": [], "entities": []}], "introductionContent": [{"text": "The application of machine learning to new problems is slowed by the need for labeled training data.", "labels": [], "entities": []}, {"text": "When output variables are structured, annotation can be particularly difficult and timeconsuming.", "labels": [], "entities": []}, {"text": "For example, when training a conditional random field () to extract fields such as rent, contact, features, and utilities from apartment classifieds, labeling 22 instances (2,540 tokens) provides only 66.1% accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 207, "end_pos": 215, "type": "METRIC", "confidence": 0.9980917572975159}]}, {"text": "Recent work has used unlabeled data and limited prior information about input features to bootstrap accurate structured output models.", "labels": [], "entities": []}, {"text": "For example, both and  have demonstrated results better than 66.1% on the apartments task described above using only a list of 33 highly discriminative features and the labels they indicate.", "labels": [], "entities": []}, {"text": "However, these methods have only been applied in scenarios in which the user supplies such prior knowledge before learning begins.", "labels": [], "entities": []}, {"text": "Averaged over 10 randomly selected sets of 22 instances.", "labels": [], "entities": []}, {"text": "In traditional active learning, the machine queries the user for only the labels of instances that would be most helpful to the machine.", "labels": [], "entities": []}, {"text": "This paper proposes an active learning approach in which the user provides \"labels\" for input features, rather than instances.", "labels": [], "entities": []}, {"text": "A labeled input feature denotes that a particular input feature, for example the word call, is highly indicative of a particular label, such as contact.", "labels": [], "entities": []}, {"text": "provides an excerpt of a feature active learning session.", "labels": [], "entities": []}, {"text": "In this paper, we advocate using generalized expectation (GE) criteria  for learning with labeled features.", "labels": [], "entities": [{"text": "generalized expectation (GE)", "start_pos": 33, "end_pos": 61, "type": "METRIC", "confidence": 0.8442392706871032}]}, {"text": "We provide an alternate treatment of the GE objective function used by  and a novel speedup to the gradient computation.", "labels": [], "entities": []}, {"text": "We then provide a pool-based feature active learning algorithm that includes an option to skip queries, for cases in which a feature has no clear label.", "labels": [], "entities": []}, {"text": "We propose and evaluate feature query selection algorithms that aim to reduce model uncertainty, and compare to several baselines.", "labels": [], "entities": [{"text": "feature query selection", "start_pos": 24, "end_pos": 47, "type": "TASK", "confidence": 0.6305041015148163}]}, {"text": "We evaluate our method using both real and simulated user experiments on two sequence labeling tasks.", "labels": [], "entities": []}, {"text": "Compared to previous approaches, our method can be used for both classification and structured tasks, and the feature query selection methods we propose perform better.", "labels": [], "entities": []}, {"text": "We use experiments with simulated labelers on real data to extensively compare feature query selection algorithms and evaluate on multiple random splits.", "labels": [], "entities": []}, {"text": "To make these simulations more realistic, the effort required to perform different labeling actions is estimated from additional experiments with real users.", "labels": [], "entities": []}, {"text": "The results show that active learning with features outperforms both passive learning with features and traditional active learning with instances.", "labels": [], "entities": []}, {"text": "In the user experiments, each annotator actively labels instances, actively labels features one at a time, and actively labels batches of features orga- nized using a \"grid\" interface.", "labels": [], "entities": []}, {"text": "The results support the findings of the simulated experiments and provide evidence that the \"grid\" interface can facilitate more efficient annotation.", "labels": [], "entities": []}], "datasetContent": [{"text": "Another advantage of feature queries is that feature names are concise enough to be browsed, rather than considered individually.", "labels": [], "entities": []}, {"text": "This allows the design of improved interfaces that can further increase the speed of feature active learning.", "labels": [], "entities": []}, {"text": "We built a prototype interface that allows the user to quickly browse many candidate features.", "labels": [], "entities": []}, {"text": "The features are split into groups of five features each.", "labels": [], "entities": []}, {"text": "Each group contains features that are related, as measured by distributional similarity.", "labels": [], "entities": []}, {"text": "The features within each group are sorted according to the active learning metric.", "labels": [], "entities": []}, {"text": "This interface, displayed in, maybe useful because features in the same group are likely to have the same label.", "labels": [], "entities": []}, {"text": "We conduct three types of experiments.", "labels": [], "entities": []}, {"text": "First, a user labels instances selected by information density, and models are trained using ER.", "labels": [], "entities": []}, {"text": "The instance labeling interface allows the user to label tokens quickly by extending the current selection one token at a time and only requiring a single keystroke to label an entire segment.", "labels": [], "entities": [{"text": "instance labeling", "start_pos": 4, "end_pos": 21, "type": "TASK", "confidence": 0.6949644088745117}]}, {"text": "Second, the user labels features presented one-at-a-time by weighted uncertainty, and models are trained using GE.", "labels": [], "entities": [{"text": "GE", "start_pos": 111, "end_pos": 113, "type": "METRIC", "confidence": 0.47969645261764526}]}, {"text": "To aid the user in understanding the function of the feature quickly, we provide several examples of the feature occurring in context and the model's current predicted label distribution for the feature.", "labels": [], "entities": []}, {"text": "Finally, the user labels features organized using the grid interface described in the previous paragraph.", "labels": [], "entities": []}, {"text": "Weighted uncertainty is used to sort feature queries within each group, and GE is used to train models.", "labels": [], "entities": [{"text": "GE", "start_pos": 76, "end_pos": 78, "type": "METRIC", "confidence": 0.9693069458007812}]}, {"text": "Each iteration of labeling lasts two minutes, and there are five iterations.", "labels": [], "entities": [{"text": "labeling", "start_pos": 18, "end_pos": 26, "type": "TASK", "confidence": 0.9733607769012451}]}, {"text": "Retraining with ER between iterations takes an average of 5 minutes on cora and 3 minutes on apartments.", "labels": [], "entities": [{"text": "ER", "start_pos": 16, "end_pos": 18, "type": "METRIC", "confidence": 0.9871799349784851}]}, {"text": "With GE, the retraining times are on average 6 minutes on cora and 4 minutes on apartments.", "labels": [], "entities": [{"text": "GE", "start_pos": 5, "end_pos": 7, "type": "DATASET", "confidence": 0.935126543045044}]}, {"text": "Consequently, even when viewed with total time, rather than annotation time, feature active learning is beneficial.", "labels": [], "entities": []}, {"text": "While waiting for models to retrain, users can perform other tasks.", "labels": [], "entities": []}, {"text": "User 1 labeled apartments data, while Users 2 and 3 labeled cora data.", "labels": [], "entities": []}, {"text": "User 1 was able to obtain much better results with feature labeling than with instance labeling, but performed slightly worse with the grid interface than with the serial interface.", "labels": [], "entities": []}, {"text": "User 1 commented that they found the label definitions for apartments to be imprecise, so the other experiments were conducted on the cora data.", "labels": [], "entities": [{"text": "cora data", "start_pos": 134, "end_pos": 143, "type": "DATASET", "confidence": 0.779611736536026}]}, {"text": "User 2 obtained better results with feature labeling than instance labeling, and obtained higher mean accuracy with the grid interface.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 102, "end_pos": 110, "type": "METRIC", "confidence": 0.9044089317321777}]}, {"text": "User 3 was much better at labeling features than instances, and performed especially well using the grid interface.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Mean and final token accuracy results.  A  *  or  \u2020 denotes that a GE method significantly  outperforms all non-GE or passive GE methods,  respectively. Bold entries significantly outperform  all others. Methods in italics are passive.", "labels": [], "entities": [{"text": "Mean", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9912317395210266}, {"text": "accuracy", "start_pos": 31, "end_pos": 39, "type": "METRIC", "confidence": 0.8922640681266785}, {"text": "A", "start_pos": 50, "end_pos": 51, "type": "METRIC", "confidence": 0.9867232441902161}]}]}