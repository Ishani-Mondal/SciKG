{"title": [], "abstractContent": [{"text": "Topic models area useful tool for analyzing large text collections, but have previously been applied in only monolingual, or at most bilingual, contexts.", "labels": [], "entities": []}, {"text": "Meanwhile , massive collections of interlinked documents in dozens of languages, such as Wikipedia, are now widely available, calling for tools that can characterize content in many languages.", "labels": [], "entities": [{"text": "Wikipedia", "start_pos": 89, "end_pos": 98, "type": "DATASET", "confidence": 0.934502363204956}]}, {"text": "We introduce a polylingual topic model that discovers topics aligned across multiple languages.", "labels": [], "entities": []}, {"text": "We explore the model's characteristics using two large corpora, each with over ten different languages, and demonstrate its usefulness in supporting machine translation and tracking topic trends across languages.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 149, "end_pos": 168, "type": "TASK", "confidence": 0.7887113988399506}]}], "introductionContent": [{"text": "Statistical topic models have emerged as an increasingly useful analysis tool for large text collections.", "labels": [], "entities": []}, {"text": "Topic models have been used for analyzing topic trends in research literature (, inferring captions for images ( , social network analysis in email), and expanding queries with topically related words in information retrieval).", "labels": [], "entities": []}, {"text": "Much of this work, however, has occurred in monolingual contexts.", "labels": [], "entities": []}, {"text": "In an increasingly connected world, the ability to access documents in many languages has become both a strategic asset and a personally enriching experience.", "labels": [], "entities": []}, {"text": "In this paper, we present the polylingual topic model (PLTM).", "labels": [], "entities": []}, {"text": "We demonstrate its utility and explore its characteristics using two polylingual corpora: proceedings of the European parliament (in eleven languages) and a collection of Wikipedia articles (in twelve languages).", "labels": [], "entities": []}, {"text": "There are many potential applications for polylingual topic models.", "labels": [], "entities": []}, {"text": "Although research literature is typically written in English, bibliographic databases often contain substantial quantities of work in other languages.", "labels": [], "entities": []}, {"text": "To perform topic-based bibliometric analysis on these collections, it is necessary to have topic models that are aligned across languages.", "labels": [], "entities": []}, {"text": "Such analysis could be significant in tracking international research trends, where language barriers slow the transfer of ideas.", "labels": [], "entities": []}, {"text": "Previous work on bilingual topic modeling has focused on machine translation applications, which rely on sentence-aligned parallel translations.", "labels": [], "entities": [{"text": "bilingual topic modeling", "start_pos": 17, "end_pos": 41, "type": "TASK", "confidence": 0.8112426996231079}, {"text": "machine translation", "start_pos": 57, "end_pos": 76, "type": "TASK", "confidence": 0.7834703624248505}, {"text": "sentence-aligned parallel translations", "start_pos": 105, "end_pos": 143, "type": "TASK", "confidence": 0.6656450629234314}]}, {"text": "However, the growth of the internet, and in particular Wikipedia, has made vast corpora of topically comparable texts-documents that are topically similar but are not direct translations of one another-considerably more abundant than ever before.", "labels": [], "entities": []}, {"text": "We argue that topic modeling is both a useful and appropriate tool for leveraging correspondences between semantically comparable documents in multiple different languages.", "labels": [], "entities": [{"text": "topic modeling", "start_pos": 14, "end_pos": 28, "type": "TASK", "confidence": 0.8049888014793396}]}, {"text": "In this paper, we use two polylingual corpora to answer various critical questions related to polylingual topic models.", "labels": [], "entities": []}, {"text": "We employ a set of direct translations, the EuroParl corpus, to evaluate whether PLTM can accurately infer topics when documents genuinely contain the same content.", "labels": [], "entities": [{"text": "EuroParl corpus", "start_pos": 44, "end_pos": 59, "type": "DATASET", "confidence": 0.9916986525058746}]}, {"text": "We also explore how the characteristics of different languages affect topic model performance.", "labels": [], "entities": []}, {"text": "The second corpus, Wikipedia articles in twelve languages, contains sets of documents that are not translations of one another, but are very likely to be about similar concepts.", "labels": [], "entities": []}, {"text": "We use this corpus to explore the ability of the model both to infer similarities between vocabularies in different languages, and to detect differences in topic emphasis between languages.", "labels": [], "entities": []}, {"text": "The internet makes it possible for people allover the world to access documents from different cultures, but readers will not be fluent in this wide variety of languages.", "labels": [], "entities": []}, {"text": "By linking topics across languages, polylingual topic models can increase cross-cultural understanding by providing readers with the ability to characterize the contents of collections in unfamiliar languages and identify trends in topic prevalence.", "labels": [], "entities": []}], "datasetContent": [{"text": "A topic model specifies a probability distribution over documents, or in the case of PLTM, document tuples.", "labels": [], "entities": []}, {"text": "Given a set of training document tuples, PLTM can be used to obtain posterior estimates of \u03a6 1 , . .", "labels": [], "entities": [{"text": "PLTM", "start_pos": 41, "end_pos": 45, "type": "METRIC", "confidence": 0.581142008304596}]}, {"text": "The probability of previously unseen held-out document tuples given these estimates can then be computed.", "labels": [], "entities": []}, {"text": "The higher the probability of the held-out document tuples, the better the generalization ability of the model.", "labels": [], "entities": []}, {"text": "Analytically calculating the probability of a set of held-out document tuples given \u03a6 1 , . .", "labels": [], "entities": []}, {"text": ", \u03a6 Land \u03b1m is intractable, due to the summation over an exponential number of topic assignments for these held-out documents.", "labels": [], "entities": []}, {"text": "However, recently developed methods provide efficient, accurate estimates of this probability.", "labels": [], "entities": []}, {"text": "We use the \"left-to-right\" method of ().", "labels": [], "entities": []}, {"text": "We perform five estimation runs for each document and then calculate standard errors using a bootstrap method.", "labels": [], "entities": []}, {"text": "shows the log probability of held-out data in nats per word for PLTM and LDA, both trained with 200 topics.", "labels": [], "entities": [{"text": "PLTM", "start_pos": 64, "end_pos": 68, "type": "DATASET", "confidence": 0.7675266265869141}]}, {"text": "There is substantial variation between languages.", "labels": [], "entities": []}, {"text": "Additionally, the predictive ability of PLTM is consistently slightly worse than that of (monolingual) LDA.", "labels": [], "entities": [{"text": "predictive", "start_pos": 18, "end_pos": 28, "type": "METRIC", "confidence": 0.9718213081359863}, {"text": "PLTM", "start_pos": 40, "end_pos": 44, "type": "METRIC", "confidence": 0.7686297297477722}]}, {"text": "It is important to note, however, that these results do not imply that LDA should be preferred over PLTM-that choice depends upon the needs of the modeler.", "labels": [], "entities": []}, {"text": "Rather, these results are intended as a quantitative analysis of the difference between the two models.", "labels": [], "entities": []}, {"text": "As the number of topics is increased, the word counts per topic become very sparse in monolingual LDA models, proportional to the size of the vocabulary.", "labels": [], "entities": []}, {"text": "shows the proportion of all tokens in English and Finnish assigned to each topic under LDA and PLTM with 800 topics.", "labels": [], "entities": []}, {"text": "More than 350 topics in the Finnish LDA model have zero tokens assigned to them, and almost all tokens are assigned to the largest 200 topics.", "labels": [], "entities": [{"text": "Finnish LDA model", "start_pos": 28, "end_pos": 45, "type": "DATASET", "confidence": 0.708567202091217}]}, {"text": "English has a larger tail, with non-zero counts in all but 16 topics.", "labels": [], "entities": []}, {"text": "In contrast, PLTM assigns a significant number of tokens to almost all 800 topics, in very similar proportions in both languages.", "labels": [], "entities": []}, {"text": "PLTM topics therefore have a higher granularityi.e., they are more specific.", "labels": [], "entities": []}, {"text": "This result is important: informally, we have found that increasing the granularity of topics correlates strongly with user perceptions of the utility of a topic model.", "labels": [], "entities": []}, {"text": "Finnish is in black, English is in red; LDA is solid, PLTM is dashed.", "labels": [], "entities": [{"text": "Finnish", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.8595833778381348}, {"text": "LDA", "start_pos": 40, "end_pos": 43, "type": "METRIC", "confidence": 0.895561933517456}, {"text": "PLTM", "start_pos": 54, "end_pos": 58, "type": "METRIC", "confidence": 0.8916187882423401}]}, {"text": "LDA in Finnish essentially learns a 200 topic model when given 800 topics, while PLTM uses all 800 topics.", "labels": [], "entities": [{"text": "PLTM", "start_pos": 81, "end_pos": 85, "type": "DATASET", "confidence": 0.7964930534362793}]}], "tableCaptions": [{"text": " Table 1: Average document length, # documents, and", "labels": [], "entities": [{"text": "Average document length", "start_pos": 10, "end_pos": 33, "type": "METRIC", "confidence": 0.8969902197519938}]}, {"text": " Table 2: Held-out log probability in nats/word. (Smaller", "labels": [], "entities": [{"text": "Held-out log probability", "start_pos": 10, "end_pos": 34, "type": "METRIC", "confidence": 0.7508236368497213}]}, {"text": " Table 3: The effect of the proportion p of \"glue\" tuples on", "labels": [], "entities": []}, {"text": " Table 4: Topics are meaningful within languages but di-", "labels": [], "entities": []}, {"text": " Table 5: Percent of English query documents for which the", "labels": [], "entities": [{"text": "Percent", "start_pos": 10, "end_pos": 17, "type": "METRIC", "confidence": 0.946418821811676}]}]}