{"title": [], "abstractContent": [{"text": "In this paper, we address the issue of automatic extending lexical resources by exploiting existing knowledge repositories.", "labels": [], "entities": [{"text": "automatic extending lexical resources", "start_pos": 39, "end_pos": 76, "type": "TASK", "confidence": 0.8175839185714722}]}, {"text": "In particular, we deal with the new task of linking FrameNet and Wikipedia using a word sense disambiguation system that, fora given pair frame-lexical unit (F, l), finds the Wikipage that best expresses the the meaning of l.", "labels": [], "entities": []}, {"text": "The mapping can be exploited to straightforwardly acquire new example sentences and new lexical units, both for English and for all languages available in Wikipedia.", "labels": [], "entities": []}, {"text": "In this way, it is possible to easily acquire good-quality data as a starting point for the creation of FrameNet in new languages.", "labels": [], "entities": []}, {"text": "The evaluation reported both for the monolingual and the multilingual expansion of FrameNet shows that the approach is promising.", "labels": [], "entities": [{"text": "FrameNet", "start_pos": 83, "end_pos": 91, "type": "DATASET", "confidence": 0.8557201027870178}]}], "introductionContent": [{"text": "Many applications in the context of natural language processing or information retrieval have proved to convey significant improvement by exploiting lexical databases with high-quality annotation such as FrameNet ( and WordNet.", "labels": [], "entities": [{"text": "information retrieval", "start_pos": 67, "end_pos": 88, "type": "TASK", "confidence": 0.7331836223602295}, {"text": "WordNet", "start_pos": 219, "end_pos": 226, "type": "DATASET", "confidence": 0.9504254460334778}]}, {"text": "Nevertheless, the practical use of similar resources is often biased by their limited coverage because manual annotation is time-consuming and requires a relevant financial effort.", "labels": [], "entities": []}, {"text": "For this reason, some research activities have focused on the automatic enrichment of such resources with annotated information in (near) manual quality.", "labels": [], "entities": []}, {"text": "The main strategy proposed was the mapping between resources in order to reciprocally enrich different lexical databases by linking their information layers.", "labels": [], "entities": []}, {"text": "This has proved to be useful in several tasks, from verb classification ( to semantic role labeling (), open text semantic parsing () and textual entailment).", "labels": [], "entities": [{"text": "verb classification", "start_pos": 52, "end_pos": 71, "type": "TASK", "confidence": 0.7966552078723907}, {"text": "semantic role labeling", "start_pos": 77, "end_pos": 99, "type": "TASK", "confidence": 0.635676364103953}, {"text": "open text semantic parsing", "start_pos": 104, "end_pos": 130, "type": "TASK", "confidence": 0.7276725769042969}, {"text": "textual entailment", "start_pos": 138, "end_pos": 156, "type": "TASK", "confidence": 0.7146436721086502}]}, {"text": "In this work, we focus on the automatic enrichment of the FrameNet database for English and we propose anew framework to extend this procedure to new languages.", "labels": [], "entities": [{"text": "FrameNet database", "start_pos": 58, "end_pos": 75, "type": "DATASET", "confidence": 0.8604061305522919}]}, {"text": "While similar works in the past have mainly proposed to automatically extend the FrameNet database by mapping frames and WordNet synsets,, and), we present an explorative approach that for the first time exploits Wikipedia to this purpose.", "labels": [], "entities": [{"text": "FrameNet database", "start_pos": 81, "end_pos": 98, "type": "DATASET", "confidence": 0.8559826910495758}]}, {"text": "In particular, given a lexical unit l belonging to a frame F , we devise a strategy to link l to the Wikipedia article that best captures the sense of l in F . This is basically a word disambiguation (WSD) problem and to this purpose we employ a state-of-the-art WSD system ().", "labels": [], "entities": [{"text": "word disambiguation (WSD)", "start_pos": 180, "end_pos": 205, "type": "TASK", "confidence": 0.8169076919555665}]}, {"text": "The mapping between (F, l) pairs and Wikipedia pages could then be exploited for three further subtasks: (a) automatically extract from Wikipedia all sentences pointing to the Wikipage mapped with (F, l) and assign them to F ; (b) automatically expand the lexical units sets in the English FrameNet by exploiting the redirecting and linking strategy of Wikipedia; and (c) since Wikipedia is available in 260 languages, use the English Wikipedia article linked to (F, l) as abridge to carryout sentence and lexical unit retrieval in other languages.", "labels": [], "entities": []}, {"text": "The set of automatically collected data would represent the starting point for the creation of FrameNet in new languages.", "labels": [], "entities": []}, {"text": "In fact, having a repository of sentences extracted from Wikipedia which have already been divided by sense would significantly speedup the annotation process.", "labels": [], "entities": []}, {"text": "In this way, the annotators would not need to extract all sentences in a corpus containing land classify them by sense.", "labels": [], "entities": []}, {"text": "Instead, they should simply validate the given sentences and assign the correct frame elements.", "labels": [], "entities": []}, {"text": "In the following, we start by providing a brief overview of FrameNet and Wikipedia and we present their structure and organization.", "labels": [], "entities": []}, {"text": "Next, we describe the algorithm for mapping lexical units and Wikipages and the word sense disambiguation algorithm employed by the system.", "labels": [], "entities": [{"text": "word sense disambiguation", "start_pos": 80, "end_pos": 105, "type": "TASK", "confidence": 0.677771657705307}]}, {"text": "In Section 5 we describe the dataset used in the first experiment and report evaluation results of the mapping between (F, l) pairs and Wikipedia senses.", "labels": [], "entities": []}, {"text": "In Section 6 we describe an application of the mapping, i.e. the automatic enrichment of English FrameNet.", "labels": [], "entities": [{"text": "English FrameNet", "start_pos": 89, "end_pos": 105, "type": "DATASET", "confidence": 0.8848424851894379}]}, {"text": "We describe the data extraction process and evaluate the quality of the data.", "labels": [], "entities": [{"text": "data extraction", "start_pos": 16, "end_pos": 31, "type": "TASK", "confidence": 0.7210759818553925}]}, {"text": "In Section 7 we describe and evaluate another application of the mapping, i.e. the acquisition of data for the automatic creation of Italian FrameNet using the Italian Wikipedia.", "labels": [], "entities": [{"text": "Italian FrameNet", "start_pos": 133, "end_pos": 149, "type": "DATASET", "confidence": 0.8836092948913574}, {"text": "Italian Wikipedia", "start_pos": 160, "end_pos": 177, "type": "DATASET", "confidence": 0.8373043537139893}]}, {"text": "Finally, we draw conclusions and present future research directions.", "labels": [], "entities": []}], "datasetContent": [{"text": "Similarly to the data extraction process described in Section 6, we consider for every (F, l) pair in English the linked Wikipedia sense s, in English as well.", "labels": [], "entities": [{"text": "data extraction", "start_pos": 17, "end_pos": 32, "type": "TASK", "confidence": 0.7237776517868042}]}, {"text": "Then, we retrieve the Italian Wikipedia sense s i linked to sand extract all sentences Ci in the Italian Wikipedia dump 7 with a reference to s i . In this way, we can assume that Ci are example sentences of F and that the words or expressions W i in Ci containing an embedded reference to s i are good candidate lexical units of F in the Italian FrameNet.", "labels": [], "entities": [{"text": "Italian Wikipedia dump 7", "start_pos": 97, "end_pos": 121, "type": "DATASET", "confidence": 0.8737521469593048}, {"text": "Italian FrameNet", "start_pos": 339, "end_pos": 355, "type": "DATASET", "confidence": 0.857731282711029}]}, {"text": "For example, if we link http: //en.wikipedia.org/wiki/Court to the JUDI-CIAL BODY frame, we first retrieve the Italian version of the site http://it.wikipedia.org/ wiki/Tribunale.", "labels": [], "entities": [{"text": "JUDI-CIAL BODY", "start_pos": 67, "end_pos": 81, "type": "METRIC", "confidence": 0.6196256577968597}]}, {"text": "Then, with a top-down strategy, we further extract all Italian sentences pointing to the Tribunale page and acquire as lexical units all words with an embedded reference to this concept, for example tribunale and corte.", "labels": [], "entities": [{"text": "Tribunale page", "start_pos": 89, "end_pos": 103, "type": "DATASET", "confidence": 0.8541557490825653}]}, {"text": "In this way, we can include the extracted lexical units and the sentences where they occur in the JUDI-CIAL BODY frame for Italian.", "labels": [], "entities": [{"text": "JUDI-CIAL", "start_pos": 98, "end_pos": 107, "type": "METRIC", "confidence": 0.9344269037246704}, {"text": "BODY", "start_pos": 108, "end_pos": 112, "type": "METRIC", "confidence": 0.6192939877510071}]}, {"text": "Given the 893 (F, l) pairs in English and the linked Wikipedia senses described in 6.2, we first extracted the Italian Wikipages that are linked to the English ones.", "labels": [], "entities": []}, {"text": "Then for every linked Wikipage in Italian, we retrieved all sentences with a reference pointing to that page in the Italian Wikipedia dump.", "labels": [], "entities": [{"text": "Italian Wikipedia dump", "start_pos": 116, "end_pos": 138, "type": "DATASET", "confidence": 0.8281176686286926}]}, {"text": "Statistics about the extracted data are reported in.", "labels": [], "entities": []}, {"text": "Since the Italian Wikipedia is about one fifth of the English one, it was not possible to map every English Wikipage with an Italian article.", "labels": [], "entities": []}, {"text": "In fact, only 371 senses out of 893 in English were linked to an Italian page.", "labels": [], "entities": []}, {"text": "Also the average num-.", "labels": [], "entities": [{"text": "num-", "start_pos": 17, "end_pos": 21, "type": "METRIC", "confidence": 0.9506584405899048}]}, {"text": "Anyhow, this does not represent a problem because in the English FrameNet, the lexical units whose annotation is considered to be complete are usually instantiated by set of 20 annotated sentences on average.", "labels": [], "entities": [{"text": "English FrameNet", "start_pos": 57, "end_pos": 73, "type": "DATASET", "confidence": 0.8440543711185455}]}, {"text": "So, according to the FrameNet standard, 60 sentences are more than enough to represent the meaning of a lexical unit in a frame.", "labels": [], "entities": []}, {"text": "In this evaluation part, we took into account 1,000 sentences, in order to have a comparable dataset w.r.t. the evaluation for English.", "labels": [], "entities": []}, {"text": "However, the sets of Italian sentences extracted for every (F, l), i.e. for every Wikipedia article, were much smaller, so we increased the number of randomly chosen (F, l) pairs to 80.", "labels": [], "entities": []}, {"text": "Our evaluation is focused on the quality of the sentences and aims at assessing if the given sentences are correctly assigned to the (F, l) pairs.", "labels": [], "entities": []}, {"text": "We report 69% accuracy, which is 9% lower than for English.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 14, "end_pos": 22, "type": "METRIC", "confidence": 0.999685525894165}]}, {"text": "Apart from the same errors and issues reported for English, a decrease in performance can be explained by the fact that, since less articles are present w.r.t. the English version, redirections and internal links tend to be less precise and fine-grained.", "labels": [], "entities": []}, {"text": "For example, the word \"diritti\" in the sense of \"(human) rights\" redirects to the article about Diritto, corresponding to Law as a system of rules.", "labels": [], "entities": []}, {"text": "On the contrary, Law and Rights have two different pages in English.", "labels": [], "entities": []}, {"text": "Besides, the different quality of the two resources can also depend on the smaller number of users that edit and check the Italian articles.", "labels": [], "entities": []}, {"text": "From the 1,000 sentences evaluated we extracted 145 new lexical units: since Italian FrameNet does not exist yet, every lexical unit in a sentence that is correct can be straightforwardly included in the first version of the resource.", "labels": [], "entities": [{"text": "Italian FrameNet", "start_pos": 77, "end_pos": 93, "type": "DATASET", "confidence": 0.8918768167495728}]}], "tableCaptions": []}