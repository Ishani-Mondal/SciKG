{"title": [{"text": "Automatically Evaluating Content Selection in Summarization without Human Models", "labels": [], "entities": [{"text": "Automatically Evaluating Content Selection", "start_pos": 0, "end_pos": 42, "type": "TASK", "confidence": 0.7863876298069954}, {"text": "Summarization", "start_pos": 46, "end_pos": 59, "type": "TASK", "confidence": 0.7537958025932312}]}], "abstractContent": [{"text": "We present a fully automatic method for content selection evaluation in summariza-tion that does not require the creation of human model summaries.", "labels": [], "entities": [{"text": "content selection evaluation", "start_pos": 40, "end_pos": 68, "type": "TASK", "confidence": 0.8045661449432373}]}, {"text": "Our work capitalizes on the assumption that the distribution of words in the input and an informative summary of that input should be similar to each other.", "labels": [], "entities": []}, {"text": "Results on a large scale evaluation from the Text Analysis Conference show that input-summary comparisons are very effective for the evaluation of content selection.", "labels": [], "entities": [{"text": "Text Analysis Conference", "start_pos": 45, "end_pos": 69, "type": "TASK", "confidence": 0.7412476738293966}, {"text": "content selection", "start_pos": 147, "end_pos": 164, "type": "TASK", "confidence": 0.7042984962463379}]}, {"text": "Our automatic methods rank participating systems similarly to manual model-based pyramid evaluation and to manual human judgments of responsiveness.", "labels": [], "entities": []}, {"text": "The best feature, Jensen-Shannon divergence, leads to a correlation as high as 0.88 with manual pyramid and 0.73 with responsiveness evaluations.", "labels": [], "entities": []}], "introductionContent": [{"text": "The most commonly used evaluation method for summarization during system development and for reporting results in publications is the automatic evaluation metric ROUGE).", "labels": [], "entities": [{"text": "summarization", "start_pos": 45, "end_pos": 58, "type": "TASK", "confidence": 0.985029399394989}, {"text": "ROUGE", "start_pos": 162, "end_pos": 167, "type": "METRIC", "confidence": 0.7704129219055176}]}, {"text": "ROUGE compares system summaries against one or more model summaries by computing n-gram word overlaps between the two.", "labels": [], "entities": []}, {"text": "The wide adoption of such automatic measures is understandable because they are convenient and greatly reduce the complexity of evaluations.", "labels": [], "entities": []}, {"text": "ROUGE scores also correlate well with manual evaluations of content based on comparison with a single model summary, as used in the early editions of the Document Understanding Conferences (.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 0, "end_pos": 5, "type": "METRIC", "confidence": 0.8633300065994263}, {"text": "Document Understanding Conferences", "start_pos": 154, "end_pos": 188, "type": "TASK", "confidence": 0.7742724816004435}]}, {"text": "In our work, we take the idea of automatic evaluation to an extreme and explore the feasibility of developing a fully automatic evaluation method for content selection that does not make use of human model summaries at all.", "labels": [], "entities": [{"text": "content selection", "start_pos": 150, "end_pos": 167, "type": "TASK", "confidence": 0.7175315022468567}]}, {"text": "To this end, we show that evaluating summaries by comparing them with the input obtains good correlations with manual evaluations for both query focused and update summarization tasks.", "labels": [], "entities": [{"text": "summaries", "start_pos": 37, "end_pos": 46, "type": "TASK", "confidence": 0.9687179327011108}]}, {"text": "Our results have important implications for future development of summarization systems and their evaluation.", "labels": [], "entities": [{"text": "summarization", "start_pos": 66, "end_pos": 79, "type": "TASK", "confidence": 0.9861788749694824}]}, {"text": "High correlations between system ranking produced with the fully automatic method and manual evaluations show that the new evaluation measures can be used during system development when human model summaries are not available.", "labels": [], "entities": []}, {"text": "Our results provide validation of several features that can be optimized in the development of new summarization systems when the objective is to improve content selection on average, over a collection of test inputs.", "labels": [], "entities": []}, {"text": "However, none of the features is consistently predictive of good summary content for individual inputs.", "labels": [], "entities": []}, {"text": "We find that content selection performance on standard test collections can be approximated well by the proposed fully automatic method.", "labels": [], "entities": [{"text": "content selection", "start_pos": 13, "end_pos": 30, "type": "TASK", "confidence": 0.7291170060634613}]}, {"text": "This result greatly underlines the need to require linguistic quality evaluations alongside content selection ones in future evaluations and research.", "labels": [], "entities": []}], "datasetContent": [{"text": "Proposals for developing fully automatic methods for summary evaluation have been put forward in the past.", "labels": [], "entities": [{"text": "summary evaluation", "start_pos": 53, "end_pos": 71, "type": "TASK", "confidence": 0.9288533627986908}]}, {"text": "Their attractiveness is obvious for large scale evaluations, or for evaluation on nonstandard test sets for which human models are not available.", "labels": [], "entities": []}, {"text": "For example in, a large scale fully automatic evaluation of eight summarization systems on 18,000 documents was performed without any human effort.", "labels": [], "entities": []}, {"text": "A search engine was used to rank documents according to their relevance to a given query.", "labels": [], "entities": []}, {"text": "The summaries for each document were also ranked for relevance with respect to the same query.", "labels": [], "entities": []}, {"text": "For good summarization systems, the relevance ranking of summaries is expected to be similar to that of the full documents.", "labels": [], "entities": [{"text": "summarization", "start_pos": 9, "end_pos": 22, "type": "TASK", "confidence": 0.9804797172546387}]}, {"text": "Based on this intuition, the correlation between relevance rankings of summaries and original documents was used to compare the different systems.", "labels": [], "entities": []}, {"text": "The approach was motivated by the assumption that the distribution of terms in a good summary is similar to the distribution of terms in the original document.", "labels": [], "entities": []}, {"text": "Even earlier, suggested that there are considerable benefits to be had in adopting model-free methods of evaluation involving direct comparisons between the original document and its summary.", "labels": [], "entities": []}, {"text": "The motivation for their work was the considerable variation in content selection choices in model summaries (.", "labels": [], "entities": []}, {"text": "The identity of the model writer significantly affects summary evaluations (also noted by,) and evaluations of the same systems can be rather different when different models are used.", "labels": [], "entities": []}, {"text": "In their experiments, demonstrated that the correlations between manual evaluation using a model summary and a) manual evaluation using a different model summary b) automatic evaluation by directly comparing input and summary 1 , are the same.", "labels": [], "entities": []}, {"text": "Their conclusion was that such automatic methods should be seriously considered as an alternative to model based evaluation.", "labels": [], "entities": []}, {"text": "In this paper, we present a comprehensive study of fully automatic summary evaluation without any human models.", "labels": [], "entities": [{"text": "summary evaluation", "start_pos": 67, "end_pos": 85, "type": "TASK", "confidence": 0.8539451360702515}]}, {"text": "A summary's content is judged for quality by directly estimating its closeness to the input.", "labels": [], "entities": []}, {"text": "We compare several probabilistic and information-theoretic approaches for characterizing the similarity and differences between input and summary content.", "labels": [], "entities": []}, {"text": "A simple informationtheoretic measure, Jensen Shannon divergence between input and summary, emerges as the best fea-ture.", "labels": [], "entities": []}, {"text": "System rankings produced using this measure lead to correlations as high as 0.88 with human judgements.", "labels": [], "entities": []}, {"text": "Both manual and automatic evaluations were conducted at NIST to assess the quality of summaries: Spearman correlation between manual scores and ROUGE-1 and ROUGE-2 recall.", "labels": [], "entities": [{"text": "NIST", "start_pos": 56, "end_pos": 60, "type": "DATASET", "confidence": 0.9511333107948303}, {"text": "Spearman correlation", "start_pos": 97, "end_pos": 117, "type": "METRIC", "confidence": 0.8492767512798309}, {"text": "ROUGE-1", "start_pos": 144, "end_pos": 151, "type": "METRIC", "confidence": 0.9757382273674011}, {"text": "ROUGE-2", "start_pos": 156, "end_pos": 163, "type": "METRIC", "confidence": 0.8331416845321655}, {"text": "recall", "start_pos": 164, "end_pos": 170, "type": "METRIC", "confidence": 0.566382110118866}]}, {"text": "All correlations are highly significant with p-value < 0.00001.", "labels": [], "entities": []}, {"text": "Pyramid evaluation: The pyramid evaluation method () has been developed for reliable and diagnostic assessment of content selection quality in summarization and has been used in several large scale evaluations (.", "labels": [], "entities": [{"text": "Pyramid evaluation", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.7072303146123886}, {"text": "summarization", "start_pos": 143, "end_pos": 156, "type": "TASK", "confidence": 0.959528386592865}]}, {"text": "It uses multiple human models from which annotators identify semantically defined Summary Content Units (SCU).", "labels": [], "entities": []}, {"text": "Each SCU is assigned a weight equal to the number of human model summaries that express that SCU.", "labels": [], "entities": []}, {"text": "An ideal maximally informative summary would express a subset of the most highly weighted SCUs, with multiple maximally informative summaries being possible.", "labels": [], "entities": []}, {"text": "The pyramid score fora system summary is equal to the ratio between the sum of weights of SCUs expressed in a summary (again identified manually) and the sum of weights of an ideal summary with the same number of SCUs.", "labels": [], "entities": []}, {"text": "Four human summaries provided by NIST for each input and task were used for the pyramid evaluation at TAC.", "labels": [], "entities": [{"text": "NIST", "start_pos": 33, "end_pos": 37, "type": "DATASET", "confidence": 0.8959765434265137}, {"text": "TAC", "start_pos": 102, "end_pos": 105, "type": "DATASET", "confidence": 0.8306650519371033}]}, {"text": "Responsiveness evaluation: Responsiveness of a summary is a measure of overall quality combining both content selection and linguistic quality: summaries must present useful content in a structured fashion in order to better satisfy the user's need.", "labels": [], "entities": []}, {"text": "Assessors directly assigned scores on a scale of 1 (poor summary) to 5 (very good summary) to each summary.", "labels": [], "entities": []}, {"text": "These assessments are done without reference to any model summaries.", "labels": [], "entities": []}, {"text": "The (Spearman) correlation between the pyramid and responsiveness metrics is high but not perfect: 0.88 and 0.92 respectively for query focused and update summarization.", "labels": [], "entities": []}, {"text": "ROUGE evaluation: NIST also evaluated the summaries automatically using ROUGE.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 0, "end_pos": 5, "type": "METRIC", "confidence": 0.7769607901573181}, {"text": "NIST", "start_pos": 18, "end_pos": 22, "type": "DATASET", "confidence": 0.922825276851654}]}, {"text": "Comparison between a summary and the set of four model summaries is computed using unigram (R1) and bigram overlaps (R2) . The correlations between ROUGE and manual evaluations is shown in and varies between 0.80 and 0.94.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 148, "end_pos": 153, "type": "METRIC", "confidence": 0.8314656615257263}]}, {"text": "Linguistic quality evaluation: Assessors scored summaries on a scale from 1 (very poor) to 5 (very good) for five factors of linguistic quality: grammaticality, non-redundancy, referential clarity, focus, structure and coherence.", "labels": [], "entities": []}, {"text": "We do not make use of any of the linguistic quality evaluations.", "labels": [], "entities": []}, {"text": "Our work focuses on fully automatic evaluation of content selection, so manual pyramid and responsiveness scores are used for comparison with our automatic method.", "labels": [], "entities": [{"text": "pyramid", "start_pos": 79, "end_pos": 86, "type": "METRIC", "confidence": 0.8459105491638184}]}, {"text": "The pyramid metric measures content selection exclusively, while responsiveness incorporates at least some aspects of linguistic quality.", "labels": [], "entities": []}, {"text": "We describe three classes of features to compare input and summary content: distributional similarity, summary likelihood and use of topic signatures.", "labels": [], "entities": [{"text": "summary likelihood", "start_pos": 103, "end_pos": 121, "type": "METRIC", "confidence": 0.7717899680137634}]}, {"text": "Both input and summary words were stopword filtered and stemmed before computing the features.", "labels": [], "entities": []}, {"text": "In this section, we report the correlations between system ranking using our automatic features and the manual evaluations.", "labels": [], "entities": []}, {"text": "We studied the predictive power of features in two scenarios.", "labels": [], "entities": []}, {"text": "MACRO LEVEL; PER SYSTEM: The values of features were computed for each summary submitted for evaluation.", "labels": [], "entities": [{"text": "MACRO", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.463155061006546}, {"text": "LEVEL", "start_pos": 6, "end_pos": 11, "type": "METRIC", "confidence": 0.7781575918197632}, {"text": "PER", "start_pos": 13, "end_pos": 16, "type": "METRIC", "confidence": 0.9941301941871643}]}, {"text": "For each system, the feature values were averaged across all inputs.", "labels": [], "entities": []}, {"text": "All participating systems were ranked based on the average value.", "labels": [], "entities": []}, {"text": "Similarly, the average manual score, pyramid or responsiveness, was also computed for each system.", "labels": [], "entities": []}, {"text": "The correlations between the two rankings are shown in.", "labels": [], "entities": []}, {"text": "MICRO LEVEL; PER INPUT: The systems were ranked for each input separately, and correlations between the summary rankings for each input were computed.", "labels": [], "entities": [{"text": "MICRO", "start_pos": 0, "end_pos": 5, "type": "METRIC", "confidence": 0.8973819613456726}, {"text": "LEVEL", "start_pos": 6, "end_pos": 11, "type": "METRIC", "confidence": 0.637350857257843}, {"text": "PER", "start_pos": 13, "end_pos": 16, "type": "METRIC", "confidence": 0.9947445392608643}, {"text": "INPUT", "start_pos": 17, "end_pos": 22, "type": "METRIC", "confidence": 0.7693901658058167}]}, {"text": "The two levels of analysis address different questions: Can we automatically identify system performance across all test inputs (macro level) and can we identify which summaries fora given input were good and which were bad (micro level)?", "labels": [], "entities": []}, {"text": "For the first task, the answer is a definite \"yes\" while for the second task the results are mixed.", "labels": [], "entities": []}, {"text": "In addition, we compare our results to modelbased evaluations using ROUGE and analyze the effects of stemming the input and summary vocabularies.", "labels": [], "entities": []}, {"text": "In order to allow for in-depth discussion, we will analyze our findings only for query focused summaries.", "labels": [], "entities": []}, {"text": "Similar results were obtained for the evaluation of update summaries and are described in Section 7.: Spearman correlation on macro level for the query focused task.", "labels": [], "entities": [{"text": "Spearman correlation", "start_pos": 102, "end_pos": 122, "type": "METRIC", "confidence": 0.8825556337833405}]}, {"text": "All results are highly significant with p-values < 0.000001 except unigram and multinomial summary probability, which are not significant even at the 0.05 level.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Spearman correlation between manual  scores and ROUGE-1 and ROUGE-2 recall. All  correlations are highly significant with p-value <  0.00001.", "labels": [], "entities": [{"text": "ROUGE-1", "start_pos": 58, "end_pos": 65, "type": "METRIC", "confidence": 0.9557239413261414}, {"text": "ROUGE-2", "start_pos": 70, "end_pos": 77, "type": "METRIC", "confidence": 0.9324743151664734}, {"text": "recall", "start_pos": 78, "end_pos": 84, "type": "METRIC", "confidence": 0.8175264596939087}]}, {"text": " Table 2: Spearman correlation on macro level for  the query focused task. All results are highly sig- nificant with p-values < 0.000001 except unigram  and multinomial summary probability, which are  not significant even at the 0.05 level.", "labels": [], "entities": []}, {"text": " Table 3: Spearman correlations at micro level (query focused task). Only the minimum, maximum  values of the significant correlations are reported together with the number and percentage of significant  correlations.", "labels": [], "entities": []}, {"text": " Table 4: Spearman correlations at macro level for update summarization. Results are reported separately  for features comparing update summaries with the update input only or with both update and background  inputs and averaging the two.", "labels": [], "entities": [{"text": "update summarization", "start_pos": 51, "end_pos": 71, "type": "TASK", "confidence": 0.7191214859485626}]}]}