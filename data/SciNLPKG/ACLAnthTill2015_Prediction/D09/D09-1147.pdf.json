{"title": [{"text": "Consensus Training for Consensus Decoding in Machine Translation", "labels": [], "entities": [{"text": "Machine Translation", "start_pos": 45, "end_pos": 64, "type": "TASK", "confidence": 0.6694733947515488}]}], "abstractContent": [{"text": "We propose a novel objective function for dis-criminatively tuning log-linear machine translation models.", "labels": [], "entities": [{"text": "dis-criminatively tuning log-linear machine translation", "start_pos": 42, "end_pos": 97, "type": "TASK", "confidence": 0.5517945408821106}]}, {"text": "Our objective explicitly optimizes the BLEU score of expected n-gram counts, the same quantities that arise in forest-based consensus and minimum Bayes risk decoding methods.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 39, "end_pos": 49, "type": "METRIC", "confidence": 0.9806407988071442}]}, {"text": "Our continuous objective can be optimized using simple gradient ascent.", "labels": [], "entities": []}, {"text": "However, computing critical quantities in the gradient necessitates a novel dynamic program, which we also present here.", "labels": [], "entities": []}, {"text": "Assuming BLEU as an evaluation measure, our objective function has two principle advantages over standard max BLEU tuning.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 9, "end_pos": 13, "type": "METRIC", "confidence": 0.9965744018554688}, {"text": "BLEU", "start_pos": 110, "end_pos": 114, "type": "METRIC", "confidence": 0.9272362589836121}]}, {"text": "First, it specifically optimizes model weights for downstream consensus decoding procedures.", "labels": [], "entities": []}, {"text": "An unexpected second benefit is that it reduces overfitting, which can improve test set BLEU scores when using standard Viterbi decoding.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 88, "end_pos": 92, "type": "METRIC", "confidence": 0.9874231815338135}]}], "introductionContent": [{"text": "Increasing evidence suggests that machine translation decoders should not search fora single top scoring Viterbi derivation, but should instead choose a translation that is sensitive to the model's entire predictive distribution.", "labels": [], "entities": [{"text": "machine translation decoders", "start_pos": 34, "end_pos": 62, "type": "TASK", "confidence": 0.8377024134000143}]}, {"text": "Several recent consensus decoding methods leverage compact representations of this distribution by choosing translations according to n-gram posteriors and expected counts (.", "labels": [], "entities": []}, {"text": "This change in decoding objective suggests a complementary change in tuning objective, to one that optimizes expected n-gram counts directly.", "labels": [], "entities": []}, {"text": "The ubiquitous minimum error rate training (MERT) approach optimizes Viterbi predictions, but does not explicitly boost the aggregated posterior probability of desirable n-grams.", "labels": [], "entities": [{"text": "minimum error rate training (MERT", "start_pos": 15, "end_pos": 48, "type": "METRIC", "confidence": 0.8162696262200674}]}, {"text": "We therefore propose an alternative objective function for parameter tuning, which we call consensus BLEU or CoBLEU, that is designed to maximize the expected counts of the n-grams that appear in reference translations.", "labels": [], "entities": [{"text": "parameter tuning", "start_pos": 59, "end_pos": 75, "type": "TASK", "confidence": 0.7114089876413345}, {"text": "BLEU", "start_pos": 101, "end_pos": 105, "type": "METRIC", "confidence": 0.8580222129821777}, {"text": "CoBLEU", "start_pos": 109, "end_pos": 115, "type": "METRIC", "confidence": 0.8211262822151184}]}, {"text": "To maintain consistency across the translation pipeline, we formulate CoBLEU to share the functional form of BLEU used for evaluation.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 109, "end_pos": 113, "type": "METRIC", "confidence": 0.9974783062934875}]}, {"text": "As a result, CoBLEU optimizes exactly the quantities that drive efficient consensus decoding techniques and precisely mirrors the objective used for fast consensus decoding in.", "labels": [], "entities": []}, {"text": "CoBLEU is a continuous and (mostly) differentiable function that we optimize using gradient ascent.", "labels": [], "entities": []}, {"text": "We show that this function and its gradient are efficiently computable over packed forests of translations generated by machine translation systems.", "labels": [], "entities": []}, {"text": "The gradient includes expectations of products of features and n-gram counts, a quantity that has not appeared in previous work.", "labels": [], "entities": []}, {"text": "We present anew dynamic program which allows the efficient computation of these quantities over translation forests.", "labels": [], "entities": []}, {"text": "The resulting gradient ascent procedure does not require any k-best approximations.", "labels": [], "entities": [{"text": "gradient ascent", "start_pos": 14, "end_pos": 29, "type": "TASK", "confidence": 0.8118018507957458}]}, {"text": "Optimizing over translation forests gives similar stability benefits to recent work on lattice-based minimum error rate training (  and large-margin training (.", "labels": [], "entities": []}, {"text": "We developed CoBLEU primarily to complement consensus decoding, which it does; it produces higher BLEU scores than coupling MERT with consensus decoding.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 98, "end_pos": 102, "type": "METRIC", "confidence": 0.9990031123161316}]}, {"text": "However, we found an additional empirical benefit: CoBLEU is less prone to overfitting than MERT, even when using Viterbi decoding.", "labels": [], "entities": [{"text": "MERT", "start_pos": 92, "end_pos": 96, "type": "METRIC", "confidence": 0.8934582471847534}]}, {"text": "In experiments, models trained to maximize tuning set BLEU using MERT consistently degraded in performance from tuning to test set, while CoBLEU-trained models generalized more robustly.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 54, "end_pos": 58, "type": "METRIC", "confidence": 0.8948869109153748}, {"text": "MERT", "start_pos": 65, "end_pos": 69, "type": "METRIC", "confidence": 0.910560131072998}]}, {"text": "As a result, we found that optimizing CoBLEU improved test set performance reliably using consensus decoding and occasionally using Viterbi decoding.", "labels": [], "entities": []}, {"text": "Figure 1: (a) A simple hypothesis space of translations fora single sentence containing three alternatives, each with two features.", "labels": [], "entities": []}, {"text": "The hypotheses are scored under a log-linear model with parameters \u03b8 equal to the identity vector.", "labels": [], "entities": []}, {"text": "(b) The expected counts of all bigrams that appear in the computation of consensus bigram precision.", "labels": [], "entities": [{"text": "precision", "start_pos": 90, "end_pos": 99, "type": "METRIC", "confidence": 0.8526754975318909}]}], "datasetContent": [{"text": "We compared CoBLEU training with an implementation of minimum error rate training on two language pairs.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Performance measured by BLEU using a con- sensus decoding method over translation forests shows  an improvement over MERT when using CoBLEU  training. The first two conditions were initialized by  0 vectors. The third condition was initialized by the  final parameters of MERT training. Br. indicates the  brevity penalty on the test set. The * indicates differ- ences which are not statistically significant.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 34, "end_pos": 38, "type": "METRIC", "confidence": 0.9947132468223572}, {"text": "MERT", "start_pos": 127, "end_pos": 131, "type": "METRIC", "confidence": 0.6617575287818909}, {"text": "Br.", "start_pos": 297, "end_pos": 300, "type": "METRIC", "confidence": 0.994354248046875}]}, {"text": " Table 2: Performance measured by BLEU using Viterbi  decoding indicates that CoBLEU is less prone to over- fitting than MERT.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 34, "end_pos": 38, "type": "METRIC", "confidence": 0.997751772403717}, {"text": "CoBLEU", "start_pos": 78, "end_pos": 84, "type": "METRIC", "confidence": 0.754794716835022}]}]}