{"title": [], "abstractContent": [{"text": "The problem of re-ranking initial retrieval results exploring the intrinsic structure of documents is widely researched in information retrieval (IR) and has attracted a considerable amount of time and study.", "labels": [], "entities": [{"text": "re-ranking initial retrieval", "start_pos": 15, "end_pos": 43, "type": "TASK", "confidence": 0.7213895320892334}, {"text": "information retrieval (IR)", "start_pos": 123, "end_pos": 149, "type": "TASK", "confidence": 0.8656225562095642}]}, {"text": "However, one of the drawbacks is that those algorithms treat queries and documents separately.", "labels": [], "entities": []}, {"text": "Furthermore , most of the approaches are predominantly built upon graph-based methods, which may ignore some hidden information among the retrieval set.", "labels": [], "entities": []}, {"text": "This paper proposes a novel document re-ranking method based on Latent Dirichlet Allocation (LDA) which exploits the implicit structure of the documents with respect to original queries.", "labels": [], "entities": [{"text": "Latent Dirichlet Allocation (LDA)", "start_pos": 64, "end_pos": 97, "type": "METRIC", "confidence": 0.8776986400286356}]}, {"text": "Rather than relying on graph-based techniques to identify the internal structure , the approach tries to find the latent structure of \"topics\" or \"concepts\" in the initial retrieval set.", "labels": [], "entities": []}, {"text": "Then we compute the distance between queries and initial retrieval results based on latent semantic information deduced.", "labels": [], "entities": []}, {"text": "Empirical results demonstrate that the method can comfortably achieve significant improvement over various baseline systems.", "labels": [], "entities": []}], "introductionContent": [{"text": "Consider a traditional IR problem, where there exists a set of documents \u00ed \u00b5\u00ed\u00b4\u00bb in the collection.", "labels": [], "entities": [{"text": "IR problem", "start_pos": 23, "end_pos": 33, "type": "TASK", "confidence": 0.929490715265274}]}, {"text": "In response to an information need (as expressed in a query \u00ed \u00b5\u00ed\u00b1\u009e), the system determines a best fit between the query and the documents and returns a list of retrieval results, sorted in a decreasing order of their relevancy.", "labels": [], "entities": []}, {"text": "In practice, high precision at the top rankings of the returned results is of particular interest.", "labels": [], "entities": [{"text": "precision", "start_pos": 18, "end_pos": 27, "type": "METRIC", "confidence": 0.9984096884727478}]}, {"text": "Generally, there are two ways to automatically assist in achieving this ultimate goal after an initial retrieval process): document reranking and query expansion/re-weighting.", "labels": [], "entities": [{"text": "query expansion", "start_pos": 146, "end_pos": 161, "type": "TASK", "confidence": 0.7357621192932129}]}, {"text": "Since the latter normally need a second round of retrieval process, our method focuses on the document re-ranking approach.", "labels": [], "entities": []}, {"text": "We will focus on adjusting the ranking positions directly over initial retrieval results set \u00ed \u00b5\u00ed\u00b4\u00bb \u00ed \u00b5\u00ed\u00b1\u0096\u00ed \u00b5\u00ed\u00b1\u009b\u00ed \u00b5\u00ed\u00b1\u0096\u00ed \u00b5\u00ed\u00b1\u00a1 . Recently, there is a trend of exploring the hidden structure of documents to re-rank results.", "labels": [], "entities": []}, {"text": "Some of the approaches represent the document entities as a connected graph \u00ed \u00b5\u00ed\u00b0\u00ba.", "labels": [], "entities": []}, {"text": "It is usually constructed by links inferred from the content information as a nearest-neighbor graph.", "labels": [], "entities": []}, {"text": "For example, proposed an affinity ranking graph to re-rank search results by optimizing diversity and information richness.", "labels": [], "entities": []}, {"text": "introduced a structural reranking approach by exploiting asymmetric relationships between documents induced by language models.; use a family of semi-supervised machine learning methods among documents graph constructed by incorporating different evidences.", "labels": [], "entities": []}, {"text": "However in this work we are more interested in adopting an automatic approach.", "labels": [], "entities": []}, {"text": "There are two important factors that should betaken into account when designing any reranking algorithms: the original queries and initial retrieval scores.", "labels": [], "entities": [{"text": "betaken", "start_pos": 44, "end_pos": 51, "type": "TASK", "confidence": 0.9493638277053833}]}, {"text": "One of issues is that previous structural re-ranking algorithms treat the query and the content individually when computing re-ranking scores.", "labels": [], "entities": []}, {"text": "Each document is assigned a score independent of other documents without considering of queries.", "labels": [], "entities": []}, {"text": "The problem we want to address in this paper is how we can leverage the interconnections between query and documents for the re-ranking purpose.", "labels": [], "entities": []}, {"text": "Another problem with such approaches concerns the fundamental re-ranking strategy they adopted.", "labels": [], "entities": []}, {"text": "HITS) and PageRank) style algorithms were widely used in the past.", "labels": [], "entities": [{"text": "HITS", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.7773293256759644}]}, {"text": "However, approaches depend only on the structure of the global graph or sub-graph may ignore important information content of a document entity.", "labels": [], "entities": []}, {"text": "As pointed out by, re-ranking algorithms that rely only on the structure of the global graph are likely lead to the problem of topic drift.", "labels": [], "entities": [{"text": "topic drift", "start_pos": 127, "end_pos": 138, "type": "TASK", "confidence": 0.7987118363380432}]}, {"text": "Instead, we introduce anew document reranking method based on Latent Dirichlet Allocation (LDA) () which exploits implicit structure of the documents with respect to original queries.", "labels": [], "entities": [{"text": "Latent Dirichlet Allocation (LDA)", "start_pos": 62, "end_pos": 95, "type": "METRIC", "confidence": 0.8997312287489573}]}, {"text": "Rather than relying on graphbased techniques to identify the internal structure, the approach tries to directly model the latent structure of \"topics\" or \"concepts\" in the initial retrieval set.", "labels": [], "entities": []}, {"text": "Then we can compute the distance between queries and initial retrieval results based on latent semantic information inferred.", "labels": [], "entities": []}, {"text": "To prevent the problem of topic drift, the generative probability of a document is summed overall topics induced.", "labels": [], "entities": [{"text": "topic drift", "start_pos": 26, "end_pos": 37, "type": "TASK", "confidence": 0.8071495890617371}]}, {"text": "By combining the initial retrieval scores calculated by language models, we are able to gather important information for re-ranking purposes.", "labels": [], "entities": []}, {"text": "The intuition behind this method is the hidden structural information among the documents: similar documents are likely to have the same hidden information with respect to a query.", "labels": [], "entities": []}, {"text": "In other words, if a group of documents are talking about the same topic which shares a strong similarity with a query, in our method they will get allocated similar ranking as they are more likely to be relevant to the query.", "labels": [], "entities": []}, {"text": "In addition, the refined ranking scores should be relevant to the initial ranking scores, which, in our method, are combined together with the re-ranking score either using a linear fashion or multiplication process.", "labels": [], "entities": []}, {"text": "To illustrate the effectiveness of the proposed methodology, we apply the framework to ad-hoc document retrieval and compare it with the initial language model-based method and other three PageRank style re-ranking methods.", "labels": [], "entities": []}, {"text": "Experimental results show that the improvement brought by our method is consistent and promising.", "labels": [], "entities": []}, {"text": "The rest of the paper is organized as follows.", "labels": [], "entities": []}, {"text": "Related work on re-ranking algorithms and LDA based methods is briefly summarized in Section 2.", "labels": [], "entities": []}, {"text": "Section 3 describes the re-ranking framework based on latent information induced together with details of how to build generative model.", "labels": [], "entities": []}, {"text": "In Section 4 we report on a series of experiments performed over three different test collections in English and French as well as results obtained.", "labels": [], "entities": []}, {"text": "Finally, Section 5 concludes the paper and speculates on future work.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we will empirically study the effectiveness of the latent document re-ranking method over three different data collections.", "labels": [], "entities": []}, {"text": "Data The text corpus used in our experiment was made up from elements of the CLEF-2007 and CLEF-2008 the European Library (TEL) collections 1 written in English and French.", "labels": [], "entities": [{"text": "CLEF-2007", "start_pos": 77, "end_pos": 86, "type": "DATASET", "confidence": 0.9230723977088928}, {"text": "CLEF-2008 the European Library (TEL) collections", "start_pos": 91, "end_pos": 139, "type": "DATASET", "confidence": 0.8281295299530029}]}, {"text": "These collections are described in greater detail in Table 1.", "labels": [], "entities": []}, {"text": "All of the documents in the experiment were indexed using the . The data tends to be very sparse.", "labels": [], "entities": []}, {"text": "Many records contain only title, author and subject heading information; other records provide more detail.", "labels": [], "entities": []}, {"text": "The average document lengths are 14.66 for BL and 24.19 for BNF collections after pre-processing, respectively.", "labels": [], "entities": [{"text": "BL", "start_pos": 43, "end_pos": 45, "type": "METRIC", "confidence": 0.5898872017860413}]}, {"text": "Please refer to) fora more detailed discussion about this data.", "labels": [], "entities": []}, {"text": "The reason we choose these data collections is that we wanted to test the scalability of the proposed method in different settings and over different guages.", "labels": [], "entities": []}, {"text": "In addition we also select a more tional collection (LAT from CLEF2007) as a test base.", "labels": [], "entities": [{"text": "LAT", "start_pos": 53, "end_pos": 56, "type": "METRIC", "confidence": 0.9084946513175964}, {"text": "CLEF2007", "start_pos": 62, "end_pos": 70, "type": "DATASET", "confidence": 0.8265337944030762}]}, {"text": "We also used the CLEF-2007 and CLEF-2008 query sets.", "labels": [], "entities": [{"text": "CLEF-2007", "start_pos": 17, "end_pos": 26, "type": "DATASET", "confidence": 0.9163726568222046}]}, {"text": "The query sets consist of 50 topics in English for LAT, BL and in French for BNF, all of which were used in the experiment.", "labels": [], "entities": [{"text": "LAT", "start_pos": 51, "end_pos": 54, "type": "METRIC", "confidence": 0.5961332321166992}, {"text": "BL", "start_pos": 56, "end_pos": 58, "type": "METRIC", "confidence": 0.8918370604515076}, {"text": "BNF", "start_pos": 77, "end_pos": 80, "type": "METRIC", "confidence": 0.8162233233451843}]}, {"text": "Each topic is composed of several parts such as: Title, Description, Narrative.", "labels": [], "entities": []}, {"text": "We chose to conduct Title+Description runs as queries.", "labels": [], "entities": []}, {"text": "The queries are processed similarly to the treatment in the test collections.", "labels": [], "entities": []}, {"text": "The relevance judgments are taken from the judged pool of top retrieved documents by various participating retrieval systems from previous CLEF workshops.", "labels": [], "entities": []}, {"text": "We compare the proposed latent re-ranking method with four other approaches: the initial ranker, mentioned above, is a KL-divergence retrieval function using the language models.", "labels": [], "entities": []}, {"text": "Three other baseline systems are: Kurland and Lee's structural re-ranking approach (Recursive Weighted Influx + Language Model), chosen as it demonstrates the best performance in their paper (), Zhang et al.'s affinity graph-based approach ( and a variant of Kurland and Lee's work with links in the graph calculated by the vector-space model (cosine similarity as mentioned in).", "labels": [], "entities": []}, {"text": "We denote these four systems as InR, RWILM, AFF, and VEC respectively.", "labels": [], "entities": [{"text": "InR", "start_pos": 32, "end_pos": 35, "type": "METRIC", "confidence": 0.9791042804718018}, {"text": "RWILM", "start_pos": 37, "end_pos": 42, "type": "METRIC", "confidence": 0.9752801656723022}, {"text": "AFF", "start_pos": 44, "end_pos": 47, "type": "METRIC", "confidence": 0.9952576756477356}, {"text": "VEC", "start_pos": 53, "end_pos": 56, "type": "METRIC", "confidence": 0.9537639617919922}]}, {"text": "Furthermore, we denote the permutations of our methods as follows: LDA1: \u00ed \u00b5\u00ed\u00b1 \u00ed \u00b5\u00ed\u00b1\u00862 \u00ed \u00b5\u00ed\u00b1\u00a4\u00ed \u00b5\u00ed\u00b1\u0096\u00ed \u00b5\u00ed\u00b1\u00a1\u210e \u00ed \u00b5\u00ed\u00b1 \u00ed \u00b5\u00ed\u00b1\u0086 \u00ed \u00b5\u00ed\u00b0\u00bf\u00ed \u00b5\u00ed\u00b0\u00b7\u00ed \u00b5\u00ed\u00b0\u00b4\u00ed \u00b5\u00ed\u00b0\u00b4\u00ed \u00b5\u00ed\u00b0\u00be\u00ed \u00b5\u00ed\u00b0\u00bf1 , LDA2: \u00ed \u00b5\u00ed\u00b1 \u00ed \u00b5\u00ed\u00b1\u00861 \u00ed \u00b5\u00ed\u00b1\u00a4\u00ed \u00b5\u00ed\u00b1\u0096\u00ed \u00b5\u00ed\u00b1\u00a1\u210e \u00ed \u00b5\u00ed\u00b1 \u00ed \u00b5\u00ed\u00b1\u0086 \u00ed \u00b5\u00ed\u00b0\u00bf\u00ed \u00b5\u00ed\u00b0\u00b7\u00ed \u00b5\u00ed\u00b0\u00b4 \u00ed \u00b5\u00ed\u00b0\u00be\u00ed \u00b5\u00ed\u00b0\u00bf1 , LDA3: \u00ed \u00b5\u00ed\u00b1 \u00ed \u00b5\u00ed\u00b1\u00862 \u00ed \u00b5\u00ed\u00b1\u00a4\u00ed \u00b5\u00ed\u00b1\u0096\u00ed \u00b5\u00ed\u00b1\u00a1\u210e \u00ed \u00b5\u00ed\u00b1 \u00ed \u00b5\u00ed\u00b1\u0086 \u00ed \u00b5\u00ed\u00b0\u00bf\u00ed \u00b5\u00ed\u00b0\u00b7\u00ed \u00b5\u00ed\u00b0\u00b4\u00ed \u00b5\u00ed\u00b0\u00b4\u00ed \u00b5\u00ed\u00b0\u00be\u00ed \u00b5\u00ed\u00b0\u00bf2 , LDA4: \u00ed \u00b5\u00ed\u00b1 \u00ed \u00b5\u00ed\u00b1\u00861 \u00ed \u00b5\u00ed\u00b1\u00a4\u00ed \u00b5\u00ed\u00b1\u0096\u00ed \u00b5\u00ed\u00b1\u00a1\u210e \u00ed \u00b5\u00ed\u00b1 \u00ed \u00b5\u00ed\u00b1\u0086 \u00ed \u00b5\u00ed\u00b0\u00bf\u00ed \u00b5\u00ed\u00b0\u00b7\u00ed \u00b5\u00ed\u00b0\u00b4\u00ed \u00b5\u00ed\u00b0\u00b4\u00ed \u00b5\u00ed\u00b0\u00be\u00ed \u00b5\u00ed\u00b0\u00bf2 . Because the inconsistency of the evaluation metrics employed in the past work, we choose to employ all of them to measure the effectiveness of various approaches.", "labels": [], "entities": [{"text": "LDA4", "start_pos": 341, "end_pos": 345, "type": "DATASET", "confidence": 0.939744234085083}]}, {"text": "These include: mean average precision (MAP), the precision of the top 5 documents (Prec@5), the precision of the top 10 documents (Prec@10), normalized discounted cumulative gain (NDCG)) and Bpref ().", "labels": [], "entities": [{"text": "mean average precision (MAP)", "start_pos": 15, "end_pos": 43, "type": "METRIC", "confidence": 0.9259838660558065}, {"text": "precision", "start_pos": 49, "end_pos": 58, "type": "METRIC", "confidence": 0.9974130988121033}, {"text": "precision", "start_pos": 96, "end_pos": 105, "type": "METRIC", "confidence": 0.9953457713127136}, {"text": "normalized discounted cumulative gain (NDCG))", "start_pos": 141, "end_pos": 186, "type": "METRIC", "confidence": 0.761001284633364}, {"text": "Bpref", "start_pos": 191, "end_pos": 196, "type": "METRIC", "confidence": 0.9974313378334045}]}, {"text": "Statistical-significant differences in performance were determined using a paired t-test at a confidence level of 95%.", "labels": [], "entities": []}, {"text": "It is worth pointing out that the above measurements are not directly comparable with those of the CLEF participants because we restricted our initial pool to a smaller number of documents and the main purpose in the paper is to compare the proposed method with different baseline systems.", "labels": [], "entities": []}, {"text": "The main experimental results are presented in.", "labels": [], "entities": []}, {"text": "The first four rows in each collection specify referencecomparison data.", "labels": [], "entities": []}, {"text": "The first question we are interested in is how our latent re-ranking methods perform (taken as a whole).", "labels": [], "entities": []}, {"text": "It is shown that our methods bring improvements upon the various baselines in 75% of the 48 relevant comparisons (4 latent re-ranking methods \u00d7 4 corpora \u00d7 4 baselines).", "labels": [], "entities": []}, {"text": "Only the algorithm permutation LDA3 performs less well.", "labels": [], "entities": []}, {"text": "Furthermore, our methods are able to achieve the highest performance across all the evaluation metrics over three test collections except in one case (MAP in BL collection).", "labels": [], "entities": [{"text": "MAP", "start_pos": 151, "end_pos": 154, "type": "METRIC", "confidence": 0.8706950545310974}, {"text": "BL collection", "start_pos": 158, "end_pos": 171, "type": "DATASET", "confidence": 0.6406479924917221}]}, {"text": "An even more exciting observation is that in many cases, our methods, even though tuned for MAP, can outperform various baselines for all the evaluation metrics, with statistically significant improvements in many runs.", "labels": [], "entities": [{"text": "MAP", "start_pos": 92, "end_pos": 95, "type": "METRIC", "confidence": 0.7088740468025208}]}, {"text": "A closer examination of the results in reveals some interesting properties.", "labels": [], "entities": []}, {"text": "As expected, the RWILM method bought improvements in many cases in CLEF-2008 test collections.", "labels": [], "entities": [{"text": "RWILM", "start_pos": 17, "end_pos": 22, "type": "METRIC", "confidence": 0.4660884141921997}, {"text": "CLEF-2008 test collections", "start_pos": 67, "end_pos": 93, "type": "DATASET", "confidence": 0.9150376121203104}]}, {"text": "However, the performance over CLEF-2007 collection was somewhat disappointing.", "labels": [], "entities": [{"text": "CLEF-2007 collection", "start_pos": 30, "end_pos": 50, "type": "DATASET", "confidence": 0.9646072387695312}]}, {"text": "This seems to indicate that the language model induced graph method tends to perform better in sparse data rather than longer documents.", "labels": [], "entities": []}, {"text": "Also Language Modeling requires large set training data to be effective, while the complexity of our method is only linear with number of topics and the number of documents for each iteration.", "labels": [], "entities": [{"text": "Language Modeling", "start_pos": 5, "end_pos": 22, "type": "TASK", "confidence": 0.7999913394451141}]}, {"text": "The affinity and vector graph based methods demonstrated poor performance across all the collections.", "labels": [], "entities": []}, {"text": "This maybe due to the fact that the approach) developed focuses more on diversity and information richness and cares less about the precision of the retrieval results while asymmetric graph as constructed by the vector space model fails in capturing important relationship between the documents.", "labels": [], "entities": [{"text": "precision", "start_pos": 132, "end_pos": 141, "type": "METRIC", "confidence": 0.9972419738769531}]}, {"text": "Another observation we can draw from is that the relative performance tends to be stable during test collections written in different languages.", "labels": [], "entities": []}, {"text": "This shows a promising future for studying structure of the documents with respect to queries for re-ranking purpose.", "labels": [], "entities": []}, {"text": "At the same time, efficiency is always an issue in all reranking methods.", "labels": [], "entities": []}, {"text": "Although this is not a primary concern in the current work, it would definitely worth thinking in the future.", "labels": [], "entities": []}, {"text": "We also conducted some experiments over queries constructed by using Title field only.", "labels": [], "entities": []}, {"text": "This forms some more realistic short queries.", "labels": [], "entities": []}, {"text": "The experiments showed very similar results compared to longer queries.", "labels": [], "entities": []}, {"text": "This demonstrates that the query length is a trivial issue in our methods (as in other graph-based structural reranking).", "labels": [], "entities": []}, {"text": "We examined the best and worse performed queries, their performance are generally consistent across all the methods.", "labels": [], "entities": []}, {"text": "This phenomenon should be investigated further in the followup evaluation.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1. Statistics of test collections", "labels": [], "entities": []}, {"text": " Table 2. Experimental Results. For each evaluation setting, improvements over the RWILM baseline  are given in italics (because it has highest performance); statistically significant differences between our  methods and InR, RWILM, AFF, VEC are indicated by o, l, a, v, respectively. Bold highlights the best  results over all algorithms.", "labels": [], "entities": [{"text": "InR", "start_pos": 221, "end_pos": 224, "type": "METRIC", "confidence": 0.8817121982574463}, {"text": "AFF", "start_pos": 233, "end_pos": 236, "type": "METRIC", "confidence": 0.9694815874099731}, {"text": "VEC", "start_pos": 238, "end_pos": 241, "type": "METRIC", "confidence": 0.7976093888282776}]}]}