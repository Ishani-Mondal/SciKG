{"title": [{"text": "Towards Efficient Named-Entity Rule Induction for Customizability", "labels": [], "entities": [{"text": "Efficient Named-Entity Rule Induction", "start_pos": 8, "end_pos": 45, "type": "TASK", "confidence": 0.5994163155555725}]}], "abstractContent": [{"text": "Generic rule-based systems for Information Extraction (IE) have been shown to work reasonably well out-of-the-box, and achieve state-of-the-art accuracy with further domain customization.", "labels": [], "entities": [{"text": "Information Extraction (IE)", "start_pos": 31, "end_pos": 58, "type": "TASK", "confidence": 0.8653630495071412}, {"text": "accuracy", "start_pos": 144, "end_pos": 152, "type": "METRIC", "confidence": 0.9985756874084473}]}, {"text": "However, it is generally recognized that manually building and customiz-ing rules is a complex and labor intensive process.", "labels": [], "entities": []}, {"text": "In this paper, we discuss an approach that facilitates the process of building cus-tomizable rules for Named-Entity Recognition (NER) tasks via rule induction, in the Annotation Query Language (AQL).", "labels": [], "entities": [{"text": "Named-Entity Recognition (NER) tasks", "start_pos": 103, "end_pos": 139, "type": "TASK", "confidence": 0.8151198228200277}]}, {"text": "Given a set of basic features and an annotated document collection , our goal is to generate an initial set of rules with reasonable accuracy, that are in-terpretable and thus can be easily refined by a human developer.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 133, "end_pos": 141, "type": "METRIC", "confidence": 0.99599289894104}]}, {"text": "We present an efficient rule induction process, modeled on a four-stage manual rule development process and present initial promising results with our system.", "labels": [], "entities": [{"text": "rule induction", "start_pos": 24, "end_pos": 38, "type": "TASK", "confidence": 0.8599775433540344}]}, {"text": "We also propose a simple notion of ex-tractor complexity as a first step to quantify the interpretability of an extractor, and study the effect of induction bias and customization of basic features on the accuracy and complexity of induced rules.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 205, "end_pos": 213, "type": "METRIC", "confidence": 0.9979472756385803}]}, {"text": "We demonstrate through experiments that the induced rules have good accuracy and low complexity according to our complexity measure.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 68, "end_pos": 76, "type": "METRIC", "confidence": 0.9993581175804138}, {"text": "complexity", "start_pos": 85, "end_pos": 95, "type": "METRIC", "confidence": 0.9896152019500732}]}], "introductionContent": [{"text": "Named-entity recognition (NER) is the task of identifying mentions of rigid designators from text belonging to named-entity types such as persons, organizations and locations (.", "labels": [], "entities": [{"text": "Named-entity recognition (NER)", "start_pos": 0, "end_pos": 30, "type": "TASK", "confidence": 0.8730488598346711}, {"text": "identifying mentions of rigid designators from text belonging to named-entity types such as persons, organizations and locations", "start_pos": 46, "end_pos": 174, "type": "TASK", "confidence": 0.5857862565252516}]}, {"text": "Generic NER rules have been shown to work reasonably well-out-of-the-box, and with further domain customization (), achieve quality surpassing state-of-the-art results.", "labels": [], "entities": []}, {"text": "summarizes the quality of NER rules out-of-the-box and after domain customization in the GATE) and SystemT () systems, as reported in ( and () respectively.", "labels": [], "entities": [{"text": "NER", "start_pos": 26, "end_pos": 29, "type": "TASK", "confidence": 0.926459789276123}, {"text": "GATE", "start_pos": 89, "end_pos": 93, "type": "DATASET", "confidence": 0.769953191280365}]}, {"text": "Rule-based systems are widely used in enterprise settings due to their explainability.", "labels": [], "entities": []}, {"text": "Rules are transparent, which leads to better explainability of errors.", "labels": [], "entities": []}, {"text": "One can easily identify the cause of a false positive or negative, and refine the rules without affecting other correct results identified by the system.", "labels": [], "entities": []}, {"text": "Furthermore, rules are typically easier to understand by an IE developer and can be customized fora new domain without requiring additional labeled data.", "labels": [], "entities": []}, {"text": "Typically, a rule-based NER system consists of a combination of four categories of rules (): (1) Basic Feature (BF) rules to identify components of an entity such as first name and last name.", "labels": [], "entities": []}, {"text": "(2) Candidate definition (CD) rules to identify complete occurrences of an entity by combining the output of multiple BF rules, e.g., first name followed by last name is a person candidate.", "labels": [], "entities": []}, {"text": "(3) Candidate refinement (CR) rules to refine candidates generated by CD rules.", "labels": [], "entities": [{"text": "Candidate refinement (CR)", "start_pos": 4, "end_pos": 29, "type": "TASK", "confidence": 0.5721949398517608}]}, {"text": "E.g., discard candidate persons contained within organizations.", "labels": [], "entities": []}, {"text": "(4) Consolidation rules (CO) to resolve overlapping candidates generated by multiple CD and CR rules.", "labels": [], "entities": []}, {"text": "A well-known drawback that influences the adoptability of rule-based NER systems is the man-ual effort required to build the rules.", "labels": [], "entities": []}, {"text": "A common approach to address this problem is to build a generic NER extractor and then customize it for specific domains.", "labels": [], "entities": [{"text": "NER extractor", "start_pos": 64, "end_pos": 77, "type": "TASK", "confidence": 0.8482629656791687}]}, {"text": "While this approach partially alleviates the problem, substantial manual effort (in the order of several person weeks) is still required for the two stages as reported in (.", "labels": [], "entities": []}, {"text": "In this paper, we present initial work towards facilitating the process of building a generic NER extractor using induction techniques.", "labels": [], "entities": [{"text": "NER extractor", "start_pos": 94, "end_pos": 107, "type": "TASK", "confidence": 0.9393797814846039}]}, {"text": "Specifically, given as input an annotated document corpus, a set of BF rules, and a default CO rule for each entity type, our goal is to generate a set of CD and CR rules such that the resulting extractor constitutes a good starting point for further refinement by a developer.", "labels": [], "entities": []}, {"text": "Since the generic NER extractor has to be manually customized, a major challenge is to ensure that the generated rules have good accuracy, and, at the same time, that they are not too complex, and consequently interpretable.", "labels": [], "entities": [{"text": "NER extractor", "start_pos": 18, "end_pos": 31, "type": "TASK", "confidence": 0.8401294350624084}, {"text": "accuracy", "start_pos": 129, "end_pos": 137, "type": "METRIC", "confidence": 0.9981139898300171}]}, {"text": "The main contributions in this paper are 1.", "labels": [], "entities": []}, {"text": "An efficient system for NER rule induction, using a highly expressive rule language (AQL) as the target language.", "labels": [], "entities": [{"text": "NER rule induction", "start_pos": 24, "end_pos": 42, "type": "TASK", "confidence": 0.8961619138717651}]}, {"text": "The first phase of rule induction uses a combination of clustering and relative least general generalization (RLGG) techniques to learn CD rules.", "labels": [], "entities": [{"text": "rule induction", "start_pos": 19, "end_pos": 33, "type": "TASK", "confidence": 0.833652138710022}]}, {"text": "The second phase identifies CR rules using a propositional rule learner like JRIP to learn accurate compositions of CD rules.", "labels": [], "entities": [{"text": "JRIP", "start_pos": 77, "end_pos": 81, "type": "DATASET", "confidence": 0.8637003898620605}]}, {"text": "2. Usage of induction biases to enhance the interpretability of rules.", "labels": [], "entities": []}, {"text": "These biases capture the expertise gleaned from manual rule development and constrain the search space in our induction system.", "labels": [], "entities": []}, {"text": "3. Definition of an initial notion of extractor complexity to quantify the interpretability of an extractor and to guide the process of adding induction biases to favor learning less complex extractors.", "labels": [], "entities": []}, {"text": "This is to ensure that the rules are easily customizable by the developer.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluate our system on CoNLL03, a collection of Reuters news stories.", "labels": [], "entities": [{"text": "CoNLL03, a collection of Reuters news stories", "start_pos": 26, "end_pos": 71, "type": "DATASET", "confidence": 0.9462955072522163}]}, {"text": "We used the CoNLL03 training set for induction and report results on the CoNLL03 test collection.", "labels": [], "entities": [{"text": "CoNLL03 training set", "start_pos": 12, "end_pos": 32, "type": "DATASET", "confidence": 0.9687788486480713}, {"text": "induction", "start_pos": 37, "end_pos": 46, "type": "TASK", "confidence": 0.9732844233512878}, {"text": "CoNLL03 test collection", "start_pos": 73, "end_pos": 96, "type": "DATASET", "confidence": 0.9744508067766825}]}, {"text": "The basic features (BFs) form the primary input to our induction system.", "labels": [], "entities": [{"text": "BFs", "start_pos": 20, "end_pos": 23, "type": "METRIC", "confidence": 0.9124407768249512}]}, {"text": "We experimented with three sets of BFs: Initial set(E1): The goal in this setup is to induce an initial set of rules based on a small set of reasonable BFs.", "labels": [], "entities": []}, {"text": "We use a conservative initial set consisting of 15 BFs (5 regular expressions and 10 dictionaries).", "labels": [], "entities": []}, {"text": "Enhanced set (E2): Based on the results of E1, we identify a set of additional domain independent BFs . Five views were added to the existing set in E1 (1 regular expression and 4 dictionaries).", "labels": [], "entities": []}, {"text": "The goal is to observe whether our approach yields reasonable accuracies compared to generic rules developed manually.", "labels": [], "entities": []}, {"text": "Domain customized set (E3): Based on the knowledge of the domain of the training dataset (CoNLL03), we introduced a set of features specific to this dataset.", "labels": [], "entities": []}, {"text": "These included sports-related person, organization and location dictionaries . These views were added to the existing set in E2.", "labels": [], "entities": []}, {"text": "The intended goal is to observe what are the best possible accuracies that could be achieved with BFs customized to a particular domain.", "labels": [], "entities": []}, {"text": "The set of parameters for iterative clustering on which the accuracies reported are : the precision threshold for the RLGGs of the clusters was 70%  and the number of examples covered by each RLGG was 5.", "labels": [], "entities": [{"text": "precision threshold", "start_pos": 90, "end_pos": 109, "type": "METRIC", "confidence": 0.9833607971668243}]}, {"text": "We selected the top 5 clusters from each iteration whose RLGGs crossed this threshold.", "labels": [], "entities": []}, {"text": "If there were no such clusters then we would lower the precision threshold to 35% (half of the threshold).", "labels": [], "entities": [{"text": "precision threshold", "start_pos": 55, "end_pos": 74, "type": "METRIC", "confidence": 0.9798771739006042}]}, {"text": "When no new clusters were formed, we ended the iterations.", "labels": [], "entities": []}, {"text": "Effect of Augmenting Basic Features.", "labels": [], "entities": []}, {"text": "shows the accuracy and complexity of rules induced with the three basic feature sets E1, E2 and E3, respectively . The overall F-measure on the test dataset is 48.5% with E1, it increases to around 53.3% with E2 and is highest at 68.9% with E3.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.999427080154419}, {"text": "F-measure", "start_pos": 127, "end_pos": 136, "type": "METRIC", "confidence": 0.9992390871047974}]}, {"text": "As we increase the number of BFs, the accuracies of the induced extractors increases, at the cost of an increase in complexity.", "labels": [], "entities": [{"text": "accuracies", "start_pos": 38, "end_pos": 48, "type": "METRIC", "confidence": 0.9798170924186707}, {"text": "complexity", "start_pos": 116, "end_pos": 126, "type": "METRIC", "confidence": 0.9577550888061523}]}, {"text": "In particular, the recall increases significantly across the board, and is more prominent between E2 and E3, where the additional domain specific features result in recall increase from 5.9% to 47.5% for ORG.", "labels": [], "entities": [{"text": "recall", "start_pos": 19, "end_pos": 25, "type": "METRIC", "confidence": 0.9997598528862}, {"text": "recall", "start_pos": 165, "end_pos": 171, "type": "METRIC", "confidence": 0.999515175819397}]}, {"text": "The precision increases slightly for PER, but decreases slightly for LOC and ORG with the addition of domain specific features.", "labels": [], "entities": [{"text": "precision", "start_pos": 4, "end_pos": 13, "type": "METRIC", "confidence": 0.9996812343597412}, {"text": "PER", "start_pos": 37, "end_pos": 40, "type": "METRIC", "confidence": 0.8879837989807129}, {"text": "ORG", "start_pos": 77, "end_pos": 80, "type": "METRIC", "confidence": 0.9633122682571411}]}, {"text": "Comparison with manually developed rules.", "labels": [], "entities": []}, {"text": "We compared the induced extractors with the manually developed extractors of (Chiticariu et al., 2010b), heretofore referred to as manual extractors.", "labels": [], "entities": []}, {"text": "(For a detailed analysis, we obtained the extractors from These are the results for the configuration with bias. the authors).", "labels": [], "entities": []}, {"text": "shows the accuracy and complexity of the induced rules with E2 and E3 and the manual extractors for the generic domain and, respectively, customized for the CoNLL03 domain., which is discussed later).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9995751976966858}, {"text": "CoNLL03 domain.", "start_pos": 157, "end_pos": 172, "type": "DATASET", "confidence": 0.9119795858860016}]}, {"text": "Our technique compares reasonably with the manually constructed generic extractor for two of the three entity types; and on precision for all entity types, especially since our system generated the rules in 1 hour, whereas the development of manual rules took much longer . Additional work is required to match the manual customized extractor's performance, primarily due to shortcomings in our current target language.", "labels": [], "entities": [{"text": "precision", "start_pos": 124, "end_pos": 133, "type": "METRIC", "confidence": 0.9995185136795044}]}, {"text": "Recall that our framework is limited to a small subset of AQL constructs for expressing CD and CR rules, and there is a single consolidation rule.", "labels": [], "entities": []}, {"text": "In particular, advanced constructs such as dynamic dictionaries are not supported, and the set of predicates to the Filter construct supported in our system is restricted to predicates over other concepts, which is only a subset of those used in ().", "labels": [], "entities": []}, {"text": "The manual extractors also contain a larger number of rules covering many different cases, improving the accuracy, but also leading to a higher complexity score.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 105, "end_pos": 113, "type": "METRIC", "confidence": 0.9997007846832275}, {"text": "complexity score", "start_pos": 144, "end_pos": 160, "type": "METRIC", "confidence": 0.9719611406326294}]}, {"text": "To better analyze the complexity, we also computed the average rule length for each extractor by dividing the complexity score by the number of AQL views of the extractor.", "labels": [], "entities": []}, {"text": "The average rule length is 1.78 and 1.87 for the induced extractors with E2 and E3, respectively, and 1.9 and 2.1 for the generic and customized extractors of (Chiticariu et al., 2010b), respectively.", "labels": [], "entities": [{"text": "rule length", "start_pos": 12, "end_pos": 23, "type": "METRIC", "confidence": 0.7358166873455048}]}, {"text": "The average rule length increases from the generic extractor to the customized extractor in both cases.", "labels": [], "entities": []}, {"text": "On average, however, an individual induced rule is slightly smaller than a manually developed rule.", "labels": [], "entities": []}, {"text": "The goal of this experiment is to demonstrate the importance of biases in the induction process.", "labels": [], "entities": []}, {"text": "The biases added to the system are broadly of two types: (i) Partition of basic features based on types (ii) Restriction on the type of CD views that can appear in a CR view.", "labels": [], "entities": []}, {"text": "8 Without (i) many semantically similar basic features (especially, regular expressions) would match a given token, leading to an increase in the length of a CD a rule.", "labels": [], "entities": []}, {"text": "For example, in the CD rule[CapsPerson \u2227 CapsOrg]} (\"A FirstNameDict span followed by a CapsPerson span that is also a CapsOrg span\"), CapsPerson and CapsOrg are two very similar regular expressions identifying capitalized phrases that look like person, and respectively, organization names, with small variations (e.g., the former may allow special characters such as '-').", "labels": [], "entities": []}, {"text": "Including both BFs in a CD rule leads to a larger rule that is unintuitive fora developer.", "labels": [], "entities": [{"text": "BFs", "start_pos": 15, "end_pos": 18, "type": "METRIC", "confidence": 0.9377795457839966}]}, {"text": "The former bias excludes such CD rules from consideration.", "labels": [], "entities": []}, {"text": "The latter type of bias prevents CD rules of one type to appear as positive clues fora CR rule of a different type.", "labels": [], "entities": []}, {"text": "For instance, without this bias, one of the CR rules obtained was Per \u21d0, shows the effect (for E2 and E3) on the test dataset of disabling and enabling bias during the induction of CR rules using JRIP.", "labels": [], "entities": [{"text": "JRIP", "start_pos": 196, "end_pos": 200, "type": "DATASET", "confidence": 0.9133013486862183}]}, {"text": "Adding bias improves the precision of the induced rules.", "labels": [], "entities": [{"text": "precision", "start_pos": 25, "end_pos": 34, "type": "METRIC", "confidence": 0.9993938207626343}]}, {"text": "Without bias, however, the system is less constrained in its search for high recall rules, leading to slightly higher overall F measure.", "labels": [], "entities": [{"text": "recall", "start_pos": 77, "end_pos": 83, "type": "METRIC", "confidence": 0.9980687499046326}, {"text": "F measure", "start_pos": 126, "end_pos": 135, "type": "METRIC", "confidence": 0.9925610423088074}]}, {"text": "This comes at the cost of an increase in extractor complexity and average rule length.", "labels": [], "entities": []}, {"text": "For example, for E2, the average rule length decreases from 2.17 to 1.78 after adding the bias.", "labels": [], "entities": []}, {"text": "Overall, our results show that biases lead to less complex extractors with only a very minor effect on accuracy, thus biases are important factors contributing to inducing rules that are understandable and maybe refined by humans.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 103, "end_pos": 111, "type": "METRIC", "confidence": 0.9985890984535217}]}, {"text": "Comparison with other induction systems.", "labels": [], "entities": []}, {"text": "We also experimented with two other induction systems, Aleph and ALP 10 , a package that implements one of the reportedly good information extraction algorithms).", "labels": [], "entities": [{"text": "Aleph", "start_pos": 55, "end_pos": 60, "type": "METRIC", "confidence": 0.9209866523742676}, {"text": "information extraction", "start_pos": 127, "end_pos": 149, "type": "TASK", "confidence": 0.7476350665092468}]}, {"text": "While induction in Aleph A system for inductive logic programming.", "labels": [], "entities": []}, {"text": "See http://www.cs.ox.ac.uk/activities/machlearn/Aleph/aleph.html 10 http://code.google.com/p/alpie/ was performed with the same target language as in our approach, the target language of ALP is JAPE, which has been shown () to lack in some of the constructs (such as minus) that AQL provides and which form apart of our target language (especially the rule refinement phase).", "labels": [], "entities": []}, {"text": "However, despite experimenting with all possible parameter configurations for each of these (in each of E1, E2 and E3 settings), the accuracies obtained were substantially (30-50%) worse and the extractor complexity was much (around 60%) higher when compared to our system (with or without bias).", "labels": [], "entities": []}, {"text": "Additionally, Aleph takes close to three days for induction, whereas both ALP and our system require less than an hour.", "labels": [], "entities": [{"text": "induction", "start_pos": 50, "end_pos": 59, "type": "TASK", "confidence": 0.910028874874115}]}], "tableCaptions": [{"text": " Table 3: Results on CoNLL03 dataset with different basic  feature sets", "labels": [], "entities": [{"text": "CoNLL03 dataset", "start_pos": 21, "end_pos": 36, "type": "DATASET", "confidence": 0.9646917581558228}]}, {"text": " Table 4: Comparison of induced rules (with and without bias) and manually developed rules. (CoNLL03 test dataset)", "labels": [], "entities": [{"text": "CoNLL03 test dataset", "start_pos": 93, "end_pos": 113, "type": "DATASET", "confidence": 0.9362842440605164}]}]}