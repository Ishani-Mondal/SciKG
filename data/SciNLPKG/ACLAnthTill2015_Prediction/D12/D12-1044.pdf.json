{"title": [{"text": "Dynamic Programming for Higher Order Parsing of Gap-Minding Trees", "labels": [], "entities": []}], "abstractContent": [{"text": "We introduce gap inheritance, anew structural property on trees, which provides away to quantify the degree to which intervals of descendants can be nested.", "labels": [], "entities": [{"text": "gap inheritance", "start_pos": 13, "end_pos": 28, "type": "TASK", "confidence": 0.6942388713359833}]}, {"text": "Based on this property , two new classes of trees are derived that provide a closer approximation to the set of plausible natural language dependency trees than some alternative classes of trees: unlike projective trees, a word can have descendants in more than one interval; unlike spanning trees, these intervals cannot be nested in arbitrary ways.", "labels": [], "entities": []}, {"text": "The 1-Inherit class of trees has exactly the same empirical coverage of natural language sentences as the class of mildly non-projective trees, yet the optimal scoring tree can be found in an order of magnitude less time.", "labels": [], "entities": []}, {"text": "Gap-minding trees (the second class) have the property that all edges into an interval of descendants come from the same node, and thus an algorithm which uses only single intervals can produce trees in which anode has descendants in multiple intervals.", "labels": [], "entities": []}], "introductionContent": [{"text": "Dependency parsers vary in what space of possible tree structures they search over when parsing a sentence.", "labels": [], "entities": [{"text": "Dependency parsers", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.7737295031547546}]}, {"text": "One commonly used space is the set of projective trees, in which every node's descendants form a contiguous interval in the input sentence.", "labels": [], "entities": []}, {"text": "Finding the optimal tree in the set of projective trees can be done efficiently), even when the score of a tree depends on higher order factors (.", "labels": [], "entities": []}, {"text": "However, the projectivity assumption is too strict for all natural language dependency trees; for example, only 63.6% of Dutch sentences from the CoNLL-X training set are projective.", "labels": [], "entities": [{"text": "CoNLL-X training set", "start_pos": 146, "end_pos": 166, "type": "DATASET", "confidence": 0.9192150036493937}]}, {"text": "At the other end of the spectrum, some parsers search overall spanning trees, a class of structures much larger than the set of plausible linguistic structures.", "labels": [], "entities": []}, {"text": "The maximum scoring directed spanning tree can be found efficiently when the score of a tree depends only on edge-based factors).", "labels": [], "entities": []}, {"text": "However, it is NP-hard to extend MST to include sibling or grandparent factors.", "labels": [], "entities": [{"text": "MST", "start_pos": 33, "end_pos": 36, "type": "TASK", "confidence": 0.9811526536941528}]}, {"text": "MSTbased non-projective parsers that use higher order factors, utilize different techniques than the basic MST algorithm.", "labels": [], "entities": [{"text": "MSTbased non-projective parsers", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.6581859787305196}]}, {"text": "In addition, learning is done over a relaxation of the problem, so the inference procedures at training and attest time are not identical.", "labels": [], "entities": []}, {"text": "We propose two new classes of trees between projective trees and the set of all spanning trees.", "labels": [], "entities": []}, {"text": "These two classes provide a closer approximation to the set of plausible natural language dependency trees: unlike projective trees, a word can have descendants in more than one interval; unlike spanning trees, these intervals cannot be nested in arbitrary ways.", "labels": [], "entities": []}, {"text": "We introduce gap inheritance, anew structural property on trees, which provides away to quantify the degree to which these intervals can be nested.", "labels": [], "entities": [{"text": "gap inheritance", "start_pos": 13, "end_pos": 28, "type": "TASK", "confidence": 0.696901336312294}]}, {"text": "Different levels of gap inheritance define each of these two classes (Section 3).", "labels": [], "entities": []}, {"text": "The 1-Inherit class of trees (Section 4) has exactly the same empirical coverage of natural language sentences as the class of mildly non-projective trees (), yet the optimal scoring tree can be found in an order of magnitude less time.", "labels": [], "entities": []}, {"text": "Gap-minding trees (the second class) have the property that all edges into an interval of descendants come from the same node.", "labels": [], "entities": []}, {"text": "Non-contiguous intervals are therefore decoupled given this single node, and thus an algorithm which uses only single intervals (as in projective parsing) can produce trees in which anode has descendants in multiple intervals (as in mildly non-projective parsing).", "labels": [], "entities": []}, {"text": "A procedure for finding the optimal scoring tree in this space is given in Section 5, which can be searched in yet another order of magnitude faster than the 1-Inherit class.", "labels": [], "entities": [{"text": "Section 5", "start_pos": 75, "end_pos": 84, "type": "DATASET", "confidence": 0.9135311245918274}]}, {"text": "Unlike the class of spanning trees, it is still tractable to find the optimal tree in these new spaces when higher order factors are included.", "labels": [], "entities": []}, {"text": "An extension which finds the optimal scoring gap-minding tree with scores over pairs of adjacent edges (grandparent scoring) is given in Section 6.", "labels": [], "entities": [{"text": "grandparent scoring", "start_pos": 104, "end_pos": 123, "type": "METRIC", "confidence": 0.902558445930481}]}, {"text": "These gapminding algorithms have been implemented in practice and empirical results are presented in Section 7.", "labels": [], "entities": []}], "datasetContent": [{"text": "The space of projective trees is strictly contained within the space of gap-minding trees which is strictly contained within spanning trees.", "labels": [], "entities": []}, {"text": "Which space is most appropriate for natural language parsing may depend on the particular language and the type and frequencies of non-projective structures found in it.", "labels": [], "entities": [{"text": "natural language parsing", "start_pos": 36, "end_pos": 60, "type": "TASK", "confidence": 0.6699192523956299}]}, {"text": "In this section we compare the parsing accuracy across languages fora parser which uses either the Eisner algorithm (projective), MST (spanning trees), or MaxGapMindingTree (gap-minding trees) as its decoder for both training and inference.", "labels": [], "entities": [{"text": "parsing", "start_pos": 31, "end_pos": 38, "type": "TASK", "confidence": 0.9541543126106262}, {"text": "accuracy", "start_pos": 39, "end_pos": 47, "type": "METRIC", "confidence": 0.716051459312439}]}, {"text": "We implemented both the basic gap-minding algorithm and the gap-minding algorithm with grandparent scoring as extensions to MSTParser 10 . MSTParser () uses the Margin Infused Relaxed Algorithm for discriminative training.", "labels": [], "entities": []}, {"text": "Training requires a decoder which produces the highest scoring tree (in the space of valid trees) under the current model weights.", "labels": [], "entities": []}, {"text": "This same decoder is then used to produce parses attest time.", "labels": [], "entities": []}, {"text": "MSTParser comes packaged with the Eisner algorithm (for projective trees) and MST (for spanning trees).", "labels": [], "entities": [{"text": "MSTParser", "start_pos": 0, "end_pos": 9, "type": "DATASET", "confidence": 0.9377843141555786}]}, {"text": "MSTParser also includes two second order models: one of which is a projective decoder that also scores siblings (Proj+Sib) and the other of which produces non-projective trees by rearranging edges after producing a projective tree (Proj+Sib+Rearr).", "labels": [], "entities": [{"text": "MSTParser", "start_pos": 0, "end_pos": 9, "type": "DATASET", "confidence": 0.9223768711090088}]}, {"text": "We add a further decoder with the algorithm presented here for gap minding trees, and plan to make the extension publicly available.", "labels": [], "entities": [{"text": "gap minding trees", "start_pos": 63, "end_pos": 80, "type": "TASK", "confidence": 0.7967022061347961}]}, {"text": "The gap-minding decoder has both an edge-factored implementation and aversion which scores grandparents as well.", "labels": [], "entities": [{"text": "aversion", "start_pos": 69, "end_pos": 77, "type": "METRIC", "confidence": 0.9966562986373901}]}, {"text": "The gap-minding algorithm is much more efficient when edges have been pruned so that each word has at most k potential parents.", "labels": [], "entities": []}, {"text": "We use the weights from the trained MST models combined with the Matrix Tree Theorem ( to produce marginal probabilities of each edge.", "labels": [], "entities": [{"text": "MST", "start_pos": 36, "end_pos": 39, "type": "TASK", "confidence": 0.9301010370254517}]}, {"text": "We wanted to be able to both achieve the running time bound and yet take advantage of the fact that the size of the set of reasonable parent choices is variable.", "labels": [], "entities": []}, {"text": "We therefore use a hybrid pruning strategy: each word's set of potential parents is the smaller of a) the top k parents (we chose k = 10) or b) the set of parents whose probabilities are above a threshold (we chose th = .001).", "labels": [], "entities": []}, {"text": "The running time for the gap-minding algorithm is then O(kn 4 ); with the grandparent features the gap-minding running time is O(k 2 n 4 ).", "labels": [], "entities": [{"text": "O", "start_pos": 55, "end_pos": 56, "type": "METRIC", "confidence": 0.9978058934211731}, {"text": "O", "start_pos": 127, "end_pos": 128, "type": "METRIC", "confidence": 0.9794372916221619}]}, {"text": "The training and test sets for the six languages come from the CoNLL-X shared task.", "labels": [], "entities": []}, {"text": "We train the gap-minding algorithm on sentences of length at most 100 13 (the vast majority of sentences).", "labels": [], "entities": []}, {"text": "The projective and MST models are trained on all sentences and are run without any pruning.", "labels": [], "entities": []}, {"text": "The Czech training set is much larger than the others and so for Czech only the first 10,000 training sentences were used.", "labels": [], "entities": [{"text": "Czech training set", "start_pos": 4, "end_pos": 22, "type": "DATASET", "confidence": 0.9068648020426432}]}, {"text": "Testing is on the full test set, with no length restrictions.", "labels": [], "entities": []}, {"text": "The results are shown in.", "labels": [], "entities": []}, {"text": "The first three lines show the first order gap-minding decoder compared with the first order projective and MST de- The grandparent features used were identical to the features provided within MSTParser for the second-order sibling parsers, with one exception -many features are conjoined with a direction indicator, which in the projective case has only two possibilities.", "labels": [], "entities": [{"text": "MSTParser", "start_pos": 193, "end_pos": 202, "type": "DATASET", "confidence": 0.9052836298942566}]}, {"text": "We replaced this two-way distinction with a sixway distinction of the six possible orders of the grandparent, parent, and child.", "labels": [], "entities": []}, {"text": "MSTParser produces labeled dependencies on CoNLL formatted input.", "labels": [], "entities": [{"text": "MSTParser", "start_pos": 0, "end_pos": 9, "type": "DATASET", "confidence": 0.8938647508621216}, {"text": "CoNLL formatted input", "start_pos": 43, "end_pos": 64, "type": "DATASET", "confidence": 0.7834276556968689}]}, {"text": "We replace all labels in the training set with a single dummy label to produce unlabeled dependency trees.", "labels": [], "entities": []}, {"text": "Because of long training times, the gap-minding with grandparent models for Portuguese and Swedish were trained on only sentences up to 50 words.: Unlabeled Attachment Scores on the CoNLL-X shared task test set. coders.", "labels": [], "entities": [{"text": "CoNLL-X shared task test set", "start_pos": 182, "end_pos": 210, "type": "DATASET", "confidence": 0.8491583108901978}]}, {"text": "The gap-minding decoder does better than the projective decoder on Czech, Danish, and Dutch, the three languages with the most non-projectivity, even though it was at a competitive disadvantage in terms of both pruning and (on languages with very long sentences) training data.", "labels": [], "entities": []}, {"text": "The gap-minding decoder with grandparent features is better than the projective decoder with sibling features on all six of the languages.", "labels": [], "entities": []}, {"text": "On some languages, the local search decoder with siblings has the absolute highest accuracy in; on other languages (Czech and Swedish) the gap-minding+grandparents has the highest accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 83, "end_pos": 91, "type": "METRIC", "confidence": 0.9986995458602905}, {"text": "accuracy", "start_pos": 180, "end_pos": 188, "type": "METRIC", "confidence": 0.9969351291656494}]}, {"text": "While not directly comparable because of the difference in features, the promising performance of the gap-minding+grandparents decoder shows that the space of gap-minding trees is larger than the space of projective trees, yet unlike spanning trees, it is tractable to find the best tree with higher order features.", "labels": [], "entities": []}, {"text": "It would be interesting to extend the gap-minding algorithm to include siblings as well.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: The number of sentences from the CoNLL-X training sets whose parse trees fall into each of the above  classes. The two new classes of structures, Mild+0-Inherit and Mild+1-Inherit, have more coverage of empirical data  than projective structures, yet can be parsed faster than mildly non-projective structures. Parsing times assume an edge- based factorization with no pruning of edges. The corresponding algorithms for Mild+1-Inherit and Mild+0-Inherit  are in Sections 4 and 5.", "labels": [], "entities": [{"text": "CoNLL-X training sets", "start_pos": 43, "end_pos": 64, "type": "DATASET", "confidence": 0.9026275674502054}]}, {"text": " Table 2: Unlabeled Attachment Scores on the CoNLL-X  shared task test set.", "labels": [], "entities": [{"text": "CoNLL-X  shared task test set", "start_pos": 45, "end_pos": 74, "type": "DATASET", "confidence": 0.8398707151412964}]}]}