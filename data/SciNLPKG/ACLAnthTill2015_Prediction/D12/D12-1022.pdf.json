{"title": [{"text": "Multiple Aspect Summarization Using Integer Linear Programming", "labels": [], "entities": [{"text": "Multiple Aspect Summarization", "start_pos": 0, "end_pos": 29, "type": "TASK", "confidence": 0.8415740927060446}]}], "abstractContent": [{"text": "Multi-document summarization involves many aspects of content selection and surface realization.", "labels": [], "entities": [{"text": "Multi-document summarization", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.7472846508026123}, {"text": "surface realization", "start_pos": 76, "end_pos": 95, "type": "TASK", "confidence": 0.7872138023376465}]}, {"text": "The summaries must be informative, succinct, grammatical, and obey stylistic writing conventions.", "labels": [], "entities": []}, {"text": "We present a method where such individual aspects are learned separately from data (without any hand-engineering) but optimized jointly using an integer linear programme.", "labels": [], "entities": []}, {"text": "The ILP framework allows us to combine the decisions of the expert learners and to select and rewrite source content through a mixture of objective setting, soft and hard constraints.", "labels": [], "entities": []}, {"text": "Experimental results on the TAC-08 data set show that our model achieves state-of-the-art performance using ROUGE and significantly improves the informativeness of the summaries.", "labels": [], "entities": [{"text": "TAC-08 data set", "start_pos": 28, "end_pos": 43, "type": "DATASET", "confidence": 0.9683671394983927}, {"text": "ROUGE", "start_pos": 108, "end_pos": 113, "type": "METRIC", "confidence": 0.9754673838615417}]}], "introductionContent": [{"text": "Automatic summarization has enjoyed wide popularity in natural language processing (see the proceedings of the Document Understanding and Text Analysis conferences) due to its potential for practical applications but also because it incorporates many important aspects of both natural language understanding and generation.", "labels": [], "entities": [{"text": "Automatic summarization", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.7040468752384186}, {"text": "natural language processing", "start_pos": 55, "end_pos": 82, "type": "TASK", "confidence": 0.6307094792524973}, {"text": "Document Understanding and Text Analysis", "start_pos": 111, "end_pos": 151, "type": "TASK", "confidence": 0.7599633693695068}, {"text": "natural language understanding and generation", "start_pos": 277, "end_pos": 322, "type": "TASK", "confidence": 0.7040544390678406}]}, {"text": "Of the many summarization paradigms that have been identified over the years (see Sparck Jones (1999) and for comprehensive overviews), multi-document summarization -the task of producing summaries from clusters of thematically related documents -has consistently attracted attention.", "labels": [], "entities": [{"text": "summarization", "start_pos": 12, "end_pos": 25, "type": "TASK", "confidence": 0.9716843962669373}, {"text": "Sparck Jones (1999)", "start_pos": 82, "end_pos": 101, "type": "DATASET", "confidence": 0.8366825819015503}, {"text": "multi-document summarization", "start_pos": 136, "end_pos": 164, "type": "TASK", "confidence": 0.6657195687294006}, {"text": "summaries from clusters of thematically related documents", "start_pos": 188, "end_pos": 245, "type": "TASK", "confidence": 0.6319589189120701}]}, {"text": "Despite considerable research effort, the automatic generation of multi-document summaries that resemble those written by humans remains challenging.", "labels": [], "entities": [{"text": "automatic generation of multi-document summaries", "start_pos": 42, "end_pos": 90, "type": "TASK", "confidence": 0.5306147158145904}]}, {"text": "This is primarily due to the task itself which is complex and subject to several constraints: the summary must be maximally informative and minimally redundant, grammatical, coherent, adhere to a pre-specified length and stylistic conventions.", "labels": [], "entities": []}, {"text": "An ideal model would learn to output summaries that simultaneously meet all these constraints from data (i.e., document clusters and their corresponding summaries).", "labels": [], "entities": []}, {"text": "This global inference problem is, however, hard -the solution space is large and the lack of easily accessible datasets an obstacle to joint learning.", "labels": [], "entities": []}, {"text": "It is thus no surprise that previous work has focused on specific aspects of joint learning.", "labels": [], "entities": []}, {"text": "Initial global formulations of the multi-document summarization task focused on extractive summarization and used approximate greedy algorithms for finding the sentences of the summary.", "labels": [], "entities": [{"text": "multi-document summarization task", "start_pos": 35, "end_pos": 68, "type": "TASK", "confidence": 0.6571206251780192}, {"text": "extractive summarization", "start_pos": 80, "end_pos": 104, "type": "TASK", "confidence": 0.5806281566619873}]}, {"text": "search for the set of sentences that are both relevant and non-redundant, whereas model multi-document summarization as an instance of the maximum coverage set problem.", "labels": [], "entities": []}, {"text": "More recent work improves on the search problem by considering exact solutions and permits a limited amount of rewriting.", "labels": [], "entities": []}, {"text": "proposes an integer linear programming formulation that maximizes the sum of relevance scores of the selected sentences penalized by the sum of redundancy scores of all pairs of selected sentences.", "labels": [], "entities": []}, {"text": "develop an exact solution fora model similar to under the assumption that the value of a summary is the sum of values of the unique concepts (approximated by bigrams) it contains.", "labels": [], "entities": []}, {"text": "Subsequent work) extends this model to allow sentence compression in the form of word or constituent deletion.", "labels": [], "entities": [{"text": "sentence compression", "start_pos": 45, "end_pos": 65, "type": "TASK", "confidence": 0.7314412295818329}]}, {"text": "In this paper we propose a model for multidocument summarization that attempts to cover many different aspects of the task such as content selection, surface realization, paraphrasing, and stylistic conventions.", "labels": [], "entities": [{"text": "multidocument summarization", "start_pos": 37, "end_pos": 64, "type": "TASK", "confidence": 0.693466305732727}, {"text": "surface realization", "start_pos": 150, "end_pos": 169, "type": "TASK", "confidence": 0.7632956206798553}]}, {"text": "These aspects are learned separately using specific \"expert\" predictors, but are optimized jointly using an integer linear programming model (ILP) to generate the output summary.", "labels": [], "entities": []}, {"text": "All experts are learned from data without requiring additional annotation over and above the summaries written for each document cluster.", "labels": [], "entities": []}, {"text": "Our predictors include the use of unique bigram information to model content and avoid redundancy, positional information to model important and poor locations of content, and language modeling to capture stylistic conventions.", "labels": [], "entities": []}, {"text": "Learning each predictor separately gives better generalization, while the ILP framework allows us to combine the decisions of the expert learners through the use of objectives, hard and soft constraints.", "labels": [], "entities": []}, {"text": "The experts work collaboratively to rewrite the content using rules extracted from document clusters and model summaries.", "labels": [], "entities": []}, {"text": "We adopt the synchronous tree substitution grammar (STSG) formalism) which can model non-isomorphic tree structures (the grammar rules can comprise trees of arbitrary depth) and is thus suited to text-rewriting tasks which typically involve a number of local modifications to the input text.", "labels": [], "entities": [{"text": "synchronous tree substitution grammar (STSG) formalism", "start_pos": 13, "end_pos": 67, "type": "TASK", "confidence": 0.7760132998228073}]}, {"text": "Specifically, we propose quasi-synchronous tree substitution grammar (QTSG) as a flexible formalism to learn general treeedits from loosely-aligned phrase structure trees.", "labels": [], "entities": [{"text": "quasi-synchronous tree substitution grammar (QTSG)", "start_pos": 25, "end_pos": 75, "type": "TASK", "confidence": 0.807195382458823}]}, {"text": "We evaluate our model on the 100-word \"non-2 Our task is standard multi-document summarization and should not be confused with \"guided\" summarization where system and human summarizers are given a list of important aspects to cover in the summary.", "labels": [], "entities": [{"text": "multi-document summarization", "start_pos": 66, "end_pos": 94, "type": "TASK", "confidence": 0.4886985272169113}]}, {"text": "Our usage of the term aspects broadly refers to the different types of constraints (e.g., relating to content or style) a summary must meet, but these are learned rather than specified in advance.", "labels": [], "entities": []}, {"text": "update\" summarization task as defined in the the Text Analysis Conference.", "labels": [], "entities": [{"text": "Text Analysis Conference", "start_pos": 49, "end_pos": 73, "type": "TASK", "confidence": 0.631348063548406}]}, {"text": "Experimental results show that our method obtains performance comparable and in some cases superior to state-of-the-art, in terms of ROUGE and human ratings of summary grammaticality and informativeness.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 133, "end_pos": 138, "type": "METRIC", "confidence": 0.9945989847183228}]}, {"text": "Importantly, there is nothing inherent in our model that is specific to this particular summarization task.", "labels": [], "entities": [{"text": "summarization task", "start_pos": 88, "end_pos": 106, "type": "TASK", "confidence": 0.9123860597610474}]}, {"text": "As all of the different experts are learned from data, it could easily adapt to other summarization styles or conventions as needed.", "labels": [], "entities": []}], "datasetContent": [{"text": "Data Our model was evaluated on the TAC nonupdate multi-document summarization task which involves generating a 100-word-limited summary from a cluster of 10 related input documents; additionally, TAC provides a set of four model summaries for each cluster, written by human experts.", "labels": [], "entities": [{"text": "TAC nonupdate multi-document summarization", "start_pos": 36, "end_pos": 78, "type": "TASK", "confidence": 0.6030464693903923}]}, {"text": "We used the 44 document clusters from TAC-2009 as training data, to learn the different elements of the model.", "labels": [], "entities": [{"text": "TAC-2009", "start_pos": 38, "end_pos": 46, "type": "DATASET", "confidence": 0.8899326920509338}]}, {"text": "The 48 document clusters of TAC-2008 were reserved for the generation of test summaries.", "labels": [], "entities": [{"text": "TAC-2008", "start_pos": 28, "end_pos": 36, "type": "DATASET", "confidence": 0.8532742261886597}]}, {"text": "Training The two components described in Sections 3.3 and 3.4 were trained using binary SVM classifiers, with labels inferred automatically via alignment.", "labels": [], "entities": []}, {"text": "The salience classifier was trained on 102,754 node instances (16,042 positive and 86,712 negative).", "labels": [], "entities": []}, {"text": "The style classifier was trained on 20,443 sentence instances (2,083 positive and 18,360 negative).", "labels": [], "entities": []}, {"text": "We learned the feature weights with a linear SVM, using the software SVM-OOPS).", "labels": [], "entities": []}, {"text": "Because of the high compression rate in this task, sentence alignment leads to an unbalanced data set.", "labels": [], "entities": [{"text": "sentence alignment", "start_pos": 51, "end_pos": 69, "type": "TASK", "confidence": 0.7618341147899628}]}, {"text": "We compensated for this by using different SVM hyper-parameters C + and C \u2212 as the loss multiplier for misclassification of positive and negative training samples respectively.", "labels": [], "entities": []}, {"text": "SVM hyper-parameters were chosen that gave the highest F1 values using 10-fold cross-validation.", "labels": [], "entities": [{"text": "F1", "start_pos": 55, "end_pos": 57, "type": "METRIC", "confidence": 0.9990819692611694}]}, {"text": "The salience SVM obtained a precision of 0.28 and recall of 0.43.", "labels": [], "entities": [{"text": "SVM", "start_pos": 13, "end_pos": 16, "type": "TASK", "confidence": 0.6403144001960754}, {"text": "precision", "start_pos": 28, "end_pos": 37, "type": "METRIC", "confidence": 0.9993072748184204}, {"text": "recall", "start_pos": 50, "end_pos": 56, "type": "METRIC", "confidence": 0.9998588562011719}]}, {"text": "Precision for the style SVM was 0.20 and recall 0.63, respectively.", "labels": [], "entities": [{"text": "Precision", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.9940722584724426}, {"text": "recall", "start_pos": 41, "end_pos": 47, "type": "METRIC", "confidence": 0.9992249011993408}]}, {"text": "The classifiers on their own would thus not be great predictors of salience or style, but in practice they were useful for breaking ties in bigram scores.", "labels": [], "entities": [{"text": "style", "start_pos": 79, "end_pos": 84, "type": "METRIC", "confidence": 0.9567658305168152}]}, {"text": "Aligned sentences from the training data were also used to learn the quasi-synchronous tree substitution grammar, using the process described in Section 3.6.", "labels": [], "entities": []}, {"text": "Rules seen fewer than 3 times were removed, resulting in a total of 339 QTSG rules.", "labels": [], "entities": []}, {"text": "Two unigram language models (see Section 3.5) were trained on the source articles and summaries, respectively.", "labels": [], "entities": []}, {"text": "Their probabilities were compared to give the word list shown in.", "labels": [], "entities": []}, {"text": "We removed words with a source countless than 50, providing a list of 60 lexemes.", "labels": [], "entities": []}, {"text": "The resulting integer linear programmes were solved using SCIP, and it took 55 seconds on average to read in and solve a document cluster problem.", "labels": [], "entities": []}, {"text": "Evaluation We compared our model against two systems.", "labels": [], "entities": []}, {"text": "As a baseline, we used the ICSI-1 extractive system () which is also based on ILP and was highly ranked in the TAC-2008 evaluation.", "labels": [], "entities": [{"text": "ICSI-1 extractive", "start_pos": 27, "end_pos": 44, "type": "TASK", "confidence": 0.501376748085022}, {"text": "TAC-2008 evaluation", "start_pos": 111, "end_pos": 130, "type": "DATASET", "confidence": 0.8379643559455872}]}, {"text": "We also compared against the \"learned phrase compression\" system of Berg-Kirkpatrick et will launch more missions to the moon Figure 2: Sentence representation provided to the ILP.", "labels": [], "entities": [{"text": "ILP", "start_pos": 176, "end_pos": 179, "type": "DATASET", "confidence": 0.8809968829154968}]}, {"text": "(a) The source sentence representation (child nodes condensed for space reasons).", "labels": [], "entities": []}, {"text": "Bigrams are shown in bold, slanted text indicates phrases with high salience scores f S , while said Tuesday is penalized by f LR . Alternative sub-trees (b), (c) and (d) are created using QTSG rules (dashed lines).", "labels": [], "entities": []}, {"text": "The output sentence (see) was generated from sub-trees (b) and al.", "labels": [], "entities": []}, {"text": "(2011) (henceforth B-K), which has the highest reported ROUGE scores that we are aware of.", "labels": [], "entities": [{"text": "B-K", "start_pos": 19, "end_pos": 22, "type": "METRIC", "confidence": 0.949094831943512}, {"text": "ROUGE", "start_pos": 56, "end_pos": 61, "type": "METRIC", "confidence": 0.9868401288986206}]}, {"text": "5 In addition to the full model described in Section 3, we also produced outputs where each of the five components described in Sections 3.2-3.6 were removed, to assess their individual contribution.", "labels": [], "entities": []}, {"text": "We evaluated the output summaries in two ways, using automatic measures and human judgements.", "labels": [], "entities": []}, {"text": "Automatic evaluation was performed with ROUGE () using TAC-2008 parameter settings.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 40, "end_pos": 45, "type": "METRIC", "confidence": 0.9825548529624939}]}, {"text": "We report bigram overlap (ROUGE-2) and skip-bigram (ROUGE-SU4) recall values.", "labels": [], "entities": [{"text": "bigram overlap (ROUGE-2)", "start_pos": 10, "end_pos": 34, "type": "METRIC", "confidence": 0.7412854909896851}, {"text": "skip-bigram (ROUGE-SU4)", "start_pos": 39, "end_pos": 62, "type": "METRIC", "confidence": 0.8280341178178787}, {"text": "recall", "start_pos": 63, "end_pos": 69, "type": "METRIC", "confidence": 0.5805001258850098}]}, {"text": "We also used Translation Edit Rate (TER,) to examine the systems' rewrite potential.", "labels": [], "entities": [{"text": "Translation Edit Rate (TER", "start_pos": 13, "end_pos": 39, "type": "METRIC", "confidence": 0.8546867489814758}]}, {"text": "TER is defined as the minimum number of edits (insertions, deletions, substitutions, and shifts) required to change the system output so that it exactly matches a reference (here, the reference is the most closely aligning source sentence).", "labels": [], "entities": [{"text": "TER", "start_pos": 0, "end_pos": 3, "type": "METRIC", "confidence": 0.9814577698707581}]}, {"text": "The perfect TER score is 0, however note that it can be higher than 1 due to insertions.", "labels": [], "entities": [{"text": "perfect", "start_pos": 4, "end_pos": 11, "type": "METRIC", "confidence": 0.9541233777999878}, {"text": "TER score", "start_pos": 12, "end_pos": 21, "type": "METRIC", "confidence": 0.9438154399394989}]}, {"text": "Our judgement elicitation study was conducted as follows.", "labels": [], "entities": [{"text": "judgement elicitation", "start_pos": 4, "end_pos": 25, "type": "TASK", "confidence": 0.8423396348953247}]}, {"text": "We randomly selected ten document clusters from the test set and generated summaries with our model (and its lesser variations).", "labels": [], "entities": []}, {"text": "We also included the corresponding ICSI-1 and B-K summaries, and one randomly-selected model summary.", "labels": [], "entities": [{"text": "ICSI-1", "start_pos": 35, "end_pos": 41, "type": "DATASET", "confidence": 0.6352468729019165}]}, {"text": "The study was conducted over the Internet using Mechanical Turk and was completed by 54 volunteers, all self reported native English speakers.", "labels": [], "entities": []}, {"text": "Participants were first asked to read the documents in each cluster.", "labels": [], "entities": []}, {"text": "Next, they were asked a few comprehension questions to ensure they had understood and processed the documents.", "labels": [], "entities": []}, {"text": "Finally, they were presented with a summary and asked to rate it along two dimensions: grammaticality (is the summary fluent and grammatical?), and informativeness (are the main topics captured in the summary?).", "labels": [], "entities": []}, {"text": "The subjects used a 1-5 rating scale, with half-points allowed.", "labels": [], "entities": []}, {"text": "Participants who declared themselves as nonnative English speakers, did not answer the comprehension questions correctly or took only a few minutes to complete the task were eliminated.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: Counts of lexemes in the source news articles and  summaries, and measure of the ratio of their probabilities  (for most common lexemes with ratio < \u22120.95).", "labels": [], "entities": []}, {"text": " Table 4: Performance of the multiple-aspect ILP model against comparison systems using ROUGE and the four com- ponents of TER (insertion, deletion, substitution, shifts). In the lower section, performance of our model without (w/o)  each component in turn. The final columns show the number of source sentences, the average compression ratio, and  the proportion of sentences modified.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 88, "end_pos": 93, "type": "METRIC", "confidence": 0.8677724599838257}, {"text": "TER", "start_pos": 123, "end_pos": 126, "type": "METRIC", "confidence": 0.9947139620780945}]}, {"text": " Table 5: Mean ratings on system output output.", "labels": [], "entities": [{"text": "Mean", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9878789782524109}]}]}