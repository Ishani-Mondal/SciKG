{"title": [{"text": "Learning Verb Inference Rules from Linguistically-Motivated Evidence", "labels": [], "entities": []}], "abstractContent": [{"text": "Learning inference relations between verbs is at the heart of many semantic applications.", "labels": [], "entities": []}, {"text": "However, most prior work on learning such rules focused on a rather narrow set of information sources: mainly distributional similarity , and to a lesser extent manually constructed verb co-occurrence patterns.", "labels": [], "entities": []}, {"text": "In this paper, we claim that it is imperative to utilize information from various textual scopes: verb co-occurrence within a sentence, verb co-occurrence within a document, as well as overall corpus statistics.", "labels": [], "entities": []}, {"text": "To this end, we propose a much richer novel set of linguistically motivated cues for detecting entailment between verbs and combine them as features in a supervised classification framework.", "labels": [], "entities": [{"text": "detecting entailment between verbs", "start_pos": 85, "end_pos": 119, "type": "TASK", "confidence": 0.9210261702537537}]}, {"text": "We empirically demonstrate that our model significantly outperforms previous methods and that information from each textual scope contributes to the verb entailment learning task.", "labels": [], "entities": [{"text": "verb entailment learning task", "start_pos": 149, "end_pos": 178, "type": "TASK", "confidence": 0.7426663488149643}]}], "introductionContent": [{"text": "Inference rules are an important building block of many semantic applications, such as Question Answering () and Information Extraction ().", "labels": [], "entities": [{"text": "Question Answering", "start_pos": 87, "end_pos": 105, "type": "TASK", "confidence": 0.8052896857261658}, {"text": "Information Extraction", "start_pos": 113, "end_pos": 135, "type": "TASK", "confidence": 0.8196260035037994}]}, {"text": "For example, given the sentence \"Churros are coated with sugar\", one can use the rule 'coat \u2192 cover' to answer the question \"What are Churros covered with?\".", "labels": [], "entities": []}, {"text": "Inference rules specify a directional inference relation between two text fragments, and we follow the Textual Entailment modeling of inference ( ), which refers to such rules as entailment rules.", "labels": [], "entities": []}, {"text": "In this work we focus on one of the most important rule types, namely, lexical entailment rules between verbs (verb entailment), e.g., 'whisper \u2192 talk', 'win \u2192 play' and 'buy \u2192 own'.", "labels": [], "entities": []}, {"text": "The significance of such rules has led to active research in automatic learning of entailment rules between verbs or verb-like structures ().", "labels": [], "entities": [{"text": "automatic learning of entailment rules between verbs or verb-like structures", "start_pos": 61, "end_pos": 137, "type": "TASK", "confidence": 0.7987685263156891}]}, {"text": "Most prior efforts to learn verb entailment rules from large corpora employed distributional similarity methods, assuming that verbs are semantically similar if they occur in similar contexts.", "labels": [], "entities": []}, {"text": "This led to the automatic acquisition of large scale knowledge bases, but with limited precision.", "labels": [], "entities": [{"text": "precision", "start_pos": 87, "end_pos": 96, "type": "METRIC", "confidence": 0.9969186782836914}]}, {"text": "Fewer works, such as VerbOcean (), focused on identifying verb entailment through verb instantiation of manually constructed patterns.", "labels": [], "entities": [{"text": "identifying verb entailment", "start_pos": 46, "end_pos": 73, "type": "TASK", "confidence": 0.8150923450787863}, {"text": "verb instantiation of manually constructed patterns", "start_pos": 82, "end_pos": 133, "type": "TASK", "confidence": 0.7516366491715113}]}, {"text": "For example, the sentence \"he scared and even startled me\" implies that 'startle \u2192 scare'.", "labels": [], "entities": []}, {"text": "This led to more precise rule extraction, but with poor coverage since contrary to nouns, in which patterns are common, verbs do not co-occur often within rigid patterns.", "labels": [], "entities": [{"text": "rule extraction", "start_pos": 25, "end_pos": 40, "type": "TASK", "confidence": 0.7279643714427948}, {"text": "coverage", "start_pos": 56, "end_pos": 64, "type": "METRIC", "confidence": 0.9686302542686462}]}, {"text": "However, verbs do tend to co-occur in the same document, and also in different clauses of the same sentence.", "labels": [], "entities": []}, {"text": "In this paper, we claim that on top of standard pattern-based and distributional similarity methods, corpus-based learning of verb entailment can greatly benefit from exploiting additional linguisticallymotivated cues that are specific to verbs.", "labels": [], "entities": []}, {"text": "For instance, when verbs co-occur in different clauses of the same sentence, the syntactic relation between the clauses can be viewed as a proxy for the semantic relation between the verbs.", "labels": [], "entities": []}, {"text": "Moreover, we claim that to improve performance it is crucial to combine information sources from different textual scopes: verb co-occurrence within a sentence and within a document, distributional similarity over the entire corpus, etc.", "labels": [], "entities": []}, {"text": "Our contribution in this paper is two-fold.", "labels": [], "entities": []}, {"text": "First, we suggest a novel set of entailment indicators that help to detect the likelihood of verb entailment.", "labels": [], "entities": []}, {"text": "Our novel indicators are specific to verbs and are linguistically-motivated.", "labels": [], "entities": []}, {"text": "Second, we encode our novel indicators as features within a supervised classification framework and integrate them with other standard features adapted from prior work.", "labels": [], "entities": []}, {"text": "This results in a supervised corpus-based learning method that combines verb entailment information at the sentence, document and corpus levels.", "labels": [], "entities": []}, {"text": "We test our model on a manually labeled data set, and show that it outperforms the best performing previous work by 24%.", "labels": [], "entities": []}, {"text": "In addition, we examine the effectiveness of indicators that operate at the sentence-level, document-level and corpus-level.", "labels": [], "entities": []}, {"text": "This analysis reveals that using a rich and diverse set of indicators that capture sentence-level interactions between verbs substantially improves verb entailment detection.", "labels": [], "entities": [{"text": "verb entailment detection", "start_pos": 148, "end_pos": 173, "type": "TASK", "confidence": 0.8038898011048635}]}], "datasetContent": [{"text": "To evaluate our proposed supervised model, we constructed a dataset containing labeled verb pairs.", "labels": [], "entities": []}, {"text": "We started by randomly sampling 50 verbs out of the common verbs in the RCV1 corpus 5 , which we denote here as seed verbs.", "labels": [], "entities": [{"text": "RCV1 corpus 5", "start_pos": 72, "end_pos": 85, "type": "DATASET", "confidence": 0.9793796141942342}]}, {"text": "Next, we extracted the 20 most similar verbs to each seed verb according to the Lin similarity measure, which was computed on the RCV1 corpus.", "labels": [], "entities": [{"text": "Lin similarity measure", "start_pos": 80, "end_pos": 102, "type": "METRIC", "confidence": 0.9268728693326315}, {"text": "RCV1 corpus", "start_pos": 130, "end_pos": 141, "type": "DATASET", "confidence": 0.9905208349227905}]}, {"text": "Then, for each seed verb v sand one of its extracted similar verbs vi s we generated the two directed pairs (v s , vi s ) and (v i s , v s ), which represent the candidate rules 'v s \u2192 vi s ' and 'v i s \u2192 v s ' respectively.", "labels": [], "entities": []}, {"text": "To reduce noise, we filtered out verb pairs where one of the verbs is an auxiliary or alight verb such as 'do', 'get' and 'have'.", "labels": [], "entities": []}, {"text": "This step resulted in 812 verb pairs as our dataset 6 , which were manually annotated by the authors as representing a valid entailment rule or not.", "labels": [], "entities": []}, {"text": "To annotate these pairs, we generally followed the rule-based approach for entailment rule annotation, where a rule 'v 1 \u2192 v 2 ' is considered as correct if the annotator could think of reasonable contexts under which the rule holds).", "labels": [], "entities": [{"text": "entailment rule annotation", "start_pos": 75, "end_pos": 101, "type": "TASK", "confidence": 0.8498072028160095}]}, {"text": "In total 225 verb pairs were labeled as entailing (the rule 'v 1 \u2192 v 2 ' was judged as correct) and 587 verb pairs were labeled as non-entailing (the rule 'v 1 \u2192 v 2 ' was judged as incorrect).", "labels": [], "entities": []}, {"text": "The InterAnnotator Agreement (IAA) fora random sample of 100 pairs was moderate (0.47), as expected from the rule-based approach).", "labels": [], "entities": [{"text": "InterAnnotator Agreement (IAA)", "start_pos": 4, "end_pos": 34, "type": "METRIC", "confidence": 0.8950582027435303}]}, {"text": "For each verb pair, all 63 features within our model (Section 4) were computed using the ukWaC corpus (, which contains 2 billion words.", "labels": [], "entities": [{"text": "ukWaC corpus", "start_pos": 89, "end_pos": 101, "type": "DATASET", "confidence": 0.9741216897964478}]}, {"text": "For classification, we utilized SVM-perf's () linear SVM implementation with default parameters, and evaluated our model by performing 10-fold cross validation (CV) over the labeled dataset.", "labels": [], "entities": [{"text": "classification", "start_pos": 4, "end_pos": 18, "type": "TASK", "confidence": 0.9635812640190125}]}, {"text": "http://trec.nist.gov/data/reuters/ reuters.html The data set is available at http://www.cs.biu.ac. il/ \u02dc nlp/downloads/verb-pair-annotation.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Top 10 positive and negative features according to the Pearson correlation score.", "labels": [], "entities": [{"text": "Pearson correlation score", "start_pos": 65, "end_pos": 90, "type": "METRIC", "confidence": 0.9079031348228455}]}, {"text": " Table 3: Average precision, recall, AUC and F 1 for our  method and the baselines.", "labels": [], "entities": [{"text": "Average", "start_pos": 10, "end_pos": 17, "type": "METRIC", "confidence": 0.9460240602493286}, {"text": "precision", "start_pos": 18, "end_pos": 27, "type": "METRIC", "confidence": 0.925934374332428}, {"text": "recall", "start_pos": 29, "end_pos": 35, "type": "METRIC", "confidence": 0.9997287392616272}, {"text": "AUC", "start_pos": 37, "end_pos": 40, "type": "METRIC", "confidence": 0.9981622099876404}, {"text": "F 1", "start_pos": 45, "end_pos": 48, "type": "METRIC", "confidence": 0.9951810240745544}]}, {"text": " Table 4: Average precision, recall, AUC and F 1 for each  subset of the feature groups.", "labels": [], "entities": [{"text": "Average", "start_pos": 10, "end_pos": 17, "type": "METRIC", "confidence": 0.9701607823371887}, {"text": "precision", "start_pos": 18, "end_pos": 27, "type": "METRIC", "confidence": 0.9267018437385559}, {"text": "recall", "start_pos": 29, "end_pos": 35, "type": "METRIC", "confidence": 0.9996939897537231}, {"text": "AUC", "start_pos": 37, "end_pos": 40, "type": "METRIC", "confidence": 0.995960533618927}, {"text": "F 1", "start_pos": 45, "end_pos": 48, "type": "METRIC", "confidence": 0.994289219379425}]}]}