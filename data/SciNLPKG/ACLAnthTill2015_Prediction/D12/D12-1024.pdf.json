{"title": [{"text": "Framework of Automatic Text Summarization Using Reinforcement Learning", "labels": [], "entities": [{"text": "Automatic Text Summarization", "start_pos": 13, "end_pos": 41, "type": "TASK", "confidence": 0.62081711490949}]}], "abstractContent": [{"text": "We present anew approach to the problem of automatic text summarization called Automatic Summarization using Reinforcement Learning (ASRL) in this paper, which models the process of constructing a summary within the framework of reinforcement learning and attempts to optimize the given score function with the given feature representation of a summary.", "labels": [], "entities": [{"text": "text summarization", "start_pos": 53, "end_pos": 71, "type": "TASK", "confidence": 0.6821067780256271}, {"text": "Automatic Summarization using Reinforcement Learning (ASRL)", "start_pos": 79, "end_pos": 138, "type": "TASK", "confidence": 0.6015411205589771}]}, {"text": "We demonstrate that the method of reinforcement learning can be adapted to automatic summarization problems naturally and simply, and other summarizing techniques, such as sentence compression, can be easily adapted as actions of the framework.", "labels": [], "entities": [{"text": "sentence compression", "start_pos": 172, "end_pos": 192, "type": "TASK", "confidence": 0.7105843424797058}]}, {"text": "The experimental results indicated ASRL was superior to the best performing method in DUC2004 and comparable to the state of the art ILP-style method, in terms of ROUGE scores.", "labels": [], "entities": [{"text": "ASRL", "start_pos": 35, "end_pos": 39, "type": "METRIC", "confidence": 0.9784060716629028}, {"text": "DUC2004", "start_pos": 86, "end_pos": 93, "type": "DATASET", "confidence": 0.9368032217025757}, {"text": "ROUGE", "start_pos": 163, "end_pos": 168, "type": "METRIC", "confidence": 0.9961156845092773}]}, {"text": "The results also revealed ASRL can search for sub-optimal solutions efficiently under conditions for effectively selecting features and the score function.", "labels": [], "entities": [{"text": "ASRL", "start_pos": 26, "end_pos": 30, "type": "TASK", "confidence": 0.6177865266799927}]}], "introductionContent": [{"text": "Automatic text summarization aims to automatically produce a short and well-organized summary of single or multiple documents).", "labels": [], "entities": [{"text": "text summarization", "start_pos": 10, "end_pos": 28, "type": "TASK", "confidence": 0.6465968489646912}]}, {"text": "Automatic summarization, especially multi-document summarization, has been an increasingly important task in recent years, because of the exponential explosion of available information.", "labels": [], "entities": [{"text": "Automatic summarization", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.649918407201767}, {"text": "multi-document summarization", "start_pos": 36, "end_pos": 64, "type": "TASK", "confidence": 0.6500823795795441}]}, {"text": "The brief summary that the summarization system produces allows readers to quickly and easily understand the content of original documents without having to read each individual document, and it should be helpful for dealing with enormous amounts of information.", "labels": [], "entities": []}, {"text": "The extractive approach to automatic summarization is a popular and well-known approach in this field, which creates a summary by directly selecting some textual units (e.g., words and sentences) from the original documents, because it is difficult to genuinely evaluate and guarantee the linguistic quality of the produced summary.", "labels": [], "entities": [{"text": "summarization", "start_pos": 37, "end_pos": 50, "type": "TASK", "confidence": 0.7366212606430054}]}, {"text": "One of the most well-known extractive approaches is maximal marginal relevance (MMR), which scores each textual unit and extracts the unit that has the highest score in terms of the MMR criteria ().", "labels": [], "entities": [{"text": "maximal marginal relevance (MMR)", "start_pos": 52, "end_pos": 84, "type": "METRIC", "confidence": 0.7384953697522482}]}, {"text": "Greedy MMR-style algorithms are widely used; however, they cannot take into account the whole quality of the summary due to their greediness, although a summary should convey all the information in given documents.", "labels": [], "entities": []}, {"text": "Global inference algorithms for the extractive approach have been researched widely in recent years ( to consider whether the summary is \"good\" as a whole.", "labels": [], "entities": []}, {"text": "These algorithms formulate the problem as integer linear programming (ILP) to optimize the score: however, as ILP is non-deterministic polynomialtime hard, the time complexity is very large.", "labels": [], "entities": []}, {"text": "Consequently, we need some more efficient algorithm for calculations.", "labels": [], "entities": []}, {"text": "We present anew approach to the problem of automatic text summarization called Automatic Summarization using Reinforcement Learning (ASRL), which models the process of construction of a summary within the framework of reinforcement learn-ing and attempts to optimize the given score function with the given feature representation of a summary.", "labels": [], "entities": [{"text": "text summarization", "start_pos": 53, "end_pos": 71, "type": "TASK", "confidence": 0.6899740844964981}]}, {"text": "We demonstrate that the method of reinforcement learning can be adapted to problems with automatic summarization naturally and simply, and other summarizing techniques, such as sentence compression, can be easily adapted as actions of the framework, which should be helpful to enhance the quality of the summary that is produced.", "labels": [], "entities": [{"text": "sentence compression", "start_pos": 177, "end_pos": 197, "type": "TASK", "confidence": 0.713984414935112}]}, {"text": "This is the first paper utilizing reinforcement learning for problems with automatic summarization of text.", "labels": [], "entities": [{"text": "summarization of text", "start_pos": 85, "end_pos": 106, "type": "TASK", "confidence": 0.8718749483426412}]}, {"text": "We evaluated ASRL with the DUC2004 summarization task 2, and the experimental results revealed ASRL is superior to the best method of performance in DUC2004 and comparable with the state of the art ILP-style method, based on maximum coverage with the knapsack constraint problem, in terms of ROUGE scores with experimental settings.", "labels": [], "entities": [{"text": "DUC2004 summarization task", "start_pos": 27, "end_pos": 53, "type": "DATASET", "confidence": 0.8594494859377543}, {"text": "ASRL", "start_pos": 95, "end_pos": 99, "type": "METRIC", "confidence": 0.8310466408729553}, {"text": "DUC2004", "start_pos": 149, "end_pos": 156, "type": "DATASET", "confidence": 0.9380846619606018}, {"text": "ROUGE", "start_pos": 292, "end_pos": 297, "type": "METRIC", "confidence": 0.996688187122345}]}, {"text": "We also evaluated ASRL in terms of optimality and execution time.", "labels": [], "entities": [{"text": "ASRL", "start_pos": 18, "end_pos": 22, "type": "METRIC", "confidence": 0.6912292838096619}]}, {"text": "The experimental results indicated ASRL can search the state space efficiently for some suboptimal solutions under the condition of effectively selecting features and the score function, and produce a summary whose score denotes the expectation of the score of the same features' states.", "labels": [], "entities": [{"text": "ASRL", "start_pos": 35, "end_pos": 39, "type": "METRIC", "confidence": 0.4388188123703003}]}, {"text": "The evaluation of the quality of a produced summary only depends on the given score function, and therefore it is easy to adapt the new method of evaluation without having to modify the structure of the framework.", "labels": [], "entities": []}], "datasetContent": [{"text": "We conducted three experiments in this study.", "labels": [], "entities": []}, {"text": "First, we evaluated our method with ROUGE metrics, in terms of ROUGE-1, ROUGE-2, and ROUGE-L.", "labels": [], "entities": [{"text": "ROUGE-1", "start_pos": 63, "end_pos": 70, "type": "METRIC", "confidence": 0.8578034043312073}, {"text": "ROUGE-2", "start_pos": 72, "end_pos": 79, "type": "METRIC", "confidence": 0.9154924750328064}]}, {"text": "Second, we conducted an experiment on measuring the optimization capabilities of ASRL, with the scores we obtained and the execution time.", "labels": [], "entities": [{"text": "ASRL", "start_pos": 81, "end_pos": 85, "type": "TASK", "confidence": 0.7277336120605469}]}, {"text": "Third, we evaluated ASRL taking into consideration sentence compression by using a very naive method, in terms of ROUGE-1, ROUGE-2, and ROUGE-3.", "labels": [], "entities": [{"text": "ASRL", "start_pos": 20, "end_pos": 24, "type": "TASK", "confidence": 0.8979889750480652}, {"text": "sentence compression", "start_pos": 51, "end_pos": 71, "type": "TASK", "confidence": 0.7530360817909241}, {"text": "ROUGE-1", "start_pos": 114, "end_pos": 121, "type": "METRIC", "confidence": 0.9012869000434875}, {"text": "ROUGE-2", "start_pos": 123, "end_pos": 130, "type": "METRIC", "confidence": 0.7896209955215454}]}, {"text": "We used sentences as textual units for the extractive approach in this research.", "labels": [], "entities": []}, {"text": "Each sentence and document were represented as a bag-of-words vector with tf*idf values, with stopwords removed.", "labels": [], "entities": []}, {"text": "All tokens were stemmed by using Porter's stemmer.", "labels": [], "entities": [{"text": "Porter's stemmer", "start_pos": 33, "end_pos": 49, "type": "DATASET", "confidence": 0.79524032274882}]}, {"text": "We experimented with our proposed method on the dataset of DUC2004 task2.", "labels": [], "entities": []}, {"text": "This is a multidocument summarization task that contains 50 document clusters, each of which has 10 documents.", "labels": [], "entities": []}, {"text": "We setup the length limitation to 665 bytes, used in the evaluation of DUC2004.", "labels": [], "entities": [{"text": "DUC2004", "start_pos": 71, "end_pos": 78, "type": "DATASET", "confidence": 0.9651169180870056}]}, {"text": "We setup the parameters of ASRL where the number of episodes N = 300, the training rate \u03b1 k = 0.001 \u00b7 101/(100 + k 1.1 ), and the temperature \u03c4 k = 1.0 \u00b7 0.987 k\u22121 where k was the number of episodes that decayed as learning progressed.", "labels": [], "entities": [{"text": "ASRL", "start_pos": 27, "end_pos": 31, "type": "METRIC", "confidence": 0.545759916305542}, {"text": "training rate \u03b1 k", "start_pos": 74, "end_pos": 91, "type": "METRIC", "confidence": 0.9523559659719467}]}, {"text": "Both discount rate \u03b3 and trace decay parameter \u03bb were fixed to 1 for episodic tasks.", "labels": [], "entities": [{"text": "discount rate \u03b3", "start_pos": 5, "end_pos": 20, "type": "METRIC", "confidence": 0.9520679314931234}, {"text": "trace decay parameter \u03bb", "start_pos": 25, "end_pos": 48, "type": "METRIC", "confidence": 0.894358217716217}]}, {"text": "The penalty, R penalty , was fixed to 1.", "labels": [], "entities": [{"text": "R penalty", "start_pos": 13, "end_pos": 22, "type": "METRIC", "confidence": 0.9820166230201721}]}, {"text": "We used the following score function in this study: where \u03bb sis the parameter for the trade-off between relevance and redundancy, Sim(x i , D) and Sim(x i , x j ) correspond to the cosine similarities between sentence xi and the sentence set of the given original documents D, and between sentence xi and sentence x j . P os(x i ) is the position of the occurrence of xi when we index sentences in each document from top to bottom with one origin.", "labels": [], "entities": [{"text": "P os", "start_pos": 320, "end_pos": 324, "type": "METRIC", "confidence": 0.8777270019054413}]}, {"text": "This score function was determined by reference to.", "labels": [], "entities": []}, {"text": "We set \u03bb s = 0.9 in this experiment.", "labels": [], "entities": []}, {"text": "We designed \u03d5 \u2032 (S), i.e., the vector representation of a summary, to adapt it to the summarization problem as follows.", "labels": [], "entities": [{"text": "summarization", "start_pos": 86, "end_pos": 99, "type": "TASK", "confidence": 0.9792805314064026}]}, {"text": "\u2022 Coverage of important words: The elements are the top 100 words in terms of the tf*idf of the given document with binary representation.", "labels": [], "entities": []}, {"text": "\u2022 Coverage ratio: This is calculated by counting up the number of top 100 elements included in the summary.", "labels": [], "entities": [{"text": "Coverage ratio", "start_pos": 2, "end_pos": 16, "type": "METRIC", "confidence": 0.9087833762168884}]}, {"text": "\u2022 Redundancy ratio: This is calculated by counting up the number of elements that excessively cover the top 100 elements.", "labels": [], "entities": []}, {"text": "\u2022 Length ratio: This is the ratio between the length of the summary and length limitation K.", "labels": [], "entities": [{"text": "Length ratio", "start_pos": 2, "end_pos": 14, "type": "METRIC", "confidence": 0.9921196401119232}, {"text": "length", "start_pos": 46, "end_pos": 52, "type": "METRIC", "confidence": 0.9749528169631958}, {"text": "length limitation K", "start_pos": 72, "end_pos": 91, "type": "METRIC", "confidence": 0.9111400047938029}]}, {"text": "\u2022 Position: This feature takes into consideration the position of sentence occurrences.", "labels": [], "entities": []}, {"text": "It is calculated with We executed ASRL 10 times with the settings previously described and used all the results for evaluation.", "labels": [], "entities": [{"text": "ASRL", "start_pos": 34, "end_pos": 38, "type": "METRIC", "confidence": 0.8278958201408386}]}, {"text": "We used the dataset of DUC2003, which is a similar task that contains 30 document clusters and each cluster had 10 documents, to determine \u03c4 k and \u03bb s . We determined the parameters so that they would converge properly and become close to the optimal solutions calculated by ILP, under the conditions that the described feature representation and the score function were given.", "labels": [], "entities": [{"text": "DUC2003", "start_pos": 23, "end_pos": 30, "type": "DATASET", "confidence": 0.9553770422935486}]}, {"text": "We compared ASRL with four other conventional methods.", "labels": [], "entities": [{"text": "ASRL", "start_pos": 12, "end_pos": 16, "type": "TASK", "confidence": 0.9391963481903076}]}, {"text": "\u2022 GREEDY: This method is a simple greedy algorithm, which repeats the selection of the sentence with the highest score of the remaining sentences by using an MMR-like method of scoring as follows: where S is the current summary.", "labels": [], "entities": [{"text": "GREEDY", "start_pos": 2, "end_pos": 8, "type": "METRIC", "confidence": 0.9979943037033081}]}, {"text": "\u2022 ILP: This indicates the method proposed by for maximizing the score function (14) with integer linear programming.", "labels": [], "entities": [{"text": "ILP", "start_pos": 2, "end_pos": 5, "type": "METRIC", "confidence": 0.8250361680984497}]}, {"text": "\u2022 PEER65: This is the best performing system in task 2 of the DUC2004 competition in terms of ROUGE-1 proposed by.", "labels": [], "entities": [{"text": "PEER65", "start_pos": 2, "end_pos": 8, "type": "METRIC", "confidence": 0.9927589893341064}, {"text": "ROUGE-1", "start_pos": 94, "end_pos": 101, "type": "METRIC", "confidence": 0.9211206436157227}]}, {"text": "\u2022 MCKP: This method was proposed by Takamura and Okamura.", "labels": [], "entities": []}, {"text": "MCKP defines an automatic summarization problem as a maximum coverage problem with a knapsack constraint, which uses conceptual units (, and composes the meaning of sentences, as textual units and attempts to cover as many units as possible under the knapsack constraint.", "labels": [], "entities": [{"text": "MCKP", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.9083759188652039}, {"text": "summarization", "start_pos": 26, "end_pos": 39, "type": "TASK", "confidence": 0.8842880129814148}]}, {"text": "We evaluated our method of ASRL with ROUGE, in terms of ROUGE-1, ROUGE-2, and ROUGE-L.", "labels": [], "entities": [{"text": "ASRL", "start_pos": 27, "end_pos": 31, "type": "TASK", "confidence": 0.637515664100647}, {"text": "ROUGE", "start_pos": 37, "end_pos": 42, "type": "METRIC", "confidence": 0.9627746343612671}, {"text": "ROUGE-1", "start_pos": 56, "end_pos": 63, "type": "METRIC", "confidence": 0.9349084496498108}, {"text": "ROUGE-2", "start_pos": 65, "end_pos": 72, "type": "METRIC", "confidence": 0.9331129789352417}]}, {"text": "The experimental results are summarized in lists the results for the comparison and lists all the results for ASRL peers.", "labels": [], "entities": []}, {"text": "The results imply ASRL is superior to PEER65, ILP, and GREEDY, and comparable to MCKP with these experimental settings in terms of ROUGE metrics.", "labels": [], "entities": [{"text": "ASRL", "start_pos": 18, "end_pos": 22, "type": "METRIC", "confidence": 0.9908332228660583}, {"text": "PEER65", "start_pos": 38, "end_pos": 44, "type": "METRIC", "confidence": 0.8449863791465759}, {"text": "GREEDY", "start_pos": 55, "end_pos": 61, "type": "METRIC", "confidence": 0.992668628692627}, {"text": "ROUGE", "start_pos": 131, "end_pos": 136, "type": "METRIC", "confidence": 0.9250285029411316}]}, {"text": "Note that ASRL is a kind of approximate method, because actions are selected probabilistically and the method of reinforcement learning occasionally converges with some sub-optimal solution.", "labels": [], "entities": [{"text": "ASRL", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.5104402303695679}]}, {"text": "This can be expected from, which indicates the results vary although each ASRL solution converged with some solution.", "labels": [], "entities": []}, {"text": "However, in this experiment, ASRL achieved higher ROUGE scores than ILP, which achieved optimal solutions.", "labels": [], "entities": [{"text": "ASRL", "start_pos": 29, "end_pos": 33, "type": "METRIC", "confidence": 0.7667601108551025}, {"text": "ROUGE", "start_pos": 50, "end_pos": 55, "type": "METRIC", "confidence": 0.9964120984077454}]}, {"text": "This seems to have been caused by the properties of the features, which we will discuss later.", "labels": [], "entities": []}, {"text": "It seems this feature representation is useful for efficiently searching the feature space.", "labels": [], "entities": []}, {"text": "The method of mapping a state to features is, however, approximate in the sense that some states will shrink to the same feature vector, and ASRL therefore has no tendency to converge with some stable solution.", "labels": [], "entities": [{"text": "ASRL", "start_pos": 141, "end_pos": 145, "type": "METRIC", "confidence": 0.41757798194885254}]}, {"text": "Since we proposed our method as an approach to approximate optimization, there was the possibility of convergence with some sub-optimal solution as previously discussed.", "labels": [], "entities": [{"text": "approximate optimization", "start_pos": 47, "end_pos": 71, "type": "TASK", "confidence": 0.8876019716262817}]}, {"text": "We also evaluated our approach from the point of view of the obtained scores and the execution time to confirm whether our method had.", "labels": [], "entities": []}, {"text": "The score in ASRL increases as the number of episodes increases, and overtakes the score of GREEDY at some episode.", "labels": [], "entities": [{"text": "ASRL", "start_pos": 13, "end_pos": 17, "type": "METRIC", "confidence": 0.9685985445976257}, {"text": "GREEDY", "start_pos": 92, "end_pos": 98, "type": "METRIC", "confidence": 0.9990506768226624}]}, {"text": "The agent attempts to come close to the optimal scoreline of ILP but seems to fail, and finally converges to some local optimal solution.", "labels": [], "entities": []}, {"text": "We should increase the number of episodes, adjust parameters \u03b1 and \u03c4 , and select more appropriate features for the state to improve the optimization capabilities of ASRL.", "labels": [], "entities": []}, {"text": "plots the execution time for each peer.", "labels": [], "entities": [{"text": "execution time", "start_pos": 10, "end_pos": 24, "type": "METRIC", "confidence": 0.8038590252399445}]}, {"text": "The horizontal axis is the number of textual units, i.e., the number of sentences in this experiment.", "labels": [], "entities": []}, {"text": "The vertical axis is the execution time taken by the task.", "labels": [], "entities": []}, {"text": "The plots of ASRL and ILP fit a linear function for the former and an exponential function for the latter.", "labels": [], "entities": [{"text": "ASRL", "start_pos": 13, "end_pos": 17, "type": "METRIC", "confidence": 0.5545734763145447}]}, {"text": "The experimental results indicate that while the execution time for ILP tends to increase exponentially, that for ASRL increases linearly.", "labels": [], "entities": [{"text": "ASRL", "start_pos": 114, "end_pos": 118, "type": "METRIC", "confidence": 0.5881739854812622}]}, {"text": "The time complexity of ASRL is linear with respect to the number of actions because the agent has to select the next action from the available actions for each episode, whose time complexity is naively O(|A|).", "labels": [], "entities": []}, {"text": "approach, the execution time increases linearly with respect to the number of textual units.", "labels": [], "entities": []}, {"text": "However, ILP has to take into account the combinations of textual units, whose number increases exponentially.", "labels": [], "entities": []}, {"text": "In conclusion, both the experimental results indicate that ASRL efficiently calculated a summary that was sub-optimal, but that was of relatively highquality in terms of ROUGE metrics, with the experimental settings we used.", "labels": [], "entities": [{"text": "ASRL", "start_pos": 59, "end_pos": 63, "type": "METRIC", "confidence": 0.4847315549850464}, {"text": "ROUGE", "start_pos": 170, "end_pos": 175, "type": "METRIC", "confidence": 0.8978978395462036}]}, {"text": "We also evaluated the combined approach with sentence compression.", "labels": [], "entities": [{"text": "sentence compression", "start_pos": 45, "end_pos": 65, "type": "TASK", "confidence": 0.7425044476985931}]}, {"text": "We evaluated the method described in Section 5 called ASRLC in this experiment for the sake of convenience.", "labels": [], "entities": [{"text": "ASRLC", "start_pos": 54, "end_pos": 59, "type": "METRIC", "confidence": 0.5956470370292664}]}, {"text": "We used a very naive method of sentence compression for this experiment, which compressed a sentence to only important words, i.e., selecting word order by using the tf*idf score to compress the length to about half.", "labels": [], "entities": [{"text": "sentence compression", "start_pos": 31, "end_pos": 51, "type": "TASK", "confidence": 0.7195318639278412}]}, {"text": "This method of compression did not take into consideration either readability or linguistic quality.", "labels": [], "entities": []}, {"text": "Note we wanted to confirm what effect the other methods would have, and we expected this to improve the ROUGE-1 score.", "labels": [], "entities": [{"text": "ROUGE-1 score", "start_pos": 104, "end_pos": 117, "type": "METRIC", "confidence": 0.9768129885196686}]}, {"text": "We used the ROUGE-3 score in this evaluation instead of ROUGE-L, to confirm whether naive sentence compression occurred.", "labels": [], "entities": [{"text": "ROUGE-3", "start_pos": 12, "end_pos": 19, "type": "METRIC", "confidence": 0.9937665462493896}, {"text": "ROUGE-L", "start_pos": 56, "end_pos": 63, "type": "METRIC", "confidence": 0.9818620085716248}, {"text": "sentence compression", "start_pos": 90, "end_pos": 110, "type": "TASK", "confidence": 0.699495404958725}]}, {"text": "ble 3, which indicates ROUGE-1 increases but ROUGE-2 and ROUGE-3 decrease as expected.", "labels": [], "entities": [{"text": "ROUGE-1", "start_pos": 23, "end_pos": 30, "type": "METRIC", "confidence": 0.9931089878082275}, {"text": "ROUGE-2", "start_pos": 45, "end_pos": 52, "type": "METRIC", "confidence": 0.9792863726615906}, {"text": "ROUGE-3", "start_pos": 57, "end_pos": 64, "type": "METRIC", "confidence": 0.9738165736198425}]}, {"text": "The variations, however, are small.", "labels": [], "entities": []}, {"text": "This phenomenon was reported by in that the effectiveness of sentence compression by local optimization at the sentence level was insufficient.", "labels": [], "entities": [{"text": "sentence compression", "start_pos": 61, "end_pos": 81, "type": "TASK", "confidence": 0.7483593225479126}]}, {"text": "Therefore, we would have to consider the range of applications with the combined method.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Results of ROUGE evaluation compared with  other peers in DUC2004. Scores for ILP and GREEDY  have statistically significant differences from scores of  ASRL.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 21, "end_pos": 26, "type": "METRIC", "confidence": 0.9721609950065613}, {"text": "DUC2004", "start_pos": 68, "end_pos": 75, "type": "DATASET", "confidence": 0.8941133618354797}, {"text": "GREEDY", "start_pos": 96, "end_pos": 102, "type": "METRIC", "confidence": 0.944567859172821}, {"text": "ASRL", "start_pos": 163, "end_pos": 167, "type": "METRIC", "confidence": 0.5480759143829346}]}, {"text": " Table 2: Results of ROGUE evaluation for each ASRL  peer of 10 results in DUC2004. ASRL did not converge  with stable solution with these experimental settings be- cause of property of randomness.", "labels": [], "entities": [{"text": "ROGUE", "start_pos": 21, "end_pos": 26, "type": "METRIC", "confidence": 0.9410768151283264}, {"text": "DUC2004", "start_pos": 75, "end_pos": 82, "type": "DATASET", "confidence": 0.905866265296936}]}, {"text": " Table 3: Evaluation of combined methods.", "labels": [], "entities": []}]}