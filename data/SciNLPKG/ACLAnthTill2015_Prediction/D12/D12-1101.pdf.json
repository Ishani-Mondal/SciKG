{"title": [{"text": "Monte Carlo MCMC: Efficient Inference by Approximate Sampling", "labels": [], "entities": []}], "abstractContent": [{"text": "Conditional random fields and other graphi-cal models have achieved state of the art results in a variety of tasks such as coreference, relation extraction, data integration, and parsing.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 136, "end_pos": 155, "type": "TASK", "confidence": 0.825205385684967}, {"text": "data integration", "start_pos": 157, "end_pos": 173, "type": "TASK", "confidence": 0.8480795621871948}, {"text": "parsing", "start_pos": 179, "end_pos": 186, "type": "TASK", "confidence": 0.9643675684928894}]}, {"text": "Increasingly, practitioners are using models with more complex structure-higher tree-width, larger fan-out, more features, and more data-rendering even approximate inference methods such as MCMC inefficient.", "labels": [], "entities": []}, {"text": "In this paper we propose an alternative MCMC sampling scheme in which transition probabilities are approximated by sampling from the set of relevant factors.", "labels": [], "entities": []}, {"text": "We demonstrate that our method converges more quickly than a traditional MCMC sampler for both marginal and MAP inference.", "labels": [], "entities": []}, {"text": "In an author coreference task with over 5 million mentions, we achieve a 13 times speedup over regular MCMC inference.", "labels": [], "entities": []}], "introductionContent": [{"text": "Conditional random fields and other graphical models are at the forefront of many natural language processing (NLP) and information extraction (IE) tasks because they provide a framework for discriminative modeling while succinctly representing dependencies among many related output variables.", "labels": [], "entities": [{"text": "natural language processing (NLP) and information extraction (IE)", "start_pos": 82, "end_pos": 147, "type": "TASK", "confidence": 0.7251743351419767}]}, {"text": "Previously, most applications of graphical models were limited to structures where exact inference is possible, for example linear-chain CRFs ().", "labels": [], "entities": []}, {"text": "More recently, there has been a desire to include more factors, longer range dependencies, and more sophisticated features; these include skip-chain CRFs for named entity recognition (), probabilistic DBs ( , higher-order models for dependency parsing, entity-wise models for coreference (, and global models of relations).", "labels": [], "entities": [{"text": "named entity recognition", "start_pos": 158, "end_pos": 182, "type": "TASK", "confidence": 0.7220122218132019}, {"text": "dependency parsing", "start_pos": 233, "end_pos": 251, "type": "TASK", "confidence": 0.7243521064519882}]}, {"text": "The increasing sophistication of these individual NLP components compounded with the community's desire to model these tasks jointly across cross-document considerations has resulted in graphical models for which inference is computationally intractable.", "labels": [], "entities": []}, {"text": "Even popular approximate inference techniques such as loopy belief propagation and Markov chain Monte Carlo (MCMC) maybe prohibitively slow.", "labels": [], "entities": [{"text": "belief propagation", "start_pos": 60, "end_pos": 78, "type": "TASK", "confidence": 0.7071286141872406}, {"text": "Markov chain Monte Carlo (MCMC", "start_pos": 83, "end_pos": 113, "type": "METRIC", "confidence": 0.6964826732873917}]}, {"text": "MCMC algorithms such as Metropolis-Hastings are usually efficient for graphical models because the only factors needed to score a proposal are those touching the changed variables.", "labels": [], "entities": []}, {"text": "However, MCMC is slowed in situations where a) the model exhibits variables that have a high-degree (neighbor many factors), b) proposals modify a substantial subset of the variables to satisfy domain constraints (such as transitivity in coreference), or c) evaluating a single factor is expensive, for example when features are based on string-similarity.", "labels": [], "entities": []}, {"text": "For example, the seemingly innocuous proposal changing the entity type of a single entity requires examining all its mentions, i.e. scoring a linear number of factors (in the number of mentions of that entity).", "labels": [], "entities": []}, {"text": "Similarly, evaluating coreference of a mention to an entity also requires scoring factors to all the mentions of the entity.", "labels": [], "entities": [{"text": "coreference of a mention", "start_pos": 22, "end_pos": 46, "type": "TASK", "confidence": 0.8936626762151718}]}, {"text": "Often, however, the factors are somewhat redundant, for example, not all mentions of the \"USA\" entity need to be examined to confidently conclude that it is a COUNTRY, or that it is coreferent with \"United States of In this paper we propose an approximate MCMC framework that facilitates efficient inference in highdegree graphical models.", "labels": [], "entities": []}, {"text": "In particular, we approximate the acceptance ratio in the Metropolis Hastings algorithm by replacing the exact model score with a stochastic approximation that samples from the set of relevant factors.", "labels": [], "entities": [{"text": "acceptance ratio", "start_pos": 34, "end_pos": 50, "type": "METRIC", "confidence": 0.9299753904342651}]}, {"text": "We explore two sampling strategies, a fixed proportion approach that samples the factors uniformly, and a dynamic alternative that samples factors until the method is confident about its estimate of the model score.", "labels": [], "entities": []}, {"text": "We evaluate our method empirically on both synthetic and real-world data.", "labels": [], "entities": []}, {"text": "On synthetic classification data, our approximate MCMC procedure obtains the true marginals faster than a traditional MCMC sampler.", "labels": [], "entities": []}, {"text": "On real-world tasks, our method achieves 7 times speedup on citation matching, and 13 times speedup on large-scale author disambiguation.", "labels": [], "entities": [{"text": "citation matching", "start_pos": 60, "end_pos": 77, "type": "TASK", "confidence": 0.9295726716518402}, {"text": "author disambiguation", "start_pos": 115, "end_pos": 136, "type": "TASK", "confidence": 0.7040863931179047}]}], "datasetContent": [{"text": "Although one of the benefits of MCMC lies in its ability to leverage the locality of the proposal, for some information extraction tasks this can become a crucial bottleneck.", "labels": [], "entities": [{"text": "information extraction", "start_pos": 108, "end_pos": 130, "type": "TASK", "confidence": 0.7952795028686523}]}, {"text": "In particular, evaluation of each sample requires computing the score of all the factors that are involved in the change, i.e. all factors that neighbor any variable in the set that has changed.", "labels": [], "entities": []}, {"text": "This evaluation becomes a bottleneck for tasks in which a large number of variables is involved in each proposal, or in which the model contains a number of high-degree variables, resulting in a large number of factors, or in which computing the factor score involves an expensive computation, such as string similarity between mention text.", "labels": [], "entities": []}, {"text": "Instead of evaluating the log-score \u03c8 of the model exactly, this paper proposes a Monte-Carlo estimation of the log-score.", "labels": [], "entities": []}, {"text": "In particular, if the set of factors fora given proposal y \u2192 y is F(y, y ), we use a sampled subset of the factors S \u2286 F(y, y ) as an approximation of the model score.", "labels": [], "entities": []}, {"text": "In the following we use F as an abbreviation for F(y, y ).", "labels": [], "entities": [{"text": "F", "start_pos": 24, "end_pos": 25, "type": "METRIC", "confidence": 0.99623703956604}, {"text": "F", "start_pos": 49, "end_pos": 50, "type": "METRIC", "confidence": 0.9964609742164612}]}, {"text": "Formally, We use the sample log-score (\u03c8 S ) in the acceptance probability \u03b1 to evaluate the samples.", "labels": [], "entities": [{"text": "sample log-score (\u03c8 S )", "start_pos": 21, "end_pos": 44, "type": "METRIC", "confidence": 0.7648084263006846}, {"text": "acceptance probability \u03b1", "start_pos": 52, "end_pos": 76, "type": "METRIC", "confidence": 0.7567742268244425}]}, {"text": "Since we are using a stochastic approximation to the model score, in general we need to take more MCMC samples before we converge, however, since evaluating each sample will be much faster (O(|S|) as opposed to O(|F|)), we expect overall sampling to be faster.", "labels": [], "entities": []}, {"text": "In the next sections we describe several alternative strategies for sampling the set of factors S.", "labels": [], "entities": []}, {"text": "The primary restriction on the set of samples S is that their mean should bean unbiased estimator of E F [f ].", "labels": [], "entities": [{"text": "F", "start_pos": 103, "end_pos": 104, "type": "METRIC", "confidence": 0.6089105010032654}]}, {"text": "Further, time taken to obtain the set of samples should be negligible when compared to scoring all the factors in F.", "labels": [], "entities": []}, {"text": "Note that there is an implicit minimum of 1 to the number of the sampled factors.", "labels": [], "entities": []}, {"text": "In this section we evaluate our approach for both marginal and MAP inference.", "labels": [], "entities": [{"text": "MAP inference", "start_pos": 63, "end_pos": 76, "type": "TASK", "confidence": 0.8584615886211395}]}], "tableCaptions": [{"text": " Table 1: Speedups on Cora to obtain 90% B 3 F1", "labels": [], "entities": [{"text": "B", "start_pos": 41, "end_pos": 42, "type": "METRIC", "confidence": 0.9316942691802979}]}]}