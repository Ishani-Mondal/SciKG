{"title": [{"text": "Training Factored PCFGs with Expectation Propagation", "labels": [], "entities": [{"text": "Expectation Propagation", "start_pos": 29, "end_pos": 52, "type": "METRIC", "confidence": 0.9105829894542694}]}], "abstractContent": [{"text": "PCFGs can grow exponentially as additional annotations are added to an initially simple base grammar.", "labels": [], "entities": [{"text": "PCFGs", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.855771005153656}]}, {"text": "We present an approach where multiple annotations coexist, but in a factored manner that avoids this combinatorial explosion.", "labels": [], "entities": []}, {"text": "Our method works with linguistically-motivated annotations, induced latent structure , lexicalization, or any mix of the three.", "labels": [], "entities": []}, {"text": "We use a structured expectation propagation algorithm that makes use of the factored structure in two ways.", "labels": [], "entities": [{"text": "expectation propagation", "start_pos": 20, "end_pos": 43, "type": "TASK", "confidence": 0.7609646618366241}]}, {"text": "First, by partitioning the factors , it speeds up parsing exponentially over the unfactored approach.", "labels": [], "entities": [{"text": "parsing", "start_pos": 50, "end_pos": 57, "type": "TASK", "confidence": 0.9765523076057434}]}, {"text": "Second, it minimizes the redundancy of the factors during training, improving accuracy over an independent approach.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 78, "end_pos": 86, "type": "METRIC", "confidence": 0.9991810917854309}]}, {"text": "Using purely latent variable annotations , we can efficiently train and parse with up to 8 latent bits per symbol, achieving F1 scores up to 88.4 on the Penn Treebank while using two orders of magnitudes fewer parameters compared to the na\u00a8\u0131vena\u00a8\u0131ve approach.", "labels": [], "entities": [{"text": "F1 scores", "start_pos": 125, "end_pos": 134, "type": "METRIC", "confidence": 0.9811383187770844}, {"text": "Penn Treebank", "start_pos": 153, "end_pos": 166, "type": "DATASET", "confidence": 0.9953014850616455}]}, {"text": "Combining latent, lexicalized, and unlexicalized annotations , our best parser gets 89.4 F1 on all sentences from section 23 of the Penn Treebank.", "labels": [], "entities": [{"text": "F1", "start_pos": 89, "end_pos": 91, "type": "METRIC", "confidence": 0.9734921455383301}, {"text": "Penn Treebank", "start_pos": 132, "end_pos": 145, "type": "DATASET", "confidence": 0.875122994184494}]}], "introductionContent": [{"text": "Many high-performance PCFG parsers take an initially simple base grammar over treebank labels like NP and enrich it with deeper syntactic features to improve accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 158, "end_pos": 166, "type": "METRIC", "confidence": 0.9915492534637451}]}, {"text": "This broad characterization includes lexicalized parsers, unlexicalized parsers, and latent variable parsers ().", "labels": [], "entities": []}, {"text": "Figures 1(a), 1(b), and 1(c) show small examples of contextfree trees that have been annotated in these ways.", "labels": [], "entities": []}, {"text": "When multi-part annotations are used in the same grammar, systems have generally multiplied these annotations together, in the sense that an NP that was definite, possessive, and VP-dominated would have a single unstructured PCFG symbol that encoded all three facts.", "labels": [], "entities": []}, {"text": "In addition, modulo backoff or smoothing, that unstructured symbol would often have rewrite parameters entirely distinct from, say, the indefinite but otherwise similar variant of the symbol (.", "labels": [], "entities": []}, {"text": "Therefore, when designing a grammar, one would have to carefully weigh new contextual annotations.", "labels": [], "entities": []}, {"text": "Should a definiteness annotation be included, doubling the number of NPs in the grammar and perhaps overly fragmenting statistics?", "labels": [], "entities": []}, {"text": "Or should it be excluded, thereby losing important distinctions?", "labels": [], "entities": []}, {"text": "discuss exactly such trade-offs and omit annotations that were helpful on their own because they were not worth the combinatorial or statistical cost when combined with other annotations.", "labels": [], "entities": []}, {"text": "In this paper, we argue for grammars with factored annotations, that is, grammars with annotations that have structured component parts that are partially decoupled.", "labels": [], "entities": []}, {"text": "Our annotated grammars can include both latent and explicit annotations, as illustrated in, and we demonstrate that these factored grammars outperform parsers with unstructured annotations.", "labels": [], "entities": []}, {"text": "After discussing the factored representation, we describe a method for parsing with factored annotations, using an approximate inference technique called expectation propagation.", "labels": [], "entities": [{"text": "parsing", "start_pos": 71, "end_pos": 78, "type": "TASK", "confidence": 0.9663333892822266}, {"text": "expectation propagation", "start_pos": 154, "end_pos": 177, "type": "TASK", "confidence": 0.7126454412937164}]}, {"text": "Our algorithm has runtime linear in the number of annotation factors in the grammar, improving on the na\u00a8\u0131vena\u00a8\u0131ve algorithm, which has runtime exponential in the number of annotations.", "labels": [], "entities": []}, {"text": "Our method, the Expectation Propagation for Inferring Constituency (EPIC) parser, jointly trains a model over factored annotations, where each factor naturally leverages information from other annotation factors and improves on their mistakes.", "labels": [], "entities": []}, {"text": "The president's Figure 1: Parse trees using four different annotation schemes: (a) Lexicalized annotation like that in; (b) Unlexicalized annotation like that in; (c) Latent annotation like that in; and (d) the factored, mixed annotations we argue for in our paper.", "labels": [], "entities": []}, {"text": "We demonstrate the empirical effectiveness of our approach in two ways.", "labels": [], "entities": []}, {"text": "First, we efficiently train a latent-variable grammar with 8 disjoint one-bit latent annotation factors, with scores as high as 89.7 F1 on length \u226440 sentences from the Penn Treebank ().", "labels": [], "entities": [{"text": "F1", "start_pos": 133, "end_pos": 135, "type": "METRIC", "confidence": 0.9842780232429504}, {"text": "Penn Treebank", "start_pos": 169, "end_pos": 182, "type": "DATASET", "confidence": 0.9957566857337952}]}, {"text": "This latent variable parser outscores the best of's comparable parsers while using two orders of magnitude fewer parameters.", "labels": [], "entities": []}, {"text": "Second, we combine our latent variable factors with lexicalized and unlexicalized annotations, resulting in our best F1 score of 89.4 on all sentences.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 117, "end_pos": 125, "type": "METRIC", "confidence": 0.9866247773170471}]}], "datasetContent": [{"text": "In what follows, we describe three experiments.", "labels": [], "entities": []}, {"text": "First, in a small experiment, we examine how effective the different inference algorithms are for both training and testing.", "labels": [], "entities": []}, {"text": "Second, we scale up our latent variable model into successively larger products.", "labels": [], "entities": []}, {"text": "Finally, we present a selection of the many possible model combinations, showing that combining latent and expert annotation can be quite effective.", "labels": [], "entities": []}, {"text": "For our experiments, we trained and tested on the Penn Treebank using the standard splits: sections 2-21 were training, 22 development, and 23 testing.", "labels": [], "entities": [{"text": "Penn Treebank", "start_pos": 50, "end_pos": 63, "type": "DATASET", "confidence": 0.991167426109314}]}, {"text": "In preliminary experiments, we report development set F1 on sentences up to length 40.", "labels": [], "entities": [{"text": "F1", "start_pos": 54, "end_pos": 56, "type": "METRIC", "confidence": 0.49401190876960754}]}, {"text": "For our final test set experiment, we report F1 on sentences from section 23 up to length 40, as well as all sentences from that section.", "labels": [], "entities": [{"text": "F1", "start_pos": 45, "end_pos": 47, "type": "METRIC", "confidence": 0.998881995677948}]}, {"text": "Scores reported are computed using EVALB).", "labels": [], "entities": [{"text": "EVALB", "start_pos": 35, "end_pos": 40, "type": "METRIC", "confidence": 0.6935258507728577}]}, {"text": "We binarize trees using Collins' head rules.", "labels": [], "entities": []}, {"text": "Each discriminative parser was trained using the Adaptive Gradient variant of Stochastic Gradient Descent ().", "labels": [], "entities": []}, {"text": "Smaller models were seeded from larger models.", "labels": [], "entities": []}, {"text": "That is, before training a grammar of 5 models with 1 latent bit each, we started with weights from a parser with 4 factored bits.", "labels": [], "entities": []}, {"text": "Initial experiments suggested this step did not affect final performance, but greatly decreased total training time, especially for the latent variable parsers.", "labels": [], "entities": []}, {"text": "For extracting a one-best tree, we use aversion of the Max-Recall algorithm of Goodman (1996).", "labels": [], "entities": []}, {"text": "When using EP or ADF, we initialized the core approximation q to the uniform distribution over unpruned trees.", "labels": [], "entities": []}, {"text": "When counting parameters, we consider the number of parameters per binary rule.", "labels": [], "entities": []}, {"text": "Hence, a single four-state latent model would have 64 (= 4 3 ) parameters per rule, while a product of 5 two-state models would have just 40 (= 5 \u00b7 2 3 ).", "labels": [], "entities": []}, {"text": "Most of the previous work in latent variable parsing has focused on splitting smaller unstructured annotations into larger unstructured annotations.", "labels": [], "entities": [{"text": "latent variable parsing", "start_pos": 29, "end_pos": 52, "type": "TASK", "confidence": 0.5818120241165161}]}, {"text": "Here, we consider training a joint model consisting of a large number of disjoint one-bit (i.e. two-state) latent variable annotations.", "labels": [], "entities": []}, {"text": "Specifically, we consider the performance of products of up to 8 one-bit annotations.", "labels": [], "entities": []}, {"text": "In, we show development F1 as a function of the number of latent bits.", "labels": [], "entities": [{"text": "F1", "start_pos": 24, "end_pos": 26, "type": "METRIC", "confidence": 0.8445917963981628}]}, {"text": "Improvement is roughly linear up to 3 components.", "labels": [], "entities": []}, {"text": "Performance levels off afterwards, with the top performing system scoring 89.7 F1.", "labels": [], "entities": [{"text": "F1", "start_pos": 79, "end_pos": 81, "type": "METRIC", "confidence": 0.9947084188461304}]}, {"text": "Nevertheless, these parsers outperform the comparable parsers of   48 instead of the 4096 in their best parser.", "labels": [], "entities": []}, {"text": "We also ran our best system on Section 23, where it gets 89.1 and 88.4 on sentences less than length 40 and on all sentences, respectively.", "labels": [], "entities": [{"text": "Section 23", "start_pos": 31, "end_pos": 41, "type": "DATASET", "confidence": 0.9537688195705414}]}, {"text": "This result compares favorably to the 88.8/88.3 of Petrov and Klein (2008a).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: The effect of algorithm choice for training and  parsing on a product of two 2-state parsers on F1. Petrov  is the product parser of Petrov (2010), and Indep. refers  to independently trained models. For comparison, a four- state parser achieves a score of 83.2.", "labels": [], "entities": [{"text": "parsing", "start_pos": 59, "end_pos": 66, "type": "TASK", "confidence": 0.9590590000152588}, {"text": "F1", "start_pos": 106, "end_pos": 108, "type": "METRIC", "confidence": 0.995598554611206}]}, {"text": " Table 2: Development F1 score for various model com- binations for sentences less than length 40 and all sen- tences. 3xLatent refers to a latent annotation model with  3 factored latent bits.", "labels": [], "entities": [{"text": "Development F1 score", "start_pos": 10, "end_pos": 30, "type": "METRIC", "confidence": 0.7746818661689758}]}]}