{"title": [{"text": "Universal Grapheme-to-Phoneme Prediction Over Latin Alphabets", "labels": [], "entities": []}], "abstractContent": [{"text": "We consider the problem of inducing grapheme-to-phoneme mappings for unknown languages written in a Latin alphabet.", "labels": [], "entities": []}, {"text": "First, we collect a data-set of 107 languages with known grapheme-phoneme relationships, along with a short text in each language.", "labels": [], "entities": []}, {"text": "We then cast our task in the framework of supervised learning, where each known language serves as a training example, and predictions are made on unknown languages.", "labels": [], "entities": []}, {"text": "We induce an undirected graphical model that learns phonotactic regularities, thus relating textual patterns to plausible phonemic interpretations across the entire range of languages.", "labels": [], "entities": []}, {"text": "Our model correctly predicts grapheme-phoneme pairs with over 88% F1-measure.", "labels": [], "entities": [{"text": "F1-measure", "start_pos": 66, "end_pos": 76, "type": "METRIC", "confidence": 0.9981681108474731}]}], "introductionContent": [{"text": "Written language is one of the defining technologies of human civilization, and has been independently invented at least three times through the course of history (.", "labels": [], "entities": []}, {"text": "In many ways written language reflects its more primary spoken counterpart.", "labels": [], "entities": []}, {"text": "Both are subject to some of the same forces of change, including human migration, cultural influence, and imposition by empire.", "labels": [], "entities": []}, {"text": "In other ways, written language harkens further to the past, reflecting aspects of languages long since gone from their spoken forms.", "labels": [], "entities": []}, {"text": "In this paper, we argue that this imperfect relationship between written symbol and spoken sound can be automatically inferred from textual patterns.", "labels": [], "entities": []}, {"text": "By examining data for over 100 languages, we train a statistical model to automatically relate graphemic patterns in text to phonemic sequences for never-before-seen languages.", "labels": [], "entities": []}, {"text": "We focus hereon the the alphabet, a writing system that has comedown to us from the Sumerians.", "labels": [], "entities": []}, {"text": "In an idealized alphabetic system, each phoneme in the language is unambiguously represented by a single grapheme.", "labels": [], "entities": []}, {"text": "In practice of course, this ideal is never achieved.", "labels": [], "entities": []}, {"text": "When existing alphabets are melded onto new languages, they must be imperfectly adapted to anew sound system.", "labels": [], "entities": []}, {"text": "In this paper, we exploit the fact that a single alphabet, that of the Romans, has been adapted to a very large variety of languages.", "labels": [], "entities": []}, {"text": "Recent research has demonstrated the effectiveness of cross-lingual analysis.", "labels": [], "entities": [{"text": "cross-lingual analysis", "start_pos": 54, "end_pos": 76, "type": "TASK", "confidence": 0.776644229888916}]}, {"text": "The joint analysis of several languages can increase model accuracy, and enable the development of computational tools for languages with minimal linguistic resources.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 59, "end_pos": 67, "type": "METRIC", "confidence": 0.9925665855407715}]}, {"text": "Previous work has focused on settings where just a handful of languages are available.", "labels": [], "entities": []}, {"text": "We treat the task of grapheme-to-phoneme analysis as a test case for larger scale multilingual learning, harnessing information from dozens of languages.", "labels": [], "entities": [{"text": "grapheme-to-phoneme analysis", "start_pos": 21, "end_pos": 49, "type": "TASK", "confidence": 0.7547038495540619}]}, {"text": "On a more practical note, accurately relating graphemes and phonemes to one another is crucial for tasks such as automatic speech recognition and text-to-speech generation.", "labels": [], "entities": [{"text": "automatic speech recognition", "start_pos": 113, "end_pos": 141, "type": "TASK", "confidence": 0.6088091333707174}, {"text": "text-to-speech generation", "start_pos": 146, "end_pos": 171, "type": "TASK", "confidence": 0.7405397295951843}]}, {"text": "While pronunciation dictionaries and transcribed audio are available for some languages, these resources are entirely lacking for the vast majority of the world's languages.", "labels": [], "entities": []}, {"text": "Thus, automatic and generic methods for determining sound-symbol relationships are needed.", "labels": [], "entities": []}, {"text": "Our paper is based on the following line of reasoning: that character-level textual patterns mirror phonotactic regularities; that phonotactic regularities are shared across related languages and universally constrained; and that textual patterns fora newly observed language may thus reveal its underlying phonemics.", "labels": [], "entities": []}, {"text": "Our task can be viewed as an easy case of lost language decipherment -one where the underlying alphabetic system is widely known.", "labels": [], "entities": [{"text": "lost language decipherment", "start_pos": 42, "end_pos": 68, "type": "TASK", "confidence": 0.672571619351705}]}, {"text": "Nevertheless, the task of grapheme-to-phoneme prediction is challenging.", "labels": [], "entities": [{"text": "grapheme-to-phoneme prediction", "start_pos": 26, "end_pos": 56, "type": "TASK", "confidence": 0.7185894846916199}]}, {"text": "Characters in the Roman alphabet can take a wide range of phonemic values across the world's languages.", "labels": [], "entities": []}, {"text": "For example, depending on the language, the grapheme \"c\" can represent the following phonemes: 1 \u2022 /k/ (unvoiced velar plosive) \u2022 /c/ (unvoiced palatal plosive) \u2022 /s/ (unvoiced alveolar fricative) \u2022 / > tS/ (affricated unvoiced postalveolar fricative) \u2022 / > ts/ (affricated unvoiced alveolar fricative) To make matters worse, the same language may use a single grapheme to ambiguously represent multiple phonemes.", "labels": [], "entities": []}, {"text": "For example, English orthography uses \"c\" to represent both /k/ and /s/.", "labels": [], "entities": []}, {"text": "Our task is thus to select a subset of phonemes for each language's graphemes.", "labels": [], "entities": []}, {"text": "We cast the subset selection problem as a set of related binary prediction problems, one for each possible grapheme-phoneme pair.", "labels": [], "entities": []}, {"text": "Taken together, these predictions yield the grapheme-phoneme mapping for that language.", "labels": [], "entities": []}, {"text": "We develop a probabilistic undirected graphical model for this prediction problem, where a large set of languages serve as training data and a single heldout language serves as test data.", "labels": [], "entities": []}, {"text": "Each training and test language yields an instance of the graph, bound For some brief background on phonetics, see Section 2.", "labels": [], "entities": []}, {"text": "Note that we use the term \"phoneme\" throughout the paper, though we also refer to \"phonetic\" properties.", "labels": [], "entities": []}, {"text": "As we are dealing with texts (written in a roughly phonemic writing system), we have no access to the true contextual phonetic realizations, and even using IPA symbols to relate symbols across languages is somewhat theoretically suspect.", "labels": [], "entities": []}, {"text": "together through a shared set of features and parameter values to allow cross-lingual learning and generalization.", "labels": [], "entities": []}, {"text": "In the graph corresponding to a given language, each node represents a grapheme-phoneme pair (g : p).", "labels": [], "entities": []}, {"text": "The node is labeled with a binary value to indicate whether grapheme g can represent phoneme pin the language.", "labels": [], "entities": []}, {"text": "In order to allow coupled labelings across the various grapheme-phoneme pairs of the language, we employ a connected graph structure, with an automatically learned topology shared across the languages.", "labels": [], "entities": []}, {"text": "The node and edge features are derived from textual co-occurrence statistics for the graphemes of each language, as well as general information about the language's family and region.", "labels": [], "entities": []}, {"text": "Parameters are jointly optimized over the training languages to maximize the likelihood of the node labelings given the observed feature values.", "labels": [], "entities": []}, {"text": "See Figure 1 fora snippet of the model.", "labels": [], "entities": []}, {"text": "We apply our model to a novel data-set consisting of grapheme-phoneme mappings for 107 languages with Roman alphabets and short texts.", "labels": [], "entities": []}, {"text": "In this setting, we consider each language in turn as the test language, and train our model on the remaining 106 languages.", "labels": [], "entities": []}, {"text": "Our highest performing model achieves an F1-measure of 88%, yielding perfect predictions for over 21% of languages.", "labels": [], "entities": [{"text": "F1-measure", "start_pos": 41, "end_pos": 51, "type": "METRIC", "confidence": 0.9996201992034912}]}, {"text": "These results compare quite favorably to several baselines.", "labels": [], "entities": []}, {"text": "Our experiments lead to several conclusions.", "labels": [], "entities": []}, {"text": "(i) Character co-occurence features alone are not sufficient for cross-lingual predictive accuracy in this task.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 90, "end_pos": 98, "type": "METRIC", "confidence": 0.8970928192138672}]}, {"text": "Instead, we map raw contextual counts to more linguistically meaningful generalizations to learn effective cross-lingual patterns.", "labels": [], "entities": []}, {"text": "(ii) A connected graph topology is crucial for learning linguistically coherent grapheme-to-phoneme mappings.", "labels": [], "entities": []}, {"text": "Without any edges, our model yields perfect mappings for only 10% of test languages.", "labels": [], "entities": []}, {"text": "By employing structure learning and including the induced edges, we more than double the number of test languages with perfect predictions.", "labels": [], "entities": []}, {"text": "(iii) Finally, an analysis of our grapheme-phoneme predictions shows that they do not achieve certain global characteristics observed across true phoneme inventories.", "labels": [], "entities": []}, {"text": "In particular, the level of \"feature economy\" in our predictions is too low, suggesting an avenue for future research.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we describe the set of experiments performed to evaluate the performance of our model.", "labels": [], "entities": []}, {"text": "Besides our primary undirected graphical model, we also consider several baselines and variants, in order to assess the contribution of our model's graph structure as well as the features used.", "labels": [], "entities": []}, {"text": "In all cases, we perform leave-one-out cross-validation over the 107 languages in our data-set.", "labels": [], "entities": []}, {"text": "We report our results using three evaluation metrics of increasing coarseness.", "labels": [], "entities": []}, {"text": "1. Phoneme-level: For individual graphemephoneme pairs (e.g. a:/5/, a:/2/, c:/k/, c:/tS/) our task consists of a set of binary predictions, and can thus be evaluated in terms of precision, recall, and F1-measure.", "labels": [], "entities": [{"text": "precision", "start_pos": 178, "end_pos": 187, "type": "METRIC", "confidence": 0.9995570778846741}, {"text": "recall", "start_pos": 189, "end_pos": 195, "type": "METRIC", "confidence": 0.9993624091148376}, {"text": "F1-measure", "start_pos": 201, "end_pos": 211, "type": "METRIC", "confidence": 0.9987586736679077}]}, {"text": "We report micro-averages of these quantities across all: The performance of baselines and variants of our model, evaluated at the phoneme-level (binary predictions), whole-grapheme accuracy, and whole-language accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 181, "end_pos": 189, "type": "METRIC", "confidence": 0.9724016785621643}, {"text": "accuracy", "start_pos": 210, "end_pos": 218, "type": "METRIC", "confidence": 0.9724487066268921}]}, {"text": "grapheme-phoneme pairs in all leave-one-out test languages.", "labels": [], "entities": []}, {"text": "2. Grapheme-level: We also report graphemelevel accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 48, "end_pos": 56, "type": "METRIC", "confidence": 0.976885974407196}]}, {"text": "For this metric, we consider each grapheme g and examine its predicted labels overall its possible phonemes: . If all k binary predictions are correct, then the grapheme's phoneme-set has been correctly predicted.", "labels": [], "entities": []}, {"text": "We report the percentage of all graphemes with such correct predictions (micro-averaged overall graphemes in all test language scenarios).", "labels": [], "entities": []}, {"text": "3. Language-level: Finally, we assess languagewide performance.", "labels": [], "entities": []}, {"text": "For this metric, we report the percentage of test languages for which our model achieves perfect predictions on all grapheme-phoneme pairs, yielding a perfect mapping.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Ambiguous graphemes and the set of phonemes  that they may represent among our set of 107 languages.", "labels": [], "entities": [{"text": "Ambiguous", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9662396907806396}]}, {"text": " Table 2: Number of features in each category before  and after discretization/filtering. Note that the pair-wise  conjunction features are not included in these counts.", "labels": [], "entities": []}, {"text": " Table 3: The performance of baselines and variants of our model, evaluated at the phoneme-level (binary predictions),  whole-grapheme accuracy, and whole-language accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 135, "end_pos": 143, "type": "METRIC", "confidence": 0.966915488243103}, {"text": "accuracy", "start_pos": 164, "end_pos": 172, "type": "METRIC", "confidence": 0.9626982808113098}]}, {"text": " Table 4: Measures of feature economy applied to the pre- dicted and true consonant inventories (averaged over all  107 languages).", "labels": [], "entities": []}]}