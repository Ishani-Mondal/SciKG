{"title": [{"text": "Reading The Web with Learned Syntactic-Semantic Inference Rules", "labels": [], "entities": []}], "abstractContent": [{"text": "We study how to extend a large knowledge base (Freebase) by reading relational information from a large Web text corpus.", "labels": [], "entities": []}, {"text": "Previous studies on extracting relational knowledge from text show the potential of syntactic patterns for extraction, but they do not exploit background knowledge of other relations in the knowledge base.", "labels": [], "entities": []}, {"text": "We describe a distributed, Web-scale implementation of a path-constrained random walk model that learns syntactic-semantic inference rules for binary relations from a graph representation of the parsed text and the knowledge base.", "labels": [], "entities": []}, {"text": "Experiments show significant accuracy improvements in binary relation prediction over methods that consider only text, or only the existing knowledge base.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 29, "end_pos": 37, "type": "METRIC", "confidence": 0.9981993436813354}, {"text": "binary relation prediction", "start_pos": 54, "end_pos": 80, "type": "TASK", "confidence": 0.726349393526713}]}], "introductionContent": [{"text": "Manually-created knowledge bases (KBs) often lack basic information about some entities and their relationships, either because the information was missing in the initial sources used to create the KB, or because human curators were not confident about the status of some putative fact, and so they excluded it from the KB.", "labels": [], "entities": []}, {"text": "For instance, as we will see in more detail later, many person entries in Freebase () lack nationality information.", "labels": [], "entities": [{"text": "Freebase", "start_pos": 74, "end_pos": 82, "type": "DATASET", "confidence": 0.9688047170639038}]}, {"text": "To fill those KB gaps, we might use general rules, ideally automatically learned, such as \"if person was born in town and town is in country * This research was carried out during an internship at Google Research then the person is a national of the country.\"", "labels": [], "entities": []}, {"text": "Of course, rules like this maybe defeasible, in this case for example because of naturalization or political changes.", "labels": [], "entities": []}, {"text": "Nevertheless, many such imperfect rules can be learned and combined to yield useful KB completions, as demonstrated in particular with the Path-Ranking Algorithm (PRA) (), which learns such rules on heterogenous graphs for link prediction tasks.", "labels": [], "entities": [{"text": "link prediction tasks", "start_pos": 223, "end_pos": 244, "type": "TASK", "confidence": 0.8168905774752299}]}, {"text": "Alternatively, we may attempt to fill KB gaps by applying relation extraction rules to free text.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 58, "end_pos": 77, "type": "TASK", "confidence": 0.7989456951618195}]}, {"text": "For instance, and showed the value of syntactic patterns in extracting specific relations.", "labels": [], "entities": []}, {"text": "In those approaches, KB tuples of the relation to be extracted serve as positive training examples to the extraction rule induction algorithm.", "labels": [], "entities": [{"text": "extraction rule induction", "start_pos": 106, "end_pos": 131, "type": "TASK", "confidence": 0.6748260855674744}]}, {"text": "However, the KB contains much more knowledge about other relations that could potentially be helpful in improving relation extraction accuracy and coverage, but that is not used in such purely text-based approaches.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 114, "end_pos": 133, "type": "TASK", "confidence": 0.8120134770870209}, {"text": "accuracy", "start_pos": 134, "end_pos": 142, "type": "METRIC", "confidence": 0.9308855533599854}]}, {"text": "In this work, we use PRA to learn weighted rules (represented as graph path patterns) that combine both semantic (KB) and syntactic information encoded respectively as edges in a graphstructured KB, and as syntactic dependency edges in dependency-parsed Web text.", "labels": [], "entities": []}, {"text": "Our approach can easily incorporate existing knowledge in extraction tasks, and its distributed implementation scales to the whole of the Freebase KB and 60 million parsed documents.", "labels": [], "entities": [{"text": "Freebase KB", "start_pos": 138, "end_pos": 149, "type": "DATASET", "confidence": 0.9640894532203674}]}, {"text": "To the best of our knowledge, this is the first successful attempt to apply relational learning methods to heterogeneous data with this scale.", "labels": [], "entities": []}], "datasetContent": [{"text": "We use Freebase as our knowledge base.", "labels": [], "entities": [{"text": "Freebase", "start_pos": 7, "end_pos": 15, "type": "DATASET", "confidence": 0.9877035617828369}]}, {"text": "Freebase data is harvested from many sources, including Wikipedia, AMG, and IMDB.", "labels": [], "entities": [{"text": "Freebase data", "start_pos": 0, "end_pos": 13, "type": "DATASET", "confidence": 0.9432646334171295}, {"text": "AMG", "start_pos": 67, "end_pos": 70, "type": "DATASET", "confidence": 0.9027839303016663}, {"text": "IMDB", "start_pos": 76, "end_pos": 80, "type": "DATASET", "confidence": 0.9212170243263245}]}, {"text": "As of this writing, it contains more than 21 million concepts and 70 million labeled edges.", "labels": [], "entities": []}, {"text": "For a large majority of concepts that appear both in Freebase and Wikipedia, Freebase maintains a link to the Wikipedia page of that concept.", "labels": [], "entities": [{"text": "Freebase", "start_pos": 77, "end_pos": 85, "type": "DATASET", "confidence": 0.9467373490333557}]}, {"text": "We also collect a large Web corpus and identify 60 million pages that mention concepts relevant to this study.", "labels": [], "entities": []}, {"text": "The free text on those pages are POS-tagged and dependency parsed with an accuracy comparable to that of the current Stanford dependency parser (.", "labels": [], "entities": [{"text": "dependency parsed", "start_pos": 48, "end_pos": 65, "type": "TASK", "confidence": 0.7086661756038666}, {"text": "accuracy", "start_pos": 74, "end_pos": 82, "type": "METRIC", "confidence": 0.9995039701461792}]}, {"text": "The parser produces a dependency tree for each sentence with each edge labeled with a standard dependency tag (see.", "labels": [], "entities": []}, {"text": "In each of the parsed documents, we use POS tags and dependency edges to identify potential referring noun phrases (NPs).", "labels": [], "entities": []}, {"text": "We then use a within-document coreference resolver comparable to that of to group referring NPs into co-referring clusters.", "labels": [], "entities": []}, {"text": "For each cluster that contains a proper-name mention, we find the Freebase concept or concepts, if any, with a name or alias that matches, the number of times the concept c is referred by mention m by using both the alias information in  Previous work in relation extraction from parsed text () has mostly used binary features to indicate whether a pattern is present in the sentences where two concepts are mentioned.", "labels": [], "entities": [{"text": "relation extraction from parsed text", "start_pos": 255, "end_pos": 291, "type": "TASK", "confidence": 0.8492886304855347}]}, {"text": "To investigate the benefit of having fractional valued features generated by random walks (as in PRA), we also evaluate a binarized PRA approach, for which we use the same syntactic-semantic pattern features as PRA does, but binarize the feature values from PRA: if the original fractional feature value was zero, the feature value is set to zero (equivalent to not having the feature in that example), otherwise it is set to 1.", "labels": [], "entities": []}, {"text": "shows a comparison of the results obtained using the PRA algorithm trained using only Freebase (KB), using only the text corpus graph (Text), trained with both Freebase and the text corpus (KB+Text) and the binarized PRA algorithm using both Freebase and the text corpus).", "labels": [], "entities": [{"text": "Freebase", "start_pos": 160, "end_pos": 168, "type": "DATASET", "confidence": 0.9650581479072571}, {"text": "Freebase", "start_pos": 242, "end_pos": 250, "type": "DATASET", "confidence": 0.9554916620254517}]}, {"text": "We report Mean Reciprocal Rank (MRR) where, given a set of queries Q, Comparing the results of first three columns we see that combining Freebase and text achieves significantly better results than using either Freebase or text alone.", "labels": [], "entities": [{"text": "Mean Reciprocal Rank (MRR)", "start_pos": 10, "end_pos": 36, "type": "METRIC", "confidence": 0.9656234284241995}]}, {"text": "Further comparing the results of last two columns we also observe a significant drop in MRR for the binarized version of PRA.", "labels": [], "entities": [{"text": "MRR", "start_pos": 88, "end_pos": 91, "type": "METRIC", "confidence": 0.9907470941543579}]}, {"text": "This clearly shows the importance of using the random walk probabilities.", "labels": [], "entities": []}, {"text": "It can also be seen that the MRR for the parents relation is lower than those for other relations.", "labels": [], "entities": [{"text": "MRR", "start_pos": 29, "end_pos": 32, "type": "METRIC", "confidence": 0.9981393814086914}]}, {"text": "This is mainly because there are larger number of potential answers for each query node of Parent relation than for each query node of the other two relations -all persons in Freebase versus all professions or nationalities.", "labels": [], "entities": [{"text": "Freebase", "start_pos": 175, "end_pos": 183, "type": "DATASET", "confidence": 0.974946916103363}]}, {"text": "Finally, it is important to point out that our evaluations are actually lower bounds of actual performance, because, for instance, a person might have a profession besides the ones in Freebase and in such cases, this evaluation does not give any credit for predicting those professionsthey are treated as errors.", "labels": [], "entities": []}, {"text": "We try to address this issue with the manual evaluations in the next section.", "labels": [], "entities": []}, {"text": "only reports results for the maximum path length = 4 case.", "labels": [], "entities": []}, {"text": "We found that shorter maximum path lengths give worse results: for instance, with = 3 for the profession relation, MRR drops to 0.542, from 0.583 for = 4 when using both Freebase and text.", "labels": [], "entities": [{"text": "MRR", "start_pos": 115, "end_pos": 118, "type": "METRIC", "confidence": 0.8300454020500183}]}, {"text": "This difference is significant at the 0.0001 level according to a difference of proportions test.", "labels": [], "entities": []}, {"text": "Further we find that using longer path length takes much longer time to train and test, but does not lead to significant improvements over the = 4 case.", "labels": [], "entities": []}, {"text": "For example, for profession, = 5 gives a MRR of 0.589.", "labels": [], "entities": [{"text": "MRR", "start_pos": 41, "end_pos": 44, "type": "METRIC", "confidence": 0.9980385899543762}]}, {"text": "shows the top weighted features that involve text edges for PRA models trained on both Freebase and the text corpus.", "labels": [], "entities": []}, {"text": "To make them easier to understand, we group them based on their functionality.", "labels": [], "entities": []}, {"text": "For the profession and nationality tasks, the conjunction dependency relation (in group 1,4) plays an important role: these features first find persons mentioned in conjunction with the query: Top weighted path types involving text edges for each task grouped according to functionality.", "labels": [], "entities": []}, {"text": "M relations connect each concept in knowledge base to its mentions in the corpus.", "labels": [], "entities": []}, {"text": "TW relations connect each token in a sentence to the words in the text representation of this token.", "labels": [], "entities": []}, {"text": "CW relations connect each concept in knowledge base to the words in the text representation of this concept.", "labels": [], "entities": []}, {"text": "We use lowercase names to denote dependency edges, word capitalized names to denote KB edges, and \" \u22121 \" to denote the inverse of a relation.", "labels": [], "entities": []}, {"text": "Typically, each trained model includes hundreds of paths with non-zero weights, so the bulk of classifications are not based on a few high-precisionrecall patterns, but rather on the combination of a large number of lower-precision high-recall or high-precision lower-recall rules.", "labels": [], "entities": []}, {"text": "We performed two sets of manual evaluations.", "labels": [], "entities": []}, {"text": "In each case, an annotator is presented with the triples predicted by PRA, and asked if they are correct.", "labels": [], "entities": []}, {"text": "The annotator has access to the Freebase and Wikipedia pages for the concepts (and is able to issue search queries about the concepts).", "labels": [], "entities": [{"text": "Freebase", "start_pos": 32, "end_pos": 40, "type": "DATASET", "confidence": 0.9736862778663635}]}, {"text": "In the first evaluation, we compared the performance of two PRA models, one trained using the stratified sampled queries and another trained using a randomly sampled set of queries for the profession relation.", "labels": [], "entities": []}, {"text": "For each model, we randomly sample 100 predictions from the top 1000 predictions (sorted by the scores returned by the model).", "labels": [], "entities": []}, {"text": "We found that the PRA model trained with stratified sampled queries has 0.92 precision, while the other model has only 0.84 precision (significant at the 0.02 level).", "labels": [], "entities": [{"text": "precision", "start_pos": 77, "end_pos": 86, "type": "METRIC", "confidence": 0.9976879358291626}, {"text": "precision", "start_pos": 124, "end_pos": 133, "type": "METRIC", "confidence": 0.9929044842720032}]}, {"text": "This shows that stratified sampling leads to improved We also evaluated the new beliefs proposed by the models trained for all the three relations using stratified sampled queries.", "labels": [], "entities": []}, {"text": "We estimated precision for the top 100 predictions and randomly sampled 100 predictions each from the top 1,000 and 10,000 predictions.", "labels": [], "entities": [{"text": "precision", "start_pos": 13, "end_pos": 22, "type": "METRIC", "confidence": 0.9994474053382874}]}, {"text": "Here we use the PRA model trained using both KB and text.", "labels": [], "entities": []}, {"text": "The results of this evaluation are shown in.", "labels": [], "entities": []}, {"text": "It can be seen that the PRA model is able to produce very high precision predications even when one considers the top 10,000 predictions.", "labels": [], "entities": [{"text": "PRA", "start_pos": 24, "end_pos": 27, "type": "TASK", "confidence": 0.8707277774810791}, {"text": "precision", "start_pos": 63, "end_pos": 72, "type": "METRIC", "confidence": 0.986395001411438}]}, {"text": "Finally, note that our model is inductive.", "labels": [], "entities": []}, {"text": "For instance, for the profession relation, we are able to predict professions for the around 2 million persons in Freebase.", "labels": [], "entities": [{"text": "Freebase", "start_pos": 114, "end_pos": 122, "type": "DATASET", "confidence": 0.9852472543716431}]}, {"text": "The top 1000 profession facts extracted by our system involve 970 distinct people, the top 10,000 facts involve 8,726 distinct people, and the top 100,000 facts involve 79,885 people.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Size of training and test sets for each relation.", "labels": [], "entities": []}, {"text": " Table 2: Mean Reciprocal Rank (MRR) for different approaches under closed-world assumption. Here KB, Text and  KB+Text columns represent results obtained by training a PRA model with only the KB, only text, and both KB and  text. KB+Text[b] is the binarized PRA approach trained on both KB and text. The best performing system (results  shown in bold font) is significant at 0.0001 level over its nearest competitor according to a difference of proportions  significance test.", "labels": [], "entities": [{"text": "Mean Reciprocal Rank (MRR)", "start_pos": 10, "end_pos": 36, "type": "METRIC", "confidence": 0.9351702034473419}]}, {"text": " Table 4: Human judgement for predicted new beliefs.", "labels": [], "entities": []}]}