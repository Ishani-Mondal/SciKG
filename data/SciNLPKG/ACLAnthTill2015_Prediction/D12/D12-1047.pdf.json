{"title": [{"text": "Translation Model Based Cross-Lingual Language Model Adaptation: from Word Models to Phrase Models", "labels": [], "entities": [{"text": "Translation Model Based Cross-Lingual Language Model Adaptation", "start_pos": 0, "end_pos": 63, "type": "TASK", "confidence": 0.7963921683175224}]}], "abstractContent": [{"text": "In this paper, we propose a novel translation model (TM) based cross-lingual data selection model for language model (LM) adaptation in statistical machine translation (SMT), from word models to phrase models.", "labels": [], "entities": [{"text": "language model (LM) adaptation", "start_pos": 102, "end_pos": 132, "type": "TASK", "confidence": 0.6831391652425131}, {"text": "statistical machine translation (SMT)", "start_pos": 136, "end_pos": 173, "type": "TASK", "confidence": 0.7889900306860606}]}, {"text": "Given a source sentence in the translation task, this model directly estimates the probability that a sentence in the target LM training corpus is similar.", "labels": [], "entities": [{"text": "translation task", "start_pos": 31, "end_pos": 47, "type": "TASK", "confidence": 0.9045245349407196}]}, {"text": "Compared with the traditional approaches which utilize the first pass translation hypotheses, cross-lingual data selection model avoids the problem of noisy proliferation.", "labels": [], "entities": []}, {"text": "Furthermore, phrase TM based cross-lingual data selection model is more effective than the traditional approaches based on bag-of-words models and word-based TM, because it captures contextual information in model-ing the selection of phrase as a whole.", "labels": [], "entities": []}, {"text": "Experiments conducted on large-scale data sets demonstrate that our approach significantly outperforms the state-of-the-art approaches on both LM perplexity and SMT performance.", "labels": [], "entities": [{"text": "LM perplexity", "start_pos": 143, "end_pos": 156, "type": "TASK", "confidence": 0.8785811364650726}, {"text": "SMT", "start_pos": 161, "end_pos": 164, "type": "TASK", "confidence": 0.9944156408309937}]}], "introductionContent": [{"text": "Language model (LM) plays a critical role in statistical machine translation (SMT).", "labels": [], "entities": [{"text": "statistical machine translation (SMT)", "start_pos": 45, "end_pos": 82, "type": "TASK", "confidence": 0.8257716099421183}]}, {"text": "It seems to be a universal truth that LM performance can always be improved by using more training data), but only if the training data is reasonably well-matched with the desired output.", "labels": [], "entities": [{"text": "LM", "start_pos": 38, "end_pos": 40, "type": "TASK", "confidence": 0.9501174688339233}]}, {"text": "It is also obvious that among the large training data the topics or domains of discussion will change ), which causes the mismatch problems with the translation task.", "labels": [], "entities": [{"text": "translation task", "start_pos": 149, "end_pos": 165, "type": "TASK", "confidence": 0.8998617827892303}]}, {"text": "For this reason, most researchers preferred to select similar training data from the large training corpus in the past few years ().", "labels": [], "entities": []}, {"text": "This would empirically provide more accurate lexical probabilities, and thus better match the translation task at hand).", "labels": [], "entities": []}, {"text": "Many previous data selection approaches for LM adaptation in SMT depend on the first pass translation hypotheses, they select the sentences which are similar to the translation hypotheses.", "labels": [], "entities": [{"text": "LM adaptation", "start_pos": 44, "end_pos": 57, "type": "TASK", "confidence": 0.9849492907524109}, {"text": "SMT", "start_pos": 61, "end_pos": 64, "type": "TASK", "confidence": 0.9378823041915894}]}, {"text": "These schemes are overall limited by the quality of the translation hypotheses (, and better initial translation hypotheses lead to better selected sentences ().", "labels": [], "entities": []}, {"text": "However, while SMT has achieved a great deal of development in recent years, the translation hypotheses are still far from perfect, which have many noisy data.", "labels": [], "entities": [{"text": "SMT", "start_pos": 15, "end_pos": 18, "type": "TASK", "confidence": 0.9945770502090454}]}, {"text": "The noisy translation hypotheses mislead data selection process (, and thus take noisy data into the selected training data, which causes noisy proliferation and degrades the performance of adapted LM.", "labels": [], "entities": []}, {"text": "Furthermore, traditional approaches for LM adaptation are based on bag-of-words models and considered to be context independent, despite of their state-of-the-art performance, such as TF-IDF (, centroid similarity, and cross-lingual similarity (CLS) ().", "labels": [], "entities": [{"text": "LM adaptation", "start_pos": 40, "end_pos": 53, "type": "TASK", "confidence": 0.9766805768013}, {"text": "TF-IDF", "start_pos": 184, "end_pos": 190, "type": "METRIC", "confidence": 0.8916218280792236}]}, {"text": "They all perform at the word level, exact only ter-m matching schemes, and do not take into account any contextual information when modeling the selection by single words in isolation, which degrade the quality of selected sentences.", "labels": [], "entities": []}, {"text": "In this paper, we argue that it is beneficial to model the data selection based on the source translation task directly and capture the contextual information for LM adaptation.", "labels": [], "entities": [{"text": "LM adaptation", "start_pos": 163, "end_pos": 176, "type": "TASK", "confidence": 0.9720154702663422}]}, {"text": "To this end, we propose a more principled translation model (TM) based cross-lingual data selection model for LM adaptation, from word models to phrase models.", "labels": [], "entities": [{"text": "LM adaptation", "start_pos": 110, "end_pos": 123, "type": "TASK", "confidence": 0.9508349597454071}]}, {"text": "We assume that the data selection should be performed by the cross-lingual model and at the phrase level.", "labels": [], "entities": []}, {"text": "Given a source sentence in the translation task, this model directly estimates the probability before translation that a sentence in the target LM training corpus is similar.", "labels": [], "entities": [{"text": "translation task", "start_pos": 31, "end_pos": 47, "type": "TASK", "confidence": 0.9021011590957642}]}, {"text": "Therefore, it does not require the translation task to be pre-translation as in monolingual adaptation, and can address the problem of noisy proliferation.", "labels": [], "entities": []}, {"text": "To the best of our knowledge, this is the first extensive and empirical study of using phrase T-M based cross-lingual data selection for LM adaptation.", "labels": [], "entities": [{"text": "LM adaptation", "start_pos": 137, "end_pos": 150, "type": "TASK", "confidence": 0.9675908982753754}]}, {"text": "This model learns the transform probability of a multi-term phrase in a source sentence given a phrase in the target sentence of LM training corpus.", "labels": [], "entities": []}, {"text": "Compared with bag-of-words models and word-based TM that account for selecting single words in isolation, this model performs at the phrase level and captures some contextual information in modeling the selection of phrase as a whole, thus it is potentially more effective.", "labels": [], "entities": []}, {"text": "More precise data selection can be determined for phrases than for words.", "labels": [], "entities": []}, {"text": "In this model, we propose a linear ranking model framework to further improve the performance, referred to the linear discriminant function () in pattern classification and information retrieval (IR), where different models are incorporated as features, as we will show in our experiments.", "labels": [], "entities": [{"text": "pattern classification and information retrieval (IR)", "start_pos": 146, "end_pos": 199, "type": "TASK", "confidence": 0.7595146186649799}]}, {"text": "Unlike the general TM in SMT, we explore the use of TextRank algorithm () to identify and eliminate unimportant words (e.g., non-topical words, common words) for corpus preprocessing, and construct TM by important words.", "labels": [], "entities": [{"text": "SMT", "start_pos": 25, "end_pos": 28, "type": "TASK", "confidence": 0.9094773530960083}]}, {"text": "This reduces the average number of words in crosslingual data selection model, thus improving the efficiency.", "labels": [], "entities": []}, {"text": "Moreover, TextRank utilizes the context information of words to assign term weights (, which makes phrase TM based crosslingual data selection model play its advantage of capturing the contextual information, thus further improving the performance.", "labels": [], "entities": []}, {"text": "The remainder of this paper is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 introduces the related work of LM adaptation.", "labels": [], "entities": [{"text": "LM adaptation", "start_pos": 41, "end_pos": 54, "type": "TASK", "confidence": 0.9666038751602173}]}, {"text": "Section 3 presents the framework of cross-lingual data selection for LM adaptation.", "labels": [], "entities": [{"text": "LM adaptation", "start_pos": 69, "end_pos": 82, "type": "TASK", "confidence": 0.9748750925064087}]}, {"text": "Section 4 describes our proposed TM based crosslingual data selection model: from word models to phrase models.", "labels": [], "entities": [{"text": "TM based crosslingual data selection", "start_pos": 33, "end_pos": 69, "type": "TASK", "confidence": 0.7959173798561097}]}, {"text": "In section 5 we present large-scale experiments and analyses, and followed by conclusions and future work in section 6.", "labels": [], "entities": []}], "datasetContent": [{"text": "We measure the utility of our proposed LM adaptation approach in two ways: (a) comparing reference translations based perplexity of adapted LMs with the generic LM, and (b) comparing SMT performance of adapted LMs with the generic LM.", "labels": [], "entities": [{"text": "LM adaptation", "start_pos": 39, "end_pos": 52, "type": "TASK", "confidence": 0.932951956987381}, {"text": "SMT", "start_pos": 183, "end_pos": 186, "type": "TASK", "confidence": 0.9931840300559998}]}, {"text": "We carryout translation experiments on the test set by hierarchical phrase-based (HPB) SMT) system to demonstrate the utility of LM adaptation on improving SMT performance by BLEU score ().", "labels": [], "entities": [{"text": "SMT)", "start_pos": 87, "end_pos": 91, "type": "TASK", "confidence": 0.8451518714427948}, {"text": "LM adaptation", "start_pos": 129, "end_pos": 142, "type": "TASK", "confidence": 0.8439503610134125}, {"text": "SMT", "start_pos": 156, "end_pos": 159, "type": "TASK", "confidence": 0.9931621551513672}, {"text": "BLEU score", "start_pos": 175, "end_pos": 185, "type": "METRIC", "confidence": 0.9787002503871918}]}, {"text": "The generic LM and adapted LMs are estimated as above in perplexity analysis experiments.", "labels": [], "entities": []}, {"text": "We use minimum error rate training to tune the feature weights of HPB for maximum BLEU score on the development set with serval groups of different start weights.: The impact of noisy data in the translation hypotheses on the performance of LM adaptation.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 82, "end_pos": 86, "type": "METRIC", "confidence": 0.9991862177848816}, {"text": "LM adaptation", "start_pos": 241, "end_pos": 254, "type": "TASK", "confidence": 0.9636570513248444}]}, {"text": "test sets, and the improvements are statistically significant at the 95% confidence interval with respect to the baseline.", "labels": [], "entities": []}, {"text": "From the comparison results, we get some clear trends: (1) Cross-lingual data selection model outperforms the traditional approaches which utilize the first pass translation hypotheses (row 4 vs. row2; row 11 vs. row 9), but the detailed impact of noisy data in the translation hypotheses on data selection will be shown in the next section (section 5.4).", "labels": [], "entities": [{"text": "Cross-lingual data selection", "start_pos": 59, "end_pos": 87, "type": "TASK", "confidence": 0.6543609003225962}, {"text": "data selection", "start_pos": 292, "end_pos": 306, "type": "TASK", "confidence": 0.7976093292236328}]}, {"text": "(2) CLWTM significantly outperforms CLS s (row 6 vs. row 4; row 13 vs. row 11), we suspect that word-based TM makes more accurate cross-lingual data selection model than single cross-lingual projection (.", "labels": [], "entities": []}, {"text": "(3) Compared with (, adding the smoothing mechanism in the sentence state for CLWTM significantly improves the performance (row 6 vs. row 5; row 13 vs. row 12).", "labels": [], "entities": []}, {"text": "(4) Phrase-based TM (CLPTM) significantly outperforms the state-of-the-art approaches based on bag-of-words models and word-based TM (row 7 vs. row 2, row 4, row 5 and row 6; row 14 vs. row 9, row 11, row 12 and row 13).", "labels": [], "entities": [{"text": "Phrase-based TM", "start_pos": 4, "end_pos": 19, "type": "TASK", "confidence": 0.6697413921356201}]}], "tableCaptions": [{"text": " Table 1: English reference translations based perplexi- ty of adapted LMs with different approaches on two test  sets, with the top 8K sentences on IWSLT-07 and top 16K  sentences on NIST-06, respectively.", "labels": [], "entities": [{"text": "IWSLT-07", "start_pos": 149, "end_pos": 157, "type": "DATASET", "confidence": 0.9385801553726196}, {"text": "NIST-06", "start_pos": 184, "end_pos": 191, "type": "DATASET", "confidence": 0.9785063862800598}]}, {"text": " Table 2: Comparison of SMT performance (p < 0.05)  with different approaches for LM adaptation on two test  sets.", "labels": [], "entities": [{"text": "SMT", "start_pos": 24, "end_pos": 27, "type": "TASK", "confidence": 0.9945597052574158}, {"text": "LM adaptation", "start_pos": 82, "end_pos": 95, "type": "TASK", "confidence": 0.9789809584617615}]}, {"text": " Table 3: The impact of noisy data in the translation hy- potheses on the performance of LM adaptation.", "labels": [], "entities": [{"text": "LM adaptation", "start_pos": 89, "end_pos": 102, "type": "TASK", "confidence": 0.9714238047599792}]}, {"text": " Table 4: The impact of phrase length in CLPTM on the  performance of LM adaptation, and the maximum phrase  length is four.", "labels": [], "entities": [{"text": "CLPTM", "start_pos": 41, "end_pos": 46, "type": "DATASET", "confidence": 0.857988178730011}, {"text": "LM adaptation", "start_pos": 70, "end_pos": 83, "type": "TASK", "confidence": 0.9676873087882996}]}, {"text": " Table 5: The impact of eliminating unimportant words  by TextRank algorithm on the performance of CLTM for  LM adaptation.", "labels": [], "entities": [{"text": "LM adaptation", "start_pos": 109, "end_pos": 122, "type": "TASK", "confidence": 0.9400216937065125}]}]}