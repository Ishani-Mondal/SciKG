{"title": [{"text": "Large Scale Decipherment for Out-of-Domain Machine Translation", "labels": [], "entities": [{"text": "Out-of-Domain Machine Translation", "start_pos": 29, "end_pos": 62, "type": "TASK", "confidence": 0.6476931571960449}]}], "abstractContent": [{"text": "We apply slice sampling to Bayesian de-cipherment and use our new decipherment framework to improve out-of-domain machine translation.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 114, "end_pos": 133, "type": "TASK", "confidence": 0.7188734114170074}]}, {"text": "Compared with the state of the art algorithm, our approach is highly scalable and produces better results, which allows us to decipher ciphertext with billions of tokens and hundreds of thousands of word types with high accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 220, "end_pos": 228, "type": "METRIC", "confidence": 0.9889369010925293}]}, {"text": "We decipher a large amount of monolingual data to improve out-of-domain translation and achieve significant gains of up to 3.8 BLEU points.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 127, "end_pos": 131, "type": "METRIC", "confidence": 0.9992757439613342}]}], "introductionContent": [{"text": "Nowadays, state of the art statistical machine translation (SMT) systems are built using large amounts of bilingual parallel corpora.", "labels": [], "entities": [{"text": "statistical machine translation (SMT)", "start_pos": 27, "end_pos": 64, "type": "TASK", "confidence": 0.7884027808904648}]}, {"text": "Those corpora are used to estimate probabilities of word-to-word translation, word sequences rearrangement, and even syntactic transformation.", "labels": [], "entities": [{"text": "word-to-word translation", "start_pos": 52, "end_pos": 76, "type": "TASK", "confidence": 0.6986556947231293}, {"text": "word sequences rearrangement", "start_pos": 78, "end_pos": 106, "type": "TASK", "confidence": 0.640213261047999}, {"text": "syntactic transformation", "start_pos": 117, "end_pos": 141, "type": "TASK", "confidence": 0.7751016020774841}]}, {"text": "Unfortunately, as parallel corpora are expensive and not available for every domain, performance of SMT systems drops significantly when translating out-of-domain texts.", "labels": [], "entities": [{"text": "SMT", "start_pos": 100, "end_pos": 103, "type": "TASK", "confidence": 0.9916664958000183}]}, {"text": "In general, it is easier to obtain in-domain monolingual corpora.", "labels": [], "entities": []}, {"text": "Is it possible to use domain specific monolingual data to improve an MT system trained on parallel texts from a different domain?", "labels": [], "entities": [{"text": "MT", "start_pos": 69, "end_pos": 71, "type": "TASK", "confidence": 0.9872844219207764}]}, {"text": "Some researchers have attempted to do this by adding a domain specific dictionary (, or mining unseen words) using one of several translation lexicon induction techniques (.", "labels": [], "entities": [{"text": "translation lexicon induction", "start_pos": 130, "end_pos": 159, "type": "TASK", "confidence": 0.8873990178108215}]}, {"text": "However, a dictionary is not always available, and it is difficult to assign probabilities to a translation lexicon.", "labels": [], "entities": []}, {"text": "() have shown that one can use decipherment to learn a full translation model from non-parallel data.", "labels": [], "entities": []}, {"text": "Their approach is able to find translations, and assign probabilities to them.", "labels": [], "entities": []}, {"text": "But their work also has certain limitations.", "labels": [], "entities": []}, {"text": "First of all, the corpus they use to build the translation system has a very small vocabulary.", "labels": [], "entities": []}, {"text": "Secondly, although their algorithm is able to handle word substitution ciphers with limited vocabulary, its deciphering accuracy is low.", "labels": [], "entities": [{"text": "word substitution ciphers", "start_pos": 53, "end_pos": 78, "type": "TASK", "confidence": 0.7592297891775767}, {"text": "accuracy", "start_pos": 120, "end_pos": 128, "type": "METRIC", "confidence": 0.9920927882194519}]}, {"text": "The contributions of this work are: \u2022 We improve previous decipherment work by introducing a more efficient sampling algorithm.", "labels": [], "entities": []}, {"text": "In experiments, our new method improves deciphering accuracy from 82.5% to 88.1% on)'s domain specific data set.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 52, "end_pos": 60, "type": "METRIC", "confidence": 0.9921878576278687}]}, {"text": "Furthermore, we also solve a very large word substitution cipher built from the English Gigaword corpus and achieve 92.2% deciphering accuracy on news text.", "labels": [], "entities": [{"text": "word substitution cipher", "start_pos": 40, "end_pos": 64, "type": "TASK", "confidence": 0.7703900138537089}, {"text": "English Gigaword corpus", "start_pos": 80, "end_pos": 103, "type": "DATASET", "confidence": 0.8809022903442383}, {"text": "accuracy", "start_pos": 134, "end_pos": 142, "type": "METRIC", "confidence": 0.9972352385520935}]}, {"text": "\u2022 With the ability to handle a much larger vocabulary, we learn a domain specific translation table from a large amount of monolingual data and use the translation table to improve out-ofdomain machine translation.", "labels": [], "entities": [{"text": "out-ofdomain machine translation", "start_pos": 181, "end_pos": 213, "type": "TASK", "confidence": 0.6498800615469614}]}, {"text": "In experiments, we observe significant gains of up to 3.8 BLEU points.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 58, "end_pos": 62, "type": "METRIC", "confidence": 0.9990277290344238}]}, {"text": "Unlike previous works, the translation table we build from monolingual data do not only contain unseen words but also words seen in parallel data.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we evaluate our new sampling algorithm in two different experiments.", "labels": [], "entities": []}, {"text": "In the first experiment, we compare our method with (Ravi and Knight, 2011b) on their data set to prove correctness of our approach.", "labels": [], "entities": []}, {"text": "In the second experiment, we scale up to the whole English Gigaword corpus and achieve a much higher deciphering accuracy.", "labels": [], "entities": [{"text": "English Gigaword corpus", "start_pos": 51, "end_pos": 74, "type": "DATASET", "confidence": 0.9167311787605286}, {"text": "accuracy", "start_pos": 113, "end_pos": 121, "type": "METRIC", "confidence": 0.9940134882926941}]}], "tableCaptions": [{"text": " Table 7: 10 most frequent OOV words in the table learnt  from non-parallel EMEA corpus", "labels": [], "entities": []}]}