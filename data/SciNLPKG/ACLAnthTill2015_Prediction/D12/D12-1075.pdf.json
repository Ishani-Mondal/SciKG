{"title": [{"text": "Type-Supervised Hidden Markov Models for Part-of-Speech Tagging with Incomplete Tag Dictionaries", "labels": [], "entities": [{"text": "Part-of-Speech Tagging", "start_pos": 41, "end_pos": 63, "type": "TASK", "confidence": 0.6829193085432053}]}], "abstractContent": [{"text": "Past work on learning part-of-speech taggers from tag dictionaries and raw data has reported good results, but the assumptions made about those dictionaries are often unrealistic: due to historical precedents, they assume access to information about labels in the raw and test sets.", "labels": [], "entities": []}, {"text": "Here, we demonstrate ways to learn hidden Markov model taggers from incomplete tag dictionaries.", "labels": [], "entities": []}, {"text": "Taking the MIN-GREEDY algorithm (Ravi et al., 2010) as a starting point, we improve it with several intuitive heuristics.", "labels": [], "entities": []}, {"text": "We also define a simple HMM emission initialization that takes advantage of the tag dictionary and raw data to capture both the openness of a given tag and its estimated prevalence in the raw data.", "labels": [], "entities": [{"text": "HMM emission initialization", "start_pos": 24, "end_pos": 51, "type": "TASK", "confidence": 0.9297625223795573}]}, {"text": "Altogether, our augmentations produce improvements to performance over the original MIN-GREEDY algorithm for both English and Italian data.", "labels": [], "entities": []}], "introductionContent": [{"text": "Learning accurate part-of-speech (POS) taggers based on plentiful labeled training material is generally considered a solved problem.", "labels": [], "entities": [{"text": "part-of-speech (POS) taggers", "start_pos": 18, "end_pos": 46, "type": "TASK", "confidence": 0.6583217799663543}]}, {"text": "The best taggers obtain accuracies of over 97% for English newswire text in the Penn Treebank, which can be considered as an upper-bound that matches human performance on the same task.", "labels": [], "entities": [{"text": "accuracies", "start_pos": 24, "end_pos": 34, "type": "METRIC", "confidence": 0.99710613489151}, {"text": "Penn Treebank", "start_pos": 80, "end_pos": 93, "type": "DATASET", "confidence": 0.9962811768054962}]}, {"text": "However, as Manning notes, this story changes as soon as one is working with different assumptions and data, including having less training data, different kinds of training data, other languages, and other domains.", "labels": [], "entities": []}, {"text": "Such POS tagging work has been plentiful and includes efforts to induce POS tags without labels (; learn from POS-tag dictionaries (, incomplete dictionaries) and humanconstructed dictionaries (; bootstrap taggers fora language based on knowledge about other languages (, and creating supervised taggers for new, challenging domains such as Twitter ().", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 5, "end_pos": 16, "type": "TASK", "confidence": 0.8966702818870544}]}, {"text": "Here, we focus on learning from tag dictionaries.", "labels": [], "entities": []}, {"text": "This is often characterized as unsupervised or weakly supervised training.", "labels": [], "entities": []}, {"text": "We adopt the terminology type-supervised training to distinguish it from unsupervised training from raw text and supervised training from word tokens labeled with their partsof-speech.", "labels": [], "entities": []}, {"text": "Work on type-supervision goes back to, who introduced the still standard procedure of using a bigram Hidden Markov Model (HMM) trained via Expectation Maximization.", "labels": [], "entities": [{"text": "type-supervision", "start_pos": 8, "end_pos": 24, "type": "TASK", "confidence": 0.9391701817512512}, {"text": "Expectation Maximization", "start_pos": 139, "end_pos": 163, "type": "TASK", "confidence": 0.62872514128685}]}, {"text": "Early research appeared to show that learning from types works nearly as well as learning from tokens, with researchers in the 1990s obtaining accuracies up to 96% on English (e.g.).", "labels": [], "entities": [{"text": "accuracies", "start_pos": 143, "end_pos": 153, "type": "METRIC", "confidence": 0.9939032196998596}]}, {"text": "However, the tag dictionaries in these cases were obtained from labeled tokens.", "labels": [], "entities": []}, {"text": "While replicating earlier experiments, discovered that performance was highly dependent on cleaning tag dictionaries using statistics gleaned from the tokens.", "labels": [], "entities": []}, {"text": "This greatly simplifies the job of a typesupervised HMM: it no longer must entertain entries for uncommon word-tag pairs (or mistaken pairs due to annotation errors), which otherwise stand on equal footing with the common ones.", "labels": [], "entities": []}, {"text": "When the full, noisy tag dictionary was employed, Banko and Moore found accuracies dropped from 96% to 77%.", "labels": [], "entities": [{"text": "accuracies", "start_pos": 72, "end_pos": 82, "type": "METRIC", "confidence": 0.9987910389900208}]}, {"text": "Banko and Moore's observations spurred anew line of research that sought to improve performance in the face of full, noisy dictionaries; see for an overview.", "labels": [], "entities": []}, {"text": "The highest accuracy achieved to date under these assumptions is 91.6% (.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 12, "end_pos": 20, "type": "METRIC", "confidence": 0.9994513392448425}]}, {"text": "However, as is often noted (including by the authors themselves), many papers that work on learning taggers from tag dictionaries make unrealistic assumptions about the tag dictionaries they use as input.", "labels": [], "entities": [{"text": "learning taggers from tag dictionaries", "start_pos": 91, "end_pos": 129, "type": "TASK", "confidence": 0.7028165102005005}]}, {"text": "For example, tag dictionaries are typically constructed with every token-tag pair in the data, including those that appear only in the test set.", "labels": [], "entities": []}, {"text": "This means that the evaluation of these taggers does not measure how they perform on sentences that contain unseen words or unseen word-tag pairs, a likely occurrence in real use of a trained tagger.", "labels": [], "entities": []}, {"text": "We show that it is possible to achieve good tagging accuracy using a noisy and incomplete tag dictionary that has no access to the tags of the raw and test data and no access to the tag frequency information of the labeled training data from which the dictionary is drawn.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 52, "end_pos": 60, "type": "METRIC", "confidence": 0.9339475035667419}]}, {"text": "We build on model minimization approach, which reduces dictionary noise by greedily approximating the minimum set of tag bigrams needed to cover the raw data and exploits that information as a constraint on the initialization of the model before running EM.", "labels": [], "entities": [{"text": "model minimization", "start_pos": 12, "end_pos": 30, "type": "TASK", "confidence": 0.7477622628211975}]}, {"text": "We extend their method in four distinct ways.", "labels": [], "entities": []}, {"text": "1. Enable the algorithm to be used with incomplete dictionaries by exploiting the type-based information provided by the tag dictionary and raw text to initialize EM, and by training a standard supervised HMM on the output of EM.", "labels": [], "entities": []}, {"text": "2. Improve the greedy procedure to find a better minimized set of tag-tag bigrams.", "labels": [], "entities": []}, {"text": "3. Modify the method to return only the set of bigrams required to tag sentences instead of keeping all bigrams chosen by minimization.", "labels": [], "entities": []}, {"text": "4. Exploit the paths found during minimization as a direct initialization for EM.", "labels": [], "entities": []}, {"text": "Together, these improvements make it possible to use model minimization in a realistic context, and obtain higher performance: on English, results go from 63.5% fora vanilla HMM to 82.1% for an HMM that uses strategies to deal with unknowns, then to 85.0% with Ravi and Knight's minimization and finally to 88.5% with our enhancements.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluate on the Penn Treebank (.", "labels": [], "entities": [{"text": "Penn Treebank", "start_pos": 19, "end_pos": 32, "type": "DATASET", "confidence": 0.9954109787940979}]}, {"text": "In all cases we use the first 47,996 tokens of section 16 as our raw data, sections 19-21 as our development set, and perform the final evaluation on sections 22-24.", "labels": [], "entities": []}, {"text": "We evaluate two differently sized tag dictionaries.", "labels": [], "entities": []}, {"text": "The first is extracted directly from sections 00-15 (751,059 tokens) and the second from sections 00-07 (379,908 tokens).", "labels": [], "entities": []}, {"text": "The former contains 39,087 word types, 45,331 word/tag entries, a per-type ambiguity of 1.16 and yields a per-token ambiguity of 2.21 on the raw corpus (treating unknown words as having all 45 possible tags).", "labels": [], "entities": []}, {"text": "The latter contains 26,652 word types, 30,662 word/tag entries, a pertype ambiguity of 1.15 and yields a per-token ambiguity of 2.03 on the raw corpus.", "labels": [], "entities": []}, {"text": "In both cases, every word/tag pair found in the relevant sections was used in the tag dictionary: no pruning was performed.", "labels": [], "entities": []}, {"text": "As a second evaluation, we use the TUT corpus ().", "labels": [], "entities": [{"text": "TUT corpus", "start_pos": 35, "end_pos": 45, "type": "DATASET", "confidence": 0.7307370007038116}]}, {"text": "To verify that our approach is language-independent without the need for specific tuning, we executed our tests on the Italian data without any trial runs, parameter modifications, or other changes.", "labels": [], "entities": [{"text": "Italian data", "start_pos": 119, "end_pos": 131, "type": "DATASET", "confidence": 0.8562642633914948}]}, {"text": "We divided the TUT data, taking the first half of each of the five sections as input to the tag dictionary, the next quarter as raw data, and the last quarter as test data.", "labels": [], "entities": [{"text": "TUT data", "start_pos": 15, "end_pos": 23, "type": "DATASET", "confidence": 0.7218808978796005}]}, {"text": "All together, the tag dictionary was constructed from 41,000 tokens consisting of 7,814 word types, 8,370 word/tag pairs, per-type ambiguity of 1.07 and a per-token ambiguity of 1.41 on the raw data.", "labels": [], "entities": []}, {"text": "The raw data consisted of 18,574 tokens and the test contained 18,763 tokens.", "labels": [], "entities": []}, {"text": "Results We ran eleven experiments for each data set with results shown in.", "labels": [], "entities": []}, {"text": "All scores are reported as the percentage of tokens for which the correct tag was assigned.", "labels": [], "entities": []}, {"text": "Accuracy is shown as PTB: Tagging accuracy using PTB sections 00-07 and TUT to build the tag dictionary.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 0, "end_pos": 8, "type": "METRIC", "confidence": 0.9726846218109131}, {"text": "PTB", "start_pos": 21, "end_pos": 24, "type": "METRIC", "confidence": 0.660292387008667}, {"text": "accuracy", "start_pos": 34, "end_pos": 42, "type": "METRIC", "confidence": 0.9898876547813416}, {"text": "PTB", "start_pos": 49, "end_pos": 52, "type": "DATASET", "confidence": 0.7894048094749451}, {"text": "TUT", "start_pos": 72, "end_pos": 75, "type": "METRIC", "confidence": 0.9896023869514465}]}, {"text": "Known word types are those appearing in the tag dictionary.", "labels": [], "entities": []}, {"text": "Scenario numbers correspond to the Total (all word types), Known (word types found in the tag dictionary), and Unknown (word types not found in the tag dictionary).", "labels": [], "entities": []}, {"text": "Experiments 1-3 evaluate our smoothing techniques applied directly to the task of type-supervised HMM training with EM, without MIN-GREEDY.", "labels": [], "entities": []}, {"text": "The basic HMM consistently beats the baseline random tagger, the auto-supervision technique makes an enormous improvement for both known and unknown words, and the the emission initialization yields a sizable improvement for unknown words.", "labels": [], "entities": []}, {"text": "Experiments 4-6 evaluated our reimplementation of MIN-GREEDY.", "labels": [], "entities": [{"text": "MIN-GREEDY", "start_pos": 50, "end_pos": 60, "type": "DATASET", "confidence": 0.6116300225257874}]}, {"text": "We start with the most basic level of smoothing needed to work in a type-supervised scenario.", "labels": [], "entities": []}, {"text": "For the smaller PTB tag dictionary and the TUT data, MIN-GREEDY actually has lower performance than the HMM alone.", "labels": [], "entities": [{"text": "PTB tag dictionary", "start_pos": 16, "end_pos": 34, "type": "DATASET", "confidence": 0.7653407454490662}, {"text": "TUT data", "start_pos": 43, "end_pos": 51, "type": "DATASET", "confidence": 0.8775005638599396}]}, {"text": "This indicates that if the tag dictionary has a low degree of ambiguity, then MIN-GREEDY can make the situation worse.", "labels": [], "entities": [{"text": "MIN-GREEDY", "start_pos": 78, "end_pos": 88, "type": "METRIC", "confidence": 0.8555362224578857}]}, {"text": "However, with our smoothing techniques, we regain similar improvements as with the HMM.", "labels": [], "entities": []}, {"text": "Finally we performed experiments evaluating combinations of our improvements to MIN-GREEDY.", "labels": [], "entities": [{"text": "MIN-GREEDY", "start_pos": 80, "end_pos": 90, "type": "DATASET", "confidence": 0.5560168623924255}]}, {"text": "Scenarios 7-9 show each improvement taken in turn.", "labels": [], "entities": []}, {"text": "Scenario 10 shows the results for using all three improvements.", "labels": [], "entities": []}, {"text": "For the English data, the best results are found when all the improvements are used.", "labels": [], "entities": [{"text": "English data", "start_pos": 8, "end_pos": 20, "type": "DATASET", "confidence": 0.8260934352874756}]}, {"text": "When taken individually, the bigram choice heuristic and HMM initialization from minimization output each consistently outperform the improved-MIN-GREEDY baseline on English.", "labels": [], "entities": []}, {"text": "However, restricting the tag bigrams to that in the minimizationtagged output causes problems in the smaller PTB scenario, presumably falling to a local maximum like MIN-GREEDY that the other improvements are able to help the algorithm avoid.", "labels": [], "entities": []}, {"text": "Though the accuracy improvements are less than for English, the Italian results show that our MIN-GREEDY enhancements make an appreciable difference fora language and dataset for which the approaches considered were run sight unseen.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 11, "end_pos": 19, "type": "METRIC", "confidence": 0.9995169639587402}, {"text": "MIN-GREEDY", "start_pos": 94, "end_pos": 104, "type": "METRIC", "confidence": 0.7021835446357727}]}, {"text": "Error analysis One of the primary goals of model minimization is to automatically eliminate lowprobability entries from the tag dictionary that might confuse the EM algorithm (.", "labels": [], "entities": [{"text": "Error analysis", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.704354390501976}, {"text": "model minimization", "start_pos": 43, "end_pos": 61, "type": "TASK", "confidence": 0.790086567401886}]}, {"text": "In order to see how well our techniques are able to identify and eliminate these unlikely word/tag pairs, we analyzed the tagging errors from each experiment.", "labels": [], "entities": []}, {"text": "In doing so, we discovered that the two of the most problematic words for the EM algorithm are \"a\" and \"in\".", "labels": [], "entities": []}, {"text": "We ran further experiments explore what was happening with those words.", "labels": [], "entities": []}, {"text": "The results, using PTB sections 00-07 are shown in.", "labels": [], "entities": [{"text": "PTB sections 00-07", "start_pos": 19, "end_pos": 37, "type": "DATASET", "confidence": 0.9326604803403219}]}, {"text": "In PTB sections 00-07 the word \"a\" appears 7630 times and with 7 different tags.", "labels": [], "entities": [{"text": "PTB sections 00-07", "start_pos": 3, "end_pos": 21, "type": "DATASET", "confidence": 0.9377568165461222}]}, {"text": "This includes 7621 occurrences with tag DT, 3 with tag SYM (symbol), and 1 time with LS (list item marker).", "labels": [], "entities": []}, {"text": "As such, we would want the HMM to lean heavily toward tag DT when tagging the token \"a\".", "labels": [], "entities": [{"text": "tag DT", "start_pos": 54, "end_pos": 60, "type": "METRIC", "confidence": 0.7797127962112427}]}, {"text": "Unfortunately, the rare tags confuse the EM procedure and end up with dis-: Number of times, for the words \"a\" and \"in\", the tagger trained by the particular scenario selected the given tag.", "labels": [], "entities": [{"text": "EM", "start_pos": 41, "end_pos": 43, "type": "TASK", "confidence": 0.9434229135513306}]}, {"text": "Experiments used PTB sections 00-07 for the initial tag dictionary.", "labels": [], "entities": [{"text": "PTB sections 00-07", "start_pos": 17, "end_pos": 35, "type": "DATASET", "confidence": 0.8871332009633383}]}, {"text": "Scenario numbers correspond to proportionately high probabilities.", "labels": [], "entities": []}, {"text": "Our experiment training an HMM without minimization (scenario 3) resulted in 1531 \"a\" tokens being tagged LS, 731 as SYM, and only 32 tagged as DT.", "labels": [], "entities": []}, {"text": "The situation is similar with the word \"in\", which appears 6155 times with 5 different tags in the 8 sections.", "labels": [], "entities": []}, {"text": "Of these, 6073 occurrences are tagged IN (preposition), 63 are RP (particle), and 1 is FW (foreign word).", "labels": [], "entities": [{"text": "IN", "start_pos": 38, "end_pos": 40, "type": "METRIC", "confidence": 0.9824782013893127}, {"text": "RP", "start_pos": 63, "end_pos": 65, "type": "METRIC", "confidence": 0.965829610824585}, {"text": "FW", "start_pos": 87, "end_pos": 89, "type": "METRIC", "confidence": 0.9633042216300964}]}, {"text": "Again, EM without minimization is confused by the rare tokens, assigning FW 1922 times and IN 12 times.", "labels": [], "entities": [{"text": "FW 1922 times", "start_pos": 73, "end_pos": 86, "type": "METRIC", "confidence": 0.868939220905304}, {"text": "IN", "start_pos": 91, "end_pos": 93, "type": "METRIC", "confidence": 0.9885276556015015}]}, {"text": "The minimization procedure attempts to overcome this problem by removing unlikely tags from the tag dictionary automatically.", "labels": [], "entities": []}, {"text": "As is show in, MIN-GREEDY without our enhancements is able to reject the problematic LS as a tag for \"a\", but unable to do so for SYM, resulting in 2356 tokens tagged SYM and only 4 tagged DT.", "labels": [], "entities": []}, {"text": "Similarly, MIN-GREEDY is unable to reject FW as a tag for \"in\".", "labels": [], "entities": [{"text": "FW", "start_pos": 42, "end_pos": 44, "type": "METRIC", "confidence": 0.6427797079086304}]}, {"text": "Our enhancements to MIN-GREEDY improve the situation.", "labels": [], "entities": [{"text": "MIN-GREEDY", "start_pos": 20, "end_pos": 30, "type": "METRIC", "confidence": 0.6135343313217163}]}, {"text": "More careful choosing of bigrams during minimization results in the avoidance of LS and FW (but not SYM) for \"a\" as well as FW and RP for \"in\".", "labels": [], "entities": [{"text": "minimization", "start_pos": 40, "end_pos": 52, "type": "TASK", "confidence": 0.9721697568893433}, {"text": "avoidance", "start_pos": 68, "end_pos": 77, "type": "METRIC", "confidence": 0.9658312797546387}, {"text": "FW", "start_pos": 88, "end_pos": 90, "type": "METRIC", "confidence": 0.9482136368751526}, {"text": "FW", "start_pos": 124, "end_pos": 126, "type": "METRIC", "confidence": 0.9256314635276794}]}, {"text": "Restricting the tag bigrams output from MIN-GREEDY to just those on tag paths avoids LS and FW for \"a\" and FW for \"in\".", "labels": [], "entities": [{"text": "LS", "start_pos": 85, "end_pos": 87, "type": "METRIC", "confidence": 0.9697015881538391}, {"text": "FW", "start_pos": 92, "end_pos": 94, "type": "METRIC", "confidence": 0.9169024229049683}, {"text": "FW", "start_pos": 107, "end_pos": 109, "type": "METRIC", "confidence": 0.9777447581291199}]}, {"text": "Finally, using the tagged sentences from MIN-GREEDY as noisy supervision for EM initialization eliminates all rare tags, as does the use of all three enhancements together.", "labels": [], "entities": [{"text": "EM initialization", "start_pos": 77, "end_pos": 94, "type": "TASK", "confidence": 0.9298918545246124}]}], "tableCaptions": [{"text": " Table 1: English tagging accuracy using PTB sections 00-15 to build the tag dictionary. Known word types  are those appearing in the tag dictionary.", "labels": [], "entities": [{"text": "English tagging", "start_pos": 10, "end_pos": 25, "type": "TASK", "confidence": 0.6492083668708801}, {"text": "accuracy", "start_pos": 26, "end_pos": 34, "type": "METRIC", "confidence": 0.9717357754707336}, {"text": "PTB sections 00-15", "start_pos": 41, "end_pos": 59, "type": "DATASET", "confidence": 0.8909733692804972}]}, {"text": " Table 2: Tagging accuracy using PTB sections 00-07 and TUT to build the tag dictionary. Known word  types are those appearing in the tag dictionary. Scenario numbers correspond to", "labels": [], "entities": [{"text": "accuracy", "start_pos": 18, "end_pos": 26, "type": "METRIC", "confidence": 0.9742223620414734}, {"text": "PTB", "start_pos": 33, "end_pos": 36, "type": "DATASET", "confidence": 0.7299182415008545}, {"text": "TUT", "start_pos": 56, "end_pos": 59, "type": "METRIC", "confidence": 0.9805457592010498}]}, {"text": " Table 3: Number of times, for the words \"a\" and  \"in\", the tagger trained by the particular scenario se- lected the given tag. Experiments used PTB sections  00-07 for the initial tag dictionary. Scenario num- bers correspond to", "labels": [], "entities": [{"text": "PTB sections  00-07", "start_pos": 145, "end_pos": 164, "type": "DATASET", "confidence": 0.8887543280919393}]}]}