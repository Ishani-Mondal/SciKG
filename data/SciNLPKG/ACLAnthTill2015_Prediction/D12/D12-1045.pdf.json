{"title": [{"text": "Joint Entity and Event Coreference Resolution across Documents", "labels": [], "entities": [{"text": "Event Coreference Resolution across Documents", "start_pos": 17, "end_pos": 62, "type": "TASK", "confidence": 0.7868562638759613}]}], "abstractContent": [{"text": "We introduce a novel coreference resolution system that models entities and events jointly.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 21, "end_pos": 43, "type": "TASK", "confidence": 0.8918387591838837}]}, {"text": "Our iterative method cautiously constructs clusters of entity and event mentions using linear regression to model cluster merge operations.", "labels": [], "entities": []}, {"text": "As clusters are built, information flows between entity and event clusters through features that model semantic role dependencies.", "labels": [], "entities": []}, {"text": "Our system handles nominal and verbal events as well as entities, and our joint formulation allows information from event coreference to help entity coreference, and vice versa.", "labels": [], "entities": []}, {"text": "Ina cross-document domain with comparable documents , joint coreference resolution performs significantly better (over 3 CoNLL F1 points) than two strong baselines that resolve entities and events separately.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 60, "end_pos": 82, "type": "TASK", "confidence": 0.7468855977058411}, {"text": "CoNLL F1", "start_pos": 121, "end_pos": 129, "type": "METRIC", "confidence": 0.7792516648769379}]}], "introductionContent": [{"text": "Most coreference resolution systems focus on entities and tacitly assume a correspondence between entities and noun phrases (NPs).", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 5, "end_pos": 27, "type": "TASK", "confidence": 0.9497166872024536}]}, {"text": "Focusing on NPs is away to restrict the challenging problem of coreference resolution, but misses coreference relations like the one between hanged and his suicide in (1), and between placed and put in (2).", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 63, "end_pos": 85, "type": "TASK", "confidence": 0.9702361524105072}]}, {"text": "As (1c) shows, NPs can also refer to events, and so corefer with phrases other than NPs.", "labels": [], "entities": []}, {"text": "By being anchored in spatio-temporal dimensions, events represent the most frequent referent of verbal elements.", "labels": [], "entities": []}, {"text": "In addition to time and location, events are characterized by their participants or arguments, which often correspond with discourse entities.", "labels": [], "entities": []}, {"text": "This two-way feedback between events and their arguments (or entities) is the core of our approach.", "labels": [], "entities": []}, {"text": "Since arguments play a key role in describing an event, knowing that two arguments corefer is useful for finding coreference relations between events, and knowing that two events corefer is useful for finding coreference relations between entities.", "labels": [], "entities": []}, {"text": "In (1), the coreference relation between One of the key suspected Mafia bosses arrested yesterday and Lo Presti can be found by knowing that their predicates (i.e., has hanged and had hanged) corefer.", "labels": [], "entities": []}, {"text": "On the other hand, the coreference relations between the arguments Saints and Bush in (2) helps to determine the coreference relation between their predicates placed and put.", "labels": [], "entities": []}, {"text": "In this paper, we take a holistic approach to coreference.", "labels": [], "entities": [{"text": "coreference", "start_pos": 46, "end_pos": 57, "type": "TASK", "confidence": 0.95859694480896}]}, {"text": "We annotate a corpus with cross-document coreference relations for nominal and verbal mentions.", "labels": [], "entities": []}, {"text": "We focus on both intra and inter-document coreference because this scenario is at the same time more challenging and more relevant to real-world applications such as news aggregation.", "labels": [], "entities": [{"text": "inter-document coreference", "start_pos": 27, "end_pos": 53, "type": "TASK", "confidence": 0.6745103597640991}]}, {"text": "We use this corpus to train a model that jointly addresses references to both entities and events across documents.", "labels": [], "entities": []}, {"text": "The contributions of this work are the following: \u2022 We introduce a novel approach for entity and event coreference resolution.", "labels": [], "entities": [{"text": "entity and event coreference resolution", "start_pos": 86, "end_pos": 125, "type": "TASK", "confidence": 0.6641212940216065}]}, {"text": "At the core of our approach is an iterative algorithm that cautiously constructs clusters of entity and event mentions using linear regression to model cluster merge operations.", "labels": [], "entities": []}, {"text": "Importantly, our model allows information to flow between clusters of both types through features that model context using semantic role dependencies.", "labels": [], "entities": []}, {"text": "\u2022 We annotate and release anew corpus with coreference relations between both entities and events across documents.", "labels": [], "entities": []}, {"text": "The relations annotated are both intra and inter-document, which more accurately models real-world scenarios.", "labels": [], "entities": []}, {"text": "\u2022 We evaluate our cross-document coreference resolution system on this corpus and show that our joint approach significantly outperforms two strong baselines that resolve entities and events separately.", "labels": [], "entities": [{"text": "cross-document coreference resolution", "start_pos": 18, "end_pos": 55, "type": "TASK", "confidence": 0.666702389717102}]}], "datasetContent": [{"text": "We use five coreference evaluation metrics widely used in the literature: MUC (Vilain et al., 1995) Link-based metric which measures how many predicted and gold clusters need to be merged to cover the gold and predicted clusters, respectively.", "labels": [], "entities": [{"text": "MUC", "start_pos": 74, "end_pos": 77, "type": "METRIC", "confidence": 0.9556052684783936}]}, {"text": "B 3 (Bagga and Baldwin, 1998) Mention-based metric which measures the proportion of overlap between predicted and gold clusters fora given mention.", "labels": [], "entities": []}, {"text": "CEAF () Entity-based metric that, unlike B 3 , enforces a one-to-one alignment between gold and predicted clusters.", "labels": [], "entities": [{"text": "CEAF", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.7014985680580139}]}, {"text": "We employ the entity-based version of CEAF.", "labels": [], "entities": [{"text": "CEAF", "start_pos": 38, "end_pos": 42, "type": "DATASET", "confidence": 0.8452654480934143}]}, {"text": "BLANC (Recasens and Hovy, 2011) Metric based on the Rand index) that considers both coreference and non-coreference links to address the imbalance between singleton and coreferent mentions.", "labels": [], "entities": [{"text": "BLANC", "start_pos": 0, "end_pos": 5, "type": "METRIC", "confidence": 0.9267619252204895}]}, {"text": "CoNLL F1 Average of MUC, B 3 , and CEAF-\u03c6 4 . This was the official metric in the CoNLL-2011 shared task (Pradhan et al., 2011).", "labels": [], "entities": [{"text": "CoNLL", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.8251297473907471}, {"text": "F1 Average", "start_pos": 6, "end_pos": 16, "type": "METRIC", "confidence": 0.887102335691452}, {"text": "MUC", "start_pos": 20, "end_pos": 23, "type": "METRIC", "confidence": 0.8104225993156433}, {"text": "B 3", "start_pos": 25, "end_pos": 28, "type": "METRIC", "confidence": 0.9365680515766144}, {"text": "CEAF-\u03c6", "start_pos": 35, "end_pos": 41, "type": "METRIC", "confidence": 0.7329310774803162}]}, {"text": "We followed the CoNLL-2011 evaluation methodology, that is, we removed all singleton clusters, and apposition/copular relations before scoring.", "labels": [], "entities": []}, {"text": "We evaluated the systems on three different settings: only on entity clusters, only on event clusters, and on the complete task, i.e., both entities and events.", "labels": [], "entities": []}, {"text": "Note that the gold corpus separates clusters into entity and event clusters (see), but our system does not make this distinction at runtime.", "labels": [], "entities": []}, {"text": "In order to compute the entity-only and event-only scores in, we implemented the following procedure: (a) when scoring entity clusters, we removed all mentions that were found to be coreferent with at least one gold event mention and not coreferent with any gold entity mentions; and (b) we performed the opposite action when scoring event clusters.", "labels": [], "entities": []}, {"text": "This procedure is necessary because our mention identification component is not perfect, i.e., it generates mentions that do not exist in the gold annotation.", "labels": [], "entities": [{"text": "mention identification", "start_pos": 40, "end_pos": 62, "type": "TASK", "confidence": 0.6501840651035309}]}, {"text": "Furthermore, this procedure is conservative with respect to the clustering errors of our system, e.g., all spurious mentions that our system includes in a cluster with a gold entity mention are considered for the entity score, regardless of their gold type (event or entity).", "labels": [], "entities": []}, {"text": "compares the performance of our system against two strong baselines that resolve entities and events separately.", "labels": [], "entities": []}, {"text": "Baseline 1 uses a modified Stanford coreference resolution system after our document clustering and mention identification steps.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 36, "end_pos": 58, "type": "TASK", "confidence": 0.772857666015625}, {"text": "mention identification", "start_pos": 100, "end_pos": 122, "type": "TASK", "confidence": 0.6638700217008591}]}, {"text": "Because the original Stanford system implements only entity coreference, we extended it with an extra sieve that implements lemma matching for events.", "labels": [], "entities": [{"text": "entity coreference", "start_pos": 53, "end_pos": 71, "type": "TASK", "confidence": 0.6769138276576996}]}, {"text": "This additional sieve merges two verbal clusters (i.e., clusters that contain at least one verbal mention) or a verbal and a nominal cluster when at least two lemmas of mention head words are the same between clusters, e.g., helped and the help.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 4: Performance of the two baselines and our model. We report scores for entity clusters, event clusters and the  complete task using five metrics.", "labels": [], "entities": []}, {"text": " Table 5. The table indicates that six out  of the ten features serve the purpose of passing infor-", "labels": [], "entities": [{"text": "passing infor", "start_pos": 85, "end_pos": 98, "type": "TASK", "confidence": 0.728431224822998}]}, {"text": " Table 5: Top five features with the highest weights.", "labels": [], "entities": []}]}