{"title": [{"text": "Source Language Adaptation for Resource-Poor Machine Translation", "labels": [], "entities": [{"text": "Source Language Adaptation", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.5881615479787191}, {"text": "Machine Translation", "start_pos": 45, "end_pos": 64, "type": "TASK", "confidence": 0.7060894817113876}]}], "abstractContent": [{"text": "We propose a novel, language-independent approach for improving machine translation from a resource-poor language to X by adapting a large bi-text fora related resource-rich language and X (the same target language).", "labels": [], "entities": [{"text": "machine translation", "start_pos": 64, "end_pos": 83, "type": "TASK", "confidence": 0.7927239239215851}]}, {"text": "We assume a small bi-text for the resource-poor language to X pair, which we use to learn word-level and phrase-level paraphrases and cross-lingual morphological variants between the resource-rich and the resource-poor language; we then adapt the former to get closer to the latter.", "labels": [], "entities": []}, {"text": "Our experiments for Indonesian/Malay-English translation show that using the large adapted resource-rich bi-text yields 6.7 BLEU points of improvement over the unadapted one and 2.6 BLEU points over the original small bi-text.", "labels": [], "entities": [{"text": "Indonesian/Malay-English translation", "start_pos": 20, "end_pos": 56, "type": "TASK", "confidence": 0.595566987991333}, {"text": "BLEU", "start_pos": 124, "end_pos": 128, "type": "METRIC", "confidence": 0.9992057681083679}, {"text": "BLEU", "start_pos": 182, "end_pos": 186, "type": "METRIC", "confidence": 0.99802565574646}]}, {"text": "Moreover, combining the small bi-text with the adapted bi-text outperforms the corresponding combinations with the unadapted bi-text by 1.5-3 BLEU points.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 142, "end_pos": 146, "type": "METRIC", "confidence": 0.9988455772399902}]}, {"text": "We also demonstrate applicability to other languages and domains.", "labels": [], "entities": []}], "introductionContent": [{"text": "Statistical machine translation (SMT) systems learn how to translate from large sentence-aligned bilingual corpora of human-generated translations, called bi-texts.", "labels": [], "entities": [{"text": "Statistical machine translation (SMT)", "start_pos": 0, "end_pos": 37, "type": "TASK", "confidence": 0.790961946050326}]}, {"text": "Unfortunately, collecting sufficiently large, high-quality bi-texts is hard, and thus most of the 6,500+ world languages remain resource-poor.", "labels": [], "entities": []}, {"text": "Fortunately, many of these resource-poor languages are related to some resource-rich language, with whom they overlap in vocabulary and share cognates, which offers opportunities for bi-text reuse.", "labels": [], "entities": []}, {"text": "Example pairs of such resource rich-poor languages include Spanish-Catalan, Finnish-Estonian, Swedish-Norwegian, Russian-Ukrainian, IrishGaelic Scottish, Standard German-Swiss German, Modern Standard Arabic-Dialectical Arabic (e.g., Gulf, Egyptian), Turkish-Azerbaijani, etc.", "labels": [], "entities": [{"text": "IrishGaelic Scottish", "start_pos": 132, "end_pos": 152, "type": "DATASET", "confidence": 0.9595474004745483}]}, {"text": "Previous work has already demonstrated the benefits of using a bi-text fora related resource-rich language to X (e.g., X=English) to improve machine translation from a resource-poor language to X ().", "labels": [], "entities": [{"text": "machine translation", "start_pos": 141, "end_pos": 160, "type": "TASK", "confidence": 0.7752540707588196}]}, {"text": "Here we take a different, orthogonal approach: we adapt the resource-rich language to get closer to the resource-poor one.", "labels": [], "entities": []}, {"text": "We assume a small bi-text for the resource-poor language, which we use to learn word-level and phrase-level paraphrases and cross-lingual morphological variants between the two languages.", "labels": [], "entities": []}, {"text": "Assuming translation into the same target language X, we adapt (the source side of) a large training bi-text fora related resource-rich language and X. Training on the adapted large bi-text yields very significant improvements in translation quality compared to both (a) training on the unadapted version, and (b) training on the small bi-text for the resourcepoor language.", "labels": [], "entities": []}, {"text": "We further achieve very sizable improvements when combining the small bi-text with the large adapted bi-text, compared to combining the former with the unadapted bi-text.", "labels": [], "entities": []}, {"text": "While we focus on adapting Malay to look like Indonesian in our experiments, we also demonstrate the applicability of our approach to another language pair, Bulgarian-Macedonian, which is also from a different domain.", "labels": [], "entities": []}], "datasetContent": [{"text": "We run two kinds of experiments: (a) isolated, where we train on the synthetic \"Indonesian\"-English bi-text only, and (b) combined, where we combine it with the Indonesian-English bi-text.", "labels": [], "entities": []}, {"text": "In our experiments, we use the following datasets, normally required for Indonesian-English SMT: \u2022 Indonesian-English train bi-text We also use a Malay-English set (to be turned into \"Indonesian\"-English), and monolingual Indonesian text (for decoding the confusion network): 1,132,082 sentences; 20,452,064 Indonesian tokens.", "labels": [], "entities": [{"text": "SMT", "start_pos": 92, "end_pos": 95, "type": "TASK", "confidence": 0.7580717206001282}]}, {"text": "When morphological variants are further added, the unigram precision improves by almost 1% absolute over CN:pivot \u2032 . This shows the importance of morphology for overcoming the limitations of the small Indonesian vocabulary of the IN2EN bi-text.", "labels": [], "entities": [{"text": "precision", "start_pos": 59, "end_pos": 68, "type": "METRIC", "confidence": 0.9215567708015442}]}, {"text": "The lower part of shows that phrase-level paraphrasing performs a bit better.", "labels": [], "entities": [{"text": "phrase-level paraphrasing", "start_pos": 29, "end_pos": 54, "type": "TASK", "confidence": 0.7700457274913788}]}, {"text": "This confirms the importance of modeling context for closely-related languages like Malay and Indonesian, which are rich in false friends and partial cognates.", "labels": [], "entities": []}, {"text": "We further see that using more scores in the phrase table is better.", "labels": [], "entities": []}, {"text": "Extending the Indonesian vocabulary with cross-lingual morphological variants is still helpful, though not as much as at the word-level.", "labels": [], "entities": []}, {"text": "Finally, the combination of the output of the best PPT and the best CN systems using MEMT) yields even further improvements, which shows that the two kinds of paraphrases are complementary.", "labels": [], "entities": []}, {"text": "The best overall BLEU score for our isolated experiments is 21.24, which is better than the results for all five baselines in, including the three bi-text combination baselines, which only achieve up to 20.10 BLEU.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 17, "end_pos": 27, "type": "METRIC", "confidence": 0.978203535079956}, {"text": "BLEU", "start_pos": 209, "end_pos": 213, "type": "METRIC", "confidence": 0.9967453479766846}]}, {"text": "shows the performance of the three bitext combination strategies (see Section 4.3 for additional details) when applied to combine IN2EN (1) with the original ML2EN and (2) with various adapted versions of it.", "labels": [], "entities": [{"text": "IN2EN", "start_pos": 130, "end_pos": 135, "type": "METRIC", "confidence": 0.929533839225769}]}, {"text": "We can see that for the word-level paraphrasing experiments (CN:*), all combinations except for CN:pivot perform significantly better than their corresponding baselines, but the improvements are most sizeable for the simple concatenation.", "labels": [], "entities": []}, {"text": "Note that while there is a difference of 0.31 BLEU points between the balanced concatenation and the sophisticated combination for the original ML2EN, they differ little for the adapted versions.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 46, "end_pos": 50, "type": "METRIC", "confidence": 0.9992116689682007}, {"text": "ML2EN", "start_pos": 144, "end_pos": 149, "type": "DATASET", "confidence": 0.8644827008247375}]}, {"text": "This is probably due to the sophisticated combination assuming that the second bi-text is worse than the first one, which is not really the case for the adapted versions: as shows, they all outperform IN2EN.", "labels": [], "entities": [{"text": "IN2EN", "start_pos": 201, "end_pos": 206, "type": "DATASET", "confidence": 0.9222542643547058}]}, {"text": "Overall, phrase-level paraphrasing performs a bit better than word-level paraphrasing, and system combination with MEMT improves even further.", "labels": [], "entities": [{"text": "phrase-level paraphrasing", "start_pos": 9, "end_pos": 34, "type": "TASK", "confidence": 0.7479414343833923}]}, {"text": "This is consistent with the isolated experiments.", "labels": [], "entities": []}, {"text": "The results for the baseline systems are shown in Table 2.", "labels": [], "entities": []}, {"text": "We can see that training on ML2EN instead of IN2EN yields over 4 points absolute drop in BLEU () score, even though ML2EN is about 10 times larger than IN2EN and both bi-texts are from the same domain.", "labels": [], "entities": [{"text": "BLEU () score", "start_pos": 89, "end_pos": 102, "type": "METRIC", "confidence": 0.9593952298164368}]}, {"text": "This confirms the existence of important differences between Malay and Indonesian.", "labels": [], "entities": []}, {"text": "While simple concatenation does not help, balanced concatenation with repetitions improves by 1.12 BLEU points over IN2EN, which shows the importance of giving IN2EN a proper weight in the combined bi-text.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 99, "end_pos": 103, "type": "METRIC", "confidence": 0.9995142221450806}, {"text": "IN2EN", "start_pos": 116, "end_pos": 121, "type": "METRIC", "confidence": 0.8036609292030334}]}, {"text": "This is further reconfirmed by the sophisticated phrase table combination, which yields an additional absolute gain of 0.31 BLEU points.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 124, "end_pos": 128, "type": "METRIC", "confidence": 0.9990847110748291}]}, {"text": "shows the results for the isolated experiments.", "labels": [], "entities": []}, {"text": "We can see that word-level paraphrasing improves by up to 5.56 and 1.39 BLEU points over the two baselines (both statistically significant).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 72, "end_pos": 76, "type": "METRIC", "confidence": 0.9984408020973206}]}, {"text": "Compared to ML2EN, CN:pivot yields an absolute improvement of 4.41 BLEU points, CN:pivot \u2032 adds another 0.59, and CN:pivot \u2032 +morph adds 0.56 more.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 67, "end_pos": 71, "type": "METRIC", "confidence": 0.9984493255615234}]}, {"text": "The scores for TER (v. 0.7.25) and METEOR (v. 1.3) are on par with those for BLEU (NIST v. 13).", "labels": [], "entities": [{"text": "TER", "start_pos": 15, "end_pos": 18, "type": "METRIC", "confidence": 0.9956322312355042}, {"text": "METEOR (v. 1.3)", "start_pos": 35, "end_pos": 50, "type": "METRIC", "confidence": 0.8945630550384521}, {"text": "BLEU", "start_pos": 77, "end_pos": 81, "type": "METRIC", "confidence": 0.995850682258606}, {"text": "NIST v. 13", "start_pos": 83, "end_pos": 93, "type": "DATASET", "confidence": 0.8085272908210754}]}, {"text": "further shows that the optimal parameters for the word-level SMT systems (CN:*) involve a very low probability cutoff, and a high number of n-best sentences.", "labels": [], "entities": [{"text": "SMT", "start_pos": 61, "end_pos": 64, "type": "TASK", "confidence": 0.8746097087860107}]}, {"text": "This shows that they are robust to noise, probably because bad source-side phrases are unlikely to match the test-time input.", "labels": [], "entities": []}, {"text": "Note also the effect of repetitions: good word choices are shared by many n-best sentences, and thus they would have higher probabilities compared to bad word choices.", "labels": [], "entities": []}, {"text": "The gap between ML2EN and IN2EN for unigram precision could be explained by vocabulary differences between Malay and Indonesian.", "labels": [], "entities": [{"text": "IN2EN", "start_pos": 26, "end_pos": 31, "type": "METRIC", "confidence": 0.867864191532135}, {"text": "unigram precision", "start_pos": 36, "end_pos": 53, "type": "TASK", "confidence": 0.5866253972053528}]}, {"text": "Compared to IN2EN, all CN:* models have higher 2/3/4-gram precision.", "labels": [], "entities": [{"text": "IN2EN", "start_pos": 12, "end_pos": 17, "type": "DATASET", "confidence": 0.7180176973342896}, {"text": "precision", "start_pos": 58, "end_pos": 67, "type": "METRIC", "confidence": 0.9959269165992737}]}, {"text": "However, CN:pivot has lower unigram precision, which could be due to bad word alignments, as the results for CN:pivot \u2032 show.", "labels": [], "entities": [{"text": "precision", "start_pos": 36, "end_pos": 45, "type": "METRIC", "confidence": 0.8548411130905151}]}], "tableCaptions": [{"text": " Table 2: The five baselines. The subscript indicates the  parameters found on IN2EN-dev and used for IN2EN-test.  The scores that are statistically significantly better than  ML2EN and IN2EN (p < 0.01, Collins' sign test) are  shown in bold and are underlined, respectively.", "labels": [], "entities": [{"text": "IN2EN-dev", "start_pos": 79, "end_pos": 88, "type": "DATASET", "confidence": 0.9298736453056335}, {"text": "IN2EN-test", "start_pos": 102, "end_pos": 112, "type": "DATASET", "confidence": 0.8672972917556763}, {"text": "Collins' sign test", "start_pos": 203, "end_pos": 221, "type": "DATASET", "confidence": 0.6171451310316721}]}, {"text": " Table 3: Isolated experiments. The subscript shows the best tuning parameters, and the superscript shows the absolute  test improvement over the ML2EN and the IN2EN baselines. The last line shows system combination results.", "labels": [], "entities": [{"text": "ML2EN", "start_pos": 146, "end_pos": 151, "type": "DATASET", "confidence": 0.8197996616363525}]}, {"text": " Table 4: Combined experiments: BLEU. The best tuning parameter values are in subscript, and the absolute test  improvement over the corresponding baseline (on top of each column) is in superscript.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 32, "end_pos": 36, "type": "METRIC", "confidence": 0.998013973236084}, {"text": "absolute test  improvement", "start_pos": 97, "end_pos": 123, "type": "METRIC", "confidence": 0.9217412869135538}]}, {"text": " Table 5: Paraphrasing non-Indonesian words only:  those appearing at most t times in IN-LM.", "labels": [], "entities": [{"text": "Paraphrasing non-Indonesian words", "start_pos": 10, "end_pos": 43, "type": "TASK", "confidence": 0.8942922751108805}]}, {"text": " Table 8: Improving Macedonian-English SMT by  adapting Bulgarian to Macedonian.", "labels": [], "entities": [{"text": "Improving Macedonian-English SMT", "start_pos": 10, "end_pos": 42, "type": "TASK", "confidence": 0.7748714685440063}]}]}