{"title": [], "abstractContent": [{"text": "We annotate and resolve a particular case of abstract anaphora, namely, this-issue anaphora.", "labels": [], "entities": []}, {"text": "We propose a candidate ranking model for this-issue anaphora resolution that explores different issue-specific and general abstract-anaphora features.", "labels": [], "entities": [{"text": "anaphora resolution", "start_pos": 52, "end_pos": 71, "type": "TASK", "confidence": 0.7905572354793549}]}, {"text": "The model is not restricted to nominal or verbal antecedents; rather, it is able to identify antecedents that are arbitrary spans of text.", "labels": [], "entities": []}, {"text": "Our results show that (a) the model outperforms the strong adjacent-sentence baseline; (b) general abstract-anaphora features, as distinguished from issue-specific features , play a crucial role in this-issue anaphora resolution, suggesting that our approach can be generalized for other NPs such as this problem and this debate; and (c) it is possible to reduce the search space in order to improve performance.", "labels": [], "entities": [{"text": "this-issue anaphora resolution", "start_pos": 198, "end_pos": 228, "type": "TASK", "confidence": 0.8104645411173502}]}], "introductionContent": [{"text": "Anaphora in which the anaphoric expression refers to an abstract object such as a proposition, a property, or a fact is known as abstract object anaphora.", "labels": [], "entities": []}, {"text": "This is seen in the following examples.", "labels": [], "entities": []}, {"text": "(1) These examples highlight a difficulty not found with nominal anaphora.", "labels": [], "entities": []}, {"text": "First, the anaphors refer to abstract concepts that can be expressed with different syntactic shapes which are usually not nominals.", "labels": [], "entities": []}, {"text": "The anaphor That in (1) refers to the proposition in the previous utterance, whereas the anaphor this issue in (2) refers to a clause from the previous text.", "labels": [], "entities": []}, {"text": "In (3), the anaphoric expression this decision refers to a verb phrase from the same sentence.", "labels": [], "entities": []}, {"text": "Second, the antecedents do not always have precisely defined boundaries.", "labels": [], "entities": []}, {"text": "In (2), for example, the whole sentence containing the marked clause could also bethought to be the correct antecedent.", "labels": [], "entities": []}, {"text": "Third, the actual referents are not always the precise textual antecedents.", "labels": [], "entities": []}, {"text": "The actual referent in (2), the issue to be clarified, is whether oral carvedilol is more effective than oral metoprolol in the prevention of AF after on-pump CABG or not, a variant of the antecedent text.", "labels": [], "entities": [{"text": "prevention of AF", "start_pos": 128, "end_pos": 144, "type": "TASK", "confidence": 0.8849601944287618}]}, {"text": "Generally, abstract anaphora, as distinguished from nominal anaphora, is signalled in English by pronouns this, that, and it.", "labels": [], "entities": []}, {"text": "But in abstract anaphora, English prefers demonstratives to personal pronouns and definite articles.", "labels": [], "entities": []}, {"text": "Demonstra-tives can be used in isolation (That in (1)) or with nouns (e.g., this issue in (2)).", "labels": [], "entities": []}, {"text": "The latter follows the pattern demonstrative {modifier}* noun.", "labels": [], "entities": []}, {"text": "The demonstrative acts as a determiner and the noun following the demonstrative imposes selectional constraints for the antecedent, as in examples and.", "labels": [], "entities": []}, {"text": "calls such nouns label nouns, which \"serve to encapsulate or package a stretch of discourse\".", "labels": [], "entities": []}, {"text": "refers to them as shell nouns, a metaphoric term which reflects different functions of these nouns such as encapsulation, pointing, and signalling.", "labels": [], "entities": []}, {"text": "Demonstrative nouns, along with pronouns like both and either, are referred to as sortal anaphors).", "labels": [], "entities": []}, {"text": "Casta\u00f1o et al. observed that sortal anaphors are prevalent in the biomedical literature.", "labels": [], "entities": [{"text": "sortal anaphors", "start_pos": 29, "end_pos": 44, "type": "TASK", "confidence": 0.949755072593689}]}, {"text": "They noted that among 100 distinct anaphors derived from a corpus of 70 Medline abstracts, 60% were sortal anaphors.", "labels": [], "entities": []}, {"text": "But how often do demonstrative nouns refer to abstract objects?", "labels": [], "entities": []}, {"text": "We observed that from a corpus of 74,000 randomly chosen Medline 2 abstracts, of the first 150 most frequently occurring distinct demonstrative nouns (frequency > 30), 51.3% were abstract, 41.3% were concrete, and 7.3% were discourse deictic.", "labels": [], "entities": []}, {"text": "This shows that abstract anaphora resolution is an important component of general anaphora resolution in the biomedical domain.", "labels": [], "entities": [{"text": "abstract anaphora resolution", "start_pos": 16, "end_pos": 44, "type": "TASK", "confidence": 0.6460261742273966}, {"text": "general anaphora resolution", "start_pos": 74, "end_pos": 101, "type": "TASK", "confidence": 0.7298678755760193}]}, {"text": "However, automatic resolution of this type of anaphora has not attracted much attention and the previous work for this task is limited.", "labels": [], "entities": [{"text": "automatic resolution", "start_pos": 9, "end_pos": 29, "type": "TASK", "confidence": 0.7916012108325958}]}, {"text": "The present work is a step towards resolving abstract anaphora in written text.", "labels": [], "entities": []}, {"text": "In particular, we choose the interesting abstract concept issue and demonstrate the complexities of resolving this-issue anaphora manually as well as automatically in the Medline domain.", "labels": [], "entities": []}, {"text": "We present our algorithm, results, and error analysis for this-issue anaphora resolution.", "labels": [], "entities": [{"text": "anaphora resolution", "start_pos": 69, "end_pos": 88, "type": "TASK", "confidence": 0.7412985563278198}]}, {"text": "The abstract concept issue was chosen for the following reasons.", "labels": [], "entities": []}, {"text": "First, it occurs frequently in all kinds of text from newspaper articles to novels to scientific articles.", "labels": [], "entities": []}, {"text": "There are 13,489 issue anaphora instances in the New York Times corpus and 1,116 instances in 65,000 Medline abstracts.", "labels": [], "entities": [{"text": "New York Times corpus", "start_pos": 49, "end_pos": 70, "type": "DATASET", "confidence": 0.8818566799163818}]}, {"text": "Second, it is abstract enough that it can take several syntactic and semantic forms, which makes the problem interesting and non-trivial.", "labels": [], "entities": []}, {"text": "Third, issue referents in scientific literature generally lie in the previous sentence or two, which makes the problem tractable.", "labels": [], "entities": []}, {"text": "Fourth, issues in Medline abstracts are generally associated with clinical problems in the medical domain and spell out the motivation of the research presented in the article.", "labels": [], "entities": [{"text": "Medline abstracts", "start_pos": 18, "end_pos": 35, "type": "DATASET", "confidence": 0.8953320980072021}]}, {"text": "So extraction of this information would be useful in any biomedical information retrieval system.", "labels": [], "entities": [{"text": "biomedical information retrieval", "start_pos": 57, "end_pos": 89, "type": "TASK", "confidence": 0.633292019367218}]}], "datasetContent": [{"text": "In this section we present the evaluation of each component of our resolution system.", "labels": [], "entities": []}, {"text": "The set of candidate antecedents extracted by the method from Section 4.1 contained the correct antecedent 92% of the time.", "labels": [], "entities": []}, {"text": "Each anaphor had, on average, 23.80 candidates, of which only 5.19 candidates were nominal type.", "labels": [], "entities": []}, {"text": "The accuracy dropped to 84% when we did not extract mixed type candidates.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.9997019171714783}]}, {"text": "The error analysis of the 8% of the instances where we failed to extract the correct antecedent revealed that most of these errors were parsing errors which could not be corrected by our candidate extraction method.", "labels": [], "entities": []}, {"text": "In these cases, the parts of the antecedent had been placed in completely different branches of the parse tree.", "labels": [], "entities": []}, {"text": "For example, in (5), the correct antecedent is a combination of the NP from the S \u2192 VP \u2192 NP \u2192 PP \u2192 NP branch and the PP from S \u2192 VP \u2192 PP branch.", "labels": [], "entities": []}, {"text": "In such a case, concatenating sister constituents does not help.", "labels": [], "entities": []}, {"text": "(5) The data from this pilot study (VP (VBP provide) (NP (NP no evidence) (PP (IN for) (NP a difference in hemodynamic effects between pulse HVHF and CPFA))) (PP in patients with septic shock already receiving CRRT)).", "labels": [], "entities": []}, {"text": "A larger sample size is needed to adequately explore this issue.", "labels": [], "entities": []}, {"text": "We propose two metrics for abstract anaphora evaluation.", "labels": [], "entities": [{"text": "abstract anaphora evaluation", "start_pos": 27, "end_pos": 55, "type": "TASK", "confidence": 0.5970269640286764}]}, {"text": "The simplest metric is the percentage of antecedents on which the system and the annotated gold data agree.", "labels": [], "entities": []}, {"text": "We denote this metric as EXACT-M (Exact Match) and compute it as the ratio of number of correctly identified antecedents to the total number of marked antecedents.", "labels": [], "entities": [{"text": "EXACT-M", "start_pos": 25, "end_pos": 32, "type": "METRIC", "confidence": 0.9900525212287903}, {"text": "Exact Match)", "start_pos": 34, "end_pos": 46, "type": "METRIC", "confidence": 0.8168626427650452}]}, {"text": "This metric is a good indicator of a system's performance; however, it is a rather strict evaluation because, as we noted in section 1, issues generally have no precise boundaries in the text.", "labels": [], "entities": []}, {"text": "So we propose another metric called RLL, which is similar to the ROUGE-L metric) used for the evaluation of automatic summarization.", "labels": [], "entities": [{"text": "RLL", "start_pos": 36, "end_pos": 39, "type": "METRIC", "confidence": 0.9739037752151489}, {"text": "ROUGE-L metric", "start_pos": 65, "end_pos": 79, "type": "METRIC", "confidence": 0.9280093014240265}, {"text": "summarization", "start_pos": 118, "end_pos": 131, "type": "TASK", "confidence": 0.8494366407394409}]}, {"text": "Let the marked antecedents of the gold corpus fork anaphor instances be G = g 1 , g 2 , ..., g k and the system-annotated antecedents be A = a 1 , a 2 , ..., a k . Let the number of words in G and Abe m and n respectively.", "labels": [], "entities": []}, {"text": "Let LCS(g i , a i ) be the the number of words in the longest common subsequence of g i and a i . Then the precision (P RLL ) and recall (R RLL ) over the whole data set are computed as shown in equations and.", "labels": [], "entities": [{"text": "LCS", "start_pos": 4, "end_pos": 7, "type": "METRIC", "confidence": 0.9847431182861328}, {"text": "precision (P RLL )", "start_pos": 107, "end_pos": 125, "type": "METRIC", "confidence": 0.8165510535240174}, {"text": "recall (R RLL )", "start_pos": 130, "end_pos": 145, "type": "METRIC", "confidence": 0.8499751329421997}]}, {"text": "P RLL is the total number of word overlaps between the gold and system-annotated antecedents normalized by the number of words in system-annotated antecedents and R RLL is the total number of such word overlaps normalized by the number of words in the gold antecedents.", "labels": [], "entities": [{"text": "P RLL", "start_pos": 0, "end_pos": 5, "type": "METRIC", "confidence": 0.7539142370223999}, {"text": "R RLL", "start_pos": 163, "end_pos": 168, "type": "METRIC", "confidence": 0.9695887565612793}]}, {"text": "If the system picks too much text for antecedents, R RLL is high but P RLL is low.", "labels": [], "entities": [{"text": "R RLL", "start_pos": 51, "end_pos": 56, "type": "METRIC", "confidence": 0.9318266212940216}, {"text": "P RLL", "start_pos": 69, "end_pos": 74, "type": "METRIC", "confidence": 0.8666923642158508}]}, {"text": "The F-score,  F RLL , combines these two scores.", "labels": [], "entities": [{"text": "F-score", "start_pos": 4, "end_pos": 11, "type": "METRIC", "confidence": 0.9963700771331787}, {"text": "F RLL", "start_pos": 14, "end_pos": 19, "type": "METRIC", "confidence": 0.8305230140686035}]}, {"text": "The lower bound of F RLL is 0, where no true antecedent has any common substring with the predicted antecedents and the upper bound is 1, where all the predicted and true antecedents are exactly the same.", "labels": [], "entities": [{"text": "F RLL", "start_pos": 19, "end_pos": 24, "type": "METRIC", "confidence": 0.7634077966213226}]}, {"text": "In our results we represent these scores in terms of percentage.", "labels": [], "entities": []}, {"text": "There are no implemented systems that resolve issue anaphora or abstract anaphora signalled by label nouns in arbitrary text to use as a comparison.", "labels": [], "entities": []}, {"text": "So we compare our results against two baselines: adjacent sentence and random.", "labels": [], "entities": []}, {"text": "The adjacent sentence baseline chooses the previous sentence as the correct antecedent.", "labels": [], "entities": []}, {"text": "This is a high baseline because in our data 84.1% of the antecedents lie within the adjacent sentence.", "labels": [], "entities": []}, {"text": "The random baseline chooses a candidate drawn from a uniform random distribution over the set of candidates.", "labels": [], "entities": []}, {"text": "11 Note that our F RLL scores for both baselines are rather high because candidates often have considerable overlap with one another; hence a wrong choice may still have a high F RLL score.", "labels": [], "entities": [{"text": "F RLL scores", "start_pos": 17, "end_pos": 29, "type": "METRIC", "confidence": 0.923703690369924}, {"text": "F RLL score", "start_pos": 177, "end_pos": 188, "type": "METRIC", "confidence": 0.9399995803833008}]}, {"text": "We carried out two sets of systematic experiments in which we considered all combinations of our twelve feature classes.", "labels": [], "entities": []}, {"text": "The first set consists of 5-fold cross-validation experiments on our training data.", "labels": [], "entities": []}, {"text": "The second set evaluates how well the model built on the training data works on the unseen test data.", "labels": [], "entities": []}, {"text": "gives results of our system.", "labels": [], "entities": []}, {"text": "The first two rows are the baseline results.", "labels": [], "entities": []}, {"text": "Rows 3 to 8 give results for some of the best performing feature sets.", "labels": [], "entities": []}, {"text": "All systems based on our features beat both baselines on F-scores and EXACT-M.", "labels": [], "entities": [{"text": "F-scores", "start_pos": 57, "end_pos": 65, "type": "METRIC", "confidence": 0.977961003780365}, {"text": "EXACT-M", "start_pos": 70, "end_pos": 77, "type": "METRIC", "confidence": 0.8267216086387634}]}, {"text": "The empirically derived feature sets IP (issue patterns) and D (distance) appeared in almost all best feature set combinations.", "labels": [], "entities": [{"text": "D (distance)", "start_pos": 61, "end_pos": 73, "type": "METRIC", "confidence": 0.7321457117795944}]}, {"text": "Removing D resulted in a 6 percentage points drop in F RLL and a 4 percentage points drop in EXACT-M scores.", "labels": [], "entities": [{"text": "F RLL", "start_pos": 53, "end_pos": 58, "type": "METRIC", "confidence": 0.8312821388244629}, {"text": "EXACT-M", "start_pos": 93, "end_pos": 100, "type": "METRIC", "confidence": 0.9759232997894287}]}, {"text": "Surprisingly, feature set ST (syntactic type) was not included inmost of the best performing set of feature sets.", "labels": [], "entities": []}, {"text": "The combination of syntactic and semantic feature sets {IP, D, EL, MC, L, SR, DT} gave the best F RLL and EXACT-M scores for the cross-validation experiments.", "labels": [], "entities": [{"text": "F RLL", "start_pos": 96, "end_pos": 101, "type": "METRIC", "confidence": 0.8061349391937256}]}, {"text": "For the testdata experiments, the combination of semantic and lexical features {D, C, LO, L, SC, SR, DT} gave the best F RLL results, whereas syntactic, discourse, and semantic features {IP, D, C, EL, L, SC, SR, DT} gave the best EXACT-M results.", "labels": [], "entities": [{"text": "F RLL", "start_pos": 119, "end_pos": 124, "type": "METRIC", "confidence": 0.8373883664608002}]}, {"text": "Overall, row 3 of the table gives reasonable results for both crossvalidation and test-data experiments with no statistically significant difference to the corresponding best EXACT-M scores in rows 6 and 5 respectively.", "labels": [], "entities": [{"text": "EXACT-M", "start_pos": 175, "end_pos": 182, "type": "METRIC", "confidence": 0.8475415110588074}]}, {"text": "To pinpoint the errors made by our system, we carried out three experiments.", "labels": [], "entities": []}, {"text": "In the first experiment, we examined the contribution of issue-specific features versus non-issue features (rows 9 and 10).", "labels": [], "entities": []}, {"text": "Interestingly, when we used only non-issue features, the performance dropped only slightly.", "labels": [], "entities": []}, {"text": "The F RLL results from using only issue-specific features were below baseline, suggesting that the more general features associated with abstract anaphora play a crucial role in resolving this-issue anaphora.", "labels": [], "entities": [{"text": "F RLL", "start_pos": 4, "end_pos": 9, "type": "METRIC", "confidence": 0.900139331817627}]}, {"text": "In the second experiment, we determined the error caused by the candidate extractor component of our system.", "labels": [], "entities": []}, {"text": "Row 12 of the table gives the result when an oracle candidate extractor was used to add the correct antecedent in the set of candidates whenever our candidate extractor failed.", "labels": [], "entities": []}, {"text": "This did not affect cross-validation results by much because of the rarity of such instances.", "labels": [], "entities": []}, {"text": "However, in the testdata experiment, the EXACT-M improvements that resulted were statistically significant.", "labels": [], "entities": [{"text": "EXACT-M", "start_pos": 41, "end_pos": 48, "type": "METRIC", "confidence": 0.9196363091468811}]}, {"text": "This shows that our resolution algorithm was able to identify antecedents that were arbitrary spans of text.", "labels": [], "entities": []}, {"text": "In the last experiment, we examined the effect of the reduction of the candidate search space.", "labels": [], "entities": []}, {"text": "We assumed an oracle candidate sentence extractor (Row 13) which knows the exact candidate sentence in which the antecedent lies.", "labels": [], "entities": []}, {"text": "We can see that both RLL and EXACT-M scores markedly improved in this setting.", "labels": [], "entities": [{"text": "RLL", "start_pos": 21, "end_pos": 24, "type": "METRIC", "confidence": 0.9963028430938721}, {"text": "EXACT-M", "start_pos": 29, "end_pos": 36, "type": "METRIC", "confidence": 0.9741801619529724}]}, {"text": "In response to these results, we trained a decision-tree classifier to identify the correct antecedent sentence with simple location and length features and achieved 95% accuracy in identifying the correct candidate sentence.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 170, "end_pos": 178, "type": "METRIC", "confidence": 0.9991067051887512}]}], "tableCaptions": [{"text": " Table 2: Feature sets for this-issue resolution. All features are extracted automatically.", "labels": [], "entities": [{"text": "this-issue resolution", "start_pos": 27, "end_pos": 48, "type": "TASK", "confidence": 0.7115626335144043}]}, {"text": " Table 3: this-issue resolution results with SVM rank . All means evaluation using all features. Issue-specific features =  {IP, IVERB, IHEAD}. EX-M is EXACT-M.", "labels": [], "entities": [{"text": "IVERB", "start_pos": 129, "end_pos": 134, "type": "METRIC", "confidence": 0.8274616003036499}, {"text": "IHEAD", "start_pos": 136, "end_pos": 141, "type": "METRIC", "confidence": 0.9303320646286011}]}]}