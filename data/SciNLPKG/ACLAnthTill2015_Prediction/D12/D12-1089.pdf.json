{"title": [{"text": "A Systematic Comparison of Phrase Table Pruning Techniques", "labels": [], "entities": []}], "abstractContent": [{"text": "When trained on very large parallel corpora, the phrase table component of a machine translation system grows to consume vast computational resources.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 77, "end_pos": 96, "type": "TASK", "confidence": 0.7082616239786148}]}, {"text": "In this paper, we introduce a novel pruning criterion that places phrase table pruning on a sound theoretical foundation.", "labels": [], "entities": []}, {"text": "Systematic experiments on four language pairs under various data conditions show that our principled approach is superior to existing ad hoc pruning methods.", "labels": [], "entities": []}], "introductionContent": [{"text": "Over the last years, statistical machine translation has become the dominant approach to machine translation.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 21, "end_pos": 52, "type": "TASK", "confidence": 0.6989505986372629}, {"text": "machine translation", "start_pos": 89, "end_pos": 108, "type": "TASK", "confidence": 0.8192339539527893}]}, {"text": "This is not only due to improved modeling, but also due to a significant increase in the availability of monolingual and bilingual data.", "labels": [], "entities": []}, {"text": "Here are just two examples of very large data resources that are publicly available: \u2022 The Google Web 1T 5-gram corpus available from the Linguistic Data Consortium consisting of the 5-gram counts of about one trillion words of web data.", "labels": [], "entities": [{"text": "Google Web 1T 5-gram corpus", "start_pos": 91, "end_pos": 118, "type": "DATASET", "confidence": 0.8422536134719849}]}, {"text": "1 \u2022 The 10 9 -French-English bilingual corpus with about one billion tokens from the Workshop on Statistical Machine Translation (WMT).", "labels": [], "entities": [{"text": "Statistical Machine Translation (WMT)", "start_pos": 97, "end_pos": 134, "type": "TASK", "confidence": 0.7669449498256048}]}, {"text": "These enormous data sets yield translation models that are expensive to store and process.", "labels": [], "entities": []}, {"text": "Even with 1 LDC catalog No. LDC2006T13 2 http://www.statmt.org/wmt11/translation-task.html modern computers, these large models lead to along experiment cycle that hinders progress.", "labels": [], "entities": [{"text": "LDC catalog No. LDC2006T13", "start_pos": 12, "end_pos": 38, "type": "DATASET", "confidence": 0.7065033167600632}]}, {"text": "The situation is even more severe if computational resources are limited, for instance when translating on handheld devices.", "labels": [], "entities": []}, {"text": "Then, reducing the model size is of the utmost importance.", "labels": [], "entities": []}, {"text": "The most resource-intensive components of a statistical machine translation system are the language model and the phrase table.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 44, "end_pos": 75, "type": "TASK", "confidence": 0.6397371590137482}]}, {"text": "Recently, compact representations of the language model have attracted the attention of the research community, for instance in,,, to name a few.", "labels": [], "entities": []}, {"text": "In this paper, we address the other problem of any statistical machine translation system: large phrase tables. has shown that large portions of the phrase table can be removed without loss in translation quality.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 51, "end_pos": 82, "type": "TASK", "confidence": 0.6224131385485331}]}, {"text": "This motivated us to perform a systematic comparison of different pruning methods.", "labels": [], "entities": []}, {"text": "However, we found that many existing methods employ ad-hoc heuristics without theoretical foundation.", "labels": [], "entities": []}, {"text": "The pruning criterion introduced in this work is inspired by the very successful and still state-of-theart language model pruning criterion based on entropy measures.", "labels": [], "entities": []}, {"text": "We motivate its derivation by stating the desiderata fora good phrase table pruning criterion: \u2022 Soundness: The criterion should optimize some well-understood information-theoretic measure of translation model quality.", "labels": [], "entities": []}, {"text": "\u2022 Efficiency: Pruning should be fast, i. e., run linearly in the size of the phrase table.", "labels": [], "entities": [{"text": "Efficiency", "start_pos": 2, "end_pos": 12, "type": "METRIC", "confidence": 0.9947448968887329}]}, {"text": "\u2022 Self-containedness: As a practical consideration, we want to prune phrases from an existing phrase table.", "labels": [], "entities": []}, {"text": "This means pruning should use only information contained in the model itself.", "labels": [], "entities": []}, {"text": "\u2022 Good empirical behavior: We would like to be able to prune large parts of the phrase table without significant loss in translation quality.", "labels": [], "entities": []}, {"text": "Analyzing existing pruning techniques based on these objectives, we found that they are commonly deficient in at least one of them.", "labels": [], "entities": []}, {"text": "We thus designed a novel pruning criterion that not only meets these objectives, it also performs very well in empirical evaluations.", "labels": [], "entities": []}, {"text": "The novel contributions of this paper are: 1.", "labels": [], "entities": []}, {"text": "a systematic description of existing phrase table pruning methods.", "labels": [], "entities": []}, {"text": "2. anew, theoretically sound phrase table pruning criterion.", "labels": [], "entities": []}, {"text": "3. an experimental comparison of several pruning methods for several language pairs.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Two-by-two contingency table for a phrase pair ( \u02dc  f , \u02dc  e).", "labels": [], "entities": []}, {"text": " Table 2: Example phrases from the French-English phrase table (K=thousands, M=millions).", "labels": [], "entities": []}, {"text": " Table 4:  Statistics of phrase compositionality  (M=millions).", "labels": [], "entities": [{"text": "phrase compositionality", "start_pos": 25, "end_pos": 48, "type": "TASK", "confidence": 0.6946709007024765}, {"text": "M", "start_pos": 51, "end_pos": 52, "type": "METRIC", "confidence": 0.9629999995231628}]}, {"text": " Table 5: To what degree can we prune the phrase table without losing more than 1 Bleu point? The table shows  percentage of phrases that we have to retain. ES=Spanish, EN=English, FR=French, CS=Czech, DE=German.", "labels": [], "entities": [{"text": "Bleu", "start_pos": 82, "end_pos": 86, "type": "METRIC", "confidence": 0.989514946937561}, {"text": "DE", "start_pos": 202, "end_pos": 204, "type": "METRIC", "confidence": 0.9909257888793945}]}]}