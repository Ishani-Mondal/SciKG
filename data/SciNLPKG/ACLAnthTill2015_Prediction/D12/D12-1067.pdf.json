{"title": [{"text": "Parse, Price and Cut-Delayed Column and Row Generation for Graph Based Parsers", "labels": [], "entities": []}], "abstractContent": [{"text": "Graph-based dependency parsers suffer from the sheer number of higher order edges they need to (a) score and (b) consider during optimization.", "labels": [], "entities": [{"text": "Graph-based dependency parsers", "start_pos": 0, "end_pos": 30, "type": "TASK", "confidence": 0.6039359470208486}]}, {"text": "Here we show that when working with LP relaxations, large fractions of these edges can be pruned before they are fully scored-without any loss of optimality guarantees and, hence, accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 180, "end_pos": 188, "type": "METRIC", "confidence": 0.9986411929130554}]}, {"text": "This is achieved by iteratively parsing with a subset of higher-order edges, adding higher-order edges that may improve the score of the current solution , and adding higher-order edges that are implied by the current best first order edges.", "labels": [], "entities": []}, {"text": "This amounts to delayed column and row generation in the LP relaxation and is guaranteed to provide the optimal LP solution.", "labels": [], "entities": []}, {"text": "For second order grandparent models, our method considers , or scores, no more than 6-13% of the second order edges of the full model.", "labels": [], "entities": []}, {"text": "This yields up to an eightfold parsing speedup, while providing the same empirical accuracy and certificates of optimality as working with the full LP relaxation.", "labels": [], "entities": [{"text": "parsing", "start_pos": 31, "end_pos": 38, "type": "TASK", "confidence": 0.9635490775108337}, {"text": "accuracy", "start_pos": 83, "end_pos": 91, "type": "METRIC", "confidence": 0.9771621823310852}]}, {"text": "We also provide a tighter LP formulation for grandparent models that leads to a smaller integrality gap and higher speed.", "labels": [], "entities": []}], "introductionContent": [{"text": "Many problems in NLP, and structured prediction in general, can be cast as finding high-scoring structures based on a large set of candidate parts.", "labels": [], "entities": [{"text": "structured prediction", "start_pos": 26, "end_pos": 47, "type": "TASK", "confidence": 0.7147997468709946}]}, {"text": "For example, in second order graph-based dependency parsing () we have to choose a quadratic number of first order and a cubic number of second order edges such that the graph is both high-scoring and a tree.", "labels": [], "entities": [{"text": "second order graph-based dependency parsing", "start_pos": 16, "end_pos": 59, "type": "TASK", "confidence": 0.5815886199474335}]}, {"text": "In coreference, we have to select high-scoring clusters of mentions from an exponential number of candidate clusters, such that each mention is in exactly one cluster.", "labels": [], "entities": []}, {"text": "In segmentation of citation strings, we need to consider a quadratic number of possible segments such that every token is part of exactly one segment (.", "labels": [], "entities": [{"text": "segmentation of citation strings", "start_pos": 3, "end_pos": 35, "type": "TASK", "confidence": 0.8695574700832367}]}, {"text": "What makes such problems challenging is the large number of possible parts to consider.", "labels": [], "entities": []}, {"text": "This number not only affects the cost of search or optimization but also slows down the process of scoring parts before they enter the optimization problem.", "labels": [], "entities": []}, {"text": "For example, the cubic grandparent edges in second-order dependency parsing slowdown dynamic programs), belief propagation and LP solvers, since there are more value functions to evaluate, more messages to pass, or more variables to consider.", "labels": [], "entities": [{"text": "second-order dependency parsing", "start_pos": 44, "end_pos": 75, "type": "TASK", "confidence": 0.7043892542521158}, {"text": "belief propagation", "start_pos": 104, "end_pos": 122, "type": "TASK", "confidence": 0.812544047832489}, {"text": "LP solvers", "start_pos": 127, "end_pos": 137, "type": "TASK", "confidence": 0.4892368018627167}]}, {"text": "But to even calculate the score for each part we need a cubic number of operations that usually involve expensive feature extraction.", "labels": [], "entities": []}, {"text": "This step often becomes a major bottleneck in parsing, and structured prediction in general.", "labels": [], "entities": [{"text": "parsing", "start_pos": 46, "end_pos": 53, "type": "TASK", "confidence": 0.9764118194580078}, {"text": "structured prediction", "start_pos": 59, "end_pos": 80, "type": "TASK", "confidence": 0.6697401106357574}]}, {"text": "Candidate parts can often be heuristically pruned.", "labels": [], "entities": []}, {"text": "In the case of dependency parsing, previous work has used coarse-to-fine strategies where simpler first order models are used to prune unlikely first order edges, and hence all corresponding higher order edges (;).", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 15, "end_pos": 33, "type": "TASK", "confidence": 0.8379747569561005}]}, {"text": "While such methods can be effective, they are more convoluted, often require training of addition models as well as tuning of thresholding hyper-parameters, and usually provide no guarantees of optimality.", "labels": [], "entities": []}, {"text": "We present an approach that can solve problems with large sets of candidate parts without considering all of these parts in either optimization or scor-ing.", "labels": [], "entities": []}, {"text": "And in contrast to most pruning heuristics, our algorithm can give certificates of optimality before having optimized over, or even scored, all parts.", "labels": [], "entities": []}, {"text": "It does so without the need of auxiliary models or tuning of threshold parameters.", "labels": [], "entities": []}, {"text": "This is achieved by a delayed column and row generation algorithm that iteratively solves an LP relaxation over a small subset of current candidate parts, and then finds new candidates that score highly and can be inserted into the current optimal solution without removing high scoring existing structure.", "labels": [], "entities": []}, {"text": "The latter step subtracts from the cost of apart the price of resources the part requires, and is often referred as pricing.", "labels": [], "entities": []}, {"text": "Sometimes parts may score highly after pricing, but are necessary in order to make the current solution feasible.", "labels": [], "entities": []}, {"text": "We add such parts in a step that roughly amounts to violated cuts to the LP.", "labels": [], "entities": []}, {"text": "We illustrate our approach in terms of a secondorder grandparent model for dependency parsing.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 75, "end_pos": 93, "type": "TASK", "confidence": 0.8491235673427582}]}, {"text": "We solve these models by iteratively parsing, pricing, and cutting.", "labels": [], "entities": [{"text": "parsing", "start_pos": 37, "end_pos": 44, "type": "TASK", "confidence": 0.9739136099815369}]}, {"text": "To this end we use a variant of the LP relaxation formulated by.", "labels": [], "entities": []}, {"text": "Our variant of this LP is designed to be amenable to column generation.", "labels": [], "entities": [{"text": "column generation", "start_pos": 53, "end_pos": 70, "type": "TASK", "confidence": 0.7467690408229828}]}, {"text": "It also turns out to be a tighter outer bound that leads to fewer fractional solutions and faster runtimes.", "labels": [], "entities": []}, {"text": "To find high scoring grandparent edges without explicitly enumerating all of them, we prune out a large fraction using factorized upper bounds on grandparent scores.", "labels": [], "entities": []}, {"text": "Our parse, price and cut algorithm is evaluated using a non-projective grandparent model on three languages.", "labels": [], "entities": []}, {"text": "Compared to a brute force approach of solving the full LP, we only score about 10% of the grandparent edges, consider only 8% in optimization, and so observe an increase in parsing speed of up to 750%.", "labels": [], "entities": [{"text": "parsing", "start_pos": 173, "end_pos": 180, "type": "TASK", "confidence": 0.9622038006782532}]}, {"text": "This is possible without loss of optimality, and hence accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 55, "end_pos": 63, "type": "METRIC", "confidence": 0.9981256127357483}]}, {"text": "We also find that our extended LP formulation leads to a 15% reduction of fractional solutions, up to 12 times higher speed, and generally higher accuracy when compared to the grandparent formulation of.", "labels": [], "entities": [{"text": "speed", "start_pos": 118, "end_pos": 123, "type": "METRIC", "confidence": 0.9941562414169312}, {"text": "accuracy", "start_pos": 146, "end_pos": 154, "type": "METRIC", "confidence": 0.9994645714759827}]}], "datasetContent": [{"text": "We claim that LP relaxations for higher order parsing can be solved without considering, and scoring, all candidate higher order edges.", "labels": [], "entities": []}, {"text": "In practice, how many grandparent edges do we need to score, and how many do we need to add to the optimization problem?", "labels": [], "entities": []}, {"text": "And what kind of reduction in runtime does this reduction in edges lead to?", "labels": [], "entities": []}, {"text": "We have also pointed out that our outer bound on the grandparent polytope of legal edge and grandparent vectors is tighter than the one presented by.", "labels": [], "entities": []}, {"text": "What effect does this bound have on the number of fractional solutions and the overall accuracy?", "labels": [], "entities": [{"text": "accuracy", "start_pos": 87, "end_pos": 95, "type": "METRIC", "confidence": 0.9993360638618469}]}, {"text": "To answer these questions we will focus on a set of non-projective grandparent models, but point out that our method and formulation can be easily extended to projective parsing as well as other types of higher order edges.", "labels": [], "entities": []}, {"text": "We use the Danish test data of and the Italian and Hungarian test datasets of. compares brute force optimization (BF) with the full model, in spirit of, to running parse, price and cut (PPC) on the same model.", "labels": [], "entities": [{"text": "Danish test data", "start_pos": 11, "end_pos": 27, "type": "DATASET", "confidence": 0.9079670707384745}, {"text": "Hungarian test datasets", "start_pos": 51, "end_pos": 74, "type": "DATASET", "confidence": 0.7181808451811472}, {"text": "brute force optimization (BF)", "start_pos": 88, "end_pos": 117, "type": "TASK", "confidence": 0.6733069866895676}]}, {"text": "This model contains all constraints presented in 3.2.", "labels": [], "entities": []}, {"text": "The table shows the average number of parsed sentences per second, the average objective, number of grandparent edges scored and added, all relative to the brute force approach.", "labels": [], "entities": []}, {"text": "We also present the average unlabeled accuracy, and the percentage of sentences with integer solutions.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 38, "end_pos": 46, "type": "METRIC", "confidence": 0.9914724230766296}]}, {"text": "This number shows us how often we not only found the optimal solution to the LP relaxation, but also the optimal solution to the full ILP.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Parse, Price and Cut (PPC) vs Brute Force (BF). Speed is the number of sentences per second,  relative to the speed of BF. Objective, GPs scored and added are also relative to BF.", "labels": [], "entities": [{"text": "Parse", "start_pos": 10, "end_pos": 15, "type": "METRIC", "confidence": 0.9389915466308594}, {"text": "Brute Force (BF)", "start_pos": 40, "end_pos": 56, "type": "METRIC", "confidence": 0.9254650354385376}, {"text": "Speed", "start_pos": 58, "end_pos": 63, "type": "METRIC", "confidence": 0.9837223887443542}, {"text": "BF", "start_pos": 129, "end_pos": 131, "type": "METRIC", "confidence": 0.8629013895988464}, {"text": "GPs", "start_pos": 144, "end_pos": 147, "type": "METRIC", "confidence": 0.9479273557662964}, {"text": "BF", "start_pos": 186, "end_pos": 188, "type": "METRIC", "confidence": 0.992393970489502}]}, {"text": " Table 2: Different outer bounds on the grandpar- ent polytope, for nonprojective parsing of Italian and  Danish.", "labels": [], "entities": []}]}