{"title": [{"text": "A Phrase-Discovering Topic Model Using Hierarchical Pitman-Yor Processes", "labels": [], "entities": []}], "abstractContent": [{"text": "Topic models traditionally rely on the bag-of-words assumption.", "labels": [], "entities": []}, {"text": "In data mining applications , this often results in end-users being presented with inscrutable lists of topical un-igrams, single words inferred as representative of their topics.", "labels": [], "entities": [{"text": "data mining", "start_pos": 3, "end_pos": 14, "type": "TASK", "confidence": 0.76719930768013}]}, {"text": "In this article, we present a hierarchical generative probabilistic model of topical phrases.", "labels": [], "entities": []}, {"text": "The model simultaneously infers the location, length, and topic of phrases within a corpus and relaxes the bag-of-words assumption within phrases by using a hierarchy of Pitman-Yor processes.", "labels": [], "entities": []}, {"text": "We use Markov chain Monte Carlo techniques for approximate inference in the model and perform slice sampling to learn its hyperparameters.", "labels": [], "entities": []}, {"text": "We show via an experiment on human subjects that our model finds substantially better, more interpretable topical phrases than do competing models.", "labels": [], "entities": []}], "introductionContent": [{"text": "Probabilistic topic models have been the focus of intense study in recent years.", "labels": [], "entities": []}, {"text": "The archetypal topic model, Latent Dirichlet Allocation (LDA), posits that words within a document are conditionally independent given their topic (.", "labels": [], "entities": [{"text": "Latent Dirichlet Allocation (LDA", "start_pos": 28, "end_pos": 60, "type": "METRIC", "confidence": 0.7831562995910645}]}, {"text": "This \"bag-of-words\" assumption is a common simplification in which word order is ignored, but one which introduces undesirable properties into a model meant to serve as an unsupervised exploratory tool for data analysis.", "labels": [], "entities": []}, {"text": "When an end-user runs a topic model, the output he or she is often interested in is a list of topical unigrams, words probable in a topic (hence, representative of it).", "labels": [], "entities": []}, {"text": "In many situations, such as during the use of the topic model for the analysis of anew or ill-understood corpus, these lists can be insufficiently informative.", "labels": [], "entities": []}, {"text": "For instance, if a layperson ran LDA on the NIPS corpus, he would likely get a topic whose most prominent words include policy, value, and reward.", "labels": [], "entities": [{"text": "NIPS corpus", "start_pos": 44, "end_pos": 55, "type": "DATASET", "confidence": 0.944383978843689}]}, {"text": "Seeing these words isolated from their context in a list would not be particularly insightful to the layperson unfamiliar with computer science research.", "labels": [], "entities": []}, {"text": "An alternative to LDA which produced richer output like policy iteration algorithm, value function, and model-based reinforcement learning alongside the unigrams would be much more enlightening.", "labels": [], "entities": []}, {"text": "Most situations where a topic model is actually useful for data exploration require a model whose output is rich enough to dispel the need for the user's extensive prior knowledge of the data.", "labels": [], "entities": [{"text": "data exploration", "start_pos": 59, "end_pos": 75, "type": "TASK", "confidence": 0.7502187788486481}]}, {"text": "Furthermore, lists of topical unigrams are often made only marginally interpretable by virtue of their non-compositionality, the principle that a collocation's meaning typically is not derivable from its constituent words ().", "labels": [], "entities": []}, {"text": "For example, the meaning of compact disc as a music medium comes from neither the unigram compact nor the unigram disc, but emerges from the bigram as a whole.", "labels": [], "entities": []}, {"text": "Moreover, non-compositionality is topic dependent; compact disc should be interpreted as a music medium in a music topic, and as a small region bounded by a circle in a mathematical topic.", "labels": [], "entities": []}, {"text": "LDA is prone to decompose collocations into different topics and violate the principle of noncompositionality, and its unigram lists are harder to interpret as a result.", "labels": [], "entities": []}, {"text": "We present an extension of LDA called PhraseDiscovering LDA (PDLDA) that satisfies two desiderata: providing rich, interpretable output and honoring the non-compositionality of collocations.", "labels": [], "entities": []}, {"text": "PDLDA is builtin the tradition of the \"Topical NGram\" (TNG) model of.", "labels": [], "entities": [{"text": "PDLDA", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.9130454659461975}]}, {"text": "TNG is a topic model which satisfies the first desideratum by producing lists of representative, topically cohesive n-grams of the form shown in.", "labels": [], "entities": []}, {"text": "We diverge from TNG by our addressing the second desideratum, and we do so through a more straightforward and intuitive definition of what constitutes a phrase and its topic.", "labels": [], "entities": []}, {"text": "In the furtherance of our goals, we employ a hierarchical method of modeling phrases that uses dependent Pitman-Yor processes to ameliorate overfitting.", "labels": [], "entities": []}, {"text": "Pitman-Yor processes have been successfully used in the past in n-gram) and LDA-based models) for creating Bayesian language models which exploit word order, and they prove equally useful in this scenario of exploiting both word order and topics.", "labels": [], "entities": []}, {"text": "This article is organized as follows: after describing TNG, we discuss PDLDA and how PDLDA addresses the limitations of TNG.", "labels": [], "entities": [{"text": "TNG", "start_pos": 55, "end_pos": 58, "type": "TASK", "confidence": 0.8839002251625061}]}, {"text": "We then provide details of our inference procedures and evaluate our model against competing models on a subset of the TREC AP corpus) in an experiment on human subjects which assesses the interpretability of topical n-gram lists.", "labels": [], "entities": [{"text": "TREC AP corpus", "start_pos": 119, "end_pos": 133, "type": "DATASET", "confidence": 0.9530575474103292}]}, {"text": "The experiment is premised on the notion that topic models should be evaluated through a real-world task instead of through information-theoretic measures which often negatively correlate with topic quality ().", "labels": [], "entities": []}], "datasetContent": [{"text": "Perplexity is the typical information theoretic measure of language model quality used in lieu of extrinsic measures, which are more difficult and costly to run.", "labels": [], "entities": []}, {"text": "However, it is well known that perplexity: Experimental setup of the phrase intrusion experiment in which subjects must click on the ngram that does not belong.", "labels": [], "entities": []}, {"text": "scores may negatively correlate with actual quality as assessed by humans (.", "labels": [], "entities": []}, {"text": "With that fact in mind, we expanded the methodology of to create a \"phrase intrusion\" task that quantitatively compares the quality of the topical n-gram lists produced by our model against those of other models.", "labels": [], "entities": []}, {"text": "Each of 48 subjects underwent 80 trials of a webbased experiment on Amazon Mechanical Turk, a reliable ( and increasingly common venue for conducting online experiments.", "labels": [], "entities": [{"text": "Amazon Mechanical Turk", "start_pos": 68, "end_pos": 90, "type": "DATASET", "confidence": 0.9503419200579325}]}, {"text": "In each trial, a subject is presented with a randomly ordered list of four n-grams (cf..", "labels": [], "entities": []}, {"text": "Each subject's task is to select the intruder phrase, a spurious n-gram not belonging with the others in the list.", "labels": [], "entities": []}, {"text": "If, other than the intruder, the items in the list are all on the same topic, then subjects can easily identify the intruder because the list is semantically cohesive and makes sense.", "labels": [], "entities": []}, {"text": "If the list is incohesive and has no discernible topic, subjects must guess arbitrarily and performance is at random.", "labels": [], "entities": []}, {"text": "To construct each trial's list, we chose two topics z and z (z = z ), then selected the three most probable n-grams from z and the intruder phrase, an n-gram probable in z and improbable in z.", "labels": [], "entities": []}, {"text": "This design ensures that the intruder is not identifiable due solely to its being rare.", "labels": [], "entities": []}, {"text": "Interspersed among the phrase intrusion trials were several simple screening trials intended to affirm that subjects possessed a minimal level of attentiveness and reading comprehension.", "labels": [], "entities": []}, {"text": "For example, one such screening trial presented subjects with the list banana, apple, television, orange.", "labels": [], "entities": []}, {"text": "Subjects who got any of these trials  wrong were excluded from our analyses.", "labels": [], "entities": []}, {"text": "Each subject was presented with trials constructed from the output of PDLDA and TNG for unigrams, bigrams, and trigrams.", "labels": [], "entities": [{"text": "PDLDA", "start_pos": 70, "end_pos": 75, "type": "DATASET", "confidence": 0.8256298303604126}]}, {"text": "For unigrams, we also tested the output of the original smoothed LDA ().", "labels": [], "entities": []}, {"text": "The experiment was conducted twice fora 2,246-document subset of the TREC AP corpus (: the first time proceeded as described above, but the second time did not allow word repetition within a topic's list.", "labels": [], "entities": [{"text": "TREC AP corpus", "start_pos": 69, "end_pos": 83, "type": "DATASET", "confidence": 0.9427116711934408}, {"text": "word repetition within a topic's list", "start_pos": 166, "end_pos": 203, "type": "TASK", "confidence": 0.819070679800851}]}, {"text": "The topical phrases found by TNG and PDLDA often revolve around a central n-gram, with other words pre-or post-appended to it.", "labels": [], "entities": []}, {"text": "In this intrusion experiment, any n-gram not containing the central word or phrase maybe trivially identifiable, regardless of its relevance to the topic.", "labels": [], "entities": []}, {"text": "For example, the intruder in Trial 4 of is easily identifiable even if a subject does not understand English.", "labels": [], "entities": []}, {"text": "This second experiment was designed to test whether our conclusions hinge on word repetition.", "labels": [], "entities": [{"text": "word repetition", "start_pos": 77, "end_pos": 92, "type": "TASK", "confidence": 0.7779974043369293}]}, {"text": "We used the MALLET toolbox) for the implementations of LDA and TNG.", "labels": [], "entities": [{"text": "MALLET", "start_pos": 12, "end_pos": 18, "type": "METRIC", "confidence": 0.8475943207740784}]}, {"text": "Each model was run with 100 topics for 5,000 iterations.", "labels": [], "entities": []}, {"text": "We set m = 2, \u03b1 = .01, \u03b2 = .01, \u03bb = 1, \u03c0 1 = \u03c0 2 = 1, \u03c1 1 = 10, and \u03c1 2 = .1.", "labels": [], "entities": []}, {"text": "For all models, we treated certain punctuation as the start of a phrase by setting c j = 1 for all tokens j immediately following periods, commas, semicolons, and exclamation and question marks.", "labels": [], "entities": []}, {"text": "To reduce runtime, we removed stopwords occuring in the MALLET toolbox's stopword list.", "labels": [], "entities": [{"text": "MALLET toolbox's stopword list", "start_pos": 56, "end_pos": 86, "type": "DATASET", "confidence": 0.8496429443359375}]}, {"text": "Because TNG and LDA had trouble with single character words not in the stoplist, we manually removed them before the experiment.", "labels": [], "entities": []}, {"text": "Any token immediately following a removed word was treated as if it were the start of a phrase.", "labels": [], "entities": []}, {"text": "As in, performance is measured via model precision, the fraction of subjects agreeing with the model.", "labels": [], "entities": [{"text": "precision", "start_pos": 41, "end_pos": 50, "type": "METRIC", "confidence": 0.9294047951698303}]}, {"text": "It is defined as MP m,n k = s 1(i m,n k,s = \u03c9 m,n k,s )/S where \u03c9 m,n k,s is the index of the intruding n-gram for subject s among the words generated from the kth topic of model m, i m,n k,s is the intruder selected by s, and S is the number of subjects.", "labels": [], "entities": []}, {"text": "The model precisions are shown in.", "labels": [], "entities": [{"text": "precisions", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9884089231491089}]}, {"text": "PDLDA achieves the highest precision in all conditions.", "labels": [], "entities": [{"text": "PDLDA", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.7383148074150085}, {"text": "precision", "start_pos": 27, "end_pos": 36, "type": "METRIC", "confidence": 0.9993471503257751}]}, {"text": "Model precision is low in all models, which is a reflection of how challenging the task is on a small corpus laden with proper nouns and low-frequency words.", "labels": [], "entities": [{"text": "precision", "start_pos": 6, "end_pos": 15, "type": "METRIC", "confidence": 0.9856052994728088}]}, {"text": "demonstrates that the outcome of the experiment does not depend strongly on whether the topical n-gram lists have repeated words.", "labels": [], "entities": []}], "tableCaptions": []}