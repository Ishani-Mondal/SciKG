{"title": [{"text": "Ensemble Semantics for Large-scale Unsupervised Relation Extraction", "labels": [], "entities": [{"text": "Large-scale Unsupervised Relation Extraction", "start_pos": 23, "end_pos": 67, "type": "TASK", "confidence": 0.6910437270998955}]}], "abstractContent": [{"text": "Discovering significant types of relations from the web is challenging because of its open nature.", "labels": [], "entities": []}, {"text": "Unsupervised algorithms are developed to extract relations from a corpus without knowing the relations in advance , but most of them rely on tagging arguments of predefined types.", "labels": [], "entities": []}, {"text": "Recently, anew algorithm was proposed to jointly extract relations and their argument semantic classes, taking a set of relation instances extracted by an open IE algorithm as input.", "labels": [], "entities": []}, {"text": "However, it cannot handle poly-semy of relation phrases and fails to group many similar (\"synonymous\") relation instances because of the sparseness of features.", "labels": [], "entities": []}, {"text": "In this paper, we present a novel unsupervised algorithm that provides a more general treatment of the polysemy and synonymy problems.", "labels": [], "entities": []}, {"text": "The algorithm incorporates various knowledge sources which we will show to be very effective for unsupervised extraction.", "labels": [], "entities": []}, {"text": "Moreover, it explicitly disambiguates polysemous relation phrases and groups synonymous ones.", "labels": [], "entities": []}, {"text": "While maintaining approximately the same precision, the algorithm achieves significant improvement on recall compared to the previous method.", "labels": [], "entities": [{"text": "precision", "start_pos": 41, "end_pos": 50, "type": "METRIC", "confidence": 0.9989609718322754}, {"text": "recall", "start_pos": 102, "end_pos": 108, "type": "METRIC", "confidence": 0.9995076656341553}]}, {"text": "It is also very efficient.", "labels": [], "entities": []}, {"text": "Experiments on a real-world dataset show that it can handle 14.7 million relation instances and extract a very large set of relations from the web.", "labels": [], "entities": []}], "introductionContent": [{"text": "Relation extraction aims at discovering semantic relations between entities.", "labels": [], "entities": [{"text": "Relation extraction", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.9686509668827057}]}, {"text": "It is an important task that has many applications in answering factoid questions, building knowledge bases and improving search engine relevance.", "labels": [], "entities": [{"text": "answering factoid questions", "start_pos": 54, "end_pos": 81, "type": "TASK", "confidence": 0.8817109068234762}]}, {"text": "The web has become a massive potential source of such relations.", "labels": [], "entities": []}, {"text": "However, its open nature brings an open-ended set of relation types.", "labels": [], "entities": []}, {"text": "To extract these relations, a system should not assume a fixed set of relation types, nor rely on a fixed set of relation argument types.", "labels": [], "entities": []}, {"text": "The past decade has seen some promising solutions, unsupervised relation extraction (URE) algorithms that extract relations from a corpus without knowing the relations in advance.", "labels": [], "entities": [{"text": "unsupervised relation extraction (URE)", "start_pos": 51, "end_pos": 89, "type": "TASK", "confidence": 0.7440181771914164}]}, {"text": "However, most algorithms () rely on tagging predefined types of entities as relation arguments, and thus are not well-suited for the open domain.", "labels": [], "entities": [{"text": "tagging predefined types of entities as relation arguments", "start_pos": 36, "end_pos": 94, "type": "TASK", "confidence": 0.7994489446282387}]}, {"text": "Recently, proposed Semantic Network Extractor (SNE), which generates argument semantic classes and sets of synonymous relation phrases at the same time, thus avoiding the requirement of tagging relation arguments of predefined types.", "labels": [], "entities": [{"text": "Semantic Network Extractor (SNE", "start_pos": 19, "end_pos": 50, "type": "TASK", "confidence": 0.7319526672363281}]}, {"text": "However, SNE has 2 limitations: 1) Following previous URE algorithms, it only uses features from the set of input relation instances for clustering.", "labels": [], "entities": [{"text": "SNE", "start_pos": 9, "end_pos": 12, "type": "TASK", "confidence": 0.9641777873039246}]}, {"text": "Empirically we found that it fails to group many relevant relation instances.", "labels": [], "entities": []}, {"text": "These features, such as the surface forms of arguments and lexical sequences in between, are very sparse in practice.", "labels": [], "entities": []}, {"text": "In contrast, there exist several well-known corpus-level semantic resources that can be automatically derived from a source corpus and are shown to be useful for generating the key elements of a relation: its 2 argument semantic classes and a set of synonymous phrases.", "labels": [], "entities": []}, {"text": "For example, semantic classes can be derived from a source corpus with contextual distributional similarity and web table co-occurrences.", "labels": [], "entities": []}, {"text": "The \"synonymy\" problem for clustering relation instances * Work done during an internship at Microsoft Research Asia could potentially be better solved by adding these resources.", "labels": [], "entities": [{"text": "clustering relation instances", "start_pos": 27, "end_pos": 56, "type": "TASK", "confidence": 0.8820904692014059}]}, {"text": "2) SNE assumes that each entity or relation phrase belongs to exactly one cluster, thus is notable to effectively handle polysemy of relation phrases . An example of a polysemous phrase is be the currency of as in 2 triples <Euro, be the currency of, Germany> and <authorship, be the currency of, science>.", "labels": [], "entities": [{"text": "SNE", "start_pos": 3, "end_pos": 6, "type": "TASK", "confidence": 0.868657648563385}]}, {"text": "As the target corpus expands from mostly news to the open web, polysemy becomes more important as input covers a wider range of domains.", "labels": [], "entities": []}, {"text": "In practice, around 22% (section 3) of relation phrases are polysemous.", "labels": [], "entities": []}, {"text": "Failure to handle these cases significantly limits its effectiveness.", "labels": [], "entities": []}, {"text": "To move towards a more general treatment of the polysemy and synonymy problems, we present a novel algorithm WEBRE for open-domain largescale unsupervised relation extraction without predefined relation or argument types.", "labels": [], "entities": [{"text": "open-domain largescale unsupervised relation extraction", "start_pos": 119, "end_pos": 174, "type": "TASK", "confidence": 0.5860187768936157}]}, {"text": "The contributions are: \u2022 WEBRE incorporates a wide range of corpus-level semantic resources for improving relation extraction.", "labels": [], "entities": [{"text": "WEBRE", "start_pos": 25, "end_pos": 30, "type": "DATASET", "confidence": 0.5959701538085938}, {"text": "relation extraction", "start_pos": 106, "end_pos": 125, "type": "TASK", "confidence": 0.8847226500511169}]}, {"text": "The effectiveness of each knowledge source and their combination are studied and compared.", "labels": [], "entities": []}, {"text": "To the best of our knowledge, it is the first to combine and compare them for unsupervised relation extraction.", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 91, "end_pos": 110, "type": "TASK", "confidence": 0.7824968099594116}]}, {"text": "\u2022 WEBRE explicitly disambiguates polysemous relation phrases and groups synonymous phrases, thus fundamentally it avoids the limitation of previous methods.", "labels": [], "entities": []}, {"text": "\u2022 Experiments on the Clueweb09 dataset (lemurproject.org/clueweb09.php) show that WEBRE is effective and efficient.", "labels": [], "entities": [{"text": "Clueweb09 dataset", "start_pos": 21, "end_pos": 38, "type": "DATASET", "confidence": 0.9209689497947693}, {"text": "WEBRE", "start_pos": 82, "end_pos": 87, "type": "TASK", "confidence": 0.7071738839149475}]}, {"text": "We present a large-scale evaluation and show that WEBRE can extract a very large set of high-quality relations.", "labels": [], "entities": [{"text": "WEBRE", "start_pos": 50, "end_pos": 55, "type": "DATASET", "confidence": 0.6019248962402344}]}, {"text": "Compared to the closest prior work, WEBRE significantly improves recall while maintaining the same level of precision.", "labels": [], "entities": [{"text": "WEBRE", "start_pos": 36, "end_pos": 41, "type": "METRIC", "confidence": 0.37584614753723145}, {"text": "recall", "start_pos": 65, "end_pos": 71, "type": "METRIC", "confidence": 0.9993984699249268}, {"text": "precision", "start_pos": 108, "end_pos": 117, "type": "METRIC", "confidence": 0.99704509973526}]}, {"text": "To the best of our knowledge, it handles the largest triple set to date (7-fold larger than largest previous effort).", "labels": [], "entities": []}, {"text": "Taking 14.7 million triples as input, a complete run with one CPU core takes about a day.", "labels": [], "entities": []}], "datasetContent": [{"text": "Data preparation We tested WEBRE on resources extracted from the English subset of the Clueweb09 Dataset, which contains 503 million webpages.", "labels": [], "entities": [{"text": "Data preparation", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.6398091465234756}, {"text": "WEBRE", "start_pos": 27, "end_pos": 32, "type": "DATASET", "confidence": 0.5292314887046814}, {"text": "English subset of the Clueweb09 Dataset", "start_pos": 65, "end_pos": 104, "type": "DATASET", "confidence": 0.7589260737101237}]}, {"text": "For building knowledge resources, all webpages are cleaned and then POS tagged and chunked with in-house tools.", "labels": [], "entities": []}, {"text": "We implemented the algorithms described in section 4.1 to generate the knowledge sources, including a hypernym graph, two entity similarity graphs and a relation phrase similarity graph.", "labels": [], "entities": []}, {"text": "We used Reverb Clueweb09 Extractions 1.1 (downloaded from reverb.cs.washington.edu) as the triple store (relation instances).", "labels": [], "entities": [{"text": "Reverb Clueweb09 Extractions 1.1", "start_pos": 8, "end_pos": 40, "type": "DATASET", "confidence": 0.9021952301263809}]}, {"text": "It is the complete extraction of Reverb over Clueweb09 after filtering low confidence and low frequency triples.", "labels": [], "entities": [{"text": "Reverb", "start_pos": 33, "end_pos": 39, "type": "METRIC", "confidence": 0.9516991972923279}]}, {"text": "It contains 14.7 million distinct triples with 3.3 million entities and 1.3 million relation phrases.", "labels": [], "entities": []}, {"text": "We choose it because 1) it is extracted by a stateof-the-art open IE extractor from the open-domain, and 2) to the best of our knowledge, it contains the largest number of distinct triples extracted from the open-domain and which is publicly available.", "labels": [], "entities": []}, {"text": "The evaluations are organized as follows: we evaluate Type A relation extraction and Type B relation extraction separately, and then we compare WEBRE to its closest prior work SNE.", "labels": [], "entities": [{"text": "Type A relation extraction", "start_pos": 54, "end_pos": 80, "type": "TASK", "confidence": 0.6675872653722763}, {"text": "Type B relation extraction", "start_pos": 85, "end_pos": 111, "type": "TASK", "confidence": 0.5509471371769905}, {"text": "WEBRE", "start_pos": 144, "end_pos": 149, "type": "DATASET", "confidence": 0.6879253387451172}]}, {"text": "Since both phases are essentially clustering algorithms, we compare the output clusters with human labeled gold standards and report performance measures, following most previous work such as and.", "labels": [], "entities": []}, {"text": "Three gold standards are created for evaluating Type A relations, Type B relations and the comparison to SNE, respectively.", "labels": [], "entities": []}, {"text": "In the experiments, we set \u03b1=0.6, \u00b5=0.1 and \u00ed \u00b5\u00ed\u00bc\u008e=0.02 based on trial runs on a small development set of 10k relation instances.", "labels": [], "entities": [{"text": "\u00ed \u00b5\u00ed\u00bc\u008e=0.02", "start_pos": 44, "end_pos": 55, "type": "METRIC", "confidence": 0.8949404358863831}]}, {"text": "We filtered out the Type A relations and Type B relations which only contain 1 or 2 triples since most of these relations are not different from a single relation instance and are not very interesting.", "labels": [], "entities": []}, {"text": "Overall, 0.2 million Type A relations and 84,000 Type B relations are extracted.", "labels": [], "entities": []}, {"text": "Evaluating Type A relations To understand the effectiveness of knowledge sources, we run Phase 1 multiple times taking entity similarity graphs (matrices) constructed with resources listed below: \u2022 TS: Distributional similarity based on the triple store.", "labels": [], "entities": [{"text": "TS", "start_pos": 198, "end_pos": 200, "type": "METRIC", "confidence": 0.9369568824768066}]}, {"text": "For each triple <ent 1 , ctx, ent 2 >, features of ent 1 are {ctx} and {ctx ent 2 }; features of ent 2 are {ctx} and {ent 1 ctx}.", "labels": [], "entities": []}, {"text": "Features are weighted with PMI.", "labels": [], "entities": [{"text": "PMI", "start_pos": 27, "end_pos": 30, "type": "METRIC", "confidence": 0.963331937789917}]}, {"text": "Cosine is used as similarity measure.", "labels": [], "entities": [{"text": "similarity measure", "start_pos": 18, "end_pos": 36, "type": "METRIC", "confidence": 0.9555447399616241}]}, {"text": "\u2022 LABEL: The similarity between two entities is computed according to the percentage of top hypernyms they share.", "labels": [], "entities": [{"text": "LABEL", "start_pos": 2, "end_pos": 7, "type": "METRIC", "confidence": 0.9910590052604675}, {"text": "similarity", "start_pos": 13, "end_pos": 23, "type": "METRIC", "confidence": 0.9358705282211304}]}, {"text": "\u2022 SIM: The similarity between two entities is the linear combination of their similarity scores in the distributional similarity graph and in the pattern similarity graph.", "labels": [], "entities": []}, {"text": "\u2022 SIM+LABEL SIM and LABEL are combined.", "labels": [], "entities": [{"text": "LABEL", "start_pos": 20, "end_pos": 25, "type": "METRIC", "confidence": 0.8399866223335266}]}, {"text": "Observing that SIM generates high quality but overly fine-grained semantic classes, we modify the entity clustering procedure to cluster argument entities based on SIM first, and then further clustering the results based on LABEL.", "labels": [], "entities": [{"text": "LABEL", "start_pos": 224, "end_pos": 229, "type": "DATASET", "confidence": 0.8785025477409363}]}, {"text": "The outputs of these runs are pooled and mixed for labeling.", "labels": [], "entities": [{"text": "labeling", "start_pos": 51, "end_pos": 59, "type": "TASK", "confidence": 0.9758190512657166}]}, {"text": "We randomly sampled 60 relation phrases.", "labels": [], "entities": []}, {"text": "For each phrase, we select the 5 most frequent Type A relations from each run (4\u00d75=20 6 Type A relations in all).", "labels": [], "entities": []}, {"text": "For each relation phrase, we ask a human labeler to label the mixed pool of Type A relations that share the phrase: 1) The labelers 7 are asked to first determine the major semantic relation of each Type A relation, and then label the triples as good, fair or bad based on whether they express the major relation.", "labels": [], "entities": []}, {"text": "2) The labeler also reads all Type A relations and manually merges the ones that express the same relation.", "labels": [], "entities": []}, {"text": "These 2 steps are repeated for each phrase.", "labels": [], "entities": []}, {"text": "After labeling, we create a gold standard GS1, which contains roughly 10,000 triples for 60 relation phrases.", "labels": [], "entities": []}, {"text": "On average, close to 200 triples are manu-ally labeled and clustered for each phrase.", "labels": [], "entities": []}, {"text": "This creates a large data set for evaluation.", "labels": [], "entities": []}, {"text": "We report micro-average of precision, recall and F1 on the 60 relation phrases for each method.", "labels": [], "entities": [{"text": "precision", "start_pos": 27, "end_pos": 36, "type": "METRIC", "confidence": 0.9996296167373657}, {"text": "recall", "start_pos": 38, "end_pos": 44, "type": "METRIC", "confidence": 0.9996223449707031}, {"text": "F1", "start_pos": 49, "end_pos": 51, "type": "METRIC", "confidence": 0.9995613694190979}]}, {"text": "Precision (P) and Recall (R) of a given relation phrase is defined as follows.", "labels": [], "entities": [{"text": "Precision (P)", "start_pos": 0, "end_pos": 13, "type": "METRIC", "confidence": 0.9381182193756104}, {"text": "Recall (R)", "start_pos": 18, "end_pos": 28, "type": "METRIC", "confidence": 0.9649673104286194}]}, {"text": "Here \u00ed \u00b5\u00ed\u00b1 \u00ed \u00b5\u00ed\u00b0\u00e1nd \u00ed \u00b5\u00ed\u00b1 \u00ed \u00b5\u00ed\u00b0\u00b4\u2032 \u00b5\u00ed\u00b0\u00b4\u2032 represents a Type A relation in the algorithm output and GS1, respectively.", "labels": [], "entities": [{"text": "GS1", "start_pos": 97, "end_pos": 100, "type": "DATASET", "confidence": 0.57491534948349}]}, {"text": "We use t for triples and s(t) to represent the score of the labeled triplet.", "labels": [], "entities": []}, {"text": "s(t) is set to 1.0, 0.5 or 0 fort labeled as good, fair and bad, respectively.", "labels": [], "entities": []}, {"text": "\u00ed \u00b5\u00ed\u00b1\u0083 = The results are in table 1.", "labels": [], "entities": [{"text": "\u00ed \u00b5\u00ed\u00b1\u0083", "start_pos": 0, "end_pos": 6, "type": "METRIC", "confidence": 0.6004354953765869}]}, {"text": "Overall, LABEL performs 53% better than TS in F-measure, and SIM+LABEL performs the best, 8% better than LABEL.", "labels": [], "entities": [{"text": "TS", "start_pos": 40, "end_pos": 42, "type": "METRIC", "confidence": 0.847687840461731}, {"text": "F-measure", "start_pos": 46, "end_pos": 55, "type": "METRIC", "confidence": 0.7743896842002869}]}, {"text": "Applying a simple sign test shows both differences are clearly significant (p<0.001).", "labels": [], "entities": []}, {"text": "Surprisingly, SIM, which uses the similarity matrix extracted from full text, has a F1 of 0.277, which is lower than TS.", "labels": [], "entities": [{"text": "F1", "start_pos": 84, "end_pos": 86, "type": "METRIC", "confidence": 0.9988320469856262}, {"text": "TS", "start_pos": 117, "end_pos": 119, "type": "METRIC", "confidence": 0.9693440198898315}]}, {"text": "We also tried combining TS and LABEL but did not find encouraging performance compared to SIM+LABEL.", "labels": [], "entities": [{"text": "TS", "start_pos": 24, "end_pos": 26, "type": "METRIC", "confidence": 0.5940173268318176}]}, {"text": "Among the 4 methods, SIM has the highest precision (0.964) when relation phrases for which it fails to generate any Type A relations are excluded, but its recall is low.", "labels": [], "entities": [{"text": "SIM", "start_pos": 21, "end_pos": 24, "type": "TASK", "confidence": 0.903805673122406}, {"text": "precision", "start_pos": 41, "end_pos": 50, "type": "METRIC", "confidence": 0.9986364245414734}, {"text": "recall", "start_pos": 155, "end_pos": 161, "type": "METRIC", "confidence": 0.999663233757019}]}, {"text": "Manual checking shows that SIM tends to generate overly fine-grained argument classes.", "labels": [], "entities": []}, {"text": "If fine-grained argument classes or extremely high-precision Type A relations are preferred, SIM is a good choice.", "labels": [], "entities": [{"text": "SIM", "start_pos": 93, "end_pos": 96, "type": "TASK", "confidence": 0.9194462299346924}]}, {"text": "LABEL performs significantly better than TS, which shows that hypernymy information is very useful for finding argument semantic classes.", "labels": [], "entities": [{"text": "LABEL", "start_pos": 0, "end_pos": 5, "type": "METRIC", "confidence": 0.6064111590385437}, {"text": "TS", "start_pos": 41, "end_pos": 43, "type": "METRIC", "confidence": 0.6147434711456299}]}, {"text": "However, it has coverage problems in that the hypernym finding algorithm failed to find any hypernym from the corpus for some entities.", "labels": [], "entities": [{"text": "coverage", "start_pos": 16, "end_pos": 24, "type": "METRIC", "confidence": 0.9774726629257202}, {"text": "hypernym finding", "start_pos": 46, "end_pos": 62, "type": "TASK", "confidence": 0.803021252155304}]}, {"text": "Following up, we found that SIM+LABEL has similar precision and the highest recall.", "labels": [], "entities": [{"text": "LABEL", "start_pos": 32, "end_pos": 37, "type": "METRIC", "confidence": 0.7795575857162476}, {"text": "precision", "start_pos": 50, "end_pos": 59, "type": "METRIC", "confidence": 0.9995649456977844}, {"text": "recall", "start_pos": 76, "end_pos": 82, "type": "METRIC", "confidence": 0.9993298053741455}]}, {"text": "This shows that the combination of semantic spaces is very helpful.", "labels": [], "entities": []}, {"text": "The significant recall improvement from TS to SIM+LABEL shows that the corpus-based knowledge resources significantly reduce the data sparseness, compared to using features extracted from the triple store only.", "labels": [], "entities": [{"text": "recall", "start_pos": 16, "end_pos": 22, "type": "METRIC", "confidence": 0.998957633972168}]}, {"text": "The result of the phase 1 algorithm with SIM+LABEL is used as input for phase 2.", "labels": [], "entities": [{"text": "LABEL", "start_pos": 45, "end_pos": 50, "type": "METRIC", "confidence": 0.6885911226272583}]}, {"text": "Evaluating Type B relations The goal is 2-fold: 1) to evaluate the phase 2 algorithm.", "labels": [], "entities": []}, {"text": "This involves comparing system output to a gold standard constructed by hand, and reporting performance; 2) to evaluate the quality of Type B relations.", "labels": [], "entities": []}, {"text": "For this, we will also report triple-level precision.", "labels": [], "entities": [{"text": "precision", "start_pos": 43, "end_pos": 52, "type": "METRIC", "confidence": 0.9710374474525452}]}, {"text": "We construct a gold standard GS2 8 for evaluating Type B relations as follows: We randomly sampled 178 Type B relations, which contain 1547 Type A relations and more than 100,000 triples.", "labels": [], "entities": []}, {"text": "Since the number of triples is very large, it is infeasible for labelers to manually cluster triples to construct a gold standard.", "labels": [], "entities": []}, {"text": "To report precision, we asked the labelers to label each Type A relation contained in this Type B relation as good, fair or bad based on whether it expresses the same relation.", "labels": [], "entities": [{"text": "precision", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9993528723716736}]}, {"text": "For recall evaluation, we need to know how many Type A relations are missing from each Type B relation.", "labels": [], "entities": [{"text": "recall evaluation", "start_pos": 4, "end_pos": 21, "type": "TASK", "confidence": 0.8553881645202637}]}, {"text": "We provide the full data set of Type A relations along with three additional resources: 1) a tool which, given a Type A relation, returns a ranked list of similar Type A relations based on the pairwise relation similarity metric in section 4, 2) DIRT paraphrase collection, 3) WordNet) synsets.", "labels": [], "entities": [{"text": "DIRT paraphrase collection", "start_pos": 246, "end_pos": 272, "type": "TASK", "confidence": 0.5109195113182068}]}, {"text": "The labelers are asked to find similar phrases by checking phrases which contain synonyms of the tokens in the query phrase.", "labels": [], "entities": []}, {"text": "Given a Type B relation, ideally we expect the labelers to find all missing Type A relations using these resources.", "labels": [], "entities": []}, {"text": "We report precision (P) and recall (R) as follows.", "labels": [], "entities": [{"text": "precision (P)", "start_pos": 10, "end_pos": 23, "type": "METRIC", "confidence": 0.9453595876693726}, {"text": "recall (R)", "start_pos": 28, "end_pos": 38, "type": "METRIC", "confidence": 0.9440607279539108}]}, {"text": "Here \u00ed \u00b5\u00ed\u00b1 \u00ed \u00b5\u00ed\u00b0\u00b5 and \u00ed \u00b5\u00ed\u00b1 \u00ed \u00b5\u00ed\u00b0\u00b5 \u2032 represent Type B relations in the algorithm output and GS2, respectively.", "labels": [], "entities": [{"text": "GS2", "start_pos": 92, "end_pos": 95, "type": "DATASET", "confidence": 0.7710923552513123}]}, {"text": "\u00ed \u00b5\u00ed\u00b1 \u00ed \u00b5\u00ed\u00b0\u00e1nd \u00ed \u00b5\u00ed\u00b1 \u00ed \u00b5\u00ed\u00b0\u00b4\u2032 \u00b5\u00ed\u00b0\u00b4\u2032 represent Type A relations.", "labels": [], "entities": []}, {"text": "\u00ed \u00b5\u00ed\u00b1 (\u00ed \u00b5\u00ed\u00b1 \u00ed \u00b5\u00ed\u00b0\u00b4)\u00b5\u00ed\u00b0\u00b4) denotes the score of \u00ed \u00b5\u00ed\u00b1 \u00ed \u00b5\u00ed\u00b0\u00b4.\u00b5\u00ed\u00b0\u00b4.", "labels": [], "entities": [{"text": "\u00ed \u00b5\u00ed\u00b1 (\u00ed \u00b5\u00ed\u00b1 \u00ed \u00b5\u00ed\u00b0\u00b4)\u00b5\u00ed\u00b0\u00b4)", "start_pos": 0, "end_pos": 25, "type": "METRIC", "confidence": 0.6804573219269514}, {"text": "\u00b5\u00ed\u00b0\u00b4", "start_pos": 60, "end_pos": 64, "type": "METRIC", "confidence": 0.6723271409670512}]}, {"text": "It is set to 1.0, 0.5 and 0 for good, fair or bad respectively.", "labels": [], "entities": []}, {"text": "\u00ed \u00b5\u00ed\u00b1\u0083 = sampled triples.", "labels": [], "entities": [{"text": "\u00ed \u00b5\u00ed\u00b1\u0083", "start_pos": 0, "end_pos": 6, "type": "METRIC", "confidence": 0.8140360911687216}]}, {"text": "We use \u00ed \u00b5\u00ed\u00b1\u0083 \u00ed \u00b5\u00ed\u00b1\u0096\u00ed \u00b5\u00ed\u00b1\u009b\u00ed \u00b5\u00ed\u00b1 to represent the precision calculated based on labeled triples.", "labels": [], "entities": [{"text": "precision", "start_pos": 49, "end_pos": 58, "type": "METRIC", "confidence": 0.9987779259681702}]}, {"text": "Moreover, as we are interested in how many phrases are found by our algorithm, we also include \u00ed \u00b5\u00ed\u00b1 \u00ed \u00b5\u00ed\u00b1\u009d\u210e\u00ed \u00b5\u00ed\u00b1\u009f\u00ed \u00b5\u00ed\u00b1\u008e\u00ed \u00b5\u00ed\u00b1 \u00ed \u00b5\u00ed\u00b1\u0092 , which is the recall of synonymous phrases.", "labels": [], "entities": [{"text": "recall", "start_pos": 148, "end_pos": 154, "type": "METRIC", "confidence": 0.9379209876060486}]}, {"text": "Performance for Type B relation extraction.", "labels": [], "entities": [{"text": "Type B relation extraction", "start_pos": 16, "end_pos": 42, "type": "TASK", "confidence": 0.635467641055584}]}, {"text": "The first column shows the range of the maximum sizes of Type A relations in the Type B relation.", "labels": [], "entities": []}, {"text": "The last column shows the number of Type B relations that are in this range.", "labels": [], "entities": []}, {"text": "The number in parenthesis in the third column is the recall of phrases.", "labels": [], "entities": [{"text": "recall", "start_pos": 53, "end_pos": 59, "type": "METRIC", "confidence": 0.9827002882957458}]}, {"text": "The result shows that WEBRE can extract Type B relations at high precision (both P and \u00ed \u00b5\u00ed\u00b1\u0083 \u00ed \u00b5\u00ed\u00b1\u0096\u00ed \u00b5\u00ed\u00b1\u009b\u00ed \u00b5\u00ed\u00b1 ).", "labels": [], "entities": [{"text": "WEBRE", "start_pos": 22, "end_pos": 27, "type": "DATASET", "confidence": 0.7182130813598633}, {"text": "precision", "start_pos": 65, "end_pos": 74, "type": "METRIC", "confidence": 0.9941457509994507}]}, {"text": "The overall recall is 0.684.", "labels": [], "entities": [{"text": "recall", "start_pos": 12, "end_pos": 18, "type": "METRIC", "confidence": 0.999602735042572}]}, {"text": "also shows a trend that if the maximum number of Type A relation in the target Type B relation is larger, the recall is better.", "labels": [], "entities": [{"text": "recall", "start_pos": 110, "end_pos": 116, "type": "METRIC", "confidence": 0.9997294545173645}]}, {"text": "This shows that the recall of Type B relations depends on the amount of data available for that relation.", "labels": [], "entities": [{"text": "recall", "start_pos": 20, "end_pos": 26, "type": "METRIC", "confidence": 0.9955132603645325}]}, {"text": "Some examples of Type B relations extracted are shown in.", "labels": [], "entities": []}, {"text": "Comparison with SNE We compare WEBRE's extracted Type B relations to the relations extracted by its closest prior work SNE 9 . We found SNE is notable to handle the 14.7 million triples in a foreseeable amount of time, so we randomly sampled 1 million (1M) triples 10 and test both algorithms on this set.", "labels": [], "entities": []}, {"text": "We also filtered out result clusters which have only 1 or 2 triples from both system outputs.", "labels": [], "entities": []}, {"text": "For comparison purposes, we constructed a gold standard GS3 as follows: randomly select 30 clusters from both system outputs, and then find similar clusters from the other system output, followed by manually refining the clusters Obtained from alchemy.cs.washington.edu/papers/kok08 We found that SNE's runtime on 1M triples varies from several hours to over a week, depending on the parameters.", "labels": [], "entities": []}, {"text": "The best performance is achieved with runtime of approximately 3 days.", "labels": [], "entities": []}, {"text": "We also tried SNE with 2M triples, on which many runs take several days and show no sign of convergence.", "labels": [], "entities": []}, {"text": "For fairness, the comparison was done on 1M triples. by merging similar ones and splitting non-coherent clusters.", "labels": [], "entities": []}, {"text": "GS3 contains 742 triples and 135 clusters.", "labels": [], "entities": [{"text": "GS3", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.945724368095398}]}, {"text": "We report triple-level pairwise precision, recall and F1 for both algorithms against GS3, and report results in.", "labels": [], "entities": [{"text": "precision", "start_pos": 32, "end_pos": 41, "type": "METRIC", "confidence": 0.9641591906547546}, {"text": "recall", "start_pos": 43, "end_pos": 49, "type": "METRIC", "confidence": 0.9997172951698303}, {"text": "F1", "start_pos": 54, "end_pos": 56, "type": "METRIC", "confidence": 0.999100923538208}, {"text": "GS3", "start_pos": 85, "end_pos": 88, "type": "DATASET", "confidence": 0.9111146330833435}]}, {"text": "We fine-tuned SNE (using grid search, internal cross-validation, and coarse-to-fine parameter tuning), and report its best performance.", "labels": [], "entities": [{"text": "SNE", "start_pos": 14, "end_pos": 17, "type": "TASK", "confidence": 0.866580605506897}]}, {"text": "shows that WEBRE outperforms SNE significantly in pairwise recall while having similar precision.", "labels": [], "entities": [{"text": "WEBRE", "start_pos": 11, "end_pos": 16, "type": "METRIC", "confidence": 0.9708876609802246}, {"text": "SNE", "start_pos": 29, "end_pos": 32, "type": "TASK", "confidence": 0.7969279885292053}, {"text": "recall", "start_pos": 59, "end_pos": 65, "type": "METRIC", "confidence": 0.9683065414428711}, {"text": "precision", "start_pos": 87, "end_pos": 96, "type": "METRIC", "confidence": 0.9966173768043518}]}, {"text": "First, WEBRE makes use of several corpus-level semantic sources extracted from the corpus for clustering entities and phrases while SNE uses only features in the triple store.", "labels": [], "entities": [{"text": "WEBRE", "start_pos": 7, "end_pos": 12, "type": "DATASET", "confidence": 0.7917240262031555}]}, {"text": "These semantic resources significantly reduced data sparseness.", "labels": [], "entities": []}, {"text": "Examination of the output shows that SNE is unable to group many triples from the same generally-recognized fine-grained relations.", "labels": [], "entities": [{"text": "SNE", "start_pos": 37, "end_pos": 40, "type": "TASK", "confidence": 0.8806858658790588}]}, {"text": "For example, SNE placed relation instances <Barbara, grow up in, Santa Fe> and <John, be raised mostly in, Santa Barbara> into 2 different clusters because the arguments and phrases do not share features nor could be grouped by SNE's mutual clustering.", "labels": [], "entities": []}, {"text": "In contrast, WEBRE groups them together.", "labels": [], "entities": [{"text": "WEBRE", "start_pos": 13, "end_pos": 18, "type": "DATASET", "confidence": 0.8142583966255188}]}, {"text": "Second, SNE assumes a relation phrase to be in exactly one cluster.", "labels": [], "entities": [{"text": "SNE", "start_pos": 8, "end_pos": 11, "type": "TASK", "confidence": 0.9783908724784851}]}, {"text": "For example, SNE placed be part of in the phrase cluster be city of and failed to place it in another cluster be subsidiary of.", "labels": [], "entities": []}, {"text": "This limits SNE's ability to placing relation instances with polysemous phrases into correct relation clusters.", "labels": [], "entities": [{"text": "SNE", "start_pos": 12, "end_pos": 15, "type": "TASK", "confidence": 0.9690951704978943}]}, {"text": "It should be emphasized that we use pairwise precision and recall in table 4 to be consistent with the original SNE paper.", "labels": [], "entities": [{"text": "precision", "start_pos": 45, "end_pos": 54, "type": "METRIC", "confidence": 0.9739145040512085}, {"text": "recall", "start_pos": 59, "end_pos": 65, "type": "METRIC", "confidence": 0.9993194341659546}, {"text": "SNE paper", "start_pos": 112, "end_pos": 121, "type": "DATASET", "confidence": 0.7873086333274841}]}, {"text": "Pairwise metrics are much more sensitive than instance-level metrics, and penalize recall exponentially in the worst case if an algorithm incorrectly splits a coherent cluster; therefore the absolute pairwise recall difference Pairwise precision and recall are calculated on all pairs that are in the same cluster, thus are very sensitive.", "labels": [], "entities": [{"text": "recall", "start_pos": 83, "end_pos": 89, "type": "METRIC", "confidence": 0.9823810458183289}, {"text": "precision", "start_pos": 236, "end_pos": 245, "type": "METRIC", "confidence": 0.8821022510528564}, {"text": "recall", "start_pos": 250, "end_pos": 256, "type": "METRIC", "confidence": 0.9983599781990051}]}, {"text": "For example, if an algorithm incorrectly split a cluster of size N to a smaller main cluster of size N/2 and some constant-size clusters, pairwise recall could drop to as much as \u00bc of its original value.", "labels": [], "entities": [{"text": "recall", "start_pos": 147, "end_pos": 153, "type": "METRIC", "confidence": 0.983539342880249}]}, {"text": "should not be interpreted as the same as the instance-level recall reported in previous experiments.", "labels": [], "entities": [{"text": "recall", "start_pos": 60, "end_pos": 66, "type": "METRIC", "confidence": 0.8784644603729248}]}, {"text": "On 1 million triples, WEBRE generates 12179 triple clusters with an average size 12 of 13 while SNE generate 53270 clusters with an average size 5.1.", "labels": [], "entities": [{"text": "WEBRE", "start_pos": 22, "end_pos": 27, "type": "DATASET", "confidence": 0.8833433389663696}]}, {"text": "In consequence, pairwise recall drops significantly.", "labels": [], "entities": [{"text": "recall", "start_pos": 25, "end_pos": 31, "type": "METRIC", "confidence": 0.9939340949058533}]}, {"text": "Nonetheless, at above 80% pairwise precision, it demonstrates that WEBRE can group more related triples by adding rich semantics harvested from the web and employing a more general treatment of polysemous relation phrases.", "labels": [], "entities": [{"text": "precision", "start_pos": 35, "end_pos": 44, "type": "METRIC", "confidence": 0.9802908897399902}, {"text": "WEBRE", "start_pos": 67, "end_pos": 72, "type": "DATASET", "confidence": 0.6739588379859924}]}, {"text": "On 1M triples, WEBRE finished in 40 minutes, while the run time of SNE varies from 3 hours to a few days.", "labels": [], "entities": [{"text": "1M triples", "start_pos": 3, "end_pos": 13, "type": "TASK", "confidence": 0.5549940913915634}, {"text": "WEBRE", "start_pos": 15, "end_pos": 20, "type": "METRIC", "confidence": 0.5477924942970276}]}], "tableCaptions": [{"text": " Table 1. Phase 1 performance (averaged on multiple runs) of  the 4 methods. The highest performance numbers are in bold.  (The number in parenthesis is the micro-average when empty- result relation phrases are not considered for the method).", "labels": [], "entities": []}, {"text": " Table 2. Performance for Type B relation extraction. The first  column shows the range of the maximum sizes of Type A  relations in the Type B relation. The last column shows the  number of Type B relations that are in this range. The number  in parenthesis in the third column is the recall of phrases.", "labels": [], "entities": [{"text": "Type B relation extraction", "start_pos": 26, "end_pos": 52, "type": "TASK", "confidence": 0.8024625033140182}, {"text": "recall", "start_pos": 286, "end_pos": 292, "type": "METRIC", "confidence": 0.9869486093521118}]}, {"text": " Table 4. We fine-tuned SNE (using grid  search, internal cross-validation, and coarse-to-fine  parameter tuning), and report its best performance.", "labels": [], "entities": [{"text": "SNE", "start_pos": 24, "end_pos": 27, "type": "TASK", "confidence": 0.9299308657646179}]}, {"text": " Table 4. Pairwise precision/recall/F1 of WEBRE and SNE.", "labels": [], "entities": [{"text": "precision", "start_pos": 19, "end_pos": 28, "type": "METRIC", "confidence": 0.9345585703849792}, {"text": "recall", "start_pos": 29, "end_pos": 35, "type": "METRIC", "confidence": 0.9590548276901245}, {"text": "F1", "start_pos": 36, "end_pos": 38, "type": "METRIC", "confidence": 0.9796722531318665}, {"text": "WEBRE", "start_pos": 42, "end_pos": 47, "type": "DATASET", "confidence": 0.8143070936203003}]}]}