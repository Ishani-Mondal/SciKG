{"title": [{"text": "A Beam-Search Decoder for Grammatical Error Correction", "labels": [], "entities": [{"text": "Grammatical Error Correction", "start_pos": 26, "end_pos": 54, "type": "TASK", "confidence": 0.7241271535555521}]}], "abstractContent": [{"text": "We present a novel beam-search decoder for grammatical error correction.", "labels": [], "entities": [{"text": "grammatical error correction", "start_pos": 43, "end_pos": 71, "type": "TASK", "confidence": 0.7464380462964376}]}, {"text": "The decoder iteratively generates new hypothesis corrections from current hypotheses and scores them based on features of grammatical correctness and fluency.", "labels": [], "entities": []}, {"text": "These features include scores from discriminative classifiers for specific error categories, such as articles and prepositions.", "labels": [], "entities": []}, {"text": "Unlike all previous approaches, our method is able to perform correction of whole sentences with multiple and interacting errors while still taking advantage of powerful existing classifier approaches.", "labels": [], "entities": []}, {"text": "Our decoder achieves an F 1 correction score significantly higher than all previous published scores on the Helping Our Own (HOO) shared task data set.", "labels": [], "entities": [{"text": "F 1 correction score", "start_pos": 24, "end_pos": 44, "type": "METRIC", "confidence": 0.95149265229702}, {"text": "Helping Our Own (HOO) shared task data set", "start_pos": 108, "end_pos": 150, "type": "DATASET", "confidence": 0.6938357025384903}]}], "introductionContent": [{"text": "Grammatical error correction is an important problem in natural language processing (NLP) that has attracted an increasing amount of interest over the last few years.", "labels": [], "entities": [{"text": "Grammatical error correction", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.8542130390803019}, {"text": "natural language processing (NLP)", "start_pos": 56, "end_pos": 89, "type": "TASK", "confidence": 0.818613717953364}]}, {"text": "Grammatical error correction promises to provide instantaneous accurate feedback to language learners, e.g., learners of English as a Second Language (ESL).", "labels": [], "entities": [{"text": "Grammatical error correction", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.7274503111839294}, {"text": "learners of English as a Second Language (ESL)", "start_pos": 109, "end_pos": 155, "type": "TASK", "confidence": 0.649056687951088}]}, {"text": "The dominant paradigm that underlies most error correction systems to date is multi-class classification.", "labels": [], "entities": [{"text": "error correction", "start_pos": 42, "end_pos": 58, "type": "TASK", "confidence": 0.6807477027177811}, {"text": "multi-class classification", "start_pos": 78, "end_pos": 104, "type": "TASK", "confidence": 0.7604883313179016}]}, {"text": "A classifier is trained to predict a word from a confusion set of possible correction choices, given some feature representation of the surrounding sentence context.", "labels": [], "entities": []}, {"text": "During testing, the classifier predicts the most likely correction for each test instance.", "labels": [], "entities": []}, {"text": "If the prediction differs from the observed word used by the writer and the classifier is sufficiently confident in its prediction, the observed word is replaced by the prediction.", "labels": [], "entities": []}, {"text": "Although considerable progress has been made, the classification approach suffers from some serious shortcomings.", "labels": [], "entities": [{"text": "classification", "start_pos": 50, "end_pos": 64, "type": "TASK", "confidence": 0.9758524298667908}]}, {"text": "Each classifier corrects a single word fora specific error category individually.", "labels": [], "entities": []}, {"text": "This ignores dependencies between the words in a sentence.", "labels": [], "entities": []}, {"text": "Also, by conditioning on the surrounding context, the classifier implicitly assumes that the surrounding context is free of grammatical errors, which is often not the case.", "labels": [], "entities": []}, {"text": "Finally, the classifier typically has to commit to a single onebest prediction and is notable to change its decision later or explore multiple corrections.", "labels": [], "entities": []}, {"text": "Instead of correcting each word individually, we would like to perform global inference over corrections of whole sentences which can contain multiple and interacting errors.", "labels": [], "entities": []}, {"text": "An alternative paradigm is to view error correction as a statistical machine translation (SMT) problem from \"bad\" to \"good\" English.", "labels": [], "entities": [{"text": "error correction", "start_pos": 35, "end_pos": 51, "type": "TASK", "confidence": 0.6912276595830917}, {"text": "statistical machine translation (SMT)", "start_pos": 57, "end_pos": 94, "type": "TASK", "confidence": 0.7672883023818334}]}, {"text": "While this approach can naturally correct whole sentences, a standard SMT system cannot easily incorporate models for specific grammatical errors.", "labels": [], "entities": [{"text": "SMT", "start_pos": 70, "end_pos": 73, "type": "TASK", "confidence": 0.9929157495498657}]}, {"text": "It also suffers from the paucity of error-annotated training data for grammar correction.", "labels": [], "entities": [{"text": "grammar correction", "start_pos": 70, "end_pos": 88, "type": "TASK", "confidence": 0.8884168267250061}]}, {"text": "As a result, applying a standard SMT system to error correction does not produce good results, as we show in this work.", "labels": [], "entities": [{"text": "SMT", "start_pos": 33, "end_pos": 36, "type": "TASK", "confidence": 0.9948917627334595}, {"text": "error correction", "start_pos": 47, "end_pos": 63, "type": "TASK", "confidence": 0.7649362683296204}]}, {"text": "In this work, we present a novel beam-search decoder for grammatical error correction that combines the advantages of the classification approach and the SMT approach.", "labels": [], "entities": [{"text": "grammatical error correction", "start_pos": 57, "end_pos": 85, "type": "TASK", "confidence": 0.762391726175944}, {"text": "SMT", "start_pos": 154, "end_pos": 157, "type": "TASK", "confidence": 0.98848557472229}]}, {"text": "Starting from the original input sentence, the decoder performs an iterative search over possible sentence-level hypotheses to find the best sentence-level correction.", "labels": [], "entities": []}, {"text": "In each iteration, a set of proposers generates new hypotheses by making incremental changes to the hypotheses found so far.", "labels": [], "entities": []}, {"text": "A set of experts scores the new hypotheses on criteria of grammatical correctness.", "labels": [], "entities": []}, {"text": "These experts include discriminative classifiers for specific error categories, such as articles and prepositions.", "labels": [], "entities": []}, {"text": "The decoder model calculates the overall hypothesis score for each hypothesis as a linear combination of the expert scores.", "labels": [], "entities": []}, {"text": "The weights of the decoder model are discriminatively trained on a development set of error-annotated sentences.", "labels": [], "entities": []}, {"text": "The highest scoring hypotheses are kept in the search beam for the next iteration.", "labels": [], "entities": []}, {"text": "This search procedure continues until the beam is empty or the maximum number of iterations has been reached.", "labels": [], "entities": []}, {"text": "The highest scoring hypothesis is returned as the sentence-level correction.", "labels": [], "entities": []}, {"text": "We evaluate our proposed decoder in the context of the Helping Our Own (HOO) shared task on grammatical error correction (.", "labels": [], "entities": [{"text": "grammatical error correction", "start_pos": 92, "end_pos": 120, "type": "TASK", "confidence": 0.5865171054999033}]}, {"text": "Our decoder achieves an F 1 score of 25.48% which improves upon the current state of the art.", "labels": [], "entities": [{"text": "F 1 score", "start_pos": 24, "end_pos": 33, "type": "METRIC", "confidence": 0.9911365111668905}]}, {"text": "The remainder of this paper is organized as follows.", "labels": [], "entities": []}, {"text": "The next section gives an overview of related work.", "labels": [], "entities": []}, {"text": "Section 3 describes the proposed beam-search decoder.", "labels": [], "entities": []}, {"text": "Sections 4 and 5 describe the experimental setup and results, respectively.", "labels": [], "entities": []}, {"text": "Section 6 provides further discussion.", "labels": [], "entities": []}, {"text": "Section 7 concludes the paper.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluate our decoder in the context of the HOO shared task on grammatical error correction.", "labels": [], "entities": [{"text": "grammatical error correction", "start_pos": 65, "end_pos": 93, "type": "TASK", "confidence": 0.5590569873650869}]}, {"text": "The goal of the task is to automatically correct errors in academic papers from NLP.", "labels": [], "entities": [{"text": "correct errors in academic papers from NLP", "start_pos": 41, "end_pos": 83, "type": "TASK", "confidence": 0.5709051447255271}]}, {"text": "The readers can refer to the overview paper () for details.", "labels": [], "entities": []}, {"text": "We compare our proposed method with two baselines: a phrase-based SMT system (described in Section 4.3) and a pipeline of classifiers (described in Section 4.4).", "labels": [], "entities": [{"text": "SMT", "start_pos": 66, "end_pos": 69, "type": "TASK", "confidence": 0.8298830986022949}]}, {"text": "We evaluate performance by computing precision, recall, and F 1 correction score without bonus as defined in the official HOO report (Dale and Kilgarriff, 2011) 3 . F 1 correction score is simply the F 1 measure (van) between the corrections (called edits in HOO) proposed by a system and the gold-standard corrections.", "labels": [], "entities": [{"text": "precision", "start_pos": 37, "end_pos": 46, "type": "METRIC", "confidence": 0.999488115310669}, {"text": "recall", "start_pos": 48, "end_pos": 54, "type": "METRIC", "confidence": 0.9996190071105957}, {"text": "F 1 correction score", "start_pos": 60, "end_pos": 80, "type": "METRIC", "confidence": 0.9416831135749817}, {"text": "HOO report (Dale and Kilgarriff, 2011)", "start_pos": 122, "end_pos": 160, "type": "DATASET", "confidence": 0.8628858129183451}, {"text": "F 1 correction score", "start_pos": 165, "end_pos": 185, "type": "METRIC", "confidence": 0.9660148471593857}, {"text": "F 1 measure (van)", "start_pos": 200, "end_pos": 217, "type": "METRIC", "confidence": 0.9694532255331675}]}, {"text": "Let {e 1 , . .", "labels": [], "entities": []}, {"text": ", en } be a set of test sentences and let {g 1 , . .", "labels": [], "entities": []}, {"text": ", g n } be the set of gold-standard edits for the sentences.", "labels": [], "entities": []}, {"text": "Let {h 1 , . .", "labels": [], "entities": []}, {"text": ", h n } be the set of corrected sentences output by a system.", "labels": [], "entities": []}, {"text": "One difficulty in the evaluation is that the set of system edits {d 1 , . .", "labels": [], "entities": []}, {"text": ", d n } between the test sentences and the system outputs is ambiguous.", "labels": [], "entities": []}, {"text": "For example, assume that the original test sentence is The data is similar with test set., the system output is The data is similar to the test set., and the gold-standard edits are two corrections with \u2192 to, \u2192 the that change with to to and insert the before test set.", "labels": [], "entities": []}, {"text": "The official HOO scorer however extracts a single system edit with \u2192 to the for this instance.", "labels": [], "entities": [{"text": "HOO scorer", "start_pos": 13, "end_pos": 23, "type": "DATASET", "confidence": 0.9371912479400635}, {"text": "\u2192", "start_pos": 67, "end_pos": 68, "type": "METRIC", "confidence": 0.955043375492096}]}, {"text": "As the extracted system edit is different from the gold-standard edits, the system would be considered wrong, although it proposes the exact same corrected sentence as the gold standard edits.", "labels": [], "entities": []}, {"text": "This problem has also been recognized by the HOO shared task organizers (see), Section 5).", "labels": [], "entities": [{"text": "HOO shared task organizers", "start_pos": 45, "end_pos": 71, "type": "TASK", "confidence": 0.5167362913489342}]}, {"text": "Our MaxMatch (M 2 ) scorer (Dahlmeier and Ng, 2012) overcomes this problem through an efficient algorithm that computes the set of system edits which has the maximum overlap with the goldstandard edits.", "labels": [], "entities": []}, {"text": "We use the M 2 scorer as the main evaluation metric in our experiments.", "labels": [], "entities": [{"text": "M 2 scorer", "start_pos": 11, "end_pos": 21, "type": "METRIC", "confidence": 0.9157530069351196}]}, {"text": "Additionally, we also report results with the official HOO scorer.", "labels": [], "entities": [{"text": "HOO scorer", "start_pos": 55, "end_pos": 65, "type": "DATASET", "confidence": 0.5627399682998657}]}, {"text": "Once the set of system edits is extracted, precision, recall, and F 1 measure are computed as follows.", "labels": [], "entities": [{"text": "precision", "start_pos": 43, "end_pos": 52, "type": "METRIC", "confidence": 0.9997689127922058}, {"text": "recall", "start_pos": 54, "end_pos": 60, "type": "METRIC", "confidence": 0.9997265934944153}, {"text": "F 1 measure", "start_pos": 66, "end_pos": 77, "type": "METRIC", "confidence": 0.9902631441752116}]}, {"text": "We note that the M 2 scorer and the HOO scorer adhere to the same score definition and only differ in the way the system edits are computed.", "labels": [], "entities": [{"text": "M 2 scorer", "start_pos": 17, "end_pos": 27, "type": "METRIC", "confidence": 0.7520273725191752}, {"text": "HOO scorer", "start_pos": 36, "end_pos": 46, "type": "METRIC", "confidence": 0.9248099029064178}]}, {"text": "For statistical significance testing, we use sign-test with bootstrap re-sampling) with 1,000 samples.", "labels": [], "entities": [{"text": "statistical significance testing", "start_pos": 4, "end_pos": 36, "type": "TASK", "confidence": 0.7934075991312662}]}], "tableCaptions": [{"text": " Table 1: Overview of the data sets.", "labels": [], "entities": []}, {"text": " Table 2: Experimental results on HOO-TEST. Precision, recall, and F 1 score are shown in percent. The best F 1 score  for each system is highlighted in bold. Statistically significant improvements (p < 0.01) over the pipeline baseline are  marked with an asterisk ( * ). Statistically significant improvements over the UI Run1 system are marked with a dagger  ( \u2020). All improvements of the pipeline and the decoder over the SMT baseline are statistically significant.", "labels": [], "entities": [{"text": "Precision", "start_pos": 44, "end_pos": 53, "type": "METRIC", "confidence": 0.9991891980171204}, {"text": "recall", "start_pos": 55, "end_pos": 61, "type": "METRIC", "confidence": 0.9985633492469788}, {"text": "F 1 score", "start_pos": 67, "end_pos": 76, "type": "METRIC", "confidence": 0.9916858275731405}, {"text": "F 1 score", "start_pos": 108, "end_pos": 117, "type": "METRIC", "confidence": 0.9310494860013326}, {"text": "UI Run1 system", "start_pos": 320, "end_pos": 334, "type": "DATASET", "confidence": 0.8609336217244467}, {"text": "SMT baseline", "start_pos": 425, "end_pos": 437, "type": "DATASET", "confidence": 0.8674890100955963}]}, {"text": " Table 3: PRO tuning of the full decoder model on HOO-", "labels": [], "entities": [{"text": "PRO tuning", "start_pos": 10, "end_pos": 20, "type": "TASK", "confidence": 0.8986805379390717}, {"text": "HOO", "start_pos": 50, "end_pos": 53, "type": "DATASET", "confidence": 0.882784903049469}]}, {"text": " Table 4: Example of PRO-tuned weights for article cor- rection count features for the full decoder model.", "labels": [], "entities": []}]}