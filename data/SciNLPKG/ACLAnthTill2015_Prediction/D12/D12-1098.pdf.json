{"title": [{"text": "Fast Large-Scale Approximate Graph Construction for NLP", "labels": [], "entities": [{"text": "NLP", "start_pos": 52, "end_pos": 55, "type": "TASK", "confidence": 0.6234058737754822}]}], "abstractContent": [{"text": "Many natural language processing problems involve constructing large nearest-neighbor graphs.", "labels": [], "entities": []}, {"text": "We propose a system called FLAG to construct such graphs approximately from large data sets.", "labels": [], "entities": []}, {"text": "To handle the large amount of data, our algorithm maintains approximate counts based on sketching algorithms.", "labels": [], "entities": []}, {"text": "To find the approximate nearest neighbors, our algorithm pairs anew distributed online-PMI algorithm with novel fast approximate nearest neighbor search algorithms (variants of PLEB).", "labels": [], "entities": []}, {"text": "These algorithms return the approximate nearest neighbors quickly.", "labels": [], "entities": []}, {"text": "We show our system's efficiency in both intrinsic and ex-trinsic experiments.", "labels": [], "entities": []}, {"text": "We further evaluate our fast search algorithms both quantitatively and qualitatively on two NLP applications.", "labels": [], "entities": []}], "introductionContent": [{"text": "Many natural language processing (NLP) problems involve graph construction.", "labels": [], "entities": [{"text": "natural language processing (NLP)", "start_pos": 5, "end_pos": 38, "type": "TASK", "confidence": 0.7851599156856537}, {"text": "graph construction", "start_pos": 56, "end_pos": 74, "type": "TASK", "confidence": 0.758078008890152}]}, {"text": "Examples include constructing polarity lexicons based on lexical graphs from WordNet (, constructing polarity lexicons from web data () and unsupervised part-ofspeech tagging using label propagation ().", "labels": [], "entities": [{"text": "WordNet", "start_pos": 77, "end_pos": 84, "type": "DATASET", "confidence": 0.962279736995697}, {"text": "part-ofspeech tagging", "start_pos": 153, "end_pos": 174, "type": "TASK", "confidence": 0.6785494983196259}]}, {"text": "The later two approaches construct nearest-neighbor graphs between word pairs by computing nearest neighbors between word pairs from large corpora.", "labels": [], "entities": []}, {"text": "These nearest neighbors form the edges of the graph, with weights given by the distributional similarity between terms.", "labels": [], "entities": []}, {"text": "Unfortunately, computing the distributional similarity between all words in a large vocabulary is computationally and memory intensive when working with large amounts of data (.", "labels": [], "entities": []}, {"text": "This bottleneck is typically addressed by means of commodity clusters.", "labels": [], "entities": []}, {"text": "For example, compute distributional similarity between 500 million terms over a 200 billion words in 50 hours using 100 quad-core nodes, explicitly storing a similarity matrix between 500 million terms.", "labels": [], "entities": []}, {"text": "In this work, we propose Fast Large-Scale Approximate Graph (FLAG) construction, a system that constructs a fast large-scale approximate nearest-neighbor graph from a large text corpus.", "labels": [], "entities": [{"text": "Fast Large-Scale Approximate Graph (FLAG) construction", "start_pos": 25, "end_pos": 79, "type": "METRIC", "confidence": 0.7388063184916973}]}, {"text": "To build this system, we exploit recent developments in the area of approximation, randomization and streaming for large-scale NLP problems).", "labels": [], "entities": []}, {"text": "More specifically we exploit work on Locality Sensitive Hashing (LSH)) for computing word-pair similarities from large text collections (; Van Durme and Lall, 2010).", "labels": [], "entities": [{"text": "Locality Sensitive Hashing (LSH))", "start_pos": 37, "end_pos": 70, "type": "TASK", "confidence": 0.7568385551373163}]}, {"text": "However, approach stored an enormous matrix of all unique words and their contexts in main memory, which is infeasible for very large data sets.", "labels": [], "entities": []}, {"text": "A more efficient online framework to locality sensitive hashing (Van Durme and Lall, 2010; Van Durme and Lall, 2011) computes distributional similarity in a streaming setting.", "labels": [], "entities": []}, {"text": "Unfortunately, their approach can handle only additive features like raw-counts, and not non-linear association scores like pointwise mutual information (PMI), which generates better context vectors for distributional similarity.", "labels": [], "entities": []}, {"text": "In FLAG, we first propose a novel distributed online-PMI algorithm (Section 3.1).", "labels": [], "entities": [{"text": "FLAG", "start_pos": 3, "end_pos": 7, "type": "DATASET", "confidence": 0.8103446364402771}]}, {"text": "It is a streaming method that processes large data sets in one pass while distributing the data over commodity clusters and returns context vectors weighted by pointwise mutual information (PMI) for all the words.", "labels": [], "entities": []}, {"text": "Our distributed online-PMI algorithm makes use of the Count-Min (CM) sketch algorithm () (previously shown effective for computing distributional similarity in our earlier work) to store the counts of all words, contexts and word-context pairs using only 8GB of main memory.", "labels": [], "entities": []}, {"text": "The main motivation for using the CM sketch comes from its linearity property (see last paragraph of Section 2) which makes CM sketch to be implemented in distributed setting for large data sets.", "labels": [], "entities": []}, {"text": "In our implementation, FLAG scaled up to 110 GB of web data with 866 million sentences in less than 2 days using 100 quadcore nodes.", "labels": [], "entities": [{"text": "FLAG", "start_pos": 23, "end_pos": 27, "type": "TASK", "confidence": 0.5539981126785278}]}, {"text": "Our intrinsic and extrinsic experiments demonstrate the effectiveness of distributed online-PMI.", "labels": [], "entities": []}, {"text": "After generating context vectors from distributed online-PMI algorithm, our goal is to use them to find fast approximate nearest neighbors for all words.", "labels": [], "entities": []}, {"text": "To achieve this goal, we exploit recent developments in the area of existing randomized algorithms for random projections), Locality Sensitive Hashing (LSH) and improve on previous work done on PLEB (Point Location in Equal Balls)).", "labels": [], "entities": []}, {"text": "We propose novel variants of PLEB to address the issue of reducing the pre-processing time for PLEB.", "labels": [], "entities": []}, {"text": "One of the variants of PLEB (FAST-PLEB) with considerably less pre-processing time has effectiveness comparable to PLEB.", "labels": [], "entities": [{"text": "PLEB", "start_pos": 23, "end_pos": 27, "type": "METRIC", "confidence": 0.9478858709335327}, {"text": "FAST-PLEB", "start_pos": 29, "end_pos": 38, "type": "METRIC", "confidence": 0.6212363839149475}, {"text": "PLEB", "start_pos": 115, "end_pos": 119, "type": "METRIC", "confidence": 0.7977943420410156}]}, {"text": "We evaluate these variants of PLEB both quantitatively and qualitatively on large data sets.", "labels": [], "entities": []}, {"text": "Finally, we show the applicability of large-scale graphs built from FLAG on two applications: the Google-Sets problem, and learning concrete and abstract words).", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluate our system FLAG for fast large-scale approximate graph construction.", "labels": [], "entities": [{"text": "FLAG", "start_pos": 23, "end_pos": 27, "type": "METRIC", "confidence": 0.9614251852035522}, {"text": "approximate graph construction", "start_pos": 49, "end_pos": 79, "type": "TASK", "confidence": 0.7077635924021403}]}, {"text": "First, we show that using distributed online-PMI algorithm is as effective as offline-PMI.", "labels": [], "entities": []}, {"text": "Second, we compare the approximate nearest neighbors lists generated by FLAG against the exact nearest neighbor lists.", "labels": [], "entities": [{"text": "FLAG", "start_pos": 72, "end_pos": 76, "type": "DATASET", "confidence": 0.7106264233589172}]}, {"text": "Finally, we show the quality of our approximate similarity lists generated by FLAG from the web corpus.", "labels": [], "entities": [{"text": "FLAG from the web corpus", "start_pos": 78, "end_pos": 102, "type": "DATASET", "confidence": 0.7839662551879882}]}, {"text": "Data sets: We use two data sets: Gigaword (Graff, 2003) and a copy of news web).", "labels": [], "entities": [{"text": "Gigaword (Graff, 2003)", "start_pos": 33, "end_pos": 55, "type": "DATASET", "confidence": 0.8571337461471558}]}, {"text": "For both the corpora, we split the text into sentences, tokenize and convert into lower-case.", "labels": [], "entities": []}, {"text": "To evaluate our approximate graph construction, we evaluate on three data sets: Gigaword (GW), Gigaword + 50% of web data (GWB50) and Gigaword + 100% ((GWB100)) of web data.", "labels": [], "entities": [{"text": "approximate graph construction", "start_pos": 16, "end_pos": 46, "type": "TASK", "confidence": 0.7146503329277039}]}, {"text": "Corpus statistics are shown in.", "labels": [], "entities": [{"text": "Corpus", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.7955927848815918}]}, {"text": "We define the context fora given word \"z\" as the surrounding words appearing in a window of 2 words to the left and 2 words to the right.", "labels": [], "entities": []}, {"text": "The context words are concatenated along with their positions -2, -1, +1, and +2.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1. We define the context for a  given word \"z\" as the surrounding words appearing  in a window of 2 words to the left and 2 words to  the right. The context words are concatenated along  with their positions -2, -1, +1, and +2.", "labels": [], "entities": []}, {"text": " Table 2: Evaluating word pairs ranking with online and offline", "labels": [], "entities": [{"text": "Evaluating word pairs ranking", "start_pos": 10, "end_pos": 39, "type": "TASK", "confidence": 0.8160716593265533}]}, {"text": " Table 3: Evaluation results on comparing LSH, IRP, PLEB, and FAST-PLEB with k = 3000 and b = {20, 30, 40, 50, 100} with", "labels": [], "entities": [{"text": "LSH", "start_pos": 42, "end_pos": 45, "type": "METRIC", "confidence": 0.592774510383606}, {"text": "IRP", "start_pos": 47, "end_pos": 50, "type": "METRIC", "confidence": 0.634672224521637}, {"text": "PLEB", "start_pos": 52, "end_pos": 56, "type": "METRIC", "confidence": 0.9763433337211609}, {"text": "FAST-PLEB", "start_pos": 62, "end_pos": 71, "type": "METRIC", "confidence": 0.8166619539260864}]}, {"text": " Table 5: Evaluation results on comparing LSH, IRP, PLEB, and FAST-PLEB with k = 3000, b = 40, p = 1000 and q = 10 with", "labels": [], "entities": [{"text": "LSH", "start_pos": 42, "end_pos": 45, "type": "METRIC", "confidence": 0.6023724675178528}, {"text": "IRP", "start_pos": 47, "end_pos": 50, "type": "METRIC", "confidence": 0.6711496114730835}, {"text": "PLEB", "start_pos": 52, "end_pos": 56, "type": "METRIC", "confidence": 0.9702229499816895}, {"text": "FAST-PLEB", "start_pos": 62, "end_pos": 71, "type": "METRIC", "confidence": 0.8223826289176941}]}]}