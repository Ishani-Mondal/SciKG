{"title": [{"text": "Locally Training the Log-Linear Model for SMT", "labels": [], "entities": [{"text": "SMT", "start_pos": 42, "end_pos": 45, "type": "TASK", "confidence": 0.990206778049469}]}], "abstractContent": [{"text": "In statistical machine translation, minimum error rate training (MERT) is a standard method for tuning a single weight with regard to a given development data.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 3, "end_pos": 34, "type": "TASK", "confidence": 0.6336694459120432}, {"text": "minimum error rate training (MERT", "start_pos": 36, "end_pos": 69, "type": "METRIC", "confidence": 0.8390531341234843}]}, {"text": "However, due to the diversity and uneven distribution of source sentences, there are two problems suffered by this method.", "labels": [], "entities": []}, {"text": "First, its performance is highly dependent on the choice of a development set, which may lead to an unstable performance for testing.", "labels": [], "entities": []}, {"text": "Second, translations become inconsistent at the sentence level since tuning is performed globally on a document level.", "labels": [], "entities": [{"text": "translations", "start_pos": 8, "end_pos": 20, "type": "TASK", "confidence": 0.9622405171394348}]}, {"text": "In this paper, we propose a novel local training method to address these two problems.", "labels": [], "entities": []}, {"text": "Unlike a global training method, such as MERT, in which a single weight is learned and used for all the input sentences, we perform training and testing in one step by learning a sentence-wise weight for each input sentence.", "labels": [], "entities": []}, {"text": "We propose efficient incremental training methods to put the local training into practice.", "labels": [], "entities": []}, {"text": "In NIST Chinese-to-English translation tasks, our local training method significantly outperforms MERT with the maximal improvements up to 2.0 BLEU points, meanwhile its efficiency is comparable to that of the global method.", "labels": [], "entities": [{"text": "NIST Chinese-to-English translation tasks", "start_pos": 3, "end_pos": 44, "type": "TASK", "confidence": 0.7442848980426788}, {"text": "MERT", "start_pos": 98, "end_pos": 102, "type": "METRIC", "confidence": 0.8862976431846619}, {"text": "BLEU", "start_pos": 143, "end_pos": 147, "type": "METRIC", "confidence": 0.9992721676826477}]}], "introductionContent": [{"text": "Och and introduced the log-linear model for statistical machine translation (SMT), in which translation is considered as the following optimization problem: where f and e (e ) are source and target sentences, respectively.", "labels": [], "entities": [{"text": "statistical machine translation (SMT)", "start_pos": 44, "end_pos": 81, "type": "TASK", "confidence": 0.8100406328837076}]}, {"text": "h is a feature vector which is scaled by a weight W . Parameter estimation is one of the most important components in SMT, and various training methods have been proposed to tune W . Some methods are based on likelihood, error rate), margin () and ranking (, and among which minimum error rate training (MERT) is the most popular one.", "labels": [], "entities": [{"text": "Parameter estimation", "start_pos": 54, "end_pos": 74, "type": "TASK", "confidence": 0.8680959045886993}, {"text": "SMT", "start_pos": 118, "end_pos": 121, "type": "TASK", "confidence": 0.9962639212608337}, {"text": "error rate", "start_pos": 221, "end_pos": 231, "type": "METRIC", "confidence": 0.8772639632225037}, {"text": "margin", "start_pos": 234, "end_pos": 240, "type": "METRIC", "confidence": 0.9941230416297913}, {"text": "minimum error rate training (MERT)", "start_pos": 275, "end_pos": 309, "type": "METRIC", "confidence": 0.8323631882667542}]}, {"text": "All these training methods follow the same pipeline: they train only a single weight on a given development set, and then use it to translate all the sentences in a test set.", "labels": [], "entities": []}, {"text": "We call them a global training method.", "labels": [], "entities": []}, {"text": "One of its advantages is that it allows us to train a single weight offline and thereby it is efficient.", "labels": [], "entities": []}, {"text": "However, due to the diversity and uneven distribution of source sentences(, there are some shortcomings in this pipeline.", "labels": [], "entities": []}, {"text": "Firstly, on the document level, the performance of these methods is dependent on the choice of a development set, which may potentially lead to an unstable translation performance for testing.", "labels": [], "entities": []}, {"text": "As referred in our experiment, the BLEU points on NIST08 are . The nonlinearly separable classification problem transformed from (a) via tuning as ranking ().", "labels": [], "entities": [{"text": "BLEU", "start_pos": 35, "end_pos": 39, "type": "METRIC", "confidence": 0.9990971088409424}, {"text": "NIST08", "start_pos": 50, "end_pos": 56, "type": "DATASET", "confidence": 0.982357382774353}]}, {"text": "Since score of e 11 is greater than that of e 12 , 1, 0 corresponds to a possitive example denoted as \"\u2022\", and \u22121, 0 corresponds to a negative example denoted as \"*\".", "labels": [], "entities": []}, {"text": "Since the transformed classification problem is not linearly separable, there does not exist a single weight which can obtain e 11 and e 21 as translation results meanwhile.", "labels": [], "entities": [{"text": "transformed classification", "start_pos": 10, "end_pos": 36, "type": "TASK", "confidence": 0.7003217488527298}]}, {"text": "However, one can obtain e 11 and e 21 with weights: 1, 1 and \u22121, 1, respectively.", "labels": [], "entities": []}, {"text": "19.04 when the Moses system is tuned on NIST02 by MERT.", "labels": [], "entities": [{"text": "NIST02", "start_pos": 40, "end_pos": 46, "type": "DATASET", "confidence": 0.9804093837738037}, {"text": "MERT", "start_pos": 50, "end_pos": 54, "type": "DATASET", "confidence": 0.787746250629425}]}, {"text": "However, its performance is improved to 21.28 points when tuned on NIST06.", "labels": [], "entities": [{"text": "NIST06", "start_pos": 67, "end_pos": 73, "type": "DATASET", "confidence": 0.9903207421302795}]}, {"text": "The automatic selection of a development set may partially address the problem.", "labels": [], "entities": []}, {"text": "However it is inefficient since tuning requires iteratively decoding an entire development set, which is impractical for an online service.", "labels": [], "entities": []}, {"text": "Secondly, translation becomes inconsistent on the sentence level.", "labels": [], "entities": [{"text": "translation", "start_pos": 10, "end_pos": 21, "type": "TASK", "confidence": 0.970690131187439}]}, {"text": "Global training method such as MERT tries to optimize the weight towards the best performance for the whole set, and it cannot necessarily always obtain good translation for every sentence in the development set.", "labels": [], "entities": [{"text": "MERT", "start_pos": 31, "end_pos": 35, "type": "DATASET", "confidence": 0.43669140338897705}]}, {"text": "The reason is that different sentences may need different optimal weights, and MERT cannot find a single weight to satisfy all of the sentences.", "labels": [], "entities": [{"text": "MERT", "start_pos": 79, "end_pos": 83, "type": "METRIC", "confidence": 0.49960625171661377}]}, {"text": "shows such an example, in which a development set contains two sentences f 1 and f 2 with translations e and feature vectors h.", "labels": [], "entities": []}, {"text": "When we tune examples in(a) by MERT, it can be regarded as a nonlinearly separable classification problem illustrated in.", "labels": [], "entities": [{"text": "MERT", "start_pos": 31, "end_pos": 35, "type": "METRIC", "confidence": 0.7279902100563049}]}, {"text": "Therefore, there exists no single weight W which simultaneously obtains e 11 and e 21 as translation for f 1 and f 2 via Equation (1).", "labels": [], "entities": [{"text": "Equation", "start_pos": 121, "end_pos": 129, "type": "METRIC", "confidence": 0.9543049931526184}]}, {"text": "However, we can achieve this with two weights: 1, 1 for f 1 and \u22121, 1 for f 2 . In this paper, inspired by KNN-SVM (), we propose a local training method, which trains sentence-wise weights instead of a single weight, to address the above two problems.", "labels": [], "entities": []}, {"text": "Compared with global training methods, such as MERT, in which training and testing are separated, our method works in an online fashion, in which training is performed during testing.", "labels": [], "entities": []}, {"text": "This online fashion has an advantage in that it can adapt the weights for each of the test sentences, by dynamically tuning the weights on translation examples which are similar to these test sentences.", "labels": [], "entities": []}, {"text": "Similar to the method of development set automatical selection, the local training method may also suffer the problem of efficiency.", "labels": [], "entities": []}, {"text": "To put it into practice, we propose incremental training methods which avoid retraining and iterative decoding on a development set.", "labels": [], "entities": []}, {"text": "Our local training method has two advantages: firstly, it significantly outperforms MERT, especially when test set is different from the development set; secondly, it improves the translation consistency.", "labels": [], "entities": [{"text": "MERT", "start_pos": 84, "end_pos": 88, "type": "METRIC", "confidence": 0.618686318397522}]}, {"text": "Experiments on NIST Chinese-to-English translation tasks show that our local training method significantly gains over MERT, with the maximum improvements up to 2.0 BLEU, and its efficiency is comparable to that of the global training method.", "labels": [], "entities": [{"text": "NIST Chinese-to-English translation tasks", "start_pos": 15, "end_pos": 56, "type": "TASK", "confidence": 0.7558192610740662}, {"text": "MERT", "start_pos": 118, "end_pos": 122, "type": "METRIC", "confidence": 0.8658227920532227}, {"text": "BLEU", "start_pos": 164, "end_pos": 168, "type": "METRIC", "confidence": 0.9993707537651062}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: The performance comparison of the baseline In- Hiero VS Moses and Moses hier.", "labels": [], "entities": [{"text": "In- Hiero VS Moses", "start_pos": 53, "end_pos": 71, "type": "DATASET", "confidence": 0.5918014883995056}]}, {"text": " Table 2: The efficiency of the local training and testing  measured by sentence averaged runtime.", "labels": [], "entities": []}, {"text": " Table 3: The performance comparison of local train- ing methods (MBUU and EBUU) and a global method  (MERT). NIST05 is the set used to tune \u03bb for MBUU and  EBUU, and NIST06 and NIST08 are test sets. + means  the local method is significantly better than MERT with  p < 0.05.", "labels": [], "entities": [{"text": "NIST06", "start_pos": 167, "end_pos": 173, "type": "DATASET", "confidence": 0.9398307800292969}, {"text": "NIST08", "start_pos": 178, "end_pos": 184, "type": "DATASET", "confidence": 0.921111524105072}, {"text": "MERT", "start_pos": 255, "end_pos": 259, "type": "METRIC", "confidence": 0.6430358290672302}]}, {"text": " Table 5: The comparison of MERT with different de- velopment datasets and local training method based on  EBUU.", "labels": [], "entities": [{"text": "MERT", "start_pos": 28, "end_pos": 32, "type": "METRIC", "confidence": 0.4148722290992737}, {"text": "EBUU", "start_pos": 107, "end_pos": 111, "type": "DATASET", "confidence": 0.952113687992096}]}, {"text": " Table 7: The performance comparison by varying re- trieval size in Algorithm 2 based on EBUU.", "labels": [], "entities": [{"text": "EBUU", "start_pos": 89, "end_pos": 93, "type": "DATASET", "confidence": 0.9239420890808105}]}, {"text": " Table 8: The performance of Oracle of 2-best results  which consist of 1-best resluts of MERT and 1-best  resluts of EBUU.", "labels": [], "entities": [{"text": "MERT", "start_pos": 90, "end_pos": 94, "type": "METRIC", "confidence": 0.49365726113319397}, {"text": "EBUU", "start_pos": 118, "end_pos": 122, "type": "DATASET", "confidence": 0.9692574143409729}]}]}