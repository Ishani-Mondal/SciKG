{"title": [{"text": "Modelling Sequential Text with an Adaptive Topic Model", "labels": [], "entities": []}], "abstractContent": [{"text": "Topic models are increasingly being used for text analysis tasks, oftentimes replacing earlier semantic techniques such as latent semantic analysis.", "labels": [], "entities": [{"text": "text analysis tasks", "start_pos": 45, "end_pos": 64, "type": "TASK", "confidence": 0.8349655270576477}, {"text": "latent semantic analysis", "start_pos": 123, "end_pos": 147, "type": "TASK", "confidence": 0.6485761801401774}]}, {"text": "In this paper, we develop a novel adaptive topic model with the ability to adapt topics from both the previous segment and the parent document.", "labels": [], "entities": []}, {"text": "For this proposed model, a Gibbs sampler is developed for doing posterior inference.", "labels": [], "entities": [{"text": "posterior inference", "start_pos": 64, "end_pos": 83, "type": "TASK", "confidence": 0.6650272905826569}]}, {"text": "Experimental results show that with topic adaptation, our model significantly improves over existing approaches in terms of perplexity, and is able to uncover clear sequential structure on, for example, Herman Melville's book \"Moby Dick\".", "labels": [], "entities": [{"text": "topic adaptation", "start_pos": 36, "end_pos": 52, "type": "TASK", "confidence": 0.7660844624042511}]}], "introductionContent": [{"text": "Natural language text usually consists of topically structured and coherent components, such as groups of sentences that form paragraphs and groups of paragraphs that form sections.", "labels": [], "entities": []}, {"text": "Topical coherence in documents facilitates readers' comprehension, and reflects the author's intended structure.", "labels": [], "entities": []}, {"text": "Capturing this structural topical dependency should lead to improved topic modelling.", "labels": [], "entities": [{"text": "topic modelling", "start_pos": 69, "end_pos": 84, "type": "TASK", "confidence": 0.9414690136909485}]}, {"text": "It also seems reasonable to propose that text analysis tasks that involve the structure of a document, for instance, summarisation and segmentation, should also be improved by topic models that better model that structure.", "labels": [], "entities": [{"text": "text analysis", "start_pos": 41, "end_pos": 54, "type": "TASK", "confidence": 0.760945200920105}, {"text": "summarisation and segmentation", "start_pos": 117, "end_pos": 147, "type": "TASK", "confidence": 0.6575528283913931}]}, {"text": "Recently, topic models are increasingly being used for text analysis tasks such as summarisa- * This work was partially done when Du was at College of Engineering & Computer Science, the Australian National University when working together with Buntine and Jin there.", "labels": [], "entities": [{"text": "summarisa", "start_pos": 83, "end_pos": 92, "type": "TASK", "confidence": 0.9801622629165649}]}, {"text": "tion and segmentation (, oftentimes replacing earlier semantic techniques such as latent semantic analysis).", "labels": [], "entities": [{"text": "latent semantic analysis", "start_pos": 82, "end_pos": 106, "type": "TASK", "confidence": 0.6486011743545532}]}, {"text": "Topic models can be improved by better modelling the semantic aspects of text, for instance integrating collocations into the model or encouraging topics to be more semantically coherent) based on lexical coherence models, modelling the structural aspects of documents, for instance modelling a document as a set of segments (, or improving the underlying statistical methods ( ;).", "labels": [], "entities": []}, {"text": "Topic models, like statistical parsing methods, are using more sophisticated latent variable methods in order to model different aspects of these problems.", "labels": [], "entities": [{"text": "statistical parsing", "start_pos": 19, "end_pos": 38, "type": "TASK", "confidence": 0.7262819111347198}]}, {"text": "In this paper, we are interested in developing anew topic model which can take into account the structural topic dependency by following the higher level document subject structure, but we hope to retain the general flavour of topic models, where components (e.g., sentences) can be a mixture of topics.", "labels": [], "entities": []}, {"text": "Thus we need to depart from the earlier HMM style models, see, e.g.,.", "labels": [], "entities": []}, {"text": "Inspired by the idea that documents usually exhibits internal structure (e.g., (), in which semantically related units are clustered together to form semantically structural segments, we treat documents as sequences of segments (e.g., sentences, paragraphs, sections, or chapters).", "labels": [], "entities": []}, {"text": "In this way, we can model the topic correlation be- (B): Different structural relationships for topics of sections in a 4-part document, hierarchical (H), sequential (S), both (B) or mixed (M).", "labels": [], "entities": []}, {"text": "tween the segments in a \"bag of segments\" fashion, i.e., beyond the \"bag of words\" assumption, and reveal how topics evolve among segments.", "labels": [], "entities": []}, {"text": "Indeed, we were impressed by the improvement in perplexity obtained by the segmented topic model (STM) (), so we considered the problem of whether one can add sequence information into a structured topic model as well.", "labels": [], "entities": []}, {"text": "illustrates the type of structural information being considered, where the vectors are some representation of the content.", "labels": [], "entities": []}, {"text": "STM is represented by the hierarchical model.", "labels": [], "entities": [{"text": "STM", "start_pos": 0, "end_pos": 3, "type": "TASK", "confidence": 0.7772479057312012}]}, {"text": "A strictly sequential model would seem unrealistic for some documents, for instance books.", "labels": [], "entities": []}, {"text": "A topic model using the strictly sequential model was developed () but it reportedly performs halfway between STM and LDA.", "labels": [], "entities": []}, {"text": "In this paper, we develop an adaptive topic model to go beyond a strictly sequential model while allow some hierarchical influence.", "labels": [], "entities": []}, {"text": "There are two possible hybrids, one called \"mixed\" has distinct breaks in the sequence, while the other called \"both\" overlays both sequence and hierarchy and there could be relative strengths associated with the arrows.", "labels": [], "entities": []}, {"text": "We employ the \"both\" hybrid but use the relative strengths to adaptively allow it to approximate the \"mixed\" hybrid.", "labels": [], "entities": []}, {"text": "Research in Machine Learning and Natural Language Processing has attempted to model various topical dependencies.", "labels": [], "entities": []}, {"text": "Some work considers structure within the sentence level by mixing hidden Markov models (HMMs) and topics on a word byword basis: the aspect HMM) and the HMM-LDA model () that models both short-range syntactic dependencies and longer semantic dependencies.", "labels": [], "entities": []}, {"text": "These models operate at a finer level than we are considering at a segment (like paragraph or section) level.", "labels": [], "entities": []}, {"text": "To make a tool like the HMM work at higher levels, one needs to make stronger assumptions, for instance assigning each sentence a single topic and then topic specific word models can be used: the hidden topic Markov model () that models the transitional topic structure; a global model based on the generalised Mallows model, and a HMM based content model ().", "labels": [], "entities": []}, {"text": "Researchers have also considered timeseries of topics: various kinds of dynamic topic models, following early work of), represent a collection as a sequence of subcollections in epochs.", "labels": [], "entities": []}, {"text": "Here, one is modelling the collections over broad epochs, not the structure of a single document that our model considers.", "labels": [], "entities": []}, {"text": "This paper is organised as follows.", "labels": [], "entities": []}, {"text": "We first present background theory in Section 2.", "labels": [], "entities": []}, {"text": "Then the new model is presented in Section 3, followed by Gibbs sampling theory and algorithm in Sections 4 and 5 respectively.", "labels": [], "entities": []}, {"text": "Experiments are reported in Section 6 with a conclusion in Section 7.", "labels": [], "entities": [{"text": "Section", "start_pos": 59, "end_pos": 66, "type": "DATASET", "confidence": 0.9179869890213013}]}], "datasetContent": [{"text": "In the experimental work, we have three objectives: (1) to explore the setting of hyper-parameters, (2) to compare the model with the earlier sequential LDA (SeqLDA) of (), STM of ( and standard LDA, and (3) to view the results in detail on a number of characteristic problems.", "labels": [], "entities": []}, {"text": "For general testing, five patent datasets are randomly selected from U.S. patents granted in 2009 and 2010.", "labels": [], "entities": []}, {"text": "Patents in Pat-A are selected from international patent class (IPC) \"A\", which is about \"HUMAN NECESSITIES\"; those in Pat-B are selected from class \"B60\" about \"VEHICLES IN GENERAL\"; those in Pat-H are selected from class \"H\" about \"ELECTRICITY\"; those in Pat-F are selected from class \"F\" about \"MECHAN-ICAL ENGINEERING; LIGHTING; HEATING; WEAPONS; BLASTING\"; and those in Pat-G are selected from class \"G06\" about \"COMPUTING; CALCULATING; COUNTING\".", "labels": [], "entities": [{"text": "BLASTING", "start_pos": 350, "end_pos": 358, "type": "METRIC", "confidence": 0.9707242846488953}]}, {"text": "All the patents in these five datasets are split into paragraphs that are taken as segments, and the sequence of paragraphs in each patent is reserved in order to maintain the original layout.", "labels": [], "entities": []}, {"text": "All the stop words, the top 10 common words, the uncommon words (i.e., words in less than five patents) and numbers have been removed.", "labels": [], "entities": []}, {"text": "Two books used for more detailed investigation are \"The Prince\" byNicco\u00ec o Machiavelli and \"Moby Dick\" by Herman Melville.", "labels": [], "entities": []}, {"text": "They are split into chapters and/or paragraphs which are treated as segments, and only stop-words are removed.", "labels": [], "entities": []}, {"text": "shows in detail the statistics of these datasets after preprocessing.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 4: P-values for one-tail paired t-test on the five  patent datasets.  AdaTM  Pat-G Pat-A Pat-F Pat-H Pat-B  LDA D .0001 .0001 .0002 .0001 .0001  LDA P .0041 .0030 .0022 .0071 .0096  SeqLDA .0029 .0047 .0003 .0012 .0023  STM .0220 .0066 .0210 .0629 .0853", "labels": [], "entities": [{"text": "AdaTM  Pat-G Pat-A Pat-F Pat-H Pat-B  LDA D", "start_pos": 77, "end_pos": 120, "type": "DATASET", "confidence": 0.8109773844480515}]}]}