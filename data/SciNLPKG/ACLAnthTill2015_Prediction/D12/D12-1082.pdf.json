{"title": [{"text": "No Noun Phrase Left Behind: Detecting and Typing Unlinkable Entities", "labels": [], "entities": [{"text": "No Noun Phrase Left Behind: Detecting and Typing Unlinkable Entities", "start_pos": 0, "end_pos": 68, "type": "TASK", "confidence": 0.6841384497555819}]}], "abstractContent": [{"text": "Entity linking systems link noun-phrase mentions in text to their corresponding Wikipedia articles.", "labels": [], "entities": []}, {"text": "However, NLP applications would gain from the ability to detect and type all entities mentioned in text, including the long tail of entities not prominent enough to have their own Wikipedia articles.", "labels": [], "entities": []}, {"text": "In this paper we show that once the Wikipedia entities mentioned in a corpus of textual assertions are linked, this can further enable the detection and fine-grained typing of the unlinkable entities.", "labels": [], "entities": []}, {"text": "Our proposed method for detecting un-linkable entities achieves 24% greater accuracy than a Named Entity Recognition base-line, and our method for fine-grained typing is able to propagate over 1,000 types from linked Wikipedia entities to unlinkable entities.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 76, "end_pos": 84, "type": "METRIC", "confidence": 0.9991705417633057}]}, {"text": "Detection and typing of unlinkable entities can increase yield for NLP applications such as typed question answering.", "labels": [], "entities": [{"text": "yield", "start_pos": 57, "end_pos": 62, "type": "METRIC", "confidence": 0.9795690178871155}, {"text": "typed question answering", "start_pos": 92, "end_pos": 116, "type": "TASK", "confidence": 0.6211864848931631}]}], "introductionContent": [{"text": "A key challenge in machine reading () is to identify the entities mentioned in text, and associate them with appropriate background information such as their type.", "labels": [], "entities": [{"text": "machine reading", "start_pos": 19, "end_pos": 34, "type": "TASK", "confidence": 0.820686548948288}]}, {"text": "Consider the sentence \"Some people think that pineapple juice is good for vitamin C.\"", "labels": [], "entities": []}, {"text": "To analyze this sentence, a machine should know that \"pineapple juice\" refers to a beverage, while \"vitamin C\" refers to a nutrient.", "labels": [], "entities": []}, {"text": "Entity linking ( addresses this problem by linking noun phrases within the sentence to entries in a large, fixed entity catalog (almost always example noun phrases status \"apple juice\" \"orange juice\" present \"prune juice\" \"wheatgrass juice\" absent \"radiation exposure\" \"workplace stress\" present \"asbestos exposure\" \"financial stress\" absent \"IJCAI\" \"OOPSLA\" present \"EMNLP\" \"ICAPS\" absent: Wikipedia has entries for prominent entities, while missing tail and new entities of the same types. Wikipedia).", "labels": [], "entities": [{"text": "Entity linking", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.8198689222335815}]}, {"text": "Thus, entity linking has a limited and somewhat arbitrary range.", "labels": [], "entities": [{"text": "entity linking", "start_pos": 6, "end_pos": 20, "type": "TASK", "confidence": 0.7732003033161163}]}, {"text": "In our example, systems by and) both link \"vitamin C\" correctly, but link \"pineapple juice\" to \"pineapple.\"", "labels": [], "entities": []}, {"text": "\"Pineapple juice\" is not entity linked as a beverage because it is not prominent enough to have its own Wikipedia entry.", "labels": [], "entities": [{"text": "Pineapple juice\"", "start_pos": 1, "end_pos": 17, "type": "DATASET", "confidence": 0.8738671938578287}]}, {"text": "As shows, Wikipedia often has prominent entities, while missing tail and new entities of the same types.", "labels": [], "entities": []}, {"text": "1 () notes that there are more than 900 different active shoe brands, but only 82 exist in Wikipedia.", "labels": [], "entities": []}, {"text": "In scenarios such as intelligence analysis and local search, non-Wikipedia entities are often the most important.", "labels": [], "entities": [{"text": "intelligence analysis", "start_pos": 21, "end_pos": 42, "type": "TASK", "confidence": 0.8341336846351624}]}, {"text": "Hence, we introduce the unlinkable noun phrase problem: Given a noun phrase that does not link into Wikipedia, return whether it is an entity, as well its fine-grained semantic types.", "labels": [], "entities": []}, {"text": "Deciding if a nonWikipedia noun phrase is an entity is challenging because many of them are not entities (e.g., \"Some people,\" \"an addition\" and \"nearly half\").", "labels": [], "entities": []}, {"text": "Predict-ing semantic types is a challenge because of the diversity of entity types in general text.", "labels": [], "entities": [{"text": "Predict-ing semantic types", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.8279290795326233}]}, {"text": "In our experiments, we utilized the Freebase type system, which contains over 1,000 semantic types.", "labels": [], "entities": [{"text": "Freebase type system", "start_pos": 36, "end_pos": 56, "type": "DATASET", "confidence": 0.9155969421068827}]}, {"text": "The first part of this paper proposes a novel method for detecting entities by observing that entities often have different usage-over-time characteristics than non-entities.", "labels": [], "entities": []}, {"text": "Evaluation shows that our method achieves 24% relative accuracy gain over a NER baseline.", "labels": [], "entities": [{"text": "accuracy gain", "start_pos": 55, "end_pos": 68, "type": "METRIC", "confidence": 0.953079491853714}, {"text": "NER baseline", "start_pos": 76, "end_pos": 88, "type": "DATASET", "confidence": 0.7007904499769211}]}, {"text": "The second part of this paper shows how instance-to-instance class propagation () can be adapted and scaled to semantically type general noun-phrase entities using types from linked entities, by leveraging over one million different possible textual relations.", "labels": [], "entities": [{"text": "instance-to-instance class propagation", "start_pos": 40, "end_pos": 78, "type": "TASK", "confidence": 0.7420643369356791}]}, {"text": "Contributions of our research include: \u2022 We motivate and introduce the unlinkable noun phrase problem, which extends previous work in entity linking.", "labels": [], "entities": [{"text": "entity linking", "start_pos": 134, "end_pos": 148, "type": "TASK", "confidence": 0.7631863951683044}]}, {"text": "\u2022 We propose a novel method for discriminating entities from arbitrary noun phrases, utilizing features derived from Google Books ngrams.", "labels": [], "entities": []}, {"text": "\u2022 We adapt and scale instance-to-instance class propagation in order to associate types with non-Wikipedia entities.", "labels": [], "entities": [{"text": "instance-to-instance class propagation", "start_pos": 21, "end_pos": 59, "type": "TASK", "confidence": 0.7386053800582886}]}, {"text": "\u2022 We implement and evaluate our methods, empirically verifying improvement over appropriate baselines.", "labels": [], "entities": []}], "datasetContent": [{"text": "The goal of the evaluation is to judge how well our method can predict the Freebase semantic types of entities in our scenario.", "labels": [], "entities": []}, {"text": "Our linked entities covered 1,339 Freebase types, including many interesting types such as computer operating system, religious text, airline and baseball team.", "labels": [], "entities": []}, {"text": "Human judges would have trouble manually annotating new entities with all these types because there are too many to keep in mind and understand the characteristics \"is a highway in\" \"is a university located in\" \"became the president of\" \"turned down the role of\" \"has an embassy in\": Example relations found to have high weight.", "labels": [], "entities": []}, {"text": "\"comes with\" \"is a generic term for\" \"works best on\" \"can be made from\" \"is almost identical to\" of.", "labels": [], "entities": []}, {"text": "Instead, we automatically generate testing data by sampling entities from L, and then test on ability to recover the actual Freebase types (which we know).", "labels": [], "entities": []}, {"text": "We sample a HEAD set of distinct 500 Freebase entities (drawn randomly from our set of linked extractions), and a TAIL set of 500 entities (drawn randomly from our set of linked entities).", "labels": [], "entities": []}, {"text": "An entity that occurs in many extractions is more likely to be in HEAD than TAIL.", "labels": [], "entities": [{"text": "HEAD", "start_pos": 66, "end_pos": 70, "type": "METRIC", "confidence": 0.7420893311500549}, {"text": "TAIL", "start_pos": 76, "end_pos": 80, "type": "METRIC", "confidence": 0.8521156907081604}]}, {"text": "Our sampling also picks only entities that occur with at least 10 relations, which is appropriate for the Web scenario where more instances can always be queried for.", "labels": [], "entities": []}, {"text": "For baselines we use random baseline B Random and a frequency baseline B F requency which always returns types in order of their frequency among all linked entities (e.g., always person, then location, etc).", "labels": [], "entities": []}, {"text": "We evaluate our system without relation weighting (S N oW eight ) and also with relation weighting (S W eighted ).", "labels": [], "entities": []}, {"text": "For SW eighted we leave all the test set entities out when calculating global relation weights.", "labels": [], "entities": [{"text": "SW eighted", "start_pos": 4, "end_pos": 14, "type": "DATASET", "confidence": 0.7751190066337585}]}, {"text": "Our metrics are Precision at 1 and F 1 score.", "labels": [], "entities": [{"text": "Precision", "start_pos": 16, "end_pos": 25, "type": "METRIC", "confidence": 0.9926964044570923}, {"text": "F 1 score", "start_pos": 35, "end_pos": 44, "type": "METRIC", "confidence": 0.9743408958117167}]}, {"text": "Precision at 1 measures how often the top returned type is a correct type, and is useful for applications that want one type per entity.", "labels": [], "entities": [{"text": "Precision at 1", "start_pos": 0, "end_pos": 14, "type": "METRIC", "confidence": 0.9478042920430502}]}, {"text": "F 1 measures how well the method recovers the full set of Freebase types (for each test case we graph precision/recall and take the max F 1 ), and is useful for applications such as typed question answering.  is statistically significant above all baselines, and SW eighted is statistically significant over S N oW eight on both test sets and metrics.", "labels": [], "entities": [{"text": "F 1", "start_pos": 0, "end_pos": 3, "type": "METRIC", "confidence": 0.9831697344779968}, {"text": "precision", "start_pos": 102, "end_pos": 111, "type": "METRIC", "confidence": 0.9921689629554749}, {"text": "recall", "start_pos": 112, "end_pos": 118, "type": "METRIC", "confidence": 0.7672010660171509}, {"text": "typed question answering.", "start_pos": 182, "end_pos": 207, "type": "TASK", "confidence": 0.6550801893075308}]}], "tableCaptions": [{"text": " Table 5: Evaluation on HEAD and TAIL, 500 elements each.  \u2020 indicates statistical significance over B Frequency , and   \u2021 over both B Frequency and S NoWeight . Significance is measured using the Student's t-test at 95% confidence. The  top type predicted by our S Weighted method is correct about 60% of the time, while the top type predicted by the  B Frequency baseline is correct under 30% of the time.", "labels": [], "entities": [{"text": "TAIL", "start_pos": 33, "end_pos": 37, "type": "METRIC", "confidence": 0.7070290446281433}]}]}