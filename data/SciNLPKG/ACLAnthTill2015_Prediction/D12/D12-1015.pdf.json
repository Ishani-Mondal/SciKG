{"title": [{"text": "Collocation Polarity Disambiguation Using Web-based Pseudo Contexts", "labels": [], "entities": [{"text": "Collocation Polarity Disambiguation", "start_pos": 0, "end_pos": 35, "type": "TASK", "confidence": 0.9139313896497091}]}], "abstractContent": [{"text": "This paper focuses on the task of colloca-tion polarity disambiguation.", "labels": [], "entities": [{"text": "colloca-tion polarity disambiguation", "start_pos": 34, "end_pos": 70, "type": "TASK", "confidence": 0.6160210768381754}]}, {"text": "The collocation refers to a binary tuple of a polarity word and a target (such as \u27e8long, battery life\u27e9 or \u27e8long, startup\u27e9), in which the sentiment orientation of the polarity word (\"long\") changes along with different targets (\"battery life\" or \"startup\").", "labels": [], "entities": []}, {"text": "To disambiguate a collocation's polarity, previous work always turned to investigate the polarities of its surrounding contexts, and then assigned the majority polarity to the collo-cation.", "labels": [], "entities": []}, {"text": "However, these contexts are limited, thus the resulting polarity is insufficient to be reliable.", "labels": [], "entities": []}, {"text": "We therefore propose an unsuper-vised three-component framework to expand some pseudo contexts from web, to help dis-ambiguate a collocation's polarity.Without using any additional labeled data, experiments show that our method is effective.", "labels": [], "entities": []}], "introductionContent": [{"text": "In recent years, more attention has been paid to sentiment analysis as it has been widely used in various natural language processing applications, such as question answering (, information extraction () and opinion-oriented summarization ().", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 49, "end_pos": 67, "type": "TASK", "confidence": 0.9689346551895142}, {"text": "question answering", "start_pos": 156, "end_pos": 174, "type": "TASK", "confidence": 0.8917837738990784}, {"text": "information extraction", "start_pos": 178, "end_pos": 200, "type": "TASK", "confidence": 0.8008715212345123}, {"text": "opinion-oriented summarization", "start_pos": 208, "end_pos": 238, "type": "TASK", "confidence": 0.5546292215585709}]}, {"text": "Meanwhile, it also brings us lots of interesting and challenging research topics, such as subjectivity analysis (, sentiment classification (; * Correspondence author: tliu@ir.hit.edu.cn), opinion retrieval () and soon.", "labels": [], "entities": [{"text": "subjectivity analysis", "start_pos": 90, "end_pos": 111, "type": "TASK", "confidence": 0.7149443179368973}, {"text": "sentiment classification", "start_pos": 115, "end_pos": 139, "type": "TASK", "confidence": 0.9446978867053986}, {"text": "opinion retrieval", "start_pos": 189, "end_pos": 206, "type": "TASK", "confidence": 0.8603107929229736}]}, {"text": "One fundamental task for sentiment analysis is to determine the semantic orientations of words.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 25, "end_pos": 43, "type": "TASK", "confidence": 0.9618031978607178}]}, {"text": "For example, the word \"beautiful\" is positive, while \"ugly\" is negative.", "labels": [], "entities": []}, {"text": "Many researchers have developed several algorithms for this purpose and generated large static lexicons of words marked with prior polarities (.", "labels": [], "entities": []}, {"text": "However, there exist some polarity-ambiguous words, which can dynamically reflect different polarities along with different contexts.", "labels": [], "entities": []}, {"text": "A typical polarity-ambiguous word \"\u957f\" (\"long\" in English) is shown with two example sentences as follows.", "labels": [], "entities": []}, {"text": "The phrases marked with p superscript are the polarity-ambiguous words, and the phrases marked with t superscript are targets modified by the polarity words.", "labels": [], "entities": []}, {"text": "In the above two sentences, the sentiment orientation of the polarity word \"\u957f\" (\"long\" in English) changes along with different targets.", "labels": [], "entities": []}, {"text": "When modifying the target \"\u7535\u6c60\u5bff\u547d\" (\"battery life\" in English), its polarity is positive; and when modifying \"\u542f\u52a8\u65f6\u95f4\" (\"startup\" in English), its polarity is negative.", "labels": [], "entities": []}, {"text": "In this paper, we especially define the collocation as a binary tuple of the polarity-ambiguous word and its modified target, such as \u27e8\u957f,\u7535\u6c60\u5bff\u547d\u27e9 (\u27e8long, battery life\u27e9 in English) or \u27e8\u957f,\u542f\u52a8\u65f6\u95f4\u27e9 (\u27e8long, startup\u27e9 in English).", "labels": [], "entities": []}, {"text": "This paper concentrates on the task of collocation polarity disambiguation.", "labels": [], "entities": [{"text": "collocation polarity disambiguation", "start_pos": 39, "end_pos": 74, "type": "TASK", "confidence": 0.6591155032316843}]}, {"text": "This is an important task as the problem of polarity-ambiguity is frequent.", "labels": [], "entities": []}, {"text": "We analyze 4,861 common binary tuples of polarity words and their modified targets from 478 reviews 1 , and find that over 20% of them are the collocations defined in this paper.", "labels": [], "entities": []}, {"text": "Therefore, the task of collocation polarity disambiguation is worthy of study.", "labels": [], "entities": [{"text": "collocation polarity disambiguation", "start_pos": 23, "end_pos": 58, "type": "TASK", "confidence": 0.6362148324648539}]}], "datasetContent": [{"text": "We conduct the experiments on a Chinese collocation corpus of four product domains, which is from the Task3 of the Chinese Opinion Analysis Evaluation (COAE).", "labels": [], "entities": [{"text": "Chinese Opinion Analysis Evaluation (COAE)", "start_pos": 115, "end_pos": 157, "type": "TASK", "confidence": 0.5002560274941581}]}, {"text": "describes the corpus in detail.", "labels": [], "entities": []}, {"text": "From 478 reviews, 1,001 collocations (454 positive and 547 negative) with polarity-ambiguous words are found and manually annotated by two annotators.", "labels": [], "entities": []}, {"text": "Cohen's kappa, a measure of inter-annotator agreement ranging from zero to one, is 0.83, indicating a good strength of agreement . In, Sig of the fourth column denotes the collocations that appear once in all the domainrelated reviews.", "labels": [], "entities": []}, {"text": "And multiple in the last column denotes the collocations that appear several times.", "labels": [], "entities": []}, {"text": "From, we can find that among all the reviews, nearly 60% collocations only appear once.", "labels": [], "entities": []}, {"text": "Even for the multiple collocations, they averagely appear less than 4 times.", "labels": [], "entities": []}, {"text": "Therefore, fora collocation, if we only consider its original contexts alone or the expanded pseudo contexts from the domainrelated review set alone, the contexts are obviously limited and unreliable.", "labels": [], "entities": []}, {"text": "Instead of using accuracy, we use precision (P), recall (R) and F-measure (F1) to measure the performance of this task.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 17, "end_pos": 25, "type": "METRIC", "confidence": 0.999187171459198}, {"text": "precision (P)", "start_pos": 34, "end_pos": 47, "type": "METRIC", "confidence": 0.9456243216991425}, {"text": "recall (R)", "start_pos": 49, "end_pos": 59, "type": "METRIC", "confidence": 0.9549918174743652}, {"text": "F-measure (F1)", "start_pos": 64, "end_pos": 78, "type": "METRIC", "confidence": 0.9489146769046783}]}, {"text": "That's because two kinds of collocations' polarities cannot be disambiguated.", "labels": [], "entities": []}, {"text": "One is the sparse collocations, which obtain no effective contexts.", "labels": [], "entities": []}, {"text": "The other is the collocations that acquire the same amount of positive and negative contexts.", "labels": [], "entities": []}, {"text": "The metrics are defined as follows.", "labels": [], "entities": []}, {"text": "In order to do a detailed analysis into our threecomponent framework, some deep experiments are made: Query Expansion The aim of query expansion is to retrieve lots of relative snippets, from which we can extract the useful pseudo contexts.", "labels": [], "entities": []}, {"text": "For each  snippet, if the polarity word of the collocation does modify the target, we consider this snippet as a correct query expansion result.", "labels": [], "entities": []}, {"text": "Pseudo Context For each expanded pseudo context from web, if it shows the same sentiment orientation with the collocation (or opposite with the collocation's polarity because of the usage of transitional words), we consider this context as a correct pseudo context.", "labels": [], "entities": []}, {"text": "Sentiment Analysis For each expanded pseudo context, if its polarity can be correctly recognized by the polarity computation method in, and meanwhile it shows the same sentiment orientation with the collocation, we consider this context as a correct one.", "labels": [], "entities": [{"text": "Sentiment Analysis", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.9240019917488098}]}, {"text": "illustrates the accuracy of each experiment for each strategy in detail, where 400 web retrieved snippets for Query Expansion and 400 expanded pseudo contexts for Pseudo Context and Sentiment Analysis are randomly selected and manually evaluated for each strategy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 16, "end_pos": 24, "type": "METRIC", "confidence": 0.9993463158607483}, {"text": "Pseudo Context and Sentiment Analysis", "start_pos": 163, "end_pos": 200, "type": "TASK", "confidence": 0.6188739597797394}]}, {"text": "Seen from, we can find that: 1.", "labels": [], "entities": []}, {"text": "For Query Expansion, all strategies yield good accuracies except for Strategy0.", "labels": [], "entities": []}, {"text": "This can draw a same conclusion with our analysis in Section 3.2.1.", "labels": [], "entities": []}, {"text": "The queries from Strategy0 are short, thus in many retrieved snippets, there exist no modifying relations between the polarity words and targets.", "labels": [], "entities": []}, {"text": "Accordingly, the pseudo contexts from these snippets are incorrect.", "labels": [], "entities": []}, {"text": "This can result in the low accuracy of Strategy0.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 27, "end_pos": 35, "type": "METRIC", "confidence": 0.9995546936988831}, {"text": "Strategy0", "start_pos": 39, "end_pos": 48, "type": "DATASET", "confidence": 0.7616342306137085}]}, {"text": "On the other hand, we can find that the other three query expansion strategies perform well.", "labels": [], "entities": [{"text": "query expansion", "start_pos": 52, "end_pos": 67, "type": "TASK", "confidence": 0.6978800296783447}]}, {"text": "2. Although the final result of our threecomponent framework is good, the accuracies of Pseudo Context and Sentiment Analysis for each strategy is not very high.", "labels": [], "entities": [{"text": "accuracies", "start_pos": 74, "end_pos": 84, "type": "METRIC", "confidence": 0.9931326508522034}, {"text": "Sentiment Analysis", "start_pos": 107, "end_pos": 125, "type": "TASK", "confidence": 0.7717905640602112}]}, {"text": "This is perhaps caused by unrefined work on the specific sub-stages.", "labels": [], "entities": []}, {"text": "For example, we get all the pseudo contexts using the algorithm in.", "labels": [], "entities": []}, {"text": "However, in some reviews, the two sentences before and after the target sentence have no polarity relation with the target sentence itself.", "labels": [], "entities": []}, {"text": "This can bring in some noises.", "labels": [], "entities": []}, {"text": "On the other hand, the context polarity computation algorithm in is just a simple attempt, which is not the best way to compute the context's polarity.", "labels": [], "entities": [{"text": "context polarity computation", "start_pos": 23, "end_pos": 51, "type": "TASK", "confidence": 0.7896441320578257}]}, {"text": "In fact, this paper aims to try some simple algorithms for each component to validate the effectiveness of the three-component framework.", "labels": [], "entities": []}, {"text": "We will polish every component of our framework in future.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: Statistics for the Chinese collocation corpus.", "labels": [], "entities": []}, {"text": " Table 4: Comparative results for the collocation polarity  disambiguation task.", "labels": [], "entities": [{"text": "collocation polarity  disambiguation", "start_pos": 38, "end_pos": 74, "type": "TASK", "confidence": 0.6701275904973348}]}, {"text": " Table 5: The performance of our method based on each  query expansion strategy for collocation polarity disam- biguation.", "labels": [], "entities": []}, {"text": " Table 6: The accuracies of the query expansion, pseudo context and sentiment analysis for each strategy.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 68, "end_pos": 86, "type": "TASK", "confidence": 0.8066554963588715}]}]}