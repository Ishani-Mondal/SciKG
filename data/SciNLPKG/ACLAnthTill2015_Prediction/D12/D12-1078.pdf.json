{"title": [{"text": "Re-training Monolingual Parser Bilingually for Syntactic SMT \u2020", "labels": [], "entities": [{"text": "Syntactic SMT", "start_pos": 47, "end_pos": 60, "type": "TASK", "confidence": 0.7945160865783691}]}], "abstractContent": [{"text": "The training of most syntactic SMT approaches involves two essential components, word alignment and monolingual parser.", "labels": [], "entities": [{"text": "syntactic SMT", "start_pos": 21, "end_pos": 34, "type": "TASK", "confidence": 0.5458535999059677}, {"text": "word alignment", "start_pos": 81, "end_pos": 95, "type": "TASK", "confidence": 0.7909775376319885}]}, {"text": "In the current state of the art these two components are mutually independent, thus causing problems like lack of rule generalization, and violation of syntactic correspondence in translation rules.", "labels": [], "entities": [{"text": "rule generalization", "start_pos": 114, "end_pos": 133, "type": "TASK", "confidence": 0.7012815028429031}]}, {"text": "In this paper, we propose two ways of retraining monolingual parser with the target of maximizing the consistency between parse trees and alignment matrices.", "labels": [], "entities": [{"text": "consistency", "start_pos": 102, "end_pos": 113, "type": "METRIC", "confidence": 0.979912519454956}]}, {"text": "One is targeted self-training with a simple evaluation function; the other is based on training data selection from forced alignment of bilingual data.", "labels": [], "entities": []}, {"text": "We also propose an auxiliary method for boosting alignment quality, by symmetrizing alignment matrices with respect to parse trees.", "labels": [], "entities": []}, {"text": "The best combination of these novel methods achieves 3 Bleu point gain in an IWSLT task and more than 1 Bleu point gain in NIST tasks.", "labels": [], "entities": [{"text": "Bleu point gain", "start_pos": 55, "end_pos": 70, "type": "METRIC", "confidence": 0.9669043024381002}, {"text": "Bleu point gain", "start_pos": 104, "end_pos": 119, "type": "METRIC", "confidence": 0.9420711000760397}]}], "introductionContent": [{"text": "There are many varieties in syntactic statistical machine translation (SSMT).", "labels": [], "entities": [{"text": "syntactic statistical machine translation (SSMT)", "start_pos": 28, "end_pos": 76, "type": "TASK", "confidence": 0.7313732206821442}]}, {"text": "Apart from a few attempts to use synchronous parsing to produce the tree structure of both source language (SL) and target language (TL) simultaneously, most SSMT approaches make use of monolingual parser to produce the parse tree(s) of the SL and/or TL sentences, and then linkup the information of the two languages through word alignment.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 326, "end_pos": 340, "type": "TASK", "confidence": 0.7007971554994583}]}, {"text": "In the current state of the art, word aligner and monolingual parser are trained and applied separately.", "labels": [], "entities": []}, {"text": "On the one hand, an average word aligner does not consider the syntax information of both languages, and the output links may violate syntactic correspondence.", "labels": [], "entities": []}, {"text": "That is, some SL words yielded by a SL parse tree node may not be traced to, via alignment links, some TL words with legitimate syntactic structure.", "labels": [], "entities": []}, {"text": "On the other hand, parser design is a monolingual activity and its impact on MT is not well studied.", "labels": [], "entities": [{"text": "parser design", "start_pos": 19, "end_pos": 32, "type": "TASK", "confidence": 0.8840597867965698}, {"text": "MT", "start_pos": 77, "end_pos": 79, "type": "TASK", "confidence": 0.9882833957672119}]}, {"text": "Many good translation rules may thus be filtered by a good monolingual parser.", "labels": [], "entities": []}, {"text": "In this paper we will focus on the translation task from Chinese to English, and the string-to-tree SSMT model as elaborated in ().", "labels": [], "entities": [{"text": "translation task", "start_pos": 35, "end_pos": 51, "type": "TASK", "confidence": 0.8916175067424774}, {"text": "SSMT", "start_pos": 100, "end_pos": 104, "type": "TASK", "confidence": 0.8602569103240967}]}, {"text": "There are two kinds of translation rules in this model, minimal rules, and composed rules, which are composition of minimal rules.", "labels": [], "entities": []}, {"text": "The minimal rules are extracted from a special kind of nodes, known as frontier nodes, on TL parse tree.", "labels": [], "entities": []}, {"text": "The concept of frontier node can be illustrated by, which shows two partial bilingual sentences with the corresponding TL sub-trees and word alignment links.", "labels": [], "entities": []}, {"text": "The TL words yielded by a TL parse node can be traced to the corresponding SL words through alignment links.", "labels": [], "entities": []}, {"text": "In the diagram, each parse node is represented by a rectangle, showing the phrase label, span, and complement span respectively.", "labels": [], "entities": []}, {"text": "The span of a TL node is defined as the minimal contiguous SL string that covers all the SL words reachable from . The complement span of is the union of spans of all the nodes that are neither descendants nor ancestors of (c.f.)", "labels": [], "entities": []}, {"text": "A frontier node is anode of which the span and the complement span do not overlap with each other.", "labels": [], "entities": []}, {"text": "In the diagram, frontier nodes are grey in color.", "labels": [], "entities": []}, {"text": "Frontier node is the key in the SSMT model, as it identifies the bilingual information which is consistent with both the parse tree and alignment matrix.", "labels": [], "entities": [{"text": "SSMT", "start_pos": 32, "end_pos": 36, "type": "TASK", "confidence": 0.9752824306488037}]}, {"text": "There are two major problems in the SSMT model.", "labels": [], "entities": [{"text": "SSMT", "start_pos": 36, "end_pos": 40, "type": "TASK", "confidence": 0.9705625176429749}]}, {"text": "The first one is the violation of syntactic structure by incorrect alignment links, as shown by the two dashed links in(a).", "labels": [], "entities": []}, {"text": "These two incorrect links hinder the extraction of a good minimal rule \"\u6be1\u623f \" and that of a good composed rule \"\u7267\u6c11, \u7684 NP(DT(the), NN(herdsmen), POS('s)) \".", "labels": [], "entities": []}, {"text": "By and large, incorrect alignment links lead to translation rules that are large in size, few in number, and poor in generalization ability).", "labels": [], "entities": [{"text": "translation", "start_pos": 48, "end_pos": 59, "type": "TASK", "confidence": 0.9654607772827148}]}, {"text": "The second problem is parsing error, as shown in(b).", "labels": [], "entities": [{"text": "parsing", "start_pos": 22, "end_pos": 29, "type": "TASK", "confidence": 0.9789799451828003}, {"text": "error", "start_pos": 30, "end_pos": 35, "type": "METRIC", "confidence": 0.496136873960495}]}, {"text": "The incorrect POS tagging of the word \"lectures\" causes a series of parsing errors, including the absence of the noun phrase \"NP(NN(propaganda), NN(lectures))\".", "labels": [], "entities": [{"text": "POS tagging of the word \"lectures\"", "start_pos": 14, "end_pos": 48, "type": "TASK", "confidence": 0.7716150209307671}]}, {"text": "These parsing errors hinder the extraction of good rules, such as \" \u5ba3 \u8bb2 NP(NN(propaganda), NN(lectures)) \".", "labels": [], "entities": []}, {"text": "Note that in(a), the parse tree is correct, and the incorrect alignment links might be fixed if the aligner takes the parse tree into consideration.", "labels": [], "entities": []}, {"text": "Similarly, in(b) some parsing errors might be fixed if the parser takes into consideration the correct alignment links about \"propaganda\" and \"lecture\".", "labels": [], "entities": []}, {"text": "That is, alignment errors and parsing might be fixed if word aligner and parser are not mutually independent.", "labels": [], "entities": [{"text": "parsing", "start_pos": 30, "end_pos": 37, "type": "TASK", "confidence": 0.9651741981506348}]}, {"text": "In this paper, we emphasize more on the correction of parsing errors by exploiting alignment information.", "labels": [], "entities": [{"text": "parsing", "start_pos": 54, "end_pos": 61, "type": "TASK", "confidence": 0.6419798135757446}]}, {"text": "The general approach is to re-train a parser with parse trees which are the most consistent with alignment matrices.", "labels": [], "entities": []}, {"text": "Our first strategy is to apply the idea of targeted selftraining) with the simple evaluation function of frontier set size.", "labels": [], "entities": []}, {"text": "That is to re-train the parser with the parse trees which give rise to the largest number of frontier nodes.", "labels": [], "entities": []}, {"text": "The second strategy is to apply forced alignment to bilingual data and select the parse trees generated by our SSMT system for re-training the parser.", "labels": [], "entities": []}, {"text": "Besides, although we do not invent anew word aligner exploiting syntactic information, we propose anew method to symmetrize the alignment matrices of two directions by taking parse tree into consideration.", "labels": [], "entities": []}], "datasetContent": [{"text": "The first solution is based on targeted self-training (TST)).", "labels": [], "entities": []}, {"text": "In standard selftraining, the top one parse trees produced by the current parser are taken as training data for the next round, and the training objective is still the correctness of monolingual syntactic structure.", "labels": [], "entities": []}, {"text": "In targeted self-training, the training objective shifts to certain external evaluation function.", "labels": [], "entities": []}, {"text": "For each sentence, the n-best parse trees from the current parser are re-ranked in accordance with this external evaluation function, and the top one of the re-ranked candidates is then selected as training data for the next round.", "labels": [], "entities": []}, {"text": "The key of targeted selftraining is the definition of this external evaluation function.", "labels": [], "entities": []}, {"text": "As shown by the example in(b), an incorrect parse tree is likely to hinder the extraction of good translation rules, because the number of frontier nodes in the incorrect tree is in general smaller than that in the correct tree.", "labels": [], "entities": []}, {"text": "Consider the example in, which is about the same partial bilingual sentence as in(b).", "labels": [], "entities": []}, {"text": "Although both parse trees do not have the correct syntactic structure, the tree in has more frontier nodes, leads to more valid translation rules, and is therefore more preferable.", "labels": [], "entities": []}, {"text": "This example suggests a very simple external evaluation function, viz.", "labels": [], "entities": []}, {"text": "the size of frontier set.", "labels": [], "entities": []}, {"text": "Given a bilingual sentence, its alignment matrix, and the N-best parse trees of the TL sentence, we will calculate the number of frontier nodes for each parse tree, and re-rank the parse trees in its descending order.", "labels": [], "entities": []}, {"text": "The new top one parse tree is selected as the training data for the next round of targeted self-training of the TL parser.", "labels": [], "entities": []}, {"text": "In the following we will call this approach as targeted self-training with frontier set based evaluation (TST-FS).", "labels": [], "entities": []}, {"text": "Note that the size of the N-best list should be kept small.", "labels": [], "entities": []}, {"text": "It is because sometimes a parse tree with an extremely mistaken structure happens to have perfect match with the alignment matrix, thereby giving rise to nearest the largest frontier set size.", "labels": [], "entities": []}, {"text": "It is empirically found that a 5-best list of parse trees is already sufficient to significantly improve translation performance.", "labels": [], "entities": [{"text": "translation", "start_pos": 105, "end_pos": 116, "type": "TASK", "confidence": 0.9596399068832397}]}, {"text": "In this section, we conduct experiments on Chinese to English translation task to test our proposed methods of parser re-training and word alignment symmetrization.", "labels": [], "entities": [{"text": "Chinese to English translation task", "start_pos": 43, "end_pos": 78, "type": "TASK", "confidence": 0.6599247217178345}, {"text": "parser re-training", "start_pos": 111, "end_pos": 129, "type": "TASK", "confidence": 0.9019859433174133}, {"text": "word alignment symmetrization", "start_pos": 134, "end_pos": 163, "type": "TASK", "confidence": 0.8605926036834717}]}, {"text": "The evaluation method is the case insensitive IBM BLEU-4 ().", "labels": [], "entities": [{"text": "BLEU-4", "start_pos": 50, "end_pos": 56, "type": "METRIC", "confidence": 0.9346416592597961}]}, {"text": "Significant testing is carried out using bootstrap resampling method proposed by Koehn (2004) with a 95% confidence level.", "labels": [], "entities": [{"text": "bootstrap resampling", "start_pos": 41, "end_pos": 61, "type": "TASK", "confidence": 0.7092767059803009}]}, {"text": "We test our method with two data settings: one is IWSLT data set, the other is NIST data set..", "labels": [], "entities": [{"text": "IWSLT data set", "start_pos": 50, "end_pos": 64, "type": "DATASET", "confidence": 0.9082288146018982}, {"text": "NIST data set.", "start_pos": 79, "end_pos": 93, "type": "DATASET", "confidence": 0.9478592673937479}]}, {"text": "Baselines for NIST data set Our IWSLT data is the IWSLT 2009 dialog task data set.", "labels": [], "entities": [{"text": "NIST data set", "start_pos": 14, "end_pos": 27, "type": "DATASET", "confidence": 0.9293616414070129}, {"text": "IWSLT data", "start_pos": 32, "end_pos": 42, "type": "DATASET", "confidence": 0.9078139364719391}, {"text": "IWSLT 2009 dialog task data set", "start_pos": 50, "end_pos": 81, "type": "DATASET", "confidence": 0.9337795873483022}]}, {"text": "The training data include the BTEC and SLDB training data.", "labels": [], "entities": [{"text": "BTEC", "start_pos": 30, "end_pos": 34, "type": "DATASET", "confidence": 0.9533497095108032}, {"text": "SLDB training data", "start_pos": 39, "end_pos": 57, "type": "DATASET", "confidence": 0.818399707476298}]}, {"text": "The training data contains 81k sentence pairs, 655k Chinese words and 806k English words.", "labels": [], "entities": []}, {"text": "The language model is 5-gram language model trained with the English sentences in the training data.", "labels": [], "entities": []}, {"text": "We use the combination of dev8 and dialog as development set, and dev9 as test set.", "labels": [], "entities": []}, {"text": "The TL sentences of the training data with the selected/generated trees are used as the training data to re-train the parser.", "labels": [], "entities": []}, {"text": "To get the baseline of this setting, we run IDG to combine the bidirection alignment generated by, and run Berkeley parser to parse the target sentences.", "labels": [], "entities": []}, {"text": "With the baseline alignments and syntactic trees, we extract rules and calculate features.", "labels": [], "entities": []}, {"text": "The baseline results are shown in.", "labels": [], "entities": []}, {"text": "For the NIST data set, the bilingual training data we used is NIST 2008 training set excluding the Hong Kong Law and Hong Kong Hansard.", "labels": [], "entities": [{"text": "NIST data set", "start_pos": 8, "end_pos": 21, "type": "DATASET", "confidence": 0.9894482294718424}, {"text": "NIST 2008 training set", "start_pos": 62, "end_pos": 84, "type": "DATASET", "confidence": 0.9795101583003998}, {"text": "Hong Kong Law and Hong Kong Hansard", "start_pos": 99, "end_pos": 134, "type": "DATASET", "confidence": 0.8902739541871207}]}, {"text": "The training data contains 354k sentence pairs, 8M Chinese words and 10M English words, and is also the training data for our parser re-training.", "labels": [], "entities": []}, {"text": "The language model is 5-gram language model trained with the Giga-Word corpus plus the English sentences in the training data.", "labels": [], "entities": [{"text": "Giga-Word corpus", "start_pos": 61, "end_pos": 77, "type": "DATASET", "confidence": 0.9081599116325378}]}, {"text": "The development data to tune the feature weights of our decoder is NIST 2003 evaluation set, and test sets are NIST 2005 and 2008 evaluation sets.", "labels": [], "entities": [{"text": "NIST 2003 evaluation set", "start_pos": 67, "end_pos": 91, "type": "DATASET", "confidence": 0.9310363382101059}, {"text": "NIST 2005 and 2008 evaluation sets", "start_pos": 111, "end_pos": 145, "type": "DATASET", "confidence": 0.9231038391590118}]}, {"text": "The baseline for NIST data is got in a similar way with for IWSLT, which are shown in .", "labels": [], "entities": [{"text": "NIST data", "start_pos": 17, "end_pos": 26, "type": "DATASET", "confidence": 0.891324371099472}, {"text": "IWSLT", "start_pos": 60, "end_pos": 65, "type": "DATASET", "confidence": 0.879274845123291}]}], "tableCaptions": [{"text": " Table 1. Baselines for IWSLT data set", "labels": [], "entities": [{"text": "IWSLT data set", "start_pos": 24, "end_pos": 38, "type": "DATASET", "confidence": 0.9017072717348734}]}, {"text": " Table 2. Baselines for NIST data set", "labels": [], "entities": [{"text": "NIST data set", "start_pos": 24, "end_pos": 37, "type": "DATASET", "confidence": 0.9275370836257935}]}, {"text": " Table 5. MT performance of symmetrization  methods on IWSLT data set. The results in bold  type are significantly better than the performance  of IDG.", "labels": [], "entities": [{"text": "MT", "start_pos": 10, "end_pos": 12, "type": "TASK", "confidence": 0.8328049182891846}, {"text": "IWSLT data set", "start_pos": 55, "end_pos": 69, "type": "DATASET", "confidence": 0.9707961877187093}]}, {"text": " Table 6. MT performance of symmetrization  methods on NIST data. The results in bold type are  significantly better than the performance of IDG.", "labels": [], "entities": [{"text": "MT", "start_pos": 10, "end_pos": 12, "type": "TASK", "confidence": 0.9174807071685791}, {"text": "NIST data", "start_pos": 55, "end_pos": 64, "type": "DATASET", "confidence": 0.9261516034603119}]}, {"text": " Table 3. MT performance of parser re-training  strategies on IWSLT data set. The results in  bold type are significantly better than the  baseline.", "labels": [], "entities": [{"text": "MT", "start_pos": 10, "end_pos": 12, "type": "TASK", "confidence": 0.8544524908065796}, {"text": "IWSLT data set", "start_pos": 62, "end_pos": 76, "type": "DATASET", "confidence": 0.9850569367408752}]}, {"text": " Table 4. MT performance of parser re-training  strategies on NIST data set. The results in bold  type are significantly better than the baseline.", "labels": [], "entities": [{"text": "MT", "start_pos": 10, "end_pos": 12, "type": "TASK", "confidence": 0.9333067536354065}, {"text": "NIST data set", "start_pos": 62, "end_pos": 75, "type": "DATASET", "confidence": 0.9847278594970703}]}, {"text": " Table 7. MT performance of the new methods  on IWSLT data set. The results in bold type  are significantly better than the baseline.", "labels": [], "entities": [{"text": "MT", "start_pos": 10, "end_pos": 12, "type": "TASK", "confidence": 0.7512791752815247}, {"text": "IWSLT data set", "start_pos": 48, "end_pos": 62, "type": "DATASET", "confidence": 0.9734994371732076}]}, {"text": " Table 8. MT performance of the new methods  on NIST data set. The results in bold type are  significantly better than the baseline.", "labels": [], "entities": [{"text": "MT", "start_pos": 10, "end_pos": 12, "type": "TASK", "confidence": 0.8737145066261292}, {"text": "NIST data set", "start_pos": 48, "end_pos": 61, "type": "DATASET", "confidence": 0.9607025980949402}]}]}