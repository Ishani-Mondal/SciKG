{"title": [{"text": "A Bayesian Model for Learning SCFGs with Discontiguous Rules", "labels": [], "entities": []}], "abstractContent": [{"text": "We describe a nonparametric model and corresponding inference algorithm for learning Synchronous Context Free Grammar derivations for parallel text.", "labels": [], "entities": []}, {"text": "The model employs a Pitman-Yor Process prior which uses a novel base distribution over synchronous grammar rules.", "labels": [], "entities": []}, {"text": "Through both synthetic grammar induction and statistical machine translation experiments, we show that our model learns complex translational correspondences-including discontiguous, many-to-many alignments-and produces competitive translation results.", "labels": [], "entities": [{"text": "synthetic grammar induction", "start_pos": 13, "end_pos": 40, "type": "TASK", "confidence": 0.6735727290312449}, {"text": "statistical machine translation", "start_pos": 45, "end_pos": 76, "type": "TASK", "confidence": 0.6209862033526102}]}, {"text": "Further, inference is efficient and we present results on significantly larger corpora than prior work.", "labels": [], "entities": []}], "introductionContent": [{"text": "In the twenty years since pioneered the first word-based statistical machine translation (SMT) models substantially more expressive models of translational equivalence have been developed.", "labels": [], "entities": [{"text": "word-based statistical machine translation (SMT)", "start_pos": 46, "end_pos": 94, "type": "TASK", "confidence": 0.7410512907164437}, {"text": "translational equivalence", "start_pos": 142, "end_pos": 167, "type": "TASK", "confidence": 0.9032998979091644}]}, {"text": "The prevalence of complex phrasal, discontiguous, and non-monotonic translation phenomena in real-world applications of machine translation has driven the development of hierarchical and syntactic models based on synchronous context-free grammars (SCFGs).", "labels": [], "entities": [{"text": "machine translation", "start_pos": 120, "end_pos": 139, "type": "TASK", "confidence": 0.7313879132270813}]}, {"text": "Such models are now widely used in translation and represent the state-of-the-art inmost language pairs (.", "labels": [], "entities": [{"text": "translation", "start_pos": 35, "end_pos": 46, "type": "TASK", "confidence": 0.9737547636032104}]}, {"text": "However, while the models used for translation have evolved, the way in which they are learnt has not: na\u00a8\u0131vena\u00a8\u0131ve word-based models are still used to infer translational correspondences from parallel corpora.", "labels": [], "entities": [{"text": "translation", "start_pos": 35, "end_pos": 46, "type": "TASK", "confidence": 0.9681234955787659}]}, {"text": "In this work we bring the learning of the minimal units of translation in step with the representational power of modern translation models.", "labels": [], "entities": []}, {"text": "We present a nonparametric Bayesian model of translation based on SCFGs, and we use its posterior distribution to infer synchronous derivations fora parallel corpus using a novel Gibbs sampler.", "labels": [], "entities": [{"text": "translation", "start_pos": 45, "end_pos": 56, "type": "TASK", "confidence": 0.9602923393249512}]}, {"text": "Our model is able to: 1) directly model many-to-many alignments, thereby capturing non-compositional and idiomatic translations; 2) align discontiguous phrases in both the source and target languages; 3) have no restrictions on the length of a rule, the number of nonterminal symbols per rule, or their configuration.", "labels": [], "entities": []}, {"text": "Learning synchronous grammars is hard due to the high polynomial complexity of dynamic programming and the exponential space of possible rules.", "labels": [], "entities": []}, {"text": "As such most prior work for learning SCFGs has relied on inference algorithms that were heuristically constrained or biased by word-based alignment models and small experiments.", "labels": [], "entities": []}, {"text": "In contrast to these previous attempts, our SCFG model scales to large datasets (over 1.3M sentence pairs) without imposing restrictions on the form of the grammar rules or otherwise constraining the set of learnable rules (e.g., with a word alignment).", "labels": [], "entities": [{"text": "word alignment", "start_pos": 237, "end_pos": 251, "type": "TASK", "confidence": 0.6985581070184708}]}, {"text": "We validate our sampler by demonstrating its ability to recover grammars used to generate synthetic datasets.", "labels": [], "entities": []}, {"text": "We then evaluate our model by inducing word alignments for SMT experiments in several typologically diverse language pairs and across a range of corpora sizes.", "labels": [], "entities": [{"text": "word alignments", "start_pos": 39, "end_pos": 54, "type": "TASK", "confidence": 0.7048841267824173}, {"text": "SMT", "start_pos": 59, "end_pos": 62, "type": "TASK", "confidence": 0.9928604960441589}]}, {"text": "Our results attest to our model's ability to learn synchronous grammars encoding complex translation phenomena.", "labels": [], "entities": []}], "datasetContent": [{"text": "The preceding sections have introduced a model, and accompanying inference technique, designed to induce a posterior distribution over SCFG derivations containing discontiguous and phrasal translation rules.", "labels": [], "entities": []}, {"text": "The evaluation that follows aims to determine our models ability to meet these design goals, and to do so in a range of translation scenarios.", "labels": [], "entities": []}, {"text": "In order to validate both the model and the sampler's ability to learn an SCFG we first conduct a synthetic experiment in which the true grammar is known.", "labels": [], "entities": []}, {"text": "Subsequently we conduct a series of experiments on real parallel corpora of increasing sizes to explore the empirical properties of our model.", "labels": [], "entities": []}, {"text": "Prior work on SCFG induction for SMT has validated modeling claims by reporting BLEU scores on real translation tasks.", "labels": [], "entities": [{"text": "SCFG induction", "start_pos": 14, "end_pos": 28, "type": "TASK", "confidence": 0.9164666831493378}, {"text": "SMT", "start_pos": 33, "end_pos": 36, "type": "TASK", "confidence": 0.9849494695663452}, {"text": "BLEU", "start_pos": 80, "end_pos": 84, "type": "METRIC", "confidence": 0.9971969127655029}]}, {"text": "However, the combination of noisy data and the complexity of SMT pipelines conspire to obscure whether models actually achieve their design goals, normally stated in terms of an ability to induce SCFGs with particular properties.", "labels": [], "entities": [{"text": "SMT pipelines", "start_pos": 61, "end_pos": 74, "type": "TASK", "confidence": 0.9252532124519348}]}, {"text": "Here we include a small synthetic data experiment to clearly validate our models ability to learn an SCFG that includes discontiguous and phrasal translation rules with non-monotonic word order.", "labels": [], "entities": []}, {"text": "Using the probabilistic SCFG shown in the top half of we stochastically generated three thousand parallel sentence pairs as training data for our model.", "labels": [], "entities": []}, {"text": "We then ran the Gibbs sampler for fifty iterations through the data.", "labels": [], "entities": []}, {"text": "The bottom half of lists the five rules with the highest marginal probability estimated by the sampler.", "labels": [], "entities": []}, {"text": "Encouragingly our model was able to recover a grammar very close to the original.", "labels": [], "entities": []}, {"text": "Even for such a small grammar the space of derivations is enormous and the task of recovering it from a data sample is non-trivial.", "labels": [], "entities": []}, {"text": "The divergence from the true probabilities is due to the effect of the prior assigning shorter rules higher probability.", "labels": [], "entities": []}, {"text": "With a larger data sample we would expect the influence of the prior in the posterior to diminish.", "labels": [], "entities": []}, {"text": "Ultimately the efficacy of a model for SCFG induction will be judged on its ability to underpin a stateof-the-art SMT system.", "labels": [], "entities": [{"text": "SCFG induction", "start_pos": 39, "end_pos": 53, "type": "TASK", "confidence": 0.9709070920944214}, {"text": "SMT", "start_pos": 114, "end_pos": 117, "type": "TASK", "confidence": 0.9927324652671814}]}, {"text": "Here we evaluate our model by applying it to learning word alignments for parallel corpora from which SMT systems are induced.", "labels": [], "entities": [{"text": "learning word alignments", "start_pos": 45, "end_pos": 69, "type": "TASK", "confidence": 0.6401514112949371}, {"text": "SMT", "start_pos": 102, "end_pos": 105, "type": "TASK", "confidence": 0.986478328704834}]}, {"text": "We train models across a range of corpora sizes and for language pairs that exhibit the type of complex alignment phenomena that we are interested in modeling: Chinese \u2192 English (ZH-EN), Urdu \u2192 English (UR-EN) and German \u2192 English (DE-EN).", "labels": [], "entities": []}, {"text": "To obtain the PYP-SCFG word alignments we ran the sampler for five hundred iterations for each of the language pairs and experimental conditions described below.", "labels": [], "entities": [{"text": "PYP-SCFG word alignments", "start_pos": 14, "end_pos": 38, "type": "TASK", "confidence": 0.5571827590465546}]}, {"text": "We used the approach of to distribute the sampler across multiple threads.", "labels": [], "entities": []}, {"text": "The strength \u03b8 and discount d hyperparameters of the Pitman-Yor Processes, and the terminal penalty \u03c6 (Section 3.3), were inferred using slice sampling).", "labels": [], "entities": [{"text": "terminal penalty \u03c6", "start_pos": 83, "end_pos": 101, "type": "METRIC", "confidence": 0.8626958529154459}]}, {"text": "The Gibbs sampler requires an initial set of derivations from which to commence sampling.", "labels": [], "entities": []}, {"text": "In our experiments we investigated both weak and a strong initialisations, the former based on word alignments from IBM Model 1 and the latter on alignments from an HMM model ().", "labels": [], "entities": []}, {"text": "For decoding we used the word alignments implied by the derivations in the final sample to extract a Hiero grammar with the same standard set of relative frequency, length, and language model features used for the baseline.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: Results for the SMT experiments in BLEU . The baseline is produced using a full GIZA++ run. The  MODEL 1 INITIALISATION column is from the initialisation alignments using MODEL 1 and no sampling.", "labels": [], "entities": [{"text": "SMT", "start_pos": 26, "end_pos": 29, "type": "TASK", "confidence": 0.9964832067489624}, {"text": "BLEU", "start_pos": 45, "end_pos": 49, "type": "METRIC", "confidence": 0.6883881092071533}, {"text": "MODEL 1 INITIALISATION column", "start_pos": 107, "end_pos": 136, "type": "METRIC", "confidence": 0.7395374923944473}]}]}