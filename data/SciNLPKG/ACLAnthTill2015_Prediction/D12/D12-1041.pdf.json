{"title": [{"text": "Forced Derivation Tree based Model Training to Statistical Machine Translation", "labels": [], "entities": [{"text": "Statistical Machine Translation", "start_pos": 47, "end_pos": 78, "type": "TASK", "confidence": 0.7860740025838217}]}], "abstractContent": [{"text": "A forced derivation tree (FDT) of a sentence pair {f, e} denotes a derivation tree that can translate f into its accurate target translation e.", "labels": [], "entities": []}, {"text": "In this paper, we present an approach that leverages structured knowledge contained in FDTs to train component models for statistical machine translation (SMT) systems.", "labels": [], "entities": [{"text": "FDTs", "start_pos": 87, "end_pos": 91, "type": "DATASET", "confidence": 0.8392115235328674}, {"text": "statistical machine translation (SMT)", "start_pos": 122, "end_pos": 159, "type": "TASK", "confidence": 0.8039788454771042}]}, {"text": "We first describe how to generate different FDTs for each sentence pair in training corpus, and then present how to infer the optimal FDTs based on their derivation and alignment qualities.", "labels": [], "entities": []}, {"text": "As the first step in this line of research, we verify the effectiveness of our approach in a BTG-based phrasal system, and propose four FDT-based component models.", "labels": [], "entities": []}, {"text": "Experiments are carried out on large scale English-to-Japanese and Chinese-to-English translation tasks, and significant improvements are reported on both translation quality and alignment quality.", "labels": [], "entities": [{"text": "Chinese-to-English translation tasks", "start_pos": 67, "end_pos": 103, "type": "TASK", "confidence": 0.7295621633529663}]}], "introductionContent": [{"text": "Most of today's SMT systems depends heavily on parallel corpora aligned at the word-level to train their different component models.", "labels": [], "entities": [{"text": "SMT", "start_pos": 16, "end_pos": 19, "type": "TASK", "confidence": 0.992300271987915}]}, {"text": "However, such annotations do have their drawbacks in training.", "labels": [], "entities": []}, {"text": "On one hand, word links predicted by automatic aligners such as GIZA++ () often contain errors.", "labels": [], "entities": []}, {"text": "This problem gets even worse on language pairs that differ substantially in word orders, such as English and Japanese/Korean/German.", "labels": [], "entities": []}, {"text": "The descent of the word alignment quality will lead to inaccurate component models straightforwardly.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 19, "end_pos": 33, "type": "TASK", "confidence": 0.6763907968997955}]}, {"text": "On the other hand, several component models are designed to supervise the decoding procedures, which usually rely on training examples extracted from word-aligned sentence pairs, such as distortion models) and sequence models ().", "labels": [], "entities": []}, {"text": "Ideally, training examples of models are expected to match most of the situations that could be met in decoding procedures.", "labels": [], "entities": []}, {"text": "But actually, plain structures of word alignments are too coarse to provide enough knowledge to ensure this expectation.", "labels": [], "entities": [{"text": "word alignments", "start_pos": 34, "end_pos": 49, "type": "TASK", "confidence": 0.6718482226133347}]}, {"text": "This paper presents an FDT-based model training approach to SMT systems by leveraging structured knowledge contained in FDTs.", "labels": [], "entities": [{"text": "FDT-based model training", "start_pos": 23, "end_pos": 47, "type": "TASK", "confidence": 0.6036656498908997}, {"text": "SMT", "start_pos": 60, "end_pos": 63, "type": "TASK", "confidence": 0.9964303374290466}]}, {"text": "An FDT of a sentence pair {f, e} denotes a derivation tree that can translate f into its accurate target translation e.", "labels": [], "entities": [{"text": "FDT", "start_pos": 3, "end_pos": 6, "type": "METRIC", "confidence": 0.9305315613746643}]}, {"text": "The principle advantage of this work is two-fold.", "labels": [], "entities": []}, {"text": "First, using alignments induced from the 1-best FDTs of all sentence pairs, the overall alignment quality of training corpus can be improved.", "labels": [], "entities": []}, {"text": "Second, comparing to word alignments, FDTs can provide richer structured knowledge for various component models to extract training instances.", "labels": [], "entities": [{"text": "word alignments", "start_pos": 21, "end_pos": 36, "type": "TASK", "confidence": 0.7358797788619995}]}, {"text": "Our FDT-based model training approach performs via three steps: (1) generation, where an FDT space composed of different FDTs is generated for each sentence pair in training corpus by the forced decoding technique; (2) inference, where the optimal FDTs are extracted from the FDT space of each sentence pair based on both derivation and alignment qualities measured by a memory-based re-ranking model; (3) training, where various component models are trained based on the optimal FDTs extracted in the inference step.", "labels": [], "entities": []}, {"text": "Our FDT-based model training approach can be adapted to SMT systems with arbitrary paradigms.", "labels": [], "entities": [{"text": "FDT-based model training", "start_pos": 4, "end_pos": 28, "type": "TASK", "confidence": 0.5909290313720703}, {"text": "SMT", "start_pos": 56, "end_pos": 59, "type": "TASK", "confidence": 0.9904740452766418}]}, {"text": "As the first step in this line of research, our approach is verified in a phrase-based SMT system on both English-to-Japanese and Chinese-to-English translation tasks . Significant improvements are reported on both translation quality (up to 1.31 BLEU) and word alignment quality (up to 3.15 F-score).", "labels": [], "entities": [{"text": "SMT", "start_pos": 87, "end_pos": 90, "type": "TASK", "confidence": 0.8987265229225159}, {"text": "BLEU", "start_pos": 247, "end_pos": 251, "type": "METRIC", "confidence": 0.9905799031257629}, {"text": "word alignment", "start_pos": 257, "end_pos": 271, "type": "TASK", "confidence": 0.759856253862381}, {"text": "F-score", "start_pos": 292, "end_pos": 299, "type": "METRIC", "confidence": 0.9967843294143677}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: FDT-based model training on E-J task.", "labels": [], "entities": [{"text": "FDT-based model training", "start_pos": 10, "end_pos": 34, "type": "TASK", "confidence": 0.45685962835947674}]}, {"text": " Table 2: FDT-based model training on C-E task", "labels": [], "entities": [{"text": "FDT-based model training", "start_pos": 10, "end_pos": 34, "type": "TASK", "confidence": 0.46566925446192425}]}, {"text": " Table 3: Comparison of alignment qualities predicted by  GIZA++ and induced from 1-best FDTs.", "labels": [], "entities": [{"text": "FDTs", "start_pos": 89, "end_pos": 93, "type": "DATASET", "confidence": 0.6595326662063599}]}]}