{"title": [{"text": "Entity based Q&A retrieval", "labels": [], "entities": [{"text": "Entity based Q&A retrieval", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.6023410807053248}]}], "abstractContent": [{"text": "Bridging the lexical gap between the user's question and the question-answer pairs in the Q&A archives has been a major challenge for Q&A retrieval.", "labels": [], "entities": [{"text": "Q&A archives", "start_pos": 90, "end_pos": 102, "type": "DATASET", "confidence": 0.7209813445806503}, {"text": "Q&A retrieval", "start_pos": 134, "end_pos": 147, "type": "TASK", "confidence": 0.7507138252258301}]}, {"text": "State-of-the-art approaches address this issue by implicitly expanding the queries with additional words using statistical translation models.", "labels": [], "entities": []}, {"text": "While useful, the effectiveness of these models is highly dependant on the availability of quality corpus in the absence of which they are troubled by noise issues.", "labels": [], "entities": []}, {"text": "Moreover these models perform word based expansion in a context agnostic manner resulting in translation that might be mixed and fairly general.", "labels": [], "entities": [{"text": "word based expansion", "start_pos": 30, "end_pos": 50, "type": "TASK", "confidence": 0.6335004568099976}]}, {"text": "This results in degraded retrieval performance.", "labels": [], "entities": []}, {"text": "In this work we address the above issues by extending the lexical word based translation model to incorporate semantic concepts (entities).", "labels": [], "entities": [{"text": "lexical word based translation", "start_pos": 58, "end_pos": 88, "type": "TASK", "confidence": 0.7134905606508255}]}, {"text": "We explore strategies to learn the translation probabilities between words and the concepts using the Q&A archives and a popular entity catalog.", "labels": [], "entities": []}, {"text": "Experiments conducted on a large scale real data show that the proposed techniques are promising.", "labels": [], "entities": []}], "introductionContent": [{"text": "Over the past few years community-based question answering (CQA) portals like Naver, Yahoo!", "labels": [], "entities": [{"text": "question answering (CQA)", "start_pos": 40, "end_pos": 64, "type": "TASK", "confidence": 0.8051165401935577}]}, {"text": "Answers, Baidu Zhidao and WikiAnswers have attracted great attention from both academia and industry).", "labels": [], "entities": [{"text": "Answers", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.9484652280807495}]}, {"text": "These portals foster collaborative creation of content by allowing the users to both submit questions to be answered and answer questions asked by other users.", "labels": [], "entities": []}, {"text": "These portals aim to provide highly focused access to this information by directly returning pertinent question and answer (Q&A) pairs to the users questions, instead of along list of ranked URLs.", "labels": [], "entities": [{"text": "pertinent question and answer (Q&A)", "start_pos": 93, "end_pos": 128, "type": "TASK", "confidence": 0.6513817575242784}]}, {"text": "This is in noted contrast to the usual search paradigm, where the question is used to search the database of potential answers, in this case the question is used to search the database of previous questions, which in turn are associated with answers.", "labels": [], "entities": []}, {"text": "This involves addressing the word mismatch problem between the users question and the question-answer pairs in the archive.", "labels": [], "entities": []}, {"text": "This is the major challenge for Q&A retrieval.", "labels": [], "entities": [{"text": "Q&A retrieval", "start_pos": 32, "end_pos": 45, "type": "TASK", "confidence": 0.9105885177850723}]}, {"text": "Researchers have proposed the use of translation models to solve this problem.", "labels": [], "entities": [{"text": "translation", "start_pos": 37, "end_pos": 48, "type": "TASK", "confidence": 0.969610333442688}]}, {"text": "As a principled approach to capturing semantic word relations, statistical translation language models are built by using the IBM model 1 ( and have been shown to outperform traditional document language models on Q&A retrieval task.", "labels": [], "entities": [{"text": "statistical translation language", "start_pos": 63, "end_pos": 95, "type": "TASK", "confidence": 0.7597301999727885}, {"text": "IBM model 1", "start_pos": 126, "end_pos": 137, "type": "DATASET", "confidence": 0.8925362030665079}]}, {"text": "The basic idea is to estimate the likelihood of translating a document 1 to a query by exploiting the dependencies that exists between query words and document words.", "labels": [], "entities": []}, {"text": "For example the document containing the word Wheezing may well answer the question containing the term Asthma.", "labels": [], "entities": [{"text": "Wheezing", "start_pos": 45, "end_pos": 53, "type": "TASK", "confidence": 0.8358890414237976}, {"text": "Asthma", "start_pos": 103, "end_pos": 109, "type": "METRIC", "confidence": 0.7424145340919495}]}, {"text": "They learn the these dependencies (encoded as translation probabilities) between words using parallel mono-lingual corpora created from the Q&A pairs.", "labels": [], "entities": []}, {"text": "While useful, the effectiveness of these models is highly dependant on the availability of quality corpus (Lee et al., . Also these models only capture shallow semantics between words via the co-occurrence statistics, while some of the more explicit relationships between words and entities is freely available externally.", "labels": [], "entities": []}, {"text": "Being context agnostic () is another very common criticism hailed on translation models as it results in noisy and generic translations.", "labels": [], "entities": []}, {"text": "Example shown in captures these problems.", "labels": [], "entities": []}, {"text": "Specifically, the word Blizzard can refer to an American game development company that develops World of Warcraft game or it could refer to a severe snowstorm.", "labels": [], "entities": []}, {"text": "Expanding query without taking the gaming context established by the word WOW (acronym for World of Warcraft) into account would lead to topic drift.", "labels": [], "entities": [{"text": "WOW (acronym for World of Warcraft)", "start_pos": 74, "end_pos": 109, "type": "TASK", "confidence": 0.5101024992763996}]}, {"text": "Also it would be difficult to learn relationships between World of Warcraft Burning Crusade and Blizzard from the Q&A corpus alone due to the sparsity of co-occurance counts as these can be expressed in several lexical forms, some of which are multi word phrases.", "labels": [], "entities": []}, {"text": "In this paper we argue that solution to all the above problems lies in a unified model in which entities area primary citizen.", "labels": [], "entities": []}, {"text": "The guiding hypothesis being, an entity based representation provides a less ambiguous representation of the users question and provides fora more semantically accurate expansion if the relationship between entities and words can be estimated more reliably.", "labels": [], "entities": []}], "datasetContent": [{"text": "We used 2010 version of Wikipedia as our knowledge base.", "labels": [], "entities": [{"text": "2010 version of Wikipedia", "start_pos": 8, "end_pos": 33, "type": "DATASET", "confidence": 0.8215380162000656}]}, {"text": "It contains more than 2.5 million entities.", "labels": [], "entities": []}, {"text": "Annotations were done by volunteers fluent in english.", "labels": [], "entities": []}, {"text": "Volunteers were told to be as exhaustive as possible and tag all possible name mentions, even if to mark them as \"NA\".", "labels": [], "entities": []}, {"text": "Inter-annotator agreement=92.1%; Kappa coefficient = 0.72.", "labels": [], "entities": [{"text": "Inter-annotator agreement", "start_pos": 0, "end_pos": 25, "type": "METRIC", "confidence": 0.7652839720249176}, {"text": "Kappa coefficient", "start_pos": 33, "end_pos": 50, "type": "METRIC", "confidence": 0.9853369891643524}]}, {"text": "As our corpus, we collected 8.3K manual annotations spanning 1315 Q&A pairs.", "labels": [], "entities": []}, {"text": "2.8K of the annotations were assigned to NA.", "labels": [], "entities": [{"text": "NA", "start_pos": 41, "end_pos": 43, "type": "DATASET", "confidence": 0.9350343942642212}]}, {"text": "2.1K annotations (out of 8.3K) were made in the question of which 551 were assigned to NA.", "labels": [], "entities": [{"text": "NA", "start_pos": 87, "end_pos": 89, "type": "DATASET", "confidence": 0.9533901214599609}]}, {"text": "We use Precision, Recall and F 1 score microaveraged across documents as the evaluation measures.", "labels": [], "entities": [{"text": "Precision", "start_pos": 7, "end_pos": 16, "type": "METRIC", "confidence": 0.9976771473884583}, {"text": "Recall", "start_pos": 18, "end_pos": 24, "type": "METRIC", "confidence": 0.988867998123169}, {"text": "F 1 score microaveraged", "start_pos": 29, "end_pos": 52, "type": "METRIC", "confidence": 0.9663558602333069}]}, {"text": "We do a linear scan of data to identify entity mentions by first tokenizing and then identifying token sequences that maximally match an entity ID in the entity name dictionary (constructed using Wikipedia anchor text, redirect pages).", "labels": [], "entities": []}, {"text": "outlines the performance of EA off line and EA online . We measured EA off line in 3 test data configurations; (1) EA off line : measured over entire entire Q&A pair is the context, while here only question part is the context.", "labels": [], "entities": []}, {"text": "This is done to check if separate annotators are required for online and online phase.", "labels": [], "entities": []}, {"text": "As seen in, this indeed is necessary as EA off line * performs worse than EA online . Closer look at the feature weights revealed that in EA off line context specific features have much more weightage when compared to its weight in EA online , on the contrary EA online weighs SP significantly higher.", "labels": [], "entities": [{"text": "EA", "start_pos": 40, "end_pos": 42, "type": "DATASET", "confidence": 0.8773972392082214}, {"text": "SP", "start_pos": 277, "end_pos": 279, "type": "METRIC", "confidence": 0.9614936113357544}]}, {"text": "We now describe the empirical evaluation where we compare our techniques against the baseline techniques.", "labels": [], "entities": []}, {"text": "We use several standard measures (RPrecision, MAP, MRR, Precision@k) in evaluation.", "labels": [], "entities": [{"text": "RPrecision", "start_pos": 34, "end_pos": 44, "type": "METRIC", "confidence": 0.7647849321365356}, {"text": "MAP", "start_pos": 46, "end_pos": 49, "type": "METRIC", "confidence": 0.9105128049850464}, {"text": "MRR", "start_pos": 51, "end_pos": 54, "type": "METRIC", "confidence": 0.9775571823120117}, {"text": "Precision@k", "start_pos": 56, "end_pos": 67, "type": "METRIC", "confidence": 0.9545963009198507}]}, {"text": "We first describe the dataset used followed by describing an exhaustive set of results across techniques and performance measures.", "labels": [], "entities": []}, {"text": "We crawled a dataset of \u223c5 million questions and answers from Yahoo!", "labels": [], "entities": []}, {"text": "Answers spanning all the leaf level categories.", "labels": [], "entities": []}, {"text": "Tokenization and stop word removal were the only preprocessing steps performed.", "labels": [], "entities": [{"text": "stop word removal", "start_pos": 17, "end_pos": 34, "type": "TASK", "confidence": 0.6331312358379364}]}, {"text": "We have used a stoplist 10 having a vocabulary of 429 common words to remove the stopwords.", "labels": [], "entities": []}, {"text": "In our retrieval experiments we used 339 queries (average length 5.6 words).", "labels": [], "entities": []}, {"text": "We employed pooling technique used in the TREC conference series.", "labels": [], "entities": [{"text": "TREC conference series", "start_pos": 42, "end_pos": 64, "type": "DATASET", "confidence": 0.8901671965916952}]}, {"text": "http://truereader.com/manuals/onix/stopwords1.html We pooled the top 25 Q&A pairs from retrieval results generated by varying the retrieval algorithms and the search field.", "labels": [], "entities": []}, {"text": "Relevance judgments were marked by human annotators without disclosing the identity of method used for retrieval.", "labels": [], "entities": []}, {"text": "The annotators were asked to label candidate as \"relevant\" or \"irrelevant\" based on semantic similarity with the query.", "labels": [], "entities": []}, {"text": "Answer quality/correctness was not a criteria.", "labels": [], "entities": [{"text": "Answer quality/correctness", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.6310270950198174}]}, {"text": "In case of disagreement between two volunteers, authors made the final judgment.", "labels": [], "entities": []}, {"text": "Inter-annotator agreement was 87.9% and Kappa coefficient = 0.68.", "labels": [], "entities": [{"text": "Inter-annotator agreement", "start_pos": 0, "end_pos": 25, "type": "METRIC", "confidence": 0.7030794322490692}, {"text": "Kappa coefficient", "start_pos": 40, "end_pos": 57, "type": "METRIC", "confidence": 0.9817779958248138}]}, {"text": "Over all we had collected more than 12K relevance judgements corresponding to these queries, of which >2.3K were marked as relevant.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Comparisons of retrieval models.  \u2020 indicate a statistically significant improvement over the CTM using paired  t-test with p-value < 0.05. %chg indicates change over CTM as it is the most competitive baseline", "labels": [], "entities": []}]}