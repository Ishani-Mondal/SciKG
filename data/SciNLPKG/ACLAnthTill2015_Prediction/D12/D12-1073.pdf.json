{"title": [{"text": "SSHLDA: A Semi-Supervised Hierarchical Topic Model", "labels": [], "entities": []}], "abstractContent": [{"text": "Supervised hierarchical topic modeling and unsupervised hierarchical topic modeling are usually used to obtain hierarchical topics, such as hLLDA and hLDA.", "labels": [], "entities": []}, {"text": "Supervised hierarchical topic modeling makes heavy use of the information from observed hierarchical labels, but cannot explore new topics; while unsu-pervised hierarchical topic modeling is able to detect automatically new topics in the data space, but does not make use of any information from hierarchical labels.", "labels": [], "entities": []}, {"text": "In this paper, we propose a semi-supervised hierarchical topic model which aims to explore new topics automatically in the data space while incorporating the information from observed hierarchical labels into the modeling process, called Semi-Supervised Hierarchical Latent Dirichlet Allocation (SSHLDA).", "labels": [], "entities": []}, {"text": "We also prove that hLDA and hLLDA are special cases of SSHLDA.", "labels": [], "entities": []}, {"text": "We conduct experiments on Yahoo!", "labels": [], "entities": [{"text": "Yahoo!", "start_pos": 26, "end_pos": 32, "type": "DATASET", "confidence": 0.9211044013500214}]}, {"text": "Answers and ODP datasets, and assess the performance in terms of perplexity and clustering.", "labels": [], "entities": [{"text": "ODP datasets", "start_pos": 12, "end_pos": 24, "type": "DATASET", "confidence": 0.7532290816307068}]}, {"text": "The experimental results show that predictive ability of SSHLDA is better than that of baselines, and SSHLDA can also achieve significant improvement over baselines for clustering on the FScore measure.", "labels": [], "entities": [{"text": "FScore measure", "start_pos": 187, "end_pos": 201, "type": "DATASET", "confidence": 0.8703332245349884}]}], "introductionContent": [{"text": "Topic models, such as latent Dirichlet allocation (LDA), are useful NLP tools for the statistical analysis of document collections and other discrete data.", "labels": [], "entities": [{"text": "latent Dirichlet allocation (LDA)", "start_pos": 22, "end_pos": 55, "type": "TASK", "confidence": 0.7324854334195455}]}, {"text": "Furthermore, hierarchical topic modeling is able to obtain the relations between topics -parent-child and sibling relations.", "labels": [], "entities": []}, {"text": "Unsupervised hierarchical topic modeling is able to detect automatically new topics in the data space, such as hierarchical Latent Dirichlet Allocation (hLDA) ().", "labels": [], "entities": []}, {"text": "hLDA makes use of nested Dirichlet Process to automatically obtain a L-level hierarchy of topics.", "labels": [], "entities": [{"text": "hLDA", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.934868335723877}]}, {"text": "Modern Web documents, however, are not merely collections of words.", "labels": [], "entities": []}, {"text": "They are usually documents with hierarchical labels -such as Web pages and their placement in hierarchical directories (.", "labels": [], "entities": []}, {"text": "Unsupervised hierarchical topic modeling cannot make use of any information from hierarchical labels, thus supervised hierarchical topic models, such as hierarchical Labeled Latent Dirichlet Allocation (hLLDA), are proposed to tackle this problem.", "labels": [], "entities": []}, {"text": "hLLDA uses hierarchical labels to automatically build corresponding topic for each label, but it cannot find new latent topics in the data space, only depending on hierarchy of labels.", "labels": [], "entities": []}, {"text": "As we know that only about 10% of an iceberg's mass is seen outside while about 90% of it is unseen, deep down in water.", "labels": [], "entities": []}, {"text": "We think that a corpus with hierarchical labels should include not only observed topics of labels, but also there are more latent topics, just like icebergs.", "labels": [], "entities": []}, {"text": "hLLDA can make use of the information from labels; while hLDA can explore latent topics.", "labels": [], "entities": [{"text": "hLLDA", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.8992133140563965}]}, {"text": "How can we combine the merits of the two types of models into one model?", "labels": [], "entities": []}, {"text": "An intuitive and simple combinational method is like this: first, we use hierarchy of labels as basic hierarchy, called Base Tree (BT); then we use hLDA to build automatically topic hierarchy for each leaf node in BT, called Leaf Topic Hierarchy (LTH); finally, we add each LTH to corresponding leaf in the BT and obtain a hierarchy for the entire dataset.", "labels": [], "entities": []}, {"text": "We refer the method as Simp-hLDA.", "labels": [], "entities": []}, {"text": "The performance of the Simp-hLDA is not so good, as can be seen from the example in (b).", "labels": [], "entities": []}, {"text": "The drawbacks are: (i) the leaves in BT do not obtain reasonable and right words distribution, such as \"Computers & Internet\" node in (b), its topical words, \"the to you and a\", is not about \" (ii) the non-leaf nodes in BT cannot obtain words distribution, such as \"Health\" node in (b); (iii) it is a heuristic method, and thus Simp-hLDA has no solid theoretical basis.", "labels": [], "entities": [{"text": "BT", "start_pos": 37, "end_pos": 39, "type": "DATASET", "confidence": 0.959030032157898}]}, {"text": "To tackle the above drawbacks, we explore the use of probabilistic models for such a task where the hierarchical labels are merely viewed as apart of a hierarchy of topics, and the topics of a path in the whole hierarchy generate a corresponding document.", "labels": [], "entities": []}, {"text": "Our proposed generative model learns both the latent topics of the underlying data and the labeling strategies in a joint model, by leveraging on the hierarchical structure of labels and Hierarchical Dirichlet Process.", "labels": [], "entities": []}, {"text": "We demonstrate the effectiveness of the proposed model on large, real-world datasets in the question answering and website category domains on two tasks: the topic modeling of documents, and the use of the generated topics for document clustering.", "labels": [], "entities": [{"text": "question answering", "start_pos": 92, "end_pos": 110, "type": "TASK", "confidence": 0.7106001526117325}, {"text": "document clustering", "start_pos": 227, "end_pos": 246, "type": "TASK", "confidence": 0.7026155889034271}]}, {"text": "Our results show that our joint, semi-hierarchical model outperforms the state-of-the-art supervised and unsupervised hierarchical algorithms.", "labels": [], "entities": []}, {"text": "The contributions of this paper are threefold: (1) We propose a joint, generative semi-supervised hierarchical topic model, i.e. Semi-Supervised Hierarchical Latent Dirichlet Allocation (SSHLDA), to overcome the defects of hLDA and hLLDA while combining the their merits.", "labels": [], "entities": []}, {"text": "SSHLDA is able to not only explore new latent topics in the data space, but also makes use of the information from the hierarchy of observed labels; (2) We prove that hLDA and hLLDA are special cases of SSHLDA; (3) We develop a gibbs sampling inference algorithm for the proposed model.", "labels": [], "entities": [{"text": "SSHLDA", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.8033231496810913}]}, {"text": "The remainder of this paper is organized as follows.", "labels": [], "entities": []}, {"text": "We review related work in Section 2.", "labels": [], "entities": []}, {"text": "In Section 3, we introduce some preliminaries; while we introduce SSHLDA in Section 4.", "labels": [], "entities": [{"text": "preliminaries", "start_pos": 32, "end_pos": 45, "type": "METRIC", "confidence": 0.9638747572898865}, {"text": "SSHLDA", "start_pos": 66, "end_pos": 72, "type": "DATASET", "confidence": 0.35780179500579834}]}, {"text": "Section 5 details a gibbs sampling inference algorithm for SSHLDA; while Section 6 presents the experimental results.", "labels": [], "entities": [{"text": "SSHLDA", "start_pos": 59, "end_pos": 65, "type": "TASK", "confidence": 0.6797288060188293}]}, {"text": "Finally, we conclude the paper and suggest directions for future research in Section 7.", "labels": [], "entities": []}], "datasetContent": [{"text": "We demonstrate the effectiveness of the proposed model on large, real-world datasets in the question answering and website category domains on two tasks: the topic modeling of documents, and the use of the generated topics for document clustering.", "labels": [], "entities": [{"text": "question answering", "start_pos": 92, "end_pos": 110, "type": "TASK", "confidence": 0.7106001526117325}, {"text": "document clustering", "start_pos": 227, "end_pos": 246, "type": "TASK", "confidence": 0.7026155889034271}]}, {"text": "To construct comprehensive datasets for our experiments, we crawled data from two websites.", "labels": [], "entities": []}, {"text": "First, we crawled nearly all the questions and associated answer pairs (QA pairs) of two top cat- The statistics of all datasets are summarized in Table 2.", "labels": [], "entities": []}, {"text": "From this table, we can see that these datasets are very diverse: Y Ans has much fewer labels than O Hlth and O Home, but have much more documents for each label; meanwhile the depth of hierarchical tree for O Hlth and O Home can reach level 9 or above.", "labels": [], "entities": []}, {"text": "All experiments are based on the results of models with a burn-in of 10000 Gibbs sampling iterations, symmetric priors \u03b1 = 0.1 and free parameter \u03b7 = 1.0; and for \u00b5, we can obtain the estimation of \u00b5 c i by fixed-point iteration.", "labels": [], "entities": [{"text": "symmetric priors \u03b1", "start_pos": 102, "end_pos": 120, "type": "METRIC", "confidence": 0.7912706732749939}]}, {"text": "For each dataset we obtain corresponding clusters using the various models described in previous sections.", "labels": [], "entities": []}, {"text": "Thus we can use clustering metrics to measure the quality of various algorithms by using a measure that takes into account the overall set of clusters that are represented in the new generated part of a hierarchical tree.", "labels": [], "entities": []}, {"text": "One such measure is the FScore measure, intro-duced by.", "labels": [], "entities": [{"text": "FScore measure", "start_pos": 24, "end_pos": 38, "type": "METRIC", "confidence": 0.9532754123210907}, {"text": "intro-duced", "start_pos": 40, "end_pos": 51, "type": "METRIC", "confidence": 0.936582088470459}]}, {"text": "Given a particular class Cr of size n rand a particular cluster Si of size n i , suppose n ri documents in the cluster Si belong to Cr , then the FScore of this class and cluster is defined to be where R(C r , Si ) is the recall value defined as n ri /n r , and P (C r , Si ) is the precision value defined as n ri /n i for the class Cr and the cluster Si . The FScore of the class Cr , is the maximum FScore value attained at any node in the hierarchical clustering tree T . That is, The FScore of the entire clustering solution is then defined to be the sum of the individual class FScore weighted according to the class size.", "labels": [], "entities": [{"text": "FScore", "start_pos": 146, "end_pos": 152, "type": "METRIC", "confidence": 0.9939577579498291}, {"text": "recall", "start_pos": 222, "end_pos": 228, "type": "METRIC", "confidence": 0.996969997882843}, {"text": "precision", "start_pos": 283, "end_pos": 292, "type": "METRIC", "confidence": 0.99669349193573}, {"text": "FScore", "start_pos": 362, "end_pos": 368, "type": "METRIC", "confidence": 0.9967092275619507}, {"text": "FScore", "start_pos": 402, "end_pos": 408, "type": "METRIC", "confidence": 0.9633179306983948}, {"text": "FScore", "start_pos": 489, "end_pos": 495, "type": "METRIC", "confidence": 0.9960052371025085}]}, {"text": "where c is the total number of classes.", "labels": [], "entities": []}, {"text": "In general, the higher the FScore values, the better the clustering solution is.", "labels": [], "entities": [{"text": "FScore", "start_pos": 27, "end_pos": 33, "type": "METRIC", "confidence": 0.6793757081031799}, {"text": "clustering", "start_pos": 57, "end_pos": 67, "type": "TASK", "confidence": 0.9551745057106018}]}, {"text": "Each of hLDA, Simp-hLDA and SSHLDA needs a parameter-the height of the topical tree, i.e. L; and for Simp-hLDA and SSHLDA, they need another parameter-the height of the hierarchical observed labels, i.e l.", "labels": [], "entities": []}, {"text": "The h-clustering does not have any height parameters, thus its FScore will keep the same values at different height of the topical tree.", "labels": [], "entities": [{"text": "FScore", "start_pos": 63, "end_pos": 69, "type": "METRIC", "confidence": 0.9720308780670166}]}, {"text": "With choosing the height of hierarchical labels for O Home as 4, i.e. l = 4, the results of our model and baselines with respect to the height of a hierarchy are shown in.", "labels": [], "entities": []}, {"text": "From the figure, we can see that our proposed model can achieve consistent improvement over the baseline models at different height, i.e. L \u2208 {5, 6, 7, 8}.", "labels": [], "entities": []}, {"text": "For example, the performance of SSHLDA can reach 0.396 at height 5 while the hclustering, hLDA and hLLDA only achieve 0.295, 0.328 and 0.349 at the same height.", "labels": [], "entities": []}, {"text": "The result shows that our model can achieve about 34.2%, 20.7% and 13.5% improvements over h-clustering, hLDA and hLLDA at height 5.", "labels": [], "entities": []}, {"text": "The improvements are significant by t-test at the 95% significance level.", "labels": [], "entities": [{"text": "t-test", "start_pos": 36, "end_pos": 42, "type": "METRIC", "confidence": 0.9443578720092773}, {"text": "significance", "start_pos": 54, "end_pos": 66, "type": "METRIC", "confidence": 0.9662363529205322}]}, {"text": "We can also obtain similar experimental results over Y Ans and O Hlth.", "labels": [], "entities": [{"text": "O Hlth", "start_pos": 63, "end_pos": 69, "type": "METRIC", "confidence": 0.908469945192337}]}, {"text": "However, for the same reason of limitation of space, their detailed descriptions are skipped in this paper.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: The statistics of the datasets.  Datasets #labels #paths Max level  #docs  Y Ans  46  35  4 6,345,786  O Hlth  6695  6505  10  54939  O Home  2432  2364  9  24254", "labels": [], "entities": [{"text": "Hlth", "start_pos": 115, "end_pos": 119, "type": "METRIC", "confidence": 0.8254778981208801}, {"text": "Home  2432  2364  9  24254", "start_pos": 146, "end_pos": 172, "type": "DATASET", "confidence": 0.8861726880073547}]}]}