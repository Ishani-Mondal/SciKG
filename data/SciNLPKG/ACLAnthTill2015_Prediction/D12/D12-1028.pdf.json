{"title": [{"text": "Exploiting Reducibility in Unsupervised Dependency Parsing", "labels": [], "entities": [{"text": "Parsing", "start_pos": 51, "end_pos": 58, "type": "TASK", "confidence": 0.6505134105682373}]}], "abstractContent": [{"text": "The possibility of deleting a word from a sentence without violating its syntactic correct-ness belongs to traditionally known manifestations of syntactic dependency.", "labels": [], "entities": []}, {"text": "We introduce a novel unsupervised parsing approach that is based on anew n-gram reducibility measure.", "labels": [], "entities": []}, {"text": "We perform experiments across 18 languages available in CoNLL data and we show that our approach achieves better accuracy for the majority of the languages then previously reported results.", "labels": [], "entities": [{"text": "CoNLL data", "start_pos": 56, "end_pos": 66, "type": "DATASET", "confidence": 0.9561406970024109}, {"text": "accuracy", "start_pos": 113, "end_pos": 121, "type": "METRIC", "confidence": 0.9988129138946533}]}], "introductionContent": [{"text": "The true nature of the notion of dependency (after removing sedimentary deposits of rules imposed only by more or less arbitrary conventions) remains still somewhat vague and elusive.", "labels": [], "entities": []}, {"text": "This holds in spite of a seemingly strong background intuition and even after a decade of formalized large-scale dependencybased resources being available to the research community.", "labels": [], "entities": []}, {"text": "It is undeniable that a huge progress has been reached in the field of supervised dependency parsing, especially due to the CoNLL shared task series.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 82, "end_pos": 100, "type": "TASK", "confidence": 0.7151925563812256}, {"text": "CoNLL shared task series", "start_pos": 124, "end_pos": 148, "type": "DATASET", "confidence": 0.7344149947166443}]}, {"text": "However, when it comes to unsupervised parsing, there are surprisingly few clues we could rely on.", "labels": [], "entities": []}, {"text": "As mentioned e.g. by, one of the traditional linguistic criteria for recognizing dependency relations (including their head-dependent orientation) is that ahead H of a construction C determines the syntactic category of C and can often replace C.", "labels": [], "entities": []}, {"text": "Or, in words of Dependency Analysis by), stepwise deletion of dependent elements within a sentence preserves its syntactic correctness.", "labels": [], "entities": [{"text": "Dependency Analysis", "start_pos": 16, "end_pos": 35, "type": "TASK", "confidence": 0.8220502138137817}]}, {"text": "A similar idea of dependency analysis by splitting a sentence into all possible acceptable fragments is used by.", "labels": [], "entities": [{"text": "dependency analysis", "start_pos": 18, "end_pos": 37, "type": "TASK", "confidence": 0.8889225721359253}]}, {"text": "Of course, all the above works had to respond to the notorious fact that there are many language phenomena precluding the ideal (word by word) sentence reducibility (e.g. in the case of prepositional groups, or in the case of subjects in English finite clauses).", "labels": [], "entities": []}, {"text": "However, we disregard their solutions tentatively and borrow only the very core of the reducibility idea: if a word can be removed from a sentence without damaging it, then it is likely to be dependent on some other (still present) word.", "labels": [], "entities": []}, {"text": "As it is usual with dichotomies in natural languages, it seems more adequate to use a continuous scale instead of the reducible-irreducible opposition.", "labels": [], "entities": []}, {"text": "That is why we introduce a simple reducibility measure based on n-gram corpus statistics.", "labels": [], "entities": []}, {"text": "We employ this reducibility measure as the main feature in our unsupervised parsing procedure.", "labels": [], "entities": []}, {"text": "The procedure is based on a commonly used Bayesian inference technique called Gibbs sampling ().", "labels": [], "entities": []}, {"text": "In our sampler, the more reducible a given token is, the more likely it is to be sampled as a dependant and not as ahead.", "labels": [], "entities": []}, {"text": "After certain number of sampling iterations, for each sentence a final dependency tree is created (one token per node, including punctuation) that maximizes the product of edge probabilities gathered along the sampling history.", "labels": [], "entities": []}, {"text": "Our approach allows to utilize information from very large corpora.", "labels": [], "entities": []}, {"text": "While the computationally demanding sampling procedure can be applied only on limited data, the unrepeated precomputation of statistics for reducibility estimates can easily exploit much larger data.", "labels": [], "entities": []}, {"text": "We are not aware of any other published work on unsupervised parsing employing reducibility or a similar idea.", "labels": [], "entities": []}, {"text": "Dominating approaches in unsupervised parsing are typically based on repeated patterns, and not on the possibility of a deletion inside a pattern.", "labels": [], "entities": []}, {"text": "It seems that the two views of dependency (frequent co-occurrence of head-dependant pair, versus reducibility of the dependant) are rather complementary, so fruitful combinations can be hopefully expected in future.", "labels": [], "entities": []}, {"text": "The remainder of this paper is structured as follows.", "labels": [], "entities": []}, {"text": "Section 2 briefly outlines the state of the art in unsupervised dependency parsing.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 64, "end_pos": 82, "type": "TASK", "confidence": 0.7558054029941559}]}, {"text": "Our measure of reducibility based on a large monolingual corpus is presented in Section 3.", "labels": [], "entities": []}, {"text": "Section 4 shows our models which serve for generating probability estimates for edge sampling described in Section 5.", "labels": [], "entities": [{"text": "edge sampling", "start_pos": 80, "end_pos": 93, "type": "TASK", "confidence": 0.7630127370357513}]}, {"text": "Experimental parsing results for languages included in CoNLL shared task treebanks are summarized in Section 6.", "labels": [], "entities": [{"text": "CoNLL shared task treebanks", "start_pos": 55, "end_pos": 82, "type": "DATASET", "confidence": 0.7942807376384735}]}, {"text": "Section 7 concludes this article.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluate our parser on 20 treebanks (18 languages) included in CoNLL shared tasks 2006 () and.", "labels": [], "entities": [{"text": "CoNLL shared tasks 2006", "start_pos": 66, "end_pos": 89, "type": "DATASET", "confidence": 0.79649718105793}]}, {"text": "Similarly to some previous papers on unsupervised parsing), the tuning experiments were performed on English only.", "labels": [], "entities": []}, {"text": "We used English for checking functionality of the individual models and for optimizing hyperparameter values.", "labels": [], "entities": []}, {"text": "The best configuration of the parser achieved on English development data was then used for parsing all other languages.", "labels": [], "entities": [{"text": "parsing", "start_pos": 92, "end_pos": 99, "type": "TASK", "confidence": 0.9710657596588135}]}, {"text": "This simulates the situation in which we have only one treebank (English) on which we can tune our parser and we want to parse other languages for which we have no manually annotated treebanks.: Wikipedia texts statistics  The best setting from the experiments on English is now used for evaluating our parser on all CoNLL languages.", "labels": [], "entities": [{"text": "CoNLL languages", "start_pos": 317, "end_pos": 332, "type": "DATASET", "confidence": 0.8992968797683716}]}, {"text": "To be able to compare our parser attachment score to previously published results, the following steps must be done: \u2022 We take the testing part of each treebank (the file test.conll) and remove all the punctuation marks.", "labels": [], "entities": []}, {"text": "If the punctuation node is not a leaf, its children are attached to the parent of the removed node.", "labels": [], "entities": []}, {"text": "\u2022 Some previous papers report results on up-to-10-words sentences only.", "labels": [], "entities": []}, {"text": "Therefore we extract such sentences from the test data and evaluate on this subsets as well.", "labels": [], "entities": []}, {"text": "Here we make use of manually annotated trees.", "labels": [], "entities": []}, {"text": "However, we use only English treebank an we are setting only four numbers out of several previously given values (e.g \u03b1e out of 0.01, 0.1, 1, 10).", "labels": [], "entities": [{"text": "English treebank", "start_pos": 21, "end_pos": 37, "type": "DATASET", "confidence": 0.8877407312393188}]}, {"text": "These numbers could be tuned also by inspecting the outputs.", "labels": [], "entities": []}, {"text": "So we believe this method can be treated as unsupervised.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: Reducibility scores of the most frequent  Czech n-grams. (V* are verbs, N* are nouns, P* are  pronouns, R* are prepositions, A* are adjectives, D*  are adverbs, C* are numerals, J* are conjunctions,  and Z* is punctuation)", "labels": [], "entities": []}, {"text": " Table 5: Comparison of directed attachment scores  with previously reported results on CoNLL tree- banks. The column \"gil11\" contains results reported  by", "labels": [], "entities": [{"text": "CoNLL tree- banks", "start_pos": 88, "end_pos": 105, "type": "DATASET", "confidence": 0.9457994103431702}]}, {"text": " Table 6: Ablation analysis. Unlabeled attachment  scores for different combinations of model compo- nents (fertility model, edge model, distance model  and subtree model). The scores are computed on all  sentences of the development data. Punctuation is  included into the evaluation.", "labels": [], "entities": [{"text": "Ablation analysis", "start_pos": 10, "end_pos": 27, "type": "TASK", "confidence": 0.8244099020957947}]}]}