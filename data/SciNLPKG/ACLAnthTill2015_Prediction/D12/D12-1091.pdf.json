{"title": [{"text": "An Empirical Investigation of Statistical Significance in NLP", "labels": [], "entities": [{"text": "Statistical Significance", "start_pos": 30, "end_pos": 54, "type": "TASK", "confidence": 0.6239703595638275}, {"text": "NLP", "start_pos": 58, "end_pos": 61, "type": "TASK", "confidence": 0.6119730472564697}]}], "abstractContent": [{"text": "We investigate two aspects of the empirical behavior of paired significance tests for NLP systems.", "labels": [], "entities": []}, {"text": "First, when one system appears to outperform another, how does significance level relate in practice to the magnitude of the gain, to the size of the test set, to the similarity of the systems, and so on?", "labels": [], "entities": [{"text": "significance level", "start_pos": 63, "end_pos": 81, "type": "METRIC", "confidence": 0.9210174381732941}]}, {"text": "Is it true that for each task there is again which roughly implies significance?", "labels": [], "entities": []}, {"text": "We explore these issues across a range of NLP tasks using both large collections of past systems' outputs and variants of single systems.", "labels": [], "entities": []}, {"text": "Next, once significance levels are computed, how well does the standard i.i.d. notion of significance holdup in practical settings where future distributions are neither independent nor identically distributed, such as across domains?", "labels": [], "entities": []}, {"text": "We explore this question using a range of test set variations for constituency parsing.", "labels": [], "entities": [{"text": "constituency parsing", "start_pos": 66, "end_pos": 86, "type": "TASK", "confidence": 0.900998443365097}]}], "introductionContent": [{"text": "It is, or at least should be, nearly universal that NLP evaluations include statistical significance tests to validate metric gains.", "labels": [], "entities": []}, {"text": "As important as significance testing is, relatively few papers have empirically investigated its practical properties.", "labels": [], "entities": [{"text": "significance testing", "start_pos": 16, "end_pos": 36, "type": "TASK", "confidence": 0.9748524725437164}]}, {"text": "Those that do focus on single tasks) or on the comparison of alternative hypothesis tests.", "labels": [], "entities": []}, {"text": "In this paper, we investigate two aspects of the empirical behavior of paired significance tests for NLP systems.", "labels": [], "entities": []}, {"text": "For example, all else equal, larger metric gains will tend to be more significant.", "labels": [], "entities": []}, {"text": "However, what does this relationship look like and how reliable is it?", "labels": [], "entities": []}, {"text": "What should be made of the conventional wisdom that often springs up that a certain metric gain is roughly the point of significance fora given task (e.g. 0.4 F1 in parsing or 0.5 BLEU in machine translation)?", "labels": [], "entities": [{"text": "F1", "start_pos": 159, "end_pos": 161, "type": "METRIC", "confidence": 0.9946809411048889}, {"text": "BLEU", "start_pos": 180, "end_pos": 184, "type": "METRIC", "confidence": 0.99720698595047}, {"text": "machine translation", "start_pos": 188, "end_pos": 207, "type": "TASK", "confidence": 0.6588478088378906}]}, {"text": "We show that, with heavy caveats, there are such thresholds, though we also discuss the hazards in their use.", "labels": [], "entities": []}, {"text": "In particular, many other factors contribute to the significance level, and we investigate several of them.", "labels": [], "entities": [{"text": "significance level", "start_pos": 52, "end_pos": 70, "type": "METRIC", "confidence": 0.9507125020027161}]}, {"text": "For example, what is the effect of the similarity between the two systems?", "labels": [], "entities": []}, {"text": "Here, we show that more similar systems tend to achieve significance with smaller metric gains, reflecting the fact that their outputs are more correlated.", "labels": [], "entities": []}, {"text": "What about the size of the test set?", "labels": [], "entities": []}, {"text": "For example, in designing a shared task it is important to know how large the test set must be in order for significance tests to be sensitive to small gains in the performance metric.", "labels": [], "entities": []}, {"text": "Here, we show that test size plays the largest role in determining discrimination ability, but that we get diminishing returns.", "labels": [], "entities": []}, {"text": "For example, doubling the test size will not obviate the need for significance testing.", "labels": [], "entities": [{"text": "significance testing", "start_pos": 66, "end_pos": 86, "type": "TASK", "confidence": 0.86884805560112}]}, {"text": "In order for our results to be meaningful, we must have access to the outputs of many of NLP systems.", "labels": [], "entities": []}, {"text": "Public competitions, such as the well-known CoNLL shared tasks, provide one natural way to obtain a variety of system outputs on the same test set.", "labels": [], "entities": []}, {"text": "However, for most NLP tasks, obtaining outputs from a large variety of systems is not feasible.", "labels": [], "entities": []}, {"text": "Thus, in the course of our investigations, we propose a very simple method for automatically generating arbitrary numbers of comparable system outputs and we then validate the trends revealed by our synthetic method against data from public competitions.", "labels": [], "entities": []}, {"text": "This methodology itself could be of value in, for example, the design of new shared tasks.", "labels": [], "entities": []}, {"text": "Finally, we consider a related and perhaps even more important question that can only be answered empirically: to what extent is statistical significance on a test corpus predictive of performance on other test corpora, in-domain or otherwise?", "labels": [], "entities": []}, {"text": "Focusing on constituency parsing, we investigate the relationship between significance levels and actual performance on data from outside the test set.", "labels": [], "entities": [{"text": "constituency parsing", "start_pos": 12, "end_pos": 32, "type": "TASK", "confidence": 0.8565890789031982}]}, {"text": "We show that when the test set is (artificially) drawn i.i.d. from the same distribution that generates new data, then significance levels are remarkably well-calibrated.", "labels": [], "entities": []}, {"text": "However, as the domain of the new data diverges from that of the test set, the predictive ability of significance level drops off dramatically.", "labels": [], "entities": []}], "datasetContent": [{"text": "Our first goal is to explore the relationship between metric gain, \u03b4(x), and statistical significance, p-value(x), fora range of NLP tasks.", "labels": [], "entities": [{"text": "metric gain, \u03b4(x)", "start_pos": 54, "end_pos": 71, "type": "METRIC", "confidence": 0.8038980875696454}]}, {"text": "In order to say anything meaningful, we will need to see both \u03b4(x) and p-value(x) for many pairs of systems.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Empirical calibration: p-value on section 23 of the", "labels": [], "entities": [{"text": "Empirical", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9707214832305908}]}]}