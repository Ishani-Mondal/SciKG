{"title": [{"text": "Discovering Diverse and Salient Threads in Document Collections", "labels": [], "entities": [{"text": "Discovering Diverse and Salient Threads in Document Collections", "start_pos": 0, "end_pos": 63, "type": "TASK", "confidence": 0.8813458755612373}]}], "abstractContent": [{"text": "We propose a novel probabilistic technique for modeling and extracting salient structure from large document collections.", "labels": [], "entities": [{"text": "extracting salient structure from large document collections", "start_pos": 60, "end_pos": 120, "type": "TASK", "confidence": 0.7911470276968819}]}, {"text": "As in clustering and topic modeling, our goal is to provide an organizing perspective into otherwise overwhelming amounts of information.", "labels": [], "entities": [{"text": "topic modeling", "start_pos": 21, "end_pos": 35, "type": "TASK", "confidence": 0.6644565165042877}]}, {"text": "We are particularly interested in revealing and exploiting relationships between documents.", "labels": [], "entities": []}, {"text": "To this end, we focus on extracting diverse sets of threads-singly-linked, coherent chains of important documents.", "labels": [], "entities": []}, {"text": "To illustrate, we extract research threads from citation graphs and construct timelines from news articles.", "labels": [], "entities": []}, {"text": "Our method is highly scalable, running on a corpus of over 30 million words in about four minutes, more than 75 times faster than a dynamic topic model.", "labels": [], "entities": []}, {"text": "Finally, the results from our model more closely resemble human news summaries according to several metrics and are also preferred by human judges.", "labels": [], "entities": []}], "introductionContent": [{"text": "The increasing availability of large document collections has the potential to revolutionize our ability to understand the world.", "labels": [], "entities": []}, {"text": "However, the scale and complexity of such collections frequently make it difficult to quickly grasp the important details and the relationships between them.", "labels": [], "entities": []}, {"text": "As a result, automatic interfaces for data navigation, exploration, aggregation, and analysis are becoming increasingly valuable.", "labels": [], "entities": [{"text": "data navigation", "start_pos": 38, "end_pos": 53, "type": "TASK", "confidence": 0.7423721551895142}]}, {"text": "In this work we propose a novel approach: threading structured document collections.", "labels": [], "entities": []}, {"text": "Consider a large graph, with documents as nodes and edges indicating relationships, as in.", "labels": [], "entities": []}, {"text": "Our goal is to find a diverse set of paths (or threads) through the collection that are individually coherent and together cover the most salient parts.", "labels": [], "entities": []}, {"text": "For example, given a collection of academic papers, we might want to identify the most significant lines of research, threading the citation graph to produce chains of important papers.", "labels": [], "entities": []}, {"text": "Or, given news articles connected chronologically, we might want to extract threads of articles to form timelines describing the major events from the most significant news stories.", "labels": [], "entities": []}, {"text": "Top-tier news organizations like The New York Times and The Guardian regularly publish such timelines, but have so far been limited to creating them by hand.", "labels": [], "entities": []}, {"text": "Other possibile applications might include discovering trends on social media sites, or perhaps mining blog entries for important conversations through trackback links.", "labels": [], "entities": []}, {"text": "We show how these kinds of threading tasks can be done efficiently, providing a simple, practical tool for representing graph-based data that offers new possibilities compared with existing models.", "labels": [], "entities": []}, {"text": "The Topic Detection and Tracking (TDT) program ( has recently led to some research in this direction.", "labels": [], "entities": [{"text": "Topic Detection and Tracking (TDT)", "start_pos": 4, "end_pos": 38, "type": "TASK", "confidence": 0.9130523545401437}]}, {"text": "Several of TDT's core tasks, like link detection, topic detection, and topic tracking, can be seen as subroutines for the threading problem.", "labels": [], "entities": [{"text": "link detection", "start_pos": 34, "end_pos": 48, "type": "TASK", "confidence": 0.7773011922836304}, {"text": "topic detection", "start_pos": 50, "end_pos": 65, "type": "TASK", "confidence": 0.8722634613513947}, {"text": "topic tracking", "start_pos": 71, "end_pos": 85, "type": "TASK", "confidence": 0.8518866300582886}]}, {"text": "Our work, however, addresses these tasks jointly, using a global probabilistic model with a tractable inference algorithm.", "labels": [], "entities": []}, {"text": "To achieve this, we employ structured determinantal point processes (SDPPs) (Kulesza: An illustration of document collection threading.", "labels": [], "entities": [{"text": "document collection threading", "start_pos": 105, "end_pos": 134, "type": "TASK", "confidence": 0.7126169502735138}]}, {"text": "We first build a graph from the collection, using measures of importance and relatedness to weight nodes (documents) and build edges (relationships).", "labels": [], "entities": []}, {"text": "Then, from this graph, we extract a diverse, salient set of threads to represent the collection.", "labels": [], "entities": []}, {"text": "The supplement contains aversion of this figure for our real-world news dataset., which offer a natural probabilistic model over sets of structures (such as threads) where diversity is desired, and we incorporate k-DPP extensions to control the number of threads (.", "labels": [], "entities": []}, {"text": "We apply our model to two real-world datasets, extracting threads of research papers and timelines of news articles.", "labels": [], "entities": []}, {"text": "An example of news threads extracted using our model is shown in.", "labels": [], "entities": []}, {"text": "Quantitative evaluation shows that our model significantly outperforms multiple baselines, including dynamic topic models, in comparisons with human-produced news summaries.", "labels": [], "entities": []}, {"text": "It also outperforms baseline methods in a user evaluation of thread coherence, and runs 75 times faster than a dynamic topic model.", "labels": [], "entities": []}, {"text": "The primary contributions of this paper are: (1) proposing a novel framework for finding diverse and salient sets of document threads; (2) combining SDPPs and k-DPPs to implement the proposed model; (3) introducing random projections to improve efficiency with only bounded deviation; and (4) demonstrating the model on large-scale, real-world datasets.", "labels": [], "entities": []}], "datasetContent": [{"text": "We begin by showing the performance of random projections on a small, synthetic threading task where the exact model is tractable, with n = 600 and D = 150.", "labels": [], "entities": []}, {"text": "shows the L 1 variational distance (estimated by sampling) as well as the actual memory required fora variety of projection dimensions d.", "labels": [], "entities": [{"text": "L 1 variational distance", "start_pos": 10, "end_pos": 34, "type": "METRIC", "confidence": 0.6489006653428078}]}, {"text": "Note that, as predicted by Theorem 1, fidelity to the true model increases rapidly with d.", "labels": [], "entities": [{"text": "fidelity", "start_pos": 38, "end_pos": 46, "type": "METRIC", "confidence": 0.8915249705314636}]}, {"text": "An important distinction between the baselines and the k-SDPP is that the former are topic-oriented, choosing articles that relate to broad subject areas, while our approach is storyoriented, chaining together articles with direct  individual relationships.", "labels": [], "entities": []}, {"text": "An example of this distinction can be seen in.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Similarity of automatically generated timelines to human summaries. Bold entries are significantly  higher than others in the column at 99% confidence, computed using bootstrapping (Hesterberg et al., 2003).", "labels": [], "entities": []}, {"text": " Table 2: Rating: average coherence score from 1  (worst) to 5 (best). Interlopers: average number of  interloper articles identified (out of 2). Bold entries  are significantly higher with 95% confidence.", "labels": [], "entities": []}]}