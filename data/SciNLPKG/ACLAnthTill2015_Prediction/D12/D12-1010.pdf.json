{"title": [{"text": "An Entity-Topic Model for Entity Linking", "labels": [], "entities": [{"text": "Entity Linking", "start_pos": 26, "end_pos": 40, "type": "TASK", "confidence": 0.7648299336433411}]}], "abstractContent": [{"text": "Entity Linking (EL) has received considerable attention in recent years.", "labels": [], "entities": [{"text": "Entity Linking (EL)", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.8111179649829865}]}, {"text": "Given many name mentions in a document, the goal of EL is to predict their referent entities in a knowledge base.", "labels": [], "entities": []}, {"text": "Traditionally, there have been two distinct directions of EL research: one focusing on the effects of mention's context compatibility, assuming that \"the referent entity of a mention is reflected by its context\"; the other dealing with the effects of document's topic coherence, assuming that \"a mention's referent entity should be coherent with the document's main topics\".", "labels": [], "entities": []}, {"text": "In this paper, we propose a generative model-called entity-topic model, to effectively join the above two complementary directions together.", "labels": [], "entities": []}, {"text": "By jointly modeling and exploiting the context compatibility, the topic coherence and the correlation between them, our model can accurately link all mentions in a document using both the local information (including the words and the mentions in a document) and the global knowledge (including the topic knowledge, the entity context knowledge and the entity name knowledge).", "labels": [], "entities": []}, {"text": "Experimental results demonstrate the effectiveness of the proposed model.", "labels": [], "entities": []}], "introductionContent": [{"text": "Entity Linking (EL) has received considerable research attention in recent years.", "labels": [], "entities": [{"text": "Entity Linking (EL)", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.8161629855632782}]}, {"text": "Given many name mentions in a document, the goal of EL is to predict their referent entities in a given knowledge base (KB), such as the Wikipedia . For example, as 1 www.wikipedia.org shown in, an EL system should identify the referent entities of the three mentions WWDC, Apple and Lion correspondingly are the entities Apple Worldwide Developers Conference, Apple Inc. and Mac OS X Lion in KB.", "labels": [], "entities": [{"text": "WWDC", "start_pos": 268, "end_pos": 272, "type": "DATASET", "confidence": 0.9443163871765137}]}, {"text": "The EL problem appears in many different guises throughout the areas of natural language processing, information retrieval and text mining.", "labels": [], "entities": [{"text": "natural language processing", "start_pos": 72, "end_pos": 99, "type": "TASK", "confidence": 0.6493671834468842}, {"text": "information retrieval", "start_pos": 101, "end_pos": 122, "type": "TASK", "confidence": 0.8198089003562927}, {"text": "text mining", "start_pos": 127, "end_pos": 138, "type": "TASK", "confidence": 0.814555287361145}]}, {"text": "For instance, in many applications we need to collect all appearances of a specific entity in different documents, EL is an effective way to resolve such an information integration problem.", "labels": [], "entities": [{"text": "information integration", "start_pos": 157, "end_pos": 180, "type": "TASK", "confidence": 0.7377438545227051}]}, {"text": "Furthermore, EL can bridge the mentions in documents with the semantic information in knowledge bases (e.g., Wikipedia and Freebase 2 ), thus can provide a solid foundation for knowledge-rich methods.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we evaluate our method and compare it with the traditional EL methods.", "labels": [], "entities": []}, {"text": "We first explain the experimental settings in Section 4.1-4.4, then discuss the results in Section 4.5.", "labels": [], "entities": []}, {"text": "This paper adopted the same performance metrics used in the, which includes Recall, Precision and F1.", "labels": [], "entities": [{"text": "Recall", "start_pos": 76, "end_pos": 82, "type": "METRIC", "confidence": 0.8045263290405273}, {"text": "Precision", "start_pos": 84, "end_pos": 93, "type": "METRIC", "confidence": 0.9886680245399475}, {"text": "F1", "start_pos": 98, "end_pos": 100, "type": "METRIC", "confidence": 0.998282790184021}]}, {"text": "Let M * be the golden standard set of the EL results (each EL result is a pair (m, e), with m the mention and e its referent entity), M be the set of EL results outputted by an EL system, then these metrics are computed as: where two EL results are considered equal if and only if both their mentions and referent entities are equal.", "labels": [], "entities": []}, {"text": "As the same as, http://www.cse.iitb.ac.in/~soumen/doc/QCQ/ Precision and Recall are averaged across documents and overall F1 is used as the primary performance metric by computing from average Precision and Recall.", "labels": [], "entities": [{"text": "F1", "start_pos": 122, "end_pos": 124, "type": "METRIC", "confidence": 0.9983327984809875}]}, {"text": "We also compare our method with the top 5 EL systems in TAC 2009 and the two state-of-the-art systems (EM-Model and EL-Graph) on TAC 2009 data set in (For EL-Graph and our method, a NIL threshold is used to detect whether the referent entity is contained in the knowledge base, if the knowledge base not contains the referent entity, we assign the mention to a NIL entity).", "labels": [], "entities": [{"text": "TAC 2009", "start_pos": 56, "end_pos": 64, "type": "DATASET", "confidence": 0.9107934832572937}, {"text": "TAC 2009 data set", "start_pos": 129, "end_pos": 146, "type": "DATASET", "confidence": 0.9719211906194687}]}, {"text": "From, we can see that our method is competitive: 1) Our method can achieve a 3.4% accuracy improvement over the best system in TAC 2009; 2) Our method, EM-Model and ELGraph get very close accuracies (0.854, 0.86 and 0.838 correspondingly), we believe this is because: \u25cb 1 The mentions to be linked in TAC data set are mostly salient mentions; \u25cb 2 The influence of the NIL referent entity problem, i.e., the referent entity is not contained in the given knowledge base: Most referent entities (67.5%) on TAC 2009 are NIL entity and our method has no special handling on this problem, rather than other methods such as the EM-Model, which affects the overall performance of our method.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 82, "end_pos": 90, "type": "METRIC", "confidence": 0.9986773133277893}, {"text": "TAC data set", "start_pos": 301, "end_pos": 313, "type": "DATASET", "confidence": 0.863284170627594}, {"text": "TAC 2009", "start_pos": 503, "end_pos": 511, "type": "DATASET", "confidence": 0.9225525856018066}]}], "tableCaptions": [{"text": " Table 1. The overall results on IITB data set", "labels": [], "entities": [{"text": "IITB data set", "start_pos": 33, "end_pos": 46, "type": "DATASET", "confidence": 0.8882702589035034}]}, {"text": " Table 4. The F1 using different context models", "labels": [], "entities": []}]}