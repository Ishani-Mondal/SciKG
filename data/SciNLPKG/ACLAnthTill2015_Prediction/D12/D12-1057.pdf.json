{"title": [{"text": "Excitatory or Inhibitory: A New Semantic Orientation Extracts Contradiction and Causality from the Web", "labels": [], "entities": [{"text": "Semantic Orientation Extracts Contradiction", "start_pos": 32, "end_pos": 75, "type": "TASK", "confidence": 0.7221484929323196}]}], "abstractContent": [{"text": "We propose anew semantic orientation, Ex-citation, and its automatic acquisition method.", "labels": [], "entities": []}, {"text": "Excitation is a semantic property of predicates that classifies them into excitatory, inhibitory and neutral.", "labels": [], "entities": []}, {"text": "We show that Excitation is useful for extracting contradiction pairs (e.g., destroy cancer \u22a5 develop cancer) and causality pairs (e.g., increase in crime \u21d2 heighten anxiety).", "labels": [], "entities": [{"text": "Excitation", "start_pos": 13, "end_pos": 23, "type": "TASK", "confidence": 0.9354627728462219}]}, {"text": "Our experiments show that with automatically acquired Excitation knowledge we can extract one million contradiction pairs and 500,000 causality pairs with about 70% precision from a 600 million page Web corpus.", "labels": [], "entities": [{"text": "precision", "start_pos": 165, "end_pos": 174, "type": "METRIC", "confidence": 0.9977571368217468}]}, {"text": "Furthermore, by combining these extracted causality and contradiction pairs, we can generate one million plausible causality hypotheses that are not written in any single sentence in our corpus with reasonable precision.", "labels": [], "entities": [{"text": "precision", "start_pos": 210, "end_pos": 219, "type": "METRIC", "confidence": 0.9665298461914062}]}], "introductionContent": [{"text": "Recognizing semantic relations between events in texts is crucial for such NLP tasks as question answering (QA).", "labels": [], "entities": [{"text": "Recognizing semantic relations between events in texts", "start_pos": 0, "end_pos": 54, "type": "TASK", "confidence": 0.8089532000677926}, {"text": "question answering (QA)", "start_pos": 88, "end_pos": 111, "type": "TASK", "confidence": 0.8730307459831238}]}, {"text": "For example, to answer the question \"What ruined the crops in Japan?\" a QA system must recognize that the sentence \"the Fukushima nuclear power plant caused radioactive pollution and contaminated the crops in Japan\" contains a causal relation and that contaminate crops entails ruin crops but contradicts preserve crops.", "labels": [], "entities": []}, {"text": "To facilitate the acquisition of causality, contradiction, paraphrase and entailment relations between events we propose anew semantic orientation, Excitation, that classifies unary predicates (templates, hereafter) into excitatory, inhibitory and neutral.", "labels": [], "entities": []}, {"text": "An excitatory template entails that the main function or effect of the referent of its argument is activated or enhanced (e.g., cause X, preserve X), while an inhibitory template entails that it is deactivated or suppressed (e.g., ruin X, contaminate X, prevent X).", "labels": [], "entities": []}, {"text": "Excitation is useful for extracting contradiction; if two templates with similar distributional profiles have opposite Excitation polarities, they tend to be contradictions (e.g., contaminate crops and preserve crops).", "labels": [], "entities": [{"text": "extracting contradiction", "start_pos": 25, "end_pos": 49, "type": "TASK", "confidence": 0.8952450156211853}]}, {"text": "With extracted contradictions we can distinguish paraphrases from contradictions among distributionally similar phrases.", "labels": [], "entities": []}, {"text": "Furthermore, contradiction in itself is important knowledge for Recognizing Textual Entailment (RTE).", "labels": [], "entities": [{"text": "Recognizing Textual Entailment (RTE)", "start_pos": 64, "end_pos": 100, "type": "TASK", "confidence": 0.8411731024583181}]}, {"text": "Excitation is also a powerful indicator of causality.", "labels": [], "entities": []}, {"text": "In the physical world, the activation or deactivation of one thing often causes the activation or deactivation of another.", "labels": [], "entities": []}, {"text": "Two excitatory or inhibitory templates that co-occur in some temporal or logical order in the same narrative often describe a causal chain of events, like \"the Fukushima nuclear power plant caused radioactive pollution and contaminated crops in Japan\".", "labels": [], "entities": []}, {"text": "In this paper we propose both the concept of Excitation and an automatic method for its acquisition.", "labels": [], "entities": [{"text": "Excitation", "start_pos": 45, "end_pos": 55, "type": "TASK", "confidence": 0.921796977519989}]}, {"text": "Our method acquires Excitation templates based on certain natural, language independent constraints on narrative structures found in text.", "labels": [], "entities": []}, {"text": "We also propose acquisition methods for contradiction and causality relations based on Excitation.", "labels": [], "entities": []}, {"text": "Our methods extract one million contradiction pairs with over 70% precision, and 500,000 causality pairs with about 70% precision from a 600 million page Web corpus.", "labels": [], "entities": [{"text": "precision", "start_pos": 66, "end_pos": 75, "type": "METRIC", "confidence": 0.998367965221405}, {"text": "precision", "start_pos": 120, "end_pos": 129, "type": "METRIC", "confidence": 0.9958618879318237}]}, {"text": "Moreover, by combining these extracted causality pairs and contradiction pairs, we generated one million plausible causality hypotheses that were not written in any single sentence in our corpus with reasonable precision.", "labels": [], "entities": [{"text": "precision", "start_pos": 211, "end_pos": 220, "type": "METRIC", "confidence": 0.956736147403717}]}, {"text": "For example, a causality hypothesis prevent radioactive pollution \u21d2 preserve crops can be generated from an extracted causality cause radioactive pollution \u21d2 contaminate crops.", "labels": [], "entities": []}, {"text": "We target the Japanese language in this paper.", "labels": [], "entities": []}], "datasetContent": [{"text": "This section shows that our template acquisition method acquired many Excitation templates.", "labels": [], "entities": [{"text": "template acquisition", "start_pos": 28, "end_pos": 48, "type": "TASK", "confidence": 0.8363417387008667}]}, {"text": "Moreover, using only the acquired templates we extracted one million contradiction pairs with more than 70% precision, and 500,000 causality pairs with about 70% precision.", "labels": [], "entities": [{"text": "precision", "start_pos": 108, "end_pos": 117, "type": "METRIC", "confidence": 0.99726402759552}, {"text": "precision", "start_pos": 162, "end_pos": 171, "type": "METRIC", "confidence": 0.9935206770896912}]}, {"text": "Further, using only these extracted contradiction and causality pairs we generated one million causality hypotheses with 57% precision.", "labels": [], "entities": [{"text": "precision", "start_pos": 125, "end_pos": 134, "type": "METRIC", "confidence": 0.9981036186218262}]}, {"text": "In our experiments we removed evaluation samples containing the initial seed templates and examples used for annotation instruction from the evaluation data.", "labels": [], "entities": []}, {"text": "Three annotators (not the authors) marked all evaluation samples, which were randomly shuffled so that they could not identify which sample was produced by which method.", "labels": [], "entities": []}, {"text": "Information about the predicted labels or ranks was also removed from the evaluation data.", "labels": [], "entities": []}, {"text": "Final judgments were made by majority vote between the annotators.", "labels": [], "entities": []}, {"text": "They were nonexperts without formal training in linguistics or semantics.", "labels": [], "entities": []}, {"text": "See supplementary materials for our annotation manuals (translated into English).", "labels": [], "entities": []}, {"text": "We used 600 million Japanese Web pages (Akamine et al., 2010) parsed by KNP () as a corpus.", "labels": [], "entities": []}, {"text": "We restricted the argument positions of templates to ha (topic), ga (nominative), wo (accusative), ni (dative), and de (instrumental).", "labels": [], "entities": []}, {"text": "We discarded templates appearing fewer than 20 times in compound sentences (regardless of connectives) in our corpus.", "labels": [], "entities": []}], "tableCaptions": []}