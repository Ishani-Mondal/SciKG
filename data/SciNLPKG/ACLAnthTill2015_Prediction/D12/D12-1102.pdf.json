{"title": [{"text": "On Amortizing Inference Cost for Structured Prediction", "labels": [], "entities": [{"text": "Structured Prediction", "start_pos": 33, "end_pos": 54, "type": "TASK", "confidence": 0.6957663148641586}]}], "abstractContent": [{"text": "This paper deals with the problem of predicting structures in the context of NLP.", "labels": [], "entities": [{"text": "predicting structures", "start_pos": 37, "end_pos": 58, "type": "TASK", "confidence": 0.888567179441452}]}, {"text": "Typically, in structured prediction, an inference procedure is applied to each example independently of the others.", "labels": [], "entities": [{"text": "structured prediction", "start_pos": 14, "end_pos": 35, "type": "TASK", "confidence": 0.7024949193000793}]}, {"text": "In this paper, we seek to optimize the time complexity of inference over entire datasets, rather than individual examples.", "labels": [], "entities": []}, {"text": "By considering the general inference representation provided by integer linear programs , we propose three exact inference theorems which allow us to re-use earlier solutions for certain instances, thereby completely avoiding possibly expensive calls to the inference procedure.", "labels": [], "entities": []}, {"text": "We also identify several approximation schemes which can provide further speedup.", "labels": [], "entities": []}, {"text": "We instantiate these ideas to the structured prediction task of semantic role labeling and show that we can achieve a speedup of over 2.5 using our approach while retaining the guarantees of exactness and a further speedup of over 3 using approximations that do not degrade performance.", "labels": [], "entities": [{"text": "semantic role labeling", "start_pos": 64, "end_pos": 86, "type": "TASK", "confidence": 0.6867435375849406}]}], "introductionContent": [{"text": "Typically, in structured prediction applications, every example is treated independently and an inference algorithm is applied to each one of them.", "labels": [], "entities": []}, {"text": "For example, consider a dependency parser that uses the maximum spanning tree algorithm) or its integer linear program variants () to make predictions.", "labels": [], "entities": [{"text": "dependency parser", "start_pos": 24, "end_pos": 41, "type": "TASK", "confidence": 0.7098966687917709}]}, {"text": "Given a trained model, the parser addresses * These authors contributed equally to this each sentence separately and runs the inference algorithm to predict the parse tree.", "labels": [], "entities": []}, {"text": "Thus, the time complexity of inference over the test set is linear in the size of the corpus.", "labels": [], "entities": []}, {"text": "In this paper, we ask the following question: For a given task, since the inference procedure predicts structures from the same family of structures (dependency trees, semantic role structures, etc.), can the fact that we are running inference fora large number of examples help us improve the time complexity of inference?", "labels": [], "entities": []}, {"text": "In the dependency parsing example, this question translates to asking whether, having parsed many sentences, we can decrease the parsing time for the next sentence.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 7, "end_pos": 25, "type": "TASK", "confidence": 0.8323312103748322}]}, {"text": "Since any combinatorial optimization problem can be phrased as an integer linear program (ILP), we frame inference problems as ILPs for the purpose of analysis.", "labels": [], "entities": []}, {"text": "By analyzing the objective functions of integer linear programs, we identify conditions when two ILPs have the same solution.", "labels": [], "entities": []}, {"text": "This allows us to reuse solutions of previously solved problems and theoretically guarantee the optimality of the solution.", "labels": [], "entities": []}, {"text": "Furthermore, in some cases, even when the conditions are not satisfied, we can reuse previous solutions with high probability of being correct.", "labels": [], "entities": []}, {"text": "Given the extensive use of integer linear programs for structured prediction in Natural Language Processing over the last few years, these ideas can be applied broadly to NLP problems.", "labels": [], "entities": [{"text": "structured prediction", "start_pos": 55, "end_pos": 76, "type": "TASK", "confidence": 0.7273114323616028}]}, {"text": "We instantiate our improved inference approaches in the structured prediction task of semantic role labeling, where we use an existing implementation and a previous trained model that is based on the approach of.", "labels": [], "entities": [{"text": "semantic role labeling", "start_pos": 86, "end_pos": 108, "type": "TASK", "confidence": 0.6999558607737223}]}, {"text": "We merely modify the inference pro-cess to show that we can realize the theoretical gains by making fewer calls to the underlying ILP solver.", "labels": [], "entities": [{"text": "ILP solver", "start_pos": 130, "end_pos": 140, "type": "TASK", "confidence": 0.6267470270395279}]}], "datasetContent": [{"text": "In this section, we apply the theory from Section 3 to the structure prediction problem of semantic role labeling.", "labels": [], "entities": [{"text": "structure prediction", "start_pos": 59, "end_pos": 79, "type": "TASK", "confidence": 0.7776741981506348}, {"text": "semantic role labeling", "start_pos": 91, "end_pos": 113, "type": "TASK", "confidence": 0.7523713111877441}]}, {"text": "Since the inference schemes presented above are independent of the learning aspects, we use an off-the-shelf implementation and merely modify the inference as discussed in Section 3.5.", "labels": [], "entities": []}, {"text": "The goal of the experiments is to show that using an amortized inference algorithm, we can make fewer calls to the underlying inference procedure.", "labels": [], "entities": []}, {"text": "For the exact inference algorithms, doing so will not change the performance as compared to the underlying system.", "labels": [], "entities": []}, {"text": "For the approximations, we can make a trade-off between the inference time and performance.", "labels": [], "entities": []}, {"text": "Our goal is to simulate a long-running NLP process that can use a cache of already solved problems to improve inference time.", "labels": [], "entities": []}, {"text": "Given anew input problem, our theorems require us to find all elements in the equivalence class of that problem along with their solutions.", "labels": [], "entities": []}, {"text": "Intuitively, we expect a higher probability of finding members of an arbitrary equivalence class if the size of the cache is large.", "labels": [], "entities": []}, {"text": "Hence, we processed sentences from the Gigaword corpus and cached the inference problems for our task.", "labels": [], "entities": [{"text": "Gigaword corpus", "start_pos": 39, "end_pos": 54, "type": "DATASET", "confidence": 0.9388993084430695}]}, {"text": "The wall-clock time is strongly dependent on such specific implementation of the components, which are independent of the main contributions of this work.", "labels": [], "entities": []}, {"text": "Also, inmost interesting applications, the computation time for each step will be typically dominated by the number of inference steps, especially with efficient implementations of caching and retrieval.", "labels": [], "entities": []}, {"text": "Hence, the number of calls to the underlying procedure is the appropriate complexity param-eter.", "labels": [], "entities": []}, {"text": "Let N Base be the number of times we would need to call the underlying inference procedure had we not used an amortized algorithm.", "labels": [], "entities": [{"text": "N Base", "start_pos": 4, "end_pos": 10, "type": "METRIC", "confidence": 0.48166273534297943}]}, {"text": "(This is the same as the number of inference problems.)", "labels": [], "entities": []}, {"text": "Let N Abe the number of times the underlying inference procedure is actually called using an amortized algorithm A.", "labels": [], "entities": []}, {"text": "We define the speedup of A as We also report the clock speedup of our implementation for all algorithms, which is the ratio of the wall-clock time taken by the baseline algorithm to that of the amortized algorithm.", "labels": [], "entities": [{"text": "clock speedup", "start_pos": 49, "end_pos": 62, "type": "METRIC", "confidence": 0.9093487858772278}]}, {"text": "For measuring time, we only measure the time for inference as the other aspects (feature extraction, scoring, etc.) are not changed.", "labels": [], "entities": [{"text": "feature extraction", "start_pos": 81, "end_pos": 99, "type": "TASK", "confidence": 0.6682735532522202}]}], "tableCaptions": [{"text": " Table 3: Speedup and performance for various inference methods for the task of Semantic Role Labeling.", "labels": [], "entities": [{"text": "Speedup", "start_pos": 10, "end_pos": 17, "type": "METRIC", "confidence": 0.9669581055641174}, {"text": "Semantic Role Labeling", "start_pos": 80, "end_pos": 102, "type": "TASK", "confidence": 0.8364252249399821}]}]}