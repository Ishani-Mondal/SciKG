{"title": [{"text": "Improving NLP through Marginalization of Hidden Syntactic Structure", "labels": [], "entities": [{"text": "Improving NLP", "start_pos": 0, "end_pos": 13, "type": "TASK", "confidence": 0.9111559391021729}, {"text": "Marginalization of Hidden Syntactic Structure", "start_pos": 22, "end_pos": 67, "type": "TASK", "confidence": 0.8372165560722351}]}], "abstractContent": [{"text": "Many NLP tasks make predictions that are inherently coupled to syntactic relations, but for many languages the resources required to provide such syntactic annotations are unavailable.", "labels": [], "entities": []}, {"text": "For others it is unclear exactly how much of the syntactic annotations can be effectively leveraged with current models, and what structures in the syntactic trees are most relevant to the current task.", "labels": [], "entities": []}, {"text": "We propose a novel method which avoids the need for any syntactically annotated data when predicting a related NLP task.", "labels": [], "entities": []}, {"text": "Our method couples latent syntactic representations , constrained to form valid dependency graphs or constituency parses, with the prediction task via specialized factors in a Markov random field.", "labels": [], "entities": []}, {"text": "At both training and test time we marginalize over this hidden structure, learning the optimal latent representations for the problem.", "labels": [], "entities": []}, {"text": "Results show that this approach provides significant gains over a syntactically un-informed baseline, outperforming models that observe syntax on an English relation extraction task, and performing comparably to them in semantic role labeling.", "labels": [], "entities": [{"text": "English relation extraction task", "start_pos": 149, "end_pos": 181, "type": "TASK", "confidence": 0.6797920688986778}, {"text": "semantic role labeling", "start_pos": 220, "end_pos": 242, "type": "TASK", "confidence": 0.6341072618961334}]}], "introductionContent": [{"text": "Many NLP tasks are inherently tied to syntax, and state-of-the-art solutions to these tasks often rely on syntactic annotations as either a source for useful features (, path features in relation extraction) or as a scaffolding upon which a more narrow, specialized classification can occur (as often done in semantic role labeling).", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 187, "end_pos": 206, "type": "TASK", "confidence": 0.7268119156360626}, {"text": "semantic role labeling)", "start_pos": 309, "end_pos": 332, "type": "TASK", "confidence": 0.7323670536279678}]}, {"text": "This decoupling of the end task from its intermediate representation is sometimes known as the two-stage approach ( and comes with several drawbacks.", "labels": [], "entities": []}, {"text": "Most notably this decomposition prohibits the learning method from utilizing the labels from the end task when predicting the intermediate representation, a structure which must have some correlation to the end task to provide any benefit.", "labels": [], "entities": []}, {"text": "Relying on intermediate representations that are specifically syntactic in nature introduces its own unique set of problems.", "labels": [], "entities": []}, {"text": "Large amounts of syntactically annotated data is difficult to obtain, costly to produce, and often tied to a particular domain that may vary greatly from that of the desired end task.", "labels": [], "entities": []}, {"text": "Additionally, current systems often utilize only a small amount of the annotation for any particular task.", "labels": [], "entities": []}, {"text": "For instance, performing named entity recognition (NER) jointly with constituent parsing has been shown to improve performance on both tasks, but the only aspect of the syntax which is leveraged by the NER component is the location of noun phrases.", "labels": [], "entities": [{"text": "named entity recognition (NER)", "start_pos": 25, "end_pos": 55, "type": "TASK", "confidence": 0.8052943249543508}, {"text": "constituent parsing", "start_pos": 69, "end_pos": 88, "type": "TASK", "confidence": 0.7428872883319855}]}, {"text": "By instead discovering a latent representation jointly with the end task we address all of these concerns, alleviating the need for any syntactic annotations, while simultaneously attempting to learn a latent syntax relevant to both the particular domain and structure of the end task.", "labels": [], "entities": []}, {"text": "We phrase the joint model as factor graph and marginalize over the hidden structure of the intermediate representation at both training and test time, to optimize performance on the end task.", "labels": [], "entities": []}, {"text": "Inference is done via loopy belief propagation, making this framework trivially extensible to most graph structures.", "labels": [], "entities": [{"text": "loopy belief propagation", "start_pos": 22, "end_pos": 46, "type": "TASK", "confidence": 0.7860562801361084}]}, {"text": "Computation over latent syntactic rep-resentations is made tractable with the use of special combinatorial factors which implement unlabeled variants of common dynamic-programming parsing algorithms, constraining the hidden representation to realize valid dependency graphs or constituency trees.", "labels": [], "entities": []}, {"text": "We apply this strategy to two common NLP tasks, coupling a model for the end task prediction with latent and general syntactic representations via specialized logical factors which learn associations between latent and observed structure.", "labels": [], "entities": [{"text": "end task prediction", "start_pos": 73, "end_pos": 92, "type": "TASK", "confidence": 0.7526748180389404}]}, {"text": "In comparisons with identical models which observe \"gold\" syntactic annotations, derived from off-the-shelf parsers or provided with the corpora, we find that our hidden marginalization method is comparable in both tasks and almost every language tested, sometimes significantly outperforming models which observe the true syntax.", "labels": [], "entities": []}, {"text": "The following sections serves as a preliminary, introducing an inventory of factors and variables for constructing factor graph representations of syntactically-coupled NLP tasks.", "labels": [], "entities": []}, {"text": "Section 3 explores the benefits of this method on relation extraction (RE), where we compare the use dependency and constituency structure as latent representations.", "labels": [], "entities": [{"text": "relation extraction (RE)", "start_pos": 50, "end_pos": 74, "type": "TASK", "confidence": 0.8759443998336792}]}, {"text": "We then turn to a more established semantic role labeling (SRL) task ( \u00a74) where we evaluate across a wide range of languages.", "labels": [], "entities": [{"text": "semantic role labeling (SRL)", "start_pos": 35, "end_pos": 63, "type": "TASK", "confidence": 0.7971377968788147}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Relation Extraction Results. Models using hidden constituency syntax provide significant gains over the  syntactically-uniformed baseline model in both languages, but the advantages of the latent syntax were mitigated on  the smaller Chinese data set.", "labels": [], "entities": [{"text": "Relation Extraction", "start_pos": 10, "end_pos": 29, "type": "TASK", "confidence": 0.916152685880661}, {"text": "Chinese data set", "start_pos": 244, "end_pos": 260, "type": "DATASET", "confidence": 0.8172717491785685}]}, {"text": " Table 2: SRL Results. The hidden model excels on the unlabeled prediction results, often besting the scores obtained  using the parses distributed with the CoNLL data sets. These gains did not always translate to the labeled task where  poor sense prediction hindered absolute performance.", "labels": [], "entities": [{"text": "SRL", "start_pos": 10, "end_pos": 13, "type": "TASK", "confidence": 0.9068081974983215}, {"text": "CoNLL data sets", "start_pos": 157, "end_pos": 172, "type": "DATASET", "confidence": 0.9589527646700541}]}]}