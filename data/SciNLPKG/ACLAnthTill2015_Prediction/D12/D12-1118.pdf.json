{"title": [{"text": "Besting the Quiz Master: Crowdsourcing Incremental Classification Games", "labels": [], "entities": [{"text": "Besting the Quiz Master", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.7726237624883652}, {"text": "Crowdsourcing Incremental Classification", "start_pos": 25, "end_pos": 65, "type": "TASK", "confidence": 0.6426323354244232}]}], "abstractContent": [{"text": "Cost-sensitive classification, where the features used in machine learning tasks have a cost, has been explored as a means of balancing knowledge against the expense of incrementally obtaining new features.", "labels": [], "entities": [{"text": "Cost-sensitive classification", "start_pos": 0, "end_pos": 29, "type": "TASK", "confidence": 0.7220044434070587}]}, {"text": "We introduce a setting where humans engage in classification with incrementally revealed features: the collegiate trivia circuit.", "labels": [], "entities": []}, {"text": "By providing the community with a web-based system to practice, we collected tens of thousands of implicit word-byword ratings of how useful features are for eliciting correct answers.", "labels": [], "entities": []}, {"text": "Observing humans' classification process, we improve the performance of a state-of-the art classifier.", "labels": [], "entities": []}, {"text": "We also use the dataset to evaluate a system to compete in the incremental classification task through a reduction of reinforcement learning to classification.", "labels": [], "entities": []}, {"text": "Our system learns when to answer a question, performing better than baselines and most human players.", "labels": [], "entities": []}], "introductionContent": [{"text": "A typical machine learning task takes as input a set of features and learns a mapping from features to a label.", "labels": [], "entities": []}, {"text": "In such a setting, the objective is to minimize the error of the mapping from features to labels.", "labels": [], "entities": []}, {"text": "We call this traditional setting, where all of the features are consumed, rapacious machine learning.", "labels": [], "entities": []}, {"text": "This not how humans approach the same task.", "labels": [], "entities": []}, {"text": "They do not exhaustively consider every feature.", "labels": [], "entities": []}, {"text": "After a certain point, a human has made a decision and no longer needs additional features.", "labels": [], "entities": []}, {"text": "Even indefatigable computers cannot always exhaustively consider every feature.", "labels": [], "entities": []}, {"text": "This is because the result is time sensitive, such as in interactive systems, or because processing time is limited by the sheer quantity of data, as in sifting e-mail for spam).", "labels": [], "entities": []}, {"text": "In such settings, often the best solution is incremental: allow a decision to be made without seeing all of an instance's features.", "labels": [], "entities": []}, {"text": "We discuss the incremental classification framework in Section 2.", "labels": [], "entities": []}, {"text": "Our understanding of how humans conduct incremental classification is limited.", "labels": [], "entities": [{"text": "incremental classification", "start_pos": 40, "end_pos": 66, "type": "TASK", "confidence": 0.7205483019351959}]}, {"text": "This is because complicating an already difficult annotation task is often an unwise tradeoff.", "labels": [], "entities": []}, {"text": "Instead, we adapt areal world setting where humans are already engaging (eagerly) in incremental classification-trivia games-and develop a cheap, easy method for capturing human incremental classification judgments.", "labels": [], "entities": [{"text": "capturing human incremental classification judgments", "start_pos": 162, "end_pos": 214, "type": "TASK", "confidence": 0.8027979612350464}]}, {"text": "After qualitatively examining how humans conduct incremental classification (Section 3), we show that knowledge of a human's incremental classification process improves state-of-the-art rapacious classification (Section 4).", "labels": [], "entities": [{"text": "rapacious classification", "start_pos": 186, "end_pos": 210, "type": "TASK", "confidence": 0.7804754078388214}]}, {"text": "Having established that these data contain an interesting signal, we build Bayesian models that, when embedded in a Markov decision process, can engage in effective incremental classification (Section 5), and develop new hierarchical models combining local and thematic content to better capture the underlying content (Section 7).", "labels": [], "entities": []}, {"text": "Finally, we conclude in Section 8 and discuss extensions to other problem areas.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 2: Payoff matrix (from the computer's perspective)  for when agents \"buzz\" during a question. To focus on  incremental classification, we exclude instances where the  human interrupts with an incorrect answer, as after an  opponent eliminates themselves, the answering reduces to  rapacious classification.", "labels": [], "entities": [{"text": "Payoff matrix", "start_pos": 10, "end_pos": 23, "type": "METRIC", "confidence": 0.9564879238605499}]}, {"text": " Table 3: Performance of strategies against users. The  human scoring columns show the average points per ques- tion (positive means winning on average, negative means  losing on average) that the algorithm would expect to ac- cumulate per question versus each human amalgam metric.  The index column notes the average index of the token  when the strategy chose to buzz.", "labels": [], "entities": []}]}