{"title": [{"text": "Extracting Opinion Expressions with semi-Markov Conditional Random Fields", "labels": [], "entities": []}], "abstractContent": [{"text": "Extracting opinion expressions from text is usually formulated as a token-level sequence labeling task tackled using Conditional Random Fields (CRFs).", "labels": [], "entities": [{"text": "Extracting opinion expressions from text", "start_pos": 0, "end_pos": 40, "type": "TASK", "confidence": 0.8906195402145386}, {"text": "token-level sequence labeling", "start_pos": 68, "end_pos": 97, "type": "TASK", "confidence": 0.6483292579650879}]}, {"text": "CRFs, however, do not readily model potentially useful segment-level information like syntactic constituent structure.", "labels": [], "entities": []}, {"text": "Thus, we propose a semi-CRF-based approach to the task that can perform sequence labeling at the segment level.", "labels": [], "entities": [{"text": "sequence labeling", "start_pos": 72, "end_pos": 89, "type": "TASK", "confidence": 0.6348945647478104}]}, {"text": "We extend the original semi-CRF model (Sarawagi and Co-hen, 2004) to allow the modeling of arbitrarily long expressions while accounting for their likely syntactic structure when modeling segment boundaries.", "labels": [], "entities": []}, {"text": "We evaluate performance on two opinion extraction tasks, and, in contrast to previous sequence labeling approaches to the task, explore the usefulness of segment-level syntactic parse features.", "labels": [], "entities": [{"text": "opinion extraction tasks", "start_pos": 31, "end_pos": 55, "type": "TASK", "confidence": 0.8138614296913147}]}, {"text": "Experimental results demonstrate that our approach outper-forms state-of-the-art methods for both opinion expression tasks.", "labels": [], "entities": [{"text": "opinion expression tasks", "start_pos": 98, "end_pos": 122, "type": "TASK", "confidence": 0.7991635799407959}]}], "introductionContent": [{"text": "Accurate opinion expression identification is crucial for tasks that benefit from fine-grained opinion analysis ( ): e.g., it is a first step in characterizing the sentiment and intensity of the opinion; it provides a textual anchor for identifying the opinion holder and the target or topic of an opinion; and these, in turn, form the basis of opinionoriented question answering and opinion summarization systems.", "labels": [], "entities": [{"text": "Accurate opinion expression identification", "start_pos": 0, "end_pos": 42, "type": "TASK", "confidence": 0.7137639671564102}, {"text": "characterizing the sentiment and intensity of the opinion", "start_pos": 145, "end_pos": 202, "type": "TASK", "confidence": 0.7869157791137695}, {"text": "question answering", "start_pos": 361, "end_pos": 379, "type": "TASK", "confidence": 0.6899849772453308}, {"text": "opinion summarization", "start_pos": 384, "end_pos": 405, "type": "TASK", "confidence": 0.6657405942678452}]}, {"text": "In this paper, we focus on opinion expressions as defined in  subjective expressions that denote emotions, sentiment, beliefs, opinions, judgments, or other private states in text.", "labels": [], "entities": []}, {"text": "These include direct subjective expressions (DSEs): explicit mentions of private states or speech events expressing private states; and expressive subjective expressions (ESEs): expressions that indicate sentiment, emotion, etc.", "labels": [], "entities": []}, {"text": "Following are two example sentences labeled with DSEs and ESEs.", "labels": [], "entities": [{"text": "ESEs", "start_pos": 58, "end_pos": 62, "type": "METRIC", "confidence": 0.9557572603225708}]}, {"text": "(1) The International Committee of the Red Cross, [as usual] , [has refused to make any statements] . (2) The Chief Minister that [the demon they have reared will eat up their own vitals] . As a type of information extraction task, opinion expression extraction has been successfully tackled in the past via sequence tagging methods: and, for example, apply conditional random fields (CRFs) () using sophisticated token-level features.", "labels": [], "entities": [{"text": "information extraction", "start_pos": 203, "end_pos": 225, "type": "TASK", "confidence": 0.7666496932506561}, {"text": "opinion expression extraction", "start_pos": 232, "end_pos": 261, "type": "TASK", "confidence": 0.7770113547643026}, {"text": "sequence tagging", "start_pos": 308, "end_pos": 324, "type": "TASK", "confidence": 0.7302838563919067}]}, {"text": "In token-level sequence labeling, labels are assigned to single tokens, and the label of each token depends on the current token and the label of the previous token (we consider the usual first-order assumption).", "labels": [], "entities": [{"text": "token-level sequence labeling", "start_pos": 3, "end_pos": 32, "type": "TASK", "confidence": 0.6572604576746622}]}, {"text": "Segment-based features -features that describe a set of related contiguous tokens, e.g., a phrase or constituent -might provide critical information for identifying opinion expressions; they cannot, however, be readily and naturally represented in the CRF model.", "labels": [], "entities": []}, {"text": "Our goal in this work is to extract opinion expressions at the segment level with semi-Markov conditional random fields (semi-CRFs).", "labels": [], "entities": []}, {"text": "Semi-CRFs () are more powerful than CRFs in that they allow one to construct features to capture characteristics of the subsequences of a sentence.", "labels": [], "entities": []}, {"text": "They are defined on semi-Markov chains where labels are attached to segments instead of tokens and label dependencies are modeled at the segment-level.", "labels": [], "entities": []}, {"text": "Previous work has shown that semiCRFs outperform CRFs on named entity recognition (NER) tasks ().", "labels": [], "entities": [{"text": "named entity recognition (NER) tasks", "start_pos": 57, "end_pos": 93, "type": "TASK", "confidence": 0.83745619228908}]}, {"text": "However, to the best of our knowledge, semi-CRF techniques have not been investigated for opinion expression extraction.", "labels": [], "entities": [{"text": "opinion expression extraction", "start_pos": 90, "end_pos": 119, "type": "TASK", "confidence": 0.8317644993464152}]}, {"text": "The contribution of this paper is a semi-CRFbased approach for opinion expression extraction that leverages parsing information to provide better modeling of opinion expressions.", "labels": [], "entities": [{"text": "opinion expression extraction", "start_pos": 63, "end_pos": 92, "type": "TASK", "confidence": 0.819985568523407}, {"text": "parsing information", "start_pos": 108, "end_pos": 127, "type": "TASK", "confidence": 0.8956382572650909}]}, {"text": "Specifically, possible segmentations are generated by taking into account likely syntactic structure during learning and inference.", "labels": [], "entities": []}, {"text": "As a result, arbitrarily long expressions can be modeled and their boundaries can be influenced by probable syntactic structure.", "labels": [], "entities": []}, {"text": "We also explore the impact of syntactic features for extracting opinion expressions.", "labels": [], "entities": [{"text": "extracting opinion expressions", "start_pos": 53, "end_pos": 83, "type": "TASK", "confidence": 0.8562701145807902}]}, {"text": "We evaluate our model on two opinion extraction tasks: identifying direct subjective expressions (DSEs) and expressive subjective expressions (ESEs).", "labels": [], "entities": [{"text": "opinion extraction", "start_pos": 29, "end_pos": 47, "type": "TASK", "confidence": 0.7693167328834534}]}, {"text": "Experimental results show that our approach outperforms the state-of-the-art approach for the task by a large margin.", "labels": [], "entities": []}, {"text": "We also identify useful syntactic features for the task.", "labels": [], "entities": []}], "datasetContent": [{"text": "For evaluation, we use the MPQA 1.2 corpus ( , a widely used data set for fine-grained opinion analysis.", "labels": [], "entities": [{"text": "MPQA 1.2 corpus", "start_pos": 27, "end_pos": 42, "type": "DATASET", "confidence": 0.9241244792938232}, {"text": "fine-grained opinion analysis", "start_pos": 74, "end_pos": 103, "type": "TASK", "confidence": 0.6362509429454803}]}, {"text": "It contains 535 news articles, a total of 11,114 sentences with subjectivity-related annotations at the phrase level.", "labels": [], "entities": []}, {"text": "We focus on the task of extracting two types of opinion expressions: direct subjective expressions (DSEs) and expressive subjective expressions (ESEs).", "labels": [], "entities": []}, {"text": "shows some statistics of the corpus.", "labels": [], "entities": []}, {"text": "As in prior research that uses the corpus, we set aside the standard 135 documents as a development set and use 400 documents as the evaluation set.", "labels": [], "entities": []}, {"text": "All experiments employ 10-fold cross validation on the evaluation set, and the average overall runs is reported.", "labels": [], "entities": []}, {"text": "We use precision, recall, and F-measure to evaluate the quality of the model.", "labels": [], "entities": [{"text": "precision", "start_pos": 7, "end_pos": 16, "type": "METRIC", "confidence": 0.9996412992477417}, {"text": "recall", "start_pos": 18, "end_pos": 24, "type": "METRIC", "confidence": 0.999480664730072}, {"text": "F-measure", "start_pos": 30, "end_pos": 39, "type": "METRIC", "confidence": 0.9995255470275879}]}, {"text": "Precision is defined as |C\u2229P | |P | and recall, as |C\u2229P | |C| , where C and P are the sets of correct and predicted expression spans, respectively.", "labels": [], "entities": [{"text": "Precision", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.9714714288711548}, {"text": "recall", "start_pos": 40, "end_pos": 46, "type": "METRIC", "confidence": 0.9991772770881653}]}, {"text": "F-measure is computed as 2P RP +R . Because the boundaries of opinion expressions are hard to define even for human annotators ), previous research mainly focused on soft precision and recall measures for performance evaluation.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.9162033796310425}, {"text": "precision", "start_pos": 171, "end_pos": 180, "type": "METRIC", "confidence": 0.8237286806106567}, {"text": "recall", "start_pos": 185, "end_pos": 191, "type": "METRIC", "confidence": 0.9977558255195618}]}, {"text": "introduced an overlap measure, which considers a predicted expression to be correct if it overlaps with a correct expression.", "labels": [], "entities": [{"text": "overlap measure", "start_pos": 14, "end_pos": 29, "type": "METRIC", "confidence": 0.9718257486820221}]}, {"text": "We refer to this metric as Binary Overlap.", "labels": [], "entities": []}, {"text": "Johansson and Moschitti (2010) provides a stricter measure that computes the proportion of overlapping spans: if a correct expression s overlaps with a predicted expression s , the overlap contributes value |s\u2229s | |s | to |C \u2229 P | instead of value 1.", "labels": [], "entities": []}, {"text": "We refer to this metric as Proportional Overlap.", "labels": [], "entities": [{"text": "Proportional Overlap", "start_pos": 27, "end_pos": 47, "type": "METRIC", "confidence": 0.8355515897274017}]}, {"text": "To compare with previous work, we present our results according to both metrics.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Statistics of opinion expressions in the MPQA  Corpus.", "labels": [], "entities": [{"text": "MPQA  Corpus", "start_pos": 51, "end_pos": 63, "type": "DATASET", "confidence": 0.9759874641895294}]}, {"text": " Table 2: Results for extracting opinion expressions with Binary-Overlap metric. (w/ syn) indicates the inclusion of  syntactic parse features VPpre, VParg and VPsubj. Results of new-semi-CRF that are statistically significantly greater  than semi-CRF according to a two-tailed t-test are indicated with  *  (p < 0.1),  *  *  (p < 0.05),  *  *  *  (p < 0.005). T-test  results are also shown for new-semi-CRF(w/ syn) versus semi-CRF(w/ syn).", "labels": [], "entities": [{"text": "T-test", "start_pos": 361, "end_pos": 367, "type": "METRIC", "confidence": 0.9702349901199341}]}, {"text": " Table 3: Results for extracting opinion expressions with Proportional-Overlap metric. Notation is the same as above.", "labels": [], "entities": [{"text": "Proportional-Overlap metric", "start_pos": 58, "end_pos": 85, "type": "METRIC", "confidence": 0.9654334783554077}]}, {"text": " Table 4: Effect of syntactic features on extracting opinion expressions with Binary-Overlap metric", "labels": [], "entities": []}]}