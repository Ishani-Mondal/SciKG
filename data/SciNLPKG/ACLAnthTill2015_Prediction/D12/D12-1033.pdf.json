{"title": [{"text": "Syntactic surprisal affects spoken word duration in conversational contexts", "labels": [], "entities": [{"text": "Syntactic surprisal", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.9549681842327118}, {"text": "spoken word duration", "start_pos": 28, "end_pos": 48, "type": "TASK", "confidence": 0.6776368220647176}]}], "abstractContent": [{"text": "We present results of a novel experiment to investigate speech production in conversational data that links speech rate to information density.", "labels": [], "entities": []}, {"text": "We provide the first evidence for an association between syntactic surprisal and word duration in recorded speech.", "labels": [], "entities": []}, {"text": "Using the AMI corpus which contains transcriptions of focus group meetings with precise word durations, we show that word durations correlate with syntactic surprisal estimated from the incre-mental Roark parser over and above simpler measures, such as word duration estimated from a state-of-the-art text-to-speech system and word frequencies, and that the syntactic surprisal estimates are better predictors of word durations than a simpler version of sur-prisal based on trigram probabilities.", "labels": [], "entities": [{"text": "AMI corpus", "start_pos": 10, "end_pos": 20, "type": "DATASET", "confidence": 0.9213242530822754}]}, {"text": "This result supports the uniform information density (UID) hypothesis and points away to more realistic artificial speech generation.", "labels": [], "entities": []}], "introductionContent": [{"text": "The uniform information density (UID) hypothesis suggests that speakers try to distribute information uniformly across their utterances.", "labels": [], "entities": []}, {"text": "Information density can be measured in terms of the surprisal incurred at each word, where surprisal is defined as the negative log-probability of an event.", "labels": [], "entities": [{"text": "surprisal", "start_pos": 91, "end_pos": 100, "type": "METRIC", "confidence": 0.9538286328315735}]}, {"text": "This paper sets out to test whether UID holds across different linguistic levels, i.e. whether speakers adapt word duration during production to syntactic surprisal, such that words with higher surprisal have longer durations than words with lower surprisal.", "labels": [], "entities": []}, {"text": "We investigate this question in a corpus of transcribed speech from a mix of native and nonnative English speakers, a population that is a nontrivial component of the user base for language technologies developed for English.", "labels": [], "entities": []}, {"text": "This data reflects a casual, uncontrolled conversational environment.", "labels": [], "entities": []}, {"text": "Using linear mixed-effects modeling, we found that syntactic surprisal as calculated from a topdown incremental PCFG parser accounts fora significant amount of variation in spoken word duration, using an HMM-trained text-to-speech system as a baseline.", "labels": [], "entities": []}, {"text": "The findings of this paper provide additional support the uniform information density hypothesis and furthermore have implications for the design of text-to-speech systems, which currently do not take into account higher-level linguistic information such as syntactic surprisal (or even word frequencies) for their word duration models.", "labels": [], "entities": []}], "datasetContent": [{"text": "The AMI corpus is provided in the NITE XML Toolkit (NXT) format.", "labels": [], "entities": [{"text": "AMI corpus", "start_pos": 4, "end_pos": 14, "type": "DATASET", "confidence": 0.7271274775266647}, {"text": "NITE XML Toolkit (NXT) format", "start_pos": 34, "end_pos": 63, "type": "DATASET", "confidence": 0.8614274178232465}]}, {"text": "We developed a custom interpreter to assemble the relevant data streams: words, meeting IDs, speaker IDs, speaker turns, and observed word durations.", "labels": [], "entities": []}, {"text": "In addition to grouping and re-ordering the information found in the original XML corpus, two more steps were taken to eliminate confounding noise from the data.", "labels": [], "entities": []}, {"text": "Non-words (e.g. \"uhm\", \"uh-hmm\", etc.) were filtered out, as were incomplete words or incorrectly transcribed words (e.g. \"recogn\", \"somethi\", etc); the criterion for rejection was presence in the English Gigaword corpus with subsequent minor corrections by hand, e.g., mapping unseen verbs back into the corpus and correcting obvious common misspellings.", "labels": [], "entities": [{"text": "English Gigaword corpus", "start_pos": 197, "end_pos": 220, "type": "DATASET", "confidence": 0.6759867866834005}]}, {"text": "Finally, turns that did not make for complete sentences, e.g., utterances that were interrupted in mid-2 A reviewer asks about the extent to which our Gigaword filtering process may remove words we might want to keep but admit words we want to reject.", "labels": [], "entities": []}, {"text": "As Gigaword is mostly newswire text, we do not expect the latter case to hold often.", "labels": [], "entities": []}, {"text": "AMI is hand-transcribed and uses consistent spellings for non-word interjections (easy to remove), and any spelling mistakes would have to coincide exactly with a Gigaword mistake.", "labels": [], "entities": [{"text": "AMI", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.9060079455375671}]}, {"text": "The other way around (rejecting what should be allowed) is easier to check, and we find that of 13K word types in AMI, about 7.2% are rejected for non-appearance in Gigaword, after filtering for interjections like \"mm-hmm\".", "labels": [], "entities": [{"text": "Gigaword", "start_pos": 165, "end_pos": 173, "type": "DATASET", "confidence": 0.8814460039138794}]}, {"text": "However, we manually checked them and returned all but 2.9% of word types to the corpus.", "labels": [], "entities": []}, {"text": "These tend to be very low-frequency types.", "labels": [], "entities": []}, {"text": "The manual check suggests that ultimately there would be few false rejections.", "labels": [], "entities": []}, {"text": "sentence, were filtered out in order to maximize the proportion of complete parses in surprisal calculation.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Correlations (pearson) of model predictors.", "labels": [], "entities": []}, {"text": " Table 2: Baseline linear mixed effects model of speech  durations on the AMI corpus data for MARY CONTEXT  (including the sentential context), WORDFREQUENCY  under speaker with random intercept for speaker and ran- dom slopes under speaker. Predictors are centered.", "labels": [], "entities": [{"text": "AMI corpus data", "start_pos": 74, "end_pos": 89, "type": "DATASET", "confidence": 0.9607334733009338}, {"text": "MARY CONTEXT", "start_pos": 94, "end_pos": 106, "type": "TASK", "confidence": 0.486020028591156}, {"text": "Predictors", "start_pos": 242, "end_pos": 252, "type": "METRIC", "confidence": 0.9807567000389099}]}, {"text": " Table 3: Linear mixed effects model of speech durations  including 4-gram surprisal trained on gigaword as a pre- dictor.", "labels": [], "entities": []}, {"text": " Table 5: Linear mixed effects model of residual speech  durations wrt. baseline model from Table 3, with random  intercept for speaker and random slope for structural and  lexical component of surprisal, estimated using the Roark  parser.", "labels": [], "entities": []}, {"text": " Table 6: Native speakers are possibly slightly better at adapting their speech rate to syntactic surprisal than non-native  speakers. Surprisal value is for model with residuals of other predictors as dependent variable.", "labels": [], "entities": []}]}