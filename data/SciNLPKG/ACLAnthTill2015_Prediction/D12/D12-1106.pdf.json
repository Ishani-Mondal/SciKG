{"title": [{"text": "A coherence model based on syntactic patterns", "labels": [], "entities": []}], "abstractContent": [{"text": "We introduce a model of coherence which captures the intentional discourse structure in text.", "labels": [], "entities": []}, {"text": "Our work is based on the hypothesis that syntax provides a proxy for the communicative goal of a sentence and therefore the sequence of sentences in a coherent discourse should exhibit detectable structural patterns.", "labels": [], "entities": []}, {"text": "Results show that our method has high discriminating power for separating out coherent and incoherent news articles reaching accuracies of up to 90%.", "labels": [], "entities": [{"text": "separating out coherent and incoherent news articles", "start_pos": 63, "end_pos": 115, "type": "TASK", "confidence": 0.8720491783959525}, {"text": "accuracies", "start_pos": 125, "end_pos": 135, "type": "METRIC", "confidence": 0.9912853837013245}]}, {"text": "We also show that our syntactic patterns are correlated with manual annotations of intentional structure for academic conference articles and can successfully predict the coherence of abstract, introduction and related work sections of these articles.", "labels": [], "entities": []}], "introductionContent": [{"text": "Recent studies have introduced successful automatic methods to predict the structure and coherence of texts.", "labels": [], "entities": []}, {"text": "They include entity approaches for local coherence which track the repetition and syntactic realization of entities in adjacent sentences ( and content approaches for global coherence which view texts as a sequence of topics, each characterized by a particular distribution of lexical items ().", "labels": [], "entities": []}, {"text": "Other work has shown that co-occurrence of words) and discourse relations) also predict coherence.", "labels": [], "entities": []}, {"text": "Early theories ( posited that there are three factors which collectively contribute to coherence: intentional structure (purpose of discourse), attentional structure (what items are discussed) and the organization of discourse segments.", "labels": [], "entities": []}, {"text": "The highly successful entity approaches capture attentional structure and content approaches are related to topic segments but intentional structure has largely been neglected.", "labels": [], "entities": []}, {"text": "Every discourse has a purpose: explaining a concept, narrating an event, critiquing an idea and soon.", "labels": [], "entities": []}, {"text": "As a result each sentence in the article has a communicative goal and the sequence of goals helps the author achieve the discourse purpose.", "labels": [], "entities": []}, {"text": "In this work, we introduce a model to capture coherence from the intentional structure dimension.", "labels": [], "entities": []}, {"text": "Our key proposal is that syntactic patterns area useful proxy for intentional structure.", "labels": [], "entities": []}, {"text": "This idea is motivated from the fact that certain sentence types such as questions and definitions have distinguishable and unique syntactic structure.", "labels": [], "entities": []}, {"text": "For example, consider the opening sentences of two descriptive articles 1 shown in.", "labels": [], "entities": []}, {"text": "Sentences (1a) and (2a) are typical instances of definition sentences.", "labels": [], "entities": []}, {"text": "Definitions are written with the concept to be defined expressed as a noun phrase followed by a copular verb (is/are).", "labels": [], "entities": []}, {"text": "The predicate contains two parts: the first is a noun phrase reporting the concept as part of a larger class (eg. an aqueduct is a water supply), the second component is a relative clause listing unique properties of the concept.", "labels": [], "entities": []}, {"text": "These are examples of syntactic patterns related to the communicative goals of individual sentences.", "labels": [], "entities": []}, {"text": "Similarly, sentences (1b) and (2b) which provide further details about the concept also have some distinguish-1a) An aqueduct is a water supply or navigable channel constructed to convey water.", "labels": [], "entities": []}, {"text": "b) In modern engineering, the term is used for any system of pipes, canals, tunnels, and other structures used for this purpose.", "labels": [], "entities": []}, {"text": "2a) Cytokine receptors are receptors that binds cytokines.", "labels": [], "entities": []}, {"text": "b) In recent years, the cytokine receptors have come to demand more attention because their deficiency has now been directly linked to certain debilitating immunodeficiency states.: The first two sentences of two descriptive articles ing syntactic features such as the presence of a topicalized phrase providing the focus of the sentence.", "labels": [], "entities": []}, {"text": "The two sets of sentences have similar sequence of communicative goals and so we can expect the syntax of adjacent sentences to also be related.", "labels": [], "entities": []}, {"text": "We aim to characterize this relationship on abroad scale using a coherence model based entirely on syntax.", "labels": [], "entities": []}, {"text": "The model relies on two assumptions which summarize our intuitions about syntax and intentional structure: 1.", "labels": [], "entities": []}, {"text": "Sentences with similar syntax are likely to have the same communicative goal.", "labels": [], "entities": []}, {"text": "2. Regularities in intentional structure will be manifested in syntactic regularities between adjacent sentences.", "labels": [], "entities": []}, {"text": "There is also evidence from recent work that supports these assumptions.", "labels": [], "entities": []}, {"text": "find that a better syntactic parse of a sentence can be derived when the syntax of adjacent sentences is also taken into account.", "labels": [], "entities": []}, {"text": "report that the syntactic productions in adjacent sentences are powerful features for predicting which discourse relation (cause, contrast, etc.) holds between them.", "labels": [], "entities": []}, {"text": "show that significant associations exist between certain part of speech tags and sentence types such as explanation, dialog and argumentation.", "labels": [], "entities": []}, {"text": "In our model, syntax is represented either as parse tree productions or a sequence of phrasal nodes augmented with part of speech tags.", "labels": [], "entities": []}, {"text": "Our best performing method uses a Hidden Markov Model to learn the patterns in these syntactic items.", "labels": [], "entities": []}, {"text": "Sections 3 and 5 discuss the representations and their specific implementations and relative advantages.", "labels": [], "entities": []}, {"text": "Results show that syntax models can distinguish coherent and incoherent news articles from two domains with 75-90% accuracies over a 50% baseline.", "labels": [], "entities": []}, {"text": "In addition, the syntax coherence scores turnout complementary to scores given by lexical and entity models.", "labels": [], "entities": []}, {"text": "We also study our models' predictions on academic articles, a genre where intentional structure is widely studied.", "labels": [], "entities": []}, {"text": "Sections in these articles have well-defined purposes and we find recurring sentence types such as motivation, citations, description, and speculations.", "labels": [], "entities": []}, {"text": "There is a large body of work) concerned with defining and annotating these sentence types (called zones) in conference articles.", "labels": [], "entities": []}, {"text": "In Section 6, we describe how indeed some patterns captured by the syntax-based models are correlated with zone categories that were proposed in prior literature.", "labels": [], "entities": []}, {"text": "We also present results on coherence prediction: our model can distinguish the introduction section of conference papers from its perturbed versions with over 70% accuracy.", "labels": [], "entities": [{"text": "coherence prediction", "start_pos": 27, "end_pos": 47, "type": "TASK", "confidence": 0.8018866777420044}, {"text": "accuracy", "start_pos": 163, "end_pos": 171, "type": "METRIC", "confidence": 0.9975212216377258}]}, {"text": "Further, our model is able to identify conference from workshop papers with good accuracies, given that we can expect these articles to vary in purpose.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 2: Example sentences for preferred production sequences. The span of the LHS of the corresponding production  is indicated by [] braces.", "labels": [], "entities": []}, {"text": " Table 5: Accuracies on accident and earthquake corpora", "labels": [], "entities": [{"text": "Accuracies", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9947064518928528}]}, {"text": " Table 6: Accuracies for combined approaches", "labels": [], "entities": [{"text": "Accuracies", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9988386034965515}]}, {"text": " Table 7: Cluster-Zone mappings on the ART Corpus", "labels": [], "entities": [{"text": "ART", "start_pos": 39, "end_pos": 42, "type": "DATASET", "confidence": 0.9466248750686646}]}, {"text": " Table 8: Accuracy in differentiating permutation from original sections on ACL and ART test sets.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9939044117927551}, {"text": "ACL and ART test sets", "start_pos": 76, "end_pos": 97, "type": "DATASET", "confidence": 0.7401092827320099}]}, {"text": " Table 9: Accuracy (% examples) above each confidence  level for the conference versus workshop task.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9991377592086792}]}, {"text": " Table 9. The proportion  of examples under each setting is also indicated.  When only examples above 0.6 confidence are ex- amined, the classifier has a higher accuracy of 63.8%  for abstracts and covers close to 70% of the exam- ples. Similarly, when a cutoff of 0.7 is applied to the  confidence for predicting related work sections, we  achieve 63.3% accuracy for 53% of examples. So  we can consider that 30 to 47% of the examples in  the two sections respectively are harder to tell apart.  Interestingly however even high confidence predic- tions on introductions remain incorrect.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 161, "end_pos": 169, "type": "METRIC", "confidence": 0.9968692660331726}, {"text": "predicting related work sections", "start_pos": 303, "end_pos": 335, "type": "TASK", "confidence": 0.8870996981859207}, {"text": "accuracy", "start_pos": 355, "end_pos": 363, "type": "METRIC", "confidence": 0.992916464805603}]}]}