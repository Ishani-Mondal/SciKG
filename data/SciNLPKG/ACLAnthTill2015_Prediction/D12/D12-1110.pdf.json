{"title": [{"text": "Semantic Compositionality through Recursive Matrix-Vector Spaces", "labels": [], "entities": [{"text": "Semantic Compositionality", "start_pos": 0, "end_pos": 25, "type": "TASK", "confidence": 0.7948933839797974}]}], "abstractContent": [{"text": "Single-word vector space models have been very successful at learning lexical information.", "labels": [], "entities": []}, {"text": "However, they cannot capture the com-positional meaning of longer phrases, preventing them from a deeper understanding of language.", "labels": [], "entities": []}, {"text": "We introduce a recursive neural network (RNN) model that learns compositional vector representations for phrases and sentences of arbitrary syntactic type and length.", "labels": [], "entities": []}, {"text": "Our model assigns a vector and a matrix to every node in a parse tree: the vector captures the inherent meaning of the constituent, while the matrix captures how it changes the meaning of neighboring words or phrases.", "labels": [], "entities": []}, {"text": "This matrix-vector RNN can learn the meaning of operators in propositional logic and natural language.", "labels": [], "entities": []}, {"text": "The model obtains state of the art performance on three different experiments: predicting fine-grained sentiment distributions of adverb-adjective pairs; classifying sentiment labels of movie reviews and classifying semantic relationships such as cause-effect or topic-message between nouns using the syntactic path between them.", "labels": [], "entities": [{"text": "classifying sentiment labels of movie reviews", "start_pos": 154, "end_pos": 199, "type": "TASK", "confidence": 0.8249296049276987}]}], "introductionContent": [{"text": "Semantic word vector spaces are at the core of many useful natural language applications such as search query expansions (), fact extraction for information retrieval) and automatic annotation of text with disambiguated Wikipedia links), among many others (.", "labels": [], "entities": [{"text": "search query expansions", "start_pos": 97, "end_pos": 120, "type": "TASK", "confidence": 0.6743796666463217}, {"text": "fact extraction", "start_pos": 125, "end_pos": 140, "type": "TASK", "confidence": 0.7494616508483887}, {"text": "information retrieval", "start_pos": 145, "end_pos": 166, "type": "TASK", "confidence": 0.7089726477861404}]}, {"text": "In these models the meaning of a word is encoded as a vector computed from co-occurrence statistics of a word and its neighboring words.", "labels": [], "entities": []}, {"text": "Such vectors have been shown to correlate well with human judgments of word similarity (.", "labels": [], "entities": []}, {"text": "Despite their success, single word vector models are severely limited since they do not capture compositionality, the important quality of natural language that allows speakers to determine the meaning of a longer expression based on the meanings of its words and the rules used to combine them.", "labels": [], "entities": []}, {"text": "This prevents them from gaining a deeper understanding of the semantics of longer phrases or sentences.", "labels": [], "entities": []}, {"text": "Recently, there has been much progress in capturing compositionality in vector spaces, e.g.,) (see related work).", "labels": [], "entities": []}, {"text": "We extend these approaches with a more general and powerful model of semantic composition.", "labels": [], "entities": [{"text": "semantic composition", "start_pos": 69, "end_pos": 89, "type": "TASK", "confidence": 0.78325554728508}]}, {"text": "We present a novel recursive neural network model for semantic compositionality.", "labels": [], "entities": [{"text": "semantic compositionality", "start_pos": 54, "end_pos": 79, "type": "TASK", "confidence": 0.7995986640453339}]}, {"text": "In our context, compositionality is the ability to learn compositional vector representations for various types of phrases and sentences of arbitrary length.", "labels": [], "entities": []}, {"text": "shows an illustration of the model in which each constituent (a word or longer phrase) has a matrix-vector (MV) representation.", "labels": [], "entities": []}, {"text": "The vector captures the meaning of that constituent.", "labels": [], "entities": []}, {"text": "The matrix captures how it modifies the meaning of the other word that it combines with.", "labels": [], "entities": []}, {"text": "A representation fora longer phrase is computed bottom-up by recursively combining the words according to the syntactic structure of a parse tree.", "labels": [], "entities": []}, {"text": "Since the model uses the MV representation with a neural network as the final merging function, we call our model a matrix-vector recursive neural network (MV-RNN).", "labels": [], "entities": []}, {"text": "We show that the ability to capture semantic compositionality in a syntactically plausible way translates into state of the art performance on various tasks.", "labels": [], "entities": []}, {"text": "The first experiment demonstrates that our model can learn fine-grained semantic compositionality.", "labels": [], "entities": []}, {"text": "The task is to predict a sentiment distribution over movie reviews of adverb-adjective pairs such as unbelievably sad or really awesome.", "labels": [], "entities": []}, {"text": "The MV-RNN is the only model that is able to properly negate sentiment when adjectives are combined with not.", "labels": [], "entities": []}, {"text": "The MV-RNN outperforms previous state of the art models on full sentence sentiment prediction of movie reviews.", "labels": [], "entities": [{"text": "full sentence sentiment prediction of movie reviews", "start_pos": 59, "end_pos": 110, "type": "TASK", "confidence": 0.7945255381720406}]}, {"text": "The last experiment shows that the MV-RNN can also be used to find relationships between words using the learned phrase vectors.", "labels": [], "entities": []}, {"text": "The relationship between words is recursively constructed and composed by words of arbitrary type in the variable length syntactic path between them.", "labels": [], "entities": []}, {"text": "On the associated task of classifying relationships between nouns in arbitrary positions of a sentence the model outperforms all previous approaches on the.", "labels": [], "entities": [{"text": "classifying relationships between nouns in arbitrary positions of a sentence", "start_pos": 26, "end_pos": 102, "type": "TASK", "confidence": 0.8522377610206604}]}, {"text": "It outperforms all but one of the previous approaches without using any hand-designed semantic resources such as WordNet or FrameNet.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 113, "end_pos": 120, "type": "DATASET", "confidence": 0.956278920173645}]}, {"text": "By adding WordNet hypernyms, POS and NER tags our model outperforms the state of the art that uses significantly more resources.", "labels": [], "entities": []}, {"text": "The code for our model is available at www.socher.org.", "labels": [], "entities": []}], "datasetContent": [{"text": "Evaluation of compositional vector spaces is a complex task.", "labels": [], "entities": [{"text": "Evaluation of compositional vector spaces", "start_pos": 0, "end_pos": 41, "type": "TASK", "confidence": 0.7685282170772553}]}, {"text": "Most related work compares similarity judgments of unsupervised models to those of human judgments and aims at high correlation.", "labels": [], "entities": [{"text": "correlation", "start_pos": 116, "end_pos": 127, "type": "METRIC", "confidence": 0.9519801139831543}]}, {"text": "These evaluations can give important insights.", "labels": [], "entities": []}, {"text": "However, even with good correlation the question remains how these models would perform on downstream NLP tasks such as sentiment detection.", "labels": [], "entities": [{"text": "sentiment detection", "start_pos": 120, "end_pos": 139, "type": "TASK", "confidence": 0.9631480872631073}]}, {"text": "We experimented with unsupervised learning of general vector-matrix representations by having the MV-RNN predict words in their correct context.", "labels": [], "entities": []}, {"text": "Initializing the models with these general representations, did not improve the performance on the tasks we consider.", "labels": [], "entities": []}, {"text": "For sentiment analysis, this is not surprising since antonyms often get similar vectors during unsupervised learning from co-occurrences due to high similarity of local syntactic contexts.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 4, "end_pos": 22, "type": "TASK", "confidence": 0.9754880666732788}]}, {"text": "In our experiments, the high prediction performance came from supervised learning of meaning representations using labeled data.", "labels": [], "entities": []}, {"text": "While these representations are task-specific, they could be used across tasks in a multi-task learning setup.", "labels": [], "entities": []}, {"text": "However, in order to fairly compare to related work, we use only the supervised data of each task.", "labels": [], "entities": []}, {"text": "Before we describe our fullscale experiments, we analyze the model's expressive powers.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Accuracy of classification on full length movie  review polarity (MR).", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9967915415763855}, {"text": "full length movie  review polarity (MR)", "start_pos": 40, "end_pos": 79, "type": "TASK", "confidence": 0.5528952032327652}]}, {"text": " Table 3: Examples of correct classifications of ordered, semantic relations between nouns by the MV-RNN. Note that  the final classifier is a recursive, compositional function of all the words in the syntactic path between the bracketed  words. The paths vary in length and the words vary in type.", "labels": [], "entities": []}, {"text": " Table 4: Learning methods, their feature sets and F1  results for predicting semantic relations between nouns.  The MV-RNN outperforms all but one method without  any additional feature sets. By adding three such features,  it obtains state of the art performance.", "labels": [], "entities": [{"text": "F1", "start_pos": 51, "end_pos": 53, "type": "METRIC", "confidence": 0.999226450920105}, {"text": "predicting semantic relations between nouns", "start_pos": 67, "end_pos": 110, "type": "TASK", "confidence": 0.881285572052002}]}]}