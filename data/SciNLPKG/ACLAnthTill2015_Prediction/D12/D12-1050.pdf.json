{"title": [{"text": "A Comparison of Vector-based Representations for Semantic Composition", "labels": [], "entities": [{"text": "Semantic Composition", "start_pos": 49, "end_pos": 69, "type": "TASK", "confidence": 0.7109645903110504}]}], "abstractContent": [{"text": "In this paper we address the problem of modeling compositional meaning for phrases and sentences using distributional methods.", "labels": [], "entities": []}, {"text": "We experiment with several possible combinations of representation and composition, exhibiting varying degrees of sophistication.", "labels": [], "entities": []}, {"text": "Some are shallow while others operate over syntactic structure, rely on parameter learning , or require access to very large corpora.", "labels": [], "entities": []}, {"text": "We find that shallow approaches are as good as more computationally intensive alternatives with regards to two particular tests: (1) phrase similarity and (2) paraphrase detection.", "labels": [], "entities": [{"text": "paraphrase detection", "start_pos": 159, "end_pos": 179, "type": "TASK", "confidence": 0.850836306810379}]}, {"text": "The sizes of the involved training corpora and the generated vectors are not as important as the fit between the meaning representation and compositional method.", "labels": [], "entities": []}], "introductionContent": [{"text": "Distributional models of semantics have seen considerable success at simulating a wide range of behavioral data in tasks involving semantic cognition and also in practical applications.", "labels": [], "entities": []}, {"text": "For example, they have been used to model judgments of semantic similarity) and association ( and have been shown to achieve human level performance on synonymy tests) such as those included in the Test of English as a Foreign Language (TOEFL).", "labels": [], "entities": []}, {"text": "This ability has been put to practical use in numerous natural language processing tasks such as automatic thesaurus extraction, word sense discrimination, language modeling, and the identification of analogical relations).", "labels": [], "entities": [{"text": "automatic thesaurus extraction", "start_pos": 97, "end_pos": 127, "type": "TASK", "confidence": 0.6722308993339539}, {"text": "word sense discrimination", "start_pos": 129, "end_pos": 154, "type": "TASK", "confidence": 0.7023154099782308}, {"text": "language modeling", "start_pos": 156, "end_pos": 173, "type": "TASK", "confidence": 0.7552591562271118}, {"text": "identification of analogical relations", "start_pos": 183, "end_pos": 221, "type": "TASK", "confidence": 0.8382379710674286}]}, {"text": "While much research has been directed at the most effective ways of constructing representations for individual words, there has been far less consensus regarding the representation of larger constructions such as phrases and sentences.", "labels": [], "entities": []}, {"text": "The problem has received some attention in the connectionist literature, particularly in response to criticisms of the ability of connectionist representations to handle complex structures.", "labels": [], "entities": []}, {"text": "More recently, several proposals have been put forward for computing the meaning of word combinations in vector spaces.", "labels": [], "entities": [{"text": "computing the meaning of word combinations", "start_pos": 59, "end_pos": 101, "type": "TASK", "confidence": 0.7159863114356995}]}, {"text": "This renewed interest is partly due to the popularity of distributional methods and their application potential to tasks that require an understanding of larger phrases or complete sentences.", "labels": [], "entities": []}, {"text": "For example, introduce a general framework for studying vector composition, which they formulate as a function f of two vectors u and v.", "labels": [], "entities": [{"text": "vector composition", "start_pos": 56, "end_pos": 74, "type": "TASK", "confidence": 0.7485240995883942}]}, {"text": "Different composition models arise, depending on how f is chosen.", "labels": [], "entities": []}, {"text": "Assuming that composition is a linear function of the Cartesian product of u and v allows to specify additive models which are by far the most common method of vector combination in the literature).", "labels": [], "entities": []}, {"text": "Alternatively, assuming that composition is a linear function of the tensor product of u and v, gives rise to models based on multiplication.", "labels": [], "entities": []}, {"text": "One of the most sophisticated proposals for semantic composition is that of and the more recent implementation of.", "labels": [], "entities": [{"text": "semantic composition", "start_pos": 44, "end_pos": 64, "type": "TASK", "confidence": 0.8246939182281494}]}, {"text": "Using techniques from logic, category theory, and quantum information they develop a compositional distributional semantics that brings type-logical and distributional vector space models together.", "labels": [], "entities": []}, {"text": "In their framework, words belong to different type-based categories and different categories exist in different dimensional spaces.", "labels": [], "entities": []}, {"text": "The category of a word is decided by the number and type of adjoints (arguments) it can take and the composition of a sentence results in a vector which exists in sentential space.", "labels": [], "entities": []}, {"text": "Verbs, adjectives and adverbs act as relational functions, are represented by matrices, and modify the properties of nouns, that are represented by vectors (see also fora proposal similar in spirit).", "labels": [], "entities": []}, {"text": "Clarke (2012) introduces context-theoretic semantics, a general framework for combining vector representations, based on a mathematical theory of meaning as context, and shows that it can be used to describe a variety of models including that of. and Socher et al.", "labels": [], "entities": []}, {"text": "(2011b) present a framework based on recursive neural networks that learns vector space representations for multi-word phrases and sentences.", "labels": [], "entities": []}, {"text": "The network is given a list of word vectors as input and a binary tree representing their syntactic structure.", "labels": [], "entities": []}, {"text": "Then, it computes an n-dimensional representation p of two n-dimensional children and the process is repeated at every parent node until a representation fora full tree is constructed.", "labels": [], "entities": []}, {"text": "Parent representations are computed essentially by concatenating the representations of their children.", "labels": [], "entities": []}, {"text": "During training, the model tries to minimize the reconstruction errors between the n-dimensional parent vectors and those representing their children.", "labels": [], "entities": []}, {"text": "This model can also compute compositional representations when the tree structure is not given, e.g., by greedily inferring a binary tree.", "labels": [], "entities": []}, {"text": "Although the type of function used for vector composition has attracted much attention, relatively less emphasis has been placed on the basic distributional representations on which the composition functions operate.", "labels": [], "entities": [{"text": "vector composition", "start_pos": 39, "end_pos": 57, "type": "TASK", "confidence": 0.7519320845603943}]}, {"text": "In this paper, we examine three types of distributional representation of increasing sophistication and their effect on semantic composition.", "labels": [], "entities": [{"text": "semantic composition", "start_pos": 120, "end_pos": 140, "type": "TASK", "confidence": 0.7240029871463776}]}, {"text": "These include a simple semantic space, where a word's vector represents its co-occurrence with neighboring words), a syntax-aware space based on weighted distributional tuples that encode typed co-occurrence relations among words (, and word embeddings computed with a neural language model.", "labels": [], "entities": []}, {"text": "Word embeddings are distributed representations, low-dimensional and real-valued.", "labels": [], "entities": []}, {"text": "Each dimension of the embedding represents a latent feature of the word, hopefully capturing useful syntactic and semantic properties.", "labels": [], "entities": []}, {"text": "Using these representations, we construct several compositional models, based on addition, multiplication, and recursive neural networks.", "labels": [], "entities": []}, {"text": "We assess the effectiveness of these models using two evaluation protocols.", "labels": [], "entities": []}, {"text": "The first one involves modeling similarity judgments for short phrases gathered inhuman experiments.", "labels": [], "entities": []}, {"text": "The second one is paraphrase detection, i.e., the task of examining two sentences and determining whether they have the same meaning ().", "labels": [], "entities": [{"text": "paraphrase detection", "start_pos": 18, "end_pos": 38, "type": "TASK", "confidence": 0.9267712235450745}]}, {"text": "We find that shallow approaches are as good as more computationally intensive alternatives.", "labels": [], "entities": []}, {"text": "They achieve considerable semantic expressivity without any learning, sophisticated linguistic processing, or access to very large corpora.", "labels": [], "entities": []}, {"text": "Our contributions in this work are three-fold: an empirical comparison of abroad range of compositional models, some of which are introduced here for the first time; the use of an evaluation methodology that takes into account the full spectrum of compositionality from phrases to sentences; and the empirical finding that relatively simple compositional models can be used to perform competitively on the paraphrase detection and phrase similarity tasks.", "labels": [], "entities": [{"text": "paraphrase detection and phrase similarity", "start_pos": 406, "end_pos": 448, "type": "TASK", "confidence": 0.7253853738307953}]}], "datasetContent": [{"text": "Our first experiment focused on modeling similarity judgments for short phrases gathered inhuman experiments.", "labels": [], "entities": [{"text": "modeling similarity judgments", "start_pos": 32, "end_pos": 61, "type": "TASK", "confidence": 0.6092530985673269}]}, {"text": "Distributional representations of individual words are commonly evaluated on tasks based on their ability to model semantic similarity relations, e.g., synonymy or priming.", "labels": [], "entities": []}, {"text": "Thus, it seems appropriate to evaluate phrase representations in a  similar manner.", "labels": [], "entities": []}, {"text": "Specifically, we used the dataset from which contains similarity judgments for adjective-noun, noun-noun and verb-object phrases, respectively.", "labels": [], "entities": []}, {"text": "2 Each item is a phrase pair phr 1 , phr 2 which has a human rating from 1 (very low similarity) to 7 (very high similarity).", "labels": [], "entities": []}, {"text": "Using the composition models described above, we compute the cosine similarity of phr 1 and phr 2 : Model similarities were evaluated against the human similarity ratings using Spearman's \u03c1 correlation coefficient.", "labels": [], "entities": [{"text": "\u03c1 correlation coefficient", "start_pos": 188, "end_pos": 213, "type": "METRIC", "confidence": 0.8631098667780558}]}, {"text": "summarizes the performance of the various models on the phrase similarity dataset.", "labels": [], "entities": []}, {"text": "Rows in the table correspond to different vector representations: the simple distributional semantic space (SDS) from, distributional memory tensor (DM) and the neural language model (NLM), for each phrase combination: adjective noun (Adj-N), nounnoun (N-N) and verb object (V-Obj).", "labels": [], "entities": []}, {"text": "For each phrase type we report results for each compositional model, namely additive (+), multiplicative () and recursive autoencoder (RAE).", "labels": [], "entities": [{"text": "recursive autoencoder (RAE)", "start_pos": 112, "end_pos": 139, "type": "METRIC", "confidence": 0.7961652874946594}]}, {"text": "The table also shows the dimensionality of the input vectors next to the vector representation.", "labels": [], "entities": []}, {"text": "As can be seen, for SDS the best performing model is multiplication, as it is mostly for DM.", "labels": [], "entities": [{"text": "SDS", "start_pos": 20, "end_pos": 23, "type": "TASK", "confidence": 0.9716997146606445}]}, {"text": "With regard to NLM, vector addition yields overall better results.", "labels": [], "entities": []}, {"text": "In general, neither DM or NLM in any compositional configuration are able to outperform SDS with multiplication.", "labels": [], "entities": []}, {"text": "All models in are significantly correlated with the human similarity judgments (p < 0.01).", "labels": [], "entities": []}, {"text": "Spearman's \u03c1 differences of 0.3 or more are significant at the 0.01 level, using a ttest).", "labels": [], "entities": []}, {"text": "Although the phrase similarity task gives a fairly direct insight into semantic similarity and compositional representations, it is somewhat limited in scope as it only considers two-word constructions rather than naturally occurring sentences.", "labels": [], "entities": [{"text": "phrase similarity task", "start_pos": 13, "end_pos": 35, "type": "TASK", "confidence": 0.8269306818644205}]}, {"text": "Ideally, we would like to augment our evaluation with a task which is based on large quantities of natural data and for which vector composition has practical consequences.", "labels": [], "entities": []}, {"text": "For these reasons, we used the Microsoft Research Paraphrase Corpus (MSRPC) introduced by.", "labels": [], "entities": [{"text": "Microsoft Research Paraphrase Corpus (MSRPC)", "start_pos": 31, "end_pos": 75, "type": "DATASET", "confidence": 0.8356058427265712}]}, {"text": "The corpus consists of sentence pairs Sen i 1 , Sen i 2 and labels indicating whether they are in a paraphrase relationship or not.", "labels": [], "entities": []}, {"text": "The vector representations obtained from our various compositional models were used as features for the paraphrase classification task.", "labels": [], "entities": [{"text": "paraphrase classification", "start_pos": 104, "end_pos": 129, "type": "TASK", "confidence": 0.9698508083820343}]}, {"text": "The MSRPC dataset contains 5,801 sentence pairs, we used the standard split of 4,076 training pairs (67.5% of which are paraphrases) and 1,725 test pairs (66.5% of which are paraphrases).", "labels": [], "entities": [{"text": "MSRPC dataset", "start_pos": 4, "end_pos": 17, "type": "DATASET", "confidence": 0.9268007576465607}]}, {"text": "In order to judge whether two sentences have the same meaning we employ's liblinear classifier.", "labels": [], "entities": []}, {"text": "For each of our three vector sources and three different compositional methods, we create the following features: (a) a vector representing the pair of input sentences either via concatenation (\"con\") or subtraction (\"sub\"); (b) a vector encoding which words appear therein (\"enc\"); and (c) a vector made up of the following four other pieces of information: the cosine similarity of the sentence vectors, the length of Sen i 1 , the length of Sen i 2 , and the unigram overlap among the two sentences.", "labels": [], "entities": []}, {"text": "In order to encode which words appear in: Paraphrase classification accuracy in %.", "labels": [], "entities": [{"text": "Paraphrase classification", "start_pos": 42, "end_pos": 67, "type": "TASK", "confidence": 0.8369830250740051}, {"text": "accuracy", "start_pos": 68, "end_pos": 76, "type": "METRIC", "confidence": 0.9625403881072998}]}, {"text": "Included features are in parentheses: \"con\" is sentence vector concatenation, \"sub\" is sentence vector subtraction, \"other\" stands for 4 other features (see Section 4) each sentence and how often, we define a vector wdCount i for sentence Sen i and enumerate all words occuring in the MSRPC: giving the word count vectors n MSRPC dimensions.", "labels": [], "entities": []}, {"text": "Thus the k-th component of wdCount i is the frequency with which the word w (MSRPC) k appears in fork = 1, ..., n MSRPC . Even though n MSRPC maybe large, the computer files storing our feature vectors do not explode in size because wdCount contains many zeros and the classifier allows a sparse notation of (non-zero) feature values.", "labels": [], "entities": []}, {"text": "Regarding the last four features, we measured the similarity between sentences the same way as we did with phrases in section 3.", "labels": [], "entities": []}, {"text": "Note that this is the cosine of the angle between senVec i 1 and senVec i 2 . This enables us to observe the similarity or dissimilarity of two sentences independent of their sentence length.", "labels": [], "entities": []}, {"text": "Even though each contained word increases or decreases the norm of the resulting sentence vector, this does not distort the overall similarity value, due to normalization.: Paraphrase classification F1-score in %.", "labels": [], "entities": [{"text": "F1-score", "start_pos": 199, "end_pos": 207, "type": "METRIC", "confidence": 0.8388466835021973}]}, {"text": "The involved features are exactly the same as in. ity of the intersection of each sentence's multisetbag-of-words.", "labels": [], "entities": []}, {"text": "The latter is encoded in the alreadyintroduced wdCount vectors.", "labels": [], "entities": [{"text": "wdCount vectors", "start_pos": 47, "end_pos": 62, "type": "DATASET", "confidence": 0.9209754168987274}]}, {"text": "Therefore, In order to establish which features work best for each representation and composition method, we exhaustively explored all combinations on a development set (20% of the original MSRPC training set).", "labels": [], "entities": [{"text": "MSRPC training set", "start_pos": 190, "end_pos": 208, "type": "DATASET", "confidence": 0.7431607047716776}]}, {"text": "Tables 4 (accuracy) and 5 (F1) show our results on the test set with the best feature combinations for each model (shown in parentheses).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9993886947631836}, {"text": "F1", "start_pos": 27, "end_pos": 29, "type": "METRIC", "confidence": 0.9991976618766785}]}, {"text": "Each row corresponds to a different type of composition and each column to a different word representation model.", "labels": [], "entities": []}, {"text": "As can be seen, the distributional memory (DM) is the best performing representation for the additive composition model.", "labels": [], "entities": []}, {"text": "The neural language model (NLM) gives best results for the recursive autoencoder (RAE), although the other two representations come close.", "labels": [], "entities": []}, {"text": "And finally the simple distributional semantic space (SDS) works best with multiplication.", "labels": [], "entities": []}, {"text": "Also note that the best performing models, namely DM with addition and SDS with multiplication, use a basic feature space consisting only of the cosine similarity of the composed sentence vectors, the length of the two sentences involved, and their unigram word overlap.", "labels": [], "entities": []}, {"text": "Although our intention was to use the paraphrase detection task as a test-bed for evaluating compositional models rather than achieving state-of-the-art results, compares our approach against previous work on the same task and dataset.", "labels": [], "entities": [{"text": "paraphrase detection task", "start_pos": 38, "end_pos": 63, "type": "TASK", "confidence": 0.8640723625818888}]}, {"text": "Initial research concentrated on individual words rather than sentential representations.", "labels": [], "entities": []}, {"text": "Several approaches used  WordNet in conjunction with distributional similarity in an attempt to detect meaning conveyed by synonymous words.", "labels": [], "entities": []}, {"text": "More recently, the addition of syntactic features based on dependency parse trees () has been shown to substantially boost performance.", "labels": [], "entities": [{"text": "dependency parse", "start_pos": 59, "end_pos": 75, "type": "TASK", "confidence": 0.7173122465610504}]}, {"text": "The model of, for example, uses quasi-synchronous dependency grammar to model the structure of the sentences involved in the comparison and their correspondences.", "labels": [], "entities": []}, {"text": "obtain an accuracy that is higher than previously published results.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9996035695075989}]}, {"text": "This model is more sophisticated than the one we used in our experiments (see and 5).", "labels": [], "entities": []}, {"text": "Rather than using the output of the RAE as features for the classifier, it applies dynamic pooling, a procedure that takes a similarity matrix as input (e.g., created by sentences with differing lengths) and maps it to a matrix of fixed size that represents more faithfully the global similarity structure.", "labels": [], "entities": []}, {"text": "Overall, we observe that our own models do as well as some of the models that employ WordNet and more sophisticated syntactic features.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 85, "end_pos": 92, "type": "DATASET", "confidence": 0.938403844833374}]}, {"text": "With regard to F1, we are comparable with and without using elaborate features, or any additional manipulations over and above the output of the composition functions Without dynamic pooling, their model yields an accuracy of which if added could increase performance.", "labels": [], "entities": [{"text": "F1", "start_pos": 15, "end_pos": 17, "type": "METRIC", "confidence": 0.9929534792900085}, {"text": "accuracy", "start_pos": 214, "end_pos": 222, "type": "METRIC", "confidence": 0.999504804611206}]}], "tableCaptions": [{"text": " Table 2: The 11 most frequent contexts in Baroni and  Lenci (2010)'s tensor (v and j represent verbs and adjec- tives, respectively).", "labels": [], "entities": []}, {"text": " Table 3: Correlation coefficients of model predictions  with subject similarity ratings (Spearman's \u03c1); columns  show dimensionality: fixed or varying (see Section 2.1),  composition method: + is additive vector composition,  is component-wise multiplicative vector composition,  RAE is Socher et al. (2011a)'s recursive auto-encoder.", "labels": [], "entities": [{"text": "RAE", "start_pos": 281, "end_pos": 284, "type": "METRIC", "confidence": 0.9928210377693176}]}, {"text": " Table 4: Paraphrase classification accuracy in %. In- cluded features are in parentheses: \"con\" is sentence vec- tor concatenation, \"sub\" is sentence vector subtraction,  \"other\" stands for 4 other features (see Section 4)", "labels": [], "entities": [{"text": "Paraphrase classification", "start_pos": 10, "end_pos": 35, "type": "TASK", "confidence": 0.8945622742176056}, {"text": "accuracy", "start_pos": 36, "end_pos": 44, "type": "METRIC", "confidence": 0.9821507930755615}]}, {"text": " Table 6: Overview of results on the MSRCP (test corpus).  Accuracy differences of 3.3 or more are significant at the  0.01 level (using the \u03c7 2 statistic).", "labels": [], "entities": [{"text": "MSRCP (test corpus)", "start_pos": 37, "end_pos": 56, "type": "DATASET", "confidence": 0.8935686469078064}, {"text": "Accuracy", "start_pos": 59, "end_pos": 67, "type": "METRIC", "confidence": 0.9990170001983643}]}]}