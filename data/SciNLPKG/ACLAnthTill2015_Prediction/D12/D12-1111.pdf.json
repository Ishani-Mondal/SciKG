{"title": [], "abstractContent": [{"text": "Existing vector space models typically map synonyms and antonyms to similar word vectors , and thus fail to represent antonymy.", "labels": [], "entities": []}, {"text": "We introduce anew vector space representation where antonyms lie on opposite sides of a sphere: in the word vector space, synonyms have cosine similarities close to one, while antonyms are close to minus one.", "labels": [], "entities": []}, {"text": "We derive this representation with the aid of a thesaurus and latent semantic analysis (LSA).", "labels": [], "entities": []}, {"text": "Each entry in the thesaurus-a word sense along with its synonyms and antonyms-is treated as a \"document,\" and the resulting document collection is subjected to LSA.", "labels": [], "entities": [{"text": "LSA", "start_pos": 160, "end_pos": 163, "type": "METRIC", "confidence": 0.581605851650238}]}, {"text": "The key contribution of this work is to show how to assign signs to the entries in the co-occurrence matrix on which LSA operates, so as to induce a subspace with the desired property.", "labels": [], "entities": []}, {"text": "We evaluate this procedure with the Graduate Record Examination questions of (Mo-hammed et al., 2008) and find that the method improves on the results of that study.", "labels": [], "entities": [{"text": "Graduate Record Examination questions", "start_pos": 36, "end_pos": 73, "type": "DATASET", "confidence": 0.8944683074951172}]}, {"text": "Further improvements result from refining the sub-space representation with discriminative training , and augmenting the training data with general newspaper text.", "labels": [], "entities": []}, {"text": "Altogether, we improve on the best previous results by 11 points absolute in F measure.", "labels": [], "entities": [{"text": "F measure", "start_pos": 77, "end_pos": 86, "type": "METRIC", "confidence": 0.9830228090286255}]}], "introductionContent": [{"text": "Vector space representations have proven useful across a wide variety of text processing applications ranging from document clustering to search relevance measurement.", "labels": [], "entities": [{"text": "Vector space representations", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.8542996247609457}, {"text": "document clustering", "start_pos": 115, "end_pos": 134, "type": "TASK", "confidence": 0.7512845098972321}, {"text": "search relevance measurement", "start_pos": 138, "end_pos": 166, "type": "TASK", "confidence": 0.7220226724942526}]}, {"text": "In these applications, text is represented as a vector in a multi-dimensional continuous space, and a similarity metric such as cosine similarity can be used to measure the relatedness of different items.", "labels": [], "entities": []}, {"text": "Vector space representations have been used both at the document and word levels.", "labels": [], "entities": []}, {"text": "At the document level, they are effective for applications including information retrieval), document clustering, search relevance measurement () and cross-lingual document retrieval.", "labels": [], "entities": [{"text": "information retrieval", "start_pos": 69, "end_pos": 90, "type": "TASK", "confidence": 0.7810260653495789}, {"text": "document clustering", "start_pos": 93, "end_pos": 112, "type": "TASK", "confidence": 0.7679456472396851}, {"text": "search relevance measurement", "start_pos": 114, "end_pos": 142, "type": "TASK", "confidence": 0.6238739887873331}, {"text": "cross-lingual document retrieval", "start_pos": 150, "end_pos": 182, "type": "TASK", "confidence": 0.6508703629175822}]}, {"text": "At the word level, vector representations have been used to measure word similarity and for language modeling.", "labels": [], "entities": [{"text": "language modeling", "start_pos": 92, "end_pos": 109, "type": "TASK", "confidence": 0.770638108253479}]}, {"text": "While quite successful, these applications have typically been consistent with a very general notion of similarity in which basic association is measured, and finer shades of meaning need not be distinguished.", "labels": [], "entities": []}, {"text": "For example, latent semantic analysis might assign a high degree of similarity to opposites as well as synonyms).", "labels": [], "entities": [{"text": "latent semantic analysis", "start_pos": 13, "end_pos": 37, "type": "TASK", "confidence": 0.7384919921557108}]}, {"text": "Independent of vector-space representations, a number of authors have focused on identifying different kinds of relatedness.", "labels": [], "entities": []}, {"text": "At the simplest level, we may wish to distinguish between synonyms and antonyms, which can be further differentiated.", "labels": [], "entities": []}, {"text": "For example, in synonymy, we may wish to distinguish hyponyms and hypernyms.", "labels": [], "entities": []}, {"text": "Moreover, notes that numerous kinds of antonymy are possible, for example antipodal pairs like \"top-bottom\" or gradable opposites like \"light-heavy.\"", "labels": [], "entities": []}, {"text": "Work in this area includes; van der).", "labels": [], "entities": []}, {"text": "Despite the existence of a large amount of related work in the literature, distinguishing synonyms and antonyms is still considered as a difficult open problem in general.", "labels": [], "entities": [{"text": "distinguishing synonyms and antonyms", "start_pos": 75, "end_pos": 111, "type": "TASK", "confidence": 0.7835459411144257}]}, {"text": "In this paper, we fuse these two strands of research in an attempt to develop a vector space representation in which the synonymy and antonymy are naturally differentiated.", "labels": [], "entities": []}, {"text": "We follow in requiring a representation in which two lexical items in an antonymy relation should lie at opposite ends of an axis.", "labels": [], "entities": []}, {"text": "However, in contrast to the logical axes used previously, we desire that antonyms should lie at the opposite ends of a sphere lying in a continuous and automatically induced vector space.", "labels": [], "entities": []}, {"text": "To generate this vector space, we present a novel method for assigning both negative and positive values to the TF-IDF weights used in latent semantic analysis.", "labels": [], "entities": [{"text": "latent semantic analysis", "start_pos": 135, "end_pos": 159, "type": "TASK", "confidence": 0.7295685807863871}]}, {"text": "To determine these signed values, we exploit the information present in a thesaurus.", "labels": [], "entities": []}, {"text": "The result is a vector space representation in which synonyms cluster together, and the opposites of a word tend to cluster together at the opposite end of a sphere.", "labels": [], "entities": []}, {"text": "This representation provides several advantages over the raw thesaurus.", "labels": [], "entities": []}, {"text": "First, by finding the items most and least similar to a word, we are able to discover new synonyms and antonyms.", "labels": [], "entities": []}, {"text": "Second, as discussed in Section 5, the representation provides a natural starting point for gradient-descent based optimization.", "labels": [], "entities": []}, {"text": "Thirdly, as we discuss in Section 6, it is straightforward to embed new words into the derived subspace by using information from a large unsupervised text corpus such as Wikipedia.", "labels": [], "entities": []}, {"text": "The remainder of this paper is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 describes previous work.", "labels": [], "entities": []}, {"text": "Section 3 presents the classical LSA approach and analyzes some of its limitations.", "labels": [], "entities": [{"text": "LSA", "start_pos": 33, "end_pos": 36, "type": "TASK", "confidence": 0.9116713404655457}]}, {"text": "In Section 4 we present our polarity inducing extension to LSA.", "labels": [], "entities": [{"text": "LSA", "start_pos": 59, "end_pos": 62, "type": "DATASET", "confidence": 0.8564566373825073}]}, {"text": "Section 5 further extends the approach by optimizing the vector space representation with supervised discriminative training.", "labels": [], "entities": []}, {"text": "Section 6 describes the proposed method of embedding new words in the thesaurus-derived subspace.", "labels": [], "entities": []}, {"text": "The experimental results of Section 7 indicate that the proposed method outperforms previous approaches on a GRE test of closest-opposites.", "labels": [], "entities": [{"text": "GRE", "start_pos": 109, "end_pos": 112, "type": "METRIC", "confidence": 0.8767874836921692}]}, {"text": "Finally, Section 8 concludes the paper.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we present our experimental results on applying PILSA and its extensions to answering the closest-opposite GRE questions.", "labels": [], "entities": [{"text": "PILSA", "start_pos": 65, "end_pos": 70, "type": "METRIC", "confidence": 0.5700376033782959}, {"text": "GRE", "start_pos": 124, "end_pos": 127, "type": "METRIC", "confidence": 0.5773472785949707}]}], "tableCaptions": [{"text": " Table 2: The W matrix for two thesaurus entries in  its original form. Rows represent documents; columns  words.", "labels": [], "entities": []}, {"text": " Table 3: The W matrix for two thesaurus entries in its  polarity-inducing form.", "labels": [], "entities": []}, {"text": " Table 5: The performance of PILSA vs. the number of di- mensions when applied to the closest-opposite questions  from the GRE development set. Out of the 162 ques- tions, using the Bloomsbury thesaurus data we are able  to answer 153 of them. Using 300 dimensions gives the  best precision (132/153 = 0.863). This dimension set- ting is also optimal when using the WordNet data, which  answers 100 questions correctly out of the 160 attempts  (100/160 = 0.625).", "labels": [], "entities": [{"text": "PILSA", "start_pos": 29, "end_pos": 34, "type": "METRIC", "confidence": 0.6002861857414246}, {"text": "GRE development set", "start_pos": 123, "end_pos": 142, "type": "DATASET", "confidence": 0.9592370390892029}, {"text": "Bloomsbury thesaurus data", "start_pos": 182, "end_pos": 207, "type": "DATASET", "confidence": 0.9681822657585144}, {"text": "precision", "start_pos": 281, "end_pos": 290, "type": "METRIC", "confidence": 0.998184859752655}, {"text": "WordNet data", "start_pos": 366, "end_pos": 378, "type": "DATASET", "confidence": 0.982222318649292}]}, {"text": " Table 6: The overall results. PILSA performs LSA on the signed TF-IDF vectors.", "labels": [], "entities": [{"text": "PILSA", "start_pos": 31, "end_pos": 36, "type": "DATASET", "confidence": 0.6319674849510193}]}]}