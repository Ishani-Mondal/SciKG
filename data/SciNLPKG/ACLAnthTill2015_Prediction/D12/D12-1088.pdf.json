{"title": [{"text": "Entropy-based Pruning for Phrase-based Machine Translation", "labels": [], "entities": [{"text": "Phrase-based Machine Translation", "start_pos": 26, "end_pos": 58, "type": "TASK", "confidence": 0.7981007099151611}]}], "abstractContent": [{"text": "Phrase-based machine translation models have shown to yield better translations than Word-based models, since phrase pairs encode the contextual information that is needed fora more accurate translation.", "labels": [], "entities": [{"text": "Phrase-based machine translation", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.6478707393010458}]}, {"text": "However, many phrase pairs do not encode any relevant context, which means that the translation event encoded in that phrase pair is led by smaller translation events that are independent from each other, and can be found on smaller phrase pairs, with little or no loss in translation accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 285, "end_pos": 293, "type": "METRIC", "confidence": 0.9142917990684509}]}, {"text": "In this work, we propose a relative entropy model for translation models, that measures how likely a phrase pair encodes a translation event that is derivable using smaller translation events with similar probabilities.", "labels": [], "entities": [{"text": "translation models", "start_pos": 54, "end_pos": 72, "type": "TASK", "confidence": 0.9372905194759369}]}, {"text": "This model is then applied to phrase table pruning.", "labels": [], "entities": []}, {"text": "Tests show that considerable amounts of phrase pairs can be excluded , without much impact on the translation quality.", "labels": [], "entities": []}, {"text": "In fact, we show that better translations can be obtained using our pruned models , due to the compression of the search space during decoding.", "labels": [], "entities": []}], "introductionContent": [{"text": "Phrase-based Machine Translation Models () model n-to-m translations of n source words tom target words, which are encoded in phrase pairs and stored in the translation model.", "labels": [], "entities": [{"text": "Phrase-based Machine Translation", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.6619003117084503}]}, {"text": "This approach has an advantage over Word-based Translation Models (, since translating multiple source words allows the context for each source word to be considered during translation.", "labels": [], "entities": [{"text": "Word-based Translation", "start_pos": 36, "end_pos": 58, "type": "TASK", "confidence": 0.6791632771492004}]}, {"text": "For instance, the translation of the English word \"in\" by itself to Portuguese is not obvious, since we do not have any context for the word.", "labels": [], "entities": [{"text": "translation of the English word \"in\"", "start_pos": 18, "end_pos": 54, "type": "TASK", "confidence": 0.8043300211429596}]}, {"text": "This word can be translated in the context of \"in (the box)\" to \"dentro\", or in the context of \"in (China)\" as \"na\".", "labels": [], "entities": []}, {"text": "In fact, the lexical entry for \"in\" has more than 10 good translations in Portuguese.", "labels": [], "entities": []}, {"text": "Consequently, the lexical translation entry for Word-based models splits the probabilistic mass between different translations, leaving the choice based on context to the language model.", "labels": [], "entities": []}, {"text": "On the other hand, in Phrase-based Models, we would have a phrase pair p(in the box, dentro da caixa) and p(in china, na china), where the words \"in the box\" and \"in China\" can be translated together to \"dentro da caixa\" and \"na China\", which substantially reduces the ambiguity.", "labels": [], "entities": []}, {"text": "In this case, both the translation and language models contribute to find the best translation based on the local context, which generally leads to better translations.", "labels": [], "entities": []}, {"text": "However, not all words add the same amount of contextual information.", "labels": [], "entities": []}, {"text": "Using the same example for \"in\", if we add the context \"(hid the key) in\", it is still not possible to accurately identify the best translation for the word \"in\".", "labels": [], "entities": []}, {"text": "The phrase extraction algorithm () does not discriminate which phrases pairs encode contextual information, and extracts all phrase pairs with consistent alignments.", "labels": [], "entities": [{"text": "phrase extraction", "start_pos": 4, "end_pos": 21, "type": "TASK", "confidence": 0.830820620059967}]}, {"text": "Hence, phrases that add no contextual information, such as, p(hid the key in, escondeu a chave na) and p(hid the key in, escondeu a chave dentro) are extracted.", "labels": [], "entities": []}, {"text": "This is undesirable because we are populating translation models with redundant phrase pairs, whose translations can be obtained using com-binations of other phrases with the same probabilities, namely p(hid the key, escondeu a chave), p(in, dentro) and p(in, na).", "labels": [], "entities": []}, {"text": "This is a problem that is also found in language modeling, where large amounts of redundant higher-order n-grams can make the model needlessly large.", "labels": [], "entities": [{"text": "language modeling", "start_pos": 40, "end_pos": 57, "type": "TASK", "confidence": 0.7436352968215942}]}, {"text": "For backoff language models, multiple pruning strategies based on relative entropy have been proposed, where the objective is to prune n-grams in away to minimize the relative entropy between the model before and after pruning.", "labels": [], "entities": []}, {"text": "While the concept of using relative entropy for pruning is not new and frequently used in backoff language models, there are no such models for machine translation.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 144, "end_pos": 163, "type": "TASK", "confidence": 0.771484911441803}]}, {"text": "Thus, the main contribution of our work is to propose a relative entropy pruning model for translation models used in Phrase-based Machine Translation.", "labels": [], "entities": [{"text": "Phrase-based Machine Translation", "start_pos": 118, "end_pos": 150, "type": "TASK", "confidence": 0.8474043210347494}]}, {"text": "It is shown that our pruning algorithm can eliminate phrase pairs with little or no impact in the predictions made in our translation model.", "labels": [], "entities": []}, {"text": "In fact, by reducing the search space, less search errors are made during decoding, which leads to improvements in translation quality.", "labels": [], "entities": []}, {"text": "This paper is organized as follows.", "labels": [], "entities": []}, {"text": "We describe and contrast the state of the art pruning algorithms in section 2.", "labels": [], "entities": []}, {"text": "In section 3, we describe our relativeentropy model for machine translation.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 56, "end_pos": 75, "type": "TASK", "confidence": 0.8132292032241821}]}, {"text": "Afterwards, in section 4, we apply our model for pruning in Phrase-based Machine Translation systems.", "labels": [], "entities": [{"text": "Phrase-based Machine Translation", "start_pos": 60, "end_pos": 92, "type": "TASK", "confidence": 0.7865157127380371}]}, {"text": "We perform experiments with our pruning algorithm based on phrase pair independence and analyse the results in section 5.", "labels": [], "entities": [{"text": "phrase pair independence", "start_pos": 59, "end_pos": 83, "type": "TASK", "confidence": 0.7118540008862814}]}, {"text": "Finally, we conclude in section 6.", "labels": [], "entities": []}], "datasetContent": [{"text": "We tested the performance of our system under two different environments.", "labels": [], "entities": []}, {"text": "The first is the small scale DIALOG translation task for IWSLT 2010 evaluation () using a small corpora for the Chinese-English language pair (henceforth referred to as \"IWSLT\").", "labels": [], "entities": [{"text": "DIALOG translation task", "start_pos": 29, "end_pos": 52, "type": "TASK", "confidence": 0.7666526436805725}, {"text": "IWSLT 2010 evaluation", "start_pos": 57, "end_pos": 78, "type": "DATASET", "confidence": 0.7970924774805704}]}, {"text": "The second one is a large scale test using the complete EUROPARL () corpora for the Portuguese-English language pair, which we will denote by \"EUROPARL\".", "labels": [], "entities": [{"text": "EUROPARL", "start_pos": 56, "end_pos": 64, "type": "DATASET", "confidence": 0.9010244607925415}]}], "tableCaptions": [{"text": " Table 2: Comparison between Significance Pruning (Sig- nificance Pruning) and Entropy-based pruning using the  uniform (Entropy (u) Pruning) and multinomial distribu- tions (Entropy (m) Pruning).", "labels": [], "entities": []}]}