{"title": [], "abstractContent": [{"text": "Despite significant recent work, purely unsu-pervised techniques for part-of-speech (POS) tagging have not achieved useful accuracies required by many language processing tasks.", "labels": [], "entities": [{"text": "part-of-speech (POS) tagging", "start_pos": 69, "end_pos": 97, "type": "TASK", "confidence": 0.6279019594192505}]}, {"text": "Use of parallel text between resource-rich and resource-poor languages is one source of weak supervision that significantly improves accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 133, "end_pos": 141, "type": "METRIC", "confidence": 0.9940242171287537}]}, {"text": "However, parallel text is not always available and techniques for using it require multiple complex algorithmic steps.", "labels": [], "entities": []}, {"text": "In this paper we show that we can build POS-taggers exceeding state-of-the-art bilingual methods by using simple hidden Markov models and a freely available and naturally growing resource , the Wiktionary.", "labels": [], "entities": [{"text": "Wiktionary", "start_pos": 194, "end_pos": 204, "type": "DATASET", "confidence": 0.922500491142273}]}, {"text": "Across eight languages for which we have labeled data to evaluate results, we achieve accuracy that significantly exceeds best unsupervised and parallel text methods.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 86, "end_pos": 94, "type": "METRIC", "confidence": 0.999371349811554}]}, {"text": "We achieve highest accuracy reported for several languages and show that our approach yields better out-of-domain taggers than those trained using fully supervised Penn Treebank.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 19, "end_pos": 27, "type": "METRIC", "confidence": 0.9991113543510437}, {"text": "Penn Treebank", "start_pos": 164, "end_pos": 177, "type": "DATASET", "confidence": 0.9937654137611389}]}], "introductionContent": [{"text": "Part-of-speech categories are elementary building blocks that play an important role in many natural language processing tasks, from machine translation to information extraction.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 133, "end_pos": 152, "type": "TASK", "confidence": 0.7553050816059113}, {"text": "information extraction", "start_pos": 156, "end_pos": 178, "type": "TASK", "confidence": 0.7477584481239319}]}, {"text": "Supervised learning of taggers from POS-annotated training text is a well-studied task, with several methods achieving near-human tagging accuracy.", "labels": [], "entities": [{"text": "Supervised learning of taggers from POS-annotated training text", "start_pos": 0, "end_pos": 63, "type": "TASK", "confidence": 0.65146928653121}, {"text": "accuracy", "start_pos": 138, "end_pos": 146, "type": "METRIC", "confidence": 0.9571405053138733}]}, {"text": "However, while English and a handful of other languages are fortunate enough to have comprehensive POSannotated corpora such as the Penn Treebank), most of the world's languages have no labeled corpora.", "labels": [], "entities": [{"text": "Penn Treebank", "start_pos": 132, "end_pos": 145, "type": "DATASET", "confidence": 0.993778645992279}]}, {"text": "The annotated corpora that do exist were costly to build, and are often not freely available or restricted to researchonly use.", "labels": [], "entities": []}, {"text": "Furthermore, much of the annotated text is of limited genre, normally focusing on newswire or literary text.", "labels": [], "entities": []}, {"text": "Performance of treebank-trained systems degrades significantly when applied to new domains ().", "labels": [], "entities": []}, {"text": "Unsupervised induction of POS taggers offers the possibility of avoiding costly annotation, but despite recent progress, the accuracy of unsupervised POS taggers still falls far behind supervised systems, and is not suitable for most applications.", "labels": [], "entities": [{"text": "POS taggers", "start_pos": 26, "end_pos": 37, "type": "TASK", "confidence": 0.7409549653530121}, {"text": "accuracy", "start_pos": 125, "end_pos": 133, "type": "METRIC", "confidence": 0.9991289973258972}, {"text": "POS taggers", "start_pos": 150, "end_pos": 161, "type": "TASK", "confidence": 0.7207567244768143}]}, {"text": "Using additional information, in the form of tag dictionaries or parallel text, seems unavoidable at present.", "labels": [], "entities": []}, {"text": "Early work on using tag dictionaries used a labeled corpus to extract all allowed word-tag pairs, which is quite an unrealistic scenario.", "labels": [], "entities": []}, {"text": "More recent work has used a subset of the observed word-tag pairs and focused on generalizing dictionary entries (.", "labels": [], "entities": []}, {"text": "Using corpusbased dictionaries greatly biases the test results, and gives little information about the capacity to generalize to different domains.", "labels": [], "entities": []}, {"text": "The main idea is to rely on existing dictionaries for some languages (e.g. English) and use parallel data to build a dictionary in the desired language and extend the dictionary coverage using label propagation.", "labels": [], "entities": [{"text": "label propagation", "start_pos": 193, "end_pos": 210, "type": "TASK", "confidence": 0.7256253659725189}]}, {"text": "However, parallel text does not exist for many pairs of languages and the proposed bilingual projection algorithms are fairly complex.", "labels": [], "entities": [{"text": "bilingual projection", "start_pos": 83, "end_pos": 103, "type": "TASK", "confidence": 0.7251775562763214}]}, {"text": "In this work we use the Wiktionary, a freely available, high coverage and constantly growing dictionary fora large number of languages.", "labels": [], "entities": []}, {"text": "We experiment with a very simple second-order Hidden Markov Model with feature-based emissions).", "labels": [], "entities": []}, {"text": "We outperform best current results using parallel text supervision across 8 different languages, even when the word type coverage is as low as 20%.", "labels": [], "entities": []}, {"text": "Furthermore, using the Brown corpus as out-of-domain data we show that using the Wiktionary produces better taggers than using the Penn Treebank dictionary (88.5% vs 85.9%).", "labels": [], "entities": [{"text": "Brown corpus", "start_pos": 23, "end_pos": 35, "type": "DATASET", "confidence": 0.9024709463119507}, {"text": "Penn Treebank dictionary", "start_pos": 131, "end_pos": 155, "type": "DATASET", "confidence": 0.9918033679326376}]}, {"text": "Our empirical analysis and the natural growth rate of the Wiktionary suggest that free, high-quality and multi-domain POS-taggers fora large number of languages can be obtained by standard and efficient models.", "labels": [], "entities": []}, {"text": "The source code, the dictionary mappings and the trained models described in this work are available at http://code.google.com/p/ wikily-supervised-pos-tagger/.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 2: Accuracy for Unsupervised, Bilingual, Wiktionary and Supervised models. Avg. is the average of all lan- guages except English. Unsupervised models are trained without dictionary and use an oracle to map tags to clusters.  Bilingual systems are trained using a dictionary transferred from English into the target language using word align- ments. The Projection model uses a dictionary build directly from the part-of-speech projection. The D&P model  extends the Projection model dictionary by using Label Propagation. Supervised models are trained using tree bank  information with SHMM-ME: Covered TBD used tree bank tag set for the words only if they are also in the Wiktionary  and All TBD uses tree bank tag sets for all words. 50, 100 and All Sent. models are trained in a supervised manner  using increasing numbers of training sentences.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9940162897109985}, {"text": "Avg.", "start_pos": 82, "end_pos": 86, "type": "METRIC", "confidence": 0.9990027546882629}]}]}