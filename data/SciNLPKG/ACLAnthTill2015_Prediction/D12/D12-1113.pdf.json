{"title": [{"text": "Learning-based Multi-Sieve Co-reference Resolution with Knowledge *", "labels": [], "entities": [{"text": "Co-reference Resolution", "start_pos": 27, "end_pos": 50, "type": "TASK", "confidence": 0.6659272760152817}]}], "abstractContent": [{"text": "We explore the interplay of knowledge and structure in co-reference resolution.", "labels": [], "entities": [{"text": "co-reference resolution", "start_pos": 55, "end_pos": 78, "type": "TASK", "confidence": 0.7313627302646637}]}, {"text": "To inject knowledge, we use a state-of-the-art system which cross-links (or \"grounds\") expressions in free text to Wikipedia.", "labels": [], "entities": []}, {"text": "We explore ways of using the resulting grounding to boost the performance of a state-of-the-art co-reference resolution system.", "labels": [], "entities": [{"text": "co-reference resolution", "start_pos": 96, "end_pos": 119, "type": "TASK", "confidence": 0.7208834886550903}]}, {"text": "To maximize the utility of the injected knowledge, we deploy a learning-based multi-sieve approach and develop novel entity-based features.", "labels": [], "entities": []}, {"text": "Our end system outper-forms the state-of-the-art baseline by 2 B 3 F1 points on non-transcript portion of the ACE 2004 dataset.", "labels": [], "entities": [{"text": "F1", "start_pos": 67, "end_pos": 69, "type": "METRIC", "confidence": 0.9595258831977844}, {"text": "ACE 2004 dataset", "start_pos": 110, "end_pos": 126, "type": "DATASET", "confidence": 0.9790809551874796}]}], "introductionContent": [{"text": "Co-reference resolution is the task of grouping mentions to entities.", "labels": [], "entities": [{"text": "Co-reference resolution", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.7616191506385803}, {"text": "grouping mentions to entities", "start_pos": 39, "end_pos": 68, "type": "TASK", "confidence": 0.8302818387746811}]}, {"text": "For example, consider the text snippet in 1 . The correct output groups the mentions {m 1 , m 2 , m 5 } to one entity while leaving m 3 * We thank Nicholas Rizzolo and Kai Wei Chang for their invaluable help with modifying the baseline co-reference system.", "labels": [], "entities": []}, {"text": "We thank the anonymous EMNLP reviewers for constructive comments.", "labels": [], "entities": [{"text": "EMNLP reviewers", "start_pos": 23, "end_pos": 38, "type": "DATASET", "confidence": 0.8283052444458008}]}, {"text": "This research was supported by the Army Research Laboratory (ARL) under agreement W911NF-09-2-0053 and by the Defense Advanced Research Projects Agency (DARPA) Machine Reading Program under Air Force Research Laboratory (AFRL) prime contract no.", "labels": [], "entities": [{"text": "Machine Reading", "start_pos": 160, "end_pos": 175, "type": "TASK", "confidence": 0.6775778234004974}]}, {"text": "Any opinions, findings, conclusions or recommendations are those of the authors and do not necessarily reflect the view of the ARL, DARPA, AFRL, or the US government.", "labels": [], "entities": [{"text": "ARL", "start_pos": 127, "end_pos": 130, "type": "DATASET", "confidence": 0.8323162198066711}, {"text": "AFRL", "start_pos": 139, "end_pos": 143, "type": "DATASET", "confidence": 0.8293246626853943}]}, {"text": "\u2020 The majority of this work was done while the first author was at the University of Illinois.", "labels": [], "entities": []}, {"text": "1 Throughout this paper, curly brackets {} denote the extent and square brackets [] denote the head.  and m 4 as singletons.", "labels": [], "entities": [{"text": "extent", "start_pos": 54, "end_pos": 60, "type": "METRIC", "confidence": 0.9636631608009338}]}, {"text": "Resolving co-reference is fundamental for understanding natural language.", "labels": [], "entities": []}, {"text": "For example in, to infer that Kusrk has suffered a torpedo detonation, we have to understand that {[vessel]} m1 refers to { This inference is typically trivial for humans, but proves extremely challenging for state-of-the-art coreference resolution systems.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 226, "end_pos": 248, "type": "TASK", "confidence": 0.9221251606941223}]}, {"text": "We believe that it is world knowledge that gives people the ability to understand text with such ease.", "labels": [], "entities": []}, {"text": "A human reader can infer that since Kursk sank, it must be a vessel and vessels which suffer catastrophic torpedo detonations can sink.", "labels": [], "entities": []}, {"text": "Moreover, some readers might just know that Kursk is a Russian submarine named after the city of Kursk, where the largest tank battle in history took place in 1943.", "labels": [], "entities": []}, {"text": "In this work we are using Wikipedia as a source of encyclopedic knowledge.", "labels": [], "entities": []}, {"text": "The key contributions of this work are: (1) Using Wikipedia to assign a set of knowledge attributes to mentions in a context-sensitive way.", "labels": [], "entities": []}, {"text": "For example, for the text in, our system assigns to the mention \"Kursk\" the nationalities: Russian, Soviet and the attributes ship, incident, submarine, shipwreck (as opposed to city or battle).", "labels": [], "entities": []}, {"text": "We are using a publicly available system for context-sensitive disambiguation to Wikipedia.", "labels": [], "entities": [{"text": "context-sensitive disambiguation", "start_pos": 45, "end_pos": 77, "type": "TASK", "confidence": 0.6368557214736938}]}, {"text": "We then extract attributes from the cross-linked Wikipedia pages (described in Sec.", "labels": [], "entities": []}, {"text": "3.1), assign these attributes to the document mentions (Sec.", "labels": [], "entities": []}, {"text": "3.2) and develop knowledge-rich compatibility metric between mentions (Sec.", "labels": [], "entities": []}, {"text": "3.3) 2 . (2) Integrating the strength of rule-based systems such as) into a machine learning framework.", "labels": [], "entities": []}, {"text": "We are using a multi-sieve approach, which splits pairwise \"co-reference\" vs. \"non-coreference\" decisions to different types and attempts to make the easy decisions first.", "labels": [], "entities": []}, {"text": "Our multi-sieve approach is different from) in several respects: (a) our sieves are machine-learning classifiers, (b) the same pair of mentions can fall into multiple sieves, (c) later sieves can override the decisions made by earlier sieves, allowing to recover from errors as additional evidence becomes available.", "labels": [], "entities": []}, {"text": "In our running example, the decision of whether {[vessel]} m1 refers to {} m2 is made before the decision of whether {[vessel]} m1 refers to {Norwegian [ship]} m 4 since decisions in the same sentence are believed to be easier than cross-sentence ones.", "labels": [], "entities": []}, {"text": "We describe our learningbased multi-sieve approach in Sec.", "labels": [], "entities": []}, {"text": "4. (3) A novel approach for entity-based features.", "labels": [], "entities": []}, {"text": "As sieves of classifiers are applied, our system attempts to model entities and share the attributes between the mentions belonging to the same entity.", "labels": [], "entities": []}, {"text": "Once the decision that {[vessel]} m1 and {} m2 co-refer is made, we want the two mentions to share the Russian nationality.", "labels": [], "entities": []}, {"text": "This allows us to avoid erroneously linking {[vessel]} m1 to {Norwegian [ship]} m 4 despite vessel and ship being synonyms in WordNet.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 126, "end_pos": 133, "type": "DATASET", "confidence": 0.960637092590332}]}, {"text": "However, in this work we allow the sieves to make conflicting decisions on the same pair of mentions.", "labels": [], "entities": []}, {"text": "Hence, obtaining entities and their attributes by straightforward transitive closure of co-reference predictions is impossible.", "labels": [], "entities": []}, {"text": "We describe our approach for leveraging possibly contradicting predictions in Sec.", "labels": [], "entities": []}, {"text": "5. (4) By adding word-knowledge features and us- The extracted attributes and the related resources are available for public download at http://cogcomp.cs.illinois.edu/Data/ Ace2004CorefWikiAttributes.zip Input: document d; mentions M = {m1, . .", "labels": [], "entities": []}, {"text": ", mN } 1) For each mi \u2208 M , assign it a Wikipedia page pi in a context-sensitive way (pi maybe null).", "labels": [], "entities": []}, {"text": "-If pi = null: extract knowledge attributes from pi and assign tom.", "labels": [], "entities": []}, {"text": "-Else extract knowledge attributes directly from m via noun-phrase parsing techniques.", "labels": [], "entities": [{"text": "noun-phrase parsing", "start_pos": 55, "end_pos": 74, "type": "TASK", "confidence": 0.717496931552887}]}, {"text": "3) Let Q = {(mi, mj)} i =j , be the queue of mention pairs approximately sorted by \"easy-first\").", "labels": [], "entities": []}, {"text": "4) Let G be a partial clustering graph.", "labels": [], "entities": []}, {"text": "5) While Q is not empty -Extract a pair p = (mi, mj) from Q.", "labels": [], "entities": []}, {"text": "-Using the knowledge attributes of mi and mj as well as the structure of G, classify whether p is co-referent.", "labels": [], "entities": []}, {"text": "-Update G with the classification decision.", "labels": [], "entities": [{"text": "Update", "start_pos": 1, "end_pos": 7, "type": "METRIC", "confidence": 0.9714713096618652}]}, {"text": "6) Construct an end clustering from G. ing learning-based multi-sieve approach, we improve the performance of the state-of-the-art system of () by 3 MUC, 2 B 3 and 2 CEAF F1 points on the non-transcript portion of the ACE 2004 dataset.", "labels": [], "entities": [{"text": "CEAF F1", "start_pos": 166, "end_pos": 173, "type": "METRIC", "confidence": 0.6963345110416412}, {"text": "ACE 2004 dataset", "start_pos": 218, "end_pos": 234, "type": "DATASET", "confidence": 0.981573243935903}]}, {"text": "We report our experimental results in Sec.", "labels": [], "entities": []}, {"text": "6 and conclude with discussion in Sec.", "labels": [], "entities": []}, {"text": "7. We conclude the introduction by giving a highlevel overview of our system in.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: F1 performance on co-referent mention pairs by sieve", "labels": [], "entities": [{"text": "F1", "start_pos": 10, "end_pos": 12, "type": "METRIC", "confidence": 0.9765061736106873}]}, {"text": " Table 2: Utility of knowledge and prediction features (F1 on co-referent mention pairs) by inference sieves. Both knowledge and", "labels": [], "entities": [{"text": "F1", "start_pos": 56, "end_pos": 58, "type": "METRIC", "confidence": 0.9949465394020081}]}]}