{"title": [{"text": "Improved Parsing and POS Tagging Using Inter-Sentence Consistency Constraints", "labels": [], "entities": [{"text": "POS Tagging", "start_pos": 21, "end_pos": 32, "type": "TASK", "confidence": 0.7214494943618774}]}], "abstractContent": [{"text": "State-of-the-art statistical parsers and POS taggers perform very well when trained with large amounts of in-domain data.", "labels": [], "entities": [{"text": "POS taggers", "start_pos": 41, "end_pos": 52, "type": "TASK", "confidence": 0.7876355051994324}]}, {"text": "When training data is out-of-domain or limited, accuracy degrades.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 48, "end_pos": 56, "type": "METRIC", "confidence": 0.9993706345558167}]}, {"text": "In this paper, we aim to compensate for the lack of available training data by exploiting similarities between test set sentences.", "labels": [], "entities": []}, {"text": "We show how to augment sentence-level models for parsing and POS tagging with inter-sentence consistency constraints.", "labels": [], "entities": [{"text": "parsing", "start_pos": 49, "end_pos": 56, "type": "TASK", "confidence": 0.9652833938598633}, {"text": "POS tagging", "start_pos": 61, "end_pos": 72, "type": "TASK", "confidence": 0.8259329199790955}]}, {"text": "To deal with the resulting global objective, we present an efficient and exact dual decomposition decoding algorithm.", "labels": [], "entities": []}, {"text": "In experiments, we add consistency constraints to the MST parser and the Stanford part-of-speech tagger and demonstrate significant error reduction in the domain adaptation and the lightly supervised settings across five languages.", "labels": [], "entities": []}], "introductionContent": [{"text": "State-of-the-art statistical parsers and POS taggers perform very well when trained with large amounts of data from their test domain.", "labels": [], "entities": [{"text": "POS taggers", "start_pos": 41, "end_pos": 52, "type": "TASK", "confidence": 0.7828916907310486}]}, {"text": "When training data is out-of-domain or limited, the performance of the resulting model often degrades.", "labels": [], "entities": []}, {"text": "In this paper, we aim to compensate for the lack of available training data by exploiting similarities between test set sentences.", "labels": [], "entities": []}, {"text": "Most parsing and tagging models are defined at the sentence-level, which makes such inter-sentence information sharing difficult.", "labels": [], "entities": [{"text": "parsing and tagging", "start_pos": 5, "end_pos": 24, "type": "TASK", "confidence": 0.8425519466400146}]}, {"text": "We show how to augment sentence-level models with inter-sentence constraints to encourage consistent descisions in similar * Both authors contributed equally to this contexts, and we give an efficient algorithm with formal guarantees for decoding such models.", "labels": [], "entities": []}, {"text": "In POS tagging, most taggers perform very well on word types that they have observed in training data, but they perform poorly on unknown words.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 3, "end_pos": 14, "type": "TASK", "confidence": 0.9256016612052917}]}, {"text": "With a global objective, we can include constraints that encourage a consistent tag across all occurrences of an unknown word type to improve accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 142, "end_pos": 150, "type": "METRIC", "confidence": 0.9954276084899902}]}, {"text": "In dependency parsing, the parser can benefit from surface-level features of the sentence, but with sparse or out-of-domain training data these features are very noisy.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 3, "end_pos": 21, "type": "TASK", "confidence": 0.81242635846138}]}, {"text": "Using a global objective, we can add constraints that encourage similar surface-level contexts to exhibit similar syntactic behaviour.", "labels": [], "entities": []}, {"text": "The first contribution of this work is the use of Markov random fields (MRFs) to model global constraints between sentences in dependency parsing and POS tagging.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 127, "end_pos": 145, "type": "TASK", "confidence": 0.7531701028347015}, {"text": "POS tagging", "start_pos": 150, "end_pos": 161, "type": "TASK", "confidence": 0.7919256389141083}]}, {"text": "We represent each word as anode, the tagging or parse decision as its label, and add constraints through edges.", "labels": [], "entities": []}, {"text": "MRFs allow us to include global constraints tailored to these problems, and to reason about inference in the corresponding global models.", "labels": [], "entities": []}, {"text": "The second contribution is an efficient dual decomposition algorithm for decoding a global objective with inter-sentence constraints.", "labels": [], "entities": []}, {"text": "These constraints generally make direct inference challenging since they tie together the entire test corpus.", "labels": [], "entities": []}, {"text": "To alleviate this issue, our algorithm splits the global inference problem into subproblems -decoding of individual sentences, and decoding of the global MRF.", "labels": [], "entities": []}, {"text": "These subproblems can be solved efficiently through known methods.", "labels": [], "entities": []}, {"text": "We show empirically that by iteratively solving these subproblems, we can find the exact solution to the global model.", "labels": [], "entities": []}, {"text": "We experiment with domain adaptation and lightly supervised training.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 19, "end_pos": 36, "type": "TASK", "confidence": 0.8260561525821686}]}, {"text": "We demonstrate that global models with consistency constraints can improve upon sentence-level models for dependency parsing and part-of-speech tagging.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 106, "end_pos": 124, "type": "TASK", "confidence": 0.8083576560020447}, {"text": "part-of-speech tagging", "start_pos": 129, "end_pos": 151, "type": "TASK", "confidence": 0.6951520144939423}]}, {"text": "For domain adaptation, we show an error reduction of up to 7.7% when adapting the second-order projective MST parser) from newswire to the QuestionBank domain.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 4, "end_pos": 21, "type": "TASK", "confidence": 0.7528696954250336}, {"text": "error reduction", "start_pos": 34, "end_pos": 49, "type": "METRIC", "confidence": 0.9782005548477173}, {"text": "QuestionBank domain", "start_pos": 139, "end_pos": 158, "type": "DATASET", "confidence": 0.9206505715847015}]}, {"text": "For lightly supervised learning, we show an error reduction of up to 12.8% over the same parser for five languages and an error reduction of up to 10.3% over the Stanford trigram tagger () for English POS tagging.", "labels": [], "entities": [{"text": "error reduction", "start_pos": 44, "end_pos": 59, "type": "METRIC", "confidence": 0.9735095500946045}, {"text": "error reduction", "start_pos": 122, "end_pos": 137, "type": "METRIC", "confidence": 0.979782909154892}, {"text": "POS tagging", "start_pos": 201, "end_pos": 212, "type": "TASK", "confidence": 0.636767789721489}]}, {"text": "The algorithm requires, on average, only 1.7 times the costs of sentence-level inference and finds the exact solution on the vast majority of sentences.", "labels": [], "entities": []}], "datasetContent": [{"text": "We experiment in two common scenarios where parsing performance is reduced from the fully supervised, in-domain case.", "labels": [], "entities": [{"text": "parsing", "start_pos": 44, "end_pos": 51, "type": "TASK", "confidence": 0.9720197319984436}]}, {"text": "In domain adaptation, we train our model completely in one source domain and test it on a different target domain.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 3, "end_pos": 20, "type": "TASK", "confidence": 0.772090882062912}]}, {"text": "In lightly supervised training, we simulate the case where only a limited amount of annotated data is available fora language.", "labels": [], "entities": []}, {"text": "To measure parsing performance, we use unlabeled attachment score (UAS) given by the CONLL-X dependency parsing shared task evaluation script ().", "labels": [], "entities": [{"text": "parsing", "start_pos": 11, "end_pos": 18, "type": "TASK", "confidence": 0.9775428771972656}, {"text": "unlabeled attachment score (UAS)", "start_pos": 39, "end_pos": 71, "type": "METRIC", "confidence": 0.802182118097941}, {"text": "CONLL-X dependency parsing shared task evaluation", "start_pos": 85, "end_pos": 134, "type": "TASK", "confidence": 0.8496537605921427}]}, {"text": "We compare the accuracy of dependency parsing with global constraints to the sentence-level dependency parser of and to a self-training baseline.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 15, "end_pos": 23, "type": "METRIC", "confidence": 0.9990779161453247}, {"text": "dependency parsing", "start_pos": 27, "end_pos": 45, "type": "TASK", "confidence": 0.7310831844806671}, {"text": "sentence-level dependency parser", "start_pos": 77, "end_pos": 109, "type": "TASK", "confidence": 0.619573732217153}]}, {"text": "The parsing baseline is equivalent to a single round of dual decomposition.", "labels": [], "entities": []}, {"text": "For the self-training baseline, we parse the test corpus, append the labeled test sentences to the training corpus, train anew parser, and then re-parse the test set.", "labels": [], "entities": []}, {"text": "We run this procedure fora single iteration.", "labels": [], "entities": []}, {"text": "For POS tagging we measure token level POS accuracy for all the words in the corpus and also for unknown words (words not observed in the training data).", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 4, "end_pos": 15, "type": "TASK", "confidence": 0.912062257528305}, {"text": "token level POS accuracy", "start_pos": 27, "end_pos": 51, "type": "METRIC", "confidence": 0.5240149796009064}]}, {"text": "We compare the accuracy of POS tagging with global constraints to the accuracy of the Stanford POS tagger 3 . Domain Adaptation Accuracy Results are presented in.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 15, "end_pos": 23, "type": "METRIC", "confidence": 0.9989861845970154}, {"text": "POS tagging", "start_pos": 27, "end_pos": 38, "type": "TASK", "confidence": 0.7919889986515045}, {"text": "accuracy", "start_pos": 70, "end_pos": 78, "type": "METRIC", "confidence": 0.9989795088768005}]}, {"text": "The constrained model reduces the error of the baseline on both cases.", "labels": [], "entities": [{"text": "error", "start_pos": 34, "end_pos": 39, "type": "METRIC", "confidence": 0.9745201468467712}]}, {"text": "Note that when the base parser is trained on the WSJ corpus its UAS performance on the QTB is 89.63%.", "labels": [], "entities": [{"text": "WSJ", "start_pos": 49, "end_pos": 52, "type": "DATASET", "confidence": 0.9031822085380554}, {"text": "UAS", "start_pos": 64, "end_pos": 67, "type": "METRIC", "confidence": 0.9675140380859375}, {"text": "QTB", "start_pos": 87, "end_pos": 90, "type": "DATASET", "confidence": 0.8436228036880493}]}, {"text": "Yet, the constrained model is still able to reduce the baseline error by 7.7%.", "labels": [], "entities": [{"text": "baseline error", "start_pos": 55, "end_pos": 69, "type": "METRIC", "confidence": 0.8252696990966797}]}], "tableCaptions": [{"text": " Table 1: Exploratory statistics for constraint selection.  The table shows the percentage of context types for which  the probability of the most frequent head tag is at least p.  Head in Context refers to the subset of contexts where the  most frequent head is within the context itself. Numbers  are based on Section 22 of the Wall Street Journal and are  given for contexts that appear at least 10 times.", "labels": [], "entities": [{"text": "constraint selection", "start_pos": 37, "end_pos": 57, "type": "TASK", "confidence": 0.9029771685600281}, {"text": "Section 22 of the Wall Street Journal", "start_pos": 312, "end_pos": 349, "type": "DATASET", "confidence": 0.7303317018917629}]}, {"text": " Table 3: Dependency parsing UAS by size of training set and language. English data is from the WSJ. Bulgarian,  German, Japanese, and Spanish data is from the CONLL-X data sets. Base is the second-order, projective dependency  parser of McDonald et al. (2005). ST is a self-training model based on Reichart and Rappoport (2007). Model is the  same parser augmented with inter-sentence constraints. ER is error reduction. Using the sign test with p \u2264 0.05, all  50, 100, and 200 results are significant, as are Eng and Ger 500.  50  100  200  500  Base  Model (ER)  Base Model (ER) Base Model (ER) Base Model (ER)  Acc 79.67 81.77 (10.33) 85.42 86.37 (6.52) 88.63 89.37 (6.51) 91.59 91.98 (4.64)  Unk 62.88 67.16 (11.53) 71.10 73.32 (7.68) 75.82 78.07 (9.31) 80.67 82.28 (8.33)", "labels": [], "entities": [{"text": "Dependency parsing UAS", "start_pos": 10, "end_pos": 32, "type": "TASK", "confidence": 0.7653648853302002}, {"text": "WSJ", "start_pos": 96, "end_pos": 99, "type": "DATASET", "confidence": 0.9784145951271057}, {"text": "CONLL-X data sets", "start_pos": 160, "end_pos": 177, "type": "DATASET", "confidence": 0.9595181941986084}, {"text": "error reduction", "start_pos": 405, "end_pos": 420, "type": "TASK", "confidence": 0.8129593431949615}, {"text": "Acc", "start_pos": 615, "end_pos": 618, "type": "METRIC", "confidence": 0.911187469959259}]}, {"text": " Table 4: POS tagging accuracy. Stanford POS tagger refers to the maximum entropy trigram tagger of", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 10, "end_pos": 21, "type": "TASK", "confidence": 0.73338183760643}, {"text": "accuracy", "start_pos": 22, "end_pos": 30, "type": "METRIC", "confidence": 0.9006622433662415}, {"text": "POS tagger", "start_pos": 41, "end_pos": 51, "type": "TASK", "confidence": 0.7263688445091248}]}, {"text": " Table 5: The five most effective constraint contexts from  the domain adaptation experiments. The bold POS tag  indicates the modifier word of the context.", "labels": [], "entities": [{"text": "POS", "start_pos": 104, "end_pos": 107, "type": "METRIC", "confidence": 0.8914971351623535}]}]}