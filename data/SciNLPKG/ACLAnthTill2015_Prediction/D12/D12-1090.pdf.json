{"title": [{"text": "Probabilistic Finite State Machines for Regression-based MT Evaluation", "labels": [], "entities": [{"text": "MT", "start_pos": 57, "end_pos": 59, "type": "TASK", "confidence": 0.8068917393684387}]}], "abstractContent": [{"text": "Accurate and robust metrics for automatic evaluation are key to the development of statistical machine translation (MT) systems.", "labels": [], "entities": [{"text": "statistical machine translation (MT)", "start_pos": 83, "end_pos": 119, "type": "TASK", "confidence": 0.7989207555850347}]}, {"text": "We first introduce anew regression model that uses a probabilistic finite state machine (pFSM) to compute weighted edit distance as predictions of translation quality.", "labels": [], "entities": []}, {"text": "We also propose a novel pushdown automaton extension of the pFSM model for modeling word swapping and cross alignments that cannot be captured by standard edit distance models.", "labels": [], "entities": [{"text": "word swapping and cross alignments", "start_pos": 84, "end_pos": 118, "type": "TASK", "confidence": 0.6464241147041321}]}, {"text": "Our models can easily incorporate a rich set of linguistic features, and automatically learn their weights, eliminating the need for ad-hoc parameter tuning.", "labels": [], "entities": []}, {"text": "Our methods achieve state-of-the-art correlation with human judgments on two different prediction tasks across a diverse set of standard evaluations (NIST OpenMT06,08; WMT06-08).", "labels": [], "entities": [{"text": "NIST", "start_pos": 150, "end_pos": 154, "type": "DATASET", "confidence": 0.9696933627128601}, {"text": "OpenMT06,08", "start_pos": 155, "end_pos": 166, "type": "DATASET", "confidence": 0.5209225416183472}, {"text": "WMT06-08", "start_pos": 168, "end_pos": 176, "type": "DATASET", "confidence": 0.7168603539466858}]}], "introductionContent": [{"text": "Research in automatic machine translation (MT) evaluation metrics has been a key driving force behind the recent advances of statistical machine translation (SMT) systems.", "labels": [], "entities": [{"text": "automatic machine translation (MT) evaluation metrics", "start_pos": 12, "end_pos": 65, "type": "TASK", "confidence": 0.8582590892910957}, {"text": "statistical machine translation (SMT)", "start_pos": 125, "end_pos": 162, "type": "TASK", "confidence": 0.7970027526219686}]}, {"text": "The early seminal work on automatic MT metrics (e.g., BLEU and NIST) is largely based on n-gram matches ().", "labels": [], "entities": [{"text": "MT metrics", "start_pos": 36, "end_pos": 46, "type": "TASK", "confidence": 0.9234744608402252}, {"text": "BLEU", "start_pos": 54, "end_pos": 58, "type": "METRIC", "confidence": 0.9428912997245789}, {"text": "NIST", "start_pos": 63, "end_pos": 67, "type": "DATASET", "confidence": 0.9180484414100647}]}, {"text": "Despite their simplicity, these measures have shown good correlation with human judgments, and enabled large-scale evaluations across many different MT systems, without incurring the huge labor cost of human evaluation, inter alia).", "labels": [], "entities": [{"text": "MT", "start_pos": 149, "end_pos": 151, "type": "TASK", "confidence": 0.9774271249771118}]}, {"text": "Recent studies have also confirmed that tuning MT systems against better MT metrics -using algorithms like MERT -leads to better system performance (.", "labels": [], "entities": [{"text": "MT", "start_pos": 47, "end_pos": 49, "type": "TASK", "confidence": 0.9729102253913879}]}, {"text": "Later metrics that move beyond n-grams achieve higher accuracy and improved robustness from resources like WordNet synonyms (, and paraphrasing.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 54, "end_pos": 62, "type": "METRIC", "confidence": 0.9987758994102478}]}, {"text": "But a common problem in these metrics is they typically resort to ad-hoc tuning methods instead of principled approaches to incorporate linguistic features.", "labels": [], "entities": []}, {"text": "Recent models use linear or SVM regression and train them against human judgments to automatic learn feature weights, and have shown state-of-the-art correlation with human judgments ().", "labels": [], "entities": []}, {"text": "The drawback, however, is they rely on time-consuming preprocessing modules to extract linguistic features (e.g., a full end-toend textual entailment system was needed in), which severely limits their practical use.", "labels": [], "entities": []}, {"text": "Furthermore, these models employ a large number of features (on the order of hundreds), and consequently make the model predictions opaque and hard to analyze.", "labels": [], "entities": []}, {"text": "In this paper, we propose a simple yet powerful probabilistic Finite State Machine (pFSM) for the task of MT evaluation.", "labels": [], "entities": [{"text": "MT evaluation", "start_pos": 106, "end_pos": 119, "type": "TASK", "confidence": 0.9780749380588531}]}, {"text": "It is built on the backbone of weighted edit distance models, but learns to weight edit operations in a probabilistic regression framework.", "labels": [], "entities": []}, {"text": "One of the major contributions of this paper is a novel extension of the pFSM model into a probabilistic Pushdown Automaton (pPDA), which enhances traditional edit-distance models with the ability to model phrase shift and word swapping.", "labels": [], "entities": [{"text": "phrase shift", "start_pos": 206, "end_pos": 218, "type": "TASK", "confidence": 0.7627339065074921}, {"text": "word swapping", "start_pos": 223, "end_pos": 236, "type": "TASK", "confidence": 0.7299376428127289}]}, {"text": "Furthermore, we give anew log-linear parameterization to the pFSM model, which allows it to easily incor-porate rich linguistic features.", "labels": [], "entities": []}, {"text": "We experiment with a set of simple features based on labeled head-modifier dependency structure, in order to test the hypothesis that modeling overall sentence structure can lead to more accurate evaluation measures.", "labels": [], "entities": []}, {"text": "We conducted extensive experiments on a diverse set of standard evaluation data sets (NIST OpenMT06, 08; WMT06, 07, 08).", "labels": [], "entities": [{"text": "NIST OpenMT06, 08", "start_pos": 86, "end_pos": 103, "type": "DATASET", "confidence": 0.8299224674701691}, {"text": "WMT06", "start_pos": 105, "end_pos": 110, "type": "DATASET", "confidence": 0.5942644476890564}]}, {"text": "Our model achieves or surpasses state-of-the-art results on all test sets.", "labels": [], "entities": []}], "datasetContent": [{"text": "The goal of our experiments is to test both the accuracy and robustness of the proposed new models.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 48, "end_pos": 56, "type": "METRIC", "confidence": 0.999256432056427}]}, {"text": "We then show that modeling word swapping and rich linguistics features further improve our results.", "labels": [], "entities": [{"text": "word swapping", "start_pos": 27, "end_pos": 40, "type": "TASK", "confidence": 0.7585287392139435}]}, {"text": "To better situate our work among past research and to draw meaningful comparison, we use exactly the same standard evaluation data sets and metrics as, which is currently the stateof-the-art result for regression-based MT evaluation.", "labels": [], "entities": [{"text": "MT evaluation", "start_pos": 219, "end_pos": 232, "type": "TASK", "confidence": 0.9326658546924591}]}, {"text": "We consider four widely used MT metrics (BLEU, NIST, METEOR (v0.7), and TER) as our baselines.", "labels": [], "entities": [{"text": "MT", "start_pos": 29, "end_pos": 31, "type": "TASK", "confidence": 0.9420915246009827}, {"text": "BLEU", "start_pos": 41, "end_pos": 45, "type": "METRIC", "confidence": 0.9959635734558105}, {"text": "NIST", "start_pos": 47, "end_pos": 51, "type": "DATASET", "confidence": 0.6639966368675232}, {"text": "METEOR", "start_pos": 53, "end_pos": 59, "type": "METRIC", "confidence": 0.9878091812133789}, {"text": "TER", "start_pos": 72, "end_pos": 75, "type": "METRIC", "confidence": 0.9982814788818359}]}, {"text": "Since our models are trained to regress human evaluation scores, to make a direct comparison in the same regression setting, we also train a small linear regression model for each baseline metric in the same way as described in.", "labels": [], "entities": []}, {"text": "These regression models are strictly more powerful than the baseline metrics and show higher robustness and better correlation with human judgments.", "labels": [], "entities": []}, {"text": "We also compare our models with the state-of-the-art linear regression models reported in that combine features from multiple MT evaluation metrics (MT), as well as rich linguistic features from a textual entailment system (RTE).", "labels": [], "entities": [{"text": "MT evaluation metrics (MT)", "start_pos": 126, "end_pos": 152, "type": "TASK", "confidence": 0.7008655617634455}]}, {"text": "In all of our experiments, each reference and system translation sentence pair is tokenized using the Penn Treebank () tokenization script, and lemmatized by the Porter Stemmer.", "labels": [], "entities": [{"text": "system translation sentence", "start_pos": 46, "end_pos": 73, "type": "TASK", "confidence": 0.7404311100641886}, {"text": "Penn Treebank", "start_pos": 102, "end_pos": 115, "type": "DATASET", "confidence": 0.9868732392787933}]}, {"text": "For the overall sentence structure experiment, translations are additionally part-of-speech tagged with MXPOST tagger, and parsed with MSTParser () 7 labeled dependency parser.", "labels": [], "entities": [{"text": "MSTParser", "start_pos": 135, "end_pos": 144, "type": "DATASET", "confidence": 0.899827241897583}]}, {"text": "Statistical significance tests are performed using the paired bootstrap resampling method.", "labels": [], "entities": []}, {"text": "We divide our experiments into two sections, based on two different prediction tasks -predicting absolute scores and predicting pairwise preference.", "labels": [], "entities": [{"text": "predicting pairwise preference", "start_pos": 117, "end_pos": 147, "type": "TASK", "confidence": 0.8186757564544678}]}], "tableCaptions": [{"text": " Table 1: pFSM vs. pPDA results for the round-robin  approach on OpenMT08 data set over three languages  (A=Arabic, C=Chinese, U=Urdu). Numbers in this table  are Spearman's \u03c1 for correlation between human assess- ment scores and model predictions; tr stands for training  set, and te stands for test set. nx means the model has  x-gram block edits. jy means the model has jump distance  limit y. The Best result for each test set row is highlighted  in bold.", "labels": [], "entities": [{"text": "OpenMT08 data set", "start_pos": 65, "end_pos": 82, "type": "DATASET", "confidence": 0.9403503139813741}]}, {"text": " Table 2: Overall Comparison: Results from OpenMT08 and OpenMT06 evaluation data sets. The R (as in BLEUR)  refers to the regression model trained for each baseline metric, same as Pado et al. (2009). The first three rows are  round-robin train/test results over three languages on OpenMT08 (A=Arabic, C=Chinese, U=Urdu). The last row are  results trained on entire OpenMT08 (A+C+U) and tested on OpenMT06. Numbers in this table are Spearman's rank  correlation \u03c1 between human assessment scores and model predictions. The pPDA column describes our pPDA model  with jump distance limit 5. METR is shorthand for METEORR. +f means the model includes synonyms, paraphrase  and parsing features (cf. Section 3). Best results and scores that are not statistically significantly worse are highlighted  in bold in each row.", "labels": [], "entities": [{"text": "OpenMT06 evaluation data sets", "start_pos": 56, "end_pos": 85, "type": "DATASET", "confidence": 0.9030032902956009}, {"text": "BLEUR", "start_pos": 100, "end_pos": 105, "type": "METRIC", "confidence": 0.9973227381706238}, {"text": "OpenMT06", "start_pos": 397, "end_pos": 405, "type": "DATASET", "confidence": 0.9462857246398926}, {"text": "Spearman's rank  correlation \u03c1", "start_pos": 433, "end_pos": 463, "type": "METRIC", "confidence": 0.6952512204647064}, {"text": "METR", "start_pos": 589, "end_pos": 593, "type": "METRIC", "confidence": 0.89415043592453}, {"text": "METEORR", "start_pos": 611, "end_pos": 618, "type": "METRIC", "confidence": 0.8668904304504395}]}, {"text": " Table 3: Results for OpenMT08 with linguistic features,  using the same round robin scheme as in Table 1. Numbers  in this table are Spearman's rank correlation \u03c1 between  human assessment scores and model predictions. Best  results on each test set are highlighted in bold.", "labels": [], "entities": [{"text": "Spearman's rank correlation \u03c1", "start_pos": 134, "end_pos": 163, "type": "METRIC", "confidence": 0.677678394317627}]}, {"text": " Table 4: Pairwise preference prediction results on WMT08  test set. Each column shows a different training data set.  Numbers in this table are model's consistency with human  pairwise preference judgments. Best result on each test  set is highlighted in bold.", "labels": [], "entities": [{"text": "Pairwise preference prediction", "start_pos": 10, "end_pos": 40, "type": "TASK", "confidence": 0.7460374037424723}, {"text": "WMT08  test set", "start_pos": 52, "end_pos": 67, "type": "DATASET", "confidence": 0.9836603204409281}]}]}