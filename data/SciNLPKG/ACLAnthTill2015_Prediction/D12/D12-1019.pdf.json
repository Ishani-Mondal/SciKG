{"title": [{"text": "Spectral Dependency Parsing with Latent Variables", "labels": [], "entities": [{"text": "Spectral Dependency Parsing", "start_pos": 0, "end_pos": 27, "type": "TASK", "confidence": 0.8882473905881246}]}], "abstractContent": [{"text": "Recently there has been substantial interest in using spectral methods to learn generative sequence models like HMMs.", "labels": [], "entities": []}, {"text": "Spectral methods are attractive as they provide globally consistent estimates of the model parameters and are very fast and scalable, unlike EM methods , which can get stuck in local minima.", "labels": [], "entities": []}, {"text": "In this paper, we present a novel extension of this class of spectral methods to learn dependency tree structures.", "labels": [], "entities": []}, {"text": "We propose a simple yet powerful latent variable generative model for dependency parsing, and a spectral learning method to efficiently estimate it.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 70, "end_pos": 88, "type": "TASK", "confidence": 0.8618814647197723}]}, {"text": "As a pilot experimental evaluation, we use the spectral tree probabilities estimated by our model to re-rank the outputs of a near state-of-the-art parser.", "labels": [], "entities": []}, {"text": "Our approach gives us a moderate reduction in error of up to 4.6% over the base-line re-ranker.", "labels": [], "entities": [{"text": "error", "start_pos": 46, "end_pos": 51, "type": "METRIC", "confidence": 0.959273099899292}]}], "introductionContent": [{"text": "Markov models have been for two decades a workhorse of statistical pattern recognition with applications ranging from speech to vision to language.", "labels": [], "entities": [{"text": "statistical pattern recognition", "start_pos": 55, "end_pos": 86, "type": "TASK", "confidence": 0.8154399394989014}]}, {"text": "Adding latent variables to these models gives us additional modeling power and have shown success in applications like POS tagging, speech recognition and object recognition ().", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 119, "end_pos": 130, "type": "TASK", "confidence": 0.8335147202014923}, {"text": "speech recognition", "start_pos": 132, "end_pos": 150, "type": "TASK", "confidence": 0.8072246611118317}, {"text": "object recognition", "start_pos": 155, "end_pos": 173, "type": "TASK", "confidence": 0.8333536386489868}]}, {"text": "However, this comes at the cost that the resulting parameter estimation problem becomes non-convex and techniques like EM which are used to estimate the parameters can only lead to locally optimal solutions.", "labels": [], "entities": []}, {"text": "Recent work by has shown that globally consistent estimates of the parameters of HMMs can be found by using spectral methods, particularly by singular value decomposition (SVD) of appropriately defined linear systems.", "labels": [], "entities": []}, {"text": "They avoid the NP Hard problem of the global optimization problem of the HMM parameters), by putting restrictions on the smallest singular value of the HMM parameters.", "labels": [], "entities": []}, {"text": "The main intuition behind the model is that, although the observed data (i.e. words) seem to live in a very high dimensional space, but in reality they live in a very low dimensional space (size k \u223c 30 \u2212 50) and an appropriate eigen decomposition of the observed data will reveal the underlying low dimensional dynamics and thereby revealing the parameters of the model.", "labels": [], "entities": []}, {"text": "Besides ducking the NP hard problem, the spectral methods are very fast and scalable to train compared to EM methods.", "labels": [], "entities": []}, {"text": "In this paper we generalize the approach of to learn dependency tree structures with latent variables.", "labels": [], "entities": []}, {"text": "1 and have shown that learning PCFGs and dependency grammars respectively with latent variables can produce parsers with very good generalization performance.", "labels": [], "entities": []}, {"text": "However, both these approaches rely on EM for parameter estimation and can benefit from using spectral methods.", "labels": [], "entities": []}, {"text": "We propose a simple yet powerful latent variable generative model for use with dependency pars-ing which has one hidden node for each word in the sentence, like the one shown in and workout the details for the parameter estimation of the corresponding spectral learning model.", "labels": [], "entities": []}, {"text": "At a very high level, the parameter estimation of our model involves collecting unigram, bigram and trigram counts sensitive to the underlying dependency structure of the given sentence.", "labels": [], "entities": []}, {"text": "Recently, have also proposed a spectral method for dependency parsing, however they deal with horizontal markovization and use hidden states to model sequential dependencies within a word's sequence of children.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 51, "end_pos": 69, "type": "TASK", "confidence": 0.8281204402446747}]}, {"text": "In contrast with that, in this paper, we propose a spectral learning algorithm where latent states are not restricted to HMM-like distributions of modifier sequences fora particular head, but instead allow information to be propagated through the entire tree.", "labels": [], "entities": []}, {"text": "More recently, have proposed a spectral method for learning PCFGs.", "labels": [], "entities": []}, {"text": "Its worth noting that recent work by Parikh et al.", "labels": [], "entities": []}, {"text": "(2011) also extends to latent variable dependency trees like us but under the restrictive conditions that model parameters are trained fora specified, albeit arbitrary, tree topology.", "labels": [], "entities": []}, {"text": "In other words, all training sentences and test sentences must have identical tree topologies.", "labels": [], "entities": []}, {"text": "By doing this they allow for node-specific model parameters, but must retrain the model entirely when a different tree topology is encountered.", "labels": [], "entities": []}, {"text": "Our model on the other hand allows the flexibility and efficiency of processing sentences with a variety of tree topologies from a single training run.", "labels": [], "entities": []}, {"text": "Most of the current state-of-the-art dependency parsers are discriminative parsers () due to the flexibility of representations which can be used as features leading to better accuracies and the ease of reproducibility of results.", "labels": [], "entities": []}, {"text": "However, unlike discriminative models, generative models can exploit unlabeled data.", "labels": [], "entities": []}, {"text": "Also, as is common in statistical parsing, re-ranking the outputs of a parser leads to significant reductions in error).", "labels": [], "entities": [{"text": "statistical parsing", "start_pos": 22, "end_pos": 41, "type": "TASK", "confidence": 0.6629297435283661}, {"text": "error", "start_pos": 113, "end_pos": 118, "type": "METRIC", "confidence": 0.95319002866745}]}, {"text": "Since our spectral learning algorithm uses a gen- erative model of words given a tree structure, it can score a tree structure i.e. its probability of generation.", "labels": [], "entities": []}, {"text": "Thus, it can be used to re-rank the n-best outputs of a given parser.", "labels": [], "entities": []}, {"text": "The remainder of the paper is organized as follows.", "labels": [], "entities": []}, {"text": "In the next section we introduce the notation and give a brief overview of the spectral algorithm for learning HMMs ().", "labels": [], "entities": []}, {"text": "In Section 3 we describe our proposed model for dependency parsing in detail and workout the theory behind it.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 48, "end_pos": 66, "type": "TASK", "confidence": 0.8909288942813873}]}, {"text": "Section 4 provides experimental evaluation of our model on Penn Treebank data.", "labels": [], "entities": [{"text": "Penn Treebank data", "start_pos": 59, "end_pos": 77, "type": "DATASET", "confidence": 0.9951058427492777}]}, {"text": "We conclude with a brief summary and future avenues for research.", "labels": [], "entities": []}], "datasetContent": [{"text": "Since our algorithm can score any given tree structure by computing its marginal probability, a natural way to benchmark our parser is to generate nbest dependency trees using some standard parser and then use our algorithm to re-rank the candidate dependency trees, e.g. using the log spectral probability as described in Algorithm 1 as a feature in a discriminative re-ranker.", "labels": [], "entities": []}, {"text": "Our base parser was the discriminatively trained MSTParser), which implements both first and second order parsers and is trained using MIRA) and used the standard baseline features as described in.", "labels": [], "entities": [{"text": "MSTParser", "start_pos": 49, "end_pos": 58, "type": "DATASET", "confidence": 0.8752284049987793}, {"text": "MIRA", "start_pos": 135, "end_pos": 139, "type": "METRIC", "confidence": 0.9808636903762817}]}, {"text": "We tested our methods on the English Penn Treebank (.", "labels": [], "entities": [{"text": "English Penn Treebank", "start_pos": 29, "end_pos": 50, "type": "DATASET", "confidence": 0.9316357572873434}]}, {"text": "We use the standard splits of Penn Treebank; i.e., we used sections 2-21 for training, section 22 for development and section 23 for testing.", "labels": [], "entities": [{"text": "Penn Treebank", "start_pos": 30, "end_pos": 43, "type": "DATASET", "confidence": 0.9960150122642517}]}, {"text": "We used the PennConverter 7 tool to convert Penn Treebank from constituent to dependency format.", "labels": [], "entities": [{"text": "PennConverter 7", "start_pos": 12, "end_pos": 27, "type": "DATASET", "confidence": 0.9385916888713837}, {"text": "Penn Treebank", "start_pos": 44, "end_pos": 57, "type": "DATASET", "confidence": 0.9834558665752411}]}, {"text": "Following, we used the POS tagger by Ratnaparkhi (1996) trained on the full training data to provide POS tags for development and test sets and used 10-way jackknifing to generate tags for the training set.", "labels": [], "entities": [{"text": "POS tagger", "start_pos": 23, "end_pos": 33, "type": "TASK", "confidence": 0.6240117847919464}]}, {"text": "As is common practice we stripped our sentences of all the punctuation.", "labels": [], "entities": []}, {"text": "We evaluated our approach on sentences of all lengths.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: (Unlabeled) Dependency Parse re-ranking re- sults for English test set (Section 23). Note: 1). RR =  Re-ranking 2). Accuracy is the number of words which  correctly identified their parent and Complete is the num- ber of sentences for which the entire dependency tree was  correct. 3). Base. Features are the two re-ranking fea- tures described in Section 4.3. 4). log\u02c6plog\u02c6 log\u02c6p is the spectral log  probability feature.", "labels": [], "entities": [{"text": "English test set", "start_pos": 64, "end_pos": 80, "type": "DATASET", "confidence": 0.7988129258155823}, {"text": "RR", "start_pos": 105, "end_pos": 107, "type": "METRIC", "confidence": 0.9743112921714783}, {"text": "Accuracy", "start_pos": 126, "end_pos": 134, "type": "METRIC", "confidence": 0.9968687891960144}]}]}