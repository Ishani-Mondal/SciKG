{"title": [{"text": "Unsupervised PCFG Induction for Grounded Language Learning with Highly Ambiguous Supervision", "labels": [], "entities": [{"text": "Unsupervised PCFG Induction", "start_pos": 0, "end_pos": 27, "type": "TASK", "confidence": 0.4562506675720215}]}], "abstractContent": [{"text": "\"Grounded\" language learning employs training data in the form of sentences paired with relevant but ambiguous perceptual contexts.", "labels": [], "entities": []}, {"text": "(2011) introduced an approach to grounded language learning based on unsupervised PCFG induction.", "labels": [], "entities": []}, {"text": "Their approach works well when each sentence potentially refers to one of a small set of possible meanings, such as in the sportscasting task.", "labels": [], "entities": []}, {"text": "However, it does not scale to problems with a large set of potential meanings for each sentence, such as the navigation instruction following task studied by Chen and Mooney (2011).", "labels": [], "entities": [{"text": "navigation instruction following task", "start_pos": 109, "end_pos": 146, "type": "TASK", "confidence": 0.9050999879837036}]}, {"text": "This paper presents an enhancement of the PCFG approach that scales to such problems with highly-ambiguous supervision.", "labels": [], "entities": []}, {"text": "Experimental results on the navigation task demonstrates the effectiveness of our approach.", "labels": [], "entities": [{"text": "navigation task", "start_pos": 28, "end_pos": 43, "type": "TASK", "confidence": 0.932770311832428}]}], "introductionContent": [{"text": "The ultimate goal of \"grounded\" language learning is to develop computational systems that can acquire language more like a human child.", "labels": [], "entities": []}, {"text": "Given only supervision in the form of sentences paired with relevant but ambiguous perceptual contexts, a system should learn to interpret and/or generate language describing situations and events in the world.", "labels": [], "entities": []}, {"text": "For example, systems have learned to commentate simulated robot soccer games by learning from sample sportscasts), or understand navigation instructions by learning from action traces produced when following the directions ().", "labels": [], "entities": [{"text": "understand navigation instructions", "start_pos": 118, "end_pos": 152, "type": "TASK", "confidence": 0.642155259847641}]}, {"text": "recently introduced an approach to grounded language learning using unsupervised induction of probabilistic context free grammars (PCFGs) to learn from ambiguous contextual supervision.", "labels": [], "entities": []}, {"text": "Their approach first constructs a large set of production rules from sentences paired with descriptions of their ambiguous context, and then trains the parameters of this grammar using EM.", "labels": [], "entities": []}, {"text": "Parsing a novel sentence with this grammar gives a parse tree which contains the formal meaning representation (MR) for this sentence.", "labels": [], "entities": [{"text": "formal meaning representation (MR)", "start_pos": 81, "end_pos": 115, "type": "METRIC", "confidence": 0.6730981568495432}]}, {"text": "This approach works quite well on the sportscasting task originally introduced by.", "labels": [], "entities": []}, {"text": "In this task, each sentence in a natural-language commentary describing activity in a simulated robot soccer game is paired with the small set of actions observed within the past 5 seconds, one of which is usually described by the sentence.", "labels": [], "entities": []}, {"text": "Even with this low level of ambiguity in a constrained domain, their method constructs a PCFG with about 33,000 productions.", "labels": [], "entities": []}, {"text": "More fundamentally, their approach is restricted to a finite set of potential meaning representations, and the grammar size grows at least linearly with the number of possible MRs, which in turn is inevitably exponential in the number of objects and actions in the domain.", "labels": [], "entities": []}, {"text": "The navigation task studied by provides much more ambiguous supervision.", "labels": [], "entities": [{"text": "navigation task", "start_pos": 4, "end_pos": 19, "type": "TASK", "confidence": 0.9231882691383362}]}, {"text": "In this task, each instructional sentence is paired with a formal landmarks plan (represented as a large graph) that includes a full description of the observed actions and world-states that result when someone follows this instruction.", "labels": [], "entities": []}, {"text": "An instruction generally refers to a subgraph of this large graph.", "labels": [], "entities": []}, {"text": "Therefore, there area combinatorial number of possible meanings to which a given sentence can refer.", "labels": [], "entities": []}, {"text": "Chen and Mooney (2011) circumvent this combinatorial problem by never explicitly enumerating the exponential number of potential meanings for each sentence.", "labels": [], "entities": []}, {"text": "Their system first induces a semantic lexicon that maps words and short phrases to formal representations of actions and objects in the world.", "labels": [], "entities": []}, {"text": "This lexicon is learned by finding words and phrases whose occurrence highly correlates with specific observed actions and objects in the simulated environment when executing the corresponding instruction.", "labels": [], "entities": []}, {"text": "This learned lexicon is then used to directly infer a formal MR for observed instructional sentences using a greedy covering algorithm.", "labels": [], "entities": []}, {"text": "These inferred MRs are then used to train a supervised semantic parser capable of mapping novel sentences to their formal meanings.", "labels": [], "entities": []}, {"text": "We present a novel enhancement of B\u00f6rschinger et al.'s PCFG approach that uses Chen and Mooney's lexicon learner to avoid a combinatorial explosion in the number of productions.", "labels": [], "entities": []}, {"text": "The learned lexicon is first used to build a hierarchy of semantic lexemes (i.e. lexicon entries) called the Lexeme Hierarchy Graph (LHG) for each ambiguous landmarks plan in the training data.", "labels": [], "entities": [{"text": "Lexeme Hierarchy Graph (LHG)", "start_pos": 109, "end_pos": 137, "type": "DATASET", "confidence": 0.7716551820437113}]}, {"text": "The intuition behind utilizing an LHG is that the MR for each lexeme constitutes a semantic concept that corresponds to some naturallanguage (NL) word or phrase.", "labels": [], "entities": []}, {"text": "Therefore, the LHG represents how complex semantic concepts are composed of simpler semantic concepts and ultimately connected to NL words and phrases.", "labels": [], "entities": []}, {"text": "B\u00f6rschinger et al.'s approach instead produces NL groundings at the level of atomic MR constituents, which causes an explosion in the number of PCFG productions for complex MR languages.", "labels": [], "entities": []}, {"text": "We estimated that B\u00f6rschinger et al.'s approach would require more than 20!", "labels": [], "entities": []}, {"text": "(> 10 18 ) productions for our navigation problem.", "labels": [], "entities": [{"text": "navigation problem", "start_pos": 31, "end_pos": 49, "type": "TASK", "confidence": 0.924280047416687}]}, {"text": "On the other hand, our method, which uses correspondences from the LHG at the semantic concept level, constructs a more focused PCFG of tractable size.", "labels": [], "entities": []}, {"text": "It then extracts the MR fora novel sentence from the most-probable parse tree for the resulting PCFG.", "labels": [], "entities": [{"text": "MR", "start_pos": 21, "end_pos": 23, "type": "METRIC", "confidence": 0.8730561137199402}, {"text": "PCFG", "start_pos": 96, "end_pos": 100, "type": "DATASET", "confidence": 0.9410601258277893}]}, {"text": "Our approach can produce a large, combinatorial number of different MRs fora wide range of novel sentences by composing relevant MR components from the resulting parse tree, whereas B\u00f6rschinger et al.'s approach is only able to output MRs that are explicitly included as a nonterminals in the original learned PCFG.", "labels": [], "entities": []}, {"text": "The remainder of the paper is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 reviews B\u00f6rschinger et al.'s PCFG approach as well as the navigation task and data.", "labels": [], "entities": [{"text": "navigation task", "start_pos": 68, "end_pos": 83, "type": "TASK", "confidence": 0.8967244923114777}]}, {"text": "Section 3 describes our enhanced PCFG approach and Section 4 presents an experimental evaluation of it.", "labels": [], "entities": []}, {"text": "Then, Section 5 discusses the unique aspects of our approach and Section 6 describes additional related work.", "labels": [], "entities": []}, {"text": "Finally, Section 7 presents future research directions and Section 8 gives our conclusions.", "labels": [], "entities": []}], "datasetContent": [{"text": "We employ the task and data introduced by whose goal is to interpret and follow NL navigation instructions in a virtual world.", "labels": [], "entities": [{"text": "follow NL navigation", "start_pos": 73, "end_pos": 93, "type": "TASK", "confidence": 0.5493455628554026}]}, {"text": "shows a sample execution path in a particular virtual world.", "labels": [], "entities": []}, {"text": "The challenge is learning to perform this task by simply observing humans following instructions.", "labels": [], "entities": []}, {"text": "Formally, given training data of the form {(e 1 , a 1 , w 1 ), . .", "labels": [], "entities": []}, {"text": ", (e n , an , w n )}, where e i is an NL instruction, a i is an observed action sequence, and w i is the current world state (patterns of floors and walls, positions of any objects, etc.), we want to produce the correct actions a j fora novel (e j , w j ).", "labels": [], "entities": []}, {"text": "In order to learn, their system infers the intended formal plan pi (the MR fora sentence) which produced the action sequence a i from the instruction e i . However, there is a large space of possible plans for any given action sequence.", "labels": [], "entities": []}, {"text": "Chen and Mooney first construct a formal landmarks plan, c i , for each a i , which is a graph representing the context of every action and the world-state encountered during the execution of the sequence.", "labels": [], "entities": []}, {"text": "The correct plan MR, pi , is assumed to be a subgraph of c i , and this causes a combinatorial matching problem between e i and c i in order to learn the correct meaning of e i among all the possible subgraphs of c i . The landmarks and correct plans fora sample instruction are shown in, illustrating the complexity of the MRs.", "labels": [], "entities": []}, {"text": "Instead of directly solving the combinatorial correspondence problem, they first learn a semantic lex- icon that maps words and short phrases to small subgraphs representing their inferred meanings from the (e i , c i ) pairs.", "labels": [], "entities": []}, {"text": "The lexicon is learned by evaluating pairs of n-grams, w j , and MR graphs, m j , and scoring them based on how much more likely m j is a subgraph of the context c i when w occurs in the corresponding instruction e i . This process is similar to other \"cross-situational\" approaches to learning word meanings.", "labels": [], "entities": []}, {"text": "Then, a plan refinement step estimates pi from c i by greedily selecting high-scoring lexemes of the form (w j , m j ) whose words and phrases (w j ) cover the instruction e i and introduce components (m j ) from the landmarks plan c i . The refined plans are used to construct supervised training data (e i , pi ) fora supervised semantic-parser learner.", "labels": [], "entities": []}, {"text": "The trained semantic parser can parse a novel instruction into a formal plan, which is finally executed for end-to-end evaluation.", "labels": [], "entities": []}, {"text": "As this figure indicates, our new PCFG method replaces the plan refinement and semantic parser components in their system with a unified model that both disambiguates the training data and learns a semantic parser.", "labels": [], "entities": []}, {"text": "We use the landmarks plans and the learned lexicon produced by as inputs to our system.", "labels": [], "entities": []}, {"text": "(2011), our approach learns a semantic parser directly from ambiguous supervision, specifically NL instructions paired with their complete landmarks plans as context.", "labels": [], "entities": []}, {"text": "Our method incorporates the semantic lexemes as building blocks to find correspondences between NL words and semantic concepts represented by the lexeme MRs, instead of building connections between NL words and every possible MR constituent as in B\u00f6rschinger et al.'s approach.", "labels": [], "entities": []}, {"text": "Particularly, we utilize the hierarchical subgraph relationships between the MRs in the learned semantic lexicon to produce a smaller, more focused set of PCFG rules.", "labels": [], "entities": []}, {"text": "The intuition behind our approach is analogous to the hierarchical relations between nonterminals in syntactic parsing, where higher-level categories such as S, VP, or NP are further divided into smaller categories such as V, N, or Det, thereby forming a hierarchical structure.", "labels": [], "entities": [{"text": "syntactic parsing", "start_pos": 101, "end_pos": 118, "type": "TASK", "confidence": 0.7242557853460312}]}, {"text": "Inspired by this idea, we introduce a directed acyclic graph called the Lexeme Hierarchy Graph (LHG) which represents the hierarchical relationships between lexeme MRs.", "labels": [], "entities": [{"text": "Lexeme Hierarchy Graph (LHG)", "start_pos": 72, "end_pos": 100, "type": "DATASET", "confidence": 0.8143742084503174}]}, {"text": "Since complex lexeme MRs represent complicated semantic concepts while simple MRs represent simple concepts, it is natural to construct a hierarchy amongst them.", "labels": [], "entities": []}, {"text": "The LHGs for all of the training examples are used to construct production rules for the PCFG, which are then parametrized using EM.", "labels": [], "entities": [{"text": "PCFG", "start_pos": 89, "end_pos": 93, "type": "DATASET", "confidence": 0.9027869701385498}]}, {"text": "Finally, a novel sentence is semantically parsed by computing its mostprobable parse using the trained PCFG, and then its MR is extracted from the resulting parse tree.", "labels": [], "entities": [{"text": "PCFG", "start_pos": 103, "end_pos": 107, "type": "DATASET", "confidence": 0.88885498046875}]}, {"text": "For evaluation, we used the same data and methodology as.", "labels": [], "entities": []}, {"text": "Please see their paper for more details.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Test accuracy for semantic parsing.  ' * ' denotes difference is statistically significant.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 15, "end_pos": 23, "type": "METRIC", "confidence": 0.9894329905509949}, {"text": "semantic parsing", "start_pos": 28, "end_pos": 44, "type": "TASK", "confidence": 0.8421993553638458}]}]}