{"title": [{"text": "Parser Showdown at the Wall Street Corral: An Empirical Investigation of Error Types in Parser Output", "labels": [], "entities": [{"text": "Wall Street Corral", "start_pos": 23, "end_pos": 41, "type": "DATASET", "confidence": 0.7862782875696818}]}], "abstractContent": [{"text": "Constituency parser performance is primarily interpreted through a single metric, F-score on WSJ section 23, that conveys no linguistic information regarding the remaining errors.", "labels": [], "entities": [{"text": "Constituency parser", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.7673933804035187}, {"text": "F-score", "start_pos": 82, "end_pos": 89, "type": "METRIC", "confidence": 0.9928721785545349}, {"text": "WSJ section 23", "start_pos": 93, "end_pos": 107, "type": "DATASET", "confidence": 0.8892127275466919}]}, {"text": "We classify errors within a set of linguistically meaningful types using tree transformations that repair groups of errors together.", "labels": [], "entities": []}, {"text": "We use this analysis to answer a range of questions about parser behaviour, including what linguistic constructions are difficult for state-of-the-art parsers, what types of errors are being resolved by rerankers, and what types are introduced when parsing out-of-domain text.", "labels": [], "entities": [{"text": "parsing out-of-domain text", "start_pos": 249, "end_pos": 275, "type": "TASK", "confidence": 0.8647694985071818}]}], "introductionContent": [{"text": "Parsing has been a major area of research within computational linguistics for decades, and constituent parser F-scores on WSJ section 23 have exceeded 90% (, and 92% when using self-training and reranking).", "labels": [], "entities": [{"text": "WSJ section 23", "start_pos": 123, "end_pos": 137, "type": "DATASET", "confidence": 0.8744287689526876}]}, {"text": "While these results give a useful measure of overall performance, they provide no information about the nature, or relative importance, of the remaining errors.", "labels": [], "entities": []}, {"text": "Broad investigations of parser errors beyond the PARSEVAL metric ( have either focused on specific parsers, e.g., or have involved conversion to dependencies.", "labels": [], "entities": [{"text": "PARSEVAL metric", "start_pos": 49, "end_pos": 64, "type": "DATASET", "confidence": 0.5491870790719986}]}, {"text": "In all of these cases, the analysis has not taken into consideration how a set of errors can have a common cause, e.g. a single mis-attachment can create multiple node errors.", "labels": [], "entities": []}, {"text": "We propose anew method of error classification using tree transformations.", "labels": [], "entities": [{"text": "error classification", "start_pos": 26, "end_pos": 46, "type": "TASK", "confidence": 0.6763896644115448}]}, {"text": "Errors in the parse tree are repaired using subtree movement, node creation, and node deletion.", "labels": [], "entities": []}, {"text": "Each step in the process is then associated with a linguistically meaningful error type, based on factors such as the node that is moved, its siblings, and parents.", "labels": [], "entities": []}, {"text": "Using our method we analyse the output of thirteen constituency parsers on newswire.", "labels": [], "entities": []}, {"text": "Some of the frequent error types that we identify are widely recognised as challenging, such as prepositional phrase (PP) attachment.", "labels": [], "entities": [{"text": "prepositional phrase (PP) attachment", "start_pos": 96, "end_pos": 132, "type": "TASK", "confidence": 0.6141853928565979}]}, {"text": "However, other significant types have not received as much attention, such as clause attachment and modifier attachment.", "labels": [], "entities": [{"text": "clause attachment", "start_pos": 78, "end_pos": 95, "type": "TASK", "confidence": 0.7629152238368988}, {"text": "modifier attachment", "start_pos": 100, "end_pos": 119, "type": "TASK", "confidence": 0.7086862474679947}]}, {"text": "Our method also enables us to investigate where reranking and self-training improve parsing.", "labels": [], "entities": [{"text": "parsing", "start_pos": 84, "end_pos": 91, "type": "TASK", "confidence": 0.9617960453033447}]}, {"text": "Previously, these developments were analysed only in terms of their impact on F-score.", "labels": [], "entities": [{"text": "F-score", "start_pos": 78, "end_pos": 85, "type": "METRIC", "confidence": 0.9790537357330322}]}, {"text": "Similarly, the challenge of out-of-domain parsing has only been expressed in terms of this single objective.", "labels": [], "entities": []}, {"text": "We are able to decompose the drop in performance and show that a disproportionate number of the extra errors are due to coordination and clause attachment.", "labels": [], "entities": [{"text": "clause attachment", "start_pos": 137, "end_pos": 154, "type": "TASK", "confidence": 0.7992498278617859}]}, {"text": "This work presents a comprehensive investigation of parser behaviour in terms of linguistically meaningful errors.", "labels": [], "entities": []}, {"text": "By applying our method to multiple parsers and domains we are able to answer questions about parser behaviour that were previously only approachable through approximate measures, such as counts of node errors.", "labels": [], "entities": []}, {"text": "We show which errors have been reduced over the past fifteen years of parsing research; where rerankers are making their gains and where they are not exploiting the full potential of kbest lists; and what types of errors arise when moving out-of-domain.", "labels": [], "entities": [{"text": "parsing", "start_pos": 70, "end_pos": 77, "type": "TASK", "confidence": 0.9783869981765747}]}, {"text": "We have released our system 1 to enable future work to apply our methodology.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: PARSEVAL results on WSJ section 23 for the  parsers we consider. The columns are F-score, precision,  recall, exact sentence match, and speed (sents/sec). Cov- erage was left out as it was above 99.8% for all parsers.", "labels": [], "entities": [{"text": "PARSEVAL", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.7628251314163208}, {"text": "WSJ section 23", "start_pos": 30, "end_pos": 44, "type": "DATASET", "confidence": 0.8512250383694967}, {"text": "F-score", "start_pos": 91, "end_pos": 98, "type": "METRIC", "confidence": 0.9953734278678894}, {"text": "precision", "start_pos": 100, "end_pos": 109, "type": "METRIC", "confidence": 0.999377429485321}, {"text": "recall", "start_pos": 112, "end_pos": 118, "type": "METRIC", "confidence": 0.9982922673225403}, {"text": "exact sentence match", "start_pos": 120, "end_pos": 140, "type": "METRIC", "confidence": 0.8626411557197571}, {"text": "speed", "start_pos": 146, "end_pos": 151, "type": "METRIC", "confidence": 0.9977999329566956}, {"text": "Cov- erage", "start_pos": 165, "end_pos": 175, "type": "METRIC", "confidence": 0.8481008211771647}]}, {"text": " Table 2: Average number of bracket errors per sentence due to the top ten error types. For instance, Stanford-U  produces output that has, on average, 1.12 bracket errors per sentence that are due to PP attachment. The scale for  each column is indicated by the Best and Worst values.", "labels": [], "entities": [{"text": "Average number of bracket errors", "start_pos": 10, "end_pos": 42, "type": "METRIC", "confidence": 0.7949623346328736}]}, {"text": " Table 3: Breakdown of errors on section 23 for the Char- niak parser with self-trained model and reranker. Errors  are sorted by the number of times they occur. Ratio is the  average number of node errors caused by each error we  identify (i.e. Nodes Involved / Occurrences).", "labels": [], "entities": []}, {"text": " Table 4: Average number of bracket errors per sentence for a range of K-best list lengths using the Charniak parser  with reranking and the self-trained model. The oracle results are determined by taking the parse in each K-best list  with the highest F-score.", "labels": [], "entities": [{"text": "Average number of bracket errors", "start_pos": 10, "end_pos": 42, "type": "METRIC", "confidence": 0.7873988389968872}, {"text": "F-score", "start_pos": 253, "end_pos": 260, "type": "METRIC", "confidence": 0.9925872683525085}]}, {"text": " Table 5: Average number of node errors per word for a range of domains using the Charniak parser with reranking and  the self-trained model. We use per word error rates here rather than per sentence as there is great variation in average  sentence length across the domains, skewing the per sentence results.", "labels": [], "entities": [{"text": "Average number of node errors", "start_pos": 10, "end_pos": 39, "type": "METRIC", "confidence": 0.8521694779396057}]}]}