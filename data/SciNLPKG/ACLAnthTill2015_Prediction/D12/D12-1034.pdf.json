{"title": [{"text": "Why Question Answering using Sentiment Analysis and Word Classes", "labels": [], "entities": [{"text": "Why Question Answering", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.6106789708137512}, {"text": "Sentiment Analysis", "start_pos": 29, "end_pos": 47, "type": "TASK", "confidence": 0.9151495397090912}]}], "abstractContent": [{"text": "In this paper we explore the utility of sentiment analysis and semantic word classes for improving why-question answering on a large-scale web corpus.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 40, "end_pos": 58, "type": "TASK", "confidence": 0.9610036909580231}, {"text": "why-question answering", "start_pos": 99, "end_pos": 121, "type": "TASK", "confidence": 0.8743914663791656}]}, {"text": "Our work is motivated by the observation that a why-question and its answer often follow the pattern that if something undesirable happens, the reason is also often something undesirable, and if something desirable happens, the reason is also often something desirable.", "labels": [], "entities": []}, {"text": "To the best of our knowledge, this is the first work that introduces sentiment analysis to non-factoid question answering.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 69, "end_pos": 87, "type": "TASK", "confidence": 0.9710672497749329}, {"text": "question answering", "start_pos": 103, "end_pos": 121, "type": "TASK", "confidence": 0.695407047867775}]}, {"text": "We combine this simple idea with semantic word classes for ranking answers to why-questions and show that on a set of 850 why-questions our method gains 15.2% improvement in precision at the top-1 answer over a baseline state-of-the-art QA system that achieved the best performance in a shared task of Japanese non-factoid QA in NTCIR-6.", "labels": [], "entities": [{"text": "precision", "start_pos": 174, "end_pos": 183, "type": "METRIC", "confidence": 0.9992378950119019}, {"text": "NTCIR-6", "start_pos": 329, "end_pos": 336, "type": "DATASET", "confidence": 0.9122633337974548}]}], "introductionContent": [{"text": "Question Answering (QA) research for factoid questions has recently achieved great success as demonstrated by IBM's Watson at Jeopardy: its accuracy has been reported to be around 85% on factoid questions.", "labels": [], "entities": [{"text": "Question Answering (QA)", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.8614404618740081}, {"text": "Watson at Jeopardy", "start_pos": 116, "end_pos": 134, "type": "DATASET", "confidence": 0.6161217093467712}, {"text": "accuracy", "start_pos": 140, "end_pos": 148, "type": "METRIC", "confidence": 0.9992213249206543}]}, {"text": "Although recent shared QA tasks have stimulated the research community to move beyond factoid QA, comparatively little attention has been paid to QA for non-factoid questions such as why questions and how to questions, and the performance of the state-of-art nonfactoid QA systems reported in the literature () remains considerably lower than that of factoid QA (i.e., 34% in MRR at top-150 results on why-questions ().", "labels": [], "entities": []}, {"text": "In this paper we explore the utility of sentiment analysis () and semantic word classes for improving why-question answering (why-QA) on a largescale web corpus.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 40, "end_pos": 58, "type": "TASK", "confidence": 0.9508165419101715}, {"text": "why-question answering", "start_pos": 102, "end_pos": 124, "type": "TASK", "confidence": 0.7686129808425903}]}, {"text": "The inspiration behind this work is the observation that why-questions and their answers often have the following tendency: \u2022 if something undesirable happens, the reason is often also something undesirable, and \u2022 if something desirable happens, its reason is often also desirable.", "labels": [], "entities": []}, {"text": "Consider the following question Q1, and its answer candidates A1-1 and A1-2.", "labels": [], "entities": [{"text": "A1-1", "start_pos": 62, "end_pos": 66, "type": "METRIC", "confidence": 0.9744782447814941}, {"text": "A1-2", "start_pos": 71, "end_pos": 75, "type": "METRIC", "confidence": 0.8100209832191467}]}, {"text": "\u2022 Q1: Why does cancer occur?", "labels": [], "entities": []}, {"text": "\u2022 A1-1: Carcinogens such as nitrosamine and benzopyrene may increase the risk of cancer by altering DNA in cells.", "labels": [], "entities": [{"text": "A1-1", "start_pos": 2, "end_pos": 6, "type": "METRIC", "confidence": 0.9710672497749329}]}, {"text": "\u2022 A1-2: Maintaining a healthy weight may lower the risk of various types of cancer.", "labels": [], "entities": [{"text": "A1-2", "start_pos": 2, "end_pos": 6, "type": "METRIC", "confidence": 0.993695080280304}]}, {"text": "Here A1-1 describes an undesirable event related to cancer, while A1-2 suggests a desirable action for its prevention.", "labels": [], "entities": [{"text": "A1-1", "start_pos": 5, "end_pos": 9, "type": "METRIC", "confidence": 0.9865559339523315}]}, {"text": "Our hypothesis suggests that A1-1 is more appropriate for answering Q1.", "labels": [], "entities": [{"text": "A1-1", "start_pos": 29, "end_pos": 33, "type": "METRIC", "confidence": 0.9702076315879822}, {"text": "answering Q1", "start_pos": 58, "end_pos": 70, "type": "TASK", "confidence": 0.7583889067173004}]}, {"text": "If this hypothesis holds, we can obtain a significant improvement in performance on why-QA tasks by exploiting the sentiment orientation 1 of expressions obtainable by automatic sentiment analysis of questions and answers.", "labels": [], "entities": [{"text": "sentiment analysis of questions and answers", "start_pos": 178, "end_pos": 221, "type": "TASK", "confidence": 0.821292425195376}]}, {"text": "A second observation motivating this work is that there are often significant associations between the lexico-semantic classes of words in a question and those in its answer sentence.", "labels": [], "entities": []}, {"text": "For instance, questions concerning diseases like Q1 often have answers that include references to specific semantic word classes such as chemicals (like A1-1), viruses, body parts, and soon.", "labels": [], "entities": []}, {"text": "Capturing such statistical correlations between diseases and harmful substances may lead to higher why-QA performance.", "labels": [], "entities": []}, {"text": "For this purpose we use classes of semantically similar words that were automatically acquired from a large web corpus using an EM-based clustering method.", "labels": [], "entities": []}, {"text": "Another issue is that simply introducing the sentiment orientation of words or phrases in question and answer sentences in a naive way is insufficient, since answer candidate sentences may contain multiple sentiment expressions with different polarities in answer candidates (i.e., about 33% of correct answers had such multiple sentiment expressions with different polarities in our test set).", "labels": [], "entities": []}, {"text": "For example, if A1-2 contained a second sentiment expression with negative polarity like the example below, \"Trusting a specific food is not effective for preventing cancer, but maintaining a healthy weight may help lower the risk of various types of cancer.\" both A1-1 and A1-2 would contain sentiment expressions with the same polarity as that of Q1.", "labels": [], "entities": []}, {"text": "Thus, it is difficult to expect that the sentiment orientation alone will work well for recognizing A1-1 as a correct answer to Q1.", "labels": [], "entities": []}, {"text": "To address this problem, we consider the combination of sentiment polarity and the contents of sentiment expressions associated with the polarity in questions and their answer candidates as well.", "labels": [], "entities": []}, {"text": "To deal with the data sparseness problem arising in using the content of sentiment expressions, we developed a feature set that combines the polarity and the semantic word classes effectively.", "labels": [], "entities": []}, {"text": "We exploit these two main ideas (concerned with the sentiment orientation and the semantic classes described so far) for training a supervised classifier to rank answer candidates to why-questions.", "labels": [], "entities": [{"text": "sentiment orientation", "start_pos": 52, "end_pos": 73, "type": "TASK", "confidence": 0.8702152371406555}]}, {"text": "Through a series of experiments on 850 Japanese why-questions, we showed that the proposed semantic features were effective in identifying correct answers, and our proposed method obtained more than 15% improvement in precision of its top answer (P@1) over our baseline, which achieved the best performance in the non-factoid QA task in NTCIR-6 ().", "labels": [], "entities": [{"text": "precision", "start_pos": 218, "end_pos": 227, "type": "METRIC", "confidence": 0.9986183643341064}, {"text": "P@1)", "start_pos": 247, "end_pos": 251, "type": "METRIC", "confidence": 0.8243654370307922}, {"text": "NTCIR-6", "start_pos": 337, "end_pos": 344, "type": "DATASET", "confidence": 0.9159209728240967}]}, {"text": "We also show that our method can potentially perform with high precision (64.8% in P@1) when answer candidates containing at least one correct answer are given to our re-ranker.", "labels": [], "entities": [{"text": "precision", "start_pos": 63, "end_pos": 72, "type": "METRIC", "confidence": 0.9985319375991821}, {"text": "P@1)", "start_pos": 83, "end_pos": 87, "type": "METRIC", "confidence": 0.8267832845449448}]}], "datasetContent": [{"text": "We use TinySVM 11 with a linear kernel for training our re-ranker.", "labels": [], "entities": [{"text": "TinySVM 11", "start_pos": 7, "end_pos": 17, "type": "DATASET", "confidence": 0.9000838994979858}]}, {"text": "Evaluation was done by P@1 (Precision of the top answer) and MAP (Mean Average Precision).", "labels": [], "entities": [{"text": "P@1", "start_pos": 23, "end_pos": 26, "type": "METRIC", "confidence": 0.9634100397427877}, {"text": "Precision", "start_pos": 28, "end_pos": 37, "type": "METRIC", "confidence": 0.7957358360290527}, {"text": "MAP", "start_pos": 61, "end_pos": 64, "type": "METRIC", "confidence": 0.9975136518478394}, {"text": "Mean Average Precision)", "start_pos": 66, "end_pos": 89, "type": "METRIC", "confidence": 0.9373445510864258}]}, {"text": "P@1 measures how many questions have a correct top answer candidate.", "labels": [], "entities": [{"text": "P@1", "start_pos": 0, "end_pos": 3, "type": "METRIC", "confidence": 0.9548112352689108}]}, {"text": "MAP, widely used in evaluation of IR systems, measures the overall quality of the top-n answer candidates (n=20 in this experiment) using the formula: Here Q is a set of why-questions, A q is a set of correct answers to why-question q \u2208 Q, P rec(k) is the precision at cut-off kin the top-n answer candidates, rel(k) is an indicator, 1 if the item at rank k is a correct answer in A q , 0 otherwise.", "labels": [], "entities": [{"text": "IR", "start_pos": 34, "end_pos": 36, "type": "TASK", "confidence": 0.9325243830680847}, {"text": "precision", "start_pos": 256, "end_pos": 265, "type": "METRIC", "confidence": 0.9981624484062195}]}, {"text": "We evaluated all systems using 10-fold cross validation in two ways.", "labels": [], "entities": []}, {"text": "In the first setting we performed 10-fold cross validation on Set1.", "labels": [], "entities": []}, {"text": "Set1 con-11 http://chasen.org/\u223ctaku/software/TinySVM/ sists of 10,000 question-answer pairs (500 questions with their 20 answer candidates), and was partitioned into 10 subsamples such that the questions in one subsample do not overlap with those of the other subsamples.", "labels": [], "entities": []}, {"text": "9 subsamples (9,000 questionanswer pairs) were used as training data and the remaining subsample (1,000 question-answer pairs) was retained as test data.", "labels": [], "entities": []}, {"text": "This experiment is called CV(Set1).", "labels": [], "entities": []}, {"text": "It shows the effect of answer re-ranking when evaluating our proposed method with training data built with real world why-questions alone.", "labels": [], "entities": [{"text": "answer re-ranking", "start_pos": 23, "end_pos": 40, "type": "TASK", "confidence": 0.8495851755142212}]}, {"text": "In the second setting, we used the same 10 subsamples of Set1 as in CV(Set1) and exploited Set2 (composed of 7,000 question-answer pairs) as additional training data for 10-fold cross validation.", "labels": [], "entities": [{"text": "cross validation", "start_pos": 178, "end_pos": 194, "type": "TASK", "confidence": 0.6252670884132385}]}, {"text": "As a result, in each fold 16,000 question-answer pairs (9,000 from Set1 and 7,000 from Set2) were used as training data for re-rankers, and all systems were evaluated on the remaining 1,000 questionanswer pair subsample from Set1.", "labels": [], "entities": []}, {"text": "We call this setting CV(Set1+Set2).", "labels": [], "entities": []}, {"text": "It verifies whether training data that does not necessarily reflect a real-world distribution of why-questions can improve why-QA performance on real-world questions.", "labels": [], "entities": []}, {"text": "shows the evaluation results of six different systems.", "labels": [], "entities": []}, {"text": "For each system, we represent the performance in P@1 and MAP.", "labels": [], "entities": [{"text": "MAP", "start_pos": 57, "end_pos": 60, "type": "METRIC", "confidence": 0.874675989151001}]}, {"text": "B-QA is a system of our answer retrieval and the other five re-rank top-20 answer candidates using their own re-ranker.", "labels": [], "entities": [{"text": "B-QA", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.9425020217895508}]}, {"text": "B-QA: our answer retrieval system, our implementation of.", "labels": [], "entities": [{"text": "B-QA", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.9063528180122375}, {"text": "answer retrieval", "start_pos": 10, "end_pos": 26, "type": "TASK", "confidence": 0.881727010011673}]}, {"text": "B-Ranker: a system that has a re-ranker trained with morphological and syntactic analysis (MSA) features alone.", "labels": [], "entities": [{"text": "B-Ranker", "start_pos": 0, "end_pos": 8, "type": "METRIC", "confidence": 0.9105502367019653}]}, {"text": "UpperBound: a system that ranks all n correct answers as the top n results of the 20 answer candidates if there are any.", "labels": [], "entities": [{"text": "UpperBound", "start_pos": 0, "end_pos": 10, "type": "DATASET", "confidence": 0.8812469244003296}]}, {"text": "This indicates the performance upperbound in this experiment.", "labels": [], "entities": []}, {"text": "The relative performance of each system compared to UpperBound is shown in parentheses.", "labels": [], "entities": [{"text": "UpperBound", "start_pos": 52, "end_pos": 62, "type": "DATASET", "confidence": 0.9706437587738037}]}], "tableCaptions": [{"text": " Table 2: The performance of opinion extraction tool", "labels": [], "entities": [{"text": "opinion extraction", "start_pos": 29, "end_pos": 47, "type": "TASK", "confidence": 0.7489491999149323}]}, {"text": " Table 5: Evaluation with different combination of feature  sets used in training our re-ranker", "labels": [], "entities": []}]}