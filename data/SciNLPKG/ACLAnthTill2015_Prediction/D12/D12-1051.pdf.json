{"title": [{"text": "Exploiting Chunk-level Features to Improve Phrase Chunking", "labels": [], "entities": [{"text": "Improve Phrase Chunking", "start_pos": 35, "end_pos": 58, "type": "TASK", "confidence": 0.733311245838801}]}], "abstractContent": [{"text": "Most existing systems solved the phrase chunking task with the sequence labeling approaches, in which the chunk candidates cannot be treated as a whole during parsing process so that the chunk-level features cannot be exploited in a natural way.", "labels": [], "entities": [{"text": "phrase chunking task", "start_pos": 33, "end_pos": 53, "type": "TASK", "confidence": 0.8250433206558228}]}, {"text": "In this paper, we formulate phrase chunking as a joint segmentation and labeling task.", "labels": [], "entities": [{"text": "phrase chunking", "start_pos": 28, "end_pos": 43, "type": "TASK", "confidence": 0.7458557784557343}]}, {"text": "We propose an efficient dynamic programming algorithm with pruning for decoding, which allows the direct use of the features describing the internal characteristics of chunk and the features capturing the correlations between adjacent chunks.", "labels": [], "entities": []}, {"text": "A relaxed, online maximum margin training algorithm is used for learning.", "labels": [], "entities": []}, {"text": "Within this framework, we explored a variety of effective feature representations for Chinese phrase chunking.", "labels": [], "entities": [{"text": "Chinese phrase chunking", "start_pos": 86, "end_pos": 109, "type": "TASK", "confidence": 0.5783968567848206}]}, {"text": "The experimental results show that the use of chunk-level features can lead to significant performance improvement, and that our approach achieves state-of-the-art performance.", "labels": [], "entities": []}, {"text": "In particular, our approach is much better at recognizing long and complicated phrases.", "labels": [], "entities": []}], "introductionContent": [{"text": "Phrase chunking is a Natural Language Processing task that consists in dividing a text into syntactically correlated parts of words.", "labels": [], "entities": [{"text": "Phrase chunking", "start_pos": 0, "end_pos": 15, "type": "TASK", "confidence": 0.9317693114280701}]}, {"text": "Theses phrases are non-overlapping, i.e., a word can only be a member of one chunk.", "labels": [], "entities": []}, {"text": "Generally speaking, there are two phrase chunking tasks, including text chunking (shallow parsing), and noun phrase (NP) chunking.", "labels": [], "entities": [{"text": "phrase chunking", "start_pos": 34, "end_pos": 49, "type": "TASK", "confidence": 0.7220081090927124}, {"text": "text chunking (shallow parsing", "start_pos": 67, "end_pos": 97, "type": "TASK", "confidence": 0.6651311218738556}, {"text": "noun phrase (NP) chunking", "start_pos": 104, "end_pos": 129, "type": "TASK", "confidence": 0.5788893848657608}]}, {"text": "Phrase chunking provides a key feature that helps on more elaborated NLP tasks such as parsing, semantic role tagging and information extraction.", "labels": [], "entities": [{"text": "Phrase chunking", "start_pos": 0, "end_pos": 15, "type": "TASK", "confidence": 0.905729204416275}, {"text": "parsing", "start_pos": 87, "end_pos": 94, "type": "TASK", "confidence": 0.9650394320487976}, {"text": "semantic role tagging", "start_pos": 96, "end_pos": 117, "type": "TASK", "confidence": 0.6461065709590912}, {"text": "information extraction", "start_pos": 122, "end_pos": 144, "type": "TASK", "confidence": 0.8530328571796417}]}, {"text": "There is a wide range of research work on phrase chunking based on machine learning approaches.", "labels": [], "entities": [{"text": "phrase chunking", "start_pos": 42, "end_pos": 57, "type": "TASK", "confidence": 0.8824805319309235}]}, {"text": "However, most of the previous work reduced phrase chunking to sequence labeling problems either by using the classification models, such as SVM (), Winnow and voted-perceptrons (), or by using the sequence labeling models, such as Hidden Markov Models (HMMs) () and Conditional Random Fields (CRFs).", "labels": [], "entities": [{"text": "phrase chunking", "start_pos": 43, "end_pos": 58, "type": "TASK", "confidence": 0.7857910394668579}]}, {"text": "When applying the sequence labeling approaches to phrase chunking, there exist two major problems.", "labels": [], "entities": [{"text": "phrase chunking", "start_pos": 50, "end_pos": 65, "type": "TASK", "confidence": 0.8013023734092712}]}, {"text": "Firstly, these models cannot treat globally a sequence of continuous words as a chunk candidate, and thus cannot inspect the internal structure of the candidate, which is an important aspect of information in modeling phrase chunking.", "labels": [], "entities": [{"text": "phrase chunking", "start_pos": 218, "end_pos": 233, "type": "TASK", "confidence": 0.7279645651578903}]}, {"text": "In particular, it makes impossible the use of local indicator function features of the type \"the chunk consists of POS tag sequence p 1 ...,p k \".", "labels": [], "entities": []}, {"text": "For example, the Chinese NP \" \u519c \u4e1a /NN(agriculture) \u751f \u4ea7 /NN(production) \u548c/CC(and) \u519c\u6751/NN(rural) \u7ecf\u6d4e /NN(economic) \u53d1 \u5c55 /NN(development)\" seems relatively difficult to be correctly recognized by a sequence labeling approach due to its length.", "labels": [], "entities": []}, {"text": "But if we can treat the sequence of words as a whole and describe the formation pattern of POS tags of this chunk with a regular expression-like form \"+[NN]+\", then it is more likely to be correctly recognized, since this pattern might better express the characteristics of its constituents.", "labels": [], "entities": []}, {"text": "As another example, consider the recognition of special terms.", "labels": [], "entities": [{"text": "recognition of special terms", "start_pos": 33, "end_pos": 61, "type": "TASK", "confidence": 0.8822012841701508}]}, {"text": "In Chinese corpus, there exists a kind of NPs called special terms, such as \"\u300e \u751f\u547d (Life) \u7981 \u533a (Forbidden Zone) \u300f \", which are bracketed with the particular punctuations like \" \u300e, \u300f, \u300c, \u300d, \u300a, \u300b\".", "labels": [], "entities": []}, {"text": "When recognizing the special terms, it is difficult for the sequence labeling approaches to guarantee the matching of particular punctuations appearing at the starting and ending positions of a chunk.", "labels": [], "entities": []}, {"text": "For instance, the chunk candidate \"\u300e \u751f\u547d(Life) \u7981\u533a(Forbidden Zone)\" is considered to bean invalid chunk.", "labels": [], "entities": []}, {"text": "But it is easy to check this kind of punctuation matching in a single chunk by introducing a chunklevel feature.", "labels": [], "entities": []}, {"text": "Secondly, the sequence labeling models cannot capture the correlations between adjacent chunks, which should be informative for the identification of chunk boundaries and types.", "labels": [], "entities": [{"text": "sequence labeling", "start_pos": 14, "end_pos": 31, "type": "TASK", "confidence": 0.6586489230394363}]}, {"text": "In particular, we find that some headwords in the sentence are expected to have a stronger dependency relation with their preceding headwords in preceding chunks than with their immediately preceding words within the same chunk.", "labels": [], "entities": []}, {"text": "For example, in the following sentence: \" [\u53cc\u65b9/PN(Bilateral)]_NP [\u7ecf\u8d38/NN(economic and trade) \u5173\u7cfb/NN(relations)]_NP [\u6b63/AD(just) \u7a33\u6b65/AD(steadily) \u53d1\u5c55/VV(develop)]_VP \" if we can find the three headwords \"\u53cc\u65b9\", \"\u5173\u7cfb\" and \"\u53d1\u5c55\" located in the three adjacent chunks with some head-finding rules, then the headword dependency expressed by headword bigrams or trigrams should be helpful to recognize these chunks in this sentence.", "labels": [], "entities": []}, {"text": "In summary, the inherent deficiency in applying the sequence labeling approaches to phrase chunking is that the chunk-level features one would expect to be very informative cannot be exploited in a natural way.", "labels": [], "entities": [{"text": "phrase chunking", "start_pos": 84, "end_pos": 99, "type": "TASK", "confidence": 0.8045741617679596}]}, {"text": "In this paper, we formulate phrase chunking as a joint segmentation and labeling problem, which offers advantages over previous learning methods by providing a natural formulation to exploit the features describing the internal structure of a chunk and the features capturing the correlations between the adjacent chunks.", "labels": [], "entities": [{"text": "phrase chunking", "start_pos": 28, "end_pos": 43, "type": "TASK", "confidence": 0.6872594058513641}]}, {"text": "Within this framework, we explored a variety of effective feature representations for Chinese phrase chunking.", "labels": [], "entities": [{"text": "Chinese phrase chunking", "start_pos": 86, "end_pos": 109, "type": "TASK", "confidence": 0.5783968567848206}]}, {"text": "The experimental results on Chinese chunking corpus as well as English chunking corpus show that the use of chunk-level features can lead to significant performance improvement, and that our approach performs better than other approaches based on the sequence labeling models.", "labels": [], "entities": []}], "datasetContent": [{"text": "Following previous studies on Chinese chunking in (), our experiments were performed on the CTB4 dataset.", "labels": [], "entities": [{"text": "Chinese chunking", "start_pos": 30, "end_pos": 46, "type": "TASK", "confidence": 0.5823512673377991}, {"text": "CTB4 dataset", "start_pos": 92, "end_pos": 104, "type": "DATASET", "confidence": 0.993705689907074}]}, {"text": "The dataset consists of 838 files.", "labels": [], "entities": []}, {"text": "In the experiments, we used the first 728 files (FID from chtb 001.fid to chtb 899.fid) as training data, and the other 110 files (FID from chtb 900.fid to chtb 1078.fid) as testing data.", "labels": [], "entities": [{"text": "FID", "start_pos": 49, "end_pos": 52, "type": "METRIC", "confidence": 0.9851424098014832}]}, {"text": "The training set consists of 9878 sentences, and the test set consists of 5920 sentences.", "labels": [], "entities": []}, {"text": "The standard evaluation metrics for this task are precision p (the fraction of output chunks matching the reference chunks), recall r (the fraction of reference chunks returned), and the F-measure given by F = 2pr/(p + r).", "labels": [], "entities": [{"text": "precision p", "start_pos": 50, "end_pos": 61, "type": "METRIC", "confidence": 0.97483029961586}, {"text": "recall r", "start_pos": 125, "end_pos": 133, "type": "METRIC", "confidence": 0.9911526441574097}, {"text": "F-measure", "start_pos": 187, "end_pos": 196, "type": "METRIC", "confidence": 0.9964704513549805}, {"text": "F", "start_pos": 206, "end_pos": 207, "type": "METRIC", "confidence": 0.9912591576576233}]}, {"text": "Our model has two tunable parameters: the number of training iterations N; the number of top k-best outputs.", "labels": [], "entities": []}, {"text": "Since we were interested in finding an effective feature representation at chunk-level for phrase chunking, we fixed N = 10 and k = 5 for all experiments.", "labels": [], "entities": [{"text": "phrase chunking", "start_pos": 91, "end_pos": 106, "type": "TASK", "confidence": 0.771278589963913}]}, {"text": "In the following experiments, our model has roughly comparable training time to the sequence labeling approach based on CRFs.", "labels": [], "entities": [{"text": "sequence labeling", "start_pos": 84, "end_pos": 101, "type": "TASK", "confidence": 0.6048751324415207}]}], "tableCaptions": [{"text": " Table 2: Experimental results on Chinese NP  chunking.", "labels": [], "entities": [{"text": "NP  chunking", "start_pos": 42, "end_pos": 54, "type": "TASK", "confidence": 0.6510979682207108}]}, {"text": " Table 3: Experimental results on Chinese text chunking.", "labels": [], "entities": [{"text": "Chinese text chunking", "start_pos": 34, "end_pos": 55, "type": "TASK", "confidence": 0.5935393869876862}]}, {"text": " Table 4: Comparisons of chunking performance for  Chinese NP chunking and text chunking.", "labels": [], "entities": [{"text": "text chunking", "start_pos": 75, "end_pos": 88, "type": "TASK", "confidence": 0.6806144714355469}]}, {"text": " Table 4. Since the SL-type  features consist of the features associated with  single label, not including the features associated  with label bigrams. Then, adding the Internal-type  features to the system results in significant  performance improvement on NP chunking and on  text chunking, achieving 2.53% and 1.37%,  respectively. Further, if Correlation-type features  are used, the F1-scores on NP chunking and on text  chunking are improved by 1.01% and 0.66%,  respectively. The results show a significant impact  due to the use of Internal-type features and  Correlation-type features for both NP chunking  and text chunking.", "labels": [], "entities": [{"text": "NP chunking", "start_pos": 258, "end_pos": 269, "type": "TASK", "confidence": 0.7162377834320068}, {"text": "text chunking", "start_pos": 278, "end_pos": 291, "type": "TASK", "confidence": 0.701997235417366}, {"text": "F1-scores", "start_pos": 388, "end_pos": 397, "type": "METRIC", "confidence": 0.9966983199119568}, {"text": "NP chunking", "start_pos": 401, "end_pos": 412, "type": "TASK", "confidence": 0.7501147389411926}, {"text": "NP chunking", "start_pos": 603, "end_pos": 614, "type": "TASK", "confidence": 0.7979540824890137}, {"text": "text chunking", "start_pos": 620, "end_pos": 633, "type": "TASK", "confidence": 0.6781207025051117}]}, {"text": " Table 5: Test F1-scores for different types of  features on Chinese corpus.", "labels": [], "entities": [{"text": "F1-scores", "start_pos": 15, "end_pos": 24, "type": "METRIC", "confidence": 0.4991990625858307}, {"text": "Chinese corpus", "start_pos": 61, "end_pos": 75, "type": "DATASET", "confidence": 0.8548666536808014}]}, {"text": " Table 6: Performance on English corpus.", "labels": [], "entities": [{"text": "English corpus", "start_pos": 25, "end_pos": 39, "type": "DATASET", "confidence": 0.7003763914108276}]}]}