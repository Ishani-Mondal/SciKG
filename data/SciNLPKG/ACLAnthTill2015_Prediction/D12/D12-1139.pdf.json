{"title": [{"text": "Characterizing Stylistic Elements in Syntactic Structure", "labels": [], "entities": []}], "abstractContent": [{"text": "Much of the writing styles recognized in rhetorical and composition theories involve deep syntactic elements.", "labels": [], "entities": []}, {"text": "However, most previous research for computational sty-lometric analysis has relied on shallow lexico-syntactic patterns.", "labels": [], "entities": [{"text": "sty-lometric analysis", "start_pos": 50, "end_pos": 71, "type": "TASK", "confidence": 0.7210259139537811}]}, {"text": "Some very recent work has shown that PCFG models can detect distributional difference in syntactic styles, but without offering much insights into exactly what constitute salient stylistic elements in sentence structure characterizing each authorship.", "labels": [], "entities": []}, {"text": "In this paper, we present a comprehensive exploration of syntactic elements in writing styles, with particular emphasis on inter-pretable characterization of stylistic elements.", "labels": [], "entities": []}, {"text": "We present analytic insights with respect to the authorship attribution task in two different domains.", "labels": [], "entities": [{"text": "authorship attribution task", "start_pos": 49, "end_pos": 76, "type": "TASK", "confidence": 0.8576558033625284}]}], "introductionContent": [{"text": "Much of the writing styles recognized in rhetorical and composition theories involve deep syntactic elements in style (e.g.,,).", "labels": [], "entities": []}, {"text": "However, previous research for automatic authorship attribution and computational stylometric analysis have relied mostly on shallow lexico-syntactic patterns (e.g.,,,,,,,).", "labels": [], "entities": [{"text": "automatic authorship attribution", "start_pos": 31, "end_pos": 63, "type": "TASK", "confidence": 0.6886221468448639}, {"text": "computational stylometric analysis", "start_pos": 68, "end_pos": 102, "type": "TASK", "confidence": 0.6347226897875468}]}, {"text": "Some very recent works have shown that PCFG models can detect distributional difference in sentence structure in gender attribution), authorship attribution (, and native language identification (.", "labels": [], "entities": [{"text": "authorship attribution", "start_pos": 134, "end_pos": 156, "type": "TASK", "confidence": 0.8089158535003662}, {"text": "native language identification", "start_pos": 164, "end_pos": 194, "type": "TASK", "confidence": 0.6374419828255972}]}, {"text": "However, still very little has been understood exactly what constitutes salient stylistic elements in sentence structures that characterize each author.", "labels": [], "entities": []}, {"text": "Although the work of has extracted production rules with highest information gain, their analysis stops short of providing insight any deeper than what simple n-gramlevel analysis could also provide.", "labels": [], "entities": []}, {"text": "One might even wonder whether PCFG models are hinging mostly on leaf production rules, and whether there are indeed deep syntactic differences at all.", "labels": [], "entities": []}, {"text": "This paper attempts to answer these questions.", "labels": [], "entities": []}, {"text": "As an example of syntactic stylistic elements that have been much discussed in rhetorical theories, but have not been analyzed computationally, let us consider two contrasting sentence styles: loose (cumulative) and periodic: 2 a loose sentence places the main clause at the beginning, and then appends subordinate phrases and clauses to develop the main message.", "labels": [], "entities": []}, {"text": "In contrast, a periodic sentence starts with subordinate phrases and clauses, suspending the most: Top 10 most discriminative production rules for each author in the scientific domain.", "labels": [], "entities": []}], "datasetContent": [{"text": "In our experiments, we utilize a set of features motivated by PCFG trees.", "labels": [], "entities": []}, {"text": "These consist of simple production rules and other syntactic features based on tree-traversals.", "labels": [], "entities": []}, {"text": "describes these features with examples from Tree (2), using the portion marked by the triangle.", "labels": [], "entities": []}, {"text": "These sets of production rules and syntax features are used to build SVM classifiers using LI-BLINEAR (, wherein all feature values are encoded as term-frequencies normalized by document size.", "labels": [], "entities": [{"text": "LI-BLINEAR", "start_pos": 91, "end_pos": 101, "type": "METRIC", "confidence": 0.8890172839164734}]}, {"text": "We run 5-fold crossvalidation with training and testing split first as 80%/20%, and then as 20%/80%.", "labels": [], "entities": []}, {"text": "We would like to point out that the latter configuration is of high practical importance in authorship attribution, since we may not always have sufficient training data in realistic situations, e.g., forensics.", "labels": [], "entities": [{"text": "authorship attribution", "start_pos": 92, "end_pos": 114, "type": "TASK", "confidence": 0.9390457272529602}]}, {"text": "Lexical tokens provide strong clues by creating features that are specific to each author: research topics in the scientific data, and proper nouns such as character names in novels.", "labels": [], "entities": []}, {"text": "To lessen such topical bias, we lemmatize and rank words according to their frequency (in the entire dataset), and then consider the top 2,000 words only.", "labels": [], "entities": []}, {"text": "Leaf-node productions with words outside this set are disregarded.", "labels": [], "entities": []}, {"text": "Our experimental results show that not only do deep syntactic features perform well on their own, but they also significantly improve over lexical features.", "labels": [], "entities": []}, {"text": "We also show that adding the style 11 features further improves performance.", "labels": [], "entities": []}, {"text": "E.g., VP \u2192 VBG NP synv Traversal from a non-leaf node to its grandparent (embedded rising).", "labels": [], "entities": []}, {"text": "E.g., VP\u02c6S \u2192 PP syn h Left-to-right traversal in the set of all nonleaf children of anode.", "labels": [], "entities": []}, {"text": "E.g., VBG \u2192 NP (for node VP) syn v+h synv \u222a syn h syn0 No tree traversal.", "labels": [], "entities": []}, {"text": "Feature comprises interior nodes only.", "labels": [], "entities": []}, {"text": "syn \u2193 Union of all edges to child nodes, except when child is a leaf node.", "labels": [], "entities": []}, {"text": "E.g., {VP \u2192 VBG, VP \u2192 NP} syn syn \u2193 \u222a { edge to parent node} style11 The set of 11 extra stylistic features.", "labels": [], "entities": []}, {"text": "6 values from the distribution of sentence types (Section 3), and 5 topological metrics (Section 5) characterizing the height, width and imbalance of a tree.", "labels": [], "entities": []}, {"text": "Variations\u02c6pr Variations\u02c6 Variations\u02c6pr Each production rule is augmented with the grandparent node.", "labels": [], "entities": []}, {"text": "* Terminal (leaf) nodes are included.", "labels": [], "entities": []}, {"text": "pr * denotes the set of production rules pr (including terminal productions) that are augmented with their grandparent nodes.", "labels": [], "entities": []}, {"text": "To quantify the amount of authorship information carried in the set style 11 , we experiment with a SVM classifier using only 11 features (one for each metric), and achieve accuracy of 42.0% and 52.0% with scientific data and novels, respectively.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 173, "end_pos": 181, "type": "METRIC", "confidence": 0.9992567896842957}]}, {"text": "Given that a randomguess baseline would achieve only 10% and 20% (resp.), and that the classification is based on just 11 features, this experiment demonstrates how effectively the tree topology statistics capture idiolects.", "labels": [], "entities": [{"text": "resp.", "start_pos": 66, "end_pos": 71, "type": "METRIC", "confidence": 0.9137744903564453}]}, {"text": "In general, lexicalized features yield higher performance even after removing topical words.", "labels": [], "entities": []}, {"text": "This is expected since tokens such as function words play an important role in determining authorship (e.g.,,,).", "labels": [], "entities": []}, {"text": "A more important observation, however, is that even after removing the leaf production rules, accuracy as high as 93% (scientific) and 92.2% (novels) are obtained using syntactic fea-  tures, which demonstrates that there are syntactic patterns unique to each author.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 94, "end_pos": 102, "type": "METRIC", "confidence": 0.9992759823799133}]}, {"text": "Also notice that using only production rules, we achieve higher accuracy in novels (90.1%), but the addition of style 11 features yields better results with scientific data (93.0%).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 64, "end_pos": 72, "type": "METRIC", "confidence": 0.999123752117157}]}, {"text": "Using different amounts of training data provides insight about the influence of lexical clues.", "labels": [], "entities": []}, {"text": "In the scientific dataset, increasing the amount of training data decreases the average performance difference between lexicalized and unlexicalized features: 13.5% to 11.6%.", "labels": [], "entities": []}, {"text": "In novels, however, we seethe opposite trend: 6.1% increases to 8.1%.", "labels": [], "entities": []}, {"text": "We further observe that with scientific data, increasing the amount of training data improves the average performance across all unlexicalized feature-sets from 50.0% to 82.9%, an improvement of 32.8%.", "labels": [], "entities": []}, {"text": "For novels, the corresponding improvement is small in comparison: 17.0%.", "labels": [], "entities": []}, {"text": "While authors such as Dickens or Hardy have their unique writing styles that a classifier can learn based on few documents, capturing idiolects in the more rigid domain of scientific writing is far from obvious with little training data.", "labels": [], "entities": []}, {"text": "Turning to lexicalized features, we note that with more training data, lexical cues perform better in scientific domain than in novels.", "labels": [], "entities": []}, {"text": "With 80% data used for training, the average performance of lexicalized feature-sets with science data is 94.4%, and slightly lower at 94.3% for novels.", "labels": [], "entities": []}, {"text": "With less training data, however, these figures are 63.5% and 74.3% respectively.", "labels": [], "entities": [{"text": "training", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9509078860282898}]}, {"text": "Finally, we point out that adding the style features derived from sentence types and tree topologies almost always improves the performance.", "labels": [], "entities": []}, {"text": "In scientific data, syn * v+h with style 11 features shows the best performance (96%), while syn * yields the best results for novels (95.2%).", "labels": [], "entities": []}, {"text": "For unlexicalized features, adding style 11 to syn v+h and syn yields respective improvements of 4.0% and 2.9% in the two datasets.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: Sentence Types (%) in scientific data.", "labels": [], "entities": []}, {"text": " Table 4: Sentence Types (%) in Novels", "labels": [], "entities": []}, {"text": " Table 5: Syntactic variations of different authors in the scientific domain.", "labels": [], "entities": []}, {"text": " Table 7: Most discriminative sentence outlines in the novel data. #N shows the number of unique sentence  outlines of each author.", "labels": [], "entities": []}, {"text": " Table 8: Tree topology metrics for scientific data and novels.", "labels": [], "entities": []}, {"text": " Table 9: Tree Topology Statistics for", "labels": [], "entities": []}, {"text": " Table 11: Authorship attribution with 20% train- ing data. Improvement with addition of style 11  shown in bold.", "labels": [], "entities": [{"text": "Authorship attribution", "start_pos": 11, "end_pos": 33, "type": "TASK", "confidence": 0.7997975647449493}, {"text": "train- ing", "start_pos": 43, "end_pos": 53, "type": "METRIC", "confidence": 0.7715741395950317}]}, {"text": " Table 12: Authorship attribution with 80% train- ing data.", "labels": [], "entities": [{"text": "Authorship attribution", "start_pos": 11, "end_pos": 33, "type": "TASK", "confidence": 0.8167228102684021}, {"text": "train- ing data", "start_pos": 43, "end_pos": 58, "type": "METRIC", "confidence": 0.8434374928474426}]}]}