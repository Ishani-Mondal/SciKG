{"title": [{"text": "N-gram-based Tense Models for Statistical Machine Translation", "labels": [], "entities": [{"text": "Statistical Machine Translation", "start_pos": 30, "end_pos": 61, "type": "TASK", "confidence": 0.853829006354014}]}], "abstractContent": [{"text": "Tense is a small element to a sentence, however , error tense can raise odd grammars and result in misunderstanding.", "labels": [], "entities": []}, {"text": "Recently, tense has drawn attention in many natural language processing applications.", "labels": [], "entities": []}, {"text": "However, most of current Statistical Machine Translation (SMT) systems mainly depend on translation model and language model.", "labels": [], "entities": [{"text": "Statistical Machine Translation (SMT)", "start_pos": 25, "end_pos": 62, "type": "TASK", "confidence": 0.8443269729614258}]}, {"text": "They never consider and make full use of tense information.", "labels": [], "entities": []}, {"text": "In this paper , we propose n-gram-based tense models for SMT and successfully integrate them into a state-of-the-art phrase-based SMT system via two additional features.", "labels": [], "entities": [{"text": "SMT", "start_pos": 57, "end_pos": 60, "type": "TASK", "confidence": 0.9954055547714233}, {"text": "SMT", "start_pos": 130, "end_pos": 133, "type": "TASK", "confidence": 0.8569996953010559}]}, {"text": "Experimental results on the NIST Chinese-English translation task show that our proposed tense models are very effective, contributing performance improvement by 0.62 BLUE points over a strong baseline.", "labels": [], "entities": [{"text": "NIST Chinese-English translation task", "start_pos": 28, "end_pos": 65, "type": "TASK", "confidence": 0.761819988489151}, {"text": "BLUE", "start_pos": 167, "end_pos": 171, "type": "METRIC", "confidence": 0.9983513355255127}]}], "introductionContent": [{"text": "For many NLP applications, such as event extraction and summarization, tense has been regarded as a key factor in providing temporal order.", "labels": [], "entities": [{"text": "event extraction", "start_pos": 35, "end_pos": 51, "type": "TASK", "confidence": 0.7583850920200348}, {"text": "summarization", "start_pos": 56, "end_pos": 69, "type": "TASK", "confidence": 0.9862625598907471}]}, {"text": "However, tense information has been largely overlooked by current SMT research.", "labels": [], "entities": [{"text": "SMT", "start_pos": 66, "end_pos": 69, "type": "TASK", "confidence": 0.9942060708999634}]}, {"text": "Consider the following example: Although the translated text produced by Moses 1 is understandable, it has very odd tense combination from the grammatical aspect, i.e. with tense inconsistency (is/does in REF vs. is/did in Moses).", "labels": [], "entities": []}, {"text": "Obviously, slight modification, such as changing \"is\" into \"was\", can much improve the readability of the translated text.", "labels": [], "entities": []}, {"text": "It is also interesting to note that such modification can much affect the evaluation.", "labels": [], "entities": []}, {"text": "If we change \"did\" to \"does\", the BLEU-4 score increases from 22.65 to 27.86 (as matching the 4-gram \"does not reflect the\" in REF).", "labels": [], "entities": [{"text": "BLEU-4 score", "start_pos": 34, "end_pos": 46, "type": "METRIC", "confidence": 0.9853925406932831}, {"text": "REF", "start_pos": 127, "end_pos": 130, "type": "METRIC", "confidence": 0.9828086495399475}]}, {"text": "However, if we change \"is\" to \"was\", the BLEU score decreases from 22.65 to 21.44.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 41, "end_pos": 51, "type": "METRIC", "confidence": 0.9784436225891113}]}, {"text": "The above example seems special.", "labels": [], "entities": []}, {"text": "To testify its impact on SMT in wider range, we design a special experiment based on the 2005 NIST MT data (see section 6.1).", "labels": [], "entities": [{"text": "SMT", "start_pos": 25, "end_pos": 28, "type": "TASK", "confidence": 0.996292233467102}, {"text": "NIST MT data", "start_pos": 94, "end_pos": 106, "type": "DATASET", "confidence": 0.8365819056828817}]}, {"text": "This data has 4 references.", "labels": [], "entities": []}, {"text": "We choose one reference and modify its sentences with error tense 2 . After that, we use other 3 references to measure this reference.", "labels": [], "entities": []}, {"text": "The modified reference leads to a sharp drop in BLEU-4 score, from 52.46 to 50.27 in all.", "labels": [], "entities": [{"text": "BLEU-4 score", "start_pos": 48, "end_pos": 60, "type": "METRIC", "confidence": 0.9412246346473694}]}, {"text": "So it is not a random phenomenon that tense can affect translation results.", "labels": [], "entities": []}, {"text": "The key is how to detect tense errors and choose correct tenses during the translation procedure.", "labels": [], "entities": []}, {"text": "By carefully comparing the references with Moses output, we obtain the following useful observations, Observation(1): to most simple sentences, coordinate verbs should be translated with the same tense while they have different tense in Moses output; Observation(2): to some compound sentences, the subordinate clause should have the consistent tense with its main clause while Moses fails; Observation(3): the diversity of tense usage in a document is common.", "labels": [], "entities": [{"text": "Observation", "start_pos": 102, "end_pos": 113, "type": "METRIC", "confidence": 0.9942747950553894}, {"text": "Observation", "start_pos": 391, "end_pos": 402, "type": "METRIC", "confidence": 0.9839181900024414}]}, {"text": "However, inmost cases, the neighbored sentences tends to share the same main tense.", "labels": [], "entities": []}, {"text": "In some extreme examples, one tense (past or present), even dominates the whole document.", "labels": [], "entities": []}, {"text": "One possible solution to model above observations is using rules.", "labels": [], "entities": []}, {"text": "refers to six basic English tense structures and defines the possible paired combinations of \"present, past, future\".", "labels": [], "entities": []}, {"text": "But the practical cases are very complicated, especially in news domain.", "labels": [], "entities": []}, {"text": "There area lot of complicated sentences in news articles.", "labels": [], "entities": []}, {"text": "Our preliminary investigation shows that such six paired combinations can only cover limited real cases in Chinese-English SMT.", "labels": [], "entities": [{"text": "SMT", "start_pos": 123, "end_pos": 126, "type": "TASK", "confidence": 0.6508429646492004}]}, {"text": "This paper proposes a simple yet effective method to model above observations.", "labels": [], "entities": []}, {"text": "For each target sentence in the training corpus, we first parse it and extract its tense sequence.", "labels": [], "entities": []}, {"text": "Then, a target-side tense n-gram model is constructed.", "labels": [], "entities": []}, {"text": "Such model can be used to estimate the rationality of tense combination in a sentence and thus supervise SMT to reduce tense inconsistency errors against Observations (1) and (2) in the sentence-level.", "labels": [], "entities": [{"text": "SMT", "start_pos": 105, "end_pos": 108, "type": "TASK", "confidence": 0.9903213381767273}, {"text": "Observations", "start_pos": 154, "end_pos": 166, "type": "METRIC", "confidence": 0.8980025053024292}]}, {"text": "In comparison, Observation (3) actually reflects the tense distributions among one document.", "labels": [], "entities": []}, {"text": "After extracting each main tense for each sentence, we build another tense ngram model in the document-level.", "labels": [], "entities": []}, {"text": "For clarity, this paper denotes document-level tense as \"inter-tense\" and sentence-level tense as \"intra-tense\".", "labels": [], "entities": []}, {"text": "After that, we propose to integrate such tense models into SMT systems in a dynamic way.", "labels": [], "entities": [{"text": "SMT", "start_pos": 59, "end_pos": 62, "type": "TASK", "confidence": 0.9905807375907898}]}, {"text": "It is well known there are many errors in the current MT output ().", "labels": [], "entities": [{"text": "MT", "start_pos": 54, "end_pos": 56, "type": "TASK", "confidence": 0.8413266539573669}]}, {"text": "Unlike previously making trouble with reference texts, the BLEU-4 score cannot be influenced obviously by modifying a small part of abnormal sentences in a static way.", "labels": [], "entities": [{"text": "BLEU-4 score", "start_pos": 59, "end_pos": 71, "type": "METRIC", "confidence": 0.9769479036331177}]}, {"text": "In our system, both inter-tense and intra-tense model are integrated into a SMT system via additional features and thus can supervise the decoding procedure.", "labels": [], "entities": [{"text": "SMT", "start_pos": 76, "end_pos": 79, "type": "TASK", "confidence": 0.9828041791915894}]}, {"text": "During decoding, once some words with correct tense can be determined, with the help of language model and other related features, the small component-\"tense\"-can affect surrounding words and improve the performance of the whole sentence.", "labels": [], "entities": []}, {"text": "Our experimental results (see the examples in Section 6.4) show the effectiveness of this way.", "labels": [], "entities": []}, {"text": "Rather than the rule-based model, our models are fully statistical-based.", "labels": [], "entities": []}, {"text": "So they can be easily scaled up and integrated into either phrase-based or syntaxbased SMT systems.", "labels": [], "entities": [{"text": "SMT", "start_pos": 87, "end_pos": 90, "type": "TASK", "confidence": 0.8601776957511902}]}, {"text": "In this paper, we employ a strong phrase-based SMT baseline system, as proposed in, which uses document as translation unit, for better incorporating documentlevel information.", "labels": [], "entities": [{"text": "SMT baseline", "start_pos": 47, "end_pos": 59, "type": "TASK", "confidence": 0.8281506597995758}]}, {"text": "The rest of this paper is organized as follows: Section 2 reviews the related work.", "labels": [], "entities": []}, {"text": "Section 3 and 4 are related to tense models.", "labels": [], "entities": []}, {"text": "Section 3 describes the preprocessing work for building tense models.", "labels": [], "entities": []}, {"text": "Section 4 presents how to build target-side tense models and discuss their characteristics.", "labels": [], "entities": []}, {"text": "Section 5 shows our way of integrating such tense models into a SMT system.", "labels": [], "entities": [{"text": "SMT", "start_pos": 64, "end_pos": 67, "type": "TASK", "confidence": 0.995631217956543}]}, {"text": "Session 6 gives the experimental results.", "labels": [], "entities": []}, {"text": "Finally, we conclude this paper in Section 7.", "labels": [], "entities": []}], "datasetContent": [{"text": "In our experiment, SRI language modeling toolkit was used to train a 5-Gram general language model on the Xinhua portion of the Gigaword corpus.", "labels": [], "entities": [{"text": "SRI language modeling", "start_pos": 19, "end_pos": 40, "type": "TASK", "confidence": 0.6720643440882365}, {"text": "Xinhua portion of the Gigaword corpus", "start_pos": 106, "end_pos": 143, "type": "DATASET", "confidence": 0.653140793244044}]}, {"text": "Word alignment was performed on the training parallel corpus using GIZA++) in two directions.", "labels": [], "entities": [{"text": "Word alignment", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.7352651655673981}]}, {"text": "For evaluation, the NIST BLEU script (version 13) with the default setting is used to calculate the BLEU score (), which measures case-insensitive matching of 4-grams.", "labels": [], "entities": [{"text": "NIST", "start_pos": 20, "end_pos": 24, "type": "DATASET", "confidence": 0.8224557638168335}, {"text": "BLEU", "start_pos": 25, "end_pos": 29, "type": "METRIC", "confidence": 0.9488831162452698}, {"text": "BLEU score", "start_pos": 100, "end_pos": 110, "type": "METRIC", "confidence": 0.9876258969306946}]}, {"text": "To see whether an improvement is statistically significant, we also conduct significance tests using the paired bootstrap approach.", "labels": [], "entities": []}, {"text": "In this paper, \"***\" and \"**\" denote p-values equal to 0.05, and bigger than 0.05, which mean significantly better, moderately better respectively.", "labels": [], "entities": []}, {"text": "All the experiment results are showed on the table 3.", "labels": [], "entities": []}, {"text": "Our Baseline is a modified Moses.", "labels": [], "entities": []}, {"text": "The major modification is input and output module in order to translate using document as unit.", "labels": [], "entities": []}, {"text": "The performance of our baseline exceeds the baseline reported by   The system denoted as \"Baseline+F m \" integrates the inter-tense feature.", "labels": [], "entities": []}, {"text": "The performance boosts 0.57(***) in BLEU score.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 36, "end_pos": 46, "type": "METRIC", "confidence": 0.9639876484870911}]}, {"text": "The system denoted as \"Baseline+F s \" integrates the intra-tense feature into the baseline.", "labels": [], "entities": []}, {"text": "The improvement is less than the inter-tense model, only 0.31(**).", "labels": [], "entities": []}, {"text": "It seems the tenses in one sentence has more flexible formats than the document-level ones.", "labels": [], "entities": []}, {"text": "It is worth noting, this method can gain higher performance on the develop data than the one of \"Baseline+F m \" while fail to improve the test data.", "labels": [], "entities": []}, {"text": "Maybe the related weight is tuned over-fit.", "labels": [], "entities": []}, {"text": "The system denoted as \"Baseline+F s (*)\" is slightly different from \"Baseline+F s \".", "labels": [], "entities": []}, {"text": "This experiment is to check whether the main tense has an impact on intra-tense model or not (see Section 3.2).", "labels": [], "entities": []}, {"text": "Here, the intra-tense model based on the tense sequence with main tense marker is slightly different to the model showed in.", "labels": [], "entities": []}, {"text": "The results are slightly better than the previous system by 0.13.", "labels": [], "entities": []}, {"text": "Finally, we use the two features together (Baseline+F m +F s ).", "labels": [], "entities": []}, {"text": "The best way improved the performance by 0.62(***) in BLEU score over our baseline.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 54, "end_pos": 64, "type": "METRIC", "confidence": 0.9723450839519501}]}, {"text": "shows special examples whose intra-tenses are changed in our proposed system.", "labels": [], "entities": []}, {"text": "The example 1 and 2 show such modification can improve the BLEU score but the example 3 obtains negative impact.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 59, "end_pos": 69, "type": "METRIC", "confidence": 0.9799512326717377}]}, {"text": "From these examples, we can see not only tense verbs have changed but also their surrounding words have subtle variation.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: The Chinese and English Verb Pos Alignment", "labels": [], "entities": [{"text": "Chinese and English Verb Pos Alignment", "start_pos": 14, "end_pos": 52, "type": "TASK", "confidence": 0.5184993892908096}]}, {"text": " Table 3: The performance of using different feature com- binations", "labels": [], "entities": []}]}