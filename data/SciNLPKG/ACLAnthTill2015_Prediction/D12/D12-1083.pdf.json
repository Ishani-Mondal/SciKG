{"title": [{"text": "A Novel Discriminative Framework for Sentence-Level Discourse Analysis", "labels": [], "entities": [{"text": "Sentence-Level Discourse Analysis", "start_pos": 37, "end_pos": 70, "type": "TASK", "confidence": 0.9084469278653463}]}], "abstractContent": [{"text": "We propose a complete probabilistic discrim-inative framework for performing sentence-level discourse analysis.", "labels": [], "entities": [{"text": "sentence-level discourse analysis", "start_pos": 77, "end_pos": 110, "type": "TASK", "confidence": 0.6312388777732849}]}, {"text": "Our framework comprises a discourse segmenter, based on a binary classifier, and a discourse parser, which applies an optimal CKY-like parsing algorithm to probabilities inferred from a Dynamic Conditional Random Field.", "labels": [], "entities": []}, {"text": "We show on two corpora that our approach outperforms the state-of-the-art, often by a wide margin.", "labels": [], "entities": []}], "introductionContent": [{"text": "Automatic discourse analysis has been shown to be critical in several fundamental Natural Language Processing (NLP) tasks including text generation (), summarization, sentence compression) and question answering (.", "labels": [], "entities": [{"text": "Automatic discourse analysis", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.6017244656880697}, {"text": "text generation", "start_pos": 132, "end_pos": 147, "type": "TASK", "confidence": 0.7447526454925537}, {"text": "summarization", "start_pos": 152, "end_pos": 165, "type": "TASK", "confidence": 0.9791325926780701}, {"text": "sentence compression", "start_pos": 167, "end_pos": 187, "type": "TASK", "confidence": 0.6916541308164597}, {"text": "question answering", "start_pos": 193, "end_pos": 211, "type": "TASK", "confidence": 0.890367180109024}]}, {"text": "Rhetorical Structure Theory (RST), one of the most influential theories of discourse, posits a tree representation of a discourse, known as a Discourse Tree (DT), as exemplified by the sample DT shown in.", "labels": [], "entities": [{"text": "Rhetorical Structure Theory (RST)", "start_pos": 0, "end_pos": 33, "type": "TASK", "confidence": 0.8279713988304138}]}, {"text": "The leaves of a DT correspond to contiguous atomic text spans, also called Elementary Discourse Units (EDUs) (three in the example).", "labels": [], "entities": []}, {"text": "The adjacent EDUs are connected by a rhetorical relation (e.g., ELAB-ORATION), and the resulting larger text spans are recursively also subject to this relation linking.", "labels": [], "entities": []}, {"text": "A span linked by a rhetorical relation can be either a NUCLEUS or a SATELLITE depending on how central the message is to the author.", "labels": [], "entities": [{"text": "SATELLITE", "start_pos": 68, "end_pos": 77, "type": "METRIC", "confidence": 0.9899752736091614}]}, {"text": "Discourse analysis in RST involves two subtasks: (i) breaking the text into EDUs (known as discourse segmentation) and (ii) linking the EDUs into a labeled hierarchical tree structure (known as discourse parsing).", "labels": [], "entities": [{"text": "Discourse analysis", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.7959604561328888}, {"text": "RST", "start_pos": 22, "end_pos": 25, "type": "TASK", "confidence": 0.9237545132637024}, {"text": "discourse parsing", "start_pos": 194, "end_pos": 211, "type": "TASK", "confidence": 0.7427269220352173}]}, {"text": "Previous studies on discourse analysis have been quite successful in identifying what machine learning approaches and what features are more useful for automatic discourse segmentation and parsing.", "labels": [], "entities": [{"text": "discourse analysis", "start_pos": 20, "end_pos": 38, "type": "TASK", "confidence": 0.7734125256538391}, {"text": "automatic discourse segmentation and parsing", "start_pos": 152, "end_pos": 196, "type": "TASK", "confidence": 0.5944279134273529}]}, {"text": "However, all the proposed solutions suffer from at least one of the following two key limitations: first, they make strong independence assumptions on the structure and the labels of the resulting DT, and typically model the construction of the DT and the labeling of the relations separately; second, they apply a greedy, suboptimal algorithm to build the structure of the DT.", "labels": [], "entities": []}, {"text": "In this paper, we propose anew sentence-level discourse parser that addresses both limitations.", "labels": [], "entities": [{"text": "sentence-level discourse parser", "start_pos": 31, "end_pos": 62, "type": "TASK", "confidence": 0.6696729858716329}]}, {"text": "The crucial component is a probabilistic discriminative parsing model, expressed as a Dynamic Conditional Random Field (DCRF) (.", "labels": [], "entities": []}, {"text": "By representing the structure and the relation of each discourse tree constituent jointly and by explicitly capturing the sequential and hierarchical dependencies between constituents of a discourse tree, our DCRF model does not make any independence assumption among these properties.", "labels": [], "entities": []}, {"text": "Furthermore, our parsing model supports a bottom-up parsing algorithm which is non-greedy and provably optimal.", "labels": [], "entities": [{"text": "parsing", "start_pos": 17, "end_pos": 24, "type": "TASK", "confidence": 0.9696802496910095}]}, {"text": "The discourse parser assumes that the input text has been already segmented into EDUs.", "labels": [], "entities": []}, {"text": "As an additional contribution of this paper, we propose a novel discriminative approach to discourse segmentation that not only achieves state-of-the-art performance, but also reduces the time and space complexities by using fewer features.", "labels": [], "entities": [{"text": "discourse segmentation", "start_pos": 91, "end_pos": 113, "type": "TASK", "confidence": 0.736373633146286}]}, {"text": "Notice that the combination of our segmenter with our parser forms a complete probabilistic discriminative framework for performing sentence-level discourse analysis.", "labels": [], "entities": [{"text": "sentence-level discourse analysis", "start_pos": 132, "end_pos": 165, "type": "TASK", "confidence": 0.6727481881777445}]}, {"text": "Our framework was tested in a series of experiments.", "labels": [], "entities": []}, {"text": "The empirical evaluation indicates that our approach to discourse parsing outperforms the stateof-the-art by a wide margin.", "labels": [], "entities": [{"text": "discourse parsing", "start_pos": 56, "end_pos": 73, "type": "TASK", "confidence": 0.7059813141822815}]}, {"text": "Moreover, we show this to be the case on two very different genres: news articles and instructional how-to-do manuals.", "labels": [], "entities": []}, {"text": "In the rest of the paper, after discussing related work, we present our discourse parser.", "labels": [], "entities": []}, {"text": "Then, we describe our segmenter.", "labels": [], "entities": []}, {"text": "The experiments and the corpora we used are described next, followed by a discussion of the key results and some error analysis.", "labels": [], "entities": []}], "datasetContent": [{"text": "We perform our experiments on discourse parsing in RST-DT with the 18 coarser relations (see) defined in and also used in SPADE and HILDA.", "labels": [], "entities": [{"text": "discourse parsing", "start_pos": 30, "end_pos": 47, "type": "TASK", "confidence": 0.6892388015985489}, {"text": "HILDA", "start_pos": 132, "end_pos": 137, "type": "DATASET", "confidence": 0.8098909854888916}]}, {"text": "By attaching the nuclearity statuses (i.e., NS, SN, NN) to these relations we get 39 distinct relations . Our experiments on the Instructional corpus consider the same 26 primary relations (e.g., GOAL:ACT, CAUSE:EFFECT, GENERAL-SPECIFIC) used in and also treat the reversals of non-commutative relations as separate relations.", "labels": [], "entities": [{"text": "Instructional corpus", "start_pos": 129, "end_pos": 149, "type": "DATASET", "confidence": 0.8677940964698792}, {"text": "GOAL:ACT", "start_pos": 196, "end_pos": 204, "type": "METRIC", "confidence": 0.7546495596567789}]}, {"text": "That is, PREPARATION-ACT and ACT-PREPARATION are two different relations.", "labels": [], "entities": []}, {"text": "Attaching the nuclearity statuses to these relations gives 70 distinct relations in the Instructional corpus.", "labels": [], "entities": [{"text": "Instructional corpus", "start_pos": 88, "end_pos": 108, "type": "DATASET", "confidence": 0.7424448728561401}]}, {"text": "We use SPADE as our baseline model and apply the same modifications to its default setting as described in, which delivers improved performance.", "labels": [], "entities": []}, {"text": "Specifically, in testing, we replace the Charniak parser) with a more accurate reranking parser).", "labels": [], "entities": []}, {"text": "We use the reranking parser in all our models to generate the syntactic trees.", "labels": [], "entities": []}, {"text": "This parser was trained on the sections of the Penn Treebank not included in the test set.", "labels": [], "entities": [{"text": "Penn Treebank", "start_pos": 47, "end_pos": 60, "type": "DATASET", "confidence": 0.9947552382946014}]}, {"text": "For a fair comparison, we apply the same canonical lexical head projection rules to lexicalize the syntactic trees as done in SPADE and HILDA.", "labels": [], "entities": []}, {"text": "Note that, all the previous works described in Section 2, report their models' performance on a particular test set of a specific corpus.", "labels": [], "entities": []}, {"text": "To compare our results with the previous studies, we test our models on those specific test sets.", "labels": [], "entities": []}, {"text": "In addition, we show more general performance based on 10-fold cross validation.", "labels": [], "entities": []}, {"text": "We evaluate the segmentation accuracy with respect to the intra-sentential segment boundaries following.", "labels": [], "entities": [{"text": "segmentation", "start_pos": 16, "end_pos": 28, "type": "TASK", "confidence": 0.9563464522361755}, {"text": "accuracy", "start_pos": 29, "end_pos": 37, "type": "METRIC", "confidence": 0.9388723969459534}]}, {"text": "Specifically, if a sentence contains n EDUs, which corresponds ton \u2212 1 intra-sentence segment boundaries, we measure the model's ability to correctly identify these n \u2212 1 boundaries.", "labels": [], "entities": []}, {"text": "Human agreement for this task is quite high (F-score of 98.3) on RST-DT.", "labels": [], "entities": [{"text": "agreement", "start_pos": 6, "end_pos": 15, "type": "METRIC", "confidence": 0.9909476637840271}, {"text": "F-score", "start_pos": 45, "end_pos": 52, "type": "METRIC", "confidence": 0.9995488524436951}, {"text": "RST-DT", "start_pos": 65, "end_pos": 71, "type": "DATASET", "confidence": 0.7527933120727539}]}, {"text": "shows the results of different models in (P)recision, (R)ecall, and (F)-score on the two corpora.", "labels": [], "entities": []}, {"text": "We compare our model's (LR) results with HILDA (HIL), SPADE (SP) and the results reported in on the RST-DT test set.", "labels": [], "entities": [{"text": "HILDA (HIL)", "start_pos": 41, "end_pos": 52, "type": "METRIC", "confidence": 0.8821097016334534}, {"text": "SPADE (SP)", "start_pos": 54, "end_pos": 64, "type": "METRIC", "confidence": 0.9398271441459656}, {"text": "RST-DT test set", "start_pos": 100, "end_pos": 115, "type": "DATASET", "confidence": 0.847534716129303}]}, {"text": "HILDA gives the weakest performance 7 . Our results are also much better than SPADE 8 , with an absolute F-score improvement of 4.9%, and comparable to the results of F&R, even though we use fewer features.", "labels": [], "entities": [{"text": "HILDA", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.6208658814430237}, {"text": "F-score", "start_pos": 105, "end_pos": 112, "type": "METRIC", "confidence": 0.9897921085357666}, {"text": "F&R", "start_pos": 167, "end_pos": 170, "type": "TASK", "confidence": 0.5979313850402832}]}, {"text": "Furthermore, we perform 10-fold cross validation on both corpora and compare with SPADE.", "labels": [], "entities": [{"text": "SPADE", "start_pos": 82, "end_pos": 87, "type": "DATASET", "confidence": 0.5510826110839844}]}, {"text": "However, SPADE does not come with a training module for its segmenter.", "labels": [], "entities": [{"text": "SPADE", "start_pos": 9, "end_pos": 14, "type": "TASK", "confidence": 0.8881598114967346}]}, {"text": "We reimplemented this module and verified it on the RST-DT test set.", "labels": [], "entities": [{"text": "RST-DT test set", "start_pos": 52, "end_pos": 67, "type": "DATASET", "confidence": 0.9073065121968588}]}, {"text": "Due to the lack of human-annotated syntactic trees in the Instructional corpus, we train SPADE in this corpus using the syntactic trees produced by the reranking parser.", "labels": [], "entities": [{"text": "Instructional corpus", "start_pos": 58, "end_pos": 78, "type": "DATASET", "confidence": 0.8623586297035217}]}, {"text": "Our model delivers absolute F-score improvements of 3.8% and 8.1% on the RST-DT and the Instructional corpora, respectively, which is statistically significant in both cases (p < 3.0e-06).", "labels": [], "entities": [{"text": "F-score", "start_pos": 28, "end_pos": 35, "type": "METRIC", "confidence": 0.9872626662254333}]}, {"text": "However, when we compare our results on the two corpora, we observe a substantial decrease in performance on the Instructional corpus.", "labels": [], "entities": [{"text": "Instructional corpus", "start_pos": 113, "end_pos": 133, "type": "DATASET", "confidence": 0.8949695229530334}]}, {"text": "This could be due to a smaller amount of data in this corpus and the inaccuracies in the syntactic parser and taggers, which are trained on news articles.: Segmentation results of different models.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Parsing results using manual segmentation.", "labels": [], "entities": [{"text": "Parsing", "start_pos": 10, "end_pos": 17, "type": "TASK", "confidence": 0.9722973108291626}]}, {"text": " Table 3: Parsing results based on manual segmentation  using different subsets of features on RST-DT test set.  Feature subsets S 1 = {Dominance set}, S 2 = {Dominance  set, Organizational}, S 3 = {Dominance set, Organiza- tional, N-gram}, S 4 = {Dominance set, Organizational,  N-gram, Contextual}, S 5 (all) = {Dominance set, Orga- nizational, N-gram, Contextual, Substructure}.", "labels": [], "entities": [{"text": "RST-DT test set", "start_pos": 95, "end_pos": 110, "type": "DATASET", "confidence": 0.7971442838509878}]}, {"text": " Table 4: Segmentation results of different models.", "labels": [], "entities": [{"text": "Segmentation", "start_pos": 10, "end_pos": 22, "type": "TASK", "confidence": 0.929796576499939}]}, {"text": " Table 5: Parsing results using automatic segmentation.", "labels": [], "entities": [{"text": "Parsing", "start_pos": 10, "end_pos": 17, "type": "TASK", "confidence": 0.9774038791656494}]}]}