{"title": [{"text": "Natural Language Questions for the Web of Data", "labels": [], "entities": []}], "abstractContent": [{"text": "The Linked Data initiative comprises struc-tured databases in the Semantic-Web data model RDF.", "labels": [], "entities": []}, {"text": "Exploring this heterogeneous data by structured query languages is tedious and error-prone even for skilled users.", "labels": [], "entities": []}, {"text": "To ease the task, this paper presents a methodology for translating natural language questions into structured SPARQL queries over linked-data sources.", "labels": [], "entities": []}, {"text": "Our method is based on an integer linear program to solve several disambiguation tasks jointly: the segmentation of questions into phrases; the mapping of phrases to semantic entities, classes, and relations; and the construction of SPARQL triple patterns.", "labels": [], "entities": [{"text": "segmentation of questions into phrases", "start_pos": 100, "end_pos": 138, "type": "TASK", "confidence": 0.828460419178009}]}, {"text": "Our solution harnesses the rich type system provided by knowledge bases in the web of linked data, to constrain our semantic-coherence objective function.", "labels": [], "entities": []}, {"text": "We present experiments on both the question translation and the resulting query answering.", "labels": [], "entities": [{"text": "question translation", "start_pos": 35, "end_pos": 55, "type": "TASK", "confidence": 0.7315980643033981}, {"text": "query answering", "start_pos": 74, "end_pos": 89, "type": "TASK", "confidence": 0.6612104177474976}]}], "introductionContent": [], "datasetContent": [{"text": "Our experiments are based on two collections of questions: the QALD-1 task for question answering over linked data) and a collection of questions used in) in the context of the NAGA project, for informative ranking of SPARQL query answers ( evaluated the SPARQL queries, but the underlying questions are formulated in natural language.)", "labels": [], "entities": [{"text": "question answering over linked", "start_pos": 79, "end_pos": 109, "type": "TASK", "confidence": 0.8122189044952393}]}, {"text": "The NAGA collection is based on linking data from IMDB with the Yago2 knowledge base.", "labels": [], "entities": [{"text": "NAGA collection", "start_pos": 4, "end_pos": 19, "type": "DATASET", "confidence": 0.8929736912250519}, {"text": "IMDB", "start_pos": 50, "end_pos": 54, "type": "DATASET", "confidence": 0.9304925203323364}, {"text": "Yago2 knowledge base", "start_pos": 64, "end_pos": 84, "type": "DATASET", "confidence": 0.9848382472991943}]}, {"text": "This is an interesting linkeddata case: IMDB provides data about movies, actors, directors, and movie plots (in the form of descriptive keywords and phrases); Yago2 adds semantic types and relational facts for the participating entities.", "labels": [], "entities": [{"text": "IMDB", "start_pos": 40, "end_pos": 44, "type": "DATASET", "confidence": 0.8708686232566833}]}, {"text": "Yago2 provides nearly 3 million concepts and 100 relations, of which 41 lie within the scope of our framework.", "labels": [], "entities": [{"text": "Yago2", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.9130000472068787}]}, {"text": "Typical example questions for these two collections are: \"Which software has been published by Mean Hamster Software?\" for QALD-1, and \"Which director has won the Academy Award for Best director and is married to an actress that has won the Academy Award for Best Actress?\" for NAGA.", "labels": [], "entities": [{"text": "Mean Hamster Software", "start_pos": 95, "end_pos": 116, "type": "DATASET", "confidence": 0.9200181762377421}, {"text": "QALD-1", "start_pos": 123, "end_pos": 129, "type": "DATASET", "confidence": 0.7516605854034424}, {"text": "NAGA", "start_pos": 278, "end_pos": 282, "type": "DATASET", "confidence": 0.939400851726532}]}, {"text": "For both collections, some questions are out-of-scope for our setting, because they mention entities or relations that are not available in the underlying datasets, contain date or time comparisons, or involve aggregation such as counting.", "labels": [], "entities": []}, {"text": "After re-moving these questions, our test set consists of 27 QALD-1 training questions out of a total of 50 and 44 NAGA questions, out of a total of 87.", "labels": [], "entities": [{"text": "NAGA", "start_pos": 115, "end_pos": 119, "type": "DATASET", "confidence": 0.8019051551818848}]}, {"text": "We used the 19 questions from the QALD-1 test set that are within the scope of our method for tuning the hyperparameters (\u03b1, \u03b2, \u03b3) in the ILP objective function.", "labels": [], "entities": [{"text": "QALD-1 test set", "start_pos": 34, "end_pos": 49, "type": "DATASET", "confidence": 0.9528345068295797}]}, {"text": "We evaluated the output of DEANNA at three stages in the processing pipeline: a) after the disambiguation of phrases, b) after the generation of the SPARQL query, and c) after obtaining answers from the underlying linked-data sources.", "labels": [], "entities": []}, {"text": "This way, we could obtain insights into our building blocks, in addition to assessing the end-to-end performance.", "labels": [], "entities": []}, {"text": "In particular, we could assess the goodness of the question-to-query translation independently of the actual answer quality which may depend on particularities of the underlying datasets (e.g., slight mismatches between query terminology and the names in the data.)", "labels": [], "entities": [{"text": "question-to-query translation", "start_pos": 51, "end_pos": 80, "type": "TASK", "confidence": 0.744581550359726}]}, {"text": "At each of the three stages, the output was shown to two human assessors who judged whether an output item was good or not.", "labels": [], "entities": []}, {"text": "If the two were in disagreement, then a third person resolved the judgment.", "labels": [], "entities": []}, {"text": "For the disambiguation stage, the judges looked at each q-node/s-node pair, in the context of the question and the underlying data schemas, and determined whether the mapping was corrector not and whether any expected mappings were missing.", "labels": [], "entities": []}, {"text": "For the query-generation stage, the judges looked at each triple pattern and determined whether the pattern was meaningful for the question or not and whether any expected triple pattern was missing.", "labels": [], "entities": []}, {"text": "Note that, because our approach does not use any query templates, the same question may generate semantically equivalent queries that differ widely in terms of their structure.", "labels": [], "entities": []}, {"text": "Hence, we rely on our evaluation metrics that are based on triple patterns, as there is no gold-standard query fora given question.", "labels": [], "entities": []}, {"text": "For the query-answering stage, the judges were asked to identify if the result sets for the generated queries are satisfactory.", "labels": [], "entities": []}, {"text": "With these assessments, we computed overall quality measures by both micro-averaging and macro-averaging.", "labels": [], "entities": []}, {"text": "Micro-averaging aggregates overall assessed items (e.g., q-node/s-node pairs or triple patterns) regardless of the questions to which they belong.", "labels": [], "entities": []}, {"text": "Macro-averaging first aggregates the items for the same question, and then averages the quality measure overall questions.", "labels": [], "entities": []}, {"text": "For a question q and item set sin one of the stages of evaluation, let correct(q, s) be the number of correct items in s, ideal(q) be the size of the ideal item set and retrieved(q, s) be the number of retrieved items, we define coverage and precision as follows: cov(q, s) = correct(q, s)/ideal(q) prec(q, s) = correct(q, s)/retrieved(q, s).", "labels": [], "entities": [{"text": "coverage", "start_pos": 229, "end_pos": 237, "type": "METRIC", "confidence": 0.9647035598754883}, {"text": "precision", "start_pos": 242, "end_pos": 251, "type": "METRIC", "confidence": 0.9982696771621704}]}, {"text": "shows the results for disambiguation in terms of macro and micro coverage and precision.", "labels": [], "entities": [{"text": "precision", "start_pos": 78, "end_pos": 87, "type": "METRIC", "confidence": 0.998224675655365}]}, {"text": "For both datasets, coverage is high as few mappings are missing.", "labels": [], "entities": [{"text": "coverage", "start_pos": 19, "end_pos": 27, "type": "METRIC", "confidence": 0.9983049631118774}]}, {"text": "We obtain perfect precision for QALD-1 as no mapping that we generate is incorrect, while for NAGA we generate few incorrect mappings.", "labels": [], "entities": [{"text": "precision", "start_pos": 18, "end_pos": 27, "type": "METRIC", "confidence": 0.9994540810585022}, {"text": "QALD-1", "start_pos": 32, "end_pos": 38, "type": "DATASET", "confidence": 0.7773674726486206}]}, {"text": "shows the same metrics for the generated triple patterns.", "labels": [], "entities": []}, {"text": "The results are similar to those for disambiguation.", "labels": [], "entities": [{"text": "disambiguation", "start_pos": 37, "end_pos": 51, "type": "TASK", "confidence": 0.966227650642395}]}, {"text": "Missing or incorrect triple patterns can be attributed to (i) incorrect mappings in the disambiguation stage or (ii) incorrect detection of dependencies between phrases despite having the correct mappings.", "labels": [], "entities": []}, {"text": "shows the results for query answering.", "labels": [], "entities": [{"text": "query answering", "start_pos": 22, "end_pos": 37, "type": "TASK", "confidence": 0.8726837337017059}]}, {"text": "Here, we attempt to generate answers to questions by executing the generated queries over the datasets.", "labels": [], "entities": []}, {"text": "The table shows the number of questions for which the system successfully generated SPARQL queries (#queries), and among those, how many resulted in satisfactory answers as judged by our evaluators (#satisfactory).", "labels": [], "entities": []}, {"text": "Answers were considered unsatisfactory when: 1) the generated SPARQL query was wrong, 2) the result set was empty due to the incompleteness of the underlying knowledge base, or 3) a small fraction of the result set was relevant to the question.", "labels": [], "entities": []}, {"text": "For both sets of questions, most of the queries that were perceived unsatisfactory were ones that returned no answers.: Example questions, the generated SPARQL queries and their answers", "labels": [], "entities": []}], "tableCaptions": []}