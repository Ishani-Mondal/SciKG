{"title": [{"text": "Mixed Membership Markov Models for Unsupervised Conversation Modeling", "labels": [], "entities": [{"text": "Unsupervised Conversation Modeling", "start_pos": 35, "end_pos": 69, "type": "TASK", "confidence": 0.6204828023910522}]}], "abstractContent": [{"text": "Recent work has explored the use of hidden Markov models for unsupervised discourse and conversation modeling, where each segment or block of text such as a message in a conversation is associated with a hidden state in a sequence.", "labels": [], "entities": [{"text": "conversation modeling", "start_pos": 88, "end_pos": 109, "type": "TASK", "confidence": 0.7251519858837128}]}, {"text": "We extend this approach to allow each block of text to be a mixture of multiple classes.", "labels": [], "entities": []}, {"text": "Under our model, the probability of a class in a text block is a log-linear function of the classes in the previous block.", "labels": [], "entities": []}, {"text": "We show that this model performs well at predic-tive tasks on two conversation data sets, improving thread reconstruction accuracy by up to 15 percentage points over a standard HMM.", "labels": [], "entities": [{"text": "thread reconstruction", "start_pos": 100, "end_pos": 121, "type": "TASK", "confidence": 0.6425196975469589}, {"text": "accuracy", "start_pos": 122, "end_pos": 130, "type": "METRIC", "confidence": 0.9584367275238037}]}, {"text": "Additionally, we show quantitatively that the induced word clusters correspond to speech acts more closely than baseline models.", "labels": [], "entities": []}], "introductionContent": [{"text": "The proliferation of social media in recent years has lead to an increased use of informal Web data in the language processing community.", "labels": [], "entities": []}, {"text": "With this rising interest in social domains, it is natural to consider models which explicitly incorporate the conversational patterns of social text.", "labels": [], "entities": []}, {"text": "Compared to the naive approach of treating conversations as flat documents, models which include conversation structure have been shown to improve tasks such as forum search, question answering and expert finding (, and interpersonal relationship identification (.", "labels": [], "entities": [{"text": "forum search", "start_pos": 161, "end_pos": 173, "type": "TASK", "confidence": 0.7556707561016083}, {"text": "question answering", "start_pos": 175, "end_pos": 193, "type": "TASK", "confidence": 0.8393228352069855}, {"text": "expert finding", "start_pos": 198, "end_pos": 212, "type": "TASK", "confidence": 0.8242524564266205}, {"text": "interpersonal relationship identification", "start_pos": 220, "end_pos": 261, "type": "TASK", "confidence": 0.6795933445294698}]}, {"text": "While conversational features maybe important, Web-derived corpora are not always annotated with this information, and the nature of conversations on the Web can vary wildly across domains and venues.", "labels": [], "entities": []}, {"text": "Addressing these concerns, there has been recent work with unsupervised models of Web conversations based on hidden Markov models (, where each state corresponds to a conversational class or \"act.\"", "labels": [], "entities": []}, {"text": "Unlike more traditional uses of HMMs in which a single token is emitted per time step, HMM emissions in conversations correspond to entire blocks of text, such that an entire message is generated at each step.", "labels": [], "entities": []}, {"text": "Because each time step is associated with a block of variables, we refer to this type of HMM as a block HMM.", "labels": [], "entities": []}, {"text": "While block HMMs offer a concise model of inter-message structure, they have the limitation that each text block (message) belongs to exactly one class.", "labels": [], "entities": []}, {"text": "Many modern generative models of text, in contrast, allow documents to contain many latent classes.", "labels": [], "entities": []}, {"text": "For example, topic models such as Latent Dirichlet Allocation (LDA) ( assume each document has its own distribution over multiple classes (often called \"topics\").", "labels": [], "entities": [{"text": "Latent Dirichlet Allocation (LDA)", "start_pos": 34, "end_pos": 67, "type": "TASK", "confidence": 0.6592590163151423}]}, {"text": "For many predictive tasks, topic models outperform singleclass generative models such as Naive Bayes.", "labels": [], "entities": []}, {"text": "These properties could similarly be desirable in conversation modeling.", "labels": [], "entities": [{"text": "conversation modeling", "start_pos": 49, "end_pos": 70, "type": "TASK", "confidence": 0.8731246888637543}]}, {"text": "An email might contain a request, a question, and an answer to a previous questionthree distinct dialog acts within a single message.", "labels": [], "entities": []}, {"text": "This motivates the desire to allow a message to be a mixture of classes.", "labels": [], "entities": []}, {"text": "In this paper, we introduce anew type of model which combines the functionality of topic models, which posit latent class assignments to each individual token, with Markovian sequence models, which  govern the transitions between text blocks in a sequence.", "labels": [], "entities": []}, {"text": "We generalize the block HMM approach so that there is no longer a one-to-one correspondence between states in the Markov chain and latent discourse classes.", "labels": [], "entities": []}, {"text": "Instead, we allow a state in the HMM to correspond to a mixture of many classes: we refer to this family of models as mixed membership Markov models ).", "labels": [], "entities": []}, {"text": "Instead of defining explicit transition probabilities from one class to another as in a traditional HMM, we define the distribution over classes as a function of the entire histogram of class assignments of the previous text segment.", "labels": [], "entities": []}, {"text": "We define our model using the same number of parameters as a standard HMM ( \u00a72), and we present a straightforward approximate inference algorithm ( \u00a73).", "labels": [], "entities": []}, {"text": "While we introduce a general model, we will focus on the task of unsupervised conversation modeling.", "labels": [], "entities": [{"text": "conversation modeling", "start_pos": 78, "end_pos": 99, "type": "TASK", "confidence": 0.6964235156774521}]}, {"text": "Specifically, we build off the Bayesian block HMMs used by for modeling Twitter conversations, which will be our primary baseline.", "labels": [], "entities": []}, {"text": "After discussing related work ( \u00a74), we present experimental results on a set of Twitter conversations as well as a set of threads from CNET discussion forums ( \u00a75).", "labels": [], "entities": [{"text": "CNET discussion forums", "start_pos": 136, "end_pos": 158, "type": "DATASET", "confidence": 0.8943434953689575}]}, {"text": "We show that M 4 increases thread reconstruction accuracy by up to 15% compared to the HMM of, and we reduce variation of information against speech act annotations by an average of 18% from HMM and LDA baselines.", "labels": [], "entities": [{"text": "thread reconstruction", "start_pos": 27, "end_pos": 48, "type": "TASK", "confidence": 0.6743774712085724}, {"text": "accuracy", "start_pos": 49, "end_pos": 57, "type": "METRIC", "confidence": 0.9773005247116089}]}, {"text": "To the best of our knowledge, this work is the first attempt to quantitatively compare unsupervised models against gold standard speech act annotations.", "labels": [], "entities": []}], "datasetContent": [{"text": "We experiment with two corpora of text-based asynchronous conversations on the Web.", "labels": [], "entities": []}, {"text": "One of these is annotated with speech act labels, against which we compare our unsupervised clusters.", "labels": [], "entities": []}, {"text": "We measure the predictive capabilities of the model via perplexity experiments and the task of thread reconstruction.", "labels": [], "entities": [{"text": "thread reconstruction", "start_pos": 95, "end_pos": 116, "type": "TASK", "confidence": 0.7355868220329285}]}, {"text": "All of our results are averaged across four randomly initialized chains which are run for 5000 iterations, with five samples collected during the final 500 iterations.", "labels": [], "entities": []}, {"text": "We take small gradient steps of decreasing size with \u03b7(t) = 0.1/(1000 + t).", "labels": [], "entities": []}, {"text": "We set \u03c3 2 = 10.0 as the variance of the \u03bb weights.", "labels": [], "entities": []}, {"text": "We use optimized asymmetric priors as described in \u00a75.2, and we use asymmetric Dirichlet for the word distributions, following Metropolis-Hastings proposals: we add Gaussiandistributed noise to the log of the current \u03c9, then exponentiate this to yield the proposed \u03c9 (new) . This log-space proposal ensures that \u03c9 is always positive.", "labels": [], "entities": []}, {"text": "When computing the transition distributions for M 4 , we normalize the class histograms so that the counts to sum to 1.", "labels": [], "entities": []}, {"text": "This helps with numeric stability because the input vectors stay within a small bounded range.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Average perplexity of held-out data for various  numbers of latent classes.", "labels": [], "entities": []}, {"text": " Table 1. These results demonstrate the advan- tage of models with the mixed membership property.  Although LDA outperforms both sequence models,  this is be expected. Each block's topic distribution  is stochastically generated with LDA, whereas in the  two sequence models, the distribution over classes  is simply a deterministic function of the previous  block. This allows LDA to infer parameters that fit  the data more tightly. Comparing only the two se- quence models, we find that M 4 does significantly  better than BHMM in all cases with p < 0.05.", "labels": [], "entities": [{"text": "BHMM", "start_pos": 526, "end_pos": 530, "type": "METRIC", "confidence": 0.8182756900787354}]}]}