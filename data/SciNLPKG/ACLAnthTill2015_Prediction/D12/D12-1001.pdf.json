{"title": [{"text": "Syntactic Transfer Using a Bilingual Lexicon", "labels": [], "entities": [{"text": "Syntactic Transfer", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.9509206712245941}]}], "abstractContent": [{"text": "We consider the problem of using a bilingual dictionary to transfer lexico-syntactic information from a resource-rich source language to a resource-poor target language.", "labels": [], "entities": []}, {"text": "In contrast to past work that used bitexts to transfer analyses of specific sentences at the token level, we instead use features to transfer the behavior of words at a type level.", "labels": [], "entities": []}, {"text": "Ina dis-criminative dependency parsing framework, our approach produces gains across a range of target languages, using two different low-resource training methodologies (one weakly supervised and one indirectly supervised) and two different dictionary sources (one manually constructed and one automatically constructed).", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 20, "end_pos": 38, "type": "TASK", "confidence": 0.6985698938369751}]}], "introductionContent": [{"text": "Building a high-performing parser fora language with no existing treebank is still an open problem.", "labels": [], "entities": []}, {"text": "Methods that use no supervision at all () or small amounts of manual supervision () have been extensively studied, but still do not perform well enough to be deployed in practice.", "labels": [], "entities": []}, {"text": "Projection of dependency links across aligned bitexts ( gives better performance, but crucially depends on the existence of large, in-domain bitexts.", "labels": [], "entities": []}, {"text": "A more generally applicable class of methods exploits the notion of universal part of speech tags: Sentences in English and German both containing words that mean \"demand.\"", "labels": [], "entities": []}, {"text": "The fact that the English demand takes nouns on its left and right indicates that the German verlangen should do the same, correctly suggesting attachments to Verzicht and Gewerkschaften.", "labels": [], "entities": []}, {"text": "Petrov, 2011) to train parsers that can run on any language with no adaptation ) or unsupervised adaptation).", "labels": [], "entities": []}, {"text": "While these universal parsers currently constitute the highest-performing methods for languages without treebanks, they are inherently limited by operating at the coarse POS level, as lexical features are vital to supervised parsing models.", "labels": [], "entities": []}, {"text": "In this work, we consider augmenting delexicalized parsers by transferring syntactic information through a bilingual lexicon at the word type level.", "labels": [], "entities": []}, {"text": "These parsers are delexicalized in the sense that, although they receive target language words as input, their feature sets do not include indicators on those words.", "labels": [], "entities": []}, {"text": "This setting is appropriate when there is too little target language data to learn lexical features directly.", "labels": [], "entities": []}, {"text": "Our main approach is to add features which are lexical in the sense that they compute a function of specific target language words, but are still un-lexical in the sense that all lexical knowledge comes from the bilingual lexicon and training data in the source language.", "labels": [], "entities": []}, {"text": "Consider the example English and German sentences shown in, and suppose that we wish to parse the German side without access to a German treebank.", "labels": [], "entities": [{"text": "parse", "start_pos": 88, "end_pos": 93, "type": "TASK", "confidence": 0.9654586911201477}, {"text": "German treebank", "start_pos": 130, "end_pos": 145, "type": "DATASET", "confidence": 0.757398396730423}]}, {"text": "A delexicalized parser operating at the part of speech level does not have sufficient information to make the correct decision about, for example, the choice of subcategorization frame for the verb verlangen.", "labels": [], "entities": []}, {"text": "However, demand, a possible English translation of verlangen, takes a noun on its left and a noun on its right, an observation that in this case gives us the information we need.", "labels": [], "entities": [{"text": "verlangen", "start_pos": 51, "end_pos": 60, "type": "DATASET", "confidence": 0.6320865154266357}]}, {"text": "We can fire features in our German parser on the attachments of Gewerkschaften and Verzicht to verlangen indicating that similar-looking attachments are attested in English for an English translation of verlangen.", "labels": [], "entities": []}, {"text": "This allows us to exploit fine-grained lexical cues to make German parsing decisions even when we have little or no supervised German data; moreover, this syntactic transfer is possible even in spite of the fact that demand and verlangen are not observed in parallel context.", "labels": [], "entities": [{"text": "German parsing", "start_pos": 60, "end_pos": 74, "type": "TASK", "confidence": 0.49785590171813965}, {"text": "syntactic transfer", "start_pos": 155, "end_pos": 173, "type": "TASK", "confidence": 0.7179061621427536}]}, {"text": "Using type-level transfer through a dictionary in this way allows us to decouple the lexico-syntactic projection from the data conditions under which we are learning the parser.", "labels": [], "entities": []}, {"text": "After computing feature values using source language resources and a bilingual lexicon, our model can be trained very simply using any appropriate training method fora supervised parser.", "labels": [], "entities": []}, {"text": "Furthermore, because the transfer mechanism is just a set of features over word types, we are free to derive our bilingual lexicon either from bitext or from a manually-constructed dictionary, making our method strictly more general than those of, who rely centrally on bitext.", "labels": [], "entities": []}, {"text": "This flexibility is potentially useful for resource-poor languages, where a humancurated bilingual lexicon maybe broader in coverage or more robust to noise than a small, domainlimited bitext.", "labels": [], "entities": []}, {"text": "Of course, it is an empirical question whether transferring type level information about word behavior is effective; we show that, indeed, this method compares favorably with other transfer mechanisms used in past work.", "labels": [], "entities": []}, {"text": "The actual syntactic information that we transfer consists of purely monolingual lexical attachment statistics computed on an annotated source language resource.", "labels": [], "entities": []}, {"text": "1 While the idea of using large-scale summary statistics as parser features has been considered previously (), doing so in a projection setting is novel and forces us to design features suitable for projection through a bilingual lexicon.", "labels": [], "entities": []}, {"text": "Our features must also be flexible enough to provide benefit even in the presence of cross-lingual syntactic differences and noise introduced by the bilingual dictionary.", "labels": [], "entities": []}, {"text": "Under two different training conditions and with two different varieties of bilingual lexicons, we show that our method of lexico-syntactic projection does indeed improve the performance of parsers that would otherwise be agnostic to lexical information.", "labels": [], "entities": [{"text": "lexico-syntactic projection", "start_pos": 123, "end_pos": 150, "type": "TASK", "confidence": 0.752329558134079}]}, {"text": "In all settings, we see statistically significant gains fora range of languages, with our method providing up to 3% absolute improvement in unlabeled attachment score (UAS) and 11% relative error reduction.", "labels": [], "entities": [{"text": "unlabeled attachment score (UAS)", "start_pos": 140, "end_pos": 172, "type": "METRIC", "confidence": 0.819798563917478}, {"text": "relative error reduction", "start_pos": 181, "end_pos": 205, "type": "METRIC", "confidence": 0.852727472782135}]}], "datasetContent": [{"text": "We evaluate our method on a range of languages taken from the CoNLL shared tasks on multilingual dependency parsing ().", "labels": [], "entities": [{"text": "multilingual dependency parsing", "start_pos": 84, "end_pos": 115, "type": "TASK", "confidence": 0.5629468957583109}]}, {"text": "We make use of dependency treebanks for Danish, German, Greek, Spanish, Italian, Dutch, Portuguese, and Swedish, all from the 2006 shared task.", "labels": [], "entities": []}, {"text": "For our English resource, we use 500,000 English newswire sentences from English Gigaword version 3 (, parsed with the Berkeley Parser () and converted to a dependency treebank using the head rules of Collins (1999).", "labels": [], "entities": []}, {"text": "8 Our English test set (used in Section 3.4) consists of the first 300 sentences of section 23 of the Penn treebank (, preprocessed in the same way.", "labels": [], "entities": [{"text": "English test set", "start_pos": 6, "end_pos": 22, "type": "DATASET", "confidence": 0.8414304455121359}, {"text": "Penn treebank", "start_pos": 102, "end_pos": 115, "type": "DATASET", "confidence": 0.9665294587612152}]}, {"text": "Our model does not use gold finegrained POS tags, but we douse coarse POS tags deterministically generated from the provided gold fine-grained tags in the style of and improvements from their best methods of using bitext and lexical information.", "labels": [], "entities": []}, {"text": "These results are not directly comparable to ours, as indicated by * and **.", "labels": [], "entities": []}, {"text": "However, we still see that the performance of our type-level transfer method approaches that of bitext-based methods, which require complex bilingual training for each new language.", "labels": [], "entities": [{"text": "type-level transfer", "start_pos": 50, "end_pos": 69, "type": "TASK", "confidence": 0.7686023116111755}]}, {"text": "five iterations of the HMM aligner with agreement training.", "labels": [], "entities": []}, {"text": "Our lexicon is then read off based on relative frequency counts of aligned instances of each word in the bitext.", "labels": [], "entities": []}, {"text": "We also use our method on bilingual dictionaries constructed in a more conventional way.", "labels": [], "entities": []}, {"text": "For this purpose, we scrape our MANUAL bilingual lexicons from English Wiktionary (Wikimedia Foundation, 2012).", "labels": [], "entities": [{"text": "MANUAL bilingual lexicons from English Wiktionary (Wikimedia Foundation, 2012)", "start_pos": 32, "end_pos": 110, "type": "DATASET", "confidence": 0.6311387245853742}]}, {"text": "We mine entries for English words that explicitly have foreign translations listed as well as words in each target language that have English definitions.", "labels": [], "entities": []}, {"text": "We discard all translation entries where the English side is longer than one word, except for constructions of the form \"to VERB\", where we manually remove the \"to\" and allow the word to be defined as the English infinitive.", "labels": [], "entities": [{"text": "VERB", "start_pos": 124, "end_pos": 128, "type": "METRIC", "confidence": 0.9759677648544312}]}, {"text": "Finally, because our method requires a dictionary with probability weights, we assume that each target language word translates with uniform probability into any of the candidates that we scrape.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Evaluation of features derived from AUTOMATIC and MANUAL bilingual lexicons when trained on a con- catenation of non-target-language treebanks (the BANKS setting). Values reported are UAS for sentences of all lengths  in the standard CoNLL test sets, with punctuation removed from training and test sets. Daggers indicate statistical  significance computed using bootstrap resampling; a single dagger indicates p < 0.1 and a double dagger indicates  p < 0.05. We also include the baseline results of McDonald et al. (2011) and T\u00e4ckstr\u00f6m et al.", "labels": [], "entities": [{"text": "BANKS", "start_pos": 158, "end_pos": 163, "type": "METRIC", "confidence": 0.8559266328811646}, {"text": "UAS", "start_pos": 194, "end_pos": 197, "type": "METRIC", "confidence": 0.9942604303359985}, {"text": "CoNLL test sets", "start_pos": 244, "end_pos": 259, "type": "DATASET", "confidence": 0.9337431192398071}]}, {"text": " Table 2: Evaluation of features derived from AUTOMATIC and MANUAL bilingual lexicons when trained on various  small numbers of target language trees (the SEED setting). Values reported are UAS for sentences of all lengths on  our enlarged CoNLL test sets (see text); each value is based on 50 sampled training sets of the given size. Daggers  indicate statistical significance as described in the text. Statistical significance is not reported for averages.", "labels": [], "entities": [{"text": "SEED", "start_pos": 155, "end_pos": 159, "type": "METRIC", "confidence": 0.887541651725769}, {"text": "UAS", "start_pos": 190, "end_pos": 193, "type": "METRIC", "confidence": 0.9967359900474548}, {"text": "CoNLL test sets", "start_pos": 240, "end_pos": 255, "type": "DATASET", "confidence": 0.9389966130256653}, {"text": "Statistical significance", "start_pos": 404, "end_pos": 428, "type": "METRIC", "confidence": 0.940969705581665}]}, {"text": " Table 3: Lexicon statistics for all languages for both  sources of bilingual lexicons. \"Voc\" indicates vocabulary  size and \"OCC\" indicates open-class coverage, the frac- tion of open-class tokens in the test treebanks with entries  in our bilingual lexicon.", "labels": [], "entities": []}]}