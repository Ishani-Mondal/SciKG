{"title": [{"text": "A Weakly Supervised Model for Sentence-Level Semantic Orientation Analysis with Multiple Experts", "labels": [], "entities": [{"text": "Sentence-Level Semantic Orientation Analysis", "start_pos": 30, "end_pos": 74, "type": "TASK", "confidence": 0.8162341713905334}]}], "abstractContent": [{"text": "We propose the weakly supervised Multi-Experts Model (MEM) for analyzing the semantic orientation of opinions expressed in natural language reviews.", "labels": [], "entities": [{"text": "semantic orientation of opinions expressed in natural language reviews", "start_pos": 77, "end_pos": 147, "type": "TASK", "confidence": 0.8397296865781149}]}, {"text": "In contrast to most prior work, MEM predicts both opinion polarity and opinion strength at the level of individual sentences; such fine-grained analysis helps to understand better why users like or dislike the entity under review.", "labels": [], "entities": []}, {"text": "A key challenge in this setting is that it is hard to obtain sentence-level training data for both polarity and strength.", "labels": [], "entities": []}, {"text": "For this reason, MEM is weakly supervised: It starts with potentially noisy indicators obtained from coarse-grained training data (i.e., document-level ratings), a small set of diverse base predictors, and, if available, small amounts of fine-grained training data.", "labels": [], "entities": [{"text": "MEM", "start_pos": 17, "end_pos": 20, "type": "TASK", "confidence": 0.9834005236625671}]}, {"text": "We integrate these noisy indicators into a unified probabilistic framework using ideas from ensemble learning and graph-based semi-supervised learning.", "labels": [], "entities": []}, {"text": "Our experiments indicate that MEM outperforms state-of-the-art methods by a significant margin.", "labels": [], "entities": [{"text": "MEM", "start_pos": 30, "end_pos": 33, "type": "TASK", "confidence": 0.954954981803894}]}], "introductionContent": [{"text": "Opinion mining is concerned with analyzing opinions expressed in natural language text.", "labels": [], "entities": [{"text": "Opinion mining", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.8632799983024597}, {"text": "analyzing opinions expressed in natural language text", "start_pos": 33, "end_pos": 86, "type": "TASK", "confidence": 0.7214215568133763}]}, {"text": "For example, many internet websites allow their users to provide both natural language reviews and numerical ratings to items of interest (such as products or movies).", "labels": [], "entities": []}, {"text": "In this context, opinion mining aims to uncover the relationship between users and (features of) items.", "labels": [], "entities": [{"text": "opinion mining", "start_pos": 17, "end_pos": 31, "type": "TASK", "confidence": 0.7787089049816132}]}, {"text": "Preferences of users to items can be well understood by coarse-grained methods of opinion mining, which focus on analyzing the semantic orientation of documents as a whole.", "labels": [], "entities": [{"text": "opinion mining", "start_pos": 82, "end_pos": 96, "type": "TASK", "confidence": 0.7538891434669495}]}, {"text": "To understand why users like or dislike certain items, however, we need to perform more fine-grained analysis of the review text itself.", "labels": [], "entities": []}, {"text": "In this paper, we focus on sentence-level analysis of semantic orientation (SO) in online reviews.", "labels": [], "entities": [{"text": "sentence-level analysis of semantic orientation (SO)", "start_pos": 27, "end_pos": 79, "type": "TASK", "confidence": 0.8451295755803585}]}, {"text": "The SO consists of polarity (positive, negative, or other 1 ) and strength (degree to which a sentence is positive or negative).", "labels": [], "entities": [{"text": "SO", "start_pos": 4, "end_pos": 6, "type": "TASK", "confidence": 0.9026297926902771}, {"text": "strength", "start_pos": 66, "end_pos": 74, "type": "METRIC", "confidence": 0.9650830626487732}]}, {"text": "Both quantities can be analyzed jointly by mapping them to numerical ratings: Large negative/positive ratings indicate a strong negative/positive orientation.", "labels": [], "entities": []}, {"text": "A key challenge in finegrained rating prediction is that fine-grained training data for both polarity and strength is hard to obtain.", "labels": [], "entities": [{"text": "finegrained rating prediction", "start_pos": 19, "end_pos": 48, "type": "TASK", "confidence": 0.7392627596855164}, {"text": "strength", "start_pos": 106, "end_pos": 114, "type": "METRIC", "confidence": 0.9874699711799622}]}, {"text": "We thus focus on a weakly supervised setting in which only coarse-level training data (such as document ratings and subjectivity lexicons) and, optionally, a small amount of fine-grained training data (such as sentence polarities) is available.", "labels": [], "entities": []}, {"text": "A number of lexicon-based approaches for phraselevel rating prediction has been proposed in the literature.", "labels": [], "entities": [{"text": "phraselevel rating prediction", "start_pos": 41, "end_pos": 70, "type": "TASK", "confidence": 0.8835330406824747}]}, {"text": "These methods utilize a subjectivity lexicon of words along with information about their semantic orientation; they focus on phrases that contain words from the lexicon.", "labels": [], "entities": []}, {"text": "A key advantage of sentence-level methods is that they are able to coverall sentences in a review and that phrase identification is avoided.", "labels": [], "entities": [{"text": "phrase identification", "start_pos": 107, "end_pos": 128, "type": "TASK", "confidence": 0.7736515998840332}]}, {"text": "To the best of our knowledge, the problem of rating prediction at the sentence level has not been addressed in the literature.", "labels": [], "entities": [{"text": "rating prediction", "start_pos": 45, "end_pos": 62, "type": "TASK", "confidence": 0.9543910622596741}]}, {"text": "A naive approach would be to simply average phrase-level ratings.", "labels": [], "entities": []}, {"text": "Such an approach performs poorly, however, since (1) phrases are analyzed out of context (e.g., modal verbs or conditional clauses), (2) domain-dependent information about semantic orientation is not captured in the lexicons, (3) only phrases that contain lexicon words are covered.", "labels": [], "entities": []}, {"text": "Here (1) and (2) lead to low precision, (3) to low recall.", "labels": [], "entities": [{"text": "precision", "start_pos": 29, "end_pos": 38, "type": "METRIC", "confidence": 0.9988018274307251}, {"text": "recall", "start_pos": 51, "end_pos": 57, "type": "METRIC", "confidence": 0.9989256262779236}]}, {"text": "To address the challenges outlined above, we propose the weakly supervised Multi-Experts Model (MEM) for sentence-level rating prediction.", "labels": [], "entities": [{"text": "sentence-level rating prediction", "start_pos": 105, "end_pos": 137, "type": "TASK", "confidence": 0.7361610531806946}]}, {"text": "MEM starts with a set of potentially noisy indicators of SO including phrase-level predictions, language heuristics, and co-occurrence counts.", "labels": [], "entities": [{"text": "SO", "start_pos": 57, "end_pos": 59, "type": "TASK", "confidence": 0.981872022151947}]}, {"text": "We refer to these indicators as base predictors; they constitute the set of experts used in our model.", "labels": [], "entities": []}, {"text": "MEM is designed such that new base predictors can be easily integrated.", "labels": [], "entities": []}, {"text": "Since the information provided by the base predictors can be contradicting, we use ideas from ensemble learning) to learn the most confident indicators and to exploit domain-dependent information revealed by document ratings.", "labels": [], "entities": []}, {"text": "Thus, instead of averaging base predictors, MEM integrates their features along with the available coarse-grained training data into a unified probabilistic model.", "labels": [], "entities": []}, {"text": "The integrated model can be regarded as a Gaussian process (GP) model) with a novel multi-expert prior.", "labels": [], "entities": []}, {"text": "The multi-expert prior decomposes into two component distributions.", "labels": [], "entities": []}, {"text": "The first component distribution integrates sentence-local information obtained from the base predictors.", "labels": [], "entities": []}, {"text": "It forms a special realization of stacking () but uses the features from the base predictors instead of the actual predictions.", "labels": [], "entities": []}, {"text": "The second component distribution propagates SO information across similar sentences using techniques from graphbased semi-supervised learning (GSSL) ().", "labels": [], "entities": [{"text": "SO information", "start_pos": 45, "end_pos": 59, "type": "TASK", "confidence": 0.8895414173603058}]}, {"text": "It aims to improve the predictions on sentences that are not covered well enough by our base predictors.", "labels": [], "entities": []}, {"text": "Traditional GSSL algorithms support either discrete labels (classification) or numerical labels (regression); we extend these techniques to support both types of labels simultaneously.", "labels": [], "entities": []}, {"text": "We use a novel variant of word sequence kernels () to measure sentence similarity.", "labels": [], "entities": []}, {"text": "Our kernel takes the relative positions of words but also their SO and synonymity into account.", "labels": [], "entities": []}, {"text": "Our experiments indicate that MEM significantly outperforms prior work in both sentence-level rating prediction and sentence-level polarity classification.", "labels": [], "entities": [{"text": "MEM", "start_pos": 30, "end_pos": 33, "type": "TASK", "confidence": 0.9546070098876953}, {"text": "sentence-level rating prediction", "start_pos": 79, "end_pos": 111, "type": "TASK", "confidence": 0.6985380947589874}, {"text": "sentence-level polarity classification", "start_pos": 116, "end_pos": 154, "type": "TASK", "confidence": 0.7274370094140371}]}], "datasetContent": [{"text": "We evaluated both MEM and a number of alternative approaches for both sentence-level polarity classification and sentence-level strength prediction across a number of domains.", "labels": [], "entities": [{"text": "MEM", "start_pos": 18, "end_pos": 21, "type": "TASK", "confidence": 0.8297053575515747}, {"text": "sentence-level polarity classification", "start_pos": 70, "end_pos": 108, "type": "TASK", "confidence": 0.7466342051823934}, {"text": "sentence-level strength prediction", "start_pos": 113, "end_pos": 147, "type": "TASK", "confidence": 0.6453318397204081}]}, {"text": "We found that MEM outperforms state-of-the-art approaches by a significant margin.", "labels": [], "entities": [{"text": "MEM", "start_pos": 14, "end_pos": 17, "type": "TASK", "confidence": 0.9458795785903931}]}, {"text": "We implemented MEM as well as the HCRF classifier of, which is the best-performing estimator of sentence-level polarity in the weaklysupervised setting reported in the literature.", "labels": [], "entities": []}, {"text": "We train both methods using (1) only coarse labels (MEMCoarse, HCRF-Coarse) and additionally a small number of sentence polarities (MEM-Fine, HCRFFine 5 ).", "labels": [], "entities": []}, {"text": "We also implemented a number of baselines for both polarity classification and strength prediction: a document oracle (DocOracle) that simply uses the document label for each sentence, the BoO rating predictor (Base BoO ), and the SO-CAL rating predictor (Base SO-CAL ).", "labels": [], "entities": [{"text": "polarity classification", "start_pos": 51, "end_pos": 74, "type": "TASK", "confidence": 0.7179211676120758}, {"text": "strength prediction", "start_pos": 79, "end_pos": 98, "type": "TASK", "confidence": 0.7003709673881531}, {"text": "BoO rating predictor (Base BoO )", "start_pos": 189, "end_pos": 221, "type": "METRIC", "confidence": 0.902821523802621}]}, {"text": "For polarity classification, we compare our methods also to the statistical polarity predictor (Base polarity ).", "labels": [], "entities": [{"text": "polarity classification", "start_pos": 4, "end_pos": 27, "type": "TASK", "confidence": 0.802048534154892}]}, {"text": "To judge on the effectiveness of our multi-export prior for combining base predictors, we take the majority vote of all base predictors and document polarity as an additional baseline (Majority-Vote).", "labels": [], "entities": []}, {"text": "Similarly, for strength prediction, we take the arithmetic mean of the document rating and the phrase-level predictions of Base BoO and Base SO-CAL as a baseline (Mean-Rating).", "labels": [], "entities": [{"text": "strength prediction", "start_pos": 15, "end_pos": 34, "type": "TASK", "confidence": 0.6880166381597519}, {"text": "Base BoO", "start_pos": 123, "end_pos": 131, "type": "METRIC", "confidence": 0.8576697111129761}]}, {"text": "We use the same hyperparameter setting for MEM across all our experiments.", "labels": [], "entities": [{"text": "MEM", "start_pos": 43, "end_pos": 46, "type": "TASK", "confidence": 0.9629353880882263}]}, {"text": "We evaluated all methods on Amazon reviews from different domains using the corpus of and the test set of.", "labels": [], "entities": []}, {"text": "For each domain, we constructed a large balanced dataset by randomly sampling 33,000 reviews from the corpus of.", "labels": [], "entities": []}, {"text": "We chose the books, electronics, and music domains for our experiments; the dvd domain was used for development.", "labels": [], "entities": []}, {"text": "For sentence polarity classification, we use the test set of, which contains roughly 60 reviews per domain (20 for each polarity).", "labels": [], "entities": [{"text": "sentence polarity classification", "start_pos": 4, "end_pos": 36, "type": "TASK", "confidence": 0.8314204017321268}]}, {"text": "For strength evaluation, we created a test set of 300 pairs of sentences per domain from the polarity test set.", "labels": [], "entities": []}, {"text": "Each pair consisted of two sentences of the same polarity; we manually determined which of the sentences is more positive.", "labels": [], "entities": []}, {"text": "We chose this pairwise approach because (1) we wanted the evaluation to be invariant to the scale of the predicted ratings, and (2) it much easier for human annotators to rank a pair of sentences than to rank a large collection of sentences.", "labels": [], "entities": []}, {"text": "We followed T\u00e4ckstr\u00f6m and McDonald (2011b) and used 3-fold cross-validation, where each fold consisted of a set of roughly 20 documents from the test set.", "labels": [], "entities": []}, {"text": "In each fold, we merged the test set with the reviews from the corresponding domain.", "labels": [], "entities": []}, {"text": "For MEMFine and HCRF-Fine, we use the data from the other two folds as fine-grained polarity annotations.", "labels": [], "entities": [{"text": "MEMFine", "start_pos": 4, "end_pos": 11, "type": "DATASET", "confidence": 0.5315704941749573}, {"text": "HCRF-Fine", "start_pos": 16, "end_pos": 25, "type": "DATASET", "confidence": 0.9250102639198303}]}, {"text": "For our experiments on polarity classification, we converted the predicted ratings of MEM, Base BoO , and Base SO-CAL into polarities by the method described in Sec.", "labels": [], "entities": [{"text": "polarity classification", "start_pos": 23, "end_pos": 46, "type": "TASK", "confidence": 0.8324666917324066}, {"text": "MEM", "start_pos": 86, "end_pos": 89, "type": "METRIC", "confidence": 0.7941948175430298}, {"text": "Base BoO", "start_pos": 91, "end_pos": 99, "type": "METRIC", "confidence": 0.8896860480308533}]}, {"text": "We compare the performance of each method in terms of accuracy, which is defined as the fraction of correct predictions on the test set (correct label for polarity / correct ranking for strength).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 54, "end_pos": 62, "type": "METRIC", "confidence": 0.9995144605636597}]}, {"text": "All reported numbers are averages over the three folds.", "labels": [], "entities": []}, {"text": "In our tables, boldface numbers are statistically significant against all other methods (t-test, p-value 0.05).", "labels": [], "entities": []}, {"text": "summarizes the results of our experiments for sentence polarity classification.", "labels": [], "entities": [{"text": "sentence polarity classification", "start_pos": 46, "end_pos": 78, "type": "TASK", "confidence": 0.8371519247690836}]}, {"text": "The base predictors perform poorly across all domains, mainly due to the aforementioned problems associated with averaging phrase-level predictions.", "labels": [], "entities": []}, {"text": "In fact, DocOracle performs almost always better than any of the base predictors.", "labels": [], "entities": [{"text": "DocOracle", "start_pos": 9, "end_pos": 18, "type": "DATASET", "confidence": 0.8536116480827332}]}, {"text": "However, accurracy increases when we combine base predictors and DocOracle using majority voting, which indicates that ensemble methods work well.", "labels": [], "entities": [{"text": "accurracy", "start_pos": 9, "end_pos": 18, "type": "METRIC", "confidence": 0.9985272884368896}, {"text": "DocOracle", "start_pos": 65, "end_pos": 74, "type": "DATASET", "confidence": 0.8915454149246216}]}], "tableCaptions": [{"text": " Table 1: Accuracy of polarity classification per do- main and averaged across domains.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9960578680038452}, {"text": "polarity classification", "start_pos": 22, "end_pos": 45, "type": "TASK", "confidence": 0.6766534447669983}]}, {"text": " Table 3: Accuracy of strength prediction.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9973688125610352}, {"text": "strength prediction", "start_pos": 22, "end_pos": 41, "type": "TASK", "confidence": 0.6816265732049942}]}]}