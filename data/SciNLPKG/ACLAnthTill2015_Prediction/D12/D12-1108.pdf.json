{"title": [{"text": "Document-Wide Decoding for Phrase-Based Statistical Machine Translation", "labels": [], "entities": [{"text": "Phrase-Based Statistical Machine Translation", "start_pos": 27, "end_pos": 71, "type": "TASK", "confidence": 0.74297034740448}]}], "abstractContent": [{"text": "Independence between sentences is an assumption deeply entrenched in the models and algorithms used for statistical machine translation (SMT), particularly in the popular dynamic programming beam search decoding algorithm.", "labels": [], "entities": [{"text": "statistical machine translation (SMT)", "start_pos": 104, "end_pos": 141, "type": "TASK", "confidence": 0.8196439246336619}, {"text": "dynamic programming beam search decoding algorithm", "start_pos": 171, "end_pos": 221, "type": "TASK", "confidence": 0.6943850815296173}]}, {"text": "This restriction is an obstacle to research on more sophisticated discourse-level models for SMT.", "labels": [], "entities": [{"text": "SMT", "start_pos": 93, "end_pos": 96, "type": "TASK", "confidence": 0.9944427609443665}]}, {"text": "We propose a stochastic local search decoding method for phrase-based SMT, which permits free document-wide dependencies in the models.", "labels": [], "entities": [{"text": "SMT", "start_pos": 70, "end_pos": 73, "type": "TASK", "confidence": 0.695317268371582}]}, {"text": "We explore the stability and the search parameters of this method and demonstrate that it can be successfully used to optimise a document-level semantic language model.", "labels": [], "entities": []}, {"text": "1 Motivation In the field of translation studies, it is undisputed that discourse-wide context must be considered carefully for good translation results (Hatim and Mason, 1990).", "labels": [], "entities": [{"text": "translation studies", "start_pos": 29, "end_pos": 48, "type": "TASK", "confidence": 0.9773590564727783}]}, {"text": "By contrast, the state of the art in statistical machine translation (SMT), despite significant advances in the last twenty years, still assumes that texts can be translated sentence by sentence under strict independence assumptions, even though it is well known that certain linguistic phenomena such as pronominal anaphora cannot be translated correctly without referring to extra-sentential context.", "labels": [], "entities": [{"text": "statistical machine translation (SMT)", "start_pos": 37, "end_pos": 74, "type": "TASK", "confidence": 0.7948935925960541}]}, {"text": "This is true both for the phrase-based and the syntax-based approach to SMT.", "labels": [], "entities": [{"text": "SMT", "start_pos": 72, "end_pos": 75, "type": "TASK", "confidence": 0.9927369356155396}]}, {"text": "In the rest of this paper, we shall concentrate on phrase-based SMT.", "labels": [], "entities": [{"text": "phrase-based SMT", "start_pos": 51, "end_pos": 67, "type": "TASK", "confidence": 0.6185943484306335}]}, {"text": "One reason why it is difficult to experiment with document-wide models for phrase-based SMT is that the dynamic programming (DP) algorithm which has been used almost exclusively for decoding SMT models in the recent literature has very strong assumptions of locality built into it.", "labels": [], "entities": [{"text": "SMT", "start_pos": 88, "end_pos": 91, "type": "TASK", "confidence": 0.7076902389526367}, {"text": "SMT", "start_pos": 191, "end_pos": 194, "type": "TASK", "confidence": 0.8076857328414917}]}, {"text": "DP beam search for phrase-based SMT was described by Koehn et al.", "labels": [], "entities": [{"text": "DP beam search", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.6948232650756836}, {"text": "SMT", "start_pos": 32, "end_pos": 35, "type": "TASK", "confidence": 0.7373956441879272}]}, {"text": "(2003), extending earlier work on word-based SMT (Tillmann et al., 1997; Och et al., 2001; Tillmann and Ney, 2003).", "labels": [], "entities": [{"text": "SMT", "start_pos": 45, "end_pos": 48, "type": "TASK", "confidence": 0.6801760196685791}]}, {"text": "This algorithm constructs output sentences by starting with an empty hypothesis and adding output words at the end until translations for all source words have been generated.", "labels": [], "entities": []}, {"text": "The core models of phrase-based SMT, in particular the n-gram language model (LM), only depend on a constant number of output words to the left of the word being generated.", "labels": [], "entities": [{"text": "SMT", "start_pos": 32, "end_pos": 35, "type": "TASK", "confidence": 0.6699632406234741}]}, {"text": "This fact is exploited by the search algorithm with a DP technique called hypothesis recombination (Och et al., 2001), which permits the elimination of hypotheses from the search space if they coincide in a certain number of final words with a better hypothesis and no future expansion can possibly invert the relative ranking of the two hypotheses under the given models.", "labels": [], "entities": []}, {"text": "Hypothesis recombination achieves a substantial reduction of the search space without affecting search optimal-ity and makes it possible to use aggressive pruning techniques for fast search while still obtaining good results.", "labels": [], "entities": []}, {"text": "The downside of this otherwise excellent approach is that it only works well with models that have a local dependency structure similar to that of an n-gram language model, so they only depend on a small context window for each target word.", "labels": [], "entities": []}, {"text": "Sentence-local models with longer dependencies can be added, but doing so greatly increases the risk for search errors by inhibiting hypothesis recombination.", "labels": [], "entities": []}, {"text": "Cross-sentence dependencies cannot be directly integrated into DP SMT decoding in 1179", "labels": [], "entities": [{"text": "DP SMT decoding", "start_pos": 63, "end_pos": 78, "type": "TASK", "confidence": 0.5789443155129751}, {"text": "1179", "start_pos": 82, "end_pos": 86, "type": "DATASET", "confidence": 0.8001308441162109}]}], "introductionContent": [], "datasetContent": [{"text": "In this section, we present the results of a series of experiments with our document decoder.", "labels": [], "entities": []}, {"text": "The goal of our experiments is to demonstrate the behaviour of the decoder and characterise its response to changes in the fundamental search parameters.", "labels": [], "entities": []}, {"text": "The SMT models for our experiments were created with a subset of the training data for the English-French shared task at the WMT 2011 workshop).", "labels": [], "entities": [{"text": "SMT", "start_pos": 4, "end_pos": 7, "type": "TASK", "confidence": 0.9897010326385498}, {"text": "WMT 2011 workshop", "start_pos": 125, "end_pos": 142, "type": "DATASET", "confidence": 0.8303717772165934}]}, {"text": "The phrase table was trained on Europarl, news-commentary and UN data.", "labels": [], "entities": [{"text": "Europarl", "start_pos": 32, "end_pos": 40, "type": "DATASET", "confidence": 0.9874920845031738}, {"text": "UN data", "start_pos": 62, "end_pos": 69, "type": "DATASET", "confidence": 0.8126225173473358}]}, {"text": "To reduce the training data to a manageable size, singleton phrase pairs were removed before the phrase scoring step.", "labels": [], "entities": [{"text": "phrase scoring", "start_pos": 97, "end_pos": 111, "type": "TASK", "confidence": 0.7285400331020355}]}, {"text": "Significance-based filtering () was applied to the resulting phrase table.", "labels": [], "entities": [{"text": "Significance-based filtering", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.6783452183008194}]}, {"text": "The language model was a 5-gram model with Kneser-Ney smoothing trained on the monolingual News corpus with IRSTLM.", "labels": [], "entities": [{"text": "News corpus", "start_pos": 91, "end_pos": 102, "type": "DATASET", "confidence": 0.8716713488101959}, {"text": "IRSTLM", "start_pos": 108, "end_pos": 114, "type": "DATASET", "confidence": 0.9271591901779175}]}, {"text": "Feature weights were trained with Minimum Error-Rate Training (MERT) on the news-test2008 development set using the DP beam search decoder and the MERT implementation of the Moses toolkit (.", "labels": [], "entities": [{"text": "Minimum Error-Rate Training (MERT)", "start_pos": 34, "end_pos": 68, "type": "METRIC", "confidence": 0.7036138872305552}, {"text": "news-test2008 development set", "start_pos": 76, "end_pos": 105, "type": "DATASET", "confidence": 0.9392211039861044}]}, {"text": "Experimental results are reported for the newstest2009 test set, a corpus of 111 newswire documents totalling 2,525 sentences or 65,595 English input tokens.", "labels": [], "entities": [{"text": "newstest2009 test set", "start_pos": 42, "end_pos": 63, "type": "DATASET", "confidence": 0.8931523958841959}]}], "tableCaptions": [{"text": " Table 1: Experimental results with a cross-sentence semantic language model", "labels": [], "entities": []}]}