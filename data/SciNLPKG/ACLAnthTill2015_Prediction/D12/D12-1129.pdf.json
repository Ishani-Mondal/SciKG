{"title": [{"text": "A New Minimally-Supervised Framework for Domain Word Sense Disambiguation", "labels": [], "entities": [{"text": "Domain Word Sense Disambiguation", "start_pos": 41, "end_pos": 73, "type": "TASK", "confidence": 0.8014324307441711}]}], "abstractContent": [{"text": "We present anew minimally-supervised framework for performing domain-driven Word Sense Disambiguation (WSD).", "labels": [], "entities": [{"text": "Word Sense Disambiguation (WSD)", "start_pos": 76, "end_pos": 107, "type": "TASK", "confidence": 0.7743502805630366}]}, {"text": "Glossaries for several domains are iteratively acquired from the Web by means of a boot-strapping technique.", "labels": [], "entities": []}, {"text": "The acquired glosses are then used as the sense inventory for fully-unsupervised domain WSD.", "labels": [], "entities": []}, {"text": "Our experiments, on new and gold-standard datasets, show that our wide-coverage framework enables high-performance results on dozens of domains at a coarse and fine-grained level.", "labels": [], "entities": []}], "introductionContent": [{"text": "Domain information pervades most of the text we read everyday.", "labels": [], "entities": []}, {"text": "If we just think of the Web, the vast majority of its textual content is domain oriented.", "labels": [], "entities": []}, {"text": "A casein point is Wikipedia, which provides encyclopedic coverage fora huge number of knowledge domains), but most blogs, Web sites and newspapers also provide a great deal of information focused on specific areas of knowledge.", "labels": [], "entities": [{"text": "Wikipedia", "start_pos": 18, "end_pos": 27, "type": "DATASET", "confidence": 0.923375129699707}]}, {"text": "When it comes to automatic text understanding, then, it is crucial to take into account the domain specificity of apiece of text, so as to perform a focused and as-precise-as-possible analysis which, in its turn, can enable domain-aware applications such as question answering and information extraction.", "labels": [], "entities": [{"text": "automatic text understanding", "start_pos": 17, "end_pos": 45, "type": "TASK", "confidence": 0.6143835286299387}, {"text": "question answering", "start_pos": 258, "end_pos": 276, "type": "TASK", "confidence": 0.9387115836143494}, {"text": "information extraction", "start_pos": 281, "end_pos": 303, "type": "TASK", "confidence": 0.8367330729961395}]}, {"text": "Domain knowledge also has the potential to improve open-text applications such as summarization) and machine translation).", "labels": [], "entities": [{"text": "summarization", "start_pos": 82, "end_pos": 95, "type": "TASK", "confidence": 0.9856370091438293}, {"text": "machine translation", "start_pos": 101, "end_pos": 120, "type": "TASK", "confidence": 0.8181531429290771}]}, {"text": "Research in Word Sense Disambiguation, WSD), the task aimed at the automatic labeling of text with word senses, has been oriented towards domain text understanding for several years now.", "labels": [], "entities": [{"text": "Word Sense Disambiguation, WSD)", "start_pos": 12, "end_pos": 43, "type": "TASK", "confidence": 0.7468468397855759}, {"text": "automatic labeling of text with word senses", "start_pos": 67, "end_pos": 110, "type": "TASK", "confidence": 0.7416221967765263}, {"text": "domain text understanding", "start_pos": 138, "end_pos": 163, "type": "TASK", "confidence": 0.6413745979468027}]}, {"text": "Many approaches have been devised, including the identification of domain-specific predominant senses, the development of domain resources (), their application to WSD ( ), and the effective use of link analysis algorithms such as Personalized PageRank ().", "labels": [], "entities": [{"text": "identification of domain-specific predominant senses", "start_pos": 49, "end_pos": 101, "type": "TASK", "confidence": 0.7704301357269288}, {"text": "WSD", "start_pos": 164, "end_pos": 167, "type": "TASK", "confidence": 0.6902475953102112}]}, {"text": "More recently, semi-supervised approaches to domain WSD have been proposed which aim at decreasing the amount of supervision needed to carryout the task (.", "labels": [], "entities": []}, {"text": "High-performance domain WSD, however, has been hampered by the widespread use of a generalpurpose sense inventory, i.e.,.", "labels": [], "entities": [{"text": "WSD", "start_pos": 24, "end_pos": 27, "type": "TASK", "confidence": 0.8458243012428284}]}, {"text": "Unfortunately WordNet does not contain many specialized terms, making it difficult to use it in work on arbitrary specialized domains.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 14, "end_pos": 21, "type": "DATASET", "confidence": 0.9559233784675598}]}, {"text": "While Wikipedia has recently been considered a valid alternative, it is mainly focused on covering named entities and, strictly speaking, does not contain a formal widecoverage sense inventory (not even in disambiguation pages, which are often incomplete, especially in the lexicographic sense).", "labels": [], "entities": []}, {"text": "In this paper we provide three main contributions: \u2022 We tackle the above issues by introducing anew framework based on the minimallysupervised acquisition of specialized glossaries for dozens of domains.", "labels": [], "entities": []}, {"text": "\u2022 In turn, we use the acquired domain glossaries as a sense inventory for domain WSD.", "labels": [], "entities": []}, {"text": "As a result, we redefine the domain WSD task as one of picking out the most appropriate gloss (finegrained setting) or domain (coarse-grained setting) from a multi-domain glossary.", "labels": [], "entities": []}, {"text": "\u2022 We show that our framework represents a considerable departure from the common usage of a general-purpose sense inventory such as WordNet, in that, thanks to the wide coverage of domain meanings, it enables highperformance unsupervised WSD on many domains in the range of 69-80% F1.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 132, "end_pos": 139, "type": "DATASET", "confidence": 0.9417608976364136}, {"text": "F1", "start_pos": 281, "end_pos": 283, "type": "METRIC", "confidence": 0.9978899359703064}]}, {"text": "Furthermore, our approach can be customized to any set of domains of interest, and new senses, i.e., glosses, can be added at anytime (either manually or automatically) to the multi-domain sense inventory.", "labels": [], "entities": []}], "datasetContent": [{"text": "A dataset for 30 domains We used the Gigaword corpus () to extract a 6-paragraph text snippet for each of the 30 domains.", "labels": [], "entities": [{"text": "Gigaword corpus", "start_pos": 37, "end_pos": 52, "type": "DATASET", "confidence": 0.9578284919261932}]}, {"text": "As a result, we obtained a domain dataset made up of 180 paragraphs to which we applied tokenization, lemmatization and compounding, totaling 1432 domain content words overall (47.7 content words per domain on average).", "labels": [], "entities": []}, {"text": "The average polysemy of the words in the dataset was of 9.7 glosses and 4.4 domains per word.", "labels": [], "entities": []}, {"text": "Each content word was manually tagged with the most suitable glosses from our multi-domain glossary (3.9 glosses, i.e., senses per word were assigned on average).", "labels": [], "entities": []}, {"text": "The annotation task was performed by two annotators with adjudication.", "labels": [], "entities": []}, {"text": "Sports and Finance We also experimented with the gold standard produced by.", "labels": [], "entities": []}, {"text": "The dataset covers two domains: SPORTS and FI-NANCE.", "labels": [], "entities": [{"text": "SPORTS", "start_pos": 32, "end_pos": 38, "type": "METRIC", "confidence": 0.5098196864128113}, {"text": "FI-NANCE", "start_pos": 43, "end_pos": 51, "type": "METRIC", "confidence": 0.9285366535186768}]}, {"text": "The dataset comprises 41 ambiguous words (with an average polysemy of 6.7 senses), many of which express different meanings in the two domains.", "labels": [], "entities": []}, {"text": "In each domain, and for each word, around 100 sentences were sense-annotated with WordNet.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 82, "end_pos": 89, "type": "DATASET", "confidence": 0.9866336584091187}]}, {"text": "Environment Finally, we also carried out an experiment on the ENVIRONMENT dataset from the Semeval-2010 domain WSD task ().", "labels": [], "entities": [{"text": "ENVIRONMENT dataset from the Semeval-2010 domain WSD task", "start_pos": 62, "end_pos": 119, "type": "DATASET", "confidence": 0.8399553447961807}]}, {"text": "The dataset includes 1,398 content words (of which 1,032 content nouns) tagged with WordNet senses.", "labels": [], "entities": []}, {"text": "30 domains We ran our WSD systems and the baselines on our 30-domain dataset, on a sentenceby-sentence basis.", "labels": [], "entities": []}, {"text": "We calculated results at the two levels of granularity enabled by our WSD framework: a coarse-grained setting where systems output the most appropriate domain label for each word item to be disambiguated; a fine-grained setting where systems are required to output the most suitable gloss for the input word.", "labels": [], "entities": []}, {"text": "The results are shown in.", "labels": [], "entities": []}, {"text": "Domain PPR outperforms Vanilla PPR by some points in precision, recall and F1 in both the coarse-grained and the fine-grained setting, achieving an F1 around 80% and 69%, respectively (differences in recall performance are statistically significant using a \u03c7 2 test).", "labels": [], "entities": [{"text": "precision", "start_pos": 53, "end_pos": 62, "type": "METRIC", "confidence": 0.9996651411056519}, {"text": "recall", "start_pos": 64, "end_pos": 70, "type": "METRIC", "confidence": 0.9996277093887329}, {"text": "F1", "start_pos": 75, "end_pos": 77, "type": "METRIC", "confidence": 0.9995224475860596}, {"text": "F1", "start_pos": 148, "end_pos": 150, "type": "METRIC", "confidence": 0.9975157976150513}, {"text": "recall", "start_pos": 200, "end_pos": 206, "type": "METRIC", "confidence": 0.9973644614219666}]}, {"text": "The predominant domain baseline, available only in the coarse-grained setting, lags behind Domain PPR by more than 3 points in precision and 2 in recall.", "labels": [], "entities": [{"text": "precision", "start_pos": 127, "end_pos": 136, "type": "METRIC", "confidence": 0.9994219541549683}, {"text": "recall", "start_pos": 146, "end_pos": 152, "type": "METRIC", "confidence": 0.9983416795730591}]}, {"text": "While these differences are not statistically significant, the variance across domains is much higher, thus suggesting lower reliability of the method.", "labels": [], "entities": [{"text": "reliability", "start_pos": 125, "end_pos": 136, "type": "METRIC", "confidence": 0.9774043560028076}]}, {"text": "These results were obtained in a fully unsupervised setting in which no structured knowledge was provided, unlike previous applications of PPR to WSD (  which relied on the underlying WordNet graph, a manually created resource.", "labels": [], "entities": [{"text": "WordNet graph", "start_pos": 184, "end_pos": 197, "type": "DATASET", "confidence": 0.9262098968029022}]}, {"text": "Furthermore, our graph contains \"noisy\" semantic relations, as we connect each gloss to all the senses of its gloss words (cf. Section 3.2.2).", "labels": [], "entities": []}, {"text": "Finally, we note that the results shown in could never have been obtained with WordNet.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 79, "end_pos": 86, "type": "DATASET", "confidence": 0.9865322709083557}]}, {"text": "In fact, drawing on our domain mapping, we calculated that the correct domain sense is not in WordNet for about 68% of the words in the dataset.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 94, "end_pos": 101, "type": "DATASET", "confidence": 0.9788668751716614}]}, {"text": "Instead, the results in show that our framework enables high-performance unsupervised: Performance results on the 30-domain dataset ( \u2020 differences between the two systems are statistically significant using a \u03c7 2 test, p < 0.05).", "labels": [], "entities": []}, {"text": "WSD thanks to the wide coverage of domain meanings.", "labels": [], "entities": [{"text": "WSD", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.7929487228393555}]}, {"text": "As regards the random baseline, this performs 42.5% and 44.1% in the two settings.", "labels": [], "entities": []}, {"text": "Despite the higher polysemy of glosses (9.7 glosses vs. 4.4 domains per word in the dataset), the performance is higher in the fine-grained setting because often there is more than one gloss covering the same meaning of a domain word.", "labels": [], "entities": []}, {"text": "Sports, Finance and Environment For the SPORTS, FINANCE and ENVIRONMENT datasets (cf. Section 4.3) we did not have gloss-based sense annotations, so we could not perform a fine-grained evaluation.", "labels": [], "entities": [{"text": "FINANCE", "start_pos": 48, "end_pos": 55, "type": "METRIC", "confidence": 0.9769018292427063}, {"text": "ENVIRONMENT datasets", "start_pos": 60, "end_pos": 80, "type": "DATASET", "confidence": 0.7880570590496063}]}, {"text": "Therefore, we first studied the different systems at a coarse level on the basis of the domain distribution of the senses returned for the word items in the dataset.", "labels": [], "entities": []}, {"text": "We show the 3 most frequent domain labels for each system and each dataset in.", "labels": [], "entities": []}, {"text": "The figure seems to confirm our results showing Domain PPR as being more robust than its Vanilla version.", "labels": [], "entities": []}, {"text": "Next, to get a more accurate evaluation, we randomly sampled 200 sentences from each dataset and manually validated the coarse-grained senses, i.e., domain assignments, output by each system on this set of sentences.", "labels": [], "entities": []}, {"text": "We remark that several words in the datasets did not pertain to the domain of interest.", "labels": [], "entities": []}, {"text": "For instance, will and share do not have any sports sense in WordNet, while the same applies to half and chip for the business domain.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 61, "end_pos": 68, "type": "DATASET", "confidence": 0.9770209193229675}]}, {"text": "shows the results of our validation, where a domain output by a system was considered correct if a suitable gloss existed for that domain in our inventory.", "labels": [], "entities": []}, {"text": "The results show that our framework enables coarse-grained recall in the 70-80% ballpark even on difficult gold standard datasets for which finegrained recall with WordNet struggles to surpass the 50-60% range.", "labels": [], "entities": [{"text": "recall", "start_pos": 59, "end_pos": 65, "type": "METRIC", "confidence": 0.9184337854385376}, {"text": "recall", "start_pos": 152, "end_pos": 158, "type": "METRIC", "confidence": 0.9082494974136353}, {"text": "WordNet", "start_pos": 164, "end_pos": 171, "type": "DATASET", "confidence": 0.9593657851219177}]}, {"text": "For instance, the best performance  on the ENVIRONMENT dataset was around 60% recall () using a semi-supervised WSD system, trained on the domain.", "labels": [], "entities": [{"text": "ENVIRONMENT dataset", "start_pos": 43, "end_pos": 62, "type": "DATASET", "confidence": 0.9223040640354156}, {"text": "recall", "start_pos": 78, "end_pos": 84, "type": "METRIC", "confidence": 0.9996078610420227}]}, {"text": "Similarly, both the FINANCE and SPORTS datasets are notoriously difficult gold standards on which state-of-the-art recall using WordNet is lower than 60%.", "labels": [], "entities": [{"text": "FINANCE", "start_pos": 20, "end_pos": 27, "type": "METRIC", "confidence": 0.5056723952293396}, {"text": "SPORTS datasets", "start_pos": 32, "end_pos": 47, "type": "DATASET", "confidence": 0.7826432287693024}, {"text": "recall", "start_pos": 115, "end_pos": 121, "type": "METRIC", "confidence": 0.9980362057685852}, {"text": "WordNet", "start_pos": 128, "end_pos": 135, "type": "DATASET", "confidence": 0.9511789679527283}]}, {"text": "Interestingly, the predominant domain baseline shows a bias towards BUSINESS, thus performing best on the FINANCE dataset.", "labels": [], "entities": [{"text": "BUSINESS", "start_pos": 68, "end_pos": 76, "type": "METRIC", "confidence": 0.9384387135505676}, {"text": "FINANCE dataset", "start_pos": 106, "end_pos": 121, "type": "DATASET", "confidence": 0.7911230325698853}]}, {"text": "This is because of the large number of terms covered in our domain glossary, and consequently the high overlap with cue words in context.", "labels": [], "entities": []}, {"text": "On the other two domains, we observe performance inline with our 30-domain experiment.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Examples of extracted terms, glosses and hypernyms (seeds are in bold, domain terms are underlined).", "labels": [], "entities": []}, {"text": " Table 4: Hypernymy relation seeds used to bootstrap glossary acquisition in four of the 30 domains.", "labels": [], "entities": [{"text": "bootstrap glossary acquisition", "start_pos": 43, "end_pos": 73, "type": "TASK", "confidence": 0.7127719223499298}]}, {"text": " Table 5: Statistics on the multi-domain acquired glossary.", "labels": [], "entities": []}, {"text": " Table 6: Performance results on the 30-domain dataset  (  \u2020 differences between the two systems are statistically  significant using a \u03c7 2 test, p < 0.05).", "labels": [], "entities": [{"text": "30-domain dataset", "start_pos": 37, "end_pos": 54, "type": "DATASET", "confidence": 0.6848024725914001}]}, {"text": " Table 7: Coarse-grained performance results on gold-standard domain datasets.", "labels": [], "entities": [{"text": "gold-standard domain datasets", "start_pos": 48, "end_pos": 77, "type": "DATASET", "confidence": 0.6975297530492147}]}]}