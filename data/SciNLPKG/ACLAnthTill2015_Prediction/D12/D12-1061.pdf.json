{"title": [{"text": "Learning Lexicon Models from Search Logs for Query Expansion", "labels": [], "entities": []}], "abstractContent": [{"text": "This paper explores log-based query expansion (QE) models for Web search.", "labels": [], "entities": [{"text": "log-based query expansion (QE)", "start_pos": 20, "end_pos": 50, "type": "TASK", "confidence": 0.7632143000761668}]}, {"text": "Three lexicon models are proposed to bridge the lexical gap between Web documents and user queries.", "labels": [], "entities": []}, {"text": "These models are trained on pairs of user queries and titles of clicked documents.", "labels": [], "entities": []}, {"text": "Evaluations on areal world data set show that the lexicon models, integrated into a ranker-based QE system, not only significantly improve the document retrieval performance but also outperform two state-of-the-art log-based QE methods.", "labels": [], "entities": []}], "introductionContent": [{"text": "Term mismatch is a fundamental problem in Web search, where queries and documents are composed using different vocabularies and language styles.", "labels": [], "entities": [{"text": "Term mismatch", "start_pos": 0, "end_pos": 13, "type": "TASK", "confidence": 0.9284948408603668}]}, {"text": "Query expansion (QE) is an effective strategy to address the problem.", "labels": [], "entities": [{"text": "Query expansion (QE)", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.8132056653499603}]}, {"text": "It expands a query issued by a user with additional related terms, called expansion terms, so that more relevant documents can be retrieved.", "labels": [], "entities": []}, {"text": "In this paper we explore the use of clickthrough data and translation models for QE.", "labels": [], "entities": [{"text": "QE", "start_pos": 81, "end_pos": 83, "type": "TASK", "confidence": 0.9167637228965759}]}, {"text": "We select expansion terms fora query according to how likely it is that the expansion terms occur in the title of a document that is relevant to the query.", "labels": [], "entities": []}, {"text": "Assuming that a query is parallel to the titles of documents clicked for that query (), three lexicon models are trained on query-title pairs extracted from clickthrough data.", "labels": [], "entities": []}, {"text": "The first is a word model that learns the translation probability between single words.", "labels": [], "entities": []}, {"text": "The second model uses lexicalized triplets to incorporate word dependencies for translation.", "labels": [], "entities": [{"text": "translation", "start_pos": 80, "end_pos": 91, "type": "TASK", "confidence": 0.9609677791595459}]}, {"text": "The third is a bilingual topic model, which represents a query as a distribution of hidden topics and learns the translation between a query and a title term at the semantic level.", "labels": [], "entities": []}, {"text": "We will show that the word model provides a rich set of expansion candidates while the triplet and topic models can effectively select good expansion terms, and that a ranker-based QE system which incorporates all three of these models not only significantly improves Web search result but outperforms other log-based QE methods that are stateof-the-art.", "labels": [], "entities": []}, {"text": "There is growing interest in applying user logs to improve QE.", "labels": [], "entities": [{"text": "QE", "start_pos": 59, "end_pos": 61, "type": "TASK", "confidence": 0.8969089388847351}]}, {"text": "A recent survey is due to BaezeYates and.", "labels": [], "entities": [{"text": "BaezeYates", "start_pos": 26, "end_pos": 36, "type": "DATASET", "confidence": 0.9508891105651855}]}, {"text": "Below, we briefly discuss two log-based QE methods that are closest to ours and are re-implemented in this study for comparison.", "labels": [], "entities": []}, {"text": "Both systems use the same type of log data that we used to train the lexicon models.", "labels": [], "entities": []}, {"text": "The term correlation model of is to our knowledge the first to explore querydocument relations for direct extraction of expansion terms for Web search.", "labels": [], "entities": []}, {"text": "The method outperforms traditional QE methods that do not use log data e.g. the local analysis model of.", "labels": [], "entities": []}, {"text": "In addition, as pointed out by there are three important advantages that make log-based QE a promising technology to improve the performance of commercial search engines.", "labels": [], "entities": []}, {"text": "First, unlike traditional QE methods that are based on relevance feedback, log-based QE derives expansion terms from search logs, allowing term correlations to be pre-computed offline.", "labels": [], "entities": []}, {"text": "Compared to methods that are based on thesauri either compiled manually) or derived au-tomatically from document collections, the log-based method is superior in that it explicitly captures the correlation between query terms and document terms, and thus can bridge the lexical gap between them more effectively.", "labels": [], "entities": []}, {"text": "Second, since search logs retrain querydocument pairs clicked by millions of users, the term correlations reflect the preference of the majority of users.", "labels": [], "entities": []}, {"text": "Third, the term correlations evolve along with the accumulation of user logs, thus can reflect updated user interests at a specific time.", "labels": [], "entities": []}, {"text": "However, as pointed out by, Cui et al.'s correlation-based method suffers low precision of QE partly because the correlation model does not explicitly capture context information and is susceptible to noise.", "labels": [], "entities": [{"text": "precision", "start_pos": 78, "end_pos": 87, "type": "METRIC", "confidence": 0.9989633560180664}, {"text": "QE", "start_pos": 91, "end_pos": 93, "type": "METRIC", "confidence": 0.8780425786972046}]}, {"text": "Riezler et al. developed a QE system by retraining a standard phrase-based statistical machine translation (SMT) system using query-snippet pairs extracted from clickthrough data (.", "labels": [], "entities": [{"text": "QE", "start_pos": 27, "end_pos": 29, "type": "TASK", "confidence": 0.9394624829292297}, {"text": "phrase-based statistical machine translation (SMT)", "start_pos": 62, "end_pos": 112, "type": "TASK", "confidence": 0.7581484232630048}]}, {"text": "The SMT-based system can produce cleaner, more relevant expansion terms because rich context information useful for filtering noisy expansions is captured by combining language model and phrase translation model in its decoder.", "labels": [], "entities": [{"text": "SMT-based", "start_pos": 4, "end_pos": 13, "type": "TASK", "confidence": 0.982955813407898}, {"text": "phrase translation", "start_pos": 187, "end_pos": 205, "type": "TASK", "confidence": 0.75965815782547}]}, {"text": "Furthermore, in the SMT system all component models are properly smoothed using sophisticated techniques to avoid sparse data problems while the correlation model relies on pure counts of term frequencies.", "labels": [], "entities": [{"text": "SMT", "start_pos": 20, "end_pos": 23, "type": "TASK", "confidence": 0.991967499256134}]}, {"text": "However, the SMT system is used as a black box in their experiments.", "labels": [], "entities": [{"text": "SMT", "start_pos": 13, "end_pos": 16, "type": "TASK", "confidence": 0.9821127653121948}]}, {"text": "So the relative contribution of different SMT components is not verified empirically.", "labels": [], "entities": [{"text": "SMT", "start_pos": 42, "end_pos": 45, "type": "TASK", "confidence": 0.9898343086242676}]}, {"text": "In this study we break this black box in order to build a better, simpler QE system.", "labels": [], "entities": []}, {"text": "We will show that the proposed lexicon models outperform significantly the term correlation model, and that a simpler QE system that incorporates the lexicon models can beat the sophisticated, black-box SMT system.", "labels": [], "entities": [{"text": "SMT", "start_pos": 203, "end_pos": 206, "type": "TASK", "confidence": 0.9237608909606934}]}], "datasetContent": [{"text": "We evaluate the performance of a QE method by first issuing a set of queries which are expanded using the method to a search engine and then measuring the Web search performance.", "labels": [], "entities": []}, {"text": "Better QE methods are supposed to lead to better Web search results using the correspondingly expanded query set.", "labels": [], "entities": []}, {"text": "Due to the characteristics of our QE methods, we cannot conduct experiments on standard test collections such as the TREC data because they do not contain related user logs we need.", "labels": [], "entities": [{"text": "TREC data", "start_pos": 117, "end_pos": 126, "type": "DATASET", "confidence": 0.856670081615448}]}, {"text": "Therefore, following previous studies of log-based QE (e.g.,), we use the proprietary datasets that have been developed for building a commercial search engine, and demonstrate the effectiveness of our methods by comparing them against previous state-of-the-art logbased QE methods.", "labels": [], "entities": []}, {"text": "The relevance judgment set consists of 4,000 multi-term English queries.", "labels": [], "entities": []}, {"text": "On average, each query is associated with 197 Web documents (URLs).", "labels": [], "entities": []}, {"text": "Each query-URL pair has a relevance label.", "labels": [], "entities": []}, {"text": "The label is human generated and is on a 5-level relevance scale, 0 to 4, with 4 meaning document Dis the most relevant to query Q and 0 meaning Dis not relevant to Q.", "labels": [], "entities": []}, {"text": "The relevance judgment set is constructed as follows.", "labels": [], "entities": []}, {"text": "First, the queries are sampled from a year of search engine logs.", "labels": [], "entities": []}, {"text": "Adult, spam, and bot queries are all removed.", "labels": [], "entities": []}, {"text": "Queries are \"de-duped\" so that only unique queries remain.", "labels": [], "entities": []}, {"text": "To reflect a natural query distribution, we do not try to control the quality of these queries.", "labels": [], "entities": []}, {"text": "For example, in our query sets, there are roughly 20% misspelled queries, 20% navigational queries, and 10% transactional queries.", "labels": [], "entities": []}, {"text": "Second, for each query, we collect Web documents to be judged by issuing the query to several popular search engines (e.g., Google, Bing) and fetching retrieval results from each.", "labels": [], "entities": []}, {"text": "Finally, the query-document pairs are judged by a group of well-trained assessors.", "labels": [], "entities": []}, {"text": "In this study all the queries are preprocessed as follows.", "labels": [], "entities": []}, {"text": "The text is whitespace tokenized and lowercased, numbers are retained, and no stemming/inflection treatment is performed.", "labels": [], "entities": []}, {"text": "We split the judgment set into two nonoverlapping datasets, namely training and test sets, respectively.", "labels": [], "entities": []}, {"text": "Each dataset contains 2,000 queries.", "labels": [], "entities": []}, {"text": "The query-title pairs used for model training are extracted from one year of query log files using a procedure similar to.", "labels": [], "entities": []}, {"text": "In our experiments we used a randomly sampled subset of 20,692,219 pairs that do not overlap the queries and documents in the test set.", "labels": [], "entities": []}, {"text": "Our Web document collection consists of approximately 2.5 billion Web pages.", "labels": [], "entities": []}, {"text": "In the retrieval experiments we use the index based on the content fields (i.e., body and title text) of each Web page.", "labels": [], "entities": []}, {"text": "The Web search performance is evaluated by mean NDCG.", "labels": [], "entities": [{"text": "NDCG", "start_pos": 48, "end_pos": 52, "type": "DATASET", "confidence": 0.5382581949234009}]}, {"text": "We report NDCG scores at truncation levels of 1, 3, and 10.", "labels": [], "entities": []}, {"text": "We also perform a significance test using the paired t-test.", "labels": [], "entities": [{"text": "significance", "start_pos": 18, "end_pos": 30, "type": "METRIC", "confidence": 0.9411154389381409}]}, {"text": "Differences are considered statistically significant when pvalue is less than 0.05.", "labels": [], "entities": [{"text": "pvalue", "start_pos": 58, "end_pos": 64, "type": "METRIC", "confidence": 0.9830326437950134}]}, {"text": "shows the main document ranking results using different QE systems, developed and evaluated using the datasets described above.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Ranking results using BM25 with different  query expansion systems. The superscripts  and  indicate statistically significant improvements", "labels": [], "entities": [{"text": "BM25", "start_pos": 32, "end_pos": 36, "type": "DATASET", "confidence": 0.8988174200057983}]}]}