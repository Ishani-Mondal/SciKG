{"title": [], "abstractContent": [{"text": "A popular tradition of studying semantic representation has been driven by the assumption that word meaning can be learned from the linguistic environment, despite ample evidence suggesting that language is grounded in perception and action.", "labels": [], "entities": []}, {"text": "In this paper we present a comparative study of models that represent word meaning based on linguistic and perceptual data.", "labels": [], "entities": []}, {"text": "Linguistic information is approximated by naturally occurring corpora and sensorimotor experience by feature norms (i.e., attributes native speakers consider important in describing the meaning of a word).", "labels": [], "entities": []}, {"text": "The models differ in terms of the mechanisms by which they integrate the two modalities.", "labels": [], "entities": []}, {"text": "Experimental results show that a closer correspondence to human data can be obtained by uncovering latent information shared among the textual and perceptual modalities rather than arriving at semantic knowledge by con-catenating the two.", "labels": [], "entities": []}], "introductionContent": [{"text": "Distributional models of lexical semantics have seen considerable success at accounting fora wide range of behavioral data in tasks involving semantic cognition).", "labels": [], "entities": []}, {"text": "These models have also enjoyed lasting popularity in natural language processing.", "labels": [], "entities": [{"text": "natural language processing", "start_pos": 53, "end_pos": 80, "type": "TASK", "confidence": 0.6305026710033417}]}, {"text": "Examples involve information retrieval (, word sense discrimination), text segmentation (, and numerous studies of lexicon acquisition.", "labels": [], "entities": [{"text": "information retrieval", "start_pos": 17, "end_pos": 38, "type": "TASK", "confidence": 0.7919237911701202}, {"text": "word sense discrimination)", "start_pos": 42, "end_pos": 68, "type": "TASK", "confidence": 0.7580130100250244}, {"text": "text segmentation", "start_pos": 70, "end_pos": 87, "type": "TASK", "confidence": 0.7887799143791199}]}, {"text": "Despite their widespread use, distributional models have been criticized as \"disembodied\" in that they learn exclusively from linguistic information but are not grounded in perception and action).", "labels": [], "entities": []}, {"text": "This lack of grounding contrasts with many experimental studies suggesting that word meaning is acquired not only from exposure to the linguistic environment but also from our interaction with the physical world ().", "labels": [], "entities": []}, {"text": "Beyond language acquisition, there is considerable evidence across both behavioral experiments and neuroimaging studies that the perceptual associates of words play an important role in language processing (for a review see).", "labels": [], "entities": [{"text": "language acquisition", "start_pos": 7, "end_pos": 27, "type": "TASK", "confidence": 0.7214402556419373}]}, {"text": "It is thus no surprise that recent years have witnessed the emergence of perceptually grounded distributional models.", "labels": [], "entities": []}, {"text": "An important question in the formulation of such models concerns the provenance of perceptual information.", "labels": [], "entities": []}, {"text": "A few models use feature norms as a proxy for sensorimotor experience).", "labels": [], "entities": []}, {"text": "These are obtained by asking native speakers to write down attributes they consider important in describing the meaning of a word.", "labels": [], "entities": []}, {"text": "The attributes represent perceived physical and functional properties associated with the referents of words.", "labels": [], "entities": []}, {"text": "For example, apples are typically green or red, round, shiny, smooth, crunchy, tasty, and soon; dogs have four legs and bark, whereas chairs are used for sitting.", "labels": [], "entities": []}, {"text": "Other models focus solely on the visual modality under the assumption that it represents a major source of data from which humans can learn semantic representations of both linguistic and non-linguistic communicative actions.", "labels": [], "entities": []}, {"text": "For example, learn semantic representations from corpora of texts paired with naturally co-occurring images (e.g., news articles and their associated pictures), whereas learn textual and visual representations independently from distinct data sources.", "labels": [], "entities": []}, {"text": "Aside from the type of data used to capture perceptual information, another important issue concerns how the two modalities (perceptual and textual) are integrated.", "labels": [], "entities": []}, {"text": "A simple solution would be to learn both modalities independently) or to infer one modality by means of the other and to arrive at a grounded representation simply by concatenating the two.", "labels": [], "entities": []}, {"text": "An alternative is to learn from both modalities jointly (.", "labels": [], "entities": []}, {"text": "According to this view, semantic knowledge is gained by simultaneously learning from the statistical structure within each modality assuming both data sources have been generated by a shared set of meanings or topics.", "labels": [], "entities": []}, {"text": "In this paper we undertake the first comparative study of perceptually grounded distributional models.", "labels": [], "entities": []}, {"text": "We examine three models with different assumptions regarding the integration of perceptual and linguistic data.", "labels": [], "entities": []}, {"text": "The first model, originally proposed by, is an extension of latent Dirichlet allocation (LDA,).", "labels": [], "entities": []}, {"text": "It simultaneously considers the distribution of words across contexts in a text corpus and the distribution of words across perceptual features and extracts joint information from both data sources.", "labels": [], "entities": []}, {"text": "Our second model is based on Johns and Jones (2012) who represent the meaning of a word as the concatenation of its textual and its perceptual vector.", "labels": [], "entities": []}, {"text": "Interestingly, their model allows to infer a perceptual vector for words without feature norms, simply by taking into account similar words for which perceptual information is available.", "labels": [], "entities": []}, {"text": "Finally, we propose Canonical Correlation Analysis) as our third model.", "labels": [], "entities": [{"text": "Canonical Correlation Analysis", "start_pos": 20, "end_pos": 50, "type": "TASK", "confidence": 0.7725291053454081}]}, {"text": "CCA is a data analysis and dimensionality reduction method similar to PCA.", "labels": [], "entities": [{"text": "dimensionality reduction", "start_pos": 27, "end_pos": 51, "type": "TASK", "confidence": 0.8448968827724457}]}, {"text": "While PCA deals with only one data space, CCA is a technique for joint dimensionality reduction across two Features (or more) spaces that provide heterogeneous representations of the same objects.", "labels": [], "entities": [{"text": "joint dimensionality reduction", "start_pos": 65, "end_pos": 95, "type": "TASK", "confidence": 0.5971647103627523}]}, {"text": "The assumption is that the representations in these two spaces contain some joint information that is reflected in correlations between them.", "labels": [], "entities": []}, {"text": "In all three models we use feature norms as a proxy for perceptual information.", "labels": [], "entities": []}, {"text": "Despite their shortcomings (e.g., they often cover a small fraction of the vocabulary of an adult speaker due to the effort involved in eliciting them), feature norms provide detailed knowledge about meaning representations and area useful starting point for studying the integration of perceptual and textual information without being susceptible to the effects of noise, e.g., coming from image processing.", "labels": [], "entities": []}, {"text": "In other words, feature norms can serve as an upper bound of what can be achieved when integrating detailed perceptual information with vanilla text-based distributional models.", "labels": [], "entities": []}, {"text": "Our experimental results demonstrate that joint models give a better fit to human word similarity and association data than a model that considers only one data source, or the simple concatenation of the two sources.", "labels": [], "entities": []}], "datasetContent": [{"text": "Our word similarity experiments used the WordSimilarity-353 test collection () 2 which consists of relatedness judgments for word pairs.", "labels": [], "entities": [{"text": "WordSimilarity-353 test collection", "start_pos": 41, "end_pos": 75, "type": "DATASET", "confidence": 0.9531310796737671}]}, {"text": "For each pair, a similarity judgment (on a scale of 0 to 10) was elicited from 13 or 16 human subjects (e.g., tiger-cat are very similar, whereas delay-racism are not).", "labels": [], "entities": []}, {"text": "The average rating for each pair represents an estimate of the perceived similarity of the two words.", "labels": [], "entities": []}, {"text": "The task varies slightly from word association.", "labels": [], "entities": [{"text": "word association", "start_pos": 30, "end_pos": 46, "type": "TASK", "confidence": 0.7162273526191711}]}, {"text": "Here, participants are asked to rate perceived similarity rather than to generate the first word that came to mind in response to a cue word.", "labels": [], "entities": []}, {"text": "The collection contains similarity ratings for 353 word pairs.", "labels": [], "entities": []}, {"text": "Of these, 76 pairs appeared in our corpus and 3 in norms.", "labels": [], "entities": []}, {"text": "Again, we evaluated how well model produced similarities correlate with human ratings.", "labels": [], "entities": []}, {"text": "Throughout this paper we report correlation coefficients using Pearson's r.", "labels": [], "entities": [{"text": "correlation", "start_pos": 32, "end_pos": 43, "type": "METRIC", "confidence": 0.9558465480804443}, {"text": "Pearson's r", "start_pos": 63, "end_pos": 74, "type": "METRIC", "confidence": 0.694531629482905}]}, {"text": "Our third task assessed the models' ability to infer perceptual vectors for words that have none.", "labels": [], "entities": []}, {"text": "To do this, we conducted 10-fold cross-validation on norms.", "labels": [], "entities": []}, {"text": "We treated the perceptual vectors in each test fold as unseen, and used the data in the corresponding training fold together with the models presented in Section 2 to infer them.", "labels": [], "entities": []}, {"text": "Then, for each word, we examined how close the inferred vector was to the actual one, via correlation analysis.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Feature norms for the nouns table, dog, and  apple shown as a distribution.", "labels": [], "entities": []}, {"text": " Table 3: Performance of the feature-topic, global simi- larityand CCA models on the Nelson et al. (1998) norms  (entire dataset). All correlation coefficients are statisti- cally significant (p < 0.01).", "labels": [], "entities": []}, {"text": " Table 4: Mean correlation coefficients between origi- nal and inferred feature vectors in McRae et al.'s (2005)  norms.", "labels": [], "entities": [{"text": "Mean correlation", "start_pos": 10, "end_pos": 26, "type": "METRIC", "confidence": 0.9368196725845337}, {"text": "McRae et al.'s (2005)  norms", "start_pos": 91, "end_pos": 119, "type": "DATASET", "confidence": 0.8975852330525717}]}, {"text": " Table 5: Model performance on predicting word similar- ity. All correlation coefficients are statistically significant  (p < 0.01), except for the global similarity model.", "labels": [], "entities": [{"text": "predicting word similar- ity", "start_pos": 31, "end_pos": 59, "type": "TASK", "confidence": 0.8547147750854492}]}]}