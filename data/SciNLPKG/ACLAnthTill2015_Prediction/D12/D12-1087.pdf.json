{"title": [{"text": "Exploring Topic Coherence over many models and many topics", "labels": [], "entities": [{"text": "Exploring Topic Coherence", "start_pos": 0, "end_pos": 25, "type": "TASK", "confidence": 0.747784028450648}]}], "abstractContent": [{"text": "We apply two new automated semantic evaluations to three distinct latent topic models.", "labels": [], "entities": []}, {"text": "Both metrics have been shown to align with human evaluations and provide a balance between internal measures of information gain and comparisons to human ratings of coherent topics.", "labels": [], "entities": []}, {"text": "We improve upon the measures by introducing new aggregate measures that allows for comparing complete topic models.", "labels": [], "entities": []}, {"text": "We further compare the automated measures to other metrics for topic models, comparison to manually crafted semantic tests and document classification.", "labels": [], "entities": [{"text": "document classification", "start_pos": 127, "end_pos": 150, "type": "TASK", "confidence": 0.7650786936283112}]}, {"text": "Our experiments reveal that LDA and LSA each have different strengths; LDA best learns descriptive topics while LSA is best at creating a compact semantic representation of documents and words in a corpus.", "labels": [], "entities": []}], "introductionContent": [{"text": "Topic models learn bags of related words from large corpora without any supervision.", "labels": [], "entities": []}, {"text": "Based on the words used within a document, they mine topic level relations by assuming that a single document covers a small set of concise topics.", "labels": [], "entities": []}, {"text": "Once learned, these topics often correlate well with human concepts.", "labels": [], "entities": []}, {"text": "For example, one model might produce topics that cover ideas such as government affairs, sports, and movies.", "labels": [], "entities": []}, {"text": "With these unsupervised methods, we can extract useful semantic information in a variety of tasks that depend on identifying unique topics or concepts, such as distributional semantics, word sense induction (Van de Cruys and, and information retrieval ().", "labels": [], "entities": [{"text": "word sense induction", "start_pos": 186, "end_pos": 206, "type": "TASK", "confidence": 0.7221919298171997}, {"text": "information retrieval", "start_pos": 230, "end_pos": 251, "type": "TASK", "confidence": 0.8258606195449829}]}, {"text": "When using a topic model, we are primarily concerned with the degree to which the learned topics match human judgments and help us differentiate between ideas.", "labels": [], "entities": []}, {"text": "But until recently, the evaluation of these models has been ad hoc and application specific.", "labels": [], "entities": []}, {"text": "Evaluations have ranged from fully automated intrinsic evaluations to manually crafted extrinsic evaluations.", "labels": [], "entities": []}, {"text": "Previous extrinsic evaluations have used the learned topics to compactly represent a small fixed vocabulary and compared this distributional space to human judgments of similarity.", "labels": [], "entities": []}, {"text": "But these evaluations are hand constructed and often costly to perform for domain-specific topics.", "labels": [], "entities": []}, {"text": "Conversely, intrinsic measures have evaluated the amount of information encoded by the topics, where perplexity is one common example(), however, found that these intrinsic measures do not always correlate with semantically interpretable topics.", "labels": [], "entities": []}, {"text": "Furthermore, few evaluations have used the same metrics to compare distinct approaches such as Latent Dirichlet Allocation (LDA) (, Latent Semantic Analysis (LSA), and Non-negative Matrix Factorization (NMF)).", "labels": [], "entities": [{"text": "Latent Dirichlet Allocation (LDA)", "start_pos": 95, "end_pos": 128, "type": "METRIC", "confidence": 0.8903156518936157}]}, {"text": "This has made it difficult to know which method is most useful fora given application, or in terms of extracting useful topics.", "labels": [], "entities": []}, {"text": "We now provide a comprehensive and automated evaluation of these three distinct models (LDA, LSA, NMF), for automatically learning semantic topics.", "labels": [], "entities": []}, {"text": "While these models have seen significant improvements, they still represent the core differences between each approach to modeling topics.", "labels": [], "entities": []}, {"text": "For our evaluation, we use two recent automated coherence measures originally designed for LDA that bridge the gap between comparisons to human judgments and intrinsic measures such as perplexity.", "labels": [], "entities": []}, {"text": "We consider several key questions: 1.", "labels": [], "entities": []}, {"text": "How many topics should be learned?", "labels": [], "entities": []}, {"text": "2. How many learned topics are useful?", "labels": [], "entities": []}, {"text": "3. How do these topics relate to often used semantic tests?", "labels": [], "entities": []}, {"text": "4. How well do these topics identify similar documents?", "labels": [], "entities": []}, {"text": "We begin by summarizing the three topic models and highlighting their key differences.", "labels": [], "entities": []}, {"text": "We then describe the two metrics.", "labels": [], "entities": []}, {"text": "Afterwards, we focus on a series of experiments that address our four key questions and finally conclude with some overall remarks.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluate the quality of our three topic models (LDA, SVD, and NMF) with three experiments.", "labels": [], "entities": []}, {"text": "We focus first on evaluating aggregate coherence methods fora complete topic model and consider the differences between each model as we learn an increasing number of topics.", "labels": [], "entities": []}, {"text": "Secondly, we compare coherence scores to previous semantic evaluations.", "labels": [], "entities": []}, {"text": "Lastly, we use the learned topics in a classification task and evaluate whether or not coherent topics are equally informative when discriminating between documents.", "labels": [], "entities": []}, {"text": "For all our experiments, we trained our models on 92,600 New York Times articles from 2003.", "labels": [], "entities": []}, {"text": "For all articles, we removed stop words and any words occurring less than 200 times in the corpus, which left 35,836 unique tokens.", "labels": [], "entities": []}, {"text": "All documents were tokenized with OpenNLP's MaxEnt 4 tokenizer.", "labels": [], "entities": [{"text": "OpenNLP's MaxEnt 4 tokenizer", "start_pos": 34, "end_pos": 62, "type": "DATASET", "confidence": 0.8631298184394837}]}, {"text": "For the UCI measure, we compute the PMI between words using a 20 word sliding window passed over the WaCkypedia corpus (.", "labels": [], "entities": [{"text": "PMI", "start_pos": 36, "end_pos": 39, "type": "METRIC", "confidence": 0.8137288689613342}, {"text": "WaCkypedia corpus", "start_pos": 101, "end_pos": 118, "type": "DATASET", "confidence": 0.9688305556774139}]}, {"text": "In all experiments, we compute the coherence with the top 10 words from each topic that had the highest weight, in terms of LDA and NMF this corresponds with a high probability of the term describing the topic but for SVD there is no clear semantic interpretation.", "labels": [], "entities": [{"text": "LDA", "start_pos": 124, "end_pos": 127, "type": "METRIC", "confidence": 0.9584584832191467}, {"text": "NMF", "start_pos": 132, "end_pos": 135, "type": "METRIC", "confidence": 0.8459329605102539}]}], "tableCaptions": [{"text": " Table 1: Top 10 words from several high and low quality topics when ordered by the UCI Coherence  Measure. Topic labels were chosen in an ad hoc manner only to briefly summarize the topic's focus.", "labels": [], "entities": [{"text": "UCI Coherence  Measure", "start_pos": 84, "end_pos": 106, "type": "DATASET", "confidence": 0.9438514510790507}]}, {"text": " Table 2. We then represent each document using  columns in the topic by document matrix H learned  for each topic model.", "labels": [], "entities": []}, {"text": " Table 2: Section label counts for New York Times  articles used for classification", "labels": [], "entities": [{"text": "classification", "start_pos": 69, "end_pos": 83, "type": "TASK", "confidence": 0.849480152130127}]}]}