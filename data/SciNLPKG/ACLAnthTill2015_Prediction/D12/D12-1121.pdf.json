{"title": [{"text": "Unambiguity Regularization for Unsupervised Learning of Probabilistic Grammars", "labels": [], "entities": [{"text": "Unambiguity Regularization", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.7806904911994934}]}], "abstractContent": [{"text": "We introduce a novel approach named unam-biguity regularization for unsupervised learning of probabilistic natural language grammars.", "labels": [], "entities": [{"text": "unam-biguity regularization", "start_pos": 36, "end_pos": 63, "type": "TASK", "confidence": 0.6246991902589798}]}, {"text": "The approach is based on the observation that natural language is remarkably unam-biguous in the sense that only a tiny portion of the large number of possible parses of a natural language sentence are syntactically valid.", "labels": [], "entities": []}, {"text": "We incorporate an inductive bias into grammar learning in favor of grammars that lead to unambiguous parses on natural language sentences.", "labels": [], "entities": []}, {"text": "The resulting family of algorithms includes the expectation-maximization algorithm (EM) and its variant, Viterbi EM, as well as a so-called softmax-EM algorithm.", "labels": [], "entities": []}, {"text": "The softmax-EM algorithm can be implemented with a simple and computationally efficient extension to standard EM.", "labels": [], "entities": []}, {"text": "In our experiments of unsupervised dependency grammar learning , we show that unambiguity regularization is beneficial to learning, and in combination with annealing (of the regularization strength) and sparsity priors it leads to improvement over the current state of the art.", "labels": [], "entities": []}], "introductionContent": [{"text": "Machine learning offers a potentially powerful approach to learning probabilistic grammars from data.", "labels": [], "entities": []}, {"text": "Because of the high cost of manual sentence annotation, there is substantial interest in unsupervised grammar learning, i.e., the induction of a grammar from a corpus of unannotated sentences.", "labels": [], "entities": [{"text": "induction of a grammar from a corpus of unannotated sentences", "start_pos": 130, "end_pos": 191, "type": "TASK", "confidence": 0.7649251520633698}]}, {"text": "The simplest such approaches attempt to maximize the like- * Part of the work was done while at Iowa State University.", "labels": [], "entities": [{"text": "like", "start_pos": 53, "end_pos": 57, "type": "METRIC", "confidence": 0.9889655113220215}]}, {"text": "lihood of the grammar given the training data, typically using expectation-maximization (EM)).", "labels": [], "entities": [{"text": "expectation-maximization (EM))", "start_pos": 63, "end_pos": 93, "type": "METRIC", "confidence": 0.7658502906560898}]}, {"text": "More recent approaches incorporate additional prior information of the target grammar into learning.", "labels": [], "entities": []}, {"text": "For example, used Dirichlet priors overrule probabilities to obtain smoothed estimates of the probabilities.", "labels": [], "entities": []}, {"text": "used Dirichlet priors with hyperparameters set to values less than 1 to encourage sparsity of grammar rules. and proposed to use the hierarchical Dirichlet process prior to bias learning towards concise grammars without the need to pre-specify the number of nonterminals. and employed the logistic normal prior to model the correlations between grammar symbols.", "labels": [], "entities": []}, {"text": "incorporated a sparsity bias on grammar rules into learning by means of posterior regularization.", "labels": [], "entities": []}, {"text": "More recently, and observed that the use of Viterbi EM (also called hard EM) in place of standard EM can lead to significantly improved results in unsupervised learning of probabilistic grammars from natural language and image data respectively, even if no prior information is used.", "labels": [], "entities": []}, {"text": "This finding is surprising because Viterbi EM is a degenerate case of standard EM and is therefore generally considered to be less effective in locating the optimum of the objective function.", "labels": [], "entities": []}, {"text": "speculated that the observed advantage of Viterbi EM over standard EM is due to standard EM reserving too much probability mass to spurious parses in the E-step.", "labels": [], "entities": []}, {"text": "However, it is still unclear as to why Viterbi EM can avoid this problem.", "labels": [], "entities": [{"text": "Viterbi EM", "start_pos": 39, "end_pos": 49, "type": "DATASET", "confidence": 0.7568471729755402}]}, {"text": "Against this background, we propose the use of a novel type of prior information for unsupervised learning of probabilistic natural language grammars, namely the syntactic unambiguity of natural language.", "labels": [], "entities": []}, {"text": "Although it is often possible to correctly parse a natural language sentence in more than one way, natural language is remarkably unambiguous in the sense that the number of plausible parses of a natural language sentence is rather small in comparison with the total number of possible parses.", "labels": [], "entities": []}, {"text": "Thus, we incorporate into learning an inductive bias in favor of grammars that lead to unambiguous parses on natural language sentences, by using the posterior regularization framework ( . We name this approach unambiguity regularization.", "labels": [], "entities": []}, {"text": "The resulting family of algorithms includes standard EM and Viterbi EM, as well as an algorithm that falls between standard EM and Viterbi EM which we call softmax-EM.", "labels": [], "entities": [{"text": "Viterbi EM", "start_pos": 60, "end_pos": 70, "type": "DATASET", "confidence": 0.8551978468894958}]}, {"text": "The softmax-EM algorithm can be implemented with a simple and computationally efficient extension to standard EM.", "labels": [], "entities": []}, {"text": "The fact that Viterbi EM is a special case of our approach also gives an explanation of the advantage of Viterbi EM observed in previous work: it is because Viterbi EM implicitly utilizes unambiguity regularization.", "labels": [], "entities": []}, {"text": "In our experiments of unsupervised dependency grammar learning, we show that unambiguity regularization is beneficial to learning, and in combination with annealing (of the regularization strength) and sparsity priors it leads to improvement over the current state of the art.", "labels": [], "entities": []}, {"text": "It should be noted that our approach is closely related to the deterministic annealing (DA) technique studied in the optimization literature.", "labels": [], "entities": []}, {"text": "However, DA has a very different motivation than ours and differs from our approach in a few important algorithmic details, as will be discussed in section 5.", "labels": [], "entities": [{"text": "DA", "start_pos": 9, "end_pos": 11, "type": "TASK", "confidence": 0.9529745578765869}]}, {"text": "When applied to unsupervised grammar learning, DA has been shown to lead to worse parsing accuracy than standard EM); in contrast, we show that our approach leads to significantly higher parsing accuracy than standard EM in unsupervised dependency grammar learning.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 90, "end_pos": 98, "type": "METRIC", "confidence": 0.6932415962219238}]}, {"text": "The rest of the paper is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 analyzes the degree of unambiguity of natural language grammars.", "labels": [], "entities": []}, {"text": "Section 3 introduces the unambiguity regularization approach and shows that standard EM, Viterbi EM and softmax-EM are its special cases.", "labels": [], "entities": []}, {"text": "We show the experimental results in section 4, discuss related work in section 5 and conclude the paper in section 6.", "labels": [], "entities": []}], "datasetContent": [{"text": "We tested the effectiveness of unambiguity regularization in unsupervised learning of a type of dependency grammar called the dependency model with valence (DMV) ( 4.4.", "labels": [], "entities": []}, {"text": "On each corpus, we trained the learner on the gold-standard part-of-speech tags of the sentences of length \u2264 10 with punctuation stripped off.", "labels": [], "entities": []}, {"text": "We started our algorithm with the informed initialization proposed in (), and terminated the algorithm when the increase in the value of the objective function fell below a threshold of 0.001%.", "labels": [], "entities": []}, {"text": "To evaluate a learned grammar, we used the grammar to parse the testing corpus and computed the dependency accuracy which is the percentage of the dependencies that are correctly matched between the parses generated by the grammar and the gold standard parses.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 107, "end_pos": 115, "type": "METRIC", "confidence": 0.5258677005767822}]}, {"text": "We report the dependency accuracy on subsets of the testing corpus corresponding to sentences of length \u2264 10, length \u2264 20, and the entire testing corpus.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 25, "end_pos": 33, "type": "METRIC", "confidence": 0.8486242294311523}]}], "tableCaptions": [{"text": " Table 1: The dependency accuracies of grammars learned  by our approach with different values of \u03c3.", "labels": [], "entities": []}]}