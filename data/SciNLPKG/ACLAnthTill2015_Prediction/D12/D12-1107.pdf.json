{"title": [{"text": "Language Model Rest Costs and Space-Efficient Storage", "labels": [], "entities": []}], "abstractContent": [{"text": "Approximate search algorithms, such as cube pruning in syntactic machine translation, rely on the language model to estimate probabilities of sentence fragments.", "labels": [], "entities": [{"text": "syntactic machine translation", "start_pos": 55, "end_pos": 84, "type": "TASK", "confidence": 0.6758881111939748}]}, {"text": "We contribute two changes that trade between accuracy of these estimates and memory, holding sentence-level scores constant.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 45, "end_pos": 53, "type": "METRIC", "confidence": 0.9993835687637329}, {"text": "memory", "start_pos": 77, "end_pos": 83, "type": "METRIC", "confidence": 0.9971132278442383}]}, {"text": "Common practice uses lower-order entries in an N-gram model to score the first few words of a fragment; this violates assumptions made by common smoothing strategies, including Kneser-Ney.", "labels": [], "entities": []}, {"text": "Instead, we use a unigram model to score the first word, a bigram for the second, etc.", "labels": [], "entities": []}, {"text": "This improves search at the expense of memory.", "labels": [], "entities": [{"text": "memory", "start_pos": 39, "end_pos": 45, "type": "METRIC", "confidence": 0.9400975704193115}]}, {"text": "Conversely , we show how to save memory by collapsing probability and backoff into a single value without changing sentence-level scores, at the expense of less accurate estimates for sentence fragments.", "labels": [], "entities": []}, {"text": "These changes can be stacked, achieving better estimates with unchanged memory usage.", "labels": [], "entities": []}, {"text": "In order to interpret changes in search accuracy, we adjust the pop limit so that accuracy is unchanged and report the change in CPU time.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 40, "end_pos": 48, "type": "METRIC", "confidence": 0.9773104190826416}, {"text": "accuracy", "start_pos": 82, "end_pos": 90, "type": "METRIC", "confidence": 0.9990315437316895}]}, {"text": "Ina German-English Moses system with target-side syntax, improved estimates yielded a 63% reduction in CPU time; fora Hiero-style version, the reduction is 21%.", "labels": [], "entities": []}, {"text": "The compressed language model uses 26% less RAM while equivalent search quality takes 27% more CPU.", "labels": [], "entities": [{"text": "RAM", "start_pos": 44, "end_pos": 47, "type": "METRIC", "confidence": 0.9850467443466187}]}, {"text": "Source code is released as part of KenLM.", "labels": [], "entities": [{"text": "KenLM", "start_pos": 35, "end_pos": 40, "type": "DATASET", "confidence": 0.8587548136711121}]}], "introductionContent": [{"text": "Language model storage is typically evaluated in terms of speed, space, and accuracy.", "labels": [], "entities": [{"text": "Language model storage", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.5823078354199728}, {"text": "accuracy", "start_pos": 76, "end_pos": 84, "type": "METRIC", "confidence": 0.9982391595840454}]}, {"text": "We introduce a fourth dimension, rest cost quality, that captures how well the model scores sentence fragments for purposes of approximate search.", "labels": [], "entities": [{"text": "rest cost quality", "start_pos": 33, "end_pos": 50, "type": "METRIC", "confidence": 0.6918705105781555}]}, {"text": "Rest cost quality is distinct from accuracy in the sense that the score of a complete sentence is held constant.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 35, "end_pos": 43, "type": "METRIC", "confidence": 0.9990697503089905}]}, {"text": "We first show how to improve rest cost quality over standard practice by using additional space.", "labels": [], "entities": []}, {"text": "Then, conversely, we show how to compress the language model by making a pessimistic rest cost assumption . Language models are designed to assign probability to sentences.", "labels": [], "entities": []}, {"text": "However, approximate search algorithms use estimates for sentence fragments.", "labels": [], "entities": []}, {"text": "If the language model has order N (an N -gram model), then the first N \u2212 1 words of the fragment have incomplete context and the last N \u2212 1 words have not been completely used as context.", "labels": [], "entities": []}, {"text": "Our baseline is common practice () that uses lower-order entries from the language model for the first words in the fragment and no rest cost adjustment for the last few words.", "labels": [], "entities": []}, {"text": "Formally, the baseline estimate for sentence fragment wk 1 is where each w n is a word and p N is an N -gram language model.", "labels": [], "entities": []}, {"text": "The problem with the baseline estimate lies in lower order entries p N (w n |w n\u22121 1 ).", "labels": [], "entities": []}, {"text": "Commonly used Kneser-Ney () smoothing, including the modified version, assumes that a lower-order entry will only be used because a longer match could not be found 2 . Formally, these entries actually evaluate p N (w n |w n\u22121 1 , did not find w n 0 ).", "labels": [], "entities": []}, {"text": "For purposes of scoring sentence fragments, additional context is simply indeterminate, and the assumption may not hold.", "labels": [], "entities": []}, {"text": "As an example, we built 5-gram and unigram language models with Kneser-Ney smoothing on the same data.", "labels": [], "entities": []}, {"text": "Sentence fragments frequently begin with \"the\".", "labels": [], "entities": [{"text": "Sentence fragments", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.9075198769569397}]}, {"text": "Using a lower-order entry from the 5-gram model, log 10 p 5 (the) = \u22122.49417.", "labels": [], "entities": []}, {"text": "The unigram model does not condition on backing off, assigning log 10 p 1 (the) = \u22121.28504.", "labels": [], "entities": []}, {"text": "Intuitively, the 5-gram model is surprised, by more than an order of magnitude, to see \"the\" without matching words that precede it.", "labels": [], "entities": []}, {"text": "To remedy the situation, we train N language models on the same data.", "labels": [], "entities": []}, {"text": "Each model p n is an ngram model (it has order n).", "labels": [], "entities": []}, {"text": "We then use p n to score the nth word of a sentence fragment.", "labels": [], "entities": []}, {"text": "Thus, a unigram model scores the first word of a sentence fragment, a bigram model scores the second word, and soon until either the n-gram is not present in the model or the first N \u2212 1 words have been scored.", "labels": [], "entities": []}, {"text": "Storing probabilities from these models requires one additional value per n-gram in the model, except for N -grams where this probability is already stored.", "labels": [], "entities": []}, {"text": "Conversely, we can lower memory consumption relative to the baseline at the expense of poorer rest costs.", "labels": [], "entities": []}, {"text": "Baseline models store two entries per n-gram: probability and backoff.", "labels": [], "entities": [{"text": "probability", "start_pos": 46, "end_pos": 57, "type": "METRIC", "confidence": 0.958894670009613}]}, {"text": "We will show that the probability and backoff values in a language model can be collapsed into a single value for each n-gram without changing sentence probability.", "labels": [], "entities": []}, {"text": "This transformation saves memory by halving the number of values stored per entry, but it makes rest cost estimates worse.", "labels": [], "entities": []}, {"text": "Specifically, the rest cost pessimistically assumes that the model will back off to unigrams immediately following the sentence fragment.", "labels": [], "entities": []}, {"text": "The two modifications can be used independently or simultaneously.", "labels": [], "entities": []}, {"text": "To measure the impact of their different rest costs, we experiment with cube pruning) in syntactic machine transla-tion.", "labels": [], "entities": []}, {"text": "Cube pruning's goal is to find high-scoring sentence fragments for the root non-terminal in the parse tree.", "labels": [], "entities": []}, {"text": "It does so by going bottom-up in the parse tree, searching for high-scoring sentence fragments for each non-terminal.", "labels": [], "entities": []}, {"text": "Within each non-terminal, it generates a fixed number of high-scoring sentence fragments; this is known as the pop limit.", "labels": [], "entities": []}, {"text": "Increasing the pop limit therefore makes search more accurate but costs more time.", "labels": [], "entities": [{"text": "pop limit", "start_pos": 15, "end_pos": 24, "type": "METRIC", "confidence": 0.9590442776679993}]}, {"text": "By moderating the pop limit, improved accuracy can be interpreted as a reduction in CPU time and vice-versa.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 38, "end_pos": 46, "type": "METRIC", "confidence": 0.9992793202400208}]}], "datasetContent": [{"text": "To measure the impact of different rest costs, we use the Moses chart decoder ( for the WMT 2011 German-English translation task).", "labels": [], "entities": [{"text": "WMT 2011 German-English translation task", "start_pos": 88, "end_pos": 128, "type": "TASK", "confidence": 0.7367116570472717}]}, {"text": "Using the Moses pipeline, we trained two syntactic German-English systems, one with target-side syntax and the other hierarchical with unlabeled grammar rules.", "labels": [], "entities": []}, {"text": "Grammar rules were extracted from Europarl () using the Collins parser) for syntax on the English side.", "labels": [], "entities": [{"text": "Europarl", "start_pos": 34, "end_pos": 42, "type": "DATASET", "confidence": 0.9817705750465393}, {"text": "Collins", "start_pos": 56, "end_pos": 63, "type": "DATASET", "confidence": 0.9717699885368347}]}, {"text": "The language model interpolates, on the WMT 2010 test set, separate models built on Europarl, news commentary, and the WMT news data for each year.", "labels": [], "entities": [{"text": "WMT 2010 test set", "start_pos": 40, "end_pos": 57, "type": "DATASET", "confidence": 0.9716468155384064}, {"text": "Europarl", "start_pos": 84, "end_pos": 92, "type": "DATASET", "confidence": 0.9893839955329895}, {"text": "WMT news data", "start_pos": 119, "end_pos": 132, "type": "DATASET", "confidence": 0.9633085330327352}]}, {"text": "Models were built and interpolated using SRILM) with modified Kneser-Ney smoothing and the default pruning settings.", "labels": [], "entities": []}, {"text": "In all scenarios, the primary language model has order 5.", "labels": [], "entities": []}, {"text": "For lower-order rest costs, we also built models with orders 1 through 4 then used the n-gram model to score n-grams in the 5-gram model.", "labels": [], "entities": []}, {"text": "Feature weights were trained with MERT on the baseline using a pop limit of 1000 and 100-best output.", "labels": [], "entities": [{"text": "MERT", "start_pos": 34, "end_pos": 38, "type": "METRIC", "confidence": 0.7735089659690857}]}, {"text": "Since final feature values are unchanged, we did not re-run MERT in each condition.", "labels": [], "entities": [{"text": "MERT", "start_pos": 60, "end_pos": 64, "type": "METRIC", "confidence": 0.5571991801261902}]}, {"text": "Measurements were collected by running the decoder on the 3003-sentence test set.", "labels": [], "entities": [{"text": "3003-sentence test set", "start_pos": 58, "end_pos": 80, "type": "DATASET", "confidence": 0.7809333205223083}]}], "tableCaptions": [{"text": " Table 1: Bias (mean error), mean squared error, and  variance (of the error) for the lower-order rest cost  and the baseline. Error is the estimated log prob- ability minus the final probability. Statistics were  computed separately for the first word of a fragment  (n = 1), the second word (n = 2), etc. The lower- order estimates are better across the board, reducing  error in cube pruning. All numbers are in log base  ten, as is standard for ARPA-format language mod- els. Statistics were only collected for words with  incomplete context.", "labels": [], "entities": [{"text": "Bias", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9991219639778137}, {"text": "mean squared error", "start_pos": 29, "end_pos": 47, "type": "METRIC", "confidence": 0.8409596681594849}, {"text": "Error", "start_pos": 127, "end_pos": 132, "type": "METRIC", "confidence": 0.9928641319274902}]}]}