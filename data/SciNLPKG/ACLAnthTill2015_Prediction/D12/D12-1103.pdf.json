{"title": [{"text": "Exact Sampling and Decoding in High-Order Hidden Markov Models", "labels": [], "entities": [{"text": "Exact Sampling", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.7527979612350464}]}], "abstractContent": [{"text": "We present a method for exact optimization and sampling from high order Hidden Markov Models (HMMs), which are generally handled by approximation techniques.", "labels": [], "entities": [{"text": "exact optimization", "start_pos": 24, "end_pos": 42, "type": "TASK", "confidence": 0.8855420053005219}]}, {"text": "Motivated by adaptive rejection sampling and heuris-tic search, we propose a strategy based on sequentially refining a lower-order language model that is an upper bound on the true model we wish to decode and sample from.", "labels": [], "entities": [{"text": "adaptive rejection sampling", "start_pos": 13, "end_pos": 40, "type": "TASK", "confidence": 0.6060565014680227}]}, {"text": "This allows us to build tractable variable-order HMMs.", "labels": [], "entities": []}, {"text": "The ARPA format for language models is extended to enable an efficient use of the max-backoff quantities required to compute the upper bound.", "labels": [], "entities": []}, {"text": "We evaluate our approach on two problems: a SMS-retrieval task and a POS tagging experiment using 5-gram models.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 69, "end_pos": 80, "type": "TASK", "confidence": 0.7887190282344818}]}, {"text": "Results show that the same approach can be used for exact optimization and sampling, while explicitly constructing only a fraction of the total implicit state-space.", "labels": [], "entities": [{"text": "exact optimization", "start_pos": 52, "end_pos": 70, "type": "TASK", "confidence": 0.7607839405536652}]}], "introductionContent": [{"text": "In NLP, sampling is important for many real tasks, such as: (i) diversity in language generation or machine translation (proposing multiple alternatives which are not clustered around a single maximum); (ii) Bayes error minimization, for instance in Statistical Machine Translation (); (iii) learning of parametric and non-parametric Bayesian models).", "labels": [], "entities": [{"text": "language generation", "start_pos": 77, "end_pos": 96, "type": "TASK", "confidence": 0.7217293679714203}, {"text": "machine translation", "start_pos": 100, "end_pos": 119, "type": "TASK", "confidence": 0.7671692967414856}, {"text": "Bayes error minimization", "start_pos": 208, "end_pos": 232, "type": "TASK", "confidence": 0.6130477587381998}, {"text": "Statistical Machine Translation", "start_pos": 250, "end_pos": 281, "type": "TASK", "confidence": 0.6975214382012686}]}, {"text": "However, most practical sampling algorithms are based on MCMC, i.e. they are based on local moves * This work was conducted during an internship at XRCE.", "labels": [], "entities": [{"text": "XRCE", "start_pos": 148, "end_pos": 152, "type": "DATASET", "confidence": 0.933090329170227}]}, {"text": "starting from an initial valid configuration.", "labels": [], "entities": []}, {"text": "Often, these algorithms are stuck in local minima, i.e. in a basin of attraction close to the initialization, and the method does not really sample the whole state space.", "labels": [], "entities": []}, {"text": "This is a problem when there are ambiguities in the distribution we want to sample from: by having a local approach such as MCMC, we might only explore states that are close to a given configuration.", "labels": [], "entities": [{"text": "MCMC", "start_pos": 124, "end_pos": 128, "type": "DATASET", "confidence": 0.9039525389671326}]}, {"text": "The necessity of exact sampling can be questioned in practice.", "labels": [], "entities": [{"text": "exact sampling", "start_pos": 17, "end_pos": 31, "type": "TASK", "confidence": 0.6960178911685944}]}, {"text": "Approximate sampling techniques have been developed over the last century and seem sufficient for most purposes.", "labels": [], "entities": []}, {"text": "However, the cases where one actually knows the quality of a sampling algorithm are very rare, and it is common practice to forget about the approximation and simply treat the result of a sampler as a set of i.i.d. data.", "labels": [], "entities": []}, {"text": "Exact sampling provides a de-facto guarantee that the samples are truly independent.", "labels": [], "entities": []}, {"text": "This is particularly relevant when one uses a cascade of algorithms in complex NLP processing chains, as shown by) in their work on linguistic annotation pipelines.", "labels": [], "entities": []}, {"text": "In this paper, we present an approach for exact decoding and sampling with an HMM whose hidden layer is a high-order language model (LM), which innovates on existing techniques in the following ways.", "labels": [], "entities": []}, {"text": "First, it is a joint approach to sampling and optimization (i.e. decoding), which is based on introducing a simplified, \"optimistic\", version q(x) of the underlying language model p(x), for which it is tractable to use standard dynamic programming techniques both for sampling and optimization.", "labels": [], "entities": []}, {"text": "We then formulate the problem of sampling/optimization with the original model p(x) in terms of a novel algorithm which can be viewed as a form of adaptive rejection sampling (, in which a low acceptance rate (in sampling) or a low ratio p(x * )/q(x * ) (in optimization, with x * the argmax of q) leads to a refinement of q, i.e., a slightly more complex and less optimistic q but with a higher acceptance rate or ratio.", "labels": [], "entities": [{"text": "sampling/optimization", "start_pos": 33, "end_pos": 54, "type": "TASK", "confidence": 0.8600882887840271}]}, {"text": "Second, it is the first technique that we are aware of which is able to perform exact sampling with such models.", "labels": [], "entities": []}, {"text": "Known techniques for sampling in such situations have to rely on approximation techniques such as Gibbs or Beam sampling (see e.g. (; Van).", "labels": [], "entities": []}, {"text": "By contrast, our technique produces exact samples from the start, although in principle, the first sample maybe obtained only after along series of rejections (and therefore refinements).", "labels": [], "entities": []}, {"text": "In practice, our experiments indicate that a good acceptance rate is obtained after a relatively small number of refinements.", "labels": [], "entities": [{"text": "acceptance rate", "start_pos": 50, "end_pos": 65, "type": "METRIC", "confidence": 0.9191371202468872}]}, {"text": "It should be noted that, in the case of exact optimization, a similar technique to ours has been proposed in an image processing context, but without any connection to sampling.", "labels": [], "entities": [{"text": "exact optimization", "start_pos": 40, "end_pos": 58, "type": "TASK", "confidence": 0.7972480952739716}]}, {"text": "That paper, written in the context of image processing, appears to belittle known in the NLP community.", "labels": [], "entities": [{"text": "image processing", "start_pos": 38, "end_pos": 54, "type": "TASK", "confidence": 0.8898075819015503}]}, {"text": "Overall, our method is of particular interest because it allows for exact decoding and sampling from HMMs where the applications of existing dynamic programming algorithms such as Viterbi decoding or Forward-Backward sampling) are not feasible, due to a large state space.", "labels": [], "entities": []}, {"text": "In Section 2, we present our approach and describe our joint algorithm for HMM sampling/optimization, giving details about our extension of the ARPA format and refinement procedure.", "labels": [], "entities": [{"text": "HMM sampling/optimization", "start_pos": 75, "end_pos": 100, "type": "TASK", "confidence": 0.9279816746711731}, {"text": "ARPA format", "start_pos": 144, "end_pos": 155, "type": "DATASET", "confidence": 0.8175027370452881}]}, {"text": "In Section 3 we define our two experimental tasks, SMS-retrieval and POS tagging, for which we present the results of our joint algorithm.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 69, "end_pos": 80, "type": "TASK", "confidence": 0.7219927906990051}]}, {"text": "We finally discuss perspectives and conclude in Section 4.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section we empirically evaluate our joint, exact decoder and sampler on two tasks; SMS-retrieval (Section 3.1), and supervised POS tagging (Section 3.2).", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 135, "end_pos": 146, "type": "TASK", "confidence": 0.8016582727432251}]}], "tableCaptions": [{"text": " Table 1: # of n-grams in our variable-order HMM.", "labels": [], "entities": []}, {"text": " Table 2: Viterbi paths given different q t . Here, for  the given input, it took 170 iterations to find the best  sequence according to p, so we only show every 50th  path.", "labels": [], "entities": []}, {"text": " Table 3: In this table we show the average amount of  time in seconds and the average number of iterations  (iter) taken to sample sentences of length 10 given  different values of B.", "labels": [], "entities": []}, {"text": " Table 4: Top-5 ranked samples for an example in- put. We highlight in bold the words which are differ- ent to the Viterbi best of the model. The oracle and  best are not the same for this input.", "labels": [], "entities": []}]}