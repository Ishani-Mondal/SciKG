{"title": [{"text": "Three Dependency-and-Boundary Models for Grammar Induction", "labels": [], "entities": [{"text": "Grammar Induction", "start_pos": 41, "end_pos": 58, "type": "TASK", "confidence": 0.8505429625511169}]}], "abstractContent": [{"text": "We present anew family of models for unsu-pervised parsing, Dependency and Boundary models, that use cues at constituent boundaries to inform head-outward dependency tree generation.", "labels": [], "entities": [{"text": "head-outward dependency tree generation", "start_pos": 142, "end_pos": 181, "type": "TASK", "confidence": 0.5802017897367477}]}, {"text": "We build on three intuitions that are explicit in phrase-structure grammars but only implicit in standard dependency formulations: (i) Distributions of words that occur at sentence boundaries-such as English determiners-resemble constituent edges.", "labels": [], "entities": []}, {"text": "(ii) Punctuation at sentence boundaries further helps distinguish full sentences from fragments like headlines and titles, allowing us to model grammatical differences between complete and incomplete sentences.", "labels": [], "entities": []}, {"text": "(iii) Sentence-internal punctuation boundaries help with longer-distance dependencies, since punctuation correlates with constituent edges.", "labels": [], "entities": []}, {"text": "Our models induce state-of-the-art dependency grammars for many languages without special knowledge of optimal input sentence lengths or biased, manually-tuned initializers.", "labels": [], "entities": []}], "introductionContent": [{"text": "Natural language is ripe with all manner of boundaries at the surface level that align with hierarchical syntactic structure.", "labels": [], "entities": []}, {"text": "From the significance of function words) and punctuation marks as separators between constituents in longer sentences -to the importance of isolated words in children's early vocabulary acquisition) -word boundaries play a crucial role in language learning.", "labels": [], "entities": []}, {"text": "We will show that boundary information can also be useful in dependency grammar induction models, which traditionally focus on head rather than fringe words.", "labels": [], "entities": [{"text": "dependency grammar induction", "start_pos": 61, "end_pos": 89, "type": "TASK", "confidence": 0.8060103456179301}]}, {"text": "Because the determiner (DT) appears at the left edge of the sentence, it should be possible to learn that determiners may generally be present at left edges of phrases.", "labels": [], "entities": [{"text": "determiner (DT)", "start_pos": 12, "end_pos": 27, "type": "METRIC", "confidence": 0.7903854250907898}]}, {"text": "This information could then be used to correctly parse the sentence-internal determiner in the mail.", "labels": [], "entities": []}, {"text": "Similarly, the fact that the noun head (NN) of the object the mail appears at the right edge of the sentence could help identify the noun check as the right edge of the subject NP.", "labels": [], "entities": []}, {"text": "As with jigsaw puzzles, working inwards from boundaries helps determine sentenceinternal structures of both noun phrases, neither of which would be quite so clear if viewed separately.", "labels": [], "entities": []}, {"text": "Furthermore, properties of noun-phrase edges are partially shared with prepositional-and verb-phrase units that contain these nouns.", "labels": [], "entities": []}, {"text": "Because typical headdriven grammars model valence separately for each class of head, however, they cannot see that the left fringe boundary, The check, of the verb-phrase is shared with its daughter's, check.", "labels": [], "entities": []}, {"text": "Neither of these insights is available to traditional dependency formulations, which could learn from the boundaries of this sentence only that determiners might have no left-and that nouns might have no right-dependents.", "labels": [], "entities": []}, {"text": "We propose a family of dependency parsing models that are capable of inducing longer-range implications from sentence edges than just fertilities of their fringe words.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 23, "end_pos": 41, "type": "TASK", "confidence": 0.7311747968196869}]}, {"text": "Our ideas conveniently lend themselves to implementations that can reuse much of the standard grammar induction machinery, including efficient dynamic programming routines for the relevant expectation-maximization algorithms.", "labels": [], "entities": []}], "datasetContent": [{"text": "We first motivate each model by analyzing the Wall Street Journal (WSJ) portion of the Penn English Treebank (), 3 before delving into Split-Head Dependency Grammar: Parameterizations of the split-head-outward generative process used by DBMs and in previous models.", "labels": [], "entities": [{"text": "Wall Street Journal (WSJ) portion of the Penn English Treebank (), 3", "start_pos": 46, "end_pos": 114, "type": "DATASET", "confidence": 0.9466090480486552}]}, {"text": "Although motivating solely from this treebank biases our discussion towards a very specific genre of just one language, it has the advantage of allowing us to make concrete claims that are backed up by significant statistics.", "labels": [], "entities": []}, {"text": "In the grammar induction experiments that follow, we will test each model's incremental contribution to accuracies empirically, across many disparate languages.", "labels": [], "entities": [{"text": "grammar induction", "start_pos": 7, "end_pos": 24, "type": "TASK", "confidence": 0.7410179674625397}]}, {"text": "We worked with all 23 (disjoint) train/test splits from the 2006/7 CoNLL shared tasks), spanning 19 languages.", "labels": [], "entities": []}, {"text": "For each data set, we induced a baseline grammar using the DMV.", "labels": [], "entities": []}, {"text": "We excluded all training sentences with more than 15 tokens to create a conservative bias, because in this set-up the baseline is known to excel.", "labels": [], "entities": []}, {"text": "Grammar inducers were initialized using (the same) uniformly-at-random chosen parse trees of training sentences; thereafter, we applied \"add one\" smoothing at every training step.", "labels": [], "entities": []}, {"text": "To fairly compare the models under consideration -which could have quite different starting perplexities and ensuing consecutive relative likelihoods -we experimented with two termination strategies.", "labels": [], "entities": []}, {"text": "In one case, we blindly ran each learner through 40 steps of inside-outside re-estimation, ignoring any convergence criteria; in the other case, we ran until numerical convergence of soft EM's objective function or until the likelihood of resulting Viterbi parse trees suffered -an \"early-stopping lateen EM\" strategy (Spitkovsky et al., 2011a, \u00a72.3).", "labels": [], "entities": []}, {"text": "We evaluated against all sentences of the blind test sets (except one 145-token item in Arabic '07 data).", "labels": [], "entities": [{"text": "Arabic '07 data", "start_pos": 88, "end_pos": 103, "type": "DATASET", "confidence": 0.8036230057477951}]}, {"text": "shows experimental results, averaged over 1999), discarding any empty nodes, etc., as is standard practice.", "labels": [], "entities": []}, {"text": "We did not test on WSJ data because such evaluation would not be blind, as parse trees from the PTB are our motivating examples; instead, performance on WSJ serves as a strong baseline in a separate study): bootstrapping of DBMs from mostly incomplete inter-punctuation fragments.", "labels": [], "entities": [{"text": "WSJ data", "start_pos": 19, "end_pos": 27, "type": "DATASET", "confidence": 0.8334499895572662}, {"text": "WSJ", "start_pos": 153, "end_pos": 156, "type": "DATASET", "confidence": 0.9178459644317627}]}, {"text": "all 19 languages, for the DMV baselines and DBM-1 and 2.", "labels": [], "entities": []}, {"text": "We did not test DBM-3 in this set-up because most sentence-internal punctuation occurs in longer sentences; instead, DBM-3 will be tested later (see \u00a77), using most sentences, 5 in the final training step of a curriculum strategy () that we will propose for DBMs.", "labels": [], "entities": [{"text": "DBMs", "start_pos": 258, "end_pos": 262, "type": "TASK", "confidence": 0.8683611154556274}]}, {"text": "For the three models tested on shorter inputs (up to 15 tokens) both terminating criteria exhibited the same trend; lateen EM consistently scored slightly higher than 40 EM iterations.", "labels": [], "entities": [{"text": "lateen EM", "start_pos": 116, "end_pos": 125, "type": "METRIC", "confidence": 0.773185133934021}]}, {"text": "Since it is not possible to consult parse trees during grammar induction (to check whether an input sentence is clausal), we opted fora proxy: presence of sentence-final punctuation.", "labels": [], "entities": [{"text": "grammar induction", "start_pos": 55, "end_pos": 72, "type": "TASK", "confidence": 0.7224489003419876}]}, {"text": "Using punctuation to divide input sentences into two groups, DBM-2 scored higher: 40.9, up from 39.0% accuracy (see).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 102, "end_pos": 110, "type": "METRIC", "confidence": 0.9984795451164246}]}, {"text": "After evaluating these multi-lingual experiments, we checked how well our proxy corresponds to actual clausal sentences in WSJ.", "labels": [], "entities": [{"text": "WSJ", "start_pos": 123, "end_pos": 126, "type": "DATASET", "confidence": 0.9037678837776184}]}, {"text": "include parenthesized expressions that are marked as noun-phrases, such as (See related story: \"Fed Ready to Inject Big Funds\": WSJ Oct.; false negatives can be headlines having a main verb, e.g., Population Drain Ends For Midwestern States.", "labels": [], "entities": [{"text": "WSJ Oct.", "start_pos": 128, "end_pos": 136, "type": "DATASET", "confidence": 0.9324560463428497}]}, {"text": "Thus, our proxy is not perfect but seems to be tolerable in practice.", "labels": [], "entities": []}, {"text": "We suspect that identities of punctuation marks -both sentence-final and sentence-initial -could be of extra assistance in grammar induction, specifically for grouping imperatives, questions, and so forth.", "labels": [], "entities": [{"text": "grammar induction", "start_pos": 123, "end_pos": 140, "type": "TASK", "confidence": 0.7405107021331787}]}, {"text": "As we mentioned earlier (see \u00a73), there is little point in testing DBM-3 with shorter sentences, since most sentence-internal punctuation occurs in longer inputs.", "labels": [], "entities": []}, {"text": "Instead, we will test this model in a final step of a staged training strategy, with more data (see \u00a77.3).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Directed dependency accuracies, averaged over  all 2006/7 CoNLL evaluation sets (all sentences), for the  DMV and two new dependency-and-boundary grammar  inducers (DBM-1,2) -using two termination strategies. 6", "labels": [], "entities": [{"text": "CoNLL evaluation sets", "start_pos": 68, "end_pos": 89, "type": "DATASET", "confidence": 0.808143675327301}]}, {"text": " Table 3: Coefficients of determination (R 2 ) and Akaike  information criteria (AIC), both adjusted for the number  of parameters, for several single-predictor logistic models  of non-adjacent stops, given direction dir; c h is the class  of the head, n is its number of descendants (so far) to that  side, and c e represents the farthest descendant (the edge).", "labels": [], "entities": [{"text": "Akaike  information criteria (AIC)", "start_pos": 51, "end_pos": 85, "type": "METRIC", "confidence": 0.9516210357348124}]}, {"text": " Table 4: Empirical distributions for non-punctuation part- of-speech tags in WSJ, ordered by overall frequency, as  well as distributions for sentence boundaries and for the  roots of complete and incomplete sentences. (A uniform  distribution would have 1/36 = 2.7% for all POS-tags.)", "labels": [], "entities": [{"text": "Empirical", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9794274568557739}, {"text": "WSJ", "start_pos": 78, "end_pos": 81, "type": "DATASET", "confidence": 0.8719567060470581}]}, {"text": " Table 5: A distance matrix for all pairs of probability dis- tributions over POS-tags shown in", "labels": [], "entities": []}, {"text": " Table 6: A contingency table for clausal sentences and  trailing punctuation in WSJ; the mean square contingency  coefficient r \u03c6 signifies a low degree of correlation. (For  two binary variables, r \u03c6 is equivalent to Karl Pearson's  better-known product-moment correlation coefficient, \u03c1.)", "labels": [], "entities": [{"text": "WSJ", "start_pos": 81, "end_pos": 84, "type": "DATASET", "confidence": 0.8944897651672363}, {"text": "mean square contingency  coefficient r \u03c6", "start_pos": 90, "end_pos": 130, "type": "METRIC", "confidence": 0.711825355887413}]}, {"text": " Table 7: Contingency tables for IN right-attaching MD,  among closest ordered pairs of these tokens in WSJ sen- tences with punctuation, versus: (a) presence of interven- ing punctuation; and (b) presence of intermediate words.", "labels": [], "entities": []}]}