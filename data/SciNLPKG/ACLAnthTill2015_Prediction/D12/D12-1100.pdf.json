{"title": [{"text": "Sketch Algorithms for Estimating Point Queries in NLP", "labels": [], "entities": []}], "abstractContent": [{"text": "Many NLP tasks rely on accurate statistics from large corpora.", "labels": [], "entities": []}, {"text": "Tracking complete statistics is memory intensive, so recent work has proposed using compact approximate \"sketches\" of frequency distributions.", "labels": [], "entities": [{"text": "Tracking complete statistics", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.8852361838022867}]}, {"text": "We describe 10 sketch methods, including existing and novel variants.", "labels": [], "entities": []}, {"text": "We compare and study the errors (over-estimation and underestimation) made by the sketches.", "labels": [], "entities": []}, {"text": "We evaluate several sketches on three important NLP problems.", "labels": [], "entities": []}, {"text": "Our experiments show that one sketch performs best for all the three tasks.", "labels": [], "entities": []}], "introductionContent": [{"text": "Since the emergence of the World Wide Web, social media and mobile devices, we have ever larger and richer examples of text data.", "labels": [], "entities": []}, {"text": "Such vast corpora have led to leaps in the performance of many language-based tasks: the concept is that simple models trained on big data can outperform more complex models with fewer examples.", "labels": [], "entities": []}, {"text": "However, this new view comes with its own challenges: principally, how to effectively represent such large data sets so that model parameters can be efficiently extracted?", "labels": [], "entities": []}, {"text": "One answer is to adopt compact summaries of corpora in the form of probabilistic \"sketches\".", "labels": [], "entities": []}, {"text": "In recent years, the field of Natural Language Processing (NLP) has seen tremendous growth and interest in the use of approximation, randomization, and streaming techniques for large-scale problems.", "labels": [], "entities": [{"text": "Natural Language Processing (NLP)", "start_pos": 30, "end_pos": 63, "type": "TASK", "confidence": 0.7276651163895925}]}, {"text": "Much of this work relies on tracking very many statistics.", "labels": [], "entities": []}, {"text": "For example, storing approximate counts, computing approximate association scores like Pointwise Mutual Information (), finding frequent items (like n-grams) (, building streaming language models, and distributional similarity (.", "labels": [], "entities": []}, {"text": "All these problems ultimately depend on approximate counts of items (such as n-grams, word pairs and word-context pairs).", "labels": [], "entities": []}, {"text": "Thus we focus on solving this central problem in the context of NLP applications.", "labels": [], "entities": []}, {"text": "Sketch algorithms () area memory-and time-efficient solution to answering point queries.", "labels": [], "entities": [{"text": "answering point queries", "start_pos": 64, "end_pos": 87, "type": "TASK", "confidence": 0.8825246493021647}]}, {"text": "Recently in NLP, we) demonstrated that aversion of the Count-Min sketch () accurately solves three largescale NLP problems using small bounded memory footprint.", "labels": [], "entities": []}, {"text": "However, there are several other sketch algorithms, and it is not clear why this instance should be preferred amongst these.", "labels": [], "entities": []}, {"text": "In this work, we conduct a systematic study and compare many sketch techniques which answer point queries with focus on large-scale NLP tasks.", "labels": [], "entities": []}, {"text": "While sketches have been evaluated within the database community for finding frequent items and join-size estimation (, this is the first comparative study for NLP problems.", "labels": [], "entities": []}, {"text": "Our work includes three contributions: (1) We propose novel variants of existing sketches by extending the idea of conservative update to them.", "labels": [], "entities": []}, {"text": "We propose Count sketch () with conservative update (COUNT-CU) and Count-mean-min sketch with conservative update (CMM-CU).", "labels": [], "entities": [{"text": "Count", "start_pos": 11, "end_pos": 16, "type": "METRIC", "confidence": 0.9136358499526978}, {"text": "COUNT-CU", "start_pos": 53, "end_pos": 61, "type": "METRIC", "confidence": 0.6763671040534973}]}, {"text": "The motivation behind proposing new sketches is inspired by the success of Count-Min sketch with conservative update in our earlier work.", "labels": [], "entities": []}, {"text": "We empirically compare and study the errors in approximate counts for several sketches.", "labels": [], "entities": []}, {"text": "Errors can be over-estimation, under-estimation, or a combination of the two.", "labels": [], "entities": []}, {"text": "We also evaluate their performance via Pointwise Mutual Information and LogLikelihood Ratio.", "labels": [], "entities": []}, {"text": "(3) We use sketches to solve three important NLP problems.", "labels": [], "entities": []}, {"text": "Our experiments show that sketches can be very effective for these tasks, and that the best results are obtained using the \"conservative update\" technique.", "labels": [], "entities": []}, {"text": "Across all the three tasks, one sketch (CM-CU) performs best.", "labels": [], "entities": []}], "datasetContent": [{"text": "We empirically compare and study the errors in approximate counts for all 10 sketches.", "labels": [], "entities": []}, {"text": "Errors can be over-estimation, under-estimation, or a combination of the two.", "labels": [], "entities": []}, {"text": "We also study the behavior of approximate Pointwise Mutual Information and Log Likelihood Ratio for the sketches.", "labels": [], "entities": []}, {"text": "We study three important NLP applications, and compare the three best-performing sketches: CountMin sketch with conservative update (CM-CU), Count-mean-min with conservative update (CMM-CU), and Lossy counting with conservative update (LCU-WS).", "labels": [], "entities": [{"text": "Count-mean-min", "start_pos": 141, "end_pos": 155, "type": "METRIC", "confidence": 0.9254483580589294}]}, {"text": "The above mentioned 3 sketches are selected from 10 sketches (see Section 2) considering these sketches make errors on different ranges of the counts: low, mid and, high frequency counts as seen in our intrinsic evaluations in Section 3.", "labels": [], "entities": []}, {"text": "The goal of this experiment is to show the effectiveness of sketches on large-scale language processing tasks.", "labels": [], "entities": []}, {"text": "These adhere to the premise that simple methods using large data can dominate more complex models.", "labels": [], "entities": []}, {"text": "We purposefully select simple methods as they use approximate counts and associations directly to solve these tasks.", "labels": [], "entities": []}, {"text": "This allows us to have a fair comparison among different sketches, and to more directly seethe impact of different choices of sketch on the task outcome.", "labels": [], "entities": []}, {"text": "Of course, sketches are still broadly applicable to many NLP problems where we want to count (many) items or compute associations: e.g. language models, Statistical Machine Translation, paraphrasing, bootstrapping and label propagation for automatically creating a knowledge base and finding interesting patterns in social media.", "labels": [], "entities": [{"text": "Statistical Machine Translation", "start_pos": 153, "end_pos": 184, "type": "TASK", "confidence": 0.7518847187360128}, {"text": "label propagation", "start_pos": 218, "end_pos": 235, "type": "TASK", "confidence": 0.737381249666214}]}, {"text": "Data: We use Gigaword ( and a 50% portion of a copy of news web (GWB50) crawled by).", "labels": [], "entities": [{"text": "Gigaword", "start_pos": 13, "end_pos": 21, "type": "DATASET", "confidence": 0.8745393753051758}, {"text": "news web (GWB50", "start_pos": 55, "end_pos": 70, "type": "DATASET", "confidence": 0.7390958219766617}]}, {"text": "The raw size of Gigaword (GW) and GWB50 is 9.8 GB and 49 GB with 56.78 million and 462.60 sentences respectively.", "labels": [], "entities": [{"text": "GWB50", "start_pos": 34, "end_pos": 39, "type": "DATASET", "confidence": 0.8585605621337891}]}, {"text": "For both the corpora, we split the text into sentences, tokenize and convert into lower-case.", "labels": [], "entities": []}, {"text": "In NLP, it is difficult and time consuming to create annotated test sets.", "labels": [], "entities": []}, {"text": "This problem has motivated the use of pseudo-words to automatically create the test sets without human annotation.", "labels": [], "entities": []}, {"text": "The pseudo-words area common way to evaluate selectional preferences models) that measure the strength of association between a predicate and its argument filler, e.g., that the noun \"song\" is likely to be the object of the verb \"sing\".", "labels": [], "entities": []}, {"text": "A pseudo-word is the conflation of two words (e.g. song/dance).", "labels": [], "entities": []}, {"text": "One word is the original in a sentence, and the second is the confounder.", "labels": [], "entities": []}, {"text": "For example, in our task of selectional preferences, the system has to decide for the verb \"sing\" which is the correct object between \"song\"/\"dance\".", "labels": [], "entities": []}, {"text": "Recently, Chambers and Jurafsky (2010) proposed a simple baseline based on co-occurrence counts of words, which has state-of-the-art performance on pseudo-words evaluation for selectional preferences.", "labels": [], "entities": []}, {"text": "We use a simple approach (without any typed dependency data) similar to, where we count all word pairs (except word pairs involving stop words) that appear within a window of size 3 from Gigaword (9.8 GB).", "labels": [], "entities": []}, {"text": "That generates 970 million word pair tokens (stream size) and 94 million word pair types.", "labels": [], "entities": []}, {"text": "Counts of all the 94 million unique word pairs are stored in CM-CU, CMM-CU, and LCU-WS.", "labels": [], "entities": []}, {"text": "For a target verb, we return that noun which has higher co-occurrence count with it, as the correct selectional preference.", "labels": [], "entities": []}, {"text": "We evaluate on Chambers and Jurafsky's three test sets 1 (excluding instances involving stop words) that are based on different strategies in selecting confounders: Random (4081 instances), Buckets (4028 instances), and Neighbor (3881 instances).", "labels": [], "entities": []}, {"text": "To evaluate against the exact counts, we compute exact counts for only those word pairs that are present in the test sets.", "labels": [], "entities": []}, {"text": "Accuracy is used for evaluation and is defined as the percentage of number of correctly identified pseudo words.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 0, "end_pos": 8, "type": "METRIC", "confidence": 0.9921557903289795}]}, {"text": "In, we plot the cumulative proportion of true frequency counts of all word pairs (from the three tests) in Gigaword (GW).", "labels": [], "entities": [{"text": "cumulative proportion of true frequency counts", "start_pos": 16, "end_pos": 62, "type": "METRIC", "confidence": 0.6007786293824514}, {"text": "Gigaword (GW", "start_pos": 107, "end_pos": 119, "type": "DATASET", "confidence": 0.6350668470064799}]}, {"text": "To include unseen word pairs from test set in GW on log-scale in, we increment the true counts of all the word pairs by 1.", "labels": [], "entities": []}, {"text": "This plot demonstrates that 45% of wordpairs are unseen in GW, and 67% of word pairs have counts less than 10.", "labels": [], "entities": [{"text": "GW", "start_pos": 59, "end_pos": 61, "type": "DATASET", "confidence": 0.8264806866645813}]}, {"text": "Hence, to perform better on this task, it is essential to accurately maintain counts of rare word pairs.", "labels": [], "entities": []}, {"text": "In, we vary the size of all sketches (50 million (M ), 100M , 200M , 500M and 1 billion (1B) counters) with 3 hash functions to compare them against the exact counts.", "labels": [], "entities": []}, {"text": "It takes 1.8 GB uncompressed space to maintain the exact counts on the disk.", "labels": [], "entities": []}, {"text": "shows that with sketches of size > 200M on all the three test sets, CM-CU and LCU-WS are comparable to exact.", "labels": [], "entities": [{"text": "LCU-WS", "start_pos": 78, "end_pos": 84, "type": "METRIC", "confidence": 0.7082052230834961}]}, {"text": "However, the CMM-CU sketch performs less well.", "labels": [], "entities": [{"text": "CMM-CU sketch", "start_pos": 13, "end_pos": 26, "type": "DATASET", "confidence": 0.8898090422153473}]}, {"text": "We conjecture the reason for such a behavior is due to loss of recall (information about low frequency word pairs) by under-estimation error.", "labels": [], "entities": [{"text": "recall", "start_pos": 63, "end_pos": 69, "type": "METRIC", "confidence": 0.9965341091156006}]}, {"text": "For this task CM-CU and LCU-WS scales to storing 94M unique word pairs using 200M integer (4 bytes each) counters (using 800 MB) < 1.8 GB to maintain exact counts.", "labels": [], "entities": []}, {"text": "Moreover, these results are comparable to Chambers and Jurafsky's state-of-the-art framework.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Pseudo-words evaluation on accuracy metric for selectional preferences using several sketches of different sizes against", "labels": [], "entities": [{"text": "accuracy", "start_pos": 37, "end_pos": 45, "type": "METRIC", "confidence": 0.9972158670425415}]}, {"text": " Table 2: Evaluating Semantic Orientation on accuracy metric", "labels": [], "entities": [{"text": "Evaluating Semantic Orientation", "start_pos": 10, "end_pos": 41, "type": "TASK", "confidence": 0.8233081301053365}, {"text": "accuracy", "start_pos": 45, "end_pos": 53, "type": "METRIC", "confidence": 0.9729641675949097}]}, {"text": " Table 3: Evaluating distributional similarity using sketches.", "labels": [], "entities": []}]}