{"title": [{"text": "Multi-Domain Learning: When Do Domains Matter?", "labels": [], "entities": []}], "abstractContent": [{"text": "We present a systematic analysis of existing multi-domain learning approaches with respect to two questions.", "labels": [], "entities": []}, {"text": "First, many multi-domain learning algorithms resemble ensemble learning algorithms.", "labels": [], "entities": []}, {"text": "(1) Are multi-domain learning improvements the result of ensemble learning effects?", "labels": [], "entities": []}, {"text": "Second, these algorithms are traditionally evaluated in a balanced class label setting, although in practice many multi-domain settings have domain-specific class label biases.", "labels": [], "entities": []}, {"text": "When multi-domain learning is applied to these settings, (2) are multi-domain methods improving because they capture domain-specific class biases?", "labels": [], "entities": []}, {"text": "An understanding of these two issues presents a clearer idea about where the field has had success in multi-domain learning, and it suggests some important open questions for improving beyond the current state of the art.", "labels": [], "entities": []}], "introductionContent": [{"text": "Research efforts in recent years have demonstrated the importance of domains in statistical natural language processing.", "labels": [], "entities": [{"text": "statistical natural language processing", "start_pos": 80, "end_pos": 119, "type": "TASK", "confidence": 0.7339264005422592}]}, {"text": "A mismatch between training and test domains can negatively impact system accuracy as it violates a core assumption in many machine learning algorithms: that data points are independent and identically distributed (i.i.d.).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 74, "end_pos": 82, "type": "METRIC", "confidence": 0.9936902523040771}]}, {"text": "As a result, numerous domain adaptation methods () target settings with a training set from one domain and a test set from another.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 22, "end_pos": 39, "type": "TASK", "confidence": 0.7172975689172745}]}, {"text": "Often times the training set itself violates the i.i.d. assumption and contains multiple domains.", "labels": [], "entities": []}, {"text": "In this case, training a single model obscures domain distinctions, and separating the dataset by domains reduces training data.", "labels": [], "entities": []}, {"text": "Instead, multi-domain learning (MDL) can take advantage of these domain labels to improve learning.", "labels": [], "entities": []}, {"text": "One such example is sentiment classification of product reviews.", "labels": [], "entities": [{"text": "sentiment classification of product reviews", "start_pos": 20, "end_pos": 63, "type": "TASK", "confidence": 0.9433519005775451}]}, {"text": "Training data is available from many product categories and while all data should be used to learn a model, there are important differences between the categories ( . While much prior research has shown improvements using MDL, this paper explores what properties of an MDL setting matter.", "labels": [], "entities": []}, {"text": "Are previous improvements from MDL algorithms discovering important distinctions between features in different domains, as we would hope, or are other factors contributing to learning success?", "labels": [], "entities": []}, {"text": "The key question of this paper is: when do domains matter?", "labels": [], "entities": []}, {"text": "Towards this goal we explore two issues.", "labels": [], "entities": []}, {"text": "First, we explore the question of whether domain distinctions are used by existing MDL algorithms in meaningful ways.", "labels": [], "entities": []}, {"text": "While differences in feature behaviors between domains will hurt performance), it is not clear if the improvements in MDL algorithms can be attributed to correcting these errors, or whether they are benefiting from something else.", "labels": [], "entities": []}, {"text": "In particular, there are many similarities between MDL and ensemble methods, with connections to instance bag-ging, feature bagging and classifier combination.", "labels": [], "entities": []}, {"text": "It maybe that gains in MDL are the usual ensemble learning improvements.", "labels": [], "entities": []}, {"text": "Second, one simple way in which domains can change is the distribution of the prior over the labels.", "labels": [], "entities": []}, {"text": "For example, reviews of some products maybe more positive on average than reviews of other product types.", "labels": [], "entities": []}, {"text": "Simply capturing this bias may account for significant gains inaccuracy, even though nothing is learned about the behavior of domain-specific features.", "labels": [], "entities": []}, {"text": "Most prior work considers datasets with balanced labels.", "labels": [], "entities": []}, {"text": "However, in real world applications, where labels maybe biased toward some values, gains from MDL could be attributed to simply modeling domain-specific bias.", "labels": [], "entities": []}, {"text": "A practical advantage of such a result is ease of implementation and the ability to scale to many domains.", "labels": [], "entities": []}, {"text": "Overall, irrespective of the answers to these questions, a better understanding of the performance of existing MDL algorithms in different settings will provide intuitions for improving the state of the art.", "labels": [], "entities": []}], "datasetContent": [{"text": "A variety of multi-domain datasets have been used for demonstrating MDL improvements.", "labels": [], "entities": [{"text": "MDL", "start_pos": 68, "end_pos": 71, "type": "TASK", "confidence": 0.947279155254364}]}, {"text": "In this paper, we focus on two datasets representative of many of the properties of MDL.", "labels": [], "entities": []}, {"text": "Amazon (AMAZON) Our first dataset is the MultiDomain Amazon data (version 2.0), first introduced by . The task is binary sentiment classification, in which Amazon product reviews are labeled as positive or negative.", "labels": [], "entities": [{"text": "Amazon (AMAZON)", "start_pos": 0, "end_pos": 15, "type": "DATASET", "confidence": 0.8526066541671753}, {"text": "MultiDomain Amazon data", "start_pos": 41, "end_pos": 64, "type": "DATASET", "confidence": 0.6731382608413696}, {"text": "binary sentiment classification", "start_pos": 114, "end_pos": 145, "type": "TASK", "confidence": 0.6791276137034098}]}, {"text": "Domains are defined byproduct categories.", "labels": [], "entities": []}, {"text": "We select the four domains used inmost studies: books, dvd, electronics and kitchen appliances.", "labels": [], "entities": []}, {"text": "The original dataset contained 2,000 reviews for each of the four domains, with 1,000 positive and 1,000 negative reviews per domain.", "labels": [], "entities": []}, {"text": "Feature extraction follows : we use case insensitive unigrams and bigrams, although we remove rare features (those that appear less than five times in the training set).", "labels": [], "entities": [{"text": "Feature extraction", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.755399078130722}]}, {"text": "The reduced feature set was selected given the sensitivity to feature size of some of the MDL methods.", "labels": [], "entities": []}, {"text": "ConVote (CONVOTE) Our second dataset is taken from segments of speech from United States Congress floor debates, first introduced by.", "labels": [], "entities": []}, {"text": "The binary classification task on this dataset is that of predicting whether a given speech segment supports or opposes a bill under discussion in the floor debate.", "labels": [], "entities": []}, {"text": "We select this dataset because, unlike the AMAZON data, CONVOTE can be divided into domains in several ways based on different metadata attributes available with the dataset.", "labels": [], "entities": [{"text": "AMAZON data", "start_pos": 43, "end_pos": 54, "type": "DATASET", "confidence": 0.860168844461441}]}, {"text": "We consider two types of domain divisions: the bill identifier and the political party of the speaker.", "labels": [], "entities": []}, {"text": "Division based on the bill creates domain differences in that each bill has its own topic.", "labels": [], "entities": []}, {"text": "Division based on political party implies preference for different issues and concerns, which manifest as different language.", "labels": [], "entities": []}, {"text": "We refer to these datasets as BILL and PARTY.", "labels": [], "entities": [{"text": "BILL", "start_pos": 30, "end_pos": 34, "type": "METRIC", "confidence": 0.997405469417572}, {"text": "PARTY", "start_pos": 39, "end_pos": 44, "type": "METRIC", "confidence": 0.9857608675956726}]}, {"text": "We use Version 1.1 of the CONVOTE dataset, available at http://www.cs.cornell.edu/ home/llee/data/convote.html.", "labels": [], "entities": [{"text": "CONVOTE dataset", "start_pos": 26, "end_pos": 41, "type": "DATASET", "confidence": 0.9576378762722015}]}, {"text": "More specifically, we combine the training, development and test folds from the data stage three/ version, and sub-sample to generate different versions of the dataset required for our experiments.", "labels": [], "entities": []}, {"text": "For BILL we randomly sample speech segments from three different bills.", "labels": [], "entities": [{"text": "BILL", "start_pos": 4, "end_pos": 8, "type": "METRIC", "confidence": 0.8871529698371887}]}, {"text": "The three bills and the number of instances for each were chosen such that we have sufficient data in each fold for every experiment.", "labels": [], "entities": []}, {"text": "For PARTY we randomly sample speech segments from the two major political parties (Democrats and Republicans).", "labels": [], "entities": [{"text": "PARTY", "start_pos": 4, "end_pos": 9, "type": "TASK", "confidence": 0.6683207750320435}]}, {"text": "Feature processing was identical to AMAZON, except that the threshold for feature removal was two.", "labels": [], "entities": [{"text": "AMAZON", "start_pos": 36, "end_pos": 42, "type": "METRIC", "confidence": 0.4419582486152649}, {"text": "feature removal", "start_pos": 74, "end_pos": 89, "type": "TASK", "confidence": 0.6373682469129562}]}], "tableCaptions": [{"text": " Table 1: A comparison between MDL methods with access to the \"True Domain\" labels and methods that  use \"Random Domain\" information, essentially ensemble learning. The first row has raw accuracy numbers,  whereas the remaining entries are absolute improvements over the baseline. : Significantly better than the  corresponding SVM or LR baseline, with p < 0.05, using a paired t-test. : Significantly worse than  corresponding baseline, with p < 0.05, using a paired t-test.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 187, "end_pos": 195, "type": "METRIC", "confidence": 0.9246931076049805}]}, {"text": " Table 2: The table shows the distribution of in- stances across domains and class labels within one  fold of each of the datasets, for four different class  bias trials. These datasets with varying class bias  across domains were used for the experiments de- scribed in  \u00a74.2", "labels": [], "entities": []}, {"text": " Table 3: A comparison between MDL methods with class biased data. Similar to the setup where we  evaluate the ensemble learning effect, we have a setting of using randomized domains. : Significantly  better than the corresponding SVM or LR baseline, with p < 0.05, using a paired t-test. : Significantly  worse than corresponding baseline, with p < 0.05, using a paired t-test.", "labels": [], "entities": []}, {"text": " Table 4: The table shows the distribution of in- stances across domains and class labels within one  fold of the AMAZON dataset, for four different class  bias trials. For the BILL and PARTY datasets, similar  folds with consistent bias were created (number of  examples used was different). These datasets with  consistent class bias across domains were used for  the experiments described in  \u00a74.2.1", "labels": [], "entities": [{"text": "AMAZON dataset", "start_pos": 114, "end_pos": 128, "type": "DATASET", "confidence": 0.9465802013874054}, {"text": "BILL", "start_pos": 177, "end_pos": 181, "type": "METRIC", "confidence": 0.9510440826416016}, {"text": "PARTY datasets", "start_pos": 186, "end_pos": 200, "type": "DATASET", "confidence": 0.7720508873462677}]}, {"text": " Table 5: A comparison between MDL methods with data that have a consistent class bias across domains.  Similar to the setup where we evaluate the ensemble learning effect, we have a setting of using randomized  domains. : Significantly better than the corresponding SVM or LR baseline, with p < 0.05, using a paired  t-test. : Significantly worse than corresponding baseline, with p < 0.05, using a paired t-test.", "labels": [], "entities": []}]}