{"title": [{"text": "Extending Machine Translation Evaluation Metrics with Lexical Cohesion To Document Level", "labels": [], "entities": [{"text": "Extending Machine Translation Evaluation Metrics", "start_pos": 0, "end_pos": 48, "type": "TASK", "confidence": 0.8757734775543213}]}], "abstractContent": [{"text": "This paper proposes the utilization of lexical cohesion to facilitate evaluation of machine translation at the document level.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 84, "end_pos": 103, "type": "TASK", "confidence": 0.7488590478897095}]}, {"text": "As a linguistic means to achieve text coherence, lexical cohesion ties sentences together into a meaningfully interwoven structure through words with the same or related meaning.", "labels": [], "entities": []}, {"text": "A comparison between machine and human translation is conducted to illustrate one of their critical distinctions that human translators tend to use more cohesion devices than machine.", "labels": [], "entities": []}, {"text": "Various ways to apply this feature to evaluate machine-translated documents are presented, including one without reliance on reference translation.", "labels": [], "entities": []}, {"text": "Experimental results show that incorporating this feature into sentence-level evaluation met-rics can enhance their correlation with human judgements.", "labels": [], "entities": []}], "introductionContent": [{"text": "Machine translation (MT) has benefited a lot from the advancement of automatic evaluation in the past decade.", "labels": [], "entities": [{"text": "Machine translation (MT)", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.8998714327812195}]}, {"text": "To a certain degree, its progress is also confined to the limitations of evaluation metrics in use.", "labels": [], "entities": []}, {"text": "Most efforts devoted to evaluate the quality of MT output so far have still focused on the sentence level without sufficient attention to how a larger text is structured.", "labels": [], "entities": [{"text": "MT output", "start_pos": 48, "end_pos": 57, "type": "TASK", "confidence": 0.9263354539871216}]}, {"text": "This is notably reflected in the representative MT evaluation metrics, such as BLEU (), METEOR (Banerjee and) and TER (), that adopt a sentence-by-sentence fashion to score MT outputs.", "labels": [], "entities": [{"text": "MT evaluation", "start_pos": 48, "end_pos": 61, "type": "TASK", "confidence": 0.8947567045688629}, {"text": "BLEU", "start_pos": 79, "end_pos": 83, "type": "METRIC", "confidence": 0.9983685612678528}, {"text": "METEOR", "start_pos": 88, "end_pos": 94, "type": "METRIC", "confidence": 0.9845066666603088}, {"text": "TER", "start_pos": 114, "end_pos": 117, "type": "METRIC", "confidence": 0.9920628666877747}, {"text": "MT outputs", "start_pos": 173, "end_pos": 183, "type": "TASK", "confidence": 0.9047805070877075}]}, {"text": "The evaluation result fora document by any of them is usually a simple average of its sentence scores.", "labels": [], "entities": []}, {"text": "A drawback of this kind of sentence-based evaluation is the neglect of document structure.", "labels": [], "entities": []}, {"text": "There is no guarantee for the coherence of a text if it is produced by simply putting together stand-alone sentences, no matter how well-translated, without adequate intersentential connection.", "labels": [], "entities": []}, {"text": "As a consequence, MT system optimized this way to any of these metrics can only have a very dim chance of producing translated document that reads as natural as human writing.", "labels": [], "entities": [{"text": "MT", "start_pos": 18, "end_pos": 20, "type": "TASK", "confidence": 0.9323863387107849}]}, {"text": "The accuracy of MT output at the document level is particularly important to MT users, for they care about the overall meaning of a text in question more than the grammatical correctness of each sentence.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.9991349577903748}, {"text": "MT output", "start_pos": 16, "end_pos": 25, "type": "TASK", "confidence": 0.9181737005710602}, {"text": "MT", "start_pos": 77, "end_pos": 79, "type": "TASK", "confidence": 0.9801577925682068}]}, {"text": "Post-editors particularly need to ensure the quality of a whole document of MT output when revising its sentences.", "labels": [], "entities": [{"text": "MT output", "start_pos": 76, "end_pos": 85, "type": "TASK", "confidence": 0.9062491059303284}]}, {"text": "The connectivity of sentences is surely a significant factor contributing to the understandability of a text as a whole.", "labels": [], "entities": []}, {"text": "This paper studies the inter-sentential linguistic features of cohesion and coherence and presents plausible ways to incorporate them into the sentence-based metrics to support MT evaluation at the document level.", "labels": [], "entities": [{"text": "MT evaluation", "start_pos": 177, "end_pos": 190, "type": "TASK", "confidence": 0.9711567759513855}]}, {"text": "In the Framework for MT Evaluation in the International Standards of Language Engineering (FEMTI) (), coherence is defined as \"the degree to which the reader can describe the role of each individual sentence (or group of sentences) with respect to the text as a whole\".", "labels": [], "entities": [{"text": "MT Evaluation", "start_pos": 21, "end_pos": 34, "type": "TASK", "confidence": 0.9591959416866302}, {"text": "International Standards of Language Engineering (FEMTI)", "start_pos": 42, "end_pos": 97, "type": "TASK", "confidence": 0.4489072747528553}]}, {"text": "The measurement of coherence has to rely on cohesion, referring to the \"relations of meaning that exist within the text\".", "labels": [], "entities": []}, {"text": "Cohesion is realized via the interlinkage of grammatical and lexical elements across sentences.", "labels": [], "entities": []}, {"text": "Grammatical cohesion refers to the syntactic links between text items, while lexical cohesion is achieved through the word choices in a text.", "labels": [], "entities": []}, {"text": "This paper focuses on the latter.", "labels": [], "entities": []}, {"text": "A quantitative comparison of lexical cohesion devices between MT output and human translation is first conducted, to examine the weakness of current MT systems in handling this feature.", "labels": [], "entities": [{"text": "MT output", "start_pos": 62, "end_pos": 71, "type": "TASK", "confidence": 0.8972606062889099}, {"text": "human translation", "start_pos": 76, "end_pos": 93, "type": "TASK", "confidence": 0.6844896823167801}, {"text": "MT", "start_pos": 149, "end_pos": 151, "type": "TASK", "confidence": 0.9630025029182434}]}, {"text": "Different ways of exploiting lexical cohesion devices for MT evaluation at the document level are then illustrated.", "labels": [], "entities": [{"text": "MT evaluation", "start_pos": 58, "end_pos": 71, "type": "TASK", "confidence": 0.9709782302379608}]}], "datasetContent": [{"text": "As a feature at the discourse level, lexical cohesion is a good complement to current evaluation metrics focusing on features at the sentence level.", "labels": [], "entities": []}, {"text": "illustrates an example selected from the MetricsMATR dataset, consisting two versions of MT output fora short document of two segments only.", "labels": [], "entities": [{"text": "MetricsMATR dataset", "start_pos": 41, "end_pos": 60, "type": "DATASET", "confidence": 0.9172021448612213}]}, {"text": "The n-grams matched with the reference are underlined, while the lexical cohesion devices are italicized.", "labels": [], "entities": []}, {"text": "The two MT outputs have a similar number of matched n-grams and hence receive similar BLEU scores.", "labels": [], "entities": [{"text": "MT", "start_pos": 8, "end_pos": 10, "type": "TASK", "confidence": 0.9455923438072205}, {"text": "BLEU", "start_pos": 86, "end_pos": 90, "type": "METRIC", "confidence": 0.999394416809082}]}, {"text": "These scores, however, do not reflect their real difference in quality: the second version is better, according to human assessment of adequacy.", "labels": [], "entities": []}, {"text": "Instead, their LC ratios seem to represent such a variation more accurately.", "labels": [], "entities": [{"text": "LC ratios", "start_pos": 15, "end_pos": 24, "type": "METRIC", "confidence": 0.9486300349235535}]}, {"text": "The theme of the second output is also highlighted through the lexical chains, including main/important, technology/technologies and achieve/achieving, which create a tight texture between the two sentences, a crucial factor of text quality.", "labels": [], "entities": []}, {"text": "To perform MT evaluation at the document level, the LC and RC ratios can be used alone or integrated into a sentence-level metric.", "labels": [], "entities": [{"text": "MT evaluation", "start_pos": 11, "end_pos": 24, "type": "TASK", "confidence": 0.9711547493934631}, {"text": "LC and RC ratios", "start_pos": 52, "end_pos": 68, "type": "METRIC", "confidence": 0.793123796582222}]}, {"text": "The former way has an advantage that it does not have to rely on any reference translation.", "labels": [], "entities": []}, {"text": "LC mainly requires a thesaurus for computing semantic relation, while RC only needs a morphological processor such as stemmer, both of which are available for most lan-guages.", "labels": [], "entities": []}, {"text": "Its drawback, however, lies in the risk of relying on a single discourse feature.", "labels": [], "entities": []}, {"text": "Although lexical cohesion gives a strong indication of text coherence, it is not indispensable, because a text can be coherent without any surface cohesive clue.", "labels": [], "entities": []}, {"text": "Furthermore, the quality of a document is also reflected in that of its sentences.", "labels": [], "entities": []}, {"text": "A coherent translation maybe mistranslated, and on the other hand, a text containing lots of sentence-level errors would make it difficult to determine its document-level quality.", "labels": [], "entities": []}, {"text": "A previous study comparing MT evaluation at the sentence versus document level) reports a poor consistency in the evaluation results at these two levels when the sentence-level scores of MT output are low.", "labels": [], "entities": [{"text": "MT evaluation", "start_pos": 27, "end_pos": 40, "type": "TASK", "confidence": 0.8914743363857269}, {"text": "consistency", "start_pos": 95, "end_pos": 106, "type": "METRIC", "confidence": 0.9819046854972839}]}, {"text": "In regard of these, how to integrate these two levels of MT evaluation is particularly worth studying.", "labels": [], "entities": [{"text": "MT evaluation", "start_pos": 57, "end_pos": 70, "type": "TASK", "confidence": 0.8870944082736969}]}, {"text": "We examine, through experiments, the effectiveness of using LC and RC ratios alone and integrating them into other evaluation metrics for MT evaluation at the document and system levels.", "labels": [], "entities": [{"text": "MT evaluation", "start_pos": 138, "end_pos": 151, "type": "TASK", "confidence": 0.9727799594402313}]}, {"text": "Three evaluation metrics, namely BLEU, TER and METEOR, 2 are selected for testing.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 33, "end_pos": 37, "type": "METRIC", "confidence": 0.9994022846221924}, {"text": "TER", "start_pos": 39, "end_pos": 42, "type": "METRIC", "confidence": 0.9983323216438293}, {"text": "METEOR", "start_pos": 47, "end_pos": 53, "type": "METRIC", "confidence": 0.9972545504570007}]}, {"text": "They represent three distinctive types of evaluation metrics: n-gram, editdistance, and unigram with external language resources, respectively.", "labels": [], "entities": []}, {"text": "These metrics are evaluated in terms of their correlation with human assessments, using Pearson's r correlation coefficient.", "labels": [], "entities": [{"text": "Pearson's r correlation coefficient", "start_pos": 88, "end_pos": 123, "type": "METRIC", "confidence": 0.7123694002628327}]}, {"text": "The MetricsMATR and MTC4 datasets and their adequacy assessments are used as evaluation data.", "labels": [], "entities": [{"text": "MTC4 datasets", "start_pos": 20, "end_pos": 33, "type": "DATASET", "confidence": 0.967042863368988}]}, {"text": "Note that the adequacy assessment is in fact an evaluation method for the sentence level.", "labels": [], "entities": []}, {"text": "We have to rely on an assumption that this evaluation data may emulate document-level quality, since its MT outputs were assessed sentence by sentence in sequence as in a document.", "labels": [], "entities": [{"text": "MT", "start_pos": 105, "end_pos": 107, "type": "TASK", "confidence": 0.9514133930206299}]}, {"text": "All experiments are performed under a setting of multiple reference translations.", "labels": [], "entities": []}, {"text": "The integration of the two ratios into an evaluation metric follows a simple weighted average approach.", "labels": [], "entities": []}, {"text": "A hybrid metric H is formulated as where m doc refers to the document-level feature in 2 METEOR 1.0 with default parameters optimized over the adequacy assessments.", "labels": [], "entities": [{"text": "METEOR", "start_pos": 89, "end_pos": 95, "type": "METRIC", "confidence": 0.8231163024902344}]}, {"text": "use (i.e., LC or RC), m seg to a sentence-level metric, and \u03b1 to a weight controlling their proportion.", "labels": [], "entities": []}, {"text": "The MetricsMATR dataset is used as training data to optimize the values of \u03b1 for different metrics, while the MTC4 is used as evaluation data.", "labels": [], "entities": [{"text": "MetricsMATR dataset", "start_pos": 4, "end_pos": 23, "type": "DATASET", "confidence": 0.8051159083843231}, {"text": "MTC4", "start_pos": 110, "end_pos": 114, "type": "DATASET", "confidence": 0.9274922609329224}]}, {"text": "shows the optimized weights for the metrics for evaluation at the document level.", "labels": [], "entities": []}, {"text": "presents the correlation rates of evaluation metrics obtained in our experiments under different settings, with their 95% conference intervals (CI) provided.", "labels": [], "entities": [{"text": "95% conference intervals (CI)", "start_pos": 118, "end_pos": 147, "type": "METRIC", "confidence": 0.824318894318172}]}, {"text": "The LC and RC ratios are found to have strong correlations with human assessments at the system level even when used alone, highly comparable to BLEU and TER.", "labels": [], "entities": [{"text": "LC", "start_pos": 4, "end_pos": 6, "type": "METRIC", "confidence": 0.9113300442695618}, {"text": "RC", "start_pos": 11, "end_pos": 13, "type": "METRIC", "confidence": 0.6790986061096191}, {"text": "BLEU", "start_pos": 145, "end_pos": 149, "type": "METRIC", "confidence": 0.9971629977226257}, {"text": "TER", "start_pos": 154, "end_pos": 157, "type": "METRIC", "confidence": 0.9523577690124512}]}, {"text": "At the document level, however, they are not as good as the others.", "labels": [], "entities": []}, {"text": "They show their advantages when integrated into other metrics, especially BLEU and TER.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 74, "end_pos": 78, "type": "METRIC", "confidence": 0.9969655871391296}, {"text": "TER", "start_pos": 83, "end_pos": 86, "type": "METRIC", "confidence": 0.9907861948013306}]}, {"text": "LC raises the correlation of BLEU from 0.447 to 0.472 and from 0.861 to 0.905 at the document and system levels, respectively.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 29, "end_pos": 33, "type": "METRIC", "confidence": 0.9983150959014893}]}, {"text": "It improves TER even more significantly, in that the correlation rates are boosted up from -0.326 to -0.390 at the document level, and even from -0.601 to -0.763 at the system level.", "labels": [], "entities": [{"text": "TER", "start_pos": 12, "end_pos": 15, "type": "METRIC", "confidence": 0.814021646976471}, {"text": "correlation", "start_pos": 53, "end_pos": 64, "type": "METRIC", "confidence": 0.9887784123420715}]}, {"text": "Since there are only six systems in the MTC4 data, such a dramatic change may not be as meaningful as the smooth improvement at the document level.", "labels": [], "entities": [{"text": "MTC4 data", "start_pos": 40, "end_pos": 49, "type": "DATASET", "confidence": 0.9070378541946411}]}, {"text": "ME-TEOR is a special casein this experiment.", "labels": [], "entities": [{"text": "ME-TEOR", "start_pos": 0, "end_pos": 7, "type": "METRIC", "confidence": 0.7792436480522156}]}, {"text": "Its correlation cannot be improved by integrating LC or RC, and is even slightly dropped at the document level.", "labels": [], "entities": [{"text": "correlation", "start_pos": 4, "end_pos": 15, "type": "METRIC", "confidence": 0.991390585899353}]}, {"text": "The cause for this is yet to be identified.", "labels": [], "entities": []}, {"text": "Nevertheless, these results confirm the close relationship of an MT system's capability to appropriately generate lexical cohesion devices with the quality of its output.   by combining sentence-and document-level metrics.", "labels": [], "entities": [{"text": "MT", "start_pos": 65, "end_pos": 67, "type": "TASK", "confidence": 0.9813406467437744}]}, {"text": "The table shows that the two ratios LC and RC highly correlate with each other, as if they are two variants of quantifying lexical cohesion devices.", "labels": [], "entities": [{"text": "RC", "start_pos": 43, "end_pos": 45, "type": "METRIC", "confidence": 0.7812976837158203}]}, {"text": "The three sentence-level metrics, BLEU, TER and METEOR, also show strong correlations with each other, especially between BLEU and METEOR.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 34, "end_pos": 38, "type": "METRIC", "confidence": 0.9986684322357178}, {"text": "TER", "start_pos": 40, "end_pos": 43, "type": "METRIC", "confidence": 0.9937478303909302}, {"text": "METEOR", "start_pos": 48, "end_pos": 54, "type": "METRIC", "confidence": 0.953025221824646}, {"text": "BLEU", "start_pos": 122, "end_pos": 126, "type": "METRIC", "confidence": 0.9951683282852173}, {"text": "METEOR", "start_pos": 131, "end_pos": 137, "type": "DATASET", "confidence": 0.8359654545783997}]}, {"text": "The correlations are generally weaker between sentenceand document-level metrics, for instance, 0.263 between BLEU and LC and only -0.097 between TER and LC, showing that they are quite heterogeneous in nature.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 110, "end_pos": 114, "type": "METRIC", "confidence": 0.9864287972450256}]}, {"text": "This accounts for the significant performance gain from their combination: their difference allows them to complement each other.", "labels": [], "entities": []}, {"text": "It is also worth noting that between METEOR and LC the correlation of 0.437 is mildly strong, explaining the negative result of their integration.", "labels": [], "entities": [{"text": "METEOR", "start_pos": 37, "end_pos": 43, "type": "METRIC", "confidence": 0.8561009764671326}]}, {"text": "On the one hand, lexical cohesion is word choice oriented, which is only sensitive to the reiteration and semantic relatedness of words in MT output.", "labels": [], "entities": [{"text": "MT output", "start_pos": 139, "end_pos": 148, "type": "TASK", "confidence": 0.863584578037262}]}, {"text": "On the other hand, METEOR is strong in unigram matching, with multiple strategies to maximize the matching rate between MT output and reference translation.", "labels": [], "entities": [{"text": "METEOR", "start_pos": 19, "end_pos": 25, "type": "METRIC", "confidence": 0.7056176662445068}, {"text": "unigram matching", "start_pos": 39, "end_pos": 55, "type": "TASK", "confidence": 0.7187036275863647}]}, {"text": "In this sense they are homogeneous to a certain extent, explaining the null effect of their combination.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Information about the datasets in use", "labels": [], "entities": []}, {"text": " Table 2. The frequencies are aver- aged by the number of MT/HT versions. A further  categorization breaks down content words into lex- ical cohesion devices and those that are not. The  count of each type of lexical cohesion device is also  provided. In general the two datasets provide highly  similar statistics. There are 4.7-5.1% more content  words in HT than in MT. The numbers of ordinary  content words (i.e., not lexical cohesion devices) are  close in MT and HT. The difference of content words", "labels": [], "entities": [{"text": "MT", "start_pos": 369, "end_pos": 371, "type": "TASK", "confidence": 0.7236902713775635}]}, {"text": " Table 3: An example of MT outputs of different quality (underlined: matched n-grams; italic: lexical cohesion devices)", "labels": [], "entities": [{"text": "MT outputs", "start_pos": 24, "end_pos": 34, "type": "TASK", "confidence": 0.9090482890605927}]}, {"text": " Table 4: Optimized weights for the integration of dis- course feature into sentence-level metrics", "labels": [], "entities": []}, {"text": " Table 5: Correlation of different metrics with adequacy assessment in MTC4 data", "labels": [], "entities": [{"text": "MTC4 data", "start_pos": 71, "end_pos": 80, "type": "DATASET", "confidence": 0.7962475419044495}]}, {"text": " Table 6: Correlation between the evaluation results of different metrics", "labels": [], "entities": []}]}