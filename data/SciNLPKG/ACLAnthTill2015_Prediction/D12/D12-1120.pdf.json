{"title": [{"text": "Biased Representation Learning for Domain Adaptation", "labels": [], "entities": [{"text": "Domain Adaptation", "start_pos": 35, "end_pos": 52, "type": "TASK", "confidence": 0.6821916252374649}]}], "abstractContent": [{"text": "Representation learning is a promising technique for discovering features that allow supervised classifiers to generalize from a source domain dataset to arbitrary new domains.", "labels": [], "entities": [{"text": "Representation learning", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.9716095328330994}]}, {"text": "We present a novel, formal statement of the representation learning task.", "labels": [], "entities": [{"text": "representation learning task", "start_pos": 44, "end_pos": 72, "type": "TASK", "confidence": 0.8638138373692831}]}, {"text": "We argue that because the task is computationally intractable in general, it is important fora representation learner to be able to incorporate expert knowledge during its search for helpful features.", "labels": [], "entities": []}, {"text": "Leveraging the Posterior Regularization framework, we develop an architecture for incorporating biases into representation learning.", "labels": [], "entities": [{"text": "Posterior Regularization", "start_pos": 15, "end_pos": 39, "type": "TASK", "confidence": 0.6110599637031555}]}, {"text": "We investigate three types of biases, and experiments on two domain adaptation tasks show that our biased learners identify significantly better sets of features than unbiased learners, resulting in a relative reduction in error of more than 16% for both tasks, with respect to existing state-of-the-art representation learning techniques.", "labels": [], "entities": [{"text": "error", "start_pos": 223, "end_pos": 228, "type": "METRIC", "confidence": 0.860926628112793}]}], "introductionContent": [{"text": "Supervised natural language processing (NLP) systems have been widely used and have achieved impressive performance on many NLP tasks.", "labels": [], "entities": []}, {"text": "However, they exhibit a significant drop-off in performance when tested on domains that differ from their training domains.)", "labels": [], "entities": []}, {"text": "One major cause for poor performance on out of-domain texts is the traditional representation used by supervised NLP systems).", "labels": [], "entities": []}, {"text": "Most systems depend on lexical features, which can differ greatly between domains, so that important words in the test data may never be seen in the training data.", "labels": [], "entities": []}, {"text": "The connection between words and labels may also change across domains.", "labels": [], "entities": []}, {"text": "For instance, \"signaling\" appears only as a present participle (VBG) in WSJ text (as in, \"signaling that...\"), but predominantly as a noun (as in \"signaling pathway\") in biomedical text.", "labels": [], "entities": []}, {"text": "Recently, several authors have found that learning new features based on distributional similarity can significantly improve domain adaptation).", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 125, "end_pos": 142, "type": "TASK", "confidence": 0.7628693580627441}]}, {"text": "This framework is attractive for several reasons: experimentally, learned features can yield significant improvements over standard supervised models on out-of-domain tests.", "labels": [], "entities": []}, {"text": "Moreover, since the representation-learning techniques are unsupervised, they can easily be applied to arbitrary new domains.", "labels": [], "entities": []}, {"text": "There is no need to supply additional labeled examples for each new domain.", "labels": [], "entities": []}, {"text": "Traditional representations still hold one significant advantage over representation-learning, however: because features are hand-crafted, these representations can readily incorporate the linguistic or domain expert knowledge that leads to state-ofthe-art in-domain performance.", "labels": [], "entities": []}, {"text": "In contrast, the only guide for existing representation-learning techniques is a corpus of unlabeled text.", "labels": [], "entities": []}, {"text": "To address this shortcoming, we introduce representation-learning techniques that incorporate a domain expert's preferences over the learned features.", "labels": [], "entities": []}, {"text": "For example, out of the set of all possible distributional-similarity features, we might prefer those that help predict the labels in a labeled training data set.", "labels": [], "entities": []}, {"text": "To capture this preference, we might bias a representation-learning algorithm towards features with low joint entropy with the labels in the training data.", "labels": [], "entities": []}, {"text": "This particular biased form of representation learning is a type of semi-supervised learning that allows our system to learn task-specific representations from a source domain's training data, rather than the single representation for all tasks produced by current, unsupervised representationlearning techniques.", "labels": [], "entities": [{"text": "representation learning", "start_pos": 31, "end_pos": 54, "type": "TASK", "confidence": 0.9076718389987946}]}, {"text": "We present a novel formal statement of representation learning, and demonstrate that it is computationally intractable in general.", "labels": [], "entities": [{"text": "representation learning", "start_pos": 39, "end_pos": 62, "type": "TASK", "confidence": 0.9437218308448792}]}, {"text": "It is therefore critical for representation learning to be flexible enough to incorporate the intuitions and knowledge of human experts, to guide the search for representations efficiently and effectively.", "labels": [], "entities": [{"text": "representation learning", "start_pos": 29, "end_pos": 52, "type": "TASK", "confidence": 0.9126011729240417}]}, {"text": "Leveraging the Posterior Regularization framework (, we present an architecture for learning representations for sequence-labeling tasks that allows for biases.", "labels": [], "entities": [{"text": "Posterior Regularization", "start_pos": 15, "end_pos": 39, "type": "TASK", "confidence": 0.512992262840271}]}, {"text": "In addition to a bias towards task-specific representations, we investigate a bias towards representations that have similar features across domains, to improve domain-independence; and a bias towards multi-dimensional representations, where different dimensions are independent of one another.", "labels": [], "entities": []}, {"text": "In this paper, we focus on incorporating the biases with HMM-type representations (Hidden Markov Model).", "labels": [], "entities": []}, {"text": "However, this technique can also be applied to other graphical model-based representations with little modification.", "labels": [], "entities": []}, {"text": "Our experiments show that on two different domain-adaptation tasks, our biased representations improve significantly over unbiased ones.", "labels": [], "entities": []}, {"text": "Ina part-of-speech tagging experiment, our best model provides a 25% relative reduction in error over a state-of-the-art Chinese POS tagger, and a 19% relative reduction in error over an unbiased representation from previous work.", "labels": [], "entities": [{"text": "part-of-speech tagging", "start_pos": 4, "end_pos": 26, "type": "TASK", "confidence": 0.6763487756252289}, {"text": "error", "start_pos": 91, "end_pos": 96, "type": "METRIC", "confidence": 0.866264283657074}, {"text": "error", "start_pos": 173, "end_pos": 178, "type": "METRIC", "confidence": 0.7715206742286682}]}, {"text": "The next section describes background and previous work.", "labels": [], "entities": []}, {"text": "Section 3 introduces our framework for learning biased representations.", "labels": [], "entities": []}, {"text": "Section 4 describes how we estimate parameters for the biased objective functions efficiently.", "labels": [], "entities": []}, {"text": "Section 5 details our experiments and results, and section 6 concludes and outlines directions for future work.", "labels": [], "entities": []}], "datasetContent": [{"text": "We tested the structured representations with biases on two NLP tasks: Chinese POS tagging and English NER.", "labels": [], "entities": [{"text": "Chinese POS tagging", "start_pos": 71, "end_pos": 90, "type": "TASK", "confidence": 0.6753044724464417}]}, {"text": "In both cases, we use a domain adaptation setting where no labeled data is available for the target domain -a particularly difficult setting, but one that provides a strong test for an NLP system's ability to generalize . In our work, we used a plain HMM for domain adaptation tasks in which there is labeled source data and unlabeled source and target data, but no labeled target data for training.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 24, "end_pos": 41, "type": "TASK", "confidence": 0.7125187516212463}, {"text": "domain adaptation", "start_pos": 259, "end_pos": 276, "type": "TASK", "confidence": 0.7166113555431366}]}, {"text": "Therefore, here, we use the HMM technique as a baseline, and build on it by including biases.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: POS tagging accuracy: The P-HMM+D+E tagger outperforms the unbiased HMM tagger and the  Stanford tagger on all target domains. The 'avg' column includes source-domain development data results. Differ- ences between the P-HMM+D+E and the Stanford tagger are statistically significant at p < 0.01 on average and on 11  out of 12 target domain. We used the two-tailed Chi-square test with Yates' correction.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 10, "end_pos": 21, "type": "TASK", "confidence": 0.7699036598205566}, {"text": "accuracy", "start_pos": 22, "end_pos": 30, "type": "METRIC", "confidence": 0.9163890480995178}]}, {"text": " Table 3: English Named Entity recognition results", "labels": [], "entities": [{"text": "English Named Entity recognition", "start_pos": 10, "end_pos": 42, "type": "TASK", "confidence": 0.7629872560501099}]}, {"text": " Table 4: Results of POS tagging and Named Entity  recognition tasks with different representations. With the  entropy-biased representation, the system has better per- formance on the task which the bias is trained for, but  worse performance on the other task.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 21, "end_pos": 32, "type": "TASK", "confidence": 0.8440582752227783}, {"text": "Named Entity  recognition", "start_pos": 37, "end_pos": 62, "type": "TASK", "confidence": 0.7127827405929565}]}]}