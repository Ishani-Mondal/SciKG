{"title": [{"text": "Generalized Higher-Order Dependency Parsing with Cube Pruning", "labels": [], "entities": [{"text": "Generalized Higher-Order Dependency Parsing", "start_pos": 0, "end_pos": 43, "type": "TASK", "confidence": 0.7043728306889534}]}], "abstractContent": [{"text": "State-of-the-art graph-based parsers use features over higher-order dependencies that rely on decoding algorithms that are slow and difficult to generalize.", "labels": [], "entities": []}, {"text": "On the other hand, transition-based dependency parsers can easily utilize such features without increasing the linear complexity of the shift-reduce system beyond a constant.", "labels": [], "entities": []}, {"text": "In this paper, we attempt to address this imbalance for graph-based parsing by generalizing the Eisner (1996) algorithm to handle arbitrary features over higher-order dependencies.", "labels": [], "entities": []}, {"text": "The generalization is at the cost of asymptotic efficiency.", "labels": [], "entities": []}, {"text": "To account for this, cube pruning for decoding is utilized (Chiang, 2007).", "labels": [], "entities": []}, {"text": "For the first time, label tuple and structural features such as valencies can be scored efficiently with third-order features in a graph-based parser.", "labels": [], "entities": []}, {"text": "Our parser achieves the state-of-art unlabeled accuracy of 93.06% and labeled accuracy of 91.86% on the standard test set for English, at a faster speed than a reimplementation of the third-order model of Koo et al.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 47, "end_pos": 55, "type": "METRIC", "confidence": 0.9885908961296082}, {"text": "accuracy", "start_pos": 78, "end_pos": 86, "type": "METRIC", "confidence": 0.6201328039169312}]}], "introductionContent": [{"text": "The trade-off between rich features and exact decoding in dependency parsing has been well documented).", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 58, "end_pos": 76, "type": "TASK", "confidence": 0.7264591455459595}]}, {"text": "Graph-based parsers typically tradeoff rich feature scope for exact (or near exact) decoding, whereas transition-based parsers make the opposite trade-off.", "labels": [], "entities": []}, {"text": "Recent research on both parsing paradigms has attempted to address this.", "labels": [], "entities": [{"text": "parsing paradigms", "start_pos": 24, "end_pos": 41, "type": "TASK", "confidence": 0.899683803319931}]}, {"text": "In the transition-based parsing literature, the focus has been on increasing the search space of the system at decoding time, as expanding the feature scope is often trivial and inmost cases only leads to a constant-time increase in parser complexity.", "labels": [], "entities": [{"text": "transition-based parsing", "start_pos": 7, "end_pos": 31, "type": "TASK", "confidence": 0.5576837658882141}]}, {"text": "The most common approach is to use beam search), but more principled dynamic programming solutions have been proposed.", "labels": [], "entities": [{"text": "beam search", "start_pos": 35, "end_pos": 46, "type": "TASK", "confidence": 0.8112821280956268}]}, {"text": "In all cases inference remains approximate, though a larger search space is explored.", "labels": [], "entities": []}, {"text": "In the graph-based parsing literature, the main thrust of research has been on extending the Eisner chart-parsing algorithm to incorporate higher-order features.", "labels": [], "entities": [{"text": "graph-based parsing", "start_pos": 7, "end_pos": 26, "type": "TASK", "confidence": 0.5952999889850616}]}, {"text": "A similar line of research investigated the use of integer linear programming (ILP) formulations of parsing (.", "labels": [], "entities": []}, {"text": "Both solutions allow for exact inference with higher-order features, but typically at a high cost in terms of efficiency.", "labels": [], "entities": []}, {"text": "Furthermore, specialized algorithms are required that deeply exploit the structural properties of the given model.", "labels": [], "entities": []}, {"text": "Upgrading a parser to score new types of higher-order dependencies thus requires significant changes to the underlying decoding algorithm.", "labels": [], "entities": []}, {"text": "This is in stark contrast to transition-based systems, which simply require the definition of new feature extractors.", "labels": [], "entities": []}, {"text": "In this paper, we abandon exact search in graphbased parsing in favor of freedom in feature scope.", "labels": [], "entities": []}, {"text": "We propose a parsing algorithm that keeps the backbone Eisner chart-parsing algorithm for first-order parsing unchanged.", "labels": [], "entities": [{"text": "parsing", "start_pos": 13, "end_pos": 20, "type": "TASK", "confidence": 0.9754292964935303}]}, {"text": "Incorporating higher-order features only involves changing the scoring function of potential parses in each chart cell by expanding the signature of each chart item to include all the nonlocal context required to compute features.", "labels": [], "entities": []}, {"text": "The core chart-parsing algorithm remains the same regardless of which features are incorporated.", "labels": [], "entities": []}, {"text": "To control complexity we use cube pruning) with the beam size kin each cell.", "labels": [], "entities": []}, {"text": "Furthermore, dynamic programming in the style of can be done by merging k-best items that are equivalent in scoring.", "labels": [], "entities": []}, {"text": "Thus, our method is an application of integrated decoding with a language model in MT to dependency parsing, which has previously been applied to constituent parsing.", "labels": [], "entities": [{"text": "MT", "start_pos": 83, "end_pos": 85, "type": "TASK", "confidence": 0.8708279132843018}, {"text": "dependency parsing", "start_pos": 89, "end_pos": 107, "type": "TASK", "confidence": 0.8291248977184296}, {"text": "constituent parsing", "start_pos": 146, "end_pos": 165, "type": "TASK", "confidence": 0.7181775271892548}]}, {"text": "However, unlike Huang, we only have one decoding pass and a single trained model, while Huang's constituent parser maintains a separate generative base model from a following discriminative re-ranking model.", "labels": [], "entities": []}, {"text": "We draw connections to related work in Section 6.", "labels": [], "entities": []}, {"text": "Our chart-based approximate search algorithm allows for features on dependencies of an arbitrary order -as well as over non-local structural properties of the parse trees -to be scored at will.", "labels": [], "entities": []}, {"text": "In this paper, we use first to third-order features of greater varieties than . Additionally, we look at higher-order dependency arclabel features, which is novel to graph-based parsing, though commonly exploited in transition-based parsing (.", "labels": [], "entities": []}, {"text": "This is because adding label tuple features would introduce a large constant factor of O(|L| 3 ), where |L| is the size of the label set L, into the complexity for exact thirdorder parsing.", "labels": [], "entities": [{"text": "exact thirdorder parsing", "start_pos": 164, "end_pos": 188, "type": "TASK", "confidence": 0.5235448181629181}]}, {"text": "In our formulation, only the topranked labelled arcs would survive in each cell.", "labels": [], "entities": []}, {"text": "As a result, label features can be scored without combinatorial explosion.", "labels": [], "entities": []}, {"text": "In addition, we explore the use of valency features counting how many modifiers a word can have on its left and right side.", "labels": [], "entities": []}, {"text": "In the past, only re-rankers on k-best lists of parses produced by a simpler model use such features due to the difficulty of incorporating them into search.", "labels": [], "entities": []}, {"text": "The final parser with all these features is both accurate and fast.", "labels": [], "entities": [{"text": "accurate", "start_pos": 49, "end_pos": 57, "type": "METRIC", "confidence": 0.9878051280975342}]}, {"text": "In standard experiments for English, the unlabeled attachment score (UAS) is 93.06%, and the labeled attachment score (LAS) is 91.86%.", "labels": [], "entities": [{"text": "unlabeled attachment score (UAS)", "start_pos": 41, "end_pos": 73, "type": "METRIC", "confidence": 0.8134726732969284}, {"text": "labeled attachment score (LAS)", "start_pos": 93, "end_pos": 123, "type": "METRIC", "confidence": 0.9057242572307587}]}, {"text": "The UAS score is state-of-art.", "labels": [], "entities": [{"text": "UAS score", "start_pos": 4, "end_pos": 13, "type": "DATASET", "confidence": 0.7229230999946594}]}, {"text": "The speed of our parser is 220 tokens per second, which is over 4 times faster than an exact third-order parser that at- tains UAS of 92.81% and comparable to the state-ofthe-art transition-based system of Zhang and Nivre (2011) that employs beam search.", "labels": [], "entities": []}], "datasetContent": [{"text": "We define the scoring function f (x, y) as a linear classifier between a vector of features and a corresponding weight vector, i.e., f (x, y) = w \u00b7 \u03c6(x, y).", "labels": [], "entities": []}, {"text": "The feature function \u03c6 decomposes with respect to scoring function f . We train the weights to optimize the first-best structure.", "labels": [], "entities": []}, {"text": "We use the max-loss variant of the margin infused relaxed algorithm (MIRA) () with a hamming-loss margin as is common in the dependency parsing literature.", "labels": [], "entities": [{"text": "margin infused relaxed algorithm (MIRA)", "start_pos": 35, "end_pos": 74, "type": "METRIC", "confidence": 0.7301206844193595}, {"text": "dependency parsing", "start_pos": 125, "end_pos": 143, "type": "TASK", "confidence": 0.81220743060112}]}, {"text": "MIRA only requires a first-best decoding algorithm, which in our case is the approximate chart-based parsing algorithms defined in Sections 3 and 4.", "labels": [], "entities": [{"text": "MIRA", "start_pos": 0, "end_pos": 4, "type": "TASK", "confidence": 0.8116253614425659}]}, {"text": "Because our decoding algorithm is approximate, this may lead to invalid updates given to the optimizer (Huang and  Fayong, 2012).", "labels": [], "entities": []}, {"text": "However, we found that ignoring or modifying such updates led to negligible differences in practice.", "labels": [], "entities": []}, {"text": "In all our experiments, we train MIRA for 8 epochs and use abeam of k = 5 during decoding.", "labels": [], "entities": [{"text": "MIRA", "start_pos": 33, "end_pos": 37, "type": "METRIC", "confidence": 0.7684621214866638}]}, {"text": "Both these values were determined on the English development data.", "labels": [], "entities": [{"text": "English development data", "start_pos": 41, "end_pos": 65, "type": "DATASET", "confidence": 0.9564654429753622}]}], "tableCaptions": [{"text": " Table 1: Comparing this work in terms of parsing accu- racy compared to state-of-the-art baselines on the English  test data. We also report results for a re-implementation  of exact first to third-order graph-based parsing and a re- implementation of", "labels": [], "entities": [{"text": "parsing", "start_pos": 42, "end_pos": 49, "type": "TASK", "confidence": 0.9714825749397278}, {"text": "English  test data", "start_pos": 107, "end_pos": 125, "type": "DATASET", "confidence": 0.8968935608863831}]}, {"text": " Table 3: Generalized higher-order parsing with cube  pruning using different feature sets.", "labels": [], "entities": [{"text": "higher-order parsing", "start_pos": 22, "end_pos": 42, "type": "TASK", "confidence": 0.5597012042999268}]}, {"text": " Table 4: Showing the effect of better search on accuracy  and speed on the English test data with a fixed model.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 49, "end_pos": 57, "type": "METRIC", "confidence": 0.999286949634552}, {"text": "speed", "start_pos": 63, "end_pos": 68, "type": "METRIC", "confidence": 0.9879419207572937}, {"text": "English test data", "start_pos": 76, "end_pos": 93, "type": "DATASET", "confidence": 0.838153084119161}]}, {"text": " Table 2: UAS/LAS for experiments on non-English treebanks. Numbers in bold are the highest scoring system. Zhang  and Nivre is a reimplementation of Zhang and Nivre (2011) with beams of size 64 and 256. Rush and Petrov are  the UAS results reported in Rush and Petrov (2012). N th -order exact are implementations of exact 1st-3rd order  dependency parsing.  \u2020 For reference, Zhang and Nivre (2011) report 86.0/84.4, which is previously the best result  reported on this data set.  \u2021 It should be noted that Rush and Petrov (2012) do not jointly optimize labeled and unlabeled  dependency structure, which we found to often help. This, plus extra features, accounts for the differences in UAS.", "labels": [], "entities": [{"text": "UAS/LAS", "start_pos": 10, "end_pos": 17, "type": "TASK", "confidence": 0.47547947367032367}, {"text": "dependency parsing", "start_pos": 339, "end_pos": 357, "type": "TASK", "confidence": 0.7142166793346405}]}]}