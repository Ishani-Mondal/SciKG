{"title": [{"text": "UTD-SRL: A Pipeline Architecture for Extracting Frame Semantic Structures", "labels": [], "entities": [{"text": "UTD-SRL", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.7385002374649048}]}], "abstractContent": [{"text": "This paper describes our system for the task of extracting frame semantic structures in SemEval-2007.", "labels": [], "entities": []}, {"text": "The system architecture uses two types of learning models in each part of the task: Support Vector Machines (SVM) and Maximum Entropy (ME).", "labels": [], "entities": []}, {"text": "Designed as a pipeline of classifiers, the semantic parsing system obtained competitive precision scores on the test data.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 43, "end_pos": 59, "type": "TASK", "confidence": 0.7140091359615326}, {"text": "precision", "start_pos": 88, "end_pos": 97, "type": "METRIC", "confidence": 0.9939457774162292}]}], "introductionContent": [{"text": "The SemEval-2007 task for extracting frame semantic structures relies on the human annotated data available in the FrameNet (FN) database.", "labels": [], "entities": [{"text": "extracting frame semantic structures", "start_pos": 26, "end_pos": 62, "type": "TASK", "confidence": 0.8182308077812195}, {"text": "FrameNet (FN) database", "start_pos": 115, "end_pos": 137, "type": "DATASET", "confidence": 0.7731216788291931}]}, {"text": "The Berkeley FrameNet project () is an ongoing effort of building a semantic lexicon for English based on the theory of frame semantics.", "labels": [], "entities": []}, {"text": "In frame semantics, the meaning of words or word expressions, also called target words (TW), comprises aspects of conceptual structures, or frames, that describe specific situations.", "labels": [], "entities": []}, {"text": "The semantic roles, or frame elements (FE), associated with a target word are locally defined in the frame evoked by the target word.", "labels": [], "entities": [{"text": "FE", "start_pos": 39, "end_pos": 41, "type": "METRIC", "confidence": 0.5197346806526184}]}, {"text": "Currently, the FN lexicon includes more than 135,000 sentences extracted from the British National Corpus containing more than 6,100 target words that evoke more than 825 semantic frames.", "labels": [], "entities": [{"text": "FN lexicon", "start_pos": 15, "end_pos": 25, "type": "DATASET", "confidence": 0.89397993683815}, {"text": "British National Corpus", "start_pos": 82, "end_pos": 105, "type": "DATASET", "confidence": 0.9525219996770223}]}, {"text": "For this task, we extended our previous work at Senseval-3 () by (1) experimenting with additional features, (2) adding new classification sub-tasks to accomplish all the requirements, and (3) integrating these sub-tasks into a pipeline architecture.", "labels": [], "entities": []}], "datasetContent": [{"text": "We report experimental results on all four classification sub-tasks.", "labels": [], "entities": []}, {"text": "In our experiments we trained two types of classification models for each sub-task: SVM and ME.", "labels": [], "entities": [{"text": "ME", "start_pos": 92, "end_pos": 94, "type": "METRIC", "confidence": 0.9035391807556152}]}, {"text": "In order to optimize the performance measure of each sub-task and to find the best configuration of classification models we used 20% of the sub-tasks training data as validation data.", "labels": [], "entities": []}, {"text": "lists the best configuration of classification models as well as the best sub-task results when running the experiments on the validation data.", "labels": [], "entities": []}, {"text": "For frame disambiguation, we obtained 76.71% accuracy compared to a baseline of 60.72% accuracy that always predicts the most annotated frame for each of the 556 target words.", "labels": [], "entities": [{"text": "frame disambiguation", "start_pos": 4, "end_pos": 24, "type": "TASK", "confidence": 0.7084782123565674}, {"text": "accuracy", "start_pos": 45, "end_pos": 53, "type": "METRIC", "confidence": 0.9993614554405212}, {"text": "accuracy", "start_pos": 87, "end_pos": 95, "type": "METRIC", "confidence": 0.9972705245018005}]}, {"text": "The results for GFLC and FELC sub-tasks listed in   The SemEval-2007 organizers provided fully annotated training files, a scorer to evaluate these training files, and testing files containing flat sentences.", "labels": [], "entities": [{"text": "FELC", "start_pos": 25, "end_pos": 29, "type": "METRIC", "confidence": 0.8067498207092285}, {"text": "The SemEval-2007 organizers", "start_pos": 52, "end_pos": 79, "type": "DATASET", "confidence": 0.7346989711125692}]}, {"text": "In the evaluation process, a semantic dependency graph corresponding to a fully system annotated sentence is created and then matched with its gold dependency graph.", "labels": [], "entities": []}, {"text": "The matching process not only evaluates every semantic structure of a target word, but also considers frame-to-frame and FE-to-FE graph relations between the semantic structures.", "labels": [], "entities": [{"text": "FE-to-FE", "start_pos": 121, "end_pos": 129, "type": "METRIC", "confidence": 0.987432599067688}]}, {"text": "In addition, various scoring options were considered: exact or partial frame matching, partial credit for evaluating the named entities, evaluation of the flat frame elements labels, and an option for matching only the frames in evaluation.", "labels": [], "entities": [{"text": "frame matching", "start_pos": 71, "end_pos": 85, "type": "TASK", "confidence": 0.6336564421653748}]}, {"text": "The evaluation for flat frame elements labels is similar with the evaluation performed at Senseval-3.", "labels": [], "entities": [{"text": "Senseval-3", "start_pos": 90, "end_pos": 100, "type": "DATASET", "confidence": 0.9258874654769897}]}, {"text": "The only difference is that for this scorer the FE boundaries must match exactly.", "labels": [], "entities": [{"text": "FE boundaries", "start_pos": 48, "end_pos": 61, "type": "METRIC", "confidence": 0.9674866199493408}]}, {"text": "In, we present the averaged precision, recall and F1 measures for evaluating the semantic dependency graphs and detecting the semantic frames on the testing files.", "labels": [], "entities": [{"text": "precision", "start_pos": 28, "end_pos": 37, "type": "METRIC", "confidence": 0.9854375123977661}, {"text": "recall", "start_pos": 39, "end_pos": 45, "type": "METRIC", "confidence": 0.9992343187332153}, {"text": "F1", "start_pos": 50, "end_pos": 52, "type": "METRIC", "confidence": 0.9996359348297119}]}, {"text": "The \"Options\" column represents the configuration parameters of the scorer: (E)xact/(P)artial frame matching, semantic (D)ependency or (L)abels only evaluation, and (Y)es/(N)o named entity evaluation.", "labels": [], "entities": []}, {"text": "Although the system achieved good precision scores on the test data, the recall values caused the system to obtain unsatisfactory F1-measure values.", "labels": [], "entities": [{"text": "precision", "start_pos": 34, "end_pos": 43, "type": "METRIC", "confidence": 0.9984295964241028}, {"text": "recall", "start_pos": 73, "end_pos": 79, "type": "METRIC", "confidence": 0.9992609620094299}, {"text": "F1-measure", "start_pos": 130, "end_pos": 140, "type": "METRIC", "confidence": 0.9970588684082031}]}, {"text": "We expect that the recall will increase by considering various heuristics fora better mapping of the frame elements to constituents in parse trees.", "labels": [], "entities": [{"text": "recall", "start_pos": 19, "end_pos": 25, "type": "METRIC", "confidence": 0.9996954202651978}]}], "tableCaptions": [{"text": " Table 1: Task results on the validation set.", "labels": [], "entities": [{"text": "validation", "start_pos": 30, "end_pos": 40, "type": "TASK", "confidence": 0.9517092704772949}]}, {"text": " Table 2: System results on the test set.", "labels": [], "entities": []}]}