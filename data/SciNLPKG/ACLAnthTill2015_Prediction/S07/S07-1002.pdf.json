{"title": [{"text": "Semeval-2007 Task 02: Evaluating Word Sense Induction and Discrimination Systems", "labels": [], "entities": [{"text": "Evaluating Word Sense Induction and Discrimination", "start_pos": 22, "end_pos": 72, "type": "TASK", "confidence": 0.8763481775919596}]}], "abstractContent": [{"text": "The goal of this task is to allow for comparison across sense-induction and discrimination systems, and also to compare these systems to other supervised and knowledge-based systems.", "labels": [], "entities": []}, {"text": "In total there were 6 participating systems.", "labels": [], "entities": []}, {"text": "We reused the SemEval-2007 English lexical sample subtask of task 17, and setup both clustering-style unsuper-vised evaluation (using OntoNotes senses as gold-standard) and a supervised evaluation (using the part of the dataset for mapping).", "labels": [], "entities": [{"text": "SemEval-2007 English lexical sample subtask of task 17", "start_pos": 14, "end_pos": 68, "type": "DATASET", "confidence": 0.7846680209040642}]}, {"text": "We provide a comparison to the results of the systems participating in the lexical sample subtask of task 17.", "labels": [], "entities": []}], "introductionContent": [{"text": "Word Sense Disambiguation (WSD) is a key enabling-technology.", "labels": [], "entities": [{"text": "Word Sense Disambiguation (WSD)", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.8079762756824493}]}, {"text": "Supervised WSD techniques are the best performing in public evaluations, but need large amounts of hand-tagging data.", "labels": [], "entities": [{"text": "WSD", "start_pos": 11, "end_pos": 14, "type": "TASK", "confidence": 0.9828711748123169}]}, {"text": "Existing hand-annotated corpora like, which is annotated with WordNet senses allow fora small improvement over the simple most frequent sense heuristic, as attested in the all-words track of the last Senseval competition).", "labels": [], "entities": []}, {"text": "In theory, larger amounts of training data (SemCor has approx. 500M words) would improve the performance of supervised WSD, but no current project exists to provide such an expensive resource.", "labels": [], "entities": [{"text": "WSD", "start_pos": 119, "end_pos": 122, "type": "TASK", "confidence": 0.9278930425643921}]}, {"text": "Another problem of the supervised approach is that the inventory and distribution of senses changes dramatically from one domain to the other, requiring additional hand-tagging of corpora).", "labels": [], "entities": []}, {"text": "Supervised WSD is based on the \"fixed-list of senses\" paradigm, where the senses fora target word area closed list coming from a dictionary or lexicon.", "labels": [], "entities": [{"text": "WSD", "start_pos": 11, "end_pos": 14, "type": "TASK", "confidence": 0.9132317304611206}]}, {"text": "Lexicographers and semanticists have long warned about the problems of such an approach, where senses are listed separately as discrete entities, and have argued in favor of more complex representations, where, for instance, senses are dense regions in a continuum.", "labels": [], "entities": []}, {"text": "Unsupervised Word Sense Induction and Discrimination (WSID, also known as corpus-based unsupervised systems) has followed this line of thinking, and tries to induce word senses directly from the corpus.", "labels": [], "entities": [{"text": "Unsupervised Word Sense Induction and Discrimination (WSID", "start_pos": 0, "end_pos": 58, "type": "TASK", "confidence": 0.7444504834711552}]}, {"text": "Typical WSID systems involve clustering techniques, which group together similar examples.", "labels": [], "entities": [{"text": "WSID", "start_pos": 8, "end_pos": 12, "type": "TASK", "confidence": 0.9584724307060242}]}, {"text": "Given a set of induced clusters (which represent word uses or senses 1 ), each new occurrence of the target word will be compared to the clusters and the most similar cluster will be selected as its sense.", "labels": [], "entities": []}, {"text": "One of the problems of unsupervised systems is that of managing to do a fair evaluation.", "labels": [], "entities": []}, {"text": "Most of current unsupervised systems are evaluated in-house, with a brief comparison to a re-implementation of a former system, leading to a proliferation of unsupervised systems with little ground to compare among them.", "labels": [], "entities": []}, {"text": "The goal of this task is to allow for comparison across sense-induction and discrimination systems, and also to compare these systems to other supervised and knowledge-based systems.", "labels": [], "entities": []}, {"text": "The paper is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 presents the evaluation framework used in this task.", "labels": [], "entities": []}, {"text": "Section 3 presents the systems that participated in the task, and the official results.", "labels": [], "entities": []}, {"text": "Finally, Section 5 draws the conclusions.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this setting the results of the systems are treated as clusters of examples and gold standard senses are classes.", "labels": [], "entities": []}, {"text": "In order to compare the clusters with the classes, hand annotated corpora is needed.", "labels": [], "entities": []}, {"text": "The test set is first tagged with the induced senses.", "labels": [], "entities": []}, {"text": "A perfect clustering solution will be the one where each cluster has exactly the same examples as one of the classes, and vice versa.", "labels": [], "entities": []}, {"text": "Following standard cluster evaluation practice (), we consider the FScore measure for measuring the performance of the systems.", "labels": [], "entities": [{"text": "FScore", "start_pos": 67, "end_pos": 73, "type": "METRIC", "confidence": 0.9470251798629761}]}, {"text": "The FScore is used in a similar fashion to Information Retrieval exercises, with precision and recall defined as the percentage of correctly \"retrieved\" examples fora cluster (divided by total cluster size), and recall as the percentage of correctly \"retrieved\" examples fora cluster (divided by total class size).", "labels": [], "entities": [{"text": "FScore", "start_pos": 4, "end_pos": 10, "type": "METRIC", "confidence": 0.572572648525238}, {"text": "Information Retrieval", "start_pos": 43, "end_pos": 64, "type": "TASK", "confidence": 0.819551944732666}, {"text": "precision", "start_pos": 81, "end_pos": 90, "type": "METRIC", "confidence": 0.9994391798973083}, {"text": "recall", "start_pos": 95, "end_pos": 101, "type": "METRIC", "confidence": 0.9970640540122986}, {"text": "recall", "start_pos": 212, "end_pos": 218, "type": "METRIC", "confidence": 0.9987609386444092}]}, {"text": "Given a particular class s r of size n rand a cluster hi of size n i , suppose n i r examples in the class s r belong to hi . The F value of this class and cluster is defined to be: where P (s r , hi ) = n i r nr is the precision value and R(s r , hi ) = n i r n i is the recall value defined for class s rand cluster hi . The FScore of class s r is the maximum F value attained at any cluster, that is, and the FScore of the entire clustering solution is: where q is the number of classes and n is the size of the clustering solution.", "labels": [], "entities": [{"text": "F value", "start_pos": 130, "end_pos": 137, "type": "METRIC", "confidence": 0.9737163484096527}, {"text": "precision", "start_pos": 220, "end_pos": 229, "type": "METRIC", "confidence": 0.998955488204956}, {"text": "recall", "start_pos": 272, "end_pos": 278, "type": "METRIC", "confidence": 0.9977860450744629}, {"text": "FScore", "start_pos": 327, "end_pos": 333, "type": "METRIC", "confidence": 0.9954987168312073}, {"text": "F value", "start_pos": 362, "end_pos": 369, "type": "METRIC", "confidence": 0.972004771232605}, {"text": "FScore", "start_pos": 412, "end_pos": 418, "type": "METRIC", "confidence": 0.9987772107124329}]}, {"text": "If the clustering is the identical to the original classes in the datasets, FScore will be equal to one which means that the higher the FScore, the better the clustering is.", "labels": [], "entities": [{"text": "FScore", "start_pos": 76, "end_pos": 82, "type": "METRIC", "confidence": 0.9943154454231262}, {"text": "FScore", "start_pos": 136, "end_pos": 142, "type": "METRIC", "confidence": 0.8524369597434998}]}, {"text": "For the sake of completeness we also include the standard entropy and purity measures in the unsupervised evaluation.", "labels": [], "entities": []}, {"text": "The entropy measure considers how the various classes of objects are distributed within each cluster.", "labels": [], "entities": []}, {"text": "In general, the smaller the entropy value, the better the clustering algorithm performs.", "labels": [], "entities": []}, {"text": "The purity measure considers the extent to which each cluster contained objects from primarily one class.", "labels": [], "entities": [{"text": "purity measure", "start_pos": 4, "end_pos": 18, "type": "METRIC", "confidence": 0.9703889191150665}]}, {"text": "The larger the values of purity, the better the clustering algorithm performs.", "labels": [], "entities": []}, {"text": "For a formal definition refer to ().", "labels": [], "entities": []}, {"text": "We have followed the supervised evaluation framework for evaluating WSID systems as described in).", "labels": [], "entities": [{"text": "WSID", "start_pos": 68, "end_pos": 72, "type": "TASK", "confidence": 0.8371540904045105}]}, {"text": "First, we split the corpus into a train/test part.", "labels": [], "entities": []}, {"text": "Using the hand-annotated sense information in the train part, we compute a mapping matrix M that relates clusters and senses in the following way.", "labels": [], "entities": []}, {"text": "Suppose there are m clusters and n senses for the target word.", "labels": [], "entities": []}, {"text": "Then, M = {m ij } 1 \u2264 i \u2264 m, 1 \u2264 j \u2264 n, and each m ij = P (s j |h i ), that is, m ij is the probability of a word having sense j given that it has been assigned cluster i.", "labels": [], "entities": []}, {"text": "This probability can be computed counting the times an occurrence with sense s j has been assigned cluster hi in the train corpus.", "labels": [], "entities": []}, {"text": "The mapping matrix is used to transform any cluster score vector \u00af h = (h 1 , . .", "labels": [], "entities": []}, {"text": ", h m ) returned by the WSID algorithm into a sense score vector \u00af s = (s 1 , . .", "labels": [], "entities": []}, {"text": "It suffices to multiply the score vector by M , i.e., \u00af s = \u00af hM . We use the M mapping matrix in order to convert the cluster score vector of each test corpus instance into a sense score vector, and assign the sense with maximum score to that instance.", "labels": [], "entities": []}, {"text": "Finally, the resulting test corpus is evaluated according to the usual precision and recall measures for supervised word sense disambiguation systems.", "labels": [], "entities": [{"text": "precision", "start_pos": 71, "end_pos": 80, "type": "METRIC", "confidence": 0.9992569088935852}, {"text": "recall", "start_pos": 85, "end_pos": 91, "type": "METRIC", "confidence": 0.9950188398361206}, {"text": "word sense disambiguation", "start_pos": 116, "end_pos": 141, "type": "TASK", "confidence": 0.6260379155476888}]}], "tableCaptions": [{"text": " Table 1: Number of occurrences for the 100 target words in", "labels": [], "entities": []}, {"text": " Table 2: Average number of clusters as returned by the par-", "labels": [], "entities": [{"text": "Average number of clusters", "start_pos": 10, "end_pos": 36, "type": "METRIC", "confidence": 0.9390540570020676}]}, {"text": " Table 3: Unsupervised evaluation on the test corpus (FScore),", "labels": [], "entities": [{"text": "FScore", "start_pos": 54, "end_pos": 60, "type": "METRIC", "confidence": 0.9149206280708313}]}, {"text": " Table 4: Supervised evaluation as recall. UBC-AS  *  was sub-", "labels": [], "entities": [{"text": "recall", "start_pos": 35, "end_pos": 41, "type": "METRIC", "confidence": 0.9990602135658264}]}, {"text": " Table 5: Comparing the best induction system in this task with", "labels": [], "entities": []}, {"text": " Table 6: Supervised evaluation as recall using a random", "labels": [], "entities": [{"text": "recall", "start_pos": 35, "end_pos": 41, "type": "METRIC", "confidence": 0.9931307435035706}]}]}