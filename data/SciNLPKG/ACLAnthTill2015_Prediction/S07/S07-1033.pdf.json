{"title": [], "abstractContent": [{"text": "Though the GYDER system has achieved the highest accuracy scores for the metonymy resolution shared task at SemEval-2007 in all six subtasks, we don't consider the results (72.80% accuracy for org, 84.36% for loc) particularly impressive , and argue that metonymy resolution needs more features.", "labels": [], "entities": [{"text": "GYDER", "start_pos": 11, "end_pos": 16, "type": "DATASET", "confidence": 0.8580352067947388}, {"text": "accuracy", "start_pos": 49, "end_pos": 57, "type": "METRIC", "confidence": 0.998214840888977}, {"text": "metonymy resolution shared task at SemEval-2007", "start_pos": 73, "end_pos": 120, "type": "TASK", "confidence": 0.8198413252830505}, {"text": "accuracy", "start_pos": 180, "end_pos": 188, "type": "METRIC", "confidence": 0.967521607875824}, {"text": "metonymy resolution", "start_pos": 255, "end_pos": 274, "type": "TASK", "confidence": 0.9337574541568756}]}], "introductionContent": [{"text": "In linguistics metonymy means using one term, or one specific sense of a term, to refer to another, related term or sense.", "labels": [], "entities": []}, {"text": "For example, in 'the penis mightier than the sword' pen refers to writing, the force of ideas, while sword refers to military force.", "labels": [], "entities": []}, {"text": "Named Entity Recognition (NER) is of key importance in numerous natural language processing applications ranging from information extraction to machine translation.", "labels": [], "entities": [{"text": "Named Entity Recognition (NER)", "start_pos": 0, "end_pos": 30, "type": "TASK", "confidence": 0.7635136892398199}, {"text": "information extraction", "start_pos": 118, "end_pos": 140, "type": "TASK", "confidence": 0.7948960065841675}, {"text": "machine translation", "start_pos": 144, "end_pos": 163, "type": "TASK", "confidence": 0.7683717906475067}]}, {"text": "Metonymic usage of named entities is frequent in natural language.", "labels": [], "entities": [{"text": "Metonymic usage of named entities", "start_pos": 0, "end_pos": 33, "type": "TASK", "confidence": 0.88074289560318}]}, {"text": "On the basic NER categories person, place, organisation state-of-the-art systems generally perform in the mid to the high nineties.", "labels": [], "entities": []}, {"text": "These systems typically do not distinguish between literal or metonymic usage of entity names, even though this would be helpful for most applications.", "labels": [], "entities": []}, {"text": "Resolving metonymic usage of proper names would therefore directly benefit NER and indirectly all NLP tasks (such as anaphor resolution) that require NER.", "labels": [], "entities": [{"text": "NER", "start_pos": 75, "end_pos": 78, "type": "TASK", "confidence": 0.7535834908485413}, {"text": "anaphor resolution", "start_pos": 117, "end_pos": 135, "type": "TASK", "confidence": 0.6894271224737167}]}, {"text": "Markert and Nissim (2002) outlined a corpusbased approach to proper name metonymy as a semantic classification problem that forms the basis of the 2007 SemEval metonymy resolution task.", "labels": [], "entities": [{"text": "semantic classification", "start_pos": 87, "end_pos": 110, "type": "TASK", "confidence": 0.7525138258934021}, {"text": "SemEval metonymy resolution task", "start_pos": 152, "end_pos": 184, "type": "TASK", "confidence": 0.8801541328430176}]}, {"text": "Instances like 'He was shocked by Vietnam' or 'Schengen boosted tourism' were assigned to broad categories like place-for-event, sometimes ignoring narrower distinctions, such as the fact that it wasn't the signing of the treaty at Schengen but rather its actual implementation (which didn't take place at Schengen) that boosted tourism.", "labels": [], "entities": []}, {"text": "But the corpus makes clear that even with these (sometimes coarse) class distinctions, several metonymy types seem to appear extremely rarely in actual texts.", "labels": [], "entities": []}, {"text": "The shared task focused on two broad named entity classes as metonymic sources, location and org, each having several target classes.", "labels": [], "entities": []}, {"text": "For more details on the data sets, seethe task description paper.", "labels": [], "entities": []}, {"text": "Several categories (e.g. place-for-event, organisation-for-index) did not contain a sufficient number of examples for machine learning, and we decided early onto accept the fact that these categories will not be learned and to concentrate on those classes where learning seemed feasible.", "labels": [], "entities": [{"text": "machine learning", "start_pos": 118, "end_pos": 134, "type": "TASK", "confidence": 0.7677489221096039}]}, {"text": "The shared task itself consisted of 3 subtasks of different granularity for both organisation and location names.", "labels": [], "entities": []}, {"text": "The fine-grained evaluation aimed at distinguishing between all categories, while the medium-grained evaluation grouped different types of metonymic usage together and addressed literal / mixed / metonymic usage.", "labels": [], "entities": []}, {"text": "The coarse-grained subtask was in fact a literal / nonliteral two-class classification task.", "labels": [], "entities": [{"text": "literal / nonliteral two-class classification task", "start_pos": 41, "end_pos": 91, "type": "TASK", "confidence": 0.63282310962677}]}, {"text": "Though GYDER has obtained the highest accuracy for the metonymy shared task at in all six subtasks, we don't consider the results (72.80% accuracy for org, 84.36% for loc) particularly impressive.", "labels": [], "entities": [{"text": "GYDER", "start_pos": 7, "end_pos": 12, "type": "DATASET", "confidence": 0.665236234664917}, {"text": "accuracy", "start_pos": 38, "end_pos": 46, "type": "METRIC", "confidence": 0.9988239407539368}, {"text": "accuracy", "start_pos": 138, "end_pos": 146, "type": "METRIC", "confidence": 0.969493567943573}]}, {"text": "In Section 3 we describe the feature engineering lessons learned from working on the task.", "labels": [], "entities": []}, {"text": "In Section 5 we offer some speculative remarks on what it would take to improve the results.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Accuracy of the submitted system", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9990059733390808}]}, {"text": " Table 2: Accuracy of the GYDER system for each  domain / granularity", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.998808741569519}]}, {"text": " Table 3: Per-class accuracies for both domains", "labels": [], "entities": [{"text": "accuracies", "start_pos": 20, "end_pos": 30, "type": "METRIC", "confidence": 0.6409052610397339}]}]}