{"title": [{"text": "UIUC: A Knowledge-rich Approach to Identifying Semantic Relations between Nominals", "labels": [], "entities": [{"text": "Identifying Semantic Relations between Nominals", "start_pos": 35, "end_pos": 82, "type": "TASK", "confidence": 0.8774173140525818}]}], "abstractContent": [{"text": "This paper describes a supervised, knowledge-intensive approach to the automatic identification of semantic relations between nominals in English sentences.", "labels": [], "entities": [{"text": "automatic identification of semantic relations between nominals in English sentences", "start_pos": 71, "end_pos": 155, "type": "TASK", "confidence": 0.8187401264905929}]}, {"text": "The system employs different sets of new and previously used lexical, syntactic, and semantic features extracted from various knowledge sources.", "labels": [], "entities": []}, {"text": "At SemEval 2007 the system achieved an F-measure of 72.4% and an accuracy of 76.3%.", "labels": [], "entities": [{"text": "SemEval 2007", "start_pos": 3, "end_pos": 15, "type": "DATASET", "confidence": 0.6690369248390198}, {"text": "F-measure", "start_pos": 39, "end_pos": 48, "type": "METRIC", "confidence": 0.9996416568756104}, {"text": "accuracy", "start_pos": 65, "end_pos": 73, "type": "METRIC", "confidence": 0.9996787309646606}]}], "introductionContent": [{"text": "The SemEval 2007 task on Semantic Relations between Nominals is to identify the underlying semantic relation between two nouns in the context of a sentence.", "labels": [], "entities": [{"text": "SemEval 2007 task on Semantic Relations between Nominals", "start_pos": 4, "end_pos": 60, "type": "TASK", "confidence": 0.7451286055147648}]}, {"text": "The dataset provided consists of a definition file and 140 training and about 70 test sentences for each of the seven relations considered: Cause-Effect, Instrument-Agency, ProductProducer, Origin-Entity, Theme-Tool, Part-Whole, and Content-Container.", "labels": [], "entities": []}, {"text": "The task is defined as a binary classification problem.", "labels": [], "entities": []}, {"text": "Thus, given a pair of nouns and their sentential context, the classifier decides whether the nouns are linked by the target semantic relation.", "labels": [], "entities": []}, {"text": "In each training and test example sentence, the nouns are identified and manually labeled with their corresponding WordNet 3.0 senses.", "labels": [], "entities": []}, {"text": "Moreover, each example is accompanied by the heuristic pattern (query) the annotators used to extract the sentence from the web and the position of the arguments in the relation.", "labels": [], "entities": []}, {"text": "041 \"He derives great joy and <e1>happiness</e1> from <e2>cycling</e2>.\"", "labels": [], "entities": []}, {"text": "WordNet(e1) = \"happiness%1:12:00::\", WordNet(e2) = \"cycling%1:04:00::\", Cause-Effect(e2,e1) = \"true\", Query = \"happiness from *\" Based on the information employed, systems can be classified in four types of classes: (A) systems that use neither the given WordNet synsets nor the queries, (B) systems that use only WordNet senses, (C) systems that use only the queries, and (D) systems that use both.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.9417065382003784}]}, {"text": "In this paper we present a type-B system that relies on various sets of new and previously used linguistic features employed in a supervised learning model.", "labels": [], "entities": []}], "datasetContent": [{"text": "For our experiments we chose libSVM, an open source SVM package 3 . Since some of our features are nominal, we followed the standard practice of representing a nominal feature with n discrete values as n binary features.", "labels": [], "entities": []}, {"text": "We used the RBF kernel.", "labels": [], "entities": [{"text": "RBF kernel", "start_pos": 12, "end_pos": 22, "type": "DATASET", "confidence": 0.9242074191570282}]}, {"text": "We built a binary classifier for each of the seven relations.", "labels": [], "entities": []}, {"text": "Since the size of the task training data per relation is small, we expanded it with new examples from various sources.", "labels": [], "entities": []}, {"text": "We added anew corpus of 3,000 sentences of news articles from the TREC-9 text collection and identified selectional restrictions that differentiate between containers and locations relying on taxonomies of spatial entities discussed in detail in and.", "labels": [], "entities": [{"text": "TREC-9 text collection", "start_pos": 66, "end_pos": 88, "type": "DATASET", "confidence": 0.9354376991589864}]}, {"text": "Each instance in this text collection had the target nouns identified and annotated with WordNet senses.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 89, "end_pos": 96, "type": "DATASET", "confidence": 0.9290193319320679}]}, {"text": "Since the annotations used different WordNet versions, senses were mapped to sense keys.", "labels": [], "entities": []}, {"text": "shows the performance of our system for each semantic relation.", "labels": [], "entities": []}, {"text": "Base-F indicates the baseline F-measure (all true), while Base-Acc shows the baseline accuracy score (majority).", "labels": [], "entities": [{"text": "F-measure", "start_pos": 30, "end_pos": 39, "type": "METRIC", "confidence": 0.6981856226921082}, {"text": "accuracy score", "start_pos": 86, "end_pos": 100, "type": "METRIC", "confidence": 0.9725671708583832}]}, {"text": "The Average score of precision, recall, F-measure, and accuracy is macroaveraged overall seven relations.", "labels": [], "entities": [{"text": "Average score", "start_pos": 4, "end_pos": 17, "type": "METRIC", "confidence": 0.9790759384632111}, {"text": "precision", "start_pos": 21, "end_pos": 30, "type": "METRIC", "confidence": 0.9930551648139954}, {"text": "recall", "start_pos": 32, "end_pos": 38, "type": "METRIC", "confidence": 0.999303936958313}, {"text": "F-measure", "start_pos": 40, "end_pos": 49, "type": "METRIC", "confidence": 0.997785210609436}, {"text": "accuracy", "start_pos": 55, "end_pos": 63, "type": "METRIC", "confidence": 0.9995556473731995}]}, {"text": "Overall, all features contributed to the performance, with a different contribution per relation (cf.).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Performance obtained per relation. Precision, Recall, F-measure, Accuracy, and Total (number of examples) are macro-", "labels": [], "entities": [{"text": "Precision", "start_pos": 45, "end_pos": 54, "type": "METRIC", "confidence": 0.9986940026283264}, {"text": "Recall", "start_pos": 56, "end_pos": 62, "type": "METRIC", "confidence": 0.9981575608253479}, {"text": "F-measure", "start_pos": 64, "end_pos": 73, "type": "METRIC", "confidence": 0.9945878982543945}, {"text": "Accuracy", "start_pos": 75, "end_pos": 83, "type": "METRIC", "confidence": 0.9992994070053101}, {"text": "Total", "start_pos": 89, "end_pos": 94, "type": "METRIC", "confidence": 0.9970263838768005}]}]}