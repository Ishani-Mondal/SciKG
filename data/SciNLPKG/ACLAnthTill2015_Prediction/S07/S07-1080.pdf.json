{"title": [{"text": "UCB: System Description for SemEval Task #4", "labels": [], "entities": [{"text": "SemEval Task #", "start_pos": 28, "end_pos": 42, "type": "TASK", "confidence": 0.8991657098134359}]}], "abstractContent": [{"text": "The UC Berkeley team participated in the SemEval 2007 Task #4, with an approach that leverages the vast size of the Web in order to build lexically-specific features.", "labels": [], "entities": [{"text": "SemEval 2007 Task #4", "start_pos": 41, "end_pos": 61, "type": "TASK", "confidence": 0.8635525941848755}]}, {"text": "The idea is to determine which verbs, prepositions , and conjunctions are used in sentences containing a target word pair, and to compare those to features extracted for other word pairs in order to determine which are most similar.", "labels": [], "entities": []}, {"text": "By combining these Web features with words from the sentence context, our team was able to achieve the best results for systems of category C and third best for systems of category A.", "labels": [], "entities": []}], "introductionContent": [{"text": "Semantic relation classification is an important but understudied language problem arising in many NLP applications, including question answering, information retrieval, machine translation, word sense disambiguation, information extraction, etc.", "labels": [], "entities": [{"text": "Semantic relation classification", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.8346394697825114}, {"text": "question answering", "start_pos": 127, "end_pos": 145, "type": "TASK", "confidence": 0.8848559558391571}, {"text": "information retrieval", "start_pos": 147, "end_pos": 168, "type": "TASK", "confidence": 0.7987975478172302}, {"text": "machine translation", "start_pos": 170, "end_pos": 189, "type": "TASK", "confidence": 0.8003741800785065}, {"text": "word sense disambiguation", "start_pos": 191, "end_pos": 216, "type": "TASK", "confidence": 0.6655014157295227}, {"text": "information extraction", "start_pos": 218, "end_pos": 240, "type": "TASK", "confidence": 0.8238372802734375}]}, {"text": "This year's SemEval (previously SensEval) competition has included a task targeting the important special case of Classification of Semantic Relations between Nominals.", "labels": [], "entities": [{"text": "SemEval (previously SensEval) competition", "start_pos": 12, "end_pos": 53, "type": "TASK", "confidence": 0.7491820752620697}, {"text": "Classification of Semantic Relations between Nominals", "start_pos": 114, "end_pos": 167, "type": "TASK", "confidence": 0.8864721655845642}]}, {"text": "In the present paper we describe the UCB system which took part in that competition.", "labels": [], "entities": [{"text": "UCB", "start_pos": 37, "end_pos": 40, "type": "DATASET", "confidence": 0.9079846739768982}]}, {"text": "The SemEval dataset contains a total of 7 semantic relations (not exhaustive and possibly overlapping), with 140 training and about 70 testing sentences per relation.", "labels": [], "entities": [{"text": "SemEval dataset", "start_pos": 4, "end_pos": 19, "type": "DATASET", "confidence": 0.8026548624038696}]}, {"text": "Sentence classes are approximately 50% negative and 50% positive (\"near misses\").", "labels": [], "entities": []}, {"text": "lists the 7 relations together with some examples.", "labels": [], "entities": []}], "datasetContent": [{"text": "Participants were asked to classify their systems into categories depending on whether they used the WordNet sense (WN) and/or the Google query (GC).", "labels": [], "entities": [{"text": "WordNet sense (WN)", "start_pos": 101, "end_pos": 119, "type": "DATASET", "confidence": 0.9127439737319947}]}, {"text": "Our team submitted runs for categories A (WN=no, QC=no) and C (WN=no, QC=yes) only, since we believe that having the target entities annotated with the correct WordNet senses is an unrealistic assumption fora real-world application.", "labels": [], "entities": []}, {"text": "Following and  and the vector of each of the training examples.", "labels": [], "entities": []}, {"text": "If there was a single highest-scoring training example, we predicted its class for that test example.", "labels": [], "entities": []}, {"text": "Otherwise, if there were ties for first, we assumed the class predicted by the majority of the tied examples.", "labels": [], "entities": []}, {"text": "If there was no majority, we predicted the class that was most likely on the training data.", "labels": [], "entities": []}, {"text": "Regardless of the classifier's prediction, if the head words of the two entities e 1 and e 2 had the same lemma, we classified that example as negative. and 4 show the results for our A and C runs for different amounts of training data: 45 (A1, C1), 90 (A2, C2), 105 (A3, C3) and 140 (A4, C4).", "labels": [], "entities": []}, {"text": "All results are above the baseline: always propose the majority label (\"true\"/\"false\") in the test set.", "labels": [], "entities": []}, {"text": "In fact, our category C system is the best-performing (in terms of F and Acc) among the participating systems, and we achieved the third best results for category A.", "labels": [], "entities": [{"text": "F", "start_pos": 67, "end_pos": 68, "type": "METRIC", "confidence": 0.997082531452179}, {"text": "Acc", "start_pos": 73, "end_pos": 76, "type": "METRIC", "confidence": 0.9349490404129028}]}, {"text": "Our category C results are slightly but consistently better than for A for all measures (P , R, F , Acc), which suggests that knowing the query is helpful.", "labels": [], "entities": [{"text": "Acc", "start_pos": 100, "end_pos": 103, "type": "METRIC", "confidence": 0.9496945142745972}]}, {"text": "Interestingly, systems UCB-A2 and UCB-C2 performed worse than UCB-A1 and UCB-C1, which means that having more training data does not necessarily help with a 1NN classifier.", "labels": [], "entities": []}, {"text": "shows additional analysis for A4 and C4.", "labels": [], "entities": []}, {"text": "We study the effect of adding extra Google contexts (using up to 10 stars, rather than 8), and using different subsets of features.", "labels": [], "entities": []}, {"text": "We show the results for: (a) leave-one-out cross-validation on the training data, (b) on the test data, and (c) our official UCB runs.", "labels": [], "entities": [{"text": "UCB", "start_pos": 125, "end_pos": 128, "type": "DATASET", "confidence": 0.7969878315925598}]}], "tableCaptions": [{"text": " Table 3: Task 4 results. UCB systems A1-A4.", "labels": [], "entities": [{"text": "UCB systems A1-A4", "start_pos": 26, "end_pos": 43, "type": "DATASET", "confidence": 0.9328848123550415}]}, {"text": " Table 4: Task 4 results. UCB systems C1-C4.", "labels": [], "entities": [{"text": "UCB", "start_pos": 26, "end_pos": 29, "type": "DATASET", "confidence": 0.9098207950592041}]}]}