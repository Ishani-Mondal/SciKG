{"title": [{"text": "NUS-ML: Improving Word Sense Disambiguation Using Topic Features", "labels": [], "entities": [{"text": "Improving Word Sense Disambiguation", "start_pos": 8, "end_pos": 43, "type": "TASK", "confidence": 0.8437172174453735}]}], "abstractContent": [{"text": "We participated in SemEval-1 English coarse-grained all-words task (task 7), En-glish fine-grained all-words task (task 17, subtask 3) and English coarse-grained lexical sample task (task 17, subtask 1).", "labels": [], "entities": [{"text": "SemEval-1 English coarse-grained all-words task", "start_pos": 19, "end_pos": 66, "type": "TASK", "confidence": 0.7293035864830018}]}, {"text": "The same method with different labeled data is used for the tasks; SemCor is the labeled corpus used to train our system for the all-words tasks while the labeled corpus that is provided is used for the lexical sample task.", "labels": [], "entities": []}, {"text": "The knowledge sources include part-of-speech of neighboring words, single words in the surrounding context, local col-locations, and syntactic patterns.", "labels": [], "entities": []}, {"text": "In addition , we constructed a topic feature, targeted to capture the global context information, using the latent dirichlet allocation (LDA) algorithm with unlabeled corpus.", "labels": [], "entities": []}, {"text": "A modified na\u00a8\u0131vena\u00a8\u0131ve Bayes classifier is constructed to incorporate all the features.", "labels": [], "entities": []}, {"text": "We achieved 81.6%, 57.6%, 88.7% for coarse-grained all-words task, fine-grained all-words task and coarse-grained lexical sample task respectively .", "labels": [], "entities": []}], "introductionContent": [{"text": "Supervised corpus-based approach has been the most successful in WSD to date.", "labels": [], "entities": [{"text": "WSD", "start_pos": 65, "end_pos": 68, "type": "TASK", "confidence": 0.9493860602378845}]}, {"text": "However, this approach faces severe data scarcity problem, resulting features being sparsely represented in the training data.", "labels": [], "entities": []}, {"text": "This problem is especially prominent for the bag-of-words feature.", "labels": [], "entities": []}, {"text": "A direct consequence is that the global context information, which the bag-ofwords feature is supposed to capture, maybe poorly represented.", "labels": [], "entities": []}, {"text": "Our system tries to address this problem by clustering features to relieve the scarcity problem, specifically on the bag-of-words feature.", "labels": [], "entities": []}, {"text": "In the process, we construct topic features, trained using the latent dirichlet allocation (LDA) algorithm.", "labels": [], "entities": []}, {"text": "We train the topic model () on unlabeled data, clustering the words occurring in the corpus to a predefined number of topics.", "labels": [], "entities": []}, {"text": "We then use the resulting topic model to tag the bag-of-words in the labeled corpus with topic distributions.", "labels": [], "entities": []}, {"text": "We incorporate the distributions, called the topic features, using a simple Bayesian network, modified from na\u00a8\u0131vena\u00a8\u0131ve Bayes model, alongside other features and train the model on the labeled corpus.", "labels": [], "entities": []}], "datasetContent": [{"text": "We describe here the experimental setup on the English lexical sample task and all-words task.", "labels": [], "entities": []}, {"text": "Note that we do not distinguish the two all-words tasks as the same parameters will be applied.", "labels": [], "entities": []}, {"text": "For lexical sample task, we use 5-fold cross validation on the training data provided to determine our parameters.", "labels": [], "entities": []}, {"text": "For all-words task, we use SemCor as our training data and validate on Senseval-2 and Senseval-3 all-words test data.", "labels": [], "entities": [{"text": "Senseval-3 all-words test data", "start_pos": 86, "end_pos": 116, "type": "DATASET", "confidence": 0.5995413064956665}]}, {"text": "We use MXPOST tagger (Adwait, 1996) for POS tagging, Charniak parser) for extracting syntactic relations, and David Blei's version of LDA 1 for LDA training and inference.", "labels": [], "entities": [{"text": "MXPOST tagger (Adwait, 1996)", "start_pos": 7, "end_pos": 35, "type": "TASK", "confidence": 0.5451505822794778}, {"text": "POS tagging", "start_pos": 40, "end_pos": 51, "type": "TASK", "confidence": 0.7513116896152496}]}, {"text": "All default parameters are used unless mentioned otherwise.", "labels": [], "entities": []}, {"text": "For the all-word tasks, we use sense 1 as back-off for words that have not appeared in SemCor.", "labels": [], "entities": []}, {"text": "We use the same fine-grained system for both the coarse and fine-grained all-words tasks.", "labels": [], "entities": []}, {"text": "We make predictions for all words for all the systems -precision, recall and accuracy scores are all the same.", "labels": [], "entities": [{"text": "precision", "start_pos": 55, "end_pos": 64, "type": "METRIC", "confidence": 0.9995958209037781}, {"text": "recall", "start_pos": 66, "end_pos": 72, "type": "METRIC", "confidence": 0.9989016056060791}, {"text": "accuracy scores", "start_pos": 77, "end_pos": 92, "type": "METRIC", "confidence": 0.9784969389438629}]}, {"text": "Baseline features For lexical sample task, we choose P = 3 and G = 3.", "labels": [], "entities": []}, {"text": "For all-words task, we choose P = 3 and G = 1.", "labels": [], "entities": []}, {"text": "(G = 1 means only the nearest word prior and after the test word.)", "labels": [], "entities": [{"text": "G", "start_pos": 1, "end_pos": 2, "type": "METRIC", "confidence": 0.9862253665924072}]}, {"text": "Smoothing For all standard baseline features, we use Laplace smoothing but for the soft tag (equation), we use a smoothing parameter value of 2 for all-words task and 0.1 for lexical sample task.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 4: Analysis on different POS on English lexi- cal sample task", "labels": [], "entities": [{"text": "POS", "start_pos": 32, "end_pos": 35, "type": "METRIC", "confidence": 0.8860872387886047}]}]}