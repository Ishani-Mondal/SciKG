{"title": [{"text": "FBK-IRST: Kernel Methods for Semantic Relation Extraction", "labels": [], "entities": [{"text": "FBK-IRST", "start_pos": 0, "end_pos": 8, "type": "DATASET", "confidence": 0.9393747448921204}, {"text": "Semantic Relation Extraction", "start_pos": 29, "end_pos": 57, "type": "TASK", "confidence": 0.8562566439310709}]}], "abstractContent": [{"text": "We present an approach for semantic relation extraction between nominals that combines shallow and deep syntactic processing and semantic information using kernel methods.", "labels": [], "entities": [{"text": "semantic relation extraction between nominals", "start_pos": 27, "end_pos": 72, "type": "TASK", "confidence": 0.8136525511741638}]}, {"text": "Two information sources are considered: (i) the whole sentence where the relation appears, and (ii) WordNet synsets and hypernymy relations of the candidate nom-inals.", "labels": [], "entities": []}, {"text": "Each source of information is represented by kernel functions.", "labels": [], "entities": []}, {"text": "In particular , five basic kernel functions are linearly combined and weighted under different conditions.", "labels": [], "entities": []}, {"text": "The experiments were carried out using support vector machines as classifier.", "labels": [], "entities": []}, {"text": "The system achieves an overall F 1 of 71.8% on the Classification of Semantic Relations between Nominals task at SemEval-2007.", "labels": [], "entities": [{"text": "F 1", "start_pos": 31, "end_pos": 34, "type": "METRIC", "confidence": 0.9847327172756195}, {"text": "Classification of Semantic Relations between Nominals task", "start_pos": 51, "end_pos": 109, "type": "TASK", "confidence": 0.8451923983437675}]}], "introductionContent": [{"text": "The starting point of our research is an approach for identifying relations between named entities exploiting only shallow linguistic information, such as tokenization, sentence splitting, part-of-speech tagging and lemmatization ().", "labels": [], "entities": [{"text": "tokenization", "start_pos": 155, "end_pos": 167, "type": "TASK", "confidence": 0.9468924403190613}, {"text": "sentence splitting", "start_pos": 169, "end_pos": 187, "type": "TASK", "confidence": 0.7023003548383713}, {"text": "part-of-speech tagging", "start_pos": 189, "end_pos": 211, "type": "TASK", "confidence": 0.6928226500749588}]}, {"text": "A combination of kernel functions is used to represent two distinct information sources: (i) the global context where entities appear and (ii) their local contexts.", "labels": [], "entities": []}, {"text": "The whole sentence where the entities appear (global context) is used to discover the presence of a relation between two entities.", "labels": [], "entities": []}, {"text": "Windows of limited size around the entities (local contexts) provide useful clues to identify the roles played by the entities within a relation (e.g., agent and target of a gene interaction).", "labels": [], "entities": []}, {"text": "In the task of detecting protein-protein interactions, we obtained state-of-the-art results on two biomedical data sets.", "labels": [], "entities": []}, {"text": "In addition, promising results have been recently obtained for relations such as work for and org based in in the news domain . In this paper, we investigate the use of the above approach to discover semantic relations between nominals.", "labels": [], "entities": []}, {"text": "In addition to the original feature representation, we have integrated deep syntactic processing of the global context and semantic information for each candidate nominals using WordNet as external knowledge source.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 178, "end_pos": 185, "type": "DATASET", "confidence": 0.9629474878311157}]}, {"text": "Each source of information is represented by kernel functions.", "labels": [], "entities": []}, {"text": "A tree kernel) is used to exploit the deep syntactic processing obtained using the Charniak parser).", "labels": [], "entities": []}, {"text": "On the other hand, bag of synonyms and hypernyms is used to enhance the representation of the candidate nominals.", "labels": [], "entities": []}, {"text": "The final system is based on five basic kernel functions (bag-ofwords kernel, global context kernel, tree kernel, supersense kernel, bag of synonyms and hypernyms kernel) linearly combined and weighted under different conditions.", "labels": [], "entities": []}, {"text": "The experiments were carried out using support vector machines as classifier.", "labels": [], "entities": []}, {"text": "We present results on the Classification of Semantic Relations between Nominals task at SemEval-2007, in which sentences containing ordered pairs of marked nominals, possibly semantically related, have to be classified.", "labels": [], "entities": [{"text": "Classification of Semantic Relations between Nominals task at SemEval-2007", "start_pos": 26, "end_pos": 100, "type": "TASK", "confidence": 0.7554318308830261}]}, {"text": "On this task, we achieve an overall F 1 of 71.8% (B category evaluation), largely outperforming all the baselines.", "labels": [], "entities": [{"text": "F 1", "start_pos": 36, "end_pos": 39, "type": "METRIC", "confidence": 0.9917904138565063}, {"text": "B category evaluation)", "start_pos": 50, "end_pos": 72, "type": "METRIC", "confidence": 0.9409675300121307}]}], "datasetContent": [{"text": "Sentences have been tokenized, lemmatized, and POS tagged with TextPro . We considered each relation as a different binary classification task, and each sentence in the data set is a positive or negative example for the relation.", "labels": [], "entities": [{"text": "TextPro", "start_pos": 63, "end_pos": 70, "type": "DATASET", "confidence": 0.9247100353240967}]}, {"text": "The direction of the relation is considered labelling the first argument of the relation as agent and the second as target.", "labels": [], "entities": []}, {"text": "All the experiments were performed using the SVM package SVMLight-TK 4 , customized to embed our own kernels.", "labels": [], "entities": []}, {"text": "We optimized the linear combination weights w i and regularization parameter c using 10-fold cross-validation on the training set.", "labels": [], "entities": []}, {"text": "We set the cost-factor j to be the ratio between the number of negative and positive examples.", "labels": [], "entities": []}, {"text": "shows the performance on the test set.", "labels": [], "entities": []}, {"text": "We achieve an overall F 1 of 71.8% (B category evaluation), largely outperforming all the baselines, ranging from 48.5% to 57.0%.", "labels": [], "entities": [{"text": "F 1", "start_pos": 22, "end_pos": 25, "type": "METRIC", "confidence": 0.9832360446453094}, {"text": "B category evaluation)", "start_pos": 36, "end_pos": 58, "type": "METRIC", "confidence": 0.9412061721086502}]}, {"text": "The average training plus test running time fora relation is about 10 seconds on a Intel Pentium M755 2.0 GHz.", "labels": [], "entities": []}, {"text": "shows the learning curves on the test set.", "labels": [], "entities": []}, {"text": "For all relations but theme-tool, accurate classifiers can be learned using a small fraction of training.", "labels": [], "entities": []}, {"text": "for content-container we obtain the best performance combining the tree kernel and the bag of synonyms and hypernyms kernel; on the other hand, for instrument-agency the best performance is obtained by combining the global kernel and the supersense kernel.", "labels": [], "entities": []}, {"text": "Surprisingly, the supersense kernel alone works quite well and obtains results comparable to the bag of synonyms and hypernyms kernel.", "labels": [], "entities": []}, {"text": "This result is particularly interesting as a supersense tagger can easily provide a satisfactory accuracy).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 97, "end_pos": 105, "type": "METRIC", "confidence": 0.9991952776908875}]}, {"text": "On the other hand, obtaining an acceptable accuracy in word sense disambiguation (required fora realistic application of the bag of synonyms and hypernyms kernel) is impractical as a sufficient amount of training for at least all nouns is currently not available.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 43, "end_pos": 51, "type": "METRIC", "confidence": 0.9988353848457336}, {"text": "word sense disambiguation", "start_pos": 55, "end_pos": 80, "type": "TASK", "confidence": 0.7161189615726471}]}, {"text": "Hence, the supersense could play a crucial role to improve the performance when approaching this task without the nominals disambiguated.", "labels": [], "entities": []}, {"text": "To model the global context using the Fore-Between, Between and Between-After contexts did not produce a significant improvement with respect to the bag-of-words model.", "labels": [], "entities": []}, {"text": "This is mainly due to the fact that examples have been collected from the Web using heuristic patterns/queries, most of which implying Between patterns/contexts (e.g., for the cause-effect relation \"* comes from *\", \"* out of *\" etc.).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Results on the test set.", "labels": [], "entities": []}]}