{"title": [{"text": "UVAVU: WordNet Similarity and Lexical Patterns for Semantic Relation Classification", "labels": [], "entities": [{"text": "UVAVU", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.8310999870300293}, {"text": "Semantic Relation Classification", "start_pos": 51, "end_pos": 83, "type": "TASK", "confidence": 0.8344138264656067}]}], "abstractContent": [{"text": "The system we propose to learning semantic relations consists of two parallel components.", "labels": [], "entities": []}, {"text": "For our final submission we used components based on the similarity measures defined over WordNet and the patterns extracted from the Web and WMTS.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 90, "end_pos": 97, "type": "DATASET", "confidence": 0.9540324807167053}, {"text": "WMTS", "start_pos": 142, "end_pos": 146, "type": "DATASET", "confidence": 0.9170107245445251}]}, {"text": "Other components using syntactic structures were explored but not used for the final run.", "labels": [], "entities": []}, {"text": "1 Experimental Setup The system we used to classify the semantic relations consists of two parallel binary classifiers.", "labels": [], "entities": []}, {"text": "We ran this system for each of the seven semantic relations separately.", "labels": [], "entities": []}, {"text": "Each classifier predicts for each instance of the relation whether it holds or not.", "labels": [], "entities": []}, {"text": "The predictions of all the classifiers are aggregated for each instance by disjunction.", "labels": [], "entities": []}, {"text": "That is to say, each instance is predicted to be false by default unless any of the classifiers gives evidence against this.", "labels": [], "entities": []}, {"text": "To generate the submitted predictions we used two parallel classifiers: (1) a classifier that combines eleven WordNet-based similarity measures, see Sec.", "labels": [], "entities": []}, {"text": "2.1, and (2) a classifier that learns lexical patterns from Google and the Waterloo Multi-Text System (WMTS)(Turney, 2004) snippets and applies these on the same corpora, see Sec.", "labels": [], "entities": [{"text": "Waterloo Multi-Text System (WMTS)(Turney, 2004)", "start_pos": 75, "end_pos": 122, "type": "DATASET", "confidence": 0.9011022448539734}]}, {"text": "Three other classifiers we experimented with, but that were not used to generate the submitted predictions: (3) a classifier that uses string kernel methods on the dependency paths of the training sentences, see Sec.", "labels": [], "entities": []}, {"text": "3.1, (4) a classifier that uses string kernels on the local context of the subject and object nom-inals in the training sentences, see Sec.", "labels": [], "entities": []}, {"text": "3.2 and (5) a classifier that uses handmade lexical patterns on Google and WMTS, see Sec.", "labels": [], "entities": [{"text": "WMTS", "start_pos": 75, "end_pos": 79, "type": "DATASET", "confidence": 0.8943074941635132}]}, {"text": "3.3. 2 Submitted Run 2.1 WordNet-based Similarity Measures WordNet 3.0 (Fellbaum, 1998) is the most frequently used lexical database of English.", "labels": [], "entities": [{"text": "Submitted Run 2.1 WordNet-based Similarity Measures WordNet 3.0 (Fellbaum, 1998)", "start_pos": 7, "end_pos": 87, "type": "TASK", "confidence": 0.5710116647757016}]}, {"text": "As this resource consists of lexical and semantic relations, its use constitutes an appealing option to learning relations.", "labels": [], "entities": []}, {"text": "In particular, we believe that given two mentions of the same semantic relation, their arguments should also be similar.", "labels": [], "entities": []}, {"text": "Or, in analogy learning terms, if R 1 (X 1 ,Y 1) and R 2 (X 2 ,Y 2) are relation mentions of the same type, then X 1 :: Y 1 as X 2 :: Y 2.", "labels": [], "entities": []}, {"text": "Our preliminary experiments with WordNet suggested that few arguments of each relation are connected by immediate hyperonymy or meronymy relations.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 33, "end_pos": 40, "type": "DATASET", "confidence": 0.9613913893699646}]}, {"text": "As a result , we decided to use similarity measures defined over WordNet (Pedersen et al., 2004).", "labels": [], "entities": [{"text": "WordNet", "start_pos": 65, "end_pos": 72, "type": "DATASET", "confidence": 0.9648531675338745}]}, {"text": "The Word-Net::Similarity package (Pedersen et al., 2004) includes 11 different measures, which mostly use either the WordNet glosses (lesk or vector measures) or the paths between a pair of concepts (lch; wup) to determine their relatedness.", "labels": [], "entities": []}, {"text": "To be able to use WordNet::Similarity, we mapped all WordNet sense keys from the training and test sets to the earlier WordNet version (2.1).", "labels": [], "entities": [{"text": "WordNet", "start_pos": 18, "end_pos": 25, "type": "DATASET", "confidence": 0.9428817629814148}, {"text": "WordNet", "start_pos": 119, "end_pos": 126, "type": "DATASET", "confidence": 0.9415399432182312}]}, {"text": "Given a relation R(X,Y), we computed the related-ness scores for each pair of arguments X and Y.", "labels": [], "entities": []}, {"text": "The scores together with the sense keys of arguments were further used as features for the machine learning method.", "labels": [], "entities": []}, {"text": "As there is no a priori knowledge on what measures are the most important for each rela-472", "labels": [], "entities": [{"text": "rela-472", "start_pos": 83, "end_pos": 91, "type": "DATASET", "confidence": 0.6437622904777527}]}], "introductionContent": [], "datasetContent": [{"text": "The system we used to classify the semantic relations consists of two parallel binary classifiers.", "labels": [], "entities": []}, {"text": "We ran this system for each of the seven semantic relations separately.", "labels": [], "entities": []}, {"text": "Each classifier predicts for each instance of the relation whether it holds or not.", "labels": [], "entities": []}, {"text": "The predictions of all the classifiers are aggregated for each instance by disjunction.", "labels": [], "entities": []}, {"text": "That is to say, each instance is predicted to be false by default unless any of the classifiers gives evidence against this.", "labels": [], "entities": []}, {"text": "To generate the submitted predictions we used two parallel classifiers: (1) a classifier that combines eleven WordNet-based similarity measures, see Sec.", "labels": [], "entities": []}, {"text": "2.1, and (2) a classifier that learns lexical patterns from Google and the Waterloo Multi-Text System (WMTS)) snippets and applies these on the same corpora, see Sec.", "labels": [], "entities": [{"text": "Waterloo Multi-Text System (WMTS)) snippets", "start_pos": 75, "end_pos": 118, "type": "DATASET", "confidence": 0.8952224254608154}]}, {"text": "Three other classifiers we experimented with, but that were not used to generate the submitted predictions: (3) a classifier that uses string kernel methods on the dependency paths of the training sentences, see Sec.", "labels": [], "entities": []}, {"text": "3.1, (4) a classifier that uses string kernels on the local context of the subject and object nominals in the training sentences, see Sec.", "labels": [], "entities": []}, {"text": "3.2 and a classifier that uses hand-made lexical patterns on Google and WMTS, see Sec.", "labels": [], "entities": [{"text": "WMTS", "start_pos": 72, "end_pos": 76, "type": "DATASET", "confidence": 0.9085421562194824}]}], "tableCaptions": [{"text": " Table 2: Results for hand-written lexical patterns on  Google and WMTS.", "labels": [], "entities": [{"text": "WMTS", "start_pos": 67, "end_pos": 71, "type": "DATASET", "confidence": 0.695077121257782}]}, {"text": " Table 3: Results for similarity-measure methods.", "labels": [], "entities": []}, {"text": " Table 4: Results for learnt lexical patterns on Google  and WMTS.", "labels": [], "entities": [{"text": "WMTS", "start_pos": 61, "end_pos": 65, "type": "DATASET", "confidence": 0.70822674036026}]}, {"text": " Table 5. The numbers listed there repre- sent the fraction of examples on which we agreed  with the judges of the test set. There was quite a", "labels": [], "entities": []}]}