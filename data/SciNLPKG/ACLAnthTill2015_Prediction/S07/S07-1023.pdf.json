{"title": [{"text": "CMU-AT: Semantic Distance and Background Knowledge for Identify- ing Semantic Relations", "labels": [], "entities": [{"text": "CMU-AT", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.9344719648361206}, {"text": "Identify- ing Semantic Relations", "start_pos": 55, "end_pos": 87, "type": "TASK", "confidence": 0.6496094584465026}]}], "abstractContent": [{"text": "This system uses a background knowledge base to identify semantic relations between base noun phrases in English text, as evaluated in SemEval 2007, Task 4.", "labels": [], "entities": [{"text": "SemEval 2007", "start_pos": 135, "end_pos": 147, "type": "TASK", "confidence": 0.6198941171169281}]}, {"text": "Training data for each relation is converted to statements in the Scone Knowledge Representation Language.", "labels": [], "entities": [{"text": "Scone Knowledge Representation Language", "start_pos": 66, "end_pos": 105, "type": "TASK", "confidence": 0.5623681992292404}]}, {"text": "At testing time anew Scone statement is created for the sentence under scrutiny, and presence or absence of a relation is calculated by comparing the total semantic distance between the new statement and all positive examples to the total distance between the new statement and all negative examples.", "labels": [], "entities": []}], "introductionContent": [{"text": "This paper introduces a knowledge-based approach to the task of semantic relation classification, as evaluated in SemEval 2007, Task 4: \"Classifying Relations Between Nominals\".", "labels": [], "entities": [{"text": "semantic relation classification", "start_pos": 64, "end_pos": 96, "type": "TASK", "confidence": 0.8082503080368042}]}, {"text": "In Task 4, a full sentence is presented to the system, along with the WordNet sense keys for two noun phrases which appear there and the name of a semantic relation (e.g. \"cause-effect\").", "labels": [], "entities": []}, {"text": "The system should return \"true\" if a person reading the sentence would conclude that the relation holds between the two labeled noun phrases.", "labels": [], "entities": []}, {"text": "Our system represents a test sentence with a semantic graph, including the relation being tested and both of its proposed arguments.", "labels": [], "entities": []}, {"text": "Semantic distance is calculated between this graph and a set of graphs representing the training examples relevant to the test sentence.", "labels": [], "entities": []}, {"text": "A near-match between a test sentence and a positive training example is evidence that the same relation which holds in the example also holds in the test.", "labels": [], "entities": []}, {"text": "We compute semantic distances to negative training examples as well, comparing the total positive and negative scores in order to decide whether a relation is true or false in the test sentence.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1. Recall, Precision, and F-scores, separated  by relation type. Baseline score is calculated by  guessing \"true\" for all test setences.", "labels": [], "entities": [{"text": "Recall", "start_pos": 10, "end_pos": 16, "type": "METRIC", "confidence": 0.9928410053253174}, {"text": "Precision", "start_pos": 18, "end_pos": 27, "type": "METRIC", "confidence": 0.9749794602394104}, {"text": "F-scores", "start_pos": 33, "end_pos": 41, "type": "METRIC", "confidence": 0.9909261465072632}, {"text": "Baseline score", "start_pos": 72, "end_pos": 86, "type": "METRIC", "confidence": 0.9357196688652039}]}]}