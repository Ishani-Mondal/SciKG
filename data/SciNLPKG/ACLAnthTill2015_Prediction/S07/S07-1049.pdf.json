{"title": [{"text": "MELB-KB: Nominal Classification as Noun Compound Interpretation", "labels": [], "entities": []}], "abstractContent": [{"text": "In this paper, we outline our approach to interpreting semantic relations in nominal pairs in SemEval-2007 task #4: Classification of Semantic Relations between Nomi-nals.", "labels": [], "entities": [{"text": "interpreting semantic relations in nominal pairs", "start_pos": 42, "end_pos": 90, "type": "TASK", "confidence": 0.8653254608313242}, {"text": "SemEval-2007 task #4: Classification of Semantic Relations between Nomi-nals", "start_pos": 94, "end_pos": 170, "type": "TASK", "confidence": 0.7765730890360746}]}, {"text": "We build on two baseline approaches to interpreting noun compounds: sense col-location, and constituent similarity.", "labels": [], "entities": [{"text": "interpreting noun compounds", "start_pos": 39, "end_pos": 66, "type": "TASK", "confidence": 0.8384778102238973}]}, {"text": "These are consolidated into an overall system in combination with co-training, to expand the training data.", "labels": [], "entities": []}, {"text": "Our two systems attained an average F-score over the test data of 58.7% and 57.8%, respectively.", "labels": [], "entities": [{"text": "F-score", "start_pos": 36, "end_pos": 43, "type": "METRIC", "confidence": 0.9996708631515503}]}], "introductionContent": [{"text": "This paper describes two systems entered in SemEval-2007 task #4: Classification of Semantic Relations between Nominals.", "labels": [], "entities": [{"text": "SemEval-2007 task #4", "start_pos": 44, "end_pos": 64, "type": "TASK", "confidence": 0.8781055957078934}, {"text": "Classification of Semantic Relations between Nominals", "start_pos": 66, "end_pos": 119, "type": "TASK", "confidence": 0.8558022677898407}]}, {"text": "A key contribution of this research is that we examine the compatibility of noun compound (NC) interpretation methods over the extended task of nominal classification, to gain empirical insight into the relative complexity of the two tasks.", "labels": [], "entities": [{"text": "noun compound (NC) interpretation", "start_pos": 76, "end_pos": 109, "type": "TASK", "confidence": 0.6497979313135147}, {"text": "nominal classification", "start_pos": 144, "end_pos": 166, "type": "TASK", "confidence": 0.7450382113456726}]}, {"text": "The goal of the nominal classification task is to identify the compatibility of a given semantic relation with each of a set of test nominal pairs, e.g. between climate and forest in the fragment the climate in the forest with respect to the CONTENT-CONTAINER relation.", "labels": [], "entities": [{"text": "nominal classification", "start_pos": 16, "end_pos": 38, "type": "TASK", "confidence": 0.6910440027713776}]}, {"text": "Semantic relations (or SRs) in nominals represent the underlying interpretation of the nominal, in the form of the directed relation between the two nominals.", "labels": [], "entities": [{"text": "Semantic relations (or SRs)", "start_pos": 0, "end_pos": 27, "type": "TASK", "confidence": 0.6104821215073267}]}, {"text": "The proposed task is a generalisation of the more conventional task of interpreting noun compounds (NCs), in which we take a NC such as cookie jar and interpret it according to a pre-defined inventory of semantic relations).", "labels": [], "entities": [{"text": "interpreting noun compounds (NCs)", "start_pos": 71, "end_pos": 104, "type": "TASK", "confidence": 0.857930988073349}]}, {"text": "Examples of semantic relations are MAKE, 1 , as exemplified in apple pie where the pie is made from apple(s), and POSSES-SOR, as exemplified in family car where the car is possessed by a family.", "labels": [], "entities": [{"text": "MAKE, 1", "start_pos": 35, "end_pos": 42, "type": "METRIC", "confidence": 0.9369848370552063}, {"text": "POSSES-SOR", "start_pos": 114, "end_pos": 124, "type": "METRIC", "confidence": 0.9221446514129639}]}, {"text": "In the SemEval-2007 task, SR interpretation takes the form of a binary decision fora given nominal pair in context and a given SR, in judging whether that nominal pair conforms to the SR.", "labels": [], "entities": [{"text": "SR interpretation", "start_pos": 26, "end_pos": 43, "type": "TASK", "confidence": 0.9816462397575378}]}, {"text": "Seven relations were used in the task: CAUSE-EFFECT, INSTRUMENT-AGENCY, PRODUCT-PRODUCER, ORIGIN-ENTITY, THEME-TOOL, PART-WHOLE and CONTENT-CONTAINER.", "labels": [], "entities": [{"text": "CAUSE-EFFECT", "start_pos": 39, "end_pos": 51, "type": "METRIC", "confidence": 0.837358832359314}, {"text": "INSTRUMENT-AGENCY", "start_pos": 53, "end_pos": 70, "type": "METRIC", "confidence": 0.9804650545120239}, {"text": "ORIGIN-ENTITY", "start_pos": 90, "end_pos": 103, "type": "METRIC", "confidence": 0.9596918821334839}, {"text": "THEME-TOOL", "start_pos": 105, "end_pos": 115, "type": "METRIC", "confidence": 0.943995475769043}, {"text": "PART-WHOLE", "start_pos": 117, "end_pos": 127, "type": "METRIC", "confidence": 0.8951646089553833}]}, {"text": "Our approach to the task was to: (1) naively treat all nominal pairs as NCs (e.g. the climate in the forest is treated as an instance of climate forest); and (2) translate the individual binary classification tasks into a single multiclass classification task, in the interests of benchmarking existing SR interpretation methods over a common dataset.", "labels": [], "entities": [{"text": "SR interpretation", "start_pos": 303, "end_pos": 320, "type": "TASK", "confidence": 0.8748829662799835}]}, {"text": "That is, we take all positive training instances for each SR and pool them together into a single training dataset.", "labels": [], "entities": []}, {"text": "For each test instance, we make a prediction according to one of the seven relations in the task, which we then map onto a binary classification for final evaluation purposes.", "labels": [], "entities": []}, {"text": "This mapping is achieved by determining which binary SR classification the test instance was sourced from, and returning a positive classification if the predicted SR coincides with the target SR, and a negative classification if not.", "labels": [], "entities": []}, {"text": "We make three (deliberately naive) assumptions in our approach to the nominal interpretation task.", "labels": [], "entities": [{"text": "nominal interpretation task", "start_pos": 70, "end_pos": 97, "type": "TASK", "confidence": 0.7750868201255798}]}, {"text": "First, we assume that all the positive training in-stances correspond uniquely to the SR in question, despite the task organisers making it plain that there is semantic overlap between the SRs.", "labels": [], "entities": []}, {"text": "As a machine learning task, this makes the task considerably more difficult, as the performance for the standard baselines drops considerably from that for the binary tasks.", "labels": [], "entities": []}, {"text": "Second, we assume that each nominal pair maps onto a NC.", "labels": [], "entities": []}, {"text": "This is clearly a misconstrual of the task, and intended to empirically validate whether such an approach is viable.", "labels": [], "entities": []}, {"text": "In line with this assumption, we will refer to nominal pairs as NCs for the remainder of the paper.", "labels": [], "entities": []}, {"text": "Third and finally, we assume that the SR annotation of each training and test instance is insensitive to the original context, and use only the constituent words in the NC to make our prediction.", "labels": [], "entities": []}, {"text": "This is for direct comparability with earlier research, and we acknowledge that the context (and word sense) is a strong determinant of the SR in practice.", "labels": [], "entities": [{"text": "SR", "start_pos": 140, "end_pos": 142, "type": "TASK", "confidence": 0.9279201626777649}]}, {"text": "Our aim in this paper is to demonstrate the effectiveness of general-purpose SR interpretation over the nominal classification task, and establish anew baseline for the task.", "labels": [], "entities": [{"text": "SR interpretation", "start_pos": 77, "end_pos": 94, "type": "TASK", "confidence": 0.9163591265678406}]}, {"text": "The remainder of this paper is structured as follows.", "labels": [], "entities": []}, {"text": "We present our methods in Section 2 and depict the system architectures in Section 4.", "labels": [], "entities": []}, {"text": "We then describe and discuss the performance of our methods in Section 5 and conclude the paper in Section 6.", "labels": [], "entities": []}], "datasetContent": [{"text": "We group our evaluation into two categories: (A) doesn't use WordNet 2.1 or the query context; and (B) uses WordNet 2.1 only (again without the query context).", "labels": [], "entities": [{"text": "WordNet", "start_pos": 61, "end_pos": 68, "type": "DATASET", "confidence": 0.9501540660858154}, {"text": "WordNet", "start_pos": 108, "end_pos": 115, "type": "DATASET", "confidence": 0.9287682771682739}]}, {"text": "Of our two basic methods the sense collocation method and co-training method are based on WordNet 2.1 only, while the constituent similarity method is based indirectly on WordNet 2.1, but doesn't preserve WordNet 2.1 sense information.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 90, "end_pos": 97, "type": "DATASET", "confidence": 0.9462456703186035}, {"text": "WordNet", "start_pos": 171, "end_pos": 178, "type": "DATASET", "confidence": 0.9394049048423767}, {"text": "WordNet 2.1 sense information", "start_pos": 205, "end_pos": 234, "type": "DATASET", "confidence": 0.8786180019378662}]}, {"text": "Hence, our first system is category B while our second system is (arguably) category A. presents the three baselines for the task, and the results for our two systems (System I and System II).", "labels": [], "entities": []}, {"text": "The performance for both systems exceeded all three baselines in terms of accuracy, and all but the All True baseline (i.e. every instance is judged to be compatible with the given SR) in terms show the performance of the teams which performed in the task, in categories A and B.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 74, "end_pos": 82, "type": "METRIC", "confidence": 0.9994413256645203}]}, {"text": "Team 220 in is our second system, and team 220 in is our first system.", "labels": [], "entities": []}, {"text": "In, we present a breakdown of the performance our first and second system, respectively, over the individual semantic relations.", "labels": [], "entities": []}, {"text": "Our approaches performed best for the PRODUCT-PRODUCER SR, and worst for the PART-WHOLE SR.", "labels": [], "entities": []}, {"text": "In general, our systems achieved similar performance on most SRs, with only PART-WHOLE being notably worse.", "labels": [], "entities": [{"text": "PART-WHOLE", "start_pos": 76, "end_pos": 86, "type": "METRIC", "confidence": 0.8824298977851868}]}, {"text": "The lower performance of PART-WHOLE pulls down our overall performance considerably.", "labels": [], "entities": [{"text": "PART-WHOLE", "start_pos": 25, "end_pos": 35, "type": "METRIC", "confidence": 0.37163498997688293}]}, {"text": "show the number of tagged and untagged instances for each step of System I and System II, respectively.", "labels": [], "entities": []}, {"text": "The first system tagged more than half of the data in the fifth (and final) step, where it weighs up predictions from the original and expanded training data.", "labels": [], "entities": []}, {"text": "Hence, the performance of this approach relies heavily on the similarity method and expanded training data.", "labels": [], "entities": []}, {"text": "Additionally, the difference in quality between the original and expanded training data will influence the performance of the approach appreciably.", "labels": [], "entities": []}, {"text": "On the other hand, the number of instances tagged by the second system is well distributed across each iteration.", "labels": [], "entities": []}, {"text": "However, since we accumulate generated training instances on each step, the relative noise level in the training data will   Over the trial data, we noticed that the system predictions are appreciably worse when the similarity value is low.", "labels": [], "entities": []}, {"text": "In future work, we intend to analyse what is happening in terms of the overall system performance at each step.", "labels": [], "entities": []}, {"text": "This analysis is key to improving the performance of our systems.", "labels": [], "entities": []}, {"text": "Recall that we are generalising from the set of binary classification tasks in the original task, to a multiclass classification task.", "labels": [], "entities": [{"text": "multiclass classification task", "start_pos": 103, "end_pos": 133, "type": "TASK", "confidence": 0.7760568857192993}]}, {"text": "As such, a direct comparison with the binary classification baselines is perhaps unfair (particularly All True, which has no correlate in a multiclass setting), and it is if anything remarkable that our system compares favourably compared to the baselines.", "labels": [], "entities": []}, {"text": "Similarly, while we clearly lag behind other systems participating in the: System I: Tagged data from each step (SC= sense collocation; Sim = the similarity method; extSC = SC over the expanded training data; extSim = similarity over the expanded training data; SvsExtS = the final step over both the original and expanded training data) task, we believe we have demonstrated that NC interpretation methods can be successfully deployed over the more general task of nominal pair classification.", "labels": [], "entities": [{"text": "NC interpretation", "start_pos": 381, "end_pos": 398, "type": "TASK", "confidence": 0.7549905776977539}, {"text": "nominal pair classification", "start_pos": 466, "end_pos": 493, "type": "TASK", "confidence": 0.6173034111658732}]}], "tableCaptions": [{"text": " Table 1: System results (P = precision, R = recall, F  = F-score, and A = accuracy)", "labels": [], "entities": [{"text": "precision", "start_pos": 30, "end_pos": 39, "type": "METRIC", "confidence": 0.9888277053833008}, {"text": "recall", "start_pos": 45, "end_pos": 51, "type": "METRIC", "confidence": 0.9317814707756042}, {"text": "F", "start_pos": 53, "end_pos": 54, "type": "METRIC", "confidence": 0.9356286525726318}, {"text": "F-score", "start_pos": 58, "end_pos": 65, "type": "METRIC", "confidence": 0.5904945731163025}, {"text": "A", "start_pos": 71, "end_pos": 72, "type": "METRIC", "confidence": 0.9966192245483398}, {"text": "accuracy", "start_pos": 75, "end_pos": 83, "type": "METRIC", "confidence": 0.8566802740097046}]}, {"text": " Table 2: Results of category A systems", "labels": [], "entities": []}, {"text": " Table 3: Results of category B systems", "labels": [], "entities": []}, {"text": " Table 4: System I: Tagged data from each step  (SC= sense collocation; Sim = the similarity method;  extSC = SC over the expanded training data; extSim  = similarity over the expanded training data; SvsExtS  = the final step over both the original and expanded  training data)", "labels": [], "entities": [{"text": "similarity", "start_pos": 156, "end_pos": 166, "type": "METRIC", "confidence": 0.9561668634414673}]}, {"text": " Table 5: System II: data tagged on each iteration (T  = the threshold; iX = the iteration number)", "labels": [], "entities": []}]}