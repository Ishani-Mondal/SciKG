{"title": [{"text": "SemEval'07 Task 19: Frame Semantic Structure Extraction", "labels": [], "entities": [{"text": "SemEval'07 Task", "start_pos": 0, "end_pos": 15, "type": "TASK", "confidence": 0.8673218488693237}, {"text": "Frame Semantic Structure Extraction", "start_pos": 20, "end_pos": 55, "type": "TASK", "confidence": 0.7341697067022324}]}], "abstractContent": [{"text": "This task consists of recognizing words and phrases that evoke semantic frames as defined in the FrameNet project (http: //framenet.icsi.berkeley.edu), and their semantic dependents, which are usually, but not always, their syntactic dependents (including subjects).", "labels": [], "entities": []}, {"text": "The training data was FN annotated sentences.", "labels": [], "entities": [{"text": "FN annotated sentences", "start_pos": 22, "end_pos": 44, "type": "DATASET", "confidence": 0.8177088697751363}]}, {"text": "In testing, participants automatically annotated three previously unseen texts to match gold standard (human) annotation, including predicting previously unseen frames and roles.", "labels": [], "entities": [{"text": "predicting previously unseen frames and roles", "start_pos": 132, "end_pos": 177, "type": "TASK", "confidence": 0.8560299475987753}]}, {"text": "Precision and recall were measured both for matching of labels of frames and FEs and for matching of semantic dependency trees based on the annotation.", "labels": [], "entities": [{"text": "Precision", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.9927636981010437}, {"text": "recall", "start_pos": 14, "end_pos": 20, "type": "METRIC", "confidence": 0.9993975162506104}, {"text": "FEs", "start_pos": 77, "end_pos": 80, "type": "METRIC", "confidence": 0.9063359498977661}]}], "introductionContent": [{"text": "The task of labeling frame-evoking words with appropriate frames is similar to WSD, while the task of assigning frame elements is called Semantic Role Labeling (SRL), and has been the subject of several shared tasks at ACL and CoNLL.", "labels": [], "entities": [{"text": "WSD", "start_pos": 79, "end_pos": 82, "type": "TASK", "confidence": 0.7596479654312134}, {"text": "Semantic Role Labeling (SRL)", "start_pos": 137, "end_pos": 165, "type": "TASK", "confidence": 0.7658294787009557}, {"text": "CoNLL", "start_pos": 227, "end_pos": 232, "type": "DATASET", "confidence": 0.8833008408546448}]}, {"text": "For example, in the sentence \"Matilde said, 'I rarely eat rutabaga,\"' said evokes the Statement frame, and eat evokes the Ingestion frame.", "labels": [], "entities": [{"text": "Ingestion", "start_pos": 122, "end_pos": 131, "type": "METRIC", "confidence": 0.9010629653930664}]}, {"text": "The role of SPEAKER in the Statement frame is filled by Matilda, and the role of MESSAGE, by the whole quotation.", "labels": [], "entities": [{"text": "SPEAKER", "start_pos": 12, "end_pos": 19, "type": "TASK", "confidence": 0.5571628212928772}, {"text": "Statement frame", "start_pos": 27, "end_pos": 42, "type": "DATASET", "confidence": 0.7312479615211487}, {"text": "Matilda", "start_pos": 56, "end_pos": 63, "type": "METRIC", "confidence": 0.6683477759361267}, {"text": "MESSAGE", "start_pos": 81, "end_pos": 88, "type": "METRIC", "confidence": 0.9862661957740784}]}, {"text": "In the Ingestion frame, I is the INGESTOR and rutabaga fills the INGESTIBLES role.", "labels": [], "entities": [{"text": "INGESTOR", "start_pos": 33, "end_pos": 41, "type": "METRIC", "confidence": 0.9672302603721619}, {"text": "INGESTIBLES", "start_pos": 65, "end_pos": 76, "type": "METRIC", "confidence": 0.9398879408836365}]}, {"text": "Since the ingestion event is contained within the MESSAGE of the Statement event, we can represent the fact that the message conveyed was about ingestion, just by annotating the sentence with respect to these two frames.", "labels": [], "entities": [{"text": "MESSAGE", "start_pos": 50, "end_pos": 57, "type": "METRIC", "confidence": 0.9619725942611694}]}, {"text": "After training on FN annotations, the participants' systems labeled three new texts automatically.", "labels": [], "entities": [{"text": "FN annotations", "start_pos": 18, "end_pos": 32, "type": "TASK", "confidence": 0.5742681324481964}]}, {"text": "The evaluation measured precision and recall for frames and frame elements, with partial credit for incorrect but closely related frames.", "labels": [], "entities": [{"text": "precision", "start_pos": 24, "end_pos": 33, "type": "METRIC", "confidence": 0.9996292591094971}, {"text": "recall", "start_pos": 38, "end_pos": 44, "type": "METRIC", "confidence": 0.9994283318519592}]}, {"text": "Two types of evaluation were carried out: Label matching evaluation, in which the participant's labeled data was compared directly with the gold standard labeled data, and Semantic dependency evaluation, in which both the gold standard and the submitted data were first converted to semantic dependency graphs in XML format, and then these graphs were compared.", "labels": [], "entities": [{"text": "Label matching evaluation", "start_pos": 42, "end_pos": 67, "type": "TASK", "confidence": 0.839000403881073}]}, {"text": "There are three points that make this task harder and more interesting than earlier SRL tasks: (1) while previous tasks focused on role assignment, the current task also comprises the identification of the appropriate FrameNet frame, similar to WSD, (2) the task comprises not only the labeling of individual predicates and their arguments, but also the integration of all labels into an overall semantic dependency graph, a partial semantic representation of the overall sentence meaning based on frames and roles, and (3) the test data includes occurrences of frames that are not seen in the training data.", "labels": [], "entities": [{"text": "SRL", "start_pos": 84, "end_pos": 87, "type": "TASK", "confidence": 0.9607071876525879}, {"text": "role assignment", "start_pos": 131, "end_pos": 146, "type": "TASK", "confidence": 0.7302026599645615}, {"text": "WSD", "start_pos": 245, "end_pos": 248, "type": "DATASET", "confidence": 0.7587497234344482}]}, {"text": "For these cases, participant systems have to identify the closest known frame.", "labels": [], "entities": []}, {"text": "This is a very realistic scenario, encouraging the development of robust systems showing graceful degradation in the face of unknown events.", "labels": [], "entities": []}], "datasetContent": [{"text": "The labels-only matching was similar to previous shared tasks, but the dependency structure evaluation deserves further explanation: The XML semantic dependency structure was produced by a program called fttosem, implemented in Perl, which goes sentence by sentence through a FrameNet full-text XML file, taking LU, FE, and other labels and using them to structure a syntactically unparsed piece of a sentence into a syntactic-semantic tree.", "labels": [], "entities": [{"text": "FE", "start_pos": 316, "end_pos": 318, "type": "METRIC", "confidence": 0.9558206796646118}]}, {"text": "Two basic principles allow us to produce this tree: (1) LUs are the sole syntactic head of a phrase whose semantics is expressed by their frame and (2) each label span is interpreted as the boundaries of a syntactic phrase, so that when a larger label span subsumes a smaller one, the larger span can be interpreted as a the higher node in a hierarchical tree.", "labels": [], "entities": []}, {"text": "There area fair number of complications, largely involving identifying mismatches between syntactic and semantic headedness.", "labels": [], "entities": []}, {"text": "Some of these (support verbs, copulas, modifiers, transparent nouns, relative clauses) are annotated in the data with their own labels, while others (syntactic markers, e.g. prepositions, and auxiliary verbs) must be identified using simple syntactic heuristics and part-of-speech tags.", "labels": [], "entities": []}, {"text": "For this evaluation, a non-frame node counts as matching provided that it includes the head of the gold standard, whether or not non-head children of that node are included.", "labels": [], "entities": []}, {"text": "For frame nodes, the participants got full credit if the frame of the node matched the gold standard.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Summary of Testing Data", "labels": [], "entities": [{"text": "Summary of Testing", "start_pos": 10, "end_pos": 28, "type": "TASK", "confidence": 0.7973753412564596}]}, {"text": " Table 2: Frame Recognition only", "labels": [], "entities": [{"text": "Frame Recognition", "start_pos": 10, "end_pos": 27, "type": "TASK", "confidence": 0.8463800549507141}]}, {"text": " Table 3: Results for combined Frame and FE recog- nition", "labels": [], "entities": [{"text": "FE", "start_pos": 41, "end_pos": 43, "type": "METRIC", "confidence": 0.9788749814033508}, {"text": "recog- nition", "start_pos": 44, "end_pos": 57, "type": "TASK", "confidence": 0.6593231956164042}]}, {"text": " Table 2. The  difficulty of the task is reflected in the F-scores of  around 35% for the most difficult text in the most  difficult condition, but participants still managed to  reach F-scores as high as 75% for the more limited  task of Frame Identification", "labels": [], "entities": [{"text": "F-scores", "start_pos": 58, "end_pos": 66, "type": "METRIC", "confidence": 0.998714804649353}, {"text": "F-scores", "start_pos": 185, "end_pos": 193, "type": "METRIC", "confidence": 0.9938641786575317}, {"text": "Frame Identification", "start_pos": 239, "end_pos": 259, "type": "TASK", "confidence": 0.8115995228290558}]}]}