{"title": [{"text": "UCD-S1: A hybrid model for detecting semantic relations between noun pairs in text", "labels": [], "entities": [{"text": "UCD-S1", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.7724567651748657}, {"text": "detecting semantic relations between noun pairs in text", "start_pos": 27, "end_pos": 82, "type": "TASK", "confidence": 0.8368313983082771}]}], "abstractContent": [{"text": "We describe a supervised learning approach to categorizing inter-noun relations, based on Support Vector Machines, that builds a different classifier for each of seven semantic relations.", "labels": [], "entities": []}, {"text": "Each model uses the same learning strategy, while a simple voting procedure based on five trained discriminators with various blends of features determines the final categorization.", "labels": [], "entities": []}, {"text": "The features that characterize each of the noun pairs area blend of lexical-semantic categories extracted from WordNet and several flavors of syntactic patterns extracted from various corpora, including Wikipedia and the WMTS corpus.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 111, "end_pos": 118, "type": "DATASET", "confidence": 0.9339281320571899}, {"text": "WMTS corpus", "start_pos": 221, "end_pos": 232, "type": "DATASET", "confidence": 0.9516917765140533}]}], "introductionContent": [{"text": "The SemEval task for classifying inter-noun semantic relations employs seven semantic relations that are not exhaustive: Cause-Effect, Instrument-Agency, Product-Producer OriginEntity, Theme-Tool, Part-Whole and ContentContainer.", "labels": [], "entities": [{"text": "classifying inter-noun semantic relations", "start_pos": 21, "end_pos": 62, "type": "TASK", "confidence": 0.8062769621610641}]}, {"text": "The task is to classify the relations between pairs of concepts that are part of the same syntactic structure in a given sentence.", "labels": [], "entities": []}, {"text": "This approach employs a context-dependent classification, as opposed to usual out-of-context approaches in classifying semantic relations between noun pairs (e.g.,,)).", "labels": [], "entities": []}, {"text": "Our approach is based on the Support Vector Machines learning paradigm, in which supervised machine learning is used to find the most salient combination of features for each semantic relation.", "labels": [], "entities": []}, {"text": "These features include semantic generalizations of the noun-senses as encoded as WordNet (WN) hyponyms, some manually selected linguistic features (e.g., agentive, gerundive, etc.) as well as the observed relational behaviour of the given nouns in three different corpora: the collected glosses of WordNet; the collected text of Wikipedia; and the WMTS corpus.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 298, "end_pos": 305, "type": "DATASET", "confidence": 0.9247947931289673}, {"text": "Wikipedia", "start_pos": 329, "end_pos": 338, "type": "DATASET", "confidence": 0.6890260577201843}, {"text": "WMTS corpus", "start_pos": 348, "end_pos": 359, "type": "DATASET", "confidence": 0.9819539487361908}]}, {"text": "One can find similar approaches in the literature to the semantic classification of noun compounds.", "labels": [], "entities": [{"text": "semantic classification of noun compounds", "start_pos": 57, "end_pos": 98, "type": "TASK", "confidence": 0.8441891551017762}]}, {"text": "uses automatically extracted paraphrases to build a similarity measure between pairs of concepts, proposes separate models for two different word representations when determining the semantic relation in modifier-noun compounds: a model based on the lexico-semantic aspects of words and a model that uses contextual information from corpora.", "labels": [], "entities": []}, {"text": "Our approach is different in that we use all the available features of word representations and concept interactions in a single hybrid model.", "labels": [], "entities": []}], "datasetContent": [{"text": "The SemEval data-set for each of the seven semantic relations comprises 140 annotated instances for training and between 70 to 90 for testing.", "labels": [], "entities": [{"text": "SemEval data-set", "start_pos": 4, "end_pos": 20, "type": "DATASET", "confidence": 0.7918250560760498}]}, {"text": "Each instance is manually labelled with the part of speech of each concept in a pair, as well as the WN synset-id of the intended word-sense and a sample sentential context.", "labels": [], "entities": []}, {"text": "SRD's predictions fall into evaluation category B, as the system uses WN synsetid but not the query pattern used to originally populate the data-sets with instances.", "labels": [], "entities": [{"text": "SRD", "start_pos": 0, "end_pos": 3, "type": "TASK", "confidence": 0.6257296800613403}]}, {"text": "SRD also skips those training instances where WN sense-ids are not provided, so that the actual number of training instances used ranges from 129 to 138 manually labelled examples per relation-type.", "labels": [], "entities": [{"text": "SRD", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.7005816698074341}]}, {"text": "SRD's precision, recall, F-score and accuracy for each relation is given by To assess the effect of varying quantities of training data, the model was tested on different fractions of the training data: dataset B1 comprises the first quarter of the training data, dataset B2 the first half, while B3 dataset comprises the first three quarters and B4 comprises the complete training dataset.", "labels": [], "entities": [{"text": "precision", "start_pos": 6, "end_pos": 15, "type": "METRIC", "confidence": 0.9996377229690552}, {"text": "recall", "start_pos": 17, "end_pos": 23, "type": "METRIC", "confidence": 0.999529242515564}, {"text": "F-score", "start_pos": 25, "end_pos": 32, "type": "METRIC", "confidence": 0.9984230995178223}, {"text": "accuracy", "start_pos": 37, "end_pos": 45, "type": "METRIC", "confidence": 0.9995158910751343}]}, {"text": "We report the behavior of SRD in predicting the unseen test data when learning from these datasets in represent an average of SRD's performance across all relation-types.", "labels": [], "entities": [{"text": "SRD", "start_pos": 26, "end_pos": 29, "type": "TASK", "confidence": 0.9514887928962708}]}], "tableCaptions": []}