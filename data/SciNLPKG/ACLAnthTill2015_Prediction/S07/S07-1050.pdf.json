{"title": [{"text": "MELB-MKB: Lexical Substitution System based on Relatives in Context", "labels": [], "entities": [{"text": "MELB-MKB", "start_pos": 0, "end_pos": 8, "type": "DATASET", "confidence": 0.63719242811203}]}], "abstractContent": [{"text": "In this paper we describe the MELB-MKB system, as entered in the SemEval-2007 lexical substitution task.", "labels": [], "entities": [{"text": "SemEval-2007 lexical substitution task", "start_pos": 65, "end_pos": 103, "type": "TASK", "confidence": 0.6961527541279793}]}, {"text": "The core of our system was the \"Relatives in Context\" unsuper-vised approach, which ranked the candidate substitutes by web-lookup of the word sequences built combining the target context and each substitute.", "labels": [], "entities": []}, {"text": "Our system ranked third in the final evaluation, performing close to the top-ranked system.", "labels": [], "entities": []}], "introductionContent": [{"text": "This paper describes the system we developed for the SemEval lexical substitution task, anew task in.", "labels": [], "entities": [{"text": "SemEval lexical substitution task", "start_pos": 53, "end_pos": 86, "type": "TASK", "confidence": 0.9078982919454575}]}, {"text": "Although we tested different configurations on the trial data, our basic system relied on WordNet relatives) and Google queries in order to identify the most plausible substitutes in the context.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 90, "end_pos": 97, "type": "DATASET", "confidence": 0.9556467533111572}]}, {"text": "The main goal when building our system was to study the following factors: (i) substitution candidate set, (ii) settings of the relative-based algorithm, and (iii) syntactic filtering.", "labels": [], "entities": [{"text": "syntactic filtering", "start_pos": 164, "end_pos": 183, "type": "TASK", "confidence": 0.780918538570404}]}, {"text": "We analysed these factors over the trial data provided by the organisation, and used the BEST metric to tune our system.", "labels": [], "entities": [{"text": "BEST", "start_pos": 89, "end_pos": 93, "type": "METRIC", "confidence": 0.997016191482544}]}, {"text": "This metric accepts multiple answers, and averages the score across the answers.", "labels": [], "entities": []}, {"text": "We did not experiment with the OOT (top 10 answers) and MULTIWORD metrics.", "labels": [], "entities": [{"text": "OOT", "start_pos": 31, "end_pos": 34, "type": "METRIC", "confidence": 0.9948790073394775}, {"text": "MULTIWORD", "start_pos": 56, "end_pos": 65, "type": "METRIC", "confidence": 0.9262228012084961}]}, {"text": "In the remainder of this paper we briefly introduce the basic Relatives in Context algorithm in Section 2.", "labels": [], "entities": []}, {"text": "Next we describe our experiments on the trial data in Section 3.", "labels": [], "entities": []}, {"text": "Our final system and its results are described in Section 4.", "labels": [], "entities": []}, {"text": "Finally, our conclusions are outlined in Section 5.", "labels": [], "entities": []}], "datasetContent": [{"text": "One of the limitations of the \"Relatives in Context\" algorithm is that it only relies on the local context.", "labels": [], "entities": [{"text": "Relatives in Context\"", "start_pos": 31, "end_pos": 52, "type": "TASK", "confidence": 0.7586480975151062}]}, {"text": "We wanted to explore the contribution of other words in the context for the task, and we performed an experiment including the Topical Signatures resource (Agirre and Lopez).", "labels": [], "entities": []}, {"text": "We simply counted the overlapping of words shared between the context and the different candidates.", "labels": [], "entities": []}, {"text": "We only tested this for nouns, for which the results were below baseline.", "labels": [], "entities": []}, {"text": "We then tried to integrate the topicsignature scores with the \"Relatives in Context\" algorithm, but we did not improve our basic system's results on the trial data.", "labels": [], "entities": []}, {"text": "Thus, this approach was not included in our final submission.", "labels": [], "entities": []}, {"text": "Another problem we observed in error analysis was that the Semcor-based filters were too strict in some cases, and it was desirable to have away of penalising low frequency senses without removing them completely.", "labels": [], "entities": []}, {"text": "Thus, we weighted senses by the inverse of their sense-rank.", "labels": [], "entities": []}, {"text": "As we did not have time to test this intuition properly, we opted for applying the sense-weighting only when the candidates had the same context-match length, instead of using the number of hits.", "labels": [], "entities": []}, {"text": "We will seethe effect of this method in the next section.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: WordNet coverage for different candidate  sets, based on substitute (Subs.) and instance (Inst.)  coverage.", "labels": [], "entities": []}, {"text": " Table 2: Experiments to tune parameters on the trial  data, based on the BEST metric. Scores correspond  to precision (which is the same as recall).", "labels": [], "entities": [{"text": "BEST", "start_pos": 74, "end_pos": 78, "type": "METRIC", "confidence": 0.997645914554596}, {"text": "precision", "start_pos": 109, "end_pos": 118, "type": "METRIC", "confidence": 0.9995730519294739}, {"text": "recall", "start_pos": 141, "end_pos": 147, "type": "METRIC", "confidence": 0.999016284942627}]}, {"text": " Table 3: Official results based on the BEST metric.", "labels": [], "entities": [{"text": "BEST", "start_pos": 40, "end_pos": 44, "type": "METRIC", "confidence": 0.9615649580955505}]}]}