{"title": [{"text": "I2R: Three Systems for Word Sense Discrimination, Chinese Word Sense Disambiguation, and English Word Sense Disambiguation", "labels": [], "entities": [{"text": "I2R", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.8476076722145081}, {"text": "Word Sense Discrimination", "start_pos": 23, "end_pos": 48, "type": "TASK", "confidence": 0.6224014858404795}, {"text": "Chinese Word Sense Disambiguation", "start_pos": 50, "end_pos": 83, "type": "TASK", "confidence": 0.5978494733572006}, {"text": "English Word Sense Disambiguation", "start_pos": 89, "end_pos": 122, "type": "TASK", "confidence": 0.56195367872715}]}], "abstractContent": [{"text": "This paper describes the implementation of our three systems at SemEval-2007, for task 2 (word sense discrimination), task 5 (Chinese word sense disambiguation), and the first subtask in task 17 (English word sense disambiguation).", "labels": [], "entities": [{"text": "word sense discrimination)", "start_pos": 90, "end_pos": 116, "type": "TASK", "confidence": 0.7386342436075211}, {"text": "Chinese word sense disambiguation)", "start_pos": 126, "end_pos": 160, "type": "TASK", "confidence": 0.6361240327358246}, {"text": "English word sense disambiguation)", "start_pos": 196, "end_pos": 230, "type": "TASK", "confidence": 0.6387878775596618}]}, {"text": "For task 2, we applied a cluster validation method to estimate the number of senses of a target word in untagged data, and then grouped the instances of this target word into the estimated number of clusters.", "labels": [], "entities": []}, {"text": "For both task 5 and task 17, We used the label propagation algorithm as the classifier for sense disam-biguation.", "labels": [], "entities": [{"text": "label propagation", "start_pos": 41, "end_pos": 58, "type": "TASK", "confidence": 0.6944678276777267}]}, {"text": "Our system at task 2 achieved 63.9% F-score under unsupervised evaluation , and 71.9% supervised recall with supervised evaluation.", "labels": [], "entities": [{"text": "F-score", "start_pos": 36, "end_pos": 43, "type": "METRIC", "confidence": 0.9994491934776306}, {"text": "recall", "start_pos": 97, "end_pos": 103, "type": "METRIC", "confidence": 0.9547452926635742}]}, {"text": "For task 5, our system obtained 71.2% micro-average precision and 74.7% macro-average precision.", "labels": [], "entities": [{"text": "precision", "start_pos": 52, "end_pos": 61, "type": "METRIC", "confidence": 0.8953964710235596}, {"text": "precision", "start_pos": 86, "end_pos": 95, "type": "METRIC", "confidence": 0.9669272303581238}]}, {"text": "For the lexical sample subtask for task 17, our system achieved 86.4% coarse-grained precision and recall.", "labels": [], "entities": [{"text": "coarse-grained", "start_pos": 70, "end_pos": 84, "type": "METRIC", "confidence": 0.9097189903259277}, {"text": "precision", "start_pos": 85, "end_pos": 94, "type": "METRIC", "confidence": 0.9138952493667603}, {"text": "recall", "start_pos": 99, "end_pos": 105, "type": "METRIC", "confidence": 0.9988033771514893}]}], "introductionContent": [{"text": "SemEval-2007 launches totally 18 tasks for evaluation exercise, covering word sense disambiguation, word sense discrimination, semantic role labeling, and sense disambiguation for information retrieval, and other topics in NLP.", "labels": [], "entities": [{"text": "word sense disambiguation", "start_pos": 73, "end_pos": 98, "type": "TASK", "confidence": 0.6751844882965088}, {"text": "word sense discrimination", "start_pos": 100, "end_pos": 125, "type": "TASK", "confidence": 0.7184430559476217}, {"text": "semantic role labeling", "start_pos": 127, "end_pos": 149, "type": "TASK", "confidence": 0.6854302088419596}, {"text": "sense disambiguation", "start_pos": 155, "end_pos": 175, "type": "TASK", "confidence": 0.7042495161294937}, {"text": "information retrieval", "start_pos": 180, "end_pos": 201, "type": "TASK", "confidence": 0.7083427011966705}]}, {"text": "We participated three tasks in SemEval-2007, which are task 2 (Evaluating Word Sense Induction and Discrimination Systems), task 5 (Multilingual Chinese-English Lexical Sample Task) and the first subtask at task 17 (English Lexical Sample, English Semantic Role Labeling and English All-Words Tasks).", "labels": [], "entities": [{"text": "Evaluating Word Sense Induction and Discrimination Systems)", "start_pos": 63, "end_pos": 122, "type": "TASK", "confidence": 0.8487515896558762}, {"text": "English Semantic Role Labeling", "start_pos": 240, "end_pos": 270, "type": "TASK", "confidence": 0.5278295055031776}]}, {"text": "The goal for SemEval-2007 task 2 (Evaluating Word Sense Induction and Discrimination Systems)) is to automatically discriminate the senses of English target words by the use of only untagged data.", "labels": [], "entities": [{"text": "Evaluating Word Sense Induction and Discrimination", "start_pos": 34, "end_pos": 84, "type": "TASK", "confidence": 0.6709186832110087}]}, {"text": "Here we address this word sense discrimination problem by (1) estimating the number of word senses of a target word in untagged data using a stability criterion, and then (2) grouping the instances of this target word into the estimated number of clusters according to the similarity of contexts of the instances.", "labels": [], "entities": [{"text": "word sense discrimination", "start_pos": 21, "end_pos": 46, "type": "TASK", "confidence": 0.7315741976102194}]}, {"text": "No sense-tagged data is used to help the clustering process.", "labels": [], "entities": []}, {"text": "The goal of task 5 (Chinese Word Sense Disambiguation) is to create a framework for the evaluation of word sense disambiguation in Chinese-English machine translation systems.", "labels": [], "entities": [{"text": "Chinese Word Sense Disambiguation)", "start_pos": 20, "end_pos": 54, "type": "TASK", "confidence": 0.6668029367923737}, {"text": "word sense disambiguation in Chinese-English machine translation", "start_pos": 102, "end_pos": 166, "type": "TASK", "confidence": 0.6300188302993774}]}, {"text": "Each participates of this task will be provided with sense tagged training data and untagged test data for 40 Chinese polysemous words.", "labels": [], "entities": []}, {"text": "The \"sense tags\" for the ambiguous Chinese target words are given in the form of their English translations.", "labels": [], "entities": []}, {"text": "Here we used a semisupervised classification algorithm (label propagation algorithm)) to address this Chinese word sense disambiguation problem.", "labels": [], "entities": [{"text": "label propagation", "start_pos": 56, "end_pos": 73, "type": "TASK", "confidence": 0.7069438844919205}, {"text": "Chinese word sense disambiguation", "start_pos": 102, "end_pos": 135, "type": "TASK", "confidence": 0.5882064774632454}]}, {"text": "The lexical sample subtask of task 17 (English Word Sense Disambiguation) provides sense-tagged training data and untagged test data for 35 nouns and 65 verbs.", "labels": [], "entities": [{"text": "English Word Sense Disambiguation)", "start_pos": 39, "end_pos": 73, "type": "TASK", "confidence": 0.635512751340866}]}, {"text": "This data includes, for each target word: OntoNotes sense tags (these are groupings of WordNet senses that are more coarse-grained than tradi-tional WN entries), as well as the sense inventory for these lemmas.", "labels": [], "entities": []}, {"text": "Here we used only the training data supplied in this subtask for sense disambiguation in test set.", "labels": [], "entities": [{"text": "sense disambiguation", "start_pos": 65, "end_pos": 85, "type": "TASK", "confidence": 0.7740100622177124}]}, {"text": "The label propagation algorithm) was used to perform sense disambiguation by the use of both training data and test data.", "labels": [], "entities": [{"text": "label propagation", "start_pos": 4, "end_pos": 21, "type": "TASK", "confidence": 0.7534180879592896}, {"text": "sense disambiguation", "start_pos": 53, "end_pos": 73, "type": "TASK", "confidence": 0.8409961760044098}]}, {"text": "This paper will be organized as follows.", "labels": [], "entities": []}, {"text": "First, we will provide the feature set used for task 2, task 5 and task 17 in section 2.", "labels": [], "entities": []}, {"text": "Secondly, we will present the word sense discrimination method used for task 2 in section 3.", "labels": [], "entities": [{"text": "word sense discrimination", "start_pos": 30, "end_pos": 55, "type": "TASK", "confidence": 0.7540850241978964}]}, {"text": "Then, we will give the label propagation algorithm for task 5 and task 17 in section 4.", "labels": [], "entities": [{"text": "label propagation", "start_pos": 23, "end_pos": 40, "type": "TASK", "confidence": 0.7471555173397064}]}, {"text": "Section 5 will provide the description of data sets at task 2, task 5 and task 17.", "labels": [], "entities": []}, {"text": "Then, we will present the experimental results of our systems at the three tasks in section 6.", "labels": [], "entities": []}, {"text": "Finally we will give a conclusion of our work in section 7.", "labels": [], "entities": []}], "datasetContent": [{"text": "Our system obtained the fourth place among six systems with unsupervised evaluation.", "labels": [], "entities": []}, {"text": "shows the best/worst/average supervised recall of all the systems at task 2 and the supervised recall of our system at task 2 for all target words, nouns and verbs with supervised evaluation.", "labels": [], "entities": [{"text": "recall", "start_pos": 40, "end_pos": 46, "type": "METRIC", "confidence": 0.8502837419509888}, {"text": "recall", "start_pos": 95, "end_pos": 101, "type": "METRIC", "confidence": 0.7029740810394287}]}, {"text": "Our system is ranked as the first among six systems with supervised evaluation.", "labels": [], "entities": []}, {"text": "lists the estimated sense numbers by our system for all the words at task 2.", "labels": [], "entities": []}, {"text": "The average of all the estimated sense numbers is 3.1, while the average of all the ground-truth sense numbers is 3.6 if we consider the sense inventories provided in task 17 as the answer.", "labels": [], "entities": []}, {"text": "It seems that our estimated sense numbers are close to the ground-truth ones.", "labels": [], "entities": []}, {"text": "provides the best/worst/average microaverage precision and macro-average precision of all the systems at task 5 and the micro-average precision and macro-average precision of our system at task 5.", "labels": [], "entities": [{"text": "precision", "start_pos": 45, "end_pos": 54, "type": "METRIC", "confidence": 0.8704324960708618}, {"text": "precision", "start_pos": 73, "end_pos": 82, "type": "METRIC", "confidence": 0.9057990908622742}, {"text": "precision", "start_pos": 134, "end_pos": 143, "type": "METRIC", "confidence": 0.5763056874275208}, {"text": "precision", "start_pos": 162, "end_pos": 171, "type": "METRIC", "confidence": 0.8518894910812378}]}, {"text": "Our system obtained the second place among six systems for task 5.", "labels": [], "entities": []}, {"text": "shows the best/worst/average coarsegrained score (precision) of all the systems the lexical sample subtask of task 17 and the coarse-grained score (precision) of our system at the lexical sample: The best/worst/average coarse-grained score (precision) of all the systems at the lexical sample subtask of task 17 and the coarse-grained score (precision) of our system at the lexical sample subtask of task 17.", "labels": [], "entities": [{"text": "precision", "start_pos": 50, "end_pos": 59, "type": "METRIC", "confidence": 0.5204873085021973}, {"text": "precision", "start_pos": 148, "end_pos": 157, "type": "METRIC", "confidence": 0.5646393299102783}, {"text": "precision", "start_pos": 241, "end_pos": 250, "type": "METRIC", "confidence": 0.7954578995704651}, {"text": "precision", "start_pos": 342, "end_pos": 351, "type": "METRIC", "confidence": 0.5927972197532654}]}, {"text": "Coarse-grained score (precision) Best 88.7% Worst 52.1% Average 70.0% Our system 86.4% subtask of task 17.", "labels": [], "entities": [{"text": "Coarse-grained score", "start_pos": 0, "end_pos": 20, "type": "METRIC", "confidence": 0.9344715476036072}, {"text": "precision) Best 88.7% Worst 52.1% Average 70.0", "start_pos": 22, "end_pos": 68, "type": "METRIC", "confidence": 0.781365180015564}]}, {"text": "The attempted rate of all the systems is 100%.", "labels": [], "entities": [{"text": "attempted rate", "start_pos": 4, "end_pos": 18, "type": "METRIC", "confidence": 0.9486939311027527}]}, {"text": "So the precision value is equal to the recall value for all the systems.", "labels": [], "entities": [{"text": "precision", "start_pos": 7, "end_pos": 16, "type": "METRIC", "confidence": 0.9995583891868591}, {"text": "recall", "start_pos": 39, "end_pos": 45, "type": "METRIC", "confidence": 0.9994403719902039}]}, {"text": "Here we listed only the precision for the 13 systems at this subtask.", "labels": [], "entities": [{"text": "precision", "start_pos": 24, "end_pos": 33, "type": "METRIC", "confidence": 0.9991869330406189}]}, {"text": "Our system is ranked as the third one among 13 systems.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: The best/worst/average F-score of all the  systems at task 2 and the F-score of our system at  task 2 for all target words, nouns and verbs with un- supervised evaluation.  All words Nouns Verbs  Best  78.7%  80.8% 76.3%  Worst  56.1%  65.8% 45.1%  Average  65.4%  69.0% 61.4%  Our system  63.9%  68.0% 59.3%", "labels": [], "entities": [{"text": "F-score", "start_pos": 33, "end_pos": 40, "type": "METRIC", "confidence": 0.9900341629981995}, {"text": "F-score", "start_pos": 79, "end_pos": 86, "type": "METRIC", "confidence": 0.9967407584190369}]}, {"text": " Table 4: The best/worst/average supervised recall of  all the systems at task 2 and the supervised recall of  our system at task 2 for all target words, nouns and  verbs with supervised evaluation.", "labels": [], "entities": [{"text": "recall", "start_pos": 44, "end_pos": 50, "type": "METRIC", "confidence": 0.8755211234092712}]}, {"text": " Table 5: The best/worst/average micro-average pre- cision and macro-average precision of all the sys- tems at task 5 and the micro-average precision and  macro-average precision of our system at task 5.  Micro-average Macro-average  Best  71.7%  74.9%  Worst  33.7%  39.6%  Average  58.5%  62.7%  Our system  71.2%  74.7%", "labels": [], "entities": [{"text": "precision", "start_pos": 77, "end_pos": 86, "type": "METRIC", "confidence": 0.9631006121635437}, {"text": "precision", "start_pos": 140, "end_pos": 149, "type": "METRIC", "confidence": 0.7408474087715149}, {"text": "precision", "start_pos": 169, "end_pos": 178, "type": "METRIC", "confidence": 0.9352001547813416}, {"text": "Average", "start_pos": 275, "end_pos": 282, "type": "METRIC", "confidence": 0.9709526896476746}]}]}