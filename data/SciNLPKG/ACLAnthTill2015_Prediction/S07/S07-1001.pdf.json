{"title": [{"text": "SemEval-2007 Task 01: Evaluating WSD on Cross-Language Information Retrieval", "labels": [], "entities": [{"text": "SemEval-2007 Task", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.8469792902469635}, {"text": "Evaluating WSD", "start_pos": 22, "end_pos": 36, "type": "TASK", "confidence": 0.676855742931366}, {"text": "Cross-Language Information Retrieval", "start_pos": 40, "end_pos": 76, "type": "TASK", "confidence": 0.7033481995264689}]}], "abstractContent": [{"text": "This paper presents a first attempt of an application-driven evaluation exercise of WSD.", "labels": [], "entities": [{"text": "WSD", "start_pos": 84, "end_pos": 87, "type": "TASK", "confidence": 0.956667423248291}]}, {"text": "We used a CLIR testbed from the Cross Lingual Evaluation Forum.", "labels": [], "entities": [{"text": "CLIR testbed from the Cross Lingual Evaluation Forum", "start_pos": 10, "end_pos": 62, "type": "DATASET", "confidence": 0.9102248251438141}]}, {"text": "The expansion , indexing and retrieval strategies where fixed by the organizers.", "labels": [], "entities": []}, {"text": "The participants had to return both the topics and documents tagged with WordNet 1.6 word senses.", "labels": [], "entities": [{"text": "WordNet 1.6 word senses", "start_pos": 73, "end_pos": 96, "type": "DATASET", "confidence": 0.8879521638154984}]}, {"text": "The organization provided training data in the form of a pre-processed Semcor which could be readily used by participants.", "labels": [], "entities": []}, {"text": "The task had two participants, and the organizer also provide an in-house WSD system for comparison.", "labels": [], "entities": [{"text": "WSD", "start_pos": 74, "end_pos": 77, "type": "TASK", "confidence": 0.784904956817627}]}], "introductionContent": [{"text": "Since the start of Senseval, the evaluation of Word Sense Disambiguation (WSD) as a separate task is a mature field, with both lexical-sample and all-words tasks.", "labels": [], "entities": [{"text": "Word Sense Disambiguation (WSD)", "start_pos": 47, "end_pos": 78, "type": "TASK", "confidence": 0.7675141841173172}]}, {"text": "In the first case the participants need to tag the occurrences of a few words, for which hand-tagged data has already been provided.", "labels": [], "entities": []}, {"text": "In the all-words task all the occurrences of open-class words occurring in two or three documents (a few thousand words) need to be disambiguated.", "labels": [], "entities": []}, {"text": "The community has long mentioned the necessity of evaluating WSD in an application, in order to check which WSD strategy is best, and more important, to try to show that WSD can make a difference in applications.", "labels": [], "entities": []}, {"text": "The use of WSD in Machine Translation has been the subject of some recent papers, but less attention has been paid to Information.", "labels": [], "entities": [{"text": "WSD", "start_pos": 11, "end_pos": 14, "type": "TASK", "confidence": 0.8887373805046082}, {"text": "Machine Translation", "start_pos": 18, "end_pos": 37, "type": "TASK", "confidence": 0.7015611827373505}]}, {"text": "With this proposal we want to make a first try to define a task where WSD is evaluated with respect to an Information Retrieval and Cross-Lingual Information Retrieval (CLIR) exercise.", "labels": [], "entities": [{"text": "WSD", "start_pos": 70, "end_pos": 73, "type": "TASK", "confidence": 0.8530164957046509}, {"text": "Information Retrieval and Cross-Lingual Information Retrieval (CLIR)", "start_pos": 106, "end_pos": 174, "type": "TASK", "confidence": 0.7131630745198991}]}, {"text": "From the WSD perspective, this task will evaluate all-words WSD systems indirectly on areal task.", "labels": [], "entities": []}, {"text": "From the CLIR perspective, this task will evaluate which WSD systems and strategies work best.", "labels": [], "entities": [{"text": "WSD", "start_pos": 57, "end_pos": 60, "type": "TASK", "confidence": 0.9718883037567139}]}, {"text": "We are conscious that the number of possible configurations for such an exercise is very large (including sense inventory choice, using word sense induction instead of disambiguation, query expansion, WSD strategies, IR strategies, etc.), so this first edition focuses on the following: \u2022 The IR/CLIR system is fixed.", "labels": [], "entities": [{"text": "query expansion", "start_pos": 184, "end_pos": 199, "type": "TASK", "confidence": 0.713086262345314}]}, {"text": "\u2022 The expansion / translation strategy is fixed.", "labels": [], "entities": [{"text": "expansion / translation", "start_pos": 6, "end_pos": 29, "type": "TASK", "confidence": 0.7130711476008097}]}, {"text": "\u2022 The participants can choose the best WSD strategy.", "labels": [], "entities": [{"text": "WSD", "start_pos": 39, "end_pos": 42, "type": "TASK", "confidence": 0.9641752243041992}]}, {"text": "\u2022 The IR system is used as the upperbound for the CLIR systems.", "labels": [], "entities": [{"text": "IR", "start_pos": 6, "end_pos": 8, "type": "METRIC", "confidence": 0.6832008361816406}]}, {"text": "We think that it is important to start doing this kind of application-driven evaluations, which might shed light to the intricacies in the interaction between WSD and IR strategies.", "labels": [], "entities": [{"text": "WSD", "start_pos": 159, "end_pos": 162, "type": "TASK", "confidence": 0.9618560075759888}]}, {"text": "We see this as the first of a series of exercises, and one outcome of this task should be that both WSD and CLIR communities discuss together future evaluation possibilities.", "labels": [], "entities": [{"text": "WSD", "start_pos": 100, "end_pos": 103, "type": "TASK", "confidence": 0.8050393462181091}]}, {"text": "This task has been organized in collaboration with the Cross-Language Evaluation Forum (CLEF 1 ).", "labels": [], "entities": [{"text": "Cross-Language Evaluation Forum (CLEF 1 )", "start_pos": 55, "end_pos": 96, "type": "DATASET", "confidence": 0.5943711485181536}]}, {"text": "The results will be analyzed in the CLEF-2007 workshop, and a special track will be proposed for CLEF-2008, where CLIR systems will have the opportunity to use the annotated data produced as a result of the Semeval-2007 task.", "labels": [], "entities": [{"text": "CLEF-2008", "start_pos": 97, "end_pos": 106, "type": "DATASET", "confidence": 0.8448381423950195}]}, {"text": "The task has a webpage with all the details at http://ixa2.si.ehu.es/semeval-clir.", "labels": [], "entities": []}, {"text": "This paper is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 describes the task with all the details regarding datasets, expansion/translation, the IR/CLIR system used, and steps for participation.", "labels": [], "entities": [{"text": "expansion/translation", "start_pos": 70, "end_pos": 91, "type": "TASK", "confidence": 0.8397974173227946}, {"text": "IR/CLIR", "start_pos": 97, "end_pos": 104, "type": "TASK", "confidence": 0.5513791839281718}]}, {"text": "Section 3 presents the evaluation performed and the results obtained by the participants.", "labels": [], "entities": []}, {"text": "Finally, Section 4 draws the conclusions and mention the future work.", "labels": [], "entities": []}], "datasetContent": [{"text": "The English CLEF data from years comprises corpora from 'Los Angeles Times' (year 1994) and 'Glasgow Herald' (year 1995) amounting to 169,477 documents (579 MB of raw text, 4.8GB in the XML format provided to participants, see Section 2.3) and 300 topics in English and Spanish (the topics are human translations of each other).", "labels": [], "entities": [{"text": "English CLEF data", "start_pos": 4, "end_pos": 21, "type": "DATASET", "confidence": 0.5835614999135336}, {"text": "Glasgow Herald' (year 1995)", "start_pos": 93, "end_pos": 120, "type": "DATASET", "confidence": 0.9413905143737793}]}, {"text": "The relevance judgments were taken from CLEF.", "labels": [], "entities": [{"text": "relevance", "start_pos": 4, "end_pos": 13, "type": "METRIC", "confidence": 0.9555584788322449}, {"text": "CLEF", "start_pos": 40, "end_pos": 44, "type": "DATASET", "confidence": 0.9507218599319458}]}, {"text": "This might have the disadvantage of having been produced by pooling the results of CLEF participants, and might bias the results towards systems not using WSD, specially for monolingual English retrieval.", "labels": [], "entities": []}, {"text": "We are considering the realization of a post-hoc analysis of the participants results in order to analyze the effect on the lack of pooling.", "labels": [], "entities": []}, {"text": "Due to the size of the document collection, we decided that the limited time available in the competition was too short to disambiguate the whole collection.", "labels": [], "entities": [{"text": "document collection", "start_pos": 23, "end_pos": 42, "type": "DATASET", "confidence": 0.8152100741863251}]}, {"text": "We thus chose to take a sixth part of the corpus at random, comprising 29,375 documents (874MB in the XML format distributed to participants).", "labels": [], "entities": []}, {"text": "Not all topics had relevant documents in this 17% sample, and therefore only 201 topics were effectively used for evaluation.", "labels": [], "entities": []}, {"text": "All in all, we reused 21,797 relevance judgements that contained one of the documents in the 17% sample, from which 923 are positive . For the future we would like to use the whole collection.", "labels": [], "entities": []}, {"text": "For each of the settings presented in Section 2 we present the results of the participants, as well as those of an in-house system presented by the organizers.", "labels": [], "entities": []}, {"text": "Please refer to the system description papers fora more complete description.", "labels": [], "entities": []}, {"text": "We also provide some baselines and alternative expansion (translation) strategies.", "labels": [], "entities": [{"text": "expansion (translation)", "start_pos": 47, "end_pos": 70, "type": "TASK", "confidence": 0.7120992094278336}]}, {"text": "All systems are evaluated according to their Mean Average Precision 6 (MAP) as computed by the trec eval software on the preexisting CLEF relevance-assessments.", "labels": [], "entities": [{"text": "Mean Average Precision 6 (MAP)", "start_pos": 45, "end_pos": 75, "type": "METRIC", "confidence": 0.9770313671657017}, {"text": "CLEF relevance-assessments", "start_pos": 133, "end_pos": 159, "type": "DATASET", "confidence": 0.8429864645004272}]}], "tableCaptions": [{"text": " Table 1: Retrieval results given as MAP. IRtops  stands for English IR with topic expansion. IR- docs stands for English IR with document expan- sion. CLIR stands for CLIR results for translated  documents.", "labels": [], "entities": []}, {"text": " Table 2: English WSD results in the Senseval-2 and  Senseval-3 all-words datasets.", "labels": [], "entities": [{"text": "WSD", "start_pos": 18, "end_pos": 21, "type": "TASK", "confidence": 0.43018731474876404}, {"text": "Senseval-3 all-words datasets", "start_pos": 53, "end_pos": 82, "type": "DATASET", "confidence": 0.6651897430419922}]}, {"text": " Table 3: Number of words in the document col- lection after expansion for the WSD system and all  baselines. wsdbest stands for the expansion strategy  used with participants.", "labels": [], "entities": [{"text": "WSD system", "start_pos": 79, "end_pos": 89, "type": "DATASET", "confidence": 0.6960214376449585}]}]}