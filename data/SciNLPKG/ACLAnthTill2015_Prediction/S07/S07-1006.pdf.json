{"title": [{"text": "SemEval-2007 Task 07: Coarse-Grained English All-Words Task", "labels": [], "entities": [{"text": "SemEval-2007 Task 07", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.831484297911326}]}], "abstractContent": [{"text": "This paper presents the coarse-grained En-glish all-words task at SemEval-2007.", "labels": [], "entities": []}, {"text": "We describe our experience in producing a coarse version of the WordNet sense inventory and preparing the sense-tagged corpus for the task.", "labels": [], "entities": [{"text": "WordNet sense inventory", "start_pos": 64, "end_pos": 87, "type": "DATASET", "confidence": 0.8972160816192627}]}, {"text": "We present the results of participating systems and discuss future directions .", "labels": [], "entities": []}], "introductionContent": [{"text": "It is commonly thought that one of the major obstacles to high-performance Word Sense Disambiguation (WSD) is the fine granularity of sense inventories.", "labels": [], "entities": [{"text": "Word Sense Disambiguation (WSD)", "start_pos": 75, "end_pos": 106, "type": "TASK", "confidence": 0.7619136522213618}]}, {"text": "State-of-the-art systems attained a disambiguation accuracy around 65% in the Senseval-3 all-words task (), where WordNet) was adopted as a reference sense inventory.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 51, "end_pos": 59, "type": "METRIC", "confidence": 0.8471531867980957}]}, {"text": "Unfortunately, WordNet is a fine-grained resource, encoding sense distinctions that are difficult to recognize even for human annotators ().", "labels": [], "entities": [{"text": "WordNet", "start_pos": 15, "end_pos": 22, "type": "DATASET", "confidence": 0.9596136808395386}]}, {"text": "Making WSD an enabling technique for end-to-end applications clearly depends on the ability to deal with reasonable sense distinctions.", "labels": [], "entities": [{"text": "WSD", "start_pos": 7, "end_pos": 10, "type": "TASK", "confidence": 0.9588898420333862}]}, {"text": "The aim of this task was to explicitly tackle the granularity issue and study the performance of WSD systems on an all-words basis when a coarser set of senses is provided for the target words.", "labels": [], "entities": [{"text": "WSD", "start_pos": 97, "end_pos": 100, "type": "TASK", "confidence": 0.9733799695968628}]}, {"text": "Given the need of the NLP community to work on freely available resources, the solution of adopting a different computational lexicon is not viable.", "labels": [], "entities": []}, {"text": "On the other hand, the production of a coarse-grained sense inventory is not a simple task.", "labels": [], "entities": []}, {"text": "The main issue is certainly the subjectivity of sense clusters.", "labels": [], "entities": []}, {"text": "To overcome this problem, different strategies can be adopted.", "labels": [], "entities": []}, {"text": "For instance, in the OntoNotes project) senses are grouped until a 90% inter-annotator agreement is achieved.", "labels": [], "entities": []}, {"text": "In contrast, as we describe in this paper, our approach is based on a mapping to a previously existing inventory which encodes sense distinctions at different levels of granularity, thus allowing to induce a sense clustering for the mapped senses.", "labels": [], "entities": []}, {"text": "We would like to mention that another SemEval-2007 task dealt with the issue of sense granularity for WSD, namely Task 17 (subtask #1): Coarsegrained English Lexical Sample WSD.", "labels": [], "entities": [{"text": "WSD", "start_pos": 102, "end_pos": 105, "type": "TASK", "confidence": 0.9036158323287964}, {"text": "Coarsegrained English Lexical Sample WSD", "start_pos": 136, "end_pos": 176, "type": "TASK", "confidence": 0.5715710639953613}]}, {"text": "In this paper, we report our experience in organizing Task 07.", "labels": [], "entities": [{"text": "organizing Task 07", "start_pos": 43, "end_pos": 61, "type": "TASK", "confidence": 0.8047669529914856}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Statistics about the five articles in the test  data set.", "labels": [], "entities": [{"text": "test  data set", "start_pos": 52, "end_pos": 66, "type": "DATASET", "confidence": 0.8631316820780436}]}, {"text": " Table 2: Statistics about the test set polysemy (N =  nouns, V = verbs, A = adjectives, R = adverbs).", "labels": [], "entities": []}, {"text": " Table 3: System scores sorted by F1 measure (A =  attempted, P = precision, R = recall, F1 = F1 mea- sure,  \u2020 : system from one of the task organizers).", "labels": [], "entities": [{"text": "F1 measure", "start_pos": 34, "end_pos": 44, "type": "METRIC", "confidence": 0.9883842468261719}, {"text": "precision", "start_pos": 66, "end_pos": 75, "type": "METRIC", "confidence": 0.9185464382171631}, {"text": "recall", "start_pos": 81, "end_pos": 87, "type": "METRIC", "confidence": 0.9437180757522583}, {"text": "F1", "start_pos": 89, "end_pos": 91, "type": "METRIC", "confidence": 0.9969778060913086}, {"text": "F1 mea- sure", "start_pos": 94, "end_pos": 106, "type": "METRIC", "confidence": 0.9023739546537399}]}, {"text": " Table 4: System scores sorted by F1 measure with  MFS adopted as a backoff strategy when no sense  assignment is attempted (  \u2020 : system from one of the  task organizers). Systems affected are marked in  bold.", "labels": [], "entities": [{"text": "F1 measure", "start_pos": 34, "end_pos": 44, "type": "METRIC", "confidence": 0.9759225845336914}]}, {"text": " Table 6: System scores by part-of-speech tag (N  = nouns, V = verbs, A = adjectives, R = adverbs)  sorted by overall F1 measure (best scores are marked  in bold,  \u2020 : system from one of the task organizers).", "labels": [], "entities": [{"text": "F1 measure", "start_pos": 118, "end_pos": 128, "type": "METRIC", "confidence": 0.9867182374000549}]}, {"text": " Table 5: System scores by article (best scores are marked in bold,  \u2020 : system from one of the task organizers).", "labels": [], "entities": []}]}