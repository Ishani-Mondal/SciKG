{"title": [], "abstractContent": [{"text": "This article introduces an unsupervised word sense disambiguation algorithm that is inspired by the lexical attraction models of Yuret (1998).", "labels": [], "entities": [{"text": "word sense disambiguation", "start_pos": 40, "end_pos": 65, "type": "TASK", "confidence": 0.6300496260325114}]}, {"text": "It is based on the assumption that the meanings of the words that form a sentence can be best assigned by constructing an interpretation of the whole sentence.", "labels": [], "entities": []}, {"text": "This interpretation is facilitated by a dependency-like context specification of a content word within the sentence.", "labels": [], "entities": []}, {"text": "Thus, finding the context words of a target word is a matter of finding a pseudo-syntactic dependency analysis of the sentence, called a linkage.", "labels": [], "entities": []}], "introductionContent": [{"text": "Word Sense Disambiguation (WSD) is a difficult Natural Language Processing task which requires that for every content word (noun, adjective, verb or adverb) the appropriate meaning is automatically selected from the available sense inventory . Traditionally, the WSD algorithms are divided into two rough classes: supervised and unsupervised.", "labels": [], "entities": [{"text": "Word Sense Disambiguation (WSD)", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.7988972018162409}]}, {"text": "The supervised paradigm relies on sense annotated corpora, with the assumption that neighbouring disambiguate words provide a strongly discriminating and generalizable context representation for the meaning of a target word.", "labels": [], "entities": []}, {"text": "Obviously, this approach suffers from the knowledge acquisition bottleneck in that there will never be enough training data to ensure a scalable result of such algorithms.", "labels": [], "entities": [{"text": "knowledge acquisition", "start_pos": 42, "end_pos": 63, "type": "TASK", "confidence": 0.8273988962173462}]}, {"text": "The unsupervised alternative to WSD tries to alleviate the burden of manually sense tagging the corpora, by employing algorithms that use different knowledge sources to determine the correct meaning in context.", "labels": [], "entities": [{"text": "WSD", "start_pos": 32, "end_pos": 35, "type": "TASK", "confidence": 0.8442361950874329}]}, {"text": "In fact, the \"knowledge source usage\" is another way to distinguish among the WSD methods.", "labels": [], "entities": []}, {"text": "Such methods call upon further processing of the text to be disambiguated such as parsing and/or use handcrafted, semantically rich sense inventories such as WordNet.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 158, "end_pos": 165, "type": "DATASET", "confidence": 0.9637237191200256}]}, {"text": "WSD methods in this category range from the very simple ranking based on counting the number of words occurring in both the target word's context and its sense definitions in a reference dictionary to the more elaborated approaches using the semantic lexicon's taxonomies, (shallow) parsing, collocation discovery etc.).", "labels": [], "entities": [{"text": "collocation discovery", "start_pos": 292, "end_pos": 313, "type": "TASK", "confidence": 0.7159934341907501}]}, {"text": "One of the central issues of any WSD implementation is given by the context representation.", "labels": [], "entities": [{"text": "WSD", "start_pos": 33, "end_pos": 36, "type": "TASK", "confidence": 0.9659580588340759}]}, {"text": "The standard principle that is applied when trying to disambiguate the meaning of a word is that the same word in similar contexts should have the same meaning.", "labels": [], "entities": []}, {"text": "By and large, the context of a target word is materialized by a collection of features among which are: the collocates of the target word, the part-ofspeech (POS) of the target word, \u00b1k words surrounding the target word and/or their POSes and soon.", "labels": [], "entities": []}, {"text": "More often than not, the contexts similarity is estimated by the distance in the feature vector space.", "labels": [], "entities": []}, {"text": "defines the local context of a target word by the collection of syntactic dependencies in which the word takes part.", "labels": [], "entities": []}, {"text": "According to this notion of con-text, Lin assumes that two different words are likely to have similar meanings if they occur in identical local contexts.", "labels": [], "entities": []}, {"text": "What we will attempt here is to combine the two views of context similarity/identity versus meaning similarity/identity by using a dependency-like representation of the context as a lexical attraction model.", "labels": [], "entities": []}, {"text": "More specifically, we will not consider any feature of the context and will try to maximize a meaning attraction function overall linked words of a sentence.", "labels": [], "entities": []}, {"text": "In section 2 we will describe SynWSD, an unsupervised, knowledge-based WSD algorithm and in sections 3 and 4 we will present the application of Syn-WSD to two of SEMEVAL-2007 \"all words\" tasks: English Coarse-Grained and English Fine-Grained.", "labels": [], "entities": []}, {"text": "Finally, with section 5 we will conclude the article.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": []}