{"title": [{"text": "SWAT-MP: The SemEval-2007 Systems for Task 5 and Task 14", "labels": [], "entities": [{"text": "SemEval-2007", "start_pos": 13, "end_pos": 25, "type": "TASK", "confidence": 0.9370843768119812}]}], "abstractContent": [{"text": "In this paper, we describe our two SemEval-2007 entries.", "labels": [], "entities": []}, {"text": "Our first entry, for Task 5: Multilingual Chinese-English Lexical Sample Task, is a supervised system that decides the most appropriate English translation of a Chinese target word.", "labels": [], "entities": [{"text": "Multilingual Chinese-English Lexical Sample Task", "start_pos": 29, "end_pos": 77, "type": "TASK", "confidence": 0.6781361281871796}, {"text": "English translation of a Chinese target word", "start_pos": 136, "end_pos": 180, "type": "TASK", "confidence": 0.7735624739101955}]}, {"text": "This system uses a combination of Na\u00a8\u0131veNa\u00a8\u0131ve Bayes, nearest neighbor cosine, decision lists, and latent semantic analysis.", "labels": [], "entities": []}, {"text": "Our second entry, for Task 14: Affective Text, is a supervised system that annotates headlines using a predefined list of emotions.", "labels": [], "entities": []}, {"text": "This system uses synonym expansion and matches lemmatized unigrams in the test headlines against a corpus of hand-annotated headlines.", "labels": [], "entities": [{"text": "synonym expansion", "start_pos": 17, "end_pos": 34, "type": "TASK", "confidence": 0.8748202323913574}]}], "introductionContent": [{"text": "This paper describes our two entries in.", "labels": [], "entities": []}, {"text": "The first entry, a supervised system used in the Multilingual Chinese-English Lexical Sample task, is an extension of the system described in ().", "labels": [], "entities": [{"text": "Multilingual Chinese-English Lexical Sample task", "start_pos": 49, "end_pos": 97, "type": "TASK", "confidence": 0.5120885133743286}]}, {"text": "We implement five different classifiers: a Na\u00a8\u0131veNa\u00a8\u0131ve Bayes classifier, a decision list classifier, two different nearest neighbor cosine classifiers, and a classifier based on Latent Semantic Analysis.", "labels": [], "entities": []}, {"text": "Section 2.2 describes each of the individual classifiers, Section 2.3 describes our classifier combination system, and Section 2.4 presents our results.", "labels": [], "entities": []}, {"text": "The second entry, a supervised system used in the Affective Text task, uses a corpus of headlines hand-annotated by non-experts.", "labels": [], "entities": [{"text": "Affective Text task", "start_pos": 50, "end_pos": 69, "type": "TASK", "confidence": 0.8568284710248312}]}, {"text": "It also uses an online thesaurus to match synonyms and antonyms of the sense labels.", "labels": [], "entities": []}, {"text": "Section 3.1 describes the creation of the annotated training corpus, Section 3.2 describes our method for assigning scores to the headlines, and Section 3.3 presents our results.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: The (micro-averaged) precision of each of  our classifiers in cross-validation, plus the actual re- sults from our entry in SemEval-2007.", "labels": [], "entities": [{"text": "precision", "start_pos": 31, "end_pos": 40, "type": "METRIC", "confidence": 0.9813218712806702}]}, {"text": " Table 2: On cross-validated training data, system  precision when using different smoothing parame- ters in the Na\u00a8\u0131veNa\u00a8\u0131ve Bayes and decision list classifiers.", "labels": [], "entities": [{"text": "precision", "start_pos": 52, "end_pos": 61, "type": "METRIC", "confidence": 0.9974337220191956}]}, {"text": " Table 3: On cross-validated training data, the preci- sion when using different features with each classi- fier, and with the combination of all classifiers. All  feature sets include a simple, unweighted bag-of- words in addition to the feature listed.", "labels": [], "entities": [{"text": "preci- sion", "start_pos": 48, "end_pos": 59, "type": "METRIC", "confidence": 0.9367945392926534}]}, {"text": " Table 4: Pearson correlations between trial data an- notators and our human annotators.", "labels": [], "entities": [{"text": "Pearson correlations", "start_pos": 10, "end_pos": 30, "type": "METRIC", "confidence": 0.9296125769615173}]}, {"text": " Table 5: A comparison of results on the provided  trial data as headlines are added to the training  set. The scores are given as Pearson correlations of  scores for training sets of size 100, 250, and 1000  headlines.", "labels": [], "entities": [{"text": "Pearson correlations", "start_pos": 131, "end_pos": 151, "type": "METRIC", "confidence": 0.9404910206794739}]}, {"text": " Table 6: Our full results from SemEval-2007, Task  14, as reported by the task organizers. Fine-grained  scores are given as Pearson correlations. Coarse- grained scores are given as accuracy (A), preci- sion (P), and recall (R).", "labels": [], "entities": [{"text": "Pearson correlations", "start_pos": 126, "end_pos": 146, "type": "METRIC", "confidence": 0.9173267185688019}, {"text": "accuracy (A)", "start_pos": 184, "end_pos": 196, "type": "METRIC", "confidence": 0.9437116086483002}, {"text": "preci- sion (P)", "start_pos": 198, "end_pos": 213, "type": "METRIC", "confidence": 0.8954461812973022}, {"text": "recall (R)", "start_pos": 219, "end_pos": 229, "type": "METRIC", "confidence": 0.9509437680244446}]}]}