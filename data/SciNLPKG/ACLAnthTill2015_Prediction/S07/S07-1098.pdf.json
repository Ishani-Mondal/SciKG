{"title": [{"text": "USFD: Preliminary Exploration of Features and Classifiers for the TempEval-2007 Tasks", "labels": [], "entities": [{"text": "USFD", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.940205991268158}, {"text": "TempEval-2007 Tasks", "start_pos": 66, "end_pos": 85, "type": "TASK", "confidence": 0.5243241786956787}]}], "abstractContent": [{"text": "We describe the Sheffield system used in TempEval-2007.", "labels": [], "entities": []}, {"text": "Our system takes a machine-learning (ML) based approach, treating temporal relation assignment as a simple classification task and using features easily derived from the TempEval data, i.e. which do not require 'deeper' NLP analysis.", "labels": [], "entities": [{"text": "temporal relation assignment", "start_pos": 66, "end_pos": 94, "type": "TASK", "confidence": 0.6207374234994253}, {"text": "TempEval data", "start_pos": 170, "end_pos": 183, "type": "DATASET", "confidence": 0.8905289769172668}]}, {"text": "We aimed to explore three questions: (1) How well would a 'lite' approach of this kind perform?", "labels": [], "entities": []}, {"text": "(2) Which features contribute positively to system performance?", "labels": [], "entities": []}, {"text": "(3) Which ML algorithm is better suited for the TempEval tasks?", "labels": [], "entities": [{"text": "TempEval tasks", "start_pos": 48, "end_pos": 62, "type": "TASK", "confidence": 0.636610209941864}]}, {"text": "We used the Weka ML workbench to facilitate experimenting with different ML algorithms.", "labels": [], "entities": [{"text": "Weka ML workbench", "start_pos": 12, "end_pos": 29, "type": "DATASET", "confidence": 0.8912727236747742}]}, {"text": "The paper describes our system and supplies preliminary answers to the above questions.", "labels": [], "entities": []}], "introductionContent": [{"text": "The Sheffield team were involved in TempEval as co-proposers/co-organisers of the task.", "labels": [], "entities": [{"text": "TempEval", "start_pos": 36, "end_pos": 44, "type": "TASK", "confidence": 0.6649644374847412}]}, {"text": "1 For our participation in the task, we decided to pursue an MLbased approach, the benefits of which have been explored elsewhere).", "labels": [], "entities": []}, {"text": "For the TempEval tasks, this is easily done by treating the assignment of temporal relation types as a simple classification task, using readily available information for the instance features.", "labels": [], "entities": []}, {"text": "More specifically, the features used were ones provided as We maintained a strict separation between persons assisting in annotation of the test corpus and those involved in system development.", "labels": [], "entities": []}, {"text": "attributes in the TempEval data annotation for the events/times being related, plus some additional features that could be straightforwardly computed from documents, i.e. without the use of more heavily 'engineered' NLP components.", "labels": [], "entities": [{"text": "TempEval data annotation", "start_pos": 18, "end_pos": 42, "type": "DATASET", "confidence": 0.857436994711558}]}, {"text": "The aims of this work were three-fold.", "labels": [], "entities": []}, {"text": "First, we wanted to see whether a 'lite' approach of this kind could yield reasonable performance, before pursuing possibilities that relied on using 'deeper' NLP analysis methods.", "labels": [], "entities": []}, {"text": "Secondly, we were interested to see which of the features considered would contribute positively to system performance.", "labels": [], "entities": []}, {"text": "Thirdly, rather than selecting a single ML approach (e.g. one of those currently in vogue within NLP), we wanted to look across ML algorithms to see if any approach was better suited to the TempEval tasks than any other, and consequently we used the Weka workbench) in our ML experiments.", "labels": [], "entities": [{"text": "Weka workbench", "start_pos": 250, "end_pos": 264, "type": "DATASET", "confidence": 0.894647866487503}]}, {"text": "In what follows, we will first describe how our system was constructed, before going onto discuss our main observations around the key aims mentioned above.", "labels": [], "entities": []}, {"text": "For example, in regard to our 'lite' approach, we would observe (c.f. the results reported in the Task Description paper) that although some other systems scored more highly, the score differences were relatively small.", "labels": [], "entities": []}, {"text": "Regarding features, we found for example that the system performed better for Task A when, surprisingly, the tense attribute of EVENTs was excluded.", "labels": [], "entities": [{"text": "EVENTs", "start_pos": 128, "end_pos": 134, "type": "METRIC", "confidence": 0.8756592273712158}]}, {"text": "Regarding ML algorithms, we found not only that there was substantial variation between the effectiveness of different algorithms for assigning relations (as one might expect), but also that there was considerable differences in the relative effectiveness of algorithms across tasks, i.e. so that an algorithm performing well on one task (compared to the alternatives), might perform rather poorly on another task.", "labels": [], "entities": [{"text": "ML algorithms", "start_pos": 10, "end_pos": 23, "type": "TASK", "confidence": 0.9087798893451691}]}, {"text": "The paper closes with some comments about future research directions.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 2: Comparing different algorithms (%-acc.  scores, from cross-validation over training data)", "labels": [], "entities": [{"text": "acc.", "start_pos": 44, "end_pos": 48, "type": "METRIC", "confidence": 0.9595848321914673}]}, {"text": " Table 3: Competition task scores for Sheffield sys- tem (USFD), plus average/max scores across all  competing systems", "labels": [], "entities": [{"text": "Sheffield sys- tem (USFD)", "start_pos": 38, "end_pos": 63, "type": "DATASET", "confidence": 0.8410066281046186}]}]}