{"title": [{"text": "KU: Word Sense Disambiguation by Substitution", "labels": [], "entities": [{"text": "Word Sense Disambiguation", "start_pos": 4, "end_pos": 29, "type": "TASK", "confidence": 0.7199660142262777}]}], "abstractContent": [{"text": "Data sparsity is one of the main factors that make word sense disambiguation (WSD) difficult.", "labels": [], "entities": [{"text": "word sense disambiguation (WSD)", "start_pos": 51, "end_pos": 82, "type": "TASK", "confidence": 0.8314847201108932}]}, {"text": "To overcome this problem we need to find effective ways to use resources other than sense labeled data.", "labels": [], "entities": []}, {"text": "In this paper I describe a WSD system that uses a statistical language model based on a large unanno-tated corpus.", "labels": [], "entities": [{"text": "WSD", "start_pos": 27, "end_pos": 30, "type": "TASK", "confidence": 0.9648623466491699}]}, {"text": "The model is used to evaluate the likelihood of various substitutes fora word in a given context.", "labels": [], "entities": []}, {"text": "These likelihoods are then used to determine the best sense for the word in novel contexts.", "labels": [], "entities": []}, {"text": "The resulting system participated in three tasks in the Se-mEval 2007 workshop.", "labels": [], "entities": [{"text": "Se-mEval 2007 workshop", "start_pos": 56, "end_pos": 78, "type": "DATASET", "confidence": 0.6823176145553589}]}, {"text": "The WSD of prepositions task proved to be challenging for the system, possibly illustrating some of its limitations: e.g. not all words have good substitutes.", "labels": [], "entities": [{"text": "WSD of prepositions task", "start_pos": 4, "end_pos": 28, "type": "TASK", "confidence": 0.8766046315431595}]}, {"text": "The system achieved promising results for the English lexical sample and En-glish lexical substitution tasks.", "labels": [], "entities": [{"text": "En-glish lexical substitution tasks", "start_pos": 73, "end_pos": 108, "type": "TASK", "confidence": 0.6260011345148087}]}], "introductionContent": [{"text": "A typical word sense disambiguation system is trained on a corpus of manually sense tagged text.", "labels": [], "entities": [{"text": "word sense disambiguation", "start_pos": 10, "end_pos": 35, "type": "TASK", "confidence": 0.7608372171719869}]}, {"text": "Machine learning algorithms are then employed to find the best sense fora word in a novel context by generalizing from the training examples.", "labels": [], "entities": []}, {"text": "The training data is costly to generate and inter-annotator agreement is difficult to achieve.", "labels": [], "entities": []}, {"text": "Thus there is very little training data available: the largest single corpus of sense tagged text, SemCor, has 41,497 sense tagged words.) observed that approximately half of the test instances do not match any of the contextual features learned from the training data for an all words disambiguation task.", "labels": [], "entities": []}, {"text": "( found that each successive doubling of the training data only leads to a 3-4% error reduction within their experimental range.", "labels": [], "entities": [{"text": "error", "start_pos": 80, "end_pos": 85, "type": "METRIC", "confidence": 0.9652969241142273}]}, {"text": "Humans do not seem to be cursed with an exponential training data requirement to become proficient with the use of a word.", "labels": [], "entities": []}, {"text": "Dictionaries typically contain a definition and one or two examples of usage for each sense.", "labels": [], "entities": []}, {"text": "This seems to be sufficient fora human to use the word correctly in contexts that share no surface features with the dictionary examples.", "labels": [], "entities": []}, {"text": "The 10 8 waking seconds it takes a person to become proficient in a language does not seem sufficient to master all the words and their different senses.", "labels": [], "entities": []}, {"text": "We need models that do not require large amounts of annotated text to perform WSD.", "labels": [], "entities": [{"text": "WSD", "start_pos": 78, "end_pos": 81, "type": "TASK", "confidence": 0.9405557513237}]}, {"text": "What possible process can explain our proficiency without relying on a lot of labeled data?", "labels": [], "entities": []}, {"text": "Let us look at a concrete example: The two most frequent senses of the word \"board\" according to) are the \"committee\" sense, and the \"plank\" sense.", "labels": [], "entities": []}, {"text": "When we hear a sentence like \"There was aboard meeting\", it is immediately obvious that the first sense is intended.", "labels": [], "entities": []}, {"text": "One hypothesis is that a commonsense inference engine in your brain rules out the second sense.", "labels": [], "entities": []}, {"text": "Maybe you visualize pieces of timber sitting around a meeting table and decide that it is absurd.", "labels": [], "entities": []}, {"text": "Another hypothesis is that the plank sense does not even occur to you because you hear this sentence in the middle of a conversation about corporate matters.", "labels": [], "entities": []}, {"text": "Therefore the plank sense is not psychologically \"primed\".", "labels": [], "entities": []}, {"text": "Finally, maybe you subconsciously perform a substitution and the sentence \"There was a plank meeting\" just sounds bad to your linguistic \"ear\".", "labels": [], "entities": []}, {"text": "In this paper I will describe a system that judges potential substitutions in a given context using a statistical language model as a surrogate for the linguistic \"ear\".", "labels": [], "entities": []}, {"text": "The likelihoods of the various substitutes are used to select the best sense fora target word.", "labels": [], "entities": []}, {"text": "The use of substitutes for WSD is not new.", "labels": [], "entities": [{"text": "WSD", "start_pos": 27, "end_pos": 30, "type": "TASK", "confidence": 0.7388019561767578}]}, {"text": "() demonstrated the use of related monosemous words (monosemous relatives) to collect examples fora given sense from the Internet.) used the monosemous relatives technique for bootstrapping the automatic acquisition of large sense tagged corpora.", "labels": [], "entities": [{"text": "automatic acquisition of large sense tagged corpora", "start_pos": 194, "end_pos": 245, "type": "TASK", "confidence": 0.6010958467211042}]}, {"text": "In both cases, the focus was on collecting more labeled examples to be subsequently used with supervised machine learning techniques.", "labels": [], "entities": []}, {"text": "() extended the method to make use of polysemous relatives.", "labels": [], "entities": []}, {"text": "More importantly, their method places these relatives in the context of the target word to query a search engine and uses the search results to predict the best sense in an unsupervised manner.", "labels": [], "entities": []}, {"text": "There are three areas that distinguish my system from the previous work: (i) The probabilities for substitutes in context are determined using a statistical language model rather than search hits on heuristically constructed queries, (ii) The set of substitutes are derived from multiple sources and optimized using WSD performance as the objective function, and (iii) A probabilistic generative model is used to select the best sense rather than typical machine learning algorithms or heuristics.", "labels": [], "entities": []}, {"text": "Each of these areas is explained further below.", "labels": [], "entities": []}, {"text": "Probabilities for substitutes: Statistical language modeling is the art of determining the probability of a sequence of words.", "labels": [], "entities": [{"text": "Statistical language modeling", "start_pos": 31, "end_pos": 60, "type": "TASK", "confidence": 0.8130066196123759}]}, {"text": "According to the model used in this study, the sentence \"There was a committee meeting\" is 17,629 times more likely than the sentence \"There was a plank meeting\".", "labels": [], "entities": []}, {"text": "Thus, a statistical language model can be used as a surrogate for your inner ear that decides what sounds good and what sounds bad.", "labels": [], "entities": []}, {"text": "I used a language model based on the Web 1T 5-gram dataset) which gives the counts of 1 to 5-grams in a web corpus of 10 12 words.", "labels": [], "entities": [{"text": "Web 1T 5-gram dataset", "start_pos": 37, "end_pos": 58, "type": "DATASET", "confidence": 0.7779660224914551}]}, {"text": "The details of the Web1T model are given in the Appendix.", "labels": [], "entities": []}, {"text": "Given that I criticize existing WSD algorithms for using too much data, it might seem hypocritical to employ a data source with 10 12 words.", "labels": [], "entities": [{"text": "WSD", "start_pos": 32, "end_pos": 35, "type": "TASK", "confidence": 0.9393107891082764}]}, {"text": "In my defense, from an engineering perspective, an unannotated 10 12 word corpus exists, whereas large sense tagged corpora do not.", "labels": [], "entities": []}, {"text": "From a scientific perspective, it is clear that no human ever comes close to experiencing 10 12 words, but they do outperform simple n-gram language models based on that much data in predicting the likelihood of words in novel contexts.", "labels": [], "entities": []}, {"text": "So, even though we do not know how humans do it, we do know that they have the equivalent of a powerful statistical language model in their heads.", "labels": [], "entities": []}, {"text": "Selecting the best substitutes: Perhaps more important for the performance of the system is the decision of which substitutes to try.", "labels": [], "entities": []}, {"text": "We never thought of using \"monkey\" as a potential substitute for \"board\".", "labels": [], "entities": []}, {"text": "One possibility is to use the synonyms in WordNet which were selected such that they can be interchanged in at least some contexts.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 42, "end_pos": 49, "type": "DATASET", "confidence": 0.9535188674926758}]}, {"text": "However 54% of WordNet synsets do not have any synonyms.", "labels": [], "entities": [{"text": "WordNet synsets", "start_pos": 15, "end_pos": 30, "type": "DATASET", "confidence": 0.935687929391861}]}, {"text": "Besides, synonymous words would not always help if they share similar ambiguities in meaning.", "labels": [], "entities": []}, {"text": "Substitutes that are not synonyms, on the other hand, maybe very useful such as \"hot\" vs. \"cold\" or \"car\" vs. \"truck\".", "labels": [], "entities": []}, {"text": "In general we are looking for potential substitutes that have a high likelihood of appearing in contexts that are associated with a specific sense of the target word.", "labels": [], "entities": []}, {"text": "The substitute selection method used in this work is described in Section 3.", "labels": [], "entities": []}, {"text": "Selecting the best sense: Once we have a language model and a set of substitutes to try, we need a decision procedure that picks the best sense of a word in a given context.", "labels": [], "entities": []}, {"text": "An unsupervised system can be designed to keep track of the sense associated with each substitute based on the lexical resource used.", "labels": [], "entities": []}, {"text": "However since I used multiple lexical resources, and had training data available, I chose a supervised approach.", "labels": [], "entities": []}, {"text": "For each instance in the training set, the likelihood of each substitute is determined.", "labels": [], "entities": [{"text": "likelihood", "start_pos": 43, "end_pos": 53, "type": "METRIC", "confidence": 0.9805117845535278}]}, {"text": "Then instances of a single sense are grouped together to yield a probability distribution over the substitutes for that sense.", "labels": [], "entities": []}, {"text": "When a test instance is encountered its substitute distribution is compared to that of each sense to select the most appropriate one.", "labels": [], "entities": []}, {"text": "Section 2 describes the sense selection procedure in detail.", "labels": [], "entities": [{"text": "sense selection", "start_pos": 24, "end_pos": 39, "type": "TASK", "confidence": 0.9045374691486359}]}, {"text": "We could say each context is represented with the likelihood it assigns to various substitutes rather than its surface features.", "labels": [], "entities": []}, {"text": "That way contexts that do not share any surface features can be related to each other.", "labels": [], "entities": []}, {"text": "Results: To summarize the results, in the Word Sense Disambiguation of Prepositions Task, the system achieved 54.7% accuracy 1 . This is 15.1% above the baseline of picking the most frequent sense but 14.6% below the best system.", "labels": [], "entities": [{"text": "Word Sense Disambiguation of Prepositions Task", "start_pos": 42, "end_pos": 88, "type": "TASK", "confidence": 0.7139739692211151}, {"text": "accuracy", "start_pos": 116, "end_pos": 124, "type": "METRIC", "confidence": 0.9993521571159363}]}, {"text": "In the Coarse Grained English Lexical Sample WSD Task, the system achieved 85.1% accuracy, which is 6.4% above the baseline of picking the most frequent sense and 3.6% below the best system.", "labels": [], "entities": [{"text": "Coarse Grained English Lexical Sample WSD Task", "start_pos": 7, "end_pos": 53, "type": "TASK", "confidence": 0.7167397056307111}, {"text": "accuracy", "start_pos": 81, "end_pos": 89, "type": "METRIC", "confidence": 0.9982007741928101}]}, {"text": "Finally, in the English Lexical Substitution Task, the system achieved the top result for picking the best substitute for each word.", "labels": [], "entities": [{"text": "English Lexical Substitution Task", "start_pos": 16, "end_pos": 49, "type": "TASK", "confidence": 0.6244019865989685}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: BEST and OOT results: P is precision, R  is recall, Mode indicates accuracy selecting the sin- gle preferred substitute when there is one, NMWT  is the score without items identified as multi-words,  NMWS is the score using only single word substi- tutes, RAND is the score for the items selected ran- domly, and MAN is the score for the items selected  manually.", "labels": [], "entities": [{"text": "BEST", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9977092742919922}, {"text": "OOT", "start_pos": 19, "end_pos": 22, "type": "METRIC", "confidence": 0.9736958742141724}, {"text": "precision", "start_pos": 37, "end_pos": 46, "type": "METRIC", "confidence": 0.9991395473480225}, {"text": "recall", "start_pos": 54, "end_pos": 60, "type": "METRIC", "confidence": 0.9987733960151672}, {"text": "accuracy", "start_pos": 77, "end_pos": 85, "type": "METRIC", "confidence": 0.9913712739944458}, {"text": "NMWT", "start_pos": 149, "end_pos": 153, "type": "METRIC", "confidence": 0.7867196798324585}, {"text": "RAND", "start_pos": 266, "end_pos": 270, "type": "METRIC", "confidence": 0.9233015179634094}, {"text": "MAN", "start_pos": 323, "end_pos": 326, "type": "METRIC", "confidence": 0.985091507434845}]}]}