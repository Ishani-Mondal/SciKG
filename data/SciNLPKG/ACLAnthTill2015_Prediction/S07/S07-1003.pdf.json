{"title": [{"text": "SemEval-2007 Task 04: Classification of Semantic Relations between Nominals", "labels": [], "entities": [{"text": "SemEval-2007 Task", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.8687914311885834}, {"text": "Classification of Semantic Relations between Nominals", "start_pos": 22, "end_pos": 75, "type": "TASK", "confidence": 0.8530572752157847}]}], "abstractContent": [{"text": "The NLP community has shown a renewed interest in deeper semantic analyses, among them automatic recognition of relations between pairs of words in a text.", "labels": [], "entities": [{"text": "automatic recognition of relations between pairs of words in a text", "start_pos": 87, "end_pos": 154, "type": "TASK", "confidence": 0.8625415671955455}]}, {"text": "We present an evaluation task designed to provide a framework for comparing different approaches to classifying semantic relations between nom-inals in a sentence.", "labels": [], "entities": []}, {"text": "This is part of SemEval, the 4 th edition of the semantic evaluation event previously known as SensEval.", "labels": [], "entities": []}, {"text": "We define the task, describe the training/test data and their creation, list the participating systems and discuss their results.", "labels": [], "entities": []}, {"text": "There were 14 teams who submitted 15 systems.", "labels": [], "entities": []}, {"text": "1 Task Description and Related Work The theme of Task 4 is the classification of semantic relations between simple nominals (nouns or base noun phrases) other than named entities-honey bee, for example, shows an instance of the Product-Producer relation.", "labels": [], "entities": [{"text": "classification of semantic relations between simple nominals (nouns or base noun phrases)", "start_pos": 63, "end_pos": 152, "type": "TASK", "confidence": 0.6658216800008502}]}, {"text": "The classification occurs in the context of a sentence in a written English text.", "labels": [], "entities": []}, {"text": "Algorithms for classifying semantic relations can be applied in information retrieval, information extraction , text summarization, question answering and soon.", "labels": [], "entities": [{"text": "classifying semantic relations", "start_pos": 15, "end_pos": 45, "type": "TASK", "confidence": 0.8348701397577921}, {"text": "information retrieval", "start_pos": 64, "end_pos": 85, "type": "TASK", "confidence": 0.8113793134689331}, {"text": "information extraction", "start_pos": 87, "end_pos": 109, "type": "TASK", "confidence": 0.863014280796051}, {"text": "text summarization", "start_pos": 112, "end_pos": 130, "type": "TASK", "confidence": 0.7684906125068665}, {"text": "question answering", "start_pos": 132, "end_pos": 150, "type": "TASK", "confidence": 0.9024137556552887}]}, {"text": "The recognition of textual entailment (Tatu and Moldovan, 2005) is an example of successful use of this type of deeper analysis in high-end NLP applications.", "labels": [], "entities": [{"text": "recognition of textual entailment", "start_pos": 4, "end_pos": 37, "type": "TASK", "confidence": 0.7947077304124832}]}, {"text": "The literature shows a wide variety of methods of nominal relation classification.", "labels": [], "entities": [{"text": "nominal relation classification", "start_pos": 50, "end_pos": 81, "type": "TASK", "confidence": 0.8172135949134827}]}, {"text": "They depend as much on the training data as on the domain of application and the available resources.", "labels": [], "entities": []}, {"text": "Rosario and Hearst (2001) classify noun compounds from the domain of medicine, using 13 classes that describe the semantic relation between the head noun and the modifier in a given noun compound.", "labels": [], "entities": []}, {"text": "(2002) classify noun compounds using the MeSH hierarchy and a multi-level hierarchy of semantic relations, with 15 classes at the top level.", "labels": [], "entities": [{"text": "MeSH hierarchy", "start_pos": 41, "end_pos": 55, "type": "DATASET", "confidence": 0.8589228987693787}]}, {"text": "Nastase and Szpakowicz (2003) present a two-level hierarchy for classifying noun-modifier relations in base noun phrases from general text, with 5 classes at the top and 30 classes at the bottom; other researchers (Turney and Littman, 2005; Turney, 2005; Nastase et al., 2006) have used their class scheme and data set.", "labels": [], "entities": [{"text": "classifying noun-modifier relations in base noun phrases from general text", "start_pos": 64, "end_pos": 138, "type": "TASK", "confidence": 0.766077521443367}]}, {"text": "(2004) propose a 35-class scheme to classify relations in various phrases; the same scheme has been applied to noun compounds and other noun phrases (Girju et al., 2005).", "labels": [], "entities": []}, {"text": "Chklovski and Pantel (2004) introduce a 5-class set, designed specifically for characterizing verb-verb semantic relations.", "labels": [], "entities": [{"text": "characterizing verb-verb semantic relations", "start_pos": 79, "end_pos": 122, "type": "TASK", "confidence": 0.8070013225078583}]}, {"text": "(2001) propose 17 classes targeted to relations between genes.", "labels": [], "entities": []}, {"text": "La-pata (2002) presents a binary classification of relations in nominalizations.", "labels": [], "entities": []}, {"text": "There is little consensus on the relation sets and algorithms for analyzing semantic relations, and it seems unlikely that any single scheme could work for all applications.", "labels": [], "entities": []}, {"text": "For example, the gene-gene relation scheme of Stephens et al.", "labels": [], "entities": []}, {"text": "(2001), with relations like X phosphorylates Y, is unlikely to be transferred easily to general text.", "labels": [], "entities": []}, {"text": "We have created a benchmark data set to allow the evaluation of different semantic relation classification algorithms.", "labels": [], "entities": [{"text": "semantic relation classification", "start_pos": 74, "end_pos": 106, "type": "TASK", "confidence": 0.663995494445165}]}, {"text": "We do not presume to propose a single classification scheme, however alluring it would 13", "labels": [], "entities": []}], "introductionContent": [], "datasetContent": [], "tableCaptions": [{"text": " Table 2: Baselines: precision, recall, F -measure and  accuracy averaged over the 7 binary classifications.", "labels": [], "entities": [{"text": "precision", "start_pos": 21, "end_pos": 30, "type": "METRIC", "confidence": 0.9997187256813049}, {"text": "recall", "start_pos": 32, "end_pos": 38, "type": "METRIC", "confidence": 0.9996225833892822}, {"text": "F -measure", "start_pos": 40, "end_pos": 50, "type": "METRIC", "confidence": 0.9896421035130819}, {"text": "accuracy", "start_pos": 56, "end_pos": 64, "type": "METRIC", "confidence": 0.999052107334137}]}, {"text": " Table 3: System performance grouped by category.  Precision, recall, F -measure and accuracy macro- averaged over each system's performance on all 7  relations.", "labels": [], "entities": [{"text": "Precision", "start_pos": 51, "end_pos": 60, "type": "METRIC", "confidence": 0.9991795420646667}, {"text": "recall", "start_pos": 62, "end_pos": 68, "type": "METRIC", "confidence": 0.9985502362251282}, {"text": "F -measure", "start_pos": 70, "end_pos": 80, "type": "METRIC", "confidence": 0.9928223888079325}, {"text": "accuracy macro- averaged", "start_pos": 85, "end_pos": 109, "type": "METRIC", "confidence": 0.9138826429843903}]}, {"text": " Table 5: The best results per relation. Precision, recall, F -measure and accuracy macro-averaged over each  system's performance on all 7 relations. Base-F shows the baseline F -measure (alltrue), Base-Acc -the  baseline accuracy score (majority). The last column shows the average rank for each relation.", "labels": [], "entities": [{"text": "Precision", "start_pos": 41, "end_pos": 50, "type": "METRIC", "confidence": 0.9988688826560974}, {"text": "recall", "start_pos": 52, "end_pos": 58, "type": "METRIC", "confidence": 0.998084545135498}, {"text": "F -measure", "start_pos": 60, "end_pos": 70, "type": "METRIC", "confidence": 0.9911774198214213}, {"text": "accuracy", "start_pos": 75, "end_pos": 83, "type": "METRIC", "confidence": 0.9983768463134766}, {"text": "F -measure", "start_pos": 177, "end_pos": 187, "type": "METRIC", "confidence": 0.8422924876213074}, {"text": "accuracy score", "start_pos": 223, "end_pos": 237, "type": "METRIC", "confidence": 0.9471133053302765}]}]}