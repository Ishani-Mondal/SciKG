{"title": [{"text": "SemEval-2007 Task 08: Metonymy Resolution at SemEval-2007", "labels": [], "entities": [{"text": "Metonymy Resolution", "start_pos": 22, "end_pos": 41, "type": "TASK", "confidence": 0.8999578952789307}, {"text": "SemEval-2007", "start_pos": 45, "end_pos": 57, "type": "TASK", "confidence": 0.515515923500061}]}], "abstractContent": [{"text": "We provide an overview of the metonymy resolution shared task organised within SemEval-2007.", "labels": [], "entities": [{"text": "metonymy resolution shared task", "start_pos": 30, "end_pos": 61, "type": "TASK", "confidence": 0.9311718493700027}]}, {"text": "We describe the problem, the data provided to participants, and the evaluation measures we used to assess performance.", "labels": [], "entities": []}, {"text": "We also give an overview of the systems that have taken part in the task, and discuss possible directions for future work.", "labels": [], "entities": []}], "introductionContent": [{"text": "Both word sense disambiguation and named entity recognition have benefited enormously from shared task evaluations, for example in the Senseval, MUC and CoNLL frameworks.", "labels": [], "entities": [{"text": "word sense disambiguation", "start_pos": 5, "end_pos": 30, "type": "TASK", "confidence": 0.7422924836476644}, {"text": "named entity recognition", "start_pos": 35, "end_pos": 59, "type": "TASK", "confidence": 0.6204788784186045}]}, {"text": "Similar campaigns have not been developed for the resolution of figurative language, such as metaphor, metonymy, idioms and irony.", "labels": [], "entities": []}, {"text": "However, resolution of figurative language is an important complement to and extension of word sense disambiguation as it often deals with word senses that are not listed in the lexicon.", "labels": [], "entities": [{"text": "resolution of figurative language", "start_pos": 9, "end_pos": 42, "type": "TASK", "confidence": 0.8030573278665543}, {"text": "word sense disambiguation", "start_pos": 90, "end_pos": 115, "type": "TASK", "confidence": 0.638933410247167}]}, {"text": "For example, the meaning of stopover in the sentence He saw teaching as a stopover on his way to bigger things is a metaphorical sense of the sense \"stopping place in a physical journey\", with the literal sense listed in WordNet 2.0 but the metaphorical one not being listed.", "labels": [], "entities": []}, {"text": "The same holds for the metonymic reading of rattlesnake (for the animal's meat) in Roast rattlesnake tastes like chicken.", "labels": [], "entities": [{"text": "Roast rattlesnake", "start_pos": 83, "end_pos": 100, "type": "DATASET", "confidence": 0.6967995464801788}]}, {"text": "Again, the meat read-ing of rattlesnake is not listed in WordNet whereas the meat reading for chicken is.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 57, "end_pos": 64, "type": "DATASET", "confidence": 0.9820127487182617}]}, {"text": "As there is no common framework or corpus for figurative language resolution, previous computational works, among others) carryout only smallscale evaluations.", "labels": [], "entities": [{"text": "figurative language resolution", "start_pos": 46, "end_pos": 76, "type": "TASK", "confidence": 0.8046731551488241}]}, {"text": "In recent years, there has been growing interest in metaphor and metonymy resolution that is either corpus-based or evaluated on larger datasets.", "labels": [], "entities": [{"text": "metaphor and metonymy resolution", "start_pos": 52, "end_pos": 84, "type": "TASK", "confidence": 0.6911836192011833}]}, {"text": "Still, apart from) who evaluate their work on the same dataset, results are hardly comparable as they all operate within different frameworks.", "labels": [], "entities": []}, {"text": "This situation motivated us to organise the first shared task for figurative language, concentrating on metonymy.", "labels": [], "entities": []}, {"text": "In metonymy one expression is used to refer to the referent of a related one, like the use of an animal name for its meat.", "labels": [], "entities": []}, {"text": "1, Vietnam, the name of a location, refers to an event (a war) that happened there.", "labels": [], "entities": []}, {"text": "(1) Sex, drugs, and Vietnam have haunted Bill Clinton's campaign.", "labels": [], "entities": []}, {"text": "2 and 3, BMW, the name of a company, stands for its index on the stock market, or a vehicle manufactured by BMW, respectively.", "labels": [], "entities": [{"text": "BMW", "start_pos": 9, "end_pos": 12, "type": "DATASET", "confidence": 0.8916657567024231}]}, {"text": "(2) BMW slipped 4p to 31p (3) His BMW went onto race at Le Mans The importance of resolving metonymies has been shown fora variety of NLP tasks, such as ma-chine translation, question answering, anaphora resolution) and geographical information retrieval ().", "labels": [], "entities": [{"text": "Le Mans", "start_pos": 56, "end_pos": 63, "type": "DATASET", "confidence": 0.9100094139575958}, {"text": "ma-chine translation", "start_pos": 153, "end_pos": 173, "type": "TASK", "confidence": 0.7021915018558502}, {"text": "question answering", "start_pos": 175, "end_pos": 193, "type": "TASK", "confidence": 0.8933209776878357}, {"text": "anaphora resolution", "start_pos": 195, "end_pos": 214, "type": "TASK", "confidence": 0.6923447251319885}, {"text": "geographical information retrieval", "start_pos": 220, "end_pos": 254, "type": "TASK", "confidence": 0.6603538393974304}]}, {"text": "Although metonymic readings are, like all figurative readings, potentially open ended and can be innovative, the regularity of usage for word groups helps in establishing a common evaluation framework.", "labels": [], "entities": []}, {"text": "Many other location names, for instance, can be used in the same fashion as Vietnam in Ex.", "labels": [], "entities": []}, {"text": "1. Thus, given a semantic class (e.g. location), one can specify several regular metonymic patterns (e.g. place-for-event) that instances of the class are likely to undergo.", "labels": [], "entities": []}, {"text": "In addition to literal readings, regular metonymic patterns and innovative metonymic readings, there can also be so-called mixed readings, similar to zeugma, where both a literal and a metonymic reading are evoked.", "labels": [], "entities": []}, {"text": "The metonymy task is a lexical sample task for English, consisting of two subtasks, one concentrating on the semantic class location, exemplified by country names, and another one concentrating on organisation, exemplified by company names.", "labels": [], "entities": []}, {"text": "Participants had to automatically classify preselected country/company names as having a literal or non-literal meaning, given a four-sentence context.", "labels": [], "entities": []}, {"text": "Additionally, participants could attempt finer-grained interpretations, further specifying readings into prespecified metonymic patterns (such as place-for-event) and recognising innovative readings.", "labels": [], "entities": []}], "datasetContent": [{"text": "Teams were allowed to participate in the location or organisation task or both.", "labels": [], "entities": []}, {"text": "We encouraged supervised, semi-supervised or unsupervised approaches.", "labels": [], "entities": []}, {"text": "Systems could be tailored to recognise metonymies at three different levels of granu-larity: coarse, medium, or fine, with an increasing number and specification of target classification categories, and thus difficulty.", "labels": [], "entities": []}, {"text": "At the coarse level, only a distinction between literal and non-literal was asked for; medium asked fora distinction between literal, metonymic and mixed readings; fine needed a classification into literal readings, mixed readings, any of the class-dependent and class-independent metonymic patterns (Section 2) or an innovative metonymic reading (category othermet).", "labels": [], "entities": []}, {"text": "Systems were evaluated via accuracy (acc) and coverage (cov), allowing for partial submissions.", "labels": [], "entities": [{"text": "accuracy (acc)", "start_pos": 27, "end_pos": 41, "type": "METRIC", "confidence": 0.9289153963327408}, {"text": "coverage (cov)", "start_pos": 46, "end_pos": 60, "type": "METRIC", "confidence": 0.9498544484376907}]}, {"text": "A baseline, consisting of the assignment of the most frequent category (always literal), was used for each task and granularity level.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Reading distribution for locations", "labels": [], "entities": [{"text": "Reading distribution", "start_pos": 10, "end_pos": 30, "type": "TASK", "confidence": 0.5798332840204239}]}, {"text": " Table 2: Reading distribution for organisations", "labels": [], "entities": [{"text": "Reading distribution", "start_pos": 10, "end_pos": 30, "type": "TASK", "confidence": 0.70243039727211}]}, {"text": " Table 3: Accuracy scores for all systems for all the location tasks. 8", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9991424083709717}]}, {"text": " Table 4: Accuracy scores for all systems for all the organisation tasks", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9994351267814636}]}]}