{"title": [{"text": "UTD-HLT-CG: Semantic Architecture for Metonymy Resolution and Classification of Nominal Relations", "labels": [], "entities": [{"text": "UTD-HLT-CG", "start_pos": 0, "end_pos": 10, "type": "DATASET", "confidence": 0.9152478575706482}, {"text": "Metonymy Resolution and Classification of Nominal Relations", "start_pos": 38, "end_pos": 97, "type": "TASK", "confidence": 0.7678747432572501}]}], "abstractContent": [{"text": "In this paper we present a semantic architecture that was employed for processing two different SemEval 2007 tasks: Task 4 (Classification of Semantic Relations between Nominals) and Task 8 (Metonymy Resolution).", "labels": [], "entities": [{"text": "SemEval 2007 tasks", "start_pos": 96, "end_pos": 114, "type": "TASK", "confidence": 0.8550942738850912}, {"text": "Classification of Semantic Relations between Nominals)", "start_pos": 124, "end_pos": 178, "type": "TASK", "confidence": 0.7296546101570129}, {"text": "Metonymy Resolution)", "start_pos": 191, "end_pos": 211, "type": "TASK", "confidence": 0.8091804186503092}]}, {"text": "The architecture uses multiple forms of syntactic, lexical, and semantic information to inform a classification-based approach that generates a different model for each machine learning algorithm that implements the classification.", "labels": [], "entities": []}, {"text": "We used decision trees, decision rules, logistic regression and lazy classifiers.", "labels": [], "entities": []}, {"text": "A voting module selects the best performing module for each task evaluated in SemEval 2007.", "labels": [], "entities": [{"text": "SemEval 2007", "start_pos": 78, "end_pos": 90, "type": "DATASET", "confidence": 0.6823474168777466}]}, {"text": "The paper details the results obtained when using the semantic architecture .", "labels": [], "entities": []}], "introductionContent": [{"text": "Automatic semantic interpretations of natural language text rely on (1) semantic theories that capture the subtleties employed by human communications; (2) lexico-semantic resources that encode various forms of semantic knowledge; and (3) computational methods that model the selection of the optimal interpretation derived from the textual data.", "labels": [], "entities": [{"text": "semantic interpretations of natural language text", "start_pos": 10, "end_pos": 59, "type": "TASK", "confidence": 0.7972791691621145}]}, {"text": "Two of the SemEval 2007 tasks, namely Task 4 (Classification of Semantic Relations between Nominals)and Task 8 (Metonymy Resolution) employed distinct theories for the interpretation of their corresponding semantic phenomena, but, nevertheless, they also shared several lexico-semantic resources, and, furthermore, both these tasks could have been cast as classification problems, in vein with most of the recent work in computational semantic processing.", "labels": [], "entities": [{"text": "SemEval 2007 tasks", "start_pos": 11, "end_pos": 29, "type": "TASK", "confidence": 0.8483745058377584}, {"text": "Classification of Semantic Relations between Nominals)", "start_pos": 46, "end_pos": 100, "type": "TASK", "confidence": 0.7404874818665641}, {"text": "Metonymy Resolution)", "start_pos": 112, "end_pos": 132, "type": "TASK", "confidence": 0.8231416543324789}, {"text": "computational semantic processing", "start_pos": 421, "end_pos": 454, "type": "TASK", "confidence": 0.6572115818659464}]}, {"text": "Based on this observation, we have designed and implemented a semantic architecture that was used in both tasks.", "labels": [], "entities": []}, {"text": "In Section 2 of this paper we give a brief description of the semantic theories corresponding to each of the two tasks, while in Section 3 we detail the semantic architecture.", "labels": [], "entities": []}, {"text": "Section 4 describes the experimental results and evaluation.", "labels": [], "entities": []}, {"text": "We have used three lexico-semantic resources: (i) the WordNet lexico-semantic database; (ii) VerbNet; and (iii) the Lexical Conceptual Structure (LCS) database.", "labels": [], "entities": [{"text": "WordNet lexico-semantic database", "start_pos": 54, "end_pos": 86, "type": "DATASET", "confidence": 0.927016019821167}]}, {"text": "Used only by Task 4, WordNet is a lexicosemantic database created at Princeton University 1, which encodes avast majority of the English nouns, verbs, adjectives and adverbs, and groups synonym words into synsets.", "labels": [], "entities": [{"text": "Princeton University 1", "start_pos": 69, "end_pos": 91, "type": "DATASET", "confidence": 0.9232478141784668}]}, {"text": "VerbNet 2 is a broad-coverage, comprehensive verb lexicon created at University of Pennsylvania, compatible with WordNet, but with explicitly stated syntactic and semantic information, using Levin verb classes to systematically construct lexical entities.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 113, "end_pos": 120, "type": "DATASET", "confidence": 0.9396829605102539}]}, {"text": "Classes are hierarchically organized and each class in the hierarchy has its corresponding syntactic frames, semantic predicates and a list of typical verb arguments.", "labels": [], "entities": []}, {"text": "The Lexical Conceptual Structure) is a compositional abstraction with language-independent properties.", "labels": [], "entities": []}, {"text": "An LCS is a directed graph with a root.", "labels": [], "entities": []}, {"text": "Each node is associated with certain information, including a type, a primitive and afield.", "labels": [], "entities": []}, {"text": "An LCS captures the semantics 1 http://wordnet.princeton.edu", "labels": [], "entities": []}], "datasetContent": [{"text": "Both the metonymy resolution system and the system for classification of semantic relations performed well in the SemEval 2007 competition.", "labels": [], "entities": [{"text": "metonymy resolution", "start_pos": 9, "end_pos": 28, "type": "TASK", "confidence": 0.8839586973190308}, {"text": "classification of semantic relations", "start_pos": 55, "end_pos": 91, "type": "TASK", "confidence": 0.800853356719017}, {"text": "SemEval 2007 competition", "start_pos": 114, "end_pos": 138, "type": "TASK", "confidence": 0.7505660653114319}]}, {"text": "The: Performance for the metonymy resolution system for the coarse level.", "labels": [], "entities": [{"text": "metonymy resolution", "start_pos": 25, "end_pos": 44, "type": "TASK", "confidence": 0.9045974314212799}]}, {"text": "experiments presented in this paper were done on the training and testing datasets for each subtask.", "labels": [], "entities": []}, {"text": "To note is that no other training data was collected or used than the one provided by the organizers.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 4: The number of features before and af- ter Weka selection, for each semantic relation  dataset: R1 CAUSE-EFFECT, R2 INSTRUMENT- AGENCY, R3 PRODUCT-PRODUCER, R4 ORIGIN- ENTITY, R5 THEME-TOOL, R6 PART-WHOLE, and  R7 CONTENT-CONTAINER.", "labels": [], "entities": [{"text": "CAUSE-EFFECT", "start_pos": 108, "end_pos": 120, "type": "METRIC", "confidence": 0.7716684341430664}, {"text": "INSTRUMENT- AGENCY", "start_pos": 125, "end_pos": 143, "type": "METRIC", "confidence": 0.8581512173016866}, {"text": "ORIGIN- ENTITY", "start_pos": 169, "end_pos": 183, "type": "METRIC", "confidence": 0.9258666634559631}, {"text": "THEME-TOOL", "start_pos": 188, "end_pos": 198, "type": "METRIC", "confidence": 0.9552032947540283}, {"text": "PART-WHOLE", "start_pos": 203, "end_pos": 213, "type": "METRIC", "confidence": 0.9456101655960083}, {"text": "CONTENT-CONTAINER", "start_pos": 223, "end_pos": 240, "type": "METRIC", "confidence": 0.9301446080207825}]}, {"text": " Table 5: Accuracy for the metonymy resolution sys- tem at three granularity levels.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9981252551078796}]}, {"text": " Table 6: Performance for the metonymy resolution  system for the coarse level.", "labels": [], "entities": [{"text": "metonymy resolution", "start_pos": 30, "end_pos": 49, "type": "TASK", "confidence": 0.9089989364147186}]}, {"text": " Table 7: Performance for the metonymy resolution  system for the medium level.", "labels": [], "entities": [{"text": "metonymy resolution", "start_pos": 30, "end_pos": 49, "type": "TASK", "confidence": 0.9343452751636505}]}, {"text": " Table 8: Performance for the metonymy resolution  system for the fine level.", "labels": [], "entities": [{"text": "metonymy resolution", "start_pos": 30, "end_pos": 49, "type": "TASK", "confidence": 0.9136428833007812}]}, {"text": " Table 9: Performance of the semantic relations clas- sification system for each semantic relation.", "labels": [], "entities": []}, {"text": " Table 10: Results on 10-fold crossvalidation for  each relation and each classifier.", "labels": [], "entities": []}]}