{"title": [{"text": "LCC-SRN: LCC's SRN System for SemEval 2007 Task 4", "labels": [], "entities": [{"text": "SemEval 2007 Task 4", "start_pos": 30, "end_pos": 49, "type": "TASK", "confidence": 0.8189729452133179}]}], "abstractContent": [{"text": "This document provides a description of the Language Computer Corporation (LCC) SRN System that participated in the SemE-val 2007 Semantic Relation between Nominals task.", "labels": [], "entities": [{"text": "Language Computer Corporation (LCC) SRN System", "start_pos": 44, "end_pos": 90, "type": "DATASET", "confidence": 0.611773457378149}, {"text": "SemE-val 2007 Semantic Relation between Nominals task", "start_pos": 116, "end_pos": 169, "type": "TASK", "confidence": 0.9086284722600665}]}, {"text": "The system combines the outputs of different binary and multi-class classifiers build using machine learning al", "labels": [], "entities": []}], "introductionContent": [{"text": "The Semantic Relations between Nominals task from SemEval 2007 focuses on identifying the semantic relations that hold between two arguments manually annotated with word senses (.", "labels": [], "entities": []}, {"text": "The previous work in identifying semantic relations between nominals focuses on finding one or more relations in text for specific syntactic patterns or constructions (like genitives and noun compounds) using semi-automated and automated systems.", "labels": [], "entities": [{"text": "identifying semantic relations between nominals", "start_pos": 21, "end_pos": 68, "type": "TASK", "confidence": 0.8577597856521606}]}, {"text": "An overview of some of these methods can be found in).", "labels": [], "entities": []}, {"text": "The LCC SRN system, developed during the SRN training period, was for us, the beginning of a different approach to semantic relations detection: detecting semantic relations in text without using a syntactic pattern.", "labels": [], "entities": [{"text": "LCC SRN", "start_pos": 4, "end_pos": 11, "type": "DATASET", "confidence": 0.8799520134925842}, {"text": "SRN training", "start_pos": 41, "end_pos": 53, "type": "TASK", "confidence": 0.8355951309204102}, {"text": "semantic relations detection", "start_pos": 115, "end_pos": 143, "type": "TASK", "confidence": 0.6614793340365092}]}, {"text": "Our existing work on semantic relation detection was on detecting semantic relations in text (one or more at a time) at different levels in the sentence using different syntactic patterns like genitives, noun compounds, verbarguments, etc.", "labels": [], "entities": [{"text": "semantic relation detection", "start_pos": 21, "end_pos": 48, "type": "TASK", "confidence": 0.8126450181007385}, {"text": "detecting semantic relations in text (one or more at a time) at different levels in the sentence using different syntactic patterns like genitives, noun compounds, verbarguments, etc", "start_pos": 56, "end_pos": 238, "type": "Description", "confidence": 0.7708325050771236}]}, {"text": "For SRN, we built anew system that combines the output of the pattern dependent classifiers with the new pattern-independent classifiers for better results.", "labels": [], "entities": [{"text": "SRN", "start_pos": 4, "end_pos": 7, "type": "TASK", "confidence": 0.8425595760345459}]}, {"text": "The remainder of this paper is organized as follows: Section 2 describes our system, Section 3 details the experimental results, and Section 4 summarizes the conclusions.", "labels": [], "entities": []}], "datasetContent": [{"text": "During the competition we performed several experiments to assess the correct combination of classifiers that leads to the best results.", "labels": [], "entities": []}, {"text": "The organizer provided 140 examples for each of the 7 relations.", "labels": [], "entities": []}, {"text": "For testing the classifiers we trained the system on the first 110 examples and tested it on the last 30 of them.", "labels": [], "entities": []}, {"text": "We performed different sets of experiments.", "labels": [], "entities": []}, {"text": "\ud97b\udf59 Experiments with one type of classifiers.", "labels": [], "entities": []}, {"text": "These experiments showed that ME has a best performance (55.1) 10.05 more than DT and 8.05 more than SV.", "labels": [], "entities": [{"text": "ME", "start_pos": 30, "end_pos": 32, "type": "METRIC", "confidence": 0.9134469628334045}]}, {"text": "ME also got the highest score for Cause-Effect, while DT obtained the best score for Product-Producer.", "labels": [], "entities": [{"text": "ME", "start_pos": 0, "end_pos": 2, "type": "METRIC", "confidence": 0.7570156455039978}]}, {"text": "\ud97b\udf59 Experiments with multiple classifiers.", "labels": [], "entities": []}, {"text": "These experiments showed that DT+SV+SS+ISS has the best score (66.72) followed by DT+SS+ISS with 55.66.", "labels": [], "entities": [{"text": "DT+SS+ISS", "start_pos": 82, "end_pos": 91, "type": "DATASET", "confidence": 0.6414170503616333}]}, {"text": "Also by adding the SS and ISS classifiers the DT score increased with 10.51, the SV score with 5.81 and the DT+SV with 20.57.", "labels": [], "entities": [{"text": "SS", "start_pos": 19, "end_pos": 21, "type": "METRIC", "confidence": 0.895422101020813}, {"text": "DT score", "start_pos": 46, "end_pos": 54, "type": "METRIC", "confidence": 0.9469510018825531}, {"text": "SV score", "start_pos": 81, "end_pos": 89, "type": "METRIC", "confidence": 0.9889441728591919}, {"text": "DT+SV", "start_pos": 108, "end_pos": 113, "type": "METRIC", "confidence": 0.822819987932841}]}, {"text": "\ud97b\udf59 Experiments with types of methods.", "labels": [], "entities": []}, {"text": "These experiments showed that the SRN methods (with a score 0.44) are better than the SRNPAT methods (with a score of 0.41) with 0.03 which was expected since SRN were trained on provided examples.", "labels": [], "entities": [{"text": "SRNPAT", "start_pos": 86, "end_pos": 92, "type": "DATASET", "confidence": 0.6342912912368774}]}, {"text": "shows the results of our SRN system when using specific classifiers or a combination of classifiers.", "labels": [], "entities": []}, {"text": "The time did not permit us to do any experiments with the ME and NB classifiers.", "labels": [], "entities": [{"text": "ME", "start_pos": 58, "end_pos": 60, "type": "METRIC", "confidence": 0.831710159778595}]}, {"text": "We submitted the DT+SS+ISS version because of its closeness to the normal distribution rather than DT+SV+SS+ISS that had a better f-measure but it was closer to All-True.", "labels": [], "entities": [{"text": "DT+SS+ISS", "start_pos": 17, "end_pos": 26, "type": "DATASET", "confidence": 0.7801013708114624}, {"text": "DT+SV+SS+ISS", "start_pos": 99, "end_pos": 111, "type": "DATASET", "confidence": 0.7671111226081848}]}, {"text": "The evaluation results showed that the testing examples we used were representative and the DT+SV+SS+ISS produce better results.", "labels": [], "entities": [{"text": "DT+SV+SS+ISS", "start_pos": 92, "end_pos": 104, "type": "METRIC", "confidence": 0.5138495905058724}]}, {"text": "shows the results obtained by our system on the evaluation corpus for the B4 case (using WordNet but not the query and all the training examples..", "labels": [], "entities": [{"text": "WordNet", "start_pos": 89, "end_pos": 96, "type": "DATASET", "confidence": 0.9813219308853149}]}, {"text": "The results of our system on the evaluation corpus.", "labels": [], "entities": []}, {"text": "shows a comparison of our results with the following baseline systems: All-True, a system that always returns true, Majority, a system that always returns the majority value from the training, and Prob-Match, a system that randomly generate the value.", "labels": [], "entities": []}, {"text": "We have obtained a larger precision and accuracy than the All-True and the Prob-Match systems.", "labels": [], "entities": [{"text": "precision", "start_pos": 26, "end_pos": 35, "type": "METRIC", "confidence": 0.9995145797729492}, {"text": "accuracy", "start_pos": 40, "end_pos": 48, "type": "METRIC", "confidence": 0.9994701743125916}]}, {"text": "However, we obtained a lower recall and therefore an F-measure.", "labels": [], "entities": [{"text": "recall", "start_pos": 29, "end_pos": 35, "type": "METRIC", "confidence": 0.9996393918991089}, {"text": "F-measure", "start_pos": 53, "end_pos": 62, "type": "METRIC", "confidence": 0.9973589777946472}]}], "tableCaptions": [{"text": " Table 1. The accuracy of the SRNREL classifiers  built using different machine learning algorithms.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 14, "end_pos": 22, "type": "METRIC", "confidence": 0.9996346235275269}, {"text": "SRNREL classifiers", "start_pos": 30, "end_pos": 48, "type": "TASK", "confidence": 0.6354780793190002}]}, {"text": " Table 4. The accuracy of the SRNPAT classifiers for  the list of 40 LCC relations and the Part-Whole Rela- tion.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 14, "end_pos": 22, "type": "METRIC", "confidence": 0.9996662139892578}, {"text": "SRNPAT classifiers", "start_pos": 30, "end_pos": 48, "type": "TASK", "confidence": 0.5112310200929642}]}, {"text": " Table 5. The results of some of our experiments with  the different classifiers on the testing corpus.", "labels": [], "entities": []}, {"text": " Table 6. The results of our system on the evaluation  corpus.", "labels": [], "entities": []}, {"text": " Table 7. Comparison with the baselines.", "labels": [], "entities": []}]}