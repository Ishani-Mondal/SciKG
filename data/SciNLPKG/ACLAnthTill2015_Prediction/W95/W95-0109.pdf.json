{"title": [{"text": "Automatic Construction of a Chinese Electronic Dictionary +Jing", "labels": [], "entities": [{"text": "Automatic Construction of a Chinese Electronic Dictionary", "start_pos": 0, "end_pos": 57, "type": "TASK", "confidence": 0.6603120480264936}]}], "abstractContent": [{"text": "In this paper, an unsupervised approach for constructing a large-scale Chinese electronic dictionary is surveyed.", "labels": [], "entities": []}, {"text": "The main purpose is to enable cheap and quick acquisition of a large-scale dictionary from a large untagged text corpus with the aid of the information in a small tagged seed corpus.", "labels": [], "entities": []}, {"text": "The basic model is based on a Viterbi reestimation technique.", "labels": [], "entities": []}, {"text": "During the dictionary construction process, it tries to optimize the automatic segmentation and tagging process by repeatedly refining the set of parameters of the underlying language model.", "labels": [], "entities": [{"text": "dictionary construction", "start_pos": 11, "end_pos": 34, "type": "TASK", "confidence": 0.7404510378837585}, {"text": "segmentation and tagging", "start_pos": 79, "end_pos": 103, "type": "TASK", "confidence": 0.5713008642196655}]}, {"text": "The refined parameters are then used to furtherget a better tagging result.", "labels": [], "entities": [{"text": "tagging", "start_pos": 60, "end_pos": 67, "type": "TASK", "confidence": 0.9680795669555664}]}, {"text": "In addition, a two-class classifier, which is capable of classifying an n-gram either as a word or a non-word, is used in combination with the Viterbi training module to improve the system performance.", "labels": [], "entities": []}, {"text": "Two different system configurations had been developed to construct the dictionary.", "labels": [], "entities": []}, {"text": "The configurations include (1) a Viterbi word identification module followed by a Viterbi POS tagging module and (2) a two-class classification module as the postfilter for the above Viterbi word identification module.", "labels": [], "entities": [{"text": "word identification", "start_pos": 41, "end_pos": 60, "type": "TASK", "confidence": 0.6454580873250961}, {"text": "Viterbi POS tagging", "start_pos": 82, "end_pos": 101, "type": "TASK", "confidence": 0.676976203918457}, {"text": "word identification module", "start_pos": 191, "end_pos": 217, "type": "TASK", "confidence": 0.7248609264691671}]}, {"text": "With a seed of 1,000 sentences and an untagged corpus of 311,591 sentences, the performance for bigram word identification is 56.88% in precision and 77.37% in recall when the two-class classifier is applied to the word list suggested by the Viterbi word identification module.", "labels": [], "entities": [{"text": "bigram word identification", "start_pos": 96, "end_pos": 122, "type": "TASK", "confidence": 0.6442922949790955}, {"text": "precision", "start_pos": 136, "end_pos": 145, "type": "METRIC", "confidence": 0.9995049238204956}, {"text": "recall", "start_pos": 160, "end_pos": 166, "type": "METRIC", "confidence": 0.9990988969802856}]}, {"text": "The Viterbi part of speech tag reestimation stage gives the figures of 71.16% and 71.81% weighted precision rates and 73.42% and 73.83% weighted recall rates for the 2 different configurations when using a seed corpus of 9676 sentences.", "labels": [], "entities": [{"text": "precision", "start_pos": 98, "end_pos": 107, "type": "METRIC", "confidence": 0.9477121829986572}, {"text": "recall", "start_pos": 145, "end_pos": 151, "type": "METRIC", "confidence": 0.983020544052124}]}], "introductionContent": [], "datasetContent": [{"text": "In our experiments, the untagged Chinese text corpus contains 311,591sentences (about 1,670,000 words, 9 M byteS).", "labels": [], "entities": [{"text": "Chinese text corpus", "start_pos": 33, "end_pos": 52, "type": "DATASET", "confidence": 0.6544592181841532}]}, {"text": "Its major domain is news articles and reports from the China Times daily news.", "labels": [], "entities": [{"text": "China Times daily news", "start_pos": 55, "end_pos": 77, "type": "DATASET", "confidence": 0.9831995964050293}]}, {"text": "There are 246,036 distinct n-grams in this corpus, including 3,994 1-grams, 99,407 2-grams, 99,211 3-grams and 43,424 4-grams.", "labels": [], "entities": []}, {"text": "Since most Chinese words are not longer than 4 characters, only 1 -, 2-, 3-and 4-grams are in the word candidate list.", "labels": [], "entities": []}, {"text": "A seed corpus of 9,676 sentences (127,052 words, about 415 K bytes) of computer domain is available.", "labels": [], "entities": []}, {"text": "A smaller seed of 1,000 sentences is uniformly sampled from the above corpus.", "labels": [], "entities": []}, {"text": "This small seed corpus contains 12,849 words (about 42K bytes).", "labels": [], "entities": []}, {"text": "The numbers of n-grams for n=l, 2, 3, 4 are 893, 7782, 12289 and 12989, respectively.", "labels": [], "entities": []}, {"text": "Among these n-grams, only 1275 bigrams, 317 trigrams and 40 4-grams are registered as words in a dictionary.", "labels": [], "entities": []}, {"text": "Note that, since the numbers of word n-grams for n=3 and 4 are very small, the parameters (and performances) estimated based on such n-grams will introduce large estimation errors.", "labels": [], "entities": []}, {"text": "Hence, the estimated performance will be very unreliable.", "labels": [], "entities": []}, {"text": "For this reason, the conclusions will be drawn from the 2-gram performances; the performances for 3-gram and 4-gram will be listed for reference only.", "labels": [], "entities": []}, {"text": "To get an estimation of the system performance automatically, the extracted dictionary is compared against a manually constructed standard dictionary.", "labels": [], "entities": []}, {"text": "This is required because the extracted dictionary is large, and human verification will be both subjective and time-consuming.", "labels": [], "entities": []}, {"text": "The performance will be evaluated in terms of the word precision rate and recall rate for the VTW and the TCC modules.", "labels": [], "entities": [{"text": "precision rate", "start_pos": 55, "end_pos": 69, "type": "METRIC", "confidence": 0.8951061964035034}, {"text": "recall rate", "start_pos": 74, "end_pos": 85, "type": "METRIC", "confidence": 0.9866531491279602}, {"text": "VTW", "start_pos": 94, "end_pos": 97, "type": "DATASET", "confidence": 0.9456181526184082}]}, {"text": "The word precision rate is the number of n-grams common to the extracted word list and the standard dictionary divided by the number of n-grams in the extracted word list; on the contrary, the recall is the number of common n-grams divided by the number of n-grams in the standard dictionary.", "labels": [], "entities": [{"text": "precision rate", "start_pos": 9, "end_pos": 23, "type": "METRIC", "confidence": 0.9257933795452118}, {"text": "recall", "start_pos": 193, "end_pos": 199, "type": "METRIC", "confidence": 0.9995325803756714}]}, {"text": "The VTT module will be estimated in terms of several weighted tag precision and recall rate measures.", "labels": [], "entities": [{"text": "precision", "start_pos": 66, "end_pos": 75, "type": "METRIC", "confidence": 0.8226295113563538}, {"text": "recall rate", "start_pos": 80, "end_pos": 91, "type": "METRIC", "confidence": 0.9854374527931213}]}, {"text": "The standard Word Dictionary to be compared with the extracted word list is acquired by merging the word lists of two electronically available dictionaries and the words included in the seed corpus.", "labels": [], "entities": [{"text": "Word Dictionary", "start_pos": 13, "end_pos": 28, "type": "DATASET", "confidence": 0.9424116909503937}]}, {"text": "It also excludes all n-grams which never appear in the 9767-sentence seed corpus and the untagged text corpus, because such n-grams will never be the input to the dictionary construction system.", "labels": [], "entities": [{"text": "9767-sentence seed corpus", "start_pos": 55, "end_pos": 80, "type": "DATASET", "confidence": 0.8310152292251587}]}, {"text": "The merged dictionary, excluding entries that appear less frequently than the frequency lower bound (5), contains 17,005 bigram words, 2,524 trigram words and 1,612 4-gram words.", "labels": [], "entities": []}, {"text": "The standard Word-Tag Dictionary to be compared with the extracted POSes is constructed from the BDC English-Chinese electronic dictionary].", "labels": [], "entities": [{"text": "BDC English-Chinese electronic dictionary", "start_pos": 97, "end_pos": 138, "type": "DATASET", "confidence": 0.9310658574104309}]}, {"text": "The derived Word-Tag Dictionary contains 87,551 entries, including 35,722 bigram words, 19,858 trigram words, and 24,092 4-gram words.", "labels": [], "entities": [{"text": "Word-Tag Dictionary", "start_pos": 12, "end_pos": 31, "type": "DATASET", "confidence": 0.858542263507843}]}, {"text": "The tagset used in this dictionary Contains 62 tags (including two punctuation tags).", "labels": [], "entities": []}, {"text": "Note that there are only 42 tags in the smaller seed corpus of 1000 sentences, and the whole seed corpus of 9676 sentences contains only 47 POS tags (including one punctuation tag).", "labels": [], "entities": []}, {"text": "Therefore, such missing tags will introduce some tag extracting errors in the training processes.", "labels": [], "entities": [{"text": "tag extracting", "start_pos": 49, "end_pos": 63, "type": "TASK", "confidence": 0.6969848722219467}]}, {"text": "Since the Word Dictionary and Word-Tag Dictionary, which are used for comparison with the extracted dictionary, are constructed independently of the corpus from which the lexicon entries are extracted, the reported performances could be greatly underestimated.", "labels": [], "entities": [{"text": "Word Dictionary", "start_pos": 10, "end_pos": 25, "type": "DATASET", "confidence": 0.9396010637283325}]}, {"text": "For instance, an n-gram which is identified as a lexicon entry by the system but excluded from the Word Dictionary may not necessarily be a wrong word entry if it is judged by an expert lexicographer.", "labels": [], "entities": [{"text": "Word Dictionary", "start_pos": 99, "end_pos": 114, "type": "DATASET", "confidence": 0.9601274728775024}]}, {"text": "In the ideal case, the Word Dictionary and Word-Tag Dictionary should be constructed by an expert lexicographer based on the corpus fora fair comparison.", "labels": [], "entities": [{"text": "Word Dictionary", "start_pos": 23, "end_pos": 38, "type": "DATASET", "confidence": 0.9091446697711945}]}, {"text": "Unfortunately, we are unable to afford the manpower for such an evaluation on the large corpus.", "labels": [], "entities": []}, {"text": "Therefore, special attention should betaken when interpreting the performances reported in the following sections.", "labels": [], "entities": []}, {"text": "shows the performances in different stages for the Basic Model (columns 1-4) and the Postfiltering Model (columns 1-6) by using the small (1000-sentence) seed corpus. are shared because the Postfiltering is applied immediately after the Basic Model.)", "labels": [], "entities": []}, {"text": "The numerators in the parentheses are the numbers of correctly identified n-grams; for precision, the denominators are the numbers of n-grams in the extracted word lists; and for recall, they stand for the numbers of n-grams in the standard dictionary.", "labels": [], "entities": [{"text": "precision", "start_pos": 87, "end_pos": 96, "type": "METRIC", "confidence": 0.9988037347793579}, {"text": "recall", "start_pos": 179, "end_pos": 185, "type": "METRIC", "confidence": 0.9885061979293823}]}], "tableCaptions": [{"text": " Table 2. Performance for Part-of-Speech Extraction of the Two Models  (Seed=1000 and Seed = 9676, respectively)", "labels": [], "entities": []}]}