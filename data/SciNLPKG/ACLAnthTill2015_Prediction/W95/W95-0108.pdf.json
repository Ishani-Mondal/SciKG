{"title": [], "abstractContent": [{"text": "We describe, analyze, and experimentally evaluate anew probabilistic model for word-sequence prediction in natural languages, based on prediction suffi~v trees (PSTs).", "labels": [], "entities": [{"text": "word-sequence prediction", "start_pos": 79, "end_pos": 103, "type": "TASK", "confidence": 0.7516628503799438}]}, {"text": "By using efficient data structures, we extend the notion of PST to unbounded vocabularies.", "labels": [], "entities": [{"text": "PST", "start_pos": 60, "end_pos": 63, "type": "TASK", "confidence": 0.9661928415298462}]}, {"text": "We also show how to use a Bayesian approach based on recursive priors overall possible PSTs to efficiently maintain tree mixtures.", "labels": [], "entities": []}, {"text": "These mixtures have provably and practically better performance than almost any single model.", "labels": [], "entities": []}, {"text": "Finally, we evaluate the model on several corpora.", "labels": [], "entities": []}, {"text": "The low perplexity achieved by relatively small PST mixture models suggests that they maybe an advantageous alternative, both theoretically and practically, to the widely used n-gram models.", "labels": [], "entities": []}], "introductionContent": [{"text": "Finite-state methods for the statistical prediction of word sequences in natural language have had an important role in language processing research since Markov's and Shannon's pioneering investigations (C.E..", "labels": [], "entities": [{"text": "statistical prediction of word sequences in natural language", "start_pos": 29, "end_pos": 89, "type": "TASK", "confidence": 0.8602877631783485}, {"text": "language processing research", "start_pos": 120, "end_pos": 148, "type": "TASK", "confidence": 0.7735827962557474}]}, {"text": "While it has always been clear that natural texts are not Markov processes of any finite order, because of very long range correlations between words in a text such as those arising from subject matter, low-order alphabetic n-gram models have been used very effectively for such tasks as statistical language identification and spelling correction, and low-order word n-gram models have been the tool of choice for language modeling in speech recognition.", "labels": [], "entities": [{"text": "statistical language identification", "start_pos": 288, "end_pos": 323, "type": "TASK", "confidence": 0.7573645909627279}, {"text": "spelling correction", "start_pos": 328, "end_pos": 347, "type": "TASK", "confidence": 0.8759317994117737}, {"text": "speech recognition", "start_pos": 436, "end_pos": 454, "type": "TASK", "confidence": 0.7577726244926453}]}, {"text": "However, low-order n-gram models fail to capture even relatively local dependencies that exceed model order, for instance those created by long but frequent compound names or technical terms.", "labels": [], "entities": []}, {"text": "Unfortunately, extending model order to accommodate those longer dependencies is not practical, since the size of n-gram models is in principle exponential on the order of the model.", "labels": [], "entities": []}, {"text": "Recently, several methods have been proposed that are able to model longer-range regularities over small alphabets while avoiding the size explosion caused by model order.", "labels": [], "entities": []}, {"text": "In those models, the length of contexts used to predict particular symbols is adaptively extended as long as the extension improves prediction above a given threshold.", "labels": [], "entities": []}, {"text": "The key ingredient of the model construction is the prediction suffix tree (PST), whose nodes represent suffixes of past input and specify a predictive distribution over possible successors of the suffix.", "labels": [], "entities": []}, {"text": "It was shown in) that under realistic conditions a PST is equivalent to a Markov process of variable order and can be represented efficiently by a probabilistic finite-state automaton.", "labels": [], "entities": [{"text": "PST", "start_pos": 51, "end_pos": 54, "type": "TASK", "confidence": 0.9697780013084412}]}, {"text": "For the purposes of this paper, however, we will use PSTs as our starting point.", "labels": [], "entities": [{"text": "PSTs", "start_pos": 53, "end_pos": 57, "type": "TASK", "confidence": 0.878799319267273}]}, {"text": "The problem of sequence prediction appears more difficult when the sequence elements are words rather than characters from a small fixed alphabet.", "labels": [], "entities": [{"text": "sequence prediction", "start_pos": 15, "end_pos": 34, "type": "TASK", "confidence": 0.8885021507740021}]}, {"text": "The set of words is in principle unbounded, since in natural language there is always a nonzero probability of encountering a word never seen before.", "labels": [], "entities": []}, {"text": "One of the goals of this work is to describe algorithmic and data-structure changes that support the construction of PSTs over unbounded vocabularies.", "labels": [], "entities": [{"text": "PSTs", "start_pos": 117, "end_pos": 121, "type": "TASK", "confidence": 0.8784894347190857}]}, {"text": "We also extend PSTs with a wildcard symbol that can match against any input word, thus allowing the model to capture statistical dependencies between words separated by a fixed number of irrelevant words.", "labels": [], "entities": []}, {"text": "An even more fundamental new feature of the present derivation is the ability to work with a mixture of PSTs.", "labels": [], "entities": []}, {"text": "Here we adopted two important ideas from machine learning and information theory.", "labels": [], "entities": [{"text": "information theory", "start_pos": 62, "end_pos": 80, "type": "TASK", "confidence": 0.7787596881389618}]}, {"text": "The first is the fact that a mixture over an ensemble of experts (models), when the mixture weights are properly selected, performs better than almost any individual member of that ensemble.", "labels": [], "entities": []}, {"text": "The second idea is that within a Bayesian framework the sum over exponentially many trees can be computed efficiently using a recursive structure of the tree, as was recently shown by.", "labels": [], "entities": []}, {"text": "Here we apply these ideas and demonstrate that the mixture, which can be computed as almost as easily as a single PST, performs better than the most likely (maximum aposteriori --MAP) PST.", "labels": [], "entities": []}, {"text": "One of the most important features of the present algorithm that it can work in a fully online (adaptive) mode.", "labels": [], "entities": []}, {"text": "Specifically, updates to the model structure and statistical quantities can be performed adaptively in a single pass over the data.", "labels": [], "entities": []}, {"text": "For each new word, frequency counts, mixture weights and likelihood values associated with each relevant node are appropriately updated.", "labels": [], "entities": []}, {"text": "There is not much difference in learning performance between the online and batch modes, as we will see.", "labels": [], "entities": []}, {"text": "The online mode seems much more suitable for adaptive language modeling over longer test corpora, for instance in dictation or translation, while the batch algorithm can be used in the traditional manner of n-gram models in sentence recognition and analysis.", "labels": [], "entities": [{"text": "sentence recognition and analysis", "start_pos": 224, "end_pos": 257, "type": "TASK", "confidence": 0.7444576025009155}]}, {"text": "From an information-theoretic perspective, prediction is dual to compression and statistical modeling.", "labels": [], "entities": []}, {"text": "In the coding-theoretic interpretation of the Bayesian framework, the assignment of priors to novel events is rather delicate.", "labels": [], "entities": []}, {"text": "This question is especially important when dealing with a statistically open source such as natural language.", "labels": [], "entities": []}, {"text": "In this work we had to deal with two sets of priors.", "labels": [], "entities": []}, {"text": "The first set defines a prior probability distribution overall possible PSTs in a recursive manner, and is intuitively plausible in relation to the statistical self-similarity of the tree.", "labels": [], "entities": [{"text": "PSTs", "start_pos": 72, "end_pos": 76, "type": "TASK", "confidence": 0.9581773281097412}]}, {"text": "The second set of priors deals with novel events (words observed for the first time) by assuming a scalable probability of observing anew word at each node.", "labels": [], "entities": []}, {"text": "For the novel event priors we used a simple variant of the Good-Turing method, which could be easily implemented online with our data structure.", "labels": [], "entities": []}, {"text": "It turns out that the final performance is not terribly sensitive to particular assumptions on priors.", "labels": [], "entities": []}, {"text": "Our successful application of mixture PSTs for word-sequence prediction and modeling make them a valuable approach to language modeling in speech recognition, machine translation and similar applications.", "labels": [], "entities": [{"text": "word-sequence prediction and modeling", "start_pos": 47, "end_pos": 84, "type": "TASK", "confidence": 0.7084852680563927}, {"text": "language modeling", "start_pos": 118, "end_pos": 135, "type": "TASK", "confidence": 0.732137531042099}, {"text": "speech recognition", "start_pos": 139, "end_pos": 157, "type": "TASK", "confidence": 0.7406148910522461}, {"text": "machine translation", "start_pos": 159, "end_pos": 178, "type": "TASK", "confidence": 0.7928097248077393}]}, {"text": "Nevertheless, these models still fail to represent explicitly grammatical structure and semantic relationships, even though progress has been made in other work on their statistical modeling.", "labels": [], "entities": []}, {"text": "We plan to investigate how the present work maybe usefully combined with models of those phenomena, especially local finite-state syntactic models and distributional models of semantic relations.", "labels": [], "entities": []}, {"text": "In the next sections we present PSTs and the data structure for the word prediction problem.", "labels": [], "entities": [{"text": "PSTs", "start_pos": 32, "end_pos": 36, "type": "TASK", "confidence": 0.9629822969436646}, {"text": "word prediction problem", "start_pos": 68, "end_pos": 91, "type": "TASK", "confidence": 0.8778303662935892}]}, {"text": "We then describe and briefly analyze the learning algorithm.", "labels": [], "entities": []}, {"text": "We also discuss several implementation issues.", "labels": [], "entities": []}, {"text": "We conclude with a preliminary evaluation of various aspects of the model On several English corpora.", "labels": [], "entities": []}], "datasetContent": [{"text": "We tested our algorithm in two modes.", "labels": [], "entities": []}, {"text": "In online mode, model structure and parameters (counts) are updated after each observation.", "labels": [], "entities": []}, {"text": "In batch mode, the structure and parameters are held fixed after the training phase, making it easier to compare the model to standard n-gram models.", "labels": [], "entities": []}, {"text": "Our initial experiments used the Brown corpus, the Gutenberg Bible, and Milton's Paradise Lost as sources of training and test material.", "labels": [], "entities": [{"text": "Brown corpus", "start_pos": 33, "end_pos": 45, "type": "DATASET", "confidence": 0.8852755427360535}]}, {"text": "We have also carried out a preliminary evaluation on the ARPA North-American Business News (NAB) corpus.", "labels": [], "entities": [{"text": "ARPA North-American Business News (NAB) corpus", "start_pos": 57, "end_pos": 103, "type": "DATASET", "confidence": 0.9271077066659927}]}, {"text": "For batch training, we partitioned randomly the data into training and testing sets.", "labels": [], "entities": []}, {"text": "We then trained a model by running the online algorithm on the training set, and the resulting model, kept fixed, was then used to predict the test data.", "labels": [], "entities": []}, {"text": "As a simple check of the model, we used it to generate text by performing random walks over the PST.", "labels": [], "entities": []}, {"text": "A single step of the random walk was performed by going down the tree following the current context and stop at anode with the probability assigned by the algorithm to that node.", "labels": [], "entities": []}, {"text": "Once anode is chosen, a word is picked randomly by the node's prediction function.", "labels": [], "entities": []}, {"text": "A result of such a random walk is given in.", "labels": [], "entities": []}, {"text": "The PST was trained on the Brown corpus with maximal depth of five.", "labels": [], "entities": [{"text": "PST", "start_pos": 4, "end_pos": 7, "type": "TASK", "confidence": 0.9623770713806152}, {"text": "Brown corpus", "start_pos": 27, "end_pos": 39, "type": "DATASET", "confidence": 0.9391815066337585}]}, {"text": "The output contains several well formed (meaningless) clauses and also cliches such as \"conserving our rich natural heritage,\" suggesting that the model captured some longer-term statistical dependencies.", "labels": [], "entities": []}, {"text": "every year public sentiment for conserving our rich natural heritage is growing but that heritage is shrinking even faster no joyride much of its contract if the present session of the cabdriver in the early phases conspiracy but lacking money from commercial sponsors the stations have had to reduce its vacationing In online mode the advantage of PSTs with large maximal depth is clear.", "labels": [], "entities": []}, {"text": "The perplexity of the model decreases significantly as a function of the depth.", "labels": [], "entities": []}, {"text": "Our experiments so far suggest that the resulting models are fairly insensitive to the choice of the prior probability, a, and a prior which favors deep trees performed well.", "labels": [], "entities": []}, {"text": "summarizes the results on different texts, for trees of growing maximal depth.", "labels": [], "entities": []}, {"text": "Note that a maximal depth 0 corresponds to a 'bag of words' model (zero order), 1 to a bigram model, and 2 to a trigram model.", "labels": [], "entities": []}, {"text": "In our first batch tests we trained the model on 15% of the data and tested it on the rest.", "labels": [], "entities": []}, {"text": "The results are summarized in.", "labels": [], "entities": []}, {"text": "The perplexity obtained in the batch mode is clearly higher than that of the online mode, since a small portion of the data was used to train the models.", "labels": [], "entities": []}, {"text": "Yet, even in this case the PST of maximal depth three is significantly better than a full trigram model.", "labels": [], "entities": [{"text": "PST", "start_pos": 27, "end_pos": 30, "type": "TASK", "confidence": 0.6510343551635742}]}, {"text": "In this mode we also checked the performance of the single most likely (maximum aposteriori) model compared to the mixture of PSTs.", "labels": [], "entities": []}, {"text": "This model is found by pruning the tree at the nodes that obtained the highest confidence value, Ln(s), and using only the leaves for prediction.", "labels": [], "entities": []}, {"text": "As shown in the table, the performance of the MAP model is consistently worse than the performance of the mixture of PSTs.", "labels": [], "entities": []}, {"text": "As a simple test of for applicability of the model for language modeling, we checked it on text which was corrupted in different ways.", "labels": [], "entities": [{"text": "language modeling", "start_pos": 55, "end_pos": 72, "type": "TASK", "confidence": 0.7628054916858673}]}, {"text": "This situation frequently occurs in speech and handwriting recognition systems or in machine translation.", "labels": [], "entities": [{"text": "handwriting recognition", "start_pos": 47, "end_pos": 70, "type": "TASK", "confidence": 0.7301742881536484}, {"text": "machine translation", "start_pos": 85, "end_pos": 104, "type": "TASK", "confidence": 0.7628158330917358}]}, {"text": "In such systems the last stage is a language model, usually a trigram model, that selects the most likely alternative between the several options passed by the previous; stage.", "labels": [], "entities": []}, {"text": "Here we used a PST with maximal depth 4, trained on 90% of the text of Paradise Lost.", "labels": [], "entities": []}, {"text": "Several sentences that appeared in the test data were corrupted in different ways.", "labels": [], "entities": []}, {"text": "We then used the model in the batch mode to evaluate the likelihood of each of the alternatives.", "labels": [], "entities": []}, {"text": "In we demonstrate one such case, where the first alternative is the correct one.", "labels": [], "entities": []}, {"text": "The negative log likelihood and the posterior probability, assuming that the listed sentences are all the possible alternatives, are provided.", "labels": [], "entities": [{"text": "negative log likelihood", "start_pos": 4, "end_pos": 27, "type": "METRIC", "confidence": 0.6463795801003774}, {"text": "posterior probability", "start_pos": 36, "end_pos": 57, "type": "METRIC", "confidence": 0.9624130427837372}]}, {"text": "The correct sentence gets the highest probability according to the model.", "labels": [], "entities": []}, {"text": "Finally, we trained a depth two PST on randomly selected sentences from the NAB corpus totaling approximately 32.5 million words and tested it on two corpora: a separate randomly selected set of sentences from the NAB corpus, totaling around 2.8 million words, and a standard ARPA NAB development test set of around 8 thousand words.", "labels": [], "entities": [{"text": "NAB corpus", "start_pos": 76, "end_pos": 86, "type": "DATASET", "confidence": 0.954019159078598}, {"text": "NAB corpus", "start_pos": 214, "end_pos": 224, "type": "DATASET", "confidence": 0.9643620252609253}, {"text": "ARPA NAB development test set", "start_pos": 276, "end_pos": 305, "type": "DATASET", "confidence": 0.861231553554535}]}, {"text": "The PST perplexity on the first test set was 168, and on the second 223.", "labels": [], "entities": [{"text": "PST perplexity", "start_pos": 4, "end_pos": 18, "type": "METRIC", "confidence": 0.635100394487381}]}, {"text": "In comparison, a trigram backoff model built form the same training set has perplexity of 247.7 on the second test set.", "labels": [], "entities": []}, {"text": "Further experiments using longer maximal depth and allowing comparisons with existing n-gram models trained on the full (280 million word) NAB corpus will require improved data structures and pruning policies to stay within reasonable memory limits.", "labels": [], "entities": [{"text": "NAB corpus", "start_pos": 139, "end_pos": 149, "type": "DATASET", "confidence": 0.9545911848545074}]}], "tableCaptions": [{"text": " Table 1: The perplexity of PSTs for the online mode.", "labels": [], "entities": [{"text": "PSTs", "start_pos": 28, "end_pos": 32, "type": "TASK", "confidence": 0.8263322114944458}]}, {"text": " Table 2: The perplexity of PSTs for the batch mode.", "labels": [], "entities": [{"text": "PSTs", "start_pos": 28, "end_pos": 32, "type": "TASK", "confidence": 0.9323265552520752}]}]}