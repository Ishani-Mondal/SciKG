{"title": [{"text": "Compiling Bilingual Lexicon Entries From a Non-Parallel English-Chinese Corpus", "labels": [], "entities": []}], "abstractContent": [{"text": "We propose a novel context heterogeneity similarity measure between words and their translations in helping to compile bilingual lexicon entries from a non-parallel English-Chinese corpus.", "labels": [], "entities": []}, {"text": "Current algorithms for bilingual lexicon compilation rely on occurrence frequencies, length or positional statistics derived from parallel texts.", "labels": [], "entities": [{"text": "bilingual lexicon compilation", "start_pos": 23, "end_pos": 52, "type": "TASK", "confidence": 0.7171786824862162}]}, {"text": "There is little correlation between such statistics of a word and its translation in non-parallel corpora.", "labels": [], "entities": []}, {"text": "On the other hand, we suggest that words with productive context in one language translate to words with productive context in another language, and words with rigid context translate into words With rigid context.", "labels": [], "entities": []}, {"text": "Context heterogeneity measures how productive the context of a word is in a given domain, independent of its absolute occurrence frequency in the text.", "labels": [], "entities": []}, {"text": "Based on this information, we derive statistics of bilingual word pairs from a non-parallel corpus.", "labels": [], "entities": []}, {"text": "These statistics can be used to bootstrap a bilingual dictionary compilation algorithm.", "labels": [], "entities": [{"text": "bilingual dictionary compilation", "start_pos": 44, "end_pos": 76, "type": "TASK", "confidence": 0.6110674341519674}]}], "introductionContent": [{"text": "Building a domain-specific bilingual lexicon is one significant component in machine translation and machine-aided translation systems.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 77, "end_pos": 96, "type": "TASK", "confidence": 0.8276549577713013}, {"text": "machine-aided translation", "start_pos": 101, "end_pos": 126, "type": "TASK", "confidence": 0.7043424546718597}]}, {"text": "These terms are often not found in standard dictionaries.", "labels": [], "entities": []}, {"text": "Human translators, not being experts in every technical or regional domain, cannot produce their translations effectively.", "labels": [], "entities": []}, {"text": "Automatic:compilation of such a bilingual lexicon in specific domains is therefore highly desirable.", "labels": [], "entities": []}, {"text": "We present an algorithm in finding word correlation statistics for automatic bilingual lexicon compilation from a non-parallel corpus in Chinese and English.", "labels": [], "entities": [{"text": "word correlation", "start_pos": 35, "end_pos": 51, "type": "TASK", "confidence": 0.7075881063938141}, {"text": "automatic bilingual lexicon compilation", "start_pos": 67, "end_pos": 106, "type": "TASK", "confidence": 0.6057115122675896}]}, {"text": "Most previous automatic lexicon compilation techniques require a sentence-aligned clean parallel bilingual corpus.", "labels": [], "entities": [{"text": "automatic lexicon compilation", "start_pos": 14, "end_pos": 43, "type": "TASK", "confidence": 0.7440026005109152}]}, {"text": "We have previously shown an algorithm which extracts a bilingual lexicon from noisy parallel corpus without sentence alignment.", "labels": [], "entities": [{"text": "sentence alignment", "start_pos": 108, "end_pos": 126, "type": "TASK", "confidence": 0.7345285713672638}]}, {"text": "Although bilingual parallel corpora have been available in recent years, they are still relatively few in comparison to the large amount of monolingual text.", "labels": [], "entities": []}, {"text": "Acquiring and processing of parallel corpora are usually labour-intensive and time-consuming.", "labels": [], "entities": []}, {"text": "More importantly, the existence of a parallel corpus in a particular domain means some translator has translated it, therefore, the bilingual lexicon compiled from such a corpus is at best a reverse engineering of the lexicon this translator used.", "labels": [], "entities": []}, {"text": "On the other hand, if we can compile a dictionary of domain-specific words from non-parallel corpora of monolingual texts, the results would be much more meaningful and useful.", "labels": [], "entities": []}, {"text": "As demonstrated in all the bilingual lexicon compilation algorithms, the foremost task is to identify word features which are similar between a word and its translation, yet different between a word and other words which are not its translations.", "labels": [], "entities": [{"text": "bilingual lexicon compilation", "start_pos": 27, "end_pos": 56, "type": "TASK", "confidence": 0.6591660181681315}]}, {"text": "In parallel corpora, this feature could be the positional co-occurrence of a word and its translation in the other language in the same sentences or in the same segments.", "labels": [], "entities": []}, {"text": "Ina non-parallel corpus, there is no corresponding sentence or segment pairs, so the co-occurrence feature is not applicable.", "labels": [], "entities": []}, {"text": "In;, the word feature used was the positional difference vector.", "labels": [], "entities": []}, {"text": "Whereas this is more robust than sentence co-occurrence feature, the matching between two positional difference vectors presumes the two texts are rough translations of one anther.", "labels": [], "entities": []}, {"text": "Moreover, whereas the occurrence frequency of a word and that of its translation are relatively similar in a parallel corpus, they have little correlation in non-parallel texts.", "labels": [], "entities": []}, {"text": "Our task is, therefore, to identify a word feature correlating a pair of words even if they appear in texts which are not translations of each other.", "labels": [], "entities": []}, {"text": "This feature should also be language and character set independent, i.e. it should be applicable to pairs of languages very different from each other.", "labels": [], "entities": []}, {"text": "We propose that context heterogeneity is such a feature.", "labels": [], "entities": []}], "datasetContent": [{"text": "Given the simplicity of our current context heterogeneity measures and the complexity of finding translations from a non-parallel text in which many words will not find their translations, we propose to use context heterogeneity only as a bootstrapping feature in finding a candidate list of translations fora word.", "labels": [], "entities": []}, {"text": "In our first experiment, we hand-compiled a list of 58 word pairs as in in English and Chinese, and then used 58 by 58 context heterogeneity measures to match them against each other.", "labels": [], "entities": []}, {"text": "Note that this list consists of many single character words which have ambiguities in Chinese, English words which should have been part of a compound word, multiple translations of a single word in English, etc.", "labels": [], "entities": []}, {"text": "The initial results are revealing as shown by the histograms in.", "labels": [], "entities": []}, {"text": "In the left figure, we show that 12 words have their translations in the top 5 candidates.", "labels": [], "entities": []}, {"text": "In the right we show the result of filtering out the Chinese genitive ~ from the Chinese texts.", "labels": [], "entities": []}, {"text": "In this case, we can see that over 50% of the words found their translation in the top 10 candidates, although it gives fewer words with translations in top 5.", "labels": [], "entities": []}, {"text": "In Sections 7.1 to 7.4, we will discuss the effects of various factors on our results.", "labels": [], "entities": []}, {"text": "The above experiment showed to some extent the clustering ability of context heterogeneity.", "labels": [], "entities": []}, {"text": "To test the discriminative ability of this feature, we choose two clusters of known English and Chinese word pairs debate/~-~.", "labels": [], "entities": []}, {"text": "We obtained a cluster of Chinese words centered around ~-~ by applying the Kvec segment cooccurrence score on the Chinese text with itself.", "labels": [], "entities": []}, {"text": "The Kvec algorithm was previously used to find co-occurring bilingual word pairs with many candidates.", "labels": [], "entities": []}, {"text": "In our experiment, the co-occurrence happens within the same text, and therefore we got a candidate list for ~-~ that is a cluster of words similar i to it in terms of occurrence measure.", "labels": [], "entities": []}, {"text": "This cluster was proposed as a candidate translation list for debate.", "labels": [], "entities": []}, {"text": "We applied context heterogeneity measures between debate and the Chinese word list, with the result shown in with the best translation at the top.", "labels": [], "entities": []}, {"text": "The asteriskslin indicate tokenizer error.", "labels": [], "entities": []}, {"text": "The correct translation is the third candidate.", "labels": [], "entities": []}, {"text": "Although we cannot say at this point that this result is significant, it is to some extent encouraging.", "labels": [], "entities": []}, {"text": "It is interesting to note that if we applied the same Kvec algorithm to the English part of the text, we would get a cluster of English words which contain individual translations to some of the words in the Chinese cluster.", "labels": [], "entities": []}, {"text": "This shows that co-occurrence measure can give similar clusters of words in different languages from non-parallel texts.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Part of the concordance for air in Chinese", "labels": [], "entities": []}]}