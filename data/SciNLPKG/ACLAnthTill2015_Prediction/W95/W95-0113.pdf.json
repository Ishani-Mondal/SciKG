{"title": [{"text": "Development of a Partially Bracketed Corpus with Part-of-Speech Information Only", "labels": [], "entities": []}], "abstractContent": [{"text": "Resea/ch based on a treebank is active for many natural language applications.", "labels": [], "entities": []}, {"text": "However, the work to build a large scale treebank is laborious and tedious.", "labels": [], "entities": []}, {"text": "This paper proposes a probabilistic chunker to help the development of a partially bracketed corpus.", "labels": [], "entities": []}, {"text": "The chunker partitions the part-of-speech sequence into segments called chunks.", "labels": [], "entities": []}, {"text": "Rather than using a treebank as our training corpus, a corpus which is tagged with part-of-speech information only is used.", "labels": [], "entities": []}, {"text": "The experimental results show the probabilistic chunker has more than 92% correct rate in outside test.", "labels": [], "entities": [{"text": "correct rate", "start_pos": 74, "end_pos": 86, "type": "METRIC", "confidence": 0.9794481098651886}]}, {"text": "The well-formed partially bracketed corpus is a milestone in the development of a treebank.", "labels": [], "entities": []}, {"text": "Besides, the simple but effective chunker can also be applied to many natural language applications.", "labels": [], "entities": []}], "introductionContent": [{"text": "Research based on a treebank, i.e., a corpus annotated with syntactic structures, is active for many natural language applications.", "labels": [], "entities": []}, {"text": "Framis proposes a methodology to extract selectional restrictions at a variable level of abstraction from the Penn Treebank.", "labels": [], "entities": [{"text": "Penn Treebank", "start_pos": 110, "end_pos": 123, "type": "DATASET", "confidence": 0.9958845376968384}]}, {"text": "Chen and Chen propose a probabilistic chunker to decide the implicit boundaries of constituents and utilize the linguistic knowledge to extract the noun phrases by a finite state mechanism.", "labels": [], "entities": []}, {"text": "In their study, Susanne Corpus is used as a trainmg corpus for their chunker.", "labels": [], "entities": [{"text": "Susanne Corpus", "start_pos": 16, "end_pos": 30, "type": "DATASET", "confidence": 0.9431420862674713}]}, {"text": "Pocock and Atwell investigate statistical grammars extracted from Spoken English Corpus (SEC), and apply these grammars to find the grammatically optimal path through a word lattice.", "labels": [], "entities": [{"text": "Spoken English Corpus (SEC)", "start_pos": 66, "end_pos": 93, "type": "DATASET", "confidence": 0.7016751964886984}]}, {"text": "The stochastic parsers are also developed in.", "labels": [], "entities": []}, {"text": "All these applications employ the syntactic information extracted from different treebanks and show the satisfactory results.", "labels": [], "entities": []}, {"text": "However, the work to build a large scale treebank is laborious and tedious.", "labels": [], "entities": []}, {"text": "Very few large-scale treebanks are currently available especially for languages other than English.", "labels": [], "entities": []}, {"text": "In this paper, we propose a probabilistic chunker to help the development of a partially bracketed corpus, i.e., a simpler version of a treebank.", "labels": [], "entities": []}, {"text": "The chunker partitions the part-of-speech sequence into segments called chunks.", "labels": [], "entities": []}, {"text": "Rather than using a treebank as our training corpus, a corpus which is tagged with part-of-speech information only is used.", "labels": [], "entities": []}, {"text": "In the following sections we first introduce the experimental framework of our model.", "labels": [], "entities": []}, {"text": "LancasterOslo/Bergen (LOB) Corpus and Susanne Corpus are adopted.", "labels": [], "entities": [{"text": "LancasterOslo/Bergen (LOB) Corpus", "start_pos": 0, "end_pos": 33, "type": "DATASET", "confidence": 0.9350613015038627}, {"text": "Susanne Corpus", "start_pos": 38, "end_pos": 52, "type": "DATASET", "confidence": 0.9147036969661713}]}, {"text": "Then a tag mapper and a probabilistic chunker are described.", "labels": [], "entities": []}, {"text": "Before concluding the experimental results are demonstrated.", "labels": [], "entities": []}], "datasetContent": [{"text": "Because the probabilistic chunker proposed in this paper is based on syntactic tags (parts of speech), a part-of-speech tagger is needed.", "labels": [], "entities": []}, {"text": "A word sequence Wis input to the part-of-speech tagger and a part-of-speech sequence P is generated.", "labels": [], "entities": []}, {"text": "The output of the tagger is the input of the chunker.", "labels": [], "entities": []}, {"text": "The probabilistic chunker partitions P into C, i.e., a sequence of chunks.", "labels": [], "entities": []}, {"text": "Each chunk contains one or more parts of speech.", "labels": [], "entities": []}, {"text": "Consider the example \"Attorneys for the mayor said that an amicable property settlement has been agreed upon .\".", "labels": [], "entities": []}, {"text": "This 15-word sentence is input to the part-of-speech tagger and a part-of-speech sequence \"NNS IN ATI NPT VBD CS AT JJ\" is generated.", "labels": [], "entities": [{"text": "NNS IN ATI NPT VBD CS AT JJ", "start_pos": 91, "end_pos": 118, "type": "DATASET", "confidence": 0.5280797556042671}]}, {"text": "The probabilistic chunker then partitions this sequence into several chunks.", "labels": [], "entities": []}, {"text": "The chunked result is shown as follows.", "labels": [], "entities": []}, {"text": "[ However, the pei-formance evaluation of the chunker is a sticky work.", "labels": [], "entities": []}, {"text": "To evaluate the performance of the chunker, Susanne Corpus, which is a modified and condensed version of Brown Corpus, is adopted.", "labels": [], "entities": [{"text": "Susanne Corpus", "start_pos": 44, "end_pos": 58, "type": "DATASET", "confidence": 0.9301630854606628}, {"text": "Brown Corpus", "start_pos": 105, "end_pos": 117, "type": "DATASET", "confidence": 0.980976015329361}]}, {"text": "But, the tagging sets of LOB Corpus and Susanne Corpus are different.", "labels": [], "entities": [{"text": "LOB Corpus", "start_pos": 25, "end_pos": 35, "type": "DATASET", "confidence": 0.9585836231708527}, {"text": "Susanne Corpus", "start_pos": 40, "end_pos": 54, "type": "DATASET", "confidence": 0.9491673111915588}]}, {"text": "The latter has finer tags than the former.", "labels": [], "entities": []}, {"text": "Thus, a tag mapper is introduced in the experimental framework shown as.", "labels": [], "entities": []}, {"text": "In our experiments, the test sentence Ps comes from Susanne Corpus.", "labels": [], "entities": [{"text": "Ps", "start_pos": 38, "end_pos": 40, "type": "METRIC", "confidence": 0.7916128635406494}, {"text": "Susanne Corpus", "start_pos": 52, "end_pos": 66, "type": "DATASET", "confidence": 0.9677682518959045}]}, {"text": "It is a part-of-speech sequence.", "labels": [], "entities": []}, {"text": "The corresponding syntactic structure T is regarded as an evaluation criterion for the probabilistic chunker.", "labels": [], "entities": []}, {"text": "It is sent to the performance evaluation model.", "labels": [], "entities": []}, {"text": "The tag mapper in this figure is used to transform the Susanne part-of-speech into LOB part-of-speech.", "labels": [], "entities": [{"text": "Susanne", "start_pos": 55, "end_pos": 62, "type": "DATASET", "confidence": 0.8255099654197693}]}, {"text": "Through the tag mapper, Ps is converted into PI.", "labels": [], "entities": []}, {"text": "Then, PI is input to the probabilistic chunker and a chunk sequence C is produced.", "labels": [], "entities": []}, {"text": "Finally, the performance evaluation model reports the evaluation results according to C and T.", "labels": [], "entities": []}, {"text": "LOB Corpus, which is a million-word collection of present-day British English texts, is adopted as the source of training data.", "labels": [], "entities": [{"text": "LOB Corpus", "start_pos": 0, "end_pos": 10, "type": "DATASET", "confidence": 0.9806121587753296}]}, {"text": "Susanne Corpus is adopted as the source of testing data for evaluating the performance of our probabilistic chunker.", "labels": [], "entities": [{"text": "Susanne Corpus", "start_pos": 0, "end_pos": 14, "type": "DATASET", "confidence": 0.9808716475963593}]}, {"text": "This corpus contains one tenth of Brown Corpus, but involves more syntactic and semantic information than Brown Corpus.", "labels": [], "entities": [{"text": "Brown Corpus", "start_pos": 34, "end_pos": 46, "type": "DATASET", "confidence": 0.9808995723724365}, {"text": "Brown Corpus", "start_pos": 106, "end_pos": 118, "type": "DATASET", "confidence": 0.9773333668708801}]}, {"text": "For evaluating the performance, a criterion, i.e., the content of each chunk should be dominated by one non-terminal node in Susanne parse field, is adopted.", "labels": [], "entities": []}, {"text": "The performance evaluation model compares the chunked result C with the corresponding syntactic structure T.", "labels": [], "entities": []}, {"text": "Accordmg to this criterion, the experimental results for Definitions 3 and 4 are shown in as follows.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3. Experimental Results for Ta:  Mapping Types  Subtypes  Unique Tag  Correct  Wrong  Multiple Tags  Include  Exclude  No Match  Correct  Wrong", "labels": [], "entities": [{"text": "Ta", "start_pos": 35, "end_pos": 37, "type": "TASK", "confidence": 0.9671468138694763}, {"text": "Mapping Types  Subtypes  Unique Tag  Correct  Wrong  Multiple Tags  Include  Exclude  No Match  Correct  Wrong", "start_pos": 40, "end_pos": 150, "type": "METRIC", "confidence": 0.6610791663328807}]}, {"text": " Table 4. Experimental Results After Applying the First Heuristic Rule  Mapping Types  Unique Tag", "labels": [], "entities": []}, {"text": " Table 5. Experimental Results After Applying Two Heuristic  Mapping Types  Unique Tag", "labels": [], "entities": []}, {"text": " Table 7. Experimental Results for Definition 3 and 4", "labels": [], "entities": []}, {"text": " Table 10. Ex  File  i A01  G01  J01  N01  Average", "labels": [], "entities": [{"text": "File  i A01  G01  J01  N01", "start_pos": 15, "end_pos": 41, "type": "METRIC", "confidence": 0.457533597946167}]}]}