{"title": [{"text": "Automatically Acquiring Conceptual Patterns Without an Annotated Corpus", "labels": [], "entities": []}], "abstractContent": [{"text": "Previous work on automated dictionary construction for information extraction has relied on annotated text corpora.", "labels": [], "entities": [{"text": "dictionary construction", "start_pos": 27, "end_pos": 50, "type": "TASK", "confidence": 0.6813925355672836}, {"text": "information extraction", "start_pos": 55, "end_pos": 77, "type": "TASK", "confidence": 0.8537114262580872}]}, {"text": "However, annotating a corpus is time-consuming and difficult.", "labels": [], "entities": []}, {"text": "We propose that conceptual patterns for information extraction can be acquired automatically using only a preclassified training corpus and no text annotations.", "labels": [], "entities": [{"text": "information extraction", "start_pos": 40, "end_pos": 62, "type": "TASK", "confidence": 0.881580263376236}]}, {"text": "We describe a system called AutoSlog-TS, which is a variation of our previous AutoSlog system, that runs exhaustively on an untagged text corpus.", "labels": [], "entities": []}, {"text": "Text classification experiments in the MUC-4 terrorism domain show that the AutoSlog-TS dictionary performs comparably to a hand-crafted dictionary, and actually achieves higher precision on one test set.", "labels": [], "entities": [{"text": "Text classification", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.6879406124353409}, {"text": "MUC-4 terrorism domain", "start_pos": 39, "end_pos": 61, "type": "DATASET", "confidence": 0.9020440975824991}, {"text": "precision", "start_pos": 178, "end_pos": 187, "type": "METRIC", "confidence": 0.997983455657959}]}, {"text": "For text classification, AutoSlog-TS requires no manual effort beyond the preclassified training corpus.", "labels": [], "entities": [{"text": "text classification", "start_pos": 4, "end_pos": 23, "type": "TASK", "confidence": 0.8693579137325287}]}, {"text": "Additional experiments suggest how a dictionary produced by AutoSlog-TS can be filtered automatically for information extraction tasks.", "labels": [], "entities": [{"text": "information extraction", "start_pos": 106, "end_pos": 128, "type": "TASK", "confidence": 0.8088834881782532}]}, {"text": "Some manual intervention is still required in this case, but AutoSlog-TS significantly reduces the amount of effort required to create an appropriate training corpus.", "labels": [], "entities": []}], "introductionContent": [{"text": "In the last few years, significant progress has been made toward automatically acquiring conceptual patterns for information extraction (e.g.,).", "labels": [], "entities": [{"text": "information extraction", "start_pos": 113, "end_pos": 135, "type": "TASK", "confidence": 0.7823310494422913}]}, {"text": "However, previous approaches require an annotated training corpus or some other type of manually encoded training data.", "labels": [], "entities": []}, {"text": "Annotated training corpora are expensive to build, both in terms of the time and the expertise required to create them.", "labels": [], "entities": []}, {"text": "Furthermore, training corpora for information extraction are typically annotated with domain-specific tags, in contrast to general-purpose annotations such as part-of-speech tags or noun-phrase bracketing (e.g., the Brown Corpus and the Penn Treebank).", "labels": [], "entities": [{"text": "information extraction", "start_pos": 34, "end_pos": 56, "type": "TASK", "confidence": 0.8069185614585876}, {"text": "noun-phrase bracketing", "start_pos": 182, "end_pos": 204, "type": "TASK", "confidence": 0.7229181081056595}, {"text": "Brown Corpus", "start_pos": 216, "end_pos": 228, "type": "DATASET", "confidence": 0.9522511661052704}, {"text": "Penn Treebank", "start_pos": 237, "end_pos": 250, "type": "DATASET", "confidence": 0.8127725422382355}]}, {"text": "Consequently, anew training corpus must be annotated for each domain.", "labels": [], "entities": []}, {"text": "We have begun to explore the possibility of using an untagged corpus to automatically acquire conceptual patterns for information extraction.", "labels": [], "entities": [{"text": "information extraction", "start_pos": 118, "end_pos": 140, "type": "TASK", "confidence": 0.8049332499504089}]}, {"text": "Our approach uses a combination of domainindependent linguistic rules and statistics.", "labels": [], "entities": []}, {"text": "The linguistic rules are based on our previous system, AutoSlog, which automatically constructs dictionaries for information extraction using an annotated training corpus.", "labels": [], "entities": [{"text": "information extraction", "start_pos": 113, "end_pos": 135, "type": "TASK", "confidence": 0.7331940978765488}]}, {"text": "We have put anew spin on the original system by applying it exhaustively to an untagged but preclassified training corpus (i.e., a corpus in which the texts have been manually classified as either relevant or irrelevant).", "labels": [], "entities": []}, {"text": "Statistics are then used to sift through the myriad of patterns that it produces.", "labels": [], "entities": []}, {"text": "The new system, AutoSlog-TS, can generate a conceptual dictionary of extraction patterns fora domain from a preclassified text corpus.", "labels": [], "entities": []}, {"text": "First, we give a brief overview of information extraction and the CIRCUS sentence analyzer that we used in these experiments.", "labels": [], "entities": [{"text": "information extraction", "start_pos": 35, "end_pos": 57, "type": "TASK", "confidence": 0.8318786323070526}, {"text": "CIRCUS sentence analyzer", "start_pos": 66, "end_pos": 90, "type": "TASK", "confidence": 0.776698370774587}]}, {"text": "Second, we describe the original AutoSlog system for automated dictionary construction and explain how AutoSlog was adapted to generate patterns from an untagged corpus.", "labels": [], "entities": [{"text": "dictionary construction", "start_pos": 63, "end_pos": 86, "type": "TASK", "confidence": 0.6926161199808121}]}, {"text": "Next, we present empirical results from experiments with AutoSlog-TS using the MUC-4 text corpus.", "labels": [], "entities": [{"text": "MUC-4 text corpus", "start_pos": 79, "end_pos": 96, "type": "DATASET", "confidence": 0.9561768968900045}]}], "datasetContent": [{"text": "We conducted a series of experiments with AutoSlog-TS to evaluate how well it performs on a text classification task, and to assess the viability of using it for information extraction tasks.", "labels": [], "entities": [{"text": "text classification task", "start_pos": 92, "end_pos": 116, "type": "TASK", "confidence": 0.8496988415718079}, {"text": "information extraction tasks", "start_pos": 162, "end_pos": 190, "type": "TASK", "confidence": 0.8392789959907532}]}, {"text": "First, we describe text classification results for the MUC-4 terrorism domain.", "labels": [], "entities": [{"text": "text classification", "start_pos": 19, "end_pos": 38, "type": "TASK", "confidence": 0.7551802694797516}, {"text": "MUC-4 terrorism domain", "start_pos": 55, "end_pos": 77, "type": "DATASET", "confidence": 0.8676364620526632}]}, {"text": "Second, we present data that suggests how the dictionary can be filtered automatically for information extraction.", "labels": [], "entities": [{"text": "information extraction", "start_pos": 91, "end_pos": 113, "type": "TASK", "confidence": 0.8284011483192444}]}, {"text": "In the first experiment, we applied AutoSlog-TS to 1500 texts 1\u00b0 from the MUC-4 corpus, which has been preclassified for the domain of Latin American terrorism.", "labels": [], "entities": [{"text": "MUC-4 corpus", "start_pos": 74, "end_pos": 86, "type": "DATASET", "confidence": 0.9783919453620911}]}, {"text": "Roughly 50% of the texts are classified as relevant.", "labels": [], "entities": []}, {"text": "AutoSlog-TS produced a dictionary of 32,345 unique concept nodes.", "labels": [], "entities": [{"text": "AutoSlog-TS", "start_pos": 0, "end_pos": 11, "type": "DATASET", "confidence": 0.8773455619812012}]}, {"text": "To reduce the set of patterns down to a manageable size, we eliminated all concept nodes that were proposed exactly once, under the assumption that a pattern encountered only once is unlikely to be of much value.", "labels": [], "entities": []}, {"text": "AutoSlog-TS often proposes the same pattern multiple times and keeps track of how often each pattern is proposed.", "labels": [], "entities": []}, {"text": "After frequency filtering, the AutoSlog-TS dictionary contained 11,225 unique concept nodes.", "labels": [], "entities": [{"text": "AutoSlog-TS dictionary", "start_pos": 31, "end_pos": 53, "type": "DATASET", "confidence": 0.8112646341323853}]}, {"text": "We then ran CIRCUS over the same set of texts using the new concept node dictionary.", "labels": [], "entities": []}, {"text": "For each text, we kept track of the concept nodes that were activated.", "labels": [], "entities": []}, {"text": "We expect each concept node to be activated at least once, because these texts were used to create the concept node definitions, n This data was handed off to the relevancy signatures algorithm which generates signatures for each text (by pairing each concept node with the word that triggered it), and calculates statistics for each signature to identify how often it appeared in relevant texts versus irrelevant texts.", "labels": [], "entities": []}, {"text": "The relevancy signatures algorithm uses a relevancy threshold R to identify the most relevant signatures and a frequency threshold M to eliminate signatures that were seen only a few times during training.", "labels": [], "entities": []}, {"text": "Signatures that pass both thresholds are labeled as relevancy signatures and are used to classify new texts.", "labels": [], "entities": []}, {"text": "Finally, we evaluated the system by classifying two blind sets of 100 texts each, the TST3 and TST4 test sets from the MUC-4 corpus.", "labels": [], "entities": [{"text": "TST3 and TST4 test sets", "start_pos": 86, "end_pos": 109, "type": "DATASET", "confidence": 0.715467882156372}, {"text": "MUC-4 corpus", "start_pos": 119, "end_pos": 131, "type": "DATASET", "confidence": 0.9520886540412903}]}, {"text": "Each new text was processed by CIRCUS and classified as relevant if it generated a relevancy signature.", "labels": [], "entities": [{"text": "CIRCUS", "start_pos": 31, "end_pos": 37, "type": "DATASET", "confidence": 0.9357534646987915}]}, {"text": "We compared these results with results produced 9The hand-crafted dictionary contains concept nodes that are triggered by multiple words but all of the concept nodes generated by AutoSlog are triggered by exactly one word.", "labels": [], "entities": []}, {"text": "1\u00b0The DEV, TST1, and TST2 texts.", "labels": [], "entities": [{"text": "DEV", "start_pos": 6, "end_pos": 9, "type": "DATASET", "confidence": 0.9192651510238647}, {"text": "TST1", "start_pos": 11, "end_pos": 15, "type": "DATASET", "confidence": 0.5683621168136597}, {"text": "TST2 texts", "start_pos": 21, "end_pos": 31, "type": "DATASET", "confidence": 0.8540975451469421}]}, {"text": "nA concept node maybe activated by CIRCUS more often than it is proposed by AutoSlog-TS.", "labels": [], "entities": [{"text": "AutoSlog-TS", "start_pos": 76, "end_pos": 87, "type": "DATASET", "confidence": 0.9406275749206543}]}, {"text": "For example, consider the phrase I \"the murder in Bogota by terrorists.\"", "labels": [], "entities": []}, {"text": "To extract \"terrorists\", AutoSlog-TS uses a pp-attachment algorithm which should attach the PP to the noun \"murder.\"", "labels": [], "entities": []}, {"text": "However, it often makes mistakes and might attach the PP to the noun \"Bogota.\"", "labels": [], "entities": [{"text": "PP", "start_pos": 54, "end_pos": 56, "type": "METRIC", "confidence": 0.9049692749977112}, {"text": "Bogota", "start_pos": 70, "end_pos": 76, "type": "DATASET", "confidence": 0.8849266171455383}]}, {"text": "In this case, AutoSlog-TS would not propose the pattern \"murder by X\" even though it appears in the text. by the hand-crafted MUC-4 dictionary.", "labels": [], "entities": [{"text": "MUC-4 dictionary", "start_pos": 126, "end_pos": 142, "type": "DATASET", "confidence": 0.9488199353218079}]}, {"text": "We ran each system 120 times using a variety of threshold settings: R was varied from 70 to 95 in increments of five, and M was varied from 1 to 20 in increments of one.", "labels": [], "entities": [{"text": "R", "start_pos": 68, "end_pos": 69, "type": "METRIC", "confidence": 0.977984607219696}]}, {"text": "Both text classification systems were trained on the same set of 1500 texts and were identical except that they used different concept node dictionaries.", "labels": [], "entities": [{"text": "text classification", "start_pos": 5, "end_pos": 24, "type": "TASK", "confidence": 0.7280964702367783}]}, {"text": "The AutoSlog-TS dictionary performed comparably to the hand-crafted dictionary on both test sets.", "labels": [], "entities": []}, {"text": "On TST4, the AutoSlog-TS dictionary actually achieved higher precision than the handcrafted dictionary for recall levels < 60%, and produced several data points that achieved 100% precision (the hand-crafted dictionary did not produce any).", "labels": [], "entities": [{"text": "TST4", "start_pos": 3, "end_pos": 7, "type": "DATASET", "confidence": 0.7668434381484985}, {"text": "precision", "start_pos": 61, "end_pos": 70, "type": "METRIC", "confidence": 0.9980798959732056}, {"text": "recall", "start_pos": 107, "end_pos": 113, "type": "METRIC", "confidence": 0.9986357092857361}, {"text": "precision", "start_pos": 180, "end_pos": 189, "type": "METRIC", "confidence": 0.9983968138694763}]}, {"text": "However, we see a trade-off at higher recall levels.", "labels": [], "entities": [{"text": "recall", "start_pos": 38, "end_pos": 44, "type": "METRIC", "confidence": 0.999276340007782}]}, {"text": "The AutoSlog-TS dictionary achieved higher recall (up to 100%), which makes sense considering that the AutoSlog-TS dictionary is much bigger than the hand-crafted dictionary.", "labels": [], "entities": [{"text": "AutoSlog-TS dictionary", "start_pos": 4, "end_pos": 26, "type": "DATASET", "confidence": 0.8764915764331818}, {"text": "recall", "start_pos": 43, "end_pos": 49, "type": "METRIC", "confidence": 0.9996896982192993}]}, {"text": "But the hand-crafted dictionary achieved higher precision at recall levels above 60-65%.", "labels": [], "entities": [{"text": "precision", "start_pos": 48, "end_pos": 57, "type": "METRIC", "confidence": 0.9994838237762451}, {"text": "recall", "start_pos": 61, "end_pos": 67, "type": "METRIC", "confidence": 0.9988804459571838}]}, {"text": "This is probably because the hand-crafted dictionary was filtered manually, which ensures that all of its concept nodes are relevant to the domain (although not all are useful as classifiers).", "labels": [], "entities": []}, {"text": "In contrast, the AutoSlog-TS dictionary was not filtered manually so the statistics are solely responsible for separating the relevant concept nodes from the irrelevant ones.", "labels": [], "entities": []}, {"text": "To achieve high recall, the threshold values must below which allows some irrelevant patterns to pass threshold and cause erroneous classifications.", "labels": [], "entities": [{"text": "recall", "start_pos": 16, "end_pos": 22, "type": "METRIC", "confidence": 0.9991150498390198}]}, {"text": "Overall, the text classification results from AutoSlog-TS are very encouraging.", "labels": [], "entities": [{"text": "text classification", "start_pos": 13, "end_pos": 32, "type": "TASK", "confidence": 0.7888311445713043}, {"text": "AutoSlog-TS", "start_pos": 46, "end_pos": 57, "type": "DATASET", "confidence": 0.9143301844596863}]}, {"text": "The AutoSlog-TS dictionary produced results comparable to a hand-crafted dictionary on both test sets and even surpassed the precision scores of the hand-crafted dictionary on TST4.", "labels": [], "entities": [{"text": "AutoSlog-TS dictionary", "start_pos": 4, "end_pos": 26, "type": "DATASET", "confidence": 0.8032277524471283}, {"text": "precision", "start_pos": 125, "end_pos": 134, "type": "METRIC", "confidence": 0.9992875456809998}, {"text": "TST4", "start_pos": 176, "end_pos": 180, "type": "DATASET", "confidence": 0.9114511609077454}]}, {"text": "Furthermore, the entire text classification system is constructed automatically using only a preclassified training corpus, and no text annotations or manual filtering of any kind.", "labels": [], "entities": [{"text": "text classification", "start_pos": 24, "end_pos": 43, "type": "TASK", "confidence": 0.7241760641336441}]}, {"text": "We were also interested in gathering data to suggest how the AutoSlog-TS dictionary could be filtered automatically to produce an effective dictionary for information extraction.", "labels": [], "entities": [{"text": "information extraction", "start_pos": 155, "end_pos": 177, "type": "TASK", "confidence": 0.8549683392047882}]}, {"text": "As we indicated in Section 3.2, a dictionary for text classification requires patterns that can discriminate between relevant and irrelevant texts.", "labels": [], "entities": [{"text": "text classification", "start_pos": 49, "end_pos": 68, "type": "TASK", "confidence": 0.7612232565879822}]}, {"text": "In contrast, a dictionary for information extraction requires patterns that will extract relevant information, but they may also extract irrelevant information.", "labels": [], "entities": [{"text": "information extraction", "start_pos": 30, "end_pos": 52, "type": "TASK", "confidence": 0.7512570917606354}]}, {"text": "For example, in the terrorism domain, it is essential to have a pattern for the expression \"X was killed\" because people are frequently killed in terrorist attacks.", "labels": [], "entities": []}, {"text": "However, this pattern is also likely to appear in texts that describe other types of incidents, such as accidents and military actions.", "labels": [], "entities": []}, {"text": "First, we collected data to compare the AutoSlog-TS dictionary with a dictionary produced by the original Version of AutoSlog.", "labels": [], "entities": [{"text": "AutoSlog-TS dictionary", "start_pos": 40, "end_pos": 62, "type": "DATASET", "confidence": 0.8946353197097778}]}, {"text": "The AutoSlog dictionary was generated using an annotated corpus and was subsequently filtered by a person, so it relied on two levels of human effort.", "labels": [], "entities": []}, {"text": "The AutoSlog dictionary contains 428 unique concept node patterns 12, which were all deemed to be relevant by a person.", "labels": [], "entities": [{"text": "AutoSlog dictionary", "start_pos": 4, "end_pos": 23, "type": "DATASET", "confidence": 0.8550516963005066}]}, {"text": "The AutoSlog-TS dictionary contains 32,345 unique patterns of which 398 intersect with the AutoSlog dictionary33 We experimented with automatic filtering techniques based on two criteria: frequency and relevancy.", "labels": [], "entities": [{"text": "AutoSlog-TS dictionary", "start_pos": 4, "end_pos": 26, "type": "DATASET", "confidence": 0.8496334552764893}, {"text": "AutoSlog dictionary33", "start_pos": 91, "end_pos": 112, "type": "DATASET", "confidence": 0.9321878552436829}]}, {"text": "For frequency filtering, we simply removed all concept nodes that were proposed by AutoSlog-TS less than N times.", "labels": [], "entities": [{"text": "frequency filtering", "start_pos": 4, "end_pos": 23, "type": "TASK", "confidence": 0.7615825831890106}, {"text": "AutoSlog-TS", "start_pos": 83, "end_pos": 94, "type": "DATASET", "confidence": 0.9252747893333435}]}, {"text": "For example, N=2 eliminated all concept nodes that were proposed exactly once and reduced the size of the dictionary from 32,345 to 11,225.", "labels": [], "entities": []}, {"text": "shows the intersections between the AutoSlog-TS dictionary and the AutoSlog dictionary after frequency 12The dictionary actually contains 450 concept nodes but some concept nodes represent the same pattern to extract different types of objects.", "labels": [], "entities": [{"text": "AutoSlog dictionary", "start_pos": 67, "end_pos": 86, "type": "DATASET", "confidence": 0.8993225991725922}]}, {"text": "For example, the pattern \"X was attacked\" is used to extract both victims and physical targets.", "labels": [], "entities": []}, {"text": "lain theory, AutoSlog-TS should have generated all of the patterns that were generated by AutoSlog.", "labels": [], "entities": []}, {"text": "However, AutoSlog-TS uses a slightly different version of CIRCUS and anew pp-attachment algorithm (see). filtering.", "labels": [], "entities": []}, {"text": "It is interesting to note that approximately half of the concept nodes in the AutoSlog dictionary were proposed fewer than 5 times by AutoSlog-TS.", "labels": [], "entities": [{"text": "AutoSlog dictionary", "start_pos": 78, "end_pos": 97, "type": "DATASET", "confidence": 0.9228054285049438}, {"text": "AutoSlog-TS", "start_pos": 134, "end_pos": 145, "type": "DATASET", "confidence": 0.9539715051651001}]}, {"text": "This implies that roughly half of the concept nodes in the AutoSlog dictionary occurred infrequently and probably had little impact on the overall performance of the information extraction system.", "labels": [], "entities": [{"text": "information extraction", "start_pos": 166, "end_pos": 188, "type": "TASK", "confidence": 0.8047477602958679}]}, {"text": "14 One of the problems with manual filtering is that it is difficult fora person to know whether a pattern will occur frequently or infrequently in future texts.", "labels": [], "entities": []}, {"text": "As a result, people tend to retain many patterns that are not likely to be encountered very often.", "labels": [], "entities": []}, {"text": "For relevancy filtering, we retained only the concept nodes that had > N% correlation with relevant texts.", "labels": [], "entities": [{"text": "relevancy filtering", "start_pos": 4, "end_pos": 23, "type": "TASK", "confidence": 0.9148436188697815}]}, {"text": "For example, N--80 means that we retained a concept node if > 80% of its occurrences were in relevant texts.", "labels": [], "entities": []}, {"text": "shows the intersections between the dictionaries after relevancy filtering.", "labels": [], "entities": []}, {"text": "Not surprisingly, most of the concept nodes in the AutoSlog dictionary had at least a 50% relevancy rate.", "labels": [], "entities": [{"text": "AutoSlog dictionary", "start_pos": 51, "end_pos": 70, "type": "DATASET", "confidence": 0.8930106461048126}]}, {"text": "However, the number of concept nodes drops off rapidly at higher relevancy rates.", "labels": [], "entities": []}, {"text": "Again, this is not surprising because many useful extraction patterns will be common in both relevant and irrelevant texts.", "labels": [], "entities": []}, {"text": "Finally, we filtered the AutoSlog-TS dictionary using both relevancy and frequency filtering (N=5) to get a rough idea of how many concept node definitions will be useful for information extraction.", "labels": [], "entities": [{"text": "information extraction", "start_pos": 175, "end_pos": 197, "type": "TASK", "confidence": 0.8609389960765839}]}, {"text": "shows the size of the resulting dictionaries after filtering.", "labels": [], "entities": []}, {"text": "The number of concept nodes drops off dramatically from 32,345 to 4,169 after frequency filtering alone.", "labels": [], "entities": []}, {"text": "There is a roughly linear relationship between the relevancy rate and the number of concept nodes retained.", "labels": [], "entities": []}, {"text": "It seems relatively safe to assume that concept nodes with a relevancy rate below 50% are not highly associated with the domain, and that concept nodes with a total frequency < 5 are probably not going to be encountered often.", "labels": [], "entities": []}, {"text": "Using these two threshold values, we can reduce the size of the dictionary down to 1870 definitions.", "labels": [], "entities": []}, {"text": "This dictionary is much more manageable in size 14This is consistent with earlier results which showed that a relatively small set of concept nodes typically do most of the work.  and could be easily reviewed by a person to separate the good definitions from the bad ones.", "labels": [], "entities": []}, {"text": "15 If for no other reason, a human would be required to assign semantic labels to each definition so that the system can identify the type of information that is extracted.", "labels": [], "entities": []}, {"text": "Furthermore, the AutoSlog-TS dictionary should contain a higher percentage of relevant definitions that the original AutoSlog dictionary.", "labels": [], "entities": [{"text": "AutoSlog-TS dictionary", "start_pos": 17, "end_pos": 39, "type": "DATASET", "confidence": 0.8797435462474823}, {"text": "AutoSlog dictionary", "start_pos": 117, "end_pos": 136, "type": "DATASET", "confidence": 0.9113059639930725}]}, {"text": "Since the AutoSlog-TS dictionary has been prefiltered for both frequency and relevancy, many concept nodes that represent uncommon phrases or general expressions have already been removed.", "labels": [], "entities": []}, {"text": "Because AutoSlog-TS is not constrained to consider only the annotated portions of the corpus, it found many good patterns that AutoSlog did not.", "labels": [], "entities": []}, {"text": "For example, AutoSlog-TS produced 158 concept nodes that have a relevancy rate > 90% and frequency > 5.", "labels": [], "entities": [{"text": "relevancy rate", "start_pos": 64, "end_pos": 78, "type": "METRIC", "confidence": 0.9282909631729126}]}, {"text": "Only 45 of these concept nodes were in the original AutoSlog dictionary.", "labels": [], "entities": [{"text": "AutoSlog dictionary", "start_pos": 52, "end_pos": 71, "type": "DATASET", "confidence": 0.8974584937095642}]}, {"text": "shows a sample of some of the new concept nodes that represent patterns associated with terrorism.", "labels": [], "entities": []}, {"text": "Although it may still be necessary fora human to review the resulting patterns to build an information extraction system, this approach eliminates the need for text annotations and relies only on preclassified texts.", "labels": [], "entities": [{"text": "information extraction", "start_pos": 91, "end_pos": 113, "type": "TASK", "confidence": 0.7632299065589905}]}], "tableCaptions": []}