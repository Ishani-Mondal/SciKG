{"title": [{"text": "Neural Network Probability Estimation for Broad Coverage Parsing", "labels": [], "entities": [{"text": "Neural Network Probability Estimation", "start_pos": 0, "end_pos": 37, "type": "TASK", "confidence": 0.6922698318958282}, {"text": "Broad Coverage Parsing", "start_pos": 42, "end_pos": 64, "type": "TASK", "confidence": 0.6938018004099528}]}], "abstractContent": [{"text": "We present a neural-network-based statistical parser, trained and tested on the Penn Treebank.", "labels": [], "entities": [{"text": "Penn Treebank", "start_pos": 80, "end_pos": 93, "type": "DATASET", "confidence": 0.9958688020706177}]}, {"text": "The neural network is used to estimate the parameters of a generative model of left-corner parsing, and these parameters are used to search for the most probable parse.", "labels": [], "entities": [{"text": "generative model of left-corner parsing", "start_pos": 59, "end_pos": 98, "type": "TASK", "confidence": 0.6929641962051392}]}, {"text": "The parser's performance (88.8% F-measure) is within 1% of the best current parsers for this task, despite using a small vocabulary size (512 inputs).", "labels": [], "entities": [{"text": "F-measure)", "start_pos": 32, "end_pos": 42, "type": "METRIC", "confidence": 0.9815531075000763}]}, {"text": "Crucial to this success is the neural network architecture's ability to induce a finite representation of the unbounded parse history, and the biasing of this induction in a linguistically appropriate way.", "labels": [], "entities": []}], "introductionContent": [{"text": "Many statistical parsers) are based on a history-based probability model, where the probability of each decision in a parse is conditioned on the previous decisions in the parse.", "labels": [], "entities": []}, {"text": "A major challenge in this approach is choosing a representation of the parse history from which the probability for the next parser decision can be accurately estimated.", "labels": [], "entities": []}, {"text": "Previous approaches have used a hand-crafted finite set of features to represent the unbounded parse history).", "labels": [], "entities": []}, {"text": "In the work presented here, we automatically induce a finite set of features to represent the unbounded parse history.", "labels": [], "entities": []}, {"text": "We perform this induction using an artificial neural network architecture, called Simple Synchrony Networks (SSNs).", "labels": [], "entities": []}, {"text": "Because this architecture is specifically designed for processing structures, it allows us to impose structurally specified and linguistically appropriate biases on the search fora good history representation.", "labels": [], "entities": []}, {"text": "The resulting parser achieves performance far greater than previous approaches to neural network parsing (), and only marginally below the current state-of-the-art for parsing the Penn Treebank.", "labels": [], "entities": [{"text": "neural network parsing", "start_pos": 82, "end_pos": 104, "type": "TASK", "confidence": 0.8003409504890442}, {"text": "Penn Treebank", "start_pos": 180, "end_pos": 193, "type": "DATASET", "confidence": 0.9919953644275665}]}, {"text": "We propose a hybrid parsing system consisting of two components, a neural network which estimates the parameters of a probability model for phrase structure trees, and a statistical parser which searches for the most probable phrase structure tree given these parameters.", "labels": [], "entities": []}, {"text": "We first present the probability model which is common to these two components, followed by the estimation method, the search method, and a discussion of the empirical results.", "labels": [], "entities": []}], "datasetContent": [{"text": "The generality and efficiency of the above parsing model makes it possible to test a SSN parser on the Penn Treebank (, and thereby compare its performance directly to other statistical parsing models in the literature.", "labels": [], "entities": [{"text": "SSN parser", "start_pos": 85, "end_pos": 95, "type": "TASK", "confidence": 0.8749648928642273}, {"text": "Penn Treebank", "start_pos": 103, "end_pos": 116, "type": "DATASET", "confidence": 0.9954978823661804}]}, {"text": "To test the effects of varying vocabulary sizes on performance and tractability, we trained three different models.", "labels": [], "entities": []}, {"text": "The simplest model (\"Tags\") includes no words in the vocabulary, relying completely on the information provided by the part-of-speech tags of the words.", "labels": [], "entities": []}, {"text": "The second model (\"Freq>200\") uses all tag-word pairs which occur at least 200 times in the training set.", "labels": [], "entities": [{"text": "Freq", "start_pos": 19, "end_pos": 23, "type": "METRIC", "confidence": 0.9785768985748291}]}, {"text": "The remaining words were all treated as instances of the unknown-word.", "labels": [], "entities": []}, {"text": "This resulted in a vocabulary size of 512 tag-word pairs.", "labels": [], "entities": []}, {"text": "The third model (\"Freq>20\") thresholds the vocabulary at 20 instances in the training set, resulting in 4242 tag-word pairs.", "labels": [], "entities": [{"text": "Freq", "start_pos": 18, "end_pos": 22, "type": "METRIC", "confidence": 0.9360091090202332}]}, {"text": "As is standard practice, we used sections 2-22 as the training set (39,832 sentences), section 24 as a development/validation set (1346 sentence), and section 23 as a testing set (2416 sentences).", "labels": [], "entities": []}, {"text": "We determined appropriate training parameters and network size based on our previous experience with networks similar to the models Tags and Freq>200, which had been trained and evaluated on the same training and validation sets.", "labels": [], "entities": []}, {"text": "We trained two or three networks for each of the three models and chose the best one based on their validation performance.", "labels": [], "entities": []}, {"text": "We then tested the best non-lexicalized and the best lexicalized models on the testing set.", "labels": [], "entities": []}, {"text": "Standard measures of performance are shown in table 1.", "labels": [], "entities": []}, {"text": "The top panel of table 1 lists the results for the non-lexicalized model (SSN-Tags) and the available results for three other models which only use part-of-speech tags as inputs, another neural network parser (), an earlier statis- In these experiments the tags are included in the input to the system, but, for compatibility with other parsers, we did not use the hand-corrected tags which come with the corpus.", "labels": [], "entities": []}, {"text": "We used a publicly available tagger) to tag the words and then used these in the input to the system.", "labels": [], "entities": []}, {"text": "We found that 80 hidden units produced better performance than 60 or 100.", "labels": [], "entities": []}, {"text": "Momentum was applied throughout training.", "labels": [], "entities": [{"text": "Momentum", "start_pos": 0, "end_pos": 8, "type": "METRIC", "confidence": 0.9451016783714294}]}, {"text": "Weight decay regularization was applied at the beginning of training but reduced to zero by the end of training.", "labels": [], "entities": []}, {"text": "7 A11 our results are computed with the evalb program following the now-standard criteria in).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Percentage labeled constituent recall and  precision on the testing set.", "labels": [], "entities": [{"text": "recall", "start_pos": 41, "end_pos": 47, "type": "METRIC", "confidence": 0.9505626559257507}, {"text": "precision", "start_pos": 53, "end_pos": 62, "type": "METRIC", "confidence": 0.9995284080505371}]}, {"text": " Table 2: Percentage labeled constituent recall, pre- cision, and F-measure on the validation set.", "labels": [], "entities": [{"text": "recall", "start_pos": 41, "end_pos": 47, "type": "METRIC", "confidence": 0.9022407531738281}, {"text": "F-measure", "start_pos": 66, "end_pos": 75, "type": "METRIC", "confidence": 0.9964377880096436}]}]}