{"title": [{"text": "Constraint Based Integration of Deep and Shallow Parsing Techniques", "labels": [], "entities": []}], "abstractContent": [{"text": "To investigate the contributions of tag-gers or chunkers to the performance of a deep syntactic parser, Weighted Constraint Dependency Grammars have been extended to also take into consideration information from external sources.", "labels": [], "entities": []}, {"text": "Using a weak information fusion scheme based on constraint optimization techniques, a parsing accuracy has been achieved which is comparable to other (stochastic) parsers.", "labels": [], "entities": [{"text": "parsing", "start_pos": 86, "end_pos": 93, "type": "TASK", "confidence": 0.9633772373199463}, {"text": "accuracy", "start_pos": 94, "end_pos": 102, "type": "METRIC", "confidence": 0.8555428385734558}]}], "introductionContent": [{"text": "To investigate the contributions of different shallow processing components like taggers or chunkers to the performance of a deep syntactic parser, Weighted Constraint Dependency Grammars (WCDG) have been extended to also take into consideration information from external sources.", "labels": [], "entities": []}, {"text": "Constraints in the WCDG framework are (partially) defeasible conditions on admissible structural configurations in a dependency tree fora given natural language utterance.", "labels": [], "entities": []}, {"text": "By defining conditions (1) on word forms and their positional relationships, (2) on links between word forms, and (3) on the labels attached to these links, they license certain dependency relations or combinations thereof and assign a score to them.", "labels": [], "entities": []}, {"text": "These scores give an estimate of the appropriateness of a particular partial structure.", "labels": [], "entities": []}, {"text": "Constraint optimization techniques are used to obtain the structure with the best overall score, which is chosen as the optimal representation of the utterance.", "labels": [], "entities": []}, {"text": "Due to the use of weighted constraints throughout the grammar, WCDG comes with a number of advantages: \u2022 A WCDG grammar is able to accomodate conflicting requirements as they can often be found in natural language, e.g. in case of ordering preferences.", "labels": [], "entities": []}, {"text": "WCDG shares this property with Optimality Theory (Prince and Smolensky, forthc).", "labels": [], "entities": [{"text": "WCDG", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.9197981357574463}, {"text": "Optimality Theory", "start_pos": 31, "end_pos": 48, "type": "TASK", "confidence": 0.7188254296779633}]}, {"text": "\u2022 Since constraints can be violated, the grammar also accepts unusual or even deviant input (usually with a lower global score) and assigns a structural interpretation to it.", "labels": [], "entities": []}, {"text": "\u2022 To a large degree a WCDG parser does not depend crucially on a complete set of constraints.", "labels": [], "entities": [{"text": "WCDG parser", "start_pos": 22, "end_pos": 33, "type": "TASK", "confidence": 0.7953352928161621}]}, {"text": "Although performance degrades if some constraints are deactivated, it usually does so in a graceful manner.", "labels": [], "entities": []}, {"text": "\u2022 Constraints offer an ideal interface to integrate external processing components as additional sources of evidence into the decision about the optimal structural interpretation.", "labels": [], "entities": []}, {"text": "It is this last advantage which this paper is focussing upon.", "labels": [], "entities": []}, {"text": "Shallow processing components like taggers or chunkers can be integrated into the deep analysis simply by providing additional constraints.", "labels": [], "entities": []}, {"text": "If a scoring scheme is also available for such an external contribution (and it usually is), it can be combined with the internal scores assigned by the constraints of the grammar.", "labels": [], "entities": []}, {"text": "Similar to the arbitration of conflicting requirements within the grammar itself, the general optimization procedure allows the WCDG parser to handle possible conflicts between predictions of different origin.", "labels": [], "entities": []}, {"text": "This weak integration of hypotheses not only makes the approach an ideal platform for information fusion.", "labels": [], "entities": [{"text": "information fusion", "start_pos": 86, "end_pos": 104, "type": "TASK", "confidence": 0.9020819067955017}]}, {"text": "Moreover the individual contributions certain information sources make to the overall performance of the parser can be exactly measured, simply by switching the corresponding constraints on or off.", "labels": [], "entities": []}, {"text": "solution must be found rather than just one of a number of solutions.", "labels": [], "entities": []}, {"text": "Furthermore, weighted constraints do not allow us to prune away an alternative, leading to very large search spaces.", "labels": [], "entities": []}, {"text": "In fact, analyzing long sentences with WCDG usually leads to problems that are too large to be solved exactly.", "labels": [], "entities": []}, {"text": "However, heuristic methods have been developed that can approximate the optimal solution, where the quality of the solution increases overtime as more alternatives are tried ().", "labels": [], "entities": []}], "datasetContent": [{"text": "For our experiments we analysed 1845 unedited sentences from German online newscasts on buying or selling events, with an average length of 24 word forms (see fora distribution of sentence lengths).", "labels": [], "entities": []}, {"text": "We employed a handwritten WCDG of German that aims to perform a very thorough analysis; in addition to establishing syntax structure, it assigns each dependency one of 28 labels.", "labels": [], "entities": []}, {"text": "It must also determine the exact word class and morphological form of each word.", "labels": [], "entities": []}, {"text": "At the same time the lexicon was kept incomplete; of the open word classes of German, only the verbs are modelled while most nouns and adjectives are not'.", "labels": [], "entities": []}, {"text": "Therefore, about 29% of all tokens are effectively unknown words and receive a totally underspecified representation.", "labels": [], "entities": []}, {"text": "In the experimental setup chosen we run the optimization process on each sentence, and interrupt it if it has not terminated by itself after three minutes.", "labels": [], "entities": []}, {"text": "In this case, the best dependency structure established so far is returned as the parsing result.", "labels": [], "entities": []}, {"text": "If no additional knowledge sources (like the tagger and the chunker) are used the parser terminates by itself after 134 seconds on average.", "labels": [], "entities": []}, {"text": "By the measure of labelled recall, which counts how many dependencies are established correctly along with their labels, only 50.7% recall 2 is achieved on the average; although the WCDG usually assigns the desired analysis a near-optimal score, this analysis is often not found simply because the problem However, nouns and proper names can be recognized automatically by their capital initials.", "labels": [], "entities": [{"text": "recall", "start_pos": 27, "end_pos": 33, "type": "METRIC", "confidence": 0.9048032164573669}, {"text": "recall 2", "start_pos": 132, "end_pos": 140, "type": "METRIC", "confidence": 0.9790593683719635}, {"text": "WCDG", "start_pos": 182, "end_pos": 186, "type": "DATASET", "confidence": 0.761740505695343}]}, {"text": "Since all possible dependency analyses fora sentence have the same number of dependency edges, recall and precision are always equal fora fully disambiguated tree.", "labels": [], "entities": [{"text": "recall", "start_pos": 95, "end_pos": 101, "type": "METRIC", "confidence": 0.9994348883628845}, {"text": "precision", "start_pos": 106, "end_pos": 115, "type": "METRIC", "confidence": 0.9992794394493103}]}, {"text": "The term accuracy is sometimes used instead.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 9, "end_pos": 17, "type": "METRIC", "confidence": 0.9986796975135803}]}, {"text": "To measure the effect of the time limitation on accuracy, we conducted control experiments with the time limited to 6 minutes instead for comparison; the corresponding recall measures are given in in brackets.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 48, "end_pos": 56, "type": "METRIC", "confidence": 0.9991927742958069}, {"text": "recall", "start_pos": 168, "end_pos": 174, "type": "METRIC", "confidence": 0.998602569103241}]}, {"text": "They show that the time limit is not a great source of error.", "labels": [], "entities": []}, {"text": "Obviously the parsing process would benefit from additional information about the correct analysis, even if it is uncertain, as long as it can be produced with little effort.", "labels": [], "entities": [{"text": "parsing", "start_pos": 14, "end_pos": 21, "type": "TASK", "confidence": 0.988125205039978}]}, {"text": "This is the motivation behind the following experiments: to integrate a shallow but efficient information source into a thorough but slow parser to see whether a net gain can be achieved.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Distribution of sentence length.", "labels": [], "entities": []}, {"text": " Table 2: Influence of partial parsing information on the recall of a deep syntax analysis. Numbers in  brackets are for experiments with a relaxed timeout condition.", "labels": [], "entities": [{"text": "recall", "start_pos": 58, "end_pos": 64, "type": "METRIC", "confidence": 0.9963961243629456}]}]}