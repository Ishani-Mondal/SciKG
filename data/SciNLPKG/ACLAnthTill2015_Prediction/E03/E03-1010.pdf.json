{"title": [{"text": "Applications of Automatic Evaluation Methods to Measuring a Capability of Speech Translation System", "labels": [], "entities": [{"text": "Measuring a Capability of Speech Translation", "start_pos": 48, "end_pos": 92, "type": "TASK", "confidence": 0.740984116991361}]}], "abstractContent": [{"text": "The main goal of this paper is to propose automatic schemes for the translation paired comparison method.", "labels": [], "entities": [{"text": "translation paired comparison", "start_pos": 68, "end_pos": 97, "type": "TASK", "confidence": 0.7999180754025778}]}, {"text": "This method was proposed to precisely evaluate a speech translation system's capability.", "labels": [], "entities": [{"text": "speech translation", "start_pos": 49, "end_pos": 67, "type": "TASK", "confidence": 0.7435424327850342}]}, {"text": "Furthermore, the method gives an objective evaluation result, i.e., a score of the Test of English for International Communication (TOEIC).", "labels": [], "entities": [{"text": "International Communication (TOEIC)", "start_pos": 103, "end_pos": 138, "type": "TASK", "confidence": 0.5518185257911682}]}, {"text": "The TOEIC score is used as a measure of one's speech translation capability.", "labels": [], "entities": [{"text": "TOEIC score", "start_pos": 4, "end_pos": 15, "type": "METRIC", "confidence": 0.955188512802124}, {"text": "speech translation", "start_pos": 46, "end_pos": 64, "type": "TASK", "confidence": 0.7341638952493668}]}, {"text": "However, this method requires tremendous evaluation costs.", "labels": [], "entities": []}, {"text": "Accordingly, automatiza-tion of this method is an important subject for study.", "labels": [], "entities": []}, {"text": "In the proposed method, currently available automatic evaluation methods are applied to automate the translation paired comparison method.", "labels": [], "entities": []}, {"text": "In the experiments, several automatic evaluation methods (BLEU, NIST, DP-based method) are applied.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 58, "end_pos": 62, "type": "METRIC", "confidence": 0.9967315196990967}, {"text": "NIST", "start_pos": 64, "end_pos": 68, "type": "DATASET", "confidence": 0.8514441251754761}]}, {"text": "The experimental results of these automatic measures show a good correlation with evaluation results of the translation paired comparison method.", "labels": [], "entities": []}], "introductionContent": [], "datasetContent": [{"text": "The utterance unit evaluation takes roughly the same procedure as the translation paired comparison method.", "labels": [], "entities": []}, {"text": "shows the points of difference between the translation paired comparison method and the utterance unit evaluation of the proposed method.", "labels": [], "entities": []}, {"text": "The complete flow can be obtained by substituting for the broken line area of.", "labels": [], "entities": []}, {"text": "In the regression analysis of the utterance unit evaluation, the same procedure as the original translation paired comparison method is carried out.", "labels": [], "entities": []}, {"text": "Ina sense, the test set unit evaluation follows a different procedure from the translation paired comparison method and the utterance unit evaluation.", "labels": [], "entities": []}, {"text": "The flow of the test set unit evaluation is shown in.", "labels": [], "entities": []}, {"text": "In the regression analysis of the test set unit evaluation, the evaluation result by an automatic evaluation method is used instead of IVH.", "labels": [], "entities": [{"text": "IVH", "start_pos": 135, "end_pos": 138, "type": "METRIC", "confidence": 0.8047979474067688}]}, {"text": "Eall w1...zon in sys output (1)  In this section, we briefly describe the automatic evaluation methods that are applied to the proposed method.", "labels": [], "entities": []}, {"text": "Basically, these methods are based on the same idea, that is, to compare the target translation for evaluation to high-quality human reference translations.", "labels": [], "entities": []}, {"text": "These methods, then, require a corpus of high-quality human reference translations.", "labels": [], "entities": []}, {"text": "In this section, we show experimental results of the original translation paired comparison method and the proposed method.", "labels": [], "entities": []}, {"text": "The target system to be evaluated is Transfer Driven Machine Translation (TDMT) ().", "labels": [], "entities": [{"text": "Transfer Driven Machine Translation (TDMT)", "start_pos": 37, "end_pos": 79, "type": "TASK", "confidence": 0.7243603382791791}]}, {"text": "TDMT is a language translation subsystem of the Japanese-to-English speech translation system ATR-MATRIX.", "labels": [], "entities": [{"text": "TDMT", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.8846431970596313}, {"text": "Japanese-to-English speech translation system ATR-MATRIX", "start_pos": 48, "end_pos": 104, "type": "TASK", "confidence": 0.6218325853347778}]}, {"text": "For evaluation of TDMT, the input included accurate transcriptions.", "labels": [], "entities": [{"text": "TDMT", "start_pos": 18, "end_pos": 22, "type": "TASK", "confidence": 0.8337061405181885}]}, {"text": "The total number of examinees is 29, and the range of their TOEIC score is between the 300s and 800s.", "labels": [], "entities": [{"text": "TOEIC score", "start_pos": 60, "end_pos": 71, "type": "METRIC", "confidence": 0.975034236907959}]}, {"text": "Excepting the 600s, every hundredpoint range has 5 examinees.", "labels": [], "entities": []}, {"text": "The test set consists of 330 utterances in 23 conversations from the ATR bilingual travel conversation database).", "labels": [], "entities": [{"text": "ATR bilingual travel conversation database", "start_pos": 69, "end_pos": 111, "type": "DATASET", "confidence": 0.9420621156692505}]}, {"text": "Consequently, this test set has different features from written language.", "labels": [], "entities": []}, {"text": "Most of the utterances in our task contain fewer words than the unit of segment used so far in research with BLEU and NIST.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 109, "end_pos": 113, "type": "METRIC", "confidence": 0.9311692118644714}, {"text": "NIST", "start_pos": 118, "end_pos": 122, "type": "DATASET", "confidence": 0.9596489071846008}]}, {"text": "One utterance contains 11.9 words on average.", "labels": [], "entities": []}, {"text": "The standard deviation of the number of words is 6.5.", "labels": [], "entities": []}, {"text": "The shortest utterance consists of 1 word, and the longest consists of 32 words.", "labels": [], "entities": []}, {"text": "This test set was not used to train the TDMT system.", "labels": [], "entities": []}, {"text": "For the translations of examinees, all misspellings were corrected by humans because, as mentioned in Section 2, the human evaluator ignores misspellings in the original translation paired comparison method.", "labels": [], "entities": [{"text": "translations of examinees", "start_pos": 8, "end_pos": 33, "type": "TASK", "confidence": 0.8623143633206686}]}, {"text": "Comparison Method shows the results of a comparison between TDMT and the examinees.", "labels": [], "entities": []}, {"text": "Here, the abscissa represents the TOEIC score, and the ordinate represents WH.", "labels": [], "entities": [{"text": "TOEIC score", "start_pos": 34, "end_pos": 45, "type": "METRIC", "confidence": 0.9321211278438568}, {"text": "WH", "start_pos": 75, "end_pos": 77, "type": "METRIC", "confidence": 0.983722984790802}]}, {"text": "In this figure, the straight line indicates the regression line.", "labels": [], "entities": []}, {"text": "The capabilitybalanced point between the TDMT subsystem and: Detailed results of utterance unit evaluation the examinees was determined to be the point at which the regression line crossed half the total number of test utterances, i.e., WH of 0.5.", "labels": [], "entities": [{"text": "WH", "start_pos": 237, "end_pos": 239, "type": "METRIC", "confidence": 0.9976319074630737}]}, {"text": "In, this point is 705.", "labels": [], "entities": []}, {"text": "Consequently, the translation capability of the language translation system equals that of an examinee with a score of around 700 points on the TOEIC.", "labels": [], "entities": [{"text": "translation", "start_pos": 18, "end_pos": 29, "type": "TASK", "confidence": 0.9537632465362549}, {"text": "language translation", "start_pos": 48, "end_pos": 68, "type": "TASK", "confidence": 0.7144359201192856}, {"text": "TOEIC", "start_pos": 144, "end_pos": 149, "type": "DATASET", "confidence": 0.8440520763397217}]}, {"text": "We call this point the system's TOEIC score.", "labels": [], "entities": [{"text": "TOEIC score", "start_pos": 32, "end_pos": 43, "type": "METRIC", "confidence": 0.9713424146175385}]}, {"text": "In their original forms, the maximum n-gram length for BLEU (N in Equation 3) is set at 4 and that for NIST (N in Equation 4) is set at 5.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 55, "end_pos": 59, "type": "METRIC", "confidence": 0.9974356293678284}, {"text": "NIST", "start_pos": 103, "end_pos": 107, "type": "DATASET", "confidence": 0.9462184906005859}]}, {"text": "These settings were established for evaluation of written language.", "labels": [], "entities": []}, {"text": "However, utterances in our test set contain fewer words than in typical written language.", "labels": [], "entities": []}, {"text": "Consequently, for the utterance unit evaluation, we conducted several experiments while varying N from 1 to 4 for BLEU and from 1 to 5 for NIST.", "labels": [], "entities": [{"text": "utterance unit evaluation", "start_pos": 22, "end_pos": 47, "type": "TASK", "confidence": 0.8755877216657003}, {"text": "BLEU", "start_pos": 114, "end_pos": 118, "type": "METRIC", "confidence": 0.9968959093093872}, {"text": "NIST", "start_pos": 139, "end_pos": 143, "type": "DATASET", "confidence": 0.9135775566101074}]}, {"text": "shows the detailed results of the paired comparison using automatic evaluations.", "labels": [], "entities": []}, {"text": "shows experimental results of the utterance unit  Reorrect -Ucorrect I Utotul where Utota i is the total number of translation pairs consisting of the examinees' translation and the system's translation (330 utterances x 29 examinees = 9570 pairs) and Ue0\"ect is the number of pairs where the automatic evaluation gives the same evaluation result as that of the human evaluator.", "labels": [], "entities": [{"text": "Reorrect", "start_pos": 50, "end_pos": 58, "type": "METRIC", "confidence": 0.7625551223754883}, {"text": "Ue0", "start_pos": 252, "end_pos": 255, "type": "METRIC", "confidence": 0.9537450075149536}]}, {"text": "The difference between is the number of references to be used for automatic evaluation.", "labels": [], "entities": []}, {"text": "In, there is 1 reference per utterance, while in there are 16 references per utterance.", "labels": [], "entities": []}, {"text": "In these figures, values in parentheses under the abscissa indicate the maximum n-gram length.", "labels": [], "entities": []}, {"text": "Looking at these figures, the correct ratio of BLEU changes value depending on the maximum n-gram length.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 47, "end_pos": 51, "type": "METRIC", "confidence": 0.9992514252662659}]}, {"text": "The maximum n-gram length of 1 or 2 yields a high correct ratio, and that of 3 or 4 yields a low correct ratio.", "labels": [], "entities": [{"text": "correct ratio", "start_pos": 50, "end_pos": 63, "type": "METRIC", "confidence": 0.9868107438087463}, {"text": "correct ratio", "start_pos": 97, "end_pos": 110, "type": "METRIC", "confidence": 0.9818808734416962}]}, {"text": "On the other hand, the correct ratio of NIST is not influenced by the maximum n-gram length.", "labels": [], "entities": [{"text": "correct ratio", "start_pos": 23, "end_pos": 36, "type": "METRIC", "confidence": 0.9613105952739716}, {"text": "NIST", "start_pos": 40, "end_pos": 44, "type": "DATASET", "confidence": 0.715918242931366}]}, {"text": "It seems reasonable to suppose that these phenomena are due to computation of the mean of n-gram matching.", "labels": [], "entities": []}, {"text": "As shown in Equations 3 and 4, BLEU applies a geometric mean and NIST applies an information-weighted arithmetic mean.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 31, "end_pos": 35, "type": "METRIC", "confidence": 0.9985255599021912}, {"text": "NIST", "start_pos": 65, "end_pos": 69, "type": "DATASET", "confidence": 0.8891118764877319}]}, {"text": "Computation of the geometric mean yields 0 when one of the factors is 0, i.e., the BLEU score takes 0 for all of the utterances whose word count is less than the maximum ngram length.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 83, "end_pos": 93, "type": "METRIC", "confidence": 0.9858268797397614}]}, {"text": "The correct ratio shown in is low, i.e., around 0.5.", "labels": [], "entities": []}, {"text": "Thus, even state-of-theart technology is insufficient to determine better translation in the utterance unit evaluation.", "labels": [], "entities": []}, {"text": "For a sufficient result of the utterance unit evaluation, we need a more precise automatic evaluation method or another scheme, for example, majority decision using multiple automatic evaluation methods.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Detailed results of utterance unit evalua- tion", "labels": [], "entities": []}]}