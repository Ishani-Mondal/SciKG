{"title": [{"text": "Detecting Novel Compounds: The Role of Distributional Evidence", "labels": [], "entities": [{"text": "Detecting Novel Compounds", "start_pos": 0, "end_pos": 25, "type": "TASK", "confidence": 0.9161178668340048}, {"text": "Distributional Evidence", "start_pos": 39, "end_pos": 62, "type": "TASK", "confidence": 0.8230761289596558}]}], "abstractContent": [{"text": "Research on the discovery of terms from corpora has focused on word sequences whose recurrent occurrence in a corpus is indicative of their terminological status , and has not addressed the issue of discovering terms when data is sparse.", "labels": [], "entities": []}, {"text": "This becomes apparent in the case of noun compounding, which is extremely productive: more than half of the candidate compounds extracted from a corpus are attested only once.", "labels": [], "entities": [{"text": "noun compounding", "start_pos": 37, "end_pos": 53, "type": "TASK", "confidence": 0.8570749163627625}]}, {"text": "We show how evidence about established (i.e., frequent) compounds can be used to estimate features that can discriminate rare valid compounds from rare nonce terms in addition to a variety of linguistic features than can be easily gleaned from corpora without relying on parsed text.", "labels": [], "entities": []}], "introductionContent": [{"text": "The nature and properties of compounds have been studied at length in the theoretical linguistics literature.", "labels": [], "entities": []}, {"text": "It is a well-known fact that compound noun formation in English is relatively productive (see).", "labels": [], "entities": [{"text": "compound noun formation", "start_pos": 29, "end_pos": 52, "type": "TASK", "confidence": 0.6242831349372864}]}, {"text": "Although compounds are typically binary (see (1a,b)), they can be also longer than two words (see (le)).", "labels": [], "entities": []}, {"text": "Compounds are commonly written as a concatenation of words (see (1a,b)), or as single words (see (lc)), sometimes a hyphen is also used (see (le)).", "labels": [], "entities": []}, {"text": "The use of noun compounds is frequent not only in technical writing and newswire text but also in fictional prose, and spoken language.", "labels": [], "entities": []}, {"text": "Novel compounds are used as a text compression device, i.e., to pack meaning into a minimal amount of linguistic structure, as a deictic device, or as a means to classify an entity which has no specific name.", "labels": [], "entities": [{"text": "text compression", "start_pos": 30, "end_pos": 46, "type": "TASK", "confidence": 0.717131495475769}]}, {"text": "Computational investigations of compound nouns have concentrated on their automatic acquisition from corpora, syntactic disambiguation (i.e., determine the structure of compounds like income tax relief), and semantic interpretation (i.e., determine the semantic relation between income and tax in income tax).", "labels": [], "entities": [{"text": "semantic interpretation", "start_pos": 208, "end_pos": 231, "type": "TASK", "confidence": 0.7222789973020554}]}, {"text": "The acquisition of compound nouns is usually subsumed under the general discovery of terms from corpora.", "labels": [], "entities": []}, {"text": "Terms are typically acquired by either symbolic or statistical means.", "labels": [], "entities": [{"text": "Terms", "start_pos": 0, "end_pos": 5, "type": "TASK", "confidence": 0.9692699909210205}]}, {"text": "Under a symbolic approach, candidate terms are extracted from the corpus using surface syntactic analysis) and sometimes are further submitted to experts for manual inspection.", "labels": [], "entities": []}, {"text": "The approach typically assumes no prior terminological knowledge, although proposed the detection of terminological variants in a corpus by making use of lists of existing terms.", "labels": [], "entities": []}, {"text": "The main assumption underlying the statistical approach to term acquisition is that lexically associated words tend to appear together more often than expected on the basis of their individual occurrence frequencies.", "labels": [], "entities": [{"text": "term acquisition", "start_pos": 59, "end_pos": 75, "type": "TASK", "confidence": 0.8033996224403381}]}, {"text": "Once candidate terms are detected in the corpus, statistical tests (e.g., mutual information, the log-likelihood ratio) are used to determine which co-occurrences are valid terms (see for overviews).", "labels": [], "entities": []}, {"text": "Most of the statistical tests proposed in the literature rely on the fact that candidate terms will occur frequently in the corpus ( or, when hypothesis testing is applied, on the assumption that two words form a term when they co-occur more often than chance.", "labels": [], "entities": []}, {"text": "This means that statistical tests cannot be applied reliably for candidate compounds  In this paper we present a method that attempts to distinguish compounds from non-compounds in cases where very little direct evidence is found in the corpus and therefore the assumptions underlying lexical association scores do not hold.", "labels": [], "entities": []}, {"text": "We restrict our attention to compounds formed by a concatenation of two nouns (see (1a)) and investigate how surface syntactic and semantic cues can be used to discriminate valid compounds from rare nonce terms.", "labels": [], "entities": []}], "datasetContent": [{"text": "We further examined the accuracy on the classification task when solely contextual features are used.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 24, "end_pos": 32, "type": "METRIC", "confidence": 0.999529242515564}, {"text": "classification task", "start_pos": 40, "end_pos": 59, "type": "TASK", "confidence": 0.8979231119155884}]}, {"text": "We evaluated the influence of context by varying both the position and the size of the window of words (i.e., parts of speech) surrounding the candidate compound.", "labels": [], "entities": []}, {"text": "The window size parameter was varied between one and four words before and after the candidate compounds.", "labels": [], "entities": []}, {"text": "We use symbols 1 and r for left and right context, respectively and number to denote the window size.", "labels": [], "entities": []}, {"text": "For example, 1 = 2, r = 4 represents a window of two words to the left and four words to the right of the candidate noun-noun sequence.", "labels": [], "entities": []}, {"text": "shows the performance of the two classifiers for some of the contextual feature sets we examined.", "labels": [], "entities": []}, {"text": "Good performances are attained by both learners.", "labels": [], "entities": []}, {"text": "For DT, the best accuracy (69.1%) is obtained with windows of three or four words to the left of the candidate noun-noun sequence (see / = 4 and 1 = 3 in).", "labels": [], "entities": [{"text": "DT", "start_pos": 4, "end_pos": 6, "type": "TASK", "confidence": 0.9709799885749817}, {"text": "accuracy", "start_pos": 17, "end_pos": 25, "type": "METRIC", "confidence": 0.9993457198143005}]}, {"text": "NB performs best (70.8% and 69.8%) with small window sizes (see / = 1, and 1 = 1, r = 1 in).", "labels": [], "entities": [{"text": "NB", "start_pos": 0, "end_pos": 2, "type": "DATASET", "confidence": 0.9333707690238953}]}, {"text": "All three performances area significant improvement over the baseline (p < .05).", "labels": [], "entities": []}, {"text": "In general, better performance is achieved when one type of context is used (either left or right) instead of their combination (with the exception of 1 =1, r = 1 and 1 = 2, r = 1 for NB).", "labels": [], "entities": []}, {"text": "Our results suggest that even though context is encoded naively as parts of speech without preserving any structural or semantic knowledge, it retains enough information to distinguish compounds from non-compounds.", "labels": [], "entities": []}, {"text": "This is an important result given that the best numerical predictor (i.e., f,,(ni,n2)) relies heavily on taxonomic information.", "labels": [], "entities": []}, {"text": "The contextual features are straightforward to obtain-all we need is a concordance of the candidate compound annotated with parts of speech.", "labels": [], "entities": []}, {"text": "shows various combinations of numeric features, but also the interaction between numeric and contextual features.", "labels": [], "entities": []}, {"text": "Again, we report some (i.e., the most informative) of the feature sets we examined When only numeric features are used, the best accuracy for DT is attained with the combination of fwn (ni,n2) with P(1-11n1) (67.3%) or with f\"(ni,n2) (67.4%).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 129, "end_pos": 137, "type": "METRIC", "confidence": 0.9991040825843811}, {"text": "DT", "start_pos": 142, "end_pos": 144, "type": "TASK", "confidence": 0.9000409245491028}]}, {"text": "Similar accuracies are obtained when fw, (ni , n2) is combined with two or three features (see).", "labels": [], "entities": []}, {"text": "For the NB classifier, the best overall accuracy (72.3%) is attained for the feature set ff,,,(ni, n2), P(1-11n1), 1 = 11.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 40, "end_pos": 48, "type": "METRIC", "confidence": 0.9963908791542053}]}, {"text": "This set of features yields signifiant improvement over the baseline (p < .05) and outperforms any other feature combinations including any other pairings with contextual information.", "labels": [], "entities": []}, {"text": "The DT learner's performance is consistently better when numeric features are combined with contextual ones.", "labels": [], "entities": []}, {"text": "For all feature combinations shown in the inclusion of context yields better results and accuracies around 70%.", "labels": [], "entities": [{"text": "accuracies", "start_pos": 89, "end_pos": 99, "type": "METRIC", "confidence": 0.9956961870193481}]}, {"text": "Generally, a small context (e.g., 1 = 1 or r = 1) yields better results (over a larger context) when combined with numeric features.", "labels": [], "entities": []}, {"text": "A smaller context captures local syntactic dependencies such as the fact that compound nouns are typically preceded by determiners, verbs, or adjectives and succeded by verbs, prepositions or function words (e.g., and, or).", "labels": [], "entities": []}, {"text": "On the other hand, widening the context tends to proliferate global syntactic ambiguity making local syntactic dependencies harder to learn.", "labels": [], "entities": []}, {"text": "The DT learner achieves its best performance (72.0%) for the feature sets {f(nt), f (n2), P(I-11n1), f,\"(ni.", "labels": [], "entities": []}, {"text": "n2), f\"(ni,n2), 1 = 2} and fP(Mln2) , fwn(ni, n2), fro (ni,n2), f (ni), = 11.1t is worth noting that the second best performance (71.7%) is attained by the feature set {P P(Mln2), / = 11.", "labels": [], "entities": []}, {"text": "This is an important result given   72.0 60.1 P(Kn2),Lvn(ni, n2),.fro(ni,n2),,f (n 2 ), r = 2 70.6 65.6 P(H n1),P(M n2),,f14, n(ni , n2) 'fro (n 1 , n2) 66.9 56.0 P(H ni ),P(M n2) , f,,,i (n 1 ,n2),fro(ni ,n2), 1 = 1 68.6 68.8 P( 1-Ini),P(Mn2),.fla/(ni, n2),f,-0 (ni .n2), r = 2 69.8 67.1 f (111),f (n2) ,P (fl ni),,fivn(ni ,n2),fro(ni ,n2) 66.9 66.7 54.9 f (ni),f (n2),P(H ni),P(M n2), fwn (ni ,n2),.fro(ni . n2), 1 = 1 70.5 64.3 f (111),.f (112) ,P( 1171 1),P(m n2),,fwn (ni, n2), f;-0 (n 1, n2), r = 1 71.5 64.6: Combination of numeric and categorical features that these three features can be simply estimated from the corpus without recourse to taxonomic information.", "labels": [], "entities": [{"text": "P( 1171 1)", "start_pos": 449, "end_pos": 459, "type": "METRIC", "confidence": 0.8968845725059509}]}, {"text": "When compared, the two learners yield similar performances.", "labels": [], "entities": []}, {"text": "The NB classifier yields better results with smaller numbers of features, whereas the DT's performance remains steadily good, presumably because the most informative features are selected during the learning process.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Relation of noun co-occurrence frequency  with accuracy", "labels": [], "entities": [{"text": "Relation", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.7686108350753784}, {"text": "accuracy", "start_pos": 57, "end_pos": 65, "type": "METRIC", "confidence": 0.9595249891281128}]}, {"text": " Table 2: Feature values for noun-noun sequences (with CoocF 1)", "labels": [], "entities": []}, {"text": " Table 6: Combination of numeric and categorical features", "labels": [], "entities": []}]}