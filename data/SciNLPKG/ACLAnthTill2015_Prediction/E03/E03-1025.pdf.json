{"title": [{"text": "Using Grammatical Relations to Compare Parsers", "labels": [], "entities": []}], "abstractContent": [{"text": "We use the grammatical relations (GRs) described in Carroll et al.", "labels": [], "entities": []}, {"text": "(1998) to compare a number of parsing algorithms A first ranking of the parsers is provided by comparing the extracted GRs to a gold standard GR annotation of 500 Susanne sentences: this required an implementation of GR extraction software for Penn Treebank style parsers.", "labels": [], "entities": [{"text": "GR extraction", "start_pos": 217, "end_pos": 230, "type": "TASK", "confidence": 0.7576896846294403}, {"text": "Penn Treebank style parsers", "start_pos": 244, "end_pos": 271, "type": "DATASET", "confidence": 0.9363612532615662}]}, {"text": "In addition, we perform an experiment using the extracted GRs as input to the Lappin and Leass (1994) anaphora resolution algorithm.", "labels": [], "entities": [{"text": "anaphora resolution", "start_pos": 102, "end_pos": 121, "type": "TASK", "confidence": 0.6476482897996902}]}, {"text": "This produces a second ranking of the parsers, and we investigate the number of errors that are caused by the incorrect GRs.", "labels": [], "entities": []}], "introductionContent": [{"text": "We investigate the usefulness of a grammatical relation (GR) evaluation method by using it to compare the performance of four full parsers and a GR finder based on a shallow parser.", "labels": [], "entities": []}, {"text": "It is usually difficult to compare performance of different style parsers, as the output trees can vary in structure.", "labels": [], "entities": []}, {"text": "In this paper, we use GRs to provide a common basis for comparing full and shallow parsers, and Penn Treebank and Susanne structures.", "labels": [], "entities": [{"text": "Penn Treebank and Susanne structures", "start_pos": 96, "end_pos": 132, "type": "DATASET", "confidence": 0.9462698578834534}]}, {"text": "To carryout this comparison, we implemented a GR extraction mechanism for Penn Treebank This work was supported by UK EPSRC project GR/N36462/93 'Robust Accurate Statistical parses.", "labels": [], "entities": [{"text": "GR extraction", "start_pos": 46, "end_pos": 59, "type": "TASK", "confidence": 0.7333545386791229}, {"text": "Penn Treebank", "start_pos": 74, "end_pos": 87, "type": "DATASET", "confidence": 0.9951807260513306}, {"text": "UK EPSRC project GR/N36462/93", "start_pos": 115, "end_pos": 144, "type": "DATASET", "confidence": 0.8859324902296066}, {"text": "Robust Accurate Statistical parses", "start_pos": 146, "end_pos": 180, "type": "TASK", "confidence": 0.5187837481498718}]}, {"text": "Evaluating parsers using GRs as opposed to crossing brackets or labelled precision/recall metrics can be argued to give a more robust measure of performance,).", "labels": [], "entities": [{"text": "precision", "start_pos": 73, "end_pos": 82, "type": "METRIC", "confidence": 0.9066703915596008}, {"text": "recall", "start_pos": 83, "end_pos": 89, "type": "METRIC", "confidence": 0.6553997993469238}]}, {"text": "The main novelty of this paper is the use of the Carroll et al's GR evaluation method to compare the Collins model 1 and model 2, and Charniak parsers.", "labels": [], "entities": [{"text": "Collins model 1", "start_pos": 101, "end_pos": 116, "type": "DATASET", "confidence": 0.9296905199686686}]}, {"text": "An initial evaluation is provided by comparing the extracted GRs to a gold standard GR annotation of 500 Susanne sentences due to Carroll et al.", "labels": [], "entities": []}, {"text": "To gain insight into the strengths and weaknesses of the different parsers, we present a breakdown of the results for each type of GR.", "labels": [], "entities": []}, {"text": "It is not clear whether the ranking produced from the gold standard evaluation is representative: there maybe corpus effects for parsers not trained on Susanne, and real life applications may not reflect this ranking.", "labels": [], "entities": []}, {"text": "We therefore perform an experiment using the extracted GRs as input to the anaphora resolution algorithm.", "labels": [], "entities": [{"text": "anaphora resolution", "start_pos": 75, "end_pos": 94, "type": "TASK", "confidence": 0.6831059008836746}]}, {"text": "This produces a second ranking of the parsers, and we investigate the number of errors that are caused by incorrect GRs.", "labels": [], "entities": []}, {"text": "We describe the parsers and the GR finder in Section 2.", "labels": [], "entities": [{"text": "GR finder", "start_pos": 32, "end_pos": 41, "type": "TASK", "confidence": 0.6435218304395676}]}, {"text": "We introduce GRs in Section 3 and briefly describe our GR extraction software for Penn Treebank style parses.", "labels": [], "entities": [{"text": "GR extraction", "start_pos": 55, "end_pos": 68, "type": "TASK", "confidence": 0.8038358092308044}, {"text": "Penn Treebank style parses", "start_pos": 82, "end_pos": 108, "type": "DATASET", "confidence": 0.9316362589597702}]}, {"text": "The evaluation, including a description of the evaluation corpus and performance results, is presented in Section 4.", "labels": [], "entities": []}, {"text": "The results are analyzed in Section 5 and a performance comparison in the context of anaphora resolution is presented in Section 6.", "labels": [], "entities": [{"text": "anaphora resolution", "start_pos": 85, "end_pos": 104, "type": "TASK", "confidence": 0.7039307355880737}]}, {"text": "We draw our conclusions in Section 7.", "labels": [], "entities": []}, {"text": "grammar, the parsing algorithm, the tagger and the training corpus for all the parsers that we investigate.", "labels": [], "entities": []}], "datasetContent": [{"text": "The candidate with the highest salience is proposed as the antecedent.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Summary of Published Results (LR = labelled recall, LP = labelled precision,  CB = crossing brackets)", "labels": [], "entities": [{"text": "Summary", "start_pos": 10, "end_pos": 17, "type": "METRIC", "confidence": 0.7690872550010681}, {"text": "recall", "start_pos": 54, "end_pos": 60, "type": "METRIC", "confidence": 0.8700354695320129}, {"text": "precision", "start_pos": 76, "end_pos": 85, "type": "METRIC", "confidence": 0.6418542861938477}]}, {"text": " Table 3: GR Precisions and Recalls", "labels": [], "entities": [{"text": "GR Precisions", "start_pos": 10, "end_pos": 23, "type": "TASK", "confidence": 0.44756270945072174}, {"text": "Recalls", "start_pos": 28, "end_pos": 35, "type": "METRIC", "confidence": 0.5998152494430542}]}, {"text": " Table 4: t-tests for F-measure", "labels": [], "entities": [{"text": "F-measure", "start_pos": 22, "end_pos": 31, "type": "METRIC", "confidence": 0.9360746741294861}]}, {"text": " Table 5: Percentage of Extras", "labels": [], "entities": [{"text": "Percentage of Extras", "start_pos": 10, "end_pos": 30, "type": "METRIC", "confidence": 0.898122509320577}]}, {"text": " Table 6: Percentage of Missing", "labels": [], "entities": [{"text": "Percentage", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9519416689872742}, {"text": "Missing", "start_pos": 24, "end_pos": 31, "type": "DATASET", "confidence": 0.32660821080207825}]}, {"text": " Table 10: t-tests for Anaphora Resolution Per- formance", "labels": [], "entities": [{"text": "Anaphora Resolution", "start_pos": 23, "end_pos": 42, "type": "TASK", "confidence": 0.7287579327821732}]}, {"text": " Table 10. The rank- ing obtained in this case indicates very small  differences in performance between the algo- rithms.", "labels": [], "entities": [{"text": "rank- ing", "start_pos": 15, "end_pos": 24, "type": "METRIC", "confidence": 0.893971860408783}]}, {"text": " Table 11: Number of Mistaken Systems", "labels": [], "entities": [{"text": "Number of Mistaken Systems", "start_pos": 11, "end_pos": 37, "type": "TASK", "confidence": 0.6891848519444466}]}, {"text": " Table 12: Number of Different Antecedents", "labels": [], "entities": []}]}