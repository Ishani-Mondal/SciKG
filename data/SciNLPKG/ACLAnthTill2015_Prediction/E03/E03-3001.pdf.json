{"title": [{"text": "Learning to Identify Fragmented Words in Spoken Discourse", "labels": [], "entities": [{"text": "Learning to Identify Fragmented Words in Spoken Discourse", "start_pos": 0, "end_pos": 57, "type": "TASK", "confidence": 0.70770313590765}]}], "abstractContent": [{"text": "Disfluent speech adds to the difficulty of processing spoken language utterances.", "labels": [], "entities": [{"text": "processing spoken language utterances", "start_pos": 43, "end_pos": 80, "type": "TASK", "confidence": 0.7367327958345413}]}, {"text": "In this paper we concentrate on identifying one disfluency phenomenon: fragmented words.", "labels": [], "entities": []}, {"text": "Our data, from the Spoken Dutch Corpus, samples nearly 45,000 sentences of human discourse, ranging from spontaneous chat to media broadcasts.", "labels": [], "entities": [{"text": "Spoken Dutch Corpus", "start_pos": 19, "end_pos": 38, "type": "DATASET", "confidence": 0.8256536722183228}]}, {"text": "We classify each lexical item in a sentence either as a completely or an incompletely uttered, i.e. fragmented, word.", "labels": [], "entities": []}, {"text": "The task is carried out both by the IB 1 and RIPPER machine learning algorithms, trained on a variety of features with an extensive optimization strategy.", "labels": [], "entities": [{"text": "IB 1", "start_pos": 36, "end_pos": 40, "type": "DATASET", "confidence": 0.7028023898601532}]}, {"text": "Our best classifier has a 74.9% F-score, which is a significant improvement over the baseline.", "labels": [], "entities": [{"text": "F-score", "start_pos": 32, "end_pos": 39, "type": "METRIC", "confidence": 0.9966617822647095}]}, {"text": "We discuss why memory-based learning has more success than rule induction in correctly classifying fragmented words.", "labels": [], "entities": [{"text": "rule induction", "start_pos": 59, "end_pos": 73, "type": "TASK", "confidence": 0.6895963251590729}]}], "introductionContent": [{"text": "Although human listeners are good at handling disfluent items (self-corrections, repetitions, hesitations, incompletely uttered words and the like, cf. ) in spoken language utterances, these are likely to cause confusion when used as input to automatic natural language processing (NLP) systems, resulting in poor humancomputer interaction.", "labels": [], "entities": []}, {"text": "Detecting disfluent passages can help clean the spoken input and improve further processing such as parsing.", "labels": [], "entities": [{"text": "parsing", "start_pos": 100, "end_pos": 107, "type": "TASK", "confidence": 0.9623842835426331}]}, {"text": "By treating fragments we cover a considerable portion of the occurring disfluencies as incompletely uttered words often occur as part of a speaker's self-repair (.", "labels": [], "entities": []}, {"text": "Moreover, if an incompletely pronounced item is identified, we thereby determine the interruption point, a central phenomenon in disfluencies ().", "labels": [], "entities": [{"text": "interruption point", "start_pos": 85, "end_pos": 103, "type": "METRIC", "confidence": 0.9536682069301605}]}, {"text": "The surroundings of this disfluency element are to be treated with greater care, as before an interruption point there might be word(s) meant to be erased (called the reparandum), whereas the word(s) that follow it (the repair) might be intended to replace the erased part, cf. the following example: het veilig gebruik van interne_*' 12 sorry van electronic commerce (the safe usage of interne-* sorry of electronic commerce).", "labels": [], "entities": []}, {"text": "Previous studies in the field of applying machine learning (ML) methods to disfluencies either employ classification and regression trees for identifying repair cues and for detecting disfluencies), or they use a combination of decision trees and language models to detect disfluency events () or to model repairs reparandum.", "labels": [], "entities": []}, {"text": "Although and observe that word fragments pose an unsolved problem in processing disfluencies, often the presence of a disfluent word is regarded as an integral property of a speech repair and is employed as a readily available feature in the ML tool.", "labels": [], "entities": [{"text": "speech repair", "start_pos": 174, "end_pos": 187, "type": "TASK", "confidence": 0.6968030333518982}]}, {"text": "However, automatic identification of a fragment is not straightforward, unlike the recognition of other disfluency types, such as filled pauses (\"uhm\").", "labels": [], "entities": [{"text": "automatic identification of a fragment", "start_pos": 9, "end_pos": 47, "type": "TASK", "confidence": 0.7792371511459351}]}, {"text": "Our study investigates the feasibility of automatically detecting fragments, for which we propose using learning algorithms, since they suit this problem formalised as a binary classification task of deciding whether a word is completely or incompletely uttered.", "labels": [], "entities": []}, {"text": "The current paper first describes our large-scale experimental material, after which the learning process is explained, with particular emphasis on the features employed by the two different learning algorithms and the experimental setup.", "labels": [], "entities": []}, {"text": "We also introduce the method of iterative deepening used for optimizing the parameters of both the memory-based and the rule induction classifier.", "labels": [], "entities": []}, {"text": "In Section 4 the results of the fragment identification task are reported and the behaviour of the learners is analysed.", "labels": [], "entities": [{"text": "fragment identification task", "start_pos": 32, "end_pos": 60, "type": "TASK", "confidence": 0.8858814438184103}]}, {"text": "The last section evaluates our approach and outlines the directions for further investigation.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 2: Ten instances built from the ten elements of the utterance \"<laughter> yes he is with ru-*  rugby team uh ...\" : the focus item in windowed context, the numeric features and the class symbol.", "labels": [], "entities": []}, {"text": " Table 3: Results of default and optimized IB 1 and RIPPER in 10-fold cross-validation.", "labels": [], "entities": [{"text": "IB 1", "start_pos": 43, "end_pos": 47, "type": "METRIC", "confidence": 0.5912202596664429}, {"text": "RIPPER", "start_pos": 52, "end_pos": 58, "type": "METRIC", "confidence": 0.9744014143943787}]}]}