{"title": [{"text": "An Efficient Implementation of a New POP Model", "labels": [], "entities": []}], "abstractContent": [{"text": "Two apparently opposing DOP models exist in the literature: one which computes the parse tree involving the most frequent subtrees from a treebank and one which computes the parse tree involving the fewest subtrees from a treebank.", "labels": [], "entities": []}, {"text": "This paper proposes an integration of the two models which outperforms each of them separately.", "labels": [], "entities": []}, {"text": "Together with a PCFG-reduction of DOP we obtain improved accuracy and efficiency on the Wall Street Journal treebank Our results show an 11% relative reduction in error rate over previous models, and an average processing time of 3.6 seconds per WSJ sentence.", "labels": [], "entities": [{"text": "DOP", "start_pos": 34, "end_pos": 37, "type": "METRIC", "confidence": 0.8345606327056885}, {"text": "accuracy", "start_pos": 57, "end_pos": 65, "type": "METRIC", "confidence": 0.999357283115387}, {"text": "Wall Street Journal treebank", "start_pos": 88, "end_pos": 116, "type": "DATASET", "confidence": 0.9821353107690811}, {"text": "error rate", "start_pos": 163, "end_pos": 173, "type": "METRIC", "confidence": 0.9781074225902557}]}], "introductionContent": [], "datasetContent": [{"text": "For our experiments we used the standard division of the WSJ (), with sections 2 through 21 for training (approx. 40,000 sentences) and section 23 for testing (2416 sentences 100 words); section 22 was used as development set.", "labels": [], "entities": [{"text": "WSJ", "start_pos": 57, "end_pos": 60, "type": "DATASET", "confidence": 0.8853703737258911}]}, {"text": "As usual, all trees were stripped off their semantic tags, co-reference information and quotation marks.", "labels": [], "entities": []}, {"text": "Without loss of generality, all trees were converted to binary branching (and were reconverted to n-ary trees after parsing).", "labels": [], "entities": []}, {"text": "We employed the same unknown (category) word model as in, based on statistics on word-endings, hyphenation and capitalization in combination with Good-Turing).", "labels": [], "entities": []}, {"text": "We used \"evalb\" 4 to compute the standard PARSEVAL scores for our results).", "labels": [], "entities": [{"text": "PARSEVAL scores", "start_pos": 42, "end_pos": 57, "type": "DATASET", "confidence": 0.5513337403535843}]}, {"text": "We focused on the Labeled Precision (LP) and Labeled Recall (LR) scores, as these are commonly used to rank parsing systems.", "labels": [], "entities": [{"text": "Labeled Precision (LP) and Labeled Recall (LR) scores", "start_pos": 18, "end_pos": 71, "type": "METRIC", "confidence": 0.7831731711824735}]}, {"text": "4 http://www.cs.nyu.edu/cs/projects/proteus/evalb/ Our first experimental goal was to compare the two PCFG-reductions in Section 2.2, which we will refer to resp.", "labels": [], "entities": [{"text": "PCFG-reductions in Section 2.2", "start_pos": 102, "end_pos": 132, "type": "DATASET", "confidence": 0.8305248618125916}]}, {"text": "gives the results of these experiments and compares them with some other statistical parsers (resp..", "labels": [], "entities": []}, {"text": "As to the processing time, the PCFG reduction parses each sentence 100 words) in 3.6 seconds average, while the parser in Bod, which uses over 5 million subtrees, is reported to take about 220 seconds per sentence.", "labels": [], "entities": [{"text": "PCFG reduction", "start_pos": 31, "end_pos": 45, "type": "DATASET", "confidence": 0.8461693227291107}, {"text": "Bod", "start_pos": 122, "end_pos": 125, "type": "DATASET", "confidence": 0.9914814233779907}]}, {"text": "This corresponds to a speedup of over 60 times.", "labels": [], "entities": []}, {"text": "It should be mentioned that the best precision and recall scores reported in Bod (2001) are slightly better than the ones reported here (the difference is only 0.2% for sentences 100 words).", "labels": [], "entities": [{"text": "precision", "start_pos": 37, "end_pos": 46, "type": "METRIC", "confidence": 0.9993904829025269}, {"text": "recall", "start_pos": 51, "end_pos": 57, "type": "METRIC", "confidence": 0.997731626033783}, {"text": "Bod (2001)", "start_pos": 77, "end_pos": 87, "type": "DATASET", "confidence": 0.9461092501878738}]}, {"text": "This maybe explained by the fact our best results in Bod (2001) were obtained by testing various subtree restrictions until the highest accuracy was obtained, while in the current experiment we used all subtrees as given by the PCFG-reduction.", "labels": [], "entities": [{"text": "Bod (2001)", "start_pos": 53, "end_pos": 63, "type": "DATASET", "confidence": 0.9246187806129456}, {"text": "accuracy", "start_pos": 136, "end_pos": 144, "type": "METRIC", "confidence": 0.9973182082176208}, {"text": "PCFG-reduction", "start_pos": 228, "end_pos": 242, "type": "DATASET", "confidence": 0.9778607487678528}]}, {"text": "In the following section we will see that our new definition of best parse tree also outperforms the best results obtained in. first results of SL-DOP and LS-DOP with a compact PCFG-reduction.", "labels": [], "entities": [{"text": "PCFG-reduction", "start_pos": 177, "end_pos": 191, "type": "DATASET", "confidence": 0.9275215864181519}]}], "tableCaptions": [{"text": " Table 1. Bod (2001) and Bonnema et al. (1999)  compared to other parsers", "labels": [], "entities": [{"text": "Bod (2001)", "start_pos": 10, "end_pos": 20, "type": "DATASET", "confidence": 0.8999265283346176}]}, {"text": " Table 2. Results of SL-DOP and LS-DOP on the  WSJ (sentences 100 words)", "labels": [], "entities": [{"text": "WSJ", "start_pos": 47, "end_pos": 50, "type": "DATASET", "confidence": 0.9159832000732422}]}]}