{"title": [], "abstractContent": [{"text": "We propose anew method for detecting errors in \"gold-standard\" part-of-speech annotation.", "labels": [], "entities": []}, {"text": "The approach locates errors with high precision based on n-grams occurring in the corpus with multiple taggings.", "labels": [], "entities": [{"text": "precision", "start_pos": 38, "end_pos": 47, "type": "METRIC", "confidence": 0.9957441687583923}]}, {"text": "Two further techniques , closed-class analysis and finite-state tagging guide patterns, are discussed.", "labels": [], "entities": [{"text": "closed-class analysis", "start_pos": 25, "end_pos": 46, "type": "TASK", "confidence": 0.7034128457307816}, {"text": "finite-state tagging", "start_pos": 51, "end_pos": 71, "type": "TASK", "confidence": 0.6946640312671661}]}, {"text": "The success of the three approaches is illustrated for the Wall Street Journal corpus as part of the Penn Tree-bank.", "labels": [], "entities": [{"text": "Wall Street Journal corpus", "start_pos": 59, "end_pos": 85, "type": "DATASET", "confidence": 0.9699436575174332}, {"text": "Penn Tree-bank", "start_pos": 101, "end_pos": 115, "type": "DATASET", "confidence": 0.9924165904521942}]}], "introductionContent": [{"text": "Part-of-speech (pos) annotated reference corpora, such as the British National Corpus (, the Penn Treebank (, or the German Negra Treebank ( play an important role for current work in computational linguistics.", "labels": [], "entities": [{"text": "British National Corpus", "start_pos": 62, "end_pos": 85, "type": "DATASET", "confidence": 0.9037003715833029}, {"text": "Penn Treebank", "start_pos": 93, "end_pos": 106, "type": "DATASET", "confidence": 0.9942653477191925}, {"text": "German Negra Treebank", "start_pos": 117, "end_pos": 138, "type": "DATASET", "confidence": 0.7436995704968771}]}, {"text": "They provide training material for research on tagging algorithms and they serve as a gold standard for evaluating the performance of such tools.", "labels": [], "entities": [{"text": "tagging algorithms", "start_pos": 47, "end_pos": 65, "type": "TASK", "confidence": 0.9429718852043152}]}, {"text": "High quality, pos-annotated text is also relevant as input for syntactic processing, for practical applications such as information extraction, and for linguistic research making use of pos-based corpus queries.", "labels": [], "entities": [{"text": "syntactic processing", "start_pos": 63, "end_pos": 83, "type": "TASK", "confidence": 0.7283503264188766}, {"text": "information extraction", "start_pos": 120, "end_pos": 142, "type": "TASK", "confidence": 0.8487696051597595}]}, {"text": "The gold-standard pos-annotation for such large reference corpora is generally obtained using an automatic tagger to produce a first annotation, followed by human post-editing.", "labels": [], "entities": []}, {"text": "While Sinclair (1992) provides some arguments for prioritizing a fully automated analysis, human post-editing has been shown to significantly reduce the number of pos-annotation errors.", "labels": [], "entities": []}, {"text": "discusses that a single human post-editor reduces the 3.3% error rate in the STTS annotation of the German Negra corpus produced by the TnT tagger to 1.2%.", "labels": [], "entities": [{"text": "error rate", "start_pos": 59, "end_pos": 69, "type": "METRIC", "confidence": 0.9584029316902161}, {"text": "STTS annotation of the German Negra corpus produced by the TnT tagger", "start_pos": 77, "end_pos": 146, "type": "DATASET", "confidence": 0.7327101106444994}]}, {"text": "also reports an improvement of around 2% fora similar experiment carried out for an English sample originally tagged with 96.95% accuracy by the CLAWS tagger.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 129, "end_pos": 137, "type": "METRIC", "confidence": 0.9964995384216309}, {"text": "CLAWS tagger", "start_pos": 145, "end_pos": 157, "type": "DATASET", "confidence": 0.9360526204109192}]}, {"text": "And reports that manual post-editing and correction done for the 2-million word core corpus portion of the BNC, the BNC-sampler, reduced the approximate error rate of 1.7% for the automatically obtained annotation to less than 0.3%.", "labels": [], "entities": [{"text": "BNC", "start_pos": 107, "end_pos": 110, "type": "DATASET", "confidence": 0.6938449740409851}, {"text": "BNC-sampler", "start_pos": 116, "end_pos": 127, "type": "DATASET", "confidence": 0.9059802293777466}, {"text": "approximate error rate", "start_pos": 141, "end_pos": 163, "type": "METRIC", "confidence": 0.8839709361394247}]}, {"text": "While the last figure clearly is a remarkable result, van, working with the written half of the BNC-sampler, reports that in 13.6% of the cases where his WPDV tagger disagrees with the BNC annotation, the cause is an error in the BNC annotation.", "labels": [], "entities": [{"text": "BNC-sampler", "start_pos": 96, "end_pos": 107, "type": "DATASET", "confidence": 0.8957055807113647}, {"text": "WPDV", "start_pos": 154, "end_pos": 158, "type": "DATASET", "confidence": 0.9059805274009705}, {"text": "BNC", "start_pos": 230, "end_pos": 233, "type": "DATASET", "confidence": 0.8961712121963501}]}, {"text": "1 Improving the correctness of such gold-standard annotation thus is important for obtaining reliable testing material for pos-tagger research, as well as for the other uses of gold-standard annotation mentioned at the beginning of this section-a point which becomes even stronger when one considers that the posannotation of most reference corpora contain significantly more errors than the 0.3% figure reported for the In this paper, we present three methods for automatic detection of annotation errors which remainThe percentage of disagreement caused by BNC errors rises to 20.5% fora tagger trained on the entire corpus.", "labels": [], "entities": []}, {"text": "despite human post-editing, and sometimes are actually caused by it.", "labels": [], "entities": []}, {"text": "Our main proposal discussed in section 2.1 is independent of the language and tagset of the corpus and requires no additional language resources such as lexica.", "labels": [], "entities": []}, {"text": "It detects variation in the pos-annotation of a corpus by searching for n-grams which occur more than once in the corpus and include at least one difference in their annotation.", "labels": [], "entities": []}, {"text": "We discuss how all such variation n-grams of a corpus can be obtained and show that together with some heuristics they are highly accurate predictors of annotation errors.", "labels": [], "entities": []}, {"text": "In section 2.2 we turn to two other simple ideas for detecting posannotation errors, closed-class analysis and finitestate tagging guide patterns.", "labels": [], "entities": [{"text": "finitestate tagging", "start_pos": 111, "end_pos": 130, "type": "TASK", "confidence": 0.7343607842922211}]}, {"text": "Finally, in section 3 we relate our research to several recent publications addressing the topic of pos-error correction.", "labels": [], "entities": [{"text": "pos-error correction", "start_pos": 100, "end_pos": 120, "type": "TASK", "confidence": 0.8213748335838318}]}], "datasetContent": [], "tableCaptions": []}