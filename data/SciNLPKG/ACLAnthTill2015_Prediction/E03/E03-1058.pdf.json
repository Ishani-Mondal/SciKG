{"title": [{"text": "Linear Text Segmentation using a Dynamic Programming Algorithm", "labels": [], "entities": [{"text": "Linear Text Segmentation", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.6430513759454092}]}], "abstractContent": [{"text": "In this paper we introduce a dynamic programming algorithm to perform linear text segmentation by global minimization of a segmentation cost function which consists of: (a) within-segment word similarity and (b) prior information about segment length.", "labels": [], "entities": [{"text": "linear text segmentation", "start_pos": 70, "end_pos": 94, "type": "TASK", "confidence": 0.6656964222590128}]}, {"text": "The evaluation of the segmentation accuracy of the algorithm on Choi's text collection showed that the algorithm achieves the best segmentation accuracy so far reported in the literature.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 35, "end_pos": 43, "type": "METRIC", "confidence": 0.451240599155426}, {"text": "Choi's text collection", "start_pos": 64, "end_pos": 86, "type": "DATASET", "confidence": 0.9051050841808319}, {"text": "accuracy", "start_pos": 144, "end_pos": 152, "type": "METRIC", "confidence": 0.8357112407684326}]}], "introductionContent": [{"text": "Text segmentation is an important problem in information retrieval.", "labels": [], "entities": [{"text": "Text segmentation", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.7358930855989456}, {"text": "information retrieval", "start_pos": 45, "end_pos": 66, "type": "TASK", "confidence": 0.8384657800197601}]}, {"text": "Its goal is the division of a text into homogeneous (\"lexically coherent\") segments, i.e segments exhibiting the following properties: (a) each segment deals with a particular subject and (b) contiguous segments deal with different subjects.", "labels": [], "entities": []}, {"text": "Those segments can be retrieved from a large database of unformatted (or loosely formatted) text as being relevant to a query.", "labels": [], "entities": []}, {"text": "This paper presents a dynamic programming algorithm which performs linear segmentation 1 by global minimization of a segmentation cost.", "labels": [], "entities": [{"text": "linear segmentation 1", "start_pos": 67, "end_pos": 88, "type": "TASK", "confidence": 0.709697425365448}]}, {"text": "The segmentation cost is defined by a function consisting of two factors: (a) within-segment word similarity and (b) prior information about segment length.", "labels": [], "entities": [{"text": "segmentation", "start_pos": 4, "end_pos": 16, "type": "TASK", "confidence": 0.9669856429100037}]}, {"text": "Our algorithm has the advantage of being able to be applied to either large texts -to segment them into their constituent parts (e.g. to segment an article into sections) -or to a stream of independent, concatenated texts (e.g. to segment a transcript of news into separate stories).", "labels": [], "entities": []}, {"text": "For the calculation of the segment homogeneity (or alternatively heterogeneity) of a text, several segmentation algorithms using a variety of criteria have been proposed in the literature.", "labels": [], "entities": []}, {"text": "Some of those use linguistic criteria such as cue phrases, punctuation marks, prosodic features, reference, syntax and lexical attraction (.", "labels": [], "entities": []}, {"text": "Others, following Halliday and Hasan's theory, utilize statistical similarity measures such as word cooccurrence.", "labels": [], "entities": []}, {"text": "For example the linear discourse segmentation algorithm proposed by) is based on lexical cohesion relations determined by use of Roget's thesaurus.", "labels": [], "entities": [{"text": "linear discourse segmentation", "start_pos": 16, "end_pos": 45, "type": "TASK", "confidence": 0.7226121028264364}]}, {"text": "In the same direction Kozima's algorithm) computes the semantic similarity between words using a semantic network constructed from a subset of the Longman Dictionary of Contemporary English.", "labels": [], "entities": [{"text": "Longman Dictionary of Contemporary English", "start_pos": 147, "end_pos": 189, "type": "DATASET", "confidence": 0.9364714741706848}]}, {"text": "Local minima of the similarity scores correspond to the positions of topic boundaries in the text.", "labels": [], "entities": [{"text": "similarity", "start_pos": 20, "end_pos": 30, "type": "METRIC", "confidence": 0.950974702835083}]}], "datasetContent": [{"text": "Our experiments were conducted using Choi's publicly available text collection).", "labels": [], "entities": [{"text": "Choi's publicly available text collection", "start_pos": 37, "end_pos": 78, "type": "DATASET", "confidence": 0.8624065816402435}]}, {"text": "This collection consists of 700 texts, each text being a concatenation often text segments.", "labels": [], "entities": []}, {"text": "Each segment consists of \"the first n sentences of a randomly selected document from the Brown Corpus.", "labels": [], "entities": [{"text": "Brown Corpus", "start_pos": 89, "end_pos": 101, "type": "DATASET", "confidence": 0.9852324426174164}]}, {"text": "(News articles ca**.pos and the informative text cj**.pos)\"3 . The 700 texts can be divided into four datasets Set0, Set 1 , Set2, Set3, according to the range of n (the number of sentences in a document) as listed in.", "labels": [], "entities": []}, {"text": "The sample texts were preprocessed i.e. punctuation marks and stop words were removed, while the remaining words were stemmed according to Porter's stemming algorithm).", "labels": [], "entities": []}], "tableCaptions": []}