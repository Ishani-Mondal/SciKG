{"title": [{"text": "Transparent combination of rule-based and data-driven approaches in a speech understanding architecture", "labels": [], "entities": [{"text": "speech understanding", "start_pos": 70, "end_pos": 90, "type": "TASK", "confidence": 0.7893030643463135}]}], "abstractContent": [{"text": "We describe a domain-independent semantic interpretation architecture suitable for spoken dialogue systems, which uses a decision-list method to effect a transparent combination of rule-based and data-driven approaches.", "labels": [], "entities": [{"text": "domain-independent semantic interpretation", "start_pos": 14, "end_pos": 56, "type": "TASK", "confidence": 0.6752434472242991}]}, {"text": "The architecture has been implemented and evaluated in the context of a medium-vocabulary command and control task.", "labels": [], "entities": []}], "introductionContent": [{"text": "As the field of spoken language understanding becomes more mature, a clearer picture begins to emerge of the tradeoffs between rule-based and data-driven methods.", "labels": [], "entities": []}, {"text": "Other things being equal, there are many reasons to prefer data-driven approaches.", "labels": [], "entities": []}, {"text": "They are more robust, and reduce the heavy authoring costs associated with rule-based systems; methods are moreover starting to emerge which enable data-driven approaches to be used in areas which previously were thought to require rules, such as dialogue management.", "labels": [], "entities": [{"text": "dialogue management", "start_pos": 247, "end_pos": 266, "type": "TASK", "confidence": 0.764068990945816}]}, {"text": "A good overview of current work in this area is provided in).", "labels": [], "entities": []}, {"text": "Excellent as data-driven systems are, they have one obvious drawback: they require corpus data, usually in fairly substantial amounts.", "labels": [], "entities": []}, {"text": "Academics basically interested in pure research are free to work within a domain for which the data has already been collected, and for example can decide to use the Penn Treebank) or the ATIS corpus (.", "labels": [], "entities": [{"text": "Penn Treebank", "start_pos": 166, "end_pos": 179, "type": "DATASET", "confidence": 0.9945747256278992}, {"text": "ATIS corpus", "start_pos": 188, "end_pos": 199, "type": "DATASET", "confidence": 0.9861265122890472}]}, {"text": "If, on the other hand, the goal is to create a useful application fora specified new domain, there will in general belittle or no available data at the start of the project.", "labels": [], "entities": []}, {"text": "It is possible to create the data by using Wizard of Oz methods, or similar.", "labels": [], "entities": []}, {"text": "Wizard of Oz data collection is unattractive for many reasons: it is expensive and timeconsuming, and once the data has been collected it is not easy to change the coverage of the system.", "labels": [], "entities": [{"text": "Wizard of Oz data collection", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.582512229681015}]}, {"text": "For these reasons, commercial speech recognition platform vendors like Nuance and SpeechWorks have focussed on rule-based approaches, which allow rapid prototyping of systems from only very modest quantities of corpus data.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 30, "end_pos": 48, "type": "TASK", "confidence": 0.7241323441267014}]}, {"text": "Although most commercial rule-based spoken language dialogue systems use directed dialogue strategies and moderately simple recognition grammars, the literature now contains descriptions of several research systems built using rule-based methods, which successfully use mixed-initiative strategies and complex grammars).", "labels": [], "entities": []}, {"text": "If a project of this kind is developed over a substantial period of time, corpus material accumulates automatically as input to the system is logged.", "labels": [], "entities": []}, {"text": "The more corpus material there is, the stronger the reasons for moving towards datadriven processing; this will however only be easy if the architecture is originally setup to use statistics as well as rules.", "labels": [], "entities": []}, {"text": "Summarising the argument so far, we would like an architecture which combines rule-based and data-driven methods as transparently as possible.", "labels": [], "entities": []}, {"text": "This will allow us to shift smoothly from an initial version of the system which is entirely rule-based, to a final version which is largely data-driven.", "labels": [], "entities": []}, {"text": "In this paper, we will present a semantic interpretation architecture which conforms to the general model presented above.", "labels": [], "entities": [{"text": "semantic interpretation", "start_pos": 33, "end_pos": 56, "type": "TASK", "confidence": 0.6873237192630768}]}, {"text": "At the top level, semantic interpretation is viewed as a statistical classification task.", "labels": [], "entities": [{"text": "semantic interpretation", "start_pos": 18, "end_pos": 41, "type": "TASK", "confidence": 0.863936185836792}, {"text": "statistical classification task", "start_pos": 57, "end_pos": 88, "type": "TASK", "confidence": 0.742937038342158}]}, {"text": "An interpretation consists of a set of one or more semantic atoms.", "labels": [], "entities": []}, {"text": "Each utterance is associated with a set of features; some of these features are defined by hand-coded rules, and some by surface utterance characteristics like word Ngrams.", "labels": [], "entities": []}, {"text": "The available data is used to train statistics which evaluate each feature's reliability as a predictor of each semantic atom.", "labels": [], "entities": []}, {"text": "When only small amounts of data are used, most of the processing relies on rule-based features; as the size of the training corpus increases, the centre of gravity shifts more and more strongly towards the surface features.", "labels": [], "entities": []}, {"text": "The rest of the paper is structured as follows.", "labels": [], "entities": []}, {"text": "Section 2 describes the abstract architecture, and Section 3 a concrete realisation built on top of the Nuance Toolkit.", "labels": [], "entities": [{"text": "Nuance Toolkit", "start_pos": 104, "end_pos": 118, "type": "DATASET", "confidence": 0.8420242071151733}]}, {"text": "Section 4 gives details of experiments carried out on a medium-vocabulary command and control task from an instruction manual domain.", "labels": [], "entities": []}], "datasetContent": [{"text": "This section describes concrete experiments carried outwith ALTERF on CHECKLIST, a system related to the intelligent procedure assistant described in).", "labels": [], "entities": [{"text": "ALTERF", "start_pos": 60, "end_pos": 66, "type": "METRIC", "confidence": 0.9774617552757263}]}, {"text": "CHECKLIST, which is currently being evaluated for possible use in an astronautics domain, provides spoken dialogue support for carrying out complex procedures.", "labels": [], "entities": [{"text": "CHECKLIST", "start_pos": 0, "end_pos": 9, "type": "DATASET", "confidence": 0.9055042862892151}]}, {"text": "The most important commands cover navigation (\"go to the next line\", \"go to step fourteen\", \"go back two steps\", \"where am I\"), answering system questions (\"affirmative\", \"no\"), displaying pictures of objects used in the procedure (\"show me the wastewater bag\", \"where is the syringe\"), recording and playing voice notes (\"put a voice note on that step\", \"play the voice note for step ten\") and setting of voice alarms (\"set alarm for ten minutes from now\", \"cancel alarm\").", "labels": [], "entities": [{"text": "cancel alarm", "start_pos": 459, "end_pos": 471, "type": "METRIC", "confidence": 0.9390775263309479}]}, {"text": "There are also a number of other less important functionalities.", "labels": [], "entities": []}, {"text": "The semantic representation language currently contains a total of 46 different semantic atoms, including the generic atoms (cf. Section 3.1) *numb er * and *time* Some examples of utterances and their associated semantic representations are shown at the beginning of Section 2.", "labels": [], "entities": []}, {"text": "The speech understanding component is implemented on top of the Nuance platform; the recognition package is compiled from a unification grammar description using the REG-ULUS tool).", "labels": [], "entities": [{"text": "speech understanding", "start_pos": 4, "end_pos": 24, "type": "TASK", "confidence": 0.7796807587146759}]}, {"text": "An example of a parse representation produced by this grammar appears in Section 3.1.", "labels": [], "entities": []}, {"text": "The grammar contains 129 rules and 258 lexical items, and the compiled recogniser achieves a word error rate of approximately 19% on unseen in-domain test data using our normal software and hardware configuration.", "labels": [], "entities": [{"text": "word error rate", "start_pos": 93, "end_pos": 108, "type": "METRIC", "confidence": 0.7588055729866028}]}, {"text": "Use of a grammar-based language model implies that all utterances recognised by the system are within the coverage of the grammar.", "labels": [], "entities": []}, {"text": "At the beginning of the current phase of the project, we recorded 1302 utterances (5540 words) of speech data, using an ad hoc data collection methodology loosely based on two interviews with potential users and a short videotape of a session with a mock-up of the system; tight time constrains and lack of access to users made it difficult to do better than this.", "labels": [], "entities": []}, {"text": "We transcribed and annotated the data using a simple Java-based tool, randomly selecting 75% of it for use in training and keeping the rest for testing.", "labels": [], "entities": []}, {"text": "During the course of the project, we routinely logged speech interactions with the system, and transcribed and an-notated a further 424 utterances (906 words) of speech data.", "labels": [], "entities": []}, {"text": "75% of this was again assigned to training and the rest saved for testing.", "labels": [], "entities": []}, {"text": "The recogniser grammar was developed using only the training portion of the corpus.", "labels": [], "entities": [{"text": "recogniser grammar", "start_pos": 4, "end_pos": 22, "type": "TASK", "confidence": 0.9110628366470337}]}, {"text": "With regard to the experiments themselves, we were primarily interested in the quality of semantic classification in the ALTERF semantic interpretation module, defined as the proportion of the indomain utterances in the test set which were assigned a correct set of semantic atoms.", "labels": [], "entities": [{"text": "semantic classification", "start_pos": 90, "end_pos": 113, "type": "TASK", "confidence": 0.7495677471160889}, {"text": "ALTERF semantic interpretation", "start_pos": 121, "end_pos": 151, "type": "TASK", "confidence": 0.5886659324169159}]}, {"text": "We investigated how the semantic classification error rate was affected by the following factors: \u2022 Use of rule based features only, N-gram based features only, or both rule and N-gram based features.", "labels": [], "entities": [{"text": "semantic classification error rate", "start_pos": 24, "end_pos": 58, "type": "TASK", "confidence": 0.7079574540257454}]}, {"text": "\u2022 Quantity of training data.", "labels": [], "entities": []}, {"text": "\u2022 Modality (text or speech) of training data.", "labels": [], "entities": []}, {"text": "We randomly divided the training corpus into ten equal pieces, and trained on subsets ranging from 10% of the corpus to the full corpus in both text and speech modes.", "labels": [], "entities": []}, {"text": "We then evaluated semantic classification performance on the in-domain portion of the test data using either rules, N-gram based statistics or a combination of the two.", "labels": [], "entities": [{"text": "semantic classification", "start_pos": 18, "end_pos": 41, "type": "TASK", "confidence": 0.804962545633316}]}, {"text": "When running in speech mode, we set the Nuance rejection threshold to zero and the beam width to 1200, which on the 1.9 MHz processor we were using gave recognition processing speeds typically around 0.5 times real time.", "labels": [], "entities": [{"text": "Nuance rejection threshold", "start_pos": 40, "end_pos": 66, "type": "METRIC", "confidence": 0.9048178195953369}]}, {"text": "presents the results of the first set of experiments, using training data in speech form.", "labels": [], "entities": []}, {"text": "We see, not surprisingly, that for small amount of training data the rule-based version of the system is greatly superior to the N-gram based one.", "labels": [], "entities": []}, {"text": "For larger amounts of training data, however, the N-gram version and in particular the combined version start to overtake the pure rule-based system.", "labels": [], "entities": []}, {"text": "When all the training data is used, the combined system outperforms the rule-based system by 22.2% to 27.3% (19% relative), and outperforms the N-gram system by 22.2% to 25.6% (12% relative: Percentage semantic interpretation errors on in-domain test data for different amounts of training data and different versions of the system.", "labels": [], "entities": []}, {"text": "Training and test data both in speech form.", "labels": [], "entities": []}, {"text": "\"Data\" = proportion of training data used; \"Rules\" = system uses rules only; \"NGrams\" = N-grams only; \"Both\" = both rules and N-grams. that there are 29 utterances in the test set where the results for the combined and rule-based versions differ, split 22-7 in favour of the combined version.", "labels": [], "entities": []}, {"text": "This is significant at p < 0.01 according to the McNemar sign test.", "labels": [], "entities": [{"text": "McNemar sign test", "start_pos": 49, "end_pos": 66, "type": "METRIC", "confidence": 0.5508838097254435}]}, {"text": "Although the difference between the N-gram and combined versions is smaller, it is more one-sided (10-0), and is also significant at p < 0.01.", "labels": [], "entities": []}, {"text": "We could see two possible causal mechanisms to account for the improvement in the combined system compared to the pure rule-based one.", "labels": [], "entities": []}, {"text": "The obvious explanation is that the N-gram based discriminants are filling in holes in the rule-set; more subtly, they could be learning characteristic mistakes made by the recogniser and correcting them.", "labels": [], "entities": []}, {"text": "In order to separate these two effects, presents the results of the same experiments run with the training data in text mode.", "labels": [], "entities": []}, {"text": "Since performance of the N-gram and combined versions only degrades a little, we conclude that the second factor (learning to correct recogniser errors) is the less important one.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Percentage semantic interpretation errors  on in-domain test data for different amounts of  training data and different versions of the system.  Training and test data both in speech form. \"Data\"  = proportion of training data used; \"Rules\" = sys- tem uses rules only; \"NGrams\" = N-grams only;  \"Both\" = both rules and N-grams.", "labels": [], "entities": []}, {"text": " Table 2: As Table 1, but with training data in text  form.  first effect is the more important one.", "labels": [], "entities": []}]}