{"title": [{"text": "Bootstrapping Statistical Parsers from Small Datasets", "labels": [], "entities": []}], "abstractContent": [{"text": "We present a practical co-training method for bootstrapping statistical parsers using a small amount of manually parsed training material and a much larger pool of raw sentences.", "labels": [], "entities": [{"text": "bootstrapping statistical parsers", "start_pos": 46, "end_pos": 79, "type": "TASK", "confidence": 0.7652411262194315}]}, {"text": "Experimental results show that unlabelled sentences can be used to improve the performance of statistical parsers.", "labels": [], "entities": []}, {"text": "In addition , we consider the problem of boot-strapping parsers when the manually parsed training material is in a different domain to either the raw sentences or the testing material.", "labels": [], "entities": []}, {"text": "We show that boot-strapping continues to be useful, even though no manually produced parses from the target domain are used.", "labels": [], "entities": []}], "introductionContent": [{"text": "In this paper we describe how co-training can be used to bootstrap a pair of statistical parsers from a small amount of annotated training data.", "labels": [], "entities": []}, {"text": "Co-training is a wealdy supervised learning algorithm in which two (or more) learners are iteratively retrained on each other's output.", "labels": [], "entities": []}, {"text": "It has been applied to problems such as word-sense disambiguation, web-page classification) and named-entity recognition).", "labels": [], "entities": [{"text": "word-sense disambiguation", "start_pos": 40, "end_pos": 65, "type": "TASK", "confidence": 0.7757848799228668}, {"text": "web-page classification", "start_pos": 67, "end_pos": 90, "type": "TASK", "confidence": 0.6908152550458908}, {"text": "named-entity recognition", "start_pos": 96, "end_pos": 120, "type": "TASK", "confidence": 0.7802043259143829}]}, {"text": "However, these tasks typically involved a small set of labels (around 2-3) and a relatively small parameter space.", "labels": [], "entities": []}, {"text": "It is therefore instructive to consider co-training for more complex models.", "labels": [], "entities": []}, {"text": "Compared to these earlier models, a statistical parser has a larger parameter space, and instead of class labels, it produces recursively built parse trees as output.", "labels": [], "entities": []}, {"text": "Previous work in co-training statistical parsers) used two components of a single parsing framework (that is, a parser and a supertagger for that parser).", "labels": [], "entities": []}, {"text": "In contrast, this paper considers co-training two diverse statistical parsers: the Collins lexicalized PCFG parser and a Lexicalized Tree Adjoining Grammar (LTAG) parser.", "labels": [], "entities": [{"text": "Collins lexicalized PCFG parser", "start_pos": 83, "end_pos": 114, "type": "DATASET", "confidence": 0.8549181818962097}]}, {"text": "Section 2 reviews co-training theory.", "labels": [], "entities": []}, {"text": "Section 3 considers how co-training applied to training statistical parsers can be made computationally viable.", "labels": [], "entities": []}, {"text": "In Section 4 we show that co-training outperforms self-training, and that co-training is most beneficial when the seed set of manually created parses is small.", "labels": [], "entities": []}, {"text": "Section 4.4 shows that co-training is possible even when the set of initially labelled data is drawn from a different distribution to either the unlabelled training material or the test set; that is, we show that co-training can help in porting a parser from one genre to another.", "labels": [], "entities": []}, {"text": "Finally, section 5 reports summary results of our experiments.", "labels": [], "entities": []}], "datasetContent": [{"text": "In order to conduct co-training experiments between statistical parsers, it was necessary to choose two parsers that generate comparable output but use different statistical models.", "labels": [], "entities": []}, {"text": "We therefore chose the following parsers: 1.", "labels": [], "entities": []}, {"text": "The Collins lexicalized PCFG parser, model 2.", "labels": [], "entities": [{"text": "Collins lexicalized PCFG parser", "start_pos": 4, "end_pos": 35, "type": "DATASET", "confidence": 0.905041053891182}]}, {"text": "Some code for (re)training this parser was added to make the co-training experiments possible.", "labels": [], "entities": []}, {"text": "We refer to this parser as Collins-CFG.", "labels": [], "entities": [{"text": "Collins-CFG", "start_pos": 27, "end_pos": 38, "type": "DATASET", "confidence": 0.9807060956954956}]}, {"text": "2. The Lexicalized Tree Adjoining Grammar (LTAG) parser of Sarkar, which we refer to as the LTAG parser.", "labels": [], "entities": []}, {"text": "In order to perform the co-training experiments reported in this paper, LTAG derivation events Collins-CFG LTAG Bi-lexical dependencies are between lexicalized nonterminals Bi-lexical dependencies are between elementary trees Can produce novel elementary trees for the LTAG parser Can produce novel bi-lexical dependencies for Collins-CFG When using small amounts of seed data, abstains less often than LTAG When using small amounts of seed data, abstains more often than Collins-CFG . Summary of the different views given by the Collins-CFG parser and the LTAG parser were extracted from the head-lexicalized parse tree output produced by the Collins-CFG parser.", "labels": [], "entities": []}, {"text": "These events were used to retrain the statistical model used in the LTAG parser.", "labels": [], "entities": [{"text": "LTAG parser", "start_pos": 68, "end_pos": 79, "type": "DATASET", "confidence": 0.9054654240608215}]}, {"text": "The output of the LTAG parser was also modified in order to provide input for the re-training phase in the Collins-CFG parser.", "labels": [], "entities": [{"text": "LTAG", "start_pos": 18, "end_pos": 22, "type": "DATASET", "confidence": 0.8784167170524597}, {"text": "Collins-CFG parser", "start_pos": 107, "end_pos": 125, "type": "DATASET", "confidence": 0.945318341255188}]}, {"text": "These steps ensured that the output of the Collins-CFG parser could be used as new labelled data to re-train the LTAG parser and vice versa.", "labels": [], "entities": [{"text": "Collins-CFG", "start_pos": 43, "end_pos": 54, "type": "DATASET", "confidence": 0.9789055585861206}, {"text": "LTAG parser", "start_pos": 113, "end_pos": 124, "type": "DATASET", "confidence": 0.9207324087619781}]}, {"text": "The domains over which the two models operate are quite distinct.", "labels": [], "entities": []}, {"text": "The LTAG model uses tree fragments of the final parse tree and combines them together, while the Collins-CFG model operates on a much smaller domain of individual lexicalized non-terminals.", "labels": [], "entities": []}, {"text": "This provides a mechanism to bootstrap information between these two models when they are applied to unlabelled data.", "labels": [], "entities": []}, {"text": "LTAG can provide a larger domain over which bi-lexical information is defined due to the arbitrary depth of the elementary trees it uses, and hence can provide novel lexical relationships for the Collins-CFG model, while the Collins-CFG model can paste together novel elementary trees for the LTAG model.", "labels": [], "entities": [{"text": "LTAG", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.8514927625656128}]}, {"text": "A summary of the differences between the two models is given in, which provides an informal argument for why the two parsers provide contrastive views for the co-training experiments.", "labels": [], "entities": []}, {"text": "Of course there is still the question of whether the two parsers really are independent enough for effective co-training to be possible; in the results section we show that the Collins-CFG parser is able to learn useful information from the output of the LTAG parser.", "labels": [], "entities": [{"text": "LTAG parser", "start_pos": 255, "end_pos": 266, "type": "DATASET", "confidence": 0.8868532776832581}]}, {"text": "shows how the performance of the Collins-CFG parser varies as the amount of manually annotated training data (from the Wall Street Journal (WSJ) Penn Treebank () is increased.", "labels": [], "entities": [{"text": "Collins-CFG", "start_pos": 33, "end_pos": 44, "type": "DATASET", "confidence": 0.9328039288520813}, {"text": "Wall Street Journal (WSJ) Penn Treebank", "start_pos": 119, "end_pos": 158, "type": "DATASET", "confidence": 0.9241423606872559}]}, {"text": "The graph shows a rapid growth inaccuracy which tails off as increasing amounts of training data are added.", "labels": [], "entities": []}, {"text": "The learning curve shows that the maximum payoff from co-training is likely to occur between 500 and 1,000 sentences.", "labels": [], "entities": []}, {"text": "Therefore we used two sizes of seed data: 500 and 1,000 sentences, to see if cotraining could improve parser performance using these small amounts of labelled seed data.", "labels": [], "entities": []}, {"text": "For reference, shows a similar curve for the LTAG parser.", "labels": [], "entities": [{"text": "LTAG parser", "start_pos": 45, "end_pos": 56, "type": "DATASET", "confidence": 0.9225137829780579}]}, {"text": "Each parser was first initialized with some labelled seed data from the standard training split (sections 2 to 21) of the WSJ Penn Treebank.", "labels": [], "entities": [{"text": "WSJ Penn Treebank", "start_pos": 122, "end_pos": 139, "type": "DATASET", "confidence": 0.9083682497342428}]}, {"text": "Evaluation was in terms of, using a balanced F-score over labelled constituents from section 0 of the Treebank.", "labels": [], "entities": [{"text": "F-score", "start_pos": 45, "end_pos": 52, "type": "METRIC", "confidence": 0.9964184761047363}, {"text": "section 0 of the Treebank", "start_pos": 85, "end_pos": 110, "type": "DATASET", "confidence": 0.7452467441558838}]}, {"text": "I The Fscore values are reported for each iteration of cotraining on the development set (section 0 of the Treebank).", "labels": [], "entities": [{"text": "Fscore", "start_pos": 6, "end_pos": 12, "type": "METRIC", "confidence": 0.9917599558830261}]}, {"text": "Since we need to parse all sentences in section 0 at each iteration, in the experiments reported in this paper we only evaluated one of the parsers, the Collins-CFG parser, at each iteration.", "labels": [], "entities": []}, {"text": "All results we mention (unless stated otherwise) are F-scores for the Collins-CFG parser.", "labels": [], "entities": [{"text": "F-scores", "start_pos": 53, "end_pos": 61, "type": "METRIC", "confidence": 0.9974811673164368}, {"text": "Collins-CFG parser", "start_pos": 70, "end_pos": 88, "type": "DATASET", "confidence": 0.9554874300956726}]}, {"text": "Self-training experiments were conducted in which each parser was retrained on its own output.", "labels": [], "entities": []}, {"text": "Self-training provides a useful comparison with co-training because any difference in the results indicates how much the parsers are benefiting from being trained on the output of another parser.", "labels": [], "entities": []}, {"text": "This experiment also gives us some insight into the differences between the two parsing models.", "labels": [], "entities": []}, {"text": "Self-training was used by, where a modest gain was reported after re-training his parser on 30 million words.", "labels": [], "entities": []}, {"text": "The results are shown in.", "labels": [], "entities": []}, {"text": "Here, both parsers were initialised with the first 500 sentences from the standard training split (sections 2 to 21) of the WSJ Penn Treebank.", "labels": [], "entities": [{"text": "WSJ Penn Treebank", "start_pos": 124, "end_pos": 141, "type": "DATASET", "confidence": 0.9059859315554301}]}, {"text": "Subsequent unlabelled  sentences were also drawn from this split.", "labels": [], "entities": []}, {"text": "During each round of self-training, 30 sentences were parsed by each parser, and each parser was retrained upon the 20 self-labelled sentences which it scored most highly (each parser using its own joint probability (equation 1) as the score).", "labels": [], "entities": []}, {"text": "The results vary significantly between the Collins-CFG and the LTAG parser, which lends weight to the argument that the two parsers are largely independent of each other.", "labels": [], "entities": [{"text": "Collins-CFG", "start_pos": 43, "end_pos": 54, "type": "DATASET", "confidence": 0.9556281566619873}, {"text": "LTAG", "start_pos": 63, "end_pos": 67, "type": "DATASET", "confidence": 0.9265782237052917}]}, {"text": "It also shows that, at least for the Collins-CFG model, a minor improvement in performance can be had from selftraining.", "labels": [], "entities": [{"text": "Collins-CFG", "start_pos": 37, "end_pos": 48, "type": "DATASET", "confidence": 0.9052400588989258}]}, {"text": "The LTAG parser, by contrast, is hurt by self-training  The first co-training experiment used the first 500 sentences from sections 2-21 of the Treebank as seed data, and subsequent unlabelled sentences were drawn from the remainder of these sections.", "labels": [], "entities": [{"text": "the Treebank", "start_pos": 140, "end_pos": 152, "type": "DATASET", "confidence": 0.672677606344223}]}, {"text": "During each co-training round, the LTAG parser parsed 30 sentences, and the 20 labelled sentences with the highest scores (according to the LTAG joint probability) were added to the training data of the Collins-CFG parser.", "labels": [], "entities": [{"text": "LTAG", "start_pos": 35, "end_pos": 39, "type": "DATASET", "confidence": 0.9299864768981934}, {"text": "LTAG joint probability", "start_pos": 140, "end_pos": 162, "type": "METRIC", "confidence": 0.6393451790014902}, {"text": "Collins-CFG parser", "start_pos": 203, "end_pos": 221, "type": "DATASET", "confidence": 0.9437172710895538}]}, {"text": "The training data of the LTAG parser was augmented in the same way, using the 20 highest scoring parses from the set of 30, but using the Collins-CFG parser to label the sentences and provide the joint probability for scoring.", "labels": [], "entities": [{"text": "LTAG", "start_pos": 25, "end_pos": 29, "type": "DATASET", "confidence": 0.864086925983429}]}, {"text": "gives the results for the Collins-CFG parser, and also shows the self-training curve for The effect of seed size: The effect of varying seed size on COtraining.", "labels": [], "entities": [{"text": "Collins-CFG parser", "start_pos": 26, "end_pos": 44, "type": "DATASET", "confidence": 0.9171723127365112}]}, {"text": "The upper curve is for 1,000 sentences labelled seed data; the lower curve is for 500 sentences.", "labels": [], "entities": []}, {"text": "comparison.2 The graph shows that co-training results in higher performance than self-training.", "labels": [], "entities": []}, {"text": "The graph also shows that co-training performance levels out after around 80 rounds, and then starts to degrade.", "labels": [], "entities": []}, {"text": "The likely reason for this dip is noise in the parse trees added by cotraining.", "labels": [], "entities": []}, {"text": "Pierce and Cardie (2001) noted a similar behaviour when they co-trained shallow parsers.", "labels": [], "entities": []}, {"text": "Figures 6, 7 and 8 report the performance of the Collins-CFG parser.", "labels": [], "entities": [{"text": "Collins-CFG parser", "start_pos": 49, "end_pos": 67, "type": "DATASET", "confidence": 0.9421128630638123}]}, {"text": "We do not report the LTAG parser performance in this paper as evaluating it at the end of each co-training round was too time consuming.", "labels": [], "entities": [{"text": "LTAG parser", "start_pos": 21, "end_pos": 32, "type": "TASK", "confidence": 0.5532877743244171}]}, {"text": "We did track LTAG performance on a subset of the WSJ Section 0 and can confirm that LTAG performance also improves as a result of co-training.", "labels": [], "entities": [{"text": "WSJ Section 0", "start_pos": 49, "end_pos": 62, "type": "DATASET", "confidence": 0.9486341675122579}]}, {"text": "The second co-training experiment was the same as the first, except that more seed data was used: the first 1,000 sentences from sections 2-21 of the Treebank.", "labels": [], "entities": [{"text": "the Treebank", "start_pos": 146, "end_pos": 158, "type": "DATASET", "confidence": 0.8914169669151306}]}, {"text": "gives the results, and, for comparison, also shows the previous performance curve for the 500 seed set experiment.", "labels": [], "entities": [{"text": "500 seed set experiment", "start_pos": 90, "end_pos": 113, "type": "DATASET", "confidence": 0.8275930732488632}]}, {"text": "The key observation is that the benefit of co-training is greater when the amount of seed material is small.", "labels": [], "entities": []}, {"text": "Our hypothesis is that, when there is a paucity of initial seed data, coverage is a major obstacle that co-training can address.", "labels": [], "entities": []}, {"text": "As the amount of seed data increases, coverage becomes less of a problem, and the co-training advantage is diminished.", "labels": [], "entities": [{"text": "coverage", "start_pos": 38, "end_pos": 46, "type": "METRIC", "confidence": 0.9846758246421814}]}, {"text": "This means that, when most sentences in the testing set can be parsed, subsequent changes in performance come from better parameter estimates.", "labels": [], "entities": []}, {"text": "Although co-training boosts the performance of the parser using the 500 seed sentences from 75% to 77.8% (the performance level after 100 rounds of co-training), it does not achieve the level of performance of a parser trained on 1,000 seed sentences.", "labels": [], "entities": []}, {"text": "Some possible explanations are: that the newly labelled sentences are not reliable (i.e., they contain too many errors); that the sentences deemed reliable are not informative training examples; or a combination of both factors.", "labels": [], "entities": []}, {"text": "This experiment examines whether co-training can be used to boost performance when the un-labelled data are taken from a different source than the initial seed data.", "labels": [], "entities": []}, {"text": "Previous experiments in have shown that porting a statistical parser from a source genre to a target genre is a non-trivial task.", "labels": [], "entities": []}, {"text": "Our two different sources were the parsed section of the Brown corpus and the Penn Treebank WSJ.", "labels": [], "entities": [{"text": "Brown corpus", "start_pos": 57, "end_pos": 69, "type": "DATASET", "confidence": 0.8568634390830994}, {"text": "Penn Treebank WSJ", "start_pos": 78, "end_pos": 95, "type": "DATASET", "confidence": 0.9645710190137228}]}, {"text": "Unlike the WSJ, the Brown corpus does not contain newswire material, and so the two sources differ from each other in terms of vocabulary and syntactic constructs.", "labels": [], "entities": [{"text": "WSJ", "start_pos": 11, "end_pos": 14, "type": "DATASET", "confidence": 0.9383265972137451}, {"text": "Brown corpus", "start_pos": 20, "end_pos": 32, "type": "DATASET", "confidence": 0.9317589104175568}]}, {"text": "1,000 annotated sentences from the Brown section of the Penn Treebank were used as the seed data.", "labels": [], "entities": [{"text": "Brown section of the Penn Treebank", "start_pos": 35, "end_pos": 69, "type": "DATASET", "confidence": 0.9347130556901296}]}, {"text": "Co-training then proceeds using the WSJ.", "labels": [], "entities": [{"text": "WSJ", "start_pos": 36, "end_pos": 39, "type": "DATASET", "confidence": 0.9176813960075378}]}, {"text": "Note that no manually created parses in the WSJ domain are used by the parser, even though it is evaluated using WSJ material.", "labels": [], "entities": [{"text": "WSJ domain", "start_pos": 44, "end_pos": 54, "type": "DATASET", "confidence": 0.8585174977779388}]}, {"text": "In, the lower curve shows performance for the Collins-CFG parser (again evaluated on section 0).", "labels": [], "entities": [{"text": "Collins-CFG", "start_pos": 46, "end_pos": 57, "type": "DATASET", "confidence": 0.929021418094635}]}, {"text": "The difference in corpus domain does not hinder cotraining.", "labels": [], "entities": []}, {"text": "The parser performance is boosted from 75% to 77.3%.", "labels": [], "entities": []}, {"text": "Note that most of the improvement is within the first 5 iterations.", "labels": [], "entities": []}, {"text": "This suggests that the parsing model maybe adapting to the vocabulary of the new domain.", "labels": [], "entities": []}, {"text": "We also conducted an experiment in which the initial seed data was supplemented with a tiny amount of annotated data (100 manually annotated WSJ sentences) from the domain of the unlabelled data.", "labels": [], "entities": []}, {"text": "This experiment simulates the situation where there is only a very limited amount of labelled material in the novel domain.", "labels": [], "entities": []}, {"text": "The upper curve in shows the outcome of this experiment.", "labels": [], "entities": []}, {"text": "Not surprisingly, the 100 additional labelled WSJ sentences improved the initial performance of the parser (to 76.7%).", "labels": [], "entities": []}, {"text": "While the amount of improvement in performance is less than the previous case, co-training provides an additional boost to the parsing performance, to 78.7%.", "labels": [], "entities": [{"text": "parsing", "start_pos": 127, "end_pos": 134, "type": "TASK", "confidence": 0.9808194041252136}]}, {"text": "The various experiments are summarised in Table 1.", "labels": [], "entities": []}, {"text": "As is customary in the statistical parsing literature, we view all our previous experiments using section 0 of the Penn Treebank WSJ as contributing towards development.", "labels": [], "entities": [{"text": "statistical parsing", "start_pos": 23, "end_pos": 42, "type": "TASK", "confidence": 0.7952427566051483}, {"text": "Penn Treebank WSJ", "start_pos": 115, "end_pos": 132, "type": "DATASET", "confidence": 0.9549540678660074}]}, {"text": "Here we report on system performance on unseen material (namely The Brown corpus was chosen as the seed data and the WSJ as the unlabelled data for convenience.", "labels": [], "entities": [{"text": "Brown corpus", "start_pos": 68, "end_pos": 80, "type": "DATASET", "confidence": 0.9403960704803467}, {"text": "WSJ", "start_pos": 117, "end_pos": 120, "type": "DATASET", "confidence": 0.9546324014663696}]}, {"text": "Before The results show a modest improvement under each co-training scenario, indicating that, for the Collins-CFG parser, there is useful information to be had from the output of the LTAG parser.", "labels": [], "entities": [{"text": "Collins-CFG", "start_pos": 103, "end_pos": 114, "type": "DATASET", "confidence": 0.9354797601699829}]}, {"text": "However, the results are not as dramatic as those reported in other co-training papers, such as for web-page classification and Collins and Singer (1999) for namedentity recognition.", "labels": [], "entities": [{"text": "web-page classification", "start_pos": 100, "end_pos": 123, "type": "TASK", "confidence": 0.6399567723274231}, {"text": "namedentity recognition", "start_pos": 158, "end_pos": 181, "type": "TASK", "confidence": 0.7108612954616547}]}, {"text": "A possible reason is that parsing is a much harder task than these problems.", "labels": [], "entities": [{"text": "parsing", "start_pos": 26, "end_pos": 33, "type": "TASK", "confidence": 0.9877154231071472}]}, {"text": "An open question is whether co-training can produce results that improve upon the state-of-theart in statistical parsing.", "labels": [], "entities": [{"text": "statistical parsing", "start_pos": 101, "end_pos": 120, "type": "TASK", "confidence": 0.7874651849269867}]}, {"text": "Investigation of the convergence curves as the parsers are trained upon more and more manually-created treebank material suggests that, with the Penn Treebank, the Collins-CFG parser has nearly converged already.", "labels": [], "entities": [{"text": "Penn Treebank", "start_pos": 145, "end_pos": 158, "type": "DATASET", "confidence": 0.9946608245372772}, {"text": "Collins-CFG", "start_pos": 164, "end_pos": 175, "type": "DATASET", "confidence": 0.9438167214393616}]}, {"text": "Given 40,000 sentences of labelled data, we can obtain a projected value of how much performance can be improved with additional reliably labelled data.", "labels": [], "entities": []}, {"text": "This projected value was obtained by fitting a curve to the observed convergence results using a least-squares method from MAT LAB.", "labels": [], "entities": [{"text": "MAT LAB", "start_pos": 123, "end_pos": 130, "type": "DATASET", "confidence": 0.9015150964260101}]}, {"text": "When training data is projected to a size of 400K manually created Treebank sentences, the performance of the Collins-CFG parser is projected to be 89.2% with an absolute upper bound of 89.3%.", "labels": [], "entities": [{"text": "Collins-CFG", "start_pos": 110, "end_pos": 121, "type": "DATASET", "confidence": 0.9426212310791016}]}, {"text": "This suggests that there is very little room for performance improvement for the Collins-CFG parser by simply adding more labelled data (using co-training or other bootstrapping methods or even manually).", "labels": [], "entities": [{"text": "Collins-CFG", "start_pos": 81, "end_pos": 92, "type": "DATASET", "confidence": 0.9591553807258606}]}, {"text": "However, models whose parameters have not already converged might benefit from co-training For instance, when training data is projected to a size of 400K manually created Treebank sentences, the performance of the LTAG statistical parser would be 90.4% with an absolute upper bound of 91.6%.", "labels": [], "entities": []}, {"text": "Thus, a bootstrapping method might improve performance of the LTAG statistical parser beyond the current state-of-the-art performance on the Treebank.", "labels": [], "entities": [{"text": "LTAG statistical parser", "start_pos": 62, "end_pos": 85, "type": "DATASET", "confidence": 0.8656149903933207}]}], "tableCaptions": [{"text": " Table 1: Results on section 23 for the Collins-CFG  parser after co-training with the LTAG parser", "labels": [], "entities": [{"text": "Collins-CFG", "start_pos": 40, "end_pos": 51, "type": "DATASET", "confidence": 0.9549250602722168}, {"text": "LTAG parser", "start_pos": 87, "end_pos": 98, "type": "DATASET", "confidence": 0.8338092863559723}]}]}