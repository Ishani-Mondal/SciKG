{"title": [{"text": "Learning PP attachment for filtering prosodic phrasing Antal van den Bosch and ErwinNIarsi", "labels": [], "entities": []}], "abstractContent": [{"text": "We explore learning prepositional-phrase attachment in Dutch, to use it as a filter in prosodic phrasing.", "labels": [], "entities": [{"text": "learning prepositional-phrase attachment", "start_pos": 11, "end_pos": 51, "type": "TASK", "confidence": 0.6345112423102061}]}, {"text": "From a syntactic treebank of spoken Dutch we extract instances of the attachment of prepositional phrases to either a governing verb or noun.", "labels": [], "entities": []}, {"text": "Using cross-validated parameter and feature selection, we train two learning algorithms, TB I and RIPPER, 011 making this distinction, based on unigram and bigram lexical features and a cooccurrence feature derived from WWW counts.", "labels": [], "entities": [{"text": "RIPPER", "start_pos": 98, "end_pos": 104, "type": "METRIC", "confidence": 0.9396198987960815}]}, {"text": "We optimize the learning on noun attachment, since in a second stage we use the attachment decision for blocking the incorrect placement of phrase boundaries before prepositional phrases attached to the preceding noun.", "labels": [], "entities": [{"text": "noun attachment", "start_pos": 28, "end_pos": 43, "type": "TASK", "confidence": 0.8044043183326721}]}, {"text": "On noun attachment, IB 1 attains an F-score of 82; RIPPER an F-score of 78.", "labels": [], "entities": [{"text": "noun attachment", "start_pos": 3, "end_pos": 18, "type": "TASK", "confidence": 0.8205808997154236}, {"text": "IB 1", "start_pos": 20, "end_pos": 24, "type": "METRIC", "confidence": 0.9663443267345428}, {"text": "F-score", "start_pos": 36, "end_pos": 43, "type": "METRIC", "confidence": 0.9995860457420349}, {"text": "RIPPER", "start_pos": 51, "end_pos": 57, "type": "METRIC", "confidence": 0.9974416494369507}, {"text": "F-score", "start_pos": 61, "end_pos": 68, "type": "METRIC", "confidence": 0.9977824091911316}]}, {"text": "When used as a filter for prosodic phrasing, using attachment decisions from IB 1 yields the best improvement on precision (by six points to 71) on phrase boundary placement.", "labels": [], "entities": [{"text": "precision", "start_pos": 113, "end_pos": 122, "type": "METRIC", "confidence": 0.9992886781692505}, {"text": "phrase boundary placement", "start_pos": 148, "end_pos": 173, "type": "TASK", "confidence": 0.6554057399431864}]}], "introductionContent": [{"text": "One of the factors determining the acceptability of synthetic speech is the appropriate placement of phrase boundaries, realized typically and most audibly by pauses.", "labels": [], "entities": []}, {"text": "Incorrect prosodic phrasing may impede the listener in the correct understanding of the spoken utterance.", "labels": [], "entities": []}, {"text": "A major factor causing difficulties inappropriate phrase boundary placement is the lack of reliable information about syntactic structure.", "labels": [], "entities": [{"text": "phrase boundary placement", "start_pos": 50, "end_pos": 75, "type": "TASK", "confidence": 0.694642037153244}]}, {"text": "Even if there is no one-to-one mapping between syntax and prosody, the placement of prosodic phrase boundaries is nevertheless dependent on syntactic information).", "labels": [], "entities": []}, {"text": "To cope with this lack of syntactic information that a speech synthesis developer may face currently, e.g. in the absence of a reliable parser, several strategies have been applied to allocate phrase boundaries.", "labels": [], "entities": []}, {"text": "One strategy is to allocate phrase boundaries on the basis of punctuation only.", "labels": [], "entities": []}, {"text": "In general, however, this results in too few phrase boundaries (and some incorrect ones, e.g. in enumerations).", "labels": [], "entities": []}, {"text": "A clear example of information about syntactic structure being useful for placing phrase boundaries is the attachment of prepositional phrases (PPs).", "labels": [], "entities": [{"text": "attachment of prepositional phrases (PPs)", "start_pos": 107, "end_pos": 148, "type": "TASK", "confidence": 0.8407764860561916}]}, {"text": "When a PP is attached to the preceding NP or PP (henceforth referred to as noun attachment), such as in the structure ...", "labels": [], "entities": [{"text": "noun attachment", "start_pos": 75, "end_pos": 90, "type": "TASK", "confidence": 0.6969076842069626}]}, {"text": "eats pizza with anchovies, a phrase boundary between pizza and with is usually considered inappropriate.", "labels": [], "entities": []}, {"text": "However, when a PP is attached to the verb in the clause (verb attachment), as in the structure ...", "labels": [], "entities": []}, {"text": "eats pizza with a fork, an intervening phrase boundary between the PP and its preceding NP or PP (between pizza and with) is optional, and when placed, usually judged appropriate ().", "labels": [], "entities": []}, {"text": "Deciding about noun versus verb attachment of PPs is a known hard task in parsing, since it is un-derstood to involve knowing lexical preferences, verb subcategorization, fixed phrases, but also semantic and pragmatic \"world\" knowledge.", "labels": [], "entities": [{"text": "Deciding about noun versus verb attachment of PPs", "start_pos": 0, "end_pos": 49, "type": "TASK", "confidence": 0.6726801693439484}]}, {"text": "A typical current parser (e.g., statistical parsers such as) interleaves PP attachment with all its other disambiguation tasks.", "labels": [], "entities": [{"text": "PP attachment", "start_pos": 73, "end_pos": 86, "type": "TASK", "confidence": 0.7710262537002563}]}, {"text": "However, because of its interesting complexity, a line of work has concentrated on studying the task in isolation).", "labels": [], "entities": []}, {"text": "Our study can be seen as following these lines of isolation studies, pursuing the same process for another language, Dutch.", "labels": [], "entities": []}, {"text": "At present there are no parsers available for Dutch that disambiguate PP attachment, which leaves the comparison between PP attachment as an embedded subtask of a full parser with our approach as future work.", "labels": [], "entities": []}, {"text": "In line with these earlier studies, we assume that at least two sources of information should be used as features in training data: (i) lexical features (e.g. unigrams and bigrams of head words), and (ii) word cooccurrence strength values (the probability that two words occur together, within some defined vicinity).", "labels": [], "entities": []}, {"text": "Lexical features maybe informative when certain individual words or bigrams frequently, or exclusively, occur with either noun or verb attachment.", "labels": [], "entities": []}, {"text": "This may hold for prepositions, but also heads of the involved phrases, as well as for combinations of these words.", "labels": [], "entities": []}, {"text": "Cooccurrence strength values may provide additional clues to informational ties among words; when we investigate the cooccurrences of nouns and prepositions, and of verbs and prepositions, the cooccurrence strength value could also indicate whether the prepositional phrase is attached to the noun or to the verb in the syntactic tree.", "labels": [], "entities": []}, {"text": "In this study, we use two machine learning algorithms to perform PP attachment.", "labels": [], "entities": [{"text": "PP attachment", "start_pos": 65, "end_pos": 78, "type": "TASK", "confidence": 0.9595242440700531}]}, {"text": "In line with the case study for English introduced in, we collect a training set of Dutch PP attachment instances from a syntactic treebank.", "labels": [], "entities": []}, {"text": "Collection of this data is described in Section 2.", "labels": [], "entities": []}, {"text": "We extract lexical head features (unigram and bigram) from the treebank occurrences, and enrich this data with cooccurrence information extracted from the WWW (Section 3).", "labels": [], "entities": [{"text": "WWW", "start_pos": 155, "end_pos": 158, "type": "DATASET", "confidence": 0.9799559116363525}]}, {"text": "Using the same features, we analogously build a held-out test corpus for which prosodic labeling is available.", "labels": [], "entities": []}, {"text": "The setup of the machine learning experiments, involving automatic parameter and feature selection, is described in Section 4.", "labels": [], "entities": []}, {"text": "We give the results of the cross-validation experiments on the original data and on the held-out data in Section 5.", "labels": [], "entities": []}, {"text": "Employing the learned PP attachment modules for filtering phrase break placement is discussed in Section 6, where we test on the held-out written text corpus.", "labels": [], "entities": [{"text": "filtering phrase break placement", "start_pos": 48, "end_pos": 80, "type": "TASK", "confidence": 0.8699618428945541}]}, {"text": "We discuss our findings in Section 7.", "labels": [], "entities": [{"text": "Section 7", "start_pos": 27, "end_pos": 36, "type": "DATASET", "confidence": 0.873093992471695}]}], "datasetContent": [{"text": "We choose to use two machine learning algorithms in our study: rule induction as implemented in RIPPER (Cohen, 1995) (version 1, release 2.4) and memory-based learning IB 1 (), as implemented in the TiMBL software package ().", "labels": [], "entities": [{"text": "rule induction", "start_pos": 63, "end_pos": 77, "type": "TASK", "confidence": 0.7425906956195831}, {"text": "TiMBL software package", "start_pos": 199, "end_pos": 221, "type": "DATASET", "confidence": 0.8885387778282166}]}, {"text": "Rule induction is an instance of \"eager\" learning, where effort is invested in searching fora minimal-description-length rule set that covers the classifications in the training data.", "labels": [], "entities": [{"text": "Rule induction", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.8840633928775787}]}, {"text": "The rule set can then be used for classifying new instances of the same task.", "labels": [], "entities": []}, {"text": "Memory-based learning, in contrast, is \"lazy\"; learning is merely the storage of learning examples in memory, while the effort is deferred to the classification of new material, which in IB 1 essentially follows the k-nearest neighbor classification rule of searching for nearest neighbors in memory, and extrapolating their (majority) class to the new instance.", "labels": [], "entities": []}, {"text": "A central issue in the application of machine learning is the setting of algorithmic parameters; both RIPPER and IBI feature several parameters of which the values can seriously affect the bias and result of learning.", "labels": [], "entities": [{"text": "IBI", "start_pos": 113, "end_pos": 116, "type": "DATASET", "confidence": 0.6944936513900757}]}, {"text": "Also, which parameters are optimal interacts with which features are selected and how much data is available.", "labels": [], "entities": []}, {"text": "Few reliable rules of thumb are available for setting parameters.", "labels": [], "entities": []}, {"text": "To estimate appropriate settings, a big search space needs to besought through in someway, after which one can only hope that the estimated best parameter setting is also good for the test material -it might be overfitted on the training material.", "labels": [], "entities": []}, {"text": "Fortunately, we were able to do a semiexhaustive search (testing a selection of sensible numeric values wherein principle there is an infinite number of settings), since the CGN data set is small (1004 instances).", "labels": [], "entities": [{"text": "CGN data set", "start_pos": 174, "end_pos": 186, "type": "DATASET", "confidence": 0.9717460672060648}]}, {"text": "For IB 1, we varied the following parameters systematically in all combinations: \u2022 the kin the k-nearest neighbor classification rule \u2022 the type of feature weighting: none, gain ratio, information gain, chi-squared, shared variance \u2022 the similarity metric: overlap, or MVDM with back-off to overlap at levels 1 (no backoff), 2, and 10 \u2022 the type of distance weighting: none, inverse distance, inverse linear distance, and exponential decay with a = 1.0 and a = 2.0 For RIPPER we varied the following parameters: \u2022 the minimal number of instances to be covered by rules: 1, 2, 5, 10, 25, 50 \u2022 the class order for which rules are induced: increasing and decreasing frequency \u2022 allowing negation in nominal tests or not \u2022 the number of rule set optimization steps: 0, 1, 2 We performed the full matrix of all combinations of these parameters for both algorithms in a nested 10-fold cross-validation experiment.", "labels": [], "entities": [{"text": "IB", "start_pos": 4, "end_pos": 6, "type": "TASK", "confidence": 0.92228764295578}]}, {"text": "First, the original data set was split in ten partitions of 90% training material and 10% test material.", "labels": [], "entities": []}, {"text": "Second, nested 10-fold cross-validation experiments were performed on each 90% data set, splitting it again ten times.", "labels": [], "entities": []}, {"text": "To each of these 10 x 10 experiments all parameter variants were applied.", "labels": [], "entities": []}, {"text": "Per main fold, a nested cross-validation average performance was computed; the setting with the average highest F-score on noun attachment is then applied to the full 90% training set, and tested on the 10% test set.", "labels": [], "entities": [{"text": "F-score", "start_pos": 112, "end_pos": 119, "type": "METRIC", "confidence": 0.9948945045471191}, {"text": "noun attachment", "start_pos": 123, "end_pos": 138, "type": "TASK", "confidence": 0.7379722595214844}]}, {"text": "As a systematic extra variant, we performed both the RIPPER and IB 1 experiments with and without the six bigram features (mentioned in \u00a73.1).", "labels": [], "entities": [{"text": "RIPPER and IB 1", "start_pos": 53, "end_pos": 68, "type": "DATASET", "confidence": 0.44277606159448624}]}], "tableCaptions": [{"text": " Table 1: Peiformance on PP attachment based on three variants of cooccurrence values.", "labels": [], "entities": [{"text": "Peiformance", "start_pos": 10, "end_pos": 21, "type": "METRIC", "confidence": 0.8089991211891174}, {"text": "PP attachment", "start_pos": 25, "end_pos": 38, "type": "TASK", "confidence": 0.9113466739654541}]}, {"text": " Table 2: Peiformance measures on PP attachment in the CGN material by RIPPER and IB 1.", "labels": [], "entities": [{"text": "Peiformance", "start_pos": 10, "end_pos": 21, "type": "METRIC", "confidence": 0.9746320247650146}, {"text": "CGN material", "start_pos": 55, "end_pos": 67, "type": "DATASET", "confidence": 0.8604154884815216}, {"text": "RIPPER", "start_pos": 71, "end_pos": 77, "type": "METRIC", "confidence": 0.8272032737731934}, {"text": "IB", "start_pos": 82, "end_pos": 84, "type": "METRIC", "confidence": 0.7743867039680481}]}, {"text": " Table 3: Petformance on PP attachment in newspaper and e-mail material by RIPPER and IB 1.", "labels": [], "entities": [{"text": "RIPPER", "start_pos": 75, "end_pos": 81, "type": "DATASET", "confidence": 0.5509666800498962}, {"text": "IB 1", "start_pos": 86, "end_pos": 90, "type": "DATASET", "confidence": 0.7119382917881012}]}, {"text": " Table 4: Peiformance on phrasing complemented with PP attachment information from RIPPER and IB 1  with and without bigram features.", "labels": [], "entities": [{"text": "Peiformance", "start_pos": 10, "end_pos": 21, "type": "METRIC", "confidence": 0.9886677861213684}, {"text": "IB 1", "start_pos": 94, "end_pos": 98, "type": "DATASET", "confidence": 0.7225838899612427}]}]}