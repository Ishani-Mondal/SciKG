{"title": [{"text": "QUALIFIER: Question Answering by Lexical Fabric and External Resources", "labels": [], "entities": [{"text": "Question Answering", "start_pos": 11, "end_pos": 29, "type": "TASK", "confidence": 0.8272266089916229}]}], "abstractContent": [{"text": "One of the major challenges in TREC-style question-answering (QA) is to overcome the mismatch in the lexical representations in the query space and document space.", "labels": [], "entities": [{"text": "TREC-style question-answering (QA)", "start_pos": 31, "end_pos": 65, "type": "TASK", "confidence": 0.821060860157013}]}, {"text": "This is particularly severe in QA as exact answers, rather than documents, are required in response to questions.", "labels": [], "entities": [{"text": "QA", "start_pos": 31, "end_pos": 33, "type": "TASK", "confidence": 0.8592721819877625}]}, {"text": "Most current approaches overcome the mismatch problem by employing either data redundancy strategy through the use of Web or linguistic resources.", "labels": [], "entities": []}, {"text": "This paper investigates the integration of lexical relations and Web knowledge to tackle this problem.", "labels": [], "entities": []}, {"text": "The results obtained on TREC11 QA corpus indicate that our approach is both feasible and effective.", "labels": [], "entities": [{"text": "TREC11 QA corpus", "start_pos": 24, "end_pos": 40, "type": "DATASET", "confidence": 0.9081451694170634}]}], "introductionContent": [{"text": "Open domain Question Answering (QA) is an information retrieval paradigm that is attracting increasing attention from the information retrieval (IR), information extraction (IE), and natural language processing (NLP) communities (AAAI Spring Symposium Series 2002, ACL-EACL 2002).", "labels": [], "entities": [{"text": "Open domain Question Answering (QA)", "start_pos": 0, "end_pos": 35, "type": "TASK", "confidence": 0.7809410691261292}, {"text": "information retrieval paradigm", "start_pos": 42, "end_pos": 72, "type": "TASK", "confidence": 0.7787506977717081}, {"text": "information retrieval (IR), information extraction (IE)", "start_pos": 122, "end_pos": 177, "type": "TASK", "confidence": 0.7783878716555509}]}, {"text": "A QA system retrieves concise answers to open-domain natural language questions, where a large text collection (termed the QA corpus) is used as the source for these answers.", "labels": [], "entities": [{"text": "QA system retrieves concise answers to open-domain natural language questions", "start_pos": 2, "end_pos": 79, "type": "TASK", "confidence": 0.6547467410564423}]}, {"text": "Contrary to traditional IR tasks, it is not acceptable fora QA system to retrieve a full document, or a paragraph, in response to a question.", "labels": [], "entities": [{"text": "IR tasks", "start_pos": 24, "end_pos": 32, "type": "TASK", "confidence": 0.9017385244369507}]}, {"text": "Contrary to traditional IE tasks, no prespecified domain restrictions are placed on the questions, which maybe of any type and in any topic.", "labels": [], "entities": [{"text": "IE tasks", "start_pos": 24, "end_pos": 32, "type": "TASK", "confidence": 0.9229754209518433}]}, {"text": "Modern QA systems must therefore combine the strengths of traditional IR and NLP/IE to provide an apposite way to answering questions.", "labels": [], "entities": []}, {"text": "The QA task in the TREC conference series) has motivated much of the recent works focusing on fact-based, short-answer questions.", "labels": [], "entities": [{"text": "TREC conference series", "start_pos": 19, "end_pos": 41, "type": "DATASET", "confidence": 0.8232342402140299}]}, {"text": "Examples of such questions include: \"Who is Tom Cruise married to?\" or \"How many chromosomes does a human zygote have?\".", "labels": [], "entities": []}, {"text": "For the most recent TREC-11 conference, the task consists of 500 questions posed over a QA corpus containing more than one million newspaper articles.", "labels": [], "entities": [{"text": "TREC-11 conference", "start_pos": 20, "end_pos": 38, "type": "TASK", "confidence": 0.5020948350429535}]}, {"text": "Instead of previous years' 50-byte or 250-byte text fragments, exact answers are expected from the QA corpus with supports of documentary evidences.", "labels": [], "entities": [{"text": "QA corpus", "start_pos": 99, "end_pos": 108, "type": "DATASET", "confidence": 0.8862721621990204}]}, {"text": "One of the major challenges in TREC-style QA is to overcome the mismatch in the lexical representations between the query space and document space.", "labels": [], "entities": [{"text": "TREC-style QA", "start_pos": 31, "end_pos": 44, "type": "TASK", "confidence": 0.6669074296951294}]}, {"text": "This mismatch, also known as the QA gap, is caused by the differences in the set of terms used in the question formulation and answer strings in the corpus.", "labels": [], "entities": []}, {"text": "Given a source, such as the QA corpus, that contains only a relatively small number of answers to a query, we are faced with the difficulty to map the questions to answers byway of uncovering the complex lexical, syntactic, or semantic relationships between the question and the answer strings.", "labels": [], "entities": [{"text": "QA corpus", "start_pos": 28, "end_pos": 37, "type": "DATASET", "confidence": 0.8111630380153656}]}, {"text": "Recent redundancy-based approaches proposed the use of data, in-stead of methods, to do most of the work to bridge the QA gap.", "labels": [], "entities": []}, {"text": "These methods suggest that the greater the answer redundancy in the source data collection, the more likely that we can find an answer that occurs in a simple relation to the question.", "labels": [], "entities": []}, {"text": "With the availability of rich linguistic resources, we can also minimize the need to perform complex linguistic processing.", "labels": [], "entities": []}, {"text": "However, this does not mean that NLP is now out of the picture.", "labels": [], "entities": []}, {"text": "For some question/answer pairs, deep reasoning is still needed to relate the two.", "labels": [], "entities": []}, {"text": "Many QA research groups have used a variety of linguistic resources -part-of-speech tagging, syntactic parsing, semantic relations, named entity extraction, WordNet, on-line dictionaries, query logs and ontologies, etc (Harabagiu et al 2002,.", "labels": [], "entities": [{"text": "part-of-speech tagging", "start_pos": 69, "end_pos": 91, "type": "TASK", "confidence": 0.7547068297863007}, {"text": "syntactic parsing", "start_pos": 93, "end_pos": 110, "type": "TASK", "confidence": 0.7643288671970367}, {"text": "named entity extraction", "start_pos": 132, "end_pos": 155, "type": "TASK", "confidence": 0.652349183956782}]}, {"text": "This paper investigates the integration of both linguistic knowledge and external resources for TREC-style question answering.", "labels": [], "entities": [{"text": "TREC-style question answering", "start_pos": 96, "end_pos": 125, "type": "TASK", "confidence": 0.8686452905337015}]}, {"text": "In particular, we describe a high performance question answering system called QUALIFIER (QUestion Answering by Lexical Fabric and External Resources) and analyze its effectiveness using the TREC-11 benchmark.", "labels": [], "entities": [{"text": "question answering", "start_pos": 46, "end_pos": 64, "type": "TASK", "confidence": 0.7450598478317261}, {"text": "QUestion Answering", "start_pos": 90, "end_pos": 108, "type": "TASK", "confidence": 0.6942479163408279}, {"text": "TREC-11 benchmark", "start_pos": 191, "end_pos": 208, "type": "DATASET", "confidence": 0.8311295807361603}]}, {"text": "Our results show that combining lexical information and external resources with a custom text search produces an effective question-answering system.", "labels": [], "entities": []}, {"text": "The rest of the paper is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 presents related work.", "labels": [], "entities": []}, {"text": "Sections 3 and 4 respectively discuss the design and architecture of the system.", "labels": [], "entities": []}, {"text": "Section 5 elaborates on the use of external resources for QA, while Section 6 details the experimental results.", "labels": [], "entities": [{"text": "QA", "start_pos": 58, "end_pos": 60, "type": "TASK", "confidence": 0.9038180112838745}]}, {"text": "Section 7 concludes the paper with discussions for future work.", "labels": [], "entities": []}], "datasetContent": [{"text": "We use all the 500 questions of TREC-11 QA track as our test set.", "labels": [], "entities": [{"text": "TREC-11 QA track", "start_pos": 32, "end_pos": 48, "type": "DATASET", "confidence": 0.8487125833829244}]}, {"text": "The performance of QUALIFIER without the use of WordNet and web is considered as the baseline.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: The Precision Score of 25 Web Runs", "labels": [], "entities": [{"text": "Precision Score", "start_pos": 14, "end_pos": 29, "type": "METRIC", "confidence": 0.7643839418888092}]}, {"text": " Table 4: Different Query Formulation Methods", "labels": [], "entities": [{"text": "Query Formulation", "start_pos": 20, "end_pos": 37, "type": "TASK", "confidence": 0.6534878015518188}]}, {"text": " Table 5. As can be  seen, the best result is obtained when performing  up to 5 successive relaxation iterations of Boo- lean search followed by a similarity-based  search. This is the most thorough search process  we have conducted with the aim of finding an  exact answer if possible and only returning a NIL  answer as the last resort. It works well as our an- swer selection process is quite strict.", "labels": [], "entities": []}, {"text": " Table 5: Results of Boolean vs Similarity Search", "labels": [], "entities": [{"text": "Boolean vs Similarity Search", "start_pos": 21, "end_pos": 49, "type": "TASK", "confidence": 0.6125292181968689}]}]}