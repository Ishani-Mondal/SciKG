{"title": [], "abstractContent": [{"text": "In this paper, a word alignment approach is presented which is based on a combination of clues.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 17, "end_pos": 31, "type": "TASK", "confidence": 0.8031810224056244}]}, {"text": "Word alignment clues indicate associations between words and phrases.", "labels": [], "entities": [{"text": "Word alignment", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.647555485367775}]}, {"text": "They can be based on features such as frequency, part-of-speech, phrase type, and the actual wordform strings.", "labels": [], "entities": []}, {"text": "Clues can be found by calculating similarity measures or learned from word aligned data.", "labels": [], "entities": []}, {"text": "The clue alignment approach, which is proposed in this paper, makes it possible to combine association clues taking different kinds of linguistic information into account.", "labels": [], "entities": [{"text": "clue alignment", "start_pos": 4, "end_pos": 18, "type": "TASK", "confidence": 0.7803187668323517}]}, {"text": "It allows a dynamic to-kenization into token units of varying size.", "labels": [], "entities": []}, {"text": "The approach has been applied to an English/Swedish parallel text with promising results.", "labels": [], "entities": []}], "introductionContent": [{"text": "Parallel corpora carry a huge amount of bilingual lexical information.", "labels": [], "entities": []}, {"text": "Word alignment approaches focus on the automatic identification of translation relations in translated texts.", "labels": [], "entities": [{"text": "Word alignment", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.7812944054603577}, {"text": "automatic identification of translation relations in translated texts", "start_pos": 39, "end_pos": 108, "type": "TASK", "confidence": 0.7872373759746552}]}, {"text": "Alignments are usually represented as a set of links between words and phrases of source and target language segments.", "labels": [], "entities": []}, {"text": "An alignment can be complete, i.e. all items in both segments have been linked to corresponding items in the other language, or incomplete, otherwise.", "labels": [], "entities": []}, {"text": "Alignments may include \"null links\" which can be modeled as links to an \"empty element\".", "labels": [], "entities": []}, {"text": "In word alignment, we have to \u2022 find an appropriate model M for the alignment of source and target language texts (modeling) \u2022 estimate parameters of the model M, e.g. from empirical data (parameter estimation) \u2022 find the optimal alignment of words and phrases fora given translation according to the model M and its parameters (alignment recovery).", "labels": [], "entities": [{"text": "word alignment", "start_pos": 3, "end_pos": 17, "type": "TASK", "confidence": 0.8000972867012024}]}, {"text": "Modeling the relations between lexical units of translated texts is not a trivial task due to the diversity of natural languages.", "labels": [], "entities": [{"text": "Modeling the relations between lexical units of translated texts", "start_pos": 0, "end_pos": 64, "type": "TASK", "confidence": 0.690268337726593}]}, {"text": "There are generally two approaches, the estimation approach which is used in, e.g., statistical machine translation, and the association approach which is used in, e.g., automatic extraction of bilingual terminology.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 84, "end_pos": 115, "type": "TASK", "confidence": 0.7317800323168436}, {"text": "automatic extraction of bilingual terminology", "start_pos": 170, "end_pos": 215, "type": "TASK", "confidence": 0.8327661871910095}]}, {"text": "In the estimation approach, alignment parameters are modeled as hidden parameters in a statistical translation model ).", "labels": [], "entities": []}, {"text": "Association approaches base the alignment on similarity measures and association tests such as Dice scores, t-scores () log-likelihood measures, and longest common subsequence ratios.", "labels": [], "entities": []}, {"text": "One of the main difficulties in all alignment strategies is the identification of appropriate units in the source and the target language to be aligned.", "labels": [], "entities": []}, {"text": "This task is hard even for human experts as can be seen in the detailed guidelines which are required for manual alignments).", "labels": [], "entities": []}, {"text": "Many translation relations involve multiword units such as phrasal compounds, idiomatic expressions, and complex terms.", "labels": [], "entities": []}, {"text": "Syntactic shifts can also require the consideration of a context larger than a single word.", "labels": [], "entities": [{"text": "Syntactic shifts", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.877365916967392}]}, {"text": "Some items are not translated at all.", "labels": [], "entities": []}, {"text": "Splitting source and target language texts into appropriate units for alignment (henceforth: tokenization) is often not possible without considering the translation relations.", "labels": [], "entities": [{"text": "Splitting source and target language texts", "start_pos": 0, "end_pos": 42, "type": "TASK", "confidence": 0.8630629082520803}, {"text": "alignment", "start_pos": 70, "end_pos": 79, "type": "TASK", "confidence": 0.9584312438964844}, {"text": "tokenization)", "start_pos": 93, "end_pos": 106, "type": "TASK", "confidence": 0.9359666705131531}]}, {"text": "In other words, initial tokenization borders may change when the translation relations are investigated.", "labels": [], "entities": []}, {"text": "Human aligners frequently expand token units when aligning sentences manually depending on the context ().", "labels": [], "entities": []}, {"text": "Previous approaches use either iterative procedures to re-estimate alignment parameters () or preprocessing steps for the identification of token Ngrams (.", "labels": [], "entities": []}, {"text": "In our approach, we combine simple techniques for prior tokenization with dynamic techniques during the alignment phase.", "labels": [], "entities": []}, {"text": "The second problem of traditional word alignment approaches is the fact that parameter estimations are usually based on plain text items only.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 34, "end_pos": 48, "type": "TASK", "confidence": 0.7758025825023651}]}, {"text": "Linguistic data, which could be used to identify associations between lexical items are often ignored.", "labels": [], "entities": []}, {"text": "Linguistic tools such as part-of-speech taggers, (shallow) parsers, named-entity recognizers become more and more robust and available for more languages.", "labels": [], "entities": [{"text": "part-of-speech taggers", "start_pos": 25, "end_pos": 47, "type": "TASK", "confidence": 0.7180780172348022}]}, {"text": "Linguistic information including contextual features could be used to improve alignment strategies.", "labels": [], "entities": []}, {"text": "The third problem, alignment recovery, is a search problem.", "labels": [], "entities": [{"text": "alignment recovery", "start_pos": 19, "end_pos": 37, "type": "TASK", "confidence": 0.9896447658538818}]}, {"text": "Using the alignment model and its parameters, we have to find the optimal alignment fora given pair of source and target language segments.", "labels": [], "entities": []}, {"text": "In, the author points out that a sentence pair with a maximum of n token units in both sentences has n!", "labels": [], "entities": []}, {"text": "possible alignments in a simple directed alignment model with a fixed tokenization.", "labels": [], "entities": []}, {"text": "Furthermore, a search strategy becomes very complex if we allow dyn am i c tokeni zati on borders (overlapping N-gram s, inclusions), which leads us not only to a larger number of possible combinations but also to the problem of comparing alignments with variable length (number of links) The clue alignment approach, which we propose here, addresses the three problems which were mentioned above.", "labels": [], "entities": [{"text": "clue alignment", "start_pos": 293, "end_pos": 307, "type": "TASK", "confidence": 0.7669655680656433}]}, {"text": "The approach allows the combination of association measures for any features of translation units of varying size.", "labels": [], "entities": []}, {"text": "Overlapping units are allowed as well as inclusions.", "labels": [], "entities": []}, {"text": "Association scores are organized in a clue matrix and we present a simple approach for approximating the optimal alignment.", "labels": [], "entities": []}, {"text": "Section 2 describes the clue alignment model and ways of estimating parameters from association scores.", "labels": [], "entities": [{"text": "clue alignment", "start_pos": 24, "end_pos": 38, "type": "TASK", "confidence": 0.7715922892093658}]}, {"text": "Section 3 introduces the alignment approach which is based on word alignment clues.", "labels": [], "entities": [{"text": "alignment", "start_pos": 25, "end_pos": 34, "type": "TASK", "confidence": 0.95990389585495}, {"text": "word alignment clues", "start_pos": 62, "end_pos": 82, "type": "TASK", "confidence": 0.7515635192394257}]}, {"text": "Section 4 gives examples of learning clues from previous alignments.", "labels": [], "entities": []}, {"text": "Section 5 summarizes alignment experiments and, finally, section 6 contains conclusions and a discussion.", "labels": [], "entities": [{"text": "alignment", "start_pos": 21, "end_pos": 30, "type": "TASK", "confidence": 0.9700207114219666}]}], "datasetContent": [], "tableCaptions": []}