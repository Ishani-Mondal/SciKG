{"title": [{"text": "Combining Distributional and Morphological Information for Part of Speech Induction", "labels": [], "entities": [{"text": "Speech Induction", "start_pos": 67, "end_pos": 83, "type": "TASK", "confidence": 0.6992539316415787}]}], "abstractContent": [{"text": "In this paper we discuss algorithms for clustering words into classes from un-labelled text using unsupervised algorithms , based on distributional and morphological information.", "labels": [], "entities": [{"text": "clustering words into classes from un-labelled text", "start_pos": 40, "end_pos": 91, "type": "TASK", "confidence": 0.8408667530332293}]}, {"text": "We show how the use of morphological information can improve the performance on rare words, and that this is robust across a wide range of languages.", "labels": [], "entities": []}], "introductionContent": [{"text": "The task studied in this paper is the unsupervised learning of parts-of-speech, that is to say lexical categories corresponding to traditional notions of, for example, nouns and verbs.", "labels": [], "entities": []}, {"text": "As is often the casein machine learning of natural language, there are two parallel motivations: first a simple engineering one -the induction of these categories can help in smoothing and generalising other models, particularly in language modelling for speech recognition as explored by and secondly a cognitive science motivation -exploring how evidence in the primary linguistic data can account for first language acquisition by infant children.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 255, "end_pos": 273, "type": "TASK", "confidence": 0.712186649441719}, {"text": "first language acquisition", "start_pos": 404, "end_pos": 430, "type": "TASK", "confidence": 0.656703511873881}]}, {"text": "At this early phase of learning, only limited sources of information can be used: primarily distributional evidence, about the contexts in which words occur, and morphological evidence, (more strictly phonotactic or orthotactic evidence) about the sequence of symbols (letters or phonemes) of which each word is formed.", "labels": [], "entities": []}, {"text": "A number of different approaches have been presented for this task using exclusively distributional evidence to cluster the words together, starting with and these have been shown to produce good results in English, Japanese and Chinese.", "labels": [], "entities": []}, {"text": "These languages have however rather simple morphology and thus words will tend to have higher frequency than in more morphologically complex languages.", "labels": [], "entities": []}, {"text": "In this paper we will address two issues: first, whether the existing algorithms work adequately on a range of languages and secondly how we can incorporate morphological information.", "labels": [], "entities": []}, {"text": "We are particularly interested in rare words: as) points out, it is most important to cluster the infrequent words, as we will have reliable information about the frequent words; and yet it is these words that are most difficult to cluster.", "labels": [], "entities": []}, {"text": "We accordingly focus both in our algorithms and our evaluation on how to cluster words effectively that occur only a few times (or not at all) in the training data.", "labels": [], "entities": []}, {"text": "In addition we are interested primarily in inducing small numbers of clusters (at most 128) from comparatively small amounts of data using limited or no sources of external knowledge, and in approaches that will work across a wide range of languages, rather than inducing large numbers (say 1000) from hundreds of millions of words.", "labels": [], "entities": []}, {"text": "Note this is different from the common task of guessing the word category of an unknown word given a pre-existing set of parts-of-speech, a task which has been studied extensively.", "labels": [], "entities": [{"text": "guessing the word category of an unknown word", "start_pos": 47, "end_pos": 92, "type": "TASK", "confidence": 0.855950266122818}]}, {"text": "ical information of a restricted form into a distributional clustering algorithm.", "labels": [], "entities": []}, {"text": "In addition we will use a very limited sort of frequency information, since rare words tend to belong to open class categories.", "labels": [], "entities": []}, {"text": "The input to the algorithm is a sequence of tokens, each of which is considered as a sequence of characters in a standard encoding.", "labels": [], "entities": []}, {"text": "The rest of this paper is structured as follows: we will first discuss the evaluation of the models in some detail and present some simple experiments we have performed here (Section 2).", "labels": [], "entities": []}, {"text": "We will then discuss the basic algorithm that is the starting point for our research in Section 3.", "labels": [], "entities": []}, {"text": "Then we show how we can incorporate a limited form of morphological information into this algorithm in Section 4.", "labels": [], "entities": []}, {"text": "Section 5 presents the results of our evaluations on a number of data sets drawn from typologically distinct languages.", "labels": [], "entities": []}, {"text": "We then briefly discuss the use of ambiguous models or soft clustering in Section 6, and then finish with our conclusions and proposals for future work.", "labels": [], "entities": []}], "datasetContent": [{"text": "A number of different approaches to evaluation have been proposed in the past.", "labels": [], "entities": []}, {"text": "First, early work used an informal evaluation of manually comparing the clusters or dendrograms produced by the algorithms with the authors' intuitive judgment of the lexical categories.", "labels": [], "entities": []}, {"text": "This is inadequate fora number of obvious reasons -first it does not allow adequate comparison of different techniques, and secondly it restricts the languages that can easily be studied to those in which the researcher has competence thus limiting experimentation on a narrow range of languages.", "labels": [], "entities": []}, {"text": "A second form of evaluation is to use some data that has been manually or semi-automatically annotated with part of speech (POS) tags, and to use some information theoretic measure to look at the correlation between the 'correct' data and the induced POS tags.", "labels": [], "entities": []}, {"text": "Specifically, one could look at the conditional entropy of the gold standard tags given the induced tags.", "labels": [], "entities": []}, {"text": "We use the symbol W to refer to the random variable related to the word, G for the associated gold standard tag, and T for the tag produced by one of our algorithms.", "labels": [], "entities": []}, {"text": "Recall that   We used texts prepared for the MULTEXT-East project) which consists of data in seven languages: the original English together with Romanian, Czech, Slovene, Bulgarian, Estonian, and Hungarian.", "labels": [], "entities": [{"text": "MULTEXT-East project", "start_pos": 45, "end_pos": 65, "type": "DATASET", "confidence": 0.7324159443378448}]}, {"text": "As can be seen they cover a wide range of language families; furthermore Bulgarian is written in Cyrillic, which slightly stretches the range.", "labels": [], "entities": []}, {"text": "Token-type ratios range from 12.1 for English to 4.84 for Hungarian.", "labels": [], "entities": []}, {"text": "The tags used are extremely fine-grained, and incorporate a great deal of information about case, gender and soon -in Hungarian for example 400 tags are used with 86 tags used only once.", "labels": [], "entities": [{"text": "soon", "start_pos": 109, "end_pos": 113, "type": "METRIC", "confidence": 0.9470052123069763}]}, {"text": "shows the result of our cross-linguistic evaluation on this data.", "labels": [], "entities": []}, {"text": "Since the data sets are so small we decided to use the conditional entropy evaluation.", "labels": [], "entities": []}, {"text": "Here DO refers to the distributional clustering algorithm where all words are clustered; D5 leaves all words with frequency at most 5 in a seperate cluster, DM uses morphological information as well, DF uses frequency information and DMF uses morphological and frequency information.", "labels": [], "entities": []}, {"text": "We evaluated it for all words, and also for words with frequency at most 5.", "labels": [], "entities": []}, {"text": "We can see that the use of morphological information consistently improves the results on the rare words by a substantial margin.", "labels": [], "entities": []}, {"text": "In some cases, however, a simpler algorithm performs better when all the words are considered -notably in Slovene and Estonian.", "labels": [], "entities": []}, {"text": "We have also evaluated this method by comparing the perplexity of a class-based language model de-    rived from these classes.", "labels": [], "entities": []}, {"text": "We constructed a class bigram model, using absolute interpolation with a singleton generalised distribution for the transition weights, and using absolute discounting with backing off for the membership/output function.", "labels": [], "entities": []}, {"text": "() We trained the model on sections 00-09 of the Penn Treebank, ( 518769 tokens including sentence boundaries and punctuation) and tested it on sections 10-l 9 (537639 tokens).", "labels": [], "entities": [{"text": "Penn Treebank", "start_pos": 49, "end_pos": 62, "type": "DATASET", "confidence": 0.8226265609264374}]}, {"text": "We used the full vocabulary of the training and test sets together which was 45679, of which 14576 had frequency zero in the training data and thus had to be categorised based solely on their morphology and frequency.", "labels": [], "entities": [{"text": "45679", "start_pos": 77, "end_pos": 82, "type": "DATASET", "confidence": 0.9656537175178528}]}, {"text": "We did not reduce the vocabulary or change the capitalization in anyway.", "labels": [], "entities": []}, {"text": "We compared different models with varying numbers of clusters: 32 64 and 128.", "labels": [], "entities": []}, {"text": "shows the results of the perplexity evaluation on the WSJ data.", "labels": [], "entities": [{"text": "WSJ data", "start_pos": 54, "end_pos": 62, "type": "DATASET", "confidence": 0.9669580459594727}]}, {"text": "As can be seen the models incorporating morphological information have slightly lower perplexity on the test data than the D5 model.", "labels": [], "entities": []}, {"text": "Note that this is a global evaluation overall the words in the data, including words that do not occur in the training data at all.", "labels": [], "entities": []}, {"text": "shows how the conditional entropy varies with respect to the frequency for these models.", "labels": [], "entities": []}, {"text": "As can be seen the use of morphological information improves the preformance markedly for rare words, and that this effect reduces as the frequency increases.", "labels": [], "entities": []}, {"text": "Note that the use of the frequency information worsens the performance for rare words according to this evaluation -this is because the rare words are much more tightly grouped into just a few clusters, thus the entropy of the cluster tags is lower.", "labels": [], "entities": []}, {"text": "shows a qualitative evaluation of some of the clusters produced by the best performing model for 64 clusters on the WSJ data set.", "labels": [], "entities": [{"text": "WSJ data set", "start_pos": 116, "end_pos": 128, "type": "DATASET", "confidence": 0.9838747978210449}]}, {"text": "We selected the 10 clusters with the largest number of zero frequency word types in.", "labels": [], "entities": []}, {"text": "We examined each cluster and chose a simple regular expression to describe it, and calculated the precision and recall for words of all frequency, and for words of zero frequency.", "labels": [], "entities": [{"text": "precision", "start_pos": 98, "end_pos": 107, "type": "METRIC", "confidence": 0.9996215105056763}, {"text": "recall", "start_pos": 112, "end_pos": 118, "type": "METRIC", "confidence": 0.9992417097091675}]}, {"text": "Note that several of the clusters capture syntactically salient morphological regularities: regular verb suffixes, noun suffixes and the presence of capitalisation are all detected, together with a class for numbers.", "labels": [], "entities": []}, {"text": "In some cases these are split amongst more than one class, thus giving classes with high precision and low recall.", "labels": [], "entities": [{"text": "precision", "start_pos": 89, "end_pos": 98, "type": "METRIC", "confidence": 0.999198853969574}, {"text": "recall", "start_pos": 107, "end_pos": 113, "type": "METRIC", "confidence": 0.9989615678787231}]}, {"text": "We made no attempt to adjust the regular expressions to make these scores high -we merely present them as an aid to an intuitive understanding of the composition of these clusters.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Comparison of different baseline", "labels": [], "entities": []}, {"text": " Table 2: Data sets from Multext East Project  Language  Family", "labels": [], "entities": [{"text": "Multext East Project  Language  Family", "start_pos": 25, "end_pos": 63, "type": "DATASET", "confidence": 0.9658435940742492}]}, {"text": " Table 3: Cross-linguistic evaluation: 64 clusters, left all words, right f < 5. We compare the baseline", "labels": [], "entities": []}, {"text": " Table 4: Perplexities on training data (left) and test  data(right) using WSJ data  Clusters 32  64  128 32  64  128", "labels": [], "entities": [{"text": "WSJ data  Clusters 32  64  128", "start_pos": 75, "end_pos": 105, "type": "DATASET", "confidence": 0.9215115706125895}]}, {"text": " Table 5: The 10 most productive classes together with a qualitative analysis of their contents", "labels": [], "entities": []}]}