{"title": [{"text": "Evaluating and Combining Approaches to Selectional Preference Acquisition", "labels": [], "entities": [{"text": "Selectional Preference Acquisition", "start_pos": 39, "end_pos": 73, "type": "TASK", "confidence": 0.8792840639750162}]}], "abstractContent": [{"text": "Previous work on the induction of se-lectional preferences has been mainly carried out for English and has concentrated almost exclusively on verbs and their direct objects.", "labels": [], "entities": [{"text": "induction of se-lectional preferences", "start_pos": 21, "end_pos": 58, "type": "TASK", "confidence": 0.8621244877576828}]}, {"text": "In this paper, we focus on class-based models of selec-tional preferences for German verbs and take into account not only direct objects , but also subjects and prepositional complements.", "labels": [], "entities": []}, {"text": "We evaluate model performance against human judgments and show that there is no single method that overall performs best.", "labels": [], "entities": []}, {"text": "We explore a variety of parametrizations for our models and demonstrate that model combination enhances agreement with human ratings.", "labels": [], "entities": []}], "introductionContent": [{"text": "Selectional preferences or constraints are the semantic restrictions that a word imposes on the environment in which it occurs.", "labels": [], "entities": []}, {"text": "A verb like eat typically takes animate entities as its subject and edible entities as its object.", "labels": [], "entities": []}, {"text": "Selectional preferences can most easily be observed in situations where they are violated.", "labels": [], "entities": []}, {"text": "For example, in the sentence \"The mountain eats sincerity.\" both subject and object preferences for the verb eat are violated.", "labels": [], "entities": []}, {"text": "The problem of quantifying the degree to which a given predicate (e.g., eat) semantically fits its arguments has received a lot of attention within computational linguistics.", "labels": [], "entities": []}, {"text": "Several approaches have been developed for the induction of selectional preferences, and almost all of them rely on the availability of large machine-readable corpora.", "labels": [], "entities": [{"text": "induction of selectional preferences", "start_pos": 47, "end_pos": 83, "type": "TASK", "confidence": 0.807635024189949}]}, {"text": "Probably the most primitive corpus-based model of selectional preferences is co-occurrence frequency.", "labels": [], "entities": []}, {"text": "Inspection in a corpus of the types of nouns eat admits as its objects will reveal that food, meal, meat, or lunch are frequent complements, whereas river, mountain, or moon are rather unlikely.", "labels": [], "entities": []}, {"text": "The obvious disadvantage of the frequency-based approach is that no generalizations emerge with respect to the observed preferences as it embodies no notion of semantic relatedness or proximity Ideally, one would like to infer from the corpus that eat is semantically congruent with food-related objects and incongruent with natural objects.", "labels": [], "entities": []}, {"text": "Another related limitation of the frequency-based account is that it cannot make any predictions for words that never occurred in the corpus.", "labels": [], "entities": []}, {"text": "A zero co-occurrence count might be due to insufficient evidence or might reflect the fact that a given word combination is inherently implausible.", "labels": [], "entities": []}, {"text": "For the above reasons, most approaches model the selectional preferences of predicates (e.g., verbs, nouns, adjectives) by combining observed frequencies with knowledge about the semantic classes of their arguments.", "labels": [], "entities": []}, {"text": "The classes can be induced directly from the corpus () or taken from a manually crafted taxonomy).", "labels": [], "entities": []}, {"text": "In the latter case the taxonomy is used to provide a mapping from words to conceptual classes, and inmost cases WordNet () is employed for this purpose.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 112, "end_pos": 119, "type": "DATASET", "confidence": 0.9170815348625183}]}, {"text": "Although most approaches agree on how selectional preferences must be represented, i.e., as a mapping cv : (p,r,c) -> a that maps each predicate p and the semantic class c of its argument with respect to role r to areal number a (), there is little agreement on how selectional preferences must be modeled (e.g., whether to use a probability model or not) and evaluated (e.g., whether to use a task-based evaluation or not).", "labels": [], "entities": []}, {"text": "Furthermore, previous work has almost exclusively focused on verbal selectional preferences in English with the exception of, who look at adjectivenoun combinations, again for English.", "labels": [], "entities": []}, {"text": "Verbs tend to impose stricter selectional preferences on their arguments than adjectives or nouns and thus provide a natural test bed for models of selectional preferences.", "labels": [], "entities": []}, {"text": "However, research on verbal selectional preferences has been relatively narrow in scope as it has primarily focused on verbs and their direct objects, ignoring the selectional preferences pertaining to subjects and prepositional complements.", "labels": [], "entities": []}, {"text": "The induction of selectional preferences typically addresses two related problems: (a) finding an appropriate class that best fits the predicate in question and (b) coming up with a statistical model or a measure that estimates how well a predicate fits its arguments.", "labels": [], "entities": [{"text": "induction of selectional preferences", "start_pos": 4, "end_pos": 40, "type": "TASK", "confidence": 0.8132131099700928}]}, {"text": "defines selectional association, an informationtheoretic measure of semantic fit of a particular semantic class c as an argument to a predicate p. use the Minimum Description Length (MDL) principle to select the the appropriate class c, employ hypothesis testing.", "labels": [], "entities": []}, {"text": "propose Hidden Markov Models as away of deriving selectional preferences over words, senses, or even classes, whereas use Bayesian Belief Networks to quantify selectional preferences.", "labels": [], "entities": []}, {"text": "Although there is no standard way to evaluate different approaches to selectional preferences, two types of evaluation are usually conducted: task-based evaluation and comparisons against human judgments.", "labels": [], "entities": []}, {"text": "Word sense disambiguation results are reported by,, and (however, on a different data set).", "labels": [], "entities": [{"text": "Word sense disambiguation", "start_pos": 0, "end_pos": 25, "type": "TASK", "confidence": 0.616731067498525}]}, {"text": "Among the first three approaches, obtain the best results.", "labels": [], "entities": []}, {"text": "evaluate their system on the task of prepositional phrase attachment, whereas use pseudodisambiguation,' a somewhat artificial task, and show that their approach outperforms.", "labels": [], "entities": [{"text": "prepositional phrase attachment", "start_pos": 37, "end_pos": 68, "type": "TASK", "confidence": 0.6665474573771158}]}, {"text": "Another way to evaluate a model's performance is agreement with human ratings.", "labels": [], "entities": []}, {"text": "This can be done by selecting predicate-argument structures randomly, using the model to predict the degree of semantic fit and then looking at how well the ratings correlate with the model's predictions).", "labels": [], "entities": []}, {"text": "This approach seems more appropriate for languages for which annotated corpora with word senses are not available.", "labels": [], "entities": []}, {"text": "It is more direct than disambiguation which relies on the assumption that models of selectional preferences have to infer the appropriate semantic class and therefore perform disambiguation as aside effect.", "labels": [], "entities": [{"text": "disambiguation", "start_pos": 23, "end_pos": 37, "type": "TASK", "confidence": 0.9642443656921387}]}, {"text": "It is also more natural than pseudo-disambiguation which relies on artificially constructed data sets.", "labels": [], "entities": []}, {"text": "Large-scale comparative studies have not, however, assessed the strengths and weaknesses of the proposed methods as far as modeling human data is concerned.", "labels": [], "entities": []}, {"text": "In this paper, we undertake such a comparative study by looking at selectional preferences of German verbs.", "labels": [], "entities": []}, {"text": "In contrast to previous work, we take into account not only verbs and their direct objects, but also subjects and prepositional complements.", "labels": [], "entities": []}, {"text": "We focus on three previously well-studied models, selectional association, probability estimation method.", "labels": [], "entities": []}, {"text": "For comparison, we also employ two models that do not incorporate any notion of semantic class, namely cooccurrence frequency and conditional probability.", "labels": [], "entities": []}, {"text": "In the remainder of this paper, we briefly review the models of selectional preferences we consider (Section 2).", "labels": [], "entities": []}, {"text": "Section 3 details our experiments, evaluation methodology, and reports our results.", "labels": [], "entities": []}, {"text": "Section 4 offers some discussion and concluding remarks.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 2: Example stimuli (with log co-occurrence frequencies in the SZ corpus)", "labels": [], "entities": [{"text": "SZ corpus", "start_pos": 69, "end_pos": 78, "type": "DATASET", "confidence": 0.7697817087173462}]}, {"text": " Table 3: Best correlations between human ratings and selectional preference models", "labels": [], "entities": []}, {"text": " Table 4: Principal component factors", "labels": [], "entities": []}]}