{"title": [{"text": "Decomposable Modeling in Natural Language Processing", "labels": [], "entities": [{"text": "Decomposable Modeling", "start_pos": 0, "end_pos": 21, "type": "TASK", "confidence": 0.8826383054256439}]}], "abstractContent": [{"text": "In this paper, we describe a framework for developing probabilistic classifiers in natural language processing.", "labels": [], "entities": []}, {"text": "Our focus is on formulating models that capture the most important interdependencies among features, to avoid overfitting the data while also characterizing the data well.", "labels": [], "entities": []}, {"text": "The class of probability models and the associated inference techniques described here were developed in mathematical statistics, and are widely used in artificial intelligence and applied statistics.", "labels": [], "entities": []}, {"text": "Our goal is to make this model selection framework accessible to researchers in NLP, and provide pointers to available software and important references.", "labels": [], "entities": []}, {"text": "In addition, we describe how the quality of the three determinants of classifier performance (the features, the form of the model, and the parameter estimates) can be separately evaluated.", "labels": [], "entities": []}, {"text": "We also demonstrate the classification performance of these models in a large-scale experiment involving the disambiguation of 34 words taken from the HECTOR word sense corpus (Hanks 1996).", "labels": [], "entities": [{"text": "HECTOR word sense corpus", "start_pos": 151, "end_pos": 175, "type": "DATASET", "confidence": 0.6525361984968185}]}, {"text": "In lO-fold cross-validations, the model search procedure performs significantly better than naive Bayes on 6 of the words without being significantly worse on any of them.", "labels": [], "entities": []}], "introductionContent": [{"text": "This paper describes a framework for developing probabilistic classifiers in natural language processing (NLP).", "labels": [], "entities": [{"text": "natural language processing (NLP)", "start_pos": 77, "end_pos": 110, "type": "TASK", "confidence": 0.7110247910022736}]}, {"text": "1 A probabilistic classifier assigns the most probable class to an object, based on a probability model of the interdependencies among the class and a set of input features.", "labels": [], "entities": []}, {"text": "This paper focuses on formulating a model that captures the most important interdependencies, to avoid overfitting the data while also characterizing the data well.", "labels": [], "entities": []}, {"text": "The goal is to achieve a balance between feasibility and expressive power, which is needed in an area as complex as NLP.", "labels": [], "entities": []}, {"text": "The class of probability models and the associated inference techniques described here were developed in mathematical statistics, and are widely used in artificial intelligence and applied statistics.", "labels": [], "entities": []}, {"text": "However, these techniques have not been widely used in NLP, although the software required to implement these procedures is freely available.", "labels": [], "entities": []}, {"text": "Within this framework, we can unify many of the metrics and types of models currently used in NLP.", "labels": [], "entities": []}, {"text": "The class of models, decomposable models, is large and expressive, yet there are computationally feasible model search procedures defined for them.", "labels": [], "entities": []}, {"text": "They can include any kind of discrete variable, and the formality of the method supports evaluation.", "labels": [], "entities": []}, {"text": "In this paper, our goal is to make this model selection framework accessible to researchers in NLP, by providing a concise explanation of the underlying theor~ pointing out relationships to existing NLP research, and providing pointers to available software and important references.", "labels": [], "entities": []}, {"text": "In addition, we describe how the quality of the three determinants of classifier performance (the features, the form of the model and the parameter estimates) can be separately evaluated.", "labels": [], "entities": []}, {"text": "We also demonstrate the classification performance of these models in a largescale experiment involving the disambiguation of 34 words taken from the HECTOR word sense corpus.", "labels": [], "entities": [{"text": "HECTOR word sense corpus", "start_pos": 150, "end_pos": 174, "type": "DATASET", "confidence": 0.6963910907506943}]}, {"text": "We compare the performance of classifiers based on models selected by this algorithm with the performance of naive Bayes classifiers (classifiers based on the naive Bayes model).", "labels": [], "entities": []}, {"text": "Naive Bayes classifiers have been found to be remarkably successful in many applications, including word sense disambiguation).", "labels": [], "entities": [{"text": "Naive Bayes classifiers", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.6522315343221029}, {"text": "word sense disambiguation", "start_pos": 100, "end_pos": 125, "type": "TASK", "confidence": 0.7098560531934103}]}, {"text": "In 10-fold cross-validations, the model search procedure achieves an overall 1.4 percentage point improvement over naive Bayes, and is significantly better on 6 of the words without being significantly worse on any of them.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": []}