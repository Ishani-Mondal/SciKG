{"title": [{"text": "Speech Repairs, Intonational Phrases, and Discourse Markers: Modeling Speakers' Utterances in Spoken Dialogue", "labels": [], "entities": [{"text": "Speech Repairs", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.7295596301555634}, {"text": "Modeling Speakers' Utterances in Spoken Dialogue", "start_pos": 61, "end_pos": 109, "type": "TASK", "confidence": 0.6877137025197347}]}], "abstractContent": [{"text": "Interactive spoken dialogue provides many new challenges for natural language understanding systems.", "labels": [], "entities": [{"text": "Interactive spoken dialogue", "start_pos": 0, "end_pos": 27, "type": "TASK", "confidence": 0.5917374889055887}, {"text": "natural language understanding", "start_pos": 61, "end_pos": 91, "type": "TASK", "confidence": 0.6485827167828878}]}, {"text": "One of the most critical challenges is simply determining the speaker's intended utterances: both segmenting a speaker's turn into utterances and determining the intended words in each utterance.", "labels": [], "entities": [{"text": "segmenting a speaker's turn into utterances", "start_pos": 98, "end_pos": 141, "type": "TASK", "confidence": 0.7808262620653424}]}, {"text": "Even assuming perfect word recognition, the latter problem is complicated by the occurrence of speech repairs, which occur where speakers go back and change (or repeat) something they just said.", "labels": [], "entities": [{"text": "word recognition", "start_pos": 22, "end_pos": 38, "type": "TASK", "confidence": 0.7617621123790741}]}, {"text": "The words that are replaced or repeated are no longer part of the intended utterance, and so need to be identified.", "labels": [], "entities": []}, {"text": "Segmenting turns and resolving repairs are strongly intertwined with a third task: identifying discourse markers.", "labels": [], "entities": []}, {"text": "Because of the interactions, and interactions with POS tagging and speech recognition, we need to address these tasks together and early on in the processing stream.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 51, "end_pos": 62, "type": "TASK", "confidence": 0.7714423835277557}, {"text": "speech recognition", "start_pos": 67, "end_pos": 85, "type": "TASK", "confidence": 0.7879626154899597}]}, {"text": "This paper presents a statistical language model in which we rede~'ne the speech recognition problem so that it includes the identification of POS tags, discourse markers, speech repairs, and intonational phrases.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 74, "end_pos": 92, "type": "TASK", "confidence": 0.7127164453268051}, {"text": "speech repairs", "start_pos": 172, "end_pos": 186, "type": "TASK", "confidence": 0.6534723341464996}]}, {"text": "By solving these simultaneously, we obtain better results on each task than addressing them separately.", "labels": [], "entities": []}, {"text": "Our model is able to identify 72% of turn-internal intonational boundaries with a precision of 71%, 97% of discourse markers with 96% precision, and detect and correct 66% of repairs with 74% precision.", "labels": [], "entities": [{"text": "precision", "start_pos": 82, "end_pos": 91, "type": "METRIC", "confidence": 0.9984885454177856}, {"text": "precision", "start_pos": 134, "end_pos": 143, "type": "METRIC", "confidence": 0.9922786951065063}, {"text": "precision", "start_pos": 192, "end_pos": 201, "type": "METRIC", "confidence": 0.9959773421287537}]}], "introductionContent": [{"text": "Consider the following example from the Trains corpus . Example 1 (d93-13.3 utt63) um it'll be there it'll get to Dansville at three a.m. and then you wanna do you take tho-want to take those back to Elmira so engine E two with three boxcars will be back in Elmira at six a.m. is that what you wanna do In order to understand what the speaker was trying to say, the reader probably segmented the above into a number of sentence-like segments, utterances, as follows.", "labels": [], "entities": [{"text": "Trains corpus", "start_pos": 40, "end_pos": 53, "type": "DATASET", "confidence": 0.8231346011161804}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 1  Frequency of discourse markers in the editing term of speech repairs  and as the alteration onset.", "labels": [], "entities": []}, {"text": " Table 2  Size of the Trains corpus.", "labels": [], "entities": [{"text": "Trains corpus", "start_pos": 22, "end_pos": 35, "type": "DATASET", "confidence": 0.7079086154699326}]}, {"text": " Table 6  Using richer histories to estimate probabilities.", "labels": [], "entities": []}, {"text": " Table 7  Effect of modeling discourse markers with  special POS tags.", "labels": [], "entities": []}, {"text": " Table 8  Occurrence of features  boundaries.", "labels": [], "entities": [{"text": "Occurrence", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9541112184524536}]}, {"text": " Table 11  Comparison of errors in detecting intonational phrase boundaries.", "labels": [], "entities": [{"text": "detecting intonational phrase boundaries", "start_pos": 35, "end_pos": 75, "type": "TASK", "confidence": 0.7846920937299728}]}, {"text": " Table 12  Comparison of errors in detecting speech repairs.", "labels": [], "entities": [{"text": "detecting speech repairs", "start_pos": 35, "end_pos": 59, "type": "TASK", "confidence": 0.8388186891873678}]}, {"text": " Table 14  Effect of collapsing modification repairs and fresh starts.", "labels": [], "entities": []}, {"text": " Table 15  Speech repair detection and correction results for full model.", "labels": [], "entities": [{"text": "Speech repair detection", "start_pos": 11, "end_pos": 34, "type": "TASK", "confidence": 0.884368379910787}]}]}