{"title": [{"text": "Beyond Grammar: An Experience-based Theory of Language", "labels": [], "entities": []}], "abstractContent": [], "introductionContent": [{"text": "Over the past few years, Rens Bod and other researchers have investigated Data Oriented Parsing (DOP) approaches to statistical parsing.", "labels": [], "entities": [{"text": "Data Oriented Parsing (DOP)", "start_pos": 74, "end_pos": 101, "type": "TASK", "confidence": 0.7744775414466858}, {"text": "statistical parsing", "start_pos": 116, "end_pos": 135, "type": "TASK", "confidence": 0.7985601127147675}]}, {"text": "This book gives theoretical background, algorithms, and evaluation of DOP models.", "labels": [], "entities": []}, {"text": "The book's initial definition (page 6) is as follows: In accordance with the general DOP architecture outlined by), a particular DOP model is described by specifying settings for the following four parameters: (1) a formal definition of a well-formed representation for utterance-analyses; (2) a definition of the fragments of the utterance-analyses that maybe used as units in constructing an analysis of anew utterance; (3) a set of composition operations by which such fragments maybe combined; and (4) a probability model that indicates how the probability of anew utterance analysis is computed on the basis of the fragments that combine to make it up.", "labels": [], "entities": []}, {"text": "Bod goes onto say: We hypothesize that human language processing can be modeled as a probabilistic process that operates on a corpus of representations of past language experiences, but we leave open how the utteranceanalyses in the corpus are represented, how fragments of these utteranceanalyses maybe combined, and what the details of the probabilistic calculations are.", "labels": [], "entities": []}, {"text": "These definitions are perhaps too general to be useful; in fact, they are probably general enough to include all statistical parsing models in the literature.", "labels": [], "entities": [{"text": "statistical parsing", "start_pos": 113, "end_pos": 132, "type": "TASK", "confidence": 0.6286479830741882}]}, {"text": "Fortunately, an earlier passage (page 5) gives a better idea of the flavor of the approaches in the book and in previous work by Bod: We should not constrain or predefine the productive units beforehand, but take all, arbitrarily large fragments of (previously experienced) utteranceanalyses as possible units and let the statistics decide.", "labels": [], "entities": [{"text": "Bod", "start_pos": 129, "end_pos": 132, "type": "DATASET", "confidence": 0.9656729698181152}]}, {"text": "This philosophy is what really distinguishes DOP from other approaches.", "labels": [], "entities": [{"text": "DOP", "start_pos": 45, "end_pos": 48, "type": "TASK", "confidence": 0.9622626304626465}]}, {"text": "Given a corpus, all subtrees seen in that corpus, regardless of size, are taken to form a grammar--thus the models are sensitive to counts of fragments that vary in size from single context-free rules to entire sentence-tree pairs.", "labels": [], "entities": []}, {"text": "The DOP methods share a common method for estimating probabilities attached to these fragments, and Monte-Carlo style parsing algorithms to search for the most likely tree.", "labels": [], "entities": []}, {"text": "For the rest of this review, I'll take the term \"DOP\" to refer to this narrower definition.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": []}