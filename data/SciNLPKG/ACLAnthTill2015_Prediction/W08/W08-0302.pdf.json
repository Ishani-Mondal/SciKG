{"title": [{"text": "Rich Source-Side Context for Statistical Machine Translation", "labels": [], "entities": [{"text": "Statistical Machine Translation", "start_pos": 29, "end_pos": 60, "type": "TASK", "confidence": 0.8467601339022318}]}], "abstractContent": [{"text": "We explore the augmentation of statistical machine translation models with features of the context of each phrase to be translated.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 31, "end_pos": 62, "type": "TASK", "confidence": 0.6187520821889242}]}, {"text": "This work extends several existing threads of research in statistical MT, including the use of context in example-based machine translation (Carl and Way, 2003) and the incorporation of word sense disambiguation into a translation model (Chan et al., 2007).", "labels": [], "entities": [{"text": "statistical MT", "start_pos": 58, "end_pos": 72, "type": "TASK", "confidence": 0.6225177943706512}, {"text": "machine translation", "start_pos": 120, "end_pos": 139, "type": "TASK", "confidence": 0.7404513955116272}, {"text": "word sense disambiguation", "start_pos": 186, "end_pos": 211, "type": "TASK", "confidence": 0.6097497443358103}]}, {"text": "The context features we consider use surrounding words and part-of-speech tags, local syntactic structure, and other properties of the source language sentence to help predict each phrase's translation.", "labels": [], "entities": []}, {"text": "Our approach requires very little computation beyond the standard phrase extraction algorithm and scales well to large data scenarios.", "labels": [], "entities": [{"text": "phrase extraction", "start_pos": 66, "end_pos": 83, "type": "TASK", "confidence": 0.8309079110622406}]}, {"text": "We report significant improvements in automatic evaluation scores for Chinese-to-English and English-to-German translation, and also describe our entry in the WMT-08 shared task based on this approach.", "labels": [], "entities": [{"text": "English-to-German translation", "start_pos": 93, "end_pos": 122, "type": "TASK", "confidence": 0.662749171257019}, {"text": "WMT-08 shared task", "start_pos": 159, "end_pos": 177, "type": "TASK", "confidence": 0.6029634475708008}]}], "introductionContent": [{"text": "Machine translation (MT) by statistical modeling of bilingual phrases is one of the most successful approaches in the past few years.", "labels": [], "entities": [{"text": "Machine translation (MT)", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.9112528443336487}]}, {"text": "Phrase-based MT systems are straightforward to train from parallel corpora ( and, like the original IBM models (, benefit from standard language models built on large monolingual, target-language corpora ().", "labels": [], "entities": [{"text": "Phrase-based MT", "start_pos": 0, "end_pos": 15, "type": "TASK", "confidence": 0.6546672880649567}]}, {"text": "Many of these systems perform well in competitive evaluations and scale well to large-data situations.", "labels": [], "entities": []}, {"text": "The weights \u03bb 1 , ..., \u03bb M are typically learned to directly minimize a standard evaluation criterion on development data (e.g., the BLEU score;) using numerical search.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 133, "end_pos": 143, "type": "METRIC", "confidence": 0.9751887619495392}]}, {"text": "Many features are used in phrase-based MT, but nearly ubiquitous are estimates of the conditional translation probabilities p(e j i | f k ) and p(f k | e j i ) for each phrase pair e j i , fl kin the candidate sentence pair.", "labels": [], "entities": [{"text": "MT", "start_pos": 39, "end_pos": 41, "type": "TASK", "confidence": 0.7846986055374146}]}, {"text": "In this paper, we add and evaluate fea- In the statistical MT literature, this is often referred to as a \"log-linear model,\" but since the score is normalized during neither parameter training nor decoding, and is never interpreted as a log-probability, it is essentially a linear combination of feature functions.", "labels": [], "entities": [{"text": "MT", "start_pos": 59, "end_pos": 61, "type": "TASK", "confidence": 0.8811137080192566}]}, {"text": "Since many of the features are actually probabilities, this linear combination is closer to a mixture model.", "labels": [], "entities": []}, {"text": "We will use x j i to denote the subsequence of x containing the ith through jth elements of tures that condition on additional context features on the source (f ) side: The advantage of considering context is wellknown and exploited in the example-based MT community.", "labels": [], "entities": [{"text": "MT", "start_pos": 254, "end_pos": 256, "type": "TASK", "confidence": 0.9133800268173218}]}, {"text": "Recently researchers have begun to use source phrase context information in statistical MT systems (.", "labels": [], "entities": [{"text": "MT", "start_pos": 88, "end_pos": 90, "type": "TASK", "confidence": 0.8554922342300415}]}, {"text": "Statistical NLP researchers understand that conditioning a probability model on more information is helpful only if there are sufficient training data to accurately estimate the context probabilities.", "labels": [], "entities": []}, {"text": "Sparse data are often the death of elaborate models, though this can be remedied through careful smoothing.", "labels": [], "entities": []}, {"text": "In this paper we leverage the existing linear model (Equation 2) to bring source-side context into phrase-based MT in away that is robust to data sparseness.", "labels": [], "entities": [{"text": "MT", "start_pos": 112, "end_pos": 114, "type": "TASK", "confidence": 0.8433331847190857}]}, {"text": "We interpret the linear model as a mixture of many probability estimates based on different context features, some of which maybe very sparse.", "labels": [], "entities": []}, {"text": "The mixture coefficients are trained in the usual way (\"minimum error-rate training,\", so that the additional context is exploited when it is useful and ignored when it isn't.", "labels": [], "entities": []}, {"text": "The paper proceeds as follows.", "labels": [], "entities": []}, {"text": "We first review related work that enriches statistical translation models using context ( \u00a72).", "labels": [], "entities": [{"text": "statistical translation", "start_pos": 43, "end_pos": 66, "type": "TASK", "confidence": 0.5947382003068924}]}, {"text": "We then propose a set of source-side features to be incorporated into the translation model, including the novel use of syntactic context from source-side parse trees and global position within f ( \u00a73).", "labels": [], "entities": []}, {"text": "We explain why analogous target-side features pose a computational challenge ( \u00a74).", "labels": [], "entities": []}, {"text": "Specific modifications to the standard training and evaluation paradigm are presented in \u00a75.", "labels": [], "entities": []}, {"text": "Experimental results are reported in \u00a76.", "labels": [], "entities": []}, {"text": "added souce-side context features to a phrase-based translation system, including conditional probabilities of the same form that we use.", "labels": [], "entities": [{"text": "phrase-based translation", "start_pos": 39, "end_pos": 63, "type": "TASK", "confidence": 0.7173165678977966}]}, {"text": "They consider up to two words and/or POS tags of context on either side.", "labels": [], "entities": []}, {"text": "Because of the aforementioned data sparseness problem, they use a decision-tree classifier that implicitly smooths relative frequency estimates.", "labels": [], "entities": []}, {"text": "The method improved over a standard phrase-based baseline trained on small amounts of data (< 50K sentence pairs) for Italian \u2192 English and Chinese \u2192 English.", "labels": [], "entities": []}, {"text": "We explore a significantly larger space of context features, a smoothing method that more naturally fits into the widely used, errordriven linear model, and report a more comprehensive experimental evaluation (including feature comparison and scaling up to very large datasets).", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section we present experimental results using our context-endowed phrase translation model with a variety of different context features, on Chinese \u2192 English, German \u2192 English, and English \u2192 Ger-  man translation tasks.", "labels": [], "entities": [{"text": "phrase translation", "start_pos": 74, "end_pos": 92, "type": "TASK", "confidence": 0.6994173377752304}, {"text": "English \u2192 Ger-  man translation tasks", "start_pos": 189, "end_pos": 226, "type": "TASK", "confidence": 0.5422803929873875}]}, {"text": "Dataset details are given in Appendices A (Chinese) and B (German).", "labels": [], "entities": [{"text": "Dataset", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.8017348051071167}]}, {"text": "Baseline We use the Moses MT system ( ) as a baseline and closely follow the example training procedure given for the WMT-07 and WMT-08 shared tasks.", "labels": [], "entities": [{"text": "Moses MT system", "start_pos": 20, "end_pos": 35, "type": "DATASET", "confidence": 0.8036383589108785}, {"text": "WMT-07 and WMT-08 shared tasks", "start_pos": 118, "end_pos": 148, "type": "DATASET", "confidence": 0.7309976756572724}]}, {"text": "In particular, we perform word alignment in each direction using GIZA++, apply the \"grow-diag-finaland\" heuristic for symmetrization and use a maximum phrase length of 7.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 26, "end_pos": 40, "type": "TASK", "confidence": 0.7648468613624573}]}, {"text": "In addition to the two phrase translation conditionals p(e | f ) and p(f | e), we use lexical translation probabilities in each direction, a word penalty, a phrase penalty, a lengthbased reordering model, a lexicalized reordering model, and an n-gram language model, SRILM implementation) with modified KneserNey smoothing.", "labels": [], "entities": [{"text": "phrase translation conditionals p", "start_pos": 23, "end_pos": 56, "type": "TASK", "confidence": 0.8002284243702888}]}, {"text": "Minimum error-rate (MER) training was applied to obtain weights (\u03bb min Equation 2) for these features.", "labels": [], "entities": [{"text": "Minimum error-rate (MER)", "start_pos": 0, "end_pos": 24, "type": "METRIC", "confidence": 0.8527116417884827}]}, {"text": "A recaser is trained on the target side of the parallel corpus using the script provided with Moses.", "labels": [], "entities": []}, {"text": "All output is recased and detokenized prior to evaluation.", "labels": [], "entities": []}, {"text": "Evaluation We evaluate translation output using three automatic evaluation measures: BLEU), NIST, and METEOR (, version 0.6).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 85, "end_pos": 89, "type": "METRIC", "confidence": 0.9984883069992065}, {"text": "NIST", "start_pos": 92, "end_pos": 96, "type": "DATASET", "confidence": 0.65627521276474}, {"text": "METEOR", "start_pos": 102, "end_pos": 108, "type": "METRIC", "confidence": 0.9830073118209839}]}, {"text": "All measures used were the case-sensitive, corpuslevel versions.", "labels": [], "entities": []}, {"text": "The version of BLEU used was that provided by NIST.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 15, "end_pos": 19, "type": "METRIC", "confidence": 0.9960810542106628}, {"text": "NIST", "start_pos": 46, "end_pos": 50, "type": "DATASET", "confidence": 0.9774320125579834}]}, {"text": "Significance was tested using a paired bootstrap) with 1000 samples (p < 0.05: Chinese \u2192 English experiments: first row shows baseline performance when training only on in-domain data for each task; all other rows show results when training on all data (UN and News).", "labels": [], "entities": [{"text": "Significance", "start_pos": 0, "end_pos": 12, "type": "METRIC", "confidence": 0.9172413945198059}, {"text": "UN and News", "start_pos": 254, "end_pos": 265, "type": "DATASET", "confidence": 0.8458741108576456}]}, {"text": "Left half shows results when tuning and testing on UN test sets; right half shows results when tuning on NIST 2004 News test set and testing on NIST 2005.", "labels": [], "entities": [{"text": "UN test sets", "start_pos": 51, "end_pos": 63, "type": "DATASET", "confidence": 0.9468981027603149}, {"text": "NIST 2004 News test set", "start_pos": 105, "end_pos": 128, "type": "DATASET", "confidence": 0.9742846965789795}, {"text": "NIST 2005", "start_pos": 144, "end_pos": 153, "type": "DATASET", "confidence": 0.9786293804645538}]}, {"text": "For feature selection, an additional set of unseen data was used: 2000 held-out sentences from the UN data for the left half and the NIST 2003 test set for the right half.", "labels": [], "entities": [{"text": "feature selection", "start_pos": 4, "end_pos": 21, "type": "TASK", "confidence": 0.7729268968105316}, {"text": "UN data", "start_pos": 99, "end_pos": 106, "type": "DATASET", "confidence": 0.9026260375976562}, {"text": "NIST 2003 test set", "start_pos": 133, "end_pos": 151, "type": "DATASET", "confidence": 0.968190386891365}]}, {"text": "Boldface marks scores that are significantly higher than the first row, in-domain baseline.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Chinese \u2192 English experiments: training and  testing on UN data. Boldface marks scores significantly  higher than \"None.\"", "labels": [], "entities": [{"text": "UN data", "start_pos": 66, "end_pos": 73, "type": "DATASET", "confidence": 0.7698172628879547}]}, {"text": " Table 5: English \u2192 German shared task system results using WMT-08 Europarl parallel data for training, dev06 for  tuning, and three test sets, including the final 2008 test set. The row labeled \"Context\" uses the top-performing feature  set {2 POS on left, 1 word on right}. Boldface marks scores that are significantly higher than the baseline.", "labels": [], "entities": [{"text": "WMT-08 Europarl parallel data", "start_pos": 60, "end_pos": 89, "type": "DATASET", "confidence": 0.8722204864025116}]}]}