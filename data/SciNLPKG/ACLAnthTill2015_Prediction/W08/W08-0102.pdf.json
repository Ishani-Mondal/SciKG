{"title": [{"text": "Response-Based Confidence Annotation for Spoken Dialogue Systems", "labels": [], "entities": [{"text": "Response-Based Confidence Annotation", "start_pos": 0, "end_pos": 36, "type": "TASK", "confidence": 0.6467110514640808}]}], "abstractContent": [{"text": "Spoken and multimodal dialogue systems typically make use of confidence scores to choose among (or reject) a speech recognizer's N-best hypotheses fora particular utterance.", "labels": [], "entities": []}, {"text": "We argue that it is beneficial to instead choose among a list of candidate system responses.", "labels": [], "entities": []}, {"text": "We propose a novel method in which a confidence score for each response is derived from a classifier trained on acoustic and lexical features emitted by the recognizer, as well as features culled from the generation of the candidate response itself.", "labels": [], "entities": []}, {"text": "Our response-based method yields statistically significant improvements in F-measure over a baseline in which hypotheses are chosen based on recognition confidence scores only.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 75, "end_pos": 84, "type": "METRIC", "confidence": 0.9803723692893982}]}], "introductionContent": [{"text": "The fundamental task for any spoken dialogue system is to determine how to respond at any given time to a user's utterance.", "labels": [], "entities": []}, {"text": "The challenge of understanding and correctly responding to a user's natural language utterance is formidable even when the words have been perfectly transcribed.", "labels": [], "entities": [{"text": "understanding and correctly responding to a user's natural language utterance", "start_pos": 17, "end_pos": 94, "type": "TASK", "confidence": 0.5026287002996965}]}, {"text": "However, dialogue system designers face a greater challenge because the speech recognition hypotheses which serve as input to the natural language understanding components of a system are often quite errorful; indeed, it is not uncommon to find word error rates of 20-30% for many dialogue systems underdevelopment in research labs.", "labels": [], "entities": [{"text": "speech recognition hypotheses", "start_pos": 72, "end_pos": 101, "type": "TASK", "confidence": 0.7935411731402079}]}, {"text": "Such high error rates often arise due to the use of out-of-vocabulary words, noise, and the increasingly large vocabularies of more capable systems which try to allow for greater naturalness and variation in user input.", "labels": [], "entities": []}, {"text": "Traditionally, dialogue systems have relied on confidence scores assigned by the speech recognizer to detect speech recognition errors.", "labels": [], "entities": [{"text": "speech recognition errors", "start_pos": 109, "end_pos": 134, "type": "TASK", "confidence": 0.7313074469566345}]}, {"text": "Ina typical setup, the dialogue system will choose to either accept (that is, attempt to understand and respond to) or reject (that is, respond to the user with an indication of non-understanding) an utterance by thresholding this confidence score.", "labels": [], "entities": []}, {"text": "Stating the problem in terms of choosing whether or not to accept a particular utterance for processing, however, misses the larger picture.", "labels": [], "entities": []}, {"text": "From the user's perspective, what is truly important is whether or not the system's response to the utterance is correct.", "labels": [], "entities": []}, {"text": "Sometimes, an errorful recognition hypothesis may result in a correct response if, for example, proper names are correctly recognized; conversely, a near-perfect hypothesis may evoke an incorrect response.", "labels": [], "entities": []}, {"text": "In light of this, the problem at hand is better formulated as one of assigning a confidence score to a system's candidate response which reflects the probability that the response is an acceptable one.", "labels": [], "entities": []}, {"text": "If the system can't formulate a response in which it has high confidence, then it should clarify, indicate non-understanding, and/or provide appropriate help.", "labels": [], "entities": []}, {"text": "In this paper, we present a method for assigning confidence scores to candidate system responses by making use not only of features obtained from the speech recognizer, but also of features culled from the process of generating a candidate system response, and derived from the distribution of candidate responses themselves.", "labels": [], "entities": []}, {"text": "We first compile a list of unique candidate system responses by processing each hypothesis on the recognizer's N-best list.", "labels": [], "entities": []}, {"text": "We then train a Support Vector Machine (SVM) to identify acceptable responses.", "labels": [], "entities": []}, {"text": "When given a novel utterance, candidate responses are ranked with scores output from the SVM.", "labels": [], "entities": [{"text": "SVM", "start_pos": 89, "end_pos": 92, "type": "DATASET", "confidence": 0.9521250128746033}]}, {"text": "Based on the scores, the system can then either respond with the highest-scoring candidate, or reject all of the candidate responses and respond by indicating non-understanding.", "labels": [], "entities": []}, {"text": "Part of the motivation for focusing our efforts on selecting a system response, rather than a recognition hypothesis, can be demonstrated by counting the number of unique responses which can be derived from an N-best list.", "labels": [], "entities": []}, {"text": "plots the mean number of unique system responses, parses, and recognition hypotheses given a particular maximum N-best list length; it was generated using the data described in section 3.", "labels": [], "entities": []}, {"text": "Generally, we observe that about half as many unique parses are generated as recognition hypotheses, and then half again as many unique responses.", "labels": [], "entities": []}, {"text": "Since many hypotheses evoke the same response, there is no value in discriminating among these hypotheses.", "labels": [], "entities": []}, {"text": "Instead, we should aim to gain information about the quality of a response by pooling knowledge gleaned from each hypothesis evoking that response.", "labels": [], "entities": []}, {"text": "We expect a similar trend of multiple hypotheses mapping to a single parse in any dialogue system where parses contain a mixture of key syntactic and semantic structure-as is the case here-or where they contain only semantic information (e.g., slot/value pairs).", "labels": [], "entities": []}, {"text": "Parsers which retain more syntactic structure would likely generate more unique parses, however many of these parses would probably map to the same system response since a response doesn't typically hinge on every syntactic detail of an input utterance.", "labels": [], "entities": []}, {"text": "The remainder of our discussion proceeds as follows.", "labels": [], "entities": []}, {"text": "In section 2 we place the method presented herein context in relation to other research.", "labels": [], "entities": []}, {"text": "In section 3, we describe the City Browser multimodal dialogue system, and the process used to collect data from users' interactions with the system.", "labels": [], "entities": []}, {"text": "We then turn to our techniques for annotating the data in section 4 and describe the features which are extracted from the labeled data in section 5.", "labels": [], "entities": []}, {"text": "Finally, we demonstrate how to build a classifier to rank candidate system responses in section 6, which we evaluate in section 7.: The mean N-best recognition hypothesis list length, mean number of unique parses derived from the N-best list of recognition hypotheses, and mean number of unique system responses derived from those parses, given a maximum recognition N-best list length.", "labels": [], "entities": []}], "datasetContent": [{"text": "The data used for the experiments which follow were collected from user interactions with City Browser, a web-based, multimodal dialogue system.", "labels": [], "entities": []}, {"text": "A thorough description of the architecture and capabilities can be found in (.", "labels": [], "entities": []}, {"text": "Briefly, the version of City Browser used for the experiments in this paper allows users to access information about restaurants, museums, and subway stations by navigating to a web page on their own computers.", "labels": [], "entities": []}, {"text": "They can also locate addresses on the map, and obtain driving directions.", "labels": [], "entities": []}, {"text": "Users can interact with City Browser's map-based graphical user interface by clicking and drawing; and they can speak with it by talking into their computer microphone and listening to a response from their speakers.", "labels": [], "entities": []}, {"text": "Speech recognition is performed via the SUMMIT recognizer, using a trigram language model with dynamically updatable classes for proper nouns such as city, street, and restaurant names-see () fora description of this capability.", "labels": [], "entities": [{"text": "Speech recognition", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.8125785887241364}, {"text": "SUMMIT recognizer", "start_pos": 40, "end_pos": 57, "type": "TASK", "confidence": 0.7463457882404327}]}, {"text": "Speech recognition results were parsed by the TINA parser (Seneff, 1992) using a hand-crafted grammar.", "labels": [], "entities": [{"text": "Speech recognition", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.7991687655448914}, {"text": "TINA parser (Seneff, 1992)", "start_pos": 46, "end_pos": 72, "type": "DATASET", "confidence": 0.8403361354555402}]}, {"text": "A discourse module () then integrates contextual knowledge.", "labels": [], "entities": []}, {"text": "The fully formed request is sent to the dialogue manager, which attempts to craft an appropriate system response-both in terms of a verbal and graphical response.", "labels": [], "entities": []}, {"text": "The GENESIS system) uses hand-crafted generation rules to produce a natural language string, which is sent to an off-the-shelf text-to-speech synthesizer.", "labels": [], "entities": [{"text": "GENESIS", "start_pos": 4, "end_pos": 11, "type": "DATASET", "confidence": 0.8802919387817383}]}, {"text": "Finally, the user hears the response, and the graphical user interface is updated to show, for example, a set of search results on the map.", "labels": [], "entities": []}, {"text": "We evaluated the response-based method using the data described in section 3, N-best lists with a maximum length of 10, and an SVM with a linear kernel.", "labels": [], "entities": []}, {"text": "We note that, in the live system, two-pass recognition is performed for some utterances, in which a key concept recognized in the first pass (e.g., a city name) causes a dynamic update to the contents of a class in the n-gram language model (e.g., a set of street names) for the second pass-as in the utterance Show me thirty two Vassar Street in Cambridge where the city name (Cambridge) triggers a second pass in which the streets in that city are given a higher weight.", "labels": [], "entities": [{"text": "two-pass recognition", "start_pos": 34, "end_pos": 54, "type": "TASK", "confidence": 0.7160268127918243}]}, {"text": "This two-pass approach has been shown previously to decrease word and concept error rates ( ), even though it can be susceptible to errors in understanding.", "labels": [], "entities": [{"text": "word and concept error rates", "start_pos": 61, "end_pos": 89, "type": "METRIC", "confidence": 0.5400936603546143}]}, {"text": "However, since all street names, for example, are active in the vocabulary at all times, the twopass approach is not strictly necessary to arrive at the correct hypotheses.", "labels": [], "entities": []}, {"text": "Hence, for simplicity, in the experiments reported here, we do not integrate the two-pass approach-as this would require us to potentially do a second recognition pass for every candidate response.", "labels": [], "entities": []}, {"text": "Ina live system, a good strategy might be to consider a second recognition pass based on the top few candidate responses alone, which would produce anew set of candidates to be scored.", "labels": [], "entities": []}, {"text": "We performed 38-fold cross validation, wherein each case the held-out test set was comprised of all the utterances of a single user.", "labels": [], "entities": []}, {"text": "This ensured that we obtained an accurate prediction of a novel user's experience, although it meant that the test sets were not of equal size.", "labels": [], "entities": []}, {"text": "We calculated F-measure for each test set, using the methodology described in (in the appendix).", "labels": [], "entities": [{"text": "F-measure", "start_pos": 14, "end_pos": 23, "type": "METRIC", "confidence": 0.9961918592453003}]}], "tableCaptions": [{"text": " Table 2: Average F-measures obtained via per-user  cross-validation of the response-based confidence scor- ing method using the feature sets described in Section 5,  as compared to a baseline system which chooses the top  hypothesis if the recognizer confidence score exceeds an  optimized rejection threshold. The starred scores are a  statistically significant (* indicates p < .05, ** indicates  p < .01) improvement over the baseline, as determined  by a paired t-test.", "labels": [], "entities": [{"text": "F-measures", "start_pos": 18, "end_pos": 28, "type": "METRIC", "confidence": 0.9778380990028381}, {"text": "response-based confidence scor- ing", "start_pos": 76, "end_pos": 111, "type": "TASK", "confidence": 0.6323644280433655}]}]}