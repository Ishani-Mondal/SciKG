{"title": [{"text": "Learning N-Best Correction Models from Implicit User Feedback in a Multi-Modal Local Search Application", "labels": [], "entities": []}], "abstractContent": [{"text": "We describe a novel n-best correction model that can leverage implicit user feedback (in the form of clicks) to improve performance in a multi-modal speech-search application.", "labels": [], "entities": [{"text": "n-best correction", "start_pos": 20, "end_pos": 37, "type": "TASK", "confidence": 0.720714271068573}]}, {"text": "The proposed model works in two stages.", "labels": [], "entities": []}, {"text": "First, the n-best list generated by the speech recognizer is expanded with additional candidates, based on confusability information captured via user click statistics.", "labels": [], "entities": [{"text": "speech recognizer", "start_pos": 40, "end_pos": 57, "type": "TASK", "confidence": 0.7060220837593079}]}, {"text": "In the second stage, this expanded list is rescored and pruned to produce a more accurate and compact n-best list.", "labels": [], "entities": []}, {"text": "Results indicate that the proposed n-best correction model leads to significant improvements over the existing baseline, as well as other traditional n-best rescoring approaches.", "labels": [], "entities": [{"text": "n-best correction", "start_pos": 35, "end_pos": 52, "type": "TASK", "confidence": 0.6056561768054962}]}], "introductionContent": [{"text": "Supported by years of research in speech recognition and related technologies, as well as advances in mobile devices, speech-enabled mobile applications are finally transitioning into day-to-day use.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 34, "end_pos": 52, "type": "TASK", "confidence": 0.7707196772098541}]}, {"text": "One example is Live Search for Windows, a speech-enabled application that allows users to get access to local information by speaking a query into their device.", "labels": [], "entities": []}, {"text": "Several other systems operating in similar domains have recently become available Traditionally, multi-modal systems leverage the additional input channels such as text or buttons to compensate for the current shortcomings of speech recognition technology.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 226, "end_pos": 244, "type": "TASK", "confidence": 0.7113294303417206}]}, {"text": "For instance, after the user speaks a query, the Live Search for Windows Mobile application displays a confirmation screen that contains the n-best recognition results.", "labels": [], "entities": []}, {"text": "The user selects the correct hypothesis using the buttons on the device, and only then the system displays the corresponding search results (see.)", "labels": [], "entities": []}, {"text": "We argue that ideally multi-modal systems could use the additional, more accurate input channels not only for confirmation or immediate correction, but also to learn from the interaction and improve their performance overtime, without explicit human supervision.", "labels": [], "entities": [{"text": "confirmation or immediate correction", "start_pos": 110, "end_pos": 146, "type": "TASK", "confidence": 0.6037767305970192}]}, {"text": "For example, in the interaction paradigm described above, apart from providing the means for selecting the correct recognition result from an n-best list, the user click on a hypothesis can provide valuable information about the errors made by system, which could be exploited to further improve performance.", "labels": [], "entities": []}, {"text": "Consider for instance the following numbers from an analysis of logged click data in the Live Search for Windows Mobile system.", "labels": [], "entities": []}, {"text": "Over a certain period of time, the results Beer and Gear were displayed together in an n-best list 122 times.", "labels": [], "entities": []}, {"text": "Out of these cases, Beer was clicked 67% of the time, and Gear was never clicked.", "labels": [], "entities": [{"text": "Beer", "start_pos": 20, "end_pos": 24, "type": "DATASET", "confidence": 0.8477481007575989}]}, {"text": "In 25% of the cases when Beer was selected, Gear was incorrectly presented above (i.e. higher than) Beer in the n-best list.", "labels": [], "entities": [{"text": "Beer", "start_pos": 25, "end_pos": 29, "type": "DATASET", "confidence": 0.8533450365066528}, {"text": "Gear", "start_pos": 44, "end_pos": 48, "type": "METRIC", "confidence": 0.8930801153182983}]}, {"text": "More importantly, there are also 82 cases in which Gear appears in an n-best list, but Beer does not.", "labels": [], "entities": []}, {"text": "A manual inspection reveals that, in 22% of these cases, the actual spoken utterance was indeed Beer.", "labels": [], "entities": []}, {"text": "The clicks therefore indicate that the engine often misrecognizes Gear instead of Beer.", "labels": [], "entities": [{"text": "Gear", "start_pos": 66, "end_pos": 70, "type": "METRIC", "confidence": 0.6883602142333984}]}, {"text": "Ideally, the system should be able to take advantage of this information and use the clicks to create an automatic positive feedback loop.", "labels": [], "entities": []}, {"text": "We can envision several ways in which this could be accomplished.", "labels": [], "entities": []}, {"text": "A possible approach would be to use all the clicked results to adapt the existing language or acoustic models.", "labels": [], "entities": []}, {"text": "Another, higher-level approach is to treat the recognition process as a black-box, and use the click feedback (perhaps also in conjunction with other high-level information) to post-process the results recognition results.", "labels": [], "entities": []}, {"text": "While both approaches have their merits, in this work we concentrate on the latter paradigm.", "labels": [], "entities": []}, {"text": "We introduce a novel n-best correction model that leverages the click data to improve performance in a speech-enabled multi-modal application.", "labels": [], "entities": []}, {"text": "The proposed model works in two stages.", "labels": [], "entities": []}, {"text": "First, the n-best list generated by the speech recognizer is expanded with additional candidates, based on results confusability information captured by the click statistics.", "labels": [], "entities": [{"text": "speech recognizer", "start_pos": 40, "end_pos": 57, "type": "TASK", "confidence": 0.6998863518238068}]}, {"text": "For instance, in the 82 cases we mentioned above when Gear was present in the n-best list but Beer was not, Beer (as well as potentially other results) would also be added to form an expanded n-best list.", "labels": [], "entities": []}, {"text": "The expanded list is then rescored and pruned to construct a corrected, more accurate n-best list.", "labels": [], "entities": []}, {"text": "The proposed approach, described in detail in Section 3, draws inspiration from earlier work in post-recognition error-correction models () and nbest rescoring.", "labels": [], "entities": []}, {"text": "The novelty of our approach lies in: (1) the use of user click data in a deployed multi-modal system for creating a positive feedback loop, and (2) the development of an n-best correction model based on implicit feedback that outperforms traditional rescoring-only approaches.", "labels": [], "entities": []}, {"text": "Later on, in Section 5, we will discuss in more detail the relationship of the proposed approach to these and other works previously reported in the literature.", "labels": [], "entities": []}, {"text": "Before moving onto describe the n-best correction model in more detail, we give a high-level overview of Live Search for Windows Mobile, the multi-modal, mobile local search application that provided the test-bed for evaluating this work.", "labels": [], "entities": []}], "datasetContent": [{"text": "We now discuss a number of experiments and the results obtained using the proposed n-best correction approach.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1. Test-set sentence-level n-best accuracy;  (0) baseline; (1)-(3) initial n-best rescoring;  (4)-", "labels": [], "entities": [{"text": "accuracy", "start_pos": 41, "end_pos": 49, "type": "METRIC", "confidence": 0.9777326583862305}, {"text": "baseline", "start_pos": 56, "end_pos": 64, "type": "METRIC", "confidence": 0.9818982481956482}]}]}