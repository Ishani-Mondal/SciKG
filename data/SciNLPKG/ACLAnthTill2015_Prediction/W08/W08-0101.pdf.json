{"title": [{"text": "Optimizing Endpointing Thresholds using Dialogue Features in a Spoken Dialogue System", "labels": [], "entities": []}], "abstractContent": [{"text": "This paper describes a novel algorithm to dynamically set endpointing thresholds based on a rich set of dialogue features to detect the end of user utterances in a dialogue system.", "labels": [], "entities": []}, {"text": "By analyzing the relationship between silences in user's speech to a spoken dialogue system and a wide range of automatically extracted features from discourse, semantics, prosody, timing and speaker characteristics, we found that all features correlate with pause duration and with whether a silence indicates the end of the turn, with semantics and timing being the most informative.", "labels": [], "entities": [{"text": "timing", "start_pos": 181, "end_pos": 187, "type": "METRIC", "confidence": 0.9745620489120483}]}, {"text": "Based on these features, the proposed method reduces latency by up to 24% over a fixed threshold baseline.", "labels": [], "entities": [{"text": "latency", "start_pos": 53, "end_pos": 60, "type": "METRIC", "confidence": 0.9528693556785583}]}, {"text": "Offline evaluation results were confirmed by implementing the proposed algorithm in the Let's Go system.", "labels": [], "entities": [{"text": "Let's Go system", "start_pos": 88, "end_pos": 103, "type": "DATASET", "confidence": 0.6769190281629562}]}], "introductionContent": [], "datasetContent": [{"text": "We evaluated the approach introduced in Section 2 on the Let's Go corpus.", "labels": [], "entities": [{"text": "Let's Go corpus", "start_pos": 57, "end_pos": 72, "type": "DATASET", "confidence": 0.5886668935418129}]}, {"text": "The set of features was extended to contain a total of 4 discourse features, 6 semantic features, 5 timing/turn-taking features, 43 prosodic features, and 6 speaker characteristic features.", "labels": [], "entities": []}, {"text": "All evaluations were performed by 10-fold cross-validation on the corpus.", "labels": [], "entities": []}, {"text": "Based on the proposed algorithm, we built a decision tree and computed optimal cluster thresholds for different overall FA rates.", "labels": [], "entities": [{"text": "FA", "start_pos": 120, "end_pos": 122, "type": "METRIC", "confidence": 0.9813634157180786}]}, {"text": "We report average latency as a function of the proportion of turns for which any pause was erroneously endpointed, which is closer to real performance than silence FA rate since, once a turn has been endpointed, all subsequent silences are irrelevant.", "labels": [], "entities": [{"text": "silence FA rate", "start_pos": 156, "end_pos": 171, "type": "METRIC", "confidence": 0.9406310121218363}]}, {"text": "We confirmed the offline evaluation's findings by implementing the proposed approach in Let's Go's Interaction Manager.", "labels": [], "entities": [{"text": "Let's Go's Interaction Manager", "start_pos": 88, "end_pos": 118, "type": "DATASET", "confidence": 0.5356467813253403}]}, {"text": "Since prosodic features were not found to be helpful and since their online extraction is costly and error-prone, we did not include them.", "labels": [], "entities": []}, {"text": "At the beginning of each dialogue, the system was randomly set as a baseline version, using a 700 ms fixed threshold, or as an experimental version using the tree learned from the offline corpus.", "labels": [], "entities": []}, {"text": "Results show that median latency (which includes both the endpointing threshold and the time to produce the system's response) is significantly shorter in the experimental version (561 ms) than in the baseline (957 ms).", "labels": [], "entities": [{"text": "median latency", "start_pos": 18, "end_pos": 32, "type": "METRIC", "confidence": 0.8470678627490997}]}, {"text": "Overall, the proposed approach reduced latency by 50% or more in about 48% of the turns.", "labels": [], "entities": [{"text": "latency", "start_pos": 39, "end_pos": 46, "type": "METRIC", "confidence": 0.993504524230957}]}, {"text": "However, global results like these might not reflect the actual improvement in user experience.", "labels": [], "entities": []}, {"text": "Indeed, we know from human-human dialogues that relatively long latencies are normal in some circumstances while very short or no latency is expected in others.", "labels": [], "entities": []}, {"text": "The proposed algorithm reproduces some of these aspects.", "labels": [], "entities": []}, {"text": "For example, after open questions, where more uncertainty and variability is expected, the experimental version is in fact slightly slower (1047 ms vs 993 ms).", "labels": [], "entities": []}, {"text": "On the other hand, it is faster after closed question (800 ms vs 965 ms) and particularly after confirmation requests (324 ms vs 965 ms), which are more predictable parts of the dialogue where high responsiveness is both achievable and natural.", "labels": [], "entities": []}, {"text": "This latter result indicates that our approach has the potential to improve explicit confirmations, which are often thought to be tedious and irritating to the user.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Effect of Dialogue Features on Pause Finality. In columns 3 and 4, the first number is for silences for which  the condition in column 2 is true, while the second number is for those silences where the condition is false. * indicates  that the results are not statistically significant at the 0.01 level.", "labels": [], "entities": [{"text": "Pause Finality", "start_pos": 41, "end_pos": 55, "type": "TASK", "confidence": 0.7866542041301727}]}]}