{"title": [{"text": "Meteor, m-bleu and m-ter: Evaluation Metrics for High-Correlation with Human Rankings of Machine Translation Output", "labels": [], "entities": [{"text": "Machine Translation Output", "start_pos": 89, "end_pos": 115, "type": "TASK", "confidence": 0.7132344047228495}]}], "abstractContent": [{"text": "This paper describes our submissions to the machine translation evaluation shared task in ACL WMT-08.", "labels": [], "entities": [{"text": "machine translation evaluation shared task", "start_pos": 44, "end_pos": 86, "type": "TASK", "confidence": 0.8783665657043457}, {"text": "ACL WMT-08", "start_pos": 90, "end_pos": 100, "type": "DATASET", "confidence": 0.7857441306114197}]}, {"text": "Our primary submission is the Meteor metric tuned for optimizing correlation with human rankings of translation hypotheses.", "labels": [], "entities": [{"text": "Meteor metric", "start_pos": 30, "end_pos": 43, "type": "DATASET", "confidence": 0.8178396821022034}]}, {"text": "We show significant improvement in correlation as compared to the earlier version of metric which was tuned to optimized correlation with traditional adequacy and fluency judgments.", "labels": [], "entities": [{"text": "correlation", "start_pos": 35, "end_pos": 46, "type": "METRIC", "confidence": 0.9916085600852966}]}, {"text": "We also describe m-bleu and m-ter, enhanced versions of two other widely used metrics bleu and ter respectively, which extend the exact word matching used in these metrics with the flexible matching based on stemming and Wordnet in Meteor .", "labels": [], "entities": [{"text": "Wordnet", "start_pos": 221, "end_pos": 228, "type": "DATASET", "confidence": 0.958164632320404}]}], "introductionContent": [{"text": "Automatic Metrics for MT evaluation have been receiving significant attention in recent years.", "labels": [], "entities": [{"text": "MT evaluation", "start_pos": 22, "end_pos": 35, "type": "TASK", "confidence": 0.9819630980491638}]}, {"text": "Evaluating an MT system using such automatic metrics is much faster, easier and cheaper compared to human evaluations, which require trained bilingual evaluators.", "labels": [], "entities": [{"text": "MT", "start_pos": 14, "end_pos": 16, "type": "TASK", "confidence": 0.9845166802406311}]}, {"text": "The most commonly used MT evaluation metric in recent years has been IBM's Bleu metric ().", "labels": [], "entities": [{"text": "MT evaluation", "start_pos": 23, "end_pos": 36, "type": "TASK", "confidence": 0.9489502012729645}, {"text": "Bleu metric", "start_pos": 75, "end_pos": 86, "type": "METRIC", "confidence": 0.8648054599761963}]}, {"text": "Bleu is fast and easy to run, and it can be used as a target function in parameter optimization training procedures that are commonly used in state-of-the-art statistical MT systems.", "labels": [], "entities": [{"text": "Bleu", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.9011459350585938}, {"text": "parameter optimization training", "start_pos": 73, "end_pos": 104, "type": "TASK", "confidence": 0.7918300231297811}, {"text": "MT", "start_pos": 171, "end_pos": 173, "type": "TASK", "confidence": 0.7600134611129761}]}, {"text": "Various researchers have noted, however, various weaknesses in the metric.", "labels": [], "entities": []}, {"text": "Most notably, Bleu does not produce very reliable sentence-level scores.", "labels": [], "entities": []}, {"text": "Meteor , as well as several other proposed metrics such as GTM (), TER) and CDER () aim to address some of these weaknesses.", "labels": [], "entities": [{"text": "Meteor", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.855767011642456}, {"text": "TER", "start_pos": 67, "end_pos": 70, "type": "METRIC", "confidence": 0.9958951473236084}]}, {"text": "Meteor , initially proposed and released in 2004 () was explicitly designed to improve correlation with human judgments of MT quality at the segment level.", "labels": [], "entities": [{"text": "Meteor", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.7914618253707886}, {"text": "MT", "start_pos": 123, "end_pos": 125, "type": "TASK", "confidence": 0.9895250797271729}]}, {"text": "Previous publications on have described the details underlying the metric and have extensively compared its performance with Bleu and several other MT evaluation metrics.", "labels": [], "entities": [{"text": "Bleu", "start_pos": 125, "end_pos": 129, "type": "METRIC", "confidence": 0.9571315050125122}, {"text": "MT evaluation", "start_pos": 148, "end_pos": 161, "type": "TASK", "confidence": 0.9030266106128693}]}, {"text": "In, we described the process of tuning free parameters within the metric to optimize the correlation with human judgments and the extension of the metric for evaluating translations in languages other than English.", "labels": [], "entities": []}, {"text": "This paper provides a brief technical description of Meteor and describes our experiments in re-tuning the metric for improving correlation with the human rankings of translation hypotheses corresponding to a single source sentence.", "labels": [], "entities": [{"text": "Meteor", "start_pos": 53, "end_pos": 59, "type": "DATASET", "confidence": 0.8611337542533875}]}, {"text": "Our experiments show significant improvement in correlation as a result of retuning which shows the importance of having a metric tunable to different testing conditions.", "labels": [], "entities": [{"text": "correlation", "start_pos": 48, "end_pos": 59, "type": "METRIC", "confidence": 0.9925302863121033}]}, {"text": "Also, in order to establish the usefulness of the flexible matching based on stemming and Wordnet, we extend two other widely used metrics bleu and ter which use exact word matching, with the matcher module of Meteor .", "labels": [], "entities": [{"text": "Wordnet", "start_pos": 90, "end_pos": 97, "type": "DATASET", "confidence": 0.9677780270576477}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Corpus Statistics for Various Languages", "labels": [], "entities": []}, {"text": " Table 2: Optimal Values of Tuned Parameters for Various  Languages", "labels": [], "entities": []}, {"text": " Table 3: Average Spearman Correlation with Human  Rankings for Meteor on Development Data", "labels": [], "entities": [{"text": "Spearman Correlation", "start_pos": 18, "end_pos": 38, "type": "TASK", "confidence": 0.6042056977748871}, {"text": "Meteor on Development Data", "start_pos": 64, "end_pos": 90, "type": "DATASET", "confidence": 0.6679964065551758}]}, {"text": " Table 4: Average Spearman Correlation with Human  Rankings for m-bleu and m-ter", "labels": [], "entities": [{"text": "Average Spearman Correlation", "start_pos": 10, "end_pos": 38, "type": "METRIC", "confidence": 0.6915779312451681}]}]}