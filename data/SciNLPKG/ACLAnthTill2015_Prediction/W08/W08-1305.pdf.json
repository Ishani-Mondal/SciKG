{"title": [{"text": "Parser Evaluation across Frameworks without Format Conversion", "labels": [], "entities": [{"text": "Parser Evaluation", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.8807211518287659}]}], "abstractContent": [{"text": "In the area of parser evaluation, formats like GR and SD which are based on dependencies, the simplest representation of syntactic information, are proposed as framework-independent metrics for parser evaluation.", "labels": [], "entities": [{"text": "parser evaluation", "start_pos": 15, "end_pos": 32, "type": "TASK", "confidence": 0.9396410286426544}, {"text": "parser evaluation", "start_pos": 194, "end_pos": 211, "type": "TASK", "confidence": 0.9011769890785217}]}, {"text": "The assumption behind these proposals is that the simplicity of dependencies would make conversion from syntactic structures and semantic representations used in other formalisms to GR/SD a easy job.", "labels": [], "entities": []}, {"text": "But (Miyao et al., 2007) reports that even conversion between these two formats is not easy at all.", "labels": [], "entities": [{"text": "conversion", "start_pos": 43, "end_pos": 53, "type": "TASK", "confidence": 0.9668673872947693}]}, {"text": "Not to mention that the 80% success rate of conversion is not meaningful for parsers that boast 90% accuracy.", "labels": [], "entities": [{"text": "conversion", "start_pos": 44, "end_pos": 54, "type": "TASK", "confidence": 0.9696862101554871}, {"text": "accuracy", "start_pos": 100, "end_pos": 108, "type": "METRIC", "confidence": 0.997006356716156}]}, {"text": "In this paper, we make an attempt at evaluation across frameworks without format conversion.", "labels": [], "entities": [{"text": "format conversion", "start_pos": 74, "end_pos": 91, "type": "TASK", "confidence": 0.6716376394033432}]}, {"text": "This is achieved by generating a list of names of phenomena with each parse.", "labels": [], "entities": []}, {"text": "These names of phenomena are matched against the phenomena given in the gold standard.", "labels": [], "entities": []}, {"text": "The number of matches found is used for evaluating the parser that produces the parses.", "labels": [], "entities": []}, {"text": "The evaluation method is more effective than evaluation methods which involve format conversion because the generation of names of phenomena from the output of a parser loaded is done by a rec-ognizer that has a 100% success rate of recognizing a phenomenon illustrated by a sentence.", "labels": [], "entities": [{"text": "format conversion", "start_pos": 78, "end_pos": 95, "type": "TASK", "confidence": 0.7310259789228439}]}, {"text": "The success rate is made possible by the reuse of native codes: codes c 2008.", "labels": [], "entities": []}, {"text": "Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/).", "labels": [], "entities": []}, {"text": "used for writing the parser and rules of the grammar loaded into the parser.", "labels": [], "entities": []}], "introductionContent": [{"text": "The traditional evaluation method fora deep parser is to test it against a list of sentences, each of which is paired with a yes or no.", "labels": [], "entities": []}, {"text": "The parser is evaluated on the number of grammatical sentences it accepts and that of ungrammatical sentences it rules out.", "labels": [], "entities": []}, {"text": "A problem with this approach to evaluation is that it neither penalizes a parser forgetting an analysis wrong fora sentence nor rewards it forgetting it right.", "labels": [], "entities": []}, {"text": "What prevents the NLP community from working out a universally applicable reward and penalty scheme is the absence of a gold standard that can be used across frameworks.", "labels": [], "entities": []}, {"text": "The correctness of an analysis produced by a parser can only be judged by matching it to the analysis produced by linguists in syntactic structures and semantic representations created specifically for the framework on which the grammar is based.", "labels": [], "entities": []}, {"text": "A match or a mismatch between analyses produced by different parsers based on different frameworks does not lend itself fora meaningful comparison that leads to a fair evaluation of the parsers.", "labels": [], "entities": []}, {"text": "To evaluate two parsers across frameworks, two kinds of methods suggest themselves: 1.", "labels": [], "entities": []}, {"text": "Converting an analysis given in a certain format native to one framework to another native to a differernt framework (e.g. converting from a CCG) derivation tree to an HPSG ( phrase structure tree with AVM) 2.", "labels": [], "entities": []}, {"text": "Converting analyses given in different framework-specific formats to some simpler format proposed as a framework-independent evaluation schema (e.g. converting from HPSG phrase structure tree with AVM to GR)) However, the feasibility of either solution is questionable.", "labels": [], "entities": []}, {"text": "Even conversion between two evaluation schemata which make use of the simplest representation of syntactic information in the form of dependencies is reported to be problematic by.", "labels": [], "entities": []}, {"text": "In this paper, therefore, we propose a different method of parser evaluation that makes no attempt at any conversion of syntactic structures and semantic representations.", "labels": [], "entities": [{"text": "parser evaluation", "start_pos": 59, "end_pos": 76, "type": "TASK", "confidence": 0.928708404302597}]}, {"text": "We remove the need for such conversion by abstracting away from comparison of syntactic structures and semantic representations.", "labels": [], "entities": []}, {"text": "The basic idea is to generate a list of names of phenomena with each parse.", "labels": [], "entities": []}, {"text": "These names of phenomena are matched against the phenomena given in the gold standard for the same sentence.", "labels": [], "entities": []}, {"text": "The number of matches found is used for evaluating the parser that produces the parse.", "labels": [], "entities": []}], "datasetContent": [{"text": "For this abstract, we evaluate ENJU), a released deep parser based on the HPSG formalism and a parser based on the Dynamic Syntax formalism () underdevelopment against the gold standard given in table 1.", "labels": [], "entities": [{"text": "ENJU", "start_pos": 31, "end_pos": 35, "type": "METRIC", "confidence": 0.7567268013954163}, {"text": "HPSG", "start_pos": 74, "end_pos": 78, "type": "DATASET", "confidence": 0.9572224020957947}]}, {"text": "The precision and recall of the two parsers (ENJU and DSPD, which stands for \"Dynamic Syntax Parser under Development\") are given in: The experiment that we report here is intended to bean experiment with the evaluation method described in the last section, rather than a very serious attempt to evaluate the two parsers in question.", "labels": [], "entities": [{"text": "precision", "start_pos": 4, "end_pos": 13, "type": "METRIC", "confidence": 0.999299168586731}, {"text": "recall", "start_pos": 18, "end_pos": 24, "type": "METRIC", "confidence": 0.9990676045417786}]}, {"text": "The sentences in table 1 are carefully selected to include both sentences that illustrate core phenomena and sentences that illustrate rarer but more interesting (to linguists) phenomena.", "labels": [], "entities": []}, {"text": "But there are too few of them.", "labels": [], "entities": []}, {"text": "In fact, the most important number that we have obtained from our experiment is the 100% success rate in recognizing the phenomena given in table 1.", "labels": [], "entities": []}, {"text": "John is dumped by Mary 4 Your walking me pleases me 5 Abandoning children increased 6 He talks to Mary 7 John makes up the story 8 It is obvious that John is a fool 9 Hardly does anyone know Mary 10 John continues to please Mary  For all measures, some distortion is unavoidable when applied to exceptional cases.", "labels": [], "entities": []}, {"text": "This is true for the classical precision and recall, and our redefined precision and recall is no exception.", "labels": [], "entities": [{"text": "precision", "start_pos": 31, "end_pos": 40, "type": "METRIC", "confidence": 0.9973078966140747}, {"text": "recall", "start_pos": 45, "end_pos": 51, "type": "METRIC", "confidence": 0.9710022807121277}, {"text": "precision", "start_pos": 71, "end_pos": 80, "type": "METRIC", "confidence": 0.9956179857254028}, {"text": "recall", "start_pos": 85, "end_pos": 91, "type": "METRIC", "confidence": 0.971279501914978}]}, {"text": "In the case of the classical precision and recall, the distortion is countered by the inverse relation between them so that even if one is distorted, we can tell from the other that how well (poorly) the object of evaluation performs.", "labels": [], "entities": [{"text": "precision", "start_pos": 29, "end_pos": 38, "type": "METRIC", "confidence": 0.9966186285018921}, {"text": "recall", "start_pos": 43, "end_pos": 49, "type": "METRIC", "confidence": 0.99631667137146}]}, {"text": "Our redefined precision and recall works pretty much the same way.", "labels": [], "entities": [{"text": "precision", "start_pos": 14, "end_pos": 23, "type": "METRIC", "confidence": 0.9989563226699829}, {"text": "recall", "start_pos": 28, "end_pos": 34, "type": "METRIC", "confidence": 0.9988309741020203}]}, {"text": "What motivates us to derive measures so closely related to the classical precision and recall is the ease to combine the redefined precision and recall obtained from our evaluation method with the classical precision and recall obtained from other evaluation methods, so as to obtain a full picture of the performance of the object of evaluation.", "labels": [], "entities": [{"text": "precision", "start_pos": 73, "end_pos": 82, "type": "METRIC", "confidence": 0.96326744556427}, {"text": "recall", "start_pos": 87, "end_pos": 93, "type": "METRIC", "confidence": 0.9807723760604858}, {"text": "precision", "start_pos": 131, "end_pos": 140, "type": "METRIC", "confidence": 0.9777994751930237}, {"text": "recall", "start_pos": 145, "end_pos": 151, "type": "METRIC", "confidence": 0.9067606925964355}, {"text": "recall", "start_pos": 221, "end_pos": 227, "type": "METRIC", "confidence": 0.9803738594055176}]}, {"text": "For example, our redefined precision and recall figures given in (or figures obtained from running the same experiment on a larger test set) for ENJU can be combined with the precision and recall figures given in for ENJU, which is based on a evaluation method that compares its predicate-argument structures those given in Penn Treebank.", "labels": [], "entities": [{"text": "precision", "start_pos": 27, "end_pos": 36, "type": "METRIC", "confidence": 0.9905676245689392}, {"text": "recall", "start_pos": 41, "end_pos": 47, "type": "METRIC", "confidence": 0.9838199615478516}, {"text": "ENJU", "start_pos": 145, "end_pos": 149, "type": "DATASET", "confidence": 0.9490416049957275}, {"text": "precision", "start_pos": 175, "end_pos": 184, "type": "METRIC", "confidence": 0.9972108006477356}, {"text": "recall", "start_pos": 189, "end_pos": 195, "type": "METRIC", "confidence": 0.9836120009422302}, {"text": "ENJU", "start_pos": 217, "end_pos": 221, "type": "DATASET", "confidence": 0.9296325445175171}, {"text": "Penn Treebank", "start_pos": 324, "end_pos": 337, "type": "DATASET", "confidence": 0.9949965476989746}]}, {"text": "Here the precision and recall figures are calculated by assigning an equal weight to every sentence in Section 23 of Penn Treebank.", "labels": [], "entities": [{"text": "precision", "start_pos": 9, "end_pos": 18, "type": "METRIC", "confidence": 0.9997203946113586}, {"text": "recall", "start_pos": 23, "end_pos": 29, "type": "METRIC", "confidence": 0.9991750121116638}, {"text": "Section 23 of Penn Treebank", "start_pos": 103, "end_pos": 130, "type": "DATASET", "confidence": 0.8378503084182739}]}, {"text": "This means that different weights are assigned to different phenomena depending on their frequency in the Penn Treebank.", "labels": [], "entities": [{"text": "Penn Treebank", "start_pos": 106, "end_pos": 119, "type": "DATASET", "confidence": 0.996426910161972}]}, {"text": "Such assignment of weights may not be desirable for linguists or developers of NLP systems who are targeting a corpus with a very different distribution of phenomena from this particular section of the Penn Treebank.", "labels": [], "entities": [{"text": "Penn Treebank", "start_pos": 202, "end_pos": 215, "type": "DATASET", "confidence": 0.9936607182025909}]}, {"text": "For example, a linguist may wish to assign an equal weight across phenomena or more weights to 'interesting' phenomena.", "labels": [], "entities": []}, {"text": "A developer of a question-answering system may wish to give more weights to questionrelated phenomena than other phenomena of less interest which are nevertheless attested more frequently in the Penn Treebank.", "labels": [], "entities": [{"text": "Penn Treebank", "start_pos": 195, "end_pos": 208, "type": "DATASET", "confidence": 0.9955284297466278}]}, {"text": "In sum, the classical precision and recall figures calculated by assigning equal weight to every sentence could be considered skewed from the perspective of phenomena, whereas our redefined precision and recall figures maybe seen as skewed from the frequency perspective.", "labels": [], "entities": [{"text": "precision", "start_pos": 22, "end_pos": 31, "type": "METRIC", "confidence": 0.9963951706886292}, {"text": "recall", "start_pos": 36, "end_pos": 42, "type": "METRIC", "confidence": 0.9810044169425964}, {"text": "precision", "start_pos": 190, "end_pos": 199, "type": "METRIC", "confidence": 0.9873213768005371}, {"text": "recall", "start_pos": 204, "end_pos": 210, "type": "METRIC", "confidence": 0.9369301199913025}]}, {"text": "Frequency is relative to domains: less common phenomena in some domains could occur more often in others.", "labels": [], "entities": [{"text": "Frequency", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.9469261765480042}]}, {"text": "Our redefined precision and recall are not only useful for those who want a performance measure skewed the way they want, but also useful for those who want a performance measure as 'unskewed' as possible.", "labels": [], "entities": [{"text": "precision", "start_pos": 14, "end_pos": 23, "type": "METRIC", "confidence": 0.9993147850036621}, {"text": "recall", "start_pos": 28, "end_pos": 34, "type": "METRIC", "confidence": 0.9994792342185974}]}, {"text": "This maybe obtained by combining our redefined precision and recall with the classical precision and recall yielded from other evaluation methods.", "labels": [], "entities": [{"text": "precision", "start_pos": 47, "end_pos": 56, "type": "METRIC", "confidence": 0.9978684186935425}, {"text": "recall", "start_pos": 61, "end_pos": 67, "type": "METRIC", "confidence": 0.9924673438072205}, {"text": "precision", "start_pos": 87, "end_pos": 96, "type": "METRIC", "confidence": 0.9796725511550903}, {"text": "recall", "start_pos": 101, "end_pos": 107, "type": "METRIC", "confidence": 0.992021381855011}]}], "tableCaptions": []}