{"title": [{"text": "Parser-Based Retraining for Domain Adaptation of Probabilistic Generators", "labels": [], "entities": [{"text": "Parser-Based Retraining", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.7011024206876755}, {"text": "Domain Adaptation of Probabilistic Generators", "start_pos": 28, "end_pos": 73, "type": "TASK", "confidence": 0.7958904027938842}]}], "abstractContent": [{"text": "While the effect of domain variation on Penn-treebank-trained probabilistic parsers has been investigated in previous work, we study its effect on a Penn-Treebank-trained probabilistic generator.", "labels": [], "entities": []}, {"text": "We show that applying the generator to data from the British National Corpus results in a performance drop (from a BLEU score of 0.66 on the standard WSJ test set to a BLEU score of 0.54 on our BNC test set).", "labels": [], "entities": [{"text": "British National Corpus", "start_pos": 53, "end_pos": 76, "type": "DATASET", "confidence": 0.936403731505076}, {"text": "BLEU score", "start_pos": 115, "end_pos": 125, "type": "METRIC", "confidence": 0.9774379730224609}, {"text": "WSJ test set", "start_pos": 150, "end_pos": 162, "type": "DATASET", "confidence": 0.97760937611262}, {"text": "BLEU", "start_pos": 168, "end_pos": 172, "type": "METRIC", "confidence": 0.9989787340164185}, {"text": "BNC test set", "start_pos": 194, "end_pos": 206, "type": "DATASET", "confidence": 0.9758569796880087}]}, {"text": "We develop a generator retraining method where the domain-specific training data is automatically produced using state-of-the-art parser output.", "labels": [], "entities": []}, {"text": "The retraining method recovers a substantial portion of the performance drop, resulting in a generator which achieves a BLEU score of 0.61 on our BNC test data.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 120, "end_pos": 130, "type": "METRIC", "confidence": 0.9849570989608765}, {"text": "BNC test data", "start_pos": 146, "end_pos": 159, "type": "DATASET", "confidence": 0.9474408825238546}]}], "introductionContent": [{"text": "Grammars extracted from the Wall Street Journal (WSJ) section of the Penn Treebank have been successfully applied to natural language parsing, and more recently, to natural language generation.", "labels": [], "entities": [{"text": "Wall Street Journal (WSJ) section of the Penn Treebank", "start_pos": 28, "end_pos": 82, "type": "DATASET", "confidence": 0.9468341794880953}, {"text": "natural language parsing", "start_pos": 117, "end_pos": 141, "type": "TASK", "confidence": 0.6376737455526987}, {"text": "natural language generation", "start_pos": 165, "end_pos": 192, "type": "TASK", "confidence": 0.6848267118136088}]}, {"text": "It is clear that high-quality grammars can be extracted for the WSJ domain but it is not so clear how these grammars scale to other text genres., for example, has shown that WSJ-trained parsers suffer a drop in performance when applied to the more varied sentences of the Brown Corpus.", "labels": [], "entities": [{"text": "WSJ domain", "start_pos": 64, "end_pos": 74, "type": "DATASET", "confidence": 0.882232666015625}, {"text": "Brown Corpus", "start_pos": 272, "end_pos": 284, "type": "DATASET", "confidence": 0.9831925332546234}]}, {"text": "We investigate the effect of domain variation in treebank-grammar-based generation by applying a WSJ-trained generator to sentences from the British National Corpus (BNC).", "labels": [], "entities": [{"text": "treebank-grammar-based generation", "start_pos": 49, "end_pos": 82, "type": "TASK", "confidence": 0.7974451184272766}, {"text": "British National Corpus (BNC)", "start_pos": 141, "end_pos": 170, "type": "DATASET", "confidence": 0.9680257240931193}]}, {"text": "As with probabilistic parsing, probabilistic generation aims to produce the most likely output(s) given the input.", "labels": [], "entities": [{"text": "probabilistic parsing", "start_pos": 8, "end_pos": 29, "type": "TASK", "confidence": 0.6198176741600037}]}, {"text": "We can distinguish three types of probabilistic generators, based on the type of probability model used to select the most likely sentence.", "labels": [], "entities": []}, {"text": "The first type uses an n-gram language model, e.g.), the second type uses a probability model defined over trees or feature-structureannotated trees, e.g.), and the third type is a mixture of the first and second type, employing n-gram and grammarbased features, e.g. ().", "labels": [], "entities": []}, {"text": "The generator used in our experiments is an instance of the second type, using a probability model defined over Lexical Functional Grammar c-structure and f-structure annotations.", "labels": [], "entities": []}, {"text": "In an initial evaluation, we apply our probabilistic WSJ-trained generator to BNC material, and show that the generator suffers a substantial performance degradation, with a drop in BLEU score from 0.66 to 0.54.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 182, "end_pos": 192, "type": "METRIC", "confidence": 0.9857355952262878}]}, {"text": "We then turn our attention to the problem of adapting the generator so that it can more accurately generate the 1,000 sentences in our BNC test set.", "labels": [], "entities": [{"text": "BNC test set", "start_pos": 135, "end_pos": 147, "type": "DATASET", "confidence": 0.9205158750216166}]}, {"text": "The problem of adapting any NLP system to a domain different from the domain upon which it has been trained and for which no gold standard training material is available is a very real one, and one which has been the focus of much recent research in parsing.", "labels": [], "entities": [{"text": "parsing", "start_pos": 250, "end_pos": 257, "type": "TASK", "confidence": 0.974528431892395}]}, {"text": "Some success has been achieved by training a parser, not on gold standard hand-corrected trees, but on parser output trees.", "labels": [], "entities": []}, {"text": "These parser output trees can by produced by a second parser in a co-training scenario (, or by the same parser with a reranking component in a type of selftraining scenario ().", "labels": [], "entities": []}, {"text": "We tackle the problem of domain adaptation in generation in a similar way, by training the generator on domain specific parser output trees instead of manually corrected gold standard trees.", "labels": [], "entities": [{"text": "domain adaptation in generation", "start_pos": 25, "end_pos": 56, "type": "TASK", "confidence": 0.8132172226905823}]}, {"text": "This experiment achieves promising results, with an increase in BLEU score from 0.54 to 0.61.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 64, "end_pos": 74, "type": "METRIC", "confidence": 0.9655152857303619}]}, {"text": "The method is generic and can be applied to other probabilistic generators (for which suitable training material can be automatically produced).", "labels": [], "entities": []}], "datasetContent": [{"text": "Experimental Setup In our first experiment, we apply the original WSJ-trained generator to our BNC test set.", "labels": [], "entities": [{"text": "WSJ-trained generator", "start_pos": 66, "end_pos": 87, "type": "DATASET", "confidence": 0.8996176719665527}, {"text": "BNC test set", "start_pos": 95, "end_pos": 107, "type": "DATASET", "confidence": 0.9330533742904663}]}, {"text": "The gold standard trees for our BNC test set differ from the gold standard Wall Street Journal trees, in that they do not contain Penn-II traces or functional tags.", "labels": [], "entities": [{"text": "BNC test set", "start_pos": 32, "end_pos": 44, "type": "DATASET", "confidence": 0.8714162111282349}, {"text": "Wall Street Journal trees", "start_pos": 75, "end_pos": 100, "type": "DATASET", "confidence": 0.9236374199390411}]}, {"text": "The process which pro-duces f-structures from trees makes use of trace and functional tag information, if available.", "labels": [], "entities": []}, {"text": "Thus, to ensure that the training and test input f-structures are created in the same way, we use aversion of the generator which is trained using gold standard WSJ trees without functional tag or trace information.", "labels": [], "entities": [{"text": "WSJ trees", "start_pos": 161, "end_pos": 170, "type": "DATASET", "confidence": 0.8826340436935425}]}, {"text": "When we test this system on the WSJ23 f-structures (produced in the same way as the WSJ training material), the BLEU score decreases slightly from 0.67 to 0.66.", "labels": [], "entities": [{"text": "WSJ23 f-structures", "start_pos": 32, "end_pos": 50, "type": "DATASET", "confidence": 0.9199281632900238}, {"text": "WSJ training material", "start_pos": 84, "end_pos": 105, "type": "DATASET", "confidence": 0.9022059440612793}, {"text": "BLEU score", "start_pos": 112, "end_pos": 122, "type": "METRIC", "confidence": 0.9785739481449127}]}, {"text": "This is our baseline system.", "labels": [], "entities": []}, {"text": "Ina further experiment, we attempt to adapt the generator to BNC data by using BNC trees as training material.", "labels": [], "entities": [{"text": "BNC data", "start_pos": 61, "end_pos": 69, "type": "DATASET", "confidence": 0.798775851726532}]}, {"text": "Because we lack gold standard BNC trees (apart from those in our test set), we try instead to use parse trees produced by an accurate parser.", "labels": [], "entities": []}, {"text": "We choose the Charniak and Johnson reranking parser because it is freely available and achieves state-of-the-art accuracy (a Parseval f-score of 91.3%) on the WSJ domain).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 113, "end_pos": 121, "type": "METRIC", "confidence": 0.9993925094604492}, {"text": "Parseval f-score", "start_pos": 125, "end_pos": 141, "type": "METRIC", "confidence": 0.9453521370887756}, {"text": "WSJ domain", "start_pos": 159, "end_pos": 169, "type": "DATASET", "confidence": 0.9852074980735779}]}, {"text": "It is, however, affected by domain variation - report that its f-score drops by approximately 8 percentage points when applied to the BNC domain.", "labels": [], "entities": [{"text": "BNC domain", "start_pos": 134, "end_pos": 144, "type": "DATASET", "confidence": 0.9342144727706909}]}, {"text": "Our training size is 500,000 sentences.", "labels": [], "entities": []}, {"text": "We conduct two experiments: the first, in which 500,000 sentences are extracted randomly from the BNC (minus the test set sentences), and the second in which only shorter sentences, of length \u2264 20 words, are chosen as training material.", "labels": [], "entities": [{"text": "BNC", "start_pos": 98, "end_pos": 101, "type": "DATASET", "confidence": 0.8208446502685547}]}, {"text": "The rationale behind the second experiment is that shorter sentences are less likely to contain parser errors.", "labels": [], "entities": []}, {"text": "We use the BLEU evaluation metric for our experiments.", "labels": [], "entities": [{"text": "BLEU evaluation metric", "start_pos": 11, "end_pos": 33, "type": "METRIC", "confidence": 0.9398330847422282}]}, {"text": "We measure both coverage and full coverage.", "labels": [], "entities": [{"text": "coverage", "start_pos": 16, "end_pos": 24, "type": "METRIC", "confidence": 0.9929376244544983}, {"text": "coverage", "start_pos": 34, "end_pos": 42, "type": "METRIC", "confidence": 0.6901603937149048}]}, {"text": "Coverage measures the number of cases for which the generator produced some kind of output.", "labels": [], "entities": [{"text": "Coverage", "start_pos": 0, "end_pos": 8, "type": "METRIC", "confidence": 0.9631778597831726}]}, {"text": "Full coverage measures the number of cases for which the generator produced a tree spanning all of the words in the input.", "labels": [], "entities": [{"text": "coverage", "start_pos": 5, "end_pos": 13, "type": "METRIC", "confidence": 0.9549212455749512}]}], "tableCaptions": []}