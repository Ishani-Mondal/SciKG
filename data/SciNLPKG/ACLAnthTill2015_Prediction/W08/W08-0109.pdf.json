{"title": [{"text": "Persistent Information State in a Data-Centric Architecture *", "labels": [], "entities": []}], "abstractContent": [{"text": "We present the ADAMACH data centric dialog system, that allows to perform on-and off-line mining of dialog context, speech recognition results and other system-generated representations , both within and across dialogs.", "labels": [], "entities": []}, {"text": "The architecture implements a \"fat pipeline\" for speech and language processing.", "labels": [], "entities": [{"text": "speech and language processing", "start_pos": 49, "end_pos": 79, "type": "TASK", "confidence": 0.6126652657985687}]}, {"text": "We detail how the approach integrates domain knowledge and evolving empirical data, based on a user study in the University Helpdesk domain.", "labels": [], "entities": [{"text": "University Helpdesk domain", "start_pos": 113, "end_pos": 139, "type": "DATASET", "confidence": 0.797320286432902}]}], "introductionContent": [{"text": "In this paper, we argue that the ability to store and query large amounts of data is a key requirement for data-driven dialog systems, in which the data is generated by the spoken dialog system (SDS) components (spoken language understanding (SLU), dialog management (DM), natural language generation (NLG) etc.) and the world it is interacting with (news streams, ambient sensors etc.).", "labels": [], "entities": [{"text": "dialog management (DM)", "start_pos": 249, "end_pos": 271, "type": "TASK", "confidence": 0.809953260421753}, {"text": "natural language generation (NLG)", "start_pos": 273, "end_pos": 306, "type": "TASK", "confidence": 0.8017169336477915}]}, {"text": "We describe an SDS that is built around a database management system (DBMS), uses the web service paradigm (in contrast to the architecture described in), and employs a Voice XML (VXML) server for interfacing with Automatic Speech Recognition (ASR) and Text-to-Speech (TTS) components.", "labels": [], "entities": []}, {"text": "We would like to emphasize upfront that this does not mean that we follow a VXML dialog model.", "labels": [], "entities": []}, {"text": "* This work was partially supported by the European Commission Marie Curie Excellence Grant for the ADAMACH project (contract No. 022593) and by LUNA STREP project (contract no33549).", "labels": [], "entities": [{"text": "European Commission Marie Curie Excellence Grant", "start_pos": 43, "end_pos": 91, "type": "DATASET", "confidence": 0.8019016981124878}, {"text": "LUNA", "start_pos": 145, "end_pos": 149, "type": "DATASET", "confidence": 0.4689306318759918}, {"text": "STREP", "start_pos": 150, "end_pos": 155, "type": "METRIC", "confidence": 0.49402135610580444}]}, {"text": "The data centric architecture we adopt has several advantages: first, the database concentrates heterogeneous types of information allowing to uniformly query the evolving data at anytime, e.g. by performing queries across various types of information.", "labels": [], "entities": []}, {"text": "Second, the architecture facilitates dialog evaluation, data mining and online learning because data is available for querying as soon as it has been stored.", "labels": [], "entities": [{"text": "dialog evaluation", "start_pos": 37, "end_pos": 54, "type": "TASK", "confidence": 0.9016909599304199}, {"text": "data mining", "start_pos": 56, "end_pos": 67, "type": "TASK", "confidence": 0.8408743739128113}]}, {"text": "Third, multiple systems/applications can be made available on the same infrastructure due to a clean separation of its processing modules (SLU, DM, NLG etc.) from data storage and persistency (DBMS), and monitoring/analysis/visualization and annotation tools.", "labels": [], "entities": []}, {"text": "Fourth, there is no need for separate 'logging' mechanisms: the state of the SDS is contained in the database, and is therefore persistently available for analysis after the dialog ends.", "labels": [], "entities": []}, {"text": "As opposed to the presented architecture, the Open Agent Architecture (OAA)) and DARPA Communicator () treat data as peripheral: they were not specifically designed to handle large volumes of data, and data is not automatically persistent.", "labels": [], "entities": []}, {"text": "In contrast to the CSLI-DM (), and TrindiKit), but similar to Communicator, the ADAMACH architecture is server-based, thus enabling continuous operation.", "labels": [], "entities": []}, {"text": "To prove our concept, we test it on a University helpdesk application (section 4).", "labels": [], "entities": [{"text": "University helpdesk application", "start_pos": 38, "end_pos": 69, "type": "DATASET", "confidence": 0.9475869139035543}]}, {"text": "shows our vision for the architecture of the ADAMACH system.", "labels": [], "entities": [{"text": "ADAMACH system", "start_pos": 45, "end_pos": 59, "type": "DATASET", "confidence": 0.7820767462253571}]}, {"text": "We implemented and evaluated the speech modality based core of this system.", "labels": [], "entities": []}, {"text": "A typical interaction is initiated by a phone call that arrives at an telephony server which routes it to a VXML platform.", "labels": [], "entities": []}, {"text": "A VXML page is continuously rewritten by the dialog manager, containing the system utterance and other TTS parameters, and the ASR recognition parameters for the next user utterance.", "labels": [], "entities": [{"text": "ASR recognition", "start_pos": 127, "end_pos": 142, "type": "TASK", "confidence": 0.7383036017417908}]}, {"text": "Thus, VXML is used as a lowlevel interface to the ASR and TTS engines, but not for representing dialog strategies.", "labels": [], "entities": []}, {"text": "Once a user utterance is recognized, a web service request is issued to a dialog management server.", "labels": [], "entities": []}], "datasetContent": [{"text": "Our current application is a University helpdesk in Italian which students call to perform 5 tasks: receive information about exams (times, rooms . .", "labels": [], "entities": []}, {"text": "), subscribe/cancel subscriptions to exams, obtain exam mark, or request to talk to an operator.", "labels": [], "entities": []}, {"text": "Following experimentations, we annotated the dialogs and conducted performance statistics using the system's built-in annotation tool.", "labels": [], "entities": []}, {"text": "Two Italian mothertongues were in charge of manually annotating a total of 423 interactions.", "labels": [], "entities": []}, {"text": "Each annotator independently annotated each dialog turn according to whether one of the five available tasks was being requested or completed in it.", "labels": [], "entities": []}, {"text": "To compute inter-annotator agreement, 24 dialogs were processed by both annotators; the remaining ones were partitioned equally among them.", "labels": [], "entities": []}, {"text": "We computed agreement at both turn and dialog level.", "labels": [], "entities": []}, {"text": "Turn level agreement is concerned with which tasks are requested and completed at a given dialog turn according to each annotator.", "labels": [], "entities": []}, {"text": "An agreement matrix is compiled where rows and columns correspond to the five task types in our application., computed over the turn matrix, gave a turn agreement of 0.72 resp.", "labels": [], "entities": []}, {"text": "completions, exceeding the recommended 0.7 threshold.", "labels": [], "entities": [{"text": "completions", "start_pos": 0, "end_pos": 11, "type": "METRIC", "confidence": 0.9849758744239807}]}, {"text": "While turn-level agreement refers to which tasks occurred and at what turn, dialog level agreement refers to how many task requests/completions occurred.", "labels": [], "entities": []}, {"text": "Also at the dialog level, the \u03ba statistic gave good results (0.71 for requests and 0.9 for completions).", "labels": [], "entities": []}, {"text": "General dialog statistics The average duration of the 423 annotated dialogs is 63.1 seconds, with an average of 7.43 turn (i.e. adjacency) pairs.", "labels": [], "entities": []}, {"text": "356 of the dialogs contained at least one task; the majority (338) contained exactly one, 17 dialogs contained 2 tasks, and one dialog contained 3.", "labels": [], "entities": []}, {"text": "In the remaining 67 dialogs, no tasks were detected: from the audio files, it seems that these generally happened by accident or in noisy environments, hence noinput/hangup events occurred shortly after the initial system prompt.", "labels": [], "entities": []}, {"text": "Furthermore, relative frequencies of task requests and task completions are reported in.", "labels": [], "entities": []}, {"text": "In total, according to the two annotators, there were 375 task requests and 234 task completions.", "labels": [], "entities": []}, {"text": "Among the requested tasks, the vast majority was composed by \"Get exam mark\" -a striking 96%-while \"Exam withdrawal\" never occurred and the three others were barely performed.", "labels": [], "entities": [{"text": "Get exam mark\"", "start_pos": 62, "end_pos": 76, "type": "METRIC", "confidence": 0.912731871008873}, {"text": "Exam withdrawal\"", "start_pos": 100, "end_pos": 116, "type": "METRIC", "confidence": 0.6197401384512583}]}, {"text": "Indeed, it seems that students preferred to use the system to carry on \"informative\" tasks such as obtaining exam marks and general information rather than \"active\" tasks such as exam subscription and withdrawal.", "labels": [], "entities": []}, {"text": "Task and dialog success Based on the annotation of task requests and completions, we defined task success as a binary measure of whether the request of a given task type is eventually followed by a task completion of the same type.", "labels": [], "entities": []}, {"text": "reports the average success of each task type according to the an-notators 1 . Our results show that the most frequently requested type, \"Get exam mark\", has a 64.64% success rate (it seems that failure was mostly due to the system's inability to recognize student IDs).", "labels": [], "entities": [{"text": "Get exam mark\"", "start_pos": 138, "end_pos": 152, "type": "METRIC", "confidence": 0.9066003859043121}]}, {"text": "In fact, while it is straightforward to obtain task success information using the manual annotation of dialogs, when the dialog system cannot rely on human judgments, unsupervised approaches must be defined fora rapid (on-line or off-line) evaluation.", "labels": [], "entities": []}, {"text": "For this purpose, an automatic approximation of the \"manual\" task success estimation has been defined using a set of database queries associated to each task type.", "labels": [], "entities": []}, {"text": "For instance, the task success query associated to \"Info on exam\" checks that two conditions are met in the current dialog: 1) it includes a turn where an action is requested the interpretation of which contains \"information\"; 2) it contains a turn where the concept Exam Name is in focus.", "labels": [], "entities": []}, {"text": "Automatic task success rates have been computed on the same dialogs for which manual task success rates were available and are reported in, col.", "labels": [], "entities": []}, {"text": "2. The comparison shows that the automatic metric sr A is more \"optimistic\" than the manual one sr M . Indeed, automatic estimators rely on \"punctual\" indicators (such as the occurrence of confirmations of a given value) in the whole dialog, regardless of the task they appear in (this information is only available from human annotation) and also of the order with which such indicators appear in the dialog.", "labels": [], "entities": []}, {"text": ", T being the set of requested tasks.", "labels": [], "entities": [{"text": "T", "start_pos": 2, "end_pos": 3, "type": "METRIC", "confidence": 0.9466937780380249}]}, {"text": "Depending on whether sr M or sr A is used, we obtain two metrics, dsr M resp.", "labels": [], "entities": []}, {"text": "dsr A . Our dialog success results (last row of) are comparable to the task success ones; also, the difference between the automatic and manual estimators of dialog success is similar to their difference at the the task level.", "labels": [], "entities": []}, {"text": "This is not surprising when considering that most of the dialogs contained only one task.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Top: annotator (sr M ) and automatic (sr A ) task  success rates. Mean \u00b1 binomial proportion confidence  interval on the average task success (\u03b1= 95%) is reported.  Bottom: mean annotator (dsr M ) and automatic (dsr A )  dialog success rates \u00b1 normal law c.i. (\u03b1= 95%).", "labels": [], "entities": [{"text": "Mean \u00b1 binomial proportion confidence  interval", "start_pos": 76, "end_pos": 123, "type": "METRIC", "confidence": 0.9439584712187449}]}]}