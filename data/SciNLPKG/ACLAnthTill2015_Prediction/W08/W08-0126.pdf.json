{"title": [{"text": "User Simulation as Testing for Spoken Dialog Systems", "labels": [], "entities": [{"text": "User Simulation", "start_pos": 0, "end_pos": 15, "type": "TASK", "confidence": 0.7937751114368439}]}], "abstractContent": [{"text": "We propose to use user simulation for testing during the development of a sophisticated dialog system.", "labels": [], "entities": []}, {"text": "While the limited behaviors of the state-of-the-art user simulation may not cover important aspects in the dialog system testing, our proposed approach extends the functional-ity of the simulation so that it can be used at least for the early stage testing before the system reaches stable performance for evaluation involving human users.", "labels": [], "entities": []}, {"text": "The proposed approach includes a set of evaluation measures that can be computed automatically from the interaction logs between the user simulator and the dialog system.", "labels": [], "entities": []}, {"text": "We first validate these measures on human user dialogs using user satisfaction scores.", "labels": [], "entities": []}, {"text": "We also build a regression model to estimate the user satisfaction scores using these evaluation measures.", "labels": [], "entities": []}, {"text": "Then, we apply the evaluation measures on a simulated dialog corpus trained from the real user corpus.", "labels": [], "entities": []}, {"text": "We show that the user satisfaction scores estimated from the simulated corpus are not statistically different from the real users' satisfaction scores.", "labels": [], "entities": []}], "introductionContent": [{"text": "Spoken dialog systems are being widely used in daily life.", "labels": [], "entities": []}, {"text": "The increasing demands of such systems require shorter system development cycles and better automatic system developing techniques.", "labels": [], "entities": []}, {"text": "As a result, machine learning techniques are applied to learn dialog strategies automatically, such as reinforcement learning, supervised learning (), etc.", "labels": [], "entities": []}, {"text": "These techniques require a significant amount of training data for the automatic learners to sufficiently explore the vast space of possible dialog states and strategies.", "labels": [], "entities": []}, {"text": "However, it is always hard to obtain training corpora that are large enough to ensure that the learned strategies are reliable.", "labels": [], "entities": []}, {"text": "User simulation is an attempt to solve this problem by generating synthetic training corpora using computer simulated users.", "labels": [], "entities": []}, {"text": "The simulated users are built to mimic real users' behaviors to some extent while allowing them to be programmed to explore unseen but still possible user behaviors.", "labels": [], "entities": []}, {"text": "These simulated users can interact with the dialog systems to generate large amounts of training data in a low-cost and time-efficient manner.", "labels": [], "entities": []}, {"text": "Many previous studies) have shown that the dialog strategies learned from the simulated training data outperform the hand-crafted strategies.", "labels": [], "entities": []}, {"text": "There are also studies that use user simulation to train speech recognition and understanding components.", "labels": [], "entities": [{"text": "speech recognition and understanding", "start_pos": 57, "end_pos": 93, "type": "TASK", "confidence": 0.7201593667268753}]}, {"text": "While user simulation is largely used in dialog system training, it has only been used in limited scope for testing specific dialog system components in the system evaluation phase ().", "labels": [], "entities": []}, {"text": "This is partly because the state-of-the-art simulated users have quite limited abilities in mimicking human users' behaviors and typically over-generate possible dialog behaviors.", "labels": [], "entities": []}, {"text": "This is not a major problem when using simulated dialog corpus as the training corpus for dialog strategy learning because the over-generated simulation behaviors would only provide the machine learners with a broader dialog state space to explore.", "labels": [], "entities": [{"text": "dialog strategy learning", "start_pos": 90, "end_pos": 114, "type": "TASK", "confidence": 0.8145289023717245}]}, {"text": "However, realistic user behaviors are highly desired in the testing phase because the systems are evaluated and adjusted based on the analysis of the dialogs generated in this phase.", "labels": [], "entities": []}, {"text": "Therefore, we would ex-pect that these user behaviors are what we will see in the final evaluation with human users.", "labels": [], "entities": []}, {"text": "In this case, any over-generated dialog behaviors may cause the system to be blamed for untargeted functions.", "labels": [], "entities": []}, {"text": "What is more, the simulated users cannot provide subjective user satisfaction feedback which is also important for improving the systems.", "labels": [], "entities": []}, {"text": "Since it is expensive and time-consuming to test every version of the system with a significant amount of paid subjects, the testing during the development is typically constrained to a limited number of users, and often, to repeated users who are colleagues or developers themselves.", "labels": [], "entities": []}, {"text": "Thus, the system performance is not always optimized for the intended users.", "labels": [], "entities": []}, {"text": "Our ultimate goal is to supplement human testing with simulated users during the development to speedup the system development towards desired performance.", "labels": [], "entities": []}, {"text": "This would be especially useful in the early development stage, since it would avoid conducting tests with human users when they may feel extremely frustrated due to the malfunction of the unstable system.", "labels": [], "entities": []}, {"text": "As a first attempt, we try to extend the state-ofthe-art user simulation by incorporating a set of new but straightforward evaluation measures for automatically assessing the dialog system performance.", "labels": [], "entities": []}, {"text": "These evaluation measures focus on three basic aspects of task-oriented dialog systems: understanding ability, efficiency, and the appropriateness of the system actions.", "labels": [], "entities": []}, {"text": "They are first applied on a corpus generated between a dialog system and a group of human users to demonstrate the validity of these measures with the human users' satisfaction scores.", "labels": [], "entities": []}, {"text": "Results show that these measures are significantly correlated with the human users' satisfactions.", "labels": [], "entities": []}, {"text": "Then, a regression model is built to predict the user satisfaction scores using these evaluation measures.", "labels": [], "entities": []}, {"text": "We also apply the regression model on a simulated dialog corpus trained from the above real user corpus, and show that the user satisfaction scores estimated from the simulated dialogs do not differ significantly from the real users' satisfaction scores.", "labels": [], "entities": []}, {"text": "Finally, we conclude that these evaluation measures can be used to assess the system performance based on the estimated user satisfaction.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we describe in detail the evaluation measures covering three basic aspects of taskoriented dialog systems: understanding ability, efficiency, and the appropriateness of the system actions.", "labels": [], "entities": []}, {"text": "In this section, we first validate the proposed measures using real users' satisfaction scores, and then show the differentiating power of these measures through the improvement curves plotted on the dry-run data.", "labels": [], "entities": []}, {"text": "To validate the evaluation measures introduced in Section 4, we use Pearson's correlation to examine how well these evaluation measures can predict the user satisfaction scores.", "labels": [], "entities": [{"text": "Pearson's correlation", "start_pos": 68, "end_pos": 89, "type": "METRIC", "confidence": 0.9448418219884237}]}, {"text": "Here, we only look at the dialog corpus in final evaluation because only these users filled out the user satisfaction surveys.", "labels": [], "entities": []}, {"text": "For each user, we compute the average value of the evaluation measures across all dialogs generated by that user.", "labels": [], "entities": []}, {"text": "lists the correlation between the evaluation measures and the user satisfaction scores, as well as the p-value for each correlation.", "labels": [], "entities": []}, {"text": "The correlation describes a linear relationship between these measures and the user satisfaction scores.", "labels": [], "entities": []}, {"text": "For the measures that describe the system's understanding abilities and the measures that describe the system's efficiency, our newly proposed measures show higher correlations with the user satisfaction scores than their counterparts.", "labels": [], "entities": []}, {"text": "Therefore, in the rest of the study, we drop the two measures used by the previous studies, i.e., semanticAccuracy and dialogTurns.", "labels": [], "entities": []}, {"text": "We observe that the user satisfaction scores are significantly positively correlated with all the three proposed measures.", "labels": [], "entities": []}, {"text": "These correlations confirms our expectations: user satisfaction is higher when the system's understanding matches better with the users' requirements; when the dialog efficiency is closer to the situation of perfect understanding; or when the system's actions are mostly appropriate.", "labels": [], "entities": []}, {"text": "We suggest that these measures can serve as indicators for user satisfaction.", "labels": [], "entities": []}, {"text": "We further use all the measures to build a regression model to predict the user satisfaction score.", "labels": [], "entities": []}, {"text": "The prediction model is: The R-square is 0.655, which indicates that 65.5% of the user satisfaction scores can be explained by this model.", "labels": [], "entities": [{"text": "R-square", "start_pos": 29, "end_pos": 37, "type": "METRIC", "confidence": 0.9965119957923889}]}, {"text": "While this prediction model has much room for improvement, we suggest that it can be used to estimate the users' satisfaction scores for simulated users in the early system testing stage to quickly assess the system's performance.", "labels": [], "entities": []}, {"text": "Since the weights are tuned based on the data from this specific application, the prediction model may not be used directly for other domains.", "labels": [], "entities": []}, {"text": "Since this set of evaluation measures intends to evaluate the system's performance in the development stage, we would like the measures to be able to reflect small changes made in the system and to indicate whether these changes show the right trend of increased user satisfaction in reality.", "labels": [], "entities": []}, {"text": "A set of good evaluation measures should be sensible to subtle system changes.", "labels": [], "entities": []}, {"text": "We assess the differentiating power of the evaluation measures using the dialog corpus collected during the dry-runs.", "labels": [], "entities": []}, {"text": "The system was tested on a weekly basis as explained in.", "labels": [], "entities": []}, {"text": "For each improvement stage, we compute the values for the three evaluation measures averaging across all dialogs from all users.", "labels": [], "entities": []}, {"text": "shows the three improvement curves based on these three measures.", "labels": [], "entities": []}, {"text": "The x-axis shows the first date of each improvement stage; the y-axis shows the value of the evaluation measures.", "labels": [], "entities": []}, {"text": "We observe that all three curves show the right trends that indicate the system's improvements over the development stages.", "labels": [], "entities": []}, {"text": "We train a goal and agenda driven user simulation model from the final evaluation dialog corpus with the real users.", "labels": [], "entities": []}, {"text": "The simulation model interacts with the dialog system 20 times (each time the simulation model represents a different simulated user), generating nine dialogs on all of the nine tasks each time.", "labels": [], "entities": []}, {"text": "In each interaction, the simulated users generate their agenda randomly based on a uniform distribution.", "labels": [], "entities": []}, {"text": "The simulated corpus consists of 180 dialogs from 20 simulated users, which is of the same size as the real user corpus.", "labels": [], "entities": []}, {"text": "The values of the evaluation measures are computed automatically at the end of each simulated dialog.", "labels": [], "entities": []}, {"text": "We compute the estimated user satisfaction score using Equation 1 for each simulated user.", "labels": [], "entities": [{"text": "Equation", "start_pos": 55, "end_pos": 63, "type": "METRIC", "confidence": 0.9885687828063965}]}, {"text": "We then compare the user satisfaction scores of the 20 simulated users with the satisfaction scores of the 20 real users.", "labels": [], "entities": []}, {"text": "The average and the standard deviation of the user satisfaction scores for real users are (3.79, 0.72), and the ones for simulated users are (3.77, 1.34).", "labels": [], "entities": []}, {"text": "Using two-tailed t-test at significance level p<0.05, we observe that there are no statistically significant differences between the two pools of scores.", "labels": [], "entities": [{"text": "significance level p", "start_pos": 27, "end_pos": 47, "type": "METRIC", "confidence": 0.9273015856742859}]}, {"text": "Therefore, we suggest that the user satisfaction estimated from the simulated dialog corpus can be used to assess the system performance.", "labels": [], "entities": []}, {"text": "However, these average scores only offer us one perspective in comparing the real with the simulated user satisfaction.", "labels": [], "entities": []}, {"text": "In the future, we would like to look further into the differences between the distributions of these user satisfaction scores.", "labels": [], "entities": []}], "tableCaptions": []}