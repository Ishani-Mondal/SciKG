{"title": [{"text": "Passage Retrieval for Question Answering using Sliding Windows", "labels": [], "entities": [{"text": "Passage Retrieval", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.8595283031463623}, {"text": "Question Answering", "start_pos": 22, "end_pos": 40, "type": "TASK", "confidence": 0.807524561882019}]}], "abstractContent": [{"text": "The information retrieval (IR) community has investigated many different techniques to retrieve passages from large collections of documents for question answering (QA).", "labels": [], "entities": [{"text": "information retrieval (IR)", "start_pos": 4, "end_pos": 30, "type": "TASK", "confidence": 0.8561777353286744}, {"text": "question answering (QA)", "start_pos": 145, "end_pos": 168, "type": "TASK", "confidence": 0.8720008969306946}]}, {"text": "In this paper, we specifically examine and quantitatively compare the impact of passage retrieval for QA using sliding windows and disjoint windows.", "labels": [], "entities": [{"text": "passage retrieval", "start_pos": 80, "end_pos": 97, "type": "TASK", "confidence": 0.8986667096614838}]}, {"text": "We consider two different data sets, the TREC 2002-2003 QA data set, and 93 why-questions against INEX Wikipedia.", "labels": [], "entities": [{"text": "TREC 2002-2003 QA data set", "start_pos": 41, "end_pos": 67, "type": "DATASET", "confidence": 0.9443733215332031}, {"text": "INEX Wikipedia", "start_pos": 98, "end_pos": 112, "type": "DATASET", "confidence": 0.9479034543037415}]}, {"text": "We discovered that, compared to disjoint windows , using sliding windows results in improved performance of TREC-QA in terms of TDRR, and in improved performance of why-QA in terms of success@n and MRR.", "labels": [], "entities": [{"text": "TREC-QA", "start_pos": 108, "end_pos": 115, "type": "METRIC", "confidence": 0.9856089949607849}, {"text": "TDRR", "start_pos": 128, "end_pos": 132, "type": "METRIC", "confidence": 0.7778724431991577}, {"text": "MRR", "start_pos": 198, "end_pos": 201, "type": "METRIC", "confidence": 0.9678942561149597}]}], "introductionContent": [{"text": "In question answering (QA), text passages are an important intermediary between full documents and exact answers.", "labels": [], "entities": [{"text": "question answering (QA)", "start_pos": 3, "end_pos": 26, "type": "TASK", "confidence": 0.888365364074707}]}, {"text": "They form a very natural unit of response for QA systems ( and it is known from user studies that users prefer answers to be embedded in paragraph-sized chunks () because they can provide the context of an answer.", "labels": [], "entities": []}, {"text": "Therefore, almost all state-of-the-art QA systems implement some technique for extracting paragraph-sized fragments of text from a large corpus.", "labels": [], "entities": []}, {"text": "Most QA systems have a pipeline architecture consisting of at least three components: question analysis, document/passage retrieval, and answer extraction).", "labels": [], "entities": [{"text": "question analysis", "start_pos": 86, "end_pos": 103, "type": "TASK", "confidence": 0.7939580976963043}, {"text": "document/passage retrieval", "start_pos": 105, "end_pos": 131, "type": "TASK", "confidence": 0.5675994157791138}, {"text": "answer extraction", "start_pos": 137, "end_pos": 154, "type": "TASK", "confidence": 0.743047222495079}]}, {"text": "The quality of a QA system heavily depends on the effectiveness of the integrated retrieval system (second step of the pipeline): if a retrieval system fails to find any relevant documents fora question, further processing steps to extract an answer will inevitably fail too.", "labels": [], "entities": []}, {"text": "This motivates the need to study passage retrieval for QA.", "labels": [], "entities": [{"text": "passage retrieval", "start_pos": 33, "end_pos": 50, "type": "TASK", "confidence": 0.9327795803546906}]}, {"text": "There are two common approaches to retrieving passages from a corpus: one is to index each passage as separate document and retrieve them as such.", "labels": [], "entities": []}, {"text": "The other option is to first retrieve relevant documents fora given question and then retrieve passages from the retrieved documents.", "labels": [], "entities": []}, {"text": "The passages themselves can vary in size and degree of overlap.", "labels": [], "entities": []}, {"text": "Their size can be fixed as a number of words or characters, or varying with the semantic content or the structure of the text.", "labels": [], "entities": []}, {"text": "The overlap between two adjacent passages can be either zero, in which case we speak of disjoint passages, or the passages maybe overlapping, which we refer to as sliding passages.", "labels": [], "entities": []}, {"text": "In this paper, we compare the effectiveness of several passage retrieval techniques with respect to their usefulness for QA.", "labels": [], "entities": [{"text": "passage retrieval", "start_pos": 55, "end_pos": 72, "type": "TASK", "confidence": 0.8616164028644562}, {"text": "QA", "start_pos": 121, "end_pos": 123, "type": "TASK", "confidence": 0.9286063313484192}]}, {"text": "Our main interest is the contribution of sliding passages as apposed to disjoint passages, and we will experiment with a number of retrieval models.", "labels": [], "entities": []}, {"text": "We evaluate the retrieval approaches on two different QA tasks: (1) factoid-QA, as defined by the test collection provided by TREC, and (2) a relatively new problem in the QA field: that of answering why-questions (why-QA).", "labels": [], "entities": [{"text": "TREC", "start_pos": 126, "end_pos": 130, "type": "DATASET", "confidence": 0.7797020673751831}]}, {"text": "The remainder of the paper is organized as follows.", "labels": [], "entities": []}, {"text": "In the next section, we describe related work on passage retrieval for QA and we motivate what the main contribution of the current paper is.", "labels": [], "entities": [{"text": "passage retrieval", "start_pos": 49, "end_pos": 66, "type": "TASK", "confidence": 0.9474152326583862}, {"text": "QA", "start_pos": 71, "end_pos": 73, "type": "TASK", "confidence": 0.7669252157211304}]}, {"text": "In section 3 we describe our general set-up for passage retrieval in both QA tasks that we consider.", "labels": [], "entities": [{"text": "passage retrieval", "start_pos": 48, "end_pos": 65, "type": "TASK", "confidence": 0.9503072500228882}]}, {"text": "In section 4, we present the results of the experiments on TREC-QA data, and in section 5 we present our results on why-QA.", "labels": [], "entities": [{"text": "TREC-QA data", "start_pos": 59, "end_pos": 71, "type": "DATASET", "confidence": 0.8042671978473663}]}, {"text": "Section 6 gives an overall conclusion.", "labels": [], "entities": []}], "datasetContent": [{"text": "The main purpose of our experiments is to study the contribution of sliding windows as apposed to disjoint windows in the context of QA.", "labels": [], "entities": []}, {"text": "Therefore, in our experiment setup, we have kept fixed the other segmentation variables, passage size and degree of overlap.", "labels": [], "entities": []}, {"text": "We set out to examine two different strategies of document segmentation (disjoint and sliding passages) with a number of retrieval models for two different QA tasks: TREC factoid-QA and why-QA.", "labels": [], "entities": [{"text": "document segmentation", "start_pos": 50, "end_pos": 71, "type": "TASK", "confidence": 0.7038186192512512}]}, {"text": "For our experiments, we use the following metrics for evaluation: Mean reciprocal rank (MRR) at n is the mean (calculated overall questions) of the reciprocal rank (which is 1 divided by the rank ordinal) of the highest ranked relevant (i.e. answer bearing) passage.", "labels": [], "entities": [{"text": "Mean reciprocal rank (MRR)", "start_pos": 66, "end_pos": 92, "type": "METRIC", "confidence": 0.9521546264489492}]}, {"text": "RR is zero fora question if no relevant passage is returned by the system at limit n.", "labels": [], "entities": [{"text": "RR", "start_pos": 0, "end_pos": 2, "type": "METRIC", "confidence": 0.976926863193512}]}, {"text": "Success at n fora question is 1 if the answer to this question is found in top n passages fetched up by our system.", "labels": [], "entities": []}, {"text": "Success@n is averaged overall questions.", "labels": [], "entities": [{"text": "Success", "start_pos": 0, "end_pos": 7, "type": "METRIC", "confidence": 0.9458196759223938}]}, {"text": "Total document reciprocal rank (TDRR)) is the sum of all reciprocal ranks of all answer bearing passages per question (averaged overall questions).", "labels": [], "entities": [{"text": "Total document reciprocal rank (TDRR))", "start_pos": 0, "end_pos": 38, "type": "METRIC", "confidence": 0.7976012442793164}]}, {"text": "The value of TDRR is maximum if all retrieved passages are relevant.", "labels": [], "entities": [{"text": "TDRR", "start_pos": 13, "end_pos": 17, "type": "METRIC", "confidence": 0.934515655040741}]}, {"text": "TDRR is an extension of MRR that favors a system that ranks more that one relevant passage higher than all non-relevant passages.", "labels": [], "entities": [{"text": "TDRR", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.8177046775817871}, {"text": "MRR", "start_pos": 24, "end_pos": 27, "type": "TASK", "confidence": 0.720487117767334}]}, {"text": "This way, TDRR extends MRR with a notion of recall.", "labels": [], "entities": [{"text": "MRR", "start_pos": 23, "end_pos": 26, "type": "TASK", "confidence": 0.8664344549179077}, {"text": "recall", "start_pos": 44, "end_pos": 50, "type": "METRIC", "confidence": 0.9940383434295654}]}, {"text": "When we compare retrieval performance of two retrieval settings (such as the use of disjoint versus sliding windows), then we obtain a list of paired scores.", "labels": [], "entities": []}, {"text": "That's why we use the Wilcoxon signedrank test to show the statistical significance of the improvements.", "labels": [], "entities": [{"text": "Wilcoxon signedrank test", "start_pos": 22, "end_pos": 46, "type": "DATASET", "confidence": 0.7016613086064657}]}, {"text": "In summary, we experiment with three retrieval models in Lemur: TFIDF, Okapi, and a language model based on the Kullback-Leibler divergence.", "labels": [], "entities": [{"text": "TFIDF", "start_pos": 64, "end_pos": 69, "type": "METRIC", "confidence": 0.7194937467575073}]}, {"text": "For each of these retrieval models, we evaluate the use of both sliding and disjoint passages.", "labels": [], "entities": []}, {"text": "This makes a total of six retrieval settings.", "labels": [], "entities": []}, {"text": "For development and testing purposes, we use the Webclopedia question set by).", "labels": [], "entities": [{"text": "Webclopedia question set", "start_pos": 49, "end_pos": 73, "type": "DATASET", "confidence": 0.9485211173693339}]}, {"text": "This set contains questions that were asked to the online QA system answers.com.", "labels": [], "entities": [{"text": "QA system answers.com", "start_pos": 58, "end_pos": 79, "type": "DATASET", "confidence": 0.7464960217475891}]}, {"text": "805 of these questions are why-questions.", "labels": [], "entities": []}, {"text": "We manually inspect a sample of 400 of the Webclopedia whyquestions.", "labels": [], "entities": [{"text": "Webclopedia whyquestions", "start_pos": 43, "end_pos": 67, "type": "DATASET", "confidence": 0.881920725107193}]}, {"text": "Of these, 93 have an answer in the Wikipedia XML corpus (see section 3).", "labels": [], "entities": [{"text": "Wikipedia XML corpus", "start_pos": 35, "end_pos": 55, "type": "DATASET", "confidence": 0.9400307536125183}]}, {"text": "Manual extraction of one correct answer for each of these questions results in a set of 93 why-questions and their reference answer.", "labels": [], "entities": []}, {"text": "In order to be able to do fast evaluation of the different evaluation settings, we manually create an answer pattern for each of the questions in our set.", "labels": [], "entities": []}, {"text": "These answer patterns are based on a set of 93 reference answers (one answer per question) that we have manually extracted from the Wikipedia corpus.", "labels": [], "entities": [{"text": "Wikipedia corpus", "start_pos": 132, "end_pos": 148, "type": "DATASET", "confidence": 0.9580157697200775}]}, {"text": "An answer pattern is a regular expression that defines which of the retrieved passages are considered a relevant answer to the input question.", "labels": [], "entities": []}, {"text": "As opposed to the answer patterns provided by NIST for the evaluation of factoid QA (see section 4), our answer patterns for why-questions are relatively strict.", "labels": [], "entities": [{"text": "NIST", "start_pos": 46, "end_pos": 50, "type": "DATASET", "confidence": 0.9394167065620422}, {"text": "evaluation of factoid QA", "start_pos": 59, "end_pos": 83, "type": "TASK", "confidence": 0.5869327560067177}]}, {"text": "A why-answer can be formulated in many different ways with different words, which may not all be in the answer pattern.", "labels": [], "entities": []}, {"text": "For a factoid question such as \"When was John Lennon born?\", the answer is only one phrase, and the answer pattern is short and unambiguous, i.e. /1940/.", "labels": [], "entities": []}, {"text": "However, if we consider the why-question \"Why are some organ transplants unsuccessful?\", the answer pattern cannot be stated in one phrase.", "labels": [], "entities": []}, {"text": "For /.*immune system.*foreign tissues.*destroy.*/.", "labels": [], "entities": []}, {"text": "It is however possible that a relevant answer is formulated in away that does not match this regular expression.", "labels": [], "entities": []}, {"text": "Thus, the use of answer patterns for the evaluation of why-QA leads to conservative results: some relevant answers maybe missed in the evaluation procedure.", "labels": [], "entities": []}, {"text": "After applying the answer patterns, we count the questions that have at least one relevant answer in the top 10 and the top 150 of the results (success@10, success@150).", "labels": [], "entities": []}, {"text": "For the highest ranked relevant answer per question, we determine the reciprocal rank (RR).", "labels": [], "entities": [{"text": "reciprocal rank (RR)", "start_pos": 70, "end_pos": 90, "type": "METRIC", "confidence": 0.9501372218132019}]}, {"text": "If there is no correct answer retrieved by the system at n = 150, the RR is 0.", "labels": [], "entities": [{"text": "RR", "start_pos": 70, "end_pos": 72, "type": "METRIC", "confidence": 0.9966436624526978}]}, {"text": "Over all questions, we calculate the MRR@150.", "labels": [], "entities": [{"text": "MRR", "start_pos": 37, "end_pos": 40, "type": "METRIC", "confidence": 0.9968442916870117}]}], "tableCaptions": [{"text": " Table 2: Results for the original why-QA pipeline system  success@10 success@150 MRR@150  Wumpus/QAP Retrieval  43.0%  73.1%  0.260  + Re-ranking module  54.8%  73.1%  0.380", "labels": [], "entities": [{"text": "MRR", "start_pos": 82, "end_pos": 85, "type": "METRIC", "confidence": 0.9856141209602356}, {"text": "Wumpus/QAP Retrieval", "start_pos": 91, "end_pos": 111, "type": "DATASET", "confidence": 0.7886531352996826}]}, {"text": " Table 3: Results for passage retrieval on why-questions against Wikipedia using disjoint windows (DW)  and sliding windows (SW)  Success@10  Success@150  MRR@150  Retrieval model  DW  SW  DW  SW  DW  SW  Baseline: Wumpus/QAP  40.9%  72.0%  0.229  Lemur/TFIDF  43.0% 45.2% 71.1% 81.7% 0.247 0.338  Lemur/Okapi  41.9% 44.1% 67.7% 79.6% 0.243 0.320  Lemur/KL  48.9% 50.0% 72.8% 77.2% 0.263 0.324", "labels": [], "entities": [{"text": "passage retrieval", "start_pos": 22, "end_pos": 39, "type": "TASK", "confidence": 0.9481804072856903}, {"text": "MRR", "start_pos": 155, "end_pos": 158, "type": "METRIC", "confidence": 0.9875627756118774}]}]}