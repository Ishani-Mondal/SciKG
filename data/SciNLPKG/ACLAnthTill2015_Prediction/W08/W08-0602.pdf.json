{"title": [{"text": "Extracting Clinical Relationships from Patient Narratives", "labels": [], "entities": [{"text": "Extracting Clinical Relationships from Patient Narratives", "start_pos": 0, "end_pos": 57, "type": "TASK", "confidence": 0.9008950293064117}]}], "abstractContent": [{"text": "The Clinical E-Science Framework (CLEF) project has built a system to extract clinically significant information from the tex-tual component of medical records, for clinical research, evidence-based healthcare and genotype-meets-phenotype informatics.", "labels": [], "entities": []}, {"text": "One part of this system is the identification of relationships between clinically important entities in the text.", "labels": [], "entities": []}, {"text": "Typical approaches to relationship extraction in this domain have used full parses, domain-specific grammars, and large knowledge bases encoding domain knowledge.", "labels": [], "entities": [{"text": "relationship extraction", "start_pos": 22, "end_pos": 45, "type": "TASK", "confidence": 0.8583623170852661}]}, {"text": "In other areas of biomedical NLP, statistical machine learning approaches are now routinely applied to relationship extraction.", "labels": [], "entities": [{"text": "biomedical NLP", "start_pos": 18, "end_pos": 32, "type": "TASK", "confidence": 0.6562142372131348}, {"text": "relationship extraction", "start_pos": 103, "end_pos": 126, "type": "TASK", "confidence": 0.8393269181251526}]}, {"text": "We report on the novel application of these statistical techniques to clinical relationships.", "labels": [], "entities": []}, {"text": "We describe a supervised machine learning system, trained with a corpus of oncology narratives hand-annotated with clinically important relationships.", "labels": [], "entities": []}, {"text": "Various shallow features are extracted from these texts, and used to train statistical classifiers.", "labels": [], "entities": []}, {"text": "We compare the suitability of these features for clinical relationship extraction, how extraction varies between inter-and intra-sentential relationships , and examine the amount of training data needed to learn various relationships.", "labels": [], "entities": [{"text": "clinical relationship extraction", "start_pos": 49, "end_pos": 81, "type": "TASK", "confidence": 0.6903450091679891}]}], "introductionContent": [{"text": "The application of Natural Language Processing (NLP) is widespread in biomedicine.", "labels": [], "entities": [{"text": "Natural Language Processing (NLP)", "start_pos": 19, "end_pos": 52, "type": "TASK", "confidence": 0.6883158683776855}]}, {"text": "Typically, it is applied to improve access to the ever-burgeoning research literature.", "labels": [], "entities": []}, {"text": "Increasingly, biomedical researchers need to relate this literature to phenotypic data: both to populations, and to individual clinical subjects.", "labels": [], "entities": []}, {"text": "The computer applications used in biomedical research, including NLP applications, therefore need to support genotype-meetsphenotype informatics and the move towards translational biology.", "labels": [], "entities": [{"text": "translational biology", "start_pos": 166, "end_pos": 187, "type": "TASK", "confidence": 0.9194541573524475}]}, {"text": "Such support will undoubtedly include linkage to the information held in individual medical records: both the structured portion, and the unstructured textual portion.", "labels": [], "entities": []}, {"text": "The Clinical E-Science Framework (CLEF) project () is building a framework for the capture, integration and presentation of this clinical information, for research and evidencebased healthcare.", "labels": [], "entities": []}, {"text": "The project's data resource is a repository of the full clinical records for over 20000 cancer patients from the Royal Marsden Hospital, Europe's largest oncology centre.", "labels": [], "entities": [{"text": "Royal Marsden Hospital", "start_pos": 113, "end_pos": 135, "type": "DATASET", "confidence": 0.9203993678092957}]}, {"text": "These records combine structured information, clinical narratives, and free text investigation reports.", "labels": [], "entities": []}, {"text": "CLEF uses information extraction (IE) technology to make information from the textual portion of the medical record available for integration with the structured record, and thus available for clinical care and research.", "labels": [], "entities": [{"text": "information extraction (IE)", "start_pos": 10, "end_pos": 37, "type": "TASK", "confidence": 0.8563961029052735}]}, {"text": "The CLEF IE system analyses the textual records to extract entities, events and the relationships between them.", "labels": [], "entities": []}, {"text": "These relationships give information that is often not available in the structured record.", "labels": [], "entities": []}, {"text": "Why was a drug given?", "labels": [], "entities": []}, {"text": "What were the results of a physical examination?", "labels": [], "entities": []}, {"text": "What problems were not present?", "labels": [], "entities": []}, {"text": "We have previously reported entity extraction in the CLEF IE system ().", "labels": [], "entities": [{"text": "entity extraction", "start_pos": 28, "end_pos": 45, "type": "TASK", "confidence": 0.8087619543075562}, {"text": "CLEF IE system", "start_pos": 53, "end_pos": 67, "type": "DATASET", "confidence": 0.7892083525657654}]}, {"text": "This paper examines relationship extraction.", "labels": [], "entities": [{"text": "relationship extraction", "start_pos": 20, "end_pos": 43, "type": "TASK", "confidence": 0.8551374077796936}]}, {"text": "Extraction of relationships from clinical text is usually carried out as part of a full clinical IE system.", "labels": [], "entities": [{"text": "Extraction of relationships from clinical text", "start_pos": 0, "end_pos": 46, "type": "TASK", "confidence": 0.8755570650100708}]}, {"text": "Several such systems have been described.", "labels": [], "entities": []}, {"text": "They generally use a syntactic parse with domainspecific grammar rules.", "labels": [], "entities": []}, {"text": "The Linguistic String project) used a full syntactic and clinical sublanguage parse to fill template data structures corresponding to medical statements.", "labels": [], "entities": []}, {"text": "These were mapped to a database model incorporating medical facts and the relationships between them., and more recently BioMedLEE () used a semantic lexicon and grammar of domain-specific semantic patterns.", "labels": [], "entities": [{"text": "BioMedLEE", "start_pos": 121, "end_pos": 130, "type": "DATASET", "confidence": 0.7721987366676331}]}, {"text": "The patterns encode the possible relationships between entities, allowing both entities and the relationships between them to be directly matched in the text.", "labels": [], "entities": []}, {"text": "Other systems have incorporated largescale domain-specific knowledge bases.) employed a rich discourse model of entities and their relationships, built using a dependency parse of texts and a description logic knowledge base re-engineered from existing terminologies.", "labels": [], "entities": []}, {"text": "MENELAS ( also used a full parse, a conceptual representation of the text, and a large scale knowledge base.", "labels": [], "entities": [{"text": "MENELAS", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.6939476132392883}]}, {"text": "In other applications of biomedical NLP, a second paradigm has become widespread: the application of statistical machine learning techniques to feature-based models of the text.", "labels": [], "entities": [{"text": "biomedical NLP", "start_pos": 25, "end_pos": 39, "type": "TASK", "confidence": 0.6428426504135132}]}, {"text": "Such approaches have typically been applied to journal texts.", "labels": [], "entities": []}, {"text": "They have been used both for entity recognition and extraction of various relations, such as protein-protein interactions (see, for example,).", "labels": [], "entities": [{"text": "entity recognition", "start_pos": 29, "end_pos": 47, "type": "TASK", "confidence": 0.803284615278244}]}, {"text": "This follows on from the success of these methods in general NLP (see for example).", "labels": [], "entities": []}, {"text": "Statistical machine learning has also been applied to clinical text, but its use has generally been limited to entity recognition.", "labels": [], "entities": [{"text": "Statistical machine learning", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.7075817982355753}, {"text": "entity recognition", "start_pos": 111, "end_pos": 129, "type": "TASK", "confidence": 0.8093666732311249}]}, {"text": "The Mayo Clinic text analysis system (), for example, uses a combination of dictionary lookup and a Na\u00a8\u0131veNa\u00a8\u0131ve Bayes classifier to identify entities for information retrieval applications.", "labels": [], "entities": [{"text": "Mayo Clinic text analysis", "start_pos": 4, "end_pos": 29, "type": "TASK", "confidence": 0.7175760418176651}]}, {"text": "To the best of our knowledge, statistical methods have not been previously applied to extraction of clinical relationships from text.", "labels": [], "entities": [{"text": "extraction of clinical relationships from text", "start_pos": 86, "end_pos": 132, "type": "TASK", "confidence": 0.8684726357460022}]}, {"text": "This paper describes experiments in the statistical machine learning of relationships from a novel text type: oncology narratives.", "labels": [], "entities": []}, {"text": "The set of relationships extracted are considered to be of interest for clinical and research applications down line of IE, such as querying to support clinical research.", "labels": [], "entities": [{"text": "IE", "start_pos": 120, "end_pos": 122, "type": "TASK", "confidence": 0.9617906212806702}]}, {"text": "We apply Support Vector Machine (SVM) classifiers to learn these relationships.", "labels": [], "entities": []}, {"text": "The classifiers are trained and evaluated using novel data: a gold standard corpus of clinical text, hand-annotated with semantic entities and relationships.", "labels": [], "entities": []}, {"text": "In order to test the applicability of this method to the clinical domain, we train classifiers using a number of comparatively simple text features, and look at the contribution of these features to system performance.", "labels": [], "entities": []}, {"text": "Clinically interesting relationships may span several sentences, and so we compare classifiers trained for both intra-and intersentential relationships (spanning one or more sentence boundaries).", "labels": [], "entities": []}, {"text": "We also examine the influence of training corpus size on performance, as hand annotation of training data is the major expense in supervised machine learning.: Relationship types and their argument type constraints.", "labels": [], "entities": []}, {"text": "The CLEF application extracts entities, relationships and modifiers from text.", "labels": [], "entities": []}, {"text": "By entity, we mean some real-world thing, event or state referred to in the text: the drugs that are mentioned, the tests that were carried out, etc.", "labels": [], "entities": []}, {"text": "Modifiers are words that qualify an entity in someway, referring e.g. to the laterality of an anatomical locus, or the negation of a condition (\"no sign of inflammation\").", "labels": [], "entities": []}, {"text": "Entities are connected to each other and to modifiers by relationships: e.g. linking a drug entity to the condition entity for which it is indicated, linking an investigation to its results, or linking a negating phrase to a condition.", "labels": [], "entities": []}, {"text": "The entities, modifiers, and relationships are described by both a formal XML schema, and by a set of detailed definitions.", "labels": [], "entities": []}, {"text": "These were developed by a group of clinical experts through an iterative process, until acceptable agreement was reached.", "labels": [], "entities": []}, {"text": "Entity types are mapped to types from the UMLS semantic network (, each CLEF en-tity type covering several UMLS types.", "labels": [], "entities": [{"text": "UMLS semantic network", "start_pos": 42, "end_pos": 63, "type": "DATASET", "confidence": 0.888602077960968}]}, {"text": "Relationship types are those that were felt necessary to capture the essential clinical dependencies between entities referred to in patient documents, and to support CLEF end user applications.", "labels": [], "entities": []}, {"text": "Each relationship type is constrained to exist between limited pairs of entity types.", "labels": [], "entities": []}, {"text": "For example, the has location relationship can only exist between a Condition entity and a Locus entity.", "labels": [], "entities": []}, {"text": "Some relationships can exist between multiple type pairs.", "labels": [], "entities": []}, {"text": "The full set of relationships and their argument type constraints are shown in.", "labels": [], "entities": []}, {"text": "Examples of each relationship are given in.", "labels": [], "entities": []}, {"text": "Some of the relationships considered important by the clinical experts were not obvious without domain knowledge.", "labels": [], "entities": []}, {"text": "For example, He is suffering from nausea and severe headaches.", "labels": [], "entities": []}, {"text": "Without domain knowledge, it is not clear that there is a has indication relationship between the \"Dolasteron\" Drug or device entity and the \"nausea\" Condition entity.", "labels": [], "entities": [{"text": "Dolasteron\" Drug or device entity", "start_pos": 99, "end_pos": 132, "type": "DATASET", "confidence": 0.8215095599492391}]}, {"text": "As in this example, many of this type of relationship are intra-sentential.", "labels": [], "entities": []}, {"text": "A single real-world entity maybe referred to several times in the same text.", "labels": [], "entities": []}, {"text": "Each of these coreferring expressions is a mention of the entity.", "labels": [], "entities": []}, {"text": "The gold standard includes annotation of co-reference between different textual mentions of the same entity.", "labels": [], "entities": []}, {"text": "For the work reported in this paper, however, co-reference is not considered.", "labels": [], "entities": []}, {"text": "Each entity is assumed to have a single mention.", "labels": [], "entities": []}, {"text": "Relationships between entities can be considered, by extension, as relationships between the single mentions of those entities.", "labels": [], "entities": []}, {"text": "The implications of this are discussed further below.", "labels": [], "entities": []}], "datasetContent": [{"text": "We used a standard ten-fold cross validation methodology and standard evaluation metrics.", "labels": [], "entities": []}, {"text": "Metrics are defined in terms of true positive, false positive and false negative matches between relationships in a system annotated response document and a gold standard key document.", "labels": [], "entities": []}, {"text": "A response relationship is a true positive if a relationship of the same type, and with the exact same arguments, exists in the key.", "labels": [], "entities": []}, {"text": "Corresponding definitions apply for false positive and false negative.", "labels": [], "entities": []}, {"text": "Counts of these matches are used to calculate standard metrics of Recall (R), Precision (P ) and F 1 measure.", "labels": [], "entities": [{"text": "Recall (R)", "start_pos": 66, "end_pos": 76, "type": "METRIC", "confidence": 0.9643467664718628}, {"text": "Precision (P )", "start_pos": 78, "end_pos": 92, "type": "METRIC", "confidence": 0.9647785276174545}, {"text": "F 1 measure", "start_pos": 97, "end_pos": 108, "type": "METRIC", "confidence": 0.9810569087664286}]}, {"text": "The metrics do not say how hard relationship extraction is.", "labels": [], "entities": [{"text": "relationship extraction", "start_pos": 32, "end_pos": 55, "type": "TASK", "confidence": 0.7842265069484711}]}, {"text": "We therefore provide a comparison with Inter Annotator Agreement (IAA) scores from the gold standard.", "labels": [], "entities": [{"text": "Inter Annotator Agreement (IAA)", "start_pos": 39, "end_pos": 70, "type": "METRIC", "confidence": 0.615317831436793}]}, {"text": "The IAA score gives the agreement between the two independent double annotators.", "labels": [], "entities": [{"text": "IAA score", "start_pos": 4, "end_pos": 13, "type": "METRIC", "confidence": 0.6863194704055786}, {"text": "agreement", "start_pos": 24, "end_pos": 33, "type": "METRIC", "confidence": 0.981012225151062}]}, {"text": "It is equivalent to scoring one annotator against the other using the F 1 metric.", "labels": [], "entities": [{"text": "F 1 metric", "start_pos": 70, "end_pos": 80, "type": "METRIC", "confidence": 0.9108511408170065}]}, {"text": "IAA scores are not directly comparable here, as relationship annotation is  a slightly different task for the human annotators.", "labels": [], "entities": [{"text": "relationship annotation", "start_pos": 48, "end_pos": 71, "type": "TASK", "confidence": 0.8205510079860687}]}, {"text": "The relationship extraction system is given entities, and finds relationships between them.", "labels": [], "entities": [{"text": "relationship extraction", "start_pos": 4, "end_pos": 27, "type": "TASK", "confidence": 0.7391702681779861}]}, {"text": "Human annotators must find both the entities and the relationships.", "labels": [], "entities": []}, {"text": "Therefore, were one human annotator to fail to find a particular entity, they could never find relationships with that entity.", "labels": [], "entities": []}, {"text": "The raw IAA score does not take this into account: if an annotator fails to find an entity, then they will also be penalised for all relationships with that entity.", "labels": [], "entities": []}, {"text": "We therefore give a Corrected IAA, CIAA, in which annotators are only compared on those relations for which they have both found the entities involved.", "labels": [], "entities": [{"text": "Corrected IAA", "start_pos": 20, "end_pos": 33, "type": "METRIC", "confidence": 0.7210790514945984}]}, {"text": "Both forms of IAA are shown in.", "labels": [], "entities": [{"text": "IAA", "start_pos": 14, "end_pos": 17, "type": "TASK", "confidence": 0.7042261362075806}]}, {"text": "It is clear that it is hard for annotators to reach agreement on relationships, and that this is compounded massively by lack of perfect agreement on entities.", "labels": [], "entities": []}, {"text": "Note that the gold standard used in training and evaluation reflects a further consensus annotation, to correct this poor agreement.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Count of relationships in 77 gold standard documents.", "labels": [], "entities": [{"text": "Count", "start_pos": 10, "end_pos": 15, "type": "METRIC", "confidence": 0.9396049380302429}]}, {"text": " Table 4: Variation in performance by feature set. Features sets are abbreviated as in", "labels": [], "entities": []}, {"text": " Table 3. For the first seven  columns, features were added cumulatively to each other. The next two columns, allgen and notok, are as de- scribed in", "labels": [], "entities": []}, {"text": " Table 3. The final two columns give inter annotator agreement and corrected inter annotator agreement, for  comparison.", "labels": [], "entities": [{"text": "corrected inter annotator agreement", "start_pos": 67, "end_pos": 102, "type": "METRIC", "confidence": 0.7250876873731613}]}, {"text": " Table 5: Variation in performance, by number of sentence boundaries (n), and by training corpus size.", "labels": [], "entities": []}]}