{"title": [{"text": "Simple is Best: Experiments with Different Document Segmentation Strategies for Passage Retrieval", "labels": [], "entities": [{"text": "Passage Retrieval", "start_pos": 80, "end_pos": 97, "type": "TASK", "confidence": 0.9229212403297424}]}], "abstractContent": [{"text": "Passage retrieval is used in QA to filter large document collections in order to find text units relevant for answering given questions.", "labels": [], "entities": [{"text": "Passage retrieval", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.8678630590438843}]}, {"text": "In our QA system we apply standard IR techniques and index-time passaging in the retrieval component.", "labels": [], "entities": [{"text": "IR", "start_pos": 35, "end_pos": 37, "type": "TASK", "confidence": 0.9357480406761169}]}, {"text": "In this paper we investigate several ways of dividing documents into passages.", "labels": [], "entities": [{"text": "dividing documents into passages", "start_pos": 45, "end_pos": 77, "type": "TASK", "confidence": 0.8705709278583527}]}, {"text": "In particular we look at semantically motivated approaches (using coreference chains and discourse clues) compared with simple window-based techniques.", "labels": [], "entities": []}, {"text": "We evaluate retrieval performance and the overall QA performance in order to study the impact of the different segmentation approaches.", "labels": [], "entities": []}, {"text": "From our experiments we can conclude that the simple techniques using fixed-sized windows clearly outperform the semantically motivated approaches, which indicates that uniformity in size seems to be more important than semantic coherence in our setup.", "labels": [], "entities": []}], "introductionContent": [{"text": "Passage retrieval in question answering is different from information retrieval in general.", "labels": [], "entities": [{"text": "Passage retrieval", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.8111520111560822}, {"text": "question answering", "start_pos": 21, "end_pos": 39, "type": "TASK", "confidence": 0.7786135971546173}, {"text": "information retrieval", "start_pos": 58, "end_pos": 79, "type": "TASK", "confidence": 0.7837413549423218}]}, {"text": "Extracting relevant passages from large document collections is only one step in answering a natural language question.", "labels": [], "entities": [{"text": "Extracting relevant passages from large document collections", "start_pos": 0, "end_pos": 60, "type": "TASK", "confidence": 0.8904983401298523}]}, {"text": "There are two main differences: i) Passage retrieval queries are generated from complete sentences (questions) compared to bag-of-keyword queries usually used in IR.", "labels": [], "entities": [{"text": "Passage retrieval queries", "start_pos": 35, "end_pos": 60, "type": "TASK", "confidence": 0.8992986679077148}]}, {"text": "ii) Retrieved passages have to be processed further in or- der to extract concrete answers to the given question.", "labels": [], "entities": []}, {"text": "Hence, the size of the passages retrieved is important and smaller units are preferred.", "labels": [], "entities": []}, {"text": "Here, the division of documents into passages is crucial.", "labels": [], "entities": [{"text": "division of documents into passages", "start_pos": 10, "end_pos": 45, "type": "TASK", "confidence": 0.8850446105003357}]}, {"text": "The textual units have to be big enough to ensure IR works properly and they have to be small enough to enable efficient and accurate QA.", "labels": [], "entities": [{"text": "IR", "start_pos": 50, "end_pos": 52, "type": "TASK", "confidence": 0.9688229560852051}, {"text": "QA", "start_pos": 134, "end_pos": 136, "type": "TASK", "confidence": 0.8563222289085388}]}, {"text": "In this study we investigate whether semantically motivated passages in the retrieval component lead to better QA performance compared to the use of document retrieval and window-based segmentation approaches.", "labels": [], "entities": []}], "datasetContent": [{"text": "There are various metrics that can be employed for evaluating passage retrieval.", "labels": [], "entities": [{"text": "passage retrieval", "start_pos": 62, "end_pos": 79, "type": "TASK", "confidence": 0.9067376255989075}]}, {"text": "Commonly it is argued that passage retrieval for QA is merely a filtering task and ranking (precision) is less important than recall.", "labels": [], "entities": [{"text": "passage retrieval", "start_pos": 27, "end_pos": 44, "type": "TASK", "confidence": 0.9207127690315247}, {"text": "precision", "start_pos": 92, "end_pos": 101, "type": "METRIC", "confidence": 0.961323082447052}, {"text": "recall", "start_pos": 126, "end_pos": 132, "type": "METRIC", "confidence": 0.9966627955436707}]}, {"text": "Therefore, the measure of redundancy has been introduced which is defined as the average number of relevant passages retrieved per question (independent of any ranking).", "labels": [], "entities": []}, {"text": "Passage retrieval is, of course, a bottleneck in QA systems that make use of such a component.", "labels": [], "entities": [{"text": "Passage retrieval", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.9016949832439423}]}, {"text": "The system has no chance to find an answer if the retrieval engine fails to return relevant passages.", "labels": [], "entities": []}, {"text": "Therefore, another measure, coverage is often used in combination with redundancy.", "labels": [], "entities": [{"text": "coverage", "start_pos": 28, "end_pos": 36, "type": "METRIC", "confidence": 0.9954015016555786}]}, {"text": "It is defined as the proportion of questions for which at least one relevant passage is found.", "labels": [], "entities": []}, {"text": "In order to validate the use of these measures in our setup we experimented with retrieving various amounts of paragraphs.", "labels": [], "entities": []}, {"text": "illustrates the relation of coverage and redundancy scores compared to the overall QA performance measured in terms of M RR scores.", "labels": [], "entities": [{"text": "coverage", "start_pos": 28, "end_pos": 36, "type": "METRIC", "confidence": 0.9929654598236084}, {"text": "M RR scores", "start_pos": 119, "end_pos": 130, "type": "METRIC", "confidence": 0.8430923620859782}]}, {"text": "From the figure we can conclude that coverage is more important than redundancy in our system.", "labels": [], "entities": [{"text": "coverage", "start_pos": 37, "end_pos": 45, "type": "METRIC", "confidence": 0.8909936547279358}]}, {"text": "In other words, our QA system is quite good in finding appropriate answers if there is at least one relevant passage in the set of retrieved ones.", "labels": [], "entities": []}, {"text": "Redundancy on the other hand does not seem to provide valuable insides for the end-to-end performance of our QA system.", "labels": [], "entities": []}, {"text": "However, our system also uses the passage retrieval score (and, hence, the ranking) as a clue for answer extraction.", "labels": [], "entities": [{"text": "answer extraction", "start_pos": 98, "end_pos": 115, "type": "TASK", "confidence": 0.8897216022014618}]}, {"text": "Therefore, other standard IR measures might be interesting for our investigations as well.", "labels": [], "entities": [{"text": "IR", "start_pos": 26, "end_pos": 28, "type": "TASK", "confidence": 0.9837368130683899}]}, {"text": "The following three metrics are common in the IR literature.", "labels": [], "entities": [{"text": "IR", "start_pos": 46, "end_pos": 48, "type": "TASK", "confidence": 0.9613057971000671}]}], "tableCaptions": [{"text": " Table 1: Baselines with sentence (sent), paragraph  (par) and document (doc) retrieval (20 units).  M RR QA is measured on the top 5 answers re-", "labels": [], "entities": [{"text": "M RR QA", "start_pos": 101, "end_pos": 108, "type": "METRIC", "confidence": 0.8618526061375936}]}, {"text": " Table 2: Passage retrieval with document segmen- tation using coreference chains and TextTiling (re- trieving a maximum of 20 passages; \u25b3 means sig- nificant with p < 0.05 and Wilcoxon Matched-pair  Signed-Ranks Test compared to paragraph base- line -only tested for M RR QA )", "labels": [], "entities": [{"text": "Passage retrieval", "start_pos": 10, "end_pos": 27, "type": "TASK", "confidence": 0.9193131327629089}, {"text": "document segmen- tation", "start_pos": 33, "end_pos": 56, "type": "TASK", "confidence": 0.5347102582454681}, {"text": "M RR QA", "start_pos": 268, "end_pos": 275, "type": "TASK", "confidence": 0.4905710617701213}]}, {"text": " Table 3: Passage retrieval with window-based doc- ument segmentation (\u25b3 means significant with  p < 0.05 and Wilcoxon Matched-pair Signed-", "labels": [], "entities": [{"text": "Passage retrieval", "start_pos": 10, "end_pos": 27, "type": "TASK", "confidence": 0.9518090486526489}, {"text": "window-based doc- ument segmentation", "start_pos": 33, "end_pos": 69, "type": "TASK", "confidence": 0.6578568875789642}]}, {"text": " Table 4: Passage retrieval with window-based doc- ument segmentation and a sliding window", "labels": [], "entities": [{"text": "Passage retrieval", "start_pos": 10, "end_pos": 27, "type": "TASK", "confidence": 0.9428533613681793}, {"text": "window-based doc- ument segmentation", "start_pos": 33, "end_pos": 69, "type": "TASK", "confidence": 0.6491317391395569}]}]}