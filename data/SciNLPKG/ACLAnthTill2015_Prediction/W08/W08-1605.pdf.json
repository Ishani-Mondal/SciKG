{"title": [{"text": "Personalized, Interactive Question Answering on the Web", "labels": [], "entities": [{"text": "Interactive Question Answering", "start_pos": 14, "end_pos": 44, "type": "TASK", "confidence": 0.6005815466245016}]}], "abstractContent": [{"text": "Two of the current issues of Question Answering (QA) systems are the lack of personaliza-tion to the individual users' needs, and the lack of interactivity by which at the end of each Q/A session the context of interaction is lost.", "labels": [], "entities": [{"text": "Question Answering (QA)", "start_pos": 29, "end_pos": 52, "type": "TASK", "confidence": 0.8609396934509277}]}, {"text": "We address these issues by designing and implementing a model of personalized, interactive QA based on a User Modelling component and on a conversational interface.", "labels": [], "entities": []}, {"text": "Our evaluation with respect to a baseline QA system yields encouraging results in both personaliza-tion and interactivity.", "labels": [], "entities": []}], "introductionContent": [{"text": "Information overload, i.e. the presence of an excessive amount of data from which to search for relevant information, is a common problem to Information Retrieval (IR) and its subdiscipline of Question Answering (QA), that aims at finding concise answers to questions in natural language.", "labels": [], "entities": [{"text": "Information Retrieval (IR)", "start_pos": 141, "end_pos": 167, "type": "TASK", "confidence": 0.8509173393249512}, {"text": "Question Answering (QA)", "start_pos": 193, "end_pos": 216, "type": "TASK", "confidence": 0.8474841594696045}]}, {"text": "In Web-based QA in particular, this problem affects the relevance of results with respect to the users' needs, as queries can be ambiguous and even answers extracted from documents with relevant content but expressed in a difficult language maybe illreceived by users.", "labels": [], "entities": []}, {"text": "While the need for user personalization has been addressed by the IR community fora longtime, very little effort has been carried out up to now in the QA community in this direction.", "labels": [], "entities": []}, {"text": "Indeed, personalized Question Answering has been advocated in TREC-QA starting from 2003; however, the issue was solved rather expeditiously by designing a scenario where an \"average news reader\" was imagined to submit the 2003 task's definition questions.", "labels": [], "entities": [{"text": "Question Answering", "start_pos": 21, "end_pos": 39, "type": "TASK", "confidence": 0.7334023416042328}, {"text": "TREC-QA", "start_pos": 62, "end_pos": 69, "type": "DATASET", "confidence": 0.7903522253036499}]}, {"text": "Moreover, a commonly observed behavior in users of IR systems is that they often issue queries not as standalone questions but in the context of a wider information need, for instance when researching a specific topic.", "labels": [], "entities": []}, {"text": "Recently, anew research direction has been proposed, which involves the integration of QA systems with dialogue interfaces in order to encourage and accommodate the submission of multiple related questions and handle the user's requests for clarification in a less artificial setting; however, Interactive QA (IQA) systems are still at an early stage or applied to closed domains ().", "labels": [], "entities": []}, {"text": "Also, the \"complex, interactive QA\" TREC track (www.umiacs.umd.edu/ \u02dc jimmylin/ciqa/) has been organized, but here the interactive aspect refers to the evaluators being enabled to interact with the systems rather than to dialogue per se.", "labels": [], "entities": [{"text": "TREC", "start_pos": 36, "end_pos": 40, "type": "METRIC", "confidence": 0.7862130403518677}]}, {"text": "In this paper, we first present an adaptation of User Modelling) to the design of personalized QA, and secondly we design and implement an interactive open-domain QA system, YourQA.", "labels": [], "entities": []}, {"text": "Section 2 briefly introduces the baseline architecture of YourQA.", "labels": [], "entities": []}, {"text": "In Section 3, we show how a model of the user's reading abilities and personal interests can be used to efficiently improve the quality of the information returned by a QA system.", "labels": [], "entities": []}, {"text": "We provide an extensive evaluation methodology to assess such efficiency by improving on our previous work in this area).", "labels": [], "entities": []}, {"text": "Moreover, we discuss our design of interactive QA in Section 4 and conduct a more rigorous evaluation of the interactive version of YourQA by comparing it to the baseline version on a set of TREC-QA questions, obtaining encouraging results.", "labels": [], "entities": [{"text": "YourQA", "start_pos": 132, "end_pos": 138, "type": "DATASET", "confidence": 0.9308648705482483}]}, {"text": "Finally, a unified model of personalized, interactive QA is described in Section 5.", "labels": [], "entities": []}], "datasetContent": [{"text": "Reading level estimation was evaluated by first assessing the robustness of the unigram language models by running 10-fold cross-validation on the set of documents used to create such models, and averaging the ratio of correctly classified documents with respect to the total number of documents for each fold.", "labels": [], "entities": [{"text": "Reading level estimation", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.7068326274553934}]}, {"text": "Our results gave a very high accuracy, i.e. 94.23% \u00b1 1.98 standard deviation.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 29, "end_pos": 37, "type": "METRIC", "confidence": 0.9994428753852844}]}, {"text": "However, this does not prove a direct effect on the user's perception of such levels.", "labels": [], "entities": []}, {"text": "For this purpose, we defined Reading level agreement (A r ) as the percentage of documents rated by the users as suitable to the reading level to which they were assigned.", "labels": [], "entities": [{"text": "Reading level agreement (A r )", "start_pos": 29, "end_pos": 59, "type": "METRIC", "confidence": 0.838803367955344}]}, {"text": "We performed a second experiment with 20 subjects aged between 16 and 52 and with a self-assessed good or medium English reading level.", "labels": [], "entities": []}, {"text": "They evaluated the answers returned by the system to 24 questions into 3 groups (basic, medium and advanced reading levels), by assessing whether they agreed that the given answer was assigned to the correct reading level.", "labels": [], "entities": []}, {"text": "Our results show that altogether, evaluators found answers appropriate for the reading levels to which they were assigned.", "labels": [], "entities": []}, {"text": "The agreement decreased from 94% for A adv to 85% for A med to 72% for A bas ; this was predictable as it is more constraining to conform to a lower reading level than to a higher one.", "labels": [], "entities": []}, {"text": "The impact of the UM profile was tested by using as a baseline the standard version of YourQA, where the UM component is inactive.", "labels": [], "entities": [{"text": "YourQA", "start_pos": 87, "end_pos": 93, "type": "DATASET", "confidence": 0.9442077875137329}]}, {"text": "Ten adult participants from various backgrounds took part in the experiment; they were invited to form an individual profile by brainstorming key-phrases for 2-3 topics of their interest chosen from the Yahoo!", "labels": [], "entities": []}, {"text": "directory (dir. yahoo.com): examples were \"ballet\", \"RPGs\" and \"dog health\".", "labels": [], "entities": []}, {"text": "For each user, we created the following 3 questions so that he/she would submit them to the QA system: Q per , related to the user's profile, for answering which the personalized version of YourQA would be used; Q bas , related to the user's profile, for which the baseline version of the system would be used; and Q unr , unrelated to the user's profile, hence not affected by personalization.", "labels": [], "entities": []}, {"text": "The reason why we handcrafted questions rather than letting users spontaneously interact with YourQA's two versions is that we wanted the results of the two versions to be different in order to measure a preference.", "labels": [], "entities": [{"text": "YourQA's", "start_pos": 94, "end_pos": 102, "type": "DATASET", "confidence": 0.9447594285011292}]}, {"text": "After examining the top 5 results to each question, users had to answer the following questionnaire 5 : \u2022 For each of the five results separately: The experiment results are summarized in.", "labels": [], "entities": []}, {"text": "The The results were compared by carrying out a oneway analysis of variance (ANOVA) and performing the Fischer test using the usefulness as factor (with the The adoption of a Likert scale made it possible to compute the average and standard deviations of the user comments with respect to each answer among the top five returned by the system.", "labels": [], "entities": [{"text": "ANOVA", "start_pos": 77, "end_pos": 82, "type": "METRIC", "confidence": 0.5838499665260315}, {"text": "Fischer test", "start_pos": 103, "end_pos": 115, "type": "METRIC", "confidence": 0.9574010670185089}]}, {"text": "It was therefore possible to replace the binary measurement of perceived usefulness, relatedness and sensitivity used in) in terms of total number of users with a more fine-grained one in terms of average computed over the users.", "labels": [], "entities": [{"text": "sensitivity", "start_pos": 101, "end_pos": 112, "type": "METRIC", "confidence": 0.9369059205055237}]}, {"text": "three queries as levels) at a 95% level of confidence.", "labels": [], "entities": []}, {"text": "The test revealed an overall significant difference between factors, confirming that users are positively biased towards questions related to their own profile when it comes to perceived utility.", "labels": [], "entities": []}, {"text": "To analyze the answers to TEST2, which measured the perceived relatedness of each answer to the current profile, we used ANOVA again and and obtained an overall significant difference.", "labels": [], "entities": [{"text": "TEST2", "start_pos": 26, "end_pos": 31, "type": "DATASET", "confidence": 0.49480441212654114}, {"text": "ANOVA", "start_pos": 121, "end_pos": 126, "type": "METRIC", "confidence": 0.9705712199211121}]}, {"text": "Hence, answers obtained without using the users' profile were perceived as significantly less related to those obtained using their own profile, i.e. there is a significant difference between Q rel and Q bas . As expected, the difference between Q rel and Q unr is even more significant.", "labels": [], "entities": []}, {"text": "Thirdly, the ANOVA table computed using average perceived time (TEST3) as variable and the three questions as factors did not give any significance, nor did any of the paired t-tests computed over each result pair.", "labels": [], "entities": [{"text": "ANOVA", "start_pos": 13, "end_pos": 18, "type": "METRIC", "confidence": 0.9346678853034973}, {"text": "average perceived time (TEST3)", "start_pos": 40, "end_pos": 70, "type": "METRIC", "confidence": 0.7847698032855988}]}, {"text": "We concluded that apparently, the time spent browsing results is not directly correlated to the personalization of results.", "labels": [], "entities": []}, {"text": "Finally, the average sensitivity of the five answers altogether (TEST4) computed over the ten participants for each query shows an overall significant difference in perceived sensitivity between the answers to question Q rel (3.9\u00b10.7) and those to question Q bas (2.5\u00b11.1) and Q unr (1.8\u00b11.2).", "labels": [], "entities": [{"text": "TEST4)", "start_pos": 65, "end_pos": 71, "type": "METRIC", "confidence": 0.9758053123950958}]}, {"text": "To conclude, our experience with profile evaluation shows that personalized QA techniques yield answers that are indeed perceived as more satisfying to users in terms of usefulness and relatedness to their own profile.", "labels": [], "entities": []}, {"text": "To assess the utility of a chatbot-based dialogue manager in an open-domain QA application, we conducted an exploratory Wizard of Oz experiment.", "labels": [], "entities": []}, {"text": "Wizard-of-Oz (WOz) experiments are usually deployed for natural language systems to obtain initial data when a full-fledged prototype is not yet available) and consist in \"hiding\" a human operator behind a computer interface to simulate a conversation with the user, who believes to be interacting with a fully automated prototype.", "labels": [], "entities": []}, {"text": "We designed six tasks reflecting the intended typical usage of the system (e.g.: \"Find out who painted Guernica and ask the system for more information about the artist\") to be carried out by 7 users by interacting with an instant messaging platform, which they were told to be the system interface.", "labels": [], "entities": []}, {"text": "The role of the Wizard was to simulate a limited range of utterances and conversational situations handled by a chatbot.", "labels": [], "entities": []}, {"text": "User feedback was collected mainly by using a post-hoc questionnaire inspired by the experiment in, which consists of questions Q 1 to Q 6 in, col.", "labels": [], "entities": []}, {"text": "1, to be answered using a scale from 1=\"Not at all\" to 5=\"Yes, absolutely\".", "labels": [], "entities": []}, {"text": "From the WOz results, reported in, col.", "labels": [], "entities": [{"text": "WOz", "start_pos": 9, "end_pos": 12, "type": "DATASET", "confidence": 0.6877826452255249}]}, {"text": "\"WOz\", users appear to be generally very satisfied with the system's performances: Q 6 obtained an average of 4.5\u00b1.5.", "labels": [], "entities": [{"text": "WOz", "start_pos": 1, "end_pos": 4, "type": "DATASET", "confidence": 0.7466026544570923}]}, {"text": "None of the users had difficulties in reformulating their questions when this was requested: Q 4 obtained 3.8\u00b1.5.", "labels": [], "entities": []}, {"text": "For the remaining questions, satisfaction levels were high: users generally thought that the system understood their information needs (Q 2 obtained 4) and were able to obtain such information (Q 1 obtained 4.3\u00b1.5).", "labels": [], "entities": [{"text": "satisfaction", "start_pos": 29, "end_pos": 41, "type": "METRIC", "confidence": 0.9795935153961182}]}, {"text": "The dialogue manager and interface of YourQA were implemented based on the dialogue scenario and the successful outcome of the WOz experiment.", "labels": [], "entities": []}, {"text": "For the evaluation of interactivity, we built on our previous results from a Wizard-of-Oz experiment and an initial evaluation conducted on a limited set of handcrafted questions).", "labels": [], "entities": []}, {"text": "We chose 9 question series from the TREC-QA 2007 campaign 8 . Three questions were retained per series to make each evaluation balanced.", "labels": [], "entities": [{"text": "TREC-QA 2007 campaign", "start_pos": 36, "end_pos": 57, "type": "DATASET", "confidence": 0.85263991355896}]}, {"text": "For instance, the three following questions were used to form one task: 266.1: \"When was Rafik Hariri born?\", 266.2: \"To what religion did he belong (including sect)?\" and 266.4: \"At what time in the day was he assassinated?\".", "labels": [], "entities": []}, {"text": "Twelve users were invited to find answers to the questions to one of them by using the standard version of the system and to the second by using the interactive version.", "labels": [], "entities": []}, {"text": "Each series was evaluated at least once using both versions of the system.", "labels": [], "entities": []}, {"text": "At the end of the experiment, users had to give feedback about both versions 7 chatterbean.bitoflife.cjb.net.", "labels": [], "entities": []}, {"text": "8 trec.nist.gov of the system by filling in the satisfaction questionnaire reported in.", "labels": [], "entities": []}, {"text": "Although the paired t-test conducted to compare questionnaire replies to the standard and interactive versions did not register statistical significance, we believe that the evidence we collected suggests a few interesting interpretations.", "labels": [], "entities": []}, {"text": "First, a good overall satisfaction appears with both versions of the system (Q 6 ), with a slight difference in favor of the interactive version.", "labels": [], "entities": []}, {"text": "The two versions of the system seem to offer different advantages: while the ease of use of the standard version was rated higher (Q 3 ), probably because the system's reformulation requests added a challenge to users used to search engine interaction, users felt they obtained more information using the interactive version (Q 1 ).", "labels": [], "entities": []}, {"text": "Concerning interaction comfort, users seemed to feel that the interactive version understood better their requests than the standard one (Q 2 ); they also found it easy to reformulate questions when the former asked to ).", "labels": [], "entities": []}, {"text": "However, while the pace of interaction was judged slightly more appropriate in the interactive case (Q 7 ), interaction was considered faster when using the standard version ).", "labels": [], "entities": []}, {"text": "This partly explains the fact that users seemed more ready to use again the standard version of the system (Q 5 ).", "labels": [], "entities": []}, {"text": "comments were mixed: while some of them were enthusiastic about the chatbot's small-talk features, others clearly said that they felt more comfortable with a search engine-like interface.", "labels": [], "entities": []}, {"text": "Most of the critical aspects emerging from our overall satisfactory evaluation depend on the specific system we have tested rather than on the nature of interactive QA, to which none of such results appear to be detrimental.", "labels": [], "entities": []}, {"text": "We believe that the search-engine-style use and interpretation of QA systems are due to the fact that QA is still a very little known technology.", "labels": [], "entities": []}, {"text": "It is a challenge for both developers and the larger public to cooperate in designing and discovering applications that take advantage of the potentials of interactivity.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Profile evaluation results (avg \u00b1 st. dev.)", "labels": [], "entities": []}, {"text": " Table 2: Interactive QA evaluation results obtained for  the WOz, Standard and Interactive versions of YourQA.  Average \u00b1 st. dev. are reported.", "labels": [], "entities": [{"text": "WOz", "start_pos": 62, "end_pos": 65, "type": "DATASET", "confidence": 0.9319330453872681}, {"text": "YourQA", "start_pos": 104, "end_pos": 110, "type": "DATASET", "confidence": 0.8618682026863098}]}, {"text": " Table 2.  Although the paired t-test conducted to compare  questionnaire replies to the standard and interactive ver- sions did not register statistical significance, we believe  that the evidence we collected suggests a few interest- ing interpretations.", "labels": [], "entities": []}]}