{"title": [{"text": "A Smorgasbord of Features for Automatic MT Evaluation", "labels": [], "entities": [{"text": "MT Evaluation", "start_pos": 40, "end_pos": 53, "type": "TASK", "confidence": 0.9184817671775818}]}], "abstractContent": [{"text": "This document describes the approach by the NLP Group at the Technical University of Cat-alonia (UPC-LSI), for the shared task on Automatic Evaluation of Machine Translation at the ACL 2008 Third SMT Workshop.", "labels": [], "entities": [{"text": "Technical University of Cat-alonia (UPC-LSI)", "start_pos": 61, "end_pos": 105, "type": "DATASET", "confidence": 0.7270771094730922}, {"text": "Automatic Evaluation of Machine Translation at the ACL 2008 Third SMT Workshop", "start_pos": 130, "end_pos": 208, "type": "TASK", "confidence": 0.765621043741703}]}], "introductionContent": [{"text": "Our proposal is based on a rich set of individual metrics operating at different linguistic levels: lexical (i.e., on word forms), shallow-syntactic (e.g., on word lemmas, part-of-speech tags, and base phrase chunks), syntactic (e.g., on dependency and constituency trees), shallow-semantic (e.g., on named entities and semantic roles), and semantic (e.g., on discourse representations).", "labels": [], "entities": []}, {"text": "Although from different viewpoints, and based on different similarity assumptions, in all cases, translation quality is measured by comparing automatic translations against human references.", "labels": [], "entities": [{"text": "translation", "start_pos": 97, "end_pos": 108, "type": "TASK", "confidence": 0.9659414291381836}]}, {"text": "Extensive details on the metric set maybe found in the IQMT technical manual.", "labels": [], "entities": [{"text": "IQMT technical manual", "start_pos": 55, "end_pos": 76, "type": "DATASET", "confidence": 0.896581749121348}]}, {"text": "Apart from individual metrics, we have also applied a simple integration scheme based on uniformly-averaged linear metric combinations ().", "labels": [], "entities": []}], "datasetContent": [{"text": "We use all into-English test beds from the 2006 and 2007 editions of the SMT workshop (.", "labels": [], "entities": [{"text": "SMT workshop", "start_pos": 73, "end_pos": 85, "type": "TASK", "confidence": 0.8743812143802643}]}, {"text": "These include the translation of three different language-pairs: German-to-English (de-en), Spanish-to-English (es-en), and French-to-English (fr-en), over two different scenarios: in-domain (European Parliament Proceedings) and out-of-domain (News Commentary Corpus) . In all cases, a single reference translation is available.", "labels": [], "entities": [{"text": "News Commentary Corpus", "start_pos": 244, "end_pos": 266, "type": "DATASET", "confidence": 0.8235484759012858}]}, {"text": "In addition, human assessments on adequacy and fluency are available fora subset of systems and sentences.", "labels": [], "entities": []}, {"text": "Each sentence has been evaluated at least by two different judges.", "labels": [], "entities": []}, {"text": "A brief numerical description of these test beds is available in '#sys' columns shows the number of systems counting on human assessments with respect to the total number of systems which participated in each task.", "labels": [], "entities": []}, {"text": "Metrics are evaluated in terms of human acceptability, i.e., according to their ability to capture the degree of acceptability to humans of automatic translations.", "labels": [], "entities": []}, {"text": "We measure human acceptability by computing Pearson correlation coefficients between automatic metric scores and human assessments of translation quality both at document and sentence level.", "labels": [], "entities": [{"text": "Pearson correlation", "start_pos": 44, "end_pos": 63, "type": "METRIC", "confidence": 0.920156866312027}]}, {"text": "We use the sum of adequacy and fluency to simulate a global assessment of quality.", "labels": [], "entities": []}, {"text": "Assessments from different judges over the same test case are averaged into a single score.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Test bed description. '#snt' columns show the  number of sentences assessed (considering all systems).", "labels": [], "entities": []}, {"text": " Table 2: Meta-evaluation results based on human acceptability for the WMT 2007 into-English translation tasks", "labels": [], "entities": [{"text": "WMT 2007 into-English translation", "start_pos": 71, "end_pos": 104, "type": "TASK", "confidence": 0.648443266749382}]}]}