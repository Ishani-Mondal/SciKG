{"title": [{"text": "Using lexico-semantic information for query expansion in passage retrieval for question answering", "labels": [], "entities": [{"text": "query expansion", "start_pos": 38, "end_pos": 53, "type": "TASK", "confidence": 0.7251339107751846}, {"text": "passage retrieval", "start_pos": 57, "end_pos": 74, "type": "TASK", "confidence": 0.8386284112930298}, {"text": "question answering", "start_pos": 79, "end_pos": 97, "type": "TASK", "confidence": 0.8462233543395996}]}], "abstractContent": [{"text": "In this paper we investigate the use of several types of lexico-semantic information for query expansion in the passage retrieval component of our QA system.", "labels": [], "entities": [{"text": "query expansion", "start_pos": 89, "end_pos": 104, "type": "TASK", "confidence": 0.7994425594806671}, {"text": "passage retrieval", "start_pos": 112, "end_pos": 129, "type": "TASK", "confidence": 0.7752238512039185}]}, {"text": "We have used four corpus-based methods to acquire semantically related words, and we have used one hand-built resource.", "labels": [], "entities": []}, {"text": "We evaluate our techniques on the Dutch CLEF QA track.", "labels": [], "entities": [{"text": "Dutch CLEF QA track", "start_pos": 34, "end_pos": 53, "type": "DATASET", "confidence": 0.9041991531848907}]}, {"text": "1 In our experiments expansions that try to bridge the terminological gap between question and document collection do not result in any improvements.", "labels": [], "entities": [{"text": "question and document collection", "start_pos": 82, "end_pos": 114, "type": "TASK", "confidence": 0.6153633818030357}]}, {"text": "However , expansions bridging the knowledge gap show modest improvements.", "labels": [], "entities": []}], "introductionContent": [{"text": "Information retrieval (IR) is used inmost QA systems to filter out relevant passages from large document collections to narrow down the search for answer extraction modules in a QA system.", "labels": [], "entities": [{"text": "Information retrieval (IR)", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.8394954800605774}, {"text": "answer extraction", "start_pos": 147, "end_pos": 164, "type": "TASK", "confidence": 0.761357843875885}]}, {"text": "Accurate IR is crucial for the success of this approach.", "labels": [], "entities": [{"text": "IR", "start_pos": 9, "end_pos": 11, "type": "TASK", "confidence": 0.9208925366401672}]}, {"text": "Answers in paragraphs that have been missed by IR are lost for the entire QA system.", "labels": [], "entities": [{"text": "IR", "start_pos": 47, "end_pos": 49, "type": "TASK", "confidence": 0.8116499781608582}]}, {"text": "Hence, high performance of IR especially in terms of recall is essential.", "labels": [], "entities": [{"text": "IR", "start_pos": 27, "end_pos": 29, "type": "TASK", "confidence": 0.9865647554397583}, {"text": "recall", "start_pos": 53, "end_pos": 59, "type": "METRIC", "confidence": 0.9990800619125366}]}, {"text": "Furthermore, high precision is desirable as IR scores are used for answer extraction heuristics and also to reduce the chance of subsequent extraction errors.", "labels": [], "entities": [{"text": "precision", "start_pos": 18, "end_pos": 27, "type": "METRIC", "confidence": 0.9992061257362366}, {"text": "IR", "start_pos": 44, "end_pos": 46, "type": "TASK", "confidence": 0.7804357409477234}, {"text": "answer extraction heuristics", "start_pos": 67, "end_pos": 95, "type": "TASK", "confidence": 0.8892616828282675}]}, {"text": "Because the user's formulation of the question is only one of the many possible ways to state the information need that the user might have, there is often a discrepancy between the terminology used by the user and the terminology used in the document collection to describe the same concept.", "labels": [], "entities": []}, {"text": "A document might hold the answer to the user's question, but it will not be found due to the TERMI-NOLOGICAL GAP.", "labels": [], "entities": [{"text": "TERMI-NOLOGICAL GAP", "start_pos": 93, "end_pos": 112, "type": "METRIC", "confidence": 0.8324097394943237}]}, {"text": "show that their system fails to answer many questions (25.7%), because of the terminological gap, i.e. keyword expansion would be desirable but is missing.", "labels": [], "entities": [{"text": "keyword expansion", "start_pos": 103, "end_pos": 120, "type": "TASK", "confidence": 0.6774997860193253}]}, {"text": "Query expansion techniques have been developed to bridge this gap.", "labels": [], "entities": [{"text": "Query expansion", "start_pos": 0, "end_pos": 15, "type": "TASK", "confidence": 0.8056017756462097}]}, {"text": "However, we believe that there is more than just a terminological gap.", "labels": [], "entities": []}, {"text": "There is also a KNOWLEDGE GAP.", "labels": [], "entities": [{"text": "KNOWLEDGE GAP", "start_pos": 16, "end_pos": 29, "type": "TASK", "confidence": 0.556768536567688}]}, {"text": "Documents are missed or do not end up high in the ranks, because additional world knowledge is missing.", "labels": [], "entities": []}, {"text": "We are not speaking of synonyms here, but words belonging to the same subject field.", "labels": [], "entities": []}, {"text": "For example, when a user is looking for information about the explosion of the first atomic bomb, in his/her head a subject field is active that could include: war, disaster, World War II.", "labels": [], "entities": []}, {"text": "We have used three corpus-based methods to acquire semantically related words: the SYNTAX-BASED METHOD, the ALIGNMENT-BASED METHOD, and the PROXIMITY-BASED METHOD.", "labels": [], "entities": []}, {"text": "The nature of the relations between words found by the three methods is very different.", "labels": [], "entities": []}, {"text": "Ranging from free associations to synonyms.", "labels": [], "entities": []}, {"text": "Apart from these resources we have used categorised named entities, such as Van Gogh IS-A painter and synsets from EWN as candidate expansion terms.", "labels": [], "entities": [{"text": "EWN", "start_pos": 115, "end_pos": 118, "type": "DATASET", "confidence": 0.9126559495925903}]}, {"text": "In this paper we have applied several types of lexico-semantic information to the task of query expansion for QA.", "labels": [], "entities": [{"text": "query expansion", "start_pos": 90, "end_pos": 105, "type": "TASK", "confidence": 0.8130739331245422}]}, {"text": "We hope that the synonyms retrieved automatically, and in particular the synonyms retrieved by the alignment-based method, as these are most precise, will help to overcome the terminological gap.", "labels": [], "entities": []}, {"text": "With respect to the knowledge gap, we expect that the proximity-based method would be most helpful as well as the list of categorised named entities.", "labels": [], "entities": []}, {"text": "For example, knowing that Monica Seles is a tennis player helps to find relevant passages regarding this tennis star.", "labels": [], "entities": []}], "datasetContent": [{"text": "For evaluation we used data collected from the CLEF Dutch QA tracks.", "labels": [], "entities": [{"text": "CLEF Dutch QA tracks", "start_pos": 47, "end_pos": 67, "type": "DATASET", "confidence": 0.9483985155820847}]}, {"text": "The CLEF text collection contains 4 years of newspaper text, approximately 80 million words and Dutch Wikipedia, approximately 50 million words.", "labels": [], "entities": [{"text": "CLEF text collection", "start_pos": 4, "end_pos": 24, "type": "DATASET", "confidence": 0.9233171939849854}, {"text": "Dutch Wikipedia", "start_pos": 96, "end_pos": 111, "type": "DATASET", "confidence": 0.852578192949295}]}, {"text": "We used the question sets from the competitions of the Dutch QA track in.", "labels": [], "entities": [{"text": "Dutch QA track", "start_pos": 55, "end_pos": 69, "type": "DATASET", "confidence": 0.9244886438051859}]}, {"text": "Questions in these sets are annotated with valid answers found by the participating teams including IDs of supporting documents in the given text collection.", "labels": [], "entities": []}, {"text": "We expanded these list of valid answers where necessary.", "labels": [], "entities": []}, {"text": "We calculated for each run the Mean Reciprocal Rank (MRR).", "labels": [], "entities": [{"text": "Mean Reciprocal Rank (MRR)", "start_pos": 31, "end_pos": 57, "type": "METRIC", "confidence": 0.961542934179306}]}, {"text": "The MRR measures the percentage of passages for which a correct answer was found in the top-k passages returned by the system.", "labels": [], "entities": [{"text": "MRR", "start_pos": 4, "end_pos": 7, "type": "METRIC", "confidence": 0.9428155422210693}]}, {"text": "The MRR score is the average of 1/R where R is the rank of the first relevant passage computed over the 20 highest ranked passages.", "labels": [], "entities": [{"text": "MRR score", "start_pos": 4, "end_pos": 13, "type": "METRIC", "confidence": 0.919310063123703}]}, {"text": "Passages retrieved were considered relevant when one of the possible answer strings was found in that passage.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: MRR scores for the IR component with  query expansion from several sources", "labels": [], "entities": [{"text": "MRR", "start_pos": 10, "end_pos": 13, "type": "METRIC", "confidence": 0.6764151453971863}, {"text": "IR", "start_pos": 29, "end_pos": 31, "type": "TASK", "confidence": 0.9707743525505066}]}, {"text": " Table 3: Number of questions that receive a higher  (+) or lower (-) RR when using expansions from  several sources", "labels": [], "entities": [{"text": "RR", "start_pos": 70, "end_pos": 72, "type": "METRIC", "confidence": 0.9792107939720154}]}, {"text": " Table 4: CLEF scores of the QA system with query  expansion from several sources", "labels": [], "entities": []}]}