{"title": [{"text": "Revisiting the impact of different annotation schemes on PCFG parsing: A grammatical dependency evaluation", "labels": [], "entities": [{"text": "PCFG parsing", "start_pos": 57, "end_pos": 69, "type": "TASK", "confidence": 0.8337874114513397}]}], "abstractContent": [{"text": "Recent parsing research has started addressing the questions a) how parsers trained on different syntactic resources differ in their performance and b) how to conduct a meaningful evaluation of the parsing results across such a range of syntactic representations.", "labels": [], "entities": [{"text": "parsing", "start_pos": 7, "end_pos": 14, "type": "TASK", "confidence": 0.96639484167099}]}, {"text": "Two Ger-man treebanks, Negra and T\u00fcBa-D/Z, constitute an interesting testing ground for such research given that the two treebanks make very different representational choices for this language , which also is of general interest given that German is situated between the extremes of fixed and free word order.", "labels": [], "entities": []}, {"text": "We show that previous work comparing PCFG parsing with these two treebanks employed PARSEVAL and grammatical function comparisons which were skewed by differences between the two corpus annotation schemes.", "labels": [], "entities": [{"text": "PCFG parsing", "start_pos": 37, "end_pos": 49, "type": "TASK", "confidence": 0.7231280952692032}, {"text": "PARSEVAL", "start_pos": 84, "end_pos": 92, "type": "METRIC", "confidence": 0.9624930024147034}]}, {"text": "Focusing on the grammatical dependency triples as an essential dimension of comparison, we show that the two very distinct corpora result in comparable parsing performance.", "labels": [], "entities": []}], "introductionContent": [{"text": "Syntactically annotated corpora have been produced fora range of languages and they differ significantly regarding which language properties are encoded and how they are represented.", "labels": [], "entities": []}, {"text": "Between the two extremes of constituency treebanks for English and dependency treebanks for free word order languages such as Czech lie languages such as German, for which two different treebanks have explored different options for encoding topology and dependency,) and T\u00fcBa-D/Z ().", "labels": [], "entities": []}, {"text": "Recent research has started addressing the question of how parsers trained on these different syntactic resources differ in their performance.", "labels": [], "entities": []}, {"text": "Such work must also address the question of how to conduct a meaningful evaluation of the parsing results across such a range of syntactic representations.", "labels": [], "entities": []}, {"text": "In this paper, we show that previous work comparing PCFG parsing for the two German treebanks used representations which cannot adequately be compared using the given PARSEVAL measures and that a grammatical dependency evaluation is more meaningful than the grammatical function evaluation provided.", "labels": [], "entities": [{"text": "PCFG parsing", "start_pos": 52, "end_pos": 64, "type": "TASK", "confidence": 0.7066892385482788}, {"text": "German treebanks", "start_pos": 77, "end_pos": 93, "type": "DATASET", "confidence": 0.7334119528532028}]}, {"text": "We present the first comparison of Negra and T\u00fcBa-D/Z using a labeled dependency evaluation based on the grammatical function labels provided in the corpora.", "labels": [], "entities": []}, {"text": "We show that, in contrast to previous literature, a labeled dependency evaluation establishes that PCFG parsers trained on the two corpora give similar parsing performance.", "labels": [], "entities": []}, {"text": "The focus on labeled dependencies also provides a direct link to recent work on dependency-based evaluation (e.g., and dependency parsing (e.g., CoNLL shared tasks 2006, 2007).", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 119, "end_pos": 137, "type": "TASK", "confidence": 0.8219985961914062}, {"text": "CoNLL shared tasks 2006, 2007", "start_pos": 145, "end_pos": 174, "type": "DATASET", "confidence": 0.7015432417392731}]}], "datasetContent": [{"text": "The goal of the following experiments is a comparison of parsing performance across different types of evaluation metrics for parsers trained on Negra (Ver. 2) and T\u00fcBa-D/Z (Ver. 2).", "labels": [], "entities": [{"text": "parsing", "start_pos": 57, "end_pos": 64, "type": "TASK", "confidence": 0.981019914150238}, {"text": "Negra", "start_pos": 145, "end_pos": 150, "type": "DATASET", "confidence": 0.9441611170768738}]}, {"text": "We trained unlexicalized PCFG parsing models using LoPar).", "labels": [], "entities": [{"text": "PCFG parsing", "start_pos": 25, "end_pos": 37, "type": "TASK", "confidence": 0.6546305119991302}]}, {"text": "Unlexicalized models Genitive objects are modified for the sake of consistency among arguments even though there are too few genitive objects to provide reliable results in the evaluation.", "labels": [], "entities": []}, {"text": "The addition of edge labels to terminal POS labels results in 337 lexical tags for Negra and 91 for T\u00fcBa-D/Z. were used to minimize the impact of other corpus differences on parsing.", "labels": [], "entities": [{"text": "Negra", "start_pos": 83, "end_pos": 88, "type": "DATASET", "confidence": 0.8966307044029236}]}, {"text": "A ten-fold cross validation was performed for all experiments.", "labels": [], "entities": []}, {"text": "As a reference point for comparison with previous work, the PARSEVAL results 8 are given in  The parser trained on T\u00fcBa-D/Z performs much better than the one trained on Negra on all labeled and unlabeled bracketing scores.", "labels": [], "entities": [{"text": "PARSEVAL", "start_pos": 60, "end_pos": 68, "type": "METRIC", "confidence": 0.9750607013702393}, {"text": "Negra", "start_pos": 169, "end_pos": 174, "type": "DATASET", "confidence": 0.8942253589630127}]}, {"text": "As we saw in section 2, Negra and T\u00fcBa-D/Z use very different syntactic annotation schemes, resulting in over 2.5 times as many non-terminals per sentence in T\u00fcBa-D/Z as in Negra with the additional unary nodes.", "labels": [], "entities": []}, {"text": "As mentioned previously, showed that PARSEVAL is affected by the ratio of terminal to non-terminal nodes, so these results are not expected to indicate the quality of the parses.", "labels": [], "entities": [{"text": "PARSEVAL", "start_pos": 37, "end_pos": 45, "type": "METRIC", "confidence": 0.9197116494178772}]}, {"text": "The comparison with grammatical function and dependency evaluations we turn to next showcases that PARSEVAL does not provide a meaningful evaluation metric across annotation schemes.", "labels": [], "entities": []}, {"text": "Complementing the issue of the ratio of terminals to non-terminals raised in the last section, one can question whether counting all brackets in the sentence equally, as done by the PARSEVAL metric, provides a good measure of how accurately the basic functor-argument structure of the sentence has been captured in a parse.", "labels": [], "entities": [{"text": "PARSEVAL", "start_pos": 182, "end_pos": 190, "type": "METRIC", "confidence": 0.8333390355110168}]}, {"text": "Thus, it is useful to per-7 Our experimental setup is designed to support a comparison between Negra and T\u00fcBa-D/Z for the three evaluation metrics and is intended to be comparable to the setup of.", "labels": [], "entities": []}, {"text": "For Negra, Dubey (2004) explores a range of parsing models and the corpus preparation he uses differs from the one discussed in this paper so that a discussion of his results is beyond the scope of the corpus comparison in this paper.", "labels": [], "entities": []}, {"text": "8 Scores were calculated using evalb.", "labels": [], "entities": []}, {"text": "form an evaluation based on the grammatical function labels that are important for determining the functor-argument structure of the sentence: subjects, accusative objects, and dative objects.", "labels": [], "entities": []}, {"text": "The first step in an evaluation of functor-argument structure is to identify whether an argument bears the correct grammatical function label.", "labels": [], "entities": []}, {"text": "present the results shown in for the parsing performance of the unlexicalized model of the Stanford Parser ().", "labels": [], "entities": [{"text": "parsing", "start_pos": 37, "end_pos": 44, "type": "TASK", "confidence": 0.9759081602096558}, {"text": "Stanford Parser", "start_pos": 91, "end_pos": 106, "type": "DATASET", "confidence": 0.8795110881328583}]}, {"text": "In this grammatical function label evaluation, T\u00fcBa-D/Z outperforms Negra for subjects, accusative objects, and dative objects based on an evaluation of phrasal arguments.", "labels": [], "entities": []}, {"text": "Note that this grammatical function label evaluation is restricted to labels on phrases; grammatical function labels on words are ignored in training and testing.", "labels": [], "entities": [{"text": "grammatical function label evaluation", "start_pos": 15, "end_pos": 52, "type": "TASK", "confidence": 0.6360194161534309}]}, {"text": "This results in an unbalanced comparison between Negra and T\u00fcBa-D/Z since, as discussed in section 2, T\u00fcBa-D/Z includes unary-branching phrases above all single-word arguments whereas Negra does not.", "labels": [], "entities": []}, {"text": "In effect, single-word arguments in Negra -mainly pronouns and bare nouns -are not considered in the evaluation from.", "labels": [], "entities": []}, {"text": "The result is thus a comparison of multiword arguments in Negra to both single-and multiword arguments in T\u00fcBa-D/Z.", "labels": [], "entities": [{"text": "Negra", "start_pos": 58, "end_pos": 63, "type": "DATASET", "confidence": 0.9284045696258545}, {"text": "T\u00fcBa-D/Z", "start_pos": 106, "end_pos": 114, "type": "DATASET", "confidence": 0.8645756443341573}]}, {"text": "Recall from section 3.1 that this is not a minor difference: single-word arguments account for 38% of subjects, accusative objects, and dative objects in Negra.", "labels": [], "entities": []}, {"text": "While determining the grammatical function of an element is an important part of determining the functor-argument structure of a sentence, the other necessary component is determining the head of each function.", "labels": [], "entities": []}, {"text": "To evaluate whether both the functor and the argument have been correctly found, an evaluation of labeled dependency triples is needed.", "labels": [], "entities": []}, {"text": "As in the previous section, we focus on the grammatical function labels for arguments of verbs.", "labels": [], "entities": []}, {"text": "To complete a labeled dependency triple for each argument, we additionally need to locate the lexical verbal head.", "labels": [], "entities": []}, {"text": "In Negra, the head is the sister of an argument marked with the function label \"HD\", however heads are only marked fora subset of the phrase categories: S, VP, AP, and AVP.", "labels": [], "entities": []}, {"text": "This subset includes the phrase categories that contain verbs and their arguments, Sand VP.", "labels": [], "entities": []}, {"text": "In our experiment, the parser finds the HD grammatical function labels with a very high f-score: 99.5% precision and 96.5% recall.", "labels": [], "entities": [{"text": "f-score", "start_pos": 88, "end_pos": 95, "type": "METRIC", "confidence": 0.9818347096443176}, {"text": "precision", "start_pos": 103, "end_pos": 112, "type": "METRIC", "confidence": 0.9962964653968811}, {"text": "recall", "start_pos": 123, "end_pos": 129, "type": "METRIC", "confidence": 0.9966742992401123}]}, {"text": "If the sister with the label HD is a word, then that word is the lexical head for the purposes of this dependency evaluation.", "labels": [], "entities": []}, {"text": "If the sister with the label HD is a phrase, then a recursive search for heads within that phrase finds a lexical head.", "labels": [], "entities": []}, {"text": "In 3.2% of cases in the gold standard, it is not possible to find a lexical head for an argument.", "labels": [], "entities": []}, {"text": "Further methods could be applied to find the remaining heads heuristically, but we avoid the additional parameters this introduces for this evaluation by ignoring these cases.", "labels": [], "entities": []}, {"text": "For T\u00fcBa-D/Z, finding the head is not as simple because the verbal head and its arguments are in different topological fields.", "labels": [], "entities": []}, {"text": "To create a parallel comparison to Negra, the finite verb from the local clause is chosen as the head for all subjects.", "labels": [], "entities": []}, {"text": "The (finite or non-finite) main full verb is designated as the head for the accusative and dative objects.", "labels": [], "entities": []}, {"text": "It is possible to automatically find an appropriate head verb for all but 2.7% of subjects, accusative objects, and dative objects.", "labels": [], "entities": []}, {"text": "As with Negra, only cases where ahead verb can be found in the gold standard are considered in the evaluation.", "labels": [], "entities": []}, {"text": "As in the grammatical function evaluation in the previous section, only the grammatical function label, not the phrase category is considered in the evaluation.", "labels": [], "entities": []}, {"text": "The results for the labeled dependency evaluation are shown in.", "labels": [], "entities": []}, {"text": "The parser trained on Negra outperforms the one trained on T\u00fcBa-D/Z for all types of arguments.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1 provides statistics  on average sentence length and the number of non- terminals per sentence.", "labels": [], "entities": []}, {"text": " Table 3: Grammatical Function Label Evaluation for  Phrasal Arguments from K\u00fcbler et al. (2006)", "labels": [], "entities": [{"text": "Grammatical Function Label Evaluation", "start_pos": 10, "end_pos": 47, "type": "TASK", "confidence": 0.5689738467335701}, {"text": "Phrasal Arguments", "start_pos": 53, "end_pos": 70, "type": "TASK", "confidence": 0.6826367825269699}]}, {"text": " Table 4: Grammatical Function Label Evaluation", "labels": [], "entities": [{"text": "Grammatical Function Label Evaluation", "start_pos": 10, "end_pos": 47, "type": "TASK", "confidence": 0.8236391097307205}]}, {"text": " Table 5: Labeled Dependency Evaluation", "labels": [], "entities": [{"text": "Labeled Dependency Evaluation", "start_pos": 10, "end_pos": 39, "type": "TASK", "confidence": 0.6324238379796346}]}]}