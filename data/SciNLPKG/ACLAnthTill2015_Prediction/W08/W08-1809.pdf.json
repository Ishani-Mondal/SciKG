{"title": [{"text": "Topic Indexing and Retrieval for Factoid QA", "labels": [], "entities": [{"text": "Topic Indexing", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.6880732476711273}, {"text": "Factoid QA", "start_pos": 33, "end_pos": 43, "type": "TASK", "confidence": 0.46747642755508423}]}], "abstractContent": [{"text": "The method of Topic Indexing and Retrieval for QA persented in this paper enables fast and efficent QA for questions with named entity answers.", "labels": [], "entities": [{"text": "Topic Indexing", "start_pos": 14, "end_pos": 28, "type": "TASK", "confidence": 0.800660252571106}]}, {"text": "This is achieved by identifying all possible named entity answers in a corpus off-line and gathering all possible evidence for their direct retrieval as answer candidates using standard IR techniques.", "labels": [], "entities": []}, {"text": "An evaluation of this method on 377 TREC questions produced a score of 0.342 in Accuracy and 0.413 in Mean Reciprocal Rank (MRR).", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 80, "end_pos": 88, "type": "METRIC", "confidence": 0.9996024966239929}, {"text": "Mean Reciprocal Rank (MRR)", "start_pos": 102, "end_pos": 128, "type": "METRIC", "confidence": 0.9629125595092773}]}], "introductionContent": [{"text": "Many textual QA systems use Information Retrieval to retrieve a subset of the documents/passages from the source corpus in order to reduce the amount of text that needs to be investigated in finding the correct answers.", "labels": [], "entities": [{"text": "textual QA", "start_pos": 5, "end_pos": 15, "type": "TASK", "confidence": 0.5694020986557007}, {"text": "Information Retrieval", "start_pos": 28, "end_pos": 49, "type": "TASK", "confidence": 0.6694297641515732}]}, {"text": "This use of Information Retrieval (IR) plays an important role, since it imposes an upper bound on the performance of the entire QA system: Subsequent answer extraction operations cannot makeup for the failure of IR to fetch text that contains correct answers.", "labels": [], "entities": [{"text": "Information Retrieval (IR)", "start_pos": 12, "end_pos": 38, "type": "TASK", "confidence": 0.8391684055328369}, {"text": "answer extraction", "start_pos": 151, "end_pos": 168, "type": "TASK", "confidence": 0.7146109193563461}]}, {"text": "Several techniques have been developed to cut down the amount of text that must be retrieved in order to ensure against the loss of answer material, but processing any text for downstream operations still takes up valuable on-line time.", "labels": [], "entities": []}, {"text": "In this paper, we present a method, Topic Indexing and Retrieval for QA, that turns factoid Question Answering into fine-grained Information Retrieval, where answer candidates are directly re- trieved instead of documents/passages.", "labels": [], "entities": [{"text": "Topic Indexing", "start_pos": 36, "end_pos": 50, "type": "TASK", "confidence": 0.7453908622264862}, {"text": "factoid Question Answering", "start_pos": 84, "end_pos": 110, "type": "TASK", "confidence": 0.6157955924669901}, {"text": "Information Retrieval", "start_pos": 129, "end_pos": 150, "type": "TASK", "confidence": 0.7506937086582184}]}, {"text": "The primary claim here is that for simple named entity answers, this can make for fast and accurate retrieval.", "labels": [], "entities": []}], "datasetContent": [{"text": "Evaluation has been carried out to determine whether Topic Indexing and Retrieval using a simple and efficient IR technique for direct answer retrieval can indeed make for an accurate QA system.", "labels": [], "entities": [{"text": "Topic Indexing", "start_pos": 53, "end_pos": 67, "type": "TASK", "confidence": 0.8123222291469574}, {"text": "answer retrieval", "start_pos": 135, "end_pos": 151, "type": "TASK", "confidence": 0.6909377872943878}]}, {"text": "This has also iluminated those features of the method that contribute to QA performance.", "labels": [], "entities": [{"text": "QA", "start_pos": 73, "end_pos": 75, "type": "TASK", "confidence": 0.9001865386962891}]}, {"text": "The questions and the corpus (AQUAINT) used for the evaluation are taken from the TREC QA track.", "labels": [], "entities": [{"text": "AQUAINT", "start_pos": 30, "end_pos": 37, "type": "METRIC", "confidence": 0.9867002964019775}, {"text": "TREC QA track", "start_pos": 82, "end_pos": 95, "type": "DATASET", "confidence": 0.7273693482081095}]}, {"text": "377 questions that have single proper names as answers (ie, excluding list questions, \"other\" questions and questions without answers) were selected from the TREC 2003/2004/2005 questions.", "labels": [], "entities": [{"text": "TREC 2003/2004/2005 questions", "start_pos": 158, "end_pos": 187, "type": "DATASET", "confidence": 0.9083883762359619}]}, {"text": "Questions from TREC 2004 and TREC 2005 are grouped around what are called targets.", "labels": [], "entities": [{"text": "TREC 2004 and TREC 2005", "start_pos": 15, "end_pos": 38, "type": "DATASET", "confidence": 0.7377194881439209}]}, {"text": "A target is basically the question topic, e.g. \"When was he born?\" where \"he\" refers to the target, e.g. \"Fred Durst\".", "labels": [], "entities": []}, {"text": "One of the experimental setups takes account of these targets by employing the Bitopic method discussed in Section 5.", "labels": [], "entities": []}, {"text": "This retrieval strategy is also applied to questions from TREC 2003 (that come with no targets), by identifying the question topic in a question and extracting it as a target automatically, in order to see whether it can benefit the QA performance even when the target is not provided manually.", "labels": [], "entities": [{"text": "TREC 2003", "start_pos": 58, "end_pos": 67, "type": "DATASET", "confidence": 0.9168701469898224}]}, {"text": "The actual evaluation of the method consists of three experiments, each of which tests a different setting.", "labels": [], "entities": []}, {"text": "The common elements for all three are the core answer retrieval system.", "labels": [], "entities": [{"text": "answer retrieval", "start_pos": 47, "end_pos": 63, "type": "TASK", "confidence": 0.9131378531455994}]}, {"text": "The aspects that differentiate the three settings are: (1) whether or not a fine-grained answer type is used for reranking, (2) whether single topic documents or bi-topic documents are retrieved.", "labels": [], "entities": []}, {"text": "The common core system that implements the answer retrieval method comprises (1) a question analysis module that analyses the question and produces the question type, answer type, the question topics and the shallow parse of the question text and (2) a retrieval module that generates the structured query, selects the appropriate index and retrieves the top 100 topics as answer candidates.", "labels": [], "entities": [{"text": "answer retrieval", "start_pos": 43, "end_pos": 59, "type": "TASK", "confidence": 0.7690016031265259}]}, {"text": "This core system performs the basic retrieval operations, to which we add further operations such as answer-type based reranking and target specific retrieval.", "labels": [], "entities": []}, {"text": "The addition of some of these features distinguish different setups for the evaluation.", "labels": [], "entities": []}, {"text": "Setup A involves just the core system on single topic document indexing of the AQUAINT corpus, as described in Section 3.2.", "labels": [], "entities": [{"text": "single topic document indexing", "start_pos": 41, "end_pos": 71, "type": "TASK", "confidence": 0.654034897685051}, {"text": "AQUAINT corpus", "start_pos": 79, "end_pos": 93, "type": "DATASET", "confidence": 0.9418068528175354}]}, {"text": "The resulting topic documents are divided into the three base types Setup B is basically the same as setup A, except for the addition of fine-grained answer type re-ranking on the one hundred topics retrieved as answer candidates.", "labels": [], "entities": []}, {"text": "That is, elements of this list are re-ranked depending on whether their finegrained answer type matches the fine-grained answer type identified from the question.", "labels": [], "entities": []}, {"text": "Note here that only the coarse answer type (PERSON, LO-CATION, ORGANISATION, TOTAL) was used for retrieval, as opposed to the fine-grained type such as PRESIDENT or COMPANY, due to the: Results for all setups for all questions fact that separate indices exist only for these coarse types.", "labels": [], "entities": [{"text": "PERSON", "start_pos": 44, "end_pos": 50, "type": "METRIC", "confidence": 0.963832676410675}, {"text": "LO-CATION", "start_pos": 52, "end_pos": 61, "type": "METRIC", "confidence": 0.9160041213035583}, {"text": "ORGANISATION", "start_pos": 63, "end_pos": 75, "type": "METRIC", "confidence": 0.9864715933799744}, {"text": "TOTAL", "start_pos": 77, "end_pos": 82, "type": "METRIC", "confidence": 0.9118108749389648}]}, {"text": "The identification of the fine type of a candidate topic is done by looking up this information in the topic-type hash table as mentioned in Section 3.", "labels": [], "entities": []}, {"text": "Again the resulting top candidate is picked as the definite answer.", "labels": [], "entities": []}, {"text": "The final setup is setup C.", "labels": [], "entities": []}, {"text": "Setup C exploits question topics (targets), as described in Section 5.", "labels": [], "entities": []}, {"text": "Targets are explicitly provided in TREC 2004 and TREC 2005 question set.", "labels": [], "entities": [{"text": "TREC 2004", "start_pos": 35, "end_pos": 44, "type": "DATASET", "confidence": 0.855454295873642}, {"text": "TREC 2005 question set", "start_pos": 49, "end_pos": 71, "type": "DATASET", "confidence": 0.9302370846271515}]}, {"text": "For the TREC 2003, the questions, which do not come with explicit targets, the system automatically extracts a target from the question using a very simple rule: any proper name in the question is regarded as a target.", "labels": [], "entities": [{"text": "TREC 2003", "start_pos": 8, "end_pos": 17, "type": "DATASET", "confidence": 0.8227493762969971}]}, {"text": "The point of this setup is to test the effectiveness of the bi-topic method discussed in Section 5.", "labels": [], "entities": []}, {"text": "The core retrieval procedure is the same as in setup B, except that the index on which the retrieval is performed is selected based on the question topic.", "labels": [], "entities": []}, {"text": "In Section 5, we mentioned that a set of indices were built with respect to 'anchor topics'.", "labels": [], "entities": []}, {"text": "So the question topic identified from the question (or provided as default) acts as the anchor topic and the index that corresponds to this anchor topic gets chosen.", "labels": [], "entities": []}, {"text": "The rest of the process is the same as setup B, and retrieved topics are re-ranked according to the finegrained answer type.: Overlap between B and C two scores separated by a colon, representing the ratio of correctly answered questions overall questions and the number of correctly answered questions.", "labels": [], "entities": []}, {"text": "The last two rows summarise the results by giving the accuracy (ACC), which is equivalent to the correctness rate at A@1 and the Mean Reciprocal Rank score (MRR).", "labels": [], "entities": [{"text": "accuracy (ACC)", "start_pos": 54, "end_pos": 68, "type": "METRIC", "confidence": 0.9225494861602783}, {"text": "correctness rate", "start_pos": 97, "end_pos": 113, "type": "METRIC", "confidence": 0.9789780974388123}, {"text": "A", "start_pos": 117, "end_pos": 118, "type": "METRIC", "confidence": 0.9042885303497314}, {"text": "Mean Reciprocal Rank score (MRR)", "start_pos": 129, "end_pos": 161, "type": "METRIC", "confidence": 0.9587936656815665}]}, {"text": "From this table, it can be seen that both setup B and setup C produced results that are superior to setup A in all measures: accuracy, A@N (for N up to 20) and MRR.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 125, "end_pos": 133, "type": "METRIC", "confidence": 0.9996626377105713}, {"text": "A", "start_pos": 135, "end_pos": 136, "type": "METRIC", "confidence": 0.9690161347389221}, {"text": "MRR", "start_pos": 160, "end_pos": 163, "type": "METRIC", "confidence": 0.9971781969070435}]}], "tableCaptions": [{"text": " Table 1: Number of Topic Docs per Types", "labels": [], "entities": []}, {"text": " Table 2: Results for all setups for all questions", "labels": [], "entities": []}, {"text": " Table 3: Overlap between B and C", "labels": [], "entities": [{"text": "Overlap", "start_pos": 10, "end_pos": 17, "type": "METRIC", "confidence": 0.9512521028518677}]}]}