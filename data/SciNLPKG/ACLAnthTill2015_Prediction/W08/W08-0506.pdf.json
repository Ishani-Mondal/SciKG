{"title": [{"text": "Software testing and the naturally occurring data assumption in natural language processing *", "labels": [], "entities": []}], "abstractContent": [{"text": "It is a widely accepted belief in natural language processing research that naturally occurring data is the best (and perhaps the only appropriate) data for testing text mining systems.", "labels": [], "entities": [{"text": "testing text mining", "start_pos": 157, "end_pos": 176, "type": "TASK", "confidence": 0.6390249133110046}]}, {"text": "This paper compares code coverage using a suite of functional tests and using a large corpus and finds that higher class, line, and branch coverage is achieved with structured tests than with even a very large corpus.", "labels": [], "entities": []}], "introductionContent": [{"text": "In 2006, Geoffrey Chang was a star of the protein crystallography world.", "labels": [], "entities": [{"text": "protein crystallography", "start_pos": 42, "end_pos": 65, "type": "TASK", "confidence": 0.7148591727018356}]}, {"text": "That year, a crucial component of his code was discovered to have a simple error with large consequences for his research.", "labels": [], "entities": []}, {"text": "The nature of the bug was to change the signs (positive versus negative) of two columns of the output.", "labels": [], "entities": []}, {"text": "The effect of this was to reverse the predicted \"handedness\" of the structure of the molecule-an important feature in predicting its interactions with other molecules.", "labels": [], "entities": []}, {"text": "The protein for his work on which Chang was best known is an important one in predicting things like human response to anticancer drugs and the likelihood of bacteria developing antibiotic resistance, so his work was quite influential and heavily cited.", "labels": [], "entities": []}, {"text": "The consequences for Chang were the withdrawal of 5 papers in some of the most prestigious journals in the world.", "labels": [], "entities": []}, {"text": "The consequences for the rest of the scientific community have not been quantified, but were substantial: prior to the retractions, publishing papers with results that did not jibe with his model's predictions was difficult, and obtaining grants based on preliminary results that seemed to contradict his published results was difficult as well.", "labels": [], "entities": []}, {"text": "The Chang story (for a succinct discussion, see, and see () for the retractions) is an object illustration of the truth of Rob Knight's observation that \"For scientific work, bugs don't just mean unhappy users who you'll never actually meet: they mean retracted publications and ended careers.", "labels": [], "entities": []}, {"text": "It is critical that your code be fully tested before you draw conclusions from results it produces\" (personal communication).", "labels": [], "entities": []}, {"text": "Nonetheless, the subject of software testing has been largely neglected in academic natural language processing.", "labels": [], "entities": [{"text": "academic natural language processing", "start_pos": 75, "end_pos": 111, "type": "TASK", "confidence": 0.6656569540500641}]}, {"text": "This paper addresses one aspect of software testing: the monitoring of testing efforts via code coverage.", "labels": [], "entities": []}], "datasetContent": [{"text": "In the most basic experiment, we contrasted class, line, and branch coverage when running the developer-constructed test suite and when running the corpus and the corpus-based rules.", "labels": [], "entities": []}, {"text": "As the first two lines of show, for the entire application (parser, rule-handling, and configuration), line coverage was higher with the test suite-56% versus 41%-and branch coverage was higher as well-41% versus 28% (see the first two lines of).", "labels": [], "entities": [{"text": "line coverage", "start_pos": 103, "end_pos": 116, "type": "METRIC", "confidence": 0.6470115780830383}]}, {"text": "We give here a more detailed discussion of the results for the entire code base.", "labels": [], "entities": []}, {"text": "(Detailed discussions for the parser and rule packages, including granular assessments of class coverage, follow.)", "labels": [], "entities": []}, {"text": "For the parser package: \u2022 Class coverage was higher with the test suite than with the corpus-88% (22/25) versus 80% (20/25).", "labels": [], "entities": [{"text": "coverage", "start_pos": 32, "end_pos": 40, "type": "METRIC", "confidence": 0.6090100407600403}]}, {"text": "\u2022 For the entire parser package, line coverage was higher with the test suite than with the corpus-55% versus 41%.", "labels": [], "entities": [{"text": "coverage", "start_pos": 38, "end_pos": 46, "type": "METRIC", "confidence": 0.5159804224967957}]}, {"text": "\u2022 For the entire parser package, branch coverage was higher with the test suite than with the corpus-57% versus 29%.", "labels": [], "entities": [{"text": "coverage", "start_pos": 40, "end_pos": 48, "type": "METRIC", "confidence": 0.6554608941078186}]}, {"text": "gives class-level data for the two main packages.", "labels": [], "entities": []}, {"text": "For the parser package: \u2022 Within the 25 individual classes of the parser package, line coverage was equal or greater with the test suite for 21/25 classes; it was not just equal but greater for 14/25 classes.", "labels": [], "entities": [{"text": "line coverage", "start_pos": 82, "end_pos": 95, "type": "METRIC", "confidence": 0.8143569231033325}]}, {"text": "\u2022 Within those 21 of the 25 individual classes that had branching logic, branch coverage was equal or greater with the test suite for 19/21 classes, and not just equal but greater for 18/21 classes.", "labels": [], "entities": [{"text": "coverage", "start_pos": 80, "end_pos": 88, "type": "METRIC", "confidence": 0.5302813649177551}]}, {"text": "For the rule-handling package: \u2022 Class coverage was higher with the test suite than with the corpus-100% (20/20) versus 90% (18/20).", "labels": [], "entities": [{"text": "coverage", "start_pos": 39, "end_pos": 47, "type": "METRIC", "confidence": 0.5915149450302124}]}, {"text": "\u2022 For the entire rules package, line coverage was higher with the test suite than with the corpus-63% versus 42%.", "labels": [], "entities": [{"text": "coverage", "start_pos": 37, "end_pos": 45, "type": "METRIC", "confidence": 0.5638809204101562}]}, {"text": "\u2022 For the entire rules package, branch coverage was higher with the test suite than with the corpus-71% versus 24%.", "labels": [], "entities": [{"text": "coverage", "start_pos": 39, "end_pos": 47, "type": "METRIC", "confidence": 0.728881299495697}]}, {"text": "gives the class-level data for the rules package: \u2022 Within the 20 individual classes of the rules package, line coverage was equal or greater with the test suite for 19/20 classes, and not just equal but greater for 6/20 classes.", "labels": [], "entities": [{"text": "line coverage", "start_pos": 107, "end_pos": 120, "type": "METRIC", "confidence": 0.866550087928772}]}, {"text": "\u2022 Within those 11 of the 20 individual classes that had branching logic, branch coverage was equal or greater with the test suite for all 11/11 classes, and not just equal but greater for (again) all 11/11 classes.", "labels": [], "entities": [{"text": "coverage", "start_pos": 80, "end_pos": 88, "type": "METRIC", "confidence": 0.5353232622146606}]}, {"text": "Pilot studies suggested (as later experiments verified) that the size of the input corpus had a negligible effect on coverage.", "labels": [], "entities": [{"text": "coverage", "start_pos": 117, "end_pos": 125, "type": "METRIC", "confidence": 0.9041253328323364}]}, {"text": "This suggested that it would be worthwhile to assess the effect of the rule set on coverage independently.", "labels": [], "entities": [{"text": "coverage", "start_pos": 83, "end_pos": 91, "type": "METRIC", "confidence": 0.6883933544158936}]}, {"text": "We used simple ablation (deletion of portions of the rule set) to vary the size of the rule set.", "labels": [], "entities": []}, {"text": "We created two versions of the original rule set.", "labels": [], "entities": []}, {"text": "We focussed only on the non-lexical, relational pattern rules, since they are completely dependent on the lexical rules.", "labels": [], "entities": []}, {"text": "Each version was about half the   size of the original set.", "labels": [], "entities": []}, {"text": "The first consisted of the first half of the rule set, which happened to consist primarily of verb-based patterns.", "labels": [], "entities": []}, {"text": "The second consisted of the second half of the rule set, which corresponded roughly to the nominalization rules.", "labels": [], "entities": []}, {"text": "The last two columns of show the package-level results.", "labels": [], "entities": []}, {"text": "Overall, on a per-package basis, there were no differences inline or branch coverage when the data was run against the full rule set or either half of the rule set.", "labels": [], "entities": []}, {"text": "(The identity of the last three columns is due to this lack of difference in results between the full rule set and the two reduced rule sets.)", "labels": [], "entities": []}, {"text": "On a per-class level, we did note minor differences, but as shows, they were within rounding error on the package level.", "labels": [], "entities": []}, {"text": "In the third experiment, we looked at how coverage varies as increasingly larger amounts of the corpus are processed.", "labels": [], "entities": []}, {"text": "This methodology is comparable to examining the closure properties of a corpus in a corpus linguistics study (see e.g. Chapter 6 of) (and as such maybe sensitive to the extent to which the contents of the corpus door do not fit the sublanguage model).", "labels": [], "entities": []}, {"text": "We counted cumulative line coverage as increasingly large amounts of the corpus were processed, ranging from 0 to 100% of its contents.", "labels": [], "entities": [{"text": "cumulative line coverage", "start_pos": 11, "end_pos": 35, "type": "METRIC", "confidence": 0.5637652973333994}]}, {"text": "The results for line coverage are shown in.", "labels": [], "entities": [{"text": "line coverage", "start_pos": 16, "end_pos": 29, "type": "TASK", "confidence": 0.7738969922065735}]}, {"text": "(The results for branch coverage are quite similar, and the graph is not shown.)", "labels": [], "entities": [{"text": "branch coverage", "start_pos": 17, "end_pos": 32, "type": "TASK", "confidence": 0.7949345707893372}]}, {"text": "Line coverage for the entire application is indicated by the thick solid line.", "labels": [], "entities": []}, {"text": "Line coverage for the parser package is indicated by the thin solid line.", "labels": [], "entities": []}, {"text": "Line coverage for the rules package is indicated by the light gray solid line.", "labels": [], "entities": []}, {"text": "The broken line indicates the number of pattern matches-quantities should be read off of the right y axis.", "labels": [], "entities": []}, {"text": "The figure shows quite graphically the lack of effect on coverage of increasing the size of the corpus.", "labels": [], "entities": [{"text": "coverage", "start_pos": 57, "end_pos": 65, "type": "METRIC", "confidence": 0.9778212308883667}]}, {"text": "For the entire application, the line coverage is 27% when an empty document has been read in, and 39% when a single sentence has been processed; it increases by one to 40% when 51 sentences have been processed, and has grown as high as it ever will-41%-by the time 1,000 sentences have been processed.", "labels": [], "entities": [{"text": "line coverage", "start_pos": 32, "end_pos": 45, "type": "METRIC", "confidence": 0.8164784610271454}]}, {"text": "Coverage at 191,478 sentences-that is, 3,947,200 words-is no higher than at 1,000 sentences, and barely higher, percentagewise, than at a single sentence.", "labels": [], "entities": [{"text": "Coverage", "start_pos": 0, "end_pos": 8, "type": "METRIC", "confidence": 0.9019204378128052}]}, {"text": "An especially notable pattern is that the huge rise in the number of matches to the rules (graphed by the broken line) between 5,000 sentences and 191K sentences has absolutely no effect on code coverage.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Distribution of functional tests.", "labels": [], "entities": []}, {"text": " Table 3: Application and package-level coverage statistics using the developer's functional tests, the full corpus with  the full set of rules, and the full corpus with two reduced sets of rules. The highest value in a row is bolded. The final  three columns are intentionally identical (see explanation in text).", "labels": [], "entities": []}, {"text": " Table 4: When individual classes were examined, both line and branch coverage were always higher with the functional  tests than with the corpus. This table shows the magnitude of the differences. >= indicates the number of classes that  had equal or greater coverage with the functional tests than with the corpus, and > indicates just the classes that had  greater coverage with the functional tests than with the corpus.", "labels": [], "entities": []}]}