{"title": [{"text": "Context-based Arabic Morphological Analysis for Machine Translation", "labels": [], "entities": [{"text": "Context-based Arabic Morphological Analysis", "start_pos": 0, "end_pos": 43, "type": "TASK", "confidence": 0.6709710210561752}, {"text": "Machine Translation", "start_pos": 48, "end_pos": 67, "type": "TASK", "confidence": 0.7440384924411774}]}], "abstractContent": [{"text": "In this paper, we present a novel morphology preprocessing technique for Arabic-English translation.", "labels": [], "entities": []}, {"text": "We exploit the Arabic morphology-English alignment to learn a model removing nonaligned Arabic morphemes.", "labels": [], "entities": []}, {"text": "The model is an instance of the Conditional Random Field (Lafferty et al., 2001) model; it deletes a morpheme based on the morpheme's context.", "labels": [], "entities": []}, {"text": "We achieved around two BLEU points improvement over the original Arabic translation for both a travel-domain system trained on 20K sentence pairs and a news domain system trained on 177K sentence pairs, and showed a potential improvement fora large-scale SMT system trained on 5 million sentence pairs.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 23, "end_pos": 27, "type": "METRIC", "confidence": 0.9994150400161743}, {"text": "SMT", "start_pos": 255, "end_pos": 258, "type": "TASK", "confidence": 0.9877548813819885}]}], "introductionContent": [{"text": "Statistical machine translation (SMT) relies heavily on the word alignment model of the source and the target language.", "labels": [], "entities": [{"text": "Statistical machine translation (SMT)", "start_pos": 0, "end_pos": 37, "type": "TASK", "confidence": 0.8292425473531088}, {"text": "word alignment", "start_pos": 60, "end_pos": 74, "type": "TASK", "confidence": 0.707110196352005}]}, {"text": "However, there is a mismatch between a rich morphology language (e.g Arabic, Czech) and a poor morphology language (e.g English).", "labels": [], "entities": []}, {"text": "An Arabic source word often corresponds to several English words.", "labels": [], "entities": []}, {"text": "Previous research has focused on attempting to apply morphological analysis to machine translation in order to reduce unknown words of highly inflected languages.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 79, "end_pos": 98, "type": "TASK", "confidence": 0.780222624540329}]}, {"text": "represented a word as a vector of morphemes and gained improvement over word-based system for German-English translation.", "labels": [], "entities": [{"text": "German-English translation", "start_pos": 94, "end_pos": 120, "type": "TASK", "confidence": 0.7016595602035522}]}, {"text": "improved Czech-English translation by applying different heuristics to increase the equivalence of Czech and English text.", "labels": [], "entities": [{"text": "Czech-English translation", "start_pos": 9, "end_pos": 34, "type": "TASK", "confidence": 0.5778665691614151}]}, {"text": "Specially for Arabic-English translation, used the Arabic part of speech and English parts of speech (POS) alignment probabilities to retain an Arabic affix, drop it from the corpus or merge it back to a stem.", "labels": [], "entities": [{"text": "Arabic-English translation", "start_pos": 14, "end_pos": 40, "type": "TASK", "confidence": 0.6873020827770233}]}, {"text": "The resulting system outperformed the original Arabic system trained on 3.3 million sentence pairs corpora when using monotone decoding.", "labels": [], "entities": []}, {"text": "However, an improvement in monotone decoding is no guarantee for an improvement over the best baseline achievable with full word forms.", "labels": [], "entities": []}, {"text": "Our experiments showed that an SMT phrase-based translation using 4 words distance reordering could gain four BLEU points over monotone decoding.", "labels": [], "entities": [{"text": "SMT phrase-based translation", "start_pos": 31, "end_pos": 59, "type": "TASK", "confidence": 0.9054754177729288}, {"text": "BLEU", "start_pos": 110, "end_pos": 114, "type": "METRIC", "confidence": 0.9991298317909241}]}, {"text": "explored a wide range of Arabic word-level preprocessing and produced better translation results fora system trained on 5 million Arabic words.", "labels": [], "entities": []}, {"text": "What all the above methodologies do not provide is a means to disambiguate morphological analysis for machine translation based on the words' contexts.", "labels": [], "entities": [{"text": "disambiguate morphological analysis", "start_pos": 62, "end_pos": 97, "type": "TASK", "confidence": 0.6418166557947794}, {"text": "machine translation", "start_pos": 102, "end_pos": 121, "type": "TASK", "confidence": 0.6995183974504471}]}, {"text": "That is, for an Arabic word analysis of the form prefix*-stem-suffix* a morpheme only is either always retained, always dropped off or always merged to the stem regardless of its surrounding text.", "labels": [], "entities": []}, {"text": "In the example in1), the Arabic word \"AlnAfi*h\"(\"window\" in English) was segmented as \"Al nAfi* ap\".", "labels": [], "entities": []}, {"text": "The morpheme \"ap\" is removed so that \"Al nAfi*\" aligned to \"the window\" of the English sentence.", "labels": [], "entities": []}, {"text": "In the sentence \"hl ldyk mqAEd bjwAr AlnAf*h ?\" (\"do you have window tables ?\" in English) the word \"AlnAfi*h\" is also segmented as \"Al nAfi* ap\".", "labels": [], "entities": []}, {"text": "But in this sentence, morphological preprocessing should remove both \"Al\" and \"ap\" so that only the remain-nu riyd u ||| mA}id ap ||| bi jAnib ||| Al nAfi* ap ||| . we want to have a table near the window . nu riyd u mA}id ap bi jAnib Al nAfi* . nryd mA}dh bjAnb AlnAf*h . ing morpheme \"nAfi*\" aligned to the word \"window\" of the English translation.", "labels": [], "entities": []}, {"text": "Thus an appropriate preprocessing technique should be guided by English translation and bring the word context into account.", "labels": [], "entities": []}, {"text": "In this paper we describe a context-based morphological analysis for Arabic-English translation that take full account morphemes alignment to English text.", "labels": [], "entities": []}, {"text": "The preprocessing uses the Arabic morphology disambiguation in () for full morphological analysis and learns the removing morphemes model based on the Viterbi alignment of English to full morphological analysis.", "labels": [], "entities": []}, {"text": "We tested the model with two training corpora of 5.2 millions Arabic words(177K sentences) in news domain and 159K Arabic words (20K sentences) in travel conversation domain and gain improvement over the original Arabic translation in both experiments.", "labels": [], "entities": []}, {"text": "The system that trained on a subsample corpora of 5 millions sentence pairs corpora also showed one BLEU score improvement over the original Arabic system on unseen test set.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 100, "end_pos": 104, "type": "METRIC", "confidence": 0.9994584918022156}, {"text": "unseen test set", "start_pos": 158, "end_pos": 173, "type": "DATASET", "confidence": 0.779153843720754}]}, {"text": "We will explain our technique in the next section and briefly review the phrase based SMT model in section 3.", "labels": [], "entities": [{"text": "SMT", "start_pos": 86, "end_pos": 89, "type": "TASK", "confidence": 0.97674959897995}]}, {"text": "The experiment results will be presented in section 4.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section we present experiment results using our Arabic morphology preprocessing technique.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: BTEC corpus statistics", "labels": [], "entities": [{"text": "BTEC corpus statistics", "start_pos": 10, "end_pos": 32, "type": "DATASET", "confidence": 0.9395758112271627}]}, {"text": " Table 5: BTEC translations results on IBM-BLEU  metrics(Case insensitive and 6 tokens distance re- ordering window). The boldface marks scores sig- nificantly higher than the original Arabic transla- tion scores.", "labels": [], "entities": [{"text": "BTEC translations", "start_pos": 10, "end_pos": 27, "type": "TASK", "confidence": 0.4862535893917084}]}, {"text": " Table 2: BTEC test set statistics", "labels": [], "entities": [{"text": "BTEC test set", "start_pos": 10, "end_pos": 23, "type": "DATASET", "confidence": 0.9678571820259094}]}, {"text": " Table 4: Newswire test set statistics", "labels": [], "entities": [{"text": "Newswire test set", "start_pos": 10, "end_pos": 27, "type": "DATASET", "confidence": 0.9746928215026855}]}, {"text": " Table 8: The impact of reordering limits on BTEC 's development set IWSLT04 and Newswire's devel- opment set MT03. The translation scores are IBM-BLEU metric", "labels": [], "entities": [{"text": "BTEC", "start_pos": 45, "end_pos": 49, "type": "DATASET", "confidence": 0.9278577566146851}, {"text": "IWSLT04", "start_pos": 69, "end_pos": 76, "type": "DATASET", "confidence": 0.6430320739746094}, {"text": "Newswire's devel- opment set MT03", "start_pos": 81, "end_pos": 114, "type": "DATASET", "confidence": 0.6746168562344143}, {"text": "translation", "start_pos": 120, "end_pos": 131, "type": "TASK", "confidence": 0.9297618269920349}]}, {"text": " Table 7: Unknown tokens count", "labels": [], "entities": []}, {"text": " Table 9: Translation results of large corpora(Case  insensitive, IBM-BLEU metric). The boldface  marks score significantly higher than the original  Arabic translation score.", "labels": [], "entities": [{"text": "Translation", "start_pos": 10, "end_pos": 21, "type": "TASK", "confidence": 0.950271487236023}]}]}