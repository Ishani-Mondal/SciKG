{"title": [{"text": "A Tree-to-String Phrase-based Model for Statistical Machine Translation", "labels": [], "entities": [{"text": "Statistical Machine Translation", "start_pos": 40, "end_pos": 71, "type": "TASK", "confidence": 0.8347634673118591}]}], "abstractContent": [{"text": "Though phrase-based SMT has achieved high translation quality, it still lacks of generalization ability to capture word order differences between languages.", "labels": [], "entities": [{"text": "SMT", "start_pos": 20, "end_pos": 23, "type": "TASK", "confidence": 0.7125571370124817}]}, {"text": "In this paper we describe a general method for tree-to-string phrase-based SMT.", "labels": [], "entities": [{"text": "SMT", "start_pos": 75, "end_pos": 78, "type": "TASK", "confidence": 0.7620623111724854}]}, {"text": "We study how syntactic transformation is incorporated into phrase-based SMT and its effectiveness.", "labels": [], "entities": [{"text": "syntactic transformation", "start_pos": 13, "end_pos": 37, "type": "TASK", "confidence": 0.7433082759380341}, {"text": "SMT", "start_pos": 72, "end_pos": 75, "type": "TASK", "confidence": 0.7035949230194092}]}, {"text": "We design syntactic transformation models using unlexicalized form of synchronous context-free grammars.", "labels": [], "entities": []}, {"text": "These models can be learned from source-parsed bitext.", "labels": [], "entities": []}, {"text": "Our system can naturally make use of both constituent and non-constituent phrasal translations in the decoding phase.", "labels": [], "entities": []}, {"text": "We considered various levels of syntactic analysis ranging from chunking to full parsing.", "labels": [], "entities": [{"text": "syntactic analysis", "start_pos": 32, "end_pos": 50, "type": "TASK", "confidence": 0.7145664542913437}]}, {"text": "Our experimental results of English-Japanese and English-Vietnamese translation showed a significant improvement over two baseline phrase-based SMT systems.", "labels": [], "entities": [{"text": "SMT", "start_pos": 144, "end_pos": 147, "type": "TASK", "confidence": 0.863189160823822}]}], "introductionContent": [{"text": "Based on the kind of linguistic information which is made use of, syntactic SMT can be divided into four types: tree-to-string, string-to-tree, tree-to-tree, and hierarchical phrase-based.", "labels": [], "entities": [{"text": "syntactic SMT", "start_pos": 66, "end_pos": 79, "type": "TASK", "confidence": 0.5300101041793823}]}, {"text": "The tree-to-string approach (;) supposes that syntax of the source language is known.", "labels": [], "entities": []}, {"text": "This approach can be applied when a source language parser is available.", "labels": [], "entities": []}, {"text": "The string-to-tree approach) focuses on syntactic modelling of the target language in cases it has syntactic resources such as treebanks and parsers.", "labels": [], "entities": []}, {"text": "The tree-to-tree approach models the syntax of both languages, therefore extra cost is required.", "labels": [], "entities": []}, {"text": "The fourth approach) constraints phrases under context-free grammar structure without any requirement of linguistic annotation.", "labels": [], "entities": []}, {"text": "In this paper, we present a tree-to-string phrasebased method which is based on synchronous CFGs.", "labels": [], "entities": []}, {"text": "This method has two important properties: syntactic transformation is used in the decoding phase including a word-to-phrase tree transformation model and a phrase reordering model; phrases are the basic unit of translation.", "labels": [], "entities": [{"text": "syntactic transformation", "start_pos": 42, "end_pos": 66, "type": "TASK", "confidence": 0.7257898449897766}]}, {"text": "Since we design syntactic transformation models using un-lexicalized synchronous CFGs, the number of rules is small 1 . Previous studies on tree-to-string SMT are different from ours.", "labels": [], "entities": [{"text": "SMT", "start_pos": 155, "end_pos": 158, "type": "TASK", "confidence": 0.8475465178489685}]}, {"text": "Collins et al. used hand crafted rules to carryout word reordering in the preprocessing phase but not decoding phase.", "labels": [], "entities": [{"text": "word reordering", "start_pos": 51, "end_pos": 66, "type": "TASK", "confidence": 0.7262381017208099}]}, {"text": "presented a more general method in which lexicalized syntactic reordering models based on PCFGs can be learned from source-parsed bitext and then applied in the preprocessing phase.", "labels": [], "entities": []}, {"text": "changed the translation unit from phrases to tree-to-string alignment templates (TATs) while we do not.", "labels": [], "entities": []}, {"text": "TATs was represented as xRs rules while we use synchronous CFG rules.", "labels": [], "entities": [{"text": "TATs", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.7234183549880981}]}, {"text": "In order to overcome the limitation that TATs cannot capture non-constituent phrasal translations, proposed forest-to-string rules while our system can naturally make use of such kind of phrasal translation by word-to-phrase tree transformation.", "labels": [], "entities": []}, {"text": "We carried out experiments with two language pairs English-Japanese and English-Vietnamese.", "labels": [], "entities": []}, {"text": "Our system achieved significant improvements over Pharaoh, a state-of-the-art phrase-based SMT system.", "labels": [], "entities": [{"text": "SMT", "start_pos": 91, "end_pos": 94, "type": "TASK", "confidence": 0.7732784152030945}]}, {"text": "We also analyzed the dependence of translation quality on the level of syntactic analysis (shallow or deep).", "labels": [], "entities": []}, {"text": "shows the architecture of our system.", "labels": [], "entities": []}, {"text": "The input of this system is a source-language tree and the output is a target-language string.", "labels": [], "entities": []}, {"text": "This system uses all features of conventional phrase-based SMT as in (.", "labels": [], "entities": [{"text": "SMT", "start_pos": 59, "end_pos": 62, "type": "TASK", "confidence": 0.7708977460861206}]}, {"text": "There are two new features including a word-to-phrase tree transformation model and a phrase reordering model.", "labels": [], "entities": [{"text": "word-to-phrase tree transformation", "start_pos": 39, "end_pos": 73, "type": "TASK", "confidence": 0.6246375938256582}]}, {"text": "The decoding algo-rithm is a tree-based search algorithm.", "labels": [], "entities": []}], "datasetContent": [{"text": "We used Reuters 3 , an English-Japanese bilingual corpus, and Conversation, an English-Vietnamese corpus).", "labels": [], "entities": [{"text": "Reuters 3", "start_pos": 8, "end_pos": 17, "type": "DATASET", "confidence": 0.9133358001708984}]}, {"text": "These corpora were split into data sets as shown in.", "labels": [], "entities": []}, {"text": "Japanese sentences were analyzed by ChaSen 4 , a word-segmentation tool.", "labels": [], "entities": []}, {"text": "A number of tools were used in our experiments.", "labels": [], "entities": []}, {"text": "Vietnamese sentences were segmented using a wordsegmentation program).", "labels": [], "entities": [{"text": "wordsegmentation", "start_pos": 44, "end_pos": 60, "type": "TASK", "confidence": 0.9752922058105469}]}, {"text": "For learning phrase translations and decoding, we used Pharaoh), a state-of-the-art phrasebased SMT system which is available for research purpose.", "labels": [], "entities": [{"text": "learning phrase translations", "start_pos": 4, "end_pos": 32, "type": "TASK", "confidence": 0.6187324921290079}, {"text": "SMT", "start_pos": 96, "end_pos": 99, "type": "TASK", "confidence": 0.7475050687789917}]}, {"text": "For word alignment, we used the GIZA++ tool).", "labels": [], "entities": [{"text": "word alignment", "start_pos": 4, "end_pos": 18, "type": "TASK", "confidence": 0.8608882427215576}]}, {"text": "For learning language models, we used SRILM toolkit).", "labels": [], "entities": [{"text": "SRILM toolkit", "start_pos": 38, "end_pos": 51, "type": "DATASET", "confidence": 0.7054348587989807}]}, {"text": "For MT evaluation, we used BLEU measure () calculated by the NIST script version 11b.", "labels": [], "entities": [{"text": "MT evaluation", "start_pos": 4, "end_pos": 17, "type": "TASK", "confidence": 0.9534842073917389}, {"text": "BLEU measure", "start_pos": 27, "end_pos": 39, "type": "METRIC", "confidence": 0.9895803332328796}, {"text": "NIST script version 11b", "start_pos": 61, "end_pos": 84, "type": "DATASET", "confidence": 0.9678052961826324}]}, {"text": "For the parsing task, we used Charniak's parser).", "labels": [], "entities": [{"text": "parsing task", "start_pos": 8, "end_pos": 20, "type": "TASK", "confidence": 0.927942544221878}]}, {"text": "For experiments with chunking (or shallow parsing), we used a CRFs-based chunking tool to split a source sentence into syntactic chunks.", "labels": [], "entities": [{"text": "shallow parsing)", "start_pos": 34, "end_pos": 50, "type": "TASK", "confidence": 0.6987527509530386}]}, {"text": "Then a pseudo CFG rule over chunks is built to generate a two-level syntactic tree.", "labels": [], "entities": []}, {"text": "This tree can be used in the + Input: A source CFG tree, a translation-option collection + Output: The best target sentence + Step 1: Allocate translation options to hypothesis collections at leaf nodes.", "labels": [], "entities": []}, {"text": "+ Step 2: Compute overlap vector for all nodes.", "labels": [], "entities": [{"text": "Compute overlap vector", "start_pos": 10, "end_pos": 32, "type": "METRIC", "confidence": 0.8915254871050516}]}, {"text": "+ Step 3: For each node, if all of its children have been translated, then for each valid sub-sequence of child list, carryout the following steps: + Step 3.1: Retrieve transformation rules + Step 3.2: Reorder the sub-sequence + Step 3.3: Translate the reordered sub-sequence and update corresponding hypothesis collections: BLEU score comparison between phrasebased SMT and syntax-directed SMT.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 325, "end_pos": 335, "type": "METRIC", "confidence": 0.9412577152252197}, {"text": "SMT", "start_pos": 367, "end_pos": 370, "type": "TASK", "confidence": 0.7549151182174683}]}, {"text": "PB=phrase-based; SD=syntax-directed same way as trees produced by Charniak's parser.", "labels": [], "entities": []}, {"text": "We built a SMT system for phrase-based log-linear translation models.", "labels": [], "entities": [{"text": "SMT", "start_pos": 11, "end_pos": 14, "type": "TASK", "confidence": 0.9930543303489685}, {"text": "phrase-based log-linear translation", "start_pos": 26, "end_pos": 61, "type": "TASK", "confidence": 0.6763194600741068}]}, {"text": "This system has two decoders: beam search and syntax-based.", "labels": [], "entities": []}, {"text": "We implemented the algorithm in Section 5 for the syntax-based decoder.", "labels": [], "entities": []}, {"text": "We also implemented a rule induction module and a module for minimum error rate training.", "labels": [], "entities": [{"text": "rule induction", "start_pos": 22, "end_pos": 36, "type": "TASK", "confidence": 0.8715815544128418}]}, {"text": "We used the system for our experiments reported later.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: Corpora and data sets.", "labels": [], "entities": []}, {"text": " Table 4: Corpus statistics of translation tasks.", "labels": [], "entities": [{"text": "translation tasks", "start_pos": 31, "end_pos": 48, "type": "TASK", "confidence": 0.8679593205451965}]}, {"text": " Table 5: Rule induction statistics.", "labels": [], "entities": [{"text": "Rule induction", "start_pos": 10, "end_pos": 24, "type": "TASK", "confidence": 0.9299193918704987}]}, {"text": " Table 6: BLEU score comparison between phrase- based SMT and syntax-directed SMT. PB=phrase- based; SD=syntax-directed", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9970986843109131}]}, {"text": " Table 7: BLEU score with different syntactic levels.  Level-i means syntactic transformation was applied to  tree nodes whose level smaller than or equal to i. The  level of a pre-terminal node (POS tag) is 0. The level  of an inner node is the maximum of its children's lev- els.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9979677796363831}]}]}