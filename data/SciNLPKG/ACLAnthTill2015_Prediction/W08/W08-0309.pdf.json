{"title": [], "abstractContent": [{"text": "This paper analyzes the translation quality of machine translation systems for 10 language pairs translating between Czech, English, French, German, Hungarian, and Spanish.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 47, "end_pos": 66, "type": "TASK", "confidence": 0.7347076535224915}]}, {"text": "We report the translation quality of over 30 diverse translation systems based on a large-scale manual evaluation involving hundreds of hours of effort.", "labels": [], "entities": [{"text": "translation", "start_pos": 14, "end_pos": 25, "type": "TASK", "confidence": 0.9671187400817871}]}, {"text": "We use the human judgments of the systems to analyze automatic evaluation metrics for translation quality, and we report the strength of the correlation with human judgments at both the system-level and at the sentence-level.", "labels": [], "entities": []}, {"text": "We validate our manual evaluation methodology by measuring intra-and inter-annotator agreement, and collecting timing information .", "labels": [], "entities": [{"text": "timing", "start_pos": 111, "end_pos": 117, "type": "METRIC", "confidence": 0.9330160021781921}]}], "introductionContent": [{"text": "This paper presents the results the shared tasks of the 2008 ACL Workshop on Statistical Machine Translation, which builds on two past workshops (.", "labels": [], "entities": [{"text": "ACL Workshop on Statistical Machine Translation", "start_pos": 61, "end_pos": 108, "type": "TASK", "confidence": 0.588004857301712}]}, {"text": "There were two shared tasks this year: a translation task which evaluated translation between 10 pairs of European languages, and an evaluation task which examines automatic evaluation metrics.", "labels": [], "entities": [{"text": "translation task", "start_pos": 41, "end_pos": 57, "type": "TASK", "confidence": 0.9006991684436798}]}, {"text": "There were a number of differences between this year's workshop and last year's workshop: \u2022 Test set selection -Instead of creating our test set by reserving a portion of the training data, we instead hired translators to translate a set of newspaper articles from a number of different sources.", "labels": [], "entities": []}, {"text": "This out-of-domain test set contrasts with the in-domain Europarl test set.", "labels": [], "entities": [{"text": "Europarl test set", "start_pos": 57, "end_pos": 74, "type": "DATASET", "confidence": 0.9852956930796305}]}, {"text": "\u2022 New language pairs -We evaluated the quality of Hungarian-English machine translation.", "labels": [], "entities": [{"text": "Hungarian-English machine translation", "start_pos": 50, "end_pos": 87, "type": "TASK", "confidence": 0.5825192431608835}]}, {"text": "Hungarian is a challenging language because it is agglutinative, has many cases and verb conjugations, and has freer word order.", "labels": [], "entities": []}, {"text": "GermanSpanish was our first language pair that did not include English, but was not manually evaluated since it attracted minimal participation.", "labels": [], "entities": [{"text": "GermanSpanish", "start_pos": 0, "end_pos": 13, "type": "DATASET", "confidence": 0.919833242893219}]}, {"text": "\u2022 System combination -Saarland University entered a system combination over a number of rule-based MT systems, and provided their output, which were also treated as fully fledged entries in the manual evaluation.", "labels": [], "entities": [{"text": "MT", "start_pos": 99, "end_pos": 101, "type": "TASK", "confidence": 0.867385745048523}]}, {"text": "Three additional groups were invited to apply their system combination algorithms to all systems.", "labels": [], "entities": []}, {"text": "\u2022 Refined manual evaluation -Because last year's study indicated that fluency and adequacy judgments were slow and unreliable, we dropped them from manual evaluation.", "labels": [], "entities": [{"text": "Refined manual evaluation", "start_pos": 2, "end_pos": 27, "type": "TASK", "confidence": 0.7042383352915446}]}, {"text": "We replaced them with yes/no judgments about the acceptability of translations of shorter phrases.", "labels": [], "entities": []}, {"text": "\u2022 Sentence-level correlation -In addition to measuring the correlation of automatic evaluation metrics with human judgments at the system level, we also measured how consistent they were with the human rankings of individual sentences.", "labels": [], "entities": []}, {"text": "The remainder of this paper is organized as follows: Section 2 gives an overview of the shared translation task, describing the test sets, the materials that were provided to participants, and a list of the groups who participated.", "labels": [], "entities": [{"text": "shared translation task", "start_pos": 88, "end_pos": 111, "type": "TASK", "confidence": 0.6956125696500143}]}, {"text": "Section 3 describes the manual evaluation of the translations, including information about the different types of judgments that were solicited and how much data was collected.", "labels": [], "entities": []}, {"text": "Section 4 presents the results of the manual evaluation.", "labels": [], "entities": []}, {"text": "Section 5 gives an overview of the shared evaluation task, describes which automatic metrics were submitted, and tells how they were evaluated.", "labels": [], "entities": []}, {"text": "Section 6 presents the results of the evaluation task.", "labels": [], "entities": []}, {"text": "Section 7 validates the manual evaluation methodology.", "labels": [], "entities": []}], "datasetContent": [{"text": "As with last year's workshop, we placed greater emphasis on the human evaluation than on the automatic evaluation metric scores.", "labels": [], "entities": []}, {"text": "It is our contention that automatic measures are an imperfect substitute for human assessment of translation quality.", "labels": [], "entities": []}, {"text": "Therefore, rather than select an official automatic evaluation metric like the NIST Machine Translation Workshop does, we define the manual evaluation to be primary, and use Figure 1: Properties of the training and test sets used in the shared task.", "labels": [], "entities": [{"text": "NIST Machine Translation", "start_pos": 79, "end_pos": 103, "type": "TASK", "confidence": 0.7806808749834696}]}, {"text": "The training data is drawn from the Europarl corpus and from the Project Syndicate, a website which collects political commentary in multiple languages.", "labels": [], "entities": [{"text": "Europarl corpus", "start_pos": 36, "end_pos": 51, "type": "DATASET", "confidence": 0.9945283532142639}]}, {"text": "For Czech and Hungarian we use other available parallel corpora.", "labels": [], "entities": []}, {"text": "Note that the number of words is computed based on the provided tokenizer and that the number of distinct words is the based on lowercased tokens.", "labels": [], "entities": []}, {"text": "the human judgments to validate automatic metrics.", "labels": [], "entities": []}, {"text": "Manual evaluation is time consuming, and it requires a monumental effort to conduct it on the scale of our workshop.", "labels": [], "entities": [{"text": "Manual evaluation", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.7329969108104706}]}, {"text": "We distributed the workload across a number of people, including shared task participants, interested volunteers, and a small number of paid annotators.", "labels": [], "entities": []}, {"text": "More than 100 people participated in the manual evaluation, with 75 people putting in more than an hour's worth of effort, and 25 putting in more than four hours.", "labels": [], "entities": []}, {"text": "A collective total of 266 hours of labor was invested.", "labels": [], "entities": []}, {"text": "We wanted to ensure that we were using our annotators' time effectively, so we carefully designed the manual evaluation process.", "labels": [], "entities": []}, {"text": "In our analysis of last year's manual evaluation we found that the NISTstyle fluency and adequacy scores were overly time consuming and inconsistent.", "labels": [], "entities": [{"text": "NISTstyle", "start_pos": 67, "end_pos": 76, "type": "DATASET", "confidence": 0.9207791090011597}]}, {"text": "We therefore abandoned this method of evaluating the translations.", "labels": [], "entities": []}, {"text": "We asked people to evaluate the systems' output in three different ways: \u2022 Ranking translated sentences relative to each other \u2022 Ranking the translations of syntactic constituents drawn from the source sentence \u2022 Assigning absolute yes or no judgments to the translations of the syntactic constituents.", "labels": [], "entities": [{"text": "Assigning absolute yes or no judgments", "start_pos": 213, "end_pos": 251, "type": "TASK", "confidence": 0.847590982913971}]}, {"text": "The manual evaluation software asked for repeated judgments from the same individual, and had multiple people judge the same item, and logged the time it took to complete each judgment.", "labels": [], "entities": []}, {"text": "This allowed us to measure intra-and inter-annotator agreement, and to analyze the average amount of time it takes to collect the different kinds of judgments.", "labels": [], "entities": []}, {"text": "Our analysis is presented in Section 7.", "labels": [], "entities": []}, {"text": "The manual evaluation data provides a rich source of information beyond simply analyzing the quality of translations produced by different systems.", "labels": [], "entities": []}, {"text": "In particular, it is especially useful for validating the automatic metrics which are frequently used by the machine translation research community.", "labels": [], "entities": [{"text": "machine translation research", "start_pos": 109, "end_pos": 137, "type": "TASK", "confidence": 0.8350465297698975}]}, {"text": "We continued the shared task which we debuted last year, by examining how well various automatic metrics correlate with human judgments.", "labels": [], "entities": []}, {"text": "In addition to examining how well the automatic evaluation metrics predict human judgments at the system-level, this year we have also started to measure their ability to predict sentence-level judgments.", "labels": [], "entities": []}, {"text": "The automatic metrics that were evaluated in this year's shared task were the following: \u2022 Bleu ()-Bleu remains the de facto standard in machine translation evaluation.", "labels": [], "entities": [{"text": "Bleu ()-Bleu", "start_pos": 91, "end_pos": 103, "type": "METRIC", "confidence": 0.8289129137992859}, {"text": "machine translation evaluation", "start_pos": 137, "end_pos": 167, "type": "TASK", "confidence": 0.8601446946461996}]}, {"text": "It calculates n-gram precision and a brevity penalty, and can make use of multiple reference translations as away of capturing some of the allowable variation in translation.", "labels": [], "entities": [{"text": "precision", "start_pos": 21, "end_pos": 30, "type": "METRIC", "confidence": 0.9830080270767212}]}, {"text": "We use a single reference translation in our experiments.", "labels": [], "entities": []}, {"text": "\u2022 Meteor (Agarwal and Lavie, 2008)-Meteor measures precision and recall for unigrams and applies a fragmentation penalty.", "labels": [], "entities": [{"text": "precision", "start_pos": 51, "end_pos": 60, "type": "METRIC", "confidence": 0.9986685514450073}, {"text": "recall", "start_pos": 65, "end_pos": 71, "type": "METRIC", "confidence": 0.9984075427055359}]}, {"text": "It uses flexible word matching based on stemming and WordNet-synonymy.", "labels": [], "entities": [{"text": "word matching", "start_pos": 17, "end_pos": 30, "type": "TASK", "confidence": 0.7128341048955917}, {"text": "WordNet-synonymy", "start_pos": 53, "end_pos": 69, "type": "DATASET", "confidence": 0.9343372583389282}]}, {"text": "A number of variants are investigated here: meteor-baseline and meteorranking are optimized for correlation with adequacy and ranking judgments respectively.", "labels": [], "entities": []}, {"text": "mbleu and mter are Bleu and TER computed using the flexible matching used in Meteor.", "labels": [], "entities": [{"text": "Bleu", "start_pos": 19, "end_pos": 23, "type": "METRIC", "confidence": 0.9557980895042419}, {"text": "TER", "start_pos": 28, "end_pos": 31, "type": "METRIC", "confidence": 0.9803760051727295}, {"text": "Meteor", "start_pos": 77, "end_pos": 83, "type": "DATASET", "confidence": 0.9325287342071533}]}, {"text": "\u2022 measure overlapping grammatical dependency relationships (DP), semantic roles (SR), and discourse representations (DR).", "labels": [], "entities": []}, {"text": "The authors further investigate combining these with other metrics including TER, Bleu, GTM, Rouge, and Meteor (ULC and ULCh).", "labels": [], "entities": [{"text": "TER", "start_pos": 77, "end_pos": 80, "type": "METRIC", "confidence": 0.993370771408081}, {"text": "Bleu", "start_pos": 82, "end_pos": 86, "type": "METRIC", "confidence": 0.9214616417884827}]}, {"text": "\u2022 Popovic and Ney (2007) automatically evaluate translation quality by examining sequences of parts of speech, rather than words.", "labels": [], "entities": []}, {"text": "They calculate Bleu (posbleu) and F-measure (pos4gramFmeasure) by matching part of speech 4grams in a hypothesis translation against the reference translation.", "labels": [], "entities": [{"text": "Bleu", "start_pos": 15, "end_pos": 19, "type": "METRIC", "confidence": 0.9984354376792908}, {"text": "F-measure", "start_pos": 34, "end_pos": 43, "type": "METRIC", "confidence": 0.9888317584991455}]}, {"text": "In addition to the above metrics, which scored the translations on both the system-level and the sentence-level, there were a number of metrics which focused on the sentence-level: \u2022 Albrecht and Hwa (2008) use support vector regression to score translations using past WMT manual assessment data as training examples.", "labels": [], "entities": [{"text": "WMT manual assessment data", "start_pos": 270, "end_pos": 296, "type": "DATASET", "confidence": 0.6795376092195511}]}, {"text": "The metric uses features derived from targetside language models and machine-generated translations (svm-pseudo-ref) as well as reference human translations (svm-human-ref).", "labels": [], "entities": []}, {"text": "\u2022 Duh (2008) similarly used support vector machines to predict an ordering over a set of system translations (svm-rank).", "labels": [], "entities": []}, {"text": "Features included in's training were sentencelevel BLEU scores and intra-set ranks computed from the entire set of translations.", "labels": [], "entities": [{"text": "BLEU scores", "start_pos": 51, "end_pos": 62, "type": "METRIC", "confidence": 0.9486786127090454}]}, {"text": "\u2022 USaar's evaluation metric (alignment-prob) uses Giza++ to align outputs of multiple systems with the corresponding reference translations, with a bias towards identical one-to-one alignments through a suitably augmented corpus.", "labels": [], "entities": [{"text": "USaar", "start_pos": 2, "end_pos": 7, "type": "DATASET", "confidence": 0.8319975733757019}]}, {"text": "The Model4 log probabilities in both directions are added and normalized to a scale between 0 and 1.", "labels": [], "entities": []}, {"text": "In the reverse direction, for translations out of English into the other languages, Bleu does considerably better, placing second overall after the part-ofspeech variant on it proposed by.", "labels": [], "entities": [{"text": "Bleu", "start_pos": 84, "end_pos": 88, "type": "METRIC", "confidence": 0.9672728776931763}]}, {"text": "Yet another variant of Bleu which utilizes Meteor's flexible matching has the strongest correlation for sentence-level ranking.", "labels": [], "entities": []}, {"text": "Appendix B gives a breakdown of the correlations for each of the lan- report the consistency of the automatic evaluation metrics with human judgments on a sentence-by-sentence basis, rather than on the system level.", "labels": [], "entities": [{"text": "consistency", "start_pos": 81, "end_pos": 92, "type": "METRIC", "confidence": 0.970106303691864}]}, {"text": "For the translations into English the ULC metric (which itself combines many other metrics) had the strongest correlation with human judgments, correctly predicting the human ranking of a each pair of system translations of a sentence more than half the time.", "labels": [], "entities": []}, {"text": "This is dramatically higher than the chance baseline, which is not .5, since it must correctly rank a list of systems rather than a pair.", "labels": [], "entities": [{"text": "chance baseline", "start_pos": 37, "end_pos": 52, "type": "METRIC", "confidence": 0.9593085944652557}]}, {"text": "For the reverse direction meteor-ranking performs very strongly.", "labels": [], "entities": []}, {"text": "The svn-rank which had the lowest overall correlation at the system level does the best at consistently predicting the translations of syntactic constituents into other languages.", "labels": [], "entities": []}, {"text": "In addition to scoring the shared task entries, we also continued on our campaign for improving the process of manual evaluation.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Difficulty of the test set parts based on the  original language. For each part, we average BLEU  scores from the Edinburgh systems for 12 language  pairs of the shared task.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 102, "end_pos": 106, "type": "METRIC", "confidence": 0.9993649125099182}, {"text": "Edinburgh systems", "start_pos": 124, "end_pos": 141, "type": "DATASET", "confidence": 0.9489710330963135}]}, {"text": " Table 3: The number of items that were judged for each task during the manual evaluation. The All-English  judgments were reused in the News task for individual language pairs.", "labels": [], "entities": []}, {"text": " Table 8: Average system-level correlations for the  automatic evaluation metrics on translations into En- glish", "labels": [], "entities": []}, {"text": " Table 9: Average system-level correlations for the  automatic evaluation metrics on translations into  French, German and Spanish", "labels": [], "entities": []}, {"text": " Table 10: The percent of time that each automatic  metric was consistent with human judgments for  translations into English", "labels": [], "entities": []}, {"text": " Table 11: The percent of time that each automatic  metric was consistent with human judgments for  translations into other languages", "labels": [], "entities": []}, {"text": " Table 12: Kappa coefficient values representing the  inter-annotator agreement for the different types of  manual evaluation", "labels": [], "entities": []}, {"text": " Table 13: Kappa coefficient values for intra- annotator agreement for the different types of man- ual evaluation", "labels": [], "entities": [{"text": "Kappa coefficient", "start_pos": 11, "end_pos": 28, "type": "METRIC", "confidence": 0.9629026055335999}]}, {"text": " Table 14: Automatic evaluation metric for translations into Czech", "labels": [], "entities": []}, {"text": " Table 15: Automatic evaluation metric for translations into French", "labels": [], "entities": []}, {"text": " Table 16: Automatic evaluation metric for translations into German and Spanish", "labels": [], "entities": []}, {"text": " Table 17: Automatic evaluation metric for translations into English", "labels": [], "entities": []}, {"text": " Table 18: Automatic evaluation metric for translations into English", "labels": [], "entities": []}, {"text": " Table 19: Correlation of automatic evaluation metrics with the three types of human judgments for transla- tion into English", "labels": [], "entities": [{"text": "transla- tion", "start_pos": 99, "end_pos": 112, "type": "TASK", "confidence": 0.9172260761260986}]}, {"text": " Table 20: Correlation of automatic evaluation metrics with the three types of human judgments for transla- tion into other languages", "labels": [], "entities": [{"text": "transla- tion", "start_pos": 99, "end_pos": 112, "type": "TASK", "confidence": 0.9061549703280131}]}, {"text": " Table 21: Sentence-level ranking for the French-English News Task.", "labels": [], "entities": [{"text": "French-English News Task", "start_pos": 42, "end_pos": 66, "type": "DATASET", "confidence": 0.8373510440190634}]}, {"text": " Table 22: Sentence-level ranking for the French-English Europarl Task.", "labels": [], "entities": [{"text": "French-English Europarl Task", "start_pos": 42, "end_pos": 70, "type": "DATASET", "confidence": 0.7788485487302145}]}, {"text": " Table 23: Sentence-level ranking for the English-French News Task.", "labels": [], "entities": []}, {"text": " Table 24: Sentence-level ranking for the English-French Europarl Task.", "labels": [], "entities": [{"text": "Europarl Task", "start_pos": 57, "end_pos": 70, "type": "DATASET", "confidence": 0.8147698938846588}]}, {"text": " Table 25: Sentence-level ranking for the German-English News Task.", "labels": [], "entities": [{"text": "German-English News Task", "start_pos": 42, "end_pos": 66, "type": "DATASET", "confidence": 0.8787488540013632}]}, {"text": " Table 26: Sentence-level ranking for the German-English Europarl Task.", "labels": [], "entities": [{"text": "German-English Europarl Task", "start_pos": 42, "end_pos": 70, "type": "DATASET", "confidence": 0.7341721455256144}]}, {"text": " Table 27: Sentence-level ranking for the English-German News Task.", "labels": [], "entities": []}, {"text": " Table 28: Sentence-level ranking for the English-German Europarl Task.", "labels": [], "entities": [{"text": "Europarl Task", "start_pos": 57, "end_pos": 70, "type": "DATASET", "confidence": 0.8316748440265656}]}, {"text": " Table 29: Sentence-level ranking for the Spanish-English News Task.", "labels": [], "entities": []}, {"text": " Table 30: Sentence-level ranking for the Spanish-English Europarl Task.", "labels": [], "entities": [{"text": "Spanish-English Europarl Task", "start_pos": 42, "end_pos": 71, "type": "DATASET", "confidence": 0.6741822163263956}]}, {"text": " Table 31: Sentence-level ranking for the English-Spanish News Task.", "labels": [], "entities": []}, {"text": " Table 32: Sentence-level ranking for the English-Spanish Europarl Task.", "labels": [], "entities": [{"text": "Europarl Task", "start_pos": 58, "end_pos": 71, "type": "DATASET", "confidence": 0.8136258721351624}]}, {"text": " Table 33: Sentence-level ranking for the Czech-English News Task.", "labels": [], "entities": [{"text": "Czech-English News Task", "start_pos": 42, "end_pos": 65, "type": "DATASET", "confidence": 0.8854590654373169}]}, {"text": " Table 34: Sentence-level ranking for the Czech-English Commentary Task.", "labels": [], "entities": [{"text": "Czech-English Commentary Task", "start_pos": 42, "end_pos": 71, "type": "DATASET", "confidence": 0.8477481007575989}]}, {"text": " Table 35: Sentence-level ranking for the English-Czech News Task.", "labels": [], "entities": []}, {"text": " Table 36: Sentence-level ranking for the English-Czech Commentary Task.", "labels": [], "entities": []}, {"text": " Table 37: Sentence-level ranking for the Hungarian-English News Task.", "labels": [], "entities": [{"text": "Hungarian-English News Task", "start_pos": 42, "end_pos": 69, "type": "DATASET", "confidence": 0.8218865593274435}]}, {"text": " Table 38: Constituent ranking for the French-English News Task", "labels": [], "entities": [{"text": "French-English News Task", "start_pos": 39, "end_pos": 63, "type": "DATASET", "confidence": 0.9370008707046509}]}, {"text": " Table 39: Constituent ranking for the French-English Europarl Task", "labels": [], "entities": [{"text": "French-English Europarl Task", "start_pos": 39, "end_pos": 67, "type": "DATASET", "confidence": 0.828741709391276}]}, {"text": " Table 40: Constituent ranking for the English-French News Task", "labels": [], "entities": [{"text": "English-French News Task", "start_pos": 39, "end_pos": 63, "type": "DATASET", "confidence": 0.7497334281603495}]}, {"text": " Table 41: Constituent ranking for the English-French Europarl Task", "labels": [], "entities": [{"text": "English-French Europarl Task", "start_pos": 39, "end_pos": 67, "type": "DATASET", "confidence": 0.6760457952817281}]}, {"text": " Table 42: Constituent ranking for the German-English News Task", "labels": [], "entities": [{"text": "German-English News Task", "start_pos": 39, "end_pos": 63, "type": "DATASET", "confidence": 0.9325632055600485}]}, {"text": " Table 43: Constituent ranking for the German-English Europarl Task", "labels": [], "entities": [{"text": "German-English Europarl Task", "start_pos": 39, "end_pos": 67, "type": "DATASET", "confidence": 0.7243266304334005}]}, {"text": " Table 44: Constituent ranking for the English-German News Task", "labels": [], "entities": [{"text": "English-German News Task", "start_pos": 39, "end_pos": 63, "type": "DATASET", "confidence": 0.8118816812833151}]}, {"text": " Table 45: Constituent ranking for the English-German Europarl Task", "labels": [], "entities": [{"text": "Europarl Task", "start_pos": 54, "end_pos": 67, "type": "DATASET", "confidence": 0.8157844245433807}]}, {"text": " Table 46: Constituent ranking for the Spanish-English News Task", "labels": [], "entities": [{"text": "Spanish-English News Task", "start_pos": 39, "end_pos": 64, "type": "DATASET", "confidence": 0.8646169304847717}]}, {"text": " Table 47: Constituent ranking for the Spanish-English Europarl Task", "labels": [], "entities": [{"text": "Spanish-English Europarl Task", "start_pos": 39, "end_pos": 68, "type": "DATASET", "confidence": 0.695230225721995}]}, {"text": " Table 48: Constituent ranking for the English-Spanish News Task", "labels": [], "entities": [{"text": "English-Spanish News Task", "start_pos": 39, "end_pos": 64, "type": "DATASET", "confidence": 0.7713124255339304}]}, {"text": " Table 49: Constituent ranking for the English-Spanish Europarl Task", "labels": [], "entities": [{"text": "English-Spanish Europarl Task", "start_pos": 39, "end_pos": 68, "type": "DATASET", "confidence": 0.6411163806915283}]}, {"text": " Table 50: Constituent ranking for the English-Czech News Task", "labels": [], "entities": [{"text": "English-Czech News Task", "start_pos": 39, "end_pos": 62, "type": "DATASET", "confidence": 0.7927767733732859}]}, {"text": " Table 51: Constituent ranking for the English-Czech Commentary Task", "labels": [], "entities": []}]}