{"title": [{"text": "Improved Tree-to-string Transducer for Machine Translation", "labels": [], "entities": [{"text": "Machine Translation", "start_pos": 39, "end_pos": 58, "type": "TASK", "confidence": 0.7904657125473022}]}], "abstractContent": [{"text": "We propose three enhancements to the tree-to-string (TTS) transducer for machine translation: first-level expansion-based normaliza-tion for TTS templates, a syntactic alignment framework integrating the insertion of unaligned target words, and subtree-based n-gram model addressing the tree decomposition probability.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 73, "end_pos": 92, "type": "TASK", "confidence": 0.7488647997379303}]}, {"text": "Empirical results show that these methods improve the performance of a TTS transducer based on the standard BLEU-4 metric.", "labels": [], "entities": [{"text": "TTS transducer", "start_pos": 71, "end_pos": 85, "type": "TASK", "confidence": 0.8754921853542328}, {"text": "BLEU-4", "start_pos": 108, "end_pos": 114, "type": "METRIC", "confidence": 0.9953847527503967}]}, {"text": "We also experiment with semantic labels in a TTS transducer, and achieve improvement over our baseline system.", "labels": [], "entities": []}], "introductionContent": [{"text": "Syntax-based statistical machine translation (SSMT) has achieved significant progress during recent years, with two threads developing simultaneously: the synchronous parsing-based SSMT ( and the tree-to-string (TTS) transducer ().", "labels": [], "entities": [{"text": "Syntax-based statistical machine translation (SSMT)", "start_pos": 0, "end_pos": 51, "type": "TASK", "confidence": 0.7503252327442169}, {"text": "parsing-based SSMT", "start_pos": 167, "end_pos": 185, "type": "TASK", "confidence": 0.6432202160358429}]}, {"text": "Synchronous SSMT here denotes the systems which accept a source sentence as the input and generate the translation and the syntactic structure for both the source and the translation simultaneously.", "labels": [], "entities": [{"text": "Synchronous SSMT", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.5050817579030991}]}, {"text": "Such systems are sometimes also called TTS transducers, but in this paper, TTS transducer refers to the system which starts with the syntax tree of a source sentence and recursively transforms the tree to the target language based on TTS templates.", "labels": [], "entities": [{"text": "TTS transducers", "start_pos": 39, "end_pos": 54, "type": "TASK", "confidence": 0.8256329596042633}]}, {"text": "In synchronous SSMT, TTS templates are used similar to the context free grammar used in the standard CYK parser, thus the syntax is part of the output and can bethought of as a constraint on the translation process.", "labels": [], "entities": [{"text": "SSMT", "start_pos": 15, "end_pos": 19, "type": "TASK", "confidence": 0.8351122140884399}]}, {"text": "In the TTS transducer, since the parse tree is given, syntax can bethought of as an additional feature of the input to be used in the translation.", "labels": [], "entities": []}, {"text": "The idea of synchronous SSMT can be traced back to's Stochastic Inversion Transduction Grammars.", "labels": [], "entities": [{"text": "SSMT", "start_pos": 24, "end_pos": 28, "type": "TASK", "confidence": 0.8000146746635437}]}, {"text": "A systematic method for extracting TTS templates from parallel corpora was proposed by, and later binarized by for high efficiency and accuracy.", "labels": [], "entities": [{"text": "TTS templates from parallel corpora", "start_pos": 35, "end_pos": 70, "type": "TASK", "confidence": 0.800702428817749}, {"text": "accuracy", "start_pos": 135, "end_pos": 143, "type": "METRIC", "confidence": 0.9933220744132996}]}, {"text": "In the other track, the TTS transducer originated from the tree transducer proposed by and independently.", "labels": [], "entities": []}, {"text": "generalized the tree transducer to the TTS transducer and introduced an EM algorithm to estimate the probability of TTS templates based on a bilingual corpus with one side parsed. and  then used the TTS transducer on the task of Chineseto-English and English-to-Chinese translation, respectively, and achieved decent performance.", "labels": [], "entities": [{"text": "English-to-Chinese translation", "start_pos": 251, "end_pos": 281, "type": "TASK", "confidence": 0.7027802169322968}]}, {"text": "Despite the progress SSMT has achieved, it is still a developing field with many problems unsolved.", "labels": [], "entities": [{"text": "SSMT", "start_pos": 21, "end_pos": 25, "type": "TASK", "confidence": 0.967427670955658}]}, {"text": "For example, the word alignment computed by GIZA++ and used as a basis to extract the TTS templates inmost SSMT systems has been observed to be a problem for SSMT, due to the fact that the word-based alignment models are not aware of the syntactic structure of the sentences and could produce many syntax-violating word alignments.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 17, "end_pos": 31, "type": "TASK", "confidence": 0.713338166475296}, {"text": "SSMT", "start_pos": 158, "end_pos": 162, "type": "TASK", "confidence": 0.9509961009025574}]}, {"text": "Approaches have been proposed recently towards getting better word alignment and thus better TTS templates, such as encoding syntactic structure information into the HMM-based word alignment model, and build-ing a syntax-based word alignment model with TTS templates.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 62, "end_pos": 76, "type": "TASK", "confidence": 0.75873202085495}, {"text": "HMM-based word alignment", "start_pos": 166, "end_pos": 190, "type": "TASK", "confidence": 0.5912818809350332}]}, {"text": "Unfortunately, neither approach reports end-to-end MT performance based on the syntactic alignment.", "labels": [], "entities": [{"text": "MT", "start_pos": 51, "end_pos": 53, "type": "TASK", "confidence": 0.9907282590866089}]}, {"text": "focus on alignment and do not present MT results, while takes the syntactic re-alignment as an input to an EM algorithm where the unaligned target words are inserted into the templates and minimum templates are combined into bigger templates ().", "labels": [], "entities": [{"text": "MT", "start_pos": 38, "end_pos": 40, "type": "TASK", "confidence": 0.9865850210189819}]}, {"text": "Thus the improvement they reported is rather indirect, leading us to wonder how much improvement the syntactic alignment model can directly bring to a SSMT system.", "labels": [], "entities": [{"text": "syntactic alignment", "start_pos": 101, "end_pos": 120, "type": "TASK", "confidence": 0.6870531439781189}, {"text": "SSMT", "start_pos": 151, "end_pos": 155, "type": "TASK", "confidence": 0.9732752442359924}]}, {"text": "Some other issues of SSMT not fully addressed before are highlighted below: 1.", "labels": [], "entities": [{"text": "SSMT", "start_pos": 21, "end_pos": 25, "type": "TASK", "confidence": 0.9788784980773926}]}, {"text": "mentioned that with only the minimum templates extracted from GHKM (), normalizing the template probability based on its tree pattern \"can become extremely biased\", due to the fact that bigger templates easily get high probabilities.", "labels": [], "entities": [{"text": "GHKM", "start_pos": 62, "end_pos": 66, "type": "DATASET", "confidence": 0.9189741015434265}]}, {"text": "They instead use a joint model where the templates are normalized based on the root of their tree patterns and show empirical results for that.", "labels": [], "entities": []}, {"text": "There is no systematic comparison of different normalization methods.", "labels": [], "entities": []}], "datasetContent": [{"text": "We used 74,597 pairs of English and Chinese sentences in the FBIS data set as our experimental data, which are further divided into 500 test sentence pairs, 500 development sentence pairs and 73597 training sentence pairs.", "labels": [], "entities": [{"text": "FBIS data set", "start_pos": 61, "end_pos": 74, "type": "DATASET", "confidence": 0.9795103669166565}]}, {"text": "The test set and development set are selected as those sentences having fewer than 25 words on the Chinese side.", "labels": [], "entities": []}, {"text": "The translation is from English to Chinese, and Charniak (2000)'s parser, trained on the Penn Treebank, is used to generate the syntax trees for the English side.", "labels": [], "entities": [{"text": "Penn Treebank", "start_pos": 89, "end_pos": 102, "type": "DATASET", "confidence": 0.9954079985618591}]}, {"text": "The weights of the MT components are optimized based on the development set using a gridbased line search.", "labels": [], "entities": [{"text": "MT", "start_pos": 19, "end_pos": 21, "type": "TASK", "confidence": 0.9531061053276062}]}, {"text": "The Chinese sentence from the selected pair is used as the single reference to tune and evaluate the MT system with word-based BLEU-4 ().", "labels": [], "entities": [{"text": "MT", "start_pos": 101, "end_pos": 103, "type": "TASK", "confidence": 0.9584751725196838}, {"text": "BLEU-4", "start_pos": 127, "end_pos": 133, "type": "METRIC", "confidence": 0.9763626456260681}]}, {"text": "used character-based BLEU as away of normalizing inconsistent Chinese word segmentation, but we avoid this problem as the training, development, and test data are from the same source.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 21, "end_pos": 25, "type": "METRIC", "confidence": 0.9839670062065125}, {"text": "normalizing inconsistent Chinese word segmentation", "start_pos": 37, "end_pos": 87, "type": "TASK", "confidence": 0.6602378845214844}]}], "tableCaptions": [{"text": " Table 2: BLEU-4 scores of various systems with the syntactic alignment and subtree bigram improvements added  incrementally.", "labels": [], "entities": [{"text": "BLEU-4", "start_pos": 10, "end_pos": 16, "type": "METRIC", "confidence": 0.9984408020973206}]}, {"text": " Table 3: BLEU-4 scores of semantic-based systems on test data. As in", "labels": [], "entities": [{"text": "BLEU-4", "start_pos": 10, "end_pos": 16, "type": "METRIC", "confidence": 0.9979900121688843}]}]}