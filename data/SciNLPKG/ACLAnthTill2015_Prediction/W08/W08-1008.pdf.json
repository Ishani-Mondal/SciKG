{"title": [{"text": "The PaGe 2008 Shared Task on Parsing German *", "labels": [], "entities": [{"text": "PaGe 2008 Shared Task on Parsing German", "start_pos": 4, "end_pos": 43, "type": "DATASET", "confidence": 0.8140100411006382}]}], "abstractContent": [{"text": "The ACL 2008 Workshop on Parsing German features a shared task on parsing German.", "labels": [], "entities": [{"text": "ACL 2008 Workshop on Parsing German", "start_pos": 4, "end_pos": 39, "type": "DATASET", "confidence": 0.8341367940107981}, {"text": "parsing German", "start_pos": 66, "end_pos": 80, "type": "TASK", "confidence": 0.88205885887146}]}, {"text": "The goal of the shared task was to find reasons for the radically different behavior of parsers on the different treebanks and between constituent and dependency representations.", "labels": [], "entities": []}, {"text": "In this paper, we describe the task and the data sets.", "labels": [], "entities": []}, {"text": "In addition, we provide an overview of the test results and a first analysis.", "labels": [], "entities": []}], "introductionContent": [{"text": "German is one of the very few languages for which more than one syntactically annotated resource exists.", "labels": [], "entities": []}, {"text": "Other languages for which this is the case include English (with the Penn treebank (, the Susanne Corpus, and the British section of the ICE Corpus ()) and Italian (with ISST) and TUT ().", "labels": [], "entities": [{"text": "Penn treebank", "start_pos": 69, "end_pos": 82, "type": "DATASET", "confidence": 0.9910712242126465}, {"text": "Susanne Corpus", "start_pos": 90, "end_pos": 104, "type": "DATASET", "confidence": 0.9614086747169495}, {"text": "British section of the ICE Corpus", "start_pos": 114, "end_pos": 147, "type": "DATASET", "confidence": 0.8370698889096578}, {"text": "TUT", "start_pos": 180, "end_pos": 183, "type": "DATASET", "confidence": 0.5370355844497681}]}, {"text": "The three German treebanks are Negra (), TIGER (), and T\u00fcBa-D/Z ().", "labels": [], "entities": [{"text": "German treebanks", "start_pos": 10, "end_pos": 26, "type": "DATASET", "confidence": 0.8759072124958038}, {"text": "Negra", "start_pos": 31, "end_pos": 36, "type": "DATASET", "confidence": 0.8964999318122864}, {"text": "TIGER", "start_pos": 41, "end_pos": 46, "type": "METRIC", "confidence": 0.9436414241790771}]}, {"text": "We will concentrate on TIGER and T\u00fcBa-D/Z here; Negra is annotated with an annotation scheme very similar to TIGER but is smaller.", "labels": [], "entities": [{"text": "TIGER", "start_pos": 23, "end_pos": 28, "type": "DATASET", "confidence": 0.5654282569885254}]}, {"text": "In contrast to other languages, these two treebanks are similar on many levels: Both treebanks are based on newspaper text, both use the STTS part of speech (POS) tagset, and both use an annotation * I am very grateful to Gerald Penn, who suggested this workshop and the shared task, took over the biggest part of the workshop organization and helped with the shared task.", "labels": [], "entities": [{"text": "STTS part of speech (POS) tagset", "start_pos": 137, "end_pos": 169, "type": "TASK", "confidence": 0.4994998276233673}]}, {"text": "scheme based on constituent structure augmented with grammatical functions.", "labels": [], "entities": []}, {"text": "However, they differ in the choices made in the annotation schemes, which makes them ideally suited for an investigation of how these decisions influence parsing accuracy in different parsers.", "labels": [], "entities": [{"text": "parsing", "start_pos": 154, "end_pos": 161, "type": "TASK", "confidence": 0.9512677192687988}, {"text": "accuracy", "start_pos": 162, "end_pos": 170, "type": "METRIC", "confidence": 0.7370977997779846}]}, {"text": "On a different level, German is an interesting language for parsing because of the syntactic phenomena in which the language differs from English, the undoubtedly most studied language in parsing: German is often listed as a non-configurational language.", "labels": [], "entities": [{"text": "parsing", "start_pos": 60, "end_pos": 67, "type": "TASK", "confidence": 0.9717340469360352}]}, {"text": "However, while the word order is freer than in English, the language exhibits a less flexible word order than more typical non-configurational languages.", "labels": [], "entities": []}, {"text": "A short overview of German word order phenomena is given in section 2.", "labels": [], "entities": [{"text": "German word order phenomena", "start_pos": 20, "end_pos": 47, "type": "TASK", "confidence": 0.590798407793045}]}, {"text": "The structure of this paper is as follows: Section 2 discusses three characteristics of German word order, section 3 provides a definition of the shared task, and section 4 gives a short overview of the treebanks and their annotation schemes that were used in the shared task.", "labels": [], "entities": [{"text": "German word order", "start_pos": 88, "end_pos": 105, "type": "TASK", "confidence": 0.5375542442003886}]}, {"text": "In section 5, we give an overview of the participating systems and their results.", "labels": [], "entities": []}], "datasetContent": [{"text": "The results of the constituent analysis are shown in.", "labels": [], "entities": []}, {"text": "The evaluation was performed with regard to labels consisting of a combination of syntactic labels and grammatical functions.", "labels": [], "entities": []}, {"text": "A subject noun phrase, for example, is only counted as correct if it has the correct yield, the correct label (i.e. NP for TIGER and NX for T\u00fcBa-D/Z), and the correct grammatical function (i.e. SB for TIGER and ON for T\u00fcBa-D/Z).", "labels": [], "entities": [{"text": "ON", "start_pos": 211, "end_pos": 213, "type": "METRIC", "confidence": 0.9724729657173157}]}, {"text": "The results show that the Berkeley parser reaches the best results for both treebanks.", "labels": [], "entities": []}, {"text": "The other two parsers compete for second place.", "labels": [], "entities": []}, {"text": "For TIGER, the V\u00e4xj\u00f6 parser outperforms the Stanford parser, but for T\u00fcBa-D/Z, the situation is reversed.", "labels": [], "entities": []}, {"text": "This gives an indication that the V\u00e4xj\u00f6 parser seems better suited for the flat annotations in TIGER while the Stanford parser is better suited for the more hierarchical structure in T\u00fcBa-D/Z.", "labels": [], "entities": [{"text": "TIGER", "start_pos": 95, "end_pos": 100, "type": "DATASET", "confidence": 0.6940714716911316}]}, {"text": "Note that all parsers reach much higher F-scores for T\u00fcBa-D/Z.", "labels": [], "entities": [{"text": "F-scores", "start_pos": 40, "end_pos": 48, "type": "METRIC", "confidence": 0.9987240433692932}, {"text": "T\u00fcBa-D/Z", "start_pos": 53, "end_pos": 61, "type": "DATASET", "confidence": 0.8233751654624939}]}, {"text": "A comparison of how well suited two different annotation schemes are for parsing is a surprisingly difficult task.", "labels": [], "entities": [{"text": "parsing", "start_pos": 73, "end_pos": 80, "type": "TASK", "confidence": 0.9780369400978088}]}, {"text": "A first approach would be to compare the parser performance for specific categories, such as for noun phrases, etc.", "labels": [], "entities": []}, {"text": "However, this is not possible for TIGER and T\u00fcBa-D/Z.", "labels": [], "entities": [{"text": "TIGER", "start_pos": 34, "end_pos": 39, "type": "DATASET", "confidence": 0.709195613861084}, {"text": "T\u00fcBa-D/Z", "start_pos": 44, "end_pos": 52, "type": "DATASET", "confidence": 0.8202393651008606}]}, {"text": "On the one hand, the range of phenomena described as noun phrases, for example, is different in the two treebanks.", "labels": [], "entities": []}, {"text": "The most obvious difference in annotation schemes is that T\u00fcBa-D/Z annotates unary branching structures while TIGER does not.", "labels": [], "entities": []}, {"text": "As a consequence, in T\u00fcBa-D/Z, all pronouns and substituting demonstratives are annotated as noun phrases; in TIGER, they are attached directly to the next higher node (cf. the relative pronouns, POS tag PRELS, in. and suggest a method for comparing such different annotation schemes by approximating them stepwise so that the decisions which result in major changes can be isolated.", "labels": [], "entities": [{"text": "POS tag PRELS", "start_pos": 196, "end_pos": 209, "type": "METRIC", "confidence": 0.8856244484583536}]}, {"text": "They come to the conclusion that the differences between the two annotation schemes is a least partially due to inconsistencies introduced into TIGER style annotations during the resolution of crossing branches.", "labels": [], "entities": []}, {"text": "However, even this method cannot give any indication which annotation scheme provides more useful information for systems that use such parses as input.", "labels": [], "entities": []}, {"text": "To answer this question, an in vivo evaluation would be necessary.", "labels": [], "entities": []}, {"text": "It is, however, rather difficult to find systems into which a parser can be plugged in without too many modifications of the system.", "labels": [], "entities": []}, {"text": "On the other hand, it is a well-known fact that TIGER  the PARSEVAL measures favor annotation schemes with hierarchical structures, such as in T\u00fcBa-D/Z, in comparison to annotation schemes with flat structures.", "labels": [], "entities": [{"text": "PARSEVAL", "start_pos": 59, "end_pos": 67, "type": "METRIC", "confidence": 0.7579262256622314}]}, {"text": "Here, TIGER and T\u00fcBa-D/Z differ significantly: in TIGER, phrases receive a flat annotation.", "labels": [], "entities": [{"text": "TIGER", "start_pos": 6, "end_pos": 11, "type": "METRIC", "confidence": 0.8487073183059692}]}, {"text": "Prepositional phrases, for example, do not contain an explicitly annotated noun phrase.", "labels": [], "entities": []}, {"text": "T\u00fcBa-D/Z phrases, in contrast, are more hierarchical; preposition phrases do contain a noun phrase, and non phrases distinguish between pre-and post-modification.", "labels": [], "entities": []}, {"text": "For this reason, the evaluation presented in must betaken with more than a grain of salt as a comparison of annotation schemes.", "labels": [], "entities": []}, {"text": "However, it seems safe to follow) in the assumption that the major grammatical functions, subject (SB/ON ), accusative object (OA ), and dative object (DA/OD ) are comparable.", "labels": [], "entities": []}, {"text": "Again, this is not completely true because in the case of one-word NPs, these functions are attached to the POS tags and thus are given in the input.", "labels": [], "entities": []}, {"text": "Another solution, which was pursued by, is the introduction of new unary branching nodes in the tree in cases where such grammatical functions are originally attached to the POS tag.", "labels": [], "entities": []}, {"text": "We refrained from using this solution because it introduces further inconsistencies (only a subset of unary branching nodes are explicitly annotated), which make it difficult fora parser to decide whether to group such phrases or not.", "labels": [], "entities": []}, {"text": "The evaluation shown in is based on all nodes which were annotated with the grammatical function in question.", "labels": [], "entities": []}, {"text": "The results presented in show that the differences between the two treebanks are inconclusive.", "labels": [], "entities": []}, {"text": "While the Stanford parser performs consistently better on T\u00fcBa-D/Z, the Berkeley parser handles accusative objects better in TIGER, and the V\u00e4xj\u00f6 parser subjects and dative objects.", "labels": [], "entities": []}, {"text": "The results indicate that the Berkeley parser profits from the TIGER annotation of accusative objects, which are grouped in the verb phrase while T\u00fcBa-D/Z groups all objects in their fields directly without resorting to a verb phrase.", "labels": [], "entities": []}, {"text": "However, this does not explain why the Berkeley parser cannot profit from the subject attachment on the clause level in TIGER to the same degree.", "labels": [], "entities": []}, {"text": "The results of the dependency evaluation for the V\u00e4xj\u00f6 system are shown in  important for the comparison of constituent and dependency parsing since in the conversion to dependencies, most of the differences between the annotation schemes, and as a consequence, the preference of the PARSEVAL measures have been neutralized.", "labels": [], "entities": [{"text": "V\u00e4xj\u00f6 system", "start_pos": 49, "end_pos": 61, "type": "DATASET", "confidence": 0.8714627325534821}, {"text": "dependency parsing", "start_pos": 124, "end_pos": 142, "type": "TASK", "confidence": 0.7027806639671326}, {"text": "PARSEVAL", "start_pos": 284, "end_pos": 292, "type": "METRIC", "confidence": 0.9185711741447449}]}, {"text": "Therefore, it is interesting to see that the results for TIGER are slightly better than the results for T\u00fcBa-D/Z, both for unlabeled (UAS) and labeled attachment scores.", "labels": [], "entities": [{"text": "TIGER", "start_pos": 57, "end_pos": 62, "type": "METRIC", "confidence": 0.9373368620872498}]}, {"text": "The reasons for these differences are unclear: either the TIGER texts are easier to parse, or the (original annotation and) conversion from TIGER is more consistent.", "labels": [], "entities": [{"text": "TIGER texts", "start_pos": 58, "end_pos": 69, "type": "DATASET", "confidence": 0.9246937036514282}]}, {"text": "Another surprising fact is that the dependency results are clearly better than the constituent ones.", "labels": [], "entities": []}, {"text": "This is partly due to the fact that the dependency representation is often less informative than then constituent representation.", "labels": [], "entities": []}, {"text": "One example for this can be found in coordinations: In dependency representations, the scope ambiguity in phrases like young men and women is not resolved.", "labels": [], "entities": []}, {"text": "This gives parsers fewer opportunities to go wrong.", "labels": [], "entities": []}, {"text": "However, this cannot explain all the differences.", "labels": [], "entities": []}, {"text": "Especially the better performance on the major grammatical functions cannot be explained in this way.", "labels": [], "entities": []}, {"text": "A closer look at the grammatical functions shows that here, precision and recall are higher than for constituent parses.", "labels": [], "entities": [{"text": "precision", "start_pos": 60, "end_pos": 69, "type": "METRIC", "confidence": 0.9996103644371033}, {"text": "recall", "start_pos": 74, "end_pos": 80, "type": "METRIC", "confidence": 0.9992799162864685}]}, {"text": "This is a first indication that dependency representation maybe more appropriate for languages with freer word order.", "labels": [], "entities": [{"text": "dependency representation", "start_pos": 32, "end_pos": 57, "type": "TASK", "confidence": 0.8464187979698181}]}, {"text": "A comparison between the two treebanks is inconclusive: for the accusative object, the results are similar between the treebanks.", "labels": [], "entities": []}, {"text": "For subjects, the results for TIGER are better while for dative objects, the results for T\u00fcBa-D/Z are better.", "labels": [], "entities": [{"text": "TIGER", "start_pos": 30, "end_pos": 35, "type": "METRIC", "confidence": 0.9809752702713013}]}, {"text": "This issue requires closer investigation.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: The results of the constituent parsing task.", "labels": [], "entities": [{"text": "constituent parsing task", "start_pos": 29, "end_pos": 53, "type": "TASK", "confidence": 0.7676957050959269}]}, {"text": " Table 2: The results for subjects, accusative objects, and dative objects.", "labels": [], "entities": []}, {"text": " Table 3: The results of the dependency evaluation.", "labels": [], "entities": []}]}