{"title": [], "abstractContent": [{"text": "In this talk, I present a recursive algorithm to calculate the number of rankings that are consistent with a set of data (optimal candidates) in the framework of Optimality Theory (OT; Prince and Smolensky 1993).", "labels": [], "entities": [{"text": "OT; Prince and Smolensky 1993)", "start_pos": 181, "end_pos": 211, "type": "TASK", "confidence": 0.506156461579459}]}, {"text": "1 Computing this quantity, which I call r-volume, makes possible a simple and effective Bayesian heuristic in learning-all else equal, choose candidates that are preferred by the highest number of rankings consistent with previous observations.", "labels": [], "entities": []}, {"text": "This heuristic yields an r-volume learning algorithm (RVL) that is guaranteed to make fewer thank lg k errors while learning rankings of k constraints.", "labels": [], "entities": []}, {"text": "This log-linear error bound is an improvement over the quadratic bound of Recursive Constraint Demotion (RCD; Tesar and Smolensky 1996) and it is within a logarithmic factor of the best possible mistake bound for any OT learning algorithm.", "labels": [], "entities": [{"text": "Recursive Constraint Demotion (RCD; Tesar and Smolensky 1996)", "start_pos": 74, "end_pos": 135, "type": "TASK", "confidence": 0.7671392207795923}, {"text": "OT learning algorithm", "start_pos": 217, "end_pos": 238, "type": "TASK", "confidence": 0.9371120731035868}]}, {"text": "Computing r-volume: The violations in an OT tableau can be given as a [n \u00d7 k] array of integers in which the first row t 1 corresponds to the winner.", "labels": [], "entities": []}, {"text": "Following Prince (2002), the ranking information can be extracted by comparing t 1 with each 'losing' row t 2 , ..., tn to create an Elementary Ranking Condition as follows: erc(t 1 , t i) = \ud97b\udf59\u03b1 1 , ..., \u03b1 k \ud97b\udf59 where \u03b1 j = L if t 1,j < t i,j , \u03b1 j = W if t 1,j > t i,j , and \u03b1 j = e otherwise.", "labels": [], "entities": []}, {"text": "if xi = W for all x \u2208 Er (E \u2212 w i) otherwise Mistake bounds: To make predictions, RVL selects in each tableau the candidate that yields the highest r-volume when the ERCs that allow it to win are combined with E (the ERCs for past winners).", "labels": [], "entities": [{"text": "Mistake", "start_pos": 45, "end_pos": 52, "type": "METRIC", "confidence": 0.9943088889122009}]}, {"text": "To establish a mistake bound, assume that the RVL chooses candidate e when, in fact, candidate o was optimal according to the target ranking R T.", "labels": [], "entities": []}, {"text": "Assuming e \ud97b\udf59 = o, the rankings that make o optimal must behalf or fewer of the rankings consistent with E or else RVL would have chosen o.", "labels": [], "entities": [{"text": "RVL", "start_pos": 114, "end_pos": 117, "type": "METRIC", "confidence": 0.49993282556533813}]}, {"text": "Because all rankings that make candidates other than o optimal will be eliminated once the ERCs for o are added to E, each error reduces the number of rankings consistent with all observed data by at least half and thus there can be no more than lg k!", "labels": [], "entities": [{"text": "ERCs", "start_pos": 91, "end_pos": 95, "type": "METRIC", "confidence": 0.9625999331474304}]}, {"text": "Applications: The r-volume seems to encode 'restrictiveness' in away similar to Tesar and Prince's (1999) r-measure.", "labels": [], "entities": []}, {"text": "As a factor in learning, it predicts typological frequency (cf. Bane and Riggle 2008) and priors other than the 'flat' distribution over rankings can easily be included to test models of ranking bias.", "labels": [], "entities": []}, {"text": "More generally, this research suggests the concept of g-volume for any parameterized model of grammar.", "labels": [], "entities": []}], "introductionContent": [], "datasetContent": [], "tableCaptions": []}