{"title": [{"text": "Small Statistical Models by Random Feature Mixing", "labels": [], "entities": []}], "abstractContent": [{"text": "The application of statistical NLP systems to resource constrained devices is limited by the need to maintain parameters fora large number of features and an alphabet mapping features to parameters.", "labels": [], "entities": []}, {"text": "We introduce random feature mixing to eliminate alphabet storage and reduce the number of parameters without severely impacting model performance.", "labels": [], "entities": []}], "introductionContent": [{"text": "Statistical NLP learning systems are used for many applications but have large memory requirements, a serious problem for mobile platforms.", "labels": [], "entities": []}, {"text": "Since NLP applications use high dimensional models, a large alphabet is required to map between features and model parameters.", "labels": [], "entities": []}, {"text": "Practically, this means storing every observed feature string in memory, a prohibitive cost for systems with constrained resources.", "labels": [], "entities": []}, {"text": "Offline feature selection is a possible solution, but still requires an alphabet and eliminates the potential for learning new features after deployment, an important property for adaptive e-mail or SMS prediction and personalization tasks.", "labels": [], "entities": [{"text": "Offline feature selection", "start_pos": 0, "end_pos": 25, "type": "TASK", "confidence": 0.6834041674931844}, {"text": "SMS prediction", "start_pos": 199, "end_pos": 213, "type": "TASK", "confidence": 0.6992180347442627}]}, {"text": "We propose a simple and effective approach to eliminate the alphabet and reduce the problem of dimensionality through random feature mixing.", "labels": [], "entities": []}, {"text": "We explore this method on a variety of popular datasets and classification algorithms.", "labels": [], "entities": []}, {"text": "In addition to alphabet elimination, this reduces model size by a factor of 5-10 without a significant loss in performance.", "labels": [], "entities": [{"text": "alphabet elimination", "start_pos": 15, "end_pos": 35, "type": "TASK", "confidence": 0.9202344417572021}]}], "datasetContent": [{"text": "We evaluated the effect of random feature mixing on four popular learning methods: Perceptron, MIRA), SVM and Maximum entropy; with 4 NLP datasets: 20 Newsgroups 1 , Reuters (), Sentiment ( and).", "labels": [], "entities": [{"text": "MIRA", "start_pos": 95, "end_pos": 99, "type": "METRIC", "confidence": 0.9126744270324707}]}, {"text": "For each dataset we extracted binary unigram features and sentiment was prepared according to.", "labels": [], "entities": []}, {"text": "From 20 Newsgroups we created 3 binary decision tasks to differentiate between two similar labels from computers, science and talk.", "labels": [], "entities": []}, {"text": "We created 3 similar problems from Reuters from insurance, business services and retail distribution.", "labels": [], "entities": [{"text": "Reuters", "start_pos": 35, "end_pos": 42, "type": "DATASET", "confidence": 0.9096346497535706}]}, {"text": "Sentiment used 4 Amazon domains (book, dvd, electronics, kitchen).", "labels": [], "entities": []}, {"text": "Spam used the three users from task A data.", "labels": [], "entities": [{"text": "Spam", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.9209249019622803}]}, {"text": "Each problem had 2000 instances except for 20 Newsgroups, which used between 1850 and 1971 instances.", "labels": [], "entities": []}, {"text": "This created 13 binary classification problems across four tasks.", "labels": [], "entities": []}, {"text": "Each model was evaluated on all problems using 10-fold cross validation and parameter optimization.", "labels": [], "entities": []}, {"text": "Experiments varied model size to observe the effect of feature collisions on performance.", "labels": [], "entities": []}, {"text": "Results for sentiment classification of kitchen appliance reviews (figure 1) are typical.", "labels": [], "entities": [{"text": "sentiment classification of kitchen appliance reviews", "start_pos": 12, "end_pos": 65, "type": "TASK", "confidence": 0.8700086176395416}]}, {"text": "The original model has roughly 93.6k features and its alphabet requires 1.3MB of storage.", "labels": [], "entities": []}, {"text": "Assuming 4-byte floating point numbers the weight vector needs under 0.37MB.", "labels": [], "entities": []}, {"text": "Consequently our method reduces storage by over 78% when we keep the number of parameters constant.", "labels": [], "entities": []}, {"text": "A further reduction by a factor of 2 decreases accuracy by only 2%.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 47, "end_pos": 55, "type": "METRIC", "confidence": 0.9993775486946106}]}, {"text": "shows the results of all experiments for SVM and MIRA.", "labels": [], "entities": [{"text": "SVM", "start_pos": 41, "end_pos": 44, "type": "TASK", "confidence": 0.5006678104400635}, {"text": "MIRA", "start_pos": 49, "end_pos": 53, "type": "METRIC", "confidence": 0.41350898146629333}]}, {"text": "Each curve shows normalized dataset performance relative to the full model as the percentage of original features decrease.", "labels": [], "entities": []}, {"text": "The shaded rectangle extends one standard deviation above and   below full model performance.", "labels": [], "entities": []}, {"text": "Almost all datasets perform within one standard deviation of the full model when using feature mixing set to the total number of features for the problem, indicating that alphabet elimination is possible without hurting performance.", "labels": [], "entities": [{"text": "alphabet elimination", "start_pos": 171, "end_pos": 191, "type": "TASK", "confidence": 0.7299387454986572}]}, {"text": "One dataset (Reuters retail distribution) is a notable exception and is illustrated in detail in figure 3.", "labels": [], "entities": [{"text": "Reuters retail distribution)", "start_pos": 13, "end_pos": 41, "type": "DATASET", "confidence": 0.9141586869955063}]}, {"text": "We believe the small total number of features used for this problem is the source of this behavior.", "labels": [], "entities": []}, {"text": "On the vast majority of datasets, our method can reduce the size of the weight vector and eliminate the alphabet without any feature selection or changes to the learning algorithm.", "labels": [], "entities": []}, {"text": "When reducing weight vector size by a factor of 10, we still obtain between 96.7% and 97.4% of the performance of the original model, depending on the learning algorithm.", "labels": [], "entities": []}, {"text": "If we eliminate the alphabet but keep the same size weight vector, model the performance is between 99.3% of the original for MIRA and a slight improvement for Perceptron.", "labels": [], "entities": [{"text": "MIRA", "start_pos": 126, "end_pos": 130, "type": "METRIC", "confidence": 0.6380530595779419}, {"text": "Perceptron", "start_pos": 160, "end_pos": 170, "type": "DATASET", "confidence": 0.9370068311691284}]}, {"text": "The batch learning methods are between those two extremes at 99.4 and 99.5 for maximum entropy and SVM respectively.", "labels": [], "entities": []}, {"text": "Feature mixing yields substantial reductions in memory requirements with a minimal performance loss, a promising result for resource constrained devices.", "labels": [], "entities": []}], "tableCaptions": []}