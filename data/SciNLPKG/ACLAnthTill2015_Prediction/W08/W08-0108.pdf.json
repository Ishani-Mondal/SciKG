{"title": [{"text": "Rapidly Deploying Grammar-Based Speech Applications with Active Learning and Back-off Grammars", "labels": [], "entities": []}], "abstractContent": [{"text": "Grammar-based approaches to spoken language understanding are utilized to a great extent in industry, particularly when developers are confronted with data sparsity.", "labels": [], "entities": [{"text": "spoken language understanding", "start_pos": 28, "end_pos": 57, "type": "TASK", "confidence": 0.718937357266744}]}, {"text": "In order to ensure wide grammar coverage, developers typically modify their grammars in an iterative process of deploying the application, collecting and transcribing user utterances, and adjusting the grammar.", "labels": [], "entities": []}, {"text": "In this paper, we explore enhancing this iterative process by leve-raging active learning with back-off grammars.", "labels": [], "entities": []}, {"text": "Because the back-off grammars expand coverage of user utterances, developers have a safety net for deploying applications earlier.", "labels": [], "entities": []}, {"text": "Furthermore, the statistics related to the back-off can be used for active learning, thus reducing the effort and cost of data transcription.", "labels": [], "entities": [{"text": "data transcription", "start_pos": 122, "end_pos": 140, "type": "TASK", "confidence": 0.7267928719520569}]}, {"text": "In experiments conducted on a commercially deployed application, the approach achieved levels of semantic accuracy comparable to transcribing all failed utterances with 87% less transcriptions.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 106, "end_pos": 114, "type": "METRIC", "confidence": 0.9714815616607666}]}], "introductionContent": [{"text": "Although research in spoken language understanding is typically pursued from a statistical perspective, grammar-based approaches are utilized to a great extent in industry ().", "labels": [], "entities": [{"text": "spoken language understanding", "start_pos": 21, "end_pos": 50, "type": "TASK", "confidence": 0.7116331060727438}]}, {"text": "Speech recognition grammars are often manually authored and iteratively modified as follows: Typically, context-free grammars (CFG) are written in a format such as Speech Recognition Grammar Specification (SRGS) and deployed.", "labels": [], "entities": [{"text": "Speech recognition grammars", "start_pos": 0, "end_pos": 27, "type": "TASK", "confidence": 0.8488296469052633}, {"text": "Speech Recognition Grammar Specification (SRGS)", "start_pos": 164, "end_pos": 211, "type": "TASK", "confidence": 0.7436558476516179}]}, {"text": "Once user utterances are collected and transcribed, the grammars are then adjusted to improve their coverage.", "labels": [], "entities": []}, {"text": "This process continues until minimal OOG utterances are observed.", "labels": [], "entities": [{"text": "OOG utterances", "start_pos": 37, "end_pos": 51, "type": "TASK", "confidence": 0.7068462669849396}]}, {"text": "In this paper, we explore enhancing this iterative process of grammar modification by combining back-off grammars, which expand coverage of user utterances, with active learning, which reduces \"the number of training examples to be labeled by automatically processing unlabeled examples, and then selecting the most informative ones with respect to a specified cost function fora human to label\").", "labels": [], "entities": [{"text": "grammar modification", "start_pos": 62, "end_pos": 82, "type": "TASK", "confidence": 0.7477105557918549}]}, {"text": "This paper comprises three sections.", "labels": [], "entities": []}, {"text": "In Section 2, we describe our overall approach to rapid application development (RAD).", "labels": [], "entities": [{"text": "rapid application development (RAD)", "start_pos": 50, "end_pos": 85, "type": "TASK", "confidence": 0.7856742441654205}]}, {"text": "In Section 3, we explain how data transcription can be reduced by leveraging active learning based on statistics related to the usage of back-off grammars.", "labels": [], "entities": [{"text": "data transcription", "start_pos": 29, "end_pos": 47, "type": "TASK", "confidence": 0.7698408663272858}]}, {"text": "Finally, in Section 4, we evaluate the active learning approach with simulation experiments conducted on data collected from a commercial grammar-based speech application.", "labels": [], "entities": []}], "datasetContent": [{"text": "For evaluation, we used utterances collected from 204 users of Microsoft Voice Command, a grammar-based command-and-control (C&C) application for high-end mobile devices (see Paek et al., 2007 for details).", "labels": [], "entities": []}, {"text": "We partitioned 5061 transcribed utterances into five sets, one of which was used exclusively for testing.", "labels": [], "entities": []}, {"text": "The remaining four were used for iterative CFG modification.", "labels": [], "entities": [{"text": "CFG modification", "start_pos": 43, "end_pos": 59, "type": "TASK", "confidence": 0.8764181137084961}]}, {"text": "For the first iteration, we started with a CFG which was a degraded version of the grammar currently shipped with the Voice Command product.", "labels": [], "entities": []}, {"text": "It was obtained by using the mode, or the most frequent user utterance, for each CFG rule.", "labels": [], "entities": [{"text": "CFG", "start_pos": 81, "end_pos": 84, "type": "DATASET", "confidence": 0.8816328644752502}]}, {"text": "We compared two approaches: CFG_Full, where each iterative CFG was modified using the full set of transcribed utterances that resulted in a failure state (i.e., when a false recognition event occurred or the phrase confidence score fell below 45%, which was set by a proprietary tuning procedure for optimizing worderror rate), and CFG_Active, where each iterative CFG was modified using only those transcribed utterances corresponding to the most frequently occurring CFG back-off rules.", "labels": [], "entities": [{"text": "phrase confidence score", "start_pos": 208, "end_pos": 231, "type": "METRIC", "confidence": 0.7375710606575012}]}, {"text": "For both CFG_Full and CFG_Active, CFG i was modified using the same set of heuristics akin to minimal edit distance.", "labels": [], "entities": [{"text": "CFG_Active", "start_pos": 22, "end_pos": 32, "type": "DATASET", "confidence": 0.8294060230255127}]}, {"text": "In order to assess the value of using the back-off grammar as a safety net, we also compared CFG_Full+Back-off, where a derived CFG back-off was utilized whenever a failure state occurred with CFG_Full, and CFG_Active+Back-off, where again a CFG back-off was utilized, this time with the back-off derived from the CFG trained on selective utterances.", "labels": [], "entities": [{"text": "CFG_Full", "start_pos": 193, "end_pos": 201, "type": "DATASET", "confidence": 0.8809351523717245}]}, {"text": "As our metric, we evaluated semantic accuracy since that is what matters most in C&C settings.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 37, "end_pos": 45, "type": "METRIC", "confidence": 0.9708073139190674}]}, {"text": "Furthermore, because recognition of part of an utterance can increase the odds of ultimately achieving task completion (, we carried out separate evaluations for the functional constituents of a C&C utterance (i.e., keyword and slot) as well as the complete phrase (keyword + slot).", "labels": [], "entities": []}, {"text": "We computed accuracy as follows: For any single utterance, the recognizer can either accept or reject it.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 12, "end_pos": 20, "type": "METRIC", "confidence": 0.9992306232452393}]}, {"text": "If it is accepted, then the semantics of the utterance can either be correct (i.e., it matches what the user intended) or incorrect, hence: where CA denotes accepted commands that are correct, IA denotes accepted commands that are incorrect, and R denotes the number of rejections.", "labels": [], "entities": [{"text": "IA", "start_pos": 193, "end_pos": 195, "type": "METRIC", "confidence": 0.9542792439460754}]}, {"text": "displays semantic accuracies for both CFG_Full and CFG_Active.", "labels": [], "entities": [{"text": "CFG_Full", "start_pos": 38, "end_pos": 46, "type": "DATASET", "confidence": 0.8686060905456543}, {"text": "CFG_Active", "start_pos": 51, "end_pos": 61, "type": "DATASET", "confidence": 0.8489968578020731}]}, {"text": "Standard errors about the mean were computed using the jacknife procedure with 10 re-samples.", "labels": [], "entities": []}, {"text": "Notice that both CFG_Full and CFG_Active initially have the same accuracy levels because they start off with the same degraded CFG.", "labels": [], "entities": [{"text": "CFG_Full", "start_pos": 17, "end_pos": 25, "type": "DATASET", "confidence": 0.8163086374600729}, {"text": "CFG_Active", "start_pos": 30, "end_pos": 40, "type": "DATASET", "confidence": 0.7934809525807699}, {"text": "accuracy", "start_pos": 65, "end_pos": 73, "type": "METRIC", "confidence": 0.9990522265434265}, {"text": "CFG", "start_pos": 127, "end_pos": 130, "type": "DATASET", "confidence": 0.9481320977210999}]}, {"text": "The highest accuracies obtained almost always occurred in the second iteration after modifying the CFG with the first batch of transcriptions.", "labels": [], "entities": [{"text": "accuracies", "start_pos": 12, "end_pos": 22, "type": "METRIC", "confidence": 0.9939217567443848}, {"text": "CFG", "start_pos": 99, "end_pos": 102, "type": "DATASET", "confidence": 0.9177056550979614}]}, {"text": "Thereafter, all accuracies seem to decrease.", "labels": [], "entities": [{"text": "accuracies", "start_pos": 16, "end_pos": 26, "type": "METRIC", "confidence": 0.9970749616622925}]}, {"text": "In order to understand why this would be case, we computed the coverage of the i th CFG on the holdout set.", "labels": [], "entities": [{"text": "coverage", "start_pos": 63, "end_pos": 71, "type": "METRIC", "confidence": 0.9681407809257507}, {"text": "CFG", "start_pos": 84, "end_pos": 87, "type": "DATASET", "confidence": 0.9746952056884766}]}, {"text": "This is reported in the \"OOG%\" column.", "labels": [], "entities": [{"text": "OOG%\" column", "start_pos": 25, "end_pos": 37, "type": "METRIC", "confidence": 0.9538135329882304}]}, {"text": "Comparing CFG_Full to CFG_Active on keyword + slot accuracy, CFG_Full decreases inaccuracy after the second iteration as does CFG_Active.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 51, "end_pos": 59, "type": "METRIC", "confidence": 0.9265803694725037}, {"text": "CFG_Active", "start_pos": 126, "end_pos": 136, "type": "DATASET", "confidence": 0.8704392313957214}]}, {"text": "However, the OOG% of CFG_Full is much lower than CFG_Active.", "labels": [], "entities": [{"text": "OOG", "start_pos": 13, "end_pos": 16, "type": "METRIC", "confidence": 0.9994264841079712}, {"text": "CFG_Full", "start_pos": 21, "end_pos": 29, "type": "DATASET", "confidence": 0.9044095873832703}, {"text": "CFG_Active", "start_pos": 49, "end_pos": 59, "type": "DATASET", "confidence": 0.8902608752250671}]}, {"text": "In fact, it seems to level off after the second iteration, suggesting that perhaps the decrease in accuracies reflects the increase in grammar perplexity; that is, as the grammar covers more of the utterances, it has more hypotheses to consider, and as a result, performs slightly worse.", "labels": [], "entities": [{"text": "accuracies", "start_pos": 99, "end_pos": 109, "type": "METRIC", "confidence": 0.9956423044204712}]}, {"text": "Interestingly, after the last iteration, CFG_Active for keyword + slot and slot accuracies was slightly higher (69.06%) than CFG_Full (66.88%) (p = .05).", "labels": [], "entities": [{"text": "CFG", "start_pos": 41, "end_pos": 44, "type": "DATASET", "confidence": 0.8690233826637268}, {"text": "CFG_Full", "start_pos": 125, "end_pos": 133, "type": "DATASET", "confidence": 0.892022947470347}]}, {"text": "Furthermore, this was done with 193 utterances as opposed to 1393, or 87% less transcriptions.", "labels": [], "entities": []}, {"text": "For keyword accuracy, CFG_Active (64.09%) was slightly worse than CFG_Full (66.10%) (p < .05).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 12, "end_pos": 20, "type": "METRIC", "confidence": 0.9908305406570435}, {"text": "CFG_Active", "start_pos": 22, "end_pos": 32, "type": "DATASET", "confidence": 0.7535132765769958}, {"text": "CFG_Full", "start_pos": 66, "end_pos": 74, "type": "DATASET", "confidence": 0.877629558245341}]}, {"text": "With respect to the value of having a back-off grammar as a safety net, we found that both CFG_Full and CFG_Active achieved much higher accuracies with the back-off for keyword, slot, and keyword + slot accuracies.", "labels": [], "entities": [{"text": "CFG_Full", "start_pos": 91, "end_pos": 99, "type": "DATASET", "confidence": 0.8835680286089579}, {"text": "CFG_Active", "start_pos": 104, "end_pos": 114, "type": "DATASET", "confidence": 0.8535986940066019}]}, {"text": "Notice also that the differences between CFG_Full and CFG_Active after the last iteration were much closer to each other than without the back-off, suggesting applications should always be deployed with a back-off.", "labels": [], "entities": [{"text": "CFG_Full", "start_pos": 41, "end_pos": 49, "type": "DATASET", "confidence": 0.8777404626210531}, {"text": "CFG_Active", "start_pos": 54, "end_pos": 64, "type": "DATASET", "confidence": 0.8683408300081888}]}], "tableCaptions": []}