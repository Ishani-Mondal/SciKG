{"title": [{"text": "Fast, Easy, and Cheap: Construction of Statistical Machine Translation Models with MapReduce", "labels": [], "entities": [{"text": "Statistical Machine Translation", "start_pos": 39, "end_pos": 70, "type": "TASK", "confidence": 0.7038399676481882}]}], "abstractContent": [{"text": "In recent years, the quantity of parallel training data available for statistical machine translation has increased far more rapidly than the performance of individual computers, resulting in a potentially serious impediment to progress.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 70, "end_pos": 101, "type": "TASK", "confidence": 0.7435666720072428}]}, {"text": "Parallelization of the model-building algorithms that process this data on computer clusters is fraught with challenges such as synchronization, data exchange, and fault tolerance.", "labels": [], "entities": [{"text": "data exchange", "start_pos": 145, "end_pos": 158, "type": "TASK", "confidence": 0.6865152567625046}, {"text": "fault tolerance", "start_pos": 164, "end_pos": 179, "type": "TASK", "confidence": 0.7107128351926804}]}, {"text": "However, the MapReduce programming paradigm has recently emerged as one solution to these issues: a powerful functional abstraction hides system-level details from the researcher, allowing programs to be transparently distributed across potentially very large clusters of commodity hardware.", "labels": [], "entities": []}, {"text": "We describe MapReduce implementations of two algorithms used to estimate the parameters for two word alignment models and one phrase-based translation model, all of which rely on maximum likelihood probability estimates.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 96, "end_pos": 110, "type": "TASK", "confidence": 0.718524158000946}]}, {"text": "On a 20-machine cluster, experimental results show that our solutions exhibit good scaling characteristics compared to a hypothetical , optimally-parallelized version of current state-of-the-art single-core tools.", "labels": [], "entities": []}], "introductionContent": [{"text": "Like many other NLP problems, output quality of statistical machine translation (SMT) systems increases with the amount of training data.", "labels": [], "entities": [{"text": "statistical machine translation (SMT)", "start_pos": 48, "end_pos": 85, "type": "TASK", "confidence": 0.7773017187913259}]}, {"text": "demonstrated that increasing the quantity of training data used for language modeling significantly improves the translation quality of an ArabicEnglish MT system, even with far less sophisticated backoff models.", "labels": [], "entities": [{"text": "language modeling", "start_pos": 68, "end_pos": 85, "type": "TASK", "confidence": 0.706176832318306}, {"text": "ArabicEnglish MT", "start_pos": 139, "end_pos": 155, "type": "TASK", "confidence": 0.5605108141899109}]}, {"text": "However, the steadily increasing quantities of training data do not come without cost.", "labels": [], "entities": []}, {"text": "shows the relationship between the amount of parallel Arabic-English training data used and both the translation quality of a state-ofthe-art phrase-based SMT system and the time required to perform the training with the widely-used Moses toolkit on a commodity server.", "labels": [], "entities": [{"text": "SMT", "start_pos": 155, "end_pos": 158, "type": "TASK", "confidence": 0.8792750835418701}]}, {"text": "1 Building a model using 5M sentence pairs (the amount of Arabic-English parallel text publicly available from the LDC) takes just over two days.", "labels": [], "entities": []}, {"text": "This represents an unfortunate state of affairs for the research community: excessively long turnaround on experiments is an impediment to research progress.", "labels": [], "entities": []}, {"text": "It is clear that the needs of machine translation researchers have outgrown the capabilities of individual computers.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 30, "end_pos": 49, "type": "TASK", "confidence": 0.7527941763401031}]}, {"text": "The only practical recourse is to distribute the computation across multiple cores, processors, or machines.", "labels": [], "entities": []}, {"text": "The development of parallel algorithms involves a number of tradeoffs.", "labels": [], "entities": []}, {"text": "First is that of cost: a decision must be made between \"exotic\" hardware (e.g., large shared memory machines, InfiniBand interconnect) and commodity hardware.", "labels": [], "entities": []}, {"text": "There is significant evidence ( that solutions based on the latter are more cost effective (and for resource-constrained academic institutions, often the only option).", "labels": [], "entities": []}, {"text": "Given appropriate hardware, MT researchers must still contend with the challenge of developing software.", "labels": [], "entities": [{"text": "MT", "start_pos": 28, "end_pos": 30, "type": "TASK", "confidence": 0.9899771213531494}]}, {"text": "Quite simply, parallel programming is difficult.", "labels": [], "entities": []}, {"text": "Due to communication and synchronization issues, concurrent operations are notoriously challenging to reason about.", "labels": [], "entities": []}, {"text": "In addition, fault tolerance and scalability are serious concerns on commodity hardware prone to failure.", "labels": [], "entities": []}, {"text": "With traditional parallel programming models (e.g., MPI), the developer shoulders the burden of handling these issues.", "labels": [], "entities": []}, {"text": "As a result, just as much (if not more) effort is devoted to system issues as to solving the actual problem.", "labels": [], "entities": []}, {"text": "Recently, Google's MapReduce framework) has emerged as an attractive alternative to existing parallel programming models.", "labels": [], "entities": []}, {"text": "The MapReduce abstraction shields the programmer from having to explicitly worry about system-level issues such as synchronization, data exchange, and fault tolerance (see Section 2 for details).", "labels": [], "entities": [{"text": "data exchange", "start_pos": 132, "end_pos": 145, "type": "TASK", "confidence": 0.7006451785564423}]}, {"text": "The runtime is able to transparently distribute computations across large clusters of commodity hardware with good scaling characteristics.", "labels": [], "entities": []}, {"text": "This frees the programmer to focus on actual MT issues.", "labels": [], "entities": [{"text": "MT", "start_pos": 45, "end_pos": 47, "type": "TASK", "confidence": 0.993463397026062}]}, {"text": "In this paper we present MapReduce implementations of training algorithms for two kinds of models commonly used in statistical MT today: a phrasebased translation model ( and word alignment models based on pairwise lexical translation trained using expectation maximization.", "labels": [], "entities": [{"text": "statistical MT", "start_pos": 115, "end_pos": 129, "type": "TASK", "confidence": 0.5983603894710541}, {"text": "phrasebased translation", "start_pos": 139, "end_pos": 162, "type": "TASK", "confidence": 0.6888179779052734}]}, {"text": "Currently, such models take days to construct using standard tools with publicly available training corpora; our MapReduce implementation cuts this time to hours.", "labels": [], "entities": []}, {"text": "As an benefit to the community, it is our intention to release this code under an open source license.", "labels": [], "entities": []}, {"text": "It is worthwhile to emphasize that we present these results as a \"sweet spot\" in the complex design space of engineering decisions.", "labels": [], "entities": []}, {"text": "In light of possible tradeoffs, we argue that our solution can be considered fast (in terms of running time), easy (in terms of implementation), and cheap (in terms of hardware costs).", "labels": [], "entities": []}, {"text": "Faster running times could be achieved with more expensive hardware.", "labels": [], "entities": []}, {"text": "Similarly, a custom implementation (e.g., in MPI) could extract finergrained parallelism and also yield faster running times.", "labels": [], "entities": []}, {"text": "In our opinion, these are not worthwhile tradeoffs.", "labels": [], "entities": []}, {"text": "In the first case, financial constraints are obvious.", "labels": [], "entities": []}, {"text": "In the second case, the programmer must explicitly manage all the complexities that come with distributed processing (see above).", "labels": [], "entities": []}, {"text": "In contrast, our algorithms were developed within a matter of weeks, as part of a \"cloud computing\" course project.", "labels": [], "entities": []}, {"text": "Experimental results demonstrate that MapReduce provides nearly optimal scaling characteristics, while retaining a highlevel problem-focused abstraction.", "labels": [], "entities": []}, {"text": "The remainder of the paper is structured as follows.", "labels": [], "entities": []}, {"text": "In the next section we provide an overview of MapReduce.", "labels": [], "entities": []}, {"text": "In Section 3 we describe several general solutions to computing maximum likelihood estimates for finite, discrete probability distributions.", "labels": [], "entities": []}, {"text": "Sections 4 and 5 apply these techniques to estimate phrase translation models and perform EM for two word alignment models.", "labels": [], "entities": [{"text": "phrase translation", "start_pos": 52, "end_pos": 70, "type": "TASK", "confidence": 0.7602420747280121}, {"text": "EM", "start_pos": 90, "end_pos": 92, "type": "METRIC", "confidence": 0.6551347970962524}, {"text": "word alignment", "start_pos": 101, "end_pos": 115, "type": "TASK", "confidence": 0.7310331463813782}]}, {"text": "Section 6 reviews relevant prior work, and Section 7 concludes.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": []}