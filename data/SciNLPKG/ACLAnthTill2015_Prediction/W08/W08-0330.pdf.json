{"title": [{"text": "The Role of Pseudo References in MT Evaluation", "labels": [], "entities": [{"text": "Pseudo References", "start_pos": 12, "end_pos": 29, "type": "TASK", "confidence": 0.6888592690229416}, {"text": "MT Evaluation", "start_pos": 33, "end_pos": 46, "type": "TASK", "confidence": 0.9808309376239777}]}], "abstractContent": [{"text": "Previous studies have shown automatic evaluation metrics to be more reliable when compared against many human translations.", "labels": [], "entities": []}, {"text": "However , multiple human references may not always be available.", "labels": [], "entities": []}, {"text": "It is more common to have only a single human reference (extracted from parallel texts) or no reference at all.", "labels": [], "entities": []}, {"text": "Our earlier work suggested that one way to address this problem is to train a metric to evaluate a sentence by comparing it against pseudo references , or imperfect \"references\" produced by off-the-shelf MT systems.", "labels": [], "entities": []}, {"text": "In this paper, we further examine the approach both in terms of the training methodology and in terms of the role of the human and pseudo references.", "labels": [], "entities": []}, {"text": "Our expanded experiments show that the approach generalizes well across multiple years and different source languages.", "labels": [], "entities": []}], "introductionContent": [{"text": "Standard automatic metrics are reference-based; that is, they compare system-produced translations against human-translated references produced for the same source.", "labels": [], "entities": []}, {"text": "Since there is usually no single best way to translate a sentence, each MT output should be compared against many references.", "labels": [], "entities": [{"text": "MT", "start_pos": 72, "end_pos": 74, "type": "TASK", "confidence": 0.9654428958892822}]}, {"text": "On the other hand, creating multiple human references is itself a costly process.", "labels": [], "entities": []}, {"text": "For many naturally occurring datasets (e.g., parallel corpora) only a single reference is readily available.", "labels": [], "entities": []}, {"text": "The focus of this work is on developing automatic metrics for sentence-level evaluation with at most one human reference.", "labels": [], "entities": []}, {"text": "One way to supplement the single human reference is to use pseudo references, or sentences produced by off-the-shelf MT systems, as stand-ins for human references.", "labels": [], "entities": []}, {"text": "However, since pseudo references maybe imperfect translations themselves, the comparisons cannot be fully trusted.", "labels": [], "entities": []}, {"text": "Previously, we have taken a learningbased approach to develop a composite metric that combines measurements taken from multiple pseudo references.", "labels": [], "entities": []}, {"text": "Experimental results suggested the approach to be promising; but those studies did not consider how well the metric might generalize across multiple years and different languages.", "labels": [], "entities": []}, {"text": "In this paper, we investigate the applicability of the pseudo-reference metrics under these more general conditions.", "labels": [], "entities": []}, {"text": "Using the WMT06 Workshop shared-task results () as training examples, we train a metric that evaluates new sentences by comparing them against pseudo references produced by three off-the-shelf MT systems.", "labels": [], "entities": [{"text": "WMT06 Workshop shared-task results", "start_pos": 10, "end_pos": 44, "type": "DATASET", "confidence": 0.9357023686170578}]}, {"text": "We apply the learned metric to sentences from the WMT07 shared-task) and compare the metric's predictions against human judgments.", "labels": [], "entities": [{"text": "WMT07", "start_pos": 50, "end_pos": 55, "type": "DATASET", "confidence": 0.9009097814559937}]}, {"text": "We find that additional pseudo references improve correlations for automatic metrics.", "labels": [], "entities": [{"text": "correlations", "start_pos": 50, "end_pos": 62, "type": "METRIC", "confidence": 0.9463236927986145}]}], "datasetContent": [{"text": "For the experiments reported in this paper, we used human-evaluated MT sentences from past sharedtasks of the WMT 2006 and WMT 2007.", "labels": [], "entities": [{"text": "MT sentences", "start_pos": 68, "end_pos": 80, "type": "TASK", "confidence": 0.9030447900295258}, {"text": "WMT 2006 and WMT 2007", "start_pos": 110, "end_pos": 131, "type": "DATASET", "confidence": 0.8658198237419128}]}, {"text": "The data consists of outputs from German-English, SpanishEnglish, and French-English MT systems.", "labels": [], "entities": []}, {"text": "The outputs are translations from two corpora: Europarl and news commentary.", "labels": [], "entities": [{"text": "Europarl", "start_pos": 47, "end_pos": 55, "type": "DATASET", "confidence": 0.9768416285514832}]}, {"text": "System outputs have been evaluated by human judges on a 5-point scale).", "labels": [], "entities": []}, {"text": "We have normalized scores to reduce biases from different judges (.", "labels": [], "entities": []}, {"text": "We experimented with using four different subsets of the WMT2006 data as training examples: only German-English, only Spanish-English, only French-English, all 06 data.", "labels": [], "entities": [{"text": "WMT2006 data", "start_pos": 57, "end_pos": 69, "type": "DATASET", "confidence": 0.9747868478298187}]}, {"text": "The metrics are trained using support vector regression with a Gaussian kernel as implemented in the SVM-Light package).", "labels": [], "entities": []}, {"text": "The SVM parameters are tuned via grid-search on development data, 20% of the full training set that has been reserved for this purpose.", "labels": [], "entities": []}, {"text": "We used three MT systems to generate pseudo references: Systran 1 , GoogleMT 2 , and Moses (.", "labels": [], "entities": [{"text": "GoogleMT", "start_pos": 68, "end_pos": 76, "type": "DATASET", "confidence": 0.8731586337089539}]}, {"text": "We chose these three systems because they are widely accessible and because they take relatively different approaches.", "labels": [], "entities": []}, {"text": "Moreover, although they have not all been human-evaluated in the past WMT shared tasks, they are well-known for producing good translations.", "labels": [], "entities": [{"text": "WMT shared tasks", "start_pos": 70, "end_pos": 86, "type": "TASK", "confidence": 0.7966739137967428}]}, {"text": "A metric is evaluated based on its Spearman rank correlation coefficient between the scores it gave to the evaluative dataset and human assessments for the same data.", "labels": [], "entities": [{"text": "Spearman rank correlation coefficient", "start_pos": 35, "end_pos": 72, "type": "METRIC", "confidence": 0.7354686036705971}]}, {"text": "The correlation coefficient is areal number between -1, indicating perfect negative correlations, and +1, indicating perfect positive correlations.", "labels": [], "entities": [{"text": "correlation coefficient", "start_pos": 4, "end_pos": 27, "type": "METRIC", "confidence": 0.9797755479812622}, {"text": "areal number", "start_pos": 31, "end_pos": 43, "type": "METRIC", "confidence": 0.9521516263484955}]}, {"text": "Two standard reference-based metrics, BLEU () and METEOR (, are used for comparisons.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 38, "end_pos": 42, "type": "METRIC", "confidence": 0.9988464117050171}, {"text": "METEOR", "start_pos": 50, "end_pos": 56, "type": "METRIC", "confidence": 0.9839814305305481}]}, {"text": "BLEU is smoothed (, and it considers only matching up to bigrams because this has higher correlations with human judgments than when higherordered n-grams are included.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.9891237020492554}]}], "tableCaptions": [{"text": " Table 1: Correlation comparisons of metrics (columns) using different references (row): a single human reference  (1HR), 3 pseudo references (3PR), or all (1HR+3PR). The type of training used for the regression-trained metrics  are specified in parentheses. For each evaluated corpus, correlations higher than standard metric using one human  reference are marked by an asterisk(*).", "labels": [], "entities": []}]}