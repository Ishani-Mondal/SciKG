{"title": [{"text": "Encoding Tree Pair-based Graphs in Learning Algorithms: the Textual Entailment Recognition Case", "labels": [], "entities": [{"text": "Textual Entailment Recognition", "start_pos": 60, "end_pos": 90, "type": "TASK", "confidence": 0.7042457262674967}]}], "abstractContent": [{"text": "In this paper, we provide a statistical machine learning representation of textual en-tailment via syntactic graphs constituted by tree pairs.", "labels": [], "entities": []}, {"text": "We show that the natural way of representing the syntactic relations between text and hypothesis consists in the huge feature space of all possible syntactic tree fragment pairs, which can only be managed using kernel methods.", "labels": [], "entities": []}, {"text": "Experiments with Support Vector Machines and our new kernels for paired trees show the validity of our interpretation.", "labels": [], "entities": []}], "introductionContent": [{"text": "Recently, a lot of valuable work on the recognition of textual entailment (RTE) has been carried out).", "labels": [], "entities": [{"text": "recognition of textual entailment (RTE)", "start_pos": 40, "end_pos": 79, "type": "TASK", "confidence": 0.8747950111116681}]}, {"text": "The aim is to detect implications between sentences like: T1 \u21d2 H1 T1 \"Wanadoo bought KStones\" H1 \" where T 1 and H 1 stand for text and hypothesis, respectively.", "labels": [], "entities": [{"text": "Wanadoo bought KStones", "start_pos": 70, "end_pos": 92, "type": "DATASET", "confidence": 0.9442438880602518}]}, {"text": "Several models, ranging from the simple lexical similarity between T and H to advanced Logic Form Representations, have been proposed).", "labels": [], "entities": []}, {"text": "However, since a linguistic theory able to analytically show how to computationally solve the RTE problem has not been developed yet, to design accurate systems, we should rely upon the application of machine learning.", "labels": [], "entities": [{"text": "RTE problem", "start_pos": 94, "end_pos": 105, "type": "TASK", "confidence": 0.9298605918884277}]}, {"text": "In this perspective, TE training examples have to be represented in terms of statistical feature distributions.", "labels": [], "entities": [{"text": "TE training", "start_pos": 21, "end_pos": 32, "type": "TASK", "confidence": 0.9145559072494507}]}, {"text": "These typically consist in word sequences (along with their lexical similarity) and the syntactic structures of both text and hypothesis (e.g. their parse trees).", "labels": [], "entities": []}, {"text": "The interesting aspect with respect to other natural language problems is that, in TE, features useful at describing an example are composed by pairs of features from Text and Hypothesis.", "labels": [], "entities": []}, {"text": "For example, using a word representation, a text and hypothesis pair, T, H, can be represented by the sequences of words of the two sentences, i.e. t 1 , .., tn and h 1 , .., h m , respectively.", "labels": [], "entities": []}, {"text": "If we carryout a blind and complete statistical correlation analysis of the two sequences, the entailment property would be described by the set of subsequence pairs from T and H, i.e. the set R = {s t , sh : st = ti 1 , .., ti l , sh = h j 1 , .., h jr , l \u2264 n, r \u2264 m}.", "labels": [], "entities": []}, {"text": "The relation set R constitutes a naive and complete representation of the example T, H in the feature space {v, w : where V is the corpus vocabulary . Although the above representation is correct and complete from a statistically point of view, it suffers from two practical drawbacks: (a) it is exponential in V and (b) it is subject to high degree of data sparseness which may prevent to carryout effective learning.", "labels": [], "entities": []}, {"text": "The traditional solution for this problem relates to consider the syntactic structure of word sequences which provides their generalization.", "labels": [], "entities": []}, {"text": "The use of syntactic trees poses the problem of representing structures in learning algorithms.", "labels": [], "entities": []}, {"text": "For this purpose, kernel methods, and in particular tree kernels allow for representing trees in terms of all possible subtrees ().", "labels": [], "entities": []}, {"text": "Unfortunately, the representation in entailment recognition problems requires the definition of kernels over graphs constituted by tree pairs, which are in general different from kernels applied to single trees.", "labels": [], "entities": [{"text": "entailment recognition", "start_pos": 37, "end_pos": 59, "type": "TASK", "confidence": 0.8679631352424622}]}, {"text": "In (), this has been addressed by introducing semantic links (placeholders) between text and hypothesis parse trees and evaluating two distinct tree kernels for the trees of texts and for those of hypotheses.", "labels": [], "entities": []}, {"text": "In order to make such disjoint kernel combination effective, all possible assignments between the placeholders of the first and the second entailment pair were generated causing a remarkable slowdown.", "labels": [], "entities": []}, {"text": "In this paper, we describe the feature space of all possible tree fragment pairs and we show that it can be evaluated with a much simpler kernel than the one used in previous work, both in terms of design and computational complexity.", "labels": [], "entities": []}, {"text": "Moreover, the experiments on the RTE datasets show that our proposed kernel provides higher accuracy than the simple union of tree kernel spaces.", "labels": [], "entities": [{"text": "RTE datasets", "start_pos": 33, "end_pos": 45, "type": "DATASET", "confidence": 0.7878131866455078}, {"text": "accuracy", "start_pos": 92, "end_pos": 100, "type": "METRIC", "confidence": 0.9987803101539612}]}], "datasetContent": [{"text": "The aim of the experiments is to show that the space of tree fragment pairs is the most effective to represent Tree Pair-based Graphs for the design of Textual Entailment classifiers.", "labels": [], "entities": []}, {"text": "To compare our model with previous work we implemented the following kernels in SVM-light (Joachims, 1999): where e 1 = T 1 , H 1 and e 2 = T 2 , H 2 are two text and hypothesis pairs and K t is the syntactic tree kernel) presented in the previous section.", "labels": [], "entities": []}, {"text": "\u2022 which (as shown in the previous sections) en- codes the tree fragment pairs with and without placeholders.", "labels": [], "entities": []}, {"text": "\u2022 , where c is a possible placeholder assignment which connects nodes from the first pair with those of the second pair and \u03c6 c (\u00b7) transforms trees according to c.", "labels": [], "entities": []}, {"text": "Note that K max is the kernel proposed in () and K pmx is a hybrid kernel based on the maximum K p , which uses the space of tree fragment pairs.", "labels": [], "entities": []}, {"text": "For all the above kernels, we set the default cost factor and trade-off parameters and we set \u03bb to 0.4.", "labels": [], "entities": []}, {"text": "To experiment with entailment relations, we used the data sets made available by the first) and second) Recognizing Textual Entailment Challenge.", "labels": [], "entities": []}, {"text": "These corpora are divided in the development sets D1 and D2 and the test sets T 1 and T 2.", "labels": [], "entities": []}, {"text": "D1 contains 567 examples whereas T 1, D2 and T 2 all have the same size, i.e. 800 instances.", "labels": [], "entities": []}, {"text": "Each example is an ordered pair of texts for which the entailment relation has to be decided.", "labels": [], "entities": []}, {"text": "shows the results of the above kernels on the split used for the RTE competitions.", "labels": [], "entities": [{"text": "RTE competitions", "start_pos": 65, "end_pos": 81, "type": "DATASET", "confidence": 0.7981280386447906}]}, {"text": "The first column reports the kernel model.", "labels": [], "entities": []}, {"text": "The second and third columns illustrate the model accuracy for RTE1 whereas column 4 and 5 show the accuracy for RTE2.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 50, "end_pos": 58, "type": "METRIC", "confidence": 0.9887174367904663}, {"text": "RTE1", "start_pos": 63, "end_pos": 67, "type": "DATASET", "confidence": 0.8686252236366272}, {"text": "accuracy", "start_pos": 100, "end_pos": 108, "type": "METRIC", "confidence": 0.9990346431732178}, {"text": "RTE2", "start_pos": 113, "end_pos": 117, "type": "DATASET", "confidence": 0.8961890339851379}]}, {"text": "Moreover, \u00ac P indicates the use of standard syntactic trees and P the use of trees enriched with placeholders.", "labels": [], "entities": []}, {"text": "We note that:  First, the space of tree fragment pairs, generated by K p improves the one generated by K s (i.e. the simple union of the fragments of texts and hypotheses) of 4 (58.9% vs 54.9%) and 0.9 (53.5% vs 52.6%) points on RTE1 and RTE2, respectively.", "labels": [], "entities": [{"text": "RTE1", "start_pos": 229, "end_pos": 233, "type": "DATASET", "confidence": 0.9775214195251465}, {"text": "RTE2", "start_pos": 238, "end_pos": 242, "type": "DATASET", "confidence": 0.9117158055305481}]}, {"text": "This suggests that the fragment pairs are more effective for encoding the syntactic rules describing the entailment concept.", "labels": [], "entities": []}, {"text": "Second, on RTE1, the introduction of placeholders does not improve K p or K s suggesting that for their correct exploitation an extension of the space of tree fragment pairs should be modeled.", "labels": [], "entities": [{"text": "RTE1", "start_pos": 11, "end_pos": 15, "type": "DATASET", "confidence": 0.925729513168335}]}, {"text": "Third, on RTE2, the impact of placeholders seems more important but only K max and K s are able to fully exploit their semantic contribution.", "labels": [], "entities": [{"text": "RTE2", "start_pos": 10, "end_pos": 14, "type": "DATASET", "confidence": 0.8265030980110168}]}, {"text": "A possible explanation is that in order to use the set of all possible assignments (required by K max ), we needed to prune the \"too large\" syntactic trees as also suggested in: Accuracy of different kernel models using (P) and not using (\u00ac P) placeholder information on RTE1 and RTE2.", "labels": [], "entities": [{"text": "RTE1", "start_pos": 271, "end_pos": 275, "type": "DATASET", "confidence": 0.906385064125061}, {"text": "RTE2", "start_pos": 280, "end_pos": 284, "type": "DATASET", "confidence": 0.8444926142692566}]}, {"text": "suited for RTE than the other kernels, its accuracy is lower than the state-of-the-art in RTE.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 43, "end_pos": 51, "type": "METRIC", "confidence": 0.9996567964553833}]}, {"text": "This is because the latter uses additional models like the lexical similarity between text and hypothesis, which greatly improve accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 129, "end_pos": 137, "type": "METRIC", "confidence": 0.995922327041626}]}], "tableCaptions": [{"text": " Table 1: Accuracy of different kernel models using  (P) and not using (\u00ac P) placeholder information on  RTE1 and RTE2.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.969141960144043}, {"text": "RTE1", "start_pos": 105, "end_pos": 109, "type": "DATASET", "confidence": 0.9539980888366699}, {"text": "RTE2", "start_pos": 114, "end_pos": 118, "type": "DATASET", "confidence": 0.8867780566215515}]}]}