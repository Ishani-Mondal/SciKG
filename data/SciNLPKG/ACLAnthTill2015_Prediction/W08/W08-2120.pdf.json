{"title": [], "abstractContent": [{"text": "Previous work in referring expression generation has explored general purpose techniques for attribute selection and surface realization.", "labels": [], "entities": [{"text": "referring expression generation", "start_pos": 17, "end_pos": 48, "type": "TASK", "confidence": 0.8872404098510742}, {"text": "attribute selection", "start_pos": 93, "end_pos": 112, "type": "TASK", "confidence": 0.6896935701370239}, {"text": "surface realization", "start_pos": 117, "end_pos": 136, "type": "TASK", "confidence": 0.7702754735946655}]}, {"text": "However, most of this work did not take into account: a) stylistic differences between speakers; or b) trainable surface realization approaches that combine semantic and word order information.", "labels": [], "entities": []}, {"text": "In this paper we describe and evaluate several end-to-end referring expression generation algorithms that take into consideration speaker style and use data-driven surface realization techniques.", "labels": [], "entities": [{"text": "referring expression generation", "start_pos": 58, "end_pos": 89, "type": "TASK", "confidence": 0.6958410739898682}]}], "introductionContent": [{"text": "Natural language generation (NLG) systems have typically decomposed the problem of generating a linguistic expression from a conceptual specification into three major steps: content planning, text planning and surface realization).", "labels": [], "entities": [{"text": "Natural language generation (NLG)", "start_pos": 0, "end_pos": 33, "type": "TASK", "confidence": 0.7666372607151667}]}, {"text": "The task in content planning is to select the information that is to be conveyed to maximize communication efficiency.", "labels": [], "entities": [{"text": "content planning", "start_pos": 12, "end_pos": 28, "type": "TASK", "confidence": 0.7396053075790405}]}, {"text": "The task in text planning and surface realization is to use the available linguistic resources (words and syntax) to convey the selected information using well-formed linguistic expressions.", "labels": [], "entities": [{"text": "text planning", "start_pos": 12, "end_pos": 25, "type": "TASK", "confidence": 0.7692489624023438}, {"text": "surface realization", "start_pos": 30, "end_pos": 49, "type": "TASK", "confidence": 0.7536587119102478}]}, {"text": "During a discourse (whether written or spoken, monolog or dialog), a number of entities are introduced into the discourse context shared by the reader/hearer and the writer/speaker.", "labels": [], "entities": []}, {"text": "Constructing linguistic references to these entities efficiently and effectively is a problem that touches on all parts of an NLG system.", "labels": [], "entities": []}, {"text": "Traditionally, this problem is split into two parts.", "labels": [], "entities": []}, {"text": "The task of selecting the attributes to use in referring to an entity is the attribute selection task, performed during content planning or sentence planning.", "labels": [], "entities": [{"text": "content planning or sentence planning", "start_pos": 120, "end_pos": 157, "type": "TASK", "confidence": 0.6617825388908386}]}, {"text": "The actual construction of the referring expression is part of surface realization.", "labels": [], "entities": []}, {"text": "There now exist numerous general-purpose algorithms for attribute selection (e.g.,).", "labels": [], "entities": [{"text": "attribute selection", "start_pos": 56, "end_pos": 75, "type": "TASK", "confidence": 0.7074988335371017}]}, {"text": "However, these algorithms by-and-large focus on the algorithmic aspects of referring expression generation rather than on psycholinguistic factors that influence language production.", "labels": [], "entities": [{"text": "referring expression generation", "start_pos": 75, "end_pos": 106, "type": "TASK", "confidence": 0.7595457434654236}]}, {"text": "For example, we know that humans exhibit individual differences in language production that can be quite pronounced (e.g.).", "labels": [], "entities": []}, {"text": "We also know that the language production process is subject to lexical priming, which means that words and concepts that have been used recently are likely to appear again.", "labels": [], "entities": []}, {"text": "In this paper, we look at attribute selection and surface realization for referring expression generation using the TUNA corpus 1 , an annotated corpus of human-produced referring expressions that describe furniture and people.", "labels": [], "entities": [{"text": "referring expression generation", "start_pos": 74, "end_pos": 105, "type": "TASK", "confidence": 0.7090741395950317}, {"text": "TUNA corpus 1", "start_pos": 116, "end_pos": 129, "type": "DATASET", "confidence": 0.9002867937088013}]}, {"text": "We first explore the impact of individual style and priming on attribute selection for referring expression generation.", "labels": [], "entities": [{"text": "referring expression generation", "start_pos": 87, "end_pos": 118, "type": "TASK", "confidence": 0.797615130742391}]}, {"text": "To get an idea of the potential improvement when modeling these factors, we implemented aversion of full brevity search that uses speakerspecific constraints, and another version that also uses recency constraints.", "labels": [], "entities": []}, {"text": "We found that using speaker-specific constraints led to big performance gains for both TUNA domains, while the use of re-cency constraints was not as effective for TUNAstyle tasks.", "labels": [], "entities": []}, {"text": "We then modified Dale and Reiter's classic attribute selection algorithm to model individual differences in style, and found performance gains in this more greedy approach as well.", "labels": [], "entities": []}, {"text": "Then, we look at surface realization for referring expression generation.", "labels": [], "entities": [{"text": "referring expression generation", "start_pos": 41, "end_pos": 72, "type": "TASK", "confidence": 0.6638677418231964}]}, {"text": "There are several approaches to surface realizations described in the literature) ranging from hand-crafted template-based realizers to data-driven syntax-based realizers).", "labels": [], "entities": []}, {"text": "Template-based realization provides a straightforward method to fill out pre-defined templates with the current attribute values.", "labels": [], "entities": [{"text": "Template-based realization", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.8357867300510406}]}, {"text": "Data-driven syntaxbased methods employ techniques that incorporate the syntactic relations between words which can potentially go beyond local adjacency relations.", "labels": [], "entities": []}, {"text": "Syntactic information also helps in eliminating ungrammatical sentence realizations.", "labels": [], "entities": [{"text": "eliminating ungrammatical sentence realizations", "start_pos": 36, "end_pos": 83, "type": "TASK", "confidence": 0.6541532278060913}]}, {"text": "At the other extreme, there are techniques that exhaustively generate possible realizations with recourse to syntax in as much as it is reflected in local n-grams.", "labels": [], "entities": []}, {"text": "Such techniques have the advantage of being robust although they are inadequate to capture long-range dependencies.", "labels": [], "entities": []}, {"text": "We explore three techniques for the task of referring expression generation that are different hybrids of hand-crafted and data-driven methods.", "labels": [], "entities": [{"text": "referring expression generation", "start_pos": 44, "end_pos": 75, "type": "TASK", "confidence": 0.904061754544576}]}, {"text": "The layout of this paper is as follows: In Section 2, we describe the TUNA data set and the task of identifying target entities in the context of distractors.", "labels": [], "entities": [{"text": "TUNA data set", "start_pos": 70, "end_pos": 83, "type": "DATASET", "confidence": 0.8398702541987101}]}, {"text": "In Section 3, we present our algorithms for attribute selection.", "labels": [], "entities": [{"text": "attribute selection", "start_pos": 44, "end_pos": 63, "type": "TASK", "confidence": 0.697932556271553}]}, {"text": "Our algorithms for surface realization are presented in Section 4.", "labels": [], "entities": [{"text": "surface realization", "start_pos": 19, "end_pos": 38, "type": "TASK", "confidence": 0.7354138493537903}]}, {"text": "Our evaluation of these methods for attribute selection and surface realization are presented in Sections 5 and 6.", "labels": [], "entities": [{"text": "attribute selection", "start_pos": 36, "end_pos": 55, "type": "TASK", "confidence": 0.6892989128828049}, {"text": "surface realization", "start_pos": 60, "end_pos": 79, "type": "TASK", "confidence": 0.7549115717411041}]}], "datasetContent": [{"text": "Data Preparation The training data were used to build the models outlined above.", "labels": [], "entities": []}, {"text": "The development data were then processed one-by-one.", "labels": [], "entities": []}, {"text": "Metrics We report performance using the metrics used for the REG 2008 competition.", "labels": [], "entities": [{"text": "REG 2008 competition", "start_pos": 61, "end_pos": 81, "type": "DATASET", "confidence": 0.8603173693021139}]}, {"text": "The MASI metric is a metric used in summarization that measures agreement between two annotators (or one annotator and one system) on set-valued items ().", "labels": [], "entities": [{"text": "MASI metric", "start_pos": 4, "end_pos": 15, "type": "METRIC", "confidence": 0.7873098254203796}, {"text": "summarization", "start_pos": 36, "end_pos": 49, "type": "TASK", "confidence": 0.9877952337265015}]}, {"text": "Values range from 0 to 1, with 1 representing perfect agreement.", "labels": [], "entities": []}, {"text": "The DICE metric is also a measure of association whose value varies from 0 (no association) to 1 (total association).", "labels": [], "entities": [{"text": "DICE metric", "start_pos": 4, "end_pos": 15, "type": "METRIC", "confidence": 0.7665359377861023}]}, {"text": "The Accuracy metric is binary-valued: 1 if the attribute set is identical to that selected by the human, 0 otherwise.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.9485112428665161}]}, {"text": "The Uniqueness metric is also binary-valued: 1 if the attribute set uniquely identifies the target referent among the distractors, 0 otherwise.", "labels": [], "entities": []}, {"text": "Finally, the Minimality metric is 1 if the selected attribute set is as small as possible (while still uniquely identifying the target referent), and 0 otherwise.", "labels": [], "entities": [{"text": "Minimality", "start_pos": 13, "end_pos": 23, "type": "TASK", "confidence": 0.5743287801742554}]}, {"text": "We note that attribute selection algorithms such as Dale & Reiter's are based on the observation that humans frequently do not produce minimal referring expressions.", "labels": [], "entities": [{"text": "attribute selection", "start_pos": 13, "end_pos": 32, "type": "TASK", "confidence": 0.7036332190036774}]}, {"text": "Results shows the results for variations of full brevity.", "labels": [], "entities": []}, {"text": "As we would expect, all approaches achieve a perfect score on uniqueness.", "labels": [], "entities": []}, {"text": "For both corpora, we see a large performance jump when we use speaker constraints for all metrics other than minimality.", "labels": [], "entities": []}, {"text": "However, when we incorporate recency constraints as well performance declines slightly.", "labels": [], "entities": []}, {"text": "We think this is due to two factors: first, the speakers are not in a conversation, and selfpriming may have less impact than other-priming; and second, we do not always have the most recent prior utterance fora given speaker in the training data.", "labels": [], "entities": []}, {"text": "also shows the results for variations of Dale & Reiter's algorithm.", "labels": [], "entities": []}, {"text": "When we incorporate speaker constraints, we again see a performance jump for most metrics, although compared to the best possible case (full brevity) there is still room for improvement.", "labels": [], "entities": []}, {"text": "We conclude that speaker constraints can be successfully used in standard attribute selection algorithms to improve performance on this task.", "labels": [], "entities": []}, {"text": "The most relevant previous research is the work of), who modified Dale and Reiter's algorithm to model speaker adaptation in dialog.", "labels": [], "entities": [{"text": "speaker adaptation", "start_pos": 103, "end_pos": 121, "type": "TASK", "confidence": 0.7611276507377625}]}, {"text": "However, this corpus does not involve dialog so there are no cross-speaker constraints, only within-speaker constraints (speaker style and priming).", "labels": [], "entities": []}, {"text": "Data Preparation We first normalized the training data to correct misspellings and remove punctuation and capitalization.", "labels": [], "entities": [{"text": "Data Preparation", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.6136540472507477}]}, {"text": "We then extracted a phrasal lexicon.", "labels": [], "entities": []}, {"text": "For each attribute value we extracted the count of all realizations of that value in the training data.", "labels": [], "entities": []}, {"text": "We treated locations as a special case, storing separately the realizations of xy coordinate pairs and single x-or y-coordinates.", "labels": [], "entities": []}, {"text": "We added a small number of realizations by hand to cover possible attribute values not seen in the training data.", "labels": [], "entities": []}, {"text": "Realization We ran two realization experiments.", "labels": [], "entities": []}, {"text": "In the first experiment, we used the humanselected attribute sets in the development data as the input to realization.", "labels": [], "entities": []}, {"text": "If we want to maxi-  mize humanlikeness, then using these attribute sets should give us an idea of the best possible performance of our realization methods.", "labels": [], "entities": []}, {"text": "In the second experiment, we used the attribute sets output by our best-performing attribute selection algorithms (FB-sf and DR-sf) as the input to realization.", "labels": [], "entities": [{"text": "FB-sf", "start_pos": 115, "end_pos": 120, "type": "METRIC", "confidence": 0.49628958106040955}]}, {"text": "Metrics We report performance of our surface realizers using the metrics used for the REG 2008 shared challenge and standard metrics used in the natural language generation and machine translation communities.", "labels": [], "entities": [{"text": "REG 2008 shared challenge", "start_pos": 86, "end_pos": 111, "type": "DATASET", "confidence": 0.7234724462032318}, {"text": "natural language generation and machine translation", "start_pos": 145, "end_pos": 196, "type": "TASK", "confidence": 0.6509375870227814}]}, {"text": "String Edit Distance (SED) is a measure of the number of words that would have to be added, deleted, or replaced in order to transform the generated referring expression into the one produced by the human.", "labels": [], "entities": [{"text": "String Edit Distance (SED)", "start_pos": 0, "end_pos": 26, "type": "METRIC", "confidence": 0.7420846372842789}]}, {"text": "As used in the REG 2008 shared challenge, it is unnormalized, so its values range from zero up.", "labels": [], "entities": [{"text": "REG 2008 shared challenge", "start_pos": 15, "end_pos": 40, "type": "DATASET", "confidence": 0.7962795495986938}]}, {"text": "Accuracy (ACC) is binary-valued: 1 if the generated referring expression is identical to that produced by the human (after spelling correction and normalization), and 0 otherwise.", "labels": [], "entities": [{"text": "Accuracy (ACC)", "start_pos": 0, "end_pos": 14, "type": "METRIC", "confidence": 0.9549203962087631}]}, {"text": "Bleu is an n-gram based metric that counts the number of 1, 2 and 3 grams shared between the generated string and one or more (preferably more) reference strings (.", "labels": [], "entities": [{"text": "Bleu", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.9374902844429016}]}, {"text": "Bleu values are normalized and range from 0 (no match) to 1 (perfect match).", "labels": [], "entities": [{"text": "Bleu", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.9651051163673401}, {"text": "perfect match)", "start_pos": 61, "end_pos": 75, "type": "METRIC", "confidence": 0.9603753884633383}]}, {"text": "Finally, the NIST metric is a variation on the Bleu metric that, among other things, weights rare n-grams higher than frequently-occurring ones.)", "labels": [], "entities": [{"text": "NIST metric", "start_pos": 13, "end_pos": 24, "type": "DATASET", "confidence": 0.8065282702445984}, {"text": "Bleu metric", "start_pos": 47, "end_pos": 58, "type": "METRIC", "confidence": 0.8434634804725647}]}, {"text": "Our approaches work better for the furniture domain, where there are fewer attributes, than for the people domain.", "labels": [], "entities": []}, {"text": "For both domains, for automatic and human attribute selection, the speaker-dependent Template-based approach seems to perform the best, then the speakerindependent Template-based approach, and then the Permute&Rank approach.", "labels": [], "entities": [{"text": "human attribute selection", "start_pos": 36, "end_pos": 61, "type": "TASK", "confidence": 0.6312565008799235}]}, {"text": "However, we find automatic metrics for evaluating generation quality to be unreliable.", "labels": [], "entities": []}, {"text": "We looked at the output of the surface realizers for the two examples in Section 2.", "labels": [], "entities": []}, {"text": "The best output for the example in is from the FB-sf template-based speaker-dependent algorithm, which is the big red sofa.", "labels": [], "entities": []}, {"text": "The worst output is from the DR-sf dependency-based speakerdependent algorithm, which is on the left red chair with three seats.", "labels": [], "entities": []}, {"text": "The best output for the example in is from the FB-sf template-based speaker-independent algorithm, which is the man with the white beard.", "labels": [], "entities": [{"text": "FB-sf", "start_pos": 47, "end_pos": 52, "type": "DATASET", "confidence": 0.6570833325386047}]}, {"text": "The worst output is from the FB-sf dependency-based speaker-dependent algorithm, which is beard man white.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Results for attribute selection", "labels": [], "entities": [{"text": "attribute selection", "start_pos": 22, "end_pos": 41, "type": "TASK", "confidence": 0.89542156457901}]}, {"text": " Table 2: Results for realization using speakers' at- tribute selection (SED: String Edit Distance, ACC:  String Accuracy)", "labels": [], "entities": [{"text": "SED: String Edit Distance", "start_pos": 73, "end_pos": 98, "type": "METRIC", "confidence": 0.774060708284378}, {"text": "ACC:  String Accuracy", "start_pos": 100, "end_pos": 121, "type": "METRIC", "confidence": 0.8175028264522552}]}, {"text": " Table 3: Results for realization with different attribute selection algorithms", "labels": [], "entities": []}]}