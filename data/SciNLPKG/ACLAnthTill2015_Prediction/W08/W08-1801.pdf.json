{"title": [{"text": "Improving Text Retrieval Precision and Answer Accuracy in Question Answering Systems", "labels": [], "entities": [{"text": "Improving Text Retrieval Precision", "start_pos": 0, "end_pos": 34, "type": "TASK", "confidence": 0.9096545577049255}, {"text": "Answer Accuracy", "start_pos": 39, "end_pos": 54, "type": "TASK", "confidence": 0.6001451015472412}, {"text": "Question Answering", "start_pos": 58, "end_pos": 76, "type": "TASK", "confidence": 0.7771759331226349}]}], "abstractContent": [{"text": "Question Answering (QA) systems are often built modularly, with a text retrieval component feeding forward into an answer extraction component.", "labels": [], "entities": [{"text": "Question Answering (QA)", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.8570231139659882}, {"text": "answer extraction", "start_pos": 115, "end_pos": 132, "type": "TASK", "confidence": 0.6959155350923538}]}, {"text": "Conventional wisdom suggests that, the higher the quality of the retrieval results used as input to the answer extraction module, the better the extracted answers, and hence system accuracy , will be.", "labels": [], "entities": [{"text": "answer extraction module", "start_pos": 104, "end_pos": 128, "type": "TASK", "confidence": 0.8200825452804565}, {"text": "accuracy", "start_pos": 181, "end_pos": 189, "type": "METRIC", "confidence": 0.9923259019851685}]}, {"text": "This turns out to be a poor assumption, because text retrieval and answer extraction are tightly coupled.", "labels": [], "entities": [{"text": "text retrieval", "start_pos": 48, "end_pos": 62, "type": "TASK", "confidence": 0.7904232442378998}, {"text": "answer extraction", "start_pos": 67, "end_pos": 84, "type": "TASK", "confidence": 0.8473798036575317}]}, {"text": "Improvements in retrieval quality can be lost at the answer extraction module, which cannot necessarily recognize the additional answer candidates provided by improved retrieval.", "labels": [], "entities": [{"text": "answer extraction", "start_pos": 53, "end_pos": 70, "type": "TASK", "confidence": 0.8228087425231934}]}, {"text": "Going forward, to improve accuracy on the QA task, systems will need greater coordination between text retrieval and answer extraction modules.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 26, "end_pos": 34, "type": "METRIC", "confidence": 0.9979376792907715}, {"text": "QA task", "start_pos": 42, "end_pos": 49, "type": "TASK", "confidence": 0.9173175990581512}, {"text": "answer extraction", "start_pos": 117, "end_pos": 134, "type": "TASK", "confidence": 0.735143318772316}]}], "introductionContent": [{"text": "The task of Question Answering (QA) involves taking a question phrased in natural human language and locating specific answers to that question expressed within a text collection.", "labels": [], "entities": [{"text": "Question Answering (QA)", "start_pos": 12, "end_pos": 35, "type": "TASK", "confidence": 0.8986597061157227}]}, {"text": "Regardless of system architecture, or whether the system is operating over a closed text collection or the web, most QA systems use text retrieval as a first step to narrow the search space for the answer to the question to a subset of the text collection).", "labels": [], "entities": []}, {"text": "The remainder of the QA process amounts to a gradual narrowing of the search space, using successively more finely-grained filters to extract, validate and present one or more answers to the question.", "labels": [], "entities": [{"text": "QA", "start_pos": 21, "end_pos": 23, "type": "TASK", "confidence": 0.874270498752594}]}, {"text": "Perhaps the most popular system architecture in the QA research community is the modular architecture, inmost variations of which, text retrieval is represented as a separate component, isolated by a software abstraction from question analysis and answer extraction mechanisms.", "labels": [], "entities": [{"text": "answer extraction", "start_pos": 248, "end_pos": 265, "type": "TASK", "confidence": 0.7023835331201553}]}, {"text": "The widelyaccepted pipelined modular architecture imposes a strict linear ordering on the system's control flow, with the analysis of the input question used as input to the text retrieval module, and the retrieved results feeding into the downstream answer extraction components.", "labels": [], "entities": []}, {"text": "Proponents of the modular architecture naturally view the QA task as decomposable, and to a certain extent, it is.", "labels": [], "entities": []}, {"text": "The modules, however, can never be fully decoupled, because question analysis and answer extraction components, at least, depend on a common representation for answers and perhaps also a common set of text processing tools.", "labels": [], "entities": [{"text": "question analysis", "start_pos": 60, "end_pos": 77, "type": "TASK", "confidence": 0.7999454140663147}, {"text": "answer extraction", "start_pos": 82, "end_pos": 99, "type": "TASK", "confidence": 0.7774619460105896}]}, {"text": "This dependency is necessary to enable the answer extraction mechanism to determine whether answers exist in retrieved text, by analyzing it and comparing it against the question analysis module's answer specification.", "labels": [], "entities": [{"text": "answer extraction", "start_pos": 43, "end_pos": 60, "type": "TASK", "confidence": 0.8001652956008911}]}, {"text": "In practice, the text retrieval component does not use the common representation for scoring text; either the question analysis module or an explicit query formulation component maps it into a representation queryable by the text retrieval component.", "labels": [], "entities": [{"text": "text retrieval", "start_pos": 17, "end_pos": 31, "type": "TASK", "confidence": 0.7020014673471451}]}, {"text": "The pipelined modular QA system architecture also carries with it an assumption about the compositionality of the components.", "labels": [], "entities": []}, {"text": "It is easy to observe that errors cascade as the QA process moves through downstream modules, and this leads to the intuition that maximizing performance of individual modules minimizes the error at each stage of the pipeline, which, in turn, should maximize overall end-to-end system accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 285, "end_pos": 293, "type": "METRIC", "confidence": 0.9877336025238037}]}, {"text": "It is a good idea to pause to question what this intuition is telling us.", "labels": [], "entities": []}, {"text": "Is end-to-end QA system performance really a linear function of individual Figure 1: Example OpenEphyra semantic representation for the sentence, John loves Mary.", "labels": [], "entities": []}, {"text": "Note that John is identified as the ARG0, the agent, or doer, of the love action.", "labels": [], "entities": [{"text": "ARG0", "start_pos": 36, "end_pos": 40, "type": "DATASET", "confidence": 0.6760602593421936}]}, {"text": "Mary is identified as the ARG1, the patient, or to whom the love action is being done.", "labels": [], "entities": [{"text": "ARG1", "start_pos": 26, "end_pos": 30, "type": "METRIC", "confidence": 0.6089953184127808}]}, {"text": "Both John and Mary are also identified as PERSON named entity types. components?", "labels": [], "entities": []}, {"text": "Is component performance really additive?", "labels": [], "entities": []}, {"text": "This paper argues that the answer is no, not in general, and offers the counterexample of a high-precision text retrieval system that can check constraints against the common representation at retrieval time, which is integrated into a publiclyavailable pipelined modular QA system that is otherwise unchanged.", "labels": [], "entities": []}, {"text": "Ignoring the dependency between the answer extraction mechanism and the text retrieval component creates a problem.", "labels": [], "entities": [{"text": "answer extraction", "start_pos": 36, "end_pos": 53, "type": "TASK", "confidence": 0.8186921775341034}]}, {"text": "The answer extraction module is notable to handle the more sophisticated types of matches provided by the improved text retrieval module, and so it ignores them, leaving end-to-end system performance largely unchanged.", "labels": [], "entities": [{"text": "answer extraction", "start_pos": 4, "end_pos": 21, "type": "TASK", "confidence": 0.8811897933483124}, {"text": "text retrieval", "start_pos": 115, "end_pos": 129, "type": "TASK", "confidence": 0.719659149646759}]}, {"text": "The lesson learned is that a module improved in isolation does not necessarily provide an improvement in end-to-end system accuracy, and the paper concludes with recommendations for further research in bringing text retrieval and answer extraction closer together.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 123, "end_pos": 131, "type": "METRIC", "confidence": 0.9871968030929565}, {"text": "text retrieval", "start_pos": 211, "end_pos": 225, "type": "TASK", "confidence": 0.8113767802715302}, {"text": "answer extraction", "start_pos": 230, "end_pos": 247, "type": "TASK", "confidence": 0.8769627809524536}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Summary of end-to-end QA system ac- curacy and MRR when the existing text retrieval  module is replaced with a high-precision version", "labels": [], "entities": [{"text": "ac- curacy", "start_pos": 42, "end_pos": 52, "type": "METRIC", "confidence": 0.9281821250915527}, {"text": "MRR", "start_pos": 57, "end_pos": 60, "type": "METRIC", "confidence": 0.9268776774406433}]}, {"text": " Table 2: Summary of end-to-end QA system results on the question set", "labels": [], "entities": []}]}