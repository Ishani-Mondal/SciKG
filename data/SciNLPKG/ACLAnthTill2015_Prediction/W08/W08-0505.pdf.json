{"title": [{"text": "Adapting naturally occurring test suites for evaluation of clinical question answering", "labels": [], "entities": [{"text": "Adapting naturally occurring test", "start_pos": 0, "end_pos": 33, "type": "TASK", "confidence": 0.8863188177347183}, {"text": "evaluation of clinical question answering", "start_pos": 45, "end_pos": 86, "type": "TASK", "confidence": 0.643112736940384}]}], "abstractContent": [{"text": "This paper describes the structure of a test suite for evaluation of clinical question answering systems; presents several manually compiled resources found useful for test suite generation; and describes the adaptation of these resources for evaluation of a clinical question answering system.", "labels": [], "entities": [{"text": "evaluation of clinical question answering", "start_pos": 55, "end_pos": 96, "type": "TASK", "confidence": 0.6007509708404541}, {"text": "clinical question answering", "start_pos": 259, "end_pos": 286, "type": "TASK", "confidence": 0.6593758662541708}]}], "introductionContent": [{"text": "The community-wide interest in rapid development in many areas of natural language processing and information retrieval resulted in creation of reusable test collections in large-scale evaluations such as the Text REtrieval Conference (TREC) . Researchers in more specific areas, for which no TREC or other collections are available, have to create or find suitable test collections to evaluate their systems.", "labels": [], "entities": [{"text": "information retrieval", "start_pos": 98, "end_pos": 119, "type": "TASK", "confidence": 0.7504436671733856}, {"text": "Text REtrieval Conference (TREC)", "start_pos": 209, "end_pos": 241, "type": "TASK", "confidence": 0.7528747071822485}]}, {"text": "One of the benefits of focusing on a specific domain, such as clinical question answering, is that the user-needs and desirable results are well-studied and their descriptions are readily-available.", "labels": [], "entities": [{"text": "clinical question answering", "start_pos": 62, "end_pos": 89, "type": "TASK", "confidence": 0.6039222180843353}]}, {"text": "In the case of clinical question answering, clinicians' desiderata are: to see a \"bottom-line advice\" first, have on-demand access to the context that was used in generation of the advice, and finally have access to the original sources of information (.", "labels": [], "entities": [{"text": "clinical question answering", "start_pos": 15, "end_pos": 42, "type": "TASK", "confidence": 0.6906869212786356}]}, {"text": "A fair number of high-quality manually created collections present answers to clinical questions in this form and could be obtained online.", "labels": [], "entities": []}, {"text": "Three partially freely-available sources: Family Practitioner Inquiry Network (FPIN) 2 , Parkhurst Exchange Forum (PE) 3 , and BMJ Clinical Evidence (BMJ-CE) were used to design and develop the presented test suites and evaluation methods.", "labels": [], "entities": [{"text": "Parkhurst Exchange Forum (PE)", "start_pos": 89, "end_pos": 118, "type": "DATASET", "confidence": 0.8846565882364908}, {"text": "BMJ Clinical Evidence (BMJ-CE)", "start_pos": 127, "end_pos": 157, "type": "DATASET", "confidence": 0.6801597277323405}]}, {"text": "Although there seems to be a distinction between test collections and test suites) (the former defined as \"pieces of text\" and associated with corpora, the latter, as lists of specially constructed sentences, or sentence sequences, or sentence fragments (), evaluation of answers to clinical questions crosses this boundary and requires the availability of carefully generated sentence fragments as well as suitable document collections.", "labels": [], "entities": []}], "datasetContent": [{"text": "The answer presented in can be used to evaluate a system's answer to this question by extracting the reference list from the FPIN or BMJ-CE answer.", "labels": [], "entities": [{"text": "FPIN", "start_pos": 125, "end_pos": 129, "type": "DATASET", "confidence": 0.5353684425354004}, {"text": "BMJ-CE answer", "start_pos": 133, "end_pos": 146, "type": "DATASET", "confidence": 0.6956627815961838}]}, {"text": "Similarly, the second-tier summaries can be used to evaluate the context for the key-points generated by a system.", "labels": [], "entities": []}, {"text": "The references can be used to evaluate the quality of the original sources retrieved by a system if the documents in both lists are represented using their unique identifiers: DOI or a PubMed 5 identifier.", "labels": [], "entities": [{"text": "PubMed 5 identifier", "start_pos": 185, "end_pos": 204, "type": "DATASET", "confidence": 0.8828830718994141}]}, {"text": "Availability of these test suites provides for the following evaluation forms: \u2022 diagnostic, in which developers could evaluate how a tier is affected by changes in its own module(s) or in the underlying tiers; 5 http://www.ncbi.nlm.nih.gov/sites/entrez \u2022 task-oriented, in which the system is evaluated as a whole on its ability to answer clinical questions.", "labels": [], "entities": []}, {"text": "It is conceivable to evaluate a system as a whole by evaluating its performance in each tier and then combining the results.", "labels": [], "entities": []}, {"text": "Ina task-oriented evaluation, it seems reasonable to evaluate the quality of the first-tier answer and verify the adequacy of the second-tier context.", "labels": [], "entities": []}], "tableCaptions": []}