{"title": [{"text": "Mixing and Blending Syntactic and Semantic Dependencies", "labels": [], "entities": []}], "abstractContent": [{"text": "Our system for the CoNLL 2008 shared task uses a set of individual parsers, a set of stand-alone semantic role labellers, and a joint system for parsing and semantic role labelling, all blended together.", "labels": [], "entities": [{"text": "CoNLL 2008 shared task", "start_pos": 19, "end_pos": 41, "type": "TASK", "confidence": 0.7278345376253128}]}, {"text": "The system achieved a macro averaged labelled F 1-score of 79.79 (WSJ 80.92, Brown 70.49) for the overall task.", "labels": [], "entities": [{"text": "macro averaged labelled F 1-score", "start_pos": 22, "end_pos": 55, "type": "METRIC", "confidence": 0.7186696410179139}, {"text": "WSJ 80.92", "start_pos": 66, "end_pos": 75, "type": "DATASET", "confidence": 0.8052944540977478}]}, {"text": "The labelled attachment score for syntactic dependencies was 86.63 (WSJ 87.36, Brown 80.77) and the labelled F 1-score for semantic dependencies was 72.94 (WSJ 74.47, Brown 60.18).", "labels": [], "entities": [{"text": "WSJ", "start_pos": 68, "end_pos": 71, "type": "DATASET", "confidence": 0.9605826735496521}, {"text": "WSJ", "start_pos": 156, "end_pos": 159, "type": "DATASET", "confidence": 0.9653003811836243}]}], "introductionContent": [{"text": "This paper presents a system for the CoNLL 2008 shared task on joint learning of syntactic and semantic dependencies (, combining a two-step pipelined approach with a joint approach.", "labels": [], "entities": [{"text": "CoNLL 2008 shared task", "start_pos": 37, "end_pos": 59, "type": "TASK", "confidence": 0.6883965879678726}]}, {"text": "In the pipelined system, eight different syntactic parses were blended, yielding the input for two variants of a semantic role labelling (SRL) system.", "labels": [], "entities": [{"text": "semantic role labelling (SRL)", "start_pos": 113, "end_pos": 142, "type": "TASK", "confidence": 0.7053649127483368}]}, {"text": "Furthermore, one of the syntactic parses was used with an early version of the SRL system, to provide predicate predictions fora joint syntactic and semantic parser.", "labels": [], "entities": [{"text": "syntactic parses", "start_pos": 24, "end_pos": 40, "type": "TASK", "confidence": 0.7011311799287796}]}, {"text": "For the final submission, all nine syntactic parses and all three semantic parses were blended.", "labels": [], "entities": []}, {"text": "The system is outlined in; the dashed arrow indicates the potential for using the predi-  cate prediction to improve the joint syntactic and semantic system.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Labelled attachment score on the two test  sets of the best single parse, blended with weights  set to PoS labelled attachment score (LAS) and  blended with learned weights.", "labels": [], "entities": [{"text": "Labelled attachment score", "start_pos": 10, "end_pos": 35, "type": "METRIC", "confidence": 0.7326532602310181}, {"text": "PoS labelled attachment score (LAS)", "start_pos": 113, "end_pos": 148, "type": "METRIC", "confidence": 0.9432391013417926}]}, {"text": " Table 2: Semantic predicate results on the test sets.", "labels": [], "entities": [{"text": "Semantic predicate", "start_pos": 10, "end_pos": 28, "type": "TASK", "confidence": 0.8053280115127563}]}, {"text": " Table 3: Syntactic and semantic scores on the test  sets for the submitted system. The scores, from top  to bottom, are labelled macro F 1 , labelled attach- ment score and labelled F 1 .", "labels": [], "entities": [{"text": "attach- ment score", "start_pos": 151, "end_pos": 169, "type": "METRIC", "confidence": 0.784464567899704}, {"text": "F 1", "start_pos": 183, "end_pos": 186, "type": "METRIC", "confidence": 0.946387767791748}]}]}