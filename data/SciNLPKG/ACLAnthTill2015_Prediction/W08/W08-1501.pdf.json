{"title": [{"text": "Mitigation of data sparsity in classifier-based translation", "labels": [], "entities": [{"text": "Mitigation", "start_pos": 0, "end_pos": 10, "type": "TASK", "confidence": 0.9813421368598938}, {"text": "classifier-based translation", "start_pos": 31, "end_pos": 59, "type": "TASK", "confidence": 0.6573402881622314}]}], "abstractContent": [{"text": "The concept classifier has been used as a translation unit in speech-to-speech translation systems.", "labels": [], "entities": [{"text": "speech-to-speech translation", "start_pos": 62, "end_pos": 90, "type": "TASK", "confidence": 0.7036135345697403}]}, {"text": "However, the sparsity of the training data is the bottleneck of its effectiveness.", "labels": [], "entities": []}, {"text": "Here, anew method based on using a statistical machine translation system has been introduced to mitigate the effects of data sparsity for training classi-fiers.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 35, "end_pos": 66, "type": "TASK", "confidence": 0.6388648251692454}]}, {"text": "Also, the effects of the background model which is necessary to compensate the above problem, is investigated.", "labels": [], "entities": []}, {"text": "Experimental evaluation in the context of cross-lingual doctor-patient interaction application show the superiority of the proposed method.", "labels": [], "entities": []}], "introductionContent": [{"text": "Statistical machine translation (SMT) methods are well established in speech-to-speech translation systems as the main translation technique ().", "labels": [], "entities": [{"text": "Statistical machine translation (SMT)", "start_pos": 0, "end_pos": 37, "type": "TASK", "confidence": 0.8049481014410654}, {"text": "speech-to-speech translation", "start_pos": 70, "end_pos": 98, "type": "TASK", "confidence": 0.7405091524124146}]}, {"text": "Due to their flexibility these methods provide a good coverage of the dialog domain.", "labels": [], "entities": []}, {"text": "The fluency of the translation, however, is not guaranteed.", "labels": [], "entities": []}, {"text": "Disfluencies of spoken utterances plus the speech recognizer errors degrade the translation quality even more.", "labels": [], "entities": []}, {"text": "All these ultimately affect the quality of the synthesized speech output in the target language, and the effectiveness of the concept transfer.", "labels": [], "entities": [{"text": "concept transfer", "start_pos": 126, "end_pos": 142, "type": "TASK", "confidence": 0.7451672852039337}]}, {"text": "It is quite common, though, to use other means of translation in parallel to the SMT methods ().", "labels": [], "entities": [{"text": "SMT", "start_pos": 81, "end_pos": 84, "type": "TASK", "confidence": 0.9886865615844727}]}, {"text": "Concept classification, as an alternative translation method, has been successfully integrated in speech-to-speech translators ().", "labels": [], "entities": [{"text": "Concept classification", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.808268666267395}]}, {"text": "A well defined dialog domain, e.g. doctor-patient dialog, can be partly covered by a number of concept classes.", "labels": [], "entities": []}, {"text": "Upon a successful classification of the input utterance, the translation task reduces to synthesizing a previously created translation of the concept, as a mere lookup.", "labels": [], "entities": []}, {"text": "Since the main goal in such applications is an accurate exchange of concepts, this method would serve the purpose as long as the input utterance falls within the coverage of the classifier.", "labels": [], "entities": []}, {"text": "This process can be viewed as a quantization of a continuous \"semantic\" sub-space.", "labels": [], "entities": []}, {"text": "The classifier is adequate when the quantization error is small (i.e. the derived concept and input utterance are good matches), and when the utterance falls in the same sub-space (domain) as the quantizer attempts to cover.", "labels": [], "entities": []}, {"text": "Since it is not feasible to accurately cover the whole dialog domain (since a large number of quantization levels needed) the classifier should be accompanied by a translation system with a much wider range such as an SMT engine.", "labels": [], "entities": [{"text": "SMT engine", "start_pos": 218, "end_pos": 228, "type": "TASK", "confidence": 0.9067971706390381}]}, {"text": "A rejection mechanism can help identify the cases that the input utterance falls outside the classifier coverage ().", "labels": [], "entities": []}, {"text": "In spite of this shortcoming, the classifierbased translator is an attractive option for speechto-speech applications because of its tolerance to \"noisy\" input and the fluency of its output, when it operates close to its design parameters.", "labels": [], "entities": []}, {"text": "In practice this is attainable for structured dialog interactions with high levels of predictability.", "labels": [], "entities": []}, {"text": "In addition, it can provide the users with both an accurate feedback and different translation options to choose from.", "labels": [], "entities": []}, {"text": "The latter feature, specially, is useful for applications like doctor-patient dialog.", "labels": [], "entities": [{"text": "doctor-patient dialog", "start_pos": 63, "end_pos": 84, "type": "TASK", "confidence": 0.6505481600761414}]}, {"text": "Building a concept classifier starts with identifying the desired concepts and representing them with canonical utterances that express these concepts.", "labels": [], "entities": []}, {"text": "A good set of concepts should consist of the ones that are more frequent in atypical interaction in the domain.", "labels": [], "entities": []}, {"text": "For instance in a doctor-patient dialog, the utterance \"Where does it hurt?\" is quite common and therefore its concept is a good choice.", "labels": [], "entities": []}, {"text": "Phrase books, websites, and experts' judgment are some of the resources that can be used for concept selection.", "labels": [], "entities": [{"text": "concept selection", "start_pos": 93, "end_pos": 110, "type": "TASK", "confidence": 0.8213386535644531}]}, {"text": "Other frequently used concepts include those that correspond to basic communicative and social aspects of the interaction such as greeting, acknowledgment and confirmation.", "labels": [], "entities": [{"text": "greeting, acknowledgment and confirmation", "start_pos": 130, "end_pos": 171, "type": "TASK", "confidence": 0.6213818609714508}]}, {"text": "After forming the concept space, for each class, 1 utterances that convey its concept must be gathered.", "labels": [], "entities": []}, {"text": "Hence, this training corpus would consist of a group of paraphrases for each class.", "labels": [], "entities": []}, {"text": "This form of data are often very difficult to collect as the number of classes grow.", "labels": [], "entities": []}, {"text": "Therefore, the available training data are usually sparse and cannot produce a classification accuracy to the degree possible.", "labels": [], "entities": [{"text": "classification", "start_pos": 79, "end_pos": 93, "type": "TASK", "confidence": 0.9385960102081299}, {"text": "accuracy", "start_pos": 94, "end_pos": 102, "type": "METRIC", "confidence": 0.8954569697380066}]}, {"text": "Since the classifier range is limited, high accuracy within that range is quite crucial for its effectiveness.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 44, "end_pos": 52, "type": "METRIC", "confidence": 0.9986090064048767}]}, {"text": "One of the main issues is dealing with data sparsity.", "labels": [], "entities": []}, {"text": "Other techniques have also been proposed to improve the classification rates.", "labels": [], "entities": [{"text": "classification", "start_pos": 56, "end_pos": 70, "type": "TASK", "confidence": 0.9802966117858887}]}, {"text": "For example in () the accuracy has been improved by introducing a dialog model.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 22, "end_pos": 30, "type": "METRIC", "confidence": 0.9996123909950256}]}, {"text": "Also, a background model has been used to improve the discrimination ability of a given concept class model.", "labels": [], "entities": []}, {"text": "In this work a novel method for handling the sparsity is introduced.", "labels": [], "entities": []}, {"text": "This method utilizes an SMT engine to map a single utterance to a group of them.", "labels": [], "entities": [{"text": "SMT", "start_pos": 24, "end_pos": 27, "type": "TASK", "confidence": 0.973694384098053}]}, {"text": "Furthermore, the effect of the background model on classification accuracy is investigated.", "labels": [], "entities": [{"text": "classification", "start_pos": 51, "end_pos": 65, "type": "TASK", "confidence": 0.9651902914047241}, {"text": "accuracy", "start_pos": 66, "end_pos": 74, "type": "METRIC", "confidence": 0.96546471118927}]}, {"text": "Section 2 reviews the concept classification process and the background model.", "labels": [], "entities": [{"text": "concept classification", "start_pos": 22, "end_pos": 44, "type": "TASK", "confidence": 0.7219034433364868}]}, {"text": "In Section 3 the sparsity handling method using an SMT is introduced.", "labels": [], "entities": [{"text": "sparsity handling", "start_pos": 17, "end_pos": 34, "type": "TASK", "confidence": 0.6933992803096771}, {"text": "SMT", "start_pos": 51, "end_pos": 54, "type": "TASK", "confidence": 0.9170010089874268}]}, {"text": "Data and experiments are described in Section 4.", "labels": [], "entities": []}, {"text": "The results are discussed in Section 5.", "labels": [], "entities": []}], "datasetContent": [{"text": "To compare the proposed method with the conventional classification, a classifier based on each method was put to test.", "labels": [], "entities": []}, {"text": "In the proposed method, it is expected that the accuracy is affected by the length of the n-best lists.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 48, "end_pos": 56, "type": "METRIC", "confidence": 0.9994780421257019}]}, {"text": "To observe that, n-best lists of lengths 100, 500, 1000, and 2000 were used in the experiments.", "labels": [], "entities": []}, {"text": "The results are shown in.", "labels": [], "entities": []}, {"text": "In all of the above experiments the background interpolation factor was set to 0.9 which is close to the optimum value obtained in ().", "labels": [], "entities": []}, {"text": "To examine the effect of the background model, the conventional and proposed methods were tried with different values of the interpolation factor \u03bb (the background model is weighted by 1 \u2212 \u03bb).", "labels": [], "entities": []}, {"text": "For the conventional method the length of the n-best list was set to 500.", "labels": [], "entities": [{"text": "length", "start_pos": 32, "end_pos": 38, "type": "METRIC", "confidence": 0.9877267479896545}]}, {"text": "shows the accuracy changes with respect to the interpolation factor for these two methods.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9995348453521729}]}, {"text": "shows the advantage of the proposed method over the conventional classification with a relative error rate reduction up to 10.4% (achieved when the length of the SMT n-best list was 500).", "labels": [], "entities": [{"text": "error rate reduction", "start_pos": 96, "end_pos": 116, "type": "METRIC", "confidence": 0.9499446551005045}, {"text": "SMT n-best", "start_pos": 162, "end_pos": 172, "type": "TASK", "confidence": 0.7824219167232513}]}, {"text": "However, as expected, this number decreases with longer SMT n-best lists due to the increased noise present in lower ranked outputs of the SMT.", "labels": [], "entities": [{"text": "SMT", "start_pos": 56, "end_pos": 59, "type": "TASK", "confidence": 0.9853023290634155}, {"text": "SMT", "start_pos": 139, "end_pos": 142, "type": "TASK", "confidence": 0.9470339417457581}]}, {"text": "also shows the accuracy within 4-best classifier outputs for each method.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 15, "end_pos": 23, "type": "METRIC", "confidence": 0.9995571970939636}]}, {"text": "In that case the proposed method showed an error rate which was relatively 23.7% lower than the error rate of the conventional method.", "labels": [], "entities": [{"text": "error rate", "start_pos": 43, "end_pos": 53, "type": "METRIC", "confidence": 0.9872703850269318}]}, {"text": "That was achieved at the peak of the accuracy within 4-best, when the length of the SMT n-best list was 1,000.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 37, "end_pos": 45, "type": "METRIC", "confidence": 0.999606192111969}, {"text": "length", "start_pos": 70, "end_pos": 76, "type": "METRIC", "confidence": 0.9550749659538269}, {"text": "SMT", "start_pos": 84, "end_pos": 87, "type": "TASK", "confidence": 0.9143898487091064}]}, {"text": "In this case too, further increase in the length of the n-best list led to an accuracy degradation as the classifier models became noisier.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 78, "end_pos": 86, "type": "METRIC", "confidence": 0.9992284774780273}]}], "tableCaptions": [{"text": " Table 1: Classification accuracy for the conventional method  and the proposed method with different lengths of n-best list", "labels": [], "entities": [{"text": "Classification", "start_pos": 10, "end_pos": 24, "type": "TASK", "confidence": 0.8900266289710999}, {"text": "accuracy", "start_pos": 25, "end_pos": 33, "type": "METRIC", "confidence": 0.9692212343215942}]}]}