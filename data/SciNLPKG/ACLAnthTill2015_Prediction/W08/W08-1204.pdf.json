{"title": [{"text": "Human judgment as a parameter in evaluation campaigns", "labels": [], "entities": []}], "abstractContent": [{"text": "The relevance of human judgment in an evaluation campaign is illustrated here through the DEFT text mining campaigns.", "labels": [], "entities": [{"text": "DEFT text mining", "start_pos": 90, "end_pos": 106, "type": "TASK", "confidence": 0.7125716606775919}]}, {"text": "Ina first step, testing a topic fora campaign among a limited number of human evaluators informs us about the feasibility of a task.", "labels": [], "entities": []}, {"text": "This information comes from the results obtained by the judges, as well as from their personal impressions after passing the test.", "labels": [], "entities": []}, {"text": "Ina second step, results from individual judges, as well as their pairwise matching, are used in order to adjust the task (choice of a marking scale for DEFT'07 and selection of topical categories for DEFT'08).", "labels": [], "entities": [{"text": "DEFT'07", "start_pos": 153, "end_pos": 160, "type": "DATASET", "confidence": 0.8807477355003357}, {"text": "DEFT'08", "start_pos": 201, "end_pos": 208, "type": "DATASET", "confidence": 0.9220844507217407}]}, {"text": "Finally, the mutual comparison of com-petitors' results, at the end of the evaluation campaign, confirms the choices we made at its starting point, and provides means to redefine the task when we shall launch a future campaign based on the same topic.", "labels": [], "entities": []}], "introductionContent": [{"text": "For the past four years, the DEFT 1 (D\u00e9fi Fouille de Texte) campaigns have been aiming to evaluate methods and software developed by several research teams in French text mining, on a variety of topics.", "labels": [], "entities": [{"text": "DEFT 1 (D\u00e9fi Fouille de Texte)", "start_pos": 29, "end_pos": 59, "type": "DATASET", "confidence": 0.7739151678979397}, {"text": "text mining", "start_pos": 166, "end_pos": 177, "type": "TASK", "confidence": 0.6737399846315384}]}, {"text": "The different editions concerned, in this order, the identification of speakers in political speeches, the topical segmentation of political, scientific and juridical corpora (2006), the automatic affectation of opinion values to texts developing an argumented judgment, and the identification of the genre and topic of a document.", "labels": [], "entities": [{"text": "identification of speakers in political speeches", "start_pos": 53, "end_pos": 101, "type": "TASK", "confidence": 0.8803373376528422}]}, {"text": "Human judgment was used during the preparation of the last two campaigns, to assess the difficulty of the task, and to see which parameters could be modified.", "labels": [], "entities": []}, {"text": "To do this, before the participants start competing via their software, we put human judges in front of versions of the task with various sets of parameters.", "labels": [], "entities": []}, {"text": "This allows us to adjust the definition of the task according to which difficulties were encountered, and how judges agree together.", "labels": [], "entities": []}, {"text": "These human judges are in small number, and belong to our team.", "labels": [], "entities": []}, {"text": "However, results of the campaign are automatically evaluated with reference to results attached to the corpus from the start.", "labels": [], "entities": []}, {"text": "This is because the evaluation of a campaign's results by human judges is expensive.", "labels": [], "entities": []}, {"text": "For instance, TREC 2 international evaluation campaigns are supported by the NIST institute and funded by state agencies.", "labels": [], "entities": [{"text": "TREC 2 international evaluation", "start_pos": 14, "end_pos": 45, "type": "TASK", "confidence": 0.6217080056667328}, {"text": "NIST institute", "start_pos": 77, "end_pos": 91, "type": "DATASET", "confidence": 0.9709489047527313}]}, {"text": "In Europe, on the same domains, the CLEF 3 campaigns are funded by the European Commission, and in France, evaluation campaigns are also funded by projects, such as Technolangue . DEFT campaigns, however, are conducted with small budgets.", "labels": [], "entities": [{"text": "Technolangue", "start_pos": 165, "end_pos": 177, "type": "DATASET", "confidence": 0.9187914133071899}]}, {"text": "That means for us to have selected corpora that contain the desired results.", "labels": [], "entities": []}, {"text": "For instance, in a campaign for topical categorization, we must start with a topically tagged corpus.", "labels": [], "entities": [{"text": "topical categorization", "start_pos": 32, "end_pos": 54, "type": "TASK", "confidence": 0.8173753917217255}]}, {"text": "By so doing, we also can, at the end of a campaign, compare results from human judges with results from competitors, using an identical common reference.", "labels": [], "entities": []}, {"text": "In this paper, we describe experiments we performed with human judgments when preparing DEFT campaigns.", "labels": [], "entities": [{"text": "DEFT campaigns", "start_pos": 88, "end_pos": 102, "type": "TASK", "confidence": 0.7800098955631256}]}, {"text": "We survey the various steps in the preparation of the last two campaigns, and we go through the detail of how human evaluation, performed during these steps, led us to the parametrization of these two campaigns.", "labels": [], "entities": []}, {"text": "We also present a comparative analysis of results found by human judges and results submitted by competitors in the challenge.", "labels": [], "entities": []}, {"text": "We conclude about the relevance of the human evaluation of a task, prior to evaluating software dedicated to this task.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Video game corpus: wide scale, marks  from 0 to 20.", "labels": [], "entities": []}, {"text": " Table 2: Video game corpus: restricted scale,  marks from 0 to 2.", "labels": [], "entities": []}, {"text": " Table 3: Video game corpus: agreement of each  judge with himself when scales change.", "labels": [], "entities": []}, {"text": " Table 4: Film review corpus: wide scale, marks  from 0 to 4", "labels": [], "entities": [{"text": "Film review corpus", "start_pos": 10, "end_pos": 28, "type": "DATASET", "confidence": 0.6998635033766428}]}, {"text": " Table 5: Film review corpus: restricted scale,  marks from 0 to 2.", "labels": [], "entities": [{"text": "Film review corpus", "start_pos": 10, "end_pos": 28, "type": "DATASET", "confidence": 0.8060280084609985}]}, {"text": " Table 6: Correspondence between categories from  Le Monde and Wikipedia for the 8 categories in  the test.", "labels": [], "entities": [{"text": "Le Monde", "start_pos": 50, "end_pos": 58, "type": "DATASET", "confidence": 0.9474602341651917}]}, {"text": " Table 7: F-scores obtained by human judges on the  identification of genre and categories.", "labels": [], "entities": [{"text": "F-scores", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9969576597213745}]}, {"text": " Table 8: \u03ba coefficient between human judges and  the reference: Identification of genre.", "labels": [], "entities": [{"text": "\u03ba coefficient", "start_pos": 10, "end_pos": 23, "type": "METRIC", "confidence": 0.9709819555282593}, {"text": "Identification of genre", "start_pos": 65, "end_pos": 88, "type": "TASK", "confidence": 0.8060149153073629}]}, {"text": " Table 9: \u03ba coefficient between human judges and  the reference: Identification of categories.", "labels": [], "entities": [{"text": "\u03ba coefficient", "start_pos": 10, "end_pos": 23, "type": "METRIC", "confidence": 0.9698502123355865}]}, {"text": " Table 10: Minimal and maximal strict F-scores  between human evaluators and competitors in the  challenge, 2007 edition.", "labels": [], "entities": [{"text": "maximal strict F-scores", "start_pos": 23, "end_pos": 46, "type": "METRIC", "confidence": 0.7986039121945699}]}]}