{"title": [{"text": "Parsing German with Latent Variable Grammars", "labels": [], "entities": [{"text": "Parsing German", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.8619937896728516}]}], "abstractContent": [{"text": "We describe experiments on learning latent variable grammars for various German tree-banks, using a language-agnostic statistical approach.", "labels": [], "entities": []}, {"text": "In our method, a minimal initial grammar is hierarchically refined using an adaptive split-and-merge EM procedure, giving compact, accurate grammars.", "labels": [], "entities": []}, {"text": "The learning procedure directly maximizes the likelihood of the training treebank, without the use of any language specific or linguistically constrained features.", "labels": [], "entities": []}, {"text": "Nonetheless, the resulting grammars encode many linguistically inter-pretable patterns and give the best published parsing accuracies on three German treebanks.", "labels": [], "entities": [{"text": "German treebanks", "start_pos": 143, "end_pos": 159, "type": "DATASET", "confidence": 0.793758362531662}]}], "introductionContent": [{"text": "Probabilistic context-free grammars (PCFGs) underlie most high-performance parsers in one way or another).", "labels": [], "entities": []}, {"text": "However, as demonstrated in and, a PCFG which simply takes the empirical rules and probabilities off of a treebank does not perform well.", "labels": [], "entities": []}, {"text": "This naive grammar is a poor one because its contextfreedom assumptions are too strong in some ways (e.g. it assumes that subject and object NPs share the same distribution) and too weak in others (e.g. it assumes that long rewrites do not decompose into smaller steps).", "labels": [], "entities": []}, {"text": "Therefore, a variety of techniques have been developed to both enrich and generalize the naive grammar, ranging from simple tree annotation and symbol splitting to full lexicalization and intricate smoothing).", "labels": [], "entities": [{"text": "symbol splitting", "start_pos": 144, "end_pos": 160, "type": "TASK", "confidence": 0.7847386300563812}]}, {"text": "We view treebank parsing as the search for an optimally refined grammar consistent with a coarse training treebank.", "labels": [], "entities": [{"text": "treebank parsing", "start_pos": 8, "end_pos": 24, "type": "TASK", "confidence": 0.6444641500711441}]}, {"text": "As a result, we begin with the provided evaluation symbols (such as NP, VP, etc.) but split them based on the statistical patterns in the training trees.", "labels": [], "entities": []}, {"text": "A manual approach might take the symbol NP and subdivide it into one subsymbol NP\u02c6S for subjects and another subsymbol NP\u02c6VP for objects.", "labels": [], "entities": []}, {"text": "However, rather than devising linguistically motivated features or splits, we take a fully automated approach, in which each symbol is split into unconstrained subsymbols.", "labels": [], "entities": []}, {"text": "For example, NP would be split into NP-1 through NP-8.", "labels": [], "entities": []}, {"text": "We use the Expectation-Maximization (EM) to then fit our split model to the observed trees; therein the various subsymbols will specialize in ways which mayor may not correspond to our linguistic intuitions.", "labels": [], "entities": [{"text": "Expectation-Maximization (EM)", "start_pos": 11, "end_pos": 40, "type": "METRIC", "confidence": 0.9385992288589478}]}, {"text": "This approach is relatively language independent, because the hidden subsymbols are induced automatically from the training trees based solely on data likelihood, though of course it is most applicable to strongly configurational languages.", "labels": [], "entities": []}, {"text": "In our experiments, we find that we can learn compact grammars that give the highest parsing accuracies in the 2008 Parsing German shared task.", "labels": [], "entities": [{"text": "2008 Parsing German shared task", "start_pos": 111, "end_pos": 142, "type": "DATASET", "confidence": 0.5674620687961578}]}, {"text": "Our F1-scores of 69.8/84.0 (TIGER/TueBa-D/Z) are more than four points higher than those of the second best systems.", "labels": [], "entities": [{"text": "F1-scores", "start_pos": 4, "end_pos": 13, "type": "METRIC", "confidence": 0.9992782473564148}, {"text": "TIGER/TueBa-D/Z)", "start_pos": 28, "end_pos": 44, "type": "METRIC", "confidence": 0.774958590666453}]}, {"text": "Additionally, we investigate the patterns that are learned and show that the latent variable approach recovers linguistically interpretable phenomena.", "labels": [], "entities": []}, {"text": "In our analysis, we pay particular attention to similarities and differences between grammars learned from the two treebanks.", "labels": [], "entities": []}], "datasetContent": [{"text": "We conducted experiments on the two treebanks provided for the 2008 Parsing German shared task.", "labels": [], "entities": [{"text": "2008 Parsing German shared task", "start_pos": 63, "end_pos": 94, "type": "DATASET", "confidence": 0.8228705763816834}]}, {"text": "Both treebanks are annotated collections of German newspaper text, covering from similar topics.", "labels": [], "entities": []}, {"text": "They are annotated with part-of-speech (POS) tags, morphological information, phrase structure, and grammatical functions.", "labels": [], "entities": []}, {"text": "TueBa-D/Z additionally uses topological fields to describe fundamental word order restrictions in German clauses.", "labels": [], "entities": [{"text": "TueBa-D/Z", "start_pos": 0, "end_pos": 9, "type": "DATASET", "confidence": 0.9186581969261169}]}, {"text": "However, the treebanks differ significantly in their annotation schemes: while TIGER relies on crossing branches to describe long distance relationships, TueBa-D/Z uses planar tree structures with designated labels that encode long distance relationships.", "labels": [], "entities": []}, {"text": "Additionally, the annotation in TIGER is relatively flat on the phrasal level, while TueBa-D/Z annotates more internal phrase structure.", "labels": [], "entities": []}, {"text": "We used the standard splits into training and de- velopment set, containing roughly 16,000 training trees and 1,600 development trees, respectively.", "labels": [], "entities": []}, {"text": "All parsing figures in this section are on the development set, evaluating on constituents and grammatical functions using gold part-of-speech tags, unless noted otherwise.", "labels": [], "entities": []}, {"text": "Note that even when we assume gold evaluation part-of-speech tags, we still assign probabilities to the different subsymbols of the provided evaluation tag.", "labels": [], "entities": []}, {"text": "The parsing accuracies in the final results section are the official results of the 2008 Parsing German shared task.", "labels": [], "entities": [{"text": "2008 Parsing German shared task", "start_pos": 84, "end_pos": 115, "type": "DATASET", "confidence": 0.6437418758869171}]}], "tableCaptions": [{"text": " Table 1: Our split-and-merge latent variable approach  produces the best published parsing performance on  many languages.", "labels": [], "entities": []}, {"text": " Table 2: Parsing accuracies (F1-score and exact match)  with gold POS tags and automatic POS tags. Many parse  errors are due to incorrect tagging.", "labels": [], "entities": [{"text": "accuracies", "start_pos": 18, "end_pos": 28, "type": "METRIC", "confidence": 0.6792116761207581}, {"text": "F1-score", "start_pos": 30, "end_pos": 38, "type": "METRIC", "confidence": 0.9974029660224915}, {"text": "exact match", "start_pos": 43, "end_pos": 54, "type": "METRIC", "confidence": 0.9524762630462646}]}]}