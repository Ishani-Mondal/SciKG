{"title": [{"text": "Semantic Chunk Annotation for complex questions using Conditional Random Field", "labels": [], "entities": [{"text": "Semantic Chunk Annotation", "start_pos": 0, "end_pos": 25, "type": "TASK", "confidence": 0.6124381323655447}]}], "abstractContent": [{"text": "This paper presents a CRF (Conditional Random Field) model for Semantic", "labels": [], "entities": [{"text": "Semantic", "start_pos": 63, "end_pos": 71, "type": "TASK", "confidence": 0.6638661026954651}]}], "introductionContent": [], "datasetContent": [{"text": "Feature selection is important in classifying systems such as neural networks (NNs), Maximum Entropy, Conditional Random Field and etc.", "labels": [], "entities": [{"text": "Feature selection", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.7312250137329102}]}, {"text": "The problem of feature selection has been tackled by many researchers.", "labels": [], "entities": [{"text": "feature selection", "start_pos": 15, "end_pos": 32, "type": "TASK", "confidence": 0.8666587769985199}]}, {"text": "Principal component analysis (PCA) method and Rough Set Method are often used for feature selection.", "labels": [], "entities": [{"text": "Principal component analysis (PCA)", "start_pos": 0, "end_pos": 34, "type": "TASK", "confidence": 0.7349875668684641}, {"text": "feature selection", "start_pos": 82, "end_pos": 99, "type": "TASK", "confidence": 0.8180115818977356}]}, {"text": "Recent years, mutual information has received more attention for feature selection problem.", "labels": [], "entities": [{"text": "feature selection", "start_pos": 65, "end_pos": 82, "type": "TASK", "confidence": 0.865369588136673}]}, {"text": "So that the goal of a feature selection problem is to find a feature S ( ), which achieve the higher values of . The set S is a subset of F and its size should be as small as possible.", "labels": [], "entities": []}, {"text": "There are some algorithms for feature selection problem.", "labels": [], "entities": [{"text": "feature selection", "start_pos": 30, "end_pos": 47, "type": "TASK", "confidence": 0.8291738927364349}]}, {"text": "The ideal greedy selection algorithm using mutual information is realized as follows): Input: S-an empty set F-The selected feature set Output: a small reduced feature set S which is equivalent to F Step 1: calculate the MI with the Class Step 2: select the feature that maximizes , Step 3: repeat until desired number of features are selected.", "labels": [], "entities": [{"text": "Output", "start_pos": 136, "end_pos": 142, "type": "METRIC", "confidence": 0.9656345248222351}, {"text": "repeat", "start_pos": 291, "end_pos": 297, "type": "METRIC", "confidence": 0.9834253191947937}]}, {"text": "1) Calculate the MI with the Class set C and S, Step 4: Output the set S that contains the selected features To calculate MI the PDFs (Probability Distribution Functions) are required.", "labels": [], "entities": []}, {"text": "When features and classing types are dispersing, the probability can be calculated statistically.", "labels": [], "entities": []}, {"text": "In our system, the PDFs are got from the training corpus statistically.", "labels": [], "entities": []}, {"text": "The training corpus contains 14000 sentences.", "labels": [], "entities": []}, {"text": "The training corpus was divided into 10 parts, with each part 1400 sentences.", "labels": [], "entities": []}, {"text": "And each part is divided into working set and checking set.", "labels": [], "entities": [{"text": "checking", "start_pos": 46, "end_pos": 54, "type": "METRIC", "confidence": 0.9248776435852051}]}, {"text": "The working set, which contains 90% percent data, was used to select feature by MI algorithm.", "labels": [], "entities": []}, {"text": "The checking set, which contains 10% percent data, was used to test the performance of the selected feature sequence.", "labels": [], "entities": []}, {"text": "When the feature sequence was selected by the MI algorithm, a sequence of CRF models was trained by adding one feature at each time.", "labels": [], "entities": []}, {"text": "The checking data was used to test the performance of these models.", "labels": [], "entities": []}, {"text": "clear that the feature 7(Question pattern) and 10(Pattern tag) are very important, while the feature 8(Question type) and 9(Is pattern key word) are not necessary.", "labels": [], "entities": []}, {"text": "The explanation about this phenomenon is that the \"pattern key word\" and \"Question type\" information can be covered by the Question patterns.", "labels": [], "entities": []}, {"text": "So feature 8 and 9 are not used in the Conditional Random Field model.", "labels": [], "entities": []}, {"text": "The test and training data used in our system are collected from the website (Baidu knowledge and the Ask-Answer system), where people proposed questions and answers.", "labels": [], "entities": [{"text": "Baidu knowledge", "start_pos": 78, "end_pos": 93, "type": "DATASET", "confidence": 0.930204451084137}]}, {"text": "The training data consists of 14000 and the test data consists of 4000 sentences.", "labels": [], "entities": []}, {"text": "The data set consists of word tokens, POS and semantic chunk tags.", "labels": [], "entities": []}, {"text": "The POS and semantic tags are assigned to each word tokens.", "labels": [], "entities": []}, {"text": "The performance is measured with three rates: precision (Pre), recall (Rec) and F-score (F1).", "labels": [], "entities": [{"text": "precision (Pre)", "start_pos": 46, "end_pos": 61, "type": "METRIC", "confidence": 0.8156770914793015}, {"text": "recall (Rec)", "start_pos": 63, "end_pos": 75, "type": "METRIC", "confidence": 0.9466772973537445}, {"text": "F-score (F1)", "start_pos": 80, "end_pos": 92, "type": "METRIC", "confidence": 0.8545701056718826}]}, {"text": "Pre = Match/Model (5) Rec=Match/Manual (6) F1=2*Pre*Rec/(Pre+Rec) (7) Match is the count of the tags that was predicted right.", "labels": [], "entities": [{"text": "Pre", "start_pos": 0, "end_pos": 3, "type": "METRIC", "confidence": 0.9284508228302002}, {"text": "F1", "start_pos": 43, "end_pos": 45, "type": "METRIC", "confidence": 0.9488682150840759}, {"text": "Match", "start_pos": 70, "end_pos": 75, "type": "METRIC", "confidence": 0.9926776885986328}]}, {"text": "Model is the count of the tags that was predicted by the model.", "labels": [], "entities": []}, {"text": "Manual is the count of the tags that was labeled manually.", "labels": [], "entities": [{"text": "Manual", "start_pos": 0, "end_pos": 6, "type": "METRIC", "confidence": 0.9503456354141235}]}, {"text": "shows the performance of annotation of different semantic chunk types.", "labels": [], "entities": []}, {"text": "The first column is the semantic chunk tag.", "labels": [], "entities": []}, {"text": "The last three columns are precision, recall and F1 value of the semantic chunk performance, respectively.: the performance of different semantic chunk The semantic chunk type of \"Topic\" and \"Focus\" can be annotated well.", "labels": [], "entities": [{"text": "precision", "start_pos": 27, "end_pos": 36, "type": "METRIC", "confidence": 0.9996414184570312}, {"text": "recall", "start_pos": 38, "end_pos": 44, "type": "METRIC", "confidence": 0.9979901313781738}, {"text": "F1", "start_pos": 49, "end_pos": 51, "type": "METRIC", "confidence": 0.9992695450782776}]}, {"text": "Topic and focus semantic chunks have a large percentage in all the semantic chunks and they are important for question analyzing.", "labels": [], "entities": [{"text": "question analyzing", "start_pos": 110, "end_pos": 128, "type": "TASK", "confidence": 0.8489724695682526}]}, {"text": "So the result is really good for the whole Q&A system.", "labels": [], "entities": [{"text": "Q&A", "start_pos": 43, "end_pos": 46, "type": "TASK", "confidence": 0.9135286211967468}]}, {"text": "As for \"Rubbish\" semantic chunk, it only has 0.51 and 0.0 F1 measure for B-Ru and I-Ru.", "labels": [], "entities": [{"text": "F1 measure", "start_pos": 58, "end_pos": 68, "type": "METRIC", "confidence": 0.9880257248878479}]}, {"text": "One reason is lacking enough training examples, for there are only 1031 occurrences in the training data.", "labels": [], "entities": []}, {"text": "Another reason is sometimes restriction is complex.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: the feature selection result and the test result", "labels": [], "entities": []}]}