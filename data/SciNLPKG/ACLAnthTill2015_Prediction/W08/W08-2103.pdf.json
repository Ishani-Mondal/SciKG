{"title": [{"text": "A Fast Boosting-based Learner for Feature-Rich Tagging and Chunking", "labels": [], "entities": [{"text": "Feature-Rich Tagging", "start_pos": 34, "end_pos": 54, "type": "TASK", "confidence": 0.6188595741987228}, {"text": "Chunking", "start_pos": 59, "end_pos": 67, "type": "TASK", "confidence": 0.7739332914352417}]}], "abstractContent": [{"text": "Combination of features contributes to a significant improvement inaccuracy on tasks such as part-of-speech (POS) tagging and text chunking, compared with using atomic features.", "labels": [], "entities": [{"text": "part-of-speech (POS) tagging", "start_pos": 93, "end_pos": 121, "type": "TASK", "confidence": 0.6449676156044006}, {"text": "text chunking", "start_pos": 126, "end_pos": 139, "type": "TASK", "confidence": 0.7488883435726166}]}, {"text": "However, selecting combination of features on learning with large-scale and feature-rich training data requires long training time.", "labels": [], "entities": []}, {"text": "We propose a fast boosting-based algorithm for learning rules represented by combination of features.", "labels": [], "entities": []}, {"text": "Our algorithm constructs a set of rules by repeating the process to select several rules from a small proportion of candidate rules.", "labels": [], "entities": []}, {"text": "The candidate rules are generated from a subset of all the features with a technique similar to beam search.", "labels": [], "entities": [{"text": "beam search", "start_pos": 96, "end_pos": 107, "type": "TASK", "confidence": 0.7784505486488342}]}, {"text": "Then we propose POS tagging and text chunk-ing based on our learning algorithm.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 16, "end_pos": 27, "type": "TASK", "confidence": 0.8152056038379669}]}, {"text": "Our tagger and chunker use candidate POS tags or chunk tags of each word collected from automatically tagged data.", "labels": [], "entities": []}, {"text": "We evaluate our methods with English POS tagging and text chunking.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 37, "end_pos": 48, "type": "TASK", "confidence": 0.7111432403326035}, {"text": "text chunking", "start_pos": 53, "end_pos": 66, "type": "TASK", "confidence": 0.7345549166202545}]}, {"text": "The experimental results show that the training time of our algorithm are about 50 times faster than Support Vector Machines with polynomial kernel on the average while maintaining state-of-the-art accuracy and faster classification speed.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 198, "end_pos": 206, "type": "METRIC", "confidence": 0.9993664622306824}]}], "introductionContent": [{"text": "Several boosting-based learning algorithms have been applied to Natural Language Processing problems successfully.", "labels": [], "entities": []}, {"text": "These include text categorization), Natural Language Parsing), English syntactic chunking () and soon.", "labels": [], "entities": [{"text": "Natural Language Parsing", "start_pos": 36, "end_pos": 60, "type": "TASK", "confidence": 0.6088895897070566}, {"text": "English syntactic chunking", "start_pos": 63, "end_pos": 89, "type": "TASK", "confidence": 0.6618718306223551}, {"text": "soon", "start_pos": 97, "end_pos": 101, "type": "METRIC", "confidence": 0.9546602368354797}]}, {"text": "Furthermore, classifiers based on boostingbased learners have shown fast classification speed ().", "labels": [], "entities": []}, {"text": "However, boosting-based learning algorithms require long training time.", "labels": [], "entities": []}, {"text": "One of the reasons is that boosting is a method to create a final hypothesis by repeatedly generating a weak hypothesis in each training iteration with a given weak learner.", "labels": [], "entities": []}, {"text": "These weak hypotheses are combined as the final hypothesis.", "labels": [], "entities": []}, {"text": "Furthermore, the training speed of boosting-based algorithms becomes more of a problem when considering combination of features that contributes to improvement inaccuracy.", "labels": [], "entities": []}, {"text": "This paper proposes a fast boosting-based algorithm for learning rules represented by combination of features.", "labels": [], "entities": []}, {"text": "We also propose feature-rich POS tagging and text chunking based on our learning algorithm.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 29, "end_pos": 40, "type": "TASK", "confidence": 0.7836266160011292}, {"text": "text chunking", "start_pos": 45, "end_pos": 58, "type": "TASK", "confidence": 0.7270902544260025}]}, {"text": "Our POS tagger and text chunker use candidate tags of each word obtained from automatically tagged data as features.", "labels": [], "entities": [{"text": "POS tagger", "start_pos": 4, "end_pos": 14, "type": "TASK", "confidence": 0.7290946245193481}, {"text": "text chunker", "start_pos": 19, "end_pos": 31, "type": "TASK", "confidence": 0.6771968901157379}]}, {"text": "The experimental results with English POS tagging and text chunking show drastically improvement of training speeds while maintaining competitive accuracy compared with previous best results and fast classification speeds.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 38, "end_pos": 49, "type": "TASK", "confidence": 0.6877469122409821}, {"text": "text chunking", "start_pos": 54, "end_pos": 67, "type": "TASK", "confidence": 0.7381084561347961}, {"text": "accuracy", "start_pos": 146, "end_pos": 154, "type": "METRIC", "confidence": 0.9693846702575684}]}], "datasetContent": [{"text": "We compared AdaBoost.SDF with Support Vector Machines (SVM).", "labels": [], "entities": []}, {"text": "SVM has shown good performance on POS tagging ( and Text Chunking ().", "labels": [], "entities": [{"text": "SVM", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.8245909214019775}, {"text": "POS tagging", "start_pos": 34, "end_pos": 45, "type": "TASK", "confidence": 0.7260294258594513}]}, {"text": "Furthermore, SVM with polynomial kernel implicitly expands all feature combinations without increasing the computational costs.", "labels": [], "entities": []}, {"text": "Thus, we compared AdaBoost.SDF with SVM.", "labels": [], "entities": []}, {"text": "To evaluate the effectiveness of candidate features, we examined two types of experiments with candidate features and without them.", "labels": [], "entities": []}, {"text": "We list the statics of training sets in.", "labels": [], "entities": []}, {"text": "We tested R=100,000, |B|=1,000, \u03bd = {1,10,100}, \u03c9={1,10,100,\u221e}, \u03b6={1,2,3}, and \u03be={1,5} for AdaBoost.SDF.", "labels": [], "entities": [{"text": "AdaBoost.SDF", "start_pos": 91, "end_pos": 103, "type": "DATASET", "confidence": 0.9069077968597412}]}, {"text": "We tested the soft margin parameter C={0.1,1,10} and the kernel degree d={1,2,3} for SVM.", "labels": [], "entities": [{"text": "soft margin parameter C", "start_pos": 14, "end_pos": 37, "type": "METRIC", "confidence": 0.7540015578269958}]}, {"text": "We used the followings for comparison; Training time is time to learn 100,000 rules.", "labels": [], "entities": []}, {"text": "Best training time is time for generating rules to show the best F-measure (F \u03b2=1 ) on development data.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 65, "end_pos": 74, "type": "METRIC", "confidence": 0.9925243258476257}, {"text": "F \u03b2=1 )", "start_pos": 76, "end_pos": 83, "type": "METRIC", "confidence": 0.9560750722885132}]}, {"text": "Accuracy is F \u03b2=1 on a test data with the rules at best training time.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 0, "end_pos": 8, "type": "METRIC", "confidence": 0.9740877747535706}, {"text": "F \u03b2", "start_pos": 12, "end_pos": 15, "type": "METRIC", "confidence": 0.9725344777107239}]}, {"text": "We used F-dist as the distribution method.", "labels": [], "entities": [{"text": "F-dist", "start_pos": 8, "end_pos": 14, "type": "METRIC", "confidence": 0.8466442823410034}]}, {"text": "These average accuracy obtained with rules learned by AdaBoost.SDF ( \u03bd=10 ) on both tasks are competitive with the average accuracy obtained with rules learned by AdaBoost.SDF ( \u03bd=1 ).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 14, "end_pos": 22, "type": "METRIC", "confidence": 0.9977776408195496}, {"text": "AdaBoost.SDF", "start_pos": 54, "end_pos": 66, "type": "DATASET", "confidence": 0.937562108039856}, {"text": "accuracy", "start_pos": 123, "end_pos": 131, "type": "METRIC", "confidence": 0.9632666110992432}, {"text": "AdaBoost.SDF", "start_pos": 163, "end_pos": 175, "type": "DATASET", "confidence": 0.9344391822814941}]}, {"text": "These results have shown that learning several rules at each iteration contributes significant improvement of training time.", "labels": [], "entities": []}, {"text": "These results have also shown that the learning several rule at each iteration methods are more efficient than training by just using the frequency constraint \u03be. shows a snapshot for accuracy obtained with chunkers using different number of rules.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 183, "end_pos": 191, "type": "METRIC", "confidence": 0.9992992877960205}]}, {"text": "This graph shows that chunkers based on AdaBoost.SDF ( \u03bd=10,100 ) and AdaBoost.SDF ( \u03bd=1,\u03c9={1,10,100} ) have shown better accuracy than chunkers based on AdaBoost.SDF ( \u03bd=1,\u03c9=\u221e ) at each training time.", "labels": [], "entities": [{"text": "AdaBoost.SDF", "start_pos": 40, "end_pos": 52, "type": "DATASET", "confidence": 0.9423906207084656}, {"text": "AdaBoost.SDF", "start_pos": 70, "end_pos": 82, "type": "DATASET", "confidence": 0.9605868458747864}, {"text": "accuracy", "start_pos": 122, "end_pos": 130, "type": "METRIC", "confidence": 0.9986281394958496}]}, {"text": "These result have shown that learning several rules at each iteration and learning combination of features as rules with a technique similar to beam search are effective in improving training time while giving a better convergence.", "labels": [], "entities": [{"text": "beam search", "start_pos": 144, "end_pos": 155, "type": "TASK", "confidence": 0.8321376740932465}]}, {"text": "also implies that taggers and chunkers based on AdaBoost.SDF ( \u03bd=100 ) will show better or competitive accuracy than accuracy of the others by increasing numbers of rules to be learned while maintaining faster convergence speed.", "labels": [], "entities": [{"text": "AdaBoost.SDF", "start_pos": 48, "end_pos": 60, "type": "DATASET", "confidence": 0.8965879082679749}, {"text": "accuracy", "start_pos": 103, "end_pos": 111, "type": "METRIC", "confidence": 0.9891942739486694}, {"text": "accuracy", "start_pos": 117, "end_pos": 125, "type": "METRIC", "confidence": 0.998687207698822}]}, {"text": "lists average accuracy and training time on POS tagging and text chunking with respect to each (\u03bd, \u03b6) for AdaBoost.SDF and d for SVM.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 14, "end_pos": 22, "type": "METRIC", "confidence": 0.9996919631958008}, {"text": "POS tagging", "start_pos": 44, "end_pos": 55, "type": "TASK", "confidence": 0.7721634209156036}, {"text": "text chunking", "start_pos": 60, "end_pos": 73, "type": "TASK", "confidence": 0.6916837394237518}, {"text": "AdaBoost.SDF", "start_pos": 106, "end_pos": 118, "type": "DATASET", "confidence": 0.904466450214386}]}, {"text": "AdaBoost.SDF with \u03bd=10 and \u03bd=100 have shown much faster training speeds than SVM and AdaBoost.SDF ( \u03bd=1,\u03c9=\u221e ) that is equivalent to the AdaBoost.DF.", "labels": [], "entities": [{"text": "AdaBoost.SDF", "start_pos": 0, "end_pos": 12, "type": "DATASET", "confidence": 0.9556782245635986}, {"text": "AdaBoost.SDF", "start_pos": 85, "end_pos": 97, "type": "DATASET", "confidence": 0.9375258088111877}, {"text": "AdaBoost.DF", "start_pos": 136, "end_pos": 147, "type": "DATASET", "confidence": 0.9370684027671814}]}, {"text": "Furthermore, the accuracy of taggers and chunkers based on AdaBoost.SDF ( \u03bd=10 ) have shown competitive accuracy with those of SVM-based and AdaBoost.DF-based taggers and chunkers.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 17, "end_pos": 25, "type": "METRIC", "confidence": 0.999442994594574}, {"text": "accuracy", "start_pos": 104, "end_pos": 112, "type": "METRIC", "confidence": 0.998627781867981}]}, {"text": "AdaBoost.SDF ( \u03bd=10 ) showed about 6 and 54 times faster training speeds than those of AdaBoost.DF on the average in POS tagging and text chunking.", "labels": [], "entities": [{"text": "AdaBoost.SDF", "start_pos": 0, "end_pos": 12, "type": "DATASET", "confidence": 0.9374733567237854}, {"text": "POS tagging", "start_pos": 117, "end_pos": 128, "type": "TASK", "confidence": 0.8387191891670227}, {"text": "text chunking", "start_pos": 133, "end_pos": 146, "type": "TASK", "confidence": 0.7726301550865173}]}, {"text": "AdaBoost.SDF ( \u03bd=10 ) showed about 147 and 9 times faster training speeds than the training speeds of SVM on the average of POS tagging and text chunking.", "labels": [], "entities": [{"text": "AdaBoost.SDF", "start_pos": 0, "end_pos": 12, "type": "DATASET", "confidence": 0.9279201626777649}, {"text": "POS tagging", "start_pos": 124, "end_pos": 135, "type": "TASK", "confidence": 0.7447811663150787}, {"text": "text chunking", "start_pos": 140, "end_pos": 153, "type": "TASK", "confidence": 0.72272789478302}]}, {"text": "On the average of the both tasks, AdaBoost.SDF ( \u03bd=10 ) showed about 25 and 50 times faster training speed than AdaBoost.DF and SVM.", "labels": [], "entities": [{"text": "AdaBoost.SDF", "start_pos": 34, "end_pos": 46, "type": "DATASET", "confidence": 0.8652371168136597}, {"text": "AdaBoost.DF", "start_pos": 112, "end_pos": 123, "type": "DATASET", "confidence": 0.9279443621635437}]}, {"text": "These results have shown that AdaBoost.SDF with a moderate parameter \u03bd can improve training time drastically while maintaining accuracy.", "labels": [], "entities": [{"text": "AdaBoost.SDF", "start_pos": 30, "end_pos": 42, "type": "DATASET", "confidence": 0.8440210819244385}, {"text": "accuracy", "start_pos": 127, "end_pos": 135, "type": "METRIC", "confidence": 0.9981424808502197}]}], "tableCaptions": [{"text": " Table 1: Training data for experiments: of S, M , of", "labels": [], "entities": []}, {"text": " Table 2: Experimental results of POS tagging and Text", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 34, "end_pos": 45, "type": "TASK", "confidence": 0.8319322168827057}, {"text": "Text", "start_pos": 50, "end_pos": 54, "type": "TASK", "confidence": 0.7130184769630432}]}, {"text": " Table 3: Experimental results on POS tagging and Text", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 34, "end_pos": 45, "type": "TASK", "confidence": 0.8430610001087189}, {"text": "Text", "start_pos": 50, "end_pos": 54, "type": "TASK", "confidence": 0.7554901838302612}]}, {"text": " Table 4: Results obtained with taggers and chunkers based", "labels": [], "entities": []}, {"text": " Table 5: Comparison with previous best results: (Top :", "labels": [], "entities": []}]}