{"title": [{"text": "Kernel Regression Framework for Machine Translation: UCL System Description for WMT 2008 Shared Translation Task", "labels": [], "entities": [{"text": "Machine Translation", "start_pos": 32, "end_pos": 51, "type": "TASK", "confidence": 0.7183109670877457}, {"text": "WMT 2008 Shared Translation", "start_pos": 80, "end_pos": 107, "type": "TASK", "confidence": 0.6631534025073051}]}], "abstractContent": [{"text": "The novel kernel regression model for SMT only demonstrated encouraging results on small-scale toy data sets in previous works due to the complexities of kernel methods.", "labels": [], "entities": [{"text": "SMT", "start_pos": 38, "end_pos": 41, "type": "TASK", "confidence": 0.9950757622718811}]}, {"text": "It is the first time results based on the real-world data from the shared translation task will be reported at ACL 2008 Workshop on Statistical Machine Translation.", "labels": [], "entities": [{"text": "ACL 2008 Workshop on Statistical Machine Translation", "start_pos": 111, "end_pos": 163, "type": "TASK", "confidence": 0.8093104617936271}]}, {"text": "This paper presents the key modules of our system, including the kernel ridge regression model, retrieval-based sparse approximation, the decoding algorithm, as well as language modeling issues under this framework.", "labels": [], "entities": [{"text": "retrieval-based sparse approximation", "start_pos": 96, "end_pos": 132, "type": "TASK", "confidence": 0.582684854666392}]}], "introductionContent": [{"text": "This paper follows the work in () which applied the kernel regression method with high-dimensional outputs proposed originally in () to statistical machine translation (SMT) tasks.", "labels": [], "entities": [{"text": "statistical machine translation (SMT)", "start_pos": 136, "end_pos": 173, "type": "TASK", "confidence": 0.8077008823553721}]}, {"text": "In our approach, the machine translation problem is viewed as a string-to-string mapping, where both the source and the target strings are embedded into their respective kernel induced feature spaces.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 21, "end_pos": 40, "type": "TASK", "confidence": 0.8066918849945068}]}, {"text": "Then kernel ridge regression is employed to learn the mapping from the input feature space to the output one.", "labels": [], "entities": []}, {"text": "As a kernel method, this model offers the potential advantages of capturing very high-dimensional correspondences among the features of the source and target languages as well as easy integration of additional linguistic knowledge via selecting particular kernels.", "labels": [], "entities": []}, {"text": "However, unlike the sequence labeling tasks such as optical character recognition in), the complexity of the SMT problem itself together with the computational complexities of kernel methods significantly complicate the implementation of the regression technique in this field.", "labels": [], "entities": [{"text": "sequence labeling", "start_pos": 20, "end_pos": 37, "type": "TASK", "confidence": 0.6864188760519028}, {"text": "optical character recognition", "start_pos": 52, "end_pos": 81, "type": "TASK", "confidence": 0.639488528172175}, {"text": "SMT problem", "start_pos": 109, "end_pos": 120, "type": "TASK", "confidence": 0.9363563060760498}]}, {"text": "Our system is actually designed as a hybrid of the classic phrase-based SMT model ( and the kernel regression model as follows: First, for each source sentence a small relevant set of sentence pairs are retrieved from the large-scale parallel corpus.", "labels": [], "entities": [{"text": "SMT", "start_pos": 72, "end_pos": 75, "type": "TASK", "confidence": 0.8556484580039978}]}, {"text": "Then, the regression model is trained on this small relevant set only as a sparse approximation of the regression hyperplane trained on the entire training set, as proposed in ().", "labels": [], "entities": []}, {"text": "Finally, abeam search algorithm is utilized to decode the target sentence from the very noisy output feature vector we predicted, with the support of a pre-trained phrase table to generate possible hypotheses (candidate translations).", "labels": [], "entities": []}, {"text": "In addition, a language model trained on a monolingual corpus can be integrated either directly into the regression model or during the decoding procedure as an extra scoring function.", "labels": [], "entities": []}, {"text": "Before describing each key component of our system in detail, we give a block diagram overview in.", "labels": [], "entities": []}], "datasetContent": [{"text": "Preliminary experiments are carried out on the French-English portion of the Europarl corpus.", "labels": [], "entities": [{"text": "French-English portion of the Europarl corpus", "start_pos": 47, "end_pos": 92, "type": "DATASET", "confidence": 0.6983271588881811}]}, {"text": "We System BLEU (%) NIST METEOR (%) TER (%) WER (%) PER  train our regression model on the training set, and test the effects of different language models on the development set (test2007).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9984650611877441}, {"text": "NIST", "start_pos": 19, "end_pos": 23, "type": "METRIC", "confidence": 0.823794424533844}, {"text": "METEOR", "start_pos": 24, "end_pos": 30, "type": "METRIC", "confidence": 0.7931315302848816}, {"text": "TER", "start_pos": 35, "end_pos": 38, "type": "METRIC", "confidence": 0.9736167788505554}, {"text": "WER", "start_pos": 43, "end_pos": 46, "type": "METRIC", "confidence": 0.965116024017334}, {"text": "PER", "start_pos": 51, "end_pos": 54, "type": "METRIC", "confidence": 0.9579609036445618}]}, {"text": "The results evaluated by BLEU score () is shown in.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 25, "end_pos": 35, "type": "METRIC", "confidence": 0.9641061425209045}]}, {"text": "It can be found that integrating the language model into the regression framework works slightly better than just using it as an additional score component during decoding.", "labels": [], "entities": []}, {"text": "But language models of higher-order than the n-gram kernel cannot be formulated to the regression problem, which would be a drawback of our system.", "labels": [], "entities": []}, {"text": "Furthermore, the BLEU score performance suggests that our model is not very powerful, but some interesting hints can be found in when we compare our method with a 5-gram language model to a state-of-the-art system Moses () based on various evaluation metrics, including BLEU score, NIST score), METEOR (Banerjee and), TER (), WER and PER.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 17, "end_pos": 27, "type": "METRIC", "confidence": 0.9645005464553833}, {"text": "BLEU score", "start_pos": 270, "end_pos": 280, "type": "METRIC", "confidence": 0.9832794368267059}, {"text": "METEOR", "start_pos": 295, "end_pos": 301, "type": "METRIC", "confidence": 0.9892472624778748}, {"text": "TER", "start_pos": 318, "end_pos": 321, "type": "METRIC", "confidence": 0.9922382235527039}, {"text": "WER", "start_pos": 326, "end_pos": 329, "type": "METRIC", "confidence": 0.9935165643692017}, {"text": "PER", "start_pos": 334, "end_pos": 337, "type": "METRIC", "confidence": 0.985680341720581}]}, {"text": "It is shown that our system's TER, WER and PER scores are very close to Moses, though the gaps in BLEU, NIST and METEOR are significant, which suggests that we would be able to produce accurate translations but might not be good at making fluent sentences.", "labels": [], "entities": [{"text": "TER", "start_pos": 30, "end_pos": 33, "type": "METRIC", "confidence": 0.9975448250770569}, {"text": "WER", "start_pos": 35, "end_pos": 38, "type": "METRIC", "confidence": 0.9669314622879028}, {"text": "PER", "start_pos": 43, "end_pos": 46, "type": "METRIC", "confidence": 0.9519040584564209}, {"text": "BLEU", "start_pos": 98, "end_pos": 102, "type": "METRIC", "confidence": 0.9991238713264465}, {"text": "NIST", "start_pos": 104, "end_pos": 108, "type": "METRIC", "confidence": 0.7005520462989807}, {"text": "METEOR", "start_pos": 113, "end_pos": 119, "type": "METRIC", "confidence": 0.9754428863525391}]}], "tableCaptions": [{"text": " Table 3: Evaluations based on different metrics with comparison to Moses.", "labels": [], "entities": []}]}