{"title": [{"text": "The TUNA Challenge 2008: Overview and Evaluation Results", "labels": [], "entities": [{"text": "TUNA Challenge 2008", "start_pos": 4, "end_pos": 23, "type": "DATASET", "confidence": 0.7872684995333353}]}], "abstractContent": [{"text": "The TUNA Challenge was a set of three shared tasks at REG'08, all of which used data from the TUNA Corpus.", "labels": [], "entities": [{"text": "REG'08", "start_pos": 54, "end_pos": 60, "type": "DATASET", "confidence": 0.9224766492843628}, {"text": "TUNA Corpus", "start_pos": 94, "end_pos": 105, "type": "DATASET", "confidence": 0.9555608034133911}]}, {"text": "The three tasks covered attribute selection for referring expressions (TUNA-AS), realisation (TUNA-R) and end-to-end referring expression generation (TUNA-REG).", "labels": [], "entities": [{"text": "referring expression generation", "start_pos": 117, "end_pos": 148, "type": "TASK", "confidence": 0.7046083807945251}]}, {"text": "8 teams submitted a total of 33 systems to the three tasks, with an additional submission to the Open Track.", "labels": [], "entities": [{"text": "Open Track", "start_pos": 97, "end_pos": 107, "type": "DATASET", "confidence": 0.8817945122718811}]}, {"text": "The evaluation used a range of automatically computed measures.", "labels": [], "entities": []}, {"text": "In addition, an evaluation experiment was carried out using the peer outputs for the TUNA-REG task.", "labels": [], "entities": [{"text": "TUNA-REG", "start_pos": 85, "end_pos": 93, "type": "DATASET", "confidence": 0.679336667060852}]}, {"text": "This report describes each task and the evaluation methods used, and presents the evaluation results.", "labels": [], "entities": []}], "introductionContent": [{"text": "The TUNA Challenge 2008 built on the foundations laid in the, which consisted of a single shared task, based on a subset of the TUNA Corpus ( . The TUNA Corpus is a collection of human-authored descriptions of a referent, paired with a representation of the domain in which that description was elicited.", "labels": [], "entities": [{"text": "TUNA Challenge 2008", "start_pos": 4, "end_pos": 23, "type": "DATASET", "confidence": 0.76272052526474}, {"text": "TUNA Corpus", "start_pos": 128, "end_pos": 139, "type": "DATASET", "confidence": 0.9670305848121643}, {"text": "TUNA Corpus is a collection of human-authored descriptions of a referent, paired with a representation of the domain in which that description was elicited", "start_pos": 148, "end_pos": 303, "type": "Description", "confidence": 0.8158031034469605}]}, {"text": "The 2008 Challenge expanded the scope of the previous edition in a variety of ways.", "labels": [], "entities": []}, {"text": "This year, there were three shared tasks.", "labels": [], "entities": []}, {"text": "TUNA-AS is the Attribute Selection task piloted in the 2007 ASGRE Challenge, which involves the selection of a set of attributes which are true of a target referent, and help to distinguish it from its distractors in a domain.", "labels": [], "entities": [{"text": "TUNA-AS", "start_pos": 0, "end_pos": 7, "type": "METRIC", "confidence": 0.5448874235153198}, {"text": "ASGRE Challenge", "start_pos": 60, "end_pos": 75, "type": "TASK", "confidence": 0.6849827766418457}]}, {"text": "TUNA-R is a realisation task, involving the mapping from attribute sets to linguistic descriptions.", "labels": [], "entities": []}, {"text": "TUNA-REG is an 'end to end' referring expression generation task, involving a mapping from an input domain to a linguistic description of a target referent.", "labels": [], "entities": [{"text": "referring expression generation task", "start_pos": 28, "end_pos": 64, "type": "TASK", "confidence": 0.7355477958917618}]}, {"text": "In addition, there was an Open Submission Track, where participants were invited to submit a report on any interesting research that involved the shared task data, and an Evaluation Track, for which submissions were invited on proposals for evaluation methods.", "labels": [], "entities": []}, {"text": "This year's TUNA Challenge also expanded considerably on the evaluation methods used in the various tasks.", "labels": [], "entities": [{"text": "TUNA Challenge", "start_pos": 12, "end_pos": 26, "type": "TASK", "confidence": 0.5375689715147018}]}, {"text": "The measures can be divided into intrinsic, automatically computed methods, and extrinsic measures obtained through a task-oriented experiment involving human participants.", "labels": [], "entities": []}, {"text": "The training and development data for the Challenge included the full dataset used in the ASGRE Challenge, that is, all of the 2007 training, development and test data.", "labels": [], "entities": [{"text": "ASGRE Challenge", "start_pos": 90, "end_pos": 105, "type": "DATASET", "confidence": 0.624677836894989}]}, {"text": "For the 2008 edition, two new test sets were constructed.", "labels": [], "entities": []}, {"text": "Test Set 1 was used for TUNA-R, Test Set 2 was used for both TUNA-AS and TUNA-REG.", "labels": [], "entities": [{"text": "Test Set", "start_pos": 0, "end_pos": 8, "type": "DATASET", "confidence": 0.8920683860778809}, {"text": "TUNA-R", "start_pos": 24, "end_pos": 30, "type": "DATASET", "confidence": 0.7990387678146362}, {"text": "TUNA-AS", "start_pos": 61, "end_pos": 68, "type": "DATASET", "confidence": 0.8795002102851868}, {"text": "TUNA-REG", "start_pos": 73, "end_pos": 81, "type": "DATASET", "confidence": 0.8672659993171692}]}], "datasetContent": [{"text": "The evaluation methods used in each task, and the quality criteria that they assess, are summarised in.", "labels": [], "entities": []}, {"text": "Peer outputs from all tasks were evaluated using intrinsic methods.", "labels": [], "entities": []}, {"text": "All of these were automatically computed, and are subdivided into (a): Evaluation methods used per task those measures that assess humanlikeness, i.e. the degree of similarity between a peer output and a reference output; and (b) measures that assess intrinsic properties of peer outputs.", "labels": [], "entities": []}, {"text": "Peer outputs from the TUNA-REG task were also included in a human, task-oriented evaluation, which is extrinsic insofar as it measures the adequacy of a peer output in terms of its utility in an externally defined task.", "labels": [], "entities": []}, {"text": "In the remainder of this section, we summarise the properties of the intrinsic methods.", "labels": [], "entities": []}, {"text": "Section 3.1 describes the experiment conducted for the extrinsic evaluation.", "labels": [], "entities": [{"text": "extrinsic evaluation", "start_pos": 55, "end_pos": 75, "type": "TASK", "confidence": 0.8597292900085449}]}, {"text": "Dice coefficient (TUNA-AS): This is a setcomparison metric, ranging between 0 and 1, where 1 indicates a perfect match between sets.", "labels": [], "entities": [{"text": "TUNA-AS)", "start_pos": 18, "end_pos": 26, "type": "METRIC", "confidence": 0.835769385099411}]}, {"text": "For two attribute sets A and B, Dice is computed as follows: The MASI score) is an adaptation of the Jaccard coefficient which biases it in favour of similarity where one set is a subset of the other.", "labels": [], "entities": [{"text": "Dice", "start_pos": 32, "end_pos": 36, "type": "METRIC", "confidence": 0.9770796895027161}, {"text": "MASI score", "start_pos": 65, "end_pos": 75, "type": "METRIC", "confidence": 0.9838297069072723}]}, {"text": "Like Dice, it ranges between 0 and 1, where 1 indicates a perfect match.", "labels": [], "entities": [{"text": "Dice", "start_pos": 5, "end_pos": 9, "type": "TASK", "confidence": 0.7860369086265564}]}, {"text": "It is computed as follows: where \u03b4 is a monotonicity coefficient defined as follows: Accuracy (all tasks): This is computed as the proportion of the peer outputs of a system which have an exact match to a reference output.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 85, "end_pos": 93, "type": "METRIC", "confidence": 0.999000608921051}]}, {"text": "In TUNA-AS, Accuracy was computed as the proportion of times a system returned an ATTRIBUTE-SET identical to the reference ATTRIBUTE-SET produced by a human author for the same DOMAIN.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 12, "end_pos": 20, "type": "METRIC", "confidence": 0.9991604089736938}, {"text": "ATTRIBUTE-SET", "start_pos": 82, "end_pos": 95, "type": "METRIC", "confidence": 0.9657400250434875}]}, {"text": "In TUNA-R and TUNA-REG, Accuracy was computed as the proportion of times a peer WORD-STRING was identical to the reference WORD-STRING produced by an author for the same DOMAIN.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 24, "end_pos": 32, "type": "METRIC", "confidence": 0.999292254447937}]}, {"text": "String-edit distance (TUNA-R, TUNA-REG): This is the classic Levenshtein distance measure, used to compare the difference between a peer output and a reference output in the corpus, as the minimal number of insertions, deletions and/or substitutions of words required to transform one string into another.", "labels": [], "entities": [{"text": "Levenshtein distance measure", "start_pos": 61, "end_pos": 89, "type": "METRIC", "confidence": 0.6421968936920166}]}, {"text": "The cost for insertions and deletions was set to 1, that for substitutions to 2.", "labels": [], "entities": []}, {"text": "Edit distance is an integer bounded by the length of the longest description in the pair being compared.", "labels": [], "entities": [{"text": "Edit distance", "start_pos": 0, "end_pos": 13, "type": "METRIC", "confidence": 0.8868383169174194}]}, {"text": "BLEU (TUNA-R, TUNA-REG): This is an n-gram based string comparison measure, originally proposed by for evaluation of Machine Translation systems.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.985791802406311}, {"text": "TUNA-REG)", "start_pos": 14, "end_pos": 23, "type": "METRIC", "confidence": 0.8007080256938934}, {"text": "Machine Translation", "start_pos": 117, "end_pos": 136, "type": "TASK", "confidence": 0.8069634139537811}]}, {"text": "It evaluates a system based on the proportion of word n-grams (considering all n-grams of length n \u2264 4 is standard) that it shares with several reference translations.", "labels": [], "entities": []}, {"text": "Unlike Dice, MASI and String-edit, BLEU is by definition an aggregate measure (i.e. a single BLEU score is obtained fora system based on the entire set of items to be compared, and this is generally not equal to the average of BLEU scores for individual items).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 35, "end_pos": 39, "type": "METRIC", "confidence": 0.9967947602272034}, {"text": "BLEU score", "start_pos": 93, "end_pos": 103, "type": "METRIC", "confidence": 0.9719760715961456}, {"text": "BLEU", "start_pos": 227, "end_pos": 231, "type": "METRIC", "confidence": 0.9951268434524536}]}, {"text": "BLEU ranges between 0 and 1.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.9971154928207397}]}, {"text": "NIST (TUNA-R, TUNA-REG): This is aversion of BLEU, which gives more importance to less frequent (hence more informative) n-grams.", "labels": [], "entities": [{"text": "NIST", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.8640809655189514}, {"text": "BLEU", "start_pos": 45, "end_pos": 49, "type": "METRIC", "confidence": 0.9990003705024719}]}, {"text": "The range of NIST scores depends on the size of the test set.", "labels": [], "entities": []}, {"text": "Like BLEU, this is an aggregate measure.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 5, "end_pos": 9, "type": "METRIC", "confidence": 0.9930577278137207}]}, {"text": "Minimality (TUNA-AS): This measure was defined as the proportion of peer ATTRIBUTE-SETs which are minimal, where 'minimal' means that there is no attribute-set which uniquely identifies the target referent in the domain which is smaller.", "labels": [], "entities": [{"text": "TUNA-AS)", "start_pos": 12, "end_pos": 20, "type": "METRIC", "confidence": 0.902197003364563}, {"text": "ATTRIBUTE-SETs", "start_pos": 73, "end_pos": 87, "type": "METRIC", "confidence": 0.9512028098106384}]}, {"text": "Note that this definition includes Uniqueness as a prerequisite, since the description must identify the target entity uniquely in order to qualify for Minimality.", "labels": [], "entities": [{"text": "Minimality", "start_pos": 152, "end_pos": 162, "type": "DATASET", "confidence": 0.6219378709793091}]}, {"text": "All intrinsic evaluation methods except for BLEU and NIST were computed (a) overall, using the entire test data set (i.e. Test Set 1 or 2 as appropriate); and (b) by object type, that is, computing separate values for outputs referring to targets of type furniture and people.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 44, "end_pos": 48, "type": "METRIC", "confidence": 0.9584707617759705}, {"text": "NIST", "start_pos": 53, "end_pos": 57, "type": "DATASET", "confidence": 0.9272586703300476}]}, {"text": "The experiment for the extrinsic evaluation of TUNA-REG peer outputs combined a self-paced reading and identification paradigm, comparing the peer outputs from 10 of the TUNA-REG systems shown in, as well as the two sets of humanauthored reference outputs for Test Set 2.", "labels": [], "entities": []}, {"text": "We refer to the latter as HUMAN-1 and HUMAN-2 in what follows 2 . In the task given to experimental subjects, atrial consisted of a description paired with a visual domain representation corresponding to an item in Test Set 2.", "labels": [], "entities": []}, {"text": "Each trial was split into two phases: (a) in an initial reading phase, subjects were presented with the description only.", "labels": [], "entities": []}, {"text": "This phase was terminated by subjects once they had read the description.", "labels": [], "entities": []}, {"text": "(b) In the second, identification phase, subjects saw the visual domain in which the description had been produced, consisting of images of the domain entities in the same spatial configuration as that in the test set DOMAIN.", "labels": [], "entities": []}, {"text": "They clicked on the object that they thought was the intended referent of the description they had read.", "labels": [], "entities": []}, {"text": "This design differs from that used in the 2007 ASGRE Challenge, in which descriptions and visual domains were presented in a single phase (on the same screen), so that RT and IT were conflated.", "labels": [], "entities": [{"text": "ASGRE Challenge", "start_pos": 47, "end_pos": 62, "type": "TASK", "confidence": 0.6581925749778748}]}, {"text": "The new experiment replicates the methodology reported in, in a follow-up study on the ASGRE 2007 data.", "labels": [], "entities": [{"text": "ASGRE 2007 data", "start_pos": 87, "end_pos": 102, "type": "DATASET", "confidence": 0.8870903849601746}]}, {"text": "Another difference between the two experiments is that the current one is based on peer outputs which are themselves realisations, whereas the ASGRE experiment involved attribute sets which had to be realised before they could be used.", "labels": [], "entities": [{"text": "ASGRE experiment", "start_pos": 143, "end_pos": 159, "type": "DATASET", "confidence": 0.7893730103969574}]}, {"text": "Design: We used a Repeated Latin Squares design, in which each combination of SYSTEM 3 and test set item is allocated one trial.", "labels": [], "entities": []}, {"text": "Since there were 12 levels of SYSTEM, but 112 test set items, 8 randomly selected items (4 furniture and 4 people) were duplicated, yielding 120 items and 10 12 \u00d7 12 latin squares.", "labels": [], "entities": []}, {"text": "The items were divided into two sets of 60.", "labels": [], "entities": []}, {"text": "Half of the participants did the first 60 items (the first 5 latin squares), and the other half the second 60.", "labels": [], "entities": []}, {"text": "Participants and procedure: The experiment was carried out by 24 participants recruited from among the faculty and administrative staff of the University of Brighton, as well as from among the authors' acquaintances.", "labels": [], "entities": []}, {"text": "Participants carried out the experiment under supervision in a quiet room on a laptop.", "labels": [], "entities": []}, {"text": "Stimulus presentation was carried out using DMDX, a Win-32 software package for psycholinguistic experiments involving time measurements (.", "labels": [], "entities": [{"text": "Stimulus presentation", "start_pos": 0, "end_pos": 21, "type": "TASK", "confidence": 0.6884775161743164}]}, {"text": "Participants initiated each trial, which consisted of an initial warning bell and a fixation point flashed on the screen for 1000ms.", "labels": [], "entities": []}, {"text": "They then read the description and called up the visual domain to identify the referent.", "labels": [], "entities": []}, {"text": "Trials timed out after 15000ms.", "labels": [], "entities": []}, {"text": "Treatment of outliers and timeouts: Trials which timed outwith no response were discounted from the analysis.", "labels": [], "entities": []}, {"text": "Out of a total of (24 \u00d7 60 =) 1440 trials, there were 4 reading timeouts (0.3%) and 7 identification timeouts (0.5%).", "labels": [], "entities": []}, {"text": "Outliers for RT and IT were defined as those exceeding a threshold of mean \u00b12SD.", "labels": [], "entities": [{"text": "Outliers", "start_pos": 0, "end_pos": 8, "type": "METRIC", "confidence": 0.9874798655509949}, {"text": "RT", "start_pos": 13, "end_pos": 15, "type": "TASK", "confidence": 0.9562934637069702}, {"text": "IT", "start_pos": 20, "end_pos": 22, "type": "METRIC", "confidence": 0.7105897665023804}]}, {"text": "There were 64 outliers on RT (4.4%) and 191 on IT (13.3%).", "labels": [], "entities": [{"text": "RT", "start_pos": 26, "end_pos": 28, "type": "TASK", "confidence": 0.427256315946579}, {"text": "IT", "start_pos": 47, "end_pos": 49, "type": "METRIC", "confidence": 0.876788854598999}]}, {"text": "Outliers were replaced by the overall mean for RT and IT (see for discussion of this method).", "labels": [], "entities": [{"text": "RT", "start_pos": 47, "end_pos": 49, "type": "TASK", "confidence": 0.8812916278839111}]}, {"text": "This section presents results for each of the tasks.", "labels": [], "entities": []}, {"text": "For all measures, except BLEU and NIST, we present separate descriptive statistics by entity type (people vs. furniture subsets of the relevant test set), and overall.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 25, "end_pos": 29, "type": "METRIC", "confidence": 0.9889494180679321}, {"text": "NIST", "start_pos": 34, "end_pos": 38, "type": "DATASET", "confidence": 0.9241833686828613}]}], "tableCaptions": [{"text": " Table 5: Correlations for TUNA-AS; all values are signif- icant at p \u2264 .05", "labels": [], "entities": [{"text": "TUNA-AS", "start_pos": 27, "end_pos": 34, "type": "DATASET", "confidence": 0.46411311626434326}]}, {"text": " Table 7: Correlations for the TUNA-R task (  *  indicates  p \u2264 .05).", "labels": [], "entities": [{"text": "TUNA-R", "start_pos": 31, "end_pos": 37, "type": "DATASET", "confidence": 0.3964509069919586}]}, {"text": " Table 3: Descriptives for the TUNA-AS task. All means are shown by entity type; standard deviations are displayed  overall.", "labels": [], "entities": [{"text": "TUNA-AS task", "start_pos": 31, "end_pos": 43, "type": "TASK", "confidence": 0.5362505167722702}]}, {"text": " Table 4: Homogeneous subsets for systems in TUNA-AS. Systems which do not share a common letter are significantly  different at p \u2264 .05", "labels": [], "entities": [{"text": "TUNA-AS", "start_pos": 45, "end_pos": 52, "type": "DATASET", "confidence": 0.9128203392028809}]}, {"text": " Table 6: Descriptives for the TUNA-R task.", "labels": [], "entities": [{"text": "TUNA-R task", "start_pos": 31, "end_pos": 42, "type": "TASK", "confidence": 0.5931089818477631}]}, {"text": " Table 8: Descriptives for TUNA-REG on the intrinsic measures.", "labels": [], "entities": [{"text": "TUNA-REG", "start_pos": 27, "end_pos": 35, "type": "DATASET", "confidence": 0.44383949041366577}]}, {"text": " Table 9: Homogeneous subsets for systems in TUNA- REG, Edit Distance measure. Systems which do not share  a common letter are significantly different at p \u2264 .05", "labels": [], "entities": [{"text": "TUNA- REG", "start_pos": 45, "end_pos": 54, "type": "DATASET", "confidence": 0.8597465554873148}, {"text": "Edit Distance measure", "start_pos": 56, "end_pos": 77, "type": "METRIC", "confidence": 0.9279743432998657}]}, {"text": " Table 10: Correlations for TUNA-REG (  *  indicates p \u2264  .05;  *  *  indicates p \u2264 .01).", "labels": [], "entities": [{"text": "TUNA-REG", "start_pos": 28, "end_pos": 36, "type": "METRIC", "confidence": 0.5787343382835388}]}]}