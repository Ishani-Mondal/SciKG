{"title": [{"text": "Probabilistic Model for Syntactic and Semantic Dependency Parsing", "labels": [], "entities": [{"text": "Syntactic and Semantic Dependency Parsing", "start_pos": 24, "end_pos": 65, "type": "TASK", "confidence": 0.6137518107891082}]}], "abstractContent": [{"text": "This paper proposes a novel method to analyze syntactic dependencies and label semantic dependencies around both the verbal predicates and the nouns.", "labels": [], "entities": []}, {"text": "In this method, a probabilistic model is designed to obtain a global optimal result.", "labels": [], "entities": []}, {"text": "Moreover , a predicate identification model and a disambiguation model are proposed to label predicates and their senses.", "labels": [], "entities": [{"text": "predicate identification", "start_pos": 13, "end_pos": 37, "type": "TASK", "confidence": 0.832444816827774}]}, {"text": "The experimental results obtained on the wsj and brown test sets show that our system obtains 77% of labeled macro F1 score for the whole task, 84.47% of labeled attachment score for syntactic dependency task, and 69.45% of labeled F1 score for semantic dependency task.", "labels": [], "entities": [{"text": "wsj and brown test sets", "start_pos": 41, "end_pos": 64, "type": "DATASET", "confidence": 0.7701569676399231}, {"text": "labeled macro F1 score", "start_pos": 101, "end_pos": 123, "type": "METRIC", "confidence": 0.6200504079461098}, {"text": "F1 score", "start_pos": 232, "end_pos": 240, "type": "METRIC", "confidence": 0.9239025413990021}]}], "introductionContent": [{"text": "There are two difficulties in the CoNLL 2008 shared task.", "labels": [], "entities": [{"text": "CoNLL 2008 shared task", "start_pos": 34, "end_pos": 56, "type": "DATASET", "confidence": 0.7439217865467072}]}, {"text": "One is how to label semantic role on a dependency-based representation and how to label verbal predicates and nouns.", "labels": [], "entities": []}, {"text": "The other one is how to combine the syntactic task with the semantic task together.", "labels": [], "entities": []}, {"text": "On the basis of statistical analysis of labeling results, we optimize the traditional approaches of syntactic dependency parsing and semantic role labeling.", "labels": [], "entities": [{"text": "syntactic dependency parsing", "start_pos": 100, "end_pos": 128, "type": "TASK", "confidence": 0.721034586429596}, {"text": "semantic role labeling", "start_pos": 133, "end_pos": 155, "type": "TASK", "confidence": 0.6623188555240631}]}, {"text": "Moreover, we design a predicate identification model and a disambiguation model, which will be described in section 2.3, for labeling predicates and their senses.", "labels": [], "entities": [{"text": "predicate identification", "start_pos": 22, "end_pos": 46, "type": "TASK", "confidence": 0.8940420150756836}]}, {"text": "In the disambiguation model, an exhaustion method is used to find the best sense which is corresponding to a frame of predicate.", "labels": [], "entities": []}, {"text": "In order to obtain a global optimization result for every sentence, a probabilistic model is designed to combine all subtasks.", "labels": [], "entities": []}, {"text": "The rest of this paper is organized as follows: our system is described in section 2; and section 3 reports our results on development and test sets; at last we conclude the paper in section 4.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Syntactic dependency parsing results", "labels": [], "entities": [{"text": "Syntactic dependency parsing", "start_pos": 10, "end_pos": 38, "type": "TASK", "confidence": 0.9580377340316772}]}, {"text": " Table 2. The labeling for  each sentence spends about 14ms.", "labels": [], "entities": []}, {"text": " Table 2: Predicate identification results", "labels": [], "entities": [{"text": "Predicate identification", "start_pos": 10, "end_pos": 34, "type": "TASK", "confidence": 0.7986812889575958}]}, {"text": " Table 3: Semantic dependency labeling results", "labels": [], "entities": [{"text": "Semantic dependency labeling", "start_pos": 10, "end_pos": 38, "type": "TASK", "confidence": 0.7864777247111002}]}, {"text": " Table 4: The F1 values of DSRL around verbal  predicates and nouns on wsj", "labels": [], "entities": [{"text": "F1", "start_pos": 14, "end_pos": 16, "type": "METRIC", "confidence": 0.9968850016593933}]}, {"text": " Table 5: Overall macro scores (Wsem = 0.50)", "labels": [], "entities": [{"text": "Wsem", "start_pos": 32, "end_pos": 36, "type": "METRIC", "confidence": 0.9942586421966553}]}]}