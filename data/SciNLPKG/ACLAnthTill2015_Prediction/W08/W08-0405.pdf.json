{"title": [{"text": "A Rule-Driven Dynamic Programming Decoder for Statistical MT", "labels": [], "entities": [{"text": "MT", "start_pos": 58, "end_pos": 60, "type": "TASK", "confidence": 0.7175371050834656}]}], "abstractContent": [{"text": "The paper presents an extension of a dynamic programming (DP) decoder for phrase-based SMT (Koehn, 2004; Och and Ney, 2004) that tightly integrates POS-based reorder rules (Crego and Marino, 2006) into a left-to-right beam-search algorithm, rather than handling them in a pre-processing or reorder graph generation step.", "labels": [], "entities": [{"text": "SMT", "start_pos": 87, "end_pos": 90, "type": "TASK", "confidence": 0.6893007755279541}]}, {"text": "The novel decoding algorithm can handle tens of thousands of rules efficiently.", "labels": [], "entities": []}, {"text": "An improvement over a standard phrase-based decoder is shown on an Arabic-English translation task with respect to translation accuracy and speed for large reorder window sizes.", "labels": [], "entities": [{"text": "Arabic-English translation task", "start_pos": 67, "end_pos": 98, "type": "TASK", "confidence": 0.7418962319691976}, {"text": "accuracy", "start_pos": 127, "end_pos": 135, "type": "METRIC", "confidence": 0.9176676273345947}]}], "introductionContent": [{"text": "The paper presents an extension of a dynamic programming (DP) decoder for phrase-based SMT) where POSbased re-order rules) are tightly integrated into a left-to-right run over the input sentence.", "labels": [], "entities": [{"text": "SMT", "start_pos": 87, "end_pos": 90, "type": "TASK", "confidence": 0.7300165295600891}]}, {"text": "In the literature, re-order rules are applied to the source and/or target sentence as a pre-processing step (;) where the rules can be applied on both training and test data.", "labels": [], "entities": []}, {"text": "Another way of incorporating re-order rules is via extended monotone search graphs () or lattices (.", "labels": [], "entities": []}, {"text": "This paper presents away of handling POS-based re-order rules as an edge generation process: the POS-based re-order rules are tightly integrated into a left to right beam search decoder in away that 29 000 rules which may overlap in an arbitrary way (but not recursively) are handled efficiently.", "labels": [], "entities": []}, {"text": "Example rules which are used to control the novel DP-based decoder are shown in, where each POS sequence is associated with possibly several permutations \u03c0.", "labels": [], "entities": []}, {"text": "In order to apply the rules, the input sentences are POS-tagged.", "labels": [], "entities": []}, {"text": "If a POS sequence of a rule matches some identical POS sequence in the input sentence the corresponding words are re-ordered according to \u03c0.", "labels": [], "entities": []}, {"text": "The contributions of this paper are as follows: 1) The novel DP decoder can handle tens of thousands of POS-based rules efficiently rather than a few dozen rules as is typically reported in the SMT literature by tightly integrating them into abeam search algorithm.", "labels": [], "entities": [{"text": "SMT", "start_pos": 194, "end_pos": 197, "type": "TASK", "confidence": 0.9850850701332092}]}, {"text": "As a result phrase re-ordering with a large distortion window can be carried out efficiently and reliably.", "labels": [], "entities": []}, {"text": "2) The current rule-driven decoder is a first step towards including more complex rules, i.e. syntax-based rules as in ( or chunk rules as in ( ) using a decoding algorithm that is conceptually similar to an Earley-style parser.", "labels": [], "entities": []}, {"text": "More generally, 'rule-driven' decoding is tightly linked to standard phrase-based decoding.", "labels": [], "entities": []}, {"text": "In future, the edge generation technique presented in this paper might be extended to handle hierarchical rules) in a simple left-to-right beam search decoder.", "labels": [], "entities": [{"text": "edge generation", "start_pos": 15, "end_pos": 30, "type": "TASK", "confidence": 0.6684616357088089}]}, {"text": "In the next section, we briefly summarize the baseline decoder.", "labels": [], "entities": []}, {"text": "Section 3 shows the novel ruledriven DP decoder.", "labels": [], "entities": [{"text": "novel ruledriven DP decoder", "start_pos": 20, "end_pos": 47, "type": "DATASET", "confidence": 0.7845225632190704}]}, {"text": "Section 4 shows how the current decoder is related to both DP-based decoding algorithms in speech recognition and parsing.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 91, "end_pos": 109, "type": "TASK", "confidence": 0.778295248746872}]}, {"text": "Finally,  Section 5 shows experimental results.", "labels": [], "entities": []}], "datasetContent": [{"text": "We test the novel edge generation algorithm on a standard Arabic-to-English translation tasks: the MT06 Arabic-English DARPA evaluation set consisting of 1 529 sentences with 58 331 Arabic words and 4 English reference translations . The translation model is defined in Eq.", "labels": [], "entities": [{"text": "MT06 Arabic-English DARPA evaluation set", "start_pos": 99, "end_pos": 139, "type": "DATASET", "confidence": 0.8522409081459046}]}, {"text": "1 where 8 probabilistic features (language, translation,distortion model) are used.", "labels": [], "entities": []}, {"text": "The distortion model is similar to).", "labels": [], "entities": [{"text": "distortion", "start_pos": 4, "end_pos": 14, "type": "METRIC", "confidence": 0.9684382081031799}]}, {"text": "An on-line algorithm similar to () is used to train the weight vector w.", "labels": [], "entities": []}, {"text": "The decoder uses a 5-gram language model , and the phrase table consists of about 3.2 million phrase pairs.", "labels": [], "entities": []}, {"text": "The phrase table as well as the probabilistic features are trained on a much larger training data consisting of 3.8 million sentences.", "labels": [], "entities": []}, {"text": "Translation results are given in terms of the automatic BLEU evaluation metric () as well as the TER metric ().", "labels": [], "entities": [{"text": "Translation", "start_pos": 0, "end_pos": 11, "type": "TASK", "confidence": 0.95379239320755}, {"text": "BLEU evaluation metric", "start_pos": 56, "end_pos": 78, "type": "METRIC", "confidence": 0.8925351699193319}, {"text": "TER metric", "start_pos": 97, "end_pos": 107, "type": "METRIC", "confidence": 0.9822787940502167}]}, {"text": "Our baseline decoder is similar to.", "labels": [], "entities": []}, {"text": "The goal of the current paper is not to demonstrate an improvement in decoding speed but show the validity of the rule edge generation algorithm.", "labels": [], "entities": [{"text": "rule edge generation", "start_pos": 114, "end_pos": 134, "type": "TASK", "confidence": 0.703430155913035}]}, {"text": "While the baseline and the rule-driven decoder are compared with respect to speed, they are both run with conservatively large beam thresholds, e.g. abeam limit of 500 hypotheses and abeam threshold of 7.5 (logarithmic scale) per source position j.", "labels": [], "entities": []}, {"text": "The baseline decoder and the rule decoder use only 2 stacks to carryout the search (rather than a stack for each source position)).", "labels": [], "entities": []}, {"text": "No rest-cost estimation is employed.", "labels": [], "entities": []}, {"text": "For the results inline 2 the number of phrase 'holes' n in the coverage vector fora left to right traversal of the input sentence is restricted using atypical skip-based decoder.", "labels": [], "entities": []}, {"text": "Up to 2 phrases can be skipped.", "labels": [], "entities": []}, {"text": "Additionally, the phrase re-ordering is restricted to take place within a given window size w.", "labels": [], "entities": []}, {"text": "The 28, 878 rules used in this paper are obtained from 14 989 manually aligned ArabicEnglish sentences where the Arabic sentences have been segmented and POS tagged . The rule selection procedure is similar to the one used in) and rules are extracted that occur at least twice.", "labels": [], "entities": []}, {"text": "The rule-based re-ordering uses an additional probabilistic feature which is derived from the rule unigram count N (r) shown in 1: . The average number of POS sequence matches per input sentence is 34.9 where the average number of permutations that generate edges is 57.7.", "labels": [], "entities": []}, {"text": "The average number of simple edges i.e. phrase pairs per input sentence is 751.1.", "labels": [], "entities": []}, {"text": "For the rule-based decoder the average number of edges is 3187.8 which includes the simple edges.", "labels": [], "entities": []}, {"text": "presents results that compare the baseline decoder with the rule-driven decoder in terms of translation performance and decoding speed.", "labels": [], "entities": []}, {"text": "The second column shows the distortion limit used by the two decoders.", "labels": [], "entities": [{"text": "distortion", "start_pos": 28, "end_pos": 38, "type": "METRIC", "confidence": 0.9796145558357239}]}, {"text": "For the rule-based decoder a maximum distortion limit w is implemented by filtering out all the rule matches where the size of the rule in terms of number of POS symbols is greater than w, i.e. the rule edges are processed monotonically but a monotone rule edge sequence for the same rule id may not span more than w source positions.", "labels": [], "entities": []}, {"text": "The third column shows the translation speed in terms of words per second.", "labels": [], "entities": [{"text": "translation", "start_pos": 27, "end_pos": 38, "type": "TASK", "confidence": 0.9374046921730042}]}, {"text": "The fourth column shows the percentage of CPU time needed for the edge generation (including both simple and rule edges).", "labels": [], "entities": []}, {"text": "The final three columns report translation results in terms of BLEU , BLEU precision score (PREC), and TER.", "labels": [], "entities": [{"text": "translation", "start_pos": 31, "end_pos": 42, "type": "TASK", "confidence": 0.9549195170402527}, {"text": "BLEU", "start_pos": 63, "end_pos": 67, "type": "METRIC", "confidence": 0.9990441203117371}, {"text": "BLEU precision score (PREC)", "start_pos": 70, "end_pos": 97, "type": "METRIC", "confidence": 0.9318444033463796}, {"text": "TER", "start_pos": 103, "end_pos": 106, "type": "METRIC", "confidence": 0.9986269474029541}]}, {"text": "The rule-based reordering restriction obtains the best translation scores on the MT06 data: a BLEU score of 37.2 compared to a BLEU score of 36.6 for the baseline decoder.", "labels": [], "entities": [{"text": "MT06 data", "start_pos": 81, "end_pos": 90, "type": "DATASET", "confidence": 0.9555637538433075}, {"text": "BLEU", "start_pos": 94, "end_pos": 98, "type": "METRIC", "confidence": 0.9995505213737488}, {"text": "BLEU", "start_pos": 127, "end_pos": 131, "type": "METRIC", "confidence": 0.9989129304885864}]}, {"text": "The statistical significance interval is rather large: 2.9 % on this test set as text from various genres is included.", "labels": [], "entities": [{"text": "statistical significance interval", "start_pos": 4, "end_pos": 37, "type": "METRIC", "confidence": 0.6507002512613932}]}, {"text": "Additional visual evaluation on the dev set data shows that some successful phrase reordering is carried out by the rule decoder which is not handled correctly by the baseline decoder.", "labels": [], "entities": [{"text": "phrase reordering", "start_pos": 76, "end_pos": 93, "type": "TASK", "confidence": 0.719050407409668}]}, {"text": "As can be seen from the results reducing the number of rules by filtering all rules that occur at least 5 times (about 10 000 rules) slightly improves translation performance from 37.1 to 37.2.", "labels": [], "entities": [{"text": "translation", "start_pos": 151, "end_pos": 162, "type": "TASK", "confidence": 0.9299921989440918}]}, {"text": "The edge generation accounts for only a small fraction of the overall decoding time. and demonstrate additional advantages when using the rule-based decoder.", "labels": [], "entities": []}, {"text": "shows the translation BLEU score as a function of the distortion limit window w.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 22, "end_pos": 32, "type": "METRIC", "confidence": 0.9549795389175415}]}, {"text": "The BLEU score actually decreases for the baseline decoder as the size w is increased.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 4, "end_pos": 14, "type": "METRIC", "confidence": 0.9853321611881256}]}, {"text": "The optimal window size is surprisingly small: w = 2.", "labels": [], "entities": []}, {"text": "A similar behavior is also reported in where w = 5 is used . For the rule-driven decoder however the BLEU score does not decrease for large w: the rules restrict the local re-ordering in the context of potentially very long POS sequences which makes the re-ordering more reliable.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 101, "end_pos": 111, "type": "METRIC", "confidence": 0.981917142868042}]}, {"text": "which shows the decoding speed as a function of the window size w demonstrates that the rule-based decoder actually runs faster than the baseline decoder for window sizes w \u2265 5.", "labels": [], "entities": []}, {"text": "maximum window size rule-driven decoder N(r)>=5 rule-driven decoder N(r)>=2 distortion-limited phrase decoder: Decoding speed as a function of window size w.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: A list of 28 878 reorder rules sorted according to the rule occurrence count N (r) is used in this paper.  For each POS sequence the corresponding permutation \u03c0 is shown. Rule ID is the ordinal number of a rule in  the sorted list. The maximum rule length that can be handled efficiently is surprisingly long: about 20 words.  Rule ID r  POS sequence  \u03c0  N (r)  1", "labels": [], "entities": []}, {"text": " Table 2: Translation results on the MT06 data. w is the distortion limit.", "labels": [], "entities": [{"text": "Translation", "start_pos": 10, "end_pos": 21, "type": "TASK", "confidence": 0.9477066993713379}, {"text": "MT06 data", "start_pos": 37, "end_pos": 46, "type": "DATASET", "confidence": 0.9563587307929993}]}]}