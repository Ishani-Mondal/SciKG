{"title": [], "abstractContent": [{"text": "The lack of large amounts of readily available, explicitly represented knowledge has long been recognized as a barrier to applications requiring semantic knowledge such as machine translation and question answering.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 172, "end_pos": 191, "type": "TASK", "confidence": 0.7928661406040192}, {"text": "question answering", "start_pos": 196, "end_pos": 214, "type": "TASK", "confidence": 0.8891817927360535}]}, {"text": "This problem is analogous to that facing machine translation decades ago, where one proposed solution was to use human translators to post-edit automatically produced, low quality translations rather than expect a computer to independently create high-quality translations.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 41, "end_pos": 60, "type": "TASK", "confidence": 0.7648688554763794}]}, {"text": "This paper describes an attempt at implementing a semantic parser that takes unrestricted English text, uses publically available computational linguistics tools and lexical resources and as output produces semantic triples which can be used in a variety of tasks such as generating knowledge bases, providing raw material for question answering systems, or creating RDF structures.", "labels": [], "entities": [{"text": "question answering", "start_pos": 327, "end_pos": 345, "type": "TASK", "confidence": 0.8499792814254761}]}, {"text": "We describe the TEXTCAP system, detail the semantic triple representation it produces, illustrate step by step how TEXTCAP processes a short text, and use its results on unseen texts to discuss the amount of post-editing that might be realistically required.", "labels": [], "entities": []}], "introductionContent": [{"text": "A number of applications depend on explicitly represented knowledge to perform basic tasks or add customization to existing tasks.", "labels": [], "entities": []}, {"text": "Improving the quantity and quality of the knowledge contained in knowledge bases could lead to the improved performance of many applications that depend on knowledge and inference such as: \u2022 Generating scientific or educational explanations of natural or mechanical systems and phenomena), \u2022 Question answering systems () that use reasoning to solve problems rather than looking up answers, \u2022 Multimodal information presentation systems that depend on specific real world knowledge in order to describe or refer to it for audiences).", "labels": [], "entities": [{"text": "Generating scientific or educational explanations of natural or mechanical systems and phenomena", "start_pos": 191, "end_pos": 287, "type": "TASK", "confidence": 0.7643029441436132}, {"text": "Question answering", "start_pos": 292, "end_pos": 310, "type": "TASK", "confidence": 0.8371314108371735}]}, {"text": "These systems have typically relied on hand-built and domain specific knowledge bases requiring years of effort to produce.", "labels": [], "entities": []}, {"text": "The need to speedup this process as well as make the resulting representations more consistent are well-known problems that have yielded a number of potential solutions (, but large scale, domain independent, and fully automatic knowledge acquisition on unrestricted text is still in its infancy.", "labels": [], "entities": [{"text": "knowledge acquisition", "start_pos": 229, "end_pos": 250, "type": "TASK", "confidence": 0.7481019496917725}]}, {"text": "Over the last decade research in applied computational linguistics has extended the various components necessary for semantic parsing, but have tended to focus on increasing the measurable performance of individual subtask in isolation (e.g., parsing, anaphora resolution, semantic role labelling, and word sense disambiguation) rather than on an entire end-to-end system.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 117, "end_pos": 133, "type": "TASK", "confidence": 0.7762387692928314}, {"text": "parsing", "start_pos": 243, "end_pos": 250, "type": "TASK", "confidence": 0.9706546068191528}, {"text": "anaphora resolution", "start_pos": 252, "end_pos": 271, "type": "TASK", "confidence": 0.6855725944042206}, {"text": "semantic role labelling", "start_pos": 273, "end_pos": 296, "type": "TASK", "confidence": 0.6467918356259664}, {"text": "word sense disambiguation", "start_pos": 302, "end_pos": 327, "type": "TASK", "confidence": 0.659275641043981}]}, {"text": "Meanwhile, theoretical CL research has examined issues such as underspecification, scoping and reference resolution in discourse contexts, but has set aside issues such as large-scale robustness, ontology integration and evaluation which are vital for applied uses of semantic parsing.", "labels": [], "entities": [{"text": "reference resolution", "start_pos": 95, "end_pos": 115, "type": "TASK", "confidence": 0.72232486307621}, {"text": "ontology integration", "start_pos": 196, "end_pos": 216, "type": "TASK", "confidence": 0.7938914000988007}, {"text": "semantic parsing", "start_pos": 268, "end_pos": 284, "type": "TASK", "confidence": 0.7346103489398956}]}, {"text": "In this paper we discuss an implementation to automatically extract explicitly coded conceptual and ontological knowledge from unrestricted text using a pipeline of NLP components, as part of the STEP shared task).", "labels": [], "entities": [{"text": "STEP shared task", "start_pos": 196, "end_pos": 212, "type": "TASK", "confidence": 0.8270149032274882}]}, {"text": "The TEXTCAP system performs the basic steps towards this task by gluing together an off-the-shelf parser with semantic interpretation methods.", "labels": [], "entities": []}, {"text": "It is intended to be a test case for (1) establishing baseline performance measures for semantic parsing and determining what degree of post-editing might be necessary in real-world environments.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 88, "end_pos": 104, "type": "TASK", "confidence": 0.8593240678310394}]}, {"text": "Because major components of such a system would not be tailored towards the semantic parsing task, we would rightly expect its output to be imperfect.", "labels": [], "entities": [{"text": "semantic parsing task", "start_pos": 76, "end_pos": 97, "type": "TASK", "confidence": 0.7867597242196401}]}, {"text": "This problem is analogous to that facing machine translation decades ago, where one proposed solution was to use human translators to post-edit automatically produced, low quality translations rather than expect a computer to independently create high-quality translations.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 41, "end_pos": 60, "type": "TASK", "confidence": 0.7648688554763794}]}, {"text": "One aspect of this research is thus to investigate how much post-editing would be required to convert the system's output to usable semantic triples.", "labels": [], "entities": []}, {"text": "Finally, this paper presents the results of TEXTCAP on the 2008 STEP shared task corpus, giving specific comments about the difficulties in encountered.", "labels": [], "entities": [{"text": "TEXTCAP", "start_pos": 44, "end_pos": 51, "type": "METRIC", "confidence": 0.4621555805206299}, {"text": "2008 STEP shared task corpus", "start_pos": 59, "end_pos": 87, "type": "DATASET", "confidence": 0.574660736322403}]}, {"text": "Although not a formal evaluation, we were satisfied with its performance in terms of accuracy and efficiency for helping humans post-edit semantic triples.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 85, "end_pos": 93, "type": "METRIC", "confidence": 0.9993630051612854}]}], "datasetContent": [], "tableCaptions": []}