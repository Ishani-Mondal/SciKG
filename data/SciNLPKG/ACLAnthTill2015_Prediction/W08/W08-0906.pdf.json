{"title": [{"text": "Answering Learners' Questions by Retrieving Question Paraphrases from Social Q&A Sites", "labels": [], "entities": [{"text": "Answering Learners' Questions", "start_pos": 0, "end_pos": 29, "type": "TASK", "confidence": 0.8821675181388855}, {"text": "Retrieving Question Paraphrases from Social Q&A", "start_pos": 33, "end_pos": 80, "type": "TASK", "confidence": 0.8287339508533478}]}], "abstractContent": [{"text": "Information overload is a well-known problem which can be particularly detrimental to learners.", "labels": [], "entities": []}, {"text": "In this paper, we propose a method to support learners in the information seeking process which consists in answering their questions by retrieving question paraphrases and their corresponding answers from social Q&A sites.", "labels": [], "entities": [{"text": "information seeking process", "start_pos": 62, "end_pos": 89, "type": "TASK", "confidence": 0.7882936497529348}]}, {"text": "Given the novelty of this kind of data, it is crucial to get a better understanding of how questions in social Q&A sites can be automatically analysed and retrieved.", "labels": [], "entities": []}, {"text": "We discuss and evaluate several pre-processing strategies and question similarity metrics, using anew question paraphrase corpus collected from the WikiAnswers Q&A site.", "labels": [], "entities": []}, {"text": "The results show that viable performance levels of more than 80% accuracy can be obtained for the task of question paraphrase retrieval.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 65, "end_pos": 73, "type": "METRIC", "confidence": 0.9984171390533447}, {"text": "question paraphrase retrieval", "start_pos": 106, "end_pos": 135, "type": "TASK", "confidence": 0.8258840243021647}]}], "introductionContent": [{"text": "Question asking is an important component of efficient learning.", "labels": [], "entities": [{"text": "Question asking", "start_pos": 0, "end_pos": 15, "type": "TASK", "confidence": 0.8475006222724915}]}, {"text": "However, instructors are often overwhelmed with students' questions and are therefore unable to provide timely answers).", "labels": [], "entities": []}, {"text": "Information seeking is also rendered difficult by the sheer amount of learning material available, especially online.", "labels": [], "entities": [{"text": "Information seeking", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.8666441142559052}]}, {"text": "The use of advanced information retrieval and natural language processing techniques to answer learners' questions and reduce the difficulty of information seeking is henceforth particularly promising.", "labels": [], "entities": [{"text": "information retrieval", "start_pos": 20, "end_pos": 41, "type": "TASK", "confidence": 0.7089307457208633}, {"text": "information seeking", "start_pos": 144, "end_pos": 163, "type": "TASK", "confidence": 0.7247031927108765}]}, {"text": "Question Answering (QA) systems seem well suited for this task since they aim at generating precise answers to natural language questions instead of merely returning documents containing answers.", "labels": [], "entities": [{"text": "Question Answering (QA)", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.8362875044345855}]}, {"text": "However, QA systems have to be adapted to meet learners' needs.", "labels": [], "entities": []}, {"text": "Indeed, learners do not merely ask concrete or factoid questions, but rather open-ended, explanatory or methodological questions which cannot be answered by a single sentence ().", "labels": [], "entities": []}, {"text": "Despite a recent trend to render the tasks more complex at large scale QA evaluation campaigns such as TREC or CLEF, current QA systems are still ill-suited to meet these requirements.", "labels": [], "entities": []}, {"text": "A first alternative to full-fledged QA consists in making use of already available question and answer pairs extracted from archived discussions.", "labels": [], "entities": []}, {"text": "For instance, describe an intelligent discussion bot for answering student questions in forums which relies on answers retrieved from an annotated corpus of discussions.", "labels": [], "entities": []}, {"text": "This renders the task of QA easier since answers do not have to be generated from heterogeneous documents by the system.", "labels": [], "entities": [{"text": "QA", "start_pos": 25, "end_pos": 27, "type": "TASK", "confidence": 0.9843319654464722}]}, {"text": "The scope of such a discussion bot is however inherently limited since it relies on manually annotated data, taken from forums within a specific domain.", "labels": [], "entities": []}, {"text": "We propose a different solution which consists in tapping into the wisdom of crowds to answer learners' questions.", "labels": [], "entities": []}, {"text": "This approach provides the compelling advantage that it utilises the wealth of already answered questions available in online social Q&A sites.", "labels": [], "entities": []}, {"text": "The task of Question Answering can then be boiled down to the problem of finding question paraphrases in a database of answered questions.", "labels": [], "entities": [{"text": "Question Answering", "start_pos": 12, "end_pos": 30, "type": "TASK", "confidence": 0.7904084622859955}]}, {"text": "Question paraphrases are questions which have identical meanings and expect the same answer while presenting alternate wordings.", "labels": [], "entities": [{"text": "Question paraphrases", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.6528639793395996}]}, {"text": "Several methods have already been proposed to identify question paraphrases mostly in FAQs () or search engine logs (.", "labels": [], "entities": [{"text": "identify question paraphrases", "start_pos": 46, "end_pos": 75, "type": "TASK", "confidence": 0.6260494887828827}, {"text": "FAQs", "start_pos": 86, "end_pos": 90, "type": "DATASET", "confidence": 0.7936424016952515}]}, {"text": "In this paper, we focus on the problem of question paraphrase identification in social Q&A sites within a realistic information seeking scenario: given a user question, we want to retrieve the best matching question paraphrase from a database of previously answered questions in order to display the corresponding answer.", "labels": [], "entities": [{"text": "question paraphrase identification in social Q&A", "start_pos": 42, "end_pos": 90, "type": "TASK", "confidence": 0.750994473695755}]}, {"text": "The use of social Q&A sites for educational applications brings about new challenges linked to the variable quality of social media content.", "labels": [], "entities": []}, {"text": "As opposed to questions in FAQs, which are subject to editorial control, questions in social Q&A sites are often ill-formed or contain spelling errors.", "labels": [], "entities": []}, {"text": "It is therefore crucial to get a better understanding of how they can be automatically analysed and retrieved.", "labels": [], "entities": []}, {"text": "In this work, we focus on several pre-processing strategies and question similarity measures applied to the task of identifying question paraphrases in asocial Q&A site.", "labels": [], "entities": [{"text": "identifying question paraphrases in asocial Q&A site", "start_pos": 116, "end_pos": 168, "type": "TASK", "confidence": 0.7744203011194865}]}, {"text": "We chose WikiAnswers which has been ranked by comScore as the first fastest growing domain of the top 1,500 in the U.S. in 2007.", "labels": [], "entities": []}, {"text": "The remainder of the paper is organised as follows.", "labels": [], "entities": []}, {"text": "Section 2 first discusses related work on paraphrase identification and question paraphrasing.", "labels": [], "entities": [{"text": "paraphrase identification", "start_pos": 42, "end_pos": 67, "type": "TASK", "confidence": 0.9730693399906158}, {"text": "question paraphrasing", "start_pos": 72, "end_pos": 93, "type": "TASK", "confidence": 0.7911393046379089}]}, {"text": "Section 3 then presents question and answer repositories with special emphasis on social Q&A sites.", "labels": [], "entities": []}, {"text": "Our methods to identify question paraphrases are detailed in section 4.", "labels": [], "entities": [{"text": "identify question paraphrases", "start_pos": 15, "end_pos": 44, "type": "TASK", "confidence": 0.6902622083822886}]}, {"text": "Finally, we present and analyse the experimental results obtained in section 5 and conclude in section 6.", "labels": [], "entities": []}], "datasetContent": [{"text": "We use the following evaluation measures for evaluating the results: Mean Reciprocal Rank For a question, the reciprocal rank RR is 1 r where r is the rank of the correct target question, or zero if the target question was not found.", "labels": [], "entities": [{"text": "Mean Reciprocal Rank", "start_pos": 69, "end_pos": 89, "type": "METRIC", "confidence": 0.9252529144287109}, {"text": "reciprocal rank RR", "start_pos": 110, "end_pos": 128, "type": "METRIC", "confidence": 0.8103141387303671}]}, {"text": "The Mean Reciprocal Rank (MRR) is the mean of the reciprocal ranks overall the input questions.", "labels": [], "entities": [{"text": "Mean Reciprocal Rank (MRR)", "start_pos": 4, "end_pos": 30, "type": "METRIC", "confidence": 0.9520646532376608}]}, {"text": "Accuracy We define accuracy as Success@1, which is the percentage of input questions for which the correct target question has been retrieved at rank 1.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 0, "end_pos": 8, "type": "METRIC", "confidence": 0.992988646030426}, {"text": "accuracy", "start_pos": 19, "end_pos": 27, "type": "METRIC", "confidence": 0.9993137121200562}, {"text": "Success@1", "start_pos": 31, "end_pos": 40, "type": "METRIC", "confidence": 0.9577347834904989}]}, {"text": "displays the accuracy and the mean reciprocal ranks obtained with the different question similarity measures and pre-processing strategies.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 13, "end_pos": 21, "type": "METRIC", "confidence": 0.9996291399002075}]}, {"text": "As could be expected, vector space based similarity measures are consistently more accurate than simple string similarity measures.", "labels": [], "entities": []}, {"text": "Moreover, both the accuracy and the MRR are rather high for vector space metrics (accuracy around 80-85% and MRR around 0.85-0.9), which shows that good results can be obtained with these retrieval mechanisms.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 19, "end_pos": 27, "type": "METRIC", "confidence": 0.9997554421424866}, {"text": "MRR", "start_pos": 36, "end_pos": 39, "type": "METRIC", "confidence": 0.9960083961486816}, {"text": "accuracy", "start_pos": 82, "end_pos": 90, "type": "METRIC", "confidence": 0.9983468055725098}, {"text": "MRR", "start_pos": 109, "end_pos": 112, "type": "METRIC", "confidence": 0.9981498718261719}]}, {"text": "Additional pre-processing, i.e. stemming, lemmatisation and spelling correction, does not ameliorate the tokens minus stop words (T -SW) baseline.", "labels": [], "entities": [{"text": "spelling correction", "start_pos": 60, "end_pos": 79, "type": "TASK", "confidence": 0.5850505232810974}]}], "tableCaptions": []}