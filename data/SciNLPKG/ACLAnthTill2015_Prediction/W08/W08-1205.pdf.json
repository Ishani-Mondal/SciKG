{"title": [{"text": "Native Judgments of Non-Native Usage: Experiments in Preposition Error Detection", "labels": [], "entities": [{"text": "Preposition Error Detection", "start_pos": 53, "end_pos": 80, "type": "TASK", "confidence": 0.6591811875502268}]}], "abstractContent": [{"text": "Evaluation and annotation are two of the greatest challenges in developing NLP instructional or diagnostic tools to mark grammar and usage errors in the writing of non-native speakers.", "labels": [], "entities": []}, {"text": "Past approaches have commonly used only one rater to annotate a corpus of learner errors to compare to system output.", "labels": [], "entities": []}, {"text": "In this paper, we show how using only one rater can skew system evaluation and then we present a sampling approach that makes it possible to evaluate a system more efficiently.", "labels": [], "entities": []}], "introductionContent": [{"text": "In this paper, we present a series of experiments that explore the reliability of human judgments in rating preposition usage.", "labels": [], "entities": [{"text": "rating preposition usage", "start_pos": 101, "end_pos": 125, "type": "TASK", "confidence": 0.8831566770871481}]}, {"text": "While one tends to think of annotator disagreements about discourse and semantics as being quite common, our studies show that judgments of preposition usage, which is largely lexically driven, can be just as contentious.", "labels": [], "entities": []}, {"text": "As a result, this unreliability poses a serious issue for the development and evaluation of NLP tools in the task of automatically detecting preposition usage errors in the writing of non-native speakers of English.", "labels": [], "entities": []}, {"text": "To date, single human annotation has typically been the gold standard for grammatical error detection, such as in the work of ( 1) had a small evaluation of 40 prepositions and it is unclear whether they used multiple annotators or not.", "labels": [], "entities": [{"text": "grammatical error detection", "start_pos": 74, "end_pos": 101, "type": "TASK", "confidence": 0.5956890086332957}]}, {"text": "pora annotated for preposition and determiner errors (such as the Cambridge Learners Corpus 2 and the Chinese Learner English Corpus 3 ), it is unclear which portions of these, if any, were doubly annotated.", "labels": [], "entities": [{"text": "Cambridge Learners Corpus 2", "start_pos": 66, "end_pos": 93, "type": "DATASET", "confidence": 0.968960702419281}, {"text": "Chinese Learner English Corpus 3", "start_pos": 102, "end_pos": 134, "type": "DATASET", "confidence": 0.7634266257286072}]}, {"text": "This previous work has side-stepped the issue of annotator reliability, which we address here through the following three contributions: \u2022 Judgments of Native Usage To motivate our work in non-native usage, we first illustrate the difficulty of preposition selection with two experiments: a cloze test and a choice test, where native speakers judge native texts (section 4).", "labels": [], "entities": [{"text": "preposition selection", "start_pos": 245, "end_pos": 266, "type": "TASK", "confidence": 0.7212252914905548}]}, {"text": "\u2022 Judgments of Non-Native Usage As stated earlier, most computational work in the field of error detection tools for non-native speakers has relied on a single rater to annotate a gold standard corpus to check a system's output.", "labels": [], "entities": [{"text": "error detection", "start_pos": 91, "end_pos": 106, "type": "TASK", "confidence": 0.7552840709686279}]}, {"text": "We conduct an extensive doubleannotation evaluation to measure inter-rater reliability and show that using one rater can be unreliable and may produce misleading results in a system test (section 5).", "labels": [], "entities": []}, {"text": "\u2022 Sampling Approach Multiple annotation can be very costly and time-consuming, which may explain why previous work employed only one rater.", "labels": [], "entities": [{"text": "Sampling", "start_pos": 2, "end_pos": 10, "type": "TASK", "confidence": 0.9496954083442688}, {"text": "Approach", "start_pos": 11, "end_pos": 19, "type": "METRIC", "confidence": 0.7042193412780762}]}, {"text": "As an alternative to the standard exhaustive annotation, we propose a sampling approach in which estimates of the rates of hits, false positives, and misses are derived from random samples of the system's output, and then precision and recall of the system can be calculated.", "labels": [], "entities": [{"text": "precision", "start_pos": 222, "end_pos": 231, "type": "METRIC", "confidence": 0.9994588494300842}, {"text": "recall", "start_pos": 236, "end_pos": 242, "type": "METRIC", "confidence": 0.9990589022636414}]}, {"text": "We show that estimates of system performance derived from the sampling approach are comparable to those derived from an exhaustive annotation, but require only a fraction of the effort (section 6).", "labels": [], "entities": []}, {"text": "In short, through a battery of experiments we show how rating preposition usage, in either native or non-native texts, is a task that has surprisingly low inter-annotator reliability and thus greatly impacts system evaluation.", "labels": [], "entities": [{"text": "rating preposition usage", "start_pos": 55, "end_pos": 79, "type": "TASK", "confidence": 0.8111931681632996}]}, {"text": "We then describe a method for efficiently annotating nonnative texts to make multiple annotation more feasible.", "labels": [], "entities": []}, {"text": "In section 2, we discuss in more depth the motivation for detecting usage errors in non-native writing, as well as the complexities of preposition usage.", "labels": [], "entities": []}, {"text": "In section 3, we describe a system that automatically detects preposition errors involving incorrect selection and extraneous usage.", "labels": [], "entities": []}, {"text": "In sections 4 and 5 respectively, we discuss experiments on the reliability of judging native and non-native preposition usage.", "labels": [], "entities": []}, {"text": "In section 6, we present results of our system and results from comparing the sampling approach with the standard approach of exhaustive annotation.", "labels": [], "entities": []}], "datasetContent": [{"text": "Before evaluating our system on non-native writing, we evaluated how well it does on the task of preposition selection in native text, an area where there has been relatively little work to date.", "labels": [], "entities": [{"text": "preposition selection in native text", "start_pos": 97, "end_pos": 133, "type": "TASK", "confidence": 0.8276102542877197}]}, {"text": "In this task, the system predicts the writer's preposition based on its context.", "labels": [], "entities": []}, {"text": "Its prediction is scored automatically by comparison to what the writer actually wrote.", "labels": [], "entities": []}, {"text": "Most recently, () addressed preposition selection by developing a system that combined a decision tree and a language model.", "labels": [], "entities": [{"text": "preposition selection", "start_pos": 28, "end_pos": 49, "type": "TASK", "confidence": 0.9495485126972198}]}, {"text": "Besides the difference in algorithms, there is also a difference in coverage between their system, which selects among 13 prepositions plus a category for Other, and the system presented here,.2, we display their results in terms of F-measures and show the performance of our system for each preposition.", "labels": [], "entities": [{"text": "coverage", "start_pos": 68, "end_pos": 76, "type": "METRIC", "confidence": 0.9896523356437683}, {"text": "F-measures", "start_pos": 233, "end_pos": 243, "type": "METRIC", "confidence": 0.957511842250824}]}, {"text": "Our model outperforms theirs for 9 out of the 10 prepositions that both systems handle.", "labels": [], "entities": []}, {"text": "Overall accuracy for our system is 77.4% and increases to 79.0% when 7M more training examples are added.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 8, "end_pos": 16, "type": "METRIC", "confidence": 0.9997556805610657}]}, {"text": "For comparison purposes, using a majority baseline (always selecting the preposition of) in this domain results in an accuracy of) used perceptron classifiers for preposition selection in BNC News Text at 85% accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 118, "end_pos": 126, "type": "METRIC", "confidence": 0.9990358352661133}, {"text": "preposition selection", "start_pos": 163, "end_pos": 184, "type": "TASK", "confidence": 0.7892942428588867}, {"text": "BNC News Text", "start_pos": 188, "end_pos": 201, "type": "DATASET", "confidence": 0.9349124034245809}, {"text": "accuracy", "start_pos": 209, "end_pos": 217, "type": "METRIC", "confidence": 0.997734546661377}]}, {"text": "For each of the five most frequent prepositions, they used a separate binary classifier to decide whether that preposition should be used or not.", "labels": [], "entities": []}, {"text": "The classifiers are not combined into a unified model.", "labels": [], "entities": []}, {"text": "When we reconfigured our system and evaluation to be comparable to, our model achieved an accuracy of 90% on the same five prepositions when tested on Wall Street Journal News, which is similar, though not identical, to BNC News.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 90, "end_pos": 98, "type": "METRIC", "confidence": 0.999422550201416}, {"text": "Wall Street Journal News", "start_pos": 151, "end_pos": 175, "type": "DATASET", "confidence": 0.9814747422933578}, {"text": "BNC News", "start_pos": 220, "end_pos": 228, "type": "DATASET", "confidence": 0.9616294205188751}]}, {"text": "While systems can perform at close to 80% accuracy in the task of preposition selection in native texts, this high performance does not transfer to the end-task of detecting preposition errors in essays by non-native writers.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 42, "end_pos": 50, "type": "METRIC", "confidence": 0.9968525767326355}, {"text": "preposition selection in native texts", "start_pos": 66, "end_pos": 103, "type": "TASK", "confidence": 0.8316120147705078}]}, {"text": "For example, () reported precision and recall as low as 25% and 7% respectively when detecting different grammar errors (one of which was prepositions) in English essays by non-native writers.", "labels": [], "entities": [{"text": "precision", "start_pos": 25, "end_pos": 34, "type": "METRIC", "confidence": 0.9994032382965088}, {"text": "recall", "start_pos": 39, "end_pos": 45, "type": "METRIC", "confidence": 0.9990918636322021}]}, {"text": "( reported precision up to 80% in their evaluation on the CLEC corpus, but no recall figure was reported.", "labels": [], "entities": [{"text": "precision", "start_pos": 11, "end_pos": 20, "type": "METRIC", "confidence": 0.9994955062866211}, {"text": "CLEC corpus", "start_pos": 58, "end_pos": 69, "type": "DATASET", "confidence": 0.9514452815055847}, {"text": "recall", "start_pos": 78, "end_pos": 84, "type": "METRIC", "confidence": 0.9993459582328796}]}, {"text": "We have found that our system (the model which performs at 77.4%), also performs as high as 80% precision, but recall ranged from 12% to 26% depending on the non-native test corpus.", "labels": [], "entities": [{"text": "precision", "start_pos": 96, "end_pos": 105, "type": "METRIC", "confidence": 0.9993385672569275}, {"text": "recall", "start_pos": 111, "end_pos": 117, "type": "METRIC", "confidence": 0.9995914101600647}]}, {"text": "While our recall figures may seem low, especially when compared to other NLP tasks such as parsing and anaphora resolution, this is really a reflection of how difficult the task is.", "labels": [], "entities": [{"text": "recall", "start_pos": 10, "end_pos": 16, "type": "METRIC", "confidence": 0.9986721277236938}, {"text": "parsing", "start_pos": 91, "end_pos": 98, "type": "TASK", "confidence": 0.9818447828292847}, {"text": "anaphora resolution", "start_pos": 103, "end_pos": 122, "type": "TASK", "confidence": 0.6279034167528152}]}, {"text": "In addition, in error detection tasks, high precision (and thus low recall) is favored since one wants to minimize the number of false positives a student may see.", "labels": [], "entities": [{"text": "error detection tasks", "start_pos": 16, "end_pos": 37, "type": "TASK", "confidence": 0.7841395139694214}, {"text": "precision", "start_pos": 44, "end_pos": 53, "type": "METRIC", "confidence": 0.9990881681442261}, {"text": "recall", "start_pos": 68, "end_pos": 74, "type": "METRIC", "confidence": 0.9992142915725708}]}, {"text": "This is a common practice in grammatical error detection applications, such as in () and ().", "labels": [], "entities": [{"text": "grammatical error detection", "start_pos": 29, "end_pos": 56, "type": "TASK", "confidence": 0.6302012403806051}]}], "tableCaptions": [{"text": " Table 1: Comparison of F-measures on En- carta/Reuters Corpus", "labels": [], "entities": [{"text": "F-measures", "start_pos": 24, "end_pos": 34, "type": "METRIC", "confidence": 0.8331409692764282}, {"text": "En- carta/Reuters", "start_pos": 38, "end_pos": 55, "type": "DATASET", "confidence": 0.9408302307128906}]}, {"text": " Table 3.2, we dis- play their results in terms of F-measures and show  the performance of our system for each preposi- tion. Our model outperforms theirs for 9 out of the  10 prepositions that both systems handle. Over- all accuracy for our system is 77.4% and increases  to 79.0% when 7M more training examples are  added. For comparison purposes, using a major- ity baseline (always selecting the preposition of) in  this domain results in an accuracy of", "labels": [], "entities": [{"text": "F-measures", "start_pos": 51, "end_pos": 61, "type": "METRIC", "confidence": 0.972531259059906}, {"text": "accuracy", "start_pos": 225, "end_pos": 233, "type": "METRIC", "confidence": 0.696311354637146}, {"text": "accuracy", "start_pos": 446, "end_pos": 454, "type": "METRIC", "confidence": 0.9994953870773315}]}, {"text": " Table 2: Cloze Experiment on Encarta", "labels": [], "entities": []}]}