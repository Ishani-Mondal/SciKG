{"title": [{"text": "Training and Evaluation of the HIS POMDP Dialogue System in Noise", "labels": [], "entities": [{"text": "HIS POMDP Dialogue System", "start_pos": 31, "end_pos": 56, "type": "DATASET", "confidence": 0.7634314149618149}]}], "abstractContent": [{"text": "This paper investigates the claim that a dialogue manager modelled as a Partially Observable Markov Decision Process (POMDP) can achieve improved robustness to noise compared to conventional state-based dialogue managers.", "labels": [], "entities": []}, {"text": "Using the Hidden Information State (HIS) POMDP dialogue manager as an exemplar, and an MDP-based dialogue manager as a baseline, evaluation results are presented for both simulated and real dialogues in a Tourist Information Domain.", "labels": [], "entities": []}, {"text": "The results on the simulated data show that the inherent ability to model uncertainty, allows the POMDP model to exploit alternative hypotheses from the speech understanding system.", "labels": [], "entities": [{"text": "speech understanding", "start_pos": 153, "end_pos": 173, "type": "TASK", "confidence": 0.6909677684307098}]}, {"text": "The results obtained from a user trial show that the HIS system with a trained policy performed significantly better than the MDP baseline.", "labels": [], "entities": []}], "introductionContent": [{"text": "Conventional spoken dialogue systems operate by finding the most likely interpretation of each user input, updating some internal representation of the dialogue state and then outputting an appropriate response.", "labels": [], "entities": []}, {"text": "Error tolerance depends on using confidence thresholds and where they fail, the dialogue manager must resort to quite complex recovery procedures.", "labels": [], "entities": []}, {"text": "Such a system has no explicit mechanisms for representing the inevitable uncertainties associated with speech understanding or the ambiguities which naturally arise in interpreting a user's intentions.", "labels": [], "entities": [{"text": "speech understanding", "start_pos": 103, "end_pos": 123, "type": "TASK", "confidence": 0.7387937605381012}]}, {"text": "The result is a system that is inherently fragile, especially in noisy conditions or where the user is unsure of how to use the system.", "labels": [], "entities": []}, {"text": "It has been suggested that Partially Observable Markov Decision Processes (POMDPs) offer a natural framework for building spoken dialogue systems which can both model these uncertainties and support policies which are robust to their effects).", "labels": [], "entities": []}, {"text": "The key idea of the POMDP is that the underlying dialogue state is hidden and dialogue management policies must therefore be based not on a single state estimate but on a distribution overall states.", "labels": [], "entities": [{"text": "POMDP", "start_pos": 20, "end_pos": 25, "type": "DATASET", "confidence": 0.6857751607894897}]}, {"text": "Whilst POMDPs are attractive theoretically, in practice, they are notoriously intractable for anything other than small state/action spaces.", "labels": [], "entities": []}, {"text": "Hence, practical examples of their use were initially restricted to very simple domains ().", "labels": [], "entities": []}, {"text": "More recently, however, a number of techniques have been suggested which do allow POMDPs to be scaled to handle real world tasks.", "labels": [], "entities": []}, {"text": "The two generic mechanisms which facilitate this scaling are factoring the state space and performing policy optimisation in a reduced summary state space).", "labels": [], "entities": []}, {"text": "Based on these ideas, a number of real-world POMDP-based systems have recently emerged.", "labels": [], "entities": []}, {"text": "The most complex entity which must be represented in the state space is the user's goal.", "labels": [], "entities": []}, {"text": "In the Bayesian Update of Dialogue State (BUDS) system, the user's goal is further factored into conditionally independent slots.", "labels": [], "entities": [{"text": "Bayesian Update of Dialogue State (BUDS)", "start_pos": 7, "end_pos": 47, "type": "TASK", "confidence": 0.5261532515287399}]}, {"text": "The resulting system is then modelled as a dynamic Bayesian network (. A similar approach is also developed in).", "labels": [], "entities": []}, {"text": "An alternative approach taken in the Hidden Information State (HIS) system is to retain a complete representation of the user's goal, but partition states into equivalence classes and prune away very low probability partitions (.", "labels": [], "entities": []}, {"text": "Whichever approach is taken, a key issue in areal POMDP-based dialogue system is its ability to be robust to noise and that is the issue that is addressed in this paper.", "labels": [], "entities": [{"text": "POMDP-based dialogue", "start_pos": 50, "end_pos": 70, "type": "TASK", "confidence": 0.677943617105484}]}, {"text": "Using the HIS system as an exemplar, evaluation results are presented fora real-world tourist information task using both simulated and real users.", "labels": [], "entities": []}, {"text": "The results show that a POMDP system can learn noise robust policies and that N-best outputs from the speech understanding component can be exploited to further improve robustness.", "labels": [], "entities": []}, {"text": "The paper is structured as follows.", "labels": [], "entities": []}, {"text": "Firstly, in Section 2 a brief overview of the HIS system is given.", "labels": [], "entities": [{"text": "HIS", "start_pos": 46, "end_pos": 49, "type": "TASK", "confidence": 0.6915969848632812}]}, {"text": "Then in Section 3, various POMDP training regimes are described and evaluated using a simulated user at differing noise levels.", "labels": [], "entities": []}, {"text": "Section 4 then presents results from atrial in which users conducted various tasks over a range of noise levels.", "labels": [], "entities": []}, {"text": "Finally, in Section 5, we discuss our results and present our conclusions.", "labels": [], "entities": []}], "datasetContent": [{"text": "When training a system to operate robustly in noisy conditions, a variety of strategies are possible.", "labels": [], "entities": []}, {"text": "For example, the system can be trained only on noisefree interactions, it can be trained on increasing levels of noise or it can be trained on a high noise level from the outset.", "labels": [], "entities": []}, {"text": "A related issue concerns the generation of grid points and the number of training iterations to perform.", "labels": [], "entities": []}, {"text": "For example, allowing a very large number of points leads to poor performance due to over-fitting of the training data.", "labels": [], "entities": []}, {"text": "Conversely, having too few point leads to poor performance due to alack of discrimination in its dialogue strategies.", "labels": [], "entities": []}, {"text": "After some experimentation, the following training schedule was adopted.", "labels": [], "entities": []}, {"text": "Training starts in a noise free environment using a small number of grid points and it continues until the performance of the policy levels off.", "labels": [], "entities": []}, {"text": "The resulting policy is then taken as an initial policy for the next stage where the noise level is increased, the number of grid points is expanded and the number of iterations is increased.", "labels": [], "entities": []}, {"text": "This process is repeated until the highest noise level is reached.", "labels": [], "entities": []}, {"text": "This approach was motivated by the observation that a key factor in effective reinforcement learning is the balance between exploration and exploitation.", "labels": [], "entities": []}, {"text": "In POMDP policy optimisation which uses dynamically allocated grid points, maintaining this balance is crucial.", "labels": [], "entities": [{"text": "POMDP policy optimisation", "start_pos": 3, "end_pos": 28, "type": "TASK", "confidence": 0.6004995803038279}]}, {"text": "In our case, the noise introduced by the simulator is used as an implicit mechanism for increasing the exploration.", "labels": [], "entities": []}, {"text": "Each time exploration is increased, the areas of state-space that will be visited will also increase and hence the number of available grid points must also be increased.", "labels": [], "entities": []}, {"text": "At the same time, the number of iterations must be increased to ensure that all points are visited a sufficient number of times.", "labels": [], "entities": []}, {"text": "In practice we found that around 750 to 1000 grid points was sufficient and the total number of simulated dialogues needed for training was around 100,000.", "labels": [], "entities": []}, {"text": "A second issue when training in noisy conditions is whether to train on just the 1-best output from the simulator or train on the N-best outputs.", "labels": [], "entities": []}, {"text": "A limiting factor here is that the computation required for N-best training is significantly increased since the rate of partition generation in the HIS model increases exponentially with N.", "labels": [], "entities": []}, {"text": "In preliminary tests, it was found that when training with 1-best outputs, there was little difference between policies trained entirely in no noise and policies trained on increasing noise as described above.", "labels": [], "entities": []}, {"text": "However, policies trained on 2-best using the incremental strategy did exhibit increased robustness to noise.", "labels": [], "entities": []}, {"text": "To illustrate this, show the average dialogue success rates and rewards for 3 different policies, all trained on 2-best: a hand-crafted policy (hdc), a policy trained on noise-free conditions (noise free) and a policy trained using the incremental scheme described above (increm).", "labels": [], "entities": []}, {"text": "Each policy was tested using 2-best output from the simulator across a range of error rates.", "labels": [], "entities": []}, {"text": "In addition, the noise-free policy was also tested on 1-best output.", "labels": [], "entities": []}, {"text": "As can be seen, both the trained policies improve significantly on the hand-crafted policies.", "labels": [], "entities": []}, {"text": "Furthermore, although the average rewards are all broadly similar, the success rate of the incrementally trained policy is significantly better at higher error rates.", "labels": [], "entities": []}, {"text": "Hence, this latter policy was selected for the user trial described next.", "labels": [], "entities": []}, {"text": "The HIS-POMDP policy (HIS-TRA) that was incrementally trained on the simulated user using 2-best lists was tested in a user trial together with a handcrafted HIS-POMDP policy (HIS-HDC).", "labels": [], "entities": []}, {"text": "The strategy used by the latter was to first check the most likely hypothesis.", "labels": [], "entities": []}, {"text": "If it contains sufficient grounded keys to match 1 to 3 database entities, then offer is selected.", "labels": [], "entities": []}, {"text": "If any part of the hypothesis is inconsistent or the user has explicitly asked for another suggestion, then find alternative action is selected.", "labels": [], "entities": []}, {"text": "If the user has asked for information about an offered entity then inform is selected.", "labels": [], "entities": []}, {"text": "Otherwise, an ungrounded component of the top hypothesis is identified and depending on the belief, one of the confirm actions is selected.", "labels": [], "entities": []}, {"text": "In addition, an MDP-based dialogue manager developed for earlier trials was also tested.", "labels": [], "entities": []}, {"text": "Since considerable effort has been put in optimising this system, it serves as a strong baseline for comparison.", "labels": [], "entities": []}, {"text": "Again, both a trained policy (MDP-TRA) and a hand-crafted policy (MDP-HDC) were tested.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: General corpus statistics.", "labels": [], "entities": []}, {"text": " Table 3. To mirror the re- ward function used in training, the performance for  each dialogue is computed by assigning a reward of  20 points for full completion and subtracting 1 point  for the number of turns up until a successful recom- mendation (i.e., partial completion).", "labels": [], "entities": []}, {"text": " Table 3: Success rates and performance results on full  completion.", "labels": [], "entities": []}]}