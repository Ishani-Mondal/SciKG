{"title": [], "abstractContent": [{"text": "We investigate translation modeling based on exponential estimates which generalize essential components of standard translation models.", "labels": [], "entities": [{"text": "translation modeling", "start_pos": 15, "end_pos": 35, "type": "TASK", "confidence": 0.9833844304084778}]}, {"text": "In application to a hierarchical phrase-based system the simplest generalization allows its models of lexical selection and reordering to be conditioned on arbitrary attributes of the source sentence and its annotation.", "labels": [], "entities": []}, {"text": "Viewing these estimates as approximations of sentence-level probabilities motivates further elaborations that seek to exploit general syntactic and morphological patterns.", "labels": [], "entities": []}, {"text": "Dimensionality control with 1 regularizers makes it possible to negotiate the tradeoff between translation quality and decoding speed.", "labels": [], "entities": []}, {"text": "Putting together and extending several recent advances in phrase-based translation we arrive at a flexible modeling framework that allows efficient leveraging of monolingual resources and tools.", "labels": [], "entities": [{"text": "phrase-based translation", "start_pos": 58, "end_pos": 82, "type": "TASK", "confidence": 0.7802293300628662}]}, {"text": "Experiments with features derived from the output of Chinese and Arabic parsers and an Arabic lemmatizer show significant improvements over a strong baseline.", "labels": [], "entities": []}], "introductionContent": [{"text": "Effective handling of large and diverse inventories of feature functions is one of the most pressing open problems in machine translation.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 118, "end_pos": 137, "type": "TASK", "confidence": 0.8242542743682861}]}, {"text": "While minimum error training has by now become a standard tool for interpolating a small number of aggregate scores, it is not well suited for learning in high-dimensional feature spaces.", "labels": [], "entities": []}, {"text": "At the same time, although recent years have seen considerable progress in development of general methods for large-scale prediction of complex outputs (, their application to language translation has presented considerable challenges.", "labels": [], "entities": [{"text": "language translation", "start_pos": 176, "end_pos": 196, "type": "TASK", "confidence": 0.7283656895160675}]}, {"text": "Several studies have shown that large-margin methods can be adapted to the special complexities of the task (;) . However, the capacity of these algorithms to improve over state-of-the-art baselines is currently limited by their lack of robust dimensionality reduction.", "labels": [], "entities": []}, {"text": "Performance gains are closely tied to the number and variety of candidate features that enter into the model, and increasing the size of the feature space not only slows down training in terms of the number of iterations required for convergence, but can also considerably reduce decoding speed, leading to run-time costs that maybe unacceptable in industrial settings.", "labels": [], "entities": []}, {"text": "Vector space regression has shown impressive performance in other tasks involving string-to-string mappings, but its application to language translation presents a different set of open problems ().", "labels": [], "entities": [{"text": "Vector space regression", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.8809252977371216}, {"text": "language translation", "start_pos": 132, "end_pos": 152, "type": "TASK", "confidence": 0.7277997732162476}]}, {"text": "Other promising formalisms, have not yet produced end-to-end systems competitive with standard baselines, include the approach due to, the hidden-state synchronous grammar-based exponential model studied by, and a similar model incorporating target-side n-gram features proposed in.", "labels": [], "entities": []}, {"text": "Taken together the results of these studies point to a striking overarching conclusion: the humble relative frequency estimate of phrase-based models makes fora surprisingly strong baseline.", "labels": [], "entities": []}, {"text": "The present paper investigates a family of models that capitalize on this practical insight to allow efficient optimization of weights fora virtually unlimited number of features.", "labels": [], "entities": []}, {"text": "We take as a point of departure the observation that the essential translation model scores comprising standard decoding decision rules can be recovered as special cases of a more general family of models.", "labels": [], "entities": []}, {"text": "As we discuss below, they are equal to maximum likelihood solutions for locally normalized \"piecewise\" approximations to sentence-level probabilities, where word alignment is used to determine the subset of features observed in each training example.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 157, "end_pos": 171, "type": "TASK", "confidence": 0.6618003249168396}]}, {"text": "The cases for which such solutions have a closed form correspond to particular restrictions placed on the feature space.", "labels": [], "entities": []}, {"text": "Thus, relative frequency phrase models can be obtained by limiting the feature space to indicator functions for the phrase pairs consistent with an alignment.", "labels": [], "entities": []}, {"text": "By removing unnecessary restrictions we restore the full flexibility of local exponential models, including their ability to use features depending on arbitrary aspects of the source sentence and its annotation.", "labels": [], "entities": []}, {"text": "The availability of robust algorithms for dimensionality reduction with 1 regularizers) means that we can start with a virtually unlimited number of candidate features and negotiate the tradeoff between translation quality and decoding speed in away appropriate fora given setting.", "labels": [], "entities": [{"text": "dimensionality reduction", "start_pos": 42, "end_pos": 66, "type": "TASK", "confidence": 0.7011355310678482}]}, {"text": "A further attractive property of locally normalized models is the modest computational cost of their training and ease of its parallelization.", "labels": [], "entities": []}, {"text": "This is particularly so for the models we concentrate on in this paper, defined so that parameter estimation decomposes into a large number of small optimization subproblems which can be solved independently.", "labels": [], "entities": []}, {"text": "Several variants of these models beyond relative frequencies have appeared in the literature before.", "labels": [], "entities": []}, {"text": "Maximum entropy estimation for translation of individual words dates back to, and the idea of using multi-class classifiers to sharpen predictions normally made through relative frequency estimates has been recently reintroduced under the rubric of word sense disambiguation and generalized to substrings.", "labels": [], "entities": [{"text": "word sense disambiguation", "start_pos": 249, "end_pos": 274, "type": "TASK", "confidence": 0.6178916990756989}]}, {"text": "Maximum entropy models for non-lexicalized reordering rules fora phrase-based system with CKY decoding has been described by.", "labels": [], "entities": []}, {"text": "Some of our experiments, where exponential models conditioned on the source sentence and its parse annotation are associated with all rewrite rules in a hierarchical phrase-based system and all word-level probabilities in standard lexical models, maybe seen as a synthesis of these ideas.", "labels": [], "entities": []}, {"text": "The broader perspective of viewing the product of such local probabilities as a particular approximation of sentence-level likelihood points the way beyond multi-class classification, and this type of generalization is the main original contribution of the present work.", "labels": [], "entities": [{"text": "multi-class classification", "start_pos": 156, "end_pos": 182, "type": "TASK", "confidence": 0.7536353170871735}]}, {"text": "Training a classifier to predict the target phrase for every source phrase is equivalent to conjoining all contextual features of the model with an indicator function for the surface form of some rule in the grammar.", "labels": [], "entities": []}, {"text": "We can also use features based on less specific representation of a rule.", "labels": [], "entities": []}, {"text": "Of particular importance for machine translations are representations which generalize reordering information beyond identity of individual words -a type of generalization that presents a challenge in hierarchical phrase-based translation.", "labels": [], "entities": [{"text": "machine translations", "start_pos": 29, "end_pos": 49, "type": "TASK", "confidence": 0.6826402693986893}, {"text": "hierarchical phrase-based translation", "start_pos": 201, "end_pos": 238, "type": "TASK", "confidence": 0.6365990738073984}]}, {"text": "With generalized local models this can be accomplished by adding features tracking only ordering patterns of rules.", "labels": [], "entities": []}, {"text": "We experiment with a case of such models which allows us to preserve decomposition of parameter estimation into independent subproblems.", "labels": [], "entities": []}, {"text": "Besides varying the structure of the feature space, we can also extend the range of normalization for the exponential models beyond target phrases cooccurring with a given source phrase in the phrase table.", "labels": [], "entities": []}, {"text": "This choice is especially natural for richly inflected languages, since it enables us to model multiple levels of morphological representation at once and estimate probabilities for rules whose surface forms have not been observed in training.", "labels": [], "entities": []}, {"text": "We apply a simple variant of this approach to Arabic-English lexical models.", "labels": [], "entities": []}, {"text": "Experimental results across eight test sets in two language pairs support the intuition that features conjoined with indicator functions for surface forms of rules yield higher gains for test sets with better coverage in training data, while features based on less specific representations become more useful for test sets with lower baselines.", "labels": [], "entities": []}, {"text": "The types of features explored in this paper represent only a small portion of available options, and much practical experimentation remains to be done, particularly in order to find the most effective ex-tensions of the feature space beyond multiclass classification.", "labels": [], "entities": [{"text": "multiclass classification", "start_pos": 242, "end_pos": 267, "type": "TASK", "confidence": 0.7516106069087982}]}, {"text": "However, the results reported here show considerable promise and we believe that the flexibility of these models combined with their computational efficiency makes them potentially valuable as an extension fora variety of systems using translation models with local conditional probabilities and as a feature selection method for globally trained models.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": []}