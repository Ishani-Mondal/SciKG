{"title": [{"text": "Graph-Based Keyword Extraction for Single-Document Summarization", "labels": [], "entities": [{"text": "Graph-Based Keyword Extraction", "start_pos": 0, "end_pos": 30, "type": "TASK", "confidence": 0.563007781902949}, {"text": "Summarization", "start_pos": 51, "end_pos": 64, "type": "TASK", "confidence": 0.7260162830352783}]}], "abstractContent": [{"text": "In this paper, we introduce and compare between two novel approaches, supervised and unsupervised, for identifying the keywords to be used in extractive summa-rization of text documents.", "labels": [], "entities": [{"text": "summa-rization of text documents", "start_pos": 153, "end_pos": 185, "type": "TASK", "confidence": 0.7665011584758759}]}, {"text": "Both our approaches are based on the graph-based syntactic representation of text and web documents, which enhances the traditional vector-space model by taking into account some structural document features.", "labels": [], "entities": []}, {"text": "In the supervised approach, we train classification algorithms on a summarized collection of documents with the purpose of inducing a keyword identification model.", "labels": [], "entities": [{"text": "keyword identification", "start_pos": 134, "end_pos": 156, "type": "TASK", "confidence": 0.7109732180833817}]}, {"text": "In the unsupervised approach, we run the HITS algorithm on document graphs under the assumption that the top-ranked nodes should represent the document keywords.", "labels": [], "entities": []}, {"text": "Our experiments on a collection of benchmark summaries show that given a set of summarized training documents, the supervised classification provides the highest keyword identification accuracy, while the highest F-measure is reached with a simple degree-based ranking.", "labels": [], "entities": [{"text": "keyword identification", "start_pos": 162, "end_pos": 184, "type": "TASK", "confidence": 0.6383747011423111}, {"text": "accuracy", "start_pos": 185, "end_pos": 193, "type": "METRIC", "confidence": 0.882576584815979}, {"text": "F-measure", "start_pos": 213, "end_pos": 222, "type": "METRIC", "confidence": 0.9951444268226624}]}, {"text": "In addition, it is sufficient to perform only the first iteration of HITS rather than running it to its convergence .", "labels": [], "entities": [{"text": "HITS", "start_pos": 69, "end_pos": 73, "type": "DATASET", "confidence": 0.8885278105735779}]}], "introductionContent": [{"text": "the purpose of generating the summary -main document information expressed in \"a few words\".", "labels": [], "entities": []}, {"text": "In this paper, we introduce and compare between two approaches: supervised and unsupervised, for the cross-lingual keyword extraction to be used as the first step in extractive summarization of text documents.", "labels": [], "entities": [{"text": "cross-lingual keyword extraction", "start_pos": 101, "end_pos": 133, "type": "TASK", "confidence": 0.6382876038551331}, {"text": "summarization of text documents", "start_pos": 177, "end_pos": 208, "type": "TASK", "confidence": 0.8281615823507309}]}, {"text": "Thus, according to our problem statement, the keyword is a word presenting in the document summary.", "labels": [], "entities": []}, {"text": "The supervised learning approach for keywords extraction was first suggested in, where parametrized heuristic rules were combined with a genetic algorithm into a system -GenExthat automatically identified keywords in a document.", "labels": [], "entities": [{"text": "keywords extraction", "start_pos": 37, "end_pos": 56, "type": "TASK", "confidence": 0.8794614970684052}]}, {"text": "For both our approaches, we utilize a graphbased representation for text documents.", "labels": [], "entities": []}, {"text": "Such representations may vary from very simple, syntactic ones like words connected by edges representing co-occurrence relation () to more complex ones like concepts connected by semantic relations ().", "labels": [], "entities": []}, {"text": "The main advantage of a syntactic representation is its language independency, while the semantic graphs representation provide new characteristics of text such as its captured semantic structure that itself can serve as a document surrogate and provide means for document navigation.", "labels": [], "entities": [{"text": "document navigation", "start_pos": 264, "end_pos": 283, "type": "TASK", "confidence": 0.7354359328746796}]}, {"text": "Authors of () reduce the problem of summarization to acquiring machine learning models for mapping between the document graph and the graph of a summary.", "labels": [], "entities": [{"text": "summarization", "start_pos": 36, "end_pos": 49, "type": "TASK", "confidence": 0.9916096329689026}]}, {"text": "Using deep linguistic analysis, they extract sub-structures (subjectpredicateobject triples) from document semantic graphs in order to get a summary.", "labels": [], "entities": []}, {"text": "Contrary to (), both our approaches work with a syntactic representation that does not require almost any language-specific linguistic processing.", "labels": [], "entities": []}, {"text": "In this paper, we perform experiments with directed graphs, where the nodes stand for words/phrases and the edges represent syntactic relationships between them, meaning\u00a8followedmeaning\u00a8meaning\u00a8followed by\u00a8(by\u00a8().", "labels": [], "entities": []}, {"text": "Some of the most successful approaches to extractive summarization utilize supervised learning algorithms that are trained on collections of \"ground truth\" summaries built fora relatively large number of documents).", "labels": [], "entities": [{"text": "extractive summarization", "start_pos": 42, "end_pos": 66, "type": "TASK", "confidence": 0.685791015625}]}, {"text": "However, in spite of the reasonable performance of such algorithms they cannot be adapted to new languages or domains without training on each new type of data.", "labels": [], "entities": []}, {"text": "Our first approach also utilizes classification algorithms, but, thanks to the language-independent graph representation of documents, it can be applied to various languages and domains without any modifications of the graph construction procedure (except for the technical upgrade of implementation for multilingual processing of text, like reading Unicode or language-specific encodings, etc.)", "labels": [], "entities": []}, {"text": "Of course, as a supervised approach it requires high-quality training labeled data.", "labels": [], "entities": []}, {"text": "Our second approach uses a technique that does not require any training data.", "labels": [], "entities": []}, {"text": "To extract the summary keywords, we apply a ranking algorithm called HITS) to directed graphs representing source documents.", "labels": [], "entities": [{"text": "HITS", "start_pos": 69, "end_pos": 73, "type": "METRIC", "confidence": 0.7727599143981934}]}, {"text": "Authors of) applied the PageRank algorithm) for keyword extraction using a simpler graph representation (undirected unweighted graphs), and show that their results compare favorably with results on established benchmarks of manually assigned keywords.) are also using the HITS algorithm for automatic sentence extraction from documents represented by graphs built from sentences connected by similarity relationships.", "labels": [], "entities": [{"text": "keyword extraction", "start_pos": 48, "end_pos": 66, "type": "TASK", "confidence": 0.8155095279216766}, {"text": "sentence extraction from documents represented by graphs built from sentences connected by similarity relationships", "start_pos": 301, "end_pos": 416, "type": "TASK", "confidence": 0.8479076389755521}]}, {"text": "Since we work with directed graphs, HITS is the most appropriate algorithm for our task as it takes into account both in-degree and out-degree of nodes.", "labels": [], "entities": []}, {"text": "We show in our experiments that running HITS till convergence is not necessary, and initial weights that we get after the first iteration of algorithm are good enough for rank-based extraction of summary keywords.", "labels": [], "entities": []}, {"text": "Another important conclusion that was infered from our experimental results is that, given the training data in the form of annotated syntactic graphs, supervised classification is the most accurate option for identifying the salient nodes in a document graph, while a simple degreebased ranking provides the highest F-measure.", "labels": [], "entities": [{"text": "supervised classification", "start_pos": 152, "end_pos": 177, "type": "TASK", "confidence": 0.7234123349189758}, {"text": "F-measure", "start_pos": 317, "end_pos": 326, "type": "METRIC", "confidence": 0.9931117296218872}]}], "datasetContent": [{"text": "All experiments have been performed on the collection of summarized news articles provided by the Document Understanding Conference 2002).", "labels": [], "entities": [{"text": "Document Understanding Conference 2002", "start_pos": 98, "end_pos": 136, "type": "DATASET", "confidence": 0.6457822620868683}]}, {"text": "This collection contains 566 English texts along with 2-3 summaries per document on average.", "labels": [], "entities": []}, {"text": "The size 5 of syntactic graphs extracted from these texts is 196 on average, varying from 62 to 876.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Feature selection results according to GainRatio value", "labels": [], "entities": [{"text": "GainRatio", "start_pos": 49, "end_pos": 58, "type": "DATASET", "confidence": 0.8206948041915894}]}, {"text": " Table 2: Average AUC for each rank calculating function", "labels": [], "entities": [{"text": "Average", "start_pos": 10, "end_pos": 17, "type": "METRIC", "confidence": 0.9567479491233826}, {"text": "AUC", "start_pos": 18, "end_pos": 21, "type": "METRIC", "confidence": 0.5588144063949585}]}, {"text": " Table 3: Results for each supervised and unsupervised method", "labels": [], "entities": []}]}