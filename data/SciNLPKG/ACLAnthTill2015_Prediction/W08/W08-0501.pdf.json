{"title": [{"text": "Increasing Maintainability of NLP Evaluation Modules Through Declarative Implementations", "labels": [], "entities": [{"text": "Increasing Maintainability of NLP Evaluation Modules", "start_pos": 0, "end_pos": 52, "type": "TASK", "confidence": 0.7812698384126028}, {"text": "Declarative Implementations", "start_pos": 61, "end_pos": 88, "type": "TASK", "confidence": 0.7125090956687927}]}], "abstractContent": [{"text": "Computing precision and recall metrics for named entity tagging and resolution involves classifying text spans as true positives, false positives, or false negatives.", "labels": [], "entities": [{"text": "precision", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9983969330787659}, {"text": "recall", "start_pos": 24, "end_pos": 30, "type": "METRIC", "confidence": 0.996086597442627}, {"text": "named entity tagging and resolution", "start_pos": 43, "end_pos": 78, "type": "TASK", "confidence": 0.6608210742473603}]}, {"text": "There are many factors that make this classification complicated for real world systems.", "labels": [], "entities": [{"text": "classification", "start_pos": 38, "end_pos": 52, "type": "TASK", "confidence": 0.9565598368644714}]}, {"text": "We describe an evaluation system that attempts to control this complexity through a set of rules and a forward chaining inference engine.", "labels": [], "entities": []}], "introductionContent": [{"text": "Computing precision and recall metrics for named entity recognition systems involves classifying each text span that the system proposes as an entity and a subset of the text spans that the gold data specifies as an entity.", "labels": [], "entities": [{"text": "precision", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9961242079734802}, {"text": "recall", "start_pos": 24, "end_pos": 30, "type": "METRIC", "confidence": 0.9817413687705994}, {"text": "named entity recognition", "start_pos": 43, "end_pos": 67, "type": "TASK", "confidence": 0.7389944791793823}]}, {"text": "These text spans must be classified as true positives, false positives, or false negatives.", "labels": [], "entities": []}, {"text": "In the simple case, it is easy to write a procedure to walk through the list of text spans from the system and check to see if a corresponding text span exists in the gold data with the same label, mark the text span as true positive or false positive accordingly, and delete the span from the gold data set.", "labels": [], "entities": [{"text": "gold data set", "start_pos": 294, "end_pos": 307, "type": "DATASET", "confidence": 0.8455984592437744}]}, {"text": "Then the procedure need only walk through the remaining gold data set and mark these spans as false negatives.", "labels": [], "entities": []}, {"text": "The three predicates are the equality of the span's two offsets and the labels.", "labels": [], "entities": []}, {"text": "This evaluation procedure is useful for any natural language processing task that involves finding and labeling text spans.", "labels": [], "entities": [{"text": "labeling text spans", "start_pos": 103, "end_pos": 122, "type": "TASK", "confidence": 0.7942803899447123}]}, {"text": "The question this poster addresses is how best to manage the complexity of the evaluation system that results from adding a number of additional requirements to the classification of text spans.", "labels": [], "entities": []}, {"text": "The requirements may include fuzzy extent predicates, label hierarchies, confidence levels for gold data, and collapsing multiple mentions in a document to produce a single classification.", "labels": [], "entities": []}, {"text": "In addition, named entity tasks often also involve resolving a mention of an entity to an entry in an authority file (i.e., record in a relational database).", "labels": [], "entities": []}, {"text": "This extension also requires an interleaved evaluation where the error source is important.", "labels": [], "entities": []}, {"text": "We started with a standard procedural approach, encoding the logic in nested conditionals.", "labels": [], "entities": []}, {"text": "When the nesting reached a depth of five (e.g.,), we decided to try another approach.", "labels": [], "entities": []}, {"text": "We implemented the logic in a set of rules.", "labels": [], "entities": []}, {"text": "More specifically, we used the Drools rules and forward chaining engine (http://labs.jboss.com/drools/) to classify text spans as true positives, false positives, and/or false negatives.", "labels": [], "entities": [{"text": "Drools rules", "start_pos": 31, "end_pos": 43, "type": "DATASET", "confidence": 0.8995716869831085}]}, {"text": "The procedural code was 379 lines long.", "labels": [], "entities": []}, {"text": "The declarative system consists of 25 rules with 150 lines of supporting code.", "labels": [], "entities": []}, {"text": "We find the rules more modular and easier to modify and maintain.", "labels": [], "entities": []}, {"text": "However, at this time, we have no experimental result to support this opinion.", "labels": [], "entities": []}], "datasetContent": [{"text": "Matching extents and labels: A system text span may overlap a gold data span but leave out, say, punctuation.", "labels": [], "entities": []}, {"text": "This maybe deemed correct but should be recorded as a fuzzy match.", "labels": [], "entities": []}, {"text": "A match may also exist for span labels also since they maybe organized hierarchically (e.g, cities and countries are kinds of locations).", "labels": [], "entities": []}, {"text": "Thus, calling a city a location maybe considered a partial match.", "labels": [], "entities": []}, {"text": "Annotator Confidence: We allowed our annotators to mark text span gold data with an attribute of \"low confidence.\"", "labels": [], "entities": []}, {"text": "We wanted to pass this information through to the classification of the spans so that they might be filtered out for final precision and recall if desired.", "labels": [], "entities": [{"text": "precision", "start_pos": 123, "end_pos": 132, "type": "METRIC", "confidence": 0.9872038960456848}, {"text": "recall", "start_pos": 137, "end_pos": 143, "type": "METRIC", "confidence": 0.9963764548301697}]}, {"text": "Document level statistics: Some named entity tagging tasks are only interested in document level tagging.", "labels": [], "entities": [{"text": "entity tagging tasks", "start_pos": 38, "end_pos": 58, "type": "TASK", "confidence": 0.8240039547284445}, {"text": "document level tagging", "start_pos": 82, "end_pos": 104, "type": "TASK", "confidence": 0.6039385497570038}]}, {"text": "In other words, the system need only decide if an entity is mentioned in a document: how many times it is mentioned is unimportant.", "labels": [], "entities": []}, {"text": "Resolution: Many of our named entity tagging tasks go a step further and also require linking each entity mention to a record in a database of entities.", "labels": [], "entities": [{"text": "Resolution", "start_pos": 0, "end_pos": 10, "type": "TASK", "confidence": 0.9553078413009644}, {"text": "named entity tagging", "start_pos": 24, "end_pos": 44, "type": "TASK", "confidence": 0.7473945816357931}]}, {"text": "For error anal-ysis, we wished to note if a false negative/positive with respect to resolution is caused by the upstream named entity tagger.", "labels": [], "entities": [{"text": "resolution", "start_pos": 84, "end_pos": 94, "type": "METRIC", "confidence": 0.9347828030586243}]}, {"text": "Finally, our authority files often have many entries for the same entity and thus the gold data contains multiple correct ids.", "labels": [], "entities": []}], "tableCaptions": []}