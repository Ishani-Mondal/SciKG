{"title": [{"text": "TAG, Dynamic Programming, and the Perceptron for Efficient, Feature-rich Parsing", "labels": [], "entities": [{"text": "TAG", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.47106316685676575}]}], "abstractContent": [{"text": "We describe a parsing approach that makes use of the perceptron algorithm, in conjunction with dynamic programming methods, to recover full constituent-based parse trees.", "labels": [], "entities": [{"text": "parsing", "start_pos": 14, "end_pos": 21, "type": "TASK", "confidence": 0.9666867852210999}]}, {"text": "The formalism allows a rich set of parse-tree features, including PCFG-based features, bigram and trigram dependency features , and surface features.", "labels": [], "entities": []}, {"text": "A severe challenge in applying such an approach to full syntactic parsing is the efficiency of the parsing algorithms involved.", "labels": [], "entities": [{"text": "full syntactic parsing", "start_pos": 51, "end_pos": 73, "type": "TASK", "confidence": 0.5746942857901255}]}, {"text": "We show that efficient training is feasible , using a Tree Adjoining Grammar (TAG) based parsing formalism.", "labels": [], "entities": []}, {"text": "A lower-order dependency parsing model is used to restrict the search space of the full model, thereby making it efficient.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 14, "end_pos": 32, "type": "TASK", "confidence": 0.733222484588623}]}, {"text": "Experiments on the Penn WSJ treebank show that the model achieves state-of-the-art performance, for both constituent and dependency accuracy.", "labels": [], "entities": [{"text": "Penn WSJ treebank", "start_pos": 19, "end_pos": 36, "type": "DATASET", "confidence": 0.951183021068573}, {"text": "accuracy", "start_pos": 132, "end_pos": 140, "type": "METRIC", "confidence": 0.9429494142532349}]}], "introductionContent": [{"text": "In global linear models (GLMs) for structured prediction, (e.g.,), the optimal label y * for an input x is y * = arg max w \u00b7 f (x, y) where Y(x) is the set of possible labels for the input x; f (x, y) \u2208 Rd is a feature vector that represents the pair (x, y); and w is a parameter vector.", "labels": [], "entities": [{"text": "structured prediction", "start_pos": 35, "end_pos": 56, "type": "TASK", "confidence": 0.7243514955043793}]}, {"text": "This paper describes a GLM for natural language parsing, trained using the averaged perceptron.", "labels": [], "entities": [{"text": "natural language parsing", "start_pos": 31, "end_pos": 55, "type": "TASK", "confidence": 0.6290036042531332}]}, {"text": "The parser we describe recovers full syntactic representations, similar to those derived by a probabilistic context-free grammar (PCFG).", "labels": [], "entities": []}, {"text": "A key motivation for the use of GLMs in parsing is that they allow a great deal of flexibility in the features which can be included in the definition off (x, y).", "labels": [], "entities": [{"text": "parsing", "start_pos": 40, "end_pos": 47, "type": "TASK", "confidence": 0.9105658531188965}]}, {"text": "A critical problem when training a GLM for parsing is the computational complexity of the inference problem.", "labels": [], "entities": [{"text": "parsing", "start_pos": 43, "end_pos": 50, "type": "TASK", "confidence": 0.9705020189285278}]}, {"text": "The averaged perceptron requires the training set to be repeatedly decoded under the model; under even a simple PCFG representation, finding the arg max in Eq.", "labels": [], "entities": []}, {"text": "1 requires O(n 3 G) time, where n is the length of the sentence, and G is a grammar constant.", "labels": [], "entities": [{"text": "O", "start_pos": 11, "end_pos": 12, "type": "METRIC", "confidence": 0.9669811725616455}]}, {"text": "The average sentence length in the data set we use (the Penn WSJ treebank) is over 23 words; the grammar constant G can easily take a value of 1000 or greater.", "labels": [], "entities": [{"text": "Penn WSJ treebank", "start_pos": 56, "end_pos": 73, "type": "DATASET", "confidence": 0.9630999167760214}]}, {"text": "These factors make exact inference algorithms virtually intractable for training or decoding GLMs for full syntactic parsing.", "labels": [], "entities": []}, {"text": "As a result, in spite of the potential advantages of these methods, there has been very little previous work on applying GLMs for full parsing without the use of fairly severe restrictions or approximations.", "labels": [], "entities": []}, {"text": "For example, the model in () is trained on only sentences of 15 words or less; reranking models) restrict Y(x) to be a small set of parses from a first-pass parser; see section 1.1 for discussion of other related work.", "labels": [], "entities": []}, {"text": "The following ideas are central to our approach: (1) A TAG-based, splittable grammar.", "labels": [], "entities": []}, {"text": "We describe a novel, TAG-based parsing formalism that allows full constituent-based trees to be recovered.", "labels": [], "entities": [{"text": "TAG-based parsing formalism", "start_pos": 21, "end_pos": 48, "type": "TASK", "confidence": 0.8267041047414144}]}, {"text": "A driving motivation for our approach comes from the flexibility of the feature-vector representations f (x, y) that can be used in the model.", "labels": [], "entities": []}, {"text": "The formalism that we describe allows the incorporation of: (1) basic PCFG-style features; (2) the use of features that are sensitive to bigram dependencies between pairs of words; and (3) features that are sensitive to trigram dependencies.", "labels": [], "entities": []}, {"text": "Any of these feature types can be combined with surface features of the sentence x, in a similar way to the use of surface features in conditional random fields ().", "labels": [], "entities": []}, {"text": "Crucially, in spite of these relatively rich representations, the formalism can be parsed efficiently (in O(n 4 G) time) using dynamic-programming algorithms described by Eisner (2000) (unlike many other TAGrelated approaches, our formalism is \"splittable\" in the sense described by Eisner, leading to more efficient parsing algorithms).", "labels": [], "entities": []}, {"text": "(2) Use of a lower-order model for pruning.", "labels": [], "entities": []}, {"text": "The O(n 4 G) running time of the TAG parser is still too expensive for efficient training with the perceptron.", "labels": [], "entities": [{"text": "O(n 4 G) running time", "start_pos": 4, "end_pos": 25, "type": "METRIC", "confidence": 0.7840208932757378}, {"text": "TAG parser", "start_pos": 33, "end_pos": 43, "type": "TASK", "confidence": 0.7520347535610199}]}, {"text": "We describe a method that leverages a simple, first-order dependency parser to restrict the search space of the TAG parser in training and testing.", "labels": [], "entities": []}, {"text": "The lower-order parser runs in O(n 3 H) time where H \u226a G; experiments show that it is remarkably effective in pruning the search space of the full TAG parser.", "labels": [], "entities": []}, {"text": "Experiments on the Penn WSJ treebank show that the model recovers constituent structures with higher accuracy than the approaches of, and with a similar level of performance to the reranking parser of).", "labels": [], "entities": [{"text": "Penn WSJ treebank", "start_pos": 19, "end_pos": 36, "type": "DATASET", "confidence": 0.9465923110644022}, {"text": "accuracy", "start_pos": 101, "end_pos": 109, "type": "METRIC", "confidence": 0.9974651336669922}]}, {"text": "The model also recovers dependencies with significantly higher accuracy than state-of-the-art dependency parsers such as ().", "labels": [], "entities": [{"text": "accuracy", "start_pos": 63, "end_pos": 71, "type": "METRIC", "confidence": 0.9970805048942566}]}], "datasetContent": [{"text": "Sections 2-21 of the Penn Wall Street Journal treebank were used as training data in our experiments, and section 22 was used as a development set.", "labels": [], "entities": [{"text": "Penn Wall Street Journal treebank", "start_pos": 21, "end_pos": 54, "type": "DATASET", "confidence": 0.9537142992019654}]}, {"text": "Sections 23 and 24 were used as test sets.", "labels": [], "entities": []}, {"text": "The model was trained for 20 epochs with the averaged perceptron algorithm, with the development data performance being used to choose the best epoch.", "labels": [], "entities": []}, {"text": "shows the results for the method.", "labels": [], "entities": []}, {"text": "Our experiments show an improvement in performance over the results in).", "labels": [], "entities": []}, {"text": "We would argue that the Collins (2000) method is considerably more complex than ours, requiring a first-stage generative model, together with a reranking approach.", "labels": [], "entities": []}, {"text": "model is also arguably more complex, again using a carefully constructed generative model.", "labels": [], "entities": []}, {"text": "The accuracy of our approach also shows some improvement over results in . This work makes use of a PCFG with latent variables that is trained using a split/merge procedure together with the EM algorithm.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.9993736147880554}]}, {"text": "This work is in many ways complementary to ours-for example, it does not make use of GLMs, dependency features, or of representations that go beyond PCFG productions-and some combination of the two methods may give further gains., and Huang (2008), describe approaches that make use of nonlocal features in conjunction with the Charniak (2000) model; future work may consider extending our approach to include non-local features.", "labels": [], "entities": []}, {"text": "Finally, other recent work ( has had a similar goal of scaling GLMs to full syntactic parsing.", "labels": [], "entities": []}, {"text": "These models make use of PCFG representations, but do not explicitly model bigram or trigram dependencies.", "labels": [], "entities": []}, {"text": "The results in this work (88.3%/88.0% F 1 ) are lower than our F 1 score of 91.1%; this is evidence of the benefits of the richer representations enabled by our approach.", "labels": [], "entities": [{"text": "F 1 )", "start_pos": 38, "end_pos": 43, "type": "METRIC", "confidence": 0.9185042579968771}, {"text": "F 1 score", "start_pos": 63, "end_pos": 72, "type": "METRIC", "confidence": 0.9891965190569559}]}, {"text": "shows the accuracy of the model in recovering unlabeled dependencies.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9996923208236694}]}, {"text": "The method shows improvements over the method described in (, which is a state-of-the-art second-order dependency parser similar to that of), suggesting that the incorporation of constituent structure can improve dependency accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 224, "end_pos": 232, "type": "METRIC", "confidence": 0.9107826948165894}]}, {"text": "shows the effect of the beam-size on the accuracy and speed of the parser on the development set.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 41, "end_pos": 49, "type": "METRIC", "confidence": 0.9995143413543701}]}, {"text": "With the beam setting used in our experiments (\u03b1 = 10 \u22126 ), only 0.34% of possible dependencies are considered by the TAG-based model, but 99% of all correct dependencies are included.", "labels": [], "entities": []}, {"text": "At this beam size the best possible F 1 constituent score is 98.5.", "labels": [], "entities": [{"text": "F 1 constituent score", "start_pos": 36, "end_pos": 57, "type": "METRIC", "confidence": 0.9688224941492081}]}, {"text": "Tighter beams lead to faster parsing times, with slight drops inaccuracy.", "labels": [], "entities": [{"text": "parsing", "start_pos": 29, "end_pos": 36, "type": "TASK", "confidence": 0.9702277183532715}]}], "tableCaptions": [{"text": " Table 1: Results for different methods. PPK07, FKM08,", "labels": [], "entities": [{"text": "PPK07", "start_pos": 41, "end_pos": 46, "type": "DATASET", "confidence": 0.9272964000701904}, {"text": "FKM08", "start_pos": 48, "end_pos": 53, "type": "DATASET", "confidence": 0.8978959321975708}]}, {"text": " Table 2: Table showing unlabeled dependency accuracy for", "labels": [], "entities": [{"text": "accuracy", "start_pos": 45, "end_pos": 53, "type": "METRIC", "confidence": 0.9481227397918701}]}, {"text": " Table 3: Effect of the beam size, controlled by \u03b1, on the", "labels": [], "entities": []}]}