{"title": [{"text": "A Puristic Approach for Joint Dependency Parsing and Semantic Role Labeling", "labels": [], "entities": [{"text": "Joint Dependency Parsing", "start_pos": 24, "end_pos": 48, "type": "TASK", "confidence": 0.6373055577278137}, {"text": "Semantic Role Labeling", "start_pos": 53, "end_pos": 75, "type": "TASK", "confidence": 0.6772742172082266}]}], "abstractContent": [{"text": "We present a puristic approach for combining dependency parsing and semantic role labeling.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 45, "end_pos": 63, "type": "TASK", "confidence": 0.8576942980289459}, {"text": "semantic role labeling", "start_pos": 68, "end_pos": 90, "type": "TASK", "confidence": 0.6333942810694376}]}, {"text": "Ina first step, a data-driven strict incremental deterministic parser is used to compute a single syntactic dependency structure using a MEM trained on the syntactic part of the CoNLL 2008 training corpus.", "labels": [], "entities": [{"text": "CoNLL 2008 training corpus", "start_pos": 178, "end_pos": 204, "type": "DATASET", "confidence": 0.9548197686672211}]}, {"text": "Ina second step, a cascade of MEMs is used to identify predicates , and, for each found predicate, to identify its arguments and their types.", "labels": [], "entities": []}, {"text": "All the MEMs used here are trained only with labeled data from the CoNLL 2008 corpus.", "labels": [], "entities": [{"text": "CoNLL 2008 corpus", "start_pos": 67, "end_pos": 84, "type": "DATASET", "confidence": 0.9768387277921041}]}, {"text": "We participated in the closed challenge, and obtained a labeled macro F1 for WSJ+Brown of 19.93 (20.13 on WSJ only, 18.14 on Brown).", "labels": [], "entities": [{"text": "F1", "start_pos": 70, "end_pos": 72, "type": "METRIC", "confidence": 0.776168167591095}, {"text": "WSJ+Brown", "start_pos": 77, "end_pos": 86, "type": "DATASET", "confidence": 0.8977332711219788}, {"text": "WSJ", "start_pos": 106, "end_pos": 109, "type": "DATASET", "confidence": 0.9693039059638977}]}, {"text": "For the syntactic dependencies we got similar bad results (WSJ+Brown=16.25, WSJ= 16.22, Brown=16.47), as well as for the semantic dependencies (WSJ+Brown=22.36, WSJ=22.86, Brown=17.94).", "labels": [], "entities": [{"text": "WSJ", "start_pos": 76, "end_pos": 79, "type": "METRIC", "confidence": 0.7728085517883301}]}, {"text": "The current results of the experiments suggest that our risky puristic approach of following a strict incremental parsing approach together with the closed data-driven perspective of a joined syntactic and semantic labeling was actually too optimistic and eventually too puristic.", "labels": [], "entities": []}], "introductionContent": [], "datasetContent": [{"text": "As mentioned above, we started the development of the system from scratch with a very small team (actually only one programmer).", "labels": [], "entities": []}, {"text": "Therefore we wanted to focus on certain aspects, totally abandoning our claims for achieving decent results for the others.", "labels": [], "entities": []}, {"text": "One of our major goals was the construction of correct syntactic trees and the recognition of the predicateargument structure -a subtask which mainly corresponds to the unlabeled accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 179, "end_pos": 187, "type": "METRIC", "confidence": 0.9923835396766663}]}, {"text": "For that reason we reduced the scale of our experiments concerning such steps as dependency relation labeling, determining the correct reading for the predicates or the proper type of the arguments.", "labels": [], "entities": [{"text": "dependency relation labeling", "start_pos": 81, "end_pos": 109, "type": "TASK", "confidence": 0.7839312354723612}]}, {"text": "Unfortunately only the labeled accuracy was evaluated at this year's task, which was very frustrating in the end.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 31, "end_pos": 39, "type": "METRIC", "confidence": 0.9635463953018188}]}, {"text": "We have already started beginning the improvement of our parsing system, and we briefly discuss our current findings.", "labels": [], "entities": [{"text": "parsing", "start_pos": 57, "end_pos": 64, "type": "TASK", "confidence": 0.9831794500350952}]}, {"text": "On the technical level we already found a software bug that at least partially might explain the unexpected high difference in performance between the results obtained for the development set and the test set.", "labels": [], "entities": []}, {"text": "Correcting this error now yields an UAL of 53.45% and an LAL of 26.95% on the syntactic part of the Brown test data which is a LALimprovement of about 10%.", "labels": [], "entities": [{"text": "UAL", "start_pos": 36, "end_pos": 39, "type": "METRIC", "confidence": 0.999579131603241}, {"text": "LAL", "start_pos": 57, "end_pos": 60, "type": "METRIC", "confidence": 0.9992209672927856}, {"text": "Brown test data", "start_pos": 100, "end_pos": 115, "type": "DATASET", "confidence": 0.9215226968129476}, {"text": "LALimprovement", "start_pos": 127, "end_pos": 141, "type": "METRIC", "confidence": 0.9961575865745544}]}, {"text": "On the methodological level we are studying the effects of relaxing some of the assumptions of our strict incremental parsing strategy.", "labels": [], "entities": []}, {"text": "In order to do so, we developed a separate model for predicting the unlabeled edges and a separate model for labeling them.", "labels": [], "entities": []}, {"text": "In both cases we used the same features as described in sec.", "labels": [], "entities": []}, {"text": "2, but added features that used a right-context in order to take into account the PoS-tag of the N-next words viz.", "labels": [], "entities": [{"text": "PoS-tag", "start_pos": 82, "end_pos": 89, "type": "METRIC", "confidence": 0.8707333207130432}]}, {"text": "N=5 for the syntactic parser and N=3 for the labeling case.", "labels": [], "entities": []}, {"text": "Using both models during parsing interleaved, we obtained UAL=65.17% and LAL=28.47% on the development set.", "labels": [], "entities": [{"text": "parsing", "start_pos": 25, "end_pos": 32, "type": "TASK", "confidence": 0.9735710024833679}, {"text": "UAL", "start_pos": 58, "end_pos": 61, "type": "METRIC", "confidence": 0.9986488223075867}, {"text": "LAL", "start_pos": 73, "end_pos": 76, "type": "METRIC", "confidence": 0.9993119239807129}]}, {"text": "We assumed that the low LAL might have been caused by a too narrow syntactic context.", "labels": [], "entities": [{"text": "LAL", "start_pos": 24, "end_pos": 27, "type": "METRIC", "confidence": 0.9480144381523132}]}, {"text": "In order to test this assumption, we decoupled the prediction of the unlabeled edges and their labeling, such that the determination of the edge labels is performed after the complete unlabeled dependency tree is computed.", "labels": [], "entities": []}, {"text": "Labeling of the dependency edges is then simply performed by running through the constructed parse trees assigning each edge the most probable dependency type.", "labels": [], "entities": []}, {"text": "This two-phase strategy achieved an LAL of 60.44% on the development set, which means an improvement of about 43%.", "labels": [], "entities": [{"text": "LAL", "start_pos": 36, "end_pos": 39, "type": "METRIC", "confidence": 0.9996663331985474}]}, {"text": "Applying the two-phase parser on the WSJ test data resulted in UAL=65.22% and LAL=62.83%; applying it on the Brown test data resulted in UAL=66.50% and LAL=61.11%, respectively.", "labels": [], "entities": [{"text": "WSJ test data", "start_pos": 37, "end_pos": 50, "type": "DATASET", "confidence": 0.984796404838562}, {"text": "UAL", "start_pos": 63, "end_pos": 66, "type": "METRIC", "confidence": 0.9988662004470825}, {"text": "LAL", "start_pos": 78, "end_pos": 81, "type": "METRIC", "confidence": 0.9980442523956299}, {"text": "Brown test data", "start_pos": 109, "end_pos": 124, "type": "DATASET", "confidence": 0.9024043877919515}, {"text": "UAL", "start_pos": 137, "end_pos": 140, "type": "METRIC", "confidence": 0.9974547028541565}, {"text": "LAL", "start_pos": 152, "end_pos": 155, "type": "METRIC", "confidence": 0.9978020787239075}]}, {"text": "Of course, these results are far from being optimal.", "labels": [], "entities": []}, {"text": "Thus, beside testing and improving our parser on the technical level, we will run further experiments for different context sizes, exploiting different settings of parameters of the classifier and feature values, and eventually testing other ML approaches.", "labels": [], "entities": []}, {"text": "The focus here will be on the development of unlabeled edge models, because it seems that an improvement here is substantial for an overall improvement.", "labels": [], "entities": []}, {"text": "For example, applying the decoupled edge labeling model directly on the given unlabeled dependency trees of the development set (i.e. we assume an UAL of 100%) gave as an LAL of 92.88%.", "labels": [], "entities": [{"text": "UAL", "start_pos": 147, "end_pos": 150, "type": "METRIC", "confidence": 0.98863285779953}, {"text": "LAL", "start_pos": 171, "end_pos": 174, "type": "METRIC", "confidence": 0.9985044002532959}]}, {"text": "Beside this, we will also re-investigate interleaved strategies of unlabeled edge and edge labeling prediction as a basis for (mildly-) strict incremental parsing.", "labels": [], "entities": [{"text": "edge labeling prediction", "start_pos": 86, "end_pos": 110, "type": "TASK", "confidence": 0.7006433308124542}]}, {"text": "Here, it might be useful to relax the strict linear control regime by exploring beam search strategies, e.g. along the lines of.", "labels": [], "entities": []}], "tableCaptions": []}