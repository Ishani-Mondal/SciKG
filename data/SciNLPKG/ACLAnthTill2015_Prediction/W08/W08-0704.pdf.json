{"title": [{"text": "Unsupervised word segmentation for Sesotho using Adaptor Grammars", "labels": [], "entities": [{"text": "Unsupervised word segmentation", "start_pos": 0, "end_pos": 30, "type": "TASK", "confidence": 0.5927992761135101}]}], "abstractContent": [{"text": "This paper describes a variety of non-parametric Bayesian models of word segmen-tation based on Adaptor Grammars that model different aspects of the input and incorporate different kinds of prior knowledge, and applies them to the Bantu language Sesotho.", "labels": [], "entities": []}, {"text": "While we find overall word segmentation accuracies lower than these models achieve on English, we also find some interesting differences in which factors contribute to better word segmentation.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 22, "end_pos": 39, "type": "TASK", "confidence": 0.6834757179021835}, {"text": "word segmentation", "start_pos": 175, "end_pos": 192, "type": "TASK", "confidence": 0.7213169932365417}]}, {"text": "Specifically, we found little improvement to word segmentation accuracy when we modeled contextual dependencies , while modeling morphological structure did improve segmentation accuracy.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 45, "end_pos": 62, "type": "TASK", "confidence": 0.7367409616708755}, {"text": "accuracy", "start_pos": 63, "end_pos": 71, "type": "METRIC", "confidence": 0.7749921679496765}, {"text": "accuracy", "start_pos": 178, "end_pos": 186, "type": "METRIC", "confidence": 0.9448667168617249}]}], "introductionContent": [{"text": "A Bayesian approach to learning) is especially useful for computational models of language acquisition because we can use it to study the effect of different kinds and amounts of prior knowledge on the learning process.", "labels": [], "entities": []}, {"text": "The Bayesian approach is agnostic as to what this prior knowledge might consist of; the prior could encode the kinds of rich universal grammar hypothesised by e.g.,, or it could express a vague non-linguistic preference for simpler as opposed to more complex models, as in some of the grammars discussed below.", "labels": [], "entities": []}, {"text": "Clearly there's a wide range of possible priors, and one of the exciting possibilities raised by Bayesian methods is that we may soon be able to empirically evaluate the potential contribution of different kinds of prior knowledge to language learning.", "labels": [], "entities": []}, {"text": "The Bayesian framework is surprisingly flexible.", "labels": [], "entities": []}, {"text": "The bulk of the work on Bayesian inference is on parametric models, where the goal is to learn the value of a set of parameters (much as in Chomsky's Principles and Parameters conception of learning).", "labels": [], "entities": []}, {"text": "However, recently Bayesian methods for nonparametric inference have been developed, in which the parameters themselves, as well as their values, are learned from data.", "labels": [], "entities": []}, {"text": "(The term \"nonparametric\" is perhaps misleading here: it does not mean that the models have no parameters, rather it means that the learning process considers models with different sets of parameters).", "labels": [], "entities": []}, {"text": "One can think of the prior as providing an infinite set of possible parameters, from which a learner selects a subset with which to model their language.", "labels": [], "entities": []}, {"text": "If one pairs each of these infinitely-many parameters with possible structures (or equivalently, rules that generate such structures) then these nonparametric Bayesian learning methods can learn the structures relevant to a language.", "labels": [], "entities": []}, {"text": "Determining whether methods such as these can in fact learn linguistic structure bears on the nature vs. nurture debates in language acquisition, since one of the arguments for the nativist position is that there doesn't seem to be away to learn structure from the input that children receive.", "labels": [], "entities": [{"text": "language acquisition", "start_pos": 124, "end_pos": 144, "type": "TASK", "confidence": 0.7323151528835297}]}, {"text": "While there's no reason why these methods can't be used to learn the syntax and semantics of human languages, much of the work to date has focused on lower-level learning problems such as morphological structure learning () and word segmentation, where the learner is given unsegmented broad-phonemic utterance transcriptions and has to identify the word boundaries.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 228, "end_pos": 245, "type": "TASK", "confidence": 0.7535504996776581}]}, {"text": "One reason for this is that these problems seem simpler than learning syntax, where the non-linguistic context plausibly supplies important information to human learners.", "labels": [], "entities": []}, {"text": "Virtually everyone agrees that the set of possible morphemes and words, if not infinite, is astronomically large, so it seems plausible that humans use some kind of nonparametric procedure to learn the lexicon.", "labels": [], "entities": []}, {"text": "introduced Adaptor Grammars as a framework in which a wide variety of linguistically-interesting nonparametric inference problems can be formulated and evaluated, including a number of variants of the models described by. presented a variety of different adaptor grammar word segmentation models and applied them to the problem of segmenting Brent's phonemicized version of the BernsteinRatner corpus of child-directed English).", "labels": [], "entities": [{"text": "adaptor grammar word segmentation", "start_pos": 255, "end_pos": 288, "type": "TASK", "confidence": 0.7523162066936493}, {"text": "BernsteinRatner corpus of child-directed English", "start_pos": 378, "end_pos": 426, "type": "DATASET", "confidence": 0.795746922492981}]}, {"text": "The main results of that paper were the following: 1.", "labels": [], "entities": []}, {"text": "it confirmed the importance of modeling contextual dependencies above the word level for word segmentation), 2.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 89, "end_pos": 106, "type": "TASK", "confidence": 0.7526633739471436}]}, {"text": "it showed a small but significant improvement to segmentation accuracy by learning the possible syllable structures of the language together with the lexicon, and 3.", "labels": [], "entities": [{"text": "segmentation", "start_pos": 49, "end_pos": 61, "type": "TASK", "confidence": 0.9576259255409241}, {"text": "accuracy", "start_pos": 62, "end_pos": 70, "type": "METRIC", "confidence": 0.9604412317276001}]}, {"text": "it found no significant advantage to learning morphological structure together with the lexicon (indeed, that model confused morphological and lexical structure).", "labels": [], "entities": []}, {"text": "Of course the last result is a null result, and it's possible that a different model would be able to usefuly combine morphological learning with word segmentation.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 146, "end_pos": 163, "type": "TASK", "confidence": 0.7449108362197876}]}, {"text": "This paper continues that research by applying the same kinds of models to Sesotho, a Bantu language spoken in Southern Africa.", "labels": [], "entities": []}, {"text": "Bantu languages are especially interesting for this kind of study, as they have rich productive agglutinative morphologies and relatively transparent phonologies, as compared to languages such as Finnish or Turkish which have complex harmony processes and other phonological complexities.", "labels": [], "entities": []}, {"text": "The relative clarity of Bantu has inspired previous computational work, such as the algorithm for learning Swahili morphology by.", "labels": [], "entities": []}, {"text": "The Hu et al. algorithm uses a Minimum Description Length procedure) that is conceptually related to the nonparametric Bayesian procedure used here.", "labels": [], "entities": []}, {"text": "However, the work here is focused on determining whether the word segmentation methods that work well for English generalize to Sesotho and whether modeling morphological and/or syllable structure improves Sesotho word segmentation, rather than learning Sesotho morphological structure per se.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 61, "end_pos": 78, "type": "TASK", "confidence": 0.7297824323177338}, {"text": "Sesotho word segmentation", "start_pos": 206, "end_pos": 231, "type": "TASK", "confidence": 0.82700248559316}]}, {"text": "The rest of this paper is structured as follows.", "labels": [], "entities": []}, {"text": "Section 2 informally reviews adaptor grammars and describes how they are used to specify different Bayesian models.", "labels": [], "entities": []}, {"text": "Section 3 describes the Sesotho corpus we used and the specific adaptor grammars we used for word segmentation, and section 5 summarizes and concludes the paper.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 93, "end_pos": 110, "type": "TASK", "confidence": 0.7693166434764862}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Summary of word and morpheme f-scores for  the different models discussed in this paper.", "labels": [], "entities": []}]}