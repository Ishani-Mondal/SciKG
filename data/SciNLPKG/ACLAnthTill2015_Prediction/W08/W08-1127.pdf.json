{"title": [{"text": "The GREC Challenge: Overview and Evaluation Results", "labels": [], "entities": []}], "abstractContent": [{"text": "The GREC Task at REG'08 required participating systems to select coreference chains to the main subject of short encyclopaedic texts collected from Wikipedia.", "labels": [], "entities": [{"text": "GREC Task at REG'08", "start_pos": 4, "end_pos": 23, "type": "DATASET", "confidence": 0.626276895403862}]}, {"text": "Three teams submitted a total of 6 systems, and we additionally created four baseline systems.", "labels": [], "entities": []}, {"text": "Systems were tested automatically using a range of existing intrinsic metrics.", "labels": [], "entities": []}, {"text": "We also evaluated systems extrinsically by applying coreference resolution tools to the outputs and measuring the success of the tools.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 52, "end_pos": 74, "type": "TASK", "confidence": 0.8592241108417511}]}, {"text": "In addition, systems were tested in a reading/comprehension experiment involving human subjects.", "labels": [], "entities": []}, {"text": "This report describes the GREC Task and the evaluation methods, gives brief descriptions of the participating systems, and presents the evaluation results.", "labels": [], "entities": [{"text": "GREC Task", "start_pos": 26, "end_pos": 35, "type": "TASK", "confidence": 0.5096260607242584}]}], "introductionContent": [{"text": "The GREC task is about how to generate appropriate references to an entity in the context of apiece of discourse longer than a sentence.", "labels": [], "entities": [{"text": "GREC", "start_pos": 4, "end_pos": 8, "type": "METRIC", "confidence": 0.4537736773490906}]}, {"text": "Rather than requiring participants to generate referring expressions from scratch, the GREC data provides sets of possible referring expressions for selection.", "labels": [], "entities": [{"text": "GREC data", "start_pos": 87, "end_pos": 96, "type": "DATASET", "confidence": 0.8511835336685181}]}, {"text": "As this is anew referring expression generation (REG) task, the shared task definition was kept fairly simple and the aim for participating systems was to select the appropriate type of referring expression (more specifically, its REG08-TYPE, full details below).", "labels": [], "entities": [{"text": "referring expression generation (REG) task", "start_pos": 16, "end_pos": 58, "type": "TASK", "confidence": 0.8066053262778691}]}, {"text": "The immediate motivating application context for the GREC Task is the improvement of referential clarity and coherence in extractive summarisation by regenerating referring expressions in summaries.", "labels": [], "entities": [{"text": "GREC Task", "start_pos": 53, "end_pos": 62, "type": "TASK", "confidence": 0.5989803075790405}]}, {"text": "There has recently been a small flurry of work in this area.", "labels": [], "entities": []}, {"text": "In the longer term, the GREC Task is intended to be a step in the direction of the more general task of generating referential expressions in discourse context.", "labels": [], "entities": []}, {"text": "The GREC Task Corpus is an extension of GREC 1.0 which had about 1,000 texts in the subdomains of cities, countries, rivers and people.", "labels": [], "entities": [{"text": "GREC Task Corpus", "start_pos": 4, "end_pos": 20, "type": "DATASET", "confidence": 0.9041412274042765}, {"text": "GREC 1.0", "start_pos": 40, "end_pos": 48, "type": "DATASET", "confidence": 0.8949128985404968}]}, {"text": "for the purpose of the REG'08 GREC Task, we obtained an additional 1,000 texts in the new subdomain of mountain texts and developed anew XML annotation scheme (Section 2.2).", "labels": [], "entities": [{"text": "REG'08 GREC Task", "start_pos": 23, "end_pos": 39, "type": "TASK", "confidence": 0.5049650569756826}]}, {"text": "Five teams from four countries registered for the GREC Task, of which three teams eventually submitted 6 systems.", "labels": [], "entities": [{"text": "GREC Task", "start_pos": 50, "end_pos": 59, "type": "TASK", "confidence": 0.477923721075058}]}, {"text": "We also used the corpus texts themselves as 'system' outputs, and created four baseline systems.", "labels": [], "entities": []}, {"text": "We evaluated the resulting 10 systems using a range of intrinsic and extrinsic evaluation methods.", "labels": [], "entities": []}, {"text": "This report presents the results of all evaluations (Section 6), along with descriptions of the GREC data and task (Section 2), test sets (Section 3), evaluation methods (Section 4), and participating systems (Section 5).", "labels": [], "entities": [{"text": "GREC data", "start_pos": 96, "end_pos": 105, "type": "DATASET", "confidence": 0.7846555113792419}]}], "datasetContent": [{"text": "Accuracy of REG08-Type: when computed against the single-RE test sets (C-1, Land P), REG08-Type Accuracy is the proportion of REFEXs selected by a participating system that have a REG08-TYPE value identical to the one in the corpus.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 0, "end_pos": 8, "type": "METRIC", "confidence": 0.9875339269638062}]}, {"text": "When computed against the triple-RE test set (C-2), first the number of correct REG08-Types is computed at the text level for each of the three versions of a corpus text and the maximum of these is determined; then the maximum text-level numbers are summed and divided by the total number of REFs in all the texts, which gives the global REG08-Type Accuracy score.", "labels": [], "entities": [{"text": "REG08-Type Accuracy score", "start_pos": 338, "end_pos": 363, "type": "METRIC", "confidence": 0.7176842093467712}]}, {"text": "The rationale behind computing the REG08-Type Accuracy scores in this way for multiple-RE test sets (maximising scores on RE chains rather than individual REs) is that an RE is not good or bad in its own right, but depends on the other MSRs in the same text.", "labels": [], "entities": []}, {"text": "String Accuracy: This is defined just like REG08-Type Accuracy, except here what is determined is identity between REFEX word strings (the MSREs themselves), not between REG08-Types.", "labels": [], "entities": []}, {"text": "String-edit distance metrics: String-edit distance (SE) is straightforward Levenshtein distance with a substitution cost of 2 and insertion/deletion cost of 1.", "labels": [], "entities": [{"text": "String-edit distance (SE)", "start_pos": 30, "end_pos": 55, "type": "METRIC", "confidence": 0.6454275608062744}]}, {"text": "We also used the version of string-edit distance described by which normalises for length.", "labels": [], "entities": []}, {"text": "This version is denoted 'SEB' below.", "labels": [], "entities": [{"text": "SEB", "start_pos": 25, "end_pos": 28, "type": "METRIC", "confidence": 0.8945391774177551}]}, {"text": "For the single-RE test sets, the global score is simply the average of all RE-level scores.", "labels": [], "entities": [{"text": "RE-level", "start_pos": 75, "end_pos": 83, "type": "METRIC", "confidence": 0.816558301448822}]}, {"text": "For Test Set C-2, we used an approach analogous to that described above for REG08-Type Accuracy.", "labels": [], "entities": [{"text": "REG08-Type Accuracy", "start_pos": 76, "end_pos": 95, "type": "TASK", "confidence": 0.5378592610359192}]}, {"text": "We first computed the best string-edit distance at the text level (here, just the sum of RE-level distances) and then obtained the global distance by dividing the sum of best text-level distances by the number of REFs in all the texts.", "labels": [], "entities": []}, {"text": "Other metrics: BLEU is a precision metric from MT that assesses the quality of a peer translation in terms of the proportion of its word n-grams (n \u2264 4 is standard) that it shares with several reference translations.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 15, "end_pos": 19, "type": "METRIC", "confidence": 0.9988790154457092}, {"text": "precision", "start_pos": 25, "end_pos": 34, "type": "METRIC", "confidence": 0.995390772819519}, {"text": "MT", "start_pos": 47, "end_pos": 49, "type": "TASK", "confidence": 0.5354440212249756}]}, {"text": "We used BLEU-3 rather than the more standard BLEU-4 because most REs in the corpus are less than 4 tokens long.", "labels": [], "entities": [{"text": "BLEU-3", "start_pos": 8, "end_pos": 14, "type": "METRIC", "confidence": 0.9936649799346924}, {"text": "BLEU-4", "start_pos": 45, "end_pos": 51, "type": "METRIC", "confidence": 0.9771541953086853}]}, {"text": "We also used the NIST version of BLEU which weights in favour of less frequent n-grams, as well as ROUGE-2 and ROUGE-SU4 (the two official automatic scores from the DUC summarisation competitions).", "labels": [], "entities": [{"text": "NIST", "start_pos": 17, "end_pos": 21, "type": "DATASET", "confidence": 0.9051898717880249}, {"text": "BLEU", "start_pos": 33, "end_pos": 37, "type": "METRIC", "confidence": 0.9973898530006409}, {"text": "ROUGE-2", "start_pos": 99, "end_pos": 106, "type": "METRIC", "confidence": 0.9959237575531006}, {"text": "ROUGE-SU4", "start_pos": 111, "end_pos": 120, "type": "METRIC", "confidence": 0.9725728631019592}, {"text": "DUC summarisation competitions", "start_pos": 165, "end_pos": 195, "type": "DATASET", "confidence": 0.787395159403483}]}, {"text": "In all cases, we assessed just the MSREs selected by peer systems (leaving out the surrounding text), and computed scores globally (rather than averaging over RE-level scores), as this is standard for these metrics.", "labels": [], "entities": []}, {"text": "BLEU, NIST and ROUGE are designed to work with either one or multiple reference texts, so we did not need to use a different method for Test Set C-2.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.982700526714325}, {"text": "NIST", "start_pos": 6, "end_pos": 10, "type": "DATASET", "confidence": 0.8363391757011414}, {"text": "ROUGE", "start_pos": 15, "end_pos": 20, "type": "METRIC", "confidence": 0.9827708005905151}]}, {"text": "We designed a reading/comprehension experiment in which the task for subjects was to read texts one sentence at a time and then to answer three brief multiple-choice comprehension questions after reading each text.", "labels": [], "entities": []}, {"text": "The basic idea was that it seemed likely that badly chosen MSR reference chains would adversely affect ease of comprehension, and that this might in turn affect reading speed and accuracy in answering comprehension questions.", "labels": [], "entities": [{"text": "ease", "start_pos": 103, "end_pos": 107, "type": "METRIC", "confidence": 0.9967653751373291}, {"text": "accuracy", "start_pos": 179, "end_pos": 187, "type": "METRIC", "confidence": 0.9971581697463989}]}, {"text": "We used a randomly selected subset of 21 texts from Test Set C, and recruited 21 subjects from among the staff, faculty and students of Brighton and Sussex universities.", "labels": [], "entities": []}, {"text": "We used a Repeated Latin Squares design in which each combination of text and system was allocated three trials.", "labels": [], "entities": [{"text": "Repeated Latin Squares", "start_pos": 10, "end_pos": 32, "type": "TASK", "confidence": 0.5239883065223694}]}, {"text": "During the experiment we recorded SRTime, the time subjects took to read sentences (from the point when the sentence appeared on the screen to the point at which the subject requested the next sentence).", "labels": [], "entities": [{"text": "SRTime", "start_pos": 34, "end_pos": 40, "type": "METRIC", "confidence": 0.7700396180152893}]}, {"text": "We also recorded the speed and accuracy with which subjects answered the questions at the end (QTime and Q-Acc).", "labels": [], "entities": [{"text": "speed", "start_pos": 21, "end_pos": 26, "type": "METRIC", "confidence": 0.9970280528068542}, {"text": "accuracy", "start_pos": 31, "end_pos": 39, "type": "METRIC", "confidence": 0.9955860376358032}]}, {"text": "The role of the comprehension questions was to encourage subjects to read the texts properly, rather than skimming through them, and we did not necessarily expect any significant results from the associated measures.", "labels": [], "entities": []}, {"text": "The questions were designed to be of varying degrees of difficulty and predictability.", "labels": [], "entities": []}, {"text": "There was one set of three questions (each with five possible answers) associated with each text, and questions followed the same pattern across the texts: the first question was always about the subdomain of a text; the second about the location of the main subject; the third question was designed not to be predictable.", "labels": [], "entities": []}, {"text": "The order of the answers was randomised for each question and each subject.", "labels": [], "entities": []}, {"text": "The order of texts (with associated questions) was randomised for each subject.", "labels": [], "entities": []}, {"text": "We used the DMDX package for presentation of sentences and measuring reading times and question answering accuracy.", "labels": [], "entities": [{"text": "DMDX package", "start_pos": 12, "end_pos": 24, "type": "DATASET", "confidence": 0.9042028188705444}, {"text": "question answering", "start_pos": 87, "end_pos": 105, "type": "TASK", "confidence": 0.7171754837036133}, {"text": "accuracy", "start_pos": 106, "end_pos": 114, "type": "METRIC", "confidence": 0.8792902231216431}]}, {"text": "Subjects did the experiment in a quiet room, under supervision.", "labels": [], "entities": []}, {"text": "As anew and highly experimental method, we tried out an automatic approach to extrinsic evaluation.", "labels": [], "entities": [{"text": "extrinsic evaluation", "start_pos": 78, "end_pos": 98, "type": "TASK", "confidence": 0.8047335147857666}]}, {"text": "The basic idea was similar to that in the humanbased experiments described above: badly chosen reference chains seem likely to affect the reader's ability to resolve REs.", "labels": [], "entities": []}, {"text": "In the automatic version, the role of the reader is played by an automatic coreference resolution tool and the expectation is that the tool performs worse (are less able to identify coreference chains correctly) with worse MSR reference chains.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 75, "end_pos": 97, "type": "TASK", "confidence": 0.8657163977622986}]}, {"text": "To counteract the potential problem of results being a function of a specific coreference resolution algorithm or tool, we decided to use three different resolvers-those included in LingPipe, 5 JavaRap () and OpenNLP)-and to average results.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 78, "end_pos": 100, "type": "TASK", "confidence": 0.7341746985912323}, {"text": "LingPipe", "start_pos": 182, "end_pos": 190, "type": "DATASET", "confidence": 0.9739495515823364}, {"text": "OpenNLP", "start_pos": 209, "end_pos": 216, "type": "DATASET", "confidence": 0.9171614646911621}]}, {"text": "There does not appear to be a single standard eval-uation metric in the coreference resolution community, so we opted to use three: MUC-6 (, CEAF, and B-CUBED (, which seem to be the most widely accepted metrics.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 72, "end_pos": 94, "type": "TASK", "confidence": 0.9452123045921326}, {"text": "MUC-6", "start_pos": 132, "end_pos": 137, "type": "DATASET", "confidence": 0.5846839547157288}, {"text": "CEAF", "start_pos": 141, "end_pos": 145, "type": "METRIC", "confidence": 0.5557991862297058}, {"text": "B-CUBED", "start_pos": 151, "end_pos": 158, "type": "METRIC", "confidence": 0.9797232747077942}]}, {"text": "All three metrics compute Recall, Precision and F-Scores on aligned gold-standard and resolver-tool coreference chains.", "labels": [], "entities": [{"text": "Recall", "start_pos": 26, "end_pos": 32, "type": "METRIC", "confidence": 0.9696652293205261}, {"text": "Precision", "start_pos": 34, "end_pos": 43, "type": "METRIC", "confidence": 0.9777923822402954}, {"text": "F-Scores", "start_pos": 48, "end_pos": 56, "type": "METRIC", "confidence": 0.965480387210846}]}, {"text": "They differ in how the alignment is obtained and what components of coreference chains are counted for calculating scores.", "labels": [], "entities": []}, {"text": "Results for the automatic extrinsic evaluations are reported below in terms of the F-Scores from these three metrics, as well as in terms of their average.", "labels": [], "entities": [{"text": "F-Scores", "start_pos": 83, "end_pos": 91, "type": "METRIC", "confidence": 0.9970629811286926}]}], "tableCaptions": [{"text": " Table 1: Self-reported REG08-Type Accuracy scores for  development set.", "labels": [], "entities": [{"text": "REG08-Type Accuracy scores", "start_pos": 24, "end_pos": 50, "type": "METRIC", "confidence": 0.7828025023142496}]}, {"text": " Table 2: REG08-Type Accuracy scores and homogeneous subsets (Tukey HSD, alpha = .05) for single-RE test sets.  Systems that do not share a letter are significantly different.", "labels": [], "entities": [{"text": "REG08-Type Accuracy scores", "start_pos": 10, "end_pos": 36, "type": "METRIC", "confidence": 0.7991613944371542}, {"text": "Tukey HSD", "start_pos": 62, "end_pos": 71, "type": "DATASET", "confidence": 0.59681436419487}]}, {"text": " Table 3: REG08-Type Accuracy scores against Test Set C-2 for complete set and for subdomains; homogeneous subsets  (Tukey HSD, alpha = .05) for complete set only (systems that do not share a letter are significantly different).", "labels": [], "entities": [{"text": "REG08-Type", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.7305905222892761}, {"text": "Accuracy", "start_pos": 21, "end_pos": 29, "type": "METRIC", "confidence": 0.7603511810302734}]}, {"text": " Table 4: String Accuracy, BLEU, NIST, ROUGE and string-edit scores, computed on single-RE and triple-RE test  sets (systems in order of String Accuracy); homogeneous subsets (Tukey HSD, alpha = .05) for String Accuracy only  (systems that do not share a letter are significantly different).", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 17, "end_pos": 25, "type": "METRIC", "confidence": 0.862050473690033}, {"text": "BLEU", "start_pos": 27, "end_pos": 31, "type": "METRIC", "confidence": 0.9987481832504272}, {"text": "NIST", "start_pos": 33, "end_pos": 37, "type": "DATASET", "confidence": 0.5211982131004333}, {"text": "ROUGE", "start_pos": 39, "end_pos": 44, "type": "METRIC", "confidence": 0.9889857769012451}, {"text": "Tukey HSD", "start_pos": 176, "end_pos": 185, "type": "DATASET", "confidence": 0.8454159498214722}]}, {"text": " Table 5: Mean SRTimes for each system.", "labels": [], "entities": [{"text": "Mean SRTimes", "start_pos": 10, "end_pos": 22, "type": "METRIC", "confidence": 0.7710554599761963}]}, {"text": " Table 6: Question types 1-3, proportions correct; homo- geneous subsets for Q1 (Tukey HSD, alpha = .05).", "labels": [], "entities": [{"text": "Tukey HSD", "start_pos": 81, "end_pos": 90, "type": "DATASET", "confidence": 0.7326626181602478}]}, {"text": " Table 7: MUC, CEAF and B-CUBED F-Scores for all sys- tems; homogeneous subsets (Tukey HSD), alpha = .05,  for average of F-Scores.", "labels": [], "entities": [{"text": "MUC", "start_pos": 10, "end_pos": 13, "type": "DATASET", "confidence": 0.721651017665863}, {"text": "CEAF", "start_pos": 15, "end_pos": 19, "type": "DATASET", "confidence": 0.7001807689666748}, {"text": "B-CUBED F-Scores", "start_pos": 24, "end_pos": 40, "type": "METRIC", "confidence": 0.7854501903057098}, {"text": "F-Scores", "start_pos": 122, "end_pos": 130, "type": "METRIC", "confidence": 0.9842056035995483}]}]}