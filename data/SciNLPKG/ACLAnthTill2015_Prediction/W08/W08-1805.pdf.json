{"title": [{"text": "A Data Driven Approach to Query Expansion in Question Answering", "labels": [], "entities": [{"text": "Query Expansion in Question Answering", "start_pos": 26, "end_pos": 63, "type": "TASK", "confidence": 0.714608371257782}]}], "abstractContent": [{"text": "Automated answering of natural language questions is an interesting and useful problem to solve.", "labels": [], "entities": [{"text": "Automated answering of natural language questions", "start_pos": 0, "end_pos": 49, "type": "TASK", "confidence": 0.80988909304142}]}, {"text": "Question answering (QA) systems often perform information retrieval at an initial stage.", "labels": [], "entities": [{"text": "Question answering (QA)", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.8919467568397522}, {"text": "information retrieval", "start_pos": 46, "end_pos": 67, "type": "TASK", "confidence": 0.7555965483188629}]}, {"text": "Information retrieval (IR) performance, provided by engines such as Lucene, places abound on overall system performance.", "labels": [], "entities": [{"text": "Information retrieval (IR)", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.8086080729961396}, {"text": "Lucene", "start_pos": 68, "end_pos": 74, "type": "DATASET", "confidence": 0.9490031599998474}]}, {"text": "For example, no answer bearing documents are retrieved at low ranks for almost 40% of questions.", "labels": [], "entities": [{"text": "no answer bearing documents", "start_pos": 13, "end_pos": 40, "type": "TASK", "confidence": 0.6461488604545593}]}, {"text": "In this paper, answer texts from previous QA evaluations held as part of the Text REtrieval Conferences (TREC) are paired with queries and analysed in an attempt to identify performance-enhancing words.", "labels": [], "entities": [{"text": "Text REtrieval Conferences (TREC)", "start_pos": 77, "end_pos": 110, "type": "TASK", "confidence": 0.7960169712702433}]}, {"text": "These words are then used to evaluate the performance of a query expansion method.", "labels": [], "entities": []}, {"text": "Data driven extension words were found to help in over 70% of difficult questions.", "labels": [], "entities": []}, {"text": "These words can be used to improve and evaluate query expansion methods.", "labels": [], "entities": [{"text": "query expansion", "start_pos": 48, "end_pos": 63, "type": "TASK", "confidence": 0.6930505335330963}]}, {"text": "Simple blind relevance feedback (RF) was correctly predicted as unlikely to help overall performance, and an possible explanation is provided for its low value in IR for QA.", "labels": [], "entities": [{"text": "blind relevance feedback (RF)", "start_pos": 7, "end_pos": 36, "type": "METRIC", "confidence": 0.641259620587031}, {"text": "IR", "start_pos": 163, "end_pos": 165, "type": "METRIC", "confidence": 0.6186793446540833}]}], "introductionContent": [{"text": "The task of supplying an answer to a question, given some background knowledge, is often considered fairly trivial from a human point of view, as long as the question is clear and the answer is known.", "labels": [], "entities": []}, {"text": "The aim of an automated question answering system is to provide a single, unambiguous response to a natural language question, given a text collection as a knowledge source, within a certain amount of time.", "labels": [], "entities": [{"text": "question answering", "start_pos": 24, "end_pos": 42, "type": "TASK", "confidence": 0.6958063244819641}]}, {"text": "Since 1999, the Text Retrieval Conferences have included a task to evaluate such systems, based on a large pre-defined corpus (such as AQUAINT, containing around a million news articles in English) and a set of unseen questions.", "labels": [], "entities": [{"text": "Text Retrieval Conferences", "start_pos": 16, "end_pos": 42, "type": "TASK", "confidence": 0.8215447664260864}]}, {"text": "Many information retrieval systems perform document retrieval, giving a list of potentially relevant documents when queried -Google's and Yahoo!'s search products are examples of this type of application.", "labels": [], "entities": [{"text": "information retrieval", "start_pos": 5, "end_pos": 26, "type": "TASK", "confidence": 0.7346946001052856}, {"text": "document retrieval", "start_pos": 43, "end_pos": 61, "type": "TASK", "confidence": 0.7444655895233154}]}, {"text": "Users formulate a query using a few keywords that represent the task they are trying to perform; for example, one might search for \"eiffel tower height\" to determine how tall the Eiffel tower is.", "labels": [], "entities": []}, {"text": "IR engines then return a set of references to potentially relevant documents.", "labels": [], "entities": [{"text": "IR", "start_pos": 0, "end_pos": 2, "type": "TASK", "confidence": 0.8949568867683411}]}, {"text": "In contrast, QA systems must return an exact answer to the question.", "labels": [], "entities": []}, {"text": "They should be confident that the answer has been correctly selected; it is no longer down to the user to research a set of document references in order to discover the information themselves.", "labels": [], "entities": []}, {"text": "Further, the system takes a natural language question as input, instead of a few userselected key terms.", "labels": [], "entities": []}, {"text": "Once a QA system has been provided with a question, its processing steps can be described in three parts -Question Pre-Processing, Text Retrieval and Answer Extraction: 1.", "labels": [], "entities": [{"text": "Text Retrieval", "start_pos": 131, "end_pos": 145, "type": "TASK", "confidence": 0.734727680683136}, {"text": "Answer Extraction", "start_pos": 150, "end_pos": 167, "type": "TASK", "confidence": 0.7991860508918762}]}, {"text": "Question Pre-Processing TREC questions are grouped into series which relate to a given target.", "labels": [], "entities": []}, {"text": "For example, the target maybe \"Hindenburg disaster\" with questions such as \"What type of craft was the Hindenburg?\" or \"How fast could it travel?\".", "labels": [], "entities": []}, {"text": "Questions may include pronouns ref-erencing the target or even previous answers, and as such require processing before they are suitable for use.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Performance of Lucene, Indri and Terrier at para- graph level, over top 20 documents. This clearly shows the  limitations of the engines.", "labels": [], "entities": [{"text": "Lucene", "start_pos": 25, "end_pos": 31, "type": "DATASET", "confidence": 0.9303704500198364}, {"text": "Indri", "start_pos": 33, "end_pos": 38, "type": "DATASET", "confidence": 0.8148095011711121}, {"text": "Terrier", "start_pos": 43, "end_pos": 50, "type": "DATASET", "confidence": 0.7306186556816101}]}, {"text": " Table 2: Performance of Indri and Terrier at document level  IR over the AQUAINT corpus, with n = 20", "labels": [], "entities": [{"text": "AQUAINT corpus", "start_pos": 74, "end_pos": 88, "type": "DATASET", "confidence": 0.883329838514328}]}, {"text": " Table 3: Number of difficult questions, as defined by those  which have zero redundancy over both strict and lenient mea- sures, at n = 20. Questions seem to get harder each year.  Document retrieval yields fewer difficult questions, as more  text is returned for potential matching.", "labels": [], "entities": [{"text": "mea- sures", "start_pos": 118, "end_pos": 128, "type": "METRIC", "confidence": 0.804564913113912}, {"text": "Document retrieval", "start_pos": 182, "end_pos": 200, "type": "TASK", "confidence": 0.8913101553916931}]}, {"text": " Table 5: Common difficult questions (over all three engines  mentioned above) by year and match type; n = 20.", "labels": [], "entities": [{"text": "match type", "start_pos": 91, "end_pos": 101, "type": "METRIC", "confidence": 0.908500999212265}]}, {"text": " Table 6: Using Terrier Passage / strict matching, retrieving 20  docs, with TREC2006 questions / AQUAINT. Difficult ques- tions are those where no strict matches are found in the top 20  IRT from just one engine.", "labels": [], "entities": [{"text": "TREC2006", "start_pos": 77, "end_pos": 85, "type": "METRIC", "confidence": 0.9884641766548157}, {"text": "AQUAINT", "start_pos": 98, "end_pos": 105, "type": "METRIC", "confidence": 0.8690341114997864}]}, {"text": " Table 7: \"Helpful extension words\": the set of extensions that,  when added to the query, move redundancy above zero. r =  5, n = 20, using Indri at passage level.", "labels": [], "entities": []}, {"text": " Table 8: Coverage (strict) using blind RF. Both document- and paragraph-level retrieval used to determine RF terms.", "labels": [], "entities": []}, {"text": " Table 9: Queries with extensions, and their mean redundancy  using Indri at document level with n = 20. Without exten- sions, redundancy is zero.", "labels": [], "entities": []}]}