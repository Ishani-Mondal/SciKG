{"title": [{"text": "Using Shallow Syntax Information to Improve Word Alignment and Reordering for SMT", "labels": [], "entities": [{"text": "Improve Word Alignment", "start_pos": 36, "end_pos": 58, "type": "TASK", "confidence": 0.7676185270150503}, {"text": "SMT", "start_pos": 78, "end_pos": 81, "type": "TASK", "confidence": 0.9707005620002747}]}], "abstractContent": [{"text": "We describe two methods to improve SMT accuracy using shallow syntax information.", "labels": [], "entities": [{"text": "SMT", "start_pos": 35, "end_pos": 38, "type": "TASK", "confidence": 0.9974187612533569}, {"text": "accuracy", "start_pos": 39, "end_pos": 47, "type": "METRIC", "confidence": 0.8773343563079834}]}, {"text": "First, we use chunks to refine the set of word alignments typically used as a starting point in SMT systems.", "labels": [], "entities": [{"text": "word alignments", "start_pos": 42, "end_pos": 57, "type": "TASK", "confidence": 0.7202624380588531}, {"text": "SMT", "start_pos": 96, "end_pos": 99, "type": "TASK", "confidence": 0.9939488172531128}]}, {"text": "Second, we extend an N-gram-based SMT system with chunk tags to better account for long-distance reorderings.", "labels": [], "entities": [{"text": "SMT", "start_pos": 34, "end_pos": 37, "type": "TASK", "confidence": 0.9553534388542175}]}, {"text": "Experiments are reported on an Arabic-English task showing significant improvements.", "labels": [], "entities": []}, {"text": "A human error analysis indicates that long-distance re-orderings are captured effectively.", "labels": [], "entities": []}], "introductionContent": [{"text": "Much research has been done on using syntactic information in statistical machine translation.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 62, "end_pos": 93, "type": "TASK", "confidence": 0.7229894797007242}]}, {"text": "In this paper we use chunks (shallow syntax information) to improve an N -gram-based SMT system.", "labels": [], "entities": [{"text": "SMT", "start_pos": 85, "end_pos": 88, "type": "TASK", "confidence": 0.9180203080177307}]}, {"text": "We tackle both the alignment and reordering problems of a language pair with important differences in word order (Arabic-English).", "labels": [], "entities": []}, {"text": "These differences lead to noisy word alignments, which lower the accuracy of the derived translation table.", "labels": [], "entities": [{"text": "word alignments", "start_pos": 32, "end_pos": 47, "type": "TASK", "confidence": 0.6927234977483749}, {"text": "accuracy", "start_pos": 65, "end_pos": 73, "type": "METRIC", "confidence": 0.999285876750946}]}, {"text": "Additionally, word order differences, especially those spanning long distances and/or including multiple levels of reordering, area challenge for SMT decoding.", "labels": [], "entities": [{"text": "SMT decoding", "start_pos": 146, "end_pos": 158, "type": "TASK", "confidence": 0.9469713270664215}]}, {"text": "Two improvements are presented here.", "labels": [], "entities": []}, {"text": "First, we reduce the number of noisy alignments by using the idea that chunks, like raw words, have a translation correspondence in the source and target sentences.", "labels": [], "entities": []}, {"text": "Hence, word links are constrained (i.e., noisy links are pruned) using chunk information.", "labels": [], "entities": []}, {"text": "Second, we introduce rewrite rules which can handle both short/medium and long distance reorderings as well as different degrees of recursive application.", "labels": [], "entities": []}, {"text": "We build our rules with two different linguistic annotations, (local) POS tags and (long-spanning) chunk tags.", "labels": [], "entities": []}, {"text": "Despite employing an N -gram-based SMT system, the methods described here can also be applied to any phrase-based SMT system.", "labels": [], "entities": [{"text": "SMT", "start_pos": 35, "end_pos": 38, "type": "TASK", "confidence": 0.9314872622489929}, {"text": "SMT", "start_pos": 114, "end_pos": 117, "type": "TASK", "confidence": 0.7770775556564331}]}, {"text": "Alignment and reordering are similarly used in both approaches.", "labels": [], "entities": [{"text": "Alignment", "start_pos": 0, "end_pos": 9, "type": "TASK", "confidence": 0.9019290804862976}]}, {"text": "In Section 2 we discuss previous related work.", "labels": [], "entities": []}, {"text": "In Section 3, we discuss Arabic linguistic issues and motivate some of our decisions.", "labels": [], "entities": []}, {"text": "In Section 4, we describe the N -gram based SMT system which we extend in this paper.", "labels": [], "entities": [{"text": "SMT", "start_pos": 44, "end_pos": 47, "type": "TASK", "confidence": 0.978185772895813}]}, {"text": "Sections 5 and 6 detail the main contributions of this work.", "labels": [], "entities": []}, {"text": "In Section 7, we carryout evaluation experiments reporting on the accuracy results and give details of a human evaluation error analysis.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 66, "end_pos": 74, "type": "METRIC", "confidence": 0.9986956715583801}]}], "datasetContent": [{"text": "All of the training data used here is available from the Linguistic Data Consortium (LDC).", "labels": [], "entities": [{"text": "Linguistic Data Consortium (LDC)", "start_pos": 57, "end_pos": 89, "type": "DATASET", "confidence": 0.8564072251319885}]}, {"text": "We use an Arabic-English parallel corpus 4 consisting of 131K sentence pairs, with approximately 4.1M Arabic tokens and 4.4M English tokens.", "labels": [], "entities": []}, {"text": "Word alignment is done with GIZA++ (.", "labels": [], "entities": [{"text": "Word alignment", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.7131312340497971}]}, {"text": "All evaluated systems use the same surface trigram language model, trained on approximately 340 million words of English newswire text from the English Gigaword corpus (LDC2003T05).", "labels": [], "entities": [{"text": "English Gigaword corpus (LDC2003T05)", "start_pos": 144, "end_pos": 180, "type": "DATASET", "confidence": 0.8179105321566263}]}, {"text": "Additionally, we use a 5-gram language model computed over the POS tagged English side of the training corpus.", "labels": [], "entities": []}, {"text": "Language models are implemented using the SRILM toolkit).", "labels": [], "entities": [{"text": "SRILM toolkit", "start_pos": 42, "end_pos": 55, "type": "DATASET", "confidence": 0.805760383605957}]}, {"text": "For Arabic tokenization, we use the Arabic TreeBank tokenization scheme: 4-way normalized segments into conjunction, particle, word and pronominal clitic.", "labels": [], "entities": [{"text": "Arabic TreeBank", "start_pos": 36, "end_pos": 51, "type": "DATASET", "confidence": 0.7241353541612625}]}, {"text": "For POS tagging, we use the collapsed tagset for PATB (24 tags).", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 4, "end_pos": 15, "type": "TASK", "confidence": 0.8310134708881378}, {"text": "PATB", "start_pos": 49, "end_pos": 53, "type": "METRIC", "confidence": 0.8431915044784546}]}, {"text": "Tokenization and POS tagging are done using the publicly available Morphological Analysis and Disambiguation of Arabic (MADA) tool).", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 17, "end_pos": 28, "type": "TASK", "confidence": 0.8041553497314453}, {"text": "Morphological Analysis and Disambiguation of Arabic (MADA)", "start_pos": 67, "end_pos": 125, "type": "TASK", "confidence": 0.7535689373811086}]}, {"text": "For chunking Arabic, we use the AMIRA (ASVMT) toolkit ().", "labels": [], "entities": [{"text": "chunking Arabic", "start_pos": 4, "end_pos": 19, "type": "TASK", "confidence": 0.915523886680603}]}, {"text": "English preprocessing simply included down-casing, separating punctuation from words and splitting off \"'s\".", "labels": [], "entities": []}, {"text": "The English side is POS-tagged with TNT) and chunked with the freely available OpenNlp 5 tools.", "labels": [], "entities": []}, {"text": "We use the standard four-reference NIST MTEval data sets for the (henceforth MT03, MT04 and MT05, respectively) for testing and the 2002 data set for tuning.", "labels": [], "entities": [{"text": "NIST MTEval data sets", "start_pos": 35, "end_pos": 56, "type": "DATASET", "confidence": 0.901875227689743}, {"text": "MT03", "start_pos": 77, "end_pos": 81, "type": "DATASET", "confidence": 0.9431057572364807}, {"text": "MT04", "start_pos": 83, "end_pos": 87, "type": "DATASET", "confidence": 0.8108989596366882}, {"text": "MT05", "start_pos": 92, "end_pos": 96, "type": "DATASET", "confidence": 0.8637874722480774}, {"text": "2002 data set", "start_pos": 132, "end_pos": 145, "type": "DATASET", "confidence": 0.8887954552968343}]}, {"text": "BLEU-4 (), METEOR (Banerjee and) and multiple-reference Word Error Rate scores are reported.", "labels": [], "entities": [{"text": "BLEU-4", "start_pos": 0, "end_pos": 6, "type": "METRIC", "confidence": 0.9846771955490112}, {"text": "METEOR", "start_pos": 11, "end_pos": 17, "type": "METRIC", "confidence": 0.9970788955688477}, {"text": "multiple-reference Word Error Rate", "start_pos": 37, "end_pos": 71, "type": "METRIC", "confidence": 0.6093435809016228}]}, {"text": "SMT decoding is done using MARIE, 7 a freely available N -gram-based decoder implementing abeam search strategy with distortion/reordering capabilities.", "labels": [], "entities": [{"text": "SMT decoding", "start_pos": 0, "end_pos": 12, "type": "TASK", "confidence": 0.9458428621292114}, {"text": "MARIE", "start_pos": 27, "end_pos": 32, "type": "METRIC", "confidence": 0.7831251621246338}]}, {"text": "Optimization is done with an in-house implementation of the SIMPLEX (Nelder and Mead, 1965) algorithm.", "labels": [], "entities": [{"text": "SIMPLEX (Nelder and Mead, 1965) algorithm", "start_pos": 60, "end_pos": 101, "type": "TASK", "confidence": 0.5093399915430281}]}, {"text": "We contrast three systems built from different word alignments: (a.) the Union alignment set of both translation directions (U); (b.) the refined alignment set, detailed in Section 6, employing only source-side chunks (rS); (c.) the refined alignment set employing source as well as target-side chunks (rST).", "labels": [], "entities": []}, {"text": "For this experiment, the system employs an ngram bilingual translation model (TM) with n = 3 and n = 4.", "labels": [], "entities": [{"text": "ngram bilingual translation", "start_pos": 43, "end_pos": 70, "type": "TASK", "confidence": 0.47958890597025555}]}, {"text": "We also vary the use of a 5-gram targettag language model (ttLM).", "labels": [], "entities": []}, {"text": "The reordering graph is built using POS-based rules restricted to a maximum size of 6 tokens (POS tags in the left-hand side of the rule).", "labels": [], "entities": []}, {"text": "The results are shown in.", "labels": [], "entities": []}, {"text": "Results from the refined alignment (rS) system clearly outperform the results from the alignment union (U) system.", "labels": [], "entities": []}, {"text": "All measures agree in all test sets.", "labels": [], "entities": []}, {"text": "Results further improve when we employ target-side chunks to refine the alignments (rST), although not statistically significantly.", "labels": [], "entities": [{"text": "rST", "start_pos": 84, "end_pos": 87, "type": "METRIC", "confidence": 0.9113959074020386}]}, {"text": "BLEU 95% confidence intervals for the best configuration (last row) are \u00b1.0162, \u00b1.0210 and \u00b1.0135 respectively for MT03, MT04 and MT05.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.9403278231620789}, {"text": "\u00b1.", "start_pos": 72, "end_pos": 74, "type": "METRIC", "confidence": 0.9532539248466492}, {"text": "MT03", "start_pos": 115, "end_pos": 119, "type": "DATASET", "confidence": 0.9172006249427795}, {"text": "MT04", "start_pos": 121, "end_pos": 125, "type": "DATASET", "confidence": 0.8769822716712952}, {"text": "MT05", "start_pos": 130, "end_pos": 134, "type": "DATASET", "confidence": 0.9225236177444458}]}, {"text": "As anticipated, the N -gram system suffers under high reordering needs when noisy alignments produce long (sparse) tuples.", "labels": [], "entities": []}, {"text": "This can be seen by the increase in translation unit counts when refined links are used to alleviate the sparseness problem.", "labels": [], "entities": []}, {"text": "The number of links of each alignment set overall  training data is 5.5 M (U), 4.9 M (rS) and 4.6 M (rST).", "labels": [], "entities": []}, {"text": "Using the previous sets, the number of unique extracted translation units is 265.5 K (U), 346.3 K (rS) and 407.8 K (rST).", "labels": [], "entities": []}, {"text": "Extending the TM to order 4 and introducing the ttLM seems to further boost the accuracy results for all sets in terms of mWER and for MT03 and MT05 only in terms of BLEU.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 80, "end_pos": 88, "type": "METRIC", "confidence": 0.9994947910308838}, {"text": "MT03", "start_pos": 135, "end_pos": 139, "type": "DATASET", "confidence": 0.8324387073516846}, {"text": "MT05", "start_pos": 144, "end_pos": 148, "type": "DATASET", "confidence": 0.7816212177276611}, {"text": "BLEU", "start_pos": 166, "end_pos": 170, "type": "METRIC", "confidence": 0.996985137462616}]}, {"text": "We compare POS-based reordering rules with chunk-based reordering rules under different maximum rule-size constraints.", "labels": [], "entities": []}, {"text": "Results are obtained using TM n = 4, ttLM n=5 and rST refinement alignment.", "labels": [], "entities": []}, {"text": "BLEU scores are shown in measures the impact of introducing reordering rules limited to a given size (Y axis) on the permutation graphs of input sentences from the MT03 data set (composed of 663 sentences containing 18, 325 words).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.9840976595878601}, {"text": "MT03 data set", "start_pos": 164, "end_pos": 177, "type": "DATASET", "confidence": 0.9601508975028992}]}, {"text": "Column Total shows the number of additional (extended) paths introduced into the test set permutation graph (i.e., 2, 971 additional paths of size 3 POS tags were introduced).", "labels": [], "entities": []}, {"text": "Columns 3 to 8 show the number of moves made in the 1-best translation output according to the size of the move in words (i.e., 1, 652 moves of size 2 words appeared when considering POS rules of up to size 3 words).", "labels": [], "entities": []}, {"text": "The rows in correspond to the columns associated with MT03 in.", "labels": [], "entities": [{"text": "MT03", "start_pos": 54, "end_pos": 58, "type": "DATASET", "confidence": 0.8828108310699463}]}, {"text": "Notice that a chunk tag may refer to multiple words, which explains, for instance, how 42 moves of size 4 appear using chunk rules of size 2.", "labels": [], "entities": []}, {"text": "Overall, short-size reorderings are far more abundant than larger ones.", "labels": [], "entities": []}, {"text": "Differences in BLEU are very small across the alternative configurations (POS/chunk).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 15, "end_pos": 19, "type": "METRIC", "confidence": 0.9974729418754578}]}, {"text": "It seems that larger reorderings, size 7 to 14, (shown in) introduce very small accuracy variations when measured using BLEU.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 80, "end_pos": 88, "type": "METRIC", "confidence": 0.997357189655304}, {"text": "BLEU", "start_pos": 120, "end_pos": 124, "type": "METRIC", "confidence": 0.9920758008956909}]}, {"text": "POS rules are able to account for most of the necessary moves (size 2 to 6).", "labels": [], "entities": []}, {"text": "However, the presence of the larger moves when considering chunk-based rules (together with accuracy improvements) show that long-size reorderings can only be captured by chunk rules.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 92, "end_pos": 100, "type": "METRIC", "confidence": 0.9990992546081543}]}, {"text": "The largest moves taken by the decoder using POS rules consist of 2 sequences of 8 words, column 7, row 9 minus row 8).", "labels": [], "entities": []}, {"text": "The increase in the number of long moves when considering recursive chunks (7R) means that longer chunk rules provide only valid reordering paths if further (recursive) reorderings are also considered.", "labels": [], "entities": []}, {"text": "The corresponding BLEU score, last column) indicates that the new set of moves improves the resulting accuracy.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 18, "end_pos": 28, "type": "METRIC", "confidence": 0.9844629764556885}, {"text": "accuracy", "start_pos": 102, "end_pos": 110, "type": "METRIC", "confidence": 0.9983280301094055}]}, {"text": "The general lower scores and inconsistent behavior of MT04 compared to MT03/MT05 maybe a result of MT04 being a mix of genres (newswire, speeches and editorials).", "labels": [], "entities": [{"text": "MT04", "start_pos": 54, "end_pos": 58, "type": "DATASET", "confidence": 0.8517794609069824}, {"text": "MT05", "start_pos": 76, "end_pos": 80, "type": "DATASET", "confidence": 0.6966795921325684}]}], "tableCaptions": [{"text": " Table 2: Evaluation results for experiments on transla- tion units, alignment and modeling.", "labels": [], "entities": [{"text": "alignment", "start_pos": 69, "end_pos": 78, "type": "TASK", "confidence": 0.9571583867073059}]}, {"text": " Table 3: BLEU scores according to the maximum size of  rules employed.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9972620010375977}]}, {"text": " Table 3. Notice that a chunk  tag may refer to multiple words, which explains, for  instance, how 42 moves of size 4 appear using chunk  rules of size 2. Overall, short-size reorderings are far  more abundant than larger ones.", "labels": [], "entities": []}, {"text": " Table 4: Reorderings hypothesized and employed in the  1-best translation output according to their size.", "labels": [], "entities": []}]}