{"title": [{"text": "An Agreement Measure for Determining Inter-Annotator Reliability of Human Judgements on Affective Text", "labels": [], "entities": [{"text": "Inter-Annotator Reliability of Human Judgements on Affective Text", "start_pos": 37, "end_pos": 102, "type": "TASK", "confidence": 0.7840991541743279}]}], "abstractContent": [{"text": "An affective text maybe judged to belong to multiple affect categories as it may evoke different affects with varying degree of intensity.", "labels": [], "entities": []}, {"text": "For affect classification of text, it is often required to annotate text corpus with affect categories.", "labels": [], "entities": [{"text": "affect classification of text", "start_pos": 4, "end_pos": 33, "type": "TASK", "confidence": 0.8392253667116165}]}, {"text": "This task is often performed by a number of human judges.", "labels": [], "entities": []}, {"text": "This paper presents anew agreement measure inspired by Kappa coefficient to compute inter-annotator reliability when the annotators have freedom to categorize a text into more than one class.", "labels": [], "entities": []}, {"text": "The extended reliability coefficient has been applied to measure the quality of an affective text corpus.", "labels": [], "entities": [{"text": "extended reliability coefficient", "start_pos": 4, "end_pos": 36, "type": "METRIC", "confidence": 0.7106761236985525}]}, {"text": "An analysis of the factors that influence corpus quality has been provided.", "labels": [], "entities": []}], "introductionContent": [{"text": "The accuracy of a supervised machine learning task primarily depends on the annotation quality of the data, that is used for training and cross validation.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.9991552829742432}]}, {"text": "Reliability of annotation is a key requirement for the usability of an annotated corpus.", "labels": [], "entities": []}, {"text": "Inconsistency or noisy annotation may lead to the degradation of performances of supervised learning algorithms.", "labels": [], "entities": []}, {"text": "The data annotated by a single annotator maybe prone to error and hence an unreliable one.", "labels": [], "entities": []}, {"text": "This also holds for annotating an affective corpus, which is highly dependent on the mental state of the subject.", "labels": [], "entities": []}, {"text": "The recent trend in corpus development in NLP is to annotate corpus by more than one annotators independently.", "labels": [], "entities": []}, {"text": "In corpus statistics, the corpus reliability is measured by coefficient of agreement.", "labels": [], "entities": []}, {"text": "The coefficients of agreement are applied to corpus for various goals like measuring reliability, validity and stability of corpus.", "labels": [], "entities": [{"text": "reliability", "start_pos": 85, "end_pos": 96, "type": "METRIC", "confidence": 0.9910773038864136}, {"text": "validity", "start_pos": 98, "end_pos": 106, "type": "METRIC", "confidence": 0.9939028024673462}]}, {"text": "Jacob Cohen introduced Kappa statistics as a coefficient of agreement for nominal scales.", "labels": [], "entities": []}, {"text": "The Kappa coefficient measures the proportion of observed agreement over the agreement by chance and the maximum agreement attainable over chance agreement considering pairwise agreement.", "labels": [], "entities": [{"text": "Kappa coefficient", "start_pos": 4, "end_pos": 21, "type": "METRIC", "confidence": 0.9285995066165924}]}, {"text": "Later Fleiss proposed an extension to measure agreement in ordinal scale data.", "labels": [], "entities": []}, {"text": "Cohen's Kappa has been widely used in various research areas.", "labels": [], "entities": []}, {"text": "Because of its simplicity and robustness, it has become a popular approach for agreement measurement in the area of electronics, geographical informatics, medical, and many more domains.", "labels": [], "entities": [{"text": "agreement measurement", "start_pos": 79, "end_pos": 100, "type": "TASK", "confidence": 0.8746398091316223}]}, {"text": "There are other variants of Kappa like agreement measures.", "labels": [], "entities": []}, {"text": "Scott's \u03c0) was introduced to measure agreement in survey research.", "labels": [], "entities": [{"text": "Scott's \u03c0)", "start_pos": 0, "end_pos": 10, "type": "METRIC", "confidence": 0.6198318228125572}]}, {"text": "Kappa and \u03c0 measures differ in the way they determine the chance related agreements.", "labels": [], "entities": [{"text": "\u03c0", "start_pos": 10, "end_pos": 11, "type": "METRIC", "confidence": 0.8959909081459045}]}, {"text": "\u03c0-like coefficients determine the chance agreement among arbitrary coders, while \u03ba-like coefficients treats the chance of agreement among the coders who produced the reliability data.", "labels": [], "entities": []}, {"text": "One of the drawbacks of \u03c0 and Kappa like coefficients except is that they treat all kinds of disagreements in the same manner. is a reliability measure which treats different kind of disagreements separately by introducing a notion of distance between two categories.", "labels": [], "entities": []}, {"text": "It offers away to measure agreement in nominal, interval, ordinal and ratio scale data.", "labels": [], "entities": []}, {"text": "Reliability assessment of corpus is an important issue in corpus driven natural language processing and the existing reliability measures have been used in various corpus development tasks.", "labels": [], "entities": [{"text": "Reliability assessment", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.7525449097156525}, {"text": "natural language processing", "start_pos": 72, "end_pos": 99, "type": "TASK", "confidence": 0.6732715169588724}]}, {"text": "For example, Kappa coefficient has been used in developing parts of speech corpus), dialogue act tagging efforts like MapTask () and Switchboard (, subjectivity tagging task) and many more.", "labels": [], "entities": [{"text": "Kappa coefficient", "start_pos": 13, "end_pos": 30, "type": "METRIC", "confidence": 0.8907992839813232}, {"text": "dialogue act tagging", "start_pos": 84, "end_pos": 104, "type": "TASK", "confidence": 0.7061565319697062}, {"text": "subjectivity tagging task", "start_pos": 148, "end_pos": 173, "type": "TASK", "confidence": 0.7759358783562978}]}, {"text": "The \u03c0 and \u03ba coefficients measure the reliability of the annotation task where a data item can be annotated with one category.", "labels": [], "entities": [{"text": "reliability", "start_pos": 37, "end_pos": 48, "type": "METRIC", "confidence": 0.9839301705360413}]}, {"text": "() puts an effort towards measuring corpus reliability for multiply labeled data points.", "labels": [], "entities": []}, {"text": "In this measure, the annotators are allowed to mark one data point with at most two classes, one of which is primary and other is secondary.", "labels": [], "entities": []}, {"text": "This measure was used to determine the reliability of a email corpus where emails are assigned with primary and secondary labels from a set of email types.", "labels": [], "entities": [{"text": "reliability", "start_pos": 39, "end_pos": 50, "type": "METRIC", "confidence": 0.9743990302085876}]}, {"text": "Affect recognition from text is a recent and promising subarea of natural language processing.", "labels": [], "entities": [{"text": "Affect recognition from text", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.8955755680799484}, {"text": "natural language processing", "start_pos": 66, "end_pos": 93, "type": "TASK", "confidence": 0.6483700573444366}]}, {"text": "The task is to classify text segments into appropriate affect categories.", "labels": [], "entities": []}, {"text": "The supervised machine learning techniques, which requires a reliable annotated corpus, maybe applied for solving the problem.", "labels": [], "entities": []}, {"text": "In general, a blend of emotions is common in both verbal and non-verbal communication.", "labels": [], "entities": []}, {"text": "Unlike conventional annotation tasks like POS corpus development, where one data item may belong to only one category, in affective text corpus, a data item maybe fuzzy and may belong to multiple affect categories.", "labels": [], "entities": [{"text": "POS corpus development", "start_pos": 42, "end_pos": 64, "type": "TASK", "confidence": 0.6973882814248403}]}, {"text": "For example, the following sentence may belong to disgust and sad category since it may evoke both the emotions to different degrees of intensity.", "labels": [], "entities": []}, {"text": "A young married woman was burnt to death allegedly by her in-laws for dowry.", "labels": [], "entities": []}, {"text": "This property makes the existing agreement measures inapplicable for determining agreement in emotional corpus.", "labels": [], "entities": []}, {"text": "adopted a categorical scheme for annotating emotion in affective text dialogue.", "labels": [], "entities": []}, {"text": "They claimed to address the problem of agreement measurement for the data set where one data item may belong to more than one category using an extension of Krippendorff's \u03b1.", "labels": [], "entities": [{"text": "agreement measurement", "start_pos": 39, "end_pos": 60, "type": "TASK", "confidence": 0.6462887674570084}]}, {"text": "But the details of the extension is yet to be disseminated.", "labels": [], "entities": []}, {"text": "In this paper, we propose anew agreement measure for multiclass annotation which we denote by Am . The new measure is then applied to an affective text corpus to \u2022 Assess Reliability: To test whether the corpus can be used for developing computational affect recognizer.", "labels": [], "entities": [{"text": "computational affect recognizer", "start_pos": 238, "end_pos": 269, "type": "TASK", "confidence": 0.6306562920411428}]}, {"text": "\u2022 Determine Gold Standard: To define a gold standard that will be used to test the accuracy of the affect recognizer.", "labels": [], "entities": [{"text": "Determine Gold Standard", "start_pos": 2, "end_pos": 25, "type": "METRIC", "confidence": 0.7258245547612509}, {"text": "accuracy", "start_pos": 83, "end_pos": 91, "type": "METRIC", "confidence": 0.9969555139541626}, {"text": "affect recognizer", "start_pos": 99, "end_pos": 116, "type": "TASK", "confidence": 0.6931595057249069}]}, {"text": "In section 2, we describe the affective text corpus and the annotation scheme.", "labels": [], "entities": []}, {"text": "In section 3, we propose anew reliability measure (A m ) for multiclass annotated data.", "labels": [], "entities": [{"text": "reliability measure (A m )", "start_pos": 30, "end_pos": 56, "type": "METRIC", "confidence": 0.9257385234038035}]}, {"text": "In section 4, we provide an algorithm to determine gold standard data from the annotation and in section 5, we discuss about applying Am measure to the corpus developed by us and some observations related to the annotation.", "labels": [], "entities": [{"text": "gold standard data", "start_pos": 51, "end_pos": 69, "type": "DATASET", "confidence": 0.6566518247127533}, {"text": "Am measure", "start_pos": 134, "end_pos": 144, "type": "METRIC", "confidence": 0.9585658311843872}]}], "datasetContent": [{"text": "We applied the proposed Am measure to estimate the quality of the affective corpus described in section 2.", "labels": [], "entities": [{"text": "Am measure", "start_pos": 24, "end_pos": 34, "type": "METRIC", "confidence": 0.9764426946640015}]}, {"text": "Below we present the annotation experiment followed by some relevant analysis.", "labels": [], "entities": []}, {"text": "Ten human judges with the same social background participated in the study, assigning affective categories to sentences independently of one another.", "labels": [], "entities": []}, {"text": "The annotators were provided with the annotation instructions and they were trained with some sentences not belonging to the corpus.", "labels": [], "entities": []}, {"text": "The annotation was performed with the help of a web based annotation interface 2 . The corpus consists of 1000 sentences.", "labels": [], "entities": []}, {"text": "Three of judges were able to complete the task within 20 days.", "labels": [], "entities": []}, {"text": "In this paper, we report the result of applying the measure with data provided by three annotators without considering the incomplete annotations.", "labels": [], "entities": []}, {"text": "Distribution of the sentences across the affective categories for the three judges is given in.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Agreement values for the affective text  corpus.", "labels": [], "entities": []}, {"text": " Table 2: Annotator pairwise A m values.", "labels": [], "entities": []}, {"text": " Table 3: Distribution of the sentences over ob- served agreement.", "labels": [], "entities": [{"text": "Distribution", "start_pos": 10, "end_pos": 22, "type": "TASK", "confidence": 0.9739903807640076}]}, {"text": " Table 4: Categorywise disagreement for the annotator pairs.", "labels": [], "entities": []}, {"text": " Table 5: Confusion matrix for category pairs.", "labels": [], "entities": []}]}