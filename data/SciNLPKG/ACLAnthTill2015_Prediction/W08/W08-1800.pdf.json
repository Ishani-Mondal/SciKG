{"title": [{"text": "Coling 2008 22nd International Conference on Computational Linguistics Proceedings of the 2nd workshop on Information Retrieval for Question Answering", "labels": [], "entities": [{"text": "Coling 2008 22nd International Conference on Computational Linguistics", "start_pos": 0, "end_pos": 70, "type": "TASK", "confidence": 0.48264704644680023}, {"text": "Information Retrieval for Question Answering", "start_pos": 106, "end_pos": 150, "type": "TASK", "confidence": 0.6721939742565155}]}], "abstractContent": [], "introductionContent": [{"text": "Open domain question answering (QA) has become a very active research area over the past decade, due in large measure to the stimulus of the TREC Question Answering track (now a track within the recently formed Text Analysis Conference, TAC).", "labels": [], "entities": [{"text": "Open domain question answering (QA)", "start_pos": 0, "end_pos": 35, "type": "TASK", "confidence": 0.823687059538705}, {"text": "TREC Question Answering", "start_pos": 141, "end_pos": 164, "type": "TASK", "confidence": 0.8268124659856161}, {"text": "Text Analysis Conference, TAC)", "start_pos": 211, "end_pos": 241, "type": "TASK", "confidence": 0.8095757265885671}]}, {"text": "This track addresses the task of finding answers to natural language questions (e.g. \"How tall is the Eiffel Tower?\", \"Who is Aaron Copland?\", \"What effect does second-hand smoke have on non-smokers?\") from large text collections.", "labels": [], "entities": []}, {"text": "This task stands in contrast to the more conventional information retrieval (IR) task of finding documents relevant to a query, where the query maybe simply a collection of keywords (e.g. \"Eiffel Tower\", \"American composer, born Brooklyn NY 1900, ...\").", "labels": [], "entities": [{"text": "information retrieval (IR)", "start_pos": 54, "end_pos": 80, "type": "TASK", "confidence": 0.8455864548683166}]}, {"text": "Finding answers requires processing texts at a level of detail that cannot be carried out at retrieval time for very large text collections.", "labels": [], "entities": []}, {"text": "This limitation has led many researchers to rely on, broadly, a two stage approach to the QA task.", "labels": [], "entities": [{"text": "QA task", "start_pos": 90, "end_pos": 97, "type": "TASK", "confidence": 0.922513484954834}]}, {"text": "In stage one a subset of question-relevant texts are selected from the whole collection.", "labels": [], "entities": []}, {"text": "In stage two this subset is subjected to detailed processing for answer extraction.", "labels": [], "entities": [{"text": "answer extraction", "start_pos": 65, "end_pos": 82, "type": "TASK", "confidence": 0.9174469709396362}]}, {"text": "Clearly performance at stage two is bounded by performance at stage one, and previous work has shown that, despite the sophistication of standard IR ranking algorithms, they are not well suited to the stage one task of retrieving relevant documents given short natural language questions.", "labels": [], "entities": [{"text": "IR ranking", "start_pos": 146, "end_pos": 156, "type": "TASK", "confidence": 0.8752068281173706}]}, {"text": "It is likely that improvements in this area will come from linguistic insights into why QA focused IR is different from the traditional IR model.", "labels": [], "entities": [{"text": "QA focused IR", "start_pos": 88, "end_pos": 101, "type": "TASK", "confidence": 0.734046479066213}]}, {"text": "With the continued expansion of QA research into more complex question types and with the speed with which answers are returned becoming an issue, the importance of having good, QA-focused IR techniques is likely to increase.", "labels": [], "entities": [{"text": "QA-focused IR", "start_pos": 178, "end_pos": 191, "type": "TASK", "confidence": 0.7023299038410187}]}, {"text": "To date this topic has received limited explicit attention despite its obvious importance.", "labels": [], "entities": []}, {"text": "This 2nd IR4QA workshop aims to address this situation by continuing to attract the attention of researchers to the specific IR challenges raised by QA.", "labels": [], "entities": []}, {"text": "For this workshop, we solicited papers that addressed any aspect of QA-focused IR, in order to improve overall system performance, , suggesting possible topics such as: \u2022 parameterizations/optimizations of specific IR systems for QA \u2022 studies of query formation strategies suited to QA, e.g. named entity pre-processing of questions \u2022 different uses of IR for different question types (e.g. factoid, list, definition, event, how, ...)", "labels": [], "entities": [{"text": "QA-focused IR", "start_pos": 68, "end_pos": 81, "type": "TASK", "confidence": 0.6501056253910065}, {"text": "IR for different question types (e.g. factoid, list, definition, event, how, ...)", "start_pos": 353, "end_pos": 434, "type": "Description", "confidence": 0.6909412161300057}]}, {"text": "\u2022 utility of term matching constraints, e.g. term proximity, for QA \u2022 analyses of differing IR techniques for QA \u2022 impact of IR performance on overall QA performance \u2022 QA-orientated corpus pre-processing, e.g. indexing POS tags, named entities, semanticallytagged entities, relationships, etc.", "labels": [], "entities": []}, {"text": "rather than simply tokens \u2022 evaluation measures for assessing IR for QA \u2022 retrieval from semi-structured data -i.e. QA from Wikipedia articles From the papers submitted, 10 were selected following peer review.", "labels": [], "entities": [{"text": "IR", "start_pos": 62, "end_pos": 64, "type": "TASK", "confidence": 0.9610080122947693}, {"text": "QA", "start_pos": 69, "end_pos": 71, "type": "TASK", "confidence": 0.7991881966590881}]}, {"text": "These papers are included in this proceedings.", "labels": [], "entities": []}, {"text": "The enthusiastic response to this workshop confirms the belief that this is an important area of interest to a significant number of researchers.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": []}