{"title": [{"text": "Evaluating automatically generated user-focused multi-document summaries for geo-referenced images", "labels": [], "entities": [{"text": "Evaluating automatically generated user-focused multi-document summaries", "start_pos": 0, "end_pos": 72, "type": "TASK", "confidence": 0.7169245084126791}]}], "abstractContent": [{"text": "This paper reports an initial study that aims to assess the viability of a state-of-the-art multi-document summarizer for automatic captioning of geo-referenced images.", "labels": [], "entities": [{"text": "automatic captioning of geo-referenced images", "start_pos": 122, "end_pos": 167, "type": "TASK", "confidence": 0.7437831282615661}]}, {"text": "The automatic captioning procedure requires summarizing multiple web documents that contain information related to images' location.", "labels": [], "entities": [{"text": "summarizing multiple web documents", "start_pos": 44, "end_pos": 78, "type": "TASK", "confidence": 0.8341008573770523}]}, {"text": "We use SUMMA (Saggion and Gaizauskas, 2005) to generate generic and query-based multi-document summaries and evaluate them using ROUGE evaluation metrics (Lin, 2004) relative to human generated summaries.", "labels": [], "entities": []}, {"text": "Results show that, even though query-based summaries perform better than generic ones, they are still not selecting the information that human participants do.", "labels": [], "entities": []}, {"text": "In particular, the areas of interest that human summaries display (history, travel information, etc.) are not contained in the query-based summaries.", "labels": [], "entities": []}, {"text": "For our future work in automatic image captioning this result suggests that developing the query-based summarizer further and biasing it to account for user-specific requirements will prove worthwhile.", "labels": [], "entities": [{"text": "automatic image captioning", "start_pos": 23, "end_pos": 49, "type": "TASK", "confidence": 0.6680478155612946}]}], "introductionContent": [{"text": "Retrieving textual information related to a location shown in an image has many potential applications.", "labels": [], "entities": [{"text": "Retrieving textual information related to a location shown in an image", "start_pos": 0, "end_pos": 70, "type": "TASK", "confidence": 0.8044209480285645}]}, {"text": "It could help users gain quick access to the information they seek about a place of interest just by taking its picture.", "labels": [], "entities": []}, {"text": "Such textual information could also, for instance, be used by a journalist who is planning to write an article about a building, or by a tourist who seeks further interesting places to visit nearby.", "labels": [], "entities": []}, {"text": "In this paper we aim to generate such textual information automatically by utilizing multi-document summarization techniques, where documents to be summarized are web documents that contain information related to the image content.", "labels": [], "entities": [{"text": "multi-document summarization", "start_pos": 85, "end_pos": 113, "type": "TASK", "confidence": 0.6121568381786346}]}, {"text": "We focus on geo-referenced images, i.e. images tagged with coordinates (latitude and longitude) and compass information, that show things with fixed locations (e.g. buildings, mountains, etc.).", "labels": [], "entities": []}, {"text": "Attempts towards automatic generation of image-related textual information or captions have been previously reported. and).", "labels": [], "entities": [{"text": "automatic generation of image-related textual information or captions", "start_pos": 17, "end_pos": 86, "type": "TASK", "confidence": 0.7260799556970596}]}, {"text": "These approaches can address all kinds of images, but focus mostly on images of people.", "labels": [], "entities": []}, {"text": "They analyze only the immediate textual context of the image on the web and are concerned with describing what is in the image only.", "labels": [], "entities": []}, {"text": "Consequently, background information about the objects in the image is not provided.", "labels": [], "entities": []}, {"text": "Our aim, however, is to have captions that inform users' specific interests about a location, which clearly includes more than just image content description.", "labels": [], "entities": []}, {"text": "Multi-document summarization techniques offer the possibility to include image-related information from multiple documents, however, the challenge lies in being able to summarize unrestricted web documents.", "labels": [], "entities": [{"text": "Multi-document summarization", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.6315571963787079}, {"text": "summarize unrestricted web documents", "start_pos": 169, "end_pos": 205, "type": "TASK", "confidence": 0.886880025267601}]}, {"text": "Various multi-document summarization tools have been developed: SUMMA (Saggion and), MEAD (), CLASSY (), CATS) and the system of, to name just a few.", "labels": [], "entities": [{"text": "MEAD", "start_pos": 85, "end_pos": 89, "type": "METRIC", "confidence": 0.9869009852409363}, {"text": "CATS", "start_pos": 105, "end_pos": 109, "type": "METRIC", "confidence": 0.747001051902771}]}, {"text": "These systems generate either generic or query-based summaries or both.", "labels": [], "entities": []}, {"text": "Generic summaries address abroad readership whereas query-based summaries are preferred by specific groups of people aiming for quick knowledge gain about specific topics).", "labels": [], "entities": []}, {"text": "SUMMA and MEAD generate both generic and query-based multi-document summaries.", "labels": [], "entities": []}, {"text": "create only generic summaries, while CLASSY and CATS create only query-based summaries from multiple documents.", "labels": [], "entities": [{"text": "summaries", "start_pos": 20, "end_pos": 29, "type": "TASK", "confidence": 0.9392550587654114}]}, {"text": "The performance of these tools has been reported for DUC tasks . As note, although DUC tasks provide a common evaluation standard, they are restricted in topic and are somewhat idealized.", "labels": [], "entities": [{"text": "DUC tasks", "start_pos": 53, "end_pos": 62, "type": "TASK", "confidence": 0.7697974443435669}]}, {"text": "For our purposes the summarizer needs to create summaries from unrestricted web input, for which there are no previous performance reports.", "labels": [], "entities": [{"text": "summarizer", "start_pos": 21, "end_pos": 31, "type": "TASK", "confidence": 0.9674702882766724}]}, {"text": "For this reason we evaluate the performance of both a generic and a query-based summarizer and use SUMMA which provides both summarization modes.", "labels": [], "entities": []}, {"text": "We hypothesize that a query-based summarizer will better address the problem of creating summaries tailored to users' needs.", "labels": [], "entities": []}, {"text": "This is because the query itself may contain important hints as to what the user is interested in.", "labels": [], "entities": []}, {"text": "A generic summarizer generates summaries based on the topics it observes from the documents and cannot take user specific input into consideration.", "labels": [], "entities": [{"text": "summarizer generates summaries", "start_pos": 10, "end_pos": 40, "type": "TASK", "confidence": 0.7936291694641113}]}, {"text": "Using SUMMA, we generate both generic and query-based multidocument summaries of image-related documents obtained from the web.", "labels": [], "entities": []}, {"text": "In an online data collection procedure we presented a set of images with related web documents to human subjects and asked them to select from these documents the information that best describes the image.", "labels": [], "entities": []}, {"text": "Based on this user information we created model summaries against which we evaluated the automatically generated ones.", "labels": [], "entities": []}, {"text": "Section 2 in this paper describes how imagerelated documents were collected from the web.", "labels": [], "entities": []}, {"text": "In section 3 SUMMA is described in detail.", "labels": [], "entities": [{"text": "SUMMA", "start_pos": 13, "end_pos": 18, "type": "TASK", "confidence": 0.9654000401496887}]}, {"text": "In 1 http://www-nlpir.nist.gov/projects/duc/index.html section 4 we explain how the human image descriptions were collected.", "labels": [], "entities": []}, {"text": "Section 5 discusses the results, and section 6 concludes the paper and outlines directions for future work and improvements.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Number of images annotated by each particant", "labels": [], "entities": []}, {"text": " Table 2: Information related to Figure 2", "labels": [], "entities": []}, {"text": " Table 3: Comparison: Automatically generated summaries against model summaries. The column GenericToModel for", "labels": [], "entities": []}, {"text": " Table 4: Comparison: Model summaries against each other", "labels": [], "entities": [{"text": "summaries", "start_pos": 28, "end_pos": 37, "type": "TASK", "confidence": 0.8345902562141418}]}]}