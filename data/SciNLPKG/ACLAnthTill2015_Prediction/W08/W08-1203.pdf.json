{"title": [], "abstractContent": [{"text": "Many interesting phenomena in conversation can only be annotated as a subjective task, requiring interpretative judgements from annotators.", "labels": [], "entities": []}, {"text": "This leads to data which is annotated with lower levels of agreement not only due to errors in the annotation, but also due to the differences in how annotators interpret conversations.", "labels": [], "entities": []}, {"text": "This paper constitutes an attempt to find out how subjective annotations with a low level of agreement can profitably be used for machine learning purposes.", "labels": [], "entities": []}, {"text": "We analyse the (dis)agreements between annotators for two different cases in a multimodal annotated corpus and explicitly relate the results to the way machine-learning algorithms perform on the annotated data.", "labels": [], "entities": []}, {"text": "Finally we present two new concepts, namely 'subjective entity' clas-sifiers resp.", "labels": [], "entities": []}, {"text": "'consensus objective' classi-fiers, and give recommendations for using subjective data in machine-learning applications .", "labels": [], "entities": []}], "introductionContent": [{"text": "Research that makes use of multimodal annotated corpora is always presented with something of a dilemma.", "labels": [], "entities": []}, {"text": "One would prefer to have results which are reproducible and independent of the particular annotators that produced the corpus.", "labels": [], "entities": []}, {"text": "One needs data which is annotated with as few disagreements between annotators as possible.", "labels": [], "entities": []}, {"text": "But labeling a corpus is a task which involves a judgement by the an- notator and is therefore, in a sense, always a subjective task.", "labels": [], "entities": [{"text": "labeling a corpus", "start_pos": 4, "end_pos": 21, "type": "TASK", "confidence": 0.8634874025980631}]}, {"text": "Of course, for some phenomena those judgements can be expected to come out mostly the same for different annotators.", "labels": [], "entities": []}, {"text": "For other phenomena the judgements can be more dependent on the annotator interpreting the behavior being annotated, leading to annotations which are more subjective in nature.", "labels": [], "entities": []}, {"text": "The amount of overlap or agreement between annotations is then also influenced by the amount of intersubjectivity in the judgements of annotators.", "labels": [], "entities": []}, {"text": "This relates to the spectrum of content types discussed extensively by.", "labels": [], "entities": []}, {"text": "One of the major distinctions that they make is a distinction in annotation of manifest content (directly observable events), pattern latent content (events that need to be inferred indirectly from the observations), and projective latent content (loosely said, events that require a subjective interpretation from the annotator).", "labels": [], "entities": []}, {"text": "Manifest content is what is directly observable.", "labels": [], "entities": []}, {"text": "Some examples are annotation of instances where somebody raises his hand or raises an eyebrow, annotation of the words being said and indicating whether there is a person in view of the camera.", "labels": [], "entities": []}, {"text": "Annotating manifest content can be a relatively easy task.", "labels": [], "entities": [{"text": "Annotating manifest content", "start_pos": 0, "end_pos": 27, "type": "TASK", "confidence": 0.8760660886764526}]}, {"text": "Although the annotation task involves a judgement by the annotator, those judgements should not diverge a lot for different annotators.", "labels": [], "entities": []}, {"text": "At the other end of the spectrum we find projective latent content.", "labels": [], "entities": []}, {"text": "This is a type of content for which the annotation schema does not specify in extreme detail the rules and surface forms that determine the applicability of classes, but in which the coding relies on the annotators' existing mental conception 1 of the classes.", "labels": [], "entities": []}, {"text": "Such an ap-proach is useful for everyday concepts that most people understand and to a certain extent share a common meaning for, but for which it is almost impossible to provide adequately complete definitions.", "labels": [], "entities": []}, {"text": "Potter and Levine-Donnerstein use the example 'chair' for everyday concepts that are difficult to define exhaustively.", "labels": [], "entities": []}, {"text": "But this concept is also especially relevant in an application context that requires the end user of the data to agree with the distinctions being made.", "labels": [], "entities": []}, {"text": "This is very important when machine learning classifiers are developed to be used in everyday applications.", "labels": [], "entities": []}, {"text": "For example, one can make a highly circumscribed, ethologically founded definition of the class 'dominant' to guide annotation.", "labels": [], "entities": []}, {"text": "This is good for, e.g., research into social processes in multiparty conversations.", "labels": [], "entities": []}, {"text": "However, in a scenario where an automatic classifier, trained to recognize this class, is to be used in an application that gives a participant in a meeting a quiet warning when he is being too dominant one would instead prefer the class rather to fit the mental conceptions of dominance that a 'naive' user may have.", "labels": [], "entities": []}, {"text": "When one designs an annotation scheme for projective latent content, the focus of the annotation guidelines is on instructions that trigger the appropriate existing mental conceptions of the annotators rather than on writing exhaustive descriptions of how classes can be distinguished from each other).", "labels": [], "entities": []}, {"text": "Interannotator agreement takes on different roles for the two ends of the spectrum.", "labels": [], "entities": []}, {"text": "For manifest content the level of agreement tells you something about how accurate the measurement instrument (schema plus coders) is.", "labels": [], "entities": []}, {"text": "Bakeman and Gottman, in their textbook observing interaction: introduction to sequential analysis, say about this type of reliability measurement that it is a matter of \"calibrating your observers\".", "labels": [], "entities": []}, {"text": "For projective content, we have additional problems; the level of agreement maybe influenced by the level of intersubjectivity, too.", "labels": [], "entities": []}, {"text": "Where describes that annotators should be interchangeable, annotations of projective latent content can sometimes say as much about the mental conceptions of the particular annotator as about the person whose interactions are being annotated.", "labels": [], "entities": []}, {"text": "The personal interpretations of the data by the annotator should not necessarily be seen as 'errors', though, even if those interpretations lead to low inpaper to avoid confusion with the term \"annotation scheme\".", "labels": [], "entities": []}, {"text": "terannotator agreement: they may simply bean unavoidable aspect of the interesting type of data one works with.", "labels": [], "entities": []}, {"text": "Many different sources of low agreement levels, and many different solutions, are discussed in the literature.", "labels": [], "entities": []}, {"text": "It is important to note that some types of disagreement are more systematic and other types are more noise like.", "labels": [], "entities": []}, {"text": "For projective latent content one would expect more consistent structure in the disagreements between annotators as they are caused by the differences in the personal ways of interpreting multimodal interaction.", "labels": [], "entities": []}, {"text": "Such systematic disagreements are particularly problematic for subsequent use of the data, more so than noiselike disagreements.", "labels": [], "entities": []}, {"text": "Therefore, an analysis of the quality of an annotated corpus should not stop at presenting the value of a reliability metric; instead one should investigate the patterns in the disagreements and discuss the possible impact they have on the envisioned uses of the data.", "labels": [], "entities": []}, {"text": "Some sources of disagreements are the following.", "labels": [], "entities": []}, {"text": "(1) 'Clerical errors' caused by a limited view of the interactions being annotated (low quality video, no audio, occlusions, etc) or by slipshod work of the annotator or the annotator misunderstanding the instructions.", "labels": [], "entities": []}, {"text": "Some solutions are to provide better instructions and training, using only good annotators, and using high quality recordings of the interaction being annotated.", "labels": [], "entities": []}, {"text": "(2) 'Invalid or imprecise annotation schemas' that contain classes that are not relevant or do not contain classes that are relevant, or force the annotator to make choices that are not appropriate to the data (e.g. to choose one label fora unit where more labels are applicable).", "labels": [], "entities": []}, {"text": "Solutions concern redesigning the annotation schema, for example by merging difference classes, allowing annotators to use multiple labels, removing classes, or adding new classes.", "labels": [], "entities": []}, {"text": "(3) 'Genuinely ambiguous expressions' as described by.", "labels": [], "entities": [{"text": "Genuinely", "start_pos": 5, "end_pos": 14, "type": "METRIC", "confidence": 0.885503351688385}]}, {"text": "They discuss that disagreements caused by ambiguity are not so easily solved.", "labels": [], "entities": []}, {"text": "(4) 'A low level of intersubjectivity' for the interpretative judgements of the annotators, caused by the fact that there is less than perfect overlap between the mental conceptions of the annotators.", "labels": [], "entities": [{"text": "intersubjectivity", "start_pos": 20, "end_pos": 37, "type": "METRIC", "confidence": 0.9443269968032837}]}, {"text": "The solutions mentioned above for issue (2) partly also apply here.", "labels": [], "entities": []}, {"text": "However, in this article we focus on an additional, entirely different, way of coping with disagreements resulting from a low level of intersubjectivity that actively exploits the systematic differences in the annotations caused by this.", "labels": [], "entities": []}], "datasetContent": [{"text": "For the first experiment we trained a Bayesian Network adapted from Jovanovi\u010d (2007) on a mix of utterances from all contexts, and tested its performance on utterances from the three different contexts: (1) all data, (2) all data in the context 'at least some person in speaker's FOA' and (3) all data in the context 'no person in speaker's FOA during utterance'.", "labels": [], "entities": []}, {"text": "As was to be expected, the performance in the second context showed a clear gain compared to the first context, and the performance in the third context was clearly worse.", "labels": [], "entities": []}, {"text": "The performance differences, for different train/test splits, tend to be about five percent.", "labels": [], "entities": []}, {"text": "Second Experiment Because the second context showed such a better performance, we ran a second experiment where we trained the network on only data from the second context, to see if we could improve the performance in that context even more.", "labels": [], "entities": []}, {"text": "In different train/test splits this gave us another small performance increase.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Sizes of train and test data sets used and the distribution of class labels over these data sets for  the different annotators.", "labels": [], "entities": []}, {"text": " Table 2: Performance of classifiers (in terms of ac- curacy values -i.e. percentage correct predictions)  trained and tested on various data sets. Results  were obtained with a decision tree classifier, J48  in the Weka toolkit.", "labels": [], "entities": [{"text": "ac- curacy values -i.e. percentage correct predictions", "start_pos": 50, "end_pos": 104, "type": "METRIC", "confidence": 0.8247320916917589}, {"text": "Weka toolkit", "start_pos": 216, "end_pos": 228, "type": "DATASET", "confidence": 0.9642794132232666}]}, {"text": " Table 3: Performance of the MaxEnt classifiers (in  terms of accuracy values -i.e. percentage cor- rect predictions) tested on the whole test set, a mix  of three annotators data (4179 \"Yeah\" utterances).  The first column between brackets the size of the  train sets.", "labels": [], "entities": [{"text": "accuracy values -i.e. percentage cor- rect predictions)", "start_pos": 62, "end_pos": 117, "type": "METRIC", "confidence": 0.8694053292274475}]}, {"text": " Table 4: Precision values per class label for the  classifiers.", "labels": [], "entities": [{"text": "Precision", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9960659146308899}]}]}