{"title": [{"text": "Improving Word Alignment with Language Model Based Confidence Scores", "labels": [], "entities": [{"text": "Improving Word Alignment", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.8863911628723145}]}], "abstractContent": [{"text": "This paper describes the statistical machine translation systems submitted to the ACL-WMT 2008 shared translation task.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 25, "end_pos": 56, "type": "TASK", "confidence": 0.6296558479468027}, {"text": "ACL-WMT 2008 shared translation task", "start_pos": 82, "end_pos": 118, "type": "TASK", "confidence": 0.7826830267906189}]}, {"text": "Systems were submitted for two translation directions: English\u2192Spanish and Spanish\u2192English.", "labels": [], "entities": []}, {"text": "Using sentence pair confidence scores estimated with source and target language models, improvements are observed on the News-Commentary test sets.", "labels": [], "entities": [{"text": "News-Commentary test sets", "start_pos": 121, "end_pos": 146, "type": "DATASET", "confidence": 0.9455826282501221}]}, {"text": "Genre-dependent sentence pair confidence score and integration of sentence pair confidence score into phrase table are also investigated .", "labels": [], "entities": [{"text": "Genre-dependent sentence pair confidence score", "start_pos": 0, "end_pos": 46, "type": "METRIC", "confidence": 0.5869078874588013}]}], "introductionContent": [{"text": "Word alignment models area crucial component in statistical machine translation systems.", "labels": [], "entities": [{"text": "Word alignment", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.7629018425941467}, {"text": "statistical machine translation", "start_pos": 48, "end_pos": 79, "type": "TASK", "confidence": 0.675635556379954}]}, {"text": "When estimating the parameters of the word alignment models, the sentence pair probability is an important factor in the objective function and is approximated by the empirical probability.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 38, "end_pos": 52, "type": "TASK", "confidence": 0.722459152340889}]}, {"text": "The empirical probability for each sentence pair is estimated by maximum likelihood estimation over the training data.", "labels": [], "entities": []}, {"text": "Due to the limitation of training data, most sentence pairs occur only once, which makes the empirical probability almost uniform.", "labels": [], "entities": []}, {"text": "This is a rather weak approximation of the true distribution.", "labels": [], "entities": []}, {"text": "In this paper, we investigate the methods of weighting sentence pairs using language models, and extended the general weighting method to genre-dependent weight.", "labels": [], "entities": [{"text": "weighting sentence pairs", "start_pos": 45, "end_pos": 69, "type": "TASK", "confidence": 0.8816670179367065}]}, {"text": "A method of integrating the weight directly into the phrase table is also explored.", "labels": [], "entities": []}], "datasetContent": [{"text": "The first step in validating the proposed approach was to check if the different language models do assign different weights to the sentence pairs in the training corpora.", "labels": [], "entities": []}, {"text": "Using the different language models NC (NewsCommentary), EP (Europarl), NC+EP (both NC and EP) the genre-specific sentence pair confidence scores were calculated.", "labels": [], "entities": []}, {"text": "shows the distributions of the differences in these scores across the two corpora.", "labels": [], "entities": []}, {"text": "As expected, the language model build from the NC corpus assigns -on average -higher weights to sentence pairs in the NC corpus and lower weights to sentence pairs in the EP corpus (.", "labels": [], "entities": [{"text": "NC corpus", "start_pos": 47, "end_pos": 56, "type": "DATASET", "confidence": 0.8976505398750305}, {"text": "NC corpus", "start_pos": 118, "end_pos": 127, "type": "DATASET", "confidence": 0.8849336206912994}]}, {"text": "The opposite is true for the EP LM.", "labels": [], "entities": [{"text": "EP LM", "start_pos": 29, "end_pos": 34, "type": "DATASET", "confidence": 0.9717817604541779}]}, {"text": "When comparing the scores calculated from the NC LM and the combined NC+EP LM we still see a clear separation (.", "labels": [], "entities": [{"text": "NC LM", "start_pos": 46, "end_pos": 51, "type": "DATASET", "confidence": 0.8941816687583923}]}, {"text": "No marked difference can be seen between using the EP LM and the NC+EP LM (), which again is expected, as the NC corpus is very small compared to the EP corpus.", "labels": [], "entities": []}, {"text": "The next step was to retrain the word alignment models using sentences weights according to the various con- shows training and test set perplexities for IBM model 4 for both training directions.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 33, "end_pos": 47, "type": "TASK", "confidence": 0.743023693561554}]}, {"text": "Not only do we see a drop in training set perplexities, but also in test set perplexities.", "labels": [], "entities": []}, {"text": "Using the genre specific confidence scores leads to lower perplexities on the corresponding test set, which means that using the proposed method does lead to small, but consistent adjustments in the alignment models.", "labels": [], "entities": []}, {"text": "In the final step the specific alignment models were used to generate various phrase tables, which were then used in translation experiments.", "labels": [], "entities": [{"text": "translation", "start_pos": 117, "end_pos": 128, "type": "TASK", "confidence": 0.9642578959465027}]}, {"text": "Results are shown in Table 4.", "labels": [], "entities": []}, {"text": "We report lower-cased Bleu scores.", "labels": [], "entities": [{"text": "Bleu scores", "start_pos": 22, "end_pos": 33, "type": "METRIC", "confidence": 0.9720280766487122}]}, {"text": "We used ncdev2007 (NCt1) as an additional held-out evaluation set.", "labels": [], "entities": []}, {"text": "Bold cells indicate highest scores.", "labels": [], "entities": []}, {"text": "As we can see from the results, improvements are obtained by using sentence pair confidence scores.", "labels": [], "entities": []}, {"text": "Using confidence scores calculated from the EP LM gave overall the best performance.", "labels": [], "entities": [{"text": "confidence", "start_pos": 6, "end_pos": 16, "type": "METRIC", "confidence": 0.972183883190155}, {"text": "EP LM", "start_pos": 44, "end_pos": 49, "type": "DATASET", "confidence": 0.8980542123317719}]}, {"text": "While we observe only a small improvement on Europarl sets, improvements on News-Commentary sets are more pronounced, especially on held-out evaluation sets NCt and NCt1.", "labels": [], "entities": [{"text": "Europarl sets", "start_pos": 45, "end_pos": 58, "type": "DATASET", "confidence": 0.969100832939148}, {"text": "News-Commentary sets", "start_pos": 76, "end_pos": 96, "type": "DATASET", "confidence": 0.893808901309967}]}, {"text": "The experiments do not give evidence that genre-dependent confidence can improve over using the general confidence: Translation results (NIST-BLEU) using pc with different genre-specific language models for Es\u2194En systems shows experiments results in NIST-BLEU using pc score as an additional feature on phrase tables in Es\u2194En systems.", "labels": [], "entities": [{"text": "Translation", "start_pos": 116, "end_pos": 127, "type": "TASK", "confidence": 0.9559070467948914}, {"text": "NIST-BLEU", "start_pos": 137, "end_pos": 146, "type": "DATASET", "confidence": 0.9391958713531494}, {"text": "NIST-BLEU", "start_pos": 250, "end_pos": 259, "type": "DATASET", "confidence": 0.9142259359359741}]}, {"text": "We observed that across development and held-out sets the gains from pc are inconsistent, therefore our submissions are selected from the B5+EP system.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Statistics of English\u2194Spanish Europarl and News- Commentary corpora", "labels": [], "entities": [{"text": "Europarl", "start_pos": 40, "end_pos": 48, "type": "DATASET", "confidence": 0.7411481738090515}]}, {"text": " Table 2: NIST-BLEU scores of baseline and improved baseline  systems experiments on English\u2194Spanish", "labels": [], "entities": []}, {"text": " Table 3: IBM model 4 training and test set perplexities using  genre specific sentence pair confidence scores.", "labels": [], "entities": []}, {"text": " Table 4: Translation results (NIST-BLEU) using gdsc with dif- ferent genre-specific language models for Es\u2194En systems", "labels": [], "entities": [{"text": "Translation", "start_pos": 10, "end_pos": 21, "type": "TASK", "confidence": 0.9614540934562683}, {"text": "NIST-BLEU", "start_pos": 31, "end_pos": 40, "type": "DATASET", "confidence": 0.7965265512466431}]}, {"text": " Table 5: Translation results (NIST-BLEU) using pc with differ- ent genre-specific language models for Es\u2194En systems", "labels": [], "entities": [{"text": "Translation", "start_pos": 10, "end_pos": 21, "type": "TASK", "confidence": 0.9621047973632812}]}]}