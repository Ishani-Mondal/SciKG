{"title": [{"text": "Using Syntax to Improve Word Alignment Precision for Syntax-Based Machine Translation", "labels": [], "entities": [{"text": "Improve Word Alignment Precision", "start_pos": 16, "end_pos": 48, "type": "TASK", "confidence": 0.8300058990716934}, {"text": "Syntax-Based Machine Translation", "start_pos": 53, "end_pos": 85, "type": "TASK", "confidence": 0.6801934142907461}]}], "abstractContent": [{"text": "Word alignments that violate syntactic correspondences interfere with the extraction of string-to-tree transducer rules for syntax-based machine translation.", "labels": [], "entities": [{"text": "Word alignments", "start_pos": 0, "end_pos": 15, "type": "TASK", "confidence": 0.6735040098428726}, {"text": "machine translation", "start_pos": 137, "end_pos": 156, "type": "TASK", "confidence": 0.7497479021549225}]}, {"text": "We present an algorithm for identifying and deleting incorrect word alignment links, using features of the extracted rules.", "labels": [], "entities": [{"text": "identifying and deleting incorrect word alignment links", "start_pos": 28, "end_pos": 83, "type": "TASK", "confidence": 0.6308681155954089}]}, {"text": "We obtain gains in both alignment quality and translation quality in Chinese-English and Arabic-English translation experiments relative to a GIZA++ union baseline.", "labels": [], "entities": [{"text": "GIZA++ union baseline", "start_pos": 142, "end_pos": 163, "type": "DATASET", "confidence": 0.774031937122345}]}], "introductionContent": [], "datasetContent": [{"text": "AER (Alignment Error Rate) is the most widely used metric of alignment quality, but requires gold-standard alignments labelled with \"sure/possible\" annotations to compute; lacking such annotations, we can compute alignment fmeasure instead.", "labels": [], "entities": [{"text": "AER (Alignment Error Rate)", "start_pos": 0, "end_pos": 26, "type": "METRIC", "confidence": 0.7902377297480901}]}, {"text": "However, show that, in phrase-based translation, improvements in AER or f-measure do not necessarily correlate with improvements in BLEU score.", "labels": [], "entities": [{"text": "phrase-based translation", "start_pos": 23, "end_pos": 47, "type": "TASK", "confidence": 0.6446281969547272}, {"text": "AER", "start_pos": 65, "end_pos": 68, "type": "METRIC", "confidence": 0.9988731741905212}, {"text": "BLEU score", "start_pos": 132, "end_pos": 142, "type": "METRIC", "confidence": 0.9839540123939514}]}, {"text": "They propose two modifications to f-measure: varying the precision/recall tradeoff, and fully-connecting the alignment links before computing f-measure.", "labels": [], "entities": [{"text": "precision", "start_pos": 57, "end_pos": 66, "type": "METRIC", "confidence": 0.9989446997642517}, {"text": "recall", "start_pos": 67, "end_pos": 73, "type": "METRIC", "confidence": 0.8805141448974609}]}, {"text": "11 Weighted Fully-Connected F-Measure Given a hypothesized set of alignment links H and a goldstandard set of alignment links G, we define H + = f ullyConnect(H) and G + = f ullyConnect(G), and then compute: For phrase-based Chinese-English and ArabicEnglish translation tasks, obtain the closest correlation between weighted fully-connected alignment f-measure and BLEU score using \u03b1=0.5 and \u03b1=0.1, respectively.", "labels": [], "entities": [{"text": "F-Measure", "start_pos": 28, "end_pos": 37, "type": "METRIC", "confidence": 0.7372008562088013}, {"text": "phrase-based Chinese-English and ArabicEnglish translation tasks", "start_pos": 212, "end_pos": 276, "type": "TASK", "confidence": 0.6069424947102865}, {"text": "BLEU score", "start_pos": 366, "end_pos": 376, "type": "METRIC", "confidence": 0.9827895164489746}]}, {"text": "We use weighted fully-connected alignment f-measure as the training criterion for link deletion, and to evaluate alignment quality on training and test sets.", "labels": [], "entities": [{"text": "link deletion", "start_pos": 82, "end_pos": 95, "type": "TASK", "confidence": 0.6791152954101562}]}, {"text": "Rule F-Measure To evaluate the impact of link deletion upon rule quality, we compare the rule precision, recall, and f-measure of the rule set extracted In, the fully-connected version of the alignments shown would include the links \ud97b\udf59 \ud97b\udf59 \ud97b\udf59 \u00a7 \u00a7 \u00a7-starts and \ud97b\udf59 \ud97b\udf59 \ud97b\udf59 \u00a7 \u00a7 \u00a7-out.", "labels": [], "entities": [{"text": "precision", "start_pos": 94, "end_pos": 103, "type": "METRIC", "confidence": 0.8875519633293152}, {"text": "recall", "start_pos": 105, "end_pos": 111, "type": "METRIC", "confidence": 0.9992499947547913}]}, {"text": "Starting with GIZA++ union (IBM Model 4) alignments, we use perceptron training to set the weights of each feature used in link deletion in order to optimize weighted fully-connected alignment f-measure (\u03b1=0.5 for Chinese-English and \u03b1=0.1 for ArabicEnglish) on a manually aligned discriminative training set.", "labels": [], "entities": []}, {"text": "We report the (fully-connected) precision, recall, and weighted alignment f-measure on a heldout test set after running perceptron training, relative to the baseline GIZA++ union alignments.", "labels": [], "entities": [{"text": "precision", "start_pos": 32, "end_pos": 41, "type": "METRIC", "confidence": 0.9991389513015747}, {"text": "recall", "start_pos": 43, "end_pos": 49, "type": "METRIC", "confidence": 0.998745322227478}, {"text": "weighted alignment f-measure", "start_pos": 55, "end_pos": 83, "type": "METRIC", "confidence": 0.7792349259058634}]}, {"text": "Using the learned feature weights, we then perform link deletion over the GIZA++ union alignments for the entire training corpus for each translation task.", "labels": [], "entities": [{"text": "GIZA++ union alignments", "start_pos": 74, "end_pos": 97, "type": "DATASET", "confidence": 0.886021226644516}]}, {"text": "Using these alignments, which we refer to as \"GIZA++ union + link deletion\", we train a syntax-based translation system similar to that described in ().", "labels": [], "entities": []}, {"text": "After extracting string-to-tree translation rules from the aligned, parsed training corpus, the system assigns weights to each rule via frequency estimation with smoothing.", "labels": [], "entities": []}, {"text": "The rule probabilities, as well as trigram language model probabilities and a handful of additional features of each rule, are used as features during decoding.", "labels": [], "entities": []}, {"text": "The feature weights are tuned using minimum error rate training) to optimize BLEU score on a held-out development set.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 77, "end_pos": 87, "type": "METRIC", "confidence": 0.977154403924942}]}, {"text": "We then compare the BLEU score of this system against a baseline system trained using GIZA++ union alignments.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 20, "end_pos": 30, "type": "METRIC", "confidence": 0.9760288000106812}]}, {"text": "To determine which value of \u03b1 is most effective as a training criterion for link deletion, we set \u03b1=0.4 (favoring recall), 0.5, and 0.6 (favoring precision),  and compare the effect on translation quality for Chinese-English data set A.", "labels": [], "entities": [{"text": "link deletion", "start_pos": 76, "end_pos": 89, "type": "TASK", "confidence": 0.6775363236665726}, {"text": "recall", "start_pos": 114, "end_pos": 120, "type": "METRIC", "confidence": 0.9976510405540466}, {"text": "precision", "start_pos": 146, "end_pos": 155, "type": "METRIC", "confidence": 0.9983748197555542}]}], "tableCaptions": [{"text": " Table 2: Size (English words) and source of data sets used in translation tasks", "labels": [], "entities": [{"text": "Size", "start_pos": 10, "end_pos": 14, "type": "TASK", "confidence": 0.9636879563331604}]}, {"text": " Table 3: Results of link deletion. Weighted fully-connected alignment f-measure is computed on alignment test sets  (Table 1); BLEU score is computed on translation test sets (Table 2).", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 128, "end_pos": 138, "type": "METRIC", "confidence": 0.9820913672447205}]}, {"text": " Table 4: Rule precision, recall, and f-measure of rules extracted from 400 sentence pairs of Chinese-English data", "labels": [], "entities": [{"text": "precision", "start_pos": 15, "end_pos": 24, "type": "METRIC", "confidence": 0.8879895210266113}, {"text": "recall", "start_pos": 26, "end_pos": 32, "type": "METRIC", "confidence": 0.9996470212936401}, {"text": "f-measure", "start_pos": 38, "end_pos": 47, "type": "METRIC", "confidence": 0.994342565536499}]}]}