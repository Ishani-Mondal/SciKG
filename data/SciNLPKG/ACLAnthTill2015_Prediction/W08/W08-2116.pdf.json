{"title": [{"text": "Easy as ABC? Facilitating Pictorial Communication via Semantically Enhanced Layout", "labels": [], "entities": [{"text": "Facilitating Pictorial Communication", "start_pos": 13, "end_pos": 49, "type": "TASK", "confidence": 0.7848990360895792}]}], "abstractContent": [{"text": "Pictorial communication systems convert natural language text into pictures to assist people with limited literacy.", "labels": [], "entities": [{"text": "Pictorial communication", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.8667113184928894}]}, {"text": "We define a novel and challenging problem: picture layout optimization.", "labels": [], "entities": [{"text": "picture layout optimization", "start_pos": 43, "end_pos": 70, "type": "TASK", "confidence": 0.8839940230051676}]}, {"text": "Given an input sentence , we seek the optimal way to layout word icons such that the resulting picture best conveys the meaning of the input sentence.", "labels": [], "entities": []}, {"text": "To this end, we propose a family of intuitive \"ABC\" layouts, which organize icons in three groups.", "labels": [], "entities": []}, {"text": "We formalize layout optimization as a sequence labeling problem , employing conditional random fields as our machine learning method.", "labels": [], "entities": [{"text": "layout optimization", "start_pos": 13, "end_pos": 32, "type": "TASK", "confidence": 0.8798057436943054}, {"text": "sequence labeling", "start_pos": 38, "end_pos": 55, "type": "TASK", "confidence": 0.6671819388866425}]}, {"text": "Enabled by novel applications of semantic role labeling and syntactic parsing, our trained model makes layout predictions that agree well with human annotators.", "labels": [], "entities": [{"text": "semantic role labeling", "start_pos": 33, "end_pos": 55, "type": "TASK", "confidence": 0.6812652250130972}, {"text": "syntactic parsing", "start_pos": 60, "end_pos": 77, "type": "TASK", "confidence": 0.7424193322658539}]}, {"text": "In addition, we conduct a user study to compare our ABC layout versus the standard linear layout.", "labels": [], "entities": []}, {"text": "The study shows that our semantically enhanced layout is preferred by non-native speakers, suggesting it has the potential to be useful for people with other forms of limited literacy, too.", "labels": [], "entities": []}], "introductionContent": [{"text": "A picture is worth a thousand words-especially when you are someone with communicative disorders, a foreign language speaker, or a young child.", "labels": [], "entities": []}, {"text": "Pictorial communication systems aim to automatically convert general natural language text into meaningful pictures.", "labels": [], "entities": [{"text": "Pictorial communication", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.9183456003665924}]}, {"text": "A perfect pictorial communication system can turn signs and operation instructions into easy-to-understand graphical forms; combined with optical character recognition input, a personal assistant device could create such visual translations on-the-fly without the help of a caretaker.", "labels": [], "entities": []}, {"text": "Pictorial communication may also facilitate literacy development and rapid browsing of documents through pictorial summaries.", "labels": [], "entities": [{"text": "Pictorial communication", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.8652991354465485}, {"text": "literacy development", "start_pos": 44, "end_pos": 64, "type": "TASK", "confidence": 0.6953866481781006}]}, {"text": "Pictorial communication research is in its infancy with a spectrum of experimental systems, which we review in Section 2.", "labels": [], "entities": [{"text": "Pictorial communication research", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.9762252767880758}]}, {"text": "At one end of the spectrum, some systems render highly realistic 3D scenes but require specific scene-descriptive language.", "labels": [], "entities": []}, {"text": "At the other end, some systems perform dictionary-based iconic transliteration (turning words into icons 1 one by one) on arbitrary text but the pictures can be hard to understand.", "labels": [], "entities": [{"text": "dictionary-based iconic transliteration", "start_pos": 39, "end_pos": 78, "type": "TASK", "confidence": 0.6068238715330759}]}, {"text": "We are interested in using pictorial communication as an assistive communication tool.", "labels": [], "entities": []}, {"text": "Thus, our system needs to be able to handle general text yet produce easy-to-understand pictures, which is in the middle of the spectrum.", "labels": [], "entities": []}, {"text": "To this end, our system adopts a \"collage\" approach (.", "labels": [], "entities": []}, {"text": "Given apiece of text (e.g., a sentence), it first identifies important and easy-to-depict words (or phrases) with natural language processing (NLP) techniques.", "labels": [], "entities": []}, {"text": "It then finds one good icon per word, either from a manually created picture-dictionary, or via image analysis on image search results.", "labels": [], "entities": []}, {"text": "Finally, it lays out the icons to create the picture.", "labels": [], "entities": []}, {"text": "Each step involves several interesting research problems.", "labels": [], "entities": []}, {"text": "This paper focuses exclusively on the picture layout component and addresses the following question: Can we use machine learning and NLP techniques to learn a good picture layout that im-proves picture comprehension for our target audiences of limited literacy?", "labels": [], "entities": [{"text": "picture layout", "start_pos": 38, "end_pos": 52, "type": "TASK", "confidence": 0.7508179843425751}]}, {"text": "We first propose a simple yet novel picture layout scheme called \"ABC.\"", "labels": [], "entities": []}, {"text": "Next, we design a Conditional Random Fieldbased semantic tagger for predicting the ABC layout.", "labels": [], "entities": []}, {"text": "Finally, we conduct a user study contrasting our ABC layout to the linear layout used in iconic transliteration.", "labels": [], "entities": []}, {"text": "The main contribution of this paper is to introduce the novel task of layout prediction, learned using linguistic features including PropBank role labels, part-of-speech tags, and lexical features.", "labels": [], "entities": [{"text": "layout prediction", "start_pos": 70, "end_pos": 87, "type": "TASK", "confidence": 0.9724559485912323}]}], "datasetContent": [{"text": "We trained our CRF models using the MAL-LET toolkit.", "labels": [], "entities": [{"text": "MAL-LET toolkit", "start_pos": 36, "end_pos": 51, "type": "DATASET", "confidence": 0.8420304656028748}]}, {"text": "Our complete dataset consists of the 571 manually annotated sen- tences (tags mapped to chunk-level).", "labels": [], "entities": []}, {"text": "The only tuning parameter is the Gaussian prior variance, \u03c3 2 . We performed 5-fold cross validation, varying \u03c3 2 and comparing performance across models.", "labels": [], "entities": [{"text": "Gaussian prior variance", "start_pos": 33, "end_pos": 56, "type": "METRIC", "confidence": 0.8456365863482157}]}, {"text": "demonstrates that peak per-chunk accuracy (77.6%) and macro-averaged F1 scores are achieved using the most general sequence labeling model.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 33, "end_pos": 41, "type": "METRIC", "confidence": 0.9264112710952759}, {"text": "F1 scores", "start_pos": 69, "end_pos": 78, "type": "METRIC", "confidence": 0.9712723791599274}]}, {"text": "As a result, the user study in the next section is based on layouts predicted by Model 3 with \u03c3 2 = 1.0, trained on all the data.", "labels": [], "entities": []}, {"text": "To understand which features contribute most to performance, we experimented with removing each of the four types (individually).", "labels": [], "entities": []}, {"text": "Peak accuracy drops the most when lexical features are removed (76.4%), followed by PropBank features (76.5%), phrase features (76.9%), and POS features (77.1%).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 5, "end_pos": 13, "type": "METRIC", "confidence": 0.9988033771514893}]}, {"text": "The features in the final learned model make intuitive sense.", "labels": [], "entities": []}, {"text": "It prefers tag transitions A\u2192B and B\u2192C, but not A\u2192C or C\u2192A.", "labels": [], "entities": []}, {"text": "The model likes the word \"I\" and noun phrases (not nested in a verb phrase) to have tag A.", "labels": [], "entities": [{"text": "A", "start_pos": 88, "end_pos": 89, "type": "METRIC", "confidence": 0.6075345873832703}]}, {"text": "Verbs and ArgM-NEGs are frequently tagged B, while noun.object's, Arg4s, and ArgM-CAUs are typically C.", "labels": [], "entities": []}, {"text": "The model discourages Arg0s and conjunctions in B, and dislikes adverbial phrases and noun.time's in C.", "labels": [], "entities": [{"text": "Arg0s", "start_pos": 22, "end_pos": 27, "type": "METRIC", "confidence": 0.9961850047111511}]}, {"text": "While 77.6% cross validation accuracy may seem low, it is in fact close to the 81% inter annotator agreement 3 , and thus close to optimal.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 29, "end_pos": 37, "type": "METRIC", "confidence": 0.9628618955612183}]}, {"text": "The confusion matrix (not shown) reveals that most er-rors probably arise from disagreements in the individual annotators.", "labels": [], "entities": []}, {"text": "The most common errors are predicting B for chunks labeled O and confusing tags B and C.", "labels": [], "entities": [{"text": "B", "start_pos": 38, "end_pos": 39, "type": "METRIC", "confidence": 0.4846201539039612}]}, {"text": "Manually inspecting the pictures in our training set shows that annotators often omitted the verb (such as \"is\" or \"has\") and left the B position empty, since it could be inferred by the presence of the arrow and the images in A and C.", "labels": [], "entities": []}, {"text": "Also, annotators tended to disagree on the location of adverbial expressions, dividing them between positions B and C.", "labels": [], "entities": []}, {"text": "Finally, only 3.3% of chunks were incorrectly omitted from the pictures.", "labels": [], "entities": []}, {"text": "Therefore, we conclude that our CRF models are capable of predicting the ABC layouts.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: User study results.", "labels": [], "entities": []}]}