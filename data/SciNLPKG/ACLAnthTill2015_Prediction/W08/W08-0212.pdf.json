{"title": [], "abstractContent": [{"text": "Just as programming is the traditional introduction to computer science, writing grammars by hand is an excellent introduction to many topics in computational linguistics.", "labels": [], "entities": []}, {"text": "We present and justify a well-tested introductory activity in which teams of mixed background compete to write probabilistic context-free grammars of English.", "labels": [], "entities": []}, {"text": "The exercise brings together symbolic, probabilistic, al-gorithmic, and experimental issues in away that is accessible to novices and enjoyable.", "labels": [], "entities": []}], "introductionContent": [{"text": "We describe a hands-on group activity for novices that introduces several central topics in computational linguistics (CL).", "labels": [], "entities": [{"text": "computational linguistics (CL)", "start_pos": 92, "end_pos": 122, "type": "TASK", "confidence": 0.811959183216095}]}, {"text": "While the task is intellectually challenging, it requires no background other than linguistic intuitions, no programming, and only a very basic understanding of probability.", "labels": [], "entities": []}, {"text": "The activity is especially appropriate for mixed groups of linguists, computer scientists, and others, letting them collaborate effectively on small teams and learn from one another.", "labels": [], "entities": []}, {"text": "A friendly competition among the teams makes the activity intense and enjoyable and introduces quantitative evaluation.", "labels": [], "entities": []}], "datasetContent": [{"text": "Grammar development ends at an announced deadline.", "labels": [], "entities": [{"text": "Grammar development", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.9199491441249847}]}, {"text": "The grammars are now evaluated on the two measures discussed in \u00a73.", "labels": [], "entities": []}, {"text": "The instructors run a few scripts that handle most of this work.", "labels": [], "entities": []}, {"text": "First, we generate a collection C by sampling 20 sentences from each team's probabilistic grammar, using S1 as the start symbol.", "labels": [], "entities": []}, {"text": "(Thus, the backoff S2 grammar is not used for generation.)", "labels": [], "entities": []}, {"text": "We now determine, for each team, what fraction of its 20-sentence sample was grammatical.", "labels": [], "entities": []}, {"text": "The participants play the role of grammaticality judges.", "labels": [], "entities": []}, {"text": "In our randomized double-blind procedure, each individual judge receives (in his or her team directory) a file of about 20 sentences from C, with instructions to delete the ungrammatical ones and save the file, implying coarse Boolean grammaticality judgments.", "labels": [], "entities": []}, {"text": "The files are constructed so that each sentence in C is judged by 3 different participants; a sentence is considered grammatical if \u2265 2 judges thinks that it is.", "labels": [], "entities": []}, {"text": "We define the test corpus\u02c6Ccorpus\u02c6 corpus\u02c6C to consist of all sentences in C that were judged grammatical.", "labels": [], "entities": []}, {"text": "Each team's full grammar (using START as the start symbol to allow backoff) is used to pars\u00ea C.", "labels": [], "entities": [{"text": "START", "start_pos": 32, "end_pos": 37, "type": "METRIC", "confidence": 0.7129805088043213}]}, {"text": "This gives us the log 2 -probability of each sentence in\u02c6Cin\u02c6 in\u02c6C; the cross-entropy score is the sum of these log 2 -probabilities divided by the length of\u02c6Cof\u02c6 of\u02c6C.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: The S1 rules: a starting point for building an En- glish grammar. The start symbol is S1. The weights in  the first column will be normalized into generative proba- bilities; for example, the probability of expanding a given  NP with NP \u2192 Det Nbar is actually 20/(20 + 1).", "labels": [], "entities": []}, {"text": " Table 4: Teams' evaluation scores in one year, and the  number of new rules (not including weight changes) that  they wrote. Only teams A and H modified the relative  weights of the START rules (they used 80/20 and 75/25,  respectively), giving them competitive perplexity scores.  (Cross-entropy in this year was approximated by an upper  bound that uses only the probability of each sentence's  single best parse.)", "labels": [], "entities": []}]}