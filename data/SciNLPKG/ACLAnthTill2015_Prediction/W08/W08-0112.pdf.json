{"title": [{"text": "What Are Meeting Summaries? An Analysis of Human Extractive Summaries in Meeting Corpus", "labels": [], "entities": [{"text": "Analysis of Human Extractive Summaries", "start_pos": 31, "end_pos": 69, "type": "TASK", "confidence": 0.7133411049842835}]}], "abstractContent": [{"text": "Significant research efforts have been devoted to speech summarization, including automatic approaches and evaluation metrics.", "labels": [], "entities": [{"text": "speech summarization", "start_pos": 50, "end_pos": 70, "type": "TASK", "confidence": 0.7274861335754395}]}, {"text": "However, a fundamental problem about what summaries are for the speech data and whether humans agree with each other remains unclear.", "labels": [], "entities": []}, {"text": "This paper performs an analysis of human annotated extractive summaries using the ICSI meeting corpus with an aim to examine their consistency and the factors impacting human agreement.", "labels": [], "entities": [{"text": "ICSI meeting corpus", "start_pos": 82, "end_pos": 101, "type": "DATASET", "confidence": 0.9698830842971802}, {"text": "consistency", "start_pos": 131, "end_pos": 142, "type": "METRIC", "confidence": 0.9549418091773987}]}, {"text": "In addition to using Kappa statistics and ROUGE scores, we also proposed a sentence distance score and divergence distance as a quantitative measure.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 42, "end_pos": 47, "type": "METRIC", "confidence": 0.9798433184623718}]}, {"text": "This study is expected to help better define the speech summarization problem.", "labels": [], "entities": [{"text": "speech summarization", "start_pos": 49, "end_pos": 69, "type": "TASK", "confidence": 0.6991708874702454}]}], "introductionContent": [{"text": "With the fast development of recording and storage techniques in recent years, speech summarization has received more attention.", "labels": [], "entities": [{"text": "speech summarization", "start_pos": 79, "end_pos": 99, "type": "TASK", "confidence": 0.7859900593757629}]}, {"text": "A variety of approaches have been investigated for speech summarization, for example, maximum entropy, conditional random fields, latent semantic analysis, support vector machines, maximum marginal relevance.", "labels": [], "entities": [{"text": "speech summarization", "start_pos": 51, "end_pos": 71, "type": "TASK", "confidence": 0.5969182103872299}, {"text": "latent semantic analysis", "start_pos": 130, "end_pos": 154, "type": "TASK", "confidence": 0.6860779126485189}]}, {"text": "These studies used different domains, such as broadcast news, lectures, and meetings.", "labels": [], "entities": []}, {"text": "In these approaches, different information sources have been examined from both text and speech related features (e.g., prosody, speaker activity, turn-taking, discourse).", "labels": [], "entities": []}, {"text": "How to evaluate speech summaries has also been studied recently, but so far there is no consensus on evaluation yet.", "labels": [], "entities": [{"text": "evaluate speech summaries", "start_pos": 7, "end_pos": 32, "type": "TASK", "confidence": 0.6378365457057953}]}, {"text": "Often the goal in evaluation is to develop an automatic metric to have a high correlation with human evaluation scores.", "labels": [], "entities": []}, {"text": "Different methods have been used in the above summarization research to compare system generated summaries with human annotation, such as Fmeasure, ROUGE, Pyramid, sumACCY (.", "labels": [], "entities": [{"text": "summarization", "start_pos": 46, "end_pos": 59, "type": "TASK", "confidence": 0.9711767435073853}, {"text": "Fmeasure", "start_pos": 138, "end_pos": 146, "type": "METRIC", "confidence": 0.5682112574577332}, {"text": "ROUGE", "start_pos": 148, "end_pos": 153, "type": "METRIC", "confidence": 0.9911757707595825}]}, {"text": "Typically multiple reference human summaries are used in evaluation in order to account for the inconsistency among human annotations.", "labels": [], "entities": []}, {"text": "While there have been efforts on speech summarization approaches and evaluation, some fundamental problems are still unclear.", "labels": [], "entities": [{"text": "speech summarization", "start_pos": 33, "end_pos": 53, "type": "TASK", "confidence": 0.7320729494094849}]}, {"text": "For example, what are speech summaries?", "labels": [], "entities": [{"text": "speech summaries", "start_pos": 22, "end_pos": 38, "type": "TASK", "confidence": 0.6182313412427902}]}, {"text": "Do humans agree with each other on summary extraction?", "labels": [], "entities": [{"text": "summary extraction", "start_pos": 35, "end_pos": 53, "type": "TASK", "confidence": 0.9487291574478149}]}, {"text": "In this paper, we focus on the meeting domain, one of the most challenging speech genre, to analyze human summary annotation.", "labels": [], "entities": [{"text": "human summary annotation", "start_pos": 100, "end_pos": 124, "type": "TASK", "confidence": 0.6116350293159485}]}, {"text": "Meetings often have several participants.", "labels": [], "entities": []}, {"text": "Its speech is spontaneous, contains disfluencies, and lacks structure.", "labels": [], "entities": []}, {"text": "These all post new challenges to the consensus of human extracted summaries.", "labels": [], "entities": []}, {"text": "Our goal in this study is to investigate the variation of human extractive summaries, and help to better understand the gold standard reference summaries for meeting summarization.", "labels": [], "entities": [{"text": "variation of human extractive summaries", "start_pos": 45, "end_pos": 84, "type": "TASK", "confidence": 0.5685975193977356}, {"text": "summarization", "start_pos": 166, "end_pos": 179, "type": "TASK", "confidence": 0.9295316934585571}]}, {"text": "This paper aims to answer two key questions: (1) How much variation is therein human extractive meeting summaries?", "labels": [], "entities": [{"text": "extractive meeting summaries", "start_pos": 85, "end_pos": 113, "type": "TASK", "confidence": 0.8171424468358358}]}, {"text": "(2) What are the factors that may impact interannotator agreement?", "labels": [], "entities": [{"text": "interannotator agreement", "start_pos": 41, "end_pos": 65, "type": "TASK", "confidence": 0.8447701036930084}]}, {"text": "We use three different metrics to evaluate the variation among human summaries, including Kappa statistic, ROUGE score, and anew proposed divergence distance score to reflect the coherence and quality of an annotation.", "labels": [], "entities": [{"text": "Kappa statistic", "start_pos": 90, "end_pos": 105, "type": "METRIC", "confidence": 0.9081064462661743}, {"text": "ROUGE score", "start_pos": 107, "end_pos": 118, "type": "METRIC", "confidence": 0.9839009344577789}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Average Kappa scores on different data sets.", "labels": [], "entities": [{"text": "Average Kappa scores", "start_pos": 10, "end_pos": 30, "type": "METRIC", "confidence": 0.8939924836158752}]}, {"text": " Table 2: Average Kappa score with respect to the number of  speakers after removing short topics.", "labels": [], "entities": [{"text": "Average Kappa score", "start_pos": 10, "end_pos": 29, "type": "METRIC", "confidence": 0.9082424839337667}]}, {"text": " Table 3: ROUGE F-measure scores for different data sets.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 10, "end_pos": 15, "type": "METRIC", "confidence": 0.9850489497184753}, {"text": "F-measure", "start_pos": 16, "end_pos": 25, "type": "METRIC", "confidence": 0.7575415968894958}]}]}