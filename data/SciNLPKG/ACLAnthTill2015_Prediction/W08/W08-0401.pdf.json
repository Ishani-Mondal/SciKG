{"title": [{"text": "Imposing Constraints from the Source Tree on ITG Constraints for SMT", "labels": [], "entities": [{"text": "Imposing Constraints", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.90590500831604}, {"text": "SMT", "start_pos": 65, "end_pos": 68, "type": "TASK", "confidence": 0.9651200175285339}]}], "abstractContent": [{"text": "In current statistical machine translation (SMT), erroneous word reordering is one of the most serious problems.", "labels": [], "entities": [{"text": "statistical machine translation (SMT)", "start_pos": 11, "end_pos": 48, "type": "TASK", "confidence": 0.7950402150551478}, {"text": "word reordering", "start_pos": 60, "end_pos": 75, "type": "TASK", "confidence": 0.6978137046098709}]}, {"text": "To resolve this problem, many word-reordering constraint techniques have been proposed.", "labels": [], "entities": []}, {"text": "The inversion transduction grammar (ITG) is one of these constraints.", "labels": [], "entities": [{"text": "inversion transduction grammar (ITG)", "start_pos": 4, "end_pos": 40, "type": "TASK", "confidence": 0.798701544602712}]}, {"text": "In ITG constraints, target-side word order is obtained by rotating nodes of the source-side binary tree.", "labels": [], "entities": []}, {"text": "In these node rotations, the source binary tree instance is not considered.", "labels": [], "entities": []}, {"text": "Therefore, stronger constraints for word reordering can be obtained by imposing further constraints derived from the source tree on the ITG constraints.", "labels": [], "entities": [{"text": "word reordering", "start_pos": 36, "end_pos": 51, "type": "TASK", "confidence": 0.7319456338882446}]}, {"text": "For example, for the source word sequence { ab c d }, ITG constraints allow a total of twenty-two target word orderings.", "labels": [], "entities": []}, {"text": "However , when the source binary tree instance ((a b) (c d)) is given, our proposed \"imposing source tree on ITG\" (IST-ITG) constraints allow only eight word orderings.", "labels": [], "entities": []}, {"text": "The reduction in the number of word-order permutations by our proposed stronger constraints efficiently suppresses erroneous word order-ings.", "labels": [], "entities": []}, {"text": "In our experiments with IST-ITG using the NIST MT08 English-to-Chinese translation track's data, the proposed method resulted in a 1.8-points improvement in character BLEU-4 (35.2 to 37.0) and a 6.2% lower CER (74.1 to 67.9%) compared with our baseline condition.", "labels": [], "entities": [{"text": "IST-ITG", "start_pos": 24, "end_pos": 31, "type": "DATASET", "confidence": 0.8774268627166748}, {"text": "NIST MT08 English-to-Chinese translation track's data", "start_pos": 42, "end_pos": 95, "type": "DATASET", "confidence": 0.896385840007237}, {"text": "BLEU-4", "start_pos": 167, "end_pos": 173, "type": "METRIC", "confidence": 0.9846594333648682}, {"text": "CER", "start_pos": 206, "end_pos": 209, "type": "METRIC", "confidence": 0.9990124702453613}]}], "introductionContent": [{"text": "Statistical methods are widely used for machine translation.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 40, "end_pos": 59, "type": "TASK", "confidence": 0.8195551633834839}]}, {"text": "One of the popular statistical machine translation paradigms is the phrase-based model (PBSMT) ().", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 19, "end_pos": 50, "type": "TASK", "confidence": 0.6216581563154856}]}, {"text": "In PBSMT, errors in word reordering, especially in global reordering, are one of the most serious problems.", "labels": [], "entities": [{"text": "word reordering", "start_pos": 20, "end_pos": 35, "type": "TASK", "confidence": 0.7325688004493713}]}, {"text": "Approaches used to resolve this problem are categorized into two types.", "labels": [], "entities": []}, {"text": "The first type is linguistically syntax-based.", "labels": [], "entities": []}, {"text": "In this approach, source), target (), or both side) tree structures are used for model training.", "labels": [], "entities": []}, {"text": "The second type is formal constraints on word permutations.", "labels": [], "entities": []}, {"text": "IBM constraints (, lexical word reordering model, and inversion transduction grammar (ITG) constraints belong to this type of approach.", "labels": [], "entities": []}, {"text": "Our approach is an extension of ITG constraints and is a hybrid of the first and second type of approach.", "labels": [], "entities": []}, {"text": "We propose \"imposing source tree on ITG\" (IST-ITG) constraints for directly introducing source sentence structure into our set of constraints.", "labels": [], "entities": []}, {"text": "In IST-ITG, ITG constraints under the given source sentence tree structure are used as stronger constraints than the original ITG.", "labels": [], "entities": []}, {"text": "For example, IST-ITG allows only eight word orderings fora four-word sentence, even though twenty-two word orderings are possible with respect of in the original ITG constraints.", "labels": [], "entities": []}, {"text": "In Section 2, we present the proposed IST-ITG for word-based translation.", "labels": [], "entities": [{"text": "word-based translation", "start_pos": 50, "end_pos": 72, "type": "TASK", "confidence": 0.7185821831226349}]}, {"text": "In Section 3, the proposed method is extended to phrase-based translation.", "labels": [], "entities": [{"text": "phrase-based translation", "start_pos": 49, "end_pos": 73, "type": "TASK", "confidence": 0.8283257186412811}]}, {"text": "In Section 4, we present a real-time decoding algorithm for IST-ITG constraints.", "labels": [], "entities": [{"text": "IST-ITG constraints", "start_pos": 60, "end_pos": 79, "type": "TASK", "confidence": 0.5919393599033356}]}, {"text": "In Section 5, we give details of the experiments and present the results.", "labels": [], "entities": []}, {"text": "Finally, in Section 6, we offer a summary and some concluding remarks.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluated the proposed method using four evaluation measures, BLEU (), NIST (Doddington 2002), WER(word error rate), and PER(position independent word error rate).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 65, "end_pos": 69, "type": "METRIC", "confidence": 0.9988948702812195}, {"text": "NIST", "start_pos": 74, "end_pos": 78, "type": "METRIC", "confidence": 0.592393159866333}, {"text": "WER(word error rate)", "start_pos": 98, "end_pos": 118, "type": "METRIC", "confidence": 0.8744341929753622}, {"text": "PER(position independent word error rate)", "start_pos": 124, "end_pos": 165, "type": "METRIC", "confidence": 0.7742205001413822}]}, {"text": "Before discussing the evaluation, the characteristics of each one are analyzed.", "labels": [], "entities": []}, {"text": "\u2022 BLEU: This evaluation measure takes into account middle range word order, but does not take into account global word order.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 2, "end_pos": 6, "type": "METRIC", "confidence": 0.9992563128471375}]}, {"text": "When the translation result is [w 1 , w 2 , ..., w j\u22121 , X, w j+1 , ..., w n ] for reference translation [w 1 , w 2 , ..., w n ], both WER and BLEU scores will be high.", "labels": [], "entities": [{"text": "WER", "start_pos": 135, "end_pos": 138, "type": "METRIC", "confidence": 0.9977699518203735}, {"text": "BLEU", "start_pos": 143, "end_pos": 147, "type": "METRIC", "confidence": 0.9955161213874817}]}, {"text": "For a translation result [w j+1 , ..., w n , X, w 1 , w 2 , ..., w j\u22121 ], the BLEU score will be the same as the previous result since BLEU only takes into account 4grams.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 78, "end_pos": 88, "type": "METRIC", "confidence": 0.9803212285041809}, {"text": "BLEU", "start_pos": 135, "end_pos": 139, "type": "METRIC", "confidence": 0.995775043964386}]}, {"text": "However, the WER score will be zero since global word positions are taken into account.", "labels": [], "entities": [{"text": "WER score", "start_pos": 13, "end_pos": 22, "type": "METRIC", "confidence": 0.9793659448623657}]}, {"text": "Therefore, the effectiveness of the proposed method using BLEU is less than that of using WER.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 58, "end_pos": 62, "type": "METRIC", "confidence": 0.9970542192459106}, {"text": "WER", "start_pos": 90, "end_pos": 93, "type": "METRIC", "confidence": 0.5589773058891296}]}, {"text": "\u2022 NIST: This evaluation measure only takes into account n-grams like BLEU.", "labels": [], "entities": [{"text": "NIST", "start_pos": 2, "end_pos": 6, "type": "METRIC", "confidence": 0.7006601691246033}, {"text": "BLEU", "start_pos": 69, "end_pos": 73, "type": "METRIC", "confidence": 0.9958909749984741}]}, {"text": "However, importance of higher order n-grams are less than BLEU.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 58, "end_pos": 62, "type": "METRIC", "confidence": 0.9969233870506287}]}, {"text": "Therefore, the effectiveness of the proposed method using NIST will be less than that of using BLEU.", "labels": [], "entities": [{"text": "NIST", "start_pos": 58, "end_pos": 62, "type": "DATASET", "confidence": 0.8701430559158325}, {"text": "BLEU", "start_pos": 95, "end_pos": 99, "type": "METRIC", "confidence": 0.9891822934150696}]}, {"text": "\u2022 WER: This evaluation measure takes into account not only local but also global word order, and is the most suitable for evaluating our method.", "labels": [], "entities": [{"text": "WER", "start_pos": 2, "end_pos": 5, "type": "METRIC", "confidence": 0.9973553419113159}]}, {"text": "\u2022 PER: With this evaluation measure, we are almost incapable of considering word order.", "labels": [], "entities": [{"text": "PER", "start_pos": 2, "end_pos": 5, "type": "METRIC", "confidence": 0.9984322190284729}]}, {"text": "Therefore, our proposed method would seem to offer no improvement in this evaluation measure.", "labels": [], "entities": []}, {"text": "First, we conducted experiments on English and Japanese patent translations.", "labels": [], "entities": [{"text": "English and Japanese patent translations", "start_pos": 35, "end_pos": 75, "type": "TASK", "confidence": 0.5779938459396362}]}, {"text": "Details of the experimental corpus are shown in.", "labels": [], "entities": []}, {"text": "This corpus is created by automatic sentence alignment (Uchiyama 2003).", "labels": [], "entities": [{"text": "sentence alignment", "start_pos": 36, "end_pos": 54, "type": "TASK", "confidence": 0.7213321477174759}]}, {"text": "The first nine hundred sentence pairs with the best alignment scores were used as the evaluation data (single reference) and the next thousand sentence pairs were used as the development data.", "labels": [], "entities": []}, {"text": "This corpus is a subset of the training corpus that will be used in the NTCIR-7 Workshop patent translation track.", "labels": [], "entities": [{"text": "NTCIR-7 Workshop patent translation", "start_pos": 72, "end_pos": 107, "type": "TASK", "confidence": 0.7877189666032791}]}, {"text": "Next, we conducted English-to-Chinese (E-C) newspaper translation experiments for different language pairs.", "labels": [], "entities": [{"text": "English-to-Chinese (E-C) newspaper translation", "start_pos": 19, "end_pos": 65, "type": "TASK", "confidence": 0.6148669918378195}]}, {"text": "The training and evaluation corpora were used in the NIST MT08 evaluation campaign English-toChinese translation track.", "labels": [], "entities": [{"text": "NIST MT08 evaluation campaign English-toChinese translation track", "start_pos": 53, "end_pos": 118, "type": "DATASET", "confidence": 0.8395333545548576}]}, {"text": "For the translation model training, we used 6.2M bilingual sentences.", "labels": [], "entities": [{"text": "translation", "start_pos": 8, "end_pos": 19, "type": "TASK", "confidence": 0.9803422093391418}]}, {"text": "For the language model training, we used 20.1M sentences.", "labels": [], "entities": []}, {"text": "A development set with 1,664 sentences was used as evaluation data in the Chinese-to-English translation track in the NIST MT07 evaluation campaign.", "labels": [], "entities": [{"text": "NIST MT07 evaluation campaign", "start_pos": 118, "end_pos": 147, "type": "DATASET", "confidence": 0.8447626978158951}]}, {"text": "A single reference was used in the development set.", "labels": [], "entities": []}, {"text": "The evaluation set with 1,859 sentences is the same as MT08's evaluation data, with 4 references.", "labels": [], "entities": [{"text": "MT08's evaluation data", "start_pos": 55, "end_pos": 77, "type": "DATASET", "confidence": 0.9010758548974991}]}, {"text": "Model training and decoding conditions were the same as those in the E-J experiments.", "labels": [], "entities": []}, {"text": "In both baseline and proposed condition, IBM constraints and lexical reordering model were used at the same time.", "labels": [], "entities": []}, {"text": "Therefore, the baseline conditions correspond to the IBM+LR condition in the J-E experiments, the proposed conditions correspond to the IBM+LR+IST in the J-E experiments.", "labels": [], "entities": [{"text": "IBM+LR condition", "start_pos": 53, "end_pos": 69, "type": "METRIC", "confidence": 0.6037227511405945}, {"text": "IBM+LR+IST", "start_pos": 136, "end_pos": 146, "type": "DATASET", "confidence": 0.7978734731674194}]}, {"text": "The evaluation unit was both the Chinese character and word as defined by the PKU corpus.", "labels": [], "entities": [{"text": "PKU corpus", "start_pos": 78, "end_pos": 88, "type": "DATASET", "confidence": 0.9730428755283356}]}, {"text": "As in the E-J experiments, the improvements in WER and CER (character error rate) were large.", "labels": [], "entities": [{"text": "WER", "start_pos": 47, "end_pos": 50, "type": "METRIC", "confidence": 0.9887009859085083}, {"text": "CER", "start_pos": 55, "end_pos": 58, "type": "METRIC", "confidence": 0.9928151369094849}, {"text": "character error rate)", "start_pos": 60, "end_pos": 81, "type": "METRIC", "confidence": 0.7410567477345467}]}, {"text": "The improvements in WER, CER, word BLEU, and character BLEU were 5.3% (from 75.0% to 69.7%), 6.2% (from 74.1% to 67.9%), 2.2-points (from 21.0 to 23.2), and 1.8-points (from 35.2 to 37.0) respectively.", "labels": [], "entities": [{"text": "WER", "start_pos": 20, "end_pos": 23, "type": "METRIC", "confidence": 0.8673765063285828}, {"text": "CER", "start_pos": 25, "end_pos": 28, "type": "METRIC", "confidence": 0.7966699600219727}, {"text": "BLEU", "start_pos": 35, "end_pos": 39, "type": "METRIC", "confidence": 0.8334275484085083}, {"text": "BLEU", "start_pos": 55, "end_pos": 59, "type": "METRIC", "confidence": 0.8092292547225952}]}, {"text": "We again demonstrated that the proposed method is effective (especially in WER) for multiple language pairs.", "labels": [], "entities": [{"text": "WER", "start_pos": 75, "end_pos": 78, "type": "TASK", "confidence": 0.6183878779411316}]}], "tableCaptions": [{"text": " Table 1: Number of word orderings in each type of  constraint", "labels": [], "entities": []}, {"text": " Table 2: E-J patent corpus", "labels": [], "entities": [{"text": "E-J patent", "start_pos": 10, "end_pos": 20, "type": "DATASET", "confidence": 0.7697047889232635}]}, {"text": " Table 3: Evaluation results in E-J patent translation", "labels": [], "entities": []}, {"text": " Table 4: Evaluation results in J-E patent translation", "labels": [], "entities": []}]}