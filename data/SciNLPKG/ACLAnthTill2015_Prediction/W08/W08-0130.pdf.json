{"title": [{"text": "Making Grammar-Based Generation Easier to Deploy in Dialogue Systems", "labels": [], "entities": []}], "abstractContent": [{"text": "We present a development pipeline and associated algorithms designed to make grammar-based generation easier to deploy in implemented dialogue systems.", "labels": [], "entities": [{"text": "grammar-based generation", "start_pos": 77, "end_pos": 101, "type": "TASK", "confidence": 0.7104887068271637}]}, {"text": "Our approach realizes a practical trade-off between the capabilities of a system's generation component and the authoring and maintenance burdens imposed on the generation content author fora deployed system.", "labels": [], "entities": []}, {"text": "To evaluate our approach, we performed a human rating study with system builders who work on a common large-scale spoken dialogue system.", "labels": [], "entities": []}, {"text": "Our results demonstrate the viability of our approach and illustrate authoring/performance trade-offs between hand-authored text, our grammar-based approach, and a competing shallow statistical NLG technique.", "labels": [], "entities": []}], "introductionContent": [{"text": "This paper gives an overview of anew examplebased generation technique that is designed to make grammar-based generation easier to deploy in dialogue systems.", "labels": [], "entities": []}, {"text": "Dialogue systems present several specific requirements fora practical generation component.", "labels": [], "entities": []}, {"text": "First, the generator needs to be fast enough to support real-time interaction with a human user.", "labels": [], "entities": []}, {"text": "Second, the generator must provide adequate coverage for the meanings the dialogue system needs to express.", "labels": [], "entities": []}, {"text": "What counts as \"adequate\" can vary between systems, since the high-level purpose of a dialogue system can affect priorities regarding output fluency, fidelity to the requested meaning, variety of alternative outputs, and tolerance for generation failures.", "labels": [], "entities": []}, {"text": "Third, developing the necessary resources for the generation component should be relatively straightforward in terms of time and expertise required.", "labels": [], "entities": []}, {"text": "This is especially important since dialogue systems are complex systems with significant development costs.", "labels": [], "entities": []}, {"text": "Finally, it should be relatively easy for the dialogue manager to formulate a generation request in the format required by the generator.", "labels": [], "entities": []}, {"text": "Together, these requirements can reduce the attractiveness of grammar-based generation when compared to simpler template-based or canned text output solutions.", "labels": [], "entities": []}, {"text": "In terms of speed, off-theshelf, wide-coverage grammar-based realizers such as FUF/SURGE ( can be too slow for real-time interaction.", "labels": [], "entities": [{"text": "FUF", "start_pos": 79, "end_pos": 82, "type": "METRIC", "confidence": 0.6367465853691101}]}, {"text": "In terms of adequacy of coverage, in principle, grammar-based generation offers significant advantages over template-based or canned text output by providing productive coverage and greater variety.", "labels": [], "entities": []}, {"text": "However, realizing these advantages can require significant development costs.", "labels": [], "entities": []}, {"text": "Specifying the necessary connections between lexico-syntactic resources and the flat, domain-specific semantic representations that are typically available in implemented systems is a subtle, labor-intensive, and knowledgeintensive process for which attractive methodologies do not yet exist.", "labels": [], "entities": []}, {"text": "One strategy is to hand-build an applicationspecific grammar.", "labels": [], "entities": []}, {"text": "However, in our experience, this process requires a painstaking, time-consuming effort by a developer who has detailed linguistic knowledge as well as detailed domain knowledge, and the resulting coverage is inevitably limited.", "labels": [], "entities": []}, {"text": "Wide-coverage generators that aim for applicabil-ity across application domains ( provide a grammar (or language model) for free.", "labels": [], "entities": []}, {"text": "However, it is harder to tailor output to the desired wording and style fora specific dialogue system, and these generators demand a specific input format that is otherwise foreign to an existing dialogue system.", "labels": [], "entities": []}, {"text": "Unfortunately, in our experience, the development burden of implementing the translation between the system's available meaning representations and the generator's required input format is quite substantial.", "labels": [], "entities": []}, {"text": "Indeed, implementing the translation might require as much effort as would be required to build a simple custom generator; cf..", "labels": [], "entities": []}, {"text": "This development cost is exacerbated when a dialogue system's native meaning representation scheme is under revision.", "labels": [], "entities": []}, {"text": "In this paper, we survey anew example-based approach) that we have developed in order to mitigate these difficulties, so that grammar-based generation can be deployed more widely in implemented dialogue systems.", "labels": [], "entities": []}, {"text": "Our development pipeline requires a system developer to create a set of training examples which directly connect desired output texts to available application semantic forms.", "labels": [], "entities": []}, {"text": "This is achieved through a streamlined authoring task that does not require detailed linguistic knowledge.", "labels": [], "entities": [{"text": "authoring task", "start_pos": 39, "end_pos": 53, "type": "TASK", "confidence": 0.8831443190574646}]}, {"text": "Our approach then processes these training examples to automatically construct all the resources needed fora fast, highquality, run-time grammar-based generation component.", "labels": [], "entities": []}, {"text": "We evaluate this approach using a pre-existing spoken dialogue system.", "labels": [], "entities": []}, {"text": "Our results demonstrate the viability of the approach and illustrate authoring/performance trade-offs between hand-authored text, our grammar-based approach, and a competing shallow statistical NLG technique.", "labels": [], "entities": []}], "datasetContent": [{"text": "In the introduction, we identified run-time speed, adequacy of coverage, authoring burdens, and NLG request specification as important factors in the selection of a technology fora dialogue system's NLG component.", "labels": [], "entities": []}, {"text": "In this section, we evaluate our technique along these four dimensions.", "labels": [], "entities": []}, {"text": "We collected a sample of 220 instances of frames that Doctor Perez's dialogue manager had requested of the generation component in previous dialogues with users.", "labels": [], "entities": []}, {"text": "Some frames occurred more than once in this sample.", "labels": [], "entities": []}, {"text": "Each frame was associated with a single handauthored utterance.", "labels": [], "entities": []}, {"text": "Some of these utterances arose inhuman role plays for Doctor Perez; some were written by a scriptwriter; others were authored by system builders to provide coverage for specific frames.", "labels": [], "entities": []}, {"text": "All were reviewed by a system builder for appropriateness to the corresponding frame.", "labels": [], "entities": []}, {"text": "We used these 220 (frame, utterance) examples to evaluate both our approach and a shallow statistical method called sentence retriever (discussed below).", "labels": [], "entities": [{"text": "sentence retriever", "start_pos": 116, "end_pos": 134, "type": "TASK", "confidence": 0.7357698380947113}]}, {"text": "We randomly split the examples into 198 training and 22 test examples; we used the same train/test split for our approach and sentence retriever.", "labels": [], "entities": [{"text": "sentence retriever", "start_pos": 126, "end_pos": 144, "type": "TASK", "confidence": 0.7396753281354904}]}, {"text": "To train our approach, we constructed training examples in the format specified in Section 3.1.", "labels": [], "entities": []}, {"text": "Syntax posed an interesting problem, because the Charniak parser frequently produces erroneous syntactic analyses for utterances in Doctor Perez's domain, but it was not obvious how detrimental these errors would be to overall generated output.", "labels": [], "entities": []}, {"text": "We therefore constructed two alternative sets of training examplesone where the syntax of each utterance was the uncorrected output of the Charniak parser, and another where the parser output was corrected by hand (the syntax in above is the corrected version).", "labels": [], "entities": []}, {"text": "Hand correction of parser output requires considerable linguistic expertise, so uncorrected output represents a substantial reduction in authoring burden.", "labels": [], "entities": [{"text": "Hand correction of parser output", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.7995730996131897}]}, {"text": "The connections between surface expressions and frame key-value pairs were identical in both uncorrected and corrected training sets, since they are independent of the syntax.", "labels": [], "entities": []}, {"text": "For each training set, we trained our generator on the 198 training examples.", "labels": [], "entities": []}, {"text": "We then generated a single (highest-ranked) utterance for each example in both the test and training sets.", "labels": [], "entities": []}, {"text": "The generator sometimes failed to find a successful utterance within the 200ms timeout; the success rate of our generator was 95% for training ex-amples and 80% for test examples.", "labels": [], "entities": []}, {"text": "The successful utterances were rated by our judges.", "labels": [], "entities": []}, {"text": "Sentence retriever is based on the crosslanguage information retrieval techniques described in (), and is currently in use for Doctor Perez's NLG problem.", "labels": [], "entities": [{"text": "Sentence retriever", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.9225039780139923}, {"text": "crosslanguage information retrieval", "start_pos": 35, "end_pos": 70, "type": "TASK", "confidence": 0.6156276563803355}, {"text": "Doctor Perez's NLG problem", "start_pos": 127, "end_pos": 153, "type": "DATASET", "confidence": 0.808382511138916}]}, {"text": "Sentence retriever does not exploit any hierarchical syntactic analysis of utterances.", "labels": [], "entities": [{"text": "Sentence retriever", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.9114401340484619}]}, {"text": "Instead, sentence retriever views NLG as an information retrieval task in which a set of training utterances are the \"documents\" to be retrieved, and the frame to be expressed is the query.", "labels": [], "entities": [{"text": "sentence retriever", "start_pos": 9, "end_pos": 27, "type": "TASK", "confidence": 0.7311994433403015}, {"text": "information retrieval task", "start_pos": 44, "end_pos": 70, "type": "TASK", "confidence": 0.7779016097386678}]}, {"text": "At run-time, the algorithm functions essentially as a classifier: it uses a relative entropy metric to select the highest ranking training utterance for the frame that Doctor Perez wishes to express.", "labels": [], "entities": []}, {"text": "This approach has been used because it is to some extent robust against changes in internal semantic representations, and against minor deficiencies in the training corpus, but as with a canned text approach, it requires each utterance to be hand-authored before it can be used in dialogue.", "labels": [], "entities": []}, {"text": "We trained sentence retriever on the 198 training examples, and used it to generate a single (highest-ranked) utterance for each example in both the test and training sets.", "labels": [], "entities": [{"text": "sentence retriever", "start_pos": 11, "end_pos": 29, "type": "TASK", "confidence": 0.729193702340126}]}, {"text": "Sentence retriever's success rate was 96% for training examples and 90% for test examples.", "labels": [], "entities": [{"text": "Sentence retriever", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.9308449625968933}]}, {"text": "The successful utterances were rated by our judges.", "labels": [], "entities": []}, {"text": "in the Appendix illustrates the alternative utterances that were produced fora frame present in the test data but not in the training data.", "labels": [], "entities": [{"text": "Appendix", "start_pos": 7, "end_pos": 15, "type": "METRIC", "confidence": 0.8807663321495056}]}, {"text": "Both our approach and sentence retriever run within the available 200ms window.", "labels": [], "entities": [{"text": "sentence retriever", "start_pos": 22, "end_pos": 40, "type": "TASK", "confidence": 0.7547305822372437}]}, {"text": "To assess output quality, we conducted a study in which 5 human judges gave overall quality ratings for various utterances Doctor Perez might use to express specific semantic frames.", "labels": [], "entities": []}, {"text": "In total, judges rated 494 different utterances which were produced in several conditions: hand-authored (for the relevant frame), generated by our approach, and sentence retriever.", "labels": [], "entities": [{"text": "sentence retriever", "start_pos": 162, "end_pos": 180, "type": "TASK", "confidence": 0.7363774180412292}]}, {"text": "We asked our 5 judges to rate each of the 494 utterances, in relation to the specific frame for which it was produced, on a single 1 (\"very bad\") to 5 (\"very good\") scale.", "labels": [], "entities": []}, {"text": "Since ratings need to incorporate accuracy with respect to the frame, our judges had to be able to read the raw system semantic representations.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 34, "end_pos": 42, "type": "METRIC", "confidence": 0.9984068274497986}]}, {"text": "This meant we could only use judges who were deeply familiar with the dialogue system; however, the main developer of the new generation algorithms (the first author) did not participate as a judge.", "labels": [], "entities": []}, {"text": "Judges were blind to the conditions under which utterances were produced.", "labels": [], "entities": []}, {"text": "The judges rated the utterances using a custom-built application which presented a single frame together with 1 to 6 candidate utterances for that frame.", "labels": [], "entities": []}, {"text": "The rating interface is shown in in the Appendix.", "labels": [], "entities": [{"text": "Appendix", "start_pos": 40, "end_pos": 48, "type": "METRIC", "confidence": 0.7482820153236389}]}, {"text": "The order of candidate utterances for each frame was randomized, and the order in which frames appeared was randomized for each judge.", "labels": [], "entities": []}, {"text": "The judges were instructed to incorporate both fluency and accuracy with respect to the frame into a single overall rating for each utterance.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 59, "end_pos": 67, "type": "METRIC", "confidence": 0.9986716508865356}]}, {"text": "While it is possible to have human judges rate fluency and accuracy independently, ratings of fluency alone are not particularly helpful in evaluating Doctor Perez's generation component, since for Doctor Perez, a certain degree of disfluency can contribute to believability (as noted in Section 2).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 59, "end_pos": 67, "type": "METRIC", "confidence": 0.9985973238945007}]}, {"text": "We therefore asked judges to make an overall assessment of output quality for the Doctor Perez character.", "labels": [], "entities": [{"text": "Doctor Perez character", "start_pos": 82, "end_pos": 104, "type": "DATASET", "confidence": 0.8895564079284668}]}, {"text": "The judges achieved a reliability of \u03b1 = 0.708; this value shows that agreement is well above chance, and allows for tentative conclusions.", "labels": [], "entities": [{"text": "reliability", "start_pos": 22, "end_pos": 33, "type": "METRIC", "confidence": 0.993886411190033}, {"text": "\u03b1", "start_pos": 37, "end_pos": 38, "type": "METRIC", "confidence": 0.9288305044174194}, {"text": "agreement", "start_pos": 70, "end_pos": 79, "type": "METRIC", "confidence": 0.9903081059455872}]}, {"text": "Agreement between subsets of judges ranged from \u03b1 = 0.802 for the most concordant pair of judges to \u03b1 = 0.593 for the most discordant pair.", "labels": [], "entities": []}, {"text": "We also performed an ANOVA comparing three conditions (generated, retrieved and hand-authored utterances) across the five judges; we found significant main effects of condition (F (2, 3107) = 55, p < 0.001) and judge (F (4, 3107) = 17, p < 0.001), but no significant interaction (F (8, 3107) = 0.55, p > 0.8).", "labels": [], "entities": [{"text": "ANOVA", "start_pos": 21, "end_pos": 26, "type": "METRIC", "confidence": 0.5751035213470459}, {"text": "F", "start_pos": 178, "end_pos": 179, "type": "METRIC", "confidence": 0.9808158278465271}, {"text": "judge", "start_pos": 211, "end_pos": 216, "type": "METRIC", "confidence": 0.9734985828399658}]}, {"text": "We therefore conclude that the individual differences among the judges do not affect the comparison of utterances across the different conditions, so we will report the rest of the evaluation on the mean ratings per utterance.", "labels": [], "entities": []}, {"text": "Due to the large number of factors and the differences in the number of utterances corresponding to each condition, we ran a small number of planned comparisons.", "labels": [], "entities": []}, {"text": "The distribution of ratings across utterances is not normal; to validate our results we accompanied each t-test by a nonparametric Wilcoxon rank sum test, and significance always fell in the same general range.", "labels": [], "entities": []}, {"text": "shows the observed rating frequencies of sentence retriever (mean 3.0) and our approach (mean 3.6) on the test examples.", "labels": [], "entities": [{"text": "sentence retriever", "start_pos": 41, "end_pos": 59, "type": "TASK", "confidence": 0.7322741150856018}]}, {"text": "While this data does not show a significant difference, it suggests that retriever's selected sentences are most frequently either very bad or very good; this reflects the fact that the classification algorithm retrieves highly fluent hand-authored text which is sometimes semantically very incorrect.", "labels": [], "entities": []}, {"text": "in the Appendix provides such an example, in which a retrieved sentence has the wrong polarity.)", "labels": [], "entities": []}, {"text": "The quality of our generated output, by comparison, appears more graded, with very good quality the most frequent outcome and lower qualities less frequent.", "labels": [], "entities": []}, {"text": "Ina system where there is a low tolerance for very bad quality output, generated output would likely be considered preferable to retrieved output.", "labels": [], "entities": []}, {"text": "In terms of generation failures, our approach had poorer coverage of test examples than sentence retriever (80% vs. 90%).", "labels": [], "entities": [{"text": "sentence retriever", "start_pos": 88, "end_pos": 106, "type": "TASK", "confidence": 0.7478127181529999}]}, {"text": "Note however that in this study, our approach only delivered an output if it could completely cover the requested frame.", "labels": [], "entities": []}, {"text": "In the future, we believe coverage could be improved, with perhaps some reduction in quality, by allowing outputs that only partially cover requested frames.", "labels": [], "entities": []}, {"text": "In terms of output variety, in this initial study our judges rated only the highest ranked output generated or retrieved for each frame.", "labels": [], "entities": []}, {"text": "However, we observed that our generator frequently finds several alternative utterances of relatively high quality (see; thus our approach offers another potential advantage in output variety.", "labels": [], "entities": []}, {"text": "Both canned text and sentence retriever require only frames and corresponding output sentences as input.", "labels": [], "entities": [{"text": "sentence retriever", "start_pos": 21, "end_pos": 39, "type": "TASK", "confidence": 0.7069625109434128}]}, {"text": "In our approach, syntax and semantic links are additionally needed.", "labels": [], "entities": []}, {"text": "We compared the use of corrected vs. uncorrected syntax in training.", "labels": [], "entities": []}, {"text": "Surprisingly, we found no significant difference between generated output trained on corrected and uncorrected syntax (t(29) = 0.056, p > 0.9 on test items, t(498) = \u22121.1, p > 0.2 on all items).", "labels": [], "entities": []}, {"text": "This is a substantial win in terms of reduced authoring burden for our approach.", "labels": [], "entities": [{"text": "authoring", "start_pos": 46, "end_pos": 55, "type": "TASK", "confidence": 0.9580207467079163}]}, {"text": "If uncorrected syntax is used, the additional burden of our approach lies only in specifying the semantic links.", "labels": [], "entities": []}, {"text": "For the 220 examples in this study, one system builder specified these links in about 6 hours.", "labels": [], "entities": []}, {"text": "We present a detailed cost/benefit analysis of this effort in.", "labels": [], "entities": []}, {"text": "Both our approach and sentence retriever accept the dialogue manager's native semantic representation for NLG as input.", "labels": [], "entities": [{"text": "sentence retriever", "start_pos": 22, "end_pos": 40, "type": "TASK", "confidence": 0.7216794192790985}]}, {"text": "In exchange fora slightly increased authoring burden, our approach yields a generation component that generalizes to unseen test problems relatively gracefully, and does not suffer from the frequent very bad output or the necessity to author every utterance that comes with canned text or a competing statistical classification technique.", "labels": [], "entities": []}], "tableCaptions": []}