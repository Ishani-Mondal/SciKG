{"title": [{"text": "Indexing on Semantic Roles for Question Answering", "labels": [], "entities": [{"text": "Indexing", "start_pos": 0, "end_pos": 8, "type": "TASK", "confidence": 0.958997368812561}, {"text": "Question Answering", "start_pos": 31, "end_pos": 49, "type": "TASK", "confidence": 0.7223563939332962}]}], "abstractContent": [{"text": "Semantic Role Labeling (SRL) has been used successfully in several stages of automated Question Answering (QA) systems but its inherent slow procedures make it difficult to use at the indexing stage of the document retrieval component.", "labels": [], "entities": [{"text": "Semantic Role Labeling (SRL)", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.8023557464281718}, {"text": "Question Answering (QA)", "start_pos": 87, "end_pos": 110, "type": "TASK", "confidence": 0.8380531072616577}]}, {"text": "In this paper we confirm the intuition that SRL at indexing stage improves the performance of QA and propose a simplified technique named the Question Prediction Language Model (QPLM), which provides similar information with a much lower cost.", "labels": [], "entities": [{"text": "SRL", "start_pos": 44, "end_pos": 47, "type": "TASK", "confidence": 0.9711911082267761}]}, {"text": "The methods were tested on four different QA systems and the results suggest that QPLM can be used as a good compromise between speed and accuracy.", "labels": [], "entities": [{"text": "speed", "start_pos": 128, "end_pos": 133, "type": "METRIC", "confidence": 0.9751944541931152}, {"text": "accuracy", "start_pos": 138, "end_pos": 146, "type": "METRIC", "confidence": 0.9833049774169922}]}], "introductionContent": [{"text": "Semantic Role Labeling (SRL) has been implemented or suggested as a means to aid several Natural Language Processing (NLP) tasks such as information extraction), multidocument summarization () and machine translation.", "labels": [], "entities": [{"text": "Semantic Role Labeling (SRL)", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.8109424114227295}, {"text": "information extraction", "start_pos": 137, "end_pos": 159, "type": "TASK", "confidence": 0.8193122744560242}, {"text": "multidocument summarization", "start_pos": 162, "end_pos": 189, "type": "TASK", "confidence": 0.7569730281829834}, {"text": "machine translation", "start_pos": 197, "end_pos": 216, "type": "TASK", "confidence": 0.8348599672317505}]}, {"text": "Question Answering (QA) is one task that takes advantage of SRL, and in fact much of the research about the application of SRL to NLP is related to QA.", "labels": [], "entities": [{"text": "Question Answering (QA)", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.8812737584114074}, {"text": "SRL", "start_pos": 60, "end_pos": 63, "type": "TASK", "confidence": 0.9031141400337219}, {"text": "SRL", "start_pos": 123, "end_pos": 126, "type": "TASK", "confidence": 0.9552844762802124}]}, {"text": "Thus, apply the argument-predicate relationship from PropBank () together with the semantic frames from FrameNet ( to create an inference mechanism to improve QA.", "labels": [], "entities": [{"text": "PropBank", "start_pos": 53, "end_pos": 61, "type": "DATASET", "confidence": 0.9270403981208801}, {"text": "FrameNet", "start_pos": 104, "end_pos": 112, "type": "DATASET", "confidence": 0.8847761750221252}]}, {"text": "relational information in order to transform questions into information retrieval queries and further analyze the results to find the answers for natural language questions.", "labels": [], "entities": []}, {"text": "use a shallow semantic parser to create semantic roles in order to match questions and answers.", "labels": [], "entities": []}, {"text": "developed an answer extraction module that incorporates FrameNet style semantic role information.", "labels": [], "entities": [{"text": "answer extraction", "start_pos": 13, "end_pos": 30, "type": "TASK", "confidence": 0.9016391932964325}]}, {"text": "They deal with the semantic role assignment as a optimization problem in a bipartite graph and the answer extraction as a graph matching over the semantic relations.", "labels": [], "entities": [{"text": "semantic role assignment", "start_pos": 19, "end_pos": 43, "type": "TASK", "confidence": 0.7012693683306376}, {"text": "answer extraction", "start_pos": 99, "end_pos": 116, "type": "TASK", "confidence": 0.7544933557510376}]}, {"text": "Most of the studies that use SRL or similar techniques to QA apply semantic relation tools on the input or output of the Information Retrieval phase of their system.", "labels": [], "entities": [{"text": "Information Retrieval phase", "start_pos": 121, "end_pos": 148, "type": "TASK", "confidence": 0.8442701697349548}]}, {"text": "Our paper investigates the use of semantic information for indexing documents.", "labels": [], "entities": [{"text": "indexing documents", "start_pos": 59, "end_pos": 77, "type": "TASK", "confidence": 0.892846554517746}]}, {"text": "Our hypothesis is that allowing Semantic Role information at the indexing stage the question analyzer and subsequent stages of the QA system can obtain higher accuracy by providing an implicit query analyzer as well as more precise retrieval.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 159, "end_pos": 167, "type": "METRIC", "confidence": 0.9980977177619934}]}, {"text": "Theoretically, the inclusion of this information at indexing time can also speedup the overall QA process since syntactic rephrasing or re-ranking of documents based on semantic roles would not be necessary.", "labels": [], "entities": [{"text": "QA", "start_pos": 95, "end_pos": 97, "type": "TASK", "confidence": 0.9677600264549255}]}, {"text": "However, SRL techniques are still highly complex and they demand a computational power that is not yet available to most research groups when working with large corpora.", "labels": [], "entities": [{"text": "SRL", "start_pos": 9, "end_pos": 12, "type": "TASK", "confidence": 0.9928438067436218}]}, {"text": "In our experience the annotation of a 3GB corpus, such as the AQUAINT), using a semantic role labeler, for instance SwiRL from Surdeanu and can take more than one year using a standard PC configuration . In order to efficiently process a corpus with se-mantic relations, we have developed an alternative annotation strategy based on word-to-word relations instead of noun phrase-to-predicate relations.", "labels": [], "entities": [{"text": "AQUAINT", "start_pos": 62, "end_pos": 69, "type": "DATASET", "confidence": 0.8565657734870911}]}, {"text": "We define semantic triples based on syntactic clues; this approach was also studied by Litkowski (1999) but some major differences with our work are that we use automatically learned rules to generate the semantic relations, and that we use different semantic labels than those defined by Litkowski, some more specific and some more general.", "labels": [], "entities": []}, {"text": "Our annotation scheme is named the Question Prediction Language Model (QPLM) and represents relations between pairs of words using labels such as Who and When, according to how one word complements the other.", "labels": [], "entities": []}, {"text": "In the following section we provide an overview of the proposed semantic annotation module.", "labels": [], "entities": []}, {"text": "Then in Section 3 we detail the information retrieval framework used that allows the indexing and retrieval of semantic information.", "labels": [], "entities": [{"text": "information retrieval", "start_pos": 32, "end_pos": 53, "type": "TASK", "confidence": 0.7602740824222565}, {"text": "indexing and retrieval of semantic information", "start_pos": 85, "end_pos": 131, "type": "TASK", "confidence": 0.7429143488407135}]}, {"text": "Section 4 describes the experimental setup and presents the results.", "labels": [], "entities": []}, {"text": "Finally, Section 5 presents the concluding remarks and some discussion of further work.", "labels": [], "entities": []}], "datasetContent": [{"text": "We have performed a series of experiments using the techniques described on Section 3 in order to verify the usefulness of QPLM in comparison to SRL based on PropBank.", "labels": [], "entities": [{"text": "QPLM", "start_pos": 123, "end_pos": 127, "type": "DATASET", "confidence": 0.7905879020690918}, {"text": "PropBank", "start_pos": 158, "end_pos": 166, "type": "DATASET", "confidence": 0.964659571647644}]}, {"text": "We compared both semantic annotations by using it with IR and under QA evaluation methods.", "labels": [], "entities": []}, {"text": "We performed experiments using data resources from the QA track of the TREC conferences () and the evaluation scripts available at their TREC website of years.", "labels": [], "entities": [{"text": "QA track of the TREC conferences", "start_pos": 55, "end_pos": 87, "type": "DATASET", "confidence": 0.8871683776378632}, {"text": "TREC website", "start_pos": 137, "end_pos": 149, "type": "DATASET", "confidence": 0.9194190800189972}]}, {"text": "The retrieval experiments were carried out using only a reduced set of documents from the AQUAINT corpus because the semantic role labelers tested were notable to parse the full set, unlike QPLM which parsed all documents successfully.", "labels": [], "entities": [{"text": "AQUAINT corpus", "start_pos": 90, "end_pos": 104, "type": "DATASET", "confidence": 0.9362837374210358}]}, {"text": "The SRL tool SwiRL (Surdeanu and) has a good precision and coverage, however it is slow and quite unstable when parsing large amounts of data.", "labels": [], "entities": [{"text": "SRL", "start_pos": 4, "end_pos": 7, "type": "TASK", "confidence": 0.8323037624359131}, {"text": "precision", "start_pos": 45, "end_pos": 54, "type": "METRIC", "confidence": 0.9993723034858704}, {"text": "coverage", "start_pos": 59, "end_pos": 67, "type": "METRIC", "confidence": 0.9881729483604431}]}, {"text": "We have assembled a cluster of computers in order to speedup the corpus annotation, but even when having around ten dedicated computers the estimated completion time was larger than one year.", "labels": [], "entities": [{"text": "corpus annotation", "start_pos": 65, "end_pos": 82, "type": "TASK", "confidence": 0.6218984872102737}]}, {"text": "The lack of semantic annotators that can quickly evaluate large amount of data gave us the stimulus needed to use a simplified and quicker technique.", "labels": [], "entities": []}, {"text": "We used the QPLM annotation tool which takes less than 3 weeks to fully annotate the 3GB of data from the AQUAINT corpus using a single machine.", "labels": [], "entities": [{"text": "AQUAINT corpus", "start_pos": 106, "end_pos": 120, "type": "DATASET", "confidence": 0.9590631425380707}]}, {"text": "Since we wanted to determinate how QPLM compares to SRL, particularly on the basis of its usage for IR and for QA, we performed some tests using the available amount of data annotated with semantic roles, and the same documents with QPLM.", "labels": [], "entities": []}, {"text": "The part of the AQUAINT corpus annotated includes the first 41,116 documents, in chronological order, from the New York Times (NYT) newspaper.", "labels": [], "entities": [{"text": "AQUAINT corpus annotated", "start_pos": 16, "end_pos": 40, "type": "DATASET", "confidence": 0.9051246444384257}, {"text": "New York Times (NYT) newspaper", "start_pos": 111, "end_pos": 141, "type": "DATASET", "confidence": 0.681890036378588}]}, {"text": "We used the 1,448 questions from the QA track of from the TREC competition.", "labels": [], "entities": [{"text": "QA track", "start_pos": 37, "end_pos": 45, "type": "DATASET", "confidence": 0.8038119673728943}, {"text": "TREC competition", "start_pos": 58, "end_pos": 74, "type": "DATASET", "confidence": 0.7864153385162354}]}, {"text": "Since these questions are not always self contained and in some cases (OTHER-type questions) not even a proper natural language sentence, we performed some question modification so that the entire topic text could be included.", "labels": [], "entities": []}, {"text": "These modifications include substitution of key pronouns as well as the inclusion of the whole topic text when shorter representations were found.", "labels": [], "entities": []}, {"text": "In some extreme cases when no substitution was possible and the question did not mention the topic, we added a phrase containing the topic at the start of the question.", "labels": [], "entities": []}, {"text": "Some examples are presented Topic: Gordon Gekko Question: What year was the movie released?", "labels": [], "entities": []}, {"text": "Modification: Regarding Gordon Gekko, what year was the movie released?", "labels": [], "entities": [{"text": "Modification", "start_pos": 0, "end_pos": 12, "type": "METRIC", "confidence": 0.858704686164856}]}, {"text": "Question: What was Gekko's profession?", "labels": [], "entities": []}, {"text": "Modification: What was Gordon Gekko's profession?", "labels": [], "entities": []}, {"text": "Question: Other Modification: Tell me more about Gordon Gekko.", "labels": [], "entities": []}, {"text": "Using these questions as queries for our IR framework, we retrieved a set of 50 documents for every question.", "labels": [], "entities": [{"text": "IR", "start_pos": 41, "end_pos": 43, "type": "TASK", "confidence": 0.9324515461921692}]}, {"text": "We analyzed the impact of the semantic annotation when used on document indices by checking the presence of the answer string in the documents returned.", "labels": [], "entities": []}, {"text": "We also obtained a list of 50 documents using solely the BoW approach in order to compare what is the gain over standard retrieval.", "labels": [], "entities": [{"text": "BoW", "start_pos": 57, "end_pos": 60, "type": "DATASET", "confidence": 0.8883711695671082}]}, {"text": "presents the results of the retrieval set using TREC's QA track from using the BoW, the SRL and the QPLM approaches.", "labels": [], "entities": [{"text": "TREC's QA track", "start_pos": 48, "end_pos": 63, "type": "DATASET", "confidence": 0.8381478488445282}, {"text": "BoW", "start_pos": 79, "end_pos": 82, "type": "DATASET", "confidence": 0.971527636051178}]}, {"text": "Because we performed the evaluation of these documents automatically, we consider a document relevant on the only basis of the presence of the required answer string.", "labels": [], "entities": []}, {"text": "We adopted the evaluation metrics for QA documents sets proposed by.", "labels": [], "entities": []}, {"text": "We used the following metrics: p@n as the precision at n documents or percentage of documents containing an answer when retrieving at most n documents; c@n as the coverage at n documents or percentage of questions that can be answered using up ton documents for each question; and r@n as the redundancy at n document or the average number of answers found in the first n documents per question.", "labels": [], "entities": [{"text": "precision", "start_pos": 42, "end_pos": 51, "type": "METRIC", "confidence": 0.9987421631813049}]}, {"text": "As observed in, the SRL approach gives the best results for all question sets on all evaluation metrics, with the exception of c@50 on the 2006 question set.", "labels": [], "entities": [{"text": "SRL", "start_pos": 20, "end_pos": 23, "type": "TASK", "confidence": 0.841506838798523}, {"text": "2006 question set", "start_pos": 139, "end_pos": 156, "type": "DATASET", "confidence": 0.7300050059954325}]}, {"text": "In most other retrieval sets the baseline performs worse than both QPLM and SRL, however for 2004 questions it performed better than QPLM on p@50 and r@50.", "labels": [], "entities": []}, {"text": "It is interesting to observe that the QPLM results for the same year on c@50 are better than the BoW approach indicating that a larger amount of questions can potentially be answered by QPLM.: Experimental results of index approaches on TREC questions  To better understand the relation between the retrieved document sets and question answering we applied the retrieval sets to four question answering systems: \u2022 Aranea: Developed by Lin (2007), the Aranea system utilizes the redundancy from the World Wide Web using different Web Search Engines.", "labels": [], "entities": [{"text": "question answering", "start_pos": 327, "end_pos": 345, "type": "TASK", "confidence": 0.7697946727275848}]}, {"text": "The system relies on the text snippets to generate candidate answers.", "labels": [], "entities": []}, {"text": "It applies filtering techniques based on intuitive rules, as well as the expected answer classes with named-entities recognition defined by regular expressions and a fixed list for some special cases.", "labels": [], "entities": []}, {"text": "\u2022 OpenEphyra: Developed by, the OpenEphyra framework attempts to be a test bench for question answering techniques.", "labels": [], "entities": [{"text": "question answering", "start_pos": 85, "end_pos": 103, "type": "TASK", "confidence": 0.8899435698986053}]}, {"text": "The system approaches QA in a fairly standard way.", "labels": [], "entities": [{"text": "QA", "start_pos": 22, "end_pos": 24, "type": "TASK", "confidence": 0.7433487772941589}]}, {"text": "Using a three-stage QA architecture (Question Analysis, Information Retrieval, Answer Extraction), it performed reasonably well at the QA Track at TREC 2007 by using Web Search engines on its IR stage and mapping the answers back into the TREC corpus.", "labels": [], "entities": [{"text": "Question Analysis", "start_pos": 37, "end_pos": 54, "type": "TASK", "confidence": 0.7474476397037506}, {"text": "Information Retrieval", "start_pos": 56, "end_pos": 77, "type": "TASK", "confidence": 0.6811895072460175}, {"text": "Answer Extraction", "start_pos": 79, "end_pos": 96, "type": "TASK", "confidence": 0.6806683093309402}, {"text": "QA Track at TREC 2007", "start_pos": 135, "end_pos": 156, "type": "DATASET", "confidence": 0.7549067676067353}, {"text": "TREC corpus", "start_pos": 239, "end_pos": 250, "type": "DATASET", "confidence": 0.9586887657642365}]}, {"text": "\u2022 MetaQA System: Similar to the Aranea QA system, MetaQA (Pizzato and ) makes heavy use of redundancy and the information provided by Web Search Engines.", "labels": [], "entities": []}, {"text": "However it goes a step further by combining different classes of Web Search engines (including Web Question Answering Systems) and assigning different confidence scores to each of the classes.", "labels": [], "entities": [{"text": "Web Question Answering", "start_pos": 95, "end_pos": 117, "type": "TASK", "confidence": 0.6060232619444529}]}, {"text": "\u2022 AnswerFinder: Developed by Moll\u00e1 and Van Zaanen (2006), the AnswerFinder QA system unique feature is the use of QA graph rules learned automatically from a small training corpus.", "labels": [], "entities": []}, {"text": "These graph rules are based on the maximum common subgraph between the deep syntactic representation of a question and a candidate answer sentence.", "labels": [], "entities": []}, {"text": "The graphs were derived from the output of the Connexor dependency-based parser.", "labels": [], "entities": []}, {"text": "For most of these systems some modifications of the standard system configuration were required.", "labels": [], "entities": []}, {"text": "All the systems used, with the exception of AnswerFinder, make heavy use of web search engines and the redundancy obtained to find their answers.", "labels": [], "entities": []}, {"text": "For our experiments we had to turn the Web search off, causing a significant drop in performance when compared to the reported results in the literature.", "labels": [], "entities": []}, {"text": "Because AnswerFinder's IR component is performed offline, the integration is seamless and only required providing the system with a list of documents in the same format as TREC distributes the ranked list of files per topic.", "labels": [], "entities": []}, {"text": "The OpenEphyra framework is well designed and implemented, however the interaction between its components still depended on the overall system architecture, which makes the implementation of new modules for the system quite difficult.", "labels": [], "entities": []}, {"text": "With the exception of AnswerFinder, all the QA systems received a retrieval set as a collection of snippets.", "labels": [], "entities": []}, {"text": "This was based on the fact that these systems are based on Web Retrieval and they expect to receive documents in this format.", "labels": [], "entities": []}, {"text": "We extracted for every document the 255 character window where more question words (non-stopwords) were found.", "labels": [], "entities": []}, {"text": "The implementation of different ranking strategies for passage retrieval such as those described by could improve the results for individual QA systems.", "labels": [], "entities": [{"text": "passage retrieval", "start_pos": 55, "end_pos": 72, "type": "TASK", "confidence": 0.9590131342411041}]}, {"text": "However, a preliminary evaluation of the passage retrieval have shown us that the 255 character window with the current snippet construction method was enough to achieve near optimal performance on the document set used.", "labels": [], "entities": []}, {"text": "The results obtained by the QA systems were processed using the answer regular expressions distributed by TREC.", "labels": [], "entities": [{"text": "TREC", "start_pos": 106, "end_pos": 110, "type": "DATASET", "confidence": 0.7480083107948303}]}, {"text": "The numbers described in this study show the factoid score for correct answers.", "labels": [], "entities": []}, {"text": "We have not used the exact answer because it required some cleaning of the answer log files and some modification of some QA systems.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Experimental results of index approaches  on TREC questions", "labels": [], "entities": []}, {"text": " Table 2: Factoid results for C@1 on the Aranea  system", "labels": [], "entities": [{"text": "Factoid", "start_pos": 10, "end_pos": 17, "type": "METRIC", "confidence": 0.8801093101501465}, {"text": "Aranea  system", "start_pos": 41, "end_pos": 55, "type": "DATASET", "confidence": 0.9750469624996185}]}, {"text": " Table 3: Factoid results for C@1 on the OpenE- phyra system", "labels": [], "entities": [{"text": "Factoid", "start_pos": 10, "end_pos": 17, "type": "METRIC", "confidence": 0.7889936566352844}, {"text": "OpenE- phyra system", "start_pos": 41, "end_pos": 60, "type": "DATASET", "confidence": 0.8567067533731461}]}, {"text": " Table 4: Factoid results for C@10 on the MetaQA  system", "labels": [], "entities": [{"text": "Factoid", "start_pos": 10, "end_pos": 17, "type": "METRIC", "confidence": 0.8045099973678589}, {"text": "MetaQA  system", "start_pos": 42, "end_pos": 56, "type": "DATASET", "confidence": 0.9070225656032562}]}, {"text": " Table 5: Factoid results for C@1 on the An- swerFinder system", "labels": [], "entities": [{"text": "Factoid", "start_pos": 10, "end_pos": 17, "type": "METRIC", "confidence": 0.7748773694038391}]}]}