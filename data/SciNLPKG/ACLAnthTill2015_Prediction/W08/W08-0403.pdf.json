{"title": [{"text": "Prior Derivation Models For Formally Syntax-based Translation Using Linguistically Syntactic Parsing and Tree Kernels", "labels": [], "entities": [{"text": "Syntax-based Translation", "start_pos": 37, "end_pos": 61, "type": "TASK", "confidence": 0.6359368413686752}]}], "abstractContent": [{"text": "This paper presents an improved formally syntax-based SMT model, which is enriched by linguistically syntactic knowledge obtained from statistical constituent parsers.", "labels": [], "entities": [{"text": "SMT", "start_pos": 54, "end_pos": 57, "type": "TASK", "confidence": 0.9476794004440308}]}, {"text": "We propose a linguistically-motivated prior derivation model to score hypothesis derivations on top of the baseline model during the translation decoding.", "labels": [], "entities": []}, {"text": "Moreover, we devise a fast training algorithm to achieve such improved models based on tree kernel methods.", "labels": [], "entities": []}, {"text": "Experiments on an English-to-Chinese task demonstrate that our proposed models outperformed the baseline formally syntax-based models, while both of them achieved significant improvements over a state-of-the-art phrase-based SMT system.", "labels": [], "entities": [{"text": "SMT", "start_pos": 225, "end_pos": 228, "type": "TASK", "confidence": 0.8579023480415344}]}], "introductionContent": [{"text": "In recent years, syntax-based translation models) have shown promising progress in improving translation quality.", "labels": [], "entities": []}, {"text": "There are two major elements accounting for such an improvement: namely the incorporation of phrasal translation structures adopted from widely applied phrase-based models () to handle local fluency, and the engagement of synchronous context-free grammars (SCFG), which enhances the generative capacity of the underlying model that is limited by finite-state machinery.", "labels": [], "entities": []}, {"text": "Approaches to syntax-based translation models using SCFG can be further categorized into two classes, based on their dependency on annotated corpus.", "labels": [], "entities": []}, {"text": "Following Chiang, we note the following distinction between these two classes: \u2022 Linguistically syntax-based: models that utilize structures defined over linguistic theory and annotations (e.g., Penn Treebank), and SCFG rules are derived from parallel corpus that is guided by explicitly parsing on at least one side of the parallel corpus.", "labels": [], "entities": [{"text": "Penn Treebank", "start_pos": 195, "end_pos": 208, "type": "DATASET", "confidence": 0.9852768778800964}]}, {"text": "Examples among others are) and ().", "labels": [], "entities": []}, {"text": "\u2022 Formally syntax-based: models are based on hierarchical structures of natural language but synchronous grammars are automatically extracted from parallel corpus without any usage of linguistic knowledge or annotations.", "labels": [], "entities": []}, {"text": "Examples include Wu's (Wu, 1997) ITG and Chiang's hierarchical models.", "labels": [], "entities": []}, {"text": "While these two often resemble in appearance, from practical viewpoints, there are some distinctions in training and decoding procedures differentiating formally syntax-based models from linguistically syntax-based models.", "labels": [], "entities": []}, {"text": "First, the former has no dependency on available linguistic theory and annotations for targeting language pairs, and thus the training and rule extraction are more efficient.", "labels": [], "entities": [{"text": "rule extraction", "start_pos": 139, "end_pos": 154, "type": "TASK", "confidence": 0.7809292376041412}]}, {"text": "Secondly, the decoding complexity of the former is lower 1 , especially when integrating a n-gram based The complexity is dominated by synchronous parsing and boundary words keeping.", "labels": [], "entities": [{"text": "boundary words keeping", "start_pos": 159, "end_pos": 181, "type": "TASK", "confidence": 0.6533149381478628}]}, {"text": "Thus binary SCFG employed in formally syntax-based systems help to maintain efficient CKY decoding.", "labels": [], "entities": []}, {"text": "Recent work by) shows a practically efficient approach that binarizes linguistically SCFG rules when possible.", "labels": [], "entities": []}, {"text": "language model, which is a key element to ensure translation output quality.", "labels": [], "entities": []}, {"text": "On the other hand, available linguistic theory and annotations could provide invaluable benefits in grammar induction and scoring, as shown by recent progress on such models ().", "labels": [], "entities": [{"text": "grammar induction", "start_pos": 100, "end_pos": 117, "type": "TASK", "confidence": 0.7588094472885132}]}, {"text": "In contrast, formally syntax-based grammars often lack explicit linguistic constraints.", "labels": [], "entities": []}, {"text": "In this paper, we propose a scheme to enrich formally syntax-based models with linguistically syntactic knowledge.", "labels": [], "entities": []}, {"text": "In other words, we maintain our grammar to be based on formal syntax on surface, but incorporate linguistic knowledge into our models to leverage syntax theory and annotations.", "labels": [], "entities": []}, {"text": "First, how to score SCFG rules whose general abstraction forms are unseen in the training data is an important question to answer.", "labels": [], "entities": [{"text": "SCFG rules whose general abstraction", "start_pos": 20, "end_pos": 56, "type": "TASK", "confidence": 0.8201518058776855}]}, {"text": "In hierarchical models, Chiang utilizes heuristics where certain assumptions are made on rule distributions to obtain relative frequency counts.", "labels": [], "entities": []}, {"text": "We intend to explore if additional linguistically parsing information would be beneficial to improve the scoring of formally syntactic SCFG grammars.", "labels": [], "entities": []}, {"text": "Secondly, we note that SCFG-based models often come with an excessive memory consumption as its rule size is an order of magnitude larger compared to phrase-based models, which challenges its practical deployment for online real-time translation tasks.", "labels": [], "entities": [{"text": "online real-time translation tasks", "start_pos": 217, "end_pos": 251, "type": "TASK", "confidence": 0.6867319792509079}]}, {"text": "Furthermore, formal syntax rules are often redundant as they are automatically extracted without linguistic supervision.", "labels": [], "entities": []}, {"text": "Therefore, we are motivated to study approaches to further score and rank formal syntax rules based on syntax-inspired methods, and eventually to prune unnecessary rules without loss of performance in general.", "labels": [], "entities": []}, {"text": "In our study, we propose a linguisticallymotivated method to train prior derivation models for formally syntax-based translation.", "labels": [], "entities": []}, {"text": "In this framework, prior derivation models can be viewed as a smoothing of rule translation models, addressing the weakness of the baseline model estimation that relies on relative counts obtained from heuristics.", "labels": [], "entities": [{"text": "rule translation", "start_pos": 75, "end_pos": 91, "type": "TASK", "confidence": 0.7291171252727509}]}, {"text": "First, we apply automatic parsers to obtain syntax annotations on the English side of the parallel corpus.", "labels": [], "entities": []}, {"text": "Next, we extract tree fragments associated with phrase pairs, and measure similarity between such tree fragments using kernel methods).", "labels": [], "entities": []}, {"text": "Finally, we score and rank rules based on their minimal cluster similarity of their nonterminals, which is used to compute the prior distribution of hypothesis derivations during decoding for improved translation.", "labels": [], "entities": []}, {"text": "The remainder of the paper is organized as follows.", "labels": [], "entities": []}, {"text": "We start with a brief review of some related work in Sec.", "labels": [], "entities": []}, {"text": "3, we describe our formally syntax-based models and decoder implementation, that is established as our baseline system.", "labels": [], "entities": []}, {"text": "4 presents the approach to score formal SCFG rules using kernel methods.", "labels": [], "entities": []}, {"text": "Experimental results are provided in Sec.", "labels": [], "entities": []}, {"text": "6 summarized our contributions with discussions and future work.", "labels": [], "entities": []}], "datasetContent": [{"text": "We perform our experiments on an English-toChinese translation task in travel domain.", "labels": [], "entities": [{"text": "English-toChinese translation task", "start_pos": 33, "end_pos": 67, "type": "TASK", "confidence": 0.7660496532917023}]}, {"text": "Our training set contains 482017 parallel sentences (with 4.4M words on the English side), which are collected from transcription and human translation of conversations.", "labels": [], "entities": []}, {"text": "The vocabulary size is 37K for English and 44K for Chinese after segmentation.", "labels": [], "entities": []}, {"text": "Our evaluation data is a held out data set of 2755 sentences pairs.", "labels": [], "entities": []}, {"text": "We extracted everyone out of two sentence pairs into the dev-set, and left the remainder as the test-set.", "labels": [], "entities": []}, {"text": "We thereby obtained a dev-set of 1378 sentence pairs, and a test-set with 1377 sentence pairs.", "labels": [], "entities": []}, {"text": "In both cases, there are about 15K running words on English side.", "labels": [], "entities": []}, {"text": "All Chinese sentences in training, dev and test sets are all automatically segmented into words.", "labels": [], "entities": []}, {"text": "Minimum-error-rate training are conducted on dev-set to optimize feature weights maximizing the BLEU score up to 4-grams, and the obtained feature weights are blindly applied on the test-set.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 96, "end_pos": 106, "type": "METRIC", "confidence": 0.9791358709335327}]}, {"text": "To compare performances excluding tokenization effects, all BLEU scores are optimized (on dev-set) and reported (on test-set) at Chinese character-level.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 60, "end_pos": 64, "type": "METRIC", "confidence": 0.9938948750495911}]}, {"text": "From training data, we extracted an initial phrase pair set with 3.7M entries for phrases up to 8 words on Chinese side.", "labels": [], "entities": []}, {"text": "We trained a 4-gram language model for Chinese at word level, which is shared by all translation systems reported in this paper, using the Chinese side of the parallel corpus that contains around 2M segmented words.", "labels": [], "entities": []}, {"text": "We compare the proposed models with two baselines: a state-of-the-art phrase-based system and a formal syntax-based system as described in Sec.", "labels": [], "entities": []}, {"text": "3. The phrase-based system employs the 3.7M phrase pairs to build the translation model, and it contains a total set of 8 features, most of which are identical to our baseline formal syntax-based model.", "labels": [], "entities": []}, {"text": "The difference only lies on that the glue and abstraction penalty are not applicable for phrase-based system.", "labels": [], "entities": []}, {"text": "Instead, a lexicalized reordering model is trained from the word-aligned parallel corpus for the phrase-based system.", "labels": [], "entities": []}, {"text": "More details about our multiple-graph based phrasal SMT can be found in (.", "labels": [], "entities": [{"text": "SMT", "start_pos": 52, "end_pos": 55, "type": "TASK", "confidence": 0.8112680912017822}]}, {"text": "For the baseline syntax-based system, we generated a total of 15M rules and used 9 features.", "labels": [], "entities": []}, {"text": "We chose the Stanford parser () as the English parser in our experiments due to its high accuracy and relatively faster speed.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 89, "end_pos": 97, "type": "METRIC", "confidence": 0.9990265369415283}]}, {"text": "It was trained on the Wall Street Journal section of the Penn Treebank.", "labels": [], "entities": [{"text": "Wall Street Journal section of the Penn Treebank", "start_pos": 22, "end_pos": 70, "type": "DATASET", "confidence": 0.947932556271553}]}, {"text": "During the parsing, the input English sentences were tokenized first, in a style consistent with the data in the Penn Treebank.", "labels": [], "entities": [{"text": "Penn Treebank", "start_pos": 113, "end_pos": 126, "type": "DATASET", "confidence": 0.9947275817394257}]}, {"text": "We sent 482017 English sentences to the parser.", "labels": [], "entities": []}, {"text": "There were 1221 long sentences failed, less than 0.3% of the whole set.", "labels": [], "entities": []}, {"text": "After the word alignment and phrase extraction on the parallel corpus, we obtained 2.2M unique English phrases.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 10, "end_pos": 24, "type": "TASK", "confidence": 0.7429559826850891}, {"text": "phrase extraction", "start_pos": 29, "end_pos": 46, "type": "TASK", "confidence": 0.7631468772888184}]}, {"text": "Among them there are about 34K phrases having an empty tree in their corresponding tree lists, due to the failure in parsing.", "labels": [], "entities": []}, {"text": "The number of unique tree fragments for English phrases is 2.5M.", "labels": [], "entities": []}, {"text": "Out of them there are 750K marked as incomplete.", "labels": [], "entities": []}, {"text": "As mentioned previously, each rule covers a set of phrases, with each phrase linked to a tree list.", "labels": [], "entities": []}, {"text": "The total number of rules with unique English side is around 8M.", "labels": [], "entities": [{"text": "English side", "start_pos": 38, "end_pos": 50, "type": "DATASET", "confidence": 0.9415417015552521}]}, {"text": "The distribution of the number of rules over the number of corresponding trees is shown in.", "labels": [], "entities": []}, {"text": "We observe that the majority of rules in our model has less than 150 tree fragments.", "labels": [], "entities": []}, {"text": "Therefore, considering the quadratic complexity in Eq.", "labels": [], "entities": []}, {"text": "11, we punish the rules with more than 150 unique tree fragments with some floor cluster purity to speedup  the training.", "labels": [], "entities": []}, {"text": "Not surprisingly, the rules with a large number of tree fragments are typically those with few stop words as terminals.", "labels": [], "entities": []}, {"text": "For instance, the rule X \u2192< X 1 aX 2 , * > comes with more than 100K trees for the X 1 . Translation results are presented in with character-based BLEU scores using 2 references.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 147, "end_pos": 151, "type": "METRIC", "confidence": 0.9858377575874329}]}, {"text": "Our baseline formally syntax-based models achieved the BLEU score of 43.75, an absolute improvement of 1.6 point improvement over phrase-based models.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 55, "end_pos": 65, "type": "METRIC", "confidence": 0.9808880984783173}]}, {"text": "The improvement is statistically significant with p < 0.01 using the sign-test described by).", "labels": [], "entities": []}, {"text": "Applying the prior derivation model into the syntax-based system, BLEU score is further improved to 44.51, obtained an another absolute improvement of 0.8 point, which is also significantly better than our baseline syntaxbased models (p < 0.05).", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 66, "end_pos": 76, "type": "METRIC", "confidence": 0.9789555072784424}]}], "tableCaptions": [{"text": " Table 1: Distribution of rules over trees", "labels": [], "entities": [{"text": "Distribution of rules", "start_pos": 10, "end_pos": 31, "type": "TASK", "confidence": 0.8687785466512045}]}, {"text": " Table 2: English-to-Chinese BLEU score result on test- set (character-based)", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 29, "end_pos": 39, "type": "METRIC", "confidence": 0.9624113440513611}]}]}