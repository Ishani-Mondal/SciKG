{"title": [{"text": "Modelling and Detecting Decisions in Multi-party Dialogue", "labels": [], "entities": [{"text": "Modelling and Detecting Decisions", "start_pos": 0, "end_pos": 33, "type": "TASK", "confidence": 0.719424694776535}]}], "abstractContent": [{"text": "We describe a process for automatically detecting decision-making sub-dialogues in transcripts of multi-party, human-human meetings.", "labels": [], "entities": []}, {"text": "Extending our previous work on action item identification, we propose a struc-tured approach that takes into account the different roles utterances play in the decision-making process.", "labels": [], "entities": [{"text": "action item identification", "start_pos": 31, "end_pos": 57, "type": "TASK", "confidence": 0.6375188728173574}]}, {"text": "We show that this structured approach outperforms the accuracy achieved by existing decision detection systems based on flat annotations, while enabling the extraction of more fine-grained information that can be used for summarization and reporting.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 54, "end_pos": 62, "type": "METRIC", "confidence": 0.9986973404884338}, {"text": "decision detection", "start_pos": 84, "end_pos": 102, "type": "TASK", "confidence": 0.8458612263202667}, {"text": "summarization and reporting", "start_pos": 222, "end_pos": 249, "type": "TASK", "confidence": 0.6216307580471039}]}], "introductionContent": [{"text": "In collaborative and organized work environments, people share information and make decisions extensively through multi-party conversations, usually in the form of meetings.", "labels": [], "entities": []}, {"text": "When audio or video recordings are made of these meetings, it would be valuable to extract important information, such as the decisions that were made and the trains of reasoning that led to those decisions.", "labels": [], "entities": []}, {"text": "Such a capability would allow work groups to keep track of courses of action that were shelved or rejected, and could allow new team members to get quickly up to speed.", "labels": [], "entities": []}, {"text": "Thanks to the recent availability of substantial meeting corpora-such as the ISL (), ICSI (), and AMI ( ) Meeting Corpora-current research on the structure of decision-making dialogue and its use for automatic decision detection has helped to bring this vision closer to reality).", "labels": [], "entities": [{"text": "AMI ( ) Meeting Corpora-current", "start_pos": 98, "end_pos": 129, "type": "DATASET", "confidence": 0.7525028347969055}, {"text": "decision detection", "start_pos": 210, "end_pos": 228, "type": "TASK", "confidence": 0.703980103135109}]}, {"text": "Our aim here is to further that research by applying a simple notion of dialogue structure to the task of automatically detecting decisions in multiparty dialogue.", "labels": [], "entities": []}, {"text": "A central hypothesis underlying our approach is that this task is best addressed by taking into account the roles that different utterances play in the decision-making process.", "labels": [], "entities": []}, {"text": "Our claim is that this approach facilitates both the detection of regions of discourse where decisions are discussed and adopted, and also the identification of important aspects of the decision discussions themselves, thus opening the way to better and more concise reporting.", "labels": [], "entities": []}, {"text": "In the next section, we describe prior work on related efforts, including our own work on action item detection ().", "labels": [], "entities": [{"text": "action item detection", "start_pos": 90, "end_pos": 111, "type": "TASK", "confidence": 0.6058839956919352}]}, {"text": "Sections 3 and 4 then present our decision annotation scheme, which distinguishes several types of decision-related dialogue acts, and the corpus used as data (in this study a section of the AMI Meeting Corpus).", "labels": [], "entities": [{"text": "AMI Meeting Corpus", "start_pos": 191, "end_pos": 209, "type": "DATASET", "confidence": 0.9007859230041504}]}, {"text": "Next, in Section 5, we describe our experimental methodology, including the basic conception of our classification approach, the features we used in classification, and our evaluation metrics.", "labels": [], "entities": []}, {"text": "Section 6 then presents our results, obtained with a hierarchical classifier that first trains individual sub-classifiers to detect the different types of decision DAs, and then uses a superclassifier to detect decision regions on the basis of patterns of these DAs, achieving an F-score of 58%.", "labels": [], "entities": [{"text": "F-score", "start_pos": 280, "end_pos": 287, "type": "METRIC", "confidence": 0.9993996620178223}]}, {"text": "Finally, Section 7 presents some conclusions and directions for future work.", "labels": [], "entities": []}], "datasetContent": [{"text": "In all cases we perform 17-fold cross-validation, each fold training on 16 meetings and testing on the remaining one.", "labels": [], "entities": []}, {"text": "We can evaluate the performance of our approach at three levels: the accuracy of the sub-classifiers in detecting each of the DDA classes, the accuracy obtained in detecting DDA classes after the output of the sub-classifiers has been corrected by the superclassifier, and the accuracy of the super-classifier in detecting decision sub-dialogues.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 69, "end_pos": 77, "type": "METRIC", "confidence": 0.9967519044876099}, {"text": "accuracy", "start_pos": 143, "end_pos": 151, "type": "METRIC", "confidence": 0.998769223690033}, {"text": "accuracy", "start_pos": 277, "end_pos": 285, "type": "METRIC", "confidence": 0.9978100657463074}]}, {"text": "For the DDA identification task (both uncorrected and corrected) we use the same lenient-match metric as, which allows a margin of 20 seconds preceding and following a hypothesized DDA.", "labels": [], "entities": [{"text": "DDA identification task", "start_pos": 8, "end_pos": 31, "type": "TASK", "confidence": 0.9471847613652548}]}, {"text": "We take as reference the results they obtained on detecting their decision-related DAs.", "labels": [], "entities": [{"text": "detecting their decision-related DAs", "start_pos": 50, "end_pos": 86, "type": "TASK", "confidence": 0.5933212116360664}]}, {"text": "For the evaluation of the decision sub-dialogue detection task, we follow ( and use a windowed metric that divides the dialogue into 30-second windows and evaluates on a per window basis.", "labels": [], "entities": [{"text": "decision sub-dialogue detection task", "start_pos": 26, "end_pos": 62, "type": "TASK", "confidence": 0.7331962585449219}]}, {"text": "As a baseline for this task, we compare the performance of our hierarchical approach to a flat classification approach, first using the flat annotations of that only include a single DDA class, and second using our annotations, but for the binary classification of whether an utterance is decision-related or not, without distinguishing among our DDA sub-classes.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3. We can see that the use of the super- classifier to detect decision sub-dialogues gives a  significantly improved performance over the flat ap- proach. This is despite low sub-classifier perfor- mance, especially for the classes with very low fre- quency of occurrence like RR. Precision for decision  sub-dialogue detection improves around 0.5 points  (p < 0.05 on an paired t-test), boosting F-scores to  0.55 (p < 0.05). The drop in recall from 0.96 to  0.91 is not statistically significant.", "labels": [], "entities": [{"text": "Precision", "start_pos": 288, "end_pos": 297, "type": "METRIC", "confidence": 0.9847085475921631}, {"text": "decision  sub-dialogue detection", "start_pos": 302, "end_pos": 334, "type": "TASK", "confidence": 0.6421913603941599}, {"text": "F-scores", "start_pos": 404, "end_pos": 412, "type": "METRIC", "confidence": 0.9951269626617432}, {"text": "recall", "start_pos": 446, "end_pos": 452, "type": "METRIC", "confidence": 0.9994163513183594}]}, {"text": " Table 4: Hierarchical classifier with lexical features and  +/-1-utterance context", "labels": [], "entities": []}, {"text": " Table 5: Hierarchical classifier with uncorrected and corrected results for sub-classifiers, with and w/o class A; lexical,  utterance, and speaker features; +/-1-utt lexical context for I-RP-RR and +/-5-utt lexical context for A.", "labels": [], "entities": []}, {"text": " Table 6: Comparison of F-scores obtained with WCNs  and manual transcriptions", "labels": [], "entities": [{"text": "F-scores", "start_pos": 24, "end_pos": 32, "type": "METRIC", "confidence": 0.890365719795227}]}]}