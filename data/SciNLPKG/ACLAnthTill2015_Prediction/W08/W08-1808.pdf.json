{"title": [{"text": "Evaluation of Automatically Reformulated Questions in Question Series", "labels": [], "entities": [{"text": "Evaluation of Automatically Reformulated Questions in Question Series", "start_pos": 0, "end_pos": 69, "type": "TASK", "confidence": 0.8160565569996834}]}], "abstractContent": [{"text": "Having gold standards allows us to evaluate new methods and approaches against a common benchmark.", "labels": [], "entities": []}, {"text": "In this paper we describe a set of gold standard question re-formulations and associated reformulation guidelines that we have created to support research into automatic interpretation of questions in TREC question series, where questions may refer anaphorically to the target of the series or to answers to previous questions.", "labels": [], "entities": [{"text": "automatic interpretation of questions in TREC question series", "start_pos": 160, "end_pos": 221, "type": "TASK", "confidence": 0.7278454899787903}]}, {"text": "We also assess various string comparison metrics for their utility as evaluation measures of the proximity of an automated system's reformulations to the gold standard.", "labels": [], "entities": []}, {"text": "Finally we show how we have used this approach to assess the question processing capability of our own QA system and to pinpoint areas for improvement .", "labels": [], "entities": [{"text": "question processing", "start_pos": 61, "end_pos": 80, "type": "TASK", "confidence": 0.7851720154285431}]}], "introductionContent": [{"text": "The development of computational systems which can answer natural language questions using large text collections as knowledge sources is widely seen as both intellectually challenging and practically useful.", "labels": [], "entities": []}, {"text": "To stimulate research and development in this area the US National Institute of Standards and Technology (NIST) has organized a shared task evaluation as one track at the annual TExt Retrieval Conference (TREC) since 1999  therein a Big Mac?) each of which was asked in isolation to any of the others.", "labels": [], "entities": [{"text": "TExt Retrieval Conference (TREC)", "start_pos": 178, "end_pos": 210, "type": "TASK", "confidence": 0.7583531041940054}]}, {"text": "However, in an effort to move the challenge towards along term vision of interactive, dialogue-based question answering to support information analysts (), the track introduced the notion of question targets and related question series in TREC2004 (, and this approach to question presentation has remained central in each of the subsequent TRECs.", "labels": [], "entities": [{"text": "question answering", "start_pos": 101, "end_pos": 119, "type": "TASK", "confidence": 0.73577681183815}, {"text": "TREC2004", "start_pos": 239, "end_pos": 247, "type": "DATASET", "confidence": 0.9267622828483582}, {"text": "question presentation", "start_pos": 272, "end_pos": 293, "type": "TASK", "confidence": 0.7574614584445953}]}, {"text": "In this simulated task, questions are grouped into series where each series has a target of a definition associated with it (see).", "labels": [], "entities": []}, {"text": "Each question in the series asks for some information about the target and there is a final \"other\" question which is to be interpreted as \"Provide any other interesting details about the target that has not already been asked for explicitly\".", "labels": [], "entities": []}, {"text": "In this way \"each series is a (limited) abstraction of an information dialogue in which the user is trying to define the target.", "labels": [], "entities": []}, {"text": "The target and earlier questions in a series provide the context for the current question.\".", "labels": [], "entities": []}, {"text": "One consequence of putting questions into series in this way is that questions may not make much sense when removed from the context their series provides.", "labels": [], "entities": []}, {"text": "For example, the question When was he born?", "labels": [], "entities": []}, {"text": "cannot be sensibly interpreted without knowledge of the antecedent of he provided by the context (target or prior questions).", "labels": [], "entities": []}, {"text": "Interpreting questions in question series, therefore, becomes a critical component within a QA systems.", "labels": [], "entities": [{"text": "Interpreting questions in question series", "start_pos": 0, "end_pos": 41, "type": "TASK", "confidence": 0.8225231170654297}]}, {"text": "Many QA systems have an initial document retrieval stage that takes the question and derives a query from it which is then passed to a search engine whose task is to retrieve candidate answering bearing documents for processing by the rest of the system.", "labels": [], "entities": []}, {"text": "Clearly a question such as When was he born? is unlikely to retrieve documents rele- the Prophet Mohammad?", "labels": [], "entities": []}, {"text": "Q136.4 Who was the third Imam of Shiite Muslims?", "labels": [], "entities": [{"text": "Q136.4", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.9473239779472351}]}, {"text": "Q136.5 When did he die?: An Example Question Series vant to answering a question about Kafka's date of birth if passed directly to a search engine.", "labels": [], "entities": [{"text": "Q136.5", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.8358883857727051}, {"text": "answering a question about Kafka's date of birth", "start_pos": 60, "end_pos": 108, "type": "TASK", "confidence": 0.7204137146472931}]}, {"text": "This problem can be addressed in a naive way by simply appending the target to every question.", "labels": [], "entities": []}, {"text": "However, this has several disadvantages: (1) in some cases co-reference in a question series is to the answer of a previous question and not to the target, so blindly substituting the target is not appropriate; (2) some approaches to query formulation and to answer extraction from retrieved documents may require syntactically well-formed questions and maybe able to take advantage of the extra information, such as syntactic dependencies, provided in a fully de-referenced, syntactically correct question.", "labels": [], "entities": [{"text": "query formulation", "start_pos": 234, "end_pos": 251, "type": "TASK", "confidence": 0.7793245613574982}, {"text": "answer extraction from retrieved documents", "start_pos": 259, "end_pos": 301, "type": "TASK", "confidence": 0.8595938801765441}]}, {"text": "Thus, it is helpful in general if systems can automatically interpret a question in context so as to resolve co-references appropriately, and indeed most TREC QA systems do this to at least a limited extent as part of their question pre-processing.", "labels": [], "entities": []}, {"text": "Ideally one would like a system to be able to reformulate a question as a human would if they were to reexpress the question so as to make it independent of the context of the preceding portion of the question series.", "labels": [], "entities": []}, {"text": "To support the development of such systems it would useful if there were a collection of \"gold standard\" reformulated questions against which systems' outputs could be compared.", "labels": [], "entities": []}, {"text": "However, to the best of our knowledge no such resource exists.", "labels": [], "entities": []}, {"text": "In this paper we describe the creation of such a corpus of manually reformulated questions, measures we have investigated for comparing system generated reformulations against the gold standard, and experiments we have carried out comparing our TREC system's automatic question reformulator against the gold standard and insights we have obtained therefrom.", "labels": [], "entities": [{"text": "TREC system's automatic question reformulator", "start_pos": 245, "end_pos": 290, "type": "TASK", "confidence": 0.589207132657369}]}], "datasetContent": [{"text": "To assess how close a system's reformulation of a question in a questions series is to the gold standard requires a measure of proximity.", "labels": [], "entities": [{"text": "reformulation of a question in a questions series", "start_pos": 31, "end_pos": 80, "type": "TASK", "confidence": 0.8438787311315536}]}, {"text": "Whatever metric we adopt should have the property that reformulations that are closer to our gold standard reformulations get a higher score.", "labels": [], "entities": []}, {"text": "The closest possible score is achieved by getting an identical string to that of the gold standard.", "labels": [], "entities": []}, {"text": "Following conventional practice we will adopt a metric that gives us a value between 0 and 1, where 1 is highest (i.e. a score of 1 is achieved when the pre-processed reformulation and the gold standard are identical).", "labels": [], "entities": []}, {"text": "Another requirement for the metric is that the ordering of the words in the reformulation is not as important as the content of the reformulation.", "labels": [], "entities": []}, {"text": "We assume this because one key use for reformulated questions in the retrieval of candidate answer bearing documents and the presence of key content terms in a reformulation can help to find answers when it is used as a query, regardless of their order Ordering does still need to betaken into account by the metric but it should alter the scoreless than the content words in the reformulation.", "labels": [], "entities": []}, {"text": "Related to this point, is that we would like reformulations that simply append the target onto the end of the original question to score more highly on average than the original questions on their own, since this is a default strategy followed by many systems that clearly helps in many cases.", "labels": [], "entities": []}, {"text": "These requirement can help to guide metric selection.", "labels": [], "entities": [{"text": "metric selection", "start_pos": 36, "end_pos": 52, "type": "TASK", "confidence": 0.8955956697463989}]}], "tableCaptions": [{"text": " Table 1: Mean scores across the data set for each of the different question sets.", "labels": [], "entities": [{"text": "Mean", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.99425208568573}]}, {"text": " Table 2: Results for Unigram weighting", "labels": [], "entities": [{"text": "Unigram weighting", "start_pos": 22, "end_pos": 39, "type": "TASK", "confidence": 0.7358054518699646}]}, {"text": " Table 3: U:1, B:1, T:0", "labels": [], "entities": []}, {"text": " Table 4: U:1, B:1, T:1", "labels": [], "entities": []}, {"text": " Table 7: How our system compared, U:1,B:0,T:0", "labels": [], "entities": [{"text": "U:1,B:0,T:0", "start_pos": 35, "end_pos": 46, "type": "METRIC", "confidence": 0.917983625616346}]}, {"text": " Table 8: How our system compared, U:2,B:1,T:0", "labels": [], "entities": [{"text": "U:2,B:1,T:0", "start_pos": 35, "end_pos": 46, "type": "METRIC", "confidence": 0.8441294516835894}]}]}