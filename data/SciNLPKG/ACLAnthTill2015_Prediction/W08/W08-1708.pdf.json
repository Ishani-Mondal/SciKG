{"title": [{"text": "Towards Domain-Independent Deep Linguistic Processing: Ensuring Portability and Re-Usability of Lexicalised Grammars", "labels": [], "entities": []}], "abstractContent": [{"text": "In this paper we illustrate and underline the importance of making detailed linguistic information a central part of the process of automatic acquisition of large-scale lexicons as a means for enhancing robust-ness and at the same time ensuring maintainability and re-usability of deep lexi-calised grammars.", "labels": [], "entities": []}, {"text": "Using the error mining techniques proposed in (van Noord, 2004) we show very convincingly that the main hindrance to portability of deep lexicalised grammars to domains other than the ones originally developed in, as well as to ro-bustness of systems using such grammars is low lexical coverage.", "labels": [], "entities": [{"text": "error mining", "start_pos": 10, "end_pos": 22, "type": "TASK", "confidence": 0.7193240970373154}]}, {"text": "To this effect, we develop linguistically-driven methods that use detailed morphosyntactic information to automatically enhance the performance of deep lexicalised grammars maintaining at the same time their usually already achieved high linguistic quality.", "labels": [], "entities": []}], "introductionContent": [{"text": "We focus on enhancing robustness and ensuring maintainability and re-usability fora largescale deep grammar of German (GG;), developed in the framework of Headdriven Phrase Structure Grammar (HPSG).", "labels": [], "entities": [{"text": "Headdriven Phrase Structure Grammar (HPSG)", "start_pos": 155, "end_pos": 197, "type": "TASK", "confidence": 0.47657060623168945}]}, {"text": "Specifically, we show that the incorporation of detailed linguistic information into the process of automatic extension of the lexicon of such a language resource enhances its performance and provides linguistically sound and more informative predictions which bring a bigger benefit for the grammar when employed in practical real-life applications.", "labels": [], "entities": []}, {"text": "In recent years, various techniques and resources have been developed in order to improve robustness of deep grammars for real-life applications in various domains.", "labels": [], "entities": []}, {"text": "Nevertheless, low coverage of such grammars remains the main hindrance to their employment in open domain natural language processing.", "labels": [], "entities": [{"text": "open domain natural language processing", "start_pos": 94, "end_pos": 133, "type": "TASK", "confidence": 0.6181737065315247}]}, {"text": "(), as well as) and () have clearly shown that the majority of parsing failures with large-scale deep grammars are caused by missing or wrong entries in the lexicons accompanying grammars like the aforementioned ones.", "labels": [], "entities": []}, {"text": "Based on these findings, it has become clear that it is crucial to explore and develop efficient methods for automated (Deep) Lexical Acquisition (henceforward (D)LA), the process of automatically recovering missing entries in the lexicons of deep grammars.", "labels": [], "entities": []}, {"text": "Recently, various high-quality DLA approaches have been proposed.", "labels": [], "entities": []}, {"text": "(, as well as (), (van de) and () describe efficient methods towards the task of lexicon acquisition for large-scale deep grammars for English, Dutch and German.", "labels": [], "entities": [{"text": "lexicon acquisition", "start_pos": 81, "end_pos": 100, "type": "TASK", "confidence": 0.7371158003807068}]}, {"text": "They treat DLA as a classification task and make use of various robust and efficient machine learning techniques to perform the acquisition process.", "labels": [], "entities": []}, {"text": "However, it is our claim that to achieve better and more practically useful results, apart from good learning algorithms, we also need to incorporate into the learning process fine-grained linguistic information which deep grammars inherently include and provide for.", "labels": [], "entities": []}, {"text": "As we clearly show in the following, it is not sufficient to only develop and use good and complicated classification algorithms.", "labels": [], "entities": []}, {"text": "We must look at the detailed linguistic information that is already included and provided for by the grammar itself and try to capture and make as much use of it as possible, for this is the information we aim at learning when performing DLA.", "labels": [], "entities": []}, {"text": "In this way, the learning process is facilitated and at the same time it is as much as possible ensured that its outcome be linguistically more informative and, thus, practically more useful.", "labels": [], "entities": []}, {"text": "We use the GG deep grammar for the work we present in this paper because German is a language with rich morphology and free word order, which exhibits a range of interesting linguistic phenomena, a fair number of which are already analysed in the GG.", "labels": [], "entities": [{"text": "GG", "start_pos": 247, "end_pos": 249, "type": "DATASET", "confidence": 0.9539180994033813}]}, {"text": "Thus, the grammar is a valuable linguistic resource since it provides linguistically sound and detailed analyses of these phenomena.", "labels": [], "entities": []}, {"text": "Apart from the interesting syntactic structures, though, the lexical entries in the lexicon of the aforementioned grammar also exhibit a rich and complicated structure and contain various important linguistic constraints.", "labels": [], "entities": []}, {"text": "Based on our claim above, in this paper we show how the information these constraints provide can be captured and used in linguisticallymotivated DLA methods which we propose here.", "labels": [], "entities": []}, {"text": "We then apply our approach on real-life data and observe the impact it has on the the grammar coverage and its practical application.", "labels": [], "entities": []}, {"text": "In this way we try to prove our assumption that the linguistic information we incorporate into our DLA methods is vital for the good performance of the acquisition process and for the maintainability and re-usability of the grammar, as well for its successful practical application.", "labels": [], "entities": []}, {"text": "The remainder of the paper is organised as follows.", "labels": [], "entities": []}, {"text": "In Section 2 we show that low (lexical) coverage is a serious issue for the GG when employed for open domain natural language processing.", "labels": [], "entities": [{"text": "open domain natural language processing", "start_pos": 97, "end_pos": 136, "type": "TASK", "confidence": 0.6357585847377777}]}, {"text": "Section 3 presents the types in the lexical architecture of the GG that are considered to be relevant for the purposes of our experiments.", "labels": [], "entities": []}, {"text": "Section 4 describes the extensive linguistic analysis we perform in order to deal with the linguistic information these types provide and presents the target type inventory for our DLA methods.", "labels": [], "entities": []}, {"text": "Section 5 reports on statistical approaches towards automatic DLA and shows the importance of a good and linguisticallymotivated feature selection.", "labels": [], "entities": []}, {"text": "Section 6 illustrates the practical usage of the proposed DLA methods and their impact on grammar coverage.", "labels": [], "entities": [{"text": "grammar coverage", "start_pos": 90, "end_pos": 106, "type": "TASK", "confidence": 0.8212037980556488}]}, {"text": "Section 7 concludes the paper.", "labels": [], "entities": []}], "datasetContent": [{"text": "For our DLA experiments, we adopted the Maximum Entropy based model described in), which has been applied to the ERG), a widecoverage HPSG grammar for English.", "labels": [], "entities": []}, {"text": "For the proposed prediction model, the probability of a lexical type t given an unknown word and its context c is: where f i (t, c) may encode arbitrary characteristics of the context and \u0398 i is a weighting factor estimated on a training corpus.", "labels": [], "entities": []}, {"text": "Our experiments have been performed with the feature set shown in.", "labels": [], "entities": []}, {"text": "Features the prefix of the unknown word (length is lessor equal 4) the suffix of the unknown word (length is lessor equal 4) the 2 words before and after the unknown word the 2 types before and after the unknown word We have also experimented with prefix and suffix lengths up to 3.", "labels": [], "entities": []}, {"text": "To evaluate the contribution of various features and the overall precision of the ME-based unknown word prediction model, we have done a 10-fold cross validation on the Verbmobil treebanked data.", "labels": [], "entities": [{"text": "precision", "start_pos": 65, "end_pos": 74, "type": "METRIC", "confidence": 0.9993880987167358}, {"text": "ME-based unknown word prediction", "start_pos": 82, "end_pos": 114, "type": "TASK", "confidence": 0.6168484836816788}, {"text": "Verbmobil treebanked data", "start_pos": 169, "end_pos": 194, "type": "DATASET", "confidence": 0.9828465382258097}]}, {"text": "For each fold, words that do not occur in the training partition are assumed to be unknown and are temporarily removed from the lexicon.", "labels": [], "entities": []}, {"text": "For comparison, we have also built a baseline model that always assigns a majority type to each unknown word according to its POS tag.", "labels": [], "entities": []}, {"text": "Specifically, we tag the input sentence with a small POS tagset.", "labels": [], "entities": []}, {"text": "It is then mapped to a most popular lexical type for that POS.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Parsing results with the GG and the test  corpora", "labels": [], "entities": [{"text": "GG", "start_pos": 35, "end_pos": 37, "type": "DATASET", "confidence": 0.7897977828979492}]}, {"text": " Table 3: Expanded atomic lexical types", "labels": [], "entities": [{"text": "Expanded atomic lexical types", "start_pos": 10, "end_pos": 39, "type": "TASK", "confidence": 0.7067097201943398}]}, {"text": " Table 6: Precision of unknown word type predictors", "labels": [], "entities": [{"text": "Precision of unknown word type predictors", "start_pos": 10, "end_pos": 51, "type": "TASK", "confidence": 0.5312376270691553}]}]}