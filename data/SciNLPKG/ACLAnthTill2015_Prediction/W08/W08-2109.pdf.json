{"title": [{"text": "Improving Word Segmentation by Simultaneously Learning Phonotactics", "labels": [], "entities": [{"text": "Improving Word Segmentation", "start_pos": 0, "end_pos": 27, "type": "TASK", "confidence": 0.9016326268513998}]}], "abstractContent": [{"text": "The most accurate unsupervised word seg-mentation systems that are currently available (Brent, 1999; Venkataraman, 2001; Goldwater, 2007) use a simple unigram model of phonotactics.", "labels": [], "entities": []}, {"text": "While this simplifies some of the calculations, it overlooks cues that infant language acquisition researchers have shown to be useful for segmentation (Mattys et al., 1999; Mattys and Jusczyk, 2001).", "labels": [], "entities": []}, {"text": "Here we explore the utility of using bigram and trigram phono-tactic models by enhancing Brent's (1999) MBDP-1 algorithm.", "labels": [], "entities": []}, {"text": "The results show the improved MBDP-Phon model outper-forms other unsupervised word segmenta-tion systems (e.g., Brent, 1999; Venkatara-man, 2001; Goldwater, 2007).", "labels": [], "entities": []}], "introductionContent": [{"text": "How do infants come to identify words in the speech stream?", "labels": [], "entities": []}, {"text": "As adults, we breakup speech into words with such ease that we often think that there are audible pauses between words in the same sentence.", "labels": [], "entities": []}, {"text": "However, unlike some written languages, speech does not have any completely reliable markers for the breaks between words (.", "labels": [], "entities": []}, {"text": "In fact, languages vary on how they signal the ends of words, which makes the task even more daunting.", "labels": [], "entities": []}, {"text": "Adults at least have a lexicon they can use to recognize familiar words, but when an infant is firstborn, they do not have a pre-existing lexicon to consult.", "labels": [], "entities": []}, {"text": "In spite of these challenges, by the age of six months infants can begin to segment words out of speech ().", "labels": [], "entities": []}, {"text": "Here we present an efficient word segmentation system aimed to model how infants accomplish the task.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 29, "end_pos": 46, "type": "TASK", "confidence": 0.7458142638206482}]}, {"text": "While an algorithm that could reliably extract orthographic representations of both novel and familiar words from acoustic data is something we would like to see developed, following earlier researchers, we simplify the problem by using a text that does not contain any word boundary markers.", "labels": [], "entities": []}, {"text": "Hereafter, we use the phrase \"word segmentation\" to mean some process which adds word boundaries to a text that does not contain them.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 30, "end_pos": 47, "type": "TASK", "confidence": 0.7203394025564194}]}, {"text": "This paper's focus is on unsupervised, incremental word segmentation algorithms; i.e., those that do not rely on preexisting knowledge of a particular language, and those that segment the corpus one utterance at a time.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 51, "end_pos": 68, "type": "TASK", "confidence": 0.776488333940506}]}, {"text": "This is in contrast to supervised word segmentation algorithms (e.g.,), which are typically used for segmenting text in documents written in languages that do not put spaces between their words like Chinese.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 34, "end_pos": 51, "type": "TASK", "confidence": 0.7428096830844879}, {"text": "segmenting text in documents written in languages", "start_pos": 101, "end_pos": 150, "type": "TASK", "confidence": 0.8006518398012433}]}, {"text": "(Of course, unsupervised word segmentation algorithms also have this application.)", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 25, "end_pos": 42, "type": "TASK", "confidence": 0.7383750230073929}]}, {"text": "This also differs from batch segmentation algorithms, which process the entire corpus at least once before outputting a segmentation of the corpus.", "labels": [], "entities": [{"text": "batch segmentation", "start_pos": 23, "end_pos": 41, "type": "TASK", "confidence": 0.7512348592281342}]}, {"text": "Unsupervised incremental algorithms are of interest to some psycholinguists and acquisitionists interested in the problem of language learning, as well as theoretical computer scientists who are interested in what unsupervised, incremental models are capable of achieving.", "labels": [], "entities": []}, {"text": "Phonotactic patterns are the rules that determine what sequences of phonemes or allophones are allowable within words.", "labels": [], "entities": []}, {"text": "Learning the phonotactic patterns of a language is usually modeled separately from word segmentation; e.g., current phonotactic learners such as Coleman and,, or Hayes and Wilson (2008) are given word-sized units as input.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 83, "end_pos": 100, "type": "TASK", "confidence": 0.7152473479509354}]}, {"text": "However, infants appear to simultaneously learn which phoneme combinations are allowable within words and how to extract words from the input.", "labels": [], "entities": []}, {"text": "It is reasonable that the two processes feed into one another, and when infants acquire a critical mass of phonotactic knowledge, they use it to make judgements about what phoneme sequences can occur within versus across word boundaries).", "labels": [], "entities": []}, {"text": "We use this insight, also suggested by and recently utilized by in a different manner, to enhance Brent's (1999) model MBDP-1, and significantly increase segmentation accuracy.", "labels": [], "entities": [{"text": "MBDP-1", "start_pos": 119, "end_pos": 125, "type": "DATASET", "confidence": 0.654253363609314}, {"text": "segmentation", "start_pos": 154, "end_pos": 166, "type": "TASK", "confidence": 0.9741876125335693}, {"text": "accuracy", "start_pos": 167, "end_pos": 175, "type": "METRIC", "confidence": 0.9310584664344788}]}, {"text": "We call this modified segmentation model MBDP-Phon.", "labels": [], "entities": [{"text": "MBDP-Phon", "start_pos": 41, "end_pos": 50, "type": "DATASET", "confidence": 0.8648829460144043}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Precision statistics for MBDP-Phon- Bigrams, Goldwater, and Venkataraman on both  corpora over 500-utterance blocks.", "labels": [], "entities": [{"text": "Precision", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9890792965888977}, {"text": "MBDP-Phon- Bigrams", "start_pos": 35, "end_pos": 53, "type": "DATASET", "confidence": 0.8136611183484396}]}, {"text": " Table 2: Recall statistics for MBDP-Phon- Bigrams, Goldwater, and Venkataraman on both  corpora over 500-utterance blocks.", "labels": [], "entities": [{"text": "MBDP-Phon- Bigrams", "start_pos": 32, "end_pos": 50, "type": "DATASET", "confidence": 0.8258354465166727}]}, {"text": " Table 3: Lexical precision statistics for MBDP- Phon-Bigrams, Goldwater, and Venkataraman on  both corpora over 500-utterance blocks.", "labels": [], "entities": [{"text": "precision", "start_pos": 18, "end_pos": 27, "type": "METRIC", "confidence": 0.9404898285865784}, {"text": "MBDP- Phon-Bigrams", "start_pos": 43, "end_pos": 61, "type": "DATASET", "confidence": 0.8115148544311523}]}]}