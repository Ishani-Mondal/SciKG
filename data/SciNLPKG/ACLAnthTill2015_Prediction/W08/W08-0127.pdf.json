{"title": [{"text": "An Evaluation Understudy for Dialogue Coherence Models", "labels": [], "entities": []}], "abstractContent": [{"text": "Evaluating a dialogue system is seen as a major challenge within the dialogue research community.", "labels": [], "entities": []}, {"text": "Due to the very nature of the task, most of the evaluation methods need a substantial amount of human involvement.", "labels": [], "entities": []}, {"text": "Following the tradition in machine translation, summarization and discourse coherence mod-eling, we introduce the the idea of evaluation understudy for dialogue coherence models.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 27, "end_pos": 46, "type": "TASK", "confidence": 0.7309260070323944}, {"text": "summarization", "start_pos": 48, "end_pos": 61, "type": "TASK", "confidence": 0.9828039407730103}]}, {"text": "Following (Lapata, 2006), we use the information ordering task as a testbed for evaluating dialogue coherence models.", "labels": [], "entities": [{"text": "information ordering task", "start_pos": 37, "end_pos": 62, "type": "TASK", "confidence": 0.7862481077512106}]}, {"text": "This paper reports findings about the reliability of the information ordering task as applied to dialogues.", "labels": [], "entities": [{"text": "information ordering task", "start_pos": 57, "end_pos": 82, "type": "TASK", "confidence": 0.8060750762621561}]}, {"text": "We find that simple n-gram co-occurrence statistics similar in spirit to BLEU (Papineni et al., 2001) correlate very well with human judgments for dialogue coherence.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 73, "end_pos": 77, "type": "METRIC", "confidence": 0.9940553903579712}]}], "introductionContent": [{"text": "In computer science or any other research field, simply building a system that accomplishes a certain goal is not enough.", "labels": [], "entities": []}, {"text": "It needs to be thoroughly evaluated.", "labels": [], "entities": []}, {"text": "One might want to evaluate the system just to see to what degree the goal is being accomplished or to compare two or more systems with one another.", "labels": [], "entities": []}, {"text": "Evaluation can also lead to understanding the shortcomings of the system and the reasons for these.", "labels": [], "entities": []}, {"text": "Finally the evaluation results can be used as feedback in improving the system.", "labels": [], "entities": []}, {"text": "The best way to evaluate a novel algorithm or a model fora system that is designed to aid humans in processing natural language would be to employ it in areal system and allow users to interact with it.", "labels": [], "entities": []}, {"text": "The data collected by this process can then be used for evaluation.", "labels": [], "entities": []}, {"text": "Sometimes this data needs further analysis -which may include annotations, collecting subjective judgments from humans, etc.", "labels": [], "entities": []}, {"text": "Since human judgments tend to vary, we may need to employ multiple judges.", "labels": [], "entities": []}, {"text": "These are some of the reasons why evaluation is time consuming, costly and sometimes prohibitively expensive.", "labels": [], "entities": []}, {"text": "Furthermore, if the system being developed contains a machine learning component, the problem of costly evaluation becomes even more serious.", "labels": [], "entities": []}, {"text": "Machine learning components often optimize certain free parameters by using evaluation results on heldout data or by using n-fold cross-validation.", "labels": [], "entities": []}, {"text": "Evaluation results can also help with feature selection.", "labels": [], "entities": [{"text": "feature selection", "start_pos": 38, "end_pos": 55, "type": "TASK", "confidence": 0.7947572767734528}]}, {"text": "This need for repeated evaluation can forbid the use of data-driven machine learning components.", "labels": [], "entities": []}, {"text": "For these reasons, using an automatic evaluation measure as an understudy is quickly becoming a common practice in natural language processing tasks.", "labels": [], "entities": [{"text": "natural language processing tasks", "start_pos": 115, "end_pos": 148, "type": "TASK", "confidence": 0.6975722163915634}]}, {"text": "The general idea is to find an automatic evaluation metric that correlates very well with human judgments.", "labels": [], "entities": []}, {"text": "This allows developers to use the automatic metric as a stand-in for human evaluation.", "labels": [], "entities": []}, {"text": "Although it cannot replace the finesse of human evaluation, it can provide a crude idea of progress which can later be validated.", "labels": [], "entities": []}, {"text": "e.g. BLEU () for machine translation, ROUGE) for summarization.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 5, "end_pos": 9, "type": "METRIC", "confidence": 0.998441755771637}, {"text": "machine translation", "start_pos": 17, "end_pos": 36, "type": "TASK", "confidence": 0.7699749171733856}, {"text": "ROUGE", "start_pos": 38, "end_pos": 43, "type": "METRIC", "confidence": 0.9911509156227112}, {"text": "summarization", "start_pos": 49, "end_pos": 62, "type": "TASK", "confidence": 0.9798014163970947}]}, {"text": "Recently, the discourse coherence modeling community has started using the information ordering task as a testbed to test their discourse coherence models (). has proposed an au-tomatic evaluation measure for the information ordering task.", "labels": [], "entities": [{"text": "discourse coherence modeling", "start_pos": 14, "end_pos": 42, "type": "TASK", "confidence": 0.6808306773503622}, {"text": "information ordering task", "start_pos": 213, "end_pos": 238, "type": "TASK", "confidence": 0.7598020335038503}]}, {"text": "We propose to use the same task as a testbed for dialogue coherence modeling.", "labels": [], "entities": [{"text": "dialogue coherence modeling", "start_pos": 49, "end_pos": 76, "type": "TASK", "confidence": 0.7655726472536722}]}, {"text": "We evaluate the reliability of the information ordering task as applied to dialogues and propose an evaluation understudy for dialogue coherence models.", "labels": [], "entities": [{"text": "information ordering task", "start_pos": 35, "end_pos": 60, "type": "TASK", "confidence": 0.7755507330099741}]}, {"text": "In the next section, we look at related work in evaluation of dialogue systems.", "labels": [], "entities": []}, {"text": "Section 3 summarizes the information ordering task and Lapata's (2006) findings.", "labels": [], "entities": [{"text": "information ordering task", "start_pos": 25, "end_pos": 50, "type": "TASK", "confidence": 0.8974116245905558}]}, {"text": "It is followed by the details of the experiments we carried out and our observations.", "labels": [], "entities": []}, {"text": "We conclude with a summary future work directions.", "labels": [], "entities": []}], "datasetContent": [{"text": "For our experiments we used segments drawn from 9 dialogues.", "labels": [], "entities": []}, {"text": "These dialogues were two-party humanhuman dialogues.", "labels": [], "entities": []}, {"text": "To ensure applicability of our results over different types of dialogue, we chose these 9 dialogues from different sources.", "labels": [], "entities": []}, {"text": "Three of these were excerpts from role-play dialogues involving negotiations which were originally collected fora simulation training scenario ().", "labels": [], "entities": []}, {"text": "Three are from SRI's Amex Travel Agent data which are task-oriented dialogues about air travel planning (.", "labels": [], "entities": [{"text": "SRI's Amex Travel Agent data", "start_pos": 15, "end_pos": 43, "type": "DATASET", "confidence": 0.8518107483784357}, {"text": "air travel planning", "start_pos": 84, "end_pos": 103, "type": "TASK", "confidence": 0.672797441482544}]}, {"text": "The rest of the dialogues are scripts from popular television shows.", "labels": [], "entities": []}, {"text": "an example from the air-travel domain.", "labels": [], "entities": []}, {"text": "Each excerpt drawn was 10 turns long with turns strictly alternating between the two speakers.", "labels": [], "entities": []}, {"text": "Following the experimental design of (Lapata, 2006) we created random permutations for these dialogue segments.", "labels": [], "entities": []}, {"text": "We constrained our permutations so that the permutations always start with the same speaker as the original dialogue and turns strictly alternate between the speakers.", "labels": [], "entities": []}, {"text": "With these constraints there are still 5!", "labels": [], "entities": []}, {"text": "= 14400 possible permutations per dialogue.", "labels": [], "entities": []}, {"text": "We selected 3 random permutations for each of the 9 dialogues.", "labels": [], "entities": []}, {"text": "In all, we have a total of 27 dialogue permutations.", "labels": [], "entities": []}, {"text": "They are arranged in 3 sets, each set containing a permutation for all 9 dialogues.", "labels": [], "entities": []}, {"text": "We ensured that not all permutations in a given set are particularly very good or very bad.", "labels": [], "entities": []}, {"text": "We used Kendall's \u03c4 to balance the permutations across For more on the relationship between b 2 , b 3 and \u03c4 see row 3,4 of table 1 and.", "labels": [], "entities": []}, {"text": "the given set as well as across the given dialogue.", "labels": [], "entities": []}, {"text": "Unlike Lapata (2006) who chose to remove the pronouns and discourse connectives, we decided not do any pre-processing on the text like removing disfluencies or removing cohesive devices such as anaphora, ellipsis, discourse connectives, etc.", "labels": [], "entities": []}, {"text": "One of the reason is such pre-processing if done manually defeats the purpose of removing humans from the evaluation procedure.", "labels": [], "entities": []}, {"text": "Moreover it is very difficult to remove certain cohesive devices such as discourse deixis without affecting the coherence level of the original dialogues.", "labels": [], "entities": []}, {"text": "In our first experiment, we divided a total of 9 human judges among the 3 sets (3 judges per set).", "labels": [], "entities": []}, {"text": "Each judge was presented with 9 dialogue permutations.", "labels": [], "entities": []}, {"text": "They were asked to assign a single coherence rating for each dialogue permutation.", "labels": [], "entities": []}, {"text": "The ratings were on a scale of 1 to 7, with 1 being very incoherent and 7 being perfectly coherent.", "labels": [], "entities": []}, {"text": "We did not provide any additional instructions or examples of scale as we wanted to capture the intuitive idea of coherence from our judges.", "labels": [], "entities": []}, {"text": "Within each set the dialogue permutations were presented in random order.", "labels": [], "entities": []}, {"text": "We compute the inter-rater agreement by using Pearson's correlation analysis.", "labels": [], "entities": []}, {"text": "We correlate the ratings given by each judge with the average ratings given by the judges who were assigned the same set.", "labels": [], "entities": []}, {"text": "For inter-rater agreement we report the average of 9 such correlations which is 0.73 (std dev = 0.07).", "labels": [], "entities": []}, {"text": "Artstein and have argued that Krippendorff's \u03b1 can be used for interrater agreement with interval scales like the one we have.", "labels": [], "entities": []}, {"text": "In our case for the three sets \u03b1 values were 0.49, 0.58, 0.64.", "labels": [], "entities": []}, {"text": "These moderate values of alpha indicate that the task of judging coherence is indeed a difficult task, especially when detailed instructions or examples of scales are not given.", "labels": [], "entities": [{"text": "alpha", "start_pos": 25, "end_pos": 30, "type": "METRIC", "confidence": 0.9547027945518494}]}, {"text": "In order to assess whether Kendall's \u03c4 can be used as an automatic measure of dialogue coherence, we perform a correlation analysis of \u03c4 values against the average ratings by human judges.", "labels": [], "entities": []}, {"text": "The Pearson's correlation coefficient is 0.35 and it is statistically not significant (P=0.07).", "labels": [], "entities": [{"text": "Pearson's correlation coefficient", "start_pos": 4, "end_pos": 37, "type": "METRIC", "confidence": 0.9394457787275314}, {"text": "P", "start_pos": 87, "end_pos": 88, "type": "METRIC", "confidence": 0.9756676554679871}]}, {"text": "shows the relationship between coherence judgments and \u03c4 values.", "labels": [], "entities": []}, {"text": "This experiment fails to support the suitability  We also analyzed the correlation of human judgments against simple n-gram statistics, specifically shows the relationship between human judgments and the average of fraction of bigrams and fraction of trigrams that were preserved in the permutation.", "labels": [], "entities": []}, {"text": "The Pearson's correlation coefficient is 0.62 and it is statistically significant (P<0.01).", "labels": [], "entities": [{"text": "Pearson's correlation coefficient", "start_pos": 4, "end_pos": 37, "type": "METRIC", "confidence": 0.9282212853431702}, {"text": "P", "start_pos": 83, "end_pos": 84, "type": "METRIC", "confidence": 0.9771648049354553}]}, {"text": "Since human judges found it relatively hard to assign a single rating to a dialogue permutation, we decided to repeat experiment 1 with some modifications.", "labels": [], "entities": []}, {"text": "In our second experiment we asked the judges to provide coherence ratings at every turn, based on the dialogue that preceded that turn.", "labels": [], "entities": []}, {"text": "The dialogue permutations were presented to the judges through a web interface in an incremental fashion turn by turn as they rated each turn for coherence (see the appendix for the screenshot of this interface).", "labels": [], "entities": []}, {"text": "We used a scale from 1 to 5 with 1 being completely incoherent and 5 as perfectly coherent.", "labels": [], "entities": []}, {"text": "3 A total of 11 judges participated in this experiment with the first set being judged by 5 judges and the remaining two sets by 3 judges each.", "labels": [], "entities": []}, {"text": "For the rest of the analysis, we use the average coherence rating from all turns as a coherence rating for the dialogue permutation.", "labels": [], "entities": []}, {"text": "We performed the inter-rater agreement analysis as in experiment 1.", "labels": [], "entities": []}, {"text": "The average of 11 correlations is 0.83 (std dev = 0.09).", "labels": [], "entities": [{"text": "correlations", "start_pos": 18, "end_pos": 30, "type": "METRIC", "confidence": 0.9470632672309875}]}, {"text": "Although the correlation has improved, Krippendorff's \u03b1 values for the three sets are 0.49, 0.35, 0.63.", "labels": [], "entities": []}, {"text": "This shows that coherence rating is still a hard task even when judged turn by turn.", "labels": [], "entities": []}, {"text": "We assessed the relationship between the average coherence rating for dialogue permutations with Kendall's \u03c4 (see).", "labels": [], "entities": []}, {"text": "The Pearson's correlation coefficient is 0.33 and is statistically not significant (P=0.09).", "labels": [], "entities": [{"text": "Pearson's correlation coefficient", "start_pos": 4, "end_pos": 37, "type": "METRIC", "confidence": 0.9352405369281769}, {"text": "P", "start_pos": 84, "end_pos": 85, "type": "METRIC", "confidence": 0.9752309322357178}]}, {"text": "shows high correlation of average coherence ratings with the fraction of bigrams and trigrams that were preserved in permutation.", "labels": [], "entities": []}, {"text": "The Pearson's correlation coefficient is 0.75 and is statistically significant (P<0.01).", "labels": [], "entities": [{"text": "Pearson's correlation coefficient", "start_pos": 4, "end_pos": 37, "type": "METRIC", "confidence": 0.9284040033817291}, {"text": "P", "start_pos": 80, "end_pos": 81, "type": "METRIC", "confidence": 0.972908079624176}]}, {"text": "Results of both experiments suggest that, (b 2 + b 3 ) /2 correlates very well with human judgments and can be used for evaluating information ordering when applied to dialogues.", "labels": [], "entities": [{"text": "information ordering", "start_pos": 131, "end_pos": 151, "type": "TASK", "confidence": 0.7102078944444656}]}, {"text": "We wanted to know whether information ordering as applied to dialogues is a valid task or not.", "labels": [], "entities": [{"text": "information ordering", "start_pos": 26, "end_pos": 46, "type": "TASK", "confidence": 0.7705352604389191}]}, {"text": "In this experiment we seek to establish a higher baseline for  the task of information ordering in dialogues.", "labels": [], "entities": [{"text": "information ordering in dialogues", "start_pos": 75, "end_pos": 108, "type": "TASK", "confidence": 0.8114820569753647}]}, {"text": "We presented the dialogue permutations to our human judges and asked them to reorder the turns so that the resulting order is as coherent as possible.", "labels": [], "entities": []}, {"text": "All 11 judges who participated in experiment 2 also participated in this experiment.", "labels": [], "entities": []}, {"text": "They were presented with a drag and drop interface over the web that allowed them to reorder the dialogue permutations.", "labels": [], "entities": []}, {"text": "The reordering was constrained to keep the first speaker of the reordering same as that of the original dialogue and the re-orderings must have strictly alternating turns.", "labels": [], "entities": []}, {"text": "We computed the Kendall's \u03c4 and fraction of bigrams and trigrams (b 2 + b 3 ) /2 for these re-orderings.", "labels": [], "entities": [{"text": "Kendall's \u03c4", "start_pos": 16, "end_pos": 27, "type": "METRIC", "confidence": 0.6430668234825134}]}, {"text": "There were a total of 11 \u00d7 9 = 99 reordered dialogue permutations.", "labels": [], "entities": []}, {"text": "Humans achieve high values for the reordering task.", "labels": [], "entities": []}, {"text": "For Kendall's \u03c4 , the mean of the reordered dialogues is 0.82 (std dev = 0.25) and for (b 2 + b 3 ) /2, the mean is 0.71 (std dev = 0.28).", "labels": [], "entities": []}, {"text": "These values establish an upper baseline for the information ordering task.", "labels": [], "entities": [{"text": "information ordering task", "start_pos": 49, "end_pos": 74, "type": "TASK", "confidence": 0.8589063286781311}]}, {"text": "These can be compared against the random baseline.", "labels": [], "entities": []}, {"text": "For \u03c4 random performance is 0.02 4 and for (b 2 + b 3 ) /2 it is 0.11.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Examples of observed sequences and their re- spective b 2 , b 3 & \u03c4 values. Here the reference sequence  is [0,1,2,3,4,5,6,7,8,9].", "labels": [], "entities": []}]}