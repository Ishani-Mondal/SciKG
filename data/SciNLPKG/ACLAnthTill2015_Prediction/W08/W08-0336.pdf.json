{"title": [{"text": "Optimizing Chinese Word Segmentation for Machine Translation Performance", "labels": [], "entities": [{"text": "Optimizing Chinese Word Segmentation", "start_pos": 0, "end_pos": 36, "type": "TASK", "confidence": 0.8349814414978027}, {"text": "Machine Translation", "start_pos": 41, "end_pos": 60, "type": "TASK", "confidence": 0.797407329082489}]}], "abstractContent": [{"text": "Previous work has shown that Chinese word seg-mentation is useful for machine translation to En-glish, yet the way different segmentation strategies affect MT is still poorly understood.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 70, "end_pos": 89, "type": "TASK", "confidence": 0.7580929696559906}, {"text": "MT", "start_pos": 156, "end_pos": 158, "type": "TASK", "confidence": 0.9914501309394836}]}, {"text": "In this paper , we demonstrate that optimizing segmentation for an existing segmentation standard does not always yield better MT performance.", "labels": [], "entities": [{"text": "MT", "start_pos": 127, "end_pos": 129, "type": "TASK", "confidence": 0.9921095967292786}]}, {"text": "We find that other factors such as segmentation consistency and granularity of Chinese \"words\" can be more important for machine translation.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 121, "end_pos": 140, "type": "TASK", "confidence": 0.7733111381530762}]}, {"text": "Based on these findings , we implement methods inside a conditional random field segmenter that directly optimize seg-mentation granularity with respect to the MT task, providing an improvement of 0.73 BLEU.", "labels": [], "entities": [{"text": "MT task", "start_pos": 160, "end_pos": 167, "type": "TASK", "confidence": 0.880387008190155}, {"text": "BLEU", "start_pos": 202, "end_pos": 206, "type": "METRIC", "confidence": 0.998806357383728}]}, {"text": "We also show that improving segmentation consistency using external lexicon and proper noun features yields a 0.32 BLEU increase.", "labels": [], "entities": [{"text": "segmentation consistency", "start_pos": 28, "end_pos": 52, "type": "TASK", "confidence": 0.7031654417514801}, {"text": "BLEU", "start_pos": 115, "end_pos": 119, "type": "METRIC", "confidence": 0.9992716908454895}]}], "introductionContent": [{"text": "Word segmentation is considered an important first step for Chinese natural language processing tasks, because Chinese words can be composed of multiple characters but with no space appearing between words.", "labels": [], "entities": [{"text": "Word segmentation", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.7074677497148514}, {"text": "Chinese natural language processing tasks", "start_pos": 60, "end_pos": 101, "type": "TASK", "confidence": 0.6810204744338989}]}, {"text": "Almost all tasks could be expected to benefit by treating the character sequence \"Us\" together, with the meaning smallpox, rather than dealing with the individual characters \"U\" (sky) and \"s\" (flower).", "labels": [], "entities": []}, {"text": "Without a standardized notion of a word, traditionally, the task of Chinese word segmentation starts from designing a segmentation standard based on linguistic and task intuitions, and then aiming to building segmenters that output words that conform to the standard.", "labels": [], "entities": [{"text": "Chinese word segmentation", "start_pos": 68, "end_pos": 93, "type": "TASK", "confidence": 0.651127835114797}]}, {"text": "One widely used standard is the Penn Chinese Treebank (CTB) Segmentation Standard (.", "labels": [], "entities": [{"text": "Penn Chinese Treebank (CTB) Segmentation Standard", "start_pos": 32, "end_pos": 81, "type": "DATASET", "confidence": 0.9473338350653648}]}, {"text": "It has been recognized that different NLP applications have different needs for segmentation.", "labels": [], "entities": [{"text": "segmentation", "start_pos": 80, "end_pos": 92, "type": "TASK", "confidence": 0.9673293828964233}]}, {"text": "Chinese information retrieval (IR) systems benefit from a segmentation that breaks compound words into shorter \"words\" (), paralleling the IR gains from compound splitting in languages like German (), whereas automatic speech recognition (ASR) systems prefer having longer words in the speech lexicon ().", "labels": [], "entities": [{"text": "Chinese information retrieval (IR)", "start_pos": 0, "end_pos": 34, "type": "TASK", "confidence": 0.7883606354395548}, {"text": "automatic speech recognition (ASR)", "start_pos": 209, "end_pos": 243, "type": "TASK", "confidence": 0.8111320734024048}]}, {"text": "However, despite a decade of very intense work on Chinese to English machine translation (MT), the way in which Chinese word segmentation affects MT performance is very poorly understood.", "labels": [], "entities": [{"text": "Chinese to English machine translation (MT)", "start_pos": 50, "end_pos": 93, "type": "TASK", "confidence": 0.7244449444115162}, {"text": "Chinese word segmentation", "start_pos": 112, "end_pos": 137, "type": "TASK", "confidence": 0.6624578535556793}, {"text": "MT", "start_pos": 146, "end_pos": 148, "type": "TASK", "confidence": 0.9914211630821228}]}, {"text": "With current statistical phrase-based MT systems, one might hypothesize that segmenting into small chunks, including perhaps even working with individual characters would be optimal.", "labels": [], "entities": [{"text": "MT", "start_pos": 38, "end_pos": 40, "type": "TASK", "confidence": 0.860478401184082}]}, {"text": "This is because the role of a phrase table is to build domain and application appropriate larger chunks that are semantically coherent in the translation process.", "labels": [], "entities": []}, {"text": "For example, even if the word for smallpox is treated as two one-character words, they can still appear in a phrase like \"U s\u2192smallpox\", so that smallpox will still be a candidate translation when the system translates \"U\" \"s\".", "labels": [], "entities": []}, {"text": "Nevertheless, show that an MT system with a word segmenter outperforms a system working with individual characters in an alignment template approach.", "labels": [], "entities": [{"text": "MT", "start_pos": 27, "end_pos": 29, "type": "TASK", "confidence": 0.9910975694656372}, {"text": "word segmenter", "start_pos": 44, "end_pos": 58, "type": "TASK", "confidence": 0.7135260999202728}]}, {"text": "On different language pairs, ( and) showed that data-driven methods for splitting and preprocessing can improve Arabic-English and German-English MT.", "labels": [], "entities": [{"text": "MT", "start_pos": 146, "end_pos": 148, "type": "TASK", "confidence": 0.7214106321334839}]}, {"text": "Beyond this, there has been no finer-grained analysis of what style and size of word segmentation is optimal for MT.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 80, "end_pos": 97, "type": "TASK", "confidence": 0.7281835079193115}, {"text": "MT", "start_pos": 113, "end_pos": 115, "type": "TASK", "confidence": 0.9941806197166443}]}, {"text": "Moreover, most discussion of segmentation for other tasks relates to the size units to identify in the segmentation standard: whether to join or split noun compounds, for instance.", "labels": [], "entities": []}, {"text": "People generally assume that improvements in a system's word segmentation accuracy will be monotonically reflected in overall system performance.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 56, "end_pos": 73, "type": "TASK", "confidence": 0.6825003772974014}, {"text": "accuracy", "start_pos": 74, "end_pos": 82, "type": "METRIC", "confidence": 0.7114222049713135}]}, {"text": "This is the assumption that justifies the concerted recent work on the independent task of Chinese word segmentation evaluation at SIGHAN and other venues.", "labels": [], "entities": [{"text": "Chinese word segmentation evaluation", "start_pos": 91, "end_pos": 127, "type": "TASK", "confidence": 0.7023705169558525}]}, {"text": "However, we show that this assumption is false: aspects of segmenters other than error rate are more critical to their performance when embedded in an MT system.", "labels": [], "entities": [{"text": "error rate", "start_pos": 81, "end_pos": 91, "type": "METRIC", "confidence": 0.9452447593212128}, {"text": "MT", "start_pos": 151, "end_pos": 153, "type": "TASK", "confidence": 0.9521260857582092}]}, {"text": "Unless these issues are attended to, simple baseline segmenters can be more effective inside an MT system than more complex machine learning based models, with much lower word segmentation error rate.", "labels": [], "entities": [{"text": "MT", "start_pos": 96, "end_pos": 98, "type": "TASK", "confidence": 0.9646379351615906}, {"text": "word segmentation error rate", "start_pos": 171, "end_pos": 199, "type": "TASK", "confidence": 0.686767578125}]}, {"text": "In this paper, we show that even having a basic word segmenter helps MT performance, and we analyze why building an MT system over individual characters doesn't function as well.", "labels": [], "entities": [{"text": "word segmenter", "start_pos": 48, "end_pos": 62, "type": "TASK", "confidence": 0.7462455034255981}, {"text": "MT", "start_pos": 69, "end_pos": 71, "type": "TASK", "confidence": 0.9956464171409607}, {"text": "MT", "start_pos": 116, "end_pos": 118, "type": "TASK", "confidence": 0.9597781896591187}]}, {"text": "Based on an analysis of baseline MT results, we pin down four issues of word segmentation that can be improved to get better MT performance.", "labels": [], "entities": [{"text": "MT", "start_pos": 33, "end_pos": 35, "type": "TASK", "confidence": 0.969785213470459}, {"text": "word segmentation", "start_pos": 72, "end_pos": 89, "type": "TASK", "confidence": 0.7356966733932495}, {"text": "MT", "start_pos": 125, "end_pos": 127, "type": "TASK", "confidence": 0.9919237494468689}]}, {"text": "(i) While a feature-based segmenter, like a support vector machine or conditional random field (CRF) model, may have very good aggregate performance, inconsistent context-specific segmentation decisions can be quite harmful to MT system performance.", "labels": [], "entities": [{"text": "MT", "start_pos": 227, "end_pos": 229, "type": "TASK", "confidence": 0.9879332184791565}]}, {"text": "(ii) A perceived strength of feature-based systems is that they can generate out-of-vocabulary (OOV) words, but these can hurt MT performance, when they could have been split into subparts from which the meaning of the whole can be roughly compositionally derived.", "labels": [], "entities": [{"text": "MT", "start_pos": 127, "end_pos": 129, "type": "TASK", "confidence": 0.9877282381057739}]}, {"text": "(iii) Conversely, splitting OOV words into noncompositional subparts can be very harmful to an MT system: it is better to produce such OOV items than to split them into unrelated character sequences that are known to the system.", "labels": [], "entities": [{"text": "MT", "start_pos": 95, "end_pos": 97, "type": "TASK", "confidence": 0.9816566109657288}]}, {"text": "One big source of such OOV words is named entities.", "labels": [], "entities": []}, {"text": "(iv) Since the optimal granularity of words for phrase-based MT is unknown, we can benefit from a model which provides a knob for adjusting average word size.", "labels": [], "entities": [{"text": "MT", "start_pos": 61, "end_pos": 63, "type": "TASK", "confidence": 0.8173256516456604}]}, {"text": "We build several different models to address these issues and to improve segmentation for the benefit of MT.", "labels": [], "entities": [{"text": "segmentation", "start_pos": 73, "end_pos": 85, "type": "TASK", "confidence": 0.9752784371376038}, {"text": "MT", "start_pos": 105, "end_pos": 107, "type": "TASK", "confidence": 0.97929847240448}]}, {"text": "First, we emphasize lexicon-based features in a feature-based sequence classifier to deal with segmentation inconsistency and over-generating OOV words.", "labels": [], "entities": []}, {"text": "Having lexicon-based features reduced the MT training lexicon by 29.5%, reduced the MT test data OOV rate by 34.1%, and led to a 0.38 BLEU point gain on the test data (MT05).", "labels": [], "entities": [{"text": "MT training", "start_pos": 42, "end_pos": 53, "type": "TASK", "confidence": 0.8260792195796967}, {"text": "MT test data OOV rate", "start_pos": 84, "end_pos": 105, "type": "METRIC", "confidence": 0.7669912695884704}, {"text": "BLEU point gain", "start_pos": 134, "end_pos": 149, "type": "METRIC", "confidence": 0.9750533302625021}, {"text": "MT05", "start_pos": 168, "end_pos": 172, "type": "DATASET", "confidence": 0.7155115008354187}]}, {"text": "Second, we extend the CRF label set of our CRF segmenter to identify proper nouns.", "labels": [], "entities": []}, {"text": "This gives 3.3% relative improvement on the OOV recall rate, and a 0.32 improvement in BLEU.", "labels": [], "entities": [{"text": "OOV", "start_pos": 44, "end_pos": 47, "type": "METRIC", "confidence": 0.9540002346038818}, {"text": "recall rate", "start_pos": 48, "end_pos": 59, "type": "METRIC", "confidence": 0.9582330882549286}, {"text": "BLEU", "start_pos": 87, "end_pos": 91, "type": "METRIC", "confidence": 0.998932421207428}]}, {"text": "Finally, we tune the CRF model to generate shorter or longer words to directly optimize the performance of MT.", "labels": [], "entities": [{"text": "MT", "start_pos": 107, "end_pos": 109, "type": "TASK", "confidence": 0.9718548655509949}]}, {"text": "For MT, we found that it is preferred to have words slightly shorter than the CTB standard.", "labels": [], "entities": [{"text": "MT", "start_pos": 4, "end_pos": 6, "type": "TASK", "confidence": 0.9847928285598755}, {"text": "CTB standard", "start_pos": 78, "end_pos": 90, "type": "DATASET", "confidence": 0.9466436505317688}]}, {"text": "The paper is organized as follows: we describe the experimental settings for the segmentation task and the task in Section 2.", "labels": [], "entities": [{"text": "segmentation task", "start_pos": 81, "end_pos": 98, "type": "TASK", "confidence": 0.9043744206428528}]}, {"text": "In Section 3.1 we demonstrate that it is helpful to have word segmenters for MT, but that segmentation performance does not directly correlate with MT performance.", "labels": [], "entities": [{"text": "word segmenters", "start_pos": 57, "end_pos": 72, "type": "TASK", "confidence": 0.7053422331809998}, {"text": "MT", "start_pos": 77, "end_pos": 79, "type": "TASK", "confidence": 0.9654691219329834}, {"text": "MT", "start_pos": 148, "end_pos": 150, "type": "TASK", "confidence": 0.9198381900787354}]}, {"text": "We analyze what characteristics of word segmenters most affect MT performance in Section 3.2.", "labels": [], "entities": [{"text": "word segmenters", "start_pos": 35, "end_pos": 50, "type": "TASK", "confidence": 0.7068382799625397}, {"text": "MT", "start_pos": 63, "end_pos": 65, "type": "TASK", "confidence": 0.9958617687225342}]}, {"text": "In Section 4 and 5 we describe how we tune a CRF model to fit the \"word\" granularity and also how we incorporate external lexicon and information about named entities for better MT performance.", "labels": [], "entities": [{"text": "MT", "start_pos": 178, "end_pos": 180, "type": "TASK", "confidence": 0.9932867884635925}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: CharBased vs. MaxMatch", "labels": [], "entities": [{"text": "MaxMatch", "start_pos": 24, "end_pos": 32, "type": "DATASET", "confidence": 0.7101308107376099}]}, {"text": " Table 3: CRF-basic vs MaxMatch", "labels": [], "entities": [{"text": "MaxMatch", "start_pos": 23, "end_pos": 31, "type": "DATASET", "confidence": 0.8238154649734497}]}, {"text": " Table 4: MT Lexicon Statistics and Conditional Entropy of Seg- mentation Variations of three segmetners", "labels": [], "entities": [{"text": "MT Lexicon Statistics", "start_pos": 10, "end_pos": 31, "type": "TASK", "confidence": 0.5878933568795522}]}, {"text": " Table 5: Effect of the bias parameter \u03bb 0 on the average number  of character per token on MT data.", "labels": [], "entities": [{"text": "MT", "start_pos": 92, "end_pos": 94, "type": "TASK", "confidence": 0.906059980392456}]}, {"text": " Table 7: CRF-Lex-NR vs CRF-Lex", "labels": [], "entities": [{"text": "CRF-Lex", "start_pos": 24, "end_pos": 31, "type": "DATASET", "confidence": 0.7359657883644104}]}]}