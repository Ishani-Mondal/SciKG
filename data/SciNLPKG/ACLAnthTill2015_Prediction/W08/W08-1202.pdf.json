{"title": [], "abstractContent": [{"text": "We address the problem of distinguishing between two sources of disagreement in annotations: genuine subjectivity and slip of attention.", "labels": [], "entities": []}, {"text": "The latter is especially likely when the classification task has a default class, as in tasks where annotators need to find instances of the phenomenon of interest , such as in a metaphor detection task discussed here.", "labels": [], "entities": [{"text": "metaphor detection task", "start_pos": 179, "end_pos": 202, "type": "TASK", "confidence": 0.8895761966705322}]}, {"text": "We apply and extend a data analysis technique proposed by Beigman Klebanov and Shamir (2006) to first dis-till reliably deliberate (non-chance) annotations and then to estimate the amount of attention slips vs genuine disagreement in the reliably deliberate annotations.", "labels": [], "entities": []}], "introductionContent": [{"text": "Classification tasks fall into two broad categories.", "labels": [], "entities": [{"text": "Classification tasks", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.895391434431076}]}, {"text": "Those in the first category proceed by requiring that every item is explicitly assigned a tag out of a given set of tags; part-of-speech tagging is an example.", "labels": [], "entities": [{"text": "part-of-speech tagging", "start_pos": 122, "end_pos": 144, "type": "TASK", "confidence": 0.7465177774429321}]}, {"text": "In the second group of tasks, the annotator is asked to identify a phenomenon of interest, thus implicitly classifying items as belonging to the phenomenon (marked) and not belonging to it (left unmarked).", "labels": [], "entities": []}, {"text": "When the studied phenomenon is expected to have low incidence, this is a time-saving strategy, as annotators do not need to bother with explicitly marking (almost) everything as a nonphenomenon.", "labels": [], "entities": [{"text": "incidence", "start_pos": 52, "end_pos": 61, "type": "METRIC", "confidence": 0.9865230917930603}]}, {"text": "A recent example of such a task is Beigman, where annotators were asked to provide anchors for words deemed anchored in the text (i.e. associatively connected to a previous item in the text), thus leaving words that did not receive an anchor implicitly marked as un-anchored.", "labels": [], "entities": []}, {"text": "Psychological experiments where people are asked to respond to the occurrence of a given phenomenon can also be viewed as implicit classifications; for example, see work on identification of boundaries of musical phrases by listeners.", "labels": [], "entities": [{"text": "identification of boundaries of musical phrases by listeners", "start_pos": 173, "end_pos": 233, "type": "TASK", "confidence": 0.8817640468478203}]}, {"text": "The task of metaphor detection discussed in this paper also falls under the implicit classification category.", "labels": [], "entities": [{"text": "metaphor detection", "start_pos": 12, "end_pos": 30, "type": "TASK", "confidence": 0.9689123928546906}]}, {"text": "While such a strategy uses annotators' time efficiently, some of the observed disagreements could be due to an annotator missing an occurrence of the relevant phenomenon, rather than genuinely disagreeing on the matter of occurrence.", "labels": [], "entities": []}, {"text": "We show in section 2 that our metaphor identification task features less-than-perfect interannotator agreement.", "labels": [], "entities": [{"text": "metaphor identification task", "start_pos": 30, "end_pos": 58, "type": "TASK", "confidence": 0.9397748510042826}]}, {"text": "Section 3 uses Beigman methodology to find annotations that can be reliably attributed to a deliberate decision by at least some of the annotators.", "labels": [], "entities": []}, {"text": "We then discuss the use of validation experiment to distinguish between slips of attention and genuine disagreements (sections 4,5).", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Metaphor annotation data (production),  by metaphor type. The third column shows the  percentage of paragraphs (out of 2364) marked as  having a metaphor of the given type, on average  across 9 annotators.", "labels": [], "entities": []}, {"text": " Table 3: Percentage of \"Accept\" validations for re- liably deliberate (Rel) and unreliable (URel) sub- sets of the metaphor production data, given that the  subject himself did NOT produce the metaphor.", "labels": [], "entities": [{"text": "Accept", "start_pos": 25, "end_pos": 31, "type": "METRIC", "confidence": 0.9860069751739502}]}, {"text": " Table 4: Percentage of \"Reject\" validations for re- liably deliberate (Rel) and unreliable (URel) sub- sets of the metaphor production data, given that the  subject himself produced the annotation.", "labels": [], "entities": []}]}