{"title": [{"text": "An Integrated Dialog Simulation Technique for Evaluating Spoken Dialog Systems", "labels": [], "entities": [{"text": "Dialog Simulation", "start_pos": 14, "end_pos": 31, "type": "TASK", "confidence": 0.7219734191894531}]}], "abstractContent": [{"text": "This paper proposes a novel integrated dialog simulation technique for evaluating spoken dialog systems.", "labels": [], "entities": []}, {"text": "Many techniques for simulating users and errors have been proposed for use in improving and evaluating spoken dialog systems, but most of them are not easily applied to various dialog systems or domains because some are limited to specific domains or others require heuristic rules.", "labels": [], "entities": []}, {"text": "In this paper , we propose a highly-portable technique for simulating user intention, utterance and Automatic Speech Recognition (ASR) channels.", "labels": [], "entities": [{"text": "Automatic Speech Recognition (ASR) channels", "start_pos": 100, "end_pos": 143, "type": "TASK", "confidence": 0.7598179451056889}]}, {"text": "This technique can be used to rapidly build a dialog simulation system for evaluating spoken dialog systems.", "labels": [], "entities": []}, {"text": "We propose a novel user intention modeling and generating method that uses a linear-chain conditional random field, a data-driven domain specific user utterance simulation method, and a novel ASR channel simulation method with adjustable error recognition rates.", "labels": [], "entities": [{"text": "user intention modeling and generating", "start_pos": 19, "end_pos": 57, "type": "TASK", "confidence": 0.7719455242156983}, {"text": "ASR channel simulation", "start_pos": 192, "end_pos": 214, "type": "TASK", "confidence": 0.894690732161204}]}, {"text": "Experiments using these techniques were carried out to evaluate the performance and behavior of previously developed dialog systems designed for navigation dialogs, and it turned out that our approach is easy to setup and shows the similar tendencies of real users.", "labels": [], "entities": [{"text": "navigation dialogs", "start_pos": 145, "end_pos": 163, "type": "TASK", "confidence": 0.9091200828552246}]}], "introductionContent": [{"text": "Evaluation of spoken dialog systems is essential for developing and improving the systems and for assessing their performance.", "labels": [], "entities": []}, {"text": "Normally, humans are used to evaluate the systems, but training and employing human evaluators is expensive.", "labels": [], "entities": []}, {"text": "Furthermore, qualified human users are not always immediately available.", "labels": [], "entities": []}, {"text": "These inevitable difficulties of working with human users can cause huge delay in development and assessment of spoken dialog systems.", "labels": [], "entities": []}, {"text": "To avoid the problems that result from using humans to evaluate spoken dialog systems, developers have widely used dialog simulation, in which a simulated user interacts with a spoken dialog system.", "labels": [], "entities": []}, {"text": "Many techniques for user intention, utterance and error simulation have been proposed.", "labels": [], "entities": [{"text": "user intention, utterance and error simulation", "start_pos": 20, "end_pos": 66, "type": "TASK", "confidence": 0.5563927377973285}]}, {"text": "However, previously proposed simulation techniques cannot be easily applied to evaluate various dialog systems, because some of these techniques are specially designed to work with their own dialog systems, some require heuristic rules or flowcharts, and others try to build user side dialog management systems using specialized dialog managing methods.", "labels": [], "entities": []}, {"text": "These problems motivated us to develop dialog simulation techniques which allow developers to build dialog simulation systems rapidly for use in evaluating various dialog systems.", "labels": [], "entities": []}, {"text": "To be successful, a simulation approach should not depend on specific domains or rules.", "labels": [], "entities": []}, {"text": "Also it should not be coupled to a specific dialog management method.", "labels": [], "entities": []}, {"text": "Furthermore, successful dialog simulation should fully support both user simulation and environment simulation.", "labels": [], "entities": []}, {"text": "In user simulation, it must be capable of simulating both user intentions and user utterances, because user utterances are essential for testing the language understanding component of the dialog system.", "labels": [], "entities": []}, {"text": "In addition to user simulation, environment simulation such as ASR channel simulation is desirable because it allows developers to test the dialog system in various acoustic environments.", "labels": [], "entities": [{"text": "ASR channel simulation", "start_pos": 63, "end_pos": 85, "type": "TASK", "confidence": 0.9108913342157999}]}, {"text": "In this paper, we propose novel dialog simulation techniques which satisfy these requirements.", "labels": [], "entities": [{"text": "dialog simulation", "start_pos": 32, "end_pos": 49, "type": "TASK", "confidence": 0.8682671785354614}]}, {"text": "We introduce anew user intention simulation method based on the sequential graphical model, and a user utterance simulator which can generate diverse natural user utterances.", "labels": [], "entities": []}, {"text": "The user intention and utterance simulators are both fully data-driven approaches; therefore they have high domain-and language portability.", "labels": [], "entities": []}, {"text": "We also propose a novel Automatic Speech Recognizer (ASR) channel simulator which allows the developers to set the desired speech recognition performance level.", "labels": [], "entities": [{"text": "Automatic Speech Recognizer (ASR) channel simulator", "start_pos": 24, "end_pos": 75, "type": "TASK", "confidence": 0.7967202737927437}]}, {"text": "Through a case study, we showed that our approach is feasible in successful dialog simulation to evaluate spoken dialog systems.", "labels": [], "entities": []}, {"text": "This paper is structured as follows.", "labels": [], "entities": []}, {"text": "We first provide a brief introduction of other dialog simulation techniques and their differences from our approach in Section 2.", "labels": [], "entities": []}, {"text": "We then introduce the overall architecture and the detailed methods of intention, utterance and ASR channel simulation in Section 3.", "labels": [], "entities": [{"text": "ASR channel simulation", "start_pos": 96, "end_pos": 118, "type": "TASK", "confidence": 0.9270386497179667}]}, {"text": "Experiments to test the simulation techniques, and a case study are described in Section 4.", "labels": [], "entities": []}, {"text": "We conclude with a brief summary and suggest directions for future work in Section 5.", "labels": [], "entities": []}], "datasetContent": [{"text": "We proposed a method that user intention, utterance and ASR channel simulation to rapidly assemble a simulation system to evaluate dialog systems.", "labels": [], "entities": [{"text": "ASR channel", "start_pos": 56, "end_pos": 67, "type": "TASK", "confidence": 0.8594664037227631}]}, {"text": "We conducted a case study for the navigation domain Korean spoken dialog system to test our simulation method and examine the dialog behaviors using the simulator.", "labels": [], "entities": [{"text": "navigation domain Korean spoken dialog", "start_pos": 34, "end_pos": 72, "type": "TASK", "confidence": 0.8190574049949646}]}, {"text": "We used 100 dialog examples from real user and dialog system to train user intention and utterance simulator.", "labels": [], "entities": []}, {"text": "We used the SLU method of), and dialog management method of () to build the dialog system.", "labels": [], "entities": []}, {"text": "After trained user simulator, we perform simulation to collect 5000 dialog samples for each WER settings (WER = 0 \u02dc 40 %).", "labels": [], "entities": [{"text": "WER", "start_pos": 106, "end_pos": 109, "type": "METRIC", "confidence": 0.984185516834259}]}, {"text": "To verify the user intention and utterance simulation quality, we let two human judges to evaluate 200 randomly chosen dialogs and 1031 utterances from the simulated dialog examples (WER=0%).", "labels": [], "entities": [{"text": "WER", "start_pos": 183, "end_pos": 186, "type": "METRIC", "confidence": 0.9976796507835388}]}, {"text": "At first, they evaluate a dialog with three scale (1: Unnatural, 2: Possible, 3: Natural), then evaluate the utterances of a dialog with three scale (1: Unclear, 2: Understandable, 3: Natural).", "labels": [], "entities": []}, {"text": "The inter evaluator agreement (kappa) is 0.45 and 0.58 for dialog and utterance evaluation respectively, which show the moderate agreement.", "labels": [], "entities": [{"text": "inter evaluator agreement (kappa)", "start_pos": 4, "end_pos": 37, "type": "METRIC", "confidence": 0.7908446540435156}]}, {"text": "Both judges show the positive reactions for the quality of user intention and utterance, the simulated dialogs can be possibly occurred, and the quality of utterance is close to natural human utterance.", "labels": [], "entities": []}, {"text": "We also did regression analysis with the results of human evaluation and the SWB score to find out the relationship between SWB and human judgment.", "labels": [], "entities": [{"text": "SWB score", "start_pos": 77, "end_pos": 86, "type": "METRIC", "confidence": 0.5866169333457947}]}, {"text": "shows the result of polynomial regression (order 3) result.", "labels": [], "entities": []}, {"text": "It shows that 'Unclear' utterance might have 0.", "labels": [], "entities": []}, {"text": "Average human evaluation for user utterances: Relationship between SWB score and human judgment \u02dc 0.7 SWB score, 'Possible' and 'Natural' simulated utterance might have over 0.75.", "labels": [], "entities": []}, {"text": "It means that we can simulate good user utterance if we constrain the user simulator with the threshold around 0.75 SWB score.", "labels": [], "entities": [{"text": "SWB", "start_pos": 116, "end_pos": 119, "type": "METRIC", "confidence": 0.9878777265548706}]}, {"text": "To assess the ASR channel simulation quality, we compared how SLU of utterances was affected by WER.", "labels": [], "entities": [{"text": "ASR channel", "start_pos": 14, "end_pos": 25, "type": "TASK", "confidence": 0.8862370550632477}, {"text": "SLU", "start_pos": 62, "end_pos": 65, "type": "METRIC", "confidence": 0.8559405207633972}, {"text": "WER", "start_pos": 96, "end_pos": 99, "type": "METRIC", "confidence": 0.568516194820404}]}, {"text": "SLU was quantified according to sentence error rate (SER) and concept error rate (CER).", "labels": [], "entities": [{"text": "sentence error rate (SER)", "start_pos": 32, "end_pos": 57, "type": "METRIC", "confidence": 0.8478713929653168}, {"text": "concept error rate (CER)", "start_pos": 62, "end_pos": 86, "type": "METRIC", "confidence": 0.9498049020767212}]}, {"text": "Compared to WER set by the developer, measured WER was the same, SER increased more rapidly, and CER increased more slowly.", "labels": [], "entities": [{"text": "WER", "start_pos": 12, "end_pos": 15, "type": "METRIC", "confidence": 0.9362536668777466}, {"text": "WER", "start_pos": 47, "end_pos": 50, "type": "METRIC", "confidence": 0.9963108897209167}, {"text": "SER", "start_pos": 65, "end_pos": 68, "type": "METRIC", "confidence": 0.9985645413398743}, {"text": "CER", "start_pos": 97, "end_pos": 100, "type": "METRIC", "confidence": 0.9982495903968811}]}, {"text": "This means that our simulation framework models SLU errors effective as well as speech recognition errors.", "labels": [], "entities": [{"text": "SLU", "start_pos": 48, "end_pos": 51, "type": "TASK", "confidence": 0.8706845045089722}, {"text": "speech recognition", "start_pos": 80, "end_pos": 98, "type": "TASK", "confidence": 0.7519681453704834}]}, {"text": "shows the overall dialog system behaviors using the user simulator and ASR channel simulator.", "labels": [], "entities": []}, {"text": "As the WER rate increased, dialog system performance decreased and dialog length increased.", "labels": [], "entities": [{"text": "WER rate", "start_pos": 7, "end_pos": 15, "type": "METRIC", "confidence": 0.7227274477481842}, {"text": "length", "start_pos": 74, "end_pos": 80, "type": "METRIC", "confidence": 0.8622521758079529}]}, {"text": "This result is similar as observed to the dialog behaviors in real human-", "labels": [], "entities": []}], "tableCaptions": []}