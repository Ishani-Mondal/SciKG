{"title": [{"text": "Automated Metrics That Agree With Human Judgements On Generated Output for an Embodied Conversational Agent", "labels": [], "entities": []}], "abstractContent": [{"text": "When evaluating a generation system, if a corpus of target outputs is available, a common and simple strategy is to compare the system output against the corpus contents.", "labels": [], "entities": []}, {"text": "However, cross-validation metrics that test whether the system makes exactly the same choices as the corpus on each item have recently been shown not to correlate well with human judgements of quality.", "labels": [], "entities": []}, {"text": "An alternative evaluation strategy is to compute intrinsic, task-specific properties of the generated output; this requires more domain-specific metrics, but can often produce a better assessment of the output.", "labels": [], "entities": []}, {"text": "In this paper, a range of metrics using both of these techniques are used to evaluate three methods for selecting the facial displays of an embodied conversational agent, and the predictions of the metrics are compared with human judgements of the same generated output.", "labels": [], "entities": []}, {"text": "The corpus-reproduction metrics show no relationship with the human judgements, while the intrinsic metrics that capture the number and variety of facial displays show a significant correlation with the preferences of the human users.", "labels": [], "entities": []}], "introductionContent": [{"text": "Evaluating the output of a generation system is known to be difficult: since generation is an openended task, the criteria for success can be difficult to define (cf.).", "labels": [], "entities": []}, {"text": "In the current state of the art, there are two main strategies for evaluating the output of a generation system: the behaviour or preferences of humans in response to the output maybe measured, or automated measures maybe computed on the output itself.", "labels": [], "entities": []}, {"text": "A study involving human judges is the most complete and convincing evaluation of generated output.", "labels": [], "entities": []}, {"text": "However, such a study is not always practical, as recruiting sufficient subjects can be time-consuming and expensive.", "labels": [], "entities": []}, {"text": "So automated metrics are also used in addition to-or instead of-human studies.", "labels": [], "entities": []}, {"text": "When automatically evaluating generated output, the goal is to find metrics that can easily be computed and that can also be shown to correlate with human judgements of quality.", "labels": [], "entities": []}, {"text": "Such metrics have been introduced in other fields, including PAR-ADISE () for spoken dialogue systems, BLEU () for machine translation, 1 and ROUGE) for summarisation.", "labels": [], "entities": [{"text": "PAR-ADISE", "start_pos": 61, "end_pos": 70, "type": "METRIC", "confidence": 0.9403246641159058}, {"text": "BLEU", "start_pos": 103, "end_pos": 107, "type": "METRIC", "confidence": 0.9984398484230042}, {"text": "machine translation", "start_pos": 115, "end_pos": 134, "type": "TASK", "confidence": 0.7151237279176712}, {"text": "ROUGE", "start_pos": 142, "end_pos": 147, "type": "METRIC", "confidence": 0.9877073168754578}, {"text": "summarisation", "start_pos": 153, "end_pos": 166, "type": "TASK", "confidence": 0.9911565780639648}]}, {"text": "Many automated generation evaluations measure the similarity between the generated output and a corpus of \"gold-standard\" target outputs, often using measures such as precision and recall.", "labels": [], "entities": [{"text": "precision", "start_pos": 167, "end_pos": 176, "type": "METRIC", "confidence": 0.9993546605110168}, {"text": "recall", "start_pos": 181, "end_pos": 187, "type": "METRIC", "confidence": 0.9961161613464355}]}, {"text": "Such measures of corpus similarity are straightforward to compute and easy to interpret; however, they are not always appropriate for generation systems.", "labels": [], "entities": []}, {"text": "One of the main advantages of choosing dynamic generation over canned output is its flexibility and its ability to produce a range of different outputs; as pointed out by, \"valuation studies that ignore the potential of the system to generate a range of appropriate outputs will be necessarily limited.\"", "labels": [], "entities": []}, {"text": "Indeed, several recent studies have shown that strict corpus-similarity measures tend to favour repetitive generation strategies that do not diverge much, on average, from the corpus data, while human judges often prefer output with more variety.", "labels": [], "entities": []}, {"text": "Automated metrics that take into account other properties than strict corpus similarity have also been used to evaluate the output of generation systems.", "labels": [], "entities": []}, {"text": "describes several evaluations that used corpus data in a different way: each of the corpus examples was associated with some reward function (e.g., subjective user evaluation or task success), and machine-learning techniques such as reinforcement learning or boosting were then used to train the output planner.", "labels": [], "entities": []}, {"text": "found that automated metrics based on factors other than corpus similarity (e.g., the amount of variation in the output) agreed better with user preferences than did the corpus-similarity scores.", "labels": [], "entities": []}, {"text": "compare the predictions of a range of measures, both intrinsic and extrinsic, that were used to evaluate the systems in a shared-task referringexpression generation challenge.", "labels": [], "entities": [{"text": "referringexpression generation challenge", "start_pos": 134, "end_pos": 174, "type": "TASK", "confidence": 0.8409194747606913}]}, {"text": "One main finding from this comparison was that there was no significant correlation between the intrinsic and extrinsic (task success) measures for this task.", "labels": [], "entities": []}, {"text": "All of the above studies considered only systems that generate text, but many of the same factors also apply to the generation of non-verbal behaviours for an embodied conversational agent (ECA) ().", "labels": [], "entities": []}, {"text": "The behaviour of such an agent is normally based on recorded human behaviour, which can provide targets similar to those used in corpusbased evaluations of text-generation systems.", "labels": [], "entities": []}, {"text": "However, just as in text generation, a multimodal system that scores well on corpus similarity tends to produce highly repetitive non-verbal behaviours, so it is equally important to gather human judgements to accompany any automated evaluation.", "labels": [], "entities": [{"text": "text generation", "start_pos": 20, "end_pos": 35, "type": "TASK", "confidence": 0.7551893889904022}]}, {"text": "This paper presents three corpus-driven methods of selecting facial displays for an embodied conversational agent and describes two studies comparing the output of the different methods.", "labels": [], "entities": []}, {"text": "All methods are based on annotated data drawn from a corpus of human facial displays, and each uses the corpus data in a different way.", "labels": [], "entities": []}, {"text": "The first evaluation study uses human judges to compare the output of the selection methods against one another, while the second study uses a range of automated metrics: several corpusreproduction measures, along with metrics based on intrinsic properties of the outputs.", "labels": [], "entities": []}, {"text": "The results of the two studies are compared using multiple regression, and the implications are discussed.", "labels": [], "entities": []}], "datasetContent": [{"text": "Since the subjects in the user-preference studies generally selected the corpus schedules over any of the alternatives, any automated metric for this task should favour output that resembles the examples in the corpus.", "labels": [], "entities": []}, {"text": "The most obvious form of corpus similarity is exact reproduction of the displays in the corpus, which suggests using metrics such as precision and recall that favour generation strategies whose output on every item is as close as possible to what was annotated in the corpus for that sentence.", "labels": [], "entities": [{"text": "precision", "start_pos": 133, "end_pos": 142, "type": "METRIC", "confidence": 0.9991004467010498}, {"text": "recall", "start_pos": 147, "end_pos": 153, "type": "METRIC", "confidence": 0.9938739538192749}]}, {"text": "In Section 4.1, several such corpus-reproduction metrics are described and their results presented.", "labels": [], "entities": []}, {"text": "For this type of open-ended generation task, though, it can be overly restrictive to allow only the displays that were annotated in the corpus fora sentence and to penalise any deviation.", "labels": [], "entities": [{"text": "open-ended generation task", "start_pos": 17, "end_pos": 43, "type": "TASK", "confidence": 0.761813759803772}]}, {"text": "Indeed, as mentioned in the introduction, a number of previous studies have found that the output of generation systems that score well on this type of metric is often disliked in practice by users.", "labels": [], "entities": []}, {"text": "Section 4.2 therefore presents several intrinsic metrics that aim to capture corpus similarity of a different type: rather than requiring the system to exactly reproduce the corpus on each sentence, these metrics instead favour strategies resulting in global behaviour that exhibits similar patterns to those found in the corpus, without necessarily agreeing exactly with the corpus on any specific sentence.", "labels": [], "entities": []}], "tableCaptions": []}