{"title": [{"text": "Regularization and Search for Minimum Error Rate Training", "labels": [], "entities": [{"text": "Regularization", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.9504392147064209}, {"text": "Minimum Error Rate", "start_pos": 30, "end_pos": 48, "type": "METRIC", "confidence": 0.7262532313664755}]}], "abstractContent": [{"text": "Minimum error rate training (MERT) is a widely used learning procedure for statistical machine translation models.", "labels": [], "entities": [{"text": "Minimum error rate training (MERT", "start_pos": 0, "end_pos": 33, "type": "METRIC", "confidence": 0.7427300661802292}, {"text": "statistical machine translation models", "start_pos": 75, "end_pos": 113, "type": "TASK", "confidence": 0.7078232616186142}]}, {"text": "We contrast three search strategies for MERT: Powell's method, the variant of coordinate descent found in the Moses MERT utility, and a novel stochastic method.", "labels": [], "entities": [{"text": "MERT", "start_pos": 40, "end_pos": 44, "type": "TASK", "confidence": 0.9783371686935425}]}, {"text": "It is shown that the stochastic method obtains test set gains of +0.98 BLEU on MT03 and +0.61 BLEU on MT05.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 71, "end_pos": 75, "type": "METRIC", "confidence": 0.9986294507980347}, {"text": "MT03", "start_pos": 79, "end_pos": 83, "type": "DATASET", "confidence": 0.9425429701805115}, {"text": "BLEU", "start_pos": 94, "end_pos": 98, "type": "METRIC", "confidence": 0.9974617958068848}, {"text": "MT05", "start_pos": 102, "end_pos": 106, "type": "DATASET", "confidence": 0.9655886292457581}]}, {"text": "We also present a method for regularizing the MERT objective that achieves statistically significant gains when combined with both Powell's method and coordinate descent.", "labels": [], "entities": [{"text": "MERT", "start_pos": 46, "end_pos": 50, "type": "TASK", "confidence": 0.7809990048408508}]}], "introductionContent": [{"text": "Och (2003) introduced minimum error rate training (MERT) as an alternative training regime to the conditional likelihood objective previously used with log-linear translation models).", "labels": [], "entities": [{"text": "minimum error rate training (MERT)", "start_pos": 22, "end_pos": 56, "type": "METRIC", "confidence": 0.8481717024530683}]}, {"text": "This approach attempts to improve translation quality by optimizing an automatic translation evaluation metric, such as the BLEU score ().", "labels": [], "entities": [{"text": "translation", "start_pos": 34, "end_pos": 45, "type": "TASK", "confidence": 0.9641427397727966}, {"text": "BLEU score", "start_pos": 124, "end_pos": 134, "type": "METRIC", "confidence": 0.979129821062088}]}, {"text": "This is accomplished by either directly walking the error surface provided by an evaluation metric w.r.t. the model weights or by using gradientbased techniques on a continuous approximation of such a surface.", "labels": [], "entities": []}, {"text": "While the former is piecewise constant and thus cannot be optimized using gradient techniques, provides an approach that performs such training efficiently.", "labels": [], "entities": []}, {"text": "In this paper we explore a number of variations on MERT.", "labels": [], "entities": [{"text": "MERT", "start_pos": 51, "end_pos": 55, "type": "TASK", "confidence": 0.5573973059654236}]}, {"text": "First, it is shown that performance gains can be had by making use of a stochastic search strategy as compare to that obtained by Powell's method and coordinate descent.", "labels": [], "entities": [{"text": "coordinate descent", "start_pos": 150, "end_pos": 168, "type": "TASK", "confidence": 0.7322569787502289}]}, {"text": "Subsequently, results are presented for two regularization strategies . Both allow coordinate descent and Powell's method to achieve performance that is on par with stochastic search.", "labels": [], "entities": [{"text": "coordinate descent", "start_pos": 83, "end_pos": 101, "type": "TASK", "confidence": 0.6663704514503479}]}, {"text": "In what follows, we briefly review minimum error rate training, introduce our stochastic search and regularization strategies, and then present experimental results.", "labels": [], "entities": []}], "datasetContent": [{"text": "Three sets of experiments were performed.", "labels": [], "entities": []}, {"text": "For the first set, we compare the performance of Powell's method, KCD, and our novel stochastic search strategy.", "labels": [], "entities": []}, {"text": "We then evaluate the performance of all three methods when the objective is regularized using the average of adjacent plateaus for window sizes varying from 3 to 7.", "labels": [], "entities": []}, {"text": "Finally, we repeat the regularization experiment, but using the maximum objective value from the adjacent plateaus.", "labels": [], "entities": [{"text": "regularization", "start_pos": 23, "end_pos": 37, "type": "TASK", "confidence": 0.9737366437911987}]}, {"text": "These experiments were performed using the Chinese English evaluation data provided for.", "labels": [], "entities": [{"text": "Chinese English evaluation data", "start_pos": 43, "end_pos": 74, "type": "DATASET", "confidence": 0.6646155565977097}]}, {"text": "MT02 was used as a dev set for MERT learning, while MT03 and MT05 were used as our test sets.", "labels": [], "entities": [{"text": "MT02", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.9639980792999268}, {"text": "MERT learning", "start_pos": 31, "end_pos": 44, "type": "TASK", "confidence": 0.9332596659660339}, {"text": "MT03", "start_pos": 52, "end_pos": 56, "type": "DATASET", "confidence": 0.9245157837867737}, {"text": "MT05", "start_pos": 61, "end_pos": 65, "type": "DATASET", "confidence": 0.8829999566078186}]}, {"text": "For all experiments, MERT training was performed using n-best lists from the decoder of size 100.", "labels": [], "entities": [{"text": "MERT", "start_pos": 21, "end_pos": 25, "type": "TASK", "confidence": 0.8327860236167908}]}, {"text": "During each iteration, the MERT search was performed once with a starting point of the weights used to generate the most recent set of n-best lists and then 5 more times using randomly selected starting points . Of these, we retain the weights from the search that obtained the lowest objective value.", "labels": [], "entities": [{"text": "MERT search", "start_pos": 27, "end_pos": 38, "type": "TASK", "confidence": 0.6927583813667297}]}, {"text": "Training continued until either decoding produced no novel entries for the combined n-best lists or none of the parameter values changed by more than 1e-5 across subsequent iterations.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: BLEU scores obtained by models trained using  the three different parameter search strategies: Powell's  method, KCD, and stochastic search.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9981067180633545}]}, {"text": " Table 4: BLEU scores obtained when regularizing using the average loss of adjacent plateaus, left, and the maximum  loss of adjacent plateaus, right. The none entry for each search strategy represents the baseline where no regularization  is used. Statistically significant test set gains, p < 0.01, over the respective baselines are in bold face.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9986675977706909}]}]}