{"title": [{"text": "A Simple Method for Resolution of Definite Reference in a Shared Visual Context", "labels": [], "entities": [{"text": "Resolution of Definite Reference in a Shared Visual Context", "start_pos": 20, "end_pos": 79, "type": "TASK", "confidence": 0.796022527747684}]}], "abstractContent": [{"text": "We present a method for resolving definite ex-ophoric reference to visually shared objects that is based on a) an automatically learned, simple mapping of words to visual features (\"visual word semantics\"), b) an automatically learned, semantically-motivated utterance segmentation (\"visual grammar\"), and c) a procedure that, given an utterance, uses b) to combine a) to yield a resolution.", "labels": [], "entities": [{"text": "resolving definite ex-ophoric reference to visually shared objects", "start_pos": 24, "end_pos": 90, "type": "TASK", "confidence": 0.7332746908068657}]}, {"text": "We evaluated the method both on a pre-recorded corpus and in an online setting, where it performed with 81% (chance: 14%) and 66% accuracy, respectively.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 130, "end_pos": 138, "type": "METRIC", "confidence": 0.9996554851531982}]}, {"text": "This is comparable to results reported in related work on simpler settings.", "labels": [], "entities": []}], "introductionContent": [], "datasetContent": [{"text": "With an f-score of 0.985 (10-fold cross validation), the transformation-based learning of the segmentation performs quite well, roughly at the level of state-of-the-art POS-taggers (albeit with a much smaller tag inventory).", "labels": [], "entities": [{"text": "f-score", "start_pos": 8, "end_pos": 15, "type": "METRIC", "confidence": 0.980182409286499}]}, {"text": "Also evaluated via crossvalidation on the corpus, the resolution component as a whole performs with an accuracy of 80.67% (using frequency-based word-semantic features; it drops to 66.95% for average-based).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 103, "end_pos": 111, "type": "METRIC", "confidence": 0.9992055296897888}]}, {"text": "There were on average 7 objects in each scene in the corpus; i.e. the baseline of getting the reference right by chance is 14%.", "labels": [], "entities": []}, {"text": "Our system significantly improves over this baseline.", "labels": [], "entities": []}, {"text": "We also evaluated the system in a more realistic application situation.", "labels": [], "entities": []}, {"text": "We asked subjects to refer to certain pieces in presented scenes (via typed utterances); here, the system reached a success-rate of 66% (7 subjects, 100 scene / utterance pairs).", "labels": [], "entities": []}, {"text": "While this is considerably lower than the corpusbased evaluation, it is still on a par with related systems using more complicated resolution methods).", "labels": [], "entities": []}, {"text": "We also think these results represent the lower end of the performance range that can be expected in practical use, as in an interactive dialogue system users have time to adapt to the capabilities of the system.", "labels": [], "entities": []}], "tableCaptions": []}