{"title": [{"text": "Exploring an Auxiliary Distribution based approach to Domain Adaptation of a Syntactic Disambiguation Model", "labels": [], "entities": [{"text": "Domain Adaptation of a Syntactic Disambiguation", "start_pos": 54, "end_pos": 101, "type": "TASK", "confidence": 0.777354434132576}]}], "abstractContent": [{"text": "We investigate auxiliary distributions (Johnson and Riezler, 2000) for domain adaptation of a supervised parsing system of Dutch.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 71, "end_pos": 88, "type": "TASK", "confidence": 0.7273300886154175}]}, {"text": "To overcome the limited target domain training data, we exploit an original and larger out-of-domain model as auxiliary distribution.", "labels": [], "entities": []}, {"text": "However, our empirical results exhibit that the auxiliary distribution does not help: even when very little target training data is available the incorporation of the out-of-domain model does not contribute to parsing accuracy on the target domain; instead, better results are achieved either without adaptation or by simple model combination.", "labels": [], "entities": [{"text": "parsing", "start_pos": 210, "end_pos": 217, "type": "TASK", "confidence": 0.9621722102165222}, {"text": "accuracy", "start_pos": 218, "end_pos": 226, "type": "METRIC", "confidence": 0.9152437448501587}]}], "introductionContent": [{"text": "Modern statistical parsers are trained on large annotated corpora (treebanks) and their parameters are estimated to reflect properties of the training data.", "labels": [], "entities": []}, {"text": "Therefore, a disambiguation component will be successful as long as the treebank it was trained on is representative for the input the model gets.", "labels": [], "entities": []}, {"text": "However, as soon as the model is applied to another domain, or text genre (), accuracy degrades considerably.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 78, "end_pos": 86, "type": "METRIC", "confidence": 0.9991311430931091}]}, {"text": "For example, the performance of a parser trained on the Wall Street Journal (newspaper text) significantly drops when evaluated on the more varied Brown (fiction/nonfiction) corpus.", "labels": [], "entities": [{"text": "Wall Street Journal (newspaper text", "start_pos": 56, "end_pos": 91, "type": "DATASET", "confidence": 0.925539473692576}, {"text": "Brown (fiction/nonfiction) corpus", "start_pos": 147, "end_pos": 180, "type": "DATASET", "confidence": 0.6379427569253104}]}, {"text": "A simple solution to improve performance on anew domain is to construct a parser specifically for that domain.", "labels": [], "entities": []}, {"text": "However, this amounts to handlabeling a considerable amount of training data which is clearly very expensive and leads to an unsatisfactory solution.", "labels": [], "entities": []}, {"text": "In alternative, techniques for domain adaptation, also known as parser adaptation () or genre portability (), try to leverage either a small amount of already existing annotated data () or unlabeled data) of one domain to parse data from a different domain.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 31, "end_pos": 48, "type": "TASK", "confidence": 0.7425273656845093}, {"text": "parser adaptation", "start_pos": 64, "end_pos": 81, "type": "TASK", "confidence": 0.9254510700702667}, {"text": "genre portability", "start_pos": 88, "end_pos": 105, "type": "TASK", "confidence": 0.7049806416034698}]}, {"text": "In this study we examine an approach that assumes a limited amount of already annotated in-domain data.", "labels": [], "entities": []}, {"text": "We explore auxiliary distributions) for domain adaptation, originally suggested for the incorporation of lexical selectional preferences into a parsing system.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 40, "end_pos": 57, "type": "TASK", "confidence": 0.7709760069847107}]}, {"text": "We gauge the effect of exploiting a more general, out-ofdomain model for parser adaptation to overcome the limited amount of in-domain training data.", "labels": [], "entities": [{"text": "parser adaptation", "start_pos": 73, "end_pos": 90, "type": "TASK", "confidence": 0.9503214955329895}]}, {"text": "The approach is examined on two application domains, question answering and spoken data.", "labels": [], "entities": [{"text": "question answering", "start_pos": 53, "end_pos": 71, "type": "TASK", "confidence": 0.8768968880176544}]}, {"text": "For the empirical trials, we use), a robust computational analyzer for Dutch.", "labels": [], "entities": []}, {"text": "Alpino employs a discriminative approach to parse selection that bases its decision on a Maximum Entropy (MaxEnt) model.", "labels": [], "entities": [{"text": "parse selection", "start_pos": 44, "end_pos": 59, "type": "TASK", "confidence": 0.9721548557281494}]}, {"text": "Section 2 introduces the MaxEnt framework.", "labels": [], "entities": []}, {"text": "Section 3 describes our approach of exploring auxiliary distributions for domain adaptation.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 74, "end_pos": 91, "type": "TASK", "confidence": 0.7232618033885956}]}, {"text": "In section 4 the experimental design and empirical results are presented and discussed.", "labels": [], "entities": []}], "datasetContent": [{"text": "The general model is trained on the Alpino Treebank  The output of the parser is evaluated by comparing the generated dependency structure fora corpus sentence to the gold standard dependency structure in a treebank.", "labels": [], "entities": [{"text": "Alpino Treebank", "start_pos": 36, "end_pos": 51, "type": "DATASET", "confidence": 0.9109408855438232}]}, {"text": "For this comparison, we represent the dependency structure (a directed acyclic graph) as a set of named dependency relations.", "labels": [], "entities": []}, {"text": "To compare such sets of dependency relations, we count the number of dependencies that are identical in the generated parse and the stored structure, which is expressed traditionally using precision, recall and f-score ().", "labels": [], "entities": [{"text": "precision", "start_pos": 189, "end_pos": 198, "type": "METRIC", "confidence": 0.9992620348930359}, {"text": "recall", "start_pos": 200, "end_pos": 206, "type": "METRIC", "confidence": 0.9980922341346741}]}, {"text": "Let Di p be the number of dependencies produced by the parser for sentence i, Di g is the number of dependencies in the treebank parse, and Di o is the number of correct dependencies produced by the parser.", "labels": [], "entities": []}, {"text": "If no superscript is used, we aggregate overall sentences of the test set, i.e.,: Precision is the total number of correct dependencies returned by the parser, divided by the overall number of dependencies returned by the parser (precision = D o /D p ); recall is the number of correct system dependencies divided by the total number of dependencies in the treebank (recall = D o /D g ).", "labels": [], "entities": [{"text": "Precision", "start_pos": 82, "end_pos": 91, "type": "METRIC", "confidence": 0.9954001307487488}, {"text": "precision", "start_pos": 230, "end_pos": 239, "type": "METRIC", "confidence": 0.9968481659889221}, {"text": "recall", "start_pos": 254, "end_pos": 260, "type": "METRIC", "confidence": 0.9991683959960938}, {"text": "recall", "start_pos": 367, "end_pos": 373, "type": "METRIC", "confidence": 0.9919666051864624}]}, {"text": "As usual, precision and recall can be combined in a single f-score metric.", "labels": [], "entities": [{"text": "precision", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9995226860046387}, {"text": "recall", "start_pos": 24, "end_pos": 30, "type": "METRIC", "confidence": 0.9992572665214539}]}, {"text": "An alternative similarity score for dependency structures is based on the observation that fora given sentence of n words, a parser would be expected to return n dependencies.", "labels": [], "entities": []}, {"text": "In such cases, we can simply use the percentage of correct dependencies as a measure of accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 88, "end_pos": 96, "type": "METRIC", "confidence": 0.997962474822998}]}, {"text": "Such a labeled dependency accuracy is used, for instance, in the CoNLL shared task on dependency parsing (\"labeled attachment score\").", "labels": [], "entities": [{"text": "accuracy", "start_pos": 26, "end_pos": 34, "type": "METRIC", "confidence": 0.6565799117088318}, {"text": "CoNLL shared task on dependency parsing", "start_pos": 65, "end_pos": 104, "type": "TASK", "confidence": 0.5670503204067548}]}, {"text": "Our evaluation metric is a variant of labeled dependency accuracy, in which we do allow for some discrepancy between the number of returned dependencies.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 57, "end_pos": 65, "type": "METRIC", "confidence": 0.5833334922790527}]}, {"text": "Such a discrepancy can occur, for instance, because in the syntactic annotations of Alpino (inherited from the CGN) words can sometimes be dependent on more than a single head (called 'secondary edges' in CGN).", "labels": [], "entities": []}, {"text": "A further cause is parsing failure, in which case a parser might not produce any dependencies.", "labels": [], "entities": [{"text": "parsing", "start_pos": 19, "end_pos": 26, "type": "TASK", "confidence": 0.9787641763687134}]}, {"text": "We argue elsewhere (van Noord, In preparation) that a metric based on f-score can be misleading in such cases.", "labels": [], "entities": []}, {"text": "The resulting metric is called concept accuracy, in, for instance, The concept accuracy metric can be characterized as the mean of a per-sentence minimum of recall and precision.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 39, "end_pos": 47, "type": "METRIC", "confidence": 0.8518546223640442}, {"text": "recall", "start_pos": 157, "end_pos": 163, "type": "METRIC", "confidence": 0.9965470433235168}, {"text": "precision", "start_pos": 168, "end_pos": 177, "type": "METRIC", "confidence": 0.9860115647315979}]}, {"text": "The resulting CA score therefore is typically slightly lower than the corresponding f-score, and, for the purposes of this paper, equivalent to labeled dependency accuracy.", "labels": [], "entities": [{"text": "CA score", "start_pos": 14, "end_pos": 22, "type": "METRIC", "confidence": 0.9783580899238586}, {"text": "accuracy", "start_pos": 163, "end_pos": 171, "type": "METRIC", "confidence": 0.6187426447868347}]}, {"text": "In the first set of experiments we focus on the Question Answering (QA) domain (CLEF corpus).", "labels": [], "entities": [{"text": "Question Answering (QA) domain", "start_pos": 48, "end_pos": 78, "type": "TASK", "confidence": 0.8277549743652344}, {"text": "CLEF corpus)", "start_pos": 80, "end_pos": 92, "type": "DATASET", "confidence": 0.7916702628135681}]}, {"text": "Besides evaluating our auxiliary based approach (section 3), we conduct separate baseline experiments: \u2022 In-domain (CLEF): train on CLEF (baseline) \u2022 Out-domain (Alpino): train on Alpino We assess the performance of all of these models on the CLEF data by using 5-fold crossvalidation.", "labels": [], "entities": [{"text": "CLEF data", "start_pos": 243, "end_pos": 252, "type": "DATASET", "confidence": 0.8534870445728302}]}, {"text": "The results are given in table 1.", "labels": [], "entities": []}, {"text": "The CLEF model performs significantly better than the out-of-domain (Alpino) model, despite of the smaller size of the in-domain training data.", "labels": [], "entities": []}, {"text": "In contrast, the simple data combination results in a model (CLEF+Alpino) whose performance is somewhere in between.", "labels": [], "entities": []}, {"text": "It is able to contribute in some cases to disambiguate questions, while leading to wrong decisions in other cases.", "labels": [], "entities": []}, {"text": "However, for our auxiliary based approach (CLEF+Alpino aux) with its regulated contribution of the general model, the results show that adding the feature does not help.", "labels": [], "entities": []}, {"text": "On most datasets the same performance was achieved as by the indomain model, while on only two datasets) the use of the auxiliary feature results in an insignificant improvement.", "labels": [], "entities": []}, {"text": "In contrast, simple model combination works surprisingly well.", "labels": [], "entities": []}, {"text": "On two datasets) this simple technique reaches a substantial improvement overall other models.", "labels": [], "entities": []}, {"text": "On only one dataset) it falls slightly off the in-domain baseline, but still considerably outperforms data combination.", "labels": [], "entities": []}, {"text": "This is true for both model combination methods, with estimated and equal weights.", "labels": [], "entities": []}, {"text": "In general, the results show that model combination usually outperforms data combination (with the exception of one dataset, CLEF 2007), where, interestingly, the simplest model combination (equal weights) often performs best.", "labels": [], "entities": [{"text": "CLEF 2007", "start_pos": 125, "end_pos": 134, "type": "DATASET", "confidence": 0.9260663092136383}]}, {"text": "Contrary to expectations, the auxiliary based approach performs poorly and could often not even come close to the results obtained by simple model combination.", "labels": [], "entities": []}, {"text": "In the following we will explore possible reasons for this result.", "labels": [], "entities": []}, {"text": "Examining possible causes One possible point of failure could be that the auxiliary feature was simply ignored.", "labels": [], "entities": []}, {"text": "If the estimated weight would be close to zero the feature would indeed not contribute to the disambiguation task.", "labels": [], "entities": [{"text": "disambiguation", "start_pos": 94, "end_pos": 108, "type": "TASK", "confidence": 0.9634810090065002}]}, {"text": "Therefore, we examined the estimated weights for that feature.", "labels": [], "entities": []}, {"text": "From that analysis we saw that, compared to the other features, the auxiliary feature got a weight relatively far from zero.", "labels": [], "entities": []}, {"text": "It got on average a weight of \u22120.0905 in our datasets and as such is among the most influential weights, suggesting it to be important for disambiguation.", "labels": [], "entities": [{"text": "disambiguation", "start_pos": 139, "end_pos": 153, "type": "TASK", "confidence": 0.9832699298858643}]}, {"text": "Another question that needs to be asked, however, is whether the feature is modeling properly the original Alpino model.", "labels": [], "entities": []}, {"text": "For this sanity check, we create a model that contains only the single auxiliary feature and no other features.", "labels": [], "entities": []}, {"text": "The feature's weight is set to a constant negative value 4 . The resulting model's performance is assessed on the complete CLEF data.", "labels": [], "entities": [{"text": "CLEF data", "start_pos": 123, "end_pos": 132, "type": "DATASET", "confidence": 0.9641030132770538}]}, {"text": "The results (0% column in table 3) show that the auxiliary feature is indeed properly modeling the general Alpino model, as the two result in identical performance.", "labels": [], "entities": []}, {"text": "One might argue that the question domain is rather 'easy', given the already high baseline performance and the fact that few hand-annotated questions are enough to obtain a reasonable model.", "labels": [], "entities": []}, {"text": "Therefore, we examine our approach on CGN).", "labels": [], "entities": [{"text": "CGN", "start_pos": 38, "end_pos": 41, "type": "DATASET", "confidence": 0.8951898813247681}]}, {"text": "The empirical results of testing using crossvalidation within a subset of CGN subdomains are given in table 4.", "labels": [], "entities": []}, {"text": "The baseline accuracies are much lower on this more heterogeneous, spoken, data, leaving more room for potential improvements over the in-domain model.", "labels": [], "entities": [{"text": "accuracies", "start_pos": 13, "end_pos": 23, "type": "METRIC", "confidence": 0.8967030048370361}]}, {"text": "However, the results show that the auxiliary based approach does notwork on the CGN subdomains either.", "labels": [], "entities": []}, {"text": "The approach is notable to improve even on datasets where very little training data is available (e.g. comp-l), thus confirming our previous finding.", "labels": [], "entities": []}, {"text": "Moreover, in some cases the auxiliary feature rather, although only slightly, degrades performance (indicated in italic in table 4) and performs worse than the counterpart model without the additional feature.", "labels": [], "entities": []}, {"text": "Depending on the different characteristics of data/domain and its size, the best model adaptation method varies on CGN.", "labels": [], "entities": []}, {"text": "On some subdomains simple model combination performs best, while on others it is more beneficial to just apply the original, out-of-domain Alpino model.", "labels": [], "entities": []}, {"text": "To conclude, model combination achieves inmost cases a modest improvement, while we have shown empirically that our domain adaptation method based on auxiliary distributions performs just similar to a model trained on in-domain data.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 116, "end_pos": 133, "type": "TASK", "confidence": 0.7373367249965668}]}], "tableCaptions": [{"text": " Table 1: Results on the CLEF test data; underlined scores indicate results > in-domain baseline (CLEF)", "labels": [], "entities": [{"text": "CLEF test data", "start_pos": 25, "end_pos": 39, "type": "DATASET", "confidence": 0.9308855732282003}]}, {"text": " Table 2: Results on CLEF including several auxil- iary features corresponding to Alpino submodels", "labels": [], "entities": [{"text": "CLEF", "start_pos": 21, "end_pos": 25, "type": "DATASET", "confidence": 0.8295045495033264}]}, {"text": " Table 3: Results on the CLEF data with varying amount of training data", "labels": [], "entities": [{"text": "CLEF data", "start_pos": 25, "end_pos": 34, "type": "DATASET", "confidence": 0.9589493274688721}]}, {"text": " Table 4: Excerpt of results on various CGN subdomains (# of sentences in parenthesis).", "labels": [], "entities": []}]}