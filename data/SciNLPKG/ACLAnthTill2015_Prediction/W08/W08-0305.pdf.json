{"title": [{"text": "Learning Performance of a Machine Translation System: a Statistical and Computational Analysis", "labels": [], "entities": [{"text": "Machine Translation", "start_pos": 26, "end_pos": 45, "type": "TASK", "confidence": 0.6946701109409332}, {"text": "Statistical and Computational Analysis", "start_pos": 56, "end_pos": 94, "type": "TASK", "confidence": 0.6674605831503868}]}], "abstractContent": [{"text": "We present an extensive experimental study of a Statistical Machine Translation system, Moses (Koehn et al., 2007), from the point of view of its learning capabilities.", "labels": [], "entities": [{"text": "Statistical Machine Translation", "start_pos": 48, "end_pos": 79, "type": "TASK", "confidence": 0.7806088924407959}]}, {"text": "Very accurate learning curves are obtained, by using high-performance computing, and extrap-olations are provided of the projected performance of the system under different conditions.", "labels": [], "entities": [{"text": "extrap-olations", "start_pos": 85, "end_pos": 100, "type": "METRIC", "confidence": 0.9696151614189148}]}, {"text": "We provide a discussion of learning curves, and we suggest that: 1) the representation power of the system is not currently a limitation to its performance, 2) the inference of its models from finite sets of i.i.d. data is responsible for current performance limitations , 3) it is unlikely that increasing dataset sizes will result in significant improvements (at least in traditional i.i.d. setting), 4) it is unlikely that novel statistical estimation methods will result in significant improvements.", "labels": [], "entities": []}, {"text": "The current performance wall is mostly a consequence of Zipf's law, and this should betaken into account when designing a statistical machine translation system.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 122, "end_pos": 153, "type": "TASK", "confidence": 0.6259471873442332}]}, {"text": "A few possible research directions are discussed as a result of this investigation, most notably the integration of linguistic rules into the model inference phase, and the development of active learning procedures.", "labels": [], "entities": []}], "introductionContent": [], "datasetContent": [{"text": "We have performed a large number of detailed experiments.", "labels": [], "entities": []}, {"text": "In this paper we report just a few, leaving the complete account of our benchmarking to a full journal version (.", "labels": [], "entities": []}, {"text": "Three experiments allow us to assess the most promis-ing directions of research, from a machine learning point of view.", "labels": [], "entities": []}, {"text": "1. Learning curve showing translation performance as a function of training set size, where translation is performed on unseen sentences.", "labels": [], "entities": []}, {"text": "The curves, describing the statistical part of the performance, are seen to grow very slowly with training set size.", "labels": [], "entities": []}, {"text": "2. Learning curve showing translation performance as a function of training set size, where translation is performed on known sentences.", "labels": [], "entities": [{"text": "translation", "start_pos": 26, "end_pos": 37, "type": "TASK", "confidence": 0.9561684131622314}]}, {"text": "This was done to verify that the hypothesis class is indeed capable of representing high quality translations in the idealized case when all the necessary phrases have been observed in training phase.", "labels": [], "entities": []}, {"text": "By limiting phrase length to 7 words, and using test sentences mostly longer than 20 words, we have ensured that this was a genuine task of decoding.", "labels": [], "entities": []}, {"text": "We observed that translation in these idealized conditions is worse than human translation, but much better than machine translation of unseen sentences.", "labels": [], "entities": [{"text": "human translation", "start_pos": 73, "end_pos": 90, "type": "TASK", "confidence": 0.8050596117973328}, {"text": "machine translation of unseen sentences", "start_pos": 113, "end_pos": 152, "type": "TASK", "confidence": 0.834247887134552}]}, {"text": "3. Plot of performance of a model when the numeric parameters are corrupted by an increasing amount of noise.", "labels": [], "entities": [{"text": "Plot", "start_pos": 3, "end_pos": 7, "type": "METRIC", "confidence": 0.9563561081886292}]}, {"text": "This was done to simulate the effect of inaccurate parameter estimation algorithms (due either to imprecise objective functions, or to lack of sufficient statistics from the corpus).", "labels": [], "entities": []}, {"text": "We were surprised to observe that accurate estimation of these parameters accounts for at most 10% of the final score.", "labels": [], "entities": [{"text": "estimation", "start_pos": 43, "end_pos": 53, "type": "METRIC", "confidence": 0.9494444727897644}]}, {"text": "It is the actual list of phrases that forms the bulk of the knowledge in the system.", "labels": [], "entities": []}, {"text": "We conclude that the availability of the right models in the system would allow the system to have a much higher performance, but these models will not come from increased datasets or estimation procedures.", "labels": [], "entities": []}, {"text": "Instead, they will come from the results of either the introduction of linguistic knowledge, or the introduction of query algorithms, themselves resulting necessarily from confidence estimation methods.", "labels": [], "entities": []}, {"text": "Hence these appear to be the two most pressing questions in this research area.", "labels": [], "entities": []}, {"text": "In this section we analyse how performance is affected by training set size, by creating learning curves (NIST score vs training set size).", "labels": [], "entities": []}, {"text": "We have created subsets of the complete corpus by sub-sampling sentences from a uniform distribution, with replacement.", "labels": [], "entities": []}, {"text": "We have created 10 random subsets for each of the 20 chosen sizes, where each size represents 5%, 10%, etc of the complete corpus.", "labels": [], "entities": []}, {"text": "For each subset anew instance of the SMT system has been created, fora total of 200 models.", "labels": [], "entities": [{"text": "SMT", "start_pos": 37, "end_pos": 40, "type": "TASK", "confidence": 0.9864357113838196}]}, {"text": "These have been optimized using a fixed size development set (of 2,000 sentences, not included in any other phase of the experiment).", "labels": [], "entities": []}, {"text": "Two hundred experiments have then been run on an independent test set (of 2,000 sentences, also not included in any other phase of the experiment).", "labels": [], "entities": []}, {"text": "This allowed us to calculate the mean and variance of NIST scores.", "labels": [], "entities": [{"text": "mean and variance", "start_pos": 33, "end_pos": 50, "type": "METRIC", "confidence": 0.7067430317401886}, {"text": "NIST scores", "start_pos": 54, "end_pos": 65, "type": "DATASET", "confidence": 0.7655184864997864}]}, {"text": "This has been done for the models with and without the optimization step, hence producing the learning curves with error bars plotted in, representing translation performance versus training set size, in the two cases.", "labels": [], "entities": []}, {"text": "The growth of the learning curve follows atypical pattern, growing fast at first, then slowing down (traditional learning curves are power laws, in theoretical models).", "labels": [], "entities": []}, {"text": "In this case it appears to be growing even slower than a power law, which would be a surprise under traditional statistical learning theory models.", "labels": [], "entities": []}, {"text": "In any case, the addition of massive amounts of data from the same distribution will result into smaller improvements in the performance.", "labels": [], "entities": []}, {"text": "The small error bars that we have obtained also allow us to neatly observe the benefits of the optimization phase, which are small but clearly significant.", "labels": [], "entities": []}, {"text": "The performance of a learning system depends both on the statistical estimation issues discussed in the previous subsection, and on functional approximation issues: how well can the function class reproduce the desired behaviour?", "labels": [], "entities": []}, {"text": "In order to measure this quantity, we have performed an experiment much like the one described above, with one key differ-: \"Not Optimized\" has been obtained using a fixed test set and no optimization phase.", "labels": [], "entities": []}, {"text": "\"Optimized\" using a fixed test set and the optimization phase.", "labels": [], "entities": []}, {"text": "ence: the test set was selected randomly from the training set (after cleaning phase).", "labels": [], "entities": []}, {"text": "In this way we are guaranteed that the system has seen all the necessary information in training phase, and we can assess its limitations in these very ideal conditions.", "labels": [], "entities": []}, {"text": "We are aware this condition is extremely idealized and it will never happen in real life, but we wanted to have an upper bound on the performance achievable by this architecture if access to ideal data was not an issue.", "labels": [], "entities": []}, {"text": "We also made sure that the performance on translating training sentences was not due to simple memorization of the entire sentence, verifying that the vast majority of the sentences were not present in the translation table (where the maximal phrase size was 7), not even in reduced form.", "labels": [], "entities": [{"text": "translating training sentences", "start_pos": 42, "end_pos": 72, "type": "TASK", "confidence": 0.8967908024787903}]}, {"text": "Under these favourable conditions, the system obtained a NIST score of around 11, against a score of about 7.5 on unseen sentences.", "labels": [], "entities": [{"text": "NIST score", "start_pos": 57, "end_pos": 67, "type": "METRIC", "confidence": 0.6807244420051575}]}, {"text": "This suggests that the phrase-based Markov-chain representation is sufficiently rich to obtain a high score, if the necessary information is contained in the translation and language models.", "labels": [], "entities": []}, {"text": "For each model to be tested on known sentences, we have sampled ten subsets of 2,000 sentences each from the training set.", "labels": [], "entities": []}, {"text": "The \"Optimized, Test on Training Set\" learning curve, see figure 2, represents a possible upper bound on the best performance of this SMT system, since it has been computed in favourable conditions.", "labels": [], "entities": [{"text": "SMT", "start_pos": 134, "end_pos": 137, "type": "TASK", "confidence": 0.994810938835144}]}, {"text": "It does suggest that this hypothesis class has the power of approximating the target behaviour more accurately than we could think based on performance on unseen sentences.", "labels": [], "entities": []}, {"text": "If the right information has been seen, the system can reconstruct the sentences rather accurately.", "labels": [], "entities": []}, {"text": "The NIST score computed using the reference sentences as target sentences is around 15, we identify the relative curve as \"Human Translation\".", "labels": [], "entities": [{"text": "NIST score", "start_pos": 4, "end_pos": 14, "type": "DATASET", "confidence": 0.8026539087295532}, {"text": "Human Translation", "start_pos": 123, "end_pos": 140, "type": "TASK", "confidence": 0.6465317606925964}]}, {"text": "At this point, it seems likely that the process with which we learn the necessary tables representing the knowledge of the system is responsible for the performance limitations.", "labels": [], "entities": []}, {"text": "The gap between the \"Optimized, Test on Training Set\" and the \"Optimized\" curves is even more interesting if related to the slow growth rate in the previous learning curve: although the system can represent internally a good model of translation, it seems unlikely that this will ever be inferred by increasing the size of training datasets in realistic amounts.", "labels": [], "entities": []}, {"text": "The training step results in various forms of knowledge: translation table, language model and parameters from the optimization.", "labels": [], "entities": [{"text": "translation", "start_pos": 57, "end_pos": 68, "type": "TASK", "confidence": 0.9550063610076904}]}, {"text": "The internal models learnt by the system are essentially lists of phrases, with probabilities associated to them.", "labels": [], "entities": []}, {"text": "Which of these components is mostly responsible for performance limitations?", "labels": [], "entities": []}, {"text": "Much research has focused on devising improved principles for the statistical estimation of the parameters in language and translation models.", "labels": [], "entities": []}, {"text": "The introduction of discriminative graphical models has marked a departure from traditional maximum likelihood estimation principles, and various approaches have been proposed.", "labels": [], "entities": [{"text": "maximum likelihood estimation", "start_pos": 92, "end_pos": 121, "type": "TASK", "confidence": 0.6320771972338358}]}, {"text": "The question is: how much information is contained in the fine grain structure of the probabilities estimated by the model?", "labels": [], "entities": []}, {"text": "Is the performance improving with more data because certain parameters are estimated better, or just because the lists are growing?", "labels": [], "entities": []}, {"text": "In the second case, it is likely that more sophisticated statistical algorithms to improve the estimation of probabilities will have limited impact.", "labels": [], "entities": []}, {"text": "In order to simulate the effect of inaccurate estimation of the numeric parameters, we have added increasing amount of noise to them.", "labels": [], "entities": []}, {"text": "This can either represent the effect of insufficient statistics in estimating them, or the use of imperfect parameter esti- Figure 2: Four learning curves have been compared.", "labels": [], "entities": []}, {"text": "\"Not Optimized\" has been obtained using a fixed test set and no optimization phase.", "labels": [], "entities": []}, {"text": "\"Optimized\" using a fixed test set and the optimization phase.", "labels": [], "entities": []}, {"text": "\"Optimized Test On Training Set\" a test set selected by the training set for each training set size and the optimization phase.", "labels": [], "entities": []}, {"text": "\"Human Translation\" has been obtained by computing NIST using the reference English sentence of the test set as target sentences.", "labels": [], "entities": [{"text": "Human Translation", "start_pos": 1, "end_pos": 18, "type": "TASK", "confidence": 0.718429833650589}, {"text": "NIST", "start_pos": 51, "end_pos": 55, "type": "DATASET", "confidence": 0.8779596090316772}]}, {"text": "We have corrupted the parameters in the language and translation models, by adding increasing levels of noise to them, and measured the effect of this on performance.", "labels": [], "entities": []}, {"text": "One model trained with 62,995 pairs of sentences has been chosen from the experiments in Section 4.1.", "labels": [], "entities": []}, {"text": "A percentage of noise has been added to each probability in the language model, including conditional probability and back off, translation model, bidirectional translation probabilities and lexicalized weighting.", "labels": [], "entities": [{"text": "translation", "start_pos": 128, "end_pos": 139, "type": "TASK", "confidence": 0.9600757360458374}]}, {"text": "Given a probability p and a percentage of noise, pn, a value has been randomly selected from the interval, where x = p * pn, and added top.", "labels": [], "entities": []}, {"text": "If this quantity is bigger than one it has been approximated to one.", "labels": [], "entities": []}, {"text": "Different values of percentage have been used.", "labels": [], "entities": [{"text": "percentage", "start_pos": 20, "end_pos": 30, "type": "METRIC", "confidence": 0.9539875388145447}]}, {"text": "For each value of pn, five experiment have been run.", "labels": [], "entities": []}, {"text": "The optimization step has not been run.", "labels": [], "entities": []}, {"text": "We see from that the performance does not seem to depend crucially on the fine structure of the parameter vectors, and that even a large addition of noise (100%) produces a 10% decline in NIST score.", "labels": [], "entities": [{"text": "NIST score", "start_pos": 188, "end_pos": 198, "type": "METRIC", "confidence": 0.5638440549373627}]}, {"text": "This suggests that it is the list itself, rather   than the probabilities in it, that controls the performance.", "labels": [], "entities": []}, {"text": "Different estimation methods can produce different parameters, but this does not seem to matter very much.", "labels": [], "entities": []}, {"text": "The creation of a more complete list of words, however, seems to be the key to improve the score.", "labels": [], "entities": []}, {"text": "Combined with the previous findings, this would mean that neither more data nor better statistics will bridge the performance gap.", "labels": [], "entities": []}, {"text": "The solution might have to be found elsewhere, and in our Discussion section we outline a few possible avenues.", "labels": [], "entities": []}], "tableCaptions": []}