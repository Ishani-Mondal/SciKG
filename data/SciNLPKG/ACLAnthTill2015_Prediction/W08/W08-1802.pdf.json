{"title": [{"text": "Exact Phrases in Information Retrieval for Question Answering", "labels": [], "entities": [{"text": "Exact Phrases in Information Retrieval", "start_pos": 0, "end_pos": 38, "type": "TASK", "confidence": 0.707864773273468}, {"text": "Question Answering", "start_pos": 43, "end_pos": 61, "type": "TASK", "confidence": 0.7398670464754105}]}], "abstractContent": [{"text": "Question answering (QA) is the task of finding a concise answer to a natural language question.", "labels": [], "entities": [{"text": "Question answering (QA)", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.945223605632782}]}, {"text": "The first stage of QA involves information retrieval.", "labels": [], "entities": [{"text": "QA", "start_pos": 19, "end_pos": 21, "type": "TASK", "confidence": 0.982006311416626}, {"text": "information retrieval", "start_pos": 31, "end_pos": 52, "type": "TASK", "confidence": 0.8320507109165192}]}, {"text": "Therefore, performance of an information retrieval subsystem serves as an upper bound for the performance of a QA system.", "labels": [], "entities": []}, {"text": "In this work we use phrases automatically identified from questions as exact match constituents to search queries.", "labels": [], "entities": []}, {"text": "Our results show an improvement over baseline on several document and sentence retrieval measures on the WEB dataset.", "labels": [], "entities": [{"text": "baseline", "start_pos": 37, "end_pos": 45, "type": "METRIC", "confidence": 0.9627631306648254}, {"text": "sentence retrieval", "start_pos": 70, "end_pos": 88, "type": "TASK", "confidence": 0.6660652756690979}, {"text": "WEB dataset", "start_pos": 105, "end_pos": 116, "type": "DATASET", "confidence": 0.9841539561748505}]}, {"text": "We get a 20% relative improvement in MRR for sentence extraction on the WEB dataset when using automatically generated phrases and a further 9.5% relative improvement when using manually annotated phrases.", "labels": [], "entities": [{"text": "MRR", "start_pos": 37, "end_pos": 40, "type": "METRIC", "confidence": 0.8827102184295654}, {"text": "sentence extraction", "start_pos": 45, "end_pos": 64, "type": "TASK", "confidence": 0.775486409664154}, {"text": "WEB dataset", "start_pos": 72, "end_pos": 83, "type": "DATASET", "confidence": 0.9920927584171295}]}, {"text": "Surprisingly , a separate experiment on the indexed AQUAINT dataset showed no effect on IR performance of using exact phrases.", "labels": [], "entities": [{"text": "AQUAINT dataset", "start_pos": 52, "end_pos": 67, "type": "DATASET", "confidence": 0.9555719494819641}, {"text": "IR", "start_pos": 88, "end_pos": 90, "type": "TASK", "confidence": 0.9868131875991821}]}], "introductionContent": [{"text": "Question answering can be viewed as a sophisticated information retrieval (IR) task where a system automatically generates a search query from a natural language question and finds a concise answer from a set of documents.", "labels": [], "entities": [{"text": "Question answering", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.9295361936092377}, {"text": "information retrieval (IR) task where a system automatically generates a search query from a natural language question and finds a concise answer from a set of documents", "start_pos": 52, "end_pos": 221, "type": "Description", "confidence": 0.7969017953708254}]}, {"text": "In the opendomain factoid question answering task systems answer general questions like Who is the creator of The Daily Show?, or When was Mozart born?.", "labels": [], "entities": [{"text": "question answering task", "start_pos": 26, "end_pos": 49, "type": "TASK", "confidence": 0.693672368923823}, {"text": "Who is the creator of The Daily Show?", "start_pos": 88, "end_pos": 125, "type": "TASK", "confidence": 0.607315159506268}]}, {"text": "Most existing question answering systems add question analysis, sentence retrieval and answer extraction components to an IR system.", "labels": [], "entities": [{"text": "question answering", "start_pos": 14, "end_pos": 32, "type": "TASK", "confidence": 0.7912884652614594}, {"text": "question analysis", "start_pos": 45, "end_pos": 62, "type": "TASK", "confidence": 0.773189514875412}, {"text": "sentence retrieval", "start_pos": 64, "end_pos": 82, "type": "TASK", "confidence": 0.7169263660907745}, {"text": "answer extraction", "start_pos": 87, "end_pos": 104, "type": "TASK", "confidence": 0.7267964631319046}, {"text": "IR", "start_pos": 122, "end_pos": 124, "type": "TASK", "confidence": 0.9690248966217041}]}, {"text": "Since information retrieval is the first stage of question answering, its performance is an upper bound on the overall question answering system's performance.", "labels": [], "entities": [{"text": "information retrieval", "start_pos": 6, "end_pos": 27, "type": "TASK", "confidence": 0.7989447712898254}, {"text": "question answering", "start_pos": 50, "end_pos": 68, "type": "TASK", "confidence": 0.861207127571106}, {"text": "question answering", "start_pos": 119, "end_pos": 137, "type": "TASK", "confidence": 0.723989337682724}]}, {"text": "IR performance depends on the quality of document indexing and query construction.", "labels": [], "entities": [{"text": "IR", "start_pos": 0, "end_pos": 2, "type": "TASK", "confidence": 0.987015962600708}, {"text": "query construction", "start_pos": 63, "end_pos": 81, "type": "TASK", "confidence": 0.7119360566139221}]}, {"text": "Question answering systems create a search query automatically from a user's question, through various levels of sophistication.", "labels": [], "entities": [{"text": "Question answering", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.8475922644138336}]}, {"text": "The simplest way of creating a query is to treat the words in the question as the terms in the query.", "labels": [], "entities": []}, {"text": "Some question answering systems apply linguistic processing to the question, identifying named entities and other query-relevant phrases.) use ontologies to expand query terms with synonyms and hypernyms.", "labels": [], "entities": [{"text": "question answering", "start_pos": 5, "end_pos": 23, "type": "TASK", "confidence": 0.7796211838722229}]}, {"text": "IR system recall is very important for question answering.", "labels": [], "entities": [{"text": "IR system", "start_pos": 0, "end_pos": 9, "type": "TASK", "confidence": 0.8335465788841248}, {"text": "recall", "start_pos": 10, "end_pos": 16, "type": "METRIC", "confidence": 0.8744142055511475}, {"text": "question answering", "start_pos": 39, "end_pos": 57, "type": "TASK", "confidence": 0.8996024131774902}]}, {"text": "If no correct answers are present in a document, no further processing will be able to find an answer.", "labels": [], "entities": []}, {"text": "IR system precision and ranking of candidate passages can also affect question answering performance.", "labels": [], "entities": [{"text": "IR system", "start_pos": 0, "end_pos": 9, "type": "TASK", "confidence": 0.7994053959846497}, {"text": "precision", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.8088039755821228}, {"text": "question answering", "start_pos": 70, "end_pos": 88, "type": "TASK", "confidence": 0.8260495066642761}]}, {"text": "If a sentence without a correct answer is ranked highly, answer extraction may extract incorrect answers from these erroneous candidates.", "labels": [], "entities": [{"text": "answer extraction", "start_pos": 57, "end_pos": 74, "type": "TASK", "confidence": 0.8700598478317261}]}, {"text": "show that there is a consistent relationship between the quality of document retrieval and the overall performance of question answering systems.", "labels": [], "entities": [{"text": "question answering", "start_pos": 118, "end_pos": 136, "type": "TASK", "confidence": 0.7416559159755707}]}, {"text": "In this work we evaluate the use of exact phrases from a question in document and passage retrieval.", "labels": [], "entities": [{"text": "passage retrieval", "start_pos": 82, "end_pos": 99, "type": "TASK", "confidence": 0.7223271280527115}]}, {"text": "First, we analyze how different parts of a question contribute to the performance of the sentence extraction stage of question answering.", "labels": [], "entities": [{"text": "sentence extraction", "start_pos": 89, "end_pos": 108, "type": "TASK", "confidence": 0.732388898730278}, {"text": "question answering", "start_pos": 118, "end_pos": 136, "type": "TASK", "confidence": 0.7811775803565979}]}, {"text": "We ana-lyze the match between linguistic constituents of different types in questions and sentences containing candidate answers.", "labels": [], "entities": []}, {"text": "For this analysis, we use a set of questions and answers from the TREC 2006 competition as a gold standard.", "labels": [], "entities": [{"text": "TREC 2006 competition", "start_pos": 66, "end_pos": 87, "type": "DATASET", "confidence": 0.8658222158749899}]}, {"text": "Second, we evaluate the performance of document retrieval in our StoQA question answering system.", "labels": [], "entities": [{"text": "document retrieval", "start_pos": 39, "end_pos": 57, "type": "TASK", "confidence": 0.7516891956329346}, {"text": "StoQA question answering", "start_pos": 65, "end_pos": 89, "type": "TASK", "confidence": 0.7240811983744303}]}, {"text": "We compare the performance of document retrieval from the Web and from an indexed collection of documents using different methods of query construction, and identify the optimal algorithm for query construction in our system as well as its limitations.", "labels": [], "entities": [{"text": "query construction", "start_pos": 133, "end_pos": 151, "type": "TASK", "confidence": 0.7425170838832855}, {"text": "query construction", "start_pos": 192, "end_pos": 210, "type": "TASK", "confidence": 0.7129785865545273}]}, {"text": "Third, we evaluate passage extraction from a set of documents.", "labels": [], "entities": [{"text": "passage extraction", "start_pos": 19, "end_pos": 37, "type": "TASK", "confidence": 0.9293051958084106}]}, {"text": "We analyze how the specificity of a query affects sentence extraction.", "labels": [], "entities": [{"text": "sentence extraction", "start_pos": 50, "end_pos": 69, "type": "TASK", "confidence": 0.7400131374597549}]}, {"text": "The rest of the paper is organized as follows: In Section 2, we summarize recent approaches to question answering.", "labels": [], "entities": [{"text": "question answering", "start_pos": 95, "end_pos": 113, "type": "TASK", "confidence": 0.9466323256492615}]}, {"text": "In Section 3, we describe the dataset used in this experiment.", "labels": [], "entities": []}, {"text": "In Section 5, we describe our method and data analysis.", "labels": [], "entities": []}, {"text": "In Section 4, we outline the architecture of our question answering system.", "labels": [], "entities": [{"text": "question answering", "start_pos": 49, "end_pos": 67, "type": "TASK", "confidence": 0.9071364104747772}]}, {"text": "In Section 6, we describe our experiments and present our results.", "labels": [], "entities": []}, {"text": "We summarize in Section 7.", "labels": [], "entities": []}], "datasetContent": [{"text": "In these experiments we look at the impact of using exact phrases on the performance of the document retrieval and sentence extraction stages of question answering.", "labels": [], "entities": [{"text": "document retrieval", "start_pos": 92, "end_pos": 110, "type": "TASK", "confidence": 0.6920730620622635}, {"text": "sentence extraction", "start_pos": 115, "end_pos": 134, "type": "TASK", "confidence": 0.7106355875730515}, {"text": "question answering", "start_pos": 145, "end_pos": 163, "type": "TASK", "confidence": 0.832088977098465}]}, {"text": "We use our StoQA question answering system.", "labels": [], "entities": [{"text": "StoQA question answering", "start_pos": 11, "end_pos": 35, "type": "TASK", "confidence": 0.764347513516744}]}, {"text": "Questions are analyzed as described in the previous section.", "labels": [], "entities": []}, {"text": "For document retrieval we use the back-off method described in the previous sec-   tion.", "labels": [], "entities": [{"text": "document retrieval", "start_pos": 4, "end_pos": 22, "type": "TASK", "confidence": 0.7586238980293274}]}, {"text": "We performed the experiments using first automatically generated phrases, and then manually corrected phrases.", "labels": [], "entities": []}, {"text": "For document retrieval we report: 1) average recall, 2) average mean reciprocal ranking (MRR), and 3) overall document recall.", "labels": [], "entities": [{"text": "document retrieval", "start_pos": 4, "end_pos": 22, "type": "TASK", "confidence": 0.7204387187957764}, {"text": "average", "start_pos": 37, "end_pos": 44, "type": "METRIC", "confidence": 0.9379540681838989}, {"text": "recall", "start_pos": 45, "end_pos": 51, "type": "METRIC", "confidence": 0.7379701733589172}, {"text": "average mean reciprocal ranking (MRR)", "start_pos": 56, "end_pos": 93, "type": "METRIC", "confidence": 0.9102708186422076}, {"text": "recall", "start_pos": 119, "end_pos": 125, "type": "METRIC", "confidence": 0.9482966065406799}]}, {"text": "Each question has a document retrieval recall score which is the proportion of documents identified from all correct documents for this question.", "labels": [], "entities": [{"text": "recall score", "start_pos": 39, "end_pos": 51, "type": "METRIC", "confidence": 0.8996817469596863}]}, {"text": "The average recall is the individual recall averaged overall questions.", "labels": [], "entities": [{"text": "recall", "start_pos": 12, "end_pos": 18, "type": "METRIC", "confidence": 0.995999813079834}, {"text": "recall", "start_pos": 37, "end_pos": 43, "type": "METRIC", "confidence": 0.9602623581886292}]}, {"text": "MRR is the inverse index of the first correct document.", "labels": [], "entities": [{"text": "MRR", "start_pos": 0, "end_pos": 3, "type": "METRIC", "confidence": 0.8264870643615723}]}, {"text": "For example, if the first correct document appears second, the MRR score will be 1/2.", "labels": [], "entities": [{"text": "MRR score", "start_pos": 63, "end_pos": 72, "type": "METRIC", "confidence": 0.949086606502533}]}, {"text": "MRR is computed for each question and averaged overall questions.", "labels": [], "entities": [{"text": "MRR", "start_pos": 0, "end_pos": 3, "type": "METRIC", "confidence": 0.9508733153343201}]}, {"text": "Overall document recall is the percentage of questions for which at least one correct document was retrieved.", "labels": [], "entities": [{"text": "recall", "start_pos": 17, "end_pos": 23, "type": "METRIC", "confidence": 0.9845971465110779}]}, {"text": "This measure indicates the upper bound on the QA system.", "labels": [], "entities": [{"text": "QA", "start_pos": 46, "end_pos": 48, "type": "METRIC", "confidence": 0.7393721342086792}]}, {"text": "For sentence retrieval we report 1) average sentence MRR, 2) overall sentence recall, 3) average precision of the first sentence, 4) number of correct candidate sentences in the top 10 results, and 5) number of correct candidate sentences in the top 50 results . shows our experimental results.", "labels": [], "entities": [{"text": "sentence retrieval", "start_pos": 4, "end_pos": 22, "type": "TASK", "confidence": 0.7717957198619843}, {"text": "MRR", "start_pos": 53, "end_pos": 56, "type": "METRIC", "confidence": 0.9153848886489868}, {"text": "recall", "start_pos": 78, "end_pos": 84, "type": "METRIC", "confidence": 0.9689443111419678}, {"text": "precision", "start_pos": 97, "end_pos": 106, "type": "METRIC", "confidence": 0.9088078141212463}]}, {"text": "First, we evaluate the performance of document retrieval on the indexed AQUAINT dataset.", "labels": [], "entities": [{"text": "document retrieval", "start_pos": 38, "end_pos": 56, "type": "TASK", "confidence": 0.7076248079538345}, {"text": "AQUAINT dataset", "start_pos": 72, "end_pos": 87, "type": "DATASET", "confidence": 0.958566814661026}]}, {"text": "Average document recall for our baseline system is 0.53, indicating that on average half of the correct documents are retrieved.", "labels": [], "entities": [{"text": "recall", "start_pos": 17, "end_pos": 23, "type": "METRIC", "confidence": 0.9700718522071838}]}, {"text": "Average document MRR is .631, meaning that on average the first correct document appears first or second.", "labels": [], "entities": [{"text": "MRR", "start_pos": 17, "end_pos": 20, "type": "METRIC", "confidence": 0.6595068573951721}]}, {"text": "Overall document recall indicates that 75.6% of queries contain a correct document among the retrieved documents.", "labels": [], "entities": [{"text": "recall", "start_pos": 17, "end_pos": 23, "type": "METRIC", "confidence": 0.9895992279052734}]}, {"text": "Average sentence recall is lower than document recall indicating that some proportion of correct answers is not retrieved using our heuristic sentence extraction algorithm.", "labels": [], "entities": [{"text": "recall", "start_pos": 17, "end_pos": 23, "type": "METRIC", "confidence": 0.960078239440918}, {"text": "heuristic sentence extraction", "start_pos": 132, "end_pos": 161, "type": "TASK", "confidence": 0.6710648834705353}]}, {"text": "The average sentence MRR is .314 indicating that the first correct sentence is approximately third on the list.", "labels": [], "entities": [{"text": "MRR", "start_pos": 21, "end_pos": 24, "type": "METRIC", "confidence": 0.8926039338111877}]}, {"text": "With the AQUAINT dataset, we notice no improvement with exact phrases.", "labels": [], "entities": [{"text": "AQUAINT dataset", "start_pos": 9, "end_pos": 24, "type": "DATASET", "confidence": 0.8163975477218628}]}, {"text": "Next, we evaluate sentence retrieval from the WEB.", "labels": [], "entities": [{"text": "sentence retrieval", "start_pos": 18, "end_pos": 36, "type": "TASK", "confidence": 0.7551911175251007}, {"text": "WEB", "start_pos": 46, "end_pos": 49, "type": "DATASET", "confidence": 0.7927743196487427}]}, {"text": "There is no gold standard for the WEB dataset so we do not report document retrieval scores.", "labels": [], "entities": [{"text": "WEB dataset", "start_pos": 34, "end_pos": 45, "type": "DATASET", "confidence": 0.8907257616519928}]}, {"text": "Sentence scores on the WEB dataset are lower than on the AQUAINT dataset . Using back-off retrieval with automatically created phrases and named entities, we see an improvement over the baseline system performance for each of the sentence measures on the WEB dataset.", "labels": [], "entities": [{"text": "WEB dataset", "start_pos": 23, "end_pos": 34, "type": "DATASET", "confidence": 0.9767111539840698}, {"text": "AQUAINT dataset", "start_pos": 57, "end_pos": 72, "type": "DATASET", "confidence": 0.9451797902584076}, {"text": "WEB dataset", "start_pos": 255, "end_pos": 266, "type": "DATASET", "confidence": 0.9825021922588348}]}, {"text": "Average sentence MRR increases 20% from .183 in the baseline to .220 in the experimental system.", "labels": [], "entities": [{"text": "MRR", "start_pos": 17, "end_pos": 20, "type": "METRIC", "confidence": 0.7134466767311096}]}, {"text": "With manually created phrases MRR improves a further 9.5% to .241.", "labels": [], "entities": [{"text": "MRR", "start_pos": 30, "end_pos": 33, "type": "METRIC", "confidence": 0.7423387169837952}]}, {"text": "This indicates that information retrieval on the WEB dataset can benefit from a better quality of chunker and from a properly converted question phrase.", "labels": [], "entities": [{"text": "information retrieval", "start_pos": 20, "end_pos": 41, "type": "TASK", "confidence": 0.7127738147974014}, {"text": "WEB dataset", "start_pos": 49, "end_pos": 60, "type": "DATASET", "confidence": 0.9538027048110962}]}, {"text": "It also shows that the improvement is not due to simply matching random substrings from a question, but that linguistic information is useful in constructing the exact match phrases.", "labels": [], "entities": []}, {"text": "Precision of automatically detected phrases is affected by errors during automatic part-of-speech tagging of questions.", "labels": [], "entities": [{"text": "Precision", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.8402337431907654}, {"text": "part-of-speech tagging of questions", "start_pos": 83, "end_pos": 118, "type": "TASK", "confidence": 0.7948864251375198}]}, {"text": "An example of an error due to POS tagging is the identification of a phrase was Rowling born due to a failure to identify that born is a verb.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 30, "end_pos": 41, "type": "TASK", "confidence": 0.746070384979248}]}, {"text": "Our results emphasize the difference between the two datasets.", "labels": [], "entities": []}, {"text": "AQUAINT dataset is a collection of a large set of news documents, while WEB is a much larger resource of information from a variety of sources.", "labels": [], "entities": [{"text": "AQUAINT dataset", "start_pos": 0, "end_pos": 15, "type": "DATASET", "confidence": 0.9700685739517212}, {"text": "WEB", "start_pos": 72, "end_pos": 75, "type": "DATASET", "confidence": 0.7436648011207581}]}, {"text": "It is reasonable to assume that on average there are much fewer documents with query words in AQUAINT corpus than on the WEB.", "labels": [], "entities": [{"text": "AQUAINT corpus", "start_pos": 94, "end_pos": 108, "type": "DATASET", "confidence": 0.8464553654193878}, {"text": "WEB", "start_pos": 121, "end_pos": 124, "type": "DATASET", "confidence": 0.980413019657135}]}, {"text": "Proportion of correct documents from all retrieved WEB documents on average is likely to be lower than this proportion in documents retrieved from AQUAINT.", "labels": [], "entities": [{"text": "AQUAINT", "start_pos": 147, "end_pos": 154, "type": "DATASET", "confidence": 0.934408962726593}]}, {"text": "When using words on a query to AQUAINT dataset, most of the correct documents are returned in the top matches.", "labels": [], "entities": [{"text": "AQUAINT dataset", "start_pos": 31, "end_pos": 46, "type": "DATASET", "confidence": 0.8904772102832794}]}, {"text": "Our results indicate that over 50% of correct documents are retrieved in the top 20 results.", "labels": [], "entities": []}, {"text": "indicate that exactly matched phrases from a question are more precise predictors of presence of an answer.", "labels": [], "entities": []}, {"text": "Using exact matched phrases in a WEB query allows a search engine to give higher rank to more relevant documents and increases likelihood of these documents in the top 20 matches.", "labels": [], "entities": []}, {"text": "Although overall performance on the WEB dataset is lower than on AQUAINT, there is a po-7 Our decision to use only 20 documents maybe a factor.", "labels": [], "entities": [{"text": "WEB dataset", "start_pos": 36, "end_pos": 47, "type": "DATASET", "confidence": 0.9455087780952454}, {"text": "AQUAINT", "start_pos": 65, "end_pos": 72, "type": "DATASET", "confidence": 0.8585078120231628}]}, {"text": "tential for improvement by using a larger set of documents and improving our sentence extraction heuristics.", "labels": [], "entities": [{"text": "sentence extraction heuristics", "start_pos": 77, "end_pos": 107, "type": "TASK", "confidence": 0.7878695527712504}]}], "tableCaptions": [{"text": " Table 3: Query constituents in sentences of correct documents", "labels": [], "entities": [{"text": "Query constituents in sentences of correct documents", "start_pos": 10, "end_pos": 62, "type": "TASK", "confidence": 0.7986128926277161}]}, {"text": " Table 4: Document retrieval evaluation.", "labels": [], "entities": [{"text": "Document retrieval evaluation", "start_pos": 10, "end_pos": 39, "type": "TASK", "confidence": 0.9311420718828837}]}]}