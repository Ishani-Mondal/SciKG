{"title": [{"text": "Attribute Selection for Referring Expression Generation: New Algorithms and Evaluation Methods", "labels": [], "entities": [{"text": "Referring Expression Generation", "start_pos": 24, "end_pos": 55, "type": "TASK", "confidence": 0.9205727179845175}]}], "abstractContent": [{"text": "Referring expression generation has recently been the subject of the first Shared Task Challenge in NLG.", "labels": [], "entities": [{"text": "Referring expression generation", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.7677856882413229}]}, {"text": "In this paper, we analyse the systems that participated in the Challenge in terms of their algorithmic properties, comparing new techniques to classic ones, based on results from anew human task-performance experiment and from the intrinsic measures that were used in the Challenge.", "labels": [], "entities": []}, {"text": "We also consider the relationship between different evaluation methods, showing that extrinsic task-performance experiments and intrinsic evaluation methods yield results that are not significantly correlated.", "labels": [], "entities": []}, {"text": "We argue that this highlights the importance of including extrinsic evaluation methods in comparative NLG evaluations .", "labels": [], "entities": []}], "introductionContent": [], "datasetContent": [{"text": "Though ASGRE evaluations have been carried out (, these have focused on 'classic' algorithms, and have been corpus-based.", "labels": [], "entities": [{"text": "ASGRE", "start_pos": 7, "end_pos": 12, "type": "TASK", "confidence": 0.9535703659057617}]}, {"text": "The absence of task-performance evaluations is surprising, considering the well-defined nature of the ASGRE task, and the predominance of task-performance studies elsewhere in the NLG evaluation literature.", "labels": [], "entities": [{"text": "ASGRE task", "start_pos": 102, "end_pos": 112, "type": "TASK", "confidence": 0.8827735781669617}]}, {"text": "Given the widespread agreement on task definition and input/output specifications, ASGRE was an ideal candidate for the first NLG shared task evaluation challenge.", "labels": [], "entities": [{"text": "ASGRE", "start_pos": 83, "end_pos": 88, "type": "TASK", "confidence": 0.8693591356277466}, {"text": "NLG shared task evaluation", "start_pos": 126, "end_pos": 152, "type": "TASK", "confidence": 0.6955814808607101}]}, {"text": "The challenge was first discussed during a workshop held at Arlington, Va.", "labels": [], "entities": [{"text": "Arlington, Va", "start_pos": 60, "end_pos": 73, "type": "DATASET", "confidence": 0.8845136960347494}]}, {"text": "(, and eventually organised as part of the UCNLG+MT Workshop in.", "labels": [], "entities": [{"text": "UCNLG+MT Workshop", "start_pos": 43, "end_pos": 60, "type": "DATASET", "confidence": 0.9114929586648941}]}, {"text": "The ASGRE Shared Task provided an opportunity to (a) assess the extent to which the field has diversified since its inception; (b) carryout a comparative evaluation involving both automatic methods and human task-performance methods.", "labels": [], "entities": [{"text": "ASGRE Shared Task", "start_pos": 4, "end_pos": 21, "type": "TASK", "confidence": 0.7649867335955302}]}, {"text": "Evaluation methods can be characterised as either intrinsic or extrinsic.", "labels": [], "entities": []}, {"text": "While intrinsic methods evaluate the outputs of algorithms in their own right, either relative to a corpus or based on absolute evaluation metrics, extrinsic methods assess the effect of an algorithm on something external to it, such as its effect on human performance on some external task.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Results for systems and evaluation measures (in order of Dice).", "labels": [], "entities": [{"text": "Dice", "start_pos": 67, "end_pos": 71, "type": "METRIC", "confidence": 0.9628382325172424}]}, {"text": " Table 3: Means on intrinsic and extrinsic measures, by system type.", "labels": [], "entities": [{"text": "Means", "start_pos": 10, "end_pos": 15, "type": "METRIC", "confidence": 0.9674002528190613}]}, {"text": " Table 4: Multivariate tests examining impact of system  type on evaluation measures. Cells indicate F \u2212values  with 4 numerator and 2201 error degrees of freedom.  *  :  p \u2264 .05 after Bonferroni correction.", "labels": [], "entities": [{"text": "F", "start_pos": 101, "end_pos": 102, "type": "METRIC", "confidence": 0.970458447933197}]}, {"text": " Table 6: Pairwise correlations between humanlikeness  and task-performance measures (  *  : p \u2264 .05;  *  *  : p \u2264 .01)", "labels": [], "entities": []}]}