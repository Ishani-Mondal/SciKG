{"title": [{"text": "Argumentative Human Computer Dialogue for Automated Persuasion", "labels": [], "entities": [{"text": "Argumentative Human Computer Dialogue", "start_pos": 0, "end_pos": 37, "type": "TASK", "confidence": 0.6660167425870895}]}], "abstractContent": [{"text": "Argumentation is an emerging topic in the field of human computer dialogue.", "labels": [], "entities": [{"text": "human computer dialogue", "start_pos": 51, "end_pos": 74, "type": "TASK", "confidence": 0.6422766248385111}]}, {"text": "In this paper we describe a novel approach to dialogue management that has been developed to achieve persuasion using a textual argumen-tation dialogue system.", "labels": [], "entities": [{"text": "dialogue management", "start_pos": 46, "end_pos": 65, "type": "TASK", "confidence": 0.8440918624401093}]}, {"text": "The paper introduces a layered management architecture that mixes task-oriented dialogue techniques with chat-bot techniques to achieve better persuasive-ness in the dialogue.", "labels": [], "entities": []}], "introductionContent": [{"text": "Human computer dialogue is a wide research area in Artificial Intelligence.", "labels": [], "entities": [{"text": "Human computer dialogue", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.6396482388178507}]}, {"text": "Computer dialogue is now used at production stage for applications such as tutorial dialogue -that helps teaching students) -task-oriented dialogue -that achieves a particular, limited task, such as booking a trip () -and chatbot dialogue () -that is used within entertainment and help systems.", "labels": [], "entities": []}, {"text": "None of these approaches use persuasion as a mechanism to achieve dialogue goals.", "labels": [], "entities": []}, {"text": "However, research towards the use of persuasion in Human Computer Interactions has spawned around the field of natural argumentation.", "labels": [], "entities": []}, {"text": "Similarly research on Embodied Conversational Agents (ECA)) is also attempting to improve the persuasiveness of agents with persuasion techniques; however, it concentrates on the visual representation of the interlocutor rather than the dialogue management.", "labels": [], "entities": []}, {"text": "Previous research on human computer dialogue has rarely focused on persuasive techniques, initiated some research in that field).", "labels": [], "entities": [{"text": "human computer dialogue", "start_pos": 21, "end_pos": 44, "type": "TASK", "confidence": 0.6540942490100861}]}, {"text": "Our dialogue management system applies a novel method, taking advantage of persuasive and argumentation techniques to achieve persuasive dialogue.", "labels": [], "entities": []}, {"text": "According to the cognitive dissonance theory, people will try to minimise the discrepancy between their behaviour and their beliefs by integrating new beliefs or distorting existing ones.", "labels": [], "entities": []}, {"text": "In this paper, we approach persuasion as a process shaping user's beliefs to eventually change their behaviour.", "labels": [], "entities": []}, {"text": "The presented dialogue management system has been developed to work on known limitations of current dialogue systems: The impression of lack of control is an issue when the user is interacting with a purely task-oriented dialogue system).", "labels": [], "entities": []}, {"text": "The system follows a plan to achieve the particular task, and the user's dialogue moves are dictated by the planner and the plan operators.", "labels": [], "entities": []}, {"text": "The lack of empathy of computers is also a problem in human-computer interaction for applications such as health-care, where persuasive dialogue could be applied ().", "labels": [], "entities": []}, {"text": "The system does not respond to the user's personal and emotional state, which sometimes lowers the user's implication in the dialogue.", "labels": [], "entities": []}, {"text": "However, existing research) shows that a system that gives appropriate response to the user's emotion can lower frustration.", "labels": [], "entities": []}, {"text": "In human-human communication, these limitations reduce the effectiveness of persuasion).", "labels": [], "entities": []}, {"text": "Even if the response towards the computer is not always identical to the one to humans, it seems sensible to think that persuasive dialogue systems can be improved by applying known findings from human-human communication.", "labels": [], "entities": []}, {"text": "The dialogue management architecture described in this paper (see) addresses these dialogue management issues by using a novel layered approach to dialogue management, allowing the mixing of techniques from task-oriented dialogue management and chatbot techniques (see Section 4).", "labels": [], "entities": [{"text": "dialogue management", "start_pos": 147, "end_pos": 166, "type": "TASK", "confidence": 0.7372345626354218}]}, {"text": "The use of a planner guarantees the consistency of the dialogue and the achievement of persuasive goals (see Section 4.2).", "labels": [], "entities": [{"text": "consistency", "start_pos": 36, "end_pos": 47, "type": "METRIC", "confidence": 0.9536206722259521}]}, {"text": "Argumentative dialogue can be seen as a form of task-oriented dialogue where the system's task is to persuade the user by presenting the arguments.", "labels": [], "entities": []}, {"text": "Thus, the dialogue manager first uses a task-oriented dialogue methodology to create a dialogue plan that will determine the content of the dialogue.", "labels": [], "entities": []}, {"text": "The planning component's role is to guarantee the consistency of the dialogue and the achievement of the persuasive goals.", "labels": [], "entities": [{"text": "consistency", "start_pos": 50, "end_pos": 61, "type": "METRIC", "confidence": 0.9618023037910461}]}, {"text": "In state-of-the-art task-oriented dialogue management systems, the planner provides instructions fora surface realizer (), responsible of generating the utterance corresponding to the plan step.", "labels": [], "entities": []}, {"text": "Our approach is different to allow more reactivity to the user and give a feeling of control over the dialogue.", "labels": [], "entities": []}, {"text": "In this layered approach, the reactive component provides a direct reaction to the user input, generating one or more utterances fora given plan step, allowing for reactions to user's counter arguments as well as backchannel and chitchat phases without cluttering the plan.", "labels": [], "entities": []}, {"text": "Experimental results show that this layered approach allows the user to feel more comfortable in the dialogue while preserving the dialogue consistency provided by the planner.", "labels": [], "entities": []}, {"text": "Eventually, this translates into a more persuasive dialogue (see Section 6).", "labels": [], "entities": []}], "datasetContent": [{"text": "16 participants have been recruited from a variety of ages (from 20 to 59) and background.", "labels": [], "entities": []}, {"text": "They were all told to use a web application that describes the Desert Scenario (see Section 3) and proposes to undertake two instant messaging chats with two human users 5 . However, both discussions are managed by different versions of the dialogue system, following a similar protocol: \u2022 one version of the dialogue is managed by a limited version of the dialogue system, with no reactive component.", "labels": [], "entities": []}, {"text": "This version is similar to a purely task-oriented system, planning and revising the plan directly on dialogue failures, \u2022 the second version is the full dialogue system as described in this paper.", "labels": [], "entities": []}, {"text": "Each participant went through one dialogue with each system, in a random order.", "labels": [], "entities": []}, {"text": "This comparison shows that the dialogue flexibility provided by the reactive component allows a more persuasive dialogue.", "labels": [], "entities": []}, {"text": "In addition, when faced with the second dialogue, the participant has formed more beliefs about the scenario and is more able to counter argue.", "labels": [], "entities": []}, {"text": "Over all the dialogues, the full system is 18% more persuasive than the limited system.", "labels": [], "entities": []}, {"text": "This is measured by the P ersuasiveness metric introduced in Section 5.", "labels": [], "entities": [{"text": "P ersuasiveness metric", "start_pos": 24, "end_pos": 46, "type": "METRIC", "confidence": 0.9274589419364929}, {"text": "Section 5", "start_pos": 61, "end_pos": 70, "type": "DATASET", "confidence": 0.8836947977542877}]}, {"text": "With the full system, the participants did an average of 1.33 swaps of items towards the system's ranking.", "labels": [], "entities": []}, {"text": "With the limited system, the participants did an average of 0.47 swaps of items away from the system's ranking.", "labels": [], "entities": []}, {"text": "However, the answers to the self evaluated perceived persuasion question show that the participants did not see any significant difference in the ability to persuade of the limited and the full systems.", "labels": [], "entities": []}, {"text": "According to the question interpret, the participants found that the limited system understood better what they said.", "labels": [], "entities": []}, {"text": "This last result might be explained by the behavior of the systems: the limited system drops an argument at every user disagreement, making the user believe that the disagreement was understood.", "labels": [], "entities": []}, {"text": "The full system tries to defend the argument; if possible with a contextually tailored support, however, if this is not available, it may use a generic support, making the user believe he was not fully understood.", "labels": [], "entities": []}, {"text": "Our interpretation of the fact that the discrepancy between user self evaluation of the interaction with the system and the measured persuasion is that, even if the full system is more argumentative, the user didn't feel coerced . These results show that a more persuasive dialogue can be achieved without deteriorating the user perception of the interaction.", "labels": [], "entities": []}, {"text": "In the evaluation described in section 6, the participants were asked to give their level of agreement with each statement on the scale: Strongly disagree (0), Disagree (1), Neither agree nor disagree (2), Agree (3), Strongly Agree(4).", "labels": [], "entities": [{"text": "Disagree", "start_pos": 160, "end_pos": 168, "type": "METRIC", "confidence": 0.9604365229606628}, {"text": "Agree", "start_pos": 206, "end_pos": 211, "type": "METRIC", "confidence": 0.9972922205924988}, {"text": "Agree", "start_pos": 226, "end_pos": 231, "type": "METRIC", "confidence": 0.9543651342391968}]}, {"text": "provides a list of questions with the average agreement level and the result of a paired t-test between the two system results.", "labels": [], "entities": []}, {"text": "label question full system limited system ttest interpret \"In the conversation, the other user interpreted correctly what you said\" 1.73 2.13 0.06 perceived persuasion \"In the conversation, the other user was persuasive\"", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Accuracy of different agreement/disagreement  classification approaches.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9923506379127502}, {"text": "agreement/disagreement  classification", "start_pos": 32, "end_pos": 70, "type": "TASK", "confidence": 0.6157479137182236}]}]}