{"title": [{"text": "Improving English-Spanish Statistical Machine Translation: Experiments in Domain Adaptation, Sentence Paraphrasing, Tokenization, and Recasing", "labels": [], "entities": [{"text": "Statistical Machine Translation", "start_pos": 26, "end_pos": 57, "type": "TASK", "confidence": 0.6992398301760355}, {"text": "Sentence Paraphrasing", "start_pos": 93, "end_pos": 114, "type": "TASK", "confidence": 0.8372507989406586}, {"text": "Recasing", "start_pos": 134, "end_pos": 142, "type": "TASK", "confidence": 0.8047077059745789}]}], "abstractContent": [{"text": "We describe the experiments of the UC Berke-ley team on improving English-Spanish machine translation of news text, as part of the WMT'08 Shared Translation Task.", "labels": [], "entities": [{"text": "English-Spanish machine translation of news text", "start_pos": 66, "end_pos": 114, "type": "TASK", "confidence": 0.7828658719857534}, {"text": "WMT'08 Shared Translation Task", "start_pos": 131, "end_pos": 161, "type": "TASK", "confidence": 0.7975883483886719}]}, {"text": "We experiment with domain adaptation, combining a small in-domain news bi-text and a large out-of-domain one from the Europarl corpus, building two separate phrase translation models and two separate language models.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 19, "end_pos": 36, "type": "TASK", "confidence": 0.7248255908489227}, {"text": "Europarl corpus", "start_pos": 118, "end_pos": 133, "type": "DATASET", "confidence": 0.993346095085144}, {"text": "phrase translation", "start_pos": 157, "end_pos": 175, "type": "TASK", "confidence": 0.7282787710428238}]}, {"text": "We further add a third phrase translation model trained on aversion of the news bi-text augmented with monolingual sentence-level syntactic paraphrases on the source-language side, and we combine all models in a log-linear model using minimum error rate training.", "labels": [], "entities": [{"text": "phrase translation", "start_pos": 23, "end_pos": 41, "type": "TASK", "confidence": 0.733799159526825}]}, {"text": "Finally, we experiment with different tokenization and recasing rules, achieving 35.09% Bleu score on the WMT'07 news test data when translating from English to Span-ish, which is a sizable improvement over the highest Bleu score achieved on that dataset at WMT'07: 33.10% (in fact, by our system).", "labels": [], "entities": [{"text": "Bleu score", "start_pos": 88, "end_pos": 98, "type": "METRIC", "confidence": 0.9852918684482574}, {"text": "WMT'07 news test data", "start_pos": 106, "end_pos": 127, "type": "DATASET", "confidence": 0.981021836400032}, {"text": "Bleu score", "start_pos": 219, "end_pos": 229, "type": "METRIC", "confidence": 0.968400627374649}, {"text": "WMT'07", "start_pos": 258, "end_pos": 264, "type": "DATASET", "confidence": 0.95912104845047}]}, {"text": "On the WMT'08 English to Spanish news translation, we achieve 21.92%, which makes our team the second best on Bleu score.", "labels": [], "entities": [{"text": "WMT'08 English to Spanish news translation", "start_pos": 7, "end_pos": 49, "type": "DATASET", "confidence": 0.9070741633574168}, {"text": "Bleu", "start_pos": 110, "end_pos": 114, "type": "METRIC", "confidence": 0.8799355030059814}]}], "introductionContent": [{"text": "Modern Statistical Machine Translation (SMT) systems are trained on sentence-aligned bilingual corpora, typically from a single domain.", "labels": [], "entities": [{"text": "Statistical Machine Translation (SMT)", "start_pos": 7, "end_pos": 44, "type": "TASK", "confidence": 0.8210551738739014}]}, {"text": "When tested on text from that same domain, they demonstrate * After January 2008 at the Linguistic Modeling Department, Institute for Parallel Processing, Bulgarian Academy of Sciences, nakov@lml.bas.bg state-of-the art performance, but on out-of-domain test data the results can get significantly worse.", "labels": [], "entities": []}, {"text": "For example, on the WMT'06 Shared Translation Task, the scores for French to English translation dropped from about 30 to about 20 Bleu points for nearly all systems when tested on News Commentary rather than Europarl text, which was used on training ().", "labels": [], "entities": [{"text": "WMT'06 Shared Translation Task", "start_pos": 20, "end_pos": 50, "type": "TASK", "confidence": 0.7358792275190353}, {"text": "French to English translation", "start_pos": 67, "end_pos": 96, "type": "TASK", "confidence": 0.6425289660692215}, {"text": "Bleu", "start_pos": 131, "end_pos": 135, "type": "METRIC", "confidence": 0.9942929744720459}, {"text": "Europarl text", "start_pos": 209, "end_pos": 222, "type": "DATASET", "confidence": 0.950825959444046}]}, {"text": "Therefore, in 2007 the Shared Task organizers provided 1M words of bilingual News Commentary training data in addition to the 30M Europarl data, thus inviting interest in domain adaptation experiments.", "labels": [], "entities": [{"text": "Europarl data", "start_pos": 130, "end_pos": 143, "type": "DATASET", "confidence": 0.9880717098712921}, {"text": "domain adaptation", "start_pos": 171, "end_pos": 188, "type": "TASK", "confidence": 0.7132578939199448}]}, {"text": "Given the success of the idea, the same task was offered this year with slightly larger training bitexts: 1.3M and 32M words, respectively.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": []}