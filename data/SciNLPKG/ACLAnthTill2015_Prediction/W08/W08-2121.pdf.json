{"title": [{"text": "The CoNLL-2008 Shared Task on Joint Parsing of Syntactic and Semantic Dependencies", "labels": [], "entities": []}], "abstractContent": [{"text": "The Conference on Computational Natural Language Learning is accompanied every year by a shared task whose purpose is to promote natural language processing applications and evaluate them in a standard setting.", "labels": [], "entities": []}, {"text": "In 2008 the shared task was dedicated to the joint parsing of syntactic and semantic dependencies.", "labels": [], "entities": [{"text": "parsing of syntactic and semantic dependencies", "start_pos": 51, "end_pos": 97, "type": "TASK", "confidence": 0.733063538869222}]}, {"text": "This shared task not only unifies the shared tasks of the previous four years under a unique dependency-based formalism, but also extends them significantly: this year's syntactic dependencies include more information such as named-entity boundaries; the semantic dependencies model roles of both verbal and nominal predicates.", "labels": [], "entities": []}, {"text": "In this paper , we define the shared task and describe how the data sets were created.", "labels": [], "entities": []}, {"text": "Furthermore , we report and analyze the results and describe the approaches of the participating systems.", "labels": [], "entities": []}], "introductionContent": [{"text": "In 2004 and 2005 the shared tasks of the Conference on Computational Natural Language Learning (CoNLL) were dedicated to semantic role labeling (SRL), in a monolingual setting.", "labels": [], "entities": [{"text": "semantic role labeling (SRL)", "start_pos": 121, "end_pos": 149, "type": "TASK", "confidence": 0.7725782146056493}]}, {"text": "In 2006 and 2007 the shared tasks were devoted to the parsing of syntactic dependencies, using corpora from up to 13 languages.", "labels": [], "entities": [{"text": "parsing of syntactic dependencies", "start_pos": 54, "end_pos": 87, "type": "TASK", "confidence": 0.8638057559728622}]}, {"text": "The CoNLL-2008 shared task 1 proposes a unified dependency-based formalism, which models both syntactic dependencies and semantic roles.", "labels": [], "entities": [{"text": "CoNLL-2008 shared task 1", "start_pos": 4, "end_pos": 28, "type": "DATASET", "confidence": 0.8204354792833328}]}, {"text": "Using this formalism, this shared task merges both the task of syntactic dependency parsing and the task of identifying semantic arguments and labeling them with semantic roles.", "labels": [], "entities": [{"text": "syntactic dependency parsing", "start_pos": 63, "end_pos": 91, "type": "TASK", "confidence": 0.689939538637797}]}, {"text": "Conceptually, the 2008 shared task can be divided into three subtasks: (i) parsing of syntactic dependencies, (ii) identification and disambiguation of semantic predicates, and (iii) identification of arguments and assignment of semantic roles for each predicate.", "labels": [], "entities": [{"text": "parsing of syntactic dependencies", "start_pos": 75, "end_pos": 108, "type": "TASK", "confidence": 0.8310754001140594}, {"text": "identification and disambiguation of semantic predicates", "start_pos": 115, "end_pos": 171, "type": "TASK", "confidence": 0.773711770772934}, {"text": "identification of arguments", "start_pos": 183, "end_pos": 210, "type": "TASK", "confidence": 0.8589272499084473}]}, {"text": "Several objectives were addressed in this shared task: \u2022 SRL is performed and evaluated using a dependency-based representation for both syntactic and semantic dependencies.", "labels": [], "entities": [{"text": "SRL", "start_pos": 57, "end_pos": 60, "type": "TASK", "confidence": 0.9790025949478149}]}, {"text": "While SRL on top of a dependency treebank has been addressed before, our approach has several novelties: (i) our constituent-to-dependency conversion strategy transforms all annotated semantic arguments in PropBank and NomBank not just a subset; (ii) we address propositions centered around both verbal (PropBank) and nominal (NomBank) predicates.", "labels": [], "entities": [{"text": "SRL", "start_pos": 6, "end_pos": 9, "type": "TASK", "confidence": 0.9847811460494995}]}, {"text": "\u2022 Based on the observation that a richer set of syntactic dependencies improves semantic processing, the syntactic dependencies modeled are more complex than the ones used in the previous CoNLL shared tasks.", "labels": [], "entities": [{"text": "semantic processing", "start_pos": 80, "end_pos": 99, "type": "TASK", "confidence": 0.8016473650932312}, {"text": "CoNLL shared tasks", "start_pos": 188, "end_pos": 206, "type": "TASK", "confidence": 0.5742133657137553}]}, {"text": "For example, we now include apposition links, dependencies derived from named entity (NE) structures, and better modeling of long-distance grammatical relations.", "labels": [], "entities": []}, {"text": "\u2022 A practical framework is provided for the joint learning of syntactic and semantic dependencies.", "labels": [], "entities": []}, {"text": "Given the complexity of this shared task, we limited the evaluation to a monolingual, Englishonly setting.", "labels": [], "entities": []}, {"text": "The evaluation is separated into two different challenges: a closed challenge, where systems have to be trained strictly with information contained in the given training corpus, and an open challenge, where systems can be developed making use of any kind of external tools and resources.", "labels": [], "entities": []}, {"text": "The participants could submit results in either one or both challenges.", "labels": [], "entities": []}, {"text": "This paper is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 defines the task, including the format of the data, the evaluation metrics, and the two challenges.", "labels": [], "entities": []}, {"text": "Section 3 introduces the corpora used and our constituent-to-dependency conversion procedure.", "labels": [], "entities": [{"text": "constituent-to-dependency conversion", "start_pos": 46, "end_pos": 82, "type": "TASK", "confidence": 0.6897708773612976}]}, {"text": "Section 4 summarizes the results of the submitted systems.", "labels": [], "entities": []}, {"text": "Section 5 discusses the approaches implemented by participants.", "labels": [], "entities": []}, {"text": "Section 6 analyzes the results using additional non-official evaluation measures.", "labels": [], "entities": []}, {"text": "Section 7 concludes the paper.", "labels": [], "entities": []}], "datasetContent": [{"text": "We separate the evaluation measures into two groups: (i) official measures, which were used for the ranking of participating systems, and (ii) additional unofficial measures, which provide further insight into the performance of the participating systems.", "labels": [], "entities": []}, {"text": "The official evaluation measures consist of three different scores: (i) syntactic dependencies are scored using the labeled attachment score (LAS), (ii) semantic dependencies are evaluated using a labeled F 1 score, and (iii) the overall task is scored with a macro average of the two previous scores.", "labels": [], "entities": [{"text": "labeled attachment score (LAS)", "start_pos": 116, "end_pos": 146, "type": "METRIC", "confidence": 0.7954999009768168}]}, {"text": "We describe all these scoring measures next.", "labels": [], "entities": []}, {"text": "The LAS score is defined similarly as in the previous two shared tasks, as the percentage of to-2 LDC catalog number LDC2005T33.", "labels": [], "entities": [{"text": "LAS score", "start_pos": 4, "end_pos": 13, "type": "METRIC", "confidence": 0.9187724888324738}, {"text": "LDC catalog number LDC2005T33", "start_pos": 98, "end_pos": 127, "type": "DATASET", "confidence": 0.7678196802735329}]}, {"text": "kens for which a system has predicted the correct HEAD and DEPREL columns (see).", "labels": [], "entities": [{"text": "HEAD", "start_pos": 50, "end_pos": 54, "type": "METRIC", "confidence": 0.9106730222702026}, {"text": "DEPREL columns", "start_pos": 59, "end_pos": 73, "type": "METRIC", "confidence": 0.9408053755760193}]}, {"text": "Same as before, our scorer also computes the unlabeled attachment score (UAS), i.e., the percentage of tokens with correct HEAD, and label accuracy, i.e., the percentage of tokens with correct DEPREL.", "labels": [], "entities": [{"text": "unlabeled attachment score (UAS)", "start_pos": 45, "end_pos": 77, "type": "METRIC", "confidence": 0.7849412510792414}, {"text": "HEAD", "start_pos": 123, "end_pos": 127, "type": "METRIC", "confidence": 0.744114875793457}, {"text": "accuracy", "start_pos": 139, "end_pos": 147, "type": "METRIC", "confidence": 0.767958402633667}, {"text": "DEPREL", "start_pos": 193, "end_pos": 199, "type": "METRIC", "confidence": 0.9255726337432861}]}, {"text": "The semantic propositions are evaluated by converting them to semantic dependencies, i.e., we create a semantic dependency from every predicate to all its individual arguments.", "labels": [], "entities": []}, {"text": "These dependencies are labeled with the labels of the corresponding arguments.", "labels": [], "entities": []}, {"text": "Additionally, we create a semantic dependency from each predicate to a virtual ROOT node.", "labels": [], "entities": []}, {"text": "The latter dependencies are labeled with the predicate senses.", "labels": [], "entities": []}, {"text": "This approach guarantees that the semantic dependency structure conceptually forms a single-rooted, connected (but not necessarily acyclic) graph.", "labels": [], "entities": []}, {"text": "More importantly, this scoring strategy implies that if a system assigns the incorrect predicate sense, it still receives some points for the arguments correctly assigned.", "labels": [], "entities": []}, {"text": "For example, for the correct proposition: verb.01: ARG0, ARG1, ARGM-TMP the system that generates the following output for the same argument tokens: verb.02: ARG0, ARG1, ARGM-LOC receives a labeled precision score of 2/4 because two out of four semantic dependencies are incorrect: the dependency to ROOT is labeled 02 instead of 01 and the dependency to the ARGM-TMP is incorrectly labeled ARGM-LOC.", "labels": [], "entities": [{"text": "ARG0", "start_pos": 158, "end_pos": 162, "type": "DATASET", "confidence": 0.870852530002594}, {"text": "ARG1", "start_pos": 164, "end_pos": 168, "type": "DATASET", "confidence": 0.8309153914451599}, {"text": "labeled precision score", "start_pos": 190, "end_pos": 213, "type": "METRIC", "confidence": 0.7510660688082377}]}, {"text": "Using this strategy we compute precision, recall, and F 1 scores for both labeled and unlabeled semantic dependencies.", "labels": [], "entities": [{"text": "precision", "start_pos": 31, "end_pos": 40, "type": "METRIC", "confidence": 0.9992722868919373}, {"text": "recall", "start_pos": 42, "end_pos": 48, "type": "METRIC", "confidence": 0.9991381168365479}, {"text": "F 1 scores", "start_pos": 54, "end_pos": 64, "type": "METRIC", "confidence": 0.9891662001609802}]}, {"text": "Finally, we combine the syntactic and semantic measures into one global measure using macro averaging.", "labels": [], "entities": []}, {"text": "We compute macro precision and recall scores by averaging the labeled precision and recall for semantic dependencies with the LAS for syntactic dependencies: 3 where LM P is the labeled macro precision and LP sem is the labeled precision for semantic dependencies.", "labels": [], "entities": [{"text": "precision", "start_pos": 17, "end_pos": 26, "type": "METRIC", "confidence": 0.867295503616333}, {"text": "recall", "start_pos": 31, "end_pos": 37, "type": "METRIC", "confidence": 0.9942734837532043}, {"text": "recall", "start_pos": 84, "end_pos": 90, "type": "METRIC", "confidence": 0.9969490170478821}, {"text": "LAS", "start_pos": 126, "end_pos": 129, "type": "METRIC", "confidence": 0.9955671429634094}]}, {"text": "Similarly, LM R is the labeled macro recall and LR sem is the labeled recall for semantic dependencies.", "labels": [], "entities": [{"text": "recall", "start_pos": 37, "end_pos": 43, "type": "METRIC", "confidence": 0.7369614243507385}, {"text": "recall", "start_pos": 70, "end_pos": 76, "type": "METRIC", "confidence": 0.7713912129402161}]}, {"text": "W sem is the weight assigned to the semantic task.", "labels": [], "entities": []}, {"text": "The macro labeled F 1 score, which was used for the ranking of the participating systems, is computed as the harmonic mean of LM P and LM R.", "labels": [], "entities": [{"text": "F 1 score", "start_pos": 18, "end_pos": 27, "type": "METRIC", "confidence": 0.9463821053504944}]}, {"text": "We used several additional evaluation measures to further analyze the performance of the participating systems.", "labels": [], "entities": []}, {"text": "The first additional measure used is Exact Match, which reports the percentage of sentences that are completely correct, i.e., all the generated syntactic dependencies are correct and all the semantic propositions are present and correct.", "labels": [], "entities": [{"text": "Exact Match", "start_pos": 37, "end_pos": 48, "type": "METRIC", "confidence": 0.4665441960096359}]}, {"text": "While this score is significantly lower than any of the official scores, it will award systems that performed joint learning or optimization for all subtasks.", "labels": [], "entities": []}, {"text": "In the same spirit but focusing on the semantic subtasks, we report the Perfect Proposition F 1 score, where we score entire semantic frames or propositions.", "labels": [], "entities": [{"text": "Perfect Proposition F 1 score", "start_pos": 72, "end_pos": 101, "type": "METRIC", "confidence": 0.83233802318573}]}, {"text": "This measure is similar to the PProps accuracy score from the 2005 shared task, with the caveat that this year this score is implemented as an F 1 measure, because predicates are not provided in the test data.", "labels": [], "entities": [{"text": "accuracy score", "start_pos": 38, "end_pos": 52, "type": "METRIC", "confidence": 0.8926331102848053}, {"text": "F 1 measure", "start_pos": 143, "end_pos": 154, "type": "METRIC", "confidence": 0.9595478375752767}]}, {"text": "Hence, propositions maybe over or under generated at prediction time.", "labels": [], "entities": []}, {"text": "Lastly, we analyze systems based on the ratio between labeled F 1 score for semantic dependencies and the LAS for syntactic dependencies.", "labels": [], "entities": [{"text": "F 1 score", "start_pos": 62, "end_pos": 71, "type": "METRIC", "confidence": 0.9271043737729391}, {"text": "LAS", "start_pos": 106, "end_pos": 109, "type": "METRIC", "confidence": 0.9968534111976624}]}, {"text": "In other words, this measure normalizes the semantic scores relative to the performance of the parsing component.", "labels": [], "entities": []}, {"text": "This measure estimates the true overall performance of the semantic subtasks, independent of the syntactic parser.", "labels": [], "entities": []}, {"text": "For example, this score addresses the situations where the semantic labeled F 1 score of one system is artificially low because the corresponding syntactic component does not perform well.", "labels": [], "entities": [{"text": "F 1 score", "start_pos": 76, "end_pos": 85, "type": "METRIC", "confidence": 0.838792065779368}]}], "tableCaptions": [{"text": " Table 1: Column format in the closed-track data. The columns in the lower part of the table are unseen  at test time and are to be predicted by systems.", "labels": [], "entities": []}, {"text": " Table 5: Statistics for non-atomic syntactic labels  excluding gapping labels.", "labels": [], "entities": []}, {"text": " Table 6: Statistics for non-atomic labels containing  a gapping label.", "labels": [], "entities": []}, {"text": " Table 7: Statistics for nonprojective links.", "labels": [], "entities": []}, {"text": " Table 8: Statistics for predicates, by POS tags.", "labels": [], "entities": []}, {"text": " Table 9: Statistics for semantic roles.", "labels": [], "entities": [{"text": "semantic roles", "start_pos": 25, "end_pos": 39, "type": "TASK", "confidence": 0.7797924876213074}]}, {"text": " Table 13: Exact Match and Perfect Proposition F 1 scores for runs submitted in the closed and open  challenges. The closed-challenge systems are sorted in descending order of Exact Match scores on  the WSJ+Brown corpus. Open-challenge submissions are sorted in descending order of the Perfect  Proposition F 1 score. The number in parentheses next to the WSJ+Brown scores indicates the system  rank according to the corresponding scoring measure.", "labels": [], "entities": [{"text": "Perfect Proposition F 1 scores", "start_pos": 27, "end_pos": 57, "type": "METRIC", "confidence": 0.9112707495689392}, {"text": "WSJ+Brown corpus", "start_pos": 203, "end_pos": 219, "type": "DATASET", "confidence": 0.9440475404262543}, {"text": "WSJ+Brown scores", "start_pos": 356, "end_pos": 372, "type": "DATASET", "confidence": 0.9390210062265396}]}, {"text": " Table 14: Unlabeled F1-measures for nonprojec- tive links. Results are given for all links, wh- movements, split clauses, and split noun phrases.", "labels": [], "entities": [{"text": "F1-measures", "start_pos": 21, "end_pos": 32, "type": "METRIC", "confidence": 0.9105548858642578}]}, {"text": " Table 16: Labeled F 1 scores for frames centered around verbal and nominal predicates. The number in  parentheses next to the WSJ+Brown scores indicates the system rank in the corresponding data set.", "labels": [], "entities": [{"text": "F 1 scores", "start_pos": 19, "end_pos": 29, "type": "METRIC", "confidence": 0.9607742627461752}, {"text": "WSJ+Brown scores", "start_pos": 127, "end_pos": 143, "type": "DATASET", "confidence": 0.6831334978342056}]}]}