{"title": [{"text": "Learning to Map Text to Graph-based Meaning Representations via Grammar Induction", "labels": [], "entities": [{"text": "Map Text to Graph-based Meaning Representations", "start_pos": 12, "end_pos": 59, "type": "TASK", "confidence": 0.6257975200812022}]}], "abstractContent": [{"text": "We argue in favor of using a graph-based representation for language meaning and propose a novel learning method to map natural language text to its graph-based meaning representation.", "labels": [], "entities": []}, {"text": "We present a grammar formalism, which combines syntax and semantics, and has ontology constraints at the rule level.", "labels": [], "entities": []}, {"text": "These constraints establish links between language expressions and the entities they refer to in the real world.", "labels": [], "entities": []}, {"text": "We present a relational learning algorithm that learns these grammars from a small representative set of annotated examples , and show how this grammar induction framework and the ontology-based semantic representation allow us to directly map text to graph-based meaning representations.", "labels": [], "entities": []}], "introductionContent": [{"text": "Recent work) has developed learning algorithms for the problem of mapping sentences to their underlying semantic representations.", "labels": [], "entities": []}, {"text": "These semantic representations vary from \u03bb-expressions ( to DB query languages and command-like languages (RoboCup Coach Language, CLang) (Ge and.", "labels": [], "entities": []}, {"text": "In this paper we focus on an ontology-based semantic representation which allows us to encode the meaning of a text as a direct acyclic graph.", "labels": [], "entities": []}, {"text": "Recently, there is a growing interest on ontology-based NLP, starting from efforts in defining ontology-based semantic representations (), to using ontological resources in NLP applications, such as question answering (), and building annotated corpora, such as the OntoNotes project ().", "labels": [], "entities": [{"text": "question answering", "start_pos": 199, "end_pos": 217, "type": "TASK", "confidence": 0.9002107083797455}]}, {"text": "There are three novel properties to ontologybased semantics that we propose in this paper: \u2022 There is a direct link between the ontology and the grammar through constraints at the grammar rule level.", "labels": [], "entities": []}, {"text": "These ontology constraints enable access to meaning during language processing (parsing and generation).", "labels": [], "entities": [{"text": "parsing and generation", "start_pos": 80, "end_pos": 102, "type": "TASK", "confidence": 0.8371800581614176}]}, {"text": "\u2022 Our ontology-based semantic representation is expressive enough to capture various phenomena of natural language, yet restrictive enough to facilitate grammar learning.", "labels": [], "entities": []}, {"text": "The representation encodes both ontological meaning (concepts and relations among concepts) and extra-ontological meaning, such as voice, tense, aspect, modality.", "labels": [], "entities": []}, {"text": "\u2022 Our representation and grammar learning framework allow a direct mapping of text to its meaning, encoded as a direct acyclic graph (DAG).", "labels": [], "entities": []}, {"text": "We consider that \"understanding\" a text is the ability to correctly answer, at the conceptual level, all the questions asked w.r.t to that text, and thus Meaning = Text + all Questions/Answers w.r.t that Text.", "labels": [], "entities": []}, {"text": "Under this assumption, obtaining the meaning of a text is reduced to a question answering process, which in our framework is a DAG matching problem.", "labels": [], "entities": [{"text": "obtaining the meaning of a text", "start_pos": 23, "end_pos": 54, "type": "TASK", "confidence": 0.7418008148670197}, {"text": "question answering", "start_pos": 71, "end_pos": 89, "type": "TASK", "confidence": 0.7224424183368683}, {"text": "DAG matching", "start_pos": 127, "end_pos": 139, "type": "TASK", "confidence": 0.8020446300506592}]}, {"text": "First, we review our grammar formalism introduced in, called Lexicalized Well-Founded Grammars.", "labels": [], "entities": []}, {"text": "Second, we present a relational learning algorithm for inducing these grammars from a representative sample of strings annotated with their semantics, along with minimal assumptions about \u03a6o (b) returns \ud97b\udf59X 1 .isa = major, X.degree = X 1 , X.isa = damage\ud97b\udf59: Examples of three semantic molecules (I), and a constraint grammar rule together with the semantic composition and ontology-based interpretation constraints, \u03a6 c and \u03a6 o (II) syntax.", "labels": [], "entities": []}, {"text": "Then, we describe the levels of representation we use to go from utterances to their graphbased meaning representations, and show how our representation is suitable to define the meaning of an utterance/text through answers to questions.", "labels": [], "entities": []}, {"text": "As a proof of concept we discuss how our framework can be used to acquire terminological knowledge from natural language definitions and to query this knowledge using wh-questions.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": []}