{"title": [{"text": "Automatic Fine-Grained Semantic Classification for Domain Adaptation", "labels": [], "entities": [{"text": "Domain Adaptation", "start_pos": 51, "end_pos": 68, "type": "TASK", "confidence": 0.7104695588350296}]}], "abstractContent": [{"text": "Assigning arguments of verbs to different semantic classes ('semantic typing'), or alternatively, checking the 'selectional restrictions' of predicates , is a fundamental component of many natural language processing tasks.", "labels": [], "entities": []}, {"text": "However, a common experience has been that general purpose semantic classes, such as those encoded in resources like WordNet, or hand-crafted subject-specific ontologies, are seldom quite right when it comes to analysing texts from a particular domain.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 117, "end_pos": 124, "type": "DATASET", "confidence": 0.950272798538208}]}, {"text": "In this paper we describe a method of automatically deriving fine-grained, domain-specific semantic classes of arguments while simultaneously clustering verbs into semantically meaningful groups: the first step in verb sense induction.", "labels": [], "entities": [{"text": "verb sense induction", "start_pos": 214, "end_pos": 234, "type": "TASK", "confidence": 0.7243565022945404}]}, {"text": "We show that in a small pilot study on new examples from the same domain we are able to achieve almost perfect recall and reasonably high precision in the semantic typing of verb arguments in these texts.", "labels": [], "entities": [{"text": "recall", "start_pos": 111, "end_pos": 117, "type": "METRIC", "confidence": 0.9991200566291809}, {"text": "precision", "start_pos": 138, "end_pos": 147, "type": "METRIC", "confidence": 0.9988240599632263}]}], "introductionContent": [{"text": "Since the earliest days of computational linguistics the semantic properties of verbal arguments have played an important role in processing.", "labels": [], "entities": []}, {"text": "Many classic types of ambiguity, and hence their resolution, depend on this: 'flying planes can be dangerous' is ambiguous because 'flying planes' can describe an activity or a plural entity, either of which can be a semantically appropriate subject of 'be dangerous', whereas 'swallowing apples can be dangerous' does not display this ambiguity.", "labels": [], "entities": []}, {"text": "Both 'fly' and 'swallow' can be transitive or intransitive, but whereas 'planes' is both a semantically appropriate subject for intransitive 'fly' and an appropriate object for transitive 'fly', 'apples' is not a semantically appropiate subject for intransitive 'swallow'.", "labels": [], "entities": []}, {"text": "Semantic (mis)typing rules out this syntactically valid combination.", "labels": [], "entities": []}, {"text": "Similarly, an important component of reference resolution is the knowledge of what semantic category an entity falls under.", "labels": [], "entities": [{"text": "reference resolution", "start_pos": 37, "end_pos": 57, "type": "TASK", "confidence": 0.8434561192989349}]}, {"text": "For example, in 'The crop can be used to produce ethanol.", "labels": [], "entities": []}, {"text": "This can be used to power trucks or cars', knowledge that ethanol is the kind of thing that can be subject of 'power', whereas 'crop' is not, is required to successfully resolve the reference of 'this'.", "labels": [], "entities": []}, {"text": "When considering division into semantic categories one's immediate thought would be to take advantage of existing semantic resources (such as WordNet) or FrameNet ().", "labels": [], "entities": [{"text": "WordNet", "start_pos": 142, "end_pos": 149, "type": "DATASET", "confidence": 0.9319666028022766}]}, {"text": "For example, calculate the probability of a noun sense appearing as a particular argument by using WordNet to generalise over the noun sense.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 99, "end_pos": 106, "type": "DATASET", "confidence": 0.9801259636878967}]}, {"text": "However, even though WordNet has been extremely useful in numerous applications, many researchers have found that the fact that it is largely developed via the intuitions of lexicographers, rather than being empirically based, means that the semantic information often is poorly matched with word usage in a particular domain. and have pointed out that WordNet often includes many rare senses while missing out domainspecific senses and terminology.", "labels": [], "entities": []}, {"text": "Some authors, and, among others, reject the basic idea shared by WordNet and FrameNet (as well as traditional dictionaries) that there is a fixed list of senses for many verbs, arguing that individual senses will often be domain specific and should be discovered empirically by examining the syntactic and semantic contexts they occur in.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 65, "end_pos": 72, "type": "DATASET", "confidence": 0.9289907217025757}]}, {"text": "We are highly sympathetic to this view and in this work we assume, as Hanks and Pustejovsky do, that rather than relying on the intuitions of a lexicographer, it is better to try to induce verb senses and semantic types automatically from data drawn from the domain of interest.", "labels": [], "entities": []}, {"text": "In this paper we report on some experiments in learning semantic classes.", "labels": [], "entities": [{"text": "learning semantic classes", "start_pos": 47, "end_pos": 72, "type": "TASK", "confidence": 0.6810078223546346}]}, {"text": "We carryout prior syntactic and semantic analysis of a relevant corpus so that verb+argument pairs can be identified.", "labels": [], "entities": []}, {"text": "Since we are interested in domain specific semantic classification we make the 'one sense per corpus' hypothesis and ignore word sense disambiguation.", "labels": [], "entities": [{"text": "domain specific semantic classification", "start_pos": 27, "end_pos": 66, "type": "TASK", "confidence": 0.7009137272834778}, {"text": "word sense disambiguation", "start_pos": 124, "end_pos": 149, "type": "TASK", "confidence": 0.6927884618441263}]}, {"text": "For a given verb, we find the head nouns occurring in the subject, object and indirect object noun phrases (where they exist) occurring frequently within the corpus.", "labels": [], "entities": []}, {"text": "Now that we have information about nouns co-occurring in different argument slots of verbs we cluster the verbs according to shared argument slots: verbs which have an argument slot (not necessarily the same one) occupied by members of the same cluster are in turn clustered together.", "labels": [], "entities": []}, {"text": "The effect of this is to derive noun clusters characterising the semantic types of the argument slots for individual verbs (learning selectional restrictions) while simultaneously clustering verbs which have similar argument slots.", "labels": [], "entities": []}, {"text": "In the case where the same argument slot is involved across verbs, the effect of this is to induce a fine-grained semantic classification of verbs (the first step in learning verb senses).", "labels": [], "entities": []}, {"text": "Where different argument slots are involved the effect is to suggest more complex causal or inferential relations between groups of verbs.", "labels": [], "entities": []}, {"text": "To give a simple illustration, if admit, deny, suspect all take the word 'wrongdoing' as their object, then admit_arg2, deny_arg2, suspect_arg2 1 are clustered together into one group A.", "labels": [], "entities": []}, {"text": "If we also find that words like 'oversight' also appear frequently in the same argument position with roughly the same set of verbs, then 'oversight' will be clustered with the other fillers of group A.", "labels": [], "entities": []}, {"text": "A side-effect of the process is a classification of the verbs as well: if admit_arg1, deny_arg1 and admit_arg2, deny_arg2 respectively take the same values, 'deny' and 'admit' are clustered together.", "labels": [], "entities": []}, {"text": "We may also note that the same classes occur in different argument slots of different verbs: in we showed how this could lead to the discovery of causal relations specific to a domain: for example (in company succession events), that A succeeds B if B resigns from position C and A is appointed to C.", "labels": [], "entities": []}, {"text": "In the remainder of the paper we describe this clustering process in more detail.", "labels": [], "entities": [{"text": "clustering", "start_pos": 47, "end_pos": 57, "type": "TASK", "confidence": 0.9602624773979187}]}, {"text": "We also describe a simple pilot evaluation, by taking two unseen texts from the same domain, and observing to what extent the semantic groupings arrived at can be used to assign semantic types to arguments of verbs.", "labels": [], "entities": []}, {"text": "We were pleasantly surprised to find almost perfect recall, and respectable precision figures.", "labels": [], "entities": [{"text": "recall", "start_pos": 52, "end_pos": 58, "type": "METRIC", "confidence": 0.9992431402206421}, {"text": "precision", "start_pos": 76, "end_pos": 85, "type": "METRIC", "confidence": 0.9929554462432861}]}], "datasetContent": [{"text": "To evaluate the semantic types assigned by the automatically derived classes as well as the transferability of the derived CPA-like patterns to unseen instances, we performed a pilot study where we applied the patterns to two randomly selected articles from the on-line versions of the WSJ and the FT from March 2008.", "labels": [], "entities": [{"text": "WSJ and the FT from March 2008", "start_pos": 286, "end_pos": 316, "type": "DATASET", "confidence": 0.8248716081891742}]}, {"text": "We believe this to be a useful test for the validity of the patterns since the new articles are guaranteed to be distinct from the training WSJ data of the 90s, while still belonging to the same domain.", "labels": [], "entities": [{"text": "WSJ data of the 90s", "start_pos": 140, "end_pos": 159, "type": "DATASET", "confidence": 0.926287043094635}]}, {"text": "We parsed the article using the CCG parser and concentrated on its RASP option ( ) output, consisting of dependency relations.", "labels": [], "entities": []}, {"text": "Since our patterns concern the semantic typing of verb arguments, we focussed on the relations ncsubj (non-clausal subject), dobj (direct obj) and iobj (indirect obj) between a verb and the respective argument position.", "labels": [], "entities": []}, {"text": "We ignored erroneous parses as well as copular predicates with the verb to 'be', since the CCG parser's dependency relations did not maintain the connection between 'be' and the adjective or participle, making it clumsy to automatically link arguments in the way we need to.", "labels": [], "entities": []}, {"text": "We then followed the evaluation procedure below, where for each verb-argument pair token in the evaluation set: 1.", "labels": [], "entities": []}, {"text": "We looked fora pattern in the database matching the verb-argument relation and augmented the count for recall if a match was found for the right verb.", "labels": [], "entities": [{"text": "recall", "start_pos": 103, "end_pos": 109, "type": "METRIC", "confidence": 0.9984862208366394}]}, {"text": "2. We obtained the type (that is, the class ID) that the pattern assigns to the argument filler word.", "labels": [], "entities": []}, {"text": "We then checked the latter in the database, to see which classes it belongs to as well as its freq, tf-idf for each class.", "labels": [], "entities": [{"text": "freq", "start_pos": 94, "end_pos": 98, "type": "METRIC", "confidence": 0.9570385217666626}]}, {"text": "Determining which should be the correct, gold standard class of a word given the 32 classes is very difficult considering the class overlap.", "labels": [], "entities": []}, {"text": "Therefore, the three highest ranking classes were taken as describing the correct semantic type for the word.", "labels": [], "entities": []}, {"text": "Here rank is defined by looking at the 10 first classes where the term has the highest tf-Idf and returning the 3 of these with the highest frequency.", "labels": [], "entities": []}, {"text": "3. If the type assigned to the argument filler matches any of the 3 classes-semantic types, we assumed the type assignment is correct.", "labels": [], "entities": []}, {"text": "4. If the type returned was not among the 3 correct semantic types, we looked at the cluster dendrogram from the previous section and counted the distance between the correct and returned types.", "labels": [], "entities": []}, {"text": "If the correct type and the returned type are in the same cluster at the same level, we count the distance as 1.", "labels": [], "entities": []}, {"text": "If we need to go up a level from the returned type for them to be in the same cluster the distance is 2, if two levels, the distance is 3. 5. Proceeded to the next verb-argument pair.", "labels": [], "entities": []}, {"text": "To illustrate the assignment of semantic types through the application of the patterns and the ensuing evaluation procedure, we consider two example verb argument relations from the WSJ text, namely 'dobj shows declines' and 'ncsubj dropped indexes'.", "labels": [], "entities": [{"text": "WSJ text", "start_pos": 182, "end_pos": 190, "type": "DATASET", "confidence": 0.9683559834957123}]}, {"text": "In the first case, we looked in the database fora pattern of the verb 'show'.", "labels": [], "entities": []}, {"text": "The matching pattern is ' 6 show 4 14', which assigns semantic type 4 to the object of the verb, 'declines'.", "labels": [], "entities": []}, {"text": "When looking up the noun 'decline' in the database, the 3 types constituting its correct semantic type are 9, 8, 4.", "labels": [], "entities": []}, {"text": "Since 4, the type allocated by the pattern is among them, we consider this to have been the correct assignment of semantic type.", "labels": [], "entities": []}, {"text": "For the second example, the pattern available in the database of the verb 'drop' is '9 drop 8 28', which means that the pattern assigns type 9 to the word 'index'.", "labels": [], "entities": []}, {"text": "However, when we lookup the word 'index' in the database, the correct semantic types for it are 7,12,4.", "labels": [], "entities": []}, {"text": "We check in the cluster dendrogram to calculate the closest distance between type 9 and types 7,12,4 which is 2 steps, between classes 9 and 4.", "labels": [], "entities": []}, {"text": "The semantic type assignment is therefore considered once more correct.", "labels": [], "entities": [{"text": "semantic type assignment", "start_pos": 4, "end_pos": 28, "type": "TASK", "confidence": 0.6781090100606283}]}, {"text": "There were 46 distinct verbs and 78 distinct verb-argument relations that met the criteria for evaluation (out of 119 extracted predicate argument relations) in the WSJ article.", "labels": [], "entities": [{"text": "WSJ article", "start_pos": 165, "end_pos": 176, "type": "DATASET", "confidence": 0.9428163170814514}]}, {"text": "For the FT article the corresponding numbers were 25 and 53 respectively (the latter out of 129 predicate-argument relations).", "labels": [], "entities": [{"text": "FT article", "start_pos": 8, "end_pos": 18, "type": "DATASET", "confidence": 0.8895816206932068}]}, {"text": "The difference in these figures can be due to the size of the articles (6,002 words for the WSJ as opposed to only 2,702 for the FT one) as well as the preference for nominal predicates and nominalisations in the FT article.", "labels": [], "entities": [{"text": "WSJ", "start_pos": 92, "end_pos": 95, "type": "DATASET", "confidence": 0.9608966112136841}, {"text": "FT", "start_pos": 129, "end_pos": 131, "type": "DATASET", "confidence": 0.9474095106124878}, {"text": "FT article", "start_pos": 213, "end_pos": 223, "type": "DATASET", "confidence": 0.9480208158493042}]}, {"text": "A verb pattern existed for each of the verb-argument relations, which gave a perfect recall, 78/78, 53/53 (100%).", "labels": [], "entities": [{"text": "recall", "start_pos": 85, "end_pos": 91, "type": "METRIC", "confidence": 0.9957204461097717}]}, {"text": "This is gratifying since the patterns seem to cover adequately the financial domain, given that the test data come from two different newspapers.", "labels": [], "entities": []}, {"text": "When allowing a distance of up to 3 between the assigned and correct classes precision was 60/78 (76.9%) for the WSJ and 33/53 (62.2%) for the FT article.", "labels": [], "entities": [{"text": "precision", "start_pos": 77, "end_pos": 86, "type": "METRIC", "confidence": 0.9974631071090698}, {"text": "WSJ", "start_pos": 113, "end_pos": 116, "type": "DATASET", "confidence": 0.8701624870300293}, {"text": "FT article", "start_pos": 143, "end_pos": 153, "type": "DATASET", "confidence": 0.9447075426578522}]}, {"text": "For example, in the predicate 'oversees Mac', 'Mac', which is a company, was allocated to class 13 by the patterns whereas the correct class should have been one of 6, 9, 1.", "labels": [], "entities": []}, {"text": "The distance between classes 13 and 6 is 3 steps, whereas 'company' features in both classes with tf 0.0008 and 0.011 respectively.", "labels": [], "entities": []}, {"text": "The precision was reduced to 55/68 (70.5%) and 30/53 (56.6%) if we only allowed up to 2 steps (e.g. in 'index fell' 'index' was assigned to class 9 where its tf is 0.0014, as opposed to 4 where its tf is 0.0016).", "labels": [], "entities": [{"text": "precision", "start_pos": 4, "end_pos": 13, "type": "METRIC", "confidence": 0.9996086955070496}]}, {"text": "Precision fell further to 41/68 (53%) and 26/33 (49%) respectively for up to 1 steps (e.g. wherein 'reported at 75' the iobj '75' was classified as being in class 10 (tf 0.0004) as opposed to 5 (tf 0.004).", "labels": [], "entities": [{"text": "Precision", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.9944580793380737}]}, {"text": "For strictly exact matches, precision was 33/78 (43%) for the WSJ and 21/53 (39.6%) for the FT (e.g. 'director' in 'director said' being assigned to class 6 where the correct type is defined by classes 25,12,6).", "labels": [], "entities": [{"text": "precision", "start_pos": 28, "end_pos": 37, "type": "METRIC", "confidence": 0.9996860027313232}, {"text": "WSJ", "start_pos": 62, "end_pos": 65, "type": "DATASET", "confidence": 0.7996207475662231}, {"text": "FT", "start_pos": 92, "end_pos": 94, "type": "DATASET", "confidence": 0.6532865166664124}]}, {"text": "The results between the two articles are definitely comparable.", "labels": [], "entities": []}, {"text": "However, it is difficult to tell whether the observed difference at the upper end is indeed statistically significant and to what extend the difference between British English and US English plays a role here.", "labels": [], "entities": []}, {"text": "Nevertheless, even though the evaluation was only performed on a small scale, we consider the results to beat the very least, encouraging, since the texts we tested the patterns on were picked at random from the domain of financial news.", "labels": [], "entities": []}, {"text": "The perfect recall would suggest that the verb patterns provide reasonably full coverage of the domain, while we can assign informative fine-grained semantic types to arguments with a reasonable degree of precision.", "labels": [], "entities": [{"text": "recall", "start_pos": 12, "end_pos": 18, "type": "METRIC", "confidence": 0.9983419179916382}, {"text": "precision", "start_pos": 205, "end_pos": 214, "type": "METRIC", "confidence": 0.996466875076294}]}, {"text": "Of course, a larger evaluation would be desirable, as would some task-related measure of how much this semantic typing helps in accurate processing.", "labels": [], "entities": []}, {"text": "We hope to do this in future work.", "labels": [], "entities": []}], "tableCaptions": []}