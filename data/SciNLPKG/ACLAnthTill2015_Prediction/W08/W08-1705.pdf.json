{"title": [{"text": "Speeding up LFG Parsing Using C-Structure Pruning", "labels": [], "entities": [{"text": "LFG Parsing", "start_pos": 12, "end_pos": 23, "type": "TASK", "confidence": 0.5839066952466965}]}], "abstractContent": [{"text": "In this paper we present a method for greatly reducing parse times in LFG parsing , while at the same time maintaining parse accuracy.", "labels": [], "entities": [{"text": "LFG parsing", "start_pos": 70, "end_pos": 81, "type": "TASK", "confidence": 0.671958327293396}, {"text": "accuracy", "start_pos": 125, "end_pos": 133, "type": "METRIC", "confidence": 0.881524920463562}]}, {"text": "We evaluate the methodology on data from English, German and Norwegian and show that the same patterns hold across languages.", "labels": [], "entities": []}, {"text": "We achieve a speedup of 67% on the English data and 49% on the German data.", "labels": [], "entities": [{"text": "speedup", "start_pos": 13, "end_pos": 20, "type": "METRIC", "confidence": 0.9839099049568176}, {"text": "English data", "start_pos": 35, "end_pos": 47, "type": "DATASET", "confidence": 0.916951060295105}, {"text": "German data", "start_pos": 63, "end_pos": 74, "type": "DATASET", "confidence": 0.860700786113739}]}, {"text": "On a small amount of data for Norwegian, we achieve a speedup of 40%, although with more training data we expect this figure to increase .", "labels": [], "entities": [{"text": "Norwegian", "start_pos": 30, "end_pos": 39, "type": "DATASET", "confidence": 0.8542799353599548}, {"text": "speedup", "start_pos": 54, "end_pos": 61, "type": "METRIC", "confidence": 0.9936459064483643}]}], "introductionContent": [{"text": "Efficient parsing of large amounts of natural language is extremely important for any real-world application.", "labels": [], "entities": [{"text": "parsing of large amounts of natural language", "start_pos": 10, "end_pos": 54, "type": "TASK", "confidence": 0.7618480665343148}]}, {"text": "The XLE Parsing System is a largescale, hand-crafted, deep, unification-based system that processes raw text and produces both constituent structures (phrase structure trees) and feature structures (dependency attribute-value matrices).", "labels": [], "entities": []}, {"text": "A typical breakdown of parsing time of XLE components with the English grammar is Morphology (1.6%), Chart (5.8%) and Unifier (92.6%).", "labels": [], "entities": [{"text": "parsing", "start_pos": 23, "end_pos": 30, "type": "TASK", "confidence": 0.9610773324966431}, {"text": "English grammar", "start_pos": 63, "end_pos": 78, "type": "DATASET", "confidence": 0.8600811958312988}]}, {"text": "It is clear that the major bottleneck in processing is in unification.", "labels": [], "entities": [{"text": "unification", "start_pos": 58, "end_pos": 69, "type": "TASK", "confidence": 0.9701220989227295}]}, {"text": "carried out a preliminary experiment to test the theory that if fewer c-structures were passed to the unifier, overall parsing times would improve, while the accuracy of parsing would remain stable.", "labels": [], "entities": [{"text": "parsing", "start_pos": 119, "end_pos": 126, "type": "TASK", "confidence": 0.9580705761909485}, {"text": "accuracy", "start_pos": 158, "end_pos": 166, "type": "METRIC", "confidence": 0.999215841293335}, {"text": "parsing", "start_pos": 170, "end_pos": 177, "type": "TASK", "confidence": 0.944512128829956}]}, {"text": "Their experiments used state-of-the-art probabilistic treebank-based parsers to automatically mark certain constituents on the input sentences, limiting the number of c-structures the XLE parsing system would build.", "labels": [], "entities": []}, {"text": "They achieved an 18% speedup in parse times, while maintaining the accuracy of the output f-structures.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 67, "end_pos": 75, "type": "METRIC", "confidence": 0.9993407130241394}]}, {"text": "The experiments presented in used the XLE system as a black box and did not make any changes to it.", "labels": [], "entities": []}, {"text": "However, the results were encouraging enough fora c-structure pruning mechanism to be fully integrated into the XLE system.", "labels": [], "entities": []}, {"text": "The paper is structured as follows: we present the pruning model that has been integrated into the XLE system (Section 2), and how it can be applied successfully to more than one language.", "labels": [], "entities": []}, {"text": "We present experiments for English (Section 3), German (Section 4) and Norwegian (Section 5) showing that for both German and English, a significant improvement in speed is achieved, while the quality of the f-structures remains stable.", "labels": [], "entities": [{"text": "speed", "start_pos": 164, "end_pos": 169, "type": "METRIC", "confidence": 0.9946757555007935}]}, {"text": "For Norwegian a speedup is also achieved, but more training data is required to sustain the accuracy of the fstructures.", "labels": [], "entities": [{"text": "speedup", "start_pos": 16, "end_pos": 23, "type": "METRIC", "confidence": 0.9900746941566467}, {"text": "accuracy", "start_pos": 92, "end_pos": 100, "type": "METRIC", "confidence": 0.9994920492172241}]}, {"text": "In Section 7 we present an error analysis on the German data.", "labels": [], "entities": [{"text": "error", "start_pos": 27, "end_pos": 32, "type": "METRIC", "confidence": 0.953790009021759}, {"text": "German data", "start_pos": 49, "end_pos": 60, "type": "DATASET", "confidence": 0.9254768192768097}]}, {"text": "We then relate the work presented in this paper to similar efficient parsing strategies (Section 8) before concluding in Section 9.", "labels": [], "entities": []}], "datasetContent": [{"text": "We carried out a number of parsing experiments to test the effect of c-structure pruning, both in terms of time and accuracy.", "labels": [], "entities": [{"text": "parsing", "start_pos": 27, "end_pos": 34, "type": "TASK", "confidence": 0.9673086404800415}, {"text": "accuracy", "start_pos": 116, "end_pos": 124, "type": "METRIC", "confidence": 0.9949737787246704}]}, {"text": "We trained the c-structure pruning algorithm on the standard sections of Penn Treebank Wall Street Journal Text (.", "labels": [], "entities": [{"text": "Penn Treebank Wall Street Journal Text", "start_pos": 73, "end_pos": 111, "type": "DATASET", "confidence": 0.9806621968746185}]}, {"text": "The training data consists of the original WSJ strings, marked up with some of the Penn Treebank constituent information.", "labels": [], "entities": [{"text": "WSJ strings", "start_pos": 43, "end_pos": 54, "type": "DATASET", "confidence": 0.911290168762207}, {"text": "Penn Treebank constituent information", "start_pos": 83, "end_pos": 120, "type": "DATASET", "confidence": 0.9898190647363663}]}, {"text": "We marked up NPs and SBARs as well as adjective and verbal POS categories.", "labels": [], "entities": []}, {"text": "This is meant to guide the training process, so that it does learn from parses that are not compatible with the original treebank analysis.", "labels": [], "entities": []}, {"text": "We evaluated against the PARC 700 Dependency Bank (, splitting it into 140 sentences as development data and the remaining unseen 560 for final testing (as in).", "labels": [], "entities": [{"text": "PARC 700 Dependency Bank", "start_pos": 25, "end_pos": 49, "type": "DATASET", "confidence": 0.9662379920482635}]}, {"text": "We experimented with different values of the pruning cutoff on the development set; the results are given in.", "labels": [], "entities": []}, {"text": "The results show that the lower the cutoff value, the quicker the sentences can be parsed.", "labels": [], "entities": []}, {"text": "Using a cutoff of 4, the development sentences can be parsed in 100 CPU seconds, while with a cutoff of 10, the same experiment takes 182 seconds.", "labels": [], "entities": []}, {"text": "With no cutoff, the experiment takes 288 CPU seconds.", "labels": [], "entities": []}, {"text": "However, this increase in speed comes at a price.", "labels": [], "entities": [{"text": "speed", "start_pos": 26, "end_pos": 31, "type": "METRIC", "confidence": 0.9851188063621521}]}, {"text": "The number of fragment parses increases, i.e. there are more sentences that fail to be analyzed with a complete spanning parse.", "labels": [], "entities": []}, {"text": "With no pruning, the number of fragment parses is 23, while with the most aggressive pruning factor of 4, there are 39 fragment parses.", "labels": [], "entities": []}, {"text": "There are also many more skimmed sentences with no c-structure pruning, which impacts negatively on the results.", "labels": [], "entities": []}, {"text": "The oracle f-score with no pruning is 83.07, but with pruning (at all thresholds) the oracle f-score is higher.", "labels": [], "entities": []}, {"text": "This is due to less skimming when pruning is activated, since the more subtrees that are pruned, the less likely the XLE system is to run over the time or memory limits needed to trigger skimming.", "labels": [], "entities": []}, {"text": "Having established that a cutoff of 5 performs best on the development data, we carried out the final evaluation on the 560-sentence test set using this cutoff.", "labels": [], "entities": []}, {"text": "The results are given in.", "labels": [], "entities": []}, {"text": "There is a 67% speedup in parsing the 560 sentences, and the most probable f-score increases significantly from 79.93 to 82.83.", "labels": [], "entities": [{"text": "parsing", "start_pos": 26, "end_pos": 33, "type": "TASK", "confidence": 0.9729246497154236}, {"text": "f-score", "start_pos": 75, "end_pos": 82, "type": "METRIC", "confidence": 0.8373358249664307}]}, {"text": "The oracle f-score also increases, while there is a decrease in the random fscore.", "labels": [], "entities": []}, {"text": "This shows that we are throwing away good solutions during pruning, but that overall the results improve.", "labels": [], "entities": []}, {"text": "Part of this again is due to the fact that with no pruning, skimming is triggered much more often.", "labels": [], "entities": [{"text": "skimming", "start_pos": 60, "end_pos": 68, "type": "TASK", "confidence": 0.9737424254417419}]}, {"text": "With a pruning factor of 5, there are no skimmed sentences.", "labels": [], "entities": []}, {"text": "There is also one sentence that timed outwith no pruning, which also lowers the most probable and oracle f-scores.", "labels": [], "entities": []}, {"text": "We carried out a similar set of experiments on German data to test whether the methodology described above ported to a language other than English.", "labels": [], "entities": []}, {"text": "In the case of German, the typical time of XLE components is: Morphology (22.5%), Chart (3.5%) and Unifier (74%).", "labels": [], "entities": [{"text": "Morphology", "start_pos": 62, "end_pos": 72, "type": "METRIC", "confidence": 0.8919984698295593}, {"text": "Chart", "start_pos": 82, "end_pos": 87, "type": "METRIC", "confidence": 0.8956731557846069}]}, {"text": "As training data we used the TIGER corpus ().", "labels": [], "entities": [{"text": "TIGER corpus", "start_pos": 29, "end_pos": 41, "type": "DATASET", "confidence": 0.7949455678462982}]}, {"text": "Setting aside 2000 sentences for development and testing, we used the remaining 48,474 sentences as training data.", "labels": [], "entities": []}, {"text": "In order to create the partially bracketed input required for training, we converted the original TIGER graphs into Penn-style trees with empty nodes and retained bracketed constituents of the type NP, S, PN and AP.", "labels": [], "entities": [{"text": "AP", "start_pos": 212, "end_pos": 214, "type": "METRIC", "confidence": 0.9591817855834961}]}, {"text": "The training data was parsed by the German ParGram LFG).", "labels": [], "entities": [{"text": "German ParGram LFG", "start_pos": 36, "end_pos": 54, "type": "DATASET", "confidence": 0.76362278064092}]}, {"text": "This resulted in 25,677 full parses, 21,279 fragmented parses and 1,518 parse failures.", "labels": [], "entities": []}, {"text": "There are 52,959 features in the final pruning model.", "labels": [], "entities": []}, {"text": "To establish the optimal pruning settings for German, we split the 2,000 saved sentences into 371 development sentences and 1495 test sentences for final evaluation.", "labels": [], "entities": []}, {"text": "We evaluated against the TiGer Dependency Bank () (TiGerDB), a dependency-based gold standard for German parsers that encodes grammatical relations similar to, though more fine-grained than, the ones in the TIGER Treebank as well as morphosyntactic features.", "labels": [], "entities": [{"text": "TIGER Treebank", "start_pos": 207, "end_pos": 221, "type": "DATASET", "confidence": 0.8431861400604248}]}, {"text": "We experimented with the same pruning levels as in the English experiments.", "labels": [], "entities": []}, {"text": "The results are given in.", "labels": [], "entities": []}, {"text": "The results on the development set show a similar trend to the English results.", "labels": [], "entities": []}, {"text": "A cutoff of 4 results in the fastest system, however at the expense: Results of c-structure pruning experiments on German test data of accuracy.", "labels": [], "entities": [{"text": "German test data", "start_pos": 115, "end_pos": 131, "type": "DATASET", "confidence": 0.8184178272883097}, {"text": "accuracy", "start_pos": 135, "end_pos": 143, "type": "METRIC", "confidence": 0.9993046522140503}]}, {"text": "A cutoff of 5 seems to provide the best tradeoff between time and accuracy.", "labels": [], "entities": [{"text": "cutoff", "start_pos": 2, "end_pos": 8, "type": "METRIC", "confidence": 0.9612376093864441}, {"text": "time", "start_pos": 57, "end_pos": 61, "type": "METRIC", "confidence": 0.9569284319877625}, {"text": "accuracy", "start_pos": 66, "end_pos": 74, "type": "METRIC", "confidence": 0.9923243522644043}]}, {"text": "Again, most of the gain in oracle f-score is due to fewer timeouts, rather than improved f-structures.", "labels": [], "entities": []}, {"text": "In the German development set, a cutoff of 5 leads to a speedup of over 64% and a small increase in oracle f-score of 0.33 points.", "labels": [], "entities": [{"text": "German development set", "start_pos": 7, "end_pos": 29, "type": "DATASET", "confidence": 0.9182360370953878}, {"text": "speedup", "start_pos": 56, "end_pos": 63, "type": "METRIC", "confidence": 0.9870671629905701}]}, {"text": "Therefore, for the final evaluation on the unseen test-set, we choose a cutoff of 5.", "labels": [], "entities": []}, {"text": "The results are given in.", "labels": [], "entities": []}, {"text": "We achieve a speedup of 49% and a non-significant increase inmost probable f-score of 0.094.", "labels": [], "entities": [{"text": "speedup", "start_pos": 13, "end_pos": 20, "type": "METRIC", "confidence": 0.995144784450531}, {"text": "f-score", "start_pos": 75, "end_pos": 82, "type": "METRIC", "confidence": 0.725914478302002}]}, {"text": "The time spent by the system on morphology is much higher for German than for English.", "labels": [], "entities": []}, {"text": "If we only take the unification stage of the process into account, the German experiments show a speedup of 65.5%.", "labels": [], "entities": [{"text": "unification", "start_pos": 20, "end_pos": 31, "type": "TASK", "confidence": 0.9640916585922241}]}, {"text": "As there is no treebank currently available for Norwegian, we were unable to train the c-structure pruning mechanism for Norwegian in the same way as was done for English and German.", "labels": [], "entities": []}, {"text": "There is, however, some LFG-parsed data that has been completely disambiguated using the techniques described in From the disambiguated text, we can automatically extract partially bracketed sentences as input to the c-structure pruning training method.", "labels": [], "entities": []}, {"text": "We can also extract sentences for training that are partially disambiguated, but these cannot be used as part of the test data.", "labels": [], "entities": []}, {"text": "To do this, we extract the bracketed string for each solution.", "labels": [], "entities": []}, {"text": "If all the solutions produce the same bracketed string, then this is added to the training data.", "labels": [], "entities": []}, {"text": "This results in an average of 4556 features.", "labels": [], "entities": []}, {"text": "As the data set is small, we do not split it into development, training and test sections as was done for English and German.", "labels": [], "entities": []}, {"text": "Instead we carryout a 10-fold cross validation over the entire set.", "labels": [], "entities": []}, {"text": "The results for each pruning level are given in.", "labels": [], "entities": []}, {"text": "The results in show that the pattern that held for English and German does not quite hold for Norwegian.", "labels": [], "entities": []}, {"text": "While, as expected, the time taken to parse the test set is greatly reduced when using c-structure pruning, there is also a negative impact on the quality of the f-structures.", "labels": [], "entities": []}, {"text": "One reason for this is that there are now sentences that could previously be parsed, and that now no longer can be parsed, even with a fragment grammar.", "labels": [], "entities": []}, {"text": "2 With cstructure pruning, the number of fragment parses increases for all thresholds, apart from 10.", "labels": [], "entities": []}, {"text": "It is also difficult to compare the Norwegian experiment to the English and German, since the gold standard is constrained to only consist of sentences that can be parsed by the grammar.", "labels": [], "entities": []}, {"text": "Theoretically the oracle f-score for the experiment with no prun-  Figure 3: The lower-bound results for each of the 10 cross validation runs across the thresholds ing should be 100.", "labels": [], "entities": []}, {"text": "The slight drop is due to a slightly different morphological analyzer used in the final experiments that treats compound nouns differently.", "labels": [], "entities": []}, {"text": "A threshold of 10 gives the best results, with a speedup of 40% and a drop in f-score of 0.43 points.", "labels": [], "entities": [{"text": "speedup", "start_pos": 49, "end_pos": 56, "type": "METRIC", "confidence": 0.9954643845558167}, {"text": "f-score", "start_pos": 78, "end_pos": 85, "type": "METRIC", "confidence": 0.9990352392196655}]}, {"text": "It is difficult to choose the \"best\" threshold, as the amount of training data is probably not enough to get an accurate picture of the data.", "labels": [], "entities": []}, {"text": "For example, shows the lower-bound results for each of the 10 runs.", "labels": [], "entities": []}, {"text": "It is difficult to see a clear pattern for all the runs, indicating that the amount of training data is probably not enough fora reliable experiment.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Results of c-structure pruning experi- ments on English test data", "labels": [], "entities": [{"text": "English test data", "start_pos": 58, "end_pos": 75, "type": "DATASET", "confidence": 0.8455615639686584}]}, {"text": " Table 1: Results of c-structure pruning experiments on English development data", "labels": [], "entities": [{"text": "English development data", "start_pos": 56, "end_pos": 80, "type": "DATASET", "confidence": 0.6031672855218252}]}, {"text": " Table 3: Results of c-structure pruning experiments on German development data", "labels": [], "entities": [{"text": "German development data", "start_pos": 56, "end_pos": 79, "type": "DATASET", "confidence": 0.7426701386769613}]}, {"text": " Table 5: Results of c-structure pruning 10-fold cross validation experiments on Norwegian data", "labels": [], "entities": [{"text": "Norwegian data", "start_pos": 81, "end_pos": 95, "type": "DATASET", "confidence": 0.7379157543182373}]}]}