{"title": [{"text": "A Scalable Decoder for Parsing-based Machine Translation with Equivalent Language Model State Maintenance", "labels": [], "entities": [{"text": "Parsing-based Machine Translation", "start_pos": 23, "end_pos": 56, "type": "TASK", "confidence": 0.7587299148241679}]}], "abstractContent": [{"text": "We describe a scalable decoder for parsing-based machine translation.", "labels": [], "entities": [{"text": "parsing-based machine translation", "start_pos": 35, "end_pos": 68, "type": "TASK", "confidence": 0.8854608734448751}]}, {"text": "The decoder is written in JAVA and implements all the essential algorithms described in Chiang (2007): chart-parsing, m-gram language model integration , beam-and cube-pruning, and unique k-best extraction.", "labels": [], "entities": [{"text": "JAVA", "start_pos": 26, "end_pos": 30, "type": "DATASET", "confidence": 0.9531382918357849}, {"text": "m-gram language model integration", "start_pos": 118, "end_pos": 151, "type": "TASK", "confidence": 0.6377927213907242}, {"text": "k-best extraction", "start_pos": 188, "end_pos": 205, "type": "TASK", "confidence": 0.7143687903881073}]}, {"text": "Additionally, parallel and distributed computing techniques are exploited to make it scalable.", "labels": [], "entities": []}, {"text": "We also propose an algorithm to maintain equivalent language model states that exploits the back-off property of m-gram language models: instead of maintaining a separate state for each distinguished sequence of \"state\" words, we merge multiple states that can be made equivalent for language model probability calculations due to back-off.", "labels": [], "entities": []}, {"text": "We demonstrate experimentally that our decoder is more than 30 times faster than a baseline decoder written in PYTHON.", "labels": [], "entities": []}, {"text": "We propose to release our decoder as an open-source toolkit.", "labels": [], "entities": []}], "introductionContent": [{"text": "Large-scale parsing-based statistical machine translation (MT) has made remarkable progress in the last few years.", "labels": [], "entities": [{"text": "parsing-based statistical machine translation (MT)", "start_pos": 12, "end_pos": 62, "type": "TASK", "confidence": 0.8926801681518555}]}, {"text": "The systems being developed differ in whether they use source-or target-language syntax.", "labels": [], "entities": []}, {"text": "For instance, the hierarchical translation system of Chiang  A critical component in parsing-based MT systems is the decoder, which is complex to implement and scale up.", "labels": [], "entities": [{"text": "parsing-based MT", "start_pos": 85, "end_pos": 101, "type": "TASK", "confidence": 0.7474343180656433}]}, {"text": "Most of the systems described above employ tailor-made, dedicated decoders that are not open-source, which results in a high barrier to entry for other researchers in the field.", "labels": [], "entities": []}, {"text": "However, with the algorithms proposed in, it is possible to develop a general-purpose decoder that can be used by all the parsing-based systems.", "labels": [], "entities": []}, {"text": "In this paper, we describe an important firststep towards an extensible, general-purpose, scalable, and open-source parsing-based MT decoder.", "labels": [], "entities": [{"text": "parsing-based MT decoder", "start_pos": 116, "end_pos": 140, "type": "TASK", "confidence": 0.7282671531041464}]}, {"text": "Our decoder is written in JAVA and implements all the essential algorithms described in: chart-parsing, m-gram language model integration, beam-and cube-pruning, and unique k-best extraction.", "labels": [], "entities": [{"text": "JAVA", "start_pos": 26, "end_pos": 30, "type": "DATASET", "confidence": 0.9484462738037109}, {"text": "m-gram language model integration", "start_pos": 104, "end_pos": 137, "type": "TASK", "confidence": 0.631889596581459}, {"text": "k-best extraction", "start_pos": 173, "end_pos": 190, "type": "TASK", "confidence": 0.7110837996006012}]}, {"text": "Additionally, parallel and distributed computing techniques are exploited to make it scalable.", "labels": [], "entities": []}, {"text": "Straightforward integration of an m-gram language model (LM) into a parsing-based decoder substantially increases its computational complexity.", "labels": [], "entities": []}, {"text": "Therefore, it is important to develop efficient methods for LM integration.", "labels": [], "entities": [{"text": "LM integration", "start_pos": 60, "end_pos": 74, "type": "TASK", "confidence": 0.9730034172534943}]}, {"text": "We propose an algorithm to maintain equivalent LM states by exploiting the back-off property of m-gram LMs.", "labels": [], "entities": []}, {"text": "Specifically, instead of maintaining a separate state for each distinguished sequence of \"state\" words, we merge multiple states that can be made equivalent for LM calculations by anticipating such back-off.", "labels": [], "entities": []}, {"text": "We demonstrate experimentally that our decoder is 38 times faster than a previous decoder written in PYTHON.", "labels": [], "entities": [{"text": "PYTHON", "start_pos": 101, "end_pos": 107, "type": "DATASET", "confidence": 0.8895878791809082}]}, {"text": "Furthermore, the distributed computing permits improving translation quality via large-scale LMs.", "labels": [], "entities": [{"text": "translation", "start_pos": 57, "end_pos": 68, "type": "TASK", "confidence": 0.9661504626274109}]}, {"text": "We have successfully use our decoder to translate about a million sentences in a parallel corpus for large-scale discriminative training experiments.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we evaluate the performance of our decoder on a Chinese to English translation task.", "labels": [], "entities": [{"text": "Chinese to English translation task", "start_pos": 65, "end_pos": 100, "type": "TASK", "confidence": 0.6787615418434143}]}], "tableCaptions": [{"text": " Table 1: Decoder Comparison: Translation speed and  quality on the 2003 and 2005 NIST MT benchmark tests.", "labels": [], "entities": [{"text": "Translation", "start_pos": 30, "end_pos": 41, "type": "TASK", "confidence": 0.9556041955947876}, {"text": "NIST MT benchmark tests", "start_pos": 82, "end_pos": 105, "type": "DATASET", "confidence": 0.8696819692850113}]}, {"text": " Table 2: Distributed language model: the 7-gram LM  cannot be loaded alongside the SCFG on a single ma- chine; via distributed computing, it yields significant im- provement in BLEU-4 over a 5-gram.", "labels": [], "entities": [{"text": "BLEU-4", "start_pos": 178, "end_pos": 184, "type": "METRIC", "confidence": 0.9161165356636047}]}]}