{"title": [{"text": "An Empirical Study in Source Word Deletion for Phrase-based Statistical Machine Translation", "labels": [], "entities": [{"text": "Phrase-based Statistical Machine Translation", "start_pos": 47, "end_pos": 91, "type": "TASK", "confidence": 0.7073766365647316}]}], "abstractContent": [{"text": "The treatment of 'spurious' words of source language is an important problem but often ignored in the discussion on phrase-based SMT.", "labels": [], "entities": [{"text": "SMT", "start_pos": 129, "end_pos": 132, "type": "TASK", "confidence": 0.7346001863479614}]}, {"text": "This paper explains why it is important and why it is not a trivial problem, and proposes three models to handle spurious source words.", "labels": [], "entities": []}, {"text": "Experiments show that any source word deletion model can improve a phrase-based system by at least 1.6 BLEU points and the most sophisticated model improves by nearly 2 BLEU points.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 103, "end_pos": 107, "type": "METRIC", "confidence": 0.9977990984916687}, {"text": "BLEU", "start_pos": 169, "end_pos": 173, "type": "METRIC", "confidence": 0.9978035092353821}]}, {"text": "This paper also explores the impact of training data size and training data domain/genre on source word deletion.", "labels": [], "entities": [{"text": "source word deletion", "start_pos": 92, "end_pos": 112, "type": "TASK", "confidence": 0.6744023164113363}]}], "introductionContent": [{"text": "It is widely known that translation is by no means word-to-word conversion.", "labels": [], "entities": [{"text": "translation", "start_pos": 24, "end_pos": 35, "type": "TASK", "confidence": 0.9868359565734863}, {"text": "word-to-word conversion", "start_pos": 51, "end_pos": 74, "type": "TASK", "confidence": 0.702029675245285}]}, {"text": "Not only because sometimes a word in some language translates as more than one word in another language, also every language has some 'spurious' words which do not have any counterpart in other languages.", "labels": [], "entities": []}, {"text": "Consequently, an MT system should be able to identify the spurious words of the source language and not translate them, as well as to generate the spurious words of the target language.", "labels": [], "entities": [{"text": "MT", "start_pos": 17, "end_pos": 19, "type": "TASK", "confidence": 0.9806555509567261}]}, {"text": "This paper focuses on the first task and studies how it can be handled in phrase-based SMT.", "labels": [], "entities": [{"text": "phrase-based SMT", "start_pos": 74, "end_pos": 90, "type": "TASK", "confidence": 0.5776613056659698}]}, {"text": "An immediate reaction to the proposal of investigating source word deletion (henceforth SWD) is: Is SWD itself worth our attention?", "labels": [], "entities": [{"text": "investigating source word deletion (henceforth SWD)", "start_pos": 41, "end_pos": 92, "type": "TASK", "confidence": 0.5945556722581387}]}, {"text": "Isn't it a trivial task that can be handled easily by existing techniques?", "labels": [], "entities": []}, {"text": "One of the reasons why we need to pay attention to SWD is its significant improvement to translation performance, which will be shown by the experiments results in section 4.2.", "labels": [], "entities": [{"text": "SWD", "start_pos": 51, "end_pos": 54, "type": "TASK", "confidence": 0.9599869847297668}, {"text": "translation", "start_pos": 89, "end_pos": 100, "type": "TASK", "confidence": 0.9718140959739685}]}, {"text": "Another reason is that SWD is not a trivial task.", "labels": [], "entities": [{"text": "SWD", "start_pos": 23, "end_pos": 26, "type": "TASK", "confidence": 0.9809004664421082}]}, {"text": "While some researchers think that the spurious words of a language are merely function words or grammatical particles, which can be handled by some simple heuristics or statistical means, there are in fact some tricky cases of SWD which need sophisticated solution.", "labels": [], "entities": [{"text": "SWD", "start_pos": 227, "end_pos": 230, "type": "TASK", "confidence": 0.9595105051994324}]}, {"text": "Consider the following example in Chinese-to-English translation: in English we have the subordinate clause \"according to NP\", where NP refers to some source of information.", "labels": [], "entities": []}, {"text": "The Chinese equivalent of this clause can sometimes be \"ACCORDING-TO/ NP EXPRESS/\"; that is, in Chinese we could have a clause rather than a noun phrase following the preposition ACCORDING-TO/ . Therefore, when translating Chinese into English, the content word EXPRESS/ should be considered spurious and not to be translated.", "labels": [], "entities": []}, {"text": "Of course, the verb EXPRESS/ is not spurious in other contexts.", "labels": [], "entities": [{"text": "EXPRESS", "start_pos": 20, "end_pos": 27, "type": "METRIC", "confidence": 0.9801293015480042}]}, {"text": "It is an example that SWD is not only about a few function words, and that the solution to SWD has to take context-sensitive factors into account.", "labels": [], "entities": [{"text": "SWD", "start_pos": 22, "end_pos": 25, "type": "TASK", "confidence": 0.9687476754188538}, {"text": "SWD", "start_pos": 91, "end_pos": 94, "type": "TASK", "confidence": 0.9153070449829102}]}, {"text": "Moreover, the solution needed for such tricky cases seems to be beyond the scope of current phrase-based SMT, unless we have a very large amount of training data which covers all possible variations of the Chinese pattern \"ACCORDING-TO/ NP EXPRESS/\".", "labels": [], "entities": [{"text": "SMT", "start_pos": 105, "end_pos": 108, "type": "TASK", "confidence": 0.8065140247344971}]}, {"text": "Despite the obvious need for handling spurious source words, it is surprising that phrasebased SMT, which is a major approach to SMT, does not well address the problem.", "labels": [], "entities": [{"text": "SMT", "start_pos": 95, "end_pos": 98, "type": "TASK", "confidence": 0.6475034952163696}, {"text": "SMT", "start_pos": 129, "end_pos": 132, "type": "TASK", "confidence": 0.991519033908844}]}, {"text": "There are two possible ways fora phrase-based system to deal with SWD.", "labels": [], "entities": [{"text": "SWD", "start_pos": 66, "end_pos": 69, "type": "TASK", "confidence": 0.9471391439437866}]}, {"text": "The first one is to allow a source language phrase to translate to nothing.", "labels": [], "entities": []}, {"text": "However, no existing literature has mentioned such a possibility and discussed the modifications required by such an extension.", "labels": [], "entities": []}, {"text": "The second way is to capture SWD within the phrase pairs in translation table.", "labels": [], "entities": []}, {"text": "That is, suppose there is a foreign phrase\u02dcFphrase\u02dc phrase\u02dcF = (f A f B f C ) and an English phrase\u02dcE phrase\u02dc phrase\u02dcE = (e A e C ), where f A is aligned toe A and f C toe C , then the phrase pair ( \u02dc F , \u02dc E) tacitly deletes the spurious word f B . Such a SWD mechanism fails when data sparseness becomes a problem.", "labels": [], "entities": [{"text": "SWD", "start_pos": 257, "end_pos": 260, "type": "TASK", "confidence": 0.9683204889297485}]}, {"text": "If the training data does not have any word sequence containing f B , then the spurious f B cannot associate with other words to form a phrase pair, and therefore cannot be deleted tacitly in some phrase pair.", "labels": [], "entities": []}, {"text": "Rather, the decoder can only give a phrase segmentation that treats f B itself as a phrase, and this phrase cannot translate into nothing, as far as the SMT training and decoding procedure reported by existing literature are used.", "labels": [], "entities": [{"text": "phrase segmentation", "start_pos": 36, "end_pos": 55, "type": "TASK", "confidence": 0.7603272497653961}, {"text": "SMT training", "start_pos": 153, "end_pos": 165, "type": "TASK", "confidence": 0.8747916519641876}]}, {"text": "In sum, the current mechanism of phrase-based SMT is not capable of handling all cases of SWD.", "labels": [], "entities": [{"text": "SMT", "start_pos": 46, "end_pos": 49, "type": "TASK", "confidence": 0.6942622661590576}, {"text": "SWD", "start_pos": 90, "end_pos": 93, "type": "TASK", "confidence": 0.965453028678894}]}, {"text": "In this paper, we will present, in section 3, three SWD models and elaborate how to apply each of them to phrase-based SMT.", "labels": [], "entities": [{"text": "SMT", "start_pos": 119, "end_pos": 122, "type": "TASK", "confidence": 0.7093502879142761}]}, {"text": "Experiment settings are described in section 4.1, followed by the report and analysis of experiment results, using BLEU as evaluation metric, in section 4.2, which also discusses the impact of training data size and training data domain on SWD models.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 115, "end_pos": 119, "type": "METRIC", "confidence": 0.998432457447052}, {"text": "SWD", "start_pos": 240, "end_pos": 243, "type": "TASK", "confidence": 0.9445439577102661}]}, {"text": "Before making our conclusions, the effect of SWD on another evaluation metric, viz.", "labels": [], "entities": [{"text": "SWD", "start_pos": 45, "end_pos": 48, "type": "TASK", "confidence": 0.8349561095237732}]}, {"text": "METEOR, is examined in section 5.", "labels": [], "entities": [{"text": "METEOR", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.6144877672195435}]}], "datasetContent": [{"text": "A series of experiments were run to compare the performance of the three SWD models against the baseline, which is the standard phrase-based approach to SMT as elaborated in ().", "labels": [], "entities": [{"text": "SMT", "start_pos": 153, "end_pos": 156, "type": "TASK", "confidence": 0.9964409470558167}]}, {"text": "For SWD model 2, the phrase enumeration step is modified as described in section 3.2.", "labels": [], "entities": []}, {"text": "We used the Stanford parser () with its default Chinese grammar for its POS-tagging as well as finding the head/dependent words of all source words.", "labels": [], "entities": [{"text": "POS-tagging", "start_pos": 72, "end_pos": 83, "type": "DATASET", "confidence": 0.8310285210609436}]}, {"text": "The CRF toolkit used for model 3 is CRF++ 2 . The training data for the CRF model should be the same as that for translation table construction.", "labels": [], "entities": [{"text": "translation table construction", "start_pos": 113, "end_pos": 143, "type": "TASK", "confidence": 0.9352389375368754}]}, {"text": "However, since there are too many instances (every single word in the training data is an instance) with a huge feature space, no publicly available CRF toolkit can handle the entire training set of NIST MT-2006.", "labels": [], "entities": [{"text": "NIST MT-2006", "start_pos": 199, "end_pos": 211, "type": "DATASET", "confidence": 0.8739717602729797}]}, {"text": "Therefore, we can use at most only about one-third of the NIST training set (comprising the FBIS, B1, and T10 sections) for CRF training.", "labels": [], "entities": [{"text": "NIST training set", "start_pos": 58, "end_pos": 75, "type": "DATASET", "confidence": 0.963726540406545}, {"text": "FBIS", "start_pos": 92, "end_pos": 96, "type": "DATASET", "confidence": 0.6850108504295349}, {"text": "CRF", "start_pos": 124, "end_pos": 127, "type": "TASK", "confidence": 0.9541833996772766}]}, {"text": "The decoder in the experiments is our reimplementation of HIERO, augmented with a 5-gram language model and a reordering model based on (.", "labels": [], "entities": [{"text": "HIERO", "start_pos": 58, "end_pos": 63, "type": "DATASET", "confidence": 0.8520756363868713}]}, {"text": "Note that no hierarchical rule is used with the decoder; the phrase pairs used are still those used in conventional phrase-based SMT.", "labels": [], "entities": [{"text": "SMT", "start_pos": 129, "end_pos": 132, "type": "TASK", "confidence": 0.7889044284820557}]}, {"text": "Note also that the decoder does not translate OOV at all even in the baseline case, and thus the SWD models do not improve performance simply by removing OOVs.", "labels": [], "entities": [{"text": "OOV", "start_pos": 46, "end_pos": 49, "type": "METRIC", "confidence": 0.9350391030311584}]}, {"text": "In order to test the effect of training data size on the performance of the SWD models, three variations of training data were used: FBIS Only the FBIS section of the NIST training set is used as training data (for both translation table and the CRF model in model 3).", "labels": [], "entities": [{"text": "NIST training set", "start_pos": 167, "end_pos": 184, "type": "DATASET", "confidence": 0.8625540335973104}]}, {"text": "This section constitutes about 10% of the entire NIST training set.", "labels": [], "entities": [{"text": "NIST training set", "start_pos": 49, "end_pos": 66, "type": "DATASET", "confidence": 0.9198461373647054}]}, {"text": "The purpose of this variation is to test the performance of each model when very small amount of data are available.", "labels": [], "entities": []}, {"text": "BFT Only the B1, FBIS, and T10 sections of the NIST training set are used as training data.", "labels": [], "entities": [{"text": "BFT", "start_pos": 0, "end_pos": 3, "type": "METRIC", "confidence": 0.522010087966919}, {"text": "B1", "start_pos": 13, "end_pos": 15, "type": "METRIC", "confidence": 0.938630223274231}, {"text": "FBIS", "start_pos": 17, "end_pos": 21, "type": "METRIC", "confidence": 0.4200155735015869}, {"text": "NIST training set", "start_pos": 47, "end_pos": 64, "type": "DATASET", "confidence": 0.9757786591847738}]}, {"text": "These sections are about one-third of the entire NIST training set.", "labels": [], "entities": [{"text": "NIST training set", "start_pos": 49, "end_pos": 66, "type": "DATASET", "confidence": 0.9131097992261251}]}, {"text": "NIST All the sections of the NIST training set are used.", "labels": [], "entities": [{"text": "NIST", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.9879146814346313}, {"text": "NIST training set", "start_pos": 29, "end_pos": 46, "type": "DATASET", "confidence": 0.9662388761838278}]}, {"text": "The purpose of this variation is to test each model when a large amount of data are available.", "labels": [], "entities": []}, {"text": "(Case-insensitive) BLEU-4 () is used as the evaluation metric.", "labels": [], "entities": [{"text": "BLEU-4", "start_pos": 19, "end_pos": 25, "type": "METRIC", "confidence": 0.9461374878883362}]}, {"text": "In each test in our experiments, maximum BLEU training were run 10 times, and thus there are 10 BLEU scores for the test set.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 41, "end_pos": 45, "type": "METRIC", "confidence": 0.9990052580833435}, {"text": "BLEU", "start_pos": 96, "end_pos": 100, "type": "METRIC", "confidence": 0.9991335272789001}]}, {"text": "In the following we will report the mean scores only.", "labels": [], "entities": [{"text": "mean scores", "start_pos": 36, "end_pos": 47, "type": "METRIC", "confidence": 0.9657649993896484}]}, {"text": "shows the results of the first experiment, which uses the NIST MT-2005 test set as development data and the NIST MT-2006 test set as test data.", "labels": [], "entities": [{"text": "NIST MT-2005 test set", "start_pos": 58, "end_pos": 79, "type": "DATASET", "confidence": 0.8910309970378876}, {"text": "NIST MT-2006 test set", "start_pos": 108, "end_pos": 129, "type": "DATASET", "confidence": 0.9196938872337341}]}, {"text": "The most obvious observation is that any SWD model achieves much higher BLEU score than the baseline, as there is at least 1.6 BLEU point improvement in each case, and in some case the improvement of using SWD is nearly 2 BLEU points.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 72, "end_pos": 82, "type": "METRIC", "confidence": 0.9757519364356995}, {"text": "BLEU", "start_pos": 127, "end_pos": 131, "type": "METRIC", "confidence": 0.9957233667373657}, {"text": "BLEU", "start_pos": 222, "end_pos": 226, "type": "METRIC", "confidence": 0.9972803592681885}]}, {"text": "This clearly proves the importance of SWD in phrase-based SMT.", "labels": [], "entities": [{"text": "SWD", "start_pos": 38, "end_pos": 41, "type": "TASK", "confidence": 0.9634440541267395}, {"text": "SMT", "start_pos": 58, "end_pos": 61, "type": "TASK", "confidence": 0.7238978743553162}]}, {"text": "The difference between the performance of the various SWD models is much smaller.", "labels": [], "entities": []}, {"text": "Yet there are still some noticeable facts.", "labels": [], "entities": []}, {"text": "The first one is that model 1 gives the best result in the case of using only FBIS as training data but it fails to do so when more training data is available.", "labels": [], "entities": []}, {"text": "This phenomenon is not strange since model 2 and model 3 are conditioned on more information and therefore they need more training data.", "labels": [], "entities": []}, {"text": "The second observation is about the strength of SWD model 3, which achieves the best BLEU score in both the BFT and NIST cases.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 85, "end_pos": 95, "type": "METRIC", "confidence": 0.9815794229507446}, {"text": "BFT", "start_pos": 108, "end_pos": 111, "type": "DATASET", "confidence": 0.6464523077011108}, {"text": "NIST", "start_pos": 116, "end_pos": 120, "type": "DATASET", "confidence": 0.6295162439346313}]}, {"text": "While its improvement over models 1 and 2 is marginal in the case of BFT, its performance in the NIST case is remarkable.", "labels": [], "entities": [{"text": "BFT", "start_pos": 69, "end_pos": 72, "type": "DATASET", "confidence": 0.7456830143928528}, {"text": "NIST", "start_pos": 97, "end_pos": 101, "type": "DATASET", "confidence": 0.8086502552032471}]}, {"text": "A suspicion to the strength of model 3 is that in the NIST case both models 1 and 2 use the entire NIST training set for estimating P (), while model 3 uses only the BFT sections to train its CRF model.", "labels": [], "entities": [{"text": "NIST", "start_pos": 54, "end_pos": 58, "type": "DATASET", "confidence": 0.9331344366073608}, {"text": "NIST training set", "start_pos": 99, "end_pos": 116, "type": "DATASET", "confidence": 0.9434923330942789}, {"text": "estimating P", "start_pos": 121, "end_pos": 133, "type": "TASK", "confidence": 0.7691711783409119}]}, {"text": "It maybe that the BFT sections are more consistent with the test data set than the other NIST sections, and therefore a SWD model trained on BFT sections only is better than that trained on the entire NIST.", "labels": [], "entities": [{"text": "NIST", "start_pos": 89, "end_pos": 93, "type": "DATASET", "confidence": 0.910759449005127}, {"text": "NIST", "start_pos": 201, "end_pos": 205, "type": "DATASET", "confidence": 0.9762100577354431}]}, {"text": "This conjecture is supported by the fact that in all four settings the BLEU scores in the NIST case are lower than those in the BFT case, which suggests that other NIST sections are noisy.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 71, "end_pos": 75, "type": "METRIC", "confidence": 0.999065101146698}, {"text": "NIST", "start_pos": 90, "end_pos": 94, "type": "DATASET", "confidence": 0.8489236831665039}]}, {"text": "While it is impossible to test model 3 with the entire NIST, it is possible to restrict the data for the estimation of P (|f ) in model 1 to the BFT sections only and check if such a restriction helps.", "labels": [], "entities": [{"text": "NIST", "start_pos": 55, "end_pos": 59, "type": "DATASET", "confidence": 0.9780451059341431}, {"text": "BFT", "start_pos": 145, "end_pos": 148, "type": "DATASET", "confidence": 0.7991756796836853}]}, {"text": "We estimated the uniform probability P () from only the BFT sections and used it with the translation table constructed from the complete NIST training set.", "labels": [], "entities": [{"text": "uniform probability P", "start_pos": 17, "end_pos": 38, "type": "METRIC", "confidence": 0.8216559489568075}, {"text": "BFT", "start_pos": 56, "end_pos": 59, "type": "DATASET", "confidence": 0.5656691193580627}, {"text": "NIST training set", "start_pos": 138, "end_pos": 155, "type": "DATASET", "confidence": 0.9492836197217306}]}, {"text": "The BLEU score thus obtained is 31.24, which is even lower than the score (31.39) of the original case of using the entire NIST for both translation table and P (|f ) estimation.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 4, "end_pos": 8, "type": "METRIC", "confidence": 0.9990643858909607}, {"text": "NIST", "start_pos": 123, "end_pos": 127, "type": "DATASET", "confidence": 0.9621890783309937}]}, {"text": "In sum, the strength of model 3 is not simply due to the choice of training data.", "labels": [], "entities": []}, {"text": "The test set used in Experiment 1 distinguishes itself from the development data and the training data by its characteristics of combining text from different genres.", "labels": [], "entities": []}, {"text": "There are three sources of the NIST MT-2006 test set, viz.", "labels": [], "entities": [{"text": "NIST MT-2006 test set", "start_pos": 31, "end_pos": 52, "type": "DATASET", "confidence": 0.9118400514125824}]}, {"text": "\"newswire\", \"newsgroup\", and \"broadcast news\", while our development data and the NIST training set comprises only newswire text and text of similar style.", "labels": [], "entities": [{"text": "NIST training set", "start_pos": 82, "end_pos": 99, "type": "DATASET", "confidence": 0.9683829148610433}]}, {"text": "It is an interesting question whether SWD only works for some genres (say, newswire) but not for other genres.", "labels": [], "entities": [{"text": "SWD", "start_pos": 38, "end_pos": 41, "type": "TASK", "confidence": 0.9017202854156494}]}, {"text": "In fact, it is dubious whether SWD fits the test set to the same extent as it fits the development set.", "labels": [], "entities": []}, {"text": "That is, perhaps SWD contributes to the improvement in Experiment 1 simply by improving the translation of the development set which is composed of newswire text only, and SWD may not benefit the translation of the test data at all.", "labels": [], "entities": []}, {"text": "In order to test this conjecture, we ran Experiment 2, in which the SWD models were still applied to the development data during training, but Unfortunately this way does notwork for model 2 as the estimation of P (|f ) and the construction of translation table are tied together.: BLEU scores in Experiment 2, which is the same as Experiment 1 but no word is deleted for test corpus.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 282, "end_pos": 286, "type": "METRIC", "confidence": 0.9995307922363281}]}, {"text": "Note: the baseline scores are the same as the baselines in Experiment 1.", "labels": [], "entities": []}, {"text": "all SWD models stopped working when translating the test data with the trained parameters.", "labels": [], "entities": []}, {"text": "The results are shown in.", "labels": [], "entities": []}, {"text": "These results are very discouraging if we compare each cell in against the corresponding cell in: in all cases SWD seems harmful to the translation of the test data.", "labels": [], "entities": []}, {"text": "It is tempting to accept the conclusion that SWD works for newswire text only.", "labels": [], "entities": []}, {"text": "To scrutinize the problem, we split up the test data set into two parts, viz.", "labels": [], "entities": []}, {"text": "the newswire section and the non-newswire section, and ran experiments separately.", "labels": [], "entities": []}, {"text": "shows the results of Experiment 3, in which the development data is still the NIST MT-2005 test set and the test data is the newswire section of NIST MT-2006 test set.", "labels": [], "entities": [{"text": "NIST MT-2005 test set", "start_pos": 78, "end_pos": 99, "type": "DATASET", "confidence": 0.893052726984024}, {"text": "NIST MT-2006 test set", "start_pos": 145, "end_pos": 166, "type": "DATASET", "confidence": 0.9271173179149628}]}, {"text": "It is confirmed that if test data shares the same genre as the training/development data, then SWD does improve translation performance a lot.", "labels": [], "entities": [{"text": "SWD", "start_pos": 95, "end_pos": 98, "type": "TASK", "confidence": 0.7188120484352112}, {"text": "translation", "start_pos": 112, "end_pos": 123, "type": "TASK", "confidence": 0.9666954278945923}]}, {"text": "It is also observed that more sophisticated SWD models perform better when provided with sufficient training data, and that model 3 exhibits remarkable improvement when it comes to the NIST case.", "labels": [], "entities": [{"text": "NIST", "start_pos": 185, "end_pos": 189, "type": "DATASET", "confidence": 0.7755608558654785}]}, {"text": "Of course, the figures in, which shows the results of Experiment 4 where the nonnewswire section of NIST MT-2006 test set is used as test data, still leave us the doubt that SWD is useful fora particular genre only.", "labels": [], "entities": [{"text": "NIST MT-2006 test set", "start_pos": 100, "end_pos": 121, "type": "DATASET", "confidence": 0.9210129678249359}]}, {"text": "After all, it is reasonable to assume that a model trained from data of a particular domain can give good performance only to data of the same domain.", "labels": [], "entities": []}, {"text": "On the other hand, the language model is another cause of the poor performance, as the GIGAWORD corpus is also of the newswire style.", "labels": [], "entities": [{"text": "GIGAWORD corpus", "start_pos": 87, "end_pos": 102, "type": "DATASET", "confidence": 0.9441950917243958}]}, {"text": "While we cannot prove the value of SWD with respect to training data of other genres in the meantime, we could test the effect of using development data of other genres.", "labels": [], "entities": []}, {"text": "In our last experiment, the first halves of both the newswire: BLEU scores in Experiment 3, which is the same as Experiments 1 and 2 but only the newswire section of NIST'06 test set is used.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 63, "end_pos": 67, "type": "METRIC", "confidence": 0.9899185299873352}, {"text": "NIST'06 test set", "start_pos": 166, "end_pos": 182, "type": "DATASET", "confidence": 0.9693433046340942}]}, {"text": "Note: the baseline scores are the same as the baselines in Experiment 1 (  The results in the last section are all evaluated using the BLEU metric only.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 135, "end_pos": 139, "type": "METRIC", "confidence": 0.9951123595237732}]}, {"text": "It is dubious whether SWD is useful regarding recall-oriented metrics like METEOR (), since SWD removes information in source sentences.", "labels": [], "entities": [{"text": "recall-oriented", "start_pos": 46, "end_pos": 61, "type": "METRIC", "confidence": 0.959617555141449}, {"text": "METEOR", "start_pos": 75, "end_pos": 81, "type": "METRIC", "confidence": 0.8631777167320251}]}, {"text": "This suspicion is to certain extent confirmed by our application of METEOR to the translation outputs of Experiment 1 (c.f., which shows that all SWD models achieve lower ME-TEOR scores than the baseline.", "labels": [], "entities": [{"text": "METEOR", "start_pos": 68, "end_pos": 74, "type": "METRIC", "confidence": 0.7295255661010742}]}, {"text": "However, SWD is not entirely harmful to METEOR: if SWD is applied to parameter tuning only but not for the test set, (i.e. Experiment 2), even higher METEOR scores can be obtained.", "labels": [], "entities": [{"text": "METEOR", "start_pos": 40, "end_pos": 46, "type": "DATASET", "confidence": 0.5187781453132629}, {"text": "METEOR", "start_pos": 150, "end_pos": 156, "type": "METRIC", "confidence": 0.7053930163383484}]}, {"text": "This puzzling observation maybe because the parameters of the decoder are optimized with respect to BLEU score, and SWD benefits parameter tuning by improving BLEU score.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 100, "end_pos": 110, "type": "METRIC", "confidence": 0.9792206287384033}, {"text": "BLEU score", "start_pos": 159, "end_pos": 169, "type": "METRIC", "confidence": 0.9829922914505005}]}, {"text": "In future experiments, maximum METEOR training should be used instead of maximum BLEU training so as to examine if SWD is really useful for parameter tuning.", "labels": [], "entities": [{"text": "METEOR", "start_pos": 31, "end_pos": 37, "type": "METRIC", "confidence": 0.9904286861419678}, {"text": "BLEU", "start_pos": 81, "end_pos": 85, "type": "METRIC", "confidence": 0.9987566471099854}, {"text": "parameter tuning", "start_pos": 140, "end_pos": 156, "type": "TASK", "confidence": 0.6921311467885971}]}], "tableCaptions": [{"text": " Table 2: BLEU scores in Experiment 1: NIST'05 as  dev and NIST'06 as test", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9987408518791199}, {"text": "NIST'05", "start_pos": 39, "end_pos": 46, "type": "DATASET", "confidence": 0.9173742532730103}, {"text": "NIST'06", "start_pos": 59, "end_pos": 66, "type": "DATASET", "confidence": 0.8651135563850403}]}, {"text": " Table 3: BLEU scores in Experiment 2, which is the  same as Experiment 1 but no word is deleted for test  corpus. Note: the baseline scores are the same as the  baselines in Experiment 1", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9991477727890015}]}, {"text": " Table 4: BLEU scores in Experiment 3, which is the same as Experiments 1 and 2 but only the newswire section  of NIST'06 test set is used. Note: the baseline scores are the same as the baselines in Experiment 1 (", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9992663264274597}, {"text": "NIST'06 test set", "start_pos": 114, "end_pos": 130, "type": "DATASET", "confidence": 0.9517719944318136}]}, {"text": " Table 5: BLEU scores in Experiment 4, which is the same as Experiments 1 and 2 but only the non-newswire  section of NIST'06 test set is used. Note: the baseline scores are the same as the baselines in Experiment 1  (Table 2).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9993566870689392}, {"text": "NIST'06 test set", "start_pos": 118, "end_pos": 134, "type": "DATASET", "confidence": 0.977713425954183}]}, {"text": " Table 6: BLEU scores in Experiment 5: which is the  same as Experiment 1 but uses half of NIST'06 as de- velopment set and another half of NIST'06 as test set.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9991212487220764}, {"text": "NIST'06", "start_pos": 91, "end_pos": 98, "type": "DATASET", "confidence": 0.95548415184021}, {"text": "de- velopment set", "start_pos": 102, "end_pos": 119, "type": "METRIC", "confidence": 0.6839558482170105}, {"text": "NIST'06", "start_pos": 140, "end_pos": 147, "type": "DATASET", "confidence": 0.9599612355232239}]}, {"text": " Table 7: METEOR scores in Experiments 1 and 2", "labels": [], "entities": [{"text": "METEOR", "start_pos": 10, "end_pos": 16, "type": "METRIC", "confidence": 0.9886506795883179}]}]}