{"title": [{"text": "KnowNet: A Proposal for Building Highly Connected and Dense Knowledge Bases from the Web", "labels": [], "entities": []}], "abstractContent": [{"text": "This paper presents anew fully automatic method for building highly dense and accurate knowledge bases from existing semantic resources.", "labels": [], "entities": []}, {"text": "Basically, the method uses a wide-coverage and accurate knowledge-based Word Sense Disambiguation algorithm to assign the most appropriate senses to large sets of topically related words acquired from the web.", "labels": [], "entities": [{"text": "Word Sense Disambiguation", "start_pos": 72, "end_pos": 97, "type": "TASK", "confidence": 0.5461102823416392}]}, {"text": "KnowNet, the resulting knowledge-base which connects large sets of semantically-related concepts is a major step towards the autonomous acquisition of knowledge from raw corpora.", "labels": [], "entities": []}, {"text": "In fact, KnowNet is several times larger than any available knowledge resource encoding relations between synsets, and the knowledge that KnowNet contains outperform any other resource when empirically evaluated in a common multilingual framework.", "labels": [], "entities": []}], "introductionContent": [{"text": "Using large-scale knowledge bases, such as WordNet, has become a usual, often necessary, practice for most current Natural Language Processing (NLP) systems.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 43, "end_pos": 50, "type": "DATASET", "confidence": 0.9161205291748047}]}, {"text": "Even now, building large and rich enough knowledge bases for broadcoverage semantic processing takes a great deal of expensive manual effort involving large research groups during long periods of development.", "labels": [], "entities": [{"text": "broadcoverage semantic processing", "start_pos": 61, "end_pos": 94, "type": "TASK", "confidence": 0.934637725353241}]}, {"text": "In fact, hundreds of person-years have been invested in the development of wordnets for various languages.", "labels": [], "entities": []}, {"text": "For example, in more than ten years of manual construction, that is from version 1.5 to 3.0), WordNet passed from 103,445 to 235,402 semantic relations . But this data does not seems to be rich enough to support advanced concept-based NLP applications directly.", "labels": [], "entities": []}, {"text": "It seems that applications will not scale up to working in open domains without more detailed and rich general-purpose (and also domain-specific) semantic knowledge built by automatic means.", "labels": [], "entities": []}, {"text": "Obviously, this fact has severely hampered the state-of-the-art of advanced NLP applications.", "labels": [], "entities": []}, {"text": "However, the Princeton WordNet is by far the most widely-used knowledge base.", "labels": [], "entities": [{"text": "Princeton WordNet", "start_pos": 13, "end_pos": 30, "type": "DATASET", "confidence": 0.844732016324997}]}, {"text": "In fact, WordNet is being used world-wide for anchoring different types of semantic knowledge including wordnets for languages other than English (), domain knowledge) or ontologies like SUMO) or the EuroWordNet Top Concept Ontology (.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 9, "end_pos": 16, "type": "DATASET", "confidence": 0.926077663898468}, {"text": "EuroWordNet Top Concept Ontology", "start_pos": 200, "end_pos": 232, "type": "DATASET", "confidence": 0.9452075362205505}]}, {"text": "It contains manually coded information about nouns, verbs, adjectives and adverbs in English and is organised around the notion of a synset.", "labels": [], "entities": []}, {"text": "A synset is a set of words with the same part-of-speech that can be interchanged in a certain context.", "labels": [], "entities": []}, {"text": "For example, <party, political_party> form a synset because they can be used to refer to the same concept.", "labels": [], "entities": []}, {"text": "A synset is often further described by a gloss, in this case: \"an organisation to gain political power\" and by explicit semantic relations to other synsets.", "labels": [], "entities": []}, {"text": "Fortunately, during the last years the research community has devised a large set of innovative methods and tools for large-scale automatic acquisition of lexical knowledge from structured and unstructured corpora.", "labels": [], "entities": [{"text": "large-scale automatic acquisition of lexical knowledge from structured and unstructured corpora", "start_pos": 118, "end_pos": 213, "type": "TASK", "confidence": 0.7767626426436685}]}, {"text": "Among others we can mention eXtended WordNet), large collections of semantic preferences acquired from SemCor () or acquired from British National Corpus (BNC)), large-scale Topic Signatures for each synset acquired from the web (Agirre and de la) or knowledge about individuals from Wikipedia (.", "labels": [], "entities": [{"text": "British National Corpus (BNC))", "start_pos": 130, "end_pos": 160, "type": "DATASET", "confidence": 0.9673491915067037}]}, {"text": "Obviously, all these semantic resources have been acquired using a very different set of processes (), tools and corpora.", "labels": [], "entities": []}, {"text": "In fact, each semantic resource has different volume and accuracy figures when evaluated in a common and controlled framework (Cuadros and).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 57, "end_pos": 65, "type": "METRIC", "confidence": 0.9936680197715759}]}, {"text": "However, not all available large-scale resources encode semantic relations between synsets.", "labels": [], "entities": []}, {"text": "In some cases, only relations between synsets and words have been acquired.", "labels": [], "entities": []}, {"text": "This is the case of the Topic Signatures () acquired from the web (Agirre and de la).", "labels": [], "entities": []}, {"text": "This is one of the largest semantic resources ever built with around one hundred million relations between synsets and semantically related words.", "labels": [], "entities": []}, {"text": "A knowledge net or KnowNet, is an extensible, large and accurate knowledge base, which has been derived by semantically disambiguating the Topic Signatures acquired from the web.", "labels": [], "entities": []}, {"text": "Basically, the method uses a robust and accurate knowledgebased Word Sense Disambiguation algorithm to assign the most appropriate senses to the topic words associated to a particular synset.", "labels": [], "entities": [{"text": "Word Sense Disambiguation", "start_pos": 64, "end_pos": 89, "type": "TASK", "confidence": 0.53391099969546}]}, {"text": "The resulting knowledge-base which connects large sets of topically-related concepts is a major step towards the autonomous acquisition of knowledge from raw text.", "labels": [], "entities": []}, {"text": "In fact, KnowNet is several times larger than WordNet and the knowledge contained in KnowNet outperforms WordNet when empirically evaluated in a common framework.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 46, "end_pos": 53, "type": "DATASET", "confidence": 0.9632785320281982}, {"text": "WordNet", "start_pos": 105, "end_pos": 112, "type": "DATASET", "confidence": 0.9503034353256226}]}, {"text": "compares the different volumes of semantic relations between synset pairs of available knowledge bases and the newly created KnowNets 3 . Varying from five to twenty the number of processed words from each Topic Signature, we created automatically four different KnowNets with millions of new semantic relations between synsets.", "labels": [], "entities": []}, {"text": "After this introduction, Section 2 describes the Topic Signatures acquired from the web.", "labels": [], "entities": []}, {"text": "Section 3 presents the approach we plan to follow for building highly dense and accurate knowledge bases.", "labels": [], "entities": []}, {"text": "Section 4 describes the methods we followed for building KnowNet.", "labels": [], "entities": []}, {"text": "In Section 5, we present the evaluation framework used in this study.", "labels": [], "entities": []}, {"text": "Section 6 describes the results when evaluating different versions of KnowNet and finally, Section 7 presents some concluding remarks and future work.", "labels": [], "entities": []}], "datasetContent": [{"text": "In order to empirically establish the relative quality of these KnowNet versions with respect already available semantic resources, we used the noun-set of Senseval-3 English Lexical Sample task which consists of 20 nouns.", "labels": [], "entities": [{"text": "Senseval-3 English Lexical Sample task", "start_pos": 156, "end_pos": 194, "type": "TASK", "confidence": 0.5357097685337067}]}, {"text": "Trying to be as neutral as possible with respect to the resources studied, we applied systematically the same disambiguation method to all of them.", "labels": [], "entities": []}, {"text": "Recall that our main goal is to establish a fair comparison of the knowledge resources rather than providing the best disambiguation technique fora particular resource.", "labels": [], "entities": []}, {"text": "Thus, all the semantic resources studied are evaluated as Topic Signatures.", "labels": [], "entities": []}, {"text": "That is, word vectors with weights associated to a particular synset (topic) which are obtained by collecting those word senses appearing in the synsets directly related to the topics.", "labels": [], "entities": []}, {"text": "A common WSD method has been applied to all knowledge resources.", "labels": [], "entities": [{"text": "WSD", "start_pos": 9, "end_pos": 12, "type": "TASK", "confidence": 0.9531225562095642}]}, {"text": "A simple word overlapping counting is performed between the Topic Signature and the test example . The synset having higher overlapping word counts is selected.", "labels": [], "entities": [{"text": "word overlapping counting", "start_pos": 9, "end_pos": 34, "type": "TASK", "confidence": 0.6799172858397166}]}, {"text": "In fact, this is a very simple WSD method which only considers the topical information around the word to be disambiguated.", "labels": [], "entities": [{"text": "WSD", "start_pos": 31, "end_pos": 34, "type": "TASK", "confidence": 0.9189173579216003}]}, {"text": "All performances are evaluated on the test data using the fine-grained scoring system provided by the organisers.", "labels": [], "entities": []}, {"text": "Finally, we should remark that the results are not skewed (for instance, for resolving ties) by the most frequent sense in WN or any other statistically predicted knowledge.", "labels": [], "entities": []}, {"text": "We evaluated KnowNet using the framework of Section 5, that is, the noun part of the test set from the Senseval-3 English lexical sample task.", "labels": [], "entities": [{"text": "Senseval-3 English lexical sample task", "start_pos": 103, "end_pos": 141, "type": "TASK", "confidence": 0.5563828885555268}]}, {"text": "presents ordered by F1 measure, the performance in terms of precision (P), recall (R) and F1 measure (F1, harmonic mean of recall and precision) of each knowledge resource on Senseval-3 and its average size of the TS per word-sense.", "labels": [], "entities": [{"text": "F1 measure", "start_pos": 20, "end_pos": 30, "type": "METRIC", "confidence": 0.9823970198631287}, {"text": "precision (P)", "start_pos": 60, "end_pos": 73, "type": "METRIC", "confidence": 0.9255285561084747}, {"text": "recall (R)", "start_pos": 75, "end_pos": 85, "type": "METRIC", "confidence": 0.9562620669603348}, {"text": "F1 measure", "start_pos": 90, "end_pos": 100, "type": "METRIC", "confidence": 0.9932942092418671}, {"text": "F1", "start_pos": 102, "end_pos": 104, "type": "METRIC", "confidence": 0.9026603698730469}, {"text": "recall", "start_pos": 123, "end_pos": 129, "type": "METRIC", "confidence": 0.5413448214530945}, {"text": "precision", "start_pos": 134, "end_pos": 143, "type": "METRIC", "confidence": 0.9585200548171997}, {"text": "TS", "start_pos": 214, "end_pos": 216, "type": "METRIC", "confidence": 0.9893814325332642}]}, {"text": "The different KnowNet versions appear marked in bold and the baselines appear in italics.", "labels": [], "entities": []}, {"text": "In this table, TRAIN has been calculated with a vector size of at maximum 450 words.", "labels": [], "entities": [{"text": "TRAIN", "start_pos": 15, "end_pos": 20, "type": "METRIC", "confidence": 0.9954692125320435}]}, {"text": "As expected, RANDOM baseline obtains the poorest result.", "labels": [], "entities": [{"text": "RANDOM", "start_pos": 13, "end_pos": 19, "type": "METRIC", "confidence": 0.6260144710540771}]}, {"text": "The most frequent senses obtained from SemCor (SEMCOR-MFS) and WN (WN-MFS) are both below the most frequent sense of the training corpus (TRAIN-MFS).", "labels": [], "entities": [{"text": "TRAIN-MFS", "start_pos": 138, "end_pos": 147, "type": "METRIC", "confidence": 0.965212881565094}]}, {"text": "However, all of them are far below to the Topic Signatures acquired using the training corpus (TRAIN).", "labels": [], "entities": [{"text": "TRAIN", "start_pos": 95, "end_pos": 100, "type": "METRIC", "confidence": 0.938155472278595}]}, {"text": "The best resources would be those obtaining better performances with a smaller number of related words per synset.", "labels": [], "entities": []}, {"text": "The best results are obtained by TSSEM (with F1 of 52.4).", "labels": [], "entities": [{"text": "TSSEM", "start_pos": 33, "end_pos": 38, "type": "METRIC", "confidence": 0.924737274646759}, {"text": "F1", "start_pos": 45, "end_pos": 47, "type": "METRIC", "confidence": 0.998964786529541}]}, {"text": "The lowest result is obtained by the knowledge directly gathered from WN mainly because of its poor coverage (R of 18.4 and F1 of 26.1).", "labels": [], "entities": [{"text": "WN", "start_pos": 70, "end_pos": 72, "type": "DATASET", "confidence": 0.9741392135620117}, {"text": "coverage", "start_pos": 100, "end_pos": 108, "type": "METRIC", "confidence": 0.9962840676307678}, {"text": "R", "start_pos": 110, "end_pos": 111, "type": "METRIC", "confidence": 0.995973527431488}, {"text": "F1", "start_pos": 124, "end_pos": 126, "type": "METRIC", "confidence": 0.9995232820510864}]}, {"text": "Interestingly, the knowledge integrated in the MCR although partly derived by automatic means performs much better in terms of precision, recall and F1 measures than using them separately (F1 with 18.4 points higher than WN, 9.1 than XWN and 3.7 than spSemCor).", "labels": [], "entities": [{"text": "precision", "start_pos": 127, "end_pos": 136, "type": "METRIC", "confidence": 0.9995785355567932}, {"text": "recall", "start_pos": 138, "end_pos": 144, "type": "METRIC", "confidence": 0.9994751811027527}, {"text": "F1 measures", "start_pos": 149, "end_pos": 160, "type": "METRIC", "confidence": 0.9766673147678375}, {"text": "F1", "start_pos": 189, "end_pos": 191, "type": "METRIC", "confidence": 0.9968584775924683}]}, {"text": "Despite its small size, the resources derived from SemCor obtain better results than its counterparts using much larger corpora (TSSEM vs. TSWEB and spSemCor vs. spBNC).", "labels": [], "entities": []}, {"text": "Regarding the baselines, all knowledge resources surpass RANDOM, but none achieves neither WN-MFS, TRAIN-MFS nor TRAIN.", "labels": [], "entities": [{"text": "RANDOM", "start_pos": 57, "end_pos": 63, "type": "DATASET", "confidence": 0.5485336184501648}, {"text": "TRAIN-MFS", "start_pos": 99, "end_pos": 108, "type": "METRIC", "confidence": 0.9927789568901062}, {"text": "TRAIN", "start_pos": 113, "end_pos": 118, "type": "METRIC", "confidence": 0.9823563694953918}]}, {"text": "Only TSSEM obtains better results than SEMCOR-MFS and is very close to the most frequent sense of WN (WN-MFS) and the training (TRAIN-MFS).", "labels": [], "entities": [{"text": "TSSEM", "start_pos": 5, "end_pos": 10, "type": "METRIC", "confidence": 0.9398394227027893}, {"text": "SEMCOR-MFS", "start_pos": 39, "end_pos": 49, "type": "DATASET", "confidence": 0.4879325032234192}, {"text": "TRAIN-MFS", "start_pos": 128, "end_pos": 137, "type": "METRIC", "confidence": 0.876289963722229}]}, {"text": "The different versions of KnowNet consistently obtain better performances as they increase the window size of processed words of TSWEB.", "labels": [], "entities": []}, {"text": "As expected, KnowNet-5 obtain the lower results.", "labels": [], "entities": []}, {"text": "However, it performs better than WN (and all its extensions) and spBNC.", "labels": [], "entities": [{"text": "WN", "start_pos": 33, "end_pos": 35, "type": "DATASET", "confidence": 0.8035228848457336}]}, {"text": "Interestingly, from KnowNet-10, all KnowNet versions surpass the knowledge resources used for their construction (WN, XWN, TSWEB and WN+XWN).", "labels": [], "entities": [{"text": "TSWEB", "start_pos": 123, "end_pos": 128, "type": "METRIC", "confidence": 0.8100321888923645}]}, {"text": "Furthermore, the integration of WN+XWN+KN\u221220 performs better than MCR and similarly to MCR 2 (having less than 50 times its size).", "labels": [], "entities": []}, {"text": "It is also interesting to note that WN+XWN+KN\u221220 has a better performance than their individual resources, indicating a complementary knowledge.", "labels": [], "entities": []}, {"text": "In fact, WN+XWN+KN\u221220 performs much better than the resources from which it derives (WN, XWN and TSWEB).", "labels": [], "entities": [{"text": "TSWEB", "start_pos": 97, "end_pos": 102, "type": "METRIC", "confidence": 0.8424919247627258}]}, {"text": "These initial results seem to be very promising.", "labels": [], "entities": []}, {"text": "If we do not consider the resources derived from manually sense annotated data (spSemCor, MCR, TSSEM, etc.), KnowNet-10 performs better that any knowledge resource derived by manual or automatic means.", "labels": [], "entities": []}, {"text": "In fact, KnowNet-15 and KnowNet-20 outperforms spSemCor which was derived from manually annotated corpora.", "labels": [], "entities": []}, {"text": "This is a very interesting result since these KnowNet versions have been derived only with the knowledge coming from WN and the web (that is, TSWEB), and WN and XWN as a knowledge source for SSI-Dijkstra (eXtended WordNet only has 17,185 manually labelled senses).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Number of synset relations", "labels": [], "entities": []}, {"text": " Table 2: TS of party#n#1 (first 10 out of 12,890 total words)", "labels": [], "entities": [{"text": "TS", "start_pos": 10, "end_pos": 12, "type": "METRIC", "confidence": 0.9047006368637085}]}, {"text": " Table 3. This TS has been obtained from  BNC using InfoMap. From the ten words appearing in the TS, two of them do not  appear in WN (corresponding to the proper names heathrow#n and gatwick#n), four  words are monosemous (airport#n, airfield#n, travelling#n and passenger#n) and four  other are polysemous (flight#n, train#n, station#n and ferry#n).", "labels": [], "entities": [{"text": "BNC", "start_pos": 42, "end_pos": 45, "type": "DATASET", "confidence": 0.9686574339866638}, {"text": "WN", "start_pos": 131, "end_pos": 133, "type": "DATASET", "confidence": 0.883880078792572}]}, {"text": " Table 3: First ten words with weigths and number of senses in WN of the Topic  Signature for airport#n#1 obtained from BNC using InfoMap", "labels": [], "entities": [{"text": "BNC", "start_pos": 120, "end_pos": 123, "type": "DATASET", "confidence": 0.8678346872329712}, {"text": "InfoMap", "start_pos": 130, "end_pos": 137, "type": "DATASET", "confidence": 0.6457439661026001}]}, {"text": " Table 4: Minimum distances from airport#n#1", "labels": [], "entities": [{"text": "Minimum", "start_pos": 10, "end_pos": 17, "type": "METRIC", "confidence": 0.8306890726089478}]}, {"text": " Table 6: Size and percentage of overlapping relations between KnowNet versions and  WN+XWN", "labels": [], "entities": [{"text": "WN+XWN", "start_pos": 85, "end_pos": 91, "type": "DATASET", "confidence": 0.6178096930185953}]}, {"text": " Table 7: Percentage of overlapping relations between KnowNet versions", "labels": [], "entities": []}, {"text": " Table 8: P, R and F1 fine-grained results for the resources evaluated at Senseval-3,  English Lexical Sample Task", "labels": [], "entities": [{"text": "F1", "start_pos": 19, "end_pos": 21, "type": "METRIC", "confidence": 0.9959971904754639}]}]}