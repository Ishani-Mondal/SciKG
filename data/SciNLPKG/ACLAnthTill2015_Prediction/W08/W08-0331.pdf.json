{"title": [{"text": "Ranking vs. Regression in Machine Translation Evaluation", "labels": [], "entities": [{"text": "Machine Translation Evaluation", "start_pos": 26, "end_pos": 56, "type": "TASK", "confidence": 0.8564487099647522}]}], "abstractContent": [{"text": "Automatic evaluation of machine translation (MT) systems is an important research topic for the advancement of MT technology.", "labels": [], "entities": [{"text": "Automatic evaluation of machine translation (MT)", "start_pos": 0, "end_pos": 48, "type": "TASK", "confidence": 0.8352617546916008}, {"text": "MT", "start_pos": 111, "end_pos": 113, "type": "TASK", "confidence": 0.9956048727035522}]}, {"text": "Most automatic evaluation methods proposed to date are score-based: they compute scores that represent translation quality, and MT systems are compared on the basis of these scores.", "labels": [], "entities": [{"text": "MT", "start_pos": 128, "end_pos": 130, "type": "TASK", "confidence": 0.9760515689849854}]}, {"text": "We advocate an alternative perspective of automatic MT evaluation based on ranking.", "labels": [], "entities": [{"text": "MT evaluation", "start_pos": 52, "end_pos": 65, "type": "TASK", "confidence": 0.9608756005764008}]}, {"text": "Instead of producing scores, we directly produce a ranking over the set of MT systems to be compared.", "labels": [], "entities": [{"text": "MT", "start_pos": 75, "end_pos": 77, "type": "TASK", "confidence": 0.9567453265190125}]}, {"text": "This perspective is often simpler when the evaluation goal is system comparison.", "labels": [], "entities": []}, {"text": "We argue that it is easier to elicit human judgments of ranking and develop a machine learning approach to train on rank data.", "labels": [], "entities": []}, {"text": "We compare this ranking method to a score-based regression method on WMT07 data.", "labels": [], "entities": [{"text": "WMT07 data", "start_pos": 69, "end_pos": 79, "type": "DATASET", "confidence": 0.9809181988239288}]}, {"text": "Results indicate that ranking achieves higher correlation to human judgments, especially in cases where ranking-specific features are used.", "labels": [], "entities": []}], "introductionContent": [], "datasetContent": [{"text": "For experiments, we split the provided development data into train, dev, and test sets (see).", "labels": [], "entities": []}, {"text": "The data split is randomized at the level of different evaluation tracks (e.g. en-es.test, de-en.test are different tracks) in order to ensure that dev/test are sufficiently novel with respect to the training data.", "labels": [], "entities": []}, {"text": "This is important since machine learning approaches have the risk of overfitting and spreading data from the same track to both train and test could lead to overoptimistic results.", "labels": [], "entities": []}, {"text": "In the first experiment, we compared Regression SVM and Rank SVM (both used Features 1-12) by training on varying amounts of training data.", "labels": [], "entities": [{"text": "Regression", "start_pos": 37, "end_pos": 47, "type": "METRIC", "confidence": 0.9502429366111755}, {"text": "Rank", "start_pos": 56, "end_pos": 60, "type": "METRIC", "confidence": 0.9493512511253357}]}, {"text": "The sentence-level rankings produced by each are compared to human judgments using the Spearman rank correlation coefficient (see).", "labels": [], "entities": [{"text": "Spearman rank correlation coefficient", "start_pos": 87, "end_pos": 124, "type": "METRIC", "confidence": 0.6576202884316444}]}], "tableCaptions": [{"text": " Table 2: Data characteristics: the training data contains  8 tracks, which contained 6528 sentence evaluations or  1504 sets of human rankings (T = 1504).", "labels": [], "entities": [{"text": "T", "start_pos": 145, "end_pos": 146, "type": "METRIC", "confidence": 0.9554694294929504}]}]}