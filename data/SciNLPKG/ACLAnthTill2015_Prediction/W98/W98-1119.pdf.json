{"title": [{"text": "A Statistical Approach to Anaphora Resolution", "labels": [], "entities": [{"text": "Statistical Approach to Anaphora", "start_pos": 2, "end_pos": 34, "type": "TASK", "confidence": 0.7909985184669495}]}], "abstractContent": [{"text": "This paper presents an algorithm for identifying pronominal anaphora and two experiments based upon this algorithm.", "labels": [], "entities": []}, {"text": "We incorporate multiple anaphora resolution factors into a statistical framework-specifically the distance between the pronoun and the proposed antecedent, gender/number/animaticity of the proposed antecedent, governing head information and noun phrase repetition.", "labels": [], "entities": [{"text": "noun phrase repetition", "start_pos": 241, "end_pos": 263, "type": "TASK", "confidence": 0.7283099293708801}]}, {"text": "We combine them into a single probability that enables us to identify the referent.", "labels": [], "entities": []}, {"text": "Our first experiment shows the relative contribution of each source Of information and demonstrates a success rate of 82.9% for all sources combined.", "labels": [], "entities": []}, {"text": "The second experiment investigates a method for unsuper-vised learning of gender/number/animaticity information.", "labels": [], "entities": [{"text": "learning of gender/number/animaticity information", "start_pos": 62, "end_pos": 111, "type": "TASK", "confidence": 0.7020962685346603}]}, {"text": "We present some experiments illustrating the accuracy of the method and note that with this information added, our pronoun resolution method achieves 84.2% accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 45, "end_pos": 53, "type": "METRIC", "confidence": 0.9994319081306458}, {"text": "pronoun resolution", "start_pos": 115, "end_pos": 133, "type": "TASK", "confidence": 0.7188891172409058}, {"text": "accuracy", "start_pos": 156, "end_pos": 164, "type": "METRIC", "confidence": 0.9977602958679199}]}], "introductionContent": [{"text": "We present a statistical method for determining pronoun anaphora.", "labels": [], "entities": []}, {"text": "This program differs from earlier work in its almost complete lack of hand-crafting, relying instead on a very small corpus of Penn Wall Street Journal Tree-bank text) that has been marked with co-reference information.", "labels": [], "entities": [{"text": "Penn Wall Street Journal Tree-bank text", "start_pos": 127, "end_pos": 166, "type": "DATASET", "confidence": 0.9651364088058472}]}, {"text": "The first sections of this paper describe this program: the probabilistic model behind it, its implementation, and its performance.", "labels": [], "entities": []}, {"text": "The second half of the paper describes a method for using (portions of) t~e aforementioned program to learn automatically the typical gender of English words, information that is itself used in the pronoun resolution program.", "labels": [], "entities": [{"text": "pronoun resolution", "start_pos": 198, "end_pos": 216, "type": "TASK", "confidence": 0.7220200747251511}]}, {"text": "In particular, the scheme infers the gender of a referent from the gender of the pronouns that 161 refer to it and selects referents using the pronoun anaphora program.", "labels": [], "entities": []}, {"text": "We present some typical results as well as the more rigorous results of a blind evaluation of its output.", "labels": [], "entities": []}], "datasetContent": [{"text": "The algorithm has two modules.", "labels": [], "entities": []}, {"text": "One collects the statistics on the training corpus required by equation (8) and the other uses these probabilities to resolve pronouns in the test corpus.", "labels": [], "entities": []}, {"text": "Our data consists of 93,931 words (3975 sentences) and contains 2477 pronouns, 1371 of which are singular (he, she and it).", "labels": [], "entities": []}, {"text": "The corpus is manually tagged with reference indices and referents\" repetition numbers.", "labels": [], "entities": []}, {"text": "The result presented here is the accuracy of the program in finding antecedents for he, she, and it and their various forms (e.g. him, his, himself, etc.)", "labels": [], "entities": [{"text": "accuracy", "start_pos": 33, "end_pos": 41, "type": "METRIC", "confidence": 0.9988214373588562}]}, {"text": "The cases where \"it\" is merely a dummy subject in a cleft sentence (example 1) or has conventional unspecified referents (example 2) are excluded from computing the precision: \u2022 Example 1: It is very hard to justify paying a silly price for Jaguar if an out-and-out bidding war were to start now.", "labels": [], "entities": [{"text": "precision", "start_pos": 165, "end_pos": 174, "type": "METRIC", "confidence": 0.9991827607154846}, {"text": "Jaguar", "start_pos": 241, "end_pos": 247, "type": "DATASET", "confidence": 0.9277698993682861}]}, {"text": "\u2022 Example 2: It is raining.", "labels": [], "entities": []}, {"text": "We performed a ten-way cross-validation where we reserved 10% of the corpus for testing and used the remaining 90% for training.", "labels": [], "entities": []}, {"text": "Our preliminary results are shown in the last line of.", "labels": [], "entities": []}, {"text": "We are also interested in finding the relative importance of each probability (i.e. each of the four factors in equation in pronoun resolution.", "labels": [], "entities": [{"text": "pronoun resolution", "start_pos": 124, "end_pos": 142, "type": "TASK", "confidence": 0.740429013967514}]}, {"text": "To this end, we ran the program \"incrementally\", each time incorporating one more probability.", "labels": [], "entities": []}, {"text": "The results are shown in (all obtained from cross-validation).", "labels": [], "entities": []}, {"text": "The last column of contains the p-values for testing the statistical significance of each improvement.", "labels": [], "entities": []}, {"text": "Due to relatively large differences between Tree:bank parse trees and Hobbs' trees, our Hobbs' implementation does not yield as high an accuracy as it would have if we had had perfect Hobbs' tree representations.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 136, "end_pos": 144, "type": "METRIC", "confidence": 0.9983158111572266}]}, {"text": "Since the Hobbs' algorithm serves as the base of our scheme, we expect the accuracy to be much higher with more accurately transformed trees.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 75, "end_pos": 83, "type": "METRIC", "confidence": 0.9996721744537354}]}, {"text": "We also note that the very simple model that ignores syntax and takes the last mentioned noun-phrase as the referent performs quite a bit worse, about 43% correct.", "labels": [], "entities": []}, {"text": "This indicates that syntax does play a very important role in anaphora resolution.", "labels": [], "entities": [{"text": "anaphora resolution", "start_pos": 62, "end_pos": 81, "type": "TASK", "confidence": 0.8219633102416992}]}, {"text": "We see a significant improvement after the word knowledge is added to the program.", "labels": [], "entities": []}, {"text": "The P(plw,d probability gives the system information about gender and animaticity.", "labels": [], "entities": [{"text": "P(plw,d probability", "start_pos": 4, "end_pos": 23, "type": "METRIC", "confidence": 0.9074443131685257}]}, {"text": "The contribution of this factor is quite significant, as ca/n be seen from.", "labels": [], "entities": []}, {"text": "The impact of this probability can be seen more clearly from another experiment in which we tested the program (using just Hobbs distance and gender information) on the training data.", "labels": [], "entities": []}, {"text": "Here the program can bethought of having \"perfect\" gender/animaticity knowledge.", "labels": [], "entities": []}, {"text": "We obtained a success rate of 89.3%.", "labels": [], "entities": []}, {"text": "Although this success rate overstates the effect, it is a clear indication that knowledge of a referent's gender and animaticity is essential to anaphora resolution.", "labels": [], "entities": [{"text": "anaphora resolution", "start_pos": 145, "end_pos": 164, "type": "TASK", "confidence": 0.9576164782047272}]}, {"text": "We hoped that the knowledge about the governing constituent would, like gender and animaticity, make a large contribution.", "labels": [], "entities": []}, {"text": "To our surprise, the improvement is only about 2.2%.", "labels": [], "entities": []}, {"text": "This is partly because selection restrictions are not clearcut in many cases.", "labels": [], "entities": []}, {"text": "Also, some head verbs are too general to restrict the selection of any NP.", "labels": [], "entities": []}, {"text": "Examples are \"is\" and \"has\", which appear frequently in Wall Street Journal: these verbs are not \"selective\" enough and the associated probability is not strong enough to rule out erroneous candidates.", "labels": [], "entities": [{"text": "Wall Street Journal", "start_pos": 56, "end_pos": 75, "type": "DATASET", "confidence": 0.9680282870928446}]}, {"text": "Sparse data also causes a problem in this statistic.", "labels": [], "entities": []}, {"text": "Consequently, we observe a relatively small enhancement to the system.", "labels": [], "entities": []}, {"text": "The mention information gives the sys~em some idea of the story's focus.", "labels": [], "entities": []}, {"text": "The more frequently an entity is repeated, the more likely it is to be the topic of the story and thus to be a candidate for pronominalization.", "labels": [], "entities": []}, {"text": "Our results show that this is indeed the case.", "labels": [], "entities": []}, {"text": "References by pronouns are closely related to the topic or the center of the discourse.", "labels": [], "entities": []}, {"text": "NP repetition is one simple way of approximately identifying the topic.", "labels": [], "entities": [{"text": "NP repetition", "start_pos": 0, "end_pos": 13, "type": "TASK", "confidence": 0.7966267764568329}]}, {"text": "The more accurately the topic of a segment can be identified, the higher the success rate we expect an anaphora resolution system can achieve.", "labels": [], "entities": [{"text": "anaphora resolution", "start_pos": 103, "end_pos": 122, "type": "TASK", "confidence": 0.7228591293096542}]}, {"text": "We ran the program on 21 million words of Wall Street Journal text.", "labels": [], "entities": [{"text": "Wall Street Journal text", "start_pos": 42, "end_pos": 66, "type": "DATASET", "confidence": 0.9670824408531189}]}, {"text": "One can judge the program informally by simply examining the results and determining if the program's gender decisions are correct (occasionally looking at the text for difficult cases).", "labels": [], "entities": []}, {"text": "shows the 43 noun phrases with the highest salience figures (run using the Hobbs algorithm).", "labels": [], "entities": []}, {"text": "An examination of these show that all but three are correct.", "labels": [], "entities": []}, {"text": "(The three mistakes are \"husband,\" \"wife,\" and \"years.\"", "labels": [], "entities": []}, {"text": "We return to the significance of these mistakes later.)", "labels": [], "entities": []}, {"text": "As a measure of the utility of these results, we also ran our pronoun-anaphora program with these statistics added.", "labels": [], "entities": []}, {"text": "This achieved an accuracy rate of 84.2%.", "labels": [], "entities": [{"text": "accuracy rate", "start_pos": 17, "end_pos": 30, "type": "METRIC", "confidence": 0.9844141900539398}]}, {"text": "This is only a small improvement over what was achieved without the data.", "labels": [], "entities": []}, {"text": "We believe, however, that there are ways to improve the accuracy of the learning method and thus increase its influence on pronoun anaphora resolution.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 56, "end_pos": 64, "type": "METRIC", "confidence": 0.9988548755645752}, {"text": "pronoun anaphora resolution", "start_pos": 123, "end_pos": 150, "type": "TASK", "confidence": 0.7105444073677063}]}, {"text": "Finally we attempted a fully automatic direct test of the accuracy of both pronoun methods for gender determination.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 58, "end_pos": 66, "type": "METRIC", "confidence": 0.9990718364715576}, {"text": "gender determination", "start_pos": 95, "end_pos": 115, "type": "TASK", "confidence": 0.7605684697628021}]}, {"text": "To that end, we devised a more objective test, useful only for scoring the subset of referents that are names of people.", "labels": [], "entities": []}, {"text": "In particular, we assume that any noun-phrase with the honorifics \"Mr.\".", "labels": [], "entities": []}, {"text": "\"Mrs.\" or \"Ms.\" maybe confidently assigned to gender classes HE, SHE, and SHE, respectively.", "labels": [], "entities": [{"text": "HE", "start_pos": 61, "end_pos": 63, "type": "METRIC", "confidence": 0.9286615252494812}, {"text": "SHE", "start_pos": 65, "end_pos": 68, "type": "METRIC", "confidence": 0.8152822852134705}, {"text": "SHE", "start_pos": 74, "end_pos": 77, "type": "METRIC", "confidence": 0.8973864316940308}]}, {"text": "Thus we compute precision as follows:  There are several things to note about these results.", "labels": [], "entities": [{"text": "precision", "start_pos": 16, "end_pos": 25, "type": "METRIC", "confidence": 0.9971990585327148}]}, {"text": "First, as one might expect given the already noted superior performance of the Hobbs scheme over last-noun, Hobbs also performs better at determining gender.", "labels": [], "entities": []}, {"text": "Secondly, at first glance,the 70.3% accuracy of the Hobbs method is disappointing, only slightly superior to the 65.3% accuracy of Hobbs at finding correct referents.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 36, "end_pos": 44, "type": "METRIC", "confidence": 0.9982233643531799}, {"text": "accuracy", "start_pos": 119, "end_pos": 127, "type": "METRIC", "confidence": 0.9975785613059998}]}, {"text": "It might have been hoped that the statistics would make things considerably more accurate.", "labels": [], "entities": []}, {"text": "In fact, the statistics do make things considerably more accurate.", "labels": [], "entities": []}, {"text": "shows average accuracy as a function of number of references fora given referent.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 14, "end_pos": 22, "type": "METRIC", "confidence": 0.9857655763626099}]}, {"text": "It can be seen that there is a significant improvement with increased referent count.", "labels": [], "entities": []}, {"text": "The reason that the average overall referents is so low is that the counts on referents obey Zipf's law, so that the mode ~f the distribution on counts is one.", "labels": [], "entities": []}, {"text": "Thus the 70.3% overall accuracy is a mix of relatively high accuracy for referents with counts greater than one, and relatively low accuracy for referents with counts of exactly one.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 23, "end_pos": 31, "type": "METRIC", "confidence": 0.9944226145744324}, {"text": "accuracy", "start_pos": 60, "end_pos": 68, "type": "METRIC", "confidence": 0.9981279969215393}, {"text": "accuracy", "start_pos": 132, "end_pos": 140, "type": "METRIC", "confidence": 0.9975613355636597}]}], "tableCaptions": [{"text": " Table 1: Cross-validation: incremental results", "labels": [], "entities": []}]}