{"title": [], "abstractContent": [{"text": "A statistical classification algorithm and its application to language identification from noisy input are described.", "labels": [], "entities": [{"text": "statistical classification", "start_pos": 2, "end_pos": 28, "type": "TASK", "confidence": 0.8465419709682465}, {"text": "language identification from noisy input", "start_pos": 62, "end_pos": 102, "type": "TASK", "confidence": 0.8188593506813049}]}, {"text": "The main innovation is to compute confidence limits on the classification, so that the algorithm terminates when enough evidence to make a clear decision has been made, and so avoiding problems with categories that have similar characteristics.", "labels": [], "entities": []}, {"text": "A second application , to genre identification, is briefly examined.", "labels": [], "entities": [{"text": "genre identification", "start_pos": 26, "end_pos": 46, "type": "TASK", "confidence": 0.9058378338813782}]}, {"text": "The results show that some of the problems of other language identification techniques can be avoided, and illustrate a more important point: that a statistical language process can be used to provide feedback about its own success rate.", "labels": [], "entities": [{"text": "language identification", "start_pos": 52, "end_pos": 75, "type": "TASK", "confidence": 0.7594857215881348}]}], "introductionContent": [{"text": "Language identification is an example of a general class of problems in which we want to assign an input data stream to one of several categories as quickly and accurately as possible.", "labels": [], "entities": [{"text": "Language identification", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.7228421717882156}]}, {"text": "It can be solved using many techniques, including knowledge-poor statistical approaches.", "labels": [], "entities": []}, {"text": "Typically, the distribution of n-grams of characters or other objects is used to form a model.", "labels": [], "entities": []}, {"text": "A comparison of the input against the model determines the language which matches best.", "labels": [], "entities": []}, {"text": "Versions of this simple technique can be found in and, while an interesting practical implementation is described by.", "labels": [], "entities": []}, {"text": "A variant of the problem is considered by, and, who look at it from the point of view of Optical Character Recognition (OCR).", "labels": [], "entities": [{"text": "Optical Character Recognition (OCR)", "start_pos": 89, "end_pos": 124, "type": "TASK", "confidence": 0.7479182531436285}]}, {"text": "Here, the language model for the OCR system cannot be selected until the language has been identified.", "labels": [], "entities": []}, {"text": "They therefore work with so-called shape tokens, which give a very approximate encoding of the characters' shapes on the printed page without needing full-scale OCR.", "labels": [], "entities": []}, {"text": "For example, all uppercase letters are treated as being one character shape, all characters with a descender are another, and soon.", "labels": [], "entities": []}, {"text": "Sequences of character shape codes separated by white space are assembled into word shape tokens.", "labels": [], "entities": []}, {"text": "Sibun and Spitz then determine the language on the basis of linear discriminant analysis (LDA) over word shape tokens, while Sibun and Reynar explore the use of entropy relative to training data for character shape unigrams, bigrams and trigrams.", "labels": [], "entities": []}, {"text": "Both techniques are capable of over 90% accuracy for most languages.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 40, "end_pos": 48, "type": "METRIC", "confidence": 0.9990473389625549}]}, {"text": "However, the LDA-based technique tends to perform significantly worse for languages which are similar to one another, such as the Norse languages.", "labels": [], "entities": []}, {"text": "Relative entropy performs better, but still has some noticeable error clusters, such as confusion between Croatian, Serbian and Slovenian.", "labels": [], "entities": []}, {"text": "What these techniques lack is a measure of when enough information has been accumulated to distinguish one language from another reliably: they examine all of the input data and then make the decision.", "labels": [], "entities": []}, {"text": "Here we will look at a different approach which attempts to overcome this by maintaining a measure of the total evidence accumulated for each language and how much confidence there is in the measure.", "labels": [], "entities": []}, {"text": "To outline the approach: 1.", "labels": [], "entities": []}, {"text": "The input is processed one (word shape) token at a time.", "labels": [], "entities": []}, {"text": "For each language, we determine the probability that the token is in that language, expressed as a 95% confidence range.", "labels": [], "entities": []}, {"text": "2. The values for each word are accumulated into an overall score with a confidence range for the input to date, and compared both to an absolute threshold, and with . each other.", "labels": [], "entities": []}, {"text": "Thus, to select a language, we require not only that it has a high score (probability, roughly), but also that it is significantly better scoring than any other.", "labels": [], "entities": []}, {"text": "If the process fails to make a decision on the data that is available, the subset of the languages which have exceeded the absolute threshold can be output, so that even if a final decision has not been made, the likely possibilities have been narrowed down.", "labels": [], "entities": []}, {"text": "We look at this procedure in more detail below, with particular emphasis on how the underlying statistical model provides confidence intervals.", "labels": [], "entities": []}, {"text": "An evaluation of the technique on data similar to that used by Sibun and Reynar follows I.", "labels": [], "entities": []}], "datasetContent": [{"text": "To evaluate the technique, a test was run using similar data to Sibun and Reynar.", "labels": [], "entities": []}, {"text": "Corpora for eighteen languages from the European Corpus Initiative CDROM 1 were extracted and split into non-overlapping files, one containing 2000 tokens 3, one containing 200 tokens, and 25 files each of 1, 5, 10 and 20 tokens.", "labels": [], "entities": [{"text": "European Corpus Initiative CDROM 1", "start_pos": 40, "end_pos": 74, "type": "DATASET", "confidence": 0.9474656462669373}]}, {"text": "The 2000 and 200 token files were used as training data, and the remainder for test data.", "labels": [], "entities": []}, {"text": "Wherever possible the texts were taken from newspaper corpora, and failing that from novels or literature.", "labels": [], "entities": []}, {"text": "The identification algorithm was run on each test file and the results placed in one of four categories: \u2022 Definitive, correct decision made.", "labels": [], "entities": []}, {"text": "\u2022 No decision made by the end of the input, but highest scoring language was correct.", "labels": [], "entities": []}, {"text": "\u2022 No decision, highest scoring language incorrect.", "labels": [], "entities": []}, {"text": "\u2022 Definitive, incorrect decision made.", "labels": [], "entities": []}, {"text": "The sum of the first two figures divided by the total number of tests gives a measure of accuracy; the sum of the first and last divided by the total gives a measure of decisiveness, expressed as the proportion of the time a definitive decision was made.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 89, "end_pos": 97, "type": "METRIC", "confidence": 0.9994750618934631}, {"text": "decisiveness", "start_pos": 169, "end_pos": 181, "type": "METRIC", "confidence": 0.9579198956489563}]}, {"text": "The tests were executed using word shape tokens on the same coding scheme as Sibun and Reynar, and using the words as they appeared in the corpus.", "labels": [], "entities": []}, {"text": "No adjustments were made for punctuation, case, etc.", "labels": [], "entities": []}, {"text": "Various activation thresholds were tried: raising the threshold increases accuracy by requiring more information before a decision is made, but reduces decisiveness.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 74, "end_pos": 82, "type": "METRIC", "confidence": 0.998149037361145}]}, {"text": "With shapes and 2000 tokens of training data, at a threshold of 14 or more, all the 20 token files gave 100% accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 109, "end_pos": 117, "type": "METRIC", "confidence": 0.9988887906074524}]}, {"text": "For words themselves, the threshold was set to 22.", "labels": [], "entities": []}, {"text": "The results of these tests appear in table 1.", "labels": [], "entities": []}, {"text": "The figures for the activation threshold were determined by experimenting on the data.", "labels": [], "entities": []}, {"text": "An interesting area for further work would be to put this aspect of the procedure on a sounder theoretical basis, perhaps by using the a priori probabilities of the individual languages.", "labels": [], "entities": []}, {"text": "3Sibun and Spitz, and Sibun and Reynar, present their results in terms of lines of input, with 1-5 lines corresponding roughly to a sentence, and 10-20 lines to a paragraph.", "labels": [], "entities": []}, {"text": "Estimating a line a.s 10 words, we are therefore working with significantly smaller data sets.", "labels": [], "entities": []}, {"text": "The accuracy figures are generally similar to or better than those of Sibun and Reynar.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.9997989535331726}]}, {"text": "The corresponding figures for 200 tokens of training data appear in table 2, for the token identification task only.", "labels": [], "entities": [{"text": "token identification task", "start_pos": 85, "end_pos": 110, "type": "TASK", "confidence": 0.8852014342943827}]}, {"text": "One of the strengths of the algorithm is that it makes a decision as soon as one can be made reliably.", "labels": [], "entities": []}, {"text": "shows the average number of tokens which have to be read before a decision can be made, for the cases where the decision was correct and incorrect, and for both cases together.", "labels": [], "entities": []}, {"text": "Again, the results are for word shape tokens, and for words alone.", "labels": [], "entities": []}, {"text": "The figures show that convergence usually happens within about 10 words, with along tailing off to the results.", "labels": [], "entities": []}, {"text": "The longest time to convergence was 153 shape tokens.", "labels": [], "entities": [{"text": "convergence", "start_pos": 20, "end_pos": 31, "type": "TASK", "confidence": 0.9368159770965576}]}, {"text": "A manual inspection of one run (2000 lines of training data, tokens, threshold=14) shows that errors are somtimes clustered, although quite weakly.", "labels": [], "entities": []}, {"text": "For example, Serbian, Croatian and Slovenian show several confusions between them, as in Sibun and Reynar's results.", "labels": [], "entities": []}, {"text": "There are two observations to be made here.", "labels": [], "entities": []}, {"text": "Firstly, there are about as many other errors between these language and languages which are unrelated to them, such as Italian, German and Norwegian, and so the errors maybe due to poor quality data rather than alack of discrimination in the algorithm.", "labels": [], "entities": []}, {"text": "For example, Croatian is incorrectly recognised as Serbian 3 times and as Slovenian once, while the languages which are misrecognised as Croatian are German and Norwegian (once each).", "labels": [], "entities": []}, {"text": "Secondly, even where there are errors, the range of possibilities has been substantially reduced, so that a more powerful process (such as full-scale OCR followed by identification on words rather than shape tokens, or a raising of the threshold and adding more data) could be brought in to finish the job off.", "labels": [], "entities": []}, {"text": "That is, the confidence limits have provided a benefit in reducing the search space.", "labels": [], "entities": []}, {"text": "The confusion matrix for this case appears in an appendix.", "labels": [], "entities": [{"text": "confusion", "start_pos": 4, "end_pos": 13, "type": "METRIC", "confidence": 0.9714805483818054}]}], "tableCaptions": [{"text": " Table 1: Performance with 2000 tokens of training data", "labels": [], "entities": []}, {"text": " Table 2: Performance with 200 tokens of training data (word shape tokens only)", "labels": [], "entities": []}, {"text": " Table 3: Average number of tokens read before convergence", "labels": [], "entities": [{"text": "Average number", "start_pos": 10, "end_pos": 24, "type": "METRIC", "confidence": 0.9458152651786804}]}]}