{"title": [{"text": "Improving summarization through rhetorical parsing tuning", "labels": [], "entities": [{"text": "Improving summarization", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.87784144282341}, {"text": "rhetorical parsing", "start_pos": 32, "end_pos": 50, "type": "TASK", "confidence": 0.8598687946796417}]}], "abstractContent": [{"text": "We study the relationship between the structure of\" discourse and a set of summarization heuristics that are employed by current systems.", "labels": [], "entities": []}, {"text": "A tight coupling of the two enables us to learn genre-specific combinations of heuristics that can be used for disambiguation during discourse parsing.", "labels": [], "entities": [{"text": "discourse parsing", "start_pos": 133, "end_pos": 150, "type": "TASK", "confidence": 0.7020095437765121}]}, {"text": "The same coupling enables us to construct discourse structures that yield summaries that contain textual units that are not only important according to a variety of position-, title-, and lexical-similarity-based heuristics, but also central to the main claims of texts.", "labels": [], "entities": []}, {"text": "A careful analysis of our results enables us to shed some new light on issues related to summary evaluation and learning.", "labels": [], "entities": [{"text": "summary evaluation", "start_pos": 89, "end_pos": 107, "type": "TASK", "confidence": 0.8092001974582672}]}], "introductionContent": [{"text": "There are two ways in which one can integrate a discourse-based measure of textual saliency, such as that described above, with measures of saliency that are based on cohesion, position, similarity with the title, etc.", "labels": [], "entities": []}, {"text": "The simplest way is to compute a probability distribution of the importance of textual units according to the discourse method and to combine it with all probability distributions produced by the other heuristics.", "labels": [], "entities": []}, {"text": "In such an approach, the discourse heuristic is just one of then heuristics that are employed by a system.", "labels": [], "entities": []}, {"text": "Obtaining good summaries amounts then to determining a good way of combining the implemented heuristics.", "labels": [], "entities": []}, {"text": "Overall, a summarization system that works along the lines described above still treats texts as flat sequences of textual units, although the discourse method internally uses a more sophisticated representation.", "labels": [], "entities": []}, {"text": "The shortcoming of such an approach is that it still permits the selection of textual units that do not play a central role in discourse.", "labels": [], "entities": []}, {"text": "For example, if the text to be summarized consists only of units 7 and 8 in text (!), it maybe possible that the combination of the position, title, and discourse heuristics will yield a higher score for unit 7 than for unit 8, although unit 8 is the nucleus of the text and expresses what is important.", "labels": [], "entities": []}, {"text": "Unfortunately, if we interpret text as a flat sequence of units, the rhetorical relation and the nuclearity assignments with respect to these units cannot be appropriately exploited.", "labels": [], "entities": []}, {"text": "A more complex way to integrate discourse, cohesion, position, and other summarization-based methods is to consider that the structure of discourse is the most important factor in determining saliency, an assumption supported by experiments done by . In such an approach, we no longer interpret texts as flat sequences of textual units, but as tree structures that reflect the nuclearity and rhetorical relations that characterize each textual span.", "labels": [], "entities": []}, {"text": "When discourse is taken to be central to the interpretation of text, obtaining good summaries amounts to finding the \"best\" discourse interpretations.", "labels": [], "entities": []}, {"text": "In the rest of the paper, we explore this approach.", "labels": [], "entities": []}, {"text": "3.2 Criteria for measuring the \"goodness\" of discourse structures In order to find the 'best' discourse interpretations, i.e., the interpretations that yield summaries that are most similar to summaries generated manually, we considered seven metrics, which we discuss below.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: The appropriateness of each of the seven metrics  for text summarization in the TREC corpus --the 10%  cutoff.", "labels": [], "entities": [{"text": "text summarization", "start_pos": 64, "end_pos": 82, "type": "TASK", "confidence": 0.7365722954273224}, {"text": "TREC corpus", "start_pos": 90, "end_pos": 101, "type": "DATASET", "confidence": 0.7964238822460175}]}, {"text": " Table 2: The appropriateness of each of the seven metrics  for text summarization in the TREC corpus --the 20%  cutoff.", "labels": [], "entities": [{"text": "text summarization", "start_pos": 64, "end_pos": 82, "type": "TASK", "confidence": 0.7351690530776978}, {"text": "TREC corpus", "start_pos": 90, "end_pos": 101, "type": "DATASET", "confidence": 0.7998407483100891}]}, {"text": " Table 3: The appropriateness of each of the seven metrics", "labels": [], "entities": []}, {"text": " Table 4: The appropriateness of each of the seven metrics", "labels": [], "entities": []}, {"text": " Table 5: The combination of heuristics that yielded the best summaries for the texts in the TREC corpus.", "labels": [], "entities": [{"text": "TREC corpus", "start_pos": 93, "end_pos": 104, "type": "DATASET", "confidence": 0.8715349733829498}]}, {"text": " Table 6: The combination of heuristics that yielded the best summaries for the texts in the Scientific American corpus.", "labels": [], "entities": [{"text": "Scientific American corpus", "start_pos": 93, "end_pos": 119, "type": "DATASET", "confidence": 0.7736202081044515}]}, {"text": " Table 7: A cross-analysis of summarization results in the TREC corpus.", "labels": [], "entities": [{"text": "summarization", "start_pos": 30, "end_pos": 43, "type": "TASK", "confidence": 0.9808835983276367}, {"text": "TREC corpus", "start_pos": 59, "end_pos": 70, "type": "DATASET", "confidence": 0.893854022026062}]}]}