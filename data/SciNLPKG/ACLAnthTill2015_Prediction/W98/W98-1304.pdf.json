{"title": [], "abstractContent": [], "introductionContent": [{"text": "Recent approaches to statistical parsing include those that estimate an approximation of a stochastic, lexicalized ~rammar directly from a treebank and others that rebuild trees with a number of tree-construction operators, which are applied in order according to a stochastic model when parsing a sentence).", "labels": [], "entities": [{"text": "statistical parsing", "start_pos": 21, "end_pos": 40, "type": "TASK", "confidence": 0.7506785988807678}]}, {"text": "The results have been around 86% in labeled precision and recall on the Wall Street Journal treebank.", "labels": [], "entities": [{"text": "labeled", "start_pos": 36, "end_pos": 43, "type": "METRIC", "confidence": 0.9099977016448975}, {"text": "precision", "start_pos": 44, "end_pos": 53, "type": "METRIC", "confidence": 0.8812176585197449}, {"text": "recall", "start_pos": 58, "end_pos": 64, "type": "METRIC", "confidence": 0.999576985836029}, {"text": "Wall Street Journal treebank", "start_pos": 72, "end_pos": 100, "type": "DATASET", "confidence": 0.9824971109628677}]}, {"text": "In this paper we take an entirely different approach to statistical parsing.", "labels": [], "entities": [{"text": "statistical parsing", "start_pos": 56, "end_pos": 75, "type": "TASK", "confidence": 0.8880359828472137}]}, {"text": "We propose a method for left to fight parsing using a Hidden Markov Model (HMM).", "labels": [], "entities": [{"text": "left to fight parsing", "start_pos": 24, "end_pos": 45, "type": "TASK", "confidence": 0.578521378338337}]}, {"text": "The results we obtain are not as good as the more general approaches mentioned above, which consider the whole sentence rather then working in an incremental fashion, but the method does give a number of interesting new perspectives.", "labels": [], "entities": []}, {"text": "In particular, it can be applied in an environment that requires left to right processing, such as a speech recognition system, it can easily process text that has not been separated into sentences (for example when punctuation is mi~ing or when~processing ungrammatical, spoken text), and it can give a shallow parse (i.e., leaving out long distance dependencies) as it is focused On local context.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 101, "end_pos": 119, "type": "TASK", "confidence": 0.7193542569875717}]}, {"text": "It also makes the parsing process closer to the way humans process language, although we do not explore this psychological aspect in this paper.", "labels": [], "entities": [{"text": "parsing process", "start_pos": 18, "end_pos": 33, "type": "TASK", "confidence": 0.9150273203849792}]}, {"text": "In the next three sections we will discuss the way we decide the syntactic context of a word (%raversal strings\"), how this can be used for parsing and how a tree can be constructed ~om them.", "labels": [], "entities": []}, {"text": "The following four sections discuss the HMM model used to predict a syntactic context for every word.", "labels": [], "entities": []}, {"text": "The last two sections discuss the results, conclusions and future perspectives.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": []}