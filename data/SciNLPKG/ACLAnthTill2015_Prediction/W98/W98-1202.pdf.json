{"title": [{"text": "Natural Language Learning by Recurrent Neural Networks: A Comparison with probabilistic approaches", "labels": [], "entities": []}], "abstractContent": [{"text": "We present preliminary results of experiments with two types of recurrent neural networks fora natural language learning task.", "labels": [], "entities": []}, {"text": "The neural networks, Elman networks and Recurrent Cascade Correlation (RCC), were trained on the text of a first-year primary school reader.", "labels": [], "entities": [{"text": "Recurrent Cascade Correlation (RCC)", "start_pos": 40, "end_pos": 75, "type": "METRIC", "confidence": 0.7943378388881683}]}, {"text": "The networks performed a one-step-look-ahead task, i.e. they had to predict the lexical category of the next following word.", "labels": [], "entities": []}, {"text": "Elman networks with 9 hidden units gave the best training results (72% correct) but scored only 63% when tested for generalisation using a \"leave-one-sentence-out\" cross-validation technique.", "labels": [], "entities": [{"text": "generalisation", "start_pos": 116, "end_pos": 130, "type": "TASK", "confidence": 0.9702156782150269}]}, {"text": "An RCC network could learn 99.6% of the training set by adding up to 42 hidden units but achieved best generalisation (63%) with only four hidden units.", "labels": [], "entities": []}, {"text": "Results are presented showing network learning in relation to bi-, t'i-, 4-and 5-gram performance.", "labels": [], "entities": []}, {"text": "Greatest prediction uncertainty (measured as the entropy of the output units) occurred, not at the sentence boundaries but when the first verb was the input.", "labels": [], "entities": []}], "introductionContent": [{"text": "emphasised that natural language input unfolds in time and therefore, recurrent networks which can accept a sequence of input patterns are the preferred choice for many connectionist natural language processing tasks.", "labels": [], "entities": [{"text": "connectionist natural language processing", "start_pos": 169, "end_pos": 210, "type": "TASK", "confidence": 0.746683269739151}]}, {"text": "In recurrent networks, knowledge is represented in activation patterns over hidden units and revealed (i.e. made explicit) by hierarchical cluster analysis or other statistical methods.", "labels": [], "entities": []}, {"text": "Furthermore, recent evidence from cognitive neuroscience points to the importance of recurrent connections for the formation of coherent cell assemblies.", "labels": [], "entities": []}, {"text": "Recent work on recurrent neural networks has focussed on formal languages.", "labels": [], "entities": []}, {"text": "In this paper, we present preliminary results of experiments with recun'ent neural networks fora natural language learning task.", "labels": [], "entities": []}, {"text": "Our strategy is to start with simple children's texts and to step-wise increase the complexity of these texts to explore the learning characteristics of recurrent neural networks.", "labels": [], "entities": []}, {"text": "In the first experiments reported here, we are starting with a firstyear primary school reader from which sentences with embedded structures have been diminatecL In future experiments, we will use unmodified first-year texts and will continue with second-year textbooks and soon.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": []}