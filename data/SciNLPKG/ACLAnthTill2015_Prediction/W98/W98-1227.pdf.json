{"title": [{"text": "A Method of Incorporating Bigram Constraints into an LR Table and Its Effectiveness in Natural Language Processing", "labels": [], "entities": []}], "abstractContent": [{"text": "In this paper, we propose a method for constructing bigram LR tables byway of incorporating bigram constraints into an LR table.", "labels": [], "entities": []}, {"text": "Using a bigram LR table, it is possible fora GLR parser to make use of both big'ram and CFG constraints in natural language processing.", "labels": [], "entities": []}, {"text": "Applying bigram LR tables to our GLR method has the following advantages: (1) Language models utilizing bigzam LR tables have lower perplexity than simple bigram language models, since local constraints (hi-gram) and global constraints (CFG) are combined in a single bigram LR table.", "labels": [], "entities": []}, {"text": "(2) Bigram constraints are easily acquired from a given corpus.", "labels": [], "entities": []}, {"text": "Therefore data sparseness is not likely to arise.", "labels": [], "entities": []}, {"text": "(3) Separation of local and global constraints keeps down the number of CFG rules.", "labels": [], "entities": [{"text": "Separation", "start_pos": 4, "end_pos": 14, "type": "TASK", "confidence": 0.9687000513076782}, {"text": "CFG", "start_pos": 72, "end_pos": 75, "type": "DATASET", "confidence": 0.6746997237205505}]}, {"text": "The first advantage leads to a reduction in complexity, and as the result, better performance in GLR parsing.", "labels": [], "entities": [{"text": "complexity", "start_pos": 44, "end_pos": 54, "type": "METRIC", "confidence": 0.9743948578834534}, {"text": "GLR parsing", "start_pos": 97, "end_pos": 108, "type": "TASK", "confidence": 0.9008970856666565}]}, {"text": "Our experiments demonstrate the effectiveness of our method.", "labels": [], "entities": []}], "introductionContent": [{"text": "In natural language processing, stochastic language models are commonly used for lexical and syntactic disambiguation (.", "labels": [], "entities": [{"text": "natural language processing", "start_pos": 3, "end_pos": 30, "type": "TASK", "confidence": 0.6727322141329447}, {"text": "syntactic disambiguation", "start_pos": 93, "end_pos": 117, "type": "TASK", "confidence": 0.7122167944908142}]}, {"text": "Stochastic language models are also helpful in reducing the complexity of speech and language processing byway of providing probabilistic linguistic constraints.", "labels": [], "entities": []}, {"text": "N-gram language models, including bigram and trigram models, are the most commonly used method of applying local probabilistic constraints.", "labels": [], "entities": []}, {"text": "However, context-free grammars (CFGs) produce more global linguistic constraints than N-gram models.", "labels": [], "entities": []}, {"text": "It seems better to combine both local and global constraints and use them both concurrently in natural language processing.", "labels": [], "entities": []}, {"text": "The reason why N-gram models are preferred over CFGs is that N-gram constraints are easily acquired from a given corpus.", "labels": [], "entities": []}, {"text": "However, the larger N is, the more serious the problem of data sparseness becomes.", "labels": [], "entities": []}, {"text": "CFGs are commonly employed in syntactic parsing as global linguistic constraints, since many eificient parsing algorithms are available.", "labels": [], "entities": [{"text": "syntactic parsing", "start_pos": 30, "end_pos": 47, "type": "TASK", "confidence": 0.7214535176753998}, {"text": "eificient parsing", "start_pos": 93, "end_pos": 110, "type": "TASK", "confidence": 0.7002503871917725}]}, {"text": "GLR (Generalized LR) is one such parsing algorithm that uses an LR table, into which CFG constraints are precompiled in advance.", "labels": [], "entities": [{"text": "GLR", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.7472183704376221}]}, {"text": "Therefore if we can incorporate N-gram constraints into an LR table, we can make concurrent use of both local and global linguistic constraints in GLR parsing.", "labels": [], "entities": [{"text": "GLR parsing", "start_pos": 147, "end_pos": 158, "type": "TASK", "confidence": 0.8567920625209808}]}, {"text": "In the following section, we will propose a method that incorporates bigram constraints into an LR table.", "labels": [], "entities": []}, {"text": "The advantages of the method are summarized as follows: First, it is expected that this method produces a lower perplexity than that fora simple bigram language model, since it is possible to utilize both local (bigram) and global (CFG) constraints in the LR table.", "labels": [], "entities": []}, {"text": "We will evidence this reduction in perplexity by considering states in an LR table for the case of GLR parsing.", "labels": [], "entities": [{"text": "GLR parsing", "start_pos": 99, "end_pos": 110, "type": "TASK", "confidence": 0.8356623649597168}]}, {"text": "Second, bigram constraints are easily acquired from smaller-sized corpora.", "labels": [], "entities": []}, {"text": "Accordingly, data sparseness is not likely to arise.", "labels": [], "entities": []}, {"text": "Third, the separation of local and global constraints makes it easy to describe CFG rules, since CFG writers need not take into'account tedious descriptions of local connection constraints within th CFG I .", "labels": [], "entities": []}], "datasetContent": [{"text": "Perplexity is a measure of the constraint imposed by the language model.", "labels": [], "entities": [{"text": "Perplexity", "start_pos": 0, "end_pos": 10, "type": "METRIC", "confidence": 0.9444496035575867}]}, {"text": "Test-set perplexity) is commonly used to measure the perplexity of a language model from a test-set.", "labels": [], "entities": []}, {"text": "Test-set perplexity fora language model L is simply the geometric mean of probabilities defined by: where Here N is the number of terminal symbols in the test set, M is the number of test sentences and P(S,) is the probability of generating i-th test sentence Si.", "labels": [], "entities": []}, {"text": "In the case of the bigram model, P~i(Si) is: On the other hand, the perplexity of the trigram language model is smaller than that of the bigram LR table.", "labels": [], "entities": []}, {"text": "However, with regard to data sparseness, the bigram LR table is better than the trigram language model because bigram constraints are more easily acquired from a given corpus than trigram constraints.", "labels": [], "entities": []}, {"text": "Although the experiment described above is concerned with natural language processing, our method is also applicable to speech recognition.", "labels": [], "entities": [{"text": "natural language processing", "start_pos": 58, "end_pos": 85, "type": "TASK", "confidence": 0.6994093259175619}, {"text": "speech recognition", "start_pos": 120, "end_pos": 138, "type": "TASK", "confidence": 0.828082799911499}]}], "tableCaptions": [{"text": " Table 1: Initial LR table for G1", "labels": [], "entities": [{"text": "Initial LR table", "start_pos": 10, "end_pos": 26, "type": "METRIC", "confidence": 0.7685807545979818}, {"text": "G1", "start_pos": 31, "end_pos": 33, "type": "TASK", "confidence": 0.450691282749176}]}, {"text": " Table 2: LR table after Steps 2 and 3", "labels": [], "entities": [{"text": "LR", "start_pos": 10, "end_pos": 12, "type": "METRIC", "confidence": 0.6701414585113525}]}, {"text": " Table 3: LR table after Step 4", "labels": [], "entities": [{"text": "LR", "start_pos": 10, "end_pos": 12, "type": "METRIC", "confidence": 0.6525911092758179}]}, {"text": " Table 4: The Bigram LR table constructed by Algorithm 1", "labels": [], "entities": [{"text": "Bigram LR table", "start_pos": 14, "end_pos": 29, "type": "DATASET", "confidence": 0.8854153752326965}]}]}