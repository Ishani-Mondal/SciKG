{"title": [{"text": "Discourse Parsing: A Decision Tree Approach", "labels": [], "entities": [{"text": "Discourse Parsing", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.6600940376520157}]}], "abstractContent": [{"text": "The paper presents anew statistical method, for parsing discourse.", "labels": [], "entities": [{"text": "parsing discourse", "start_pos": 48, "end_pos": 65, "type": "TASK", "confidence": 0.909983217716217}]}, {"text": "A parse of discourse is defined as a set of semantic dependencies among sentences that makeup the discourse.", "labels": [], "entities": []}, {"text": "A collection of news articles from a Japanese economics daily are manually marked for dependency and used as a train-ing/testing corpus.", "labels": [], "entities": []}, {"text": "We use a C4.5 decision tree method to develop a model of sentential dependencies.", "labels": [], "entities": []}, {"text": "However, rather than to use class decisions made by C4.5, we exploit information on class distributions to rank possible dependencies among sentences according to their probabilistic strength and take a parse to be a set of highest ranking dependencies.", "labels": [], "entities": []}, {"text": "We also study effects of features such as clue words, distance and similarity on the performance of the discourse parser.", "labels": [], "entities": []}, {"text": "Experiments have found that the method performs reasonably well on diverse text types, scoring an accuracy rate of over 60%.", "labels": [], "entities": [{"text": "accuracy rate", "start_pos": 98, "end_pos": 111, "type": "METRIC", "confidence": 0.9902466833591461}]}], "introductionContent": [{"text": "Attempts to the automatic identification of a structure in discourse have so far met with a limited success in the computational linguistics literature.", "labels": [], "entities": [{"text": "automatic identification of a structure in discourse", "start_pos": 16, "end_pos": 68, "type": "TASK", "confidence": 0.7547480804579598}]}, {"text": "Part of the reason is that, compared to sizable data resources available to parsing research such as the Penn Treebank (), large corpora annotated for discourse information are hard to come by.", "labels": [], "entities": [{"text": "Penn Treebank", "start_pos": 105, "end_pos": 118, "type": "DATASET", "confidence": 0.9948173463344574}]}, {"text": "Researchers in discourse usually work with a corpus of a few hundred sentences (.", "labels": [], "entities": []}, {"text": "The lack of a large-scale corpus has made it impossible to talk about results of discourse studies with the sufficient degree of reliability.", "labels": [], "entities": []}, {"text": "In the work described here, we created a corpus with discourse information, containing 645 articles from a Japanese economic paper, an order of magnitude larger than any previous work on discourse processing.", "labels": [], "entities": []}, {"text": "It had a total of 12.770 sentences and 5,352 paragraphs.", "labels": [], "entities": []}, {"text": "Each article in the corpus was manually annotated fora discourse dependency\" relation.", "labels": [], "entities": []}, {"text": "We then built a statistical discourse parser based on the C4.5 decision tree method, which was trained and tested on the corpus we have cre- ated.", "labels": [], "entities": [{"text": "statistical discourse parser", "start_pos": 16, "end_pos": 44, "type": "TASK", "confidence": 0.6043195923169454}]}, {"text": "The design of a parser was inspired by's work on statistical sentence parsing.", "labels": [], "entities": [{"text": "statistical sentence parsing", "start_pos": 49, "end_pos": 77, "type": "TASK", "confidence": 0.6556018590927124}]}, {"text": "The paper is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 presents general ideas about statistical parsing as applied to the discourse, After a brief introduction to some of the points of a decision tree model, we discuss incorporating a decision tree within a statistical parsing model.", "labels": [], "entities": [{"text": "statistical parsing", "start_pos": 39, "end_pos": 58, "type": "TASK", "confidence": 0.7668080031871796}, {"text": "statistical parsing", "start_pos": 213, "end_pos": 232, "type": "TASK", "confidence": 0.6994939744472504}]}, {"text": "In Section 3, we explain how we have built an annotated corpus.", "labels": [], "entities": []}, {"text": "There we also describe a procedure of experiments we have conducted, and conclude the section with their results.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1:  An illustration:  hotel preferences.  'Bath/shower' means a room has a bath, a shower or  none. 'Time' means the travel time in min. from an  airport. 'Class' indicates whether a particular hotel  is a customer's choice.", "labels": [], "entities": []}, {"text": " Table 4: Effects of pruning on performance. CF refers to a confidence value. Small CF values cause more  prunings than large values.", "labels": [], "entities": [{"text": "CF", "start_pos": 45, "end_pos": 47, "type": "METRIC", "confidence": 0.9937528967857361}]}, {"text": " Table 5: Measuring the significance of features. Figures below indicate how much the performance is affected  by the removal of a feature. 'REF' refers to a model where no feature is removed. 'Clues' indicates the number  of clues used for a model. A minus sign at a feature indicates the removal of that feature from a model.", "labels": [], "entities": [{"text": "REF", "start_pos": 141, "end_pos": 144, "type": "METRIC", "confidence": 0.9672788977622986}]}, {"text": " Table 6: Connectives found in the corpus. Underlined items (also marked with an asterisk) are those that  the tokenizer program erroneously identified as a connective.", "labels": [], "entities": []}]}