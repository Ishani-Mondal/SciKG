{"title": [], "abstractContent": [{"text": "We present anew method for discovering a segmental discourse structure of a document while categorizing each segment's function and importance.", "labels": [], "entities": []}, {"text": "Segments are determined by a zero-sum weighting scheme, used on occurrences of noun phrases and pronominal forms retrieved from the document.", "labels": [], "entities": []}, {"text": "Segment roles are then calculated from the distribution of the terms in the segment.", "labels": [], "entities": []}, {"text": "Finally, we present results of evaluation in terms of precision and recall which surpass earlier approaches'.", "labels": [], "entities": [{"text": "precision", "start_pos": 54, "end_pos": 63, "type": "METRIC", "confidence": 0.9993509650230408}, {"text": "recall", "start_pos": 68, "end_pos": 74, "type": "METRIC", "confidence": 0.998762845993042}]}], "introductionContent": [{"text": "Identification of discourse structure can be extremely useful to natural language processing applications such as automatic text summarization or information retrieval (IR).", "labels": [], "entities": [{"text": "Identification of discourse structure", "start_pos": 0, "end_pos": 37, "type": "TASK", "confidence": 0.8256578147411346}, {"text": "automatic text summarization", "start_pos": 114, "end_pos": 142, "type": "TASK", "confidence": 0.5839647849400839}, {"text": "information retrieval (IR)", "start_pos": 146, "end_pos": 172, "type": "TASK", "confidence": 0.8228579759597778}]}, {"text": "For example, a summarization agent might chose to summarize each discourse segment separately.", "labels": [], "entities": [{"text": "summarization", "start_pos": 15, "end_pos": 28, "type": "TASK", "confidence": 0.9834635853767395}]}, {"text": "Also, segmentation of a document into blocks of topically similar text can assist a search engine in choosing to retrieve or highlight a segment in which a query term occurs.", "labels": [], "entities": [{"text": "segmentation of a document into blocks of topically similar text", "start_pos": 6, "end_pos": 70, "type": "TASK", "confidence": 0.7613404929637909}]}, {"text": "In this paper, we present a topical segmentation program that achieves a 10% increase in both precision and recall over comparable previous work.", "labels": [], "entities": [{"text": "topical segmentation", "start_pos": 28, "end_pos": 48, "type": "TASK", "confidence": 0.8369320034980774}, {"text": "precision", "start_pos": 94, "end_pos": 103, "type": "METRIC", "confidence": 0.9995760321617126}, {"text": "recall", "start_pos": 108, "end_pos": 114, "type": "METRIC", "confidence": 0.9960179924964905}]}, {"text": "In addition to segmenting, the system also labels the function of discovered discourse ' This material is based upon work supported by the National Science Foundation under Grant No. (NSF #IRI-9618797) and by the Columbia University Center for Research on Information Access.", "labels": [], "entities": []}, {"text": "segments as to their relevance towards the whole.", "labels": [], "entities": []}, {"text": "It identifies 1) segments that contribute some detail towards the main topic of the input, 2) segments that summarize the key points, and 3) segments that contain less important information.", "labels": [], "entities": []}, {"text": "We evaluated our segment classification as part of a summarization system that utilizes highly pertinent segments to extract key sentences.", "labels": [], "entities": [{"text": "segment classification", "start_pos": 17, "end_pos": 39, "type": "TASK", "confidence": 0.7981905043125153}]}, {"text": "We investigated the applicability of this system on general domain news articles.", "labels": [], "entities": []}, {"text": "Generally, we found that longer articles, usually beyond a three-page limit, tended to have their own prior segmentation markings consisting of headers or bullets, so these were excluded.", "labels": [], "entities": []}, {"text": "We thus concentrated our work on a corpus of shorter articles, averaging roughly 800-1500 words in length: 15 from the Wall Street Journal in the Linguistic Data Consortium's 1988 collection, and 5 from the on-line The Economist from 1997.", "labels": [], "entities": [{"text": "Wall Street Journal in the Linguistic Data Consortium's 1988 collection", "start_pos": 119, "end_pos": 190, "type": "DATASET", "confidence": 0.8697371699593284}, {"text": "The Economist from 1997", "start_pos": 215, "end_pos": 238, "type": "DATASET", "confidence": 0.9344770610332489}]}, {"text": "We constructed an evaluation standard from human segmentation judgments to test our output.", "labels": [], "entities": []}], "datasetContent": [{"text": "For the segmentation algorithm we used a webbased segmentation evaluation facility to gather segmentation judgments.", "labels": [], "entities": [{"text": "segmentation", "start_pos": 8, "end_pos": 20, "type": "TASK", "confidence": 0.9798551797866821}]}, {"text": "Each of the 20 articles in the corpus was segmented by at least four human judges, and the majority opinion of segment boundaries was computed as the evaluation standard ().", "labels": [], "entities": []}, {"text": "Human judges achieved on average only 62.4% agreement with the majority opinion, as seen in. show that this surprisingly low agreement is often the result of evaluators being divided between those who regard segments as more localized and those who prefer to split only on large boundaries.", "labels": [], "entities": [{"text": "agreement", "start_pos": 44, "end_pos": 53, "type": "METRIC", "confidence": 0.9693145155906677}]}, {"text": "We then verified that the task was well defined by testing fora strong correlation between the markings of the human judges.", "labels": [], "entities": []}, {"text": "We test for inter-judge reliability using Cochran (1950)'s Qtest, also discussed in.", "labels": [], "entities": [{"text": "reliability", "start_pos": 24, "end_pos": 35, "type": "METRIC", "confidence": 0.9081540703773499}, {"text": "Cochran (1950)'s Qtest", "start_pos": 42, "end_pos": 64, "type": "DATASET", "confidence": 0.5943041245142618}]}, {"text": "We found a very high correlation between judges indicating that modeling the task was indeed feasible; the results showed that there was less than a 0.15% chance on average that the judges' segment marks agreed by chance.", "labels": [], "entities": []}, {"text": "We also calculated Kappa (K), another correlation statistic that corrects for random chance agreement.", "labels": [], "entities": [{"text": "Kappa (K)", "start_pos": 19, "end_pos": 28, "type": "METRIC", "confidence": 0.9583257138729095}]}, {"text": "Kappa values range from -1.0, showing complete negative correlation to +1.0, indicating complete positive correlation.", "labels": [], "entities": []}, {"text": "Surprisingly, the calculations of K showed only a weak level of agreer/~nt between judges (K avg = .331, S.D.= .153).", "labels": [], "entities": [{"text": "agreer", "start_pos": 64, "end_pos": 70, "type": "METRIC", "confidence": 0.9979615211486816}, {"text": "K avg", "start_pos": 91, "end_pos": 96, "type": "METRIC", "confidence": 0.9325258135795593}]}, {"text": "Calculations of the significance of K showed that results were generally significant to the 5% level, indicating that We computed SEGMENTER'S performance by completing the 4-fold cross validation on the test cases.", "labels": [], "entities": [{"text": "K", "start_pos": 36, "end_pos": 37, "type": "METRIC", "confidence": 0.7284514904022217}, {"text": "SEGMENTER'S", "start_pos": 130, "end_pos": 141, "type": "METRIC", "confidence": 0.9829930663108826}]}, {"text": "Examining SEGMENTER'S results show a significant improvement over the initial algorithm of Hearst 1994 (called TEXTTILING), both in precision and recall.", "labels": [], "entities": [{"text": "TEXTTILING", "start_pos": 111, "end_pos": 121, "type": "METRIC", "confidence": 0.9853143692016602}, {"text": "precision", "start_pos": 132, "end_pos": 141, "type": "METRIC", "confidence": 0.9996393918991089}, {"text": "recall", "start_pos": 146, "end_pos": 152, "type": "METRIC", "confidence": 0.9985669255256653}]}, {"text": "A future step could be to compare our segmenting algorithm against other more recent systems (such as.", "labels": [], "entities": []}, {"text": "We present two different baselines to compare the work against.", "labels": [], "entities": []}, {"text": "First, we applied a Monte Carlo simulation that segments at paragraph breaks with a 33% probability.", "labels": [], "entities": []}, {"text": "We executed this baseline I0,000 times on each article and averaged the scores.", "labels": [], "entities": [{"text": "I0,000", "start_pos": 26, "end_pos": 32, "type": "METRIC", "confidence": 0.942008912563324}]}, {"text": "A more informed baseline is produced by applying a hypergeometfic distribution, which calculates the probability of some number of successes by sampling without replacement.", "labels": [], "entities": []}, {"text": "For example, this distribution gives the expected number of red balls drawn from a sample of n balls from an urn containing N total balls, where only rare red.", "labels": [], "entities": []}, {"text": "If we allow the number of segments, r, to be given, we can apply this to segmentation to pick r segments from N paragraphs.", "labels": [], "entities": [{"text": "segmentation", "start_pos": 73, "end_pos": 85, "type": "TASK", "confidence": 0.9650418758392334}]}, {"text": "By comparing the results in, we can see that the correct number of segments (r) is difficult to determine.", "labels": [], "entities": []}, {"text": "TEXTTILING's performance falls below the hypergeomtfic baseline, but on the average, SEGMENTER outperforms it.", "labels": [], "entities": [{"text": "TEXTTILING", "start_pos": 0, "end_pos": 10, "type": "METRIC", "confidence": 0.7241076231002808}, {"text": "SEGMENTER", "start_pos": 85, "end_pos": 94, "type": "METRIC", "confidence": 0.9569244384765625}]}, {"text": "However, notice that the performance of the algorithm and TEX'I'TILING quoted in this paper are low in comparison to reports by others.", "labels": [], "entities": [{"text": "TEX'I'TILING", "start_pos": 58, "end_pos": 70, "type": "METRIC", "confidence": 0.9992250204086304}]}, {"text": "We believe this is due to the weak level of agreement between judges in our training/testing evaluation corpus.", "labels": [], "entities": []}, {"text": "The wide range of performance hints at the variation which segmentation algorithms may experience when faced with different kinds of input.", "labels": [], "entities": []}, {"text": "As mentioned previously, segments and segment type assessments have been integrated into a key sentence extraction program ().", "labels": [], "entities": [{"text": "sentence extraction", "start_pos": 95, "end_pos": 114, "type": "TASK", "confidence": 0.7282132506370544}]}, {"text": "~This ~ummary-directed sentence extraction differs from similar systems in its focus on high recall; further processing of the retrieved sentences would discard unimportant sentences and clauses.", "labels": [], "entities": [{"text": "ummary-directed sentence extraction", "start_pos": 7, "end_pos": 42, "type": "TASK", "confidence": 0.6255079011122385}, {"text": "recall", "start_pos": 93, "end_pos": 99, "type": "METRIC", "confidence": 0.9916261434555054}]}, {"text": "This system used the location of the first sentence of the summary segment as one input feature for deciding key sentences, along with standard features such as title words, TF*IDF weights for the words of a sentence, and the occurrences of communication verbs.", "labels": [], "entities": [{"text": "TF*IDF weights", "start_pos": 174, "end_pos": 188, "type": "METRIC", "confidence": 0.9367060959339142}]}, {"text": "This task-based evaluation of both modules together showed that combining segmentation information yielded markedly better results.", "labels": [], "entities": []}, {"text": "In some instances only segmentation was able to identify certain key sentences; all other features failed to find these sentences.", "labels": [], "entities": []}, {"text": "Overall, a 3.1% improvement in recall was directly achieved by adding segment significance output, increasing the system's recall from 39% to 42%.", "labels": [], "entities": [{"text": "recall", "start_pos": 31, "end_pos": 37, "type": "METRIC", "confidence": 0.9996335506439209}, {"text": "recall", "start_pos": 123, "end_pos": 129, "type": "METRIC", "confidence": 0.9996256828308105}]}, {"text": "Since the system was not built with precision as a priority, so although precision of the system dropped 3%, we believe the overall effects of adding the segmentation information was valuable.", "labels": [], "entities": [{"text": "precision", "start_pos": 36, "end_pos": 45, "type": "METRIC", "confidence": 0.9986767172813416}, {"text": "precision", "start_pos": 73, "end_pos": 82, "type": "METRIC", "confidence": 0.9996198415756226}, {"text": "segmentation", "start_pos": 154, "end_pos": 166, "type": "TASK", "confidence": 0.9667878150939941}]}], "tableCaptions": []}