{"title": [{"text": "Semantic Tagging using a Probabilistic Context Free Grammar *", "labels": [], "entities": [{"text": "Semantic Tagging", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.8557961583137512}]}], "abstractContent": [{"text": "This paper describes a statistical model for extraction of events at the sentence level, or \"semantic tagging\", typically the first level of processing in Information Extraction systems.", "labels": [], "entities": [{"text": "semantic tagging", "start_pos": 93, "end_pos": 109, "type": "TASK", "confidence": 0.7580201923847198}, {"text": "Information Extraction systems", "start_pos": 155, "end_pos": 185, "type": "TASK", "confidence": 0.8039431770642599}]}, {"text": "We illustrate the approach using a management succession task, tagging sentences with three slots involved in each succession event: the post, person coming into the post, and person leaving the post.", "labels": [], "entities": []}, {"text": "The approach requires very limited resources: a part-of-speech tagger; a morphological analyzer; and a set of training examples that have been labeled with the three slots and the indicator (verb or noun) used to express the event.", "labels": [], "entities": []}, {"text": "Training on 560 sentences, and testing on 356 sentences, shows the accuracy of the approach is 77.5% (if partial slot matches are deemed incorrect) or 87.8% (if partial slot matches are deemed correct).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 67, "end_pos": 75, "type": "METRIC", "confidence": 0.9996562004089355}]}], "introductionContent": [{"text": "Statistical models have been used quite successfully in natural language processing for recovery of hidden structure such as part-of-speech tags, or syntactic structure.", "labels": [], "entities": []}, {"text": "This paper considers semantic tagging of text within the context of information extraction, as in the Sixth Message Understanding Conference (MUC-6).", "labels": [], "entities": [{"text": "semantic tagging of text", "start_pos": 21, "end_pos": 45, "type": "TASK", "confidence": 0.8077330961823463}, {"text": "information extraction", "start_pos": 68, "end_pos": 90, "type": "TASK", "confidence": 0.7624631524085999}, {"text": "Sixth Message Understanding Conference (MUC-6)", "start_pos": 102, "end_pos": 148, "type": "DATASET", "confidence": 0.60928013920784}]}, {"text": "MUC-6 looked at extraction of events concerning management successions in newspaper texts: recovering the post, company, person entering and person leaving the post.", "labels": [], "entities": [{"text": "MUC-6", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.8860395550727844}]}, {"text": "We will concentrate on the initial stage of processing, extraction of events at the sentence level.", "labels": [], "entities": []}, {"text": "For example, given the sentence Last week Hensley West, 59 years old, was named as president, a surprising development.", "labels": [], "entities": [{"text": "Hensley West", "start_pos": 42, "end_pos": 54, "type": "DATASET", "confidence": 0.7674109637737274}]}, {"text": "the desired output from the system would be {IN = Hensley West, POST = president, IND = named } POST is a slot designating the title of the position, IN is the person coming in to fill the post, and IND is an \"indicator\" -usually a verb or a noun -used to express the event.", "labels": [], "entities": [{"text": "IN", "start_pos": 45, "end_pos": 47, "type": "METRIC", "confidence": 0.9548351764678955}, {"text": "Hensley West", "start_pos": 50, "end_pos": 62, "type": "DATASET", "confidence": 0.8710101544857025}, {"text": "POST", "start_pos": 96, "end_pos": 100, "type": "METRIC", "confidence": 0.6310969591140747}]}, {"text": "The traditional approach to this problem, as exemplified in SRI's FASTUS system (Appelt et al.), has been \" The work repotted here was supported in part by the Defense Advanced Research Projects Agency.", "labels": [], "entities": [{"text": "FASTUS", "start_pos": 66, "end_pos": 72, "type": "METRIC", "confidence": 0.38073116540908813}]}, {"text": "Technical agents for part of this work were Fort Huachucha under contract number DABT63-94-C-0062.", "labels": [], "entities": [{"text": "DABT63-94-C-0062", "start_pos": 81, "end_pos": 97, "type": "DATASET", "confidence": 0.7155190110206604}]}, {"text": "The views and conclusions contained in this document are those of the authors and should not be interpreted as necessarily representing the official policies, either expressed or implied, of the Defense Advanced Research Projects Agency or the United States Government.", "labels": [], "entities": []}, {"text": "~Fhe unexpected (IND departure) of (OUT Citicorp's highly regarded head of retail banking) and the appointment of a tobacco executive to fill his shoes has caught most Citicorp employees off-guard and con\" founded many analysts.", "labels": [], "entities": [{"text": "Fhe unexpected (IND departure)", "start_pos": 1, "end_pos": 31, "type": "METRIC", "confidence": 0.8298872709274292}]}, {"text": "On Friday, the bank said (OUT Pei-yuan Chia), head of retail banking \u2022 will (IND retire) this year.", "labels": [], "entities": [{"text": "OUT", "start_pos": 26, "end_pos": 29, "type": "METRIC", "confidence": 0.9664583206176758}, {"text": "IND", "start_pos": 77, "end_pos": 80, "type": "METRIC", "confidence": 0.6716971397399902}]}, {"text": "These are typically encoded in a series of finite-state transducers that progressively build information in a bottom-up fashion.", "labels": [], "entities": []}, {"text": "We are interested in developing a machine-learning approach to this problem for two reasons: First, developing hand-coded rules is a lengthy task which requires a fairly considerable amount of expertise -and anew set of rules must be developed for each new domain.", "labels": [], "entities": []}, {"text": "Annotating training text examples such as those in table 1 can conceivably be done by a non-expert.", "labels": [], "entities": []}, {"text": "Second, writing accurate rules is difficult, as there are many complex interactions between the rules, and there are many details to be covered.", "labels": [], "entities": []}, {"text": "This task becomes even more complex when the interaction between the sentence-level rules and the later stages of processing (co-reference and merging, see section 1.1) is considered.", "labels": [], "entities": []}, {"text": "Machine learning techniques have been shown to be highly effective at managing this kind of complexity in applications such as speech recognition, part-ofspeech tagging and parsing.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 127, "end_pos": 145, "type": "TASK", "confidence": 0.8436502814292908}, {"text": "part-ofspeech tagging", "start_pos": 147, "end_pos": 168, "type": "TASK", "confidence": 0.8048568367958069}]}, {"text": "Not surprisingly this problem can be approached using finite-state tagging methods, which have previously been applied to part of speech tagging (Church 88) and named-entity identification ().", "labels": [], "entities": [{"text": "speech tagging", "start_pos": 130, "end_pos": 144, "type": "TASK", "confidence": 0.7082304954528809}, {"text": "named-entity identification", "start_pos": 161, "end_pos": 188, "type": "TASK", "confidence": 0.7207322418689728}]}, {"text": "We initially consider this approach, but argue that the Markov approximation gives an extremely bad parameterization of the problem.", "labels": [], "entities": []}, {"text": "Instead, the method uses a Probabilistic Context Free Grammar (PCFG), which has the advantages of being flexible enough to allow a good parameterization of the problem, while having an efficient decoding algorithm, a variant of the CKY dynamic programming algorithm for parsing with context-free grammars.", "labels": [], "entities": [{"text": "parsing with context-free grammars", "start_pos": 270, "end_pos": 304, "type": "TASK", "confidence": 0.7918675094842911}]}, {"text": "The PCFG does not encode linguistic phrase structure; rather, it is semantically motivated, modeling choices such as the choice Of indicator, number of slots, fillers for the slots, and generation of other \"noise\" words in the sentence.", "labels": [], "entities": []}, {"text": "While this paper largely concentrates on the management succession domain, and motivates many of the choices regarding representation with examples from it, the principles should be general enough to also work for other domains --in fact, the method was originally de: veloped for an IE task involving company acquisitions (identifying the buyer, seller and item being bought), and then moved to the management succession domain with no domain-specific tuning.", "labels": [], "entities": [{"text": "IE task involving company acquisitions", "start_pos": 284, "end_pos": 322, "type": "TASK", "confidence": 0.8031882166862487}]}], "datasetContent": [{"text": "This section describes experiments on the management successions domain.", "labels": [], "entities": [{"text": "management successions domain", "start_pos": 42, "end_pos": 71, "type": "TASK", "confidence": 0.8600974877675375}]}, {"text": "Before giving the results, we discuss how to deal with sentences that have more than one indicator.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 4: Results on 356 test data sentences, training on  563 sentences", "labels": [], "entities": []}, {"text": " Table 5: Results on the jack-knifed training set (Counts)", "labels": [], "entities": []}, {"text": " Table 6: Results on the jack-knifed training set (Percent- ages)", "labels": [], "entities": [{"text": "jack-knifed training set", "start_pos": 25, "end_pos": 49, "type": "DATASET", "confidence": 0.5406319995721182}]}, {"text": " Table 7: The percentage of errors in each error category", "labels": [], "entities": []}]}