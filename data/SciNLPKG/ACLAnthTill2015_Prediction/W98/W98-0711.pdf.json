{"title": [{"text": "Automatic Adaptation of WordNet to Sublanguages and to Computational Tasks", "labels": [], "entities": [{"text": "Automatic Adaptation of WordNet", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.6397004947066307}]}], "abstractContent": [{"text": "Semantically tagging a corpus is useful for many intermediate NLP tasks such as: acquisition of word argument structures in sublanguages, acquisition of syntactic disambiguation cues, terminology learning, etc.", "labels": [], "entities": [{"text": "Semantically tagging a corpus", "start_pos": 0, "end_pos": 29, "type": "TASK", "confidence": 0.819601520895958}, {"text": "acquisition of word argument structures in sublanguages", "start_pos": 81, "end_pos": 136, "type": "TASK", "confidence": 0.8189281650951931}, {"text": "acquisition of syntactic disambiguation cues", "start_pos": 138, "end_pos": 182, "type": "TASK", "confidence": 0.8222593903541565}, {"text": "terminology learning", "start_pos": 184, "end_pos": 204, "type": "TASK", "confidence": 0.9757151305675507}]}, {"text": "Semantic categories allow the generalization of observed word patterns , and facilitate the discovery of irecurrent sublanguage phenomena and selectional rules of various types.", "labels": [], "entities": []}, {"text": "Yet, as opposed to POS tags in morphology, there is no consensus in literature about the type and granularity of the category inventory.", "labels": [], "entities": []}, {"text": "In addition, most available on-line taxonomies, as WordNet, are over ambiguous and, at the same time, may not include many domain-dependent senses of words.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 51, "end_pos": 58, "type": "DATASET", "confidence": 0.9742576479911804}]}, {"text": "In this paper we describe a method to adapt a general purpose taxonomy to an application sub[an-guage: flint, we prune branches of the Wordnet hierarchy that are too \" fine grained\" for the domain: then.", "labels": [], "entities": [{"text": "Wordnet hierarchy", "start_pos": 135, "end_pos": 152, "type": "DATASET", "confidence": 0.9294979870319366}]}, {"text": "a statistical model of classes is built from corpus contexts to sort the different classifications or assign a classification to known and unknown words, respectively.", "labels": [], "entities": []}], "introductionContent": [{"text": "Lexical learning methods based on the use of semantic categories are faced with the problem of overambiguity and entangled structures of Thesaura and dictionaries.", "labels": [], "entities": []}, {"text": "WordNet and Roget's Thesaura were not initially conceived, despite their success among researchers in lexical statistics, as tools for automatic language processing.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.9679383635520935}, {"text": "automatic language processing", "start_pos": 135, "end_pos": 164, "type": "TASK", "confidence": 0.6114829281965891}]}, {"text": "The purpose was rather to provide the linguists with a very refined, general purpose, linguistically motivated source of taxonomic knowledge.", "labels": [], "entities": []}, {"text": "As a consequence, inmost on-fine Thesaura words are extremely ambiguous. with very subtle distinctions among senses.", "labels": [], "entities": []}, {"text": "High ambiguity, entangled nodes, and asymmetry have already been emphasized in as being an obstacle to the effective use of on-line Thesaura in corpus linguistics.", "labels": [], "entities": []}, {"text": "In most cases, the noise introduced by overambiguity almost overrides the positive effect of semantic clustering.", "labels": [], "entities": [{"text": "semantic clustering", "start_pos": 93, "end_pos": 112, "type": "TASK", "confidence": 0.7354837357997894}]}, {"text": "For example, in clustering PP heads according to WordNet synsets produced only a [% improvement in a PP disambiguation task. with respect to the non-clustered method.", "labels": [], "entities": []}, {"text": "A subsequent paper reports of a 40% precision in a sense disambiguation task.", "labels": [], "entities": [{"text": "precision", "start_pos": 36, "end_pos": 45, "type": "METRIC", "confidence": 0.9985175728797913}, {"text": "sense disambiguation", "start_pos": 51, "end_pos": 71, "type": "TASK", "confidence": 0.5466432571411133}]}, {"text": "always based on generalization through WordNet synsets.", "labels": [], "entities": []}, {"text": "Context-based sense clisambiguation becomes a prohibitive task on a wide-scale basis, because when words in the context of unambiguous word are replaced by their s.vnsets, there is a multiplication of possible contexts, rather than a generalization.", "labels": [], "entities": [{"text": "Context-based sense clisambiguation", "start_pos": 0, "end_pos": 35, "type": "TASK", "confidence": 0.6941905816396078}]}, {"text": "[n () a method called Conceptual Distance is proposed to reduce this problem, but the reported performance in disambiguation still does not reach 50%.", "labels": [], "entities": []}, {"text": "On the other hand, and claim that fine-grained semantic distinctions are unlikely to be of practical value for many applications.", "labels": [], "entities": []}, {"text": "Our experience supports this claim: often, what matters is to be able to distinguish among contrastive (Pustejowsky.", "labels": [], "entities": []}, {"text": "1995) ambiguities of the bank_river bank_organisation flavor.", "labels": [], "entities": []}, {"text": "The problem however is that the notion of\"coutrastive\" is domain-dependent.", "labels": [], "entities": []}, {"text": "Depending upon the sublanguage (e.g. medicine, finance, computers. etc.) and upon the specific NLP application (e.g. Information Extraction, Dialogue etc.) a given semantic label maybe too general or too specific for the task at hand.", "labels": [], "entities": [{"text": "Information Extraction, Dialogue", "start_pos": 117, "end_pos": 149, "type": "TASK", "confidence": 0.7809231579303741}]}, {"text": "For example, the word line has 27 senses in WordNet.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 44, "end_pos": 51, "type": "DATASET", "confidence": 0.9743022322654724}]}, {"text": "many of which draw subtle distinctions e.g. line of ~cork (sense 26) and line of products (sense.", "labels": [], "entities": []}, {"text": "In application aimed at extracting information on new products in an economic domain, we would be interested in identi~-ing occurrences of such senses, but perhaps all the other senses could be clustered in one or two categories, lbr example Artifact, grouping senses such as: telephoneline, railway and cable, and Abstraction, grouping senses such as series, conformity and indication.", "labels": [], "entities": [{"text": "Abstraction", "start_pos": 315, "end_pos": 326, "type": "METRIC", "confidence": 0.9871124625205994}]}, {"text": "Vice versa, if the sublanguage is technical handbooks in computer science, we would like to distinguish the cable and the string of words senses (7 and 5, respectively), while any other distinction may not have any practical interest.", "labels": [], "entities": []}, {"text": "The research described in this paper is aimed at providing some principled, and algorithmic, methods to tune a general purpose taxonomy to specific sublanguages and domains.", "labels": [], "entities": []}, {"text": "In this paper, we propose a method by which we select a set of core semantic nodes in the WordNet taxonomy that \"optimally\" describe the semantics of a sublanguage, according to a scoring function defined as a linear combination of general and corpus-dependent performance factors.", "labels": [], "entities": [{"text": "WordNet taxonomy", "start_pos": 90, "end_pos": 106, "type": "DATASET", "confidence": 0.9491452276706696}]}, {"text": "The selected categories are used to prune WordNet branches that appear, according to our scoring function, less pertinent to the given sublanguage, thus reducing the initial ambiguity.", "labels": [], "entities": []}, {"text": "Then, we learn from the application corpus a statistical model of the core categories and use this model to further tune the initial taxonomy.", "labels": [], "entities": []}, {"text": "Tuning implies two actions: The first is to attempt a reclassification of relevant word:; in the corpus that are not covered bv the selected categories, i.e..", "labels": [], "entities": []}, {"text": "words belonging exclusively to pruned branches.", "labels": [], "entities": []}, {"text": ":hese words have domaindependent .,;enses that are not captured in the initial WordNet classification (,e.g. the software sense of release in a software handbooks sublanguage).", "labels": [], "entities": []}, {"text": "The decision to assign an unclassified word to one of the selected categories is based on a strong detected similarity between the contexts in which the word o.:curs, and the statistical model of the core categories.", "labels": [], "entities": []}, {"text": "The second iis to further reduce the ambiguitv of words that :;till have a high ambiguity, with respect to the other word.s in the corpus.", "labels": [], "entities": []}, {"text": "For example, the word stock in a financial domain still preserved the gunstock 81 sense, because instrumentality was one of the selected core categories for the domain.", "labels": [], "entities": []}, {"text": "The expectation of this sense ,nay be lowered, as before, by comparing the typical contexts of stock with the acquired model of instrumentality.", "labels": [], "entities": []}, {"text": "In the next sections, we first describe the algorithm for selecting core categories.", "labels": [], "entities": []}, {"text": "Then, we describe the method for redistributing relevant words among the nodes of the pruned hierarchy.", "labels": [], "entities": []}, {"text": "Finally, we discuss an evaluation experiment.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": []}