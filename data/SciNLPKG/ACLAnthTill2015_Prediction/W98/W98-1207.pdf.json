{"title": [], "abstractContent": [], "introductionContent": [{"text": "The emergence of new statistical NLP methods increases the demand for corpora annotated with syntactic structures.", "labels": [], "entities": []}, {"text": "The construction of such a corpus (a treebank) is a time-consuming task that can hardly be carried out unless some annotation work is automated.", "labels": [], "entities": []}, {"text": "Purely automatic annotation, however, is not reliable enough to be employed without some form of human supervision and hand-correction.", "labels": [], "entities": []}, {"text": "This interactive annotation strategy requires tools for error detection and consistency checking.", "labels": [], "entities": [{"text": "error detection", "start_pos": 56, "end_pos": 71, "type": "TASK", "confidence": 0.6561503410339355}, {"text": "consistency checking", "start_pos": 76, "end_pos": 96, "type": "TASK", "confidence": 0.7585543692111969}]}, {"text": "The present paper reviews our experience with the development of automatic annotation tools which are currently used for building a corpus of German newspaper text.", "labels": [], "entities": []}, {"text": "The next section gives an overview of the annotation format.", "labels": [], "entities": []}, {"text": "Section 3 describes three applications of statistical NLP methods to treebank annotation.", "labels": [], "entities": []}, {"text": "Finally, section 4 discusses mechanisms for comparing structures assigned by different annotators.", "labels": [], "entities": []}], "datasetContent": [{"text": "This section reports on the accuracy achieved by the methods described in the previous sections.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 28, "end_pos": 36, "type": "METRIC", "confidence": 0.9993698000907898}]}, {"text": "At present, our corpus contains approx. 6300 sentences (115,000 tokens) of German newspaper text (Frankfurter Rundschan).", "labels": [], "entities": [{"text": "Frankfurter Rundschan)", "start_pos": 98, "end_pos": 120, "type": "DATASET", "confidence": 0.9081395467122396}]}, {"text": "Results of tagging grammatical functions and phrase categories have improved slightly compared to those reported fora smaller corpus of approx. 1200 sentences.", "labels": [], "entities": [{"text": "tagging grammatical functions", "start_pos": 11, "end_pos": 40, "type": "TASK", "confidence": 0.8848287463188171}]}, {"text": "Accuracy figures for tagging the hierarchical structure are published for the first time.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 0, "end_pos": 8, "type": "METRIC", "confidence": 0.9571196436882019}]}, {"text": "For each experiment, the corpus was divided into two disjoint parts: 90% training data and 10% test data.", "labels": [], "entities": []}, {"text": "This procedure was repeated ten times, and the results were averaged.", "labels": [], "entities": []}, {"text": "The thresholds 01 and 02 determining the reliability levels were set to 91 = 5 and 02 = 100.", "labels": [], "entities": [{"text": "reliability", "start_pos": 41, "end_pos": 52, "type": "METRIC", "confidence": 0.9849947094917297}]}], "tableCaptions": [{"text": " Table 1: Levels of reliability and the percentage of  cases in which the tagger assigned a correct gram- matical function (or would have assigned ifa decision  had been forced).", "labels": [], "entities": [{"text": "reliability", "start_pos": 20, "end_pos": 31, "type": "METRIC", "confidence": 0.9758815169334412}]}, {"text": " Table 2: Levels of reliability and the percentage of  cases in which the tagger assigned a correct phrase  category (or would have assigned it if a decision had  been forced).", "labels": [], "entities": [{"text": "reliability", "start_pos": 20, "end_pos": 31, "type": "METRIC", "confidence": 0.9469869136810303}]}, {"text": " Table 3: Chunk tagger accuracy with respect to hi- erarchical structure.", "labels": [], "entities": [{"text": "Chunk tagger", "start_pos": 10, "end_pos": 22, "type": "TASK", "confidence": 0.712570995092392}, {"text": "accuracy", "start_pos": 23, "end_pos": 31, "type": "METRIC", "confidence": 0.9867475032806396}]}, {"text": " Table 4: Comparison of independent semi-automatic  annotations (1) after first, independent annotation  and (2) after comparison but before the final discus- sion (current stage).", "labels": [], "entities": []}, {"text": " Table 5: Using model perplexities to compare dif- ferent annotations: Accuracy of using the hypothe- sis that a correct annotation has a lower perplexity  than a wrong annotation.", "labels": [], "entities": []}]}