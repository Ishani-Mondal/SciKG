{"title": [{"text": "THE PRACTICAL VALUE OF N'GRAN IS IN GENERATION", "labels": [], "entities": [{"text": "VALUE", "start_pos": 14, "end_pos": 19, "type": "METRIC", "confidence": 0.6505305767059326}]}], "abstractContent": [{"text": "We examine the practical s~'nergy between symbolic and statistical language processing in a generator called Nitrogen.", "labels": [], "entities": []}, {"text": "The analysis provides insight into the kinds of linguistic decisions that bigram frequency statistics can make, and how it improves scalability..", "labels": [], "entities": []}, {"text": "We also discuss the limits of bigram statistical knowledge.", "labels": [], "entities": []}, {"text": "We focus on specific examples of Nitrogen's output.", "labels": [], "entities": []}], "introductionContent": [{"text": "Langkilde and introduced Nitrogen, a system that implements anew style of generation in which corpus-based ngram statistics are used in place of deep, extensive symbolic knowledge to provide Very large-scale generation (lexicons and knowledge bases on the order of 200,000 entities), and simultaneously simplify the input and improve robustness for sentence generation.", "labels": [], "entities": [{"text": "sentence generation", "start_pos": 349, "end_pos": 368, "type": "TASK", "confidence": 0.7353705167770386}]}, {"text": "Nitrogen's generation occurs in two stages, as shown in.", "labels": [], "entities": [{"text": "Nitrogen's generation", "start_pos": 0, "end_pos": 21, "type": "TASK", "confidence": 0.7127378384272257}]}, {"text": "First the input is mapped tO a word lattice, a compact representation of multiple generation possibilities.", "labels": [], "entities": []}, {"text": "Then, a statistical extractor selects the most fluent path through the lattice.", "labels": [], "entities": []}, {"text": "The word lattice encodes alternative English expressions for the input when the symbolic knowledge is unavailable (whether from the input, or from the knowledge bases) for making realization decisions.", "labels": [], "entities": []}, {"text": "The Nitrogen statistical extractor ranks these alternative s using bigram (adjacent word pairs) and unigram (single word) statistics Collected from two years of the Wall Street Journal.", "labels": [], "entities": [{"text": "Wall Street Journal", "start_pos": 165, "end_pos": 184, "type": "DATASET", "confidence": 0.9414072434107462}]}, {"text": "The extraction algorithm is presented in..", "labels": [], "entities": []}, {"text": "In essence, Nitrogen uses ngram statistics to robustly make a wide variety of decisions, from tense to word choice\u2022 to syntactic subcategorization, that traditionally are handled either with defaults (e.g., assume present tense, use the alphabetically-first synonyms, use nominal arguments), explicit input specification, or by using deep, detailed knowledge bases.", "labels": [], "entities": []}, {"text": "However, in scaling up a generator system, these methods become unsatisfactory.", "labels": [], "entities": []}, {"text": "Defaults are too rigid and limit quality; detailed input specs are difficult or complex to construct, or m~' be unavailable; and : : 248 smaller scale tend to be brittle.", "labels": [], "entities": []}, {"text": "This paper examines the synergy between symbolic and statistical language processing and discusses Nitrogen's performance in practice.", "labels": [], "entities": [{"text": "statistical language processing", "start_pos": 53, "end_pos": 84, "type": "TASK", "confidence": 0.6256281236807505}]}, {"text": "The analysis provides insight into the kinds of linguistic decisions that bigram frequency statistics can make, and how it improves sealability.", "labels": [], "entities": []}, {"text": "It also discusses the limits of bigram statistical knowledge.", "labels": [], "entities": []}, {"text": "It is organized around specific examples of Nitrogen's output.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": []}