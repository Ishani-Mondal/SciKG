{"title": [{"text": "An Empirical Approach to Conceptual Case Frame Acquisition", "labels": [], "entities": [{"text": "Conceptual Case Frame Acquisition", "start_pos": 25, "end_pos": 58, "type": "TASK", "confidence": 0.6306482553482056}]}], "abstractContent": [{"text": "Conceptual natural language processing systems usually rely on case frame instantiation to recognize events and role objects in text.", "labels": [], "entities": []}, {"text": "But generating a good set of case frames fora domain is time-consuming, tedious, and prone to errors of omission.", "labels": [], "entities": []}, {"text": "We have developed a corpus-based algorithm for acquiring conceptual case frames empirically from unannotated text.", "labels": [], "entities": []}, {"text": "Our algorithm builds on previous research on corpus-based methods for acquiring extraction patterns and semantic lexicons.", "labels": [], "entities": [{"text": "acquiring extraction patterns and semantic lexicons", "start_pos": 70, "end_pos": 121, "type": "TASK", "confidence": 0.8219147523244222}]}, {"text": "Given extraction patterns and a semantic lexicon fora domain , our algorithm learns semantic preferences for each extraction pattern and merges the syntactically compatible patterns to produce multi-slot case frames with selectional restrictions.", "labels": [], "entities": []}, {"text": "The case frames generate more cohesive output and produce fewer false hits than the original extraction patterns.", "labels": [], "entities": []}, {"text": "Our system requires only preclassified training texts and a few hours of manual review to filter the dictionaries , demonstrating that conceptual case frames can be acquired from unannotated text without special training resources.", "labels": [], "entities": []}], "introductionContent": [], "datasetContent": [{"text": "The purpose of the selectional restrictions is to constrain the types of information that can be instantiated by each slot.", "labels": [], "entities": []}, {"text": "Consequently, we hoped that the case frames would be more reliably instantiated than the extraction patterns, thereby producing fewer false hits.", "labels": [], "entities": []}, {"text": "To evaluate the case frames, we used the same corpus and evaluation metrics as previous experiments with AutoSlog and AutoSlog-TS () so that we can draw comparisons between them.", "labels": [], "entities": []}, {"text": "For training, we used the 1500 MUC-4 development texts to generate the extraction patterns and the semantic lexicon.", "labels": [], "entities": [{"text": "MUC-4 development texts", "start_pos": 31, "end_pos": 54, "type": "DATASET", "confidence": 0.7976237932840983}]}, {"text": "AutoSlog-TS generated 44,013 extraction patterns in its first pass.", "labels": [], "entities": [{"text": "AutoSlog-TS", "start_pos": 0, "end_pos": 11, "type": "DATASET", "confidence": 0.8436464071273804}]}, {"text": "After discarding the patterns that occurred only once, the remaining 11,517 patterns were applied to the corpus for the second pass and ranked for manual review.", "labels": [], "entities": []}, {"text": "We reviewed the top 2168 patterns 5 and kept 306 extraction patterns for the final dictionary.", "labels": [], "entities": []}, {"text": "We built a semantic lexicon for nine categories associated with terrorism: BUILDING, CIVILIAN, GOV-OFFICIAL, MILITARYPEOPLE, LOCATION, TERROR-IST~ DATEs VEHICLE, WEAPON.", "labels": [], "entities": [{"text": "BUILDING", "start_pos": 75, "end_pos": 83, "type": "METRIC", "confidence": 0.9926815629005432}, {"text": "GOV-OFFICIAL", "start_pos": 95, "end_pos": 107, "type": "METRIC", "confidence": 0.5692941546440125}, {"text": "MILITARYPEOPLE", "start_pos": 109, "end_pos": 123, "type": "METRIC", "confidence": 0.9236210584640503}, {"text": "LOCATION", "start_pos": 125, "end_pos": 133, "type": "METRIC", "confidence": 0.9459088444709778}, {"text": "TERROR-IST", "start_pos": 135, "end_pos": 145, "type": "METRIC", "confidence": 0.7720257639884949}, {"text": "VEHICLE", "start_pos": 153, "end_pos": 160, "type": "METRIC", "confidence": 0.6825658679008484}]}, {"text": "We reviewed the top 500 words for each category.", "labels": [], "entities": []}, {"text": "It takes about 30 m~nutes to review a category assuming that the reviewer is familiar with the domain.", "labels": [], "entities": []}, {"text": "Our final semantic dictionary contained 494 words.", "labels": [], "entities": []}, {"text": "In total, the review process required approximately 6 person-hours: 1.5 hours to review the extraction patterns plus 4.5 hours to review the words for 9 semantic categories.", "labels": [], "entities": []}, {"text": "From the extraction patterns and semantic lexicon, our system generated 137 conceptual case frames.", "labels": [], "entities": []}, {"text": "One important question is how to deal with unknown words during extraction.", "labels": [], "entities": []}, {"text": "This is especially important in the terrorism domain because many of the extracted items are proper names, which cannot be expected to be in the semantic lexicon.", "labels": [], "entities": []}, {"text": "We allowed unknown words to fill all eligible slots and then used a precedence scheme so that each item was instantiated by only one slot.", "labels": [], "entities": []}, {"text": "Precedence was based on the order of the roles shown in.", "labels": [], "entities": [{"text": "Precedence", "start_pos": 0, "end_pos": 10, "type": "METRIC", "confidence": 0.9276487231254578}]}, {"text": "This is not a very satisfying solution and one of the weaknesses of our current approach.", "labels": [], "entities": []}, {"text": "Handling unknown words more intelligently is an important direction for future research.", "labels": [], "entities": []}, {"text": "We compared AutoSlog-TS' extraction patterns SWe decided to review the top 2000 but continued clown the list until there were no more ties. with an item in the answer keys.", "labels": [], "entities": []}, {"text": "Correct items extracted more than once were scored as duplicates, as well as correct but underspecified extractions such as \"Kennedy\" instead of \"John F. Kennedy\" r An item was spurious if it did not appear in the answer keys.", "labels": [], "entities": []}, {"text": "All items extracted from irrelevant texts were spurious.", "labels": [], "entities": []}, {"text": "Finally, items in the answer keys that were not extracted were counted as missing.", "labels": [], "entities": []}, {"text": "Correct + missing equals the total number of items in the answer keys.S shows the results 9 for AutoSlog-TS' extraction patterns, and shows the results for the case frames.", "labels": [], "entities": []}, {"text": "We computed Recall (R) as correct / (correct + missing), and Precision (P) as (correct + duplicate) / (correct + duplicate + mislabeled + spurious).", "labels": [], "entities": [{"text": "Precision (P)", "start_pos": 61, "end_pos": 74, "type": "METRIC", "confidence": 0.9467548578977585}]}, {"text": "The extraction patterns and case frames achieved similar recall results, although the case frames missed seven correct extractions.", "labels": [], "entities": [{"text": "recall", "start_pos": 57, "end_pos": 63, "type": "METRIC", "confidence": 0.9992232322692871}]}, {"text": "However the case frames produced substantially fewer false hits, producing 82 fewer spurious extractions.", "labels": [], "entities": []}, {"text": "Note that perpetrators exhibited by far the lowest precision.", "labels": [], "entities": [{"text": "precision", "start_pos": 51, "end_pos": 60, "type": "METRIC", "confidence": 0.9996498823165894}]}, {"text": "The reason is that the perpetrator slot received highest precedence among competing slots for unknown words.", "labels": [], "entities": []}, {"text": "Changing the precedence s25 relevant texts and 25 irrelevant texts from each of the TST3 and TST4 test sets.", "labels": [], "entities": [{"text": "TST3 and TST4 test sets", "start_pos": 84, "end_pos": 107, "type": "DATASET", "confidence": 0.7739434003829956}]}, {"text": "7The rationale for scoring coreferent phrases as duplicates instead of spurious is that the extraction pattern or case frame was instantiated with a reference to the correct answer.", "labels": [], "entities": []}, {"text": "In other words, the pattern (or case frame) did the right thing.", "labels": [], "entities": []}, {"text": "Resolving coreferent phrases to produce the best answer is a problem for subsequent discourse analysis, which is not addressed by the work presented here.", "labels": [], "entities": [{"text": "discourse analysis", "start_pos": 84, "end_pos": 102, "type": "TASK", "confidence": 0.7118166983127594}]}, {"text": "SA caveat is that the MUC-4 answer keys contain some \"optional\" answers.", "labels": [], "entities": [{"text": "MUC-4", "start_pos": 22, "end_pos": 27, "type": "DATASET", "confidence": 0.7290597558021545}]}, {"text": "We scored these as correct if they were extracted but they were never scored as missing, which is how the \"optional\" items were scored in MUC-4.", "labels": [], "entities": [{"text": "MUC-4", "start_pos": 138, "end_pos": 143, "type": "DATASET", "confidence": 0.8809216618537903}]}, {"text": "Note that the number of possible extractions can vary depending on the output of the system.", "labels": [], "entities": []}, {"text": "9We reimplemented AutoSlog-TS to use a different sentence analyzer, so these results are slightly different from those reported in (: Case frame results scheme produces a bubble effect where many incorrect extractions shift to the primary default category.", "labels": [], "entities": [{"text": "9We", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.9541419744491577}]}, {"text": "The case frames therefore have the potential for even higher precision if the unknown words are handled better.", "labels": [], "entities": [{"text": "precision", "start_pos": 61, "end_pos": 70, "type": "METRIC", "confidence": 0.9983684420585632}]}, {"text": "Expanding the semantic lexicon is one option, and additional work may suggest ways to choose slots for unknown words more intelligently.", "labels": [], "entities": []}], "tableCaptions": []}