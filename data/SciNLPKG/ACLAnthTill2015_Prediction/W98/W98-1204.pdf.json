{"title": [{"text": "A Lexically-Intensive Algorithm for Domain-Specific Knowlegde Acquisition", "labels": [], "entities": []}], "abstractContent": [{"text": "This paper is an outline of a statistical learning algorithm for information extraction systems.", "labels": [], "entities": [{"text": "information extraction", "start_pos": 65, "end_pos": 87, "type": "TASK", "confidence": 0.859984278678894}]}, {"text": "It is based on a lexicaUy intensive analysis of a small number of texts that belong to one domain and provides a robust lemma-tisation of the word forms and the collection of the most important syntagmatic dependencies in weighted regular expressions.", "labels": [], "entities": []}, {"text": "The lexical and syntactical knowledge is collected in a very compact knowledge base that enables the analysis of correct and partly incorrect texts or messages, which due to transmission errors , spelling or grammatical mistakes otherwise would have been rejected by conventional systems.", "labels": [], "entities": []}], "introductionContent": [{"text": "The major tasks of information extraction systems (IE-Systems) are the unsupervised selection, fast analysis and efficient storage of relevant text patterns a person or a group of persons is interested in.", "labels": [], "entities": [{"text": "information extraction", "start_pos": 19, "end_pos": 41, "type": "TASK", "confidence": 0.7305721342563629}]}, {"text": "It accomplishes this through the use of learned or handcrafted patterns.", "labels": [], "entities": []}, {"text": "In the ideal case the results lead to an appropriate reaction, executed by the computer itself (see).", "labels": [], "entities": []}, {"text": "The extracted information is stored in a template that usually is based on a slot-and-filler model.", "labels": [], "entities": []}, {"text": "Whenever the textual information does not fit automatically into the fillers, it has to be changed adequately to the form and content requirements of the template, otherwise the text is rejected.", "labels": [], "entities": []}, {"text": "Thus, the templates architecture depends very much on the domain the IE--system was built for, i.e. before processing a text or a message and starting the linguistic analysis, the category that the text or message belong to is already This study is part of the project READ.", "labels": [], "entities": []}, {"text": "The project READ is funded by the German Ministry for Education and Research (BMBF) under grant 01IN503C.", "labels": [], "entities": [{"text": "READ", "start_pos": 12, "end_pos": 16, "type": "TASK", "confidence": 0.8490182161331177}]}, {"text": "The author is responsible for the contents of the publication.", "labels": [], "entities": []}, {"text": "known or has been labeled automatically with the aid of a categorizer.", "labels": [], "entities": []}, {"text": "In our investigation the system was built to process requests for business reports, extracting the number, years and language of business reports a certain sender asked for.", "labels": [], "entities": []}, {"text": "\u2022. i~  Although a lot of sophisticate investigation has been done in the area of information extraction (Pazienza, 1997) (esp.", "labels": [], "entities": [{"text": "information extraction", "start_pos": 81, "end_pos": 103, "type": "TASK", "confidence": 0.8352949619293213}]}, {"text": "since the start of the MUC-Conferences in 1987), only few works are concerned with the automatic acquisition of the knowledge bases that are needed for IE-tasks, which makes the construction of anew system fora different extraction task still very expensive and says much about the brittleness of \"traditional\" IE-systems.", "labels": [], "entities": [{"text": "MUC-Conferences", "start_pos": 23, "end_pos": 38, "type": "DATASET", "confidence": 0.9057689309120178}, {"text": "IE-tasks", "start_pos": 152, "end_pos": 160, "type": "TASK", "confidence": 0.9483332633972168}]}, {"text": "The problem gets worse when the information that has to be extracted is paperbound and has to be digitized by scanners to make the information available to the computer, because Optical Character Recognition (= OCR) still garbles a consLderable amount of information reduction and noise on texts, so that there is also a need for more robust information extraction systems that handle noisy information adequately.", "labels": [], "entities": [{"text": "Optical Character Recognition", "start_pos": 178, "end_pos": 207, "type": "TASK", "confidence": 0.5862043797969818}, {"text": "information extraction", "start_pos": 342, "end_pos": 364, "type": "TASK", "confidence": 0.7441096603870392}]}, {"text": "The work presented in this paper reflects a statistical approach for the automatic acquisition of a linguistic knowledge base, that allows the essential analysis for texts of a certain domain, independent of their transmission quality or pre-processing.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Unordered Lexicon Entry: rbport", "labels": [], "entities": [{"text": "rbport", "start_pos": 35, "end_pos": 41, "type": "DATASET", "confidence": 0.8218745589256287}]}, {"text": " Table 2: Core Lexicon Entry: report", "labels": [], "entities": []}]}