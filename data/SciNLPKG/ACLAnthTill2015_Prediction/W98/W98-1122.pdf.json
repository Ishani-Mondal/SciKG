{"title": [{"text": "Automatic Acquisition of Phrase Grammars for Stochastic Language Modeling", "labels": [], "entities": [{"text": "Stochastic Language Modeling", "start_pos": 45, "end_pos": 73, "type": "TASK", "confidence": 0.6411669353644053}]}], "abstractContent": [{"text": "Phrase-based language models have been recognized to have an advantage over word-based language models since they allow us to capture long spanning dependencies.", "labels": [], "entities": []}, {"text": "Class based language models have been used to improve model generalization and overcome problems with data sparseness.", "labels": [], "entities": []}, {"text": "In this paper , we present a novel approach for combining the phrase acquisition with class construction process to automatically acquire phrase-grammar fragments from a given corpus.", "labels": [], "entities": [{"text": "phrase acquisition", "start_pos": 62, "end_pos": 80, "type": "TASK", "confidence": 0.7889669835567474}, {"text": "class construction", "start_pos": 86, "end_pos": 104, "type": "TASK", "confidence": 0.7322220206260681}]}, {"text": "The phrase-grammar learning is decomposed into two sub-problems, namely the phrase acquisition and feature selection.", "labels": [], "entities": [{"text": "phrase acquisition", "start_pos": 76, "end_pos": 94, "type": "TASK", "confidence": 0.8132132291793823}, {"text": "feature selection", "start_pos": 99, "end_pos": 116, "type": "TASK", "confidence": 0.6948162317276001}]}, {"text": "The phrase acquisition is based on entropy minimization and the feature selection is driven by the entropy reduction principle.", "labels": [], "entities": [{"text": "phrase acquisition", "start_pos": 4, "end_pos": 22, "type": "TASK", "confidence": 0.7882696688175201}]}, {"text": "We further demonstrate that the phrase-grammar based n-gram language model significantly outperforms a phrase-based n-gram language model in an end-to-end evaluation of a spoken language application.", "labels": [], "entities": []}, {"text": "1 Introduction Traditionally, n-gram language models implicitly assume words as the basic lexical unit.", "labels": [], "entities": []}, {"text": "However, certain word sequences (phrases) are recurrent in constrained domain languages and can bethought as a single lexical entry (e.g. by and large, I would like to, United States of America, etc..).", "labels": [], "entities": []}, {"text": "A traditional word n-gram based language model can benefit greatly by using variable length units to capture long spanning dependencies, for any given order n of the model.", "labels": [], "entities": []}, {"text": "Furthermore, language mod-eling based on longer length units is applicable to languages which do not have a predefined notion of a word.", "labels": [], "entities": []}, {"text": "However, the problem of data sparseness is more acute in phrase-based language models than in word-based language models.", "labels": [], "entities": []}, {"text": "Clustering words into classes has been used to overcome data sparseness in word-based language models (et.al..", "labels": [], "entities": []}, {"text": "Although the automatically acquired phrases can be later clustered into classes to overcome data sparseness, we present a novel approach 188 of combining the construction of classes during the acquisition of phrases.", "labels": [], "entities": []}, {"text": "This integration of phrase acquisition and class construction results in the acquisition of phrase-grammar fragments.", "labels": [], "entities": [{"text": "phrase acquisition", "start_pos": 20, "end_pos": 38, "type": "TASK", "confidence": 0.7880796194076538}, {"text": "class construction", "start_pos": 43, "end_pos": 61, "type": "TASK", "confidence": 0.737212672829628}]}, {"text": "In (Gorin, 1996; Arai et al., 1997), grammar fragment acquisition is performed through Kullback-Liebler divergence techniques with application to topic classification from text.", "labels": [], "entities": [{"text": "grammar fragment acquisition", "start_pos": 37, "end_pos": 65, "type": "TASK", "confidence": 0.7218674222628275}, {"text": "topic classification from text", "start_pos": 146, "end_pos": 176, "type": "TASK", "confidence": 0.8086743950843811}]}, {"text": "Although phrase-grammar fragments reduce the problem of data sparseness, they can result in over-generalization.", "labels": [], "entities": []}, {"text": "For example, one of the classes induced in our experiments was C1 = {and, but, because} which one might call the class of conjunctions.", "labels": [], "entities": []}, {"text": "However, this class was part of a phrase-grammar fragment such as A T C1 T which results in phrases A T and T, A T but T, A T because T-a clear case of over-generalization given our corpus.", "labels": [], "entities": []}, {"text": "Hence we need to further stochastically separate phrases generated by a phrase-grammar fragment.", "labels": [], "entities": []}, {"text": "In this paper, we present our approach to integrating phrase acquisition and clustering and our technique to specialize the acquired phrase fragments.", "labels": [], "entities": [{"text": "phrase acquisition", "start_pos": 54, "end_pos": 72, "type": "TASK", "confidence": 0.8284069001674652}]}, {"text": "We extensively evaluate the effectiveness of phrase-grammar based n-gram language model and demonstrate that it outperforms a phrase-based n-gram language model in an end-to-end evaluation of a spoken language application.", "labels": [], "entities": []}, {"text": "The outline of the paper is as follows.", "labels": [], "entities": []}, {"text": "In Section 2, we review the phrase acquisition algorithm presented in (Riccardi et al., 1997).", "labels": [], "entities": [{"text": "phrase acquisition", "start_pos": 28, "end_pos": 46, "type": "TASK", "confidence": 0.8818367421627045}]}, {"text": "In Section 3, we discuss our approach to phrase acquisition and clustering respectively.", "labels": [], "entities": [{"text": "phrase acquisition", "start_pos": 41, "end_pos": 59, "type": "TASK", "confidence": 0.8634179532527924}]}, {"text": "The algorithm integrating the phrase acquisition and clustering processes is presented in Section 4.", "labels": [], "entities": [{"text": "phrase acquisition", "start_pos": 30, "end_pos": 48, "type": "TASK", "confidence": 0.8224001228809357}]}, {"text": "The spoken language application for automatic call routing (How May I Help You?", "labels": [], "entities": [{"text": "call routing", "start_pos": 46, "end_pos": 58, "type": "TASK", "confidence": 0.7410644739866257}]}, {"text": "(HMIHY)) that is used for evaluating our approach and the results of our experiments are described in Section 5. 2 Learning Phrases In previous work, we have shown the effectiveness of incorporating manually selected phrases for re", "labels": [], "entities": []}], "introductionContent": [{"text": "Traditionally, n-gram language models implicitly assume words as the basic lexical unit.", "labels": [], "entities": []}, {"text": "However, certain word sequences (phrases) are recurrent in constrained domain languages and can bethought as a single lexical entry (e.g. by and large, I would like to, United States of America, etc..).", "labels": [], "entities": []}, {"text": "A traditional word n-gram based language model can benefit greatly by using variable length units to capture long spanning dependencies, for any given order n of the model.", "labels": [], "entities": []}, {"text": "Furthermore, language modeling based on longer length units is applicable to languages which do not have a predefined notion of a word.", "labels": [], "entities": []}, {"text": "However, the problem of data sparseness is more acute in phrase-based language models than in word-based language models.", "labels": [], "entities": []}, {"text": "Clustering words into classes has been used to overcome data sparseness in word-based language models.", "labels": [], "entities": []}, {"text": "Although the automatically acquired phrases can be later clustered into classes to overcome data sparseness, we present a novel approach 188 of combining the construction of classes during the acquisition of phrases.", "labels": [], "entities": []}, {"text": "This integration of phrase acquisition and class construction results in the acquisition of phrase-grammar fragments.", "labels": [], "entities": [{"text": "phrase acquisition", "start_pos": 20, "end_pos": 38, "type": "TASK", "confidence": 0.7880796194076538}, {"text": "class construction", "start_pos": 43, "end_pos": 61, "type": "TASK", "confidence": 0.737212672829628}]}, {"text": "In), grammar fragment acquisition is performed through Kullback-Liebler divergence techniques with application to topic classification from text.", "labels": [], "entities": [{"text": "grammar fragment acquisition", "start_pos": 5, "end_pos": 33, "type": "TASK", "confidence": 0.7064753770828247}, {"text": "topic classification from text", "start_pos": 114, "end_pos": 144, "type": "TASK", "confidence": 0.7992418557405472}]}, {"text": "Although phrase-grammar fragments reduce the problem of data sparseness, they can result in overgeneralization.", "labels": [], "entities": []}, {"text": "For example, one of the classes induced in our experiments was C1 = {and, but, because} which one might call the class of conjunctions.", "labels": [], "entities": []}, {"text": "However, this class was part of a phrasegrammar fragment such as A T C1 T which results in phrases A T and T, A T but T, A T because T -a clear case of over-generalization given our corpus.", "labels": [], "entities": []}, {"text": "Hence we need to further stochastically separate phrases generated by a phrase-grammar fragment.", "labels": [], "entities": []}, {"text": "In this paper, we present our approach to integrating phrase acquisition and clustering and our technique to specialize the acquired phrase fragments.", "labels": [], "entities": [{"text": "phrase acquisition", "start_pos": 54, "end_pos": 72, "type": "TASK", "confidence": 0.8284069001674652}]}, {"text": "We extensively evaluate the effectiveness of phrasegrammar based n-gram language model and demonstrate that it outperforms a phrase-based n-gram language model in an end-to-end evaluation of a spoken language application.", "labels": [], "entities": []}, {"text": "The outline of the paper is as follows.", "labels": [], "entities": []}, {"text": "In Section 2, we review the phrase acquisition algorithm presented in ( ).", "labels": [], "entities": [{"text": "phrase acquisition", "start_pos": 28, "end_pos": 46, "type": "TASK", "confidence": 0.8796084225177765}]}, {"text": "In Section 3, we discuss our approach to phrase acquisition and clustering respectively.", "labels": [], "entities": [{"text": "phrase acquisition", "start_pos": 41, "end_pos": 59, "type": "TASK", "confidence": 0.8634179532527924}]}, {"text": "The algorithm integrating the phrase acquisition and clustering processes is presented in Section 4.", "labels": [], "entities": [{"text": "phrase acquisition", "start_pos": 30, "end_pos": 48, "type": "TASK", "confidence": 0.8224001228809357}]}, {"text": "The spoken language application for automatic call routing (How May I Help You?", "labels": [], "entities": [{"text": "call routing", "start_pos": 46, "end_pos": 58, "type": "TASK", "confidence": 0.7410644739866257}]}, {"text": "(HMIHY)) that is used for evaluating our approach and the results of our experiments are described in Section 5.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: The results of clustering words from tile How May I Help You ? corpus", "labels": [], "entities": [{"text": "clustering words from tile How May I Help You ?", "start_pos": 25, "end_pos": 72, "type": "TASK", "confidence": 0.6241904467344284}]}, {"text": " Table 2: The results of the first iteration of combining phrase acquistion and clustering from tile HMIHY  corpus. (Words in a phrase are separated by a \":\". Tile members of Ci's are shown in Table 1)", "labels": [], "entities": []}]}