{"title": [{"text": "Do Not Forget: Full Memory in Memory-Based Learning of Word Pronunciation *", "labels": [], "entities": []}], "abstractContent": [{"text": "Memory-based learning, keeping full memory ofleaxning material, appeaxs a viable approach to learning N-~ tasks, and is often superior in genera~sation accuracy to eager learning approaches that abstract from learning materiaL Here we investigate three pa~'tial memory-based learning approaches which remove from memory specific task instance types estimated to be exceptional.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 152, "end_pos": 160, "type": "METRIC", "confidence": 0.6121037006378174}]}, {"text": "The three approaches each implement one heuristic function for estimating exceptiona]ity of instance types: (i) typi-catty, (ii) class prediction strength, and (fii) friencfly-neighbourhood size.", "labels": [], "entities": [{"text": "class prediction", "start_pos": 129, "end_pos": 145, "type": "TASK", "confidence": 0.6596977263689041}]}, {"text": "Experiments are performed with the memory-based learning algorithm IBI-IG trained on English word pro-nunciatlon.", "labels": [], "entities": []}, {"text": "We find that removing instance types with low prediction strength (il) is the only tested method which does not seriously harm generallsation accuracy.", "labels": [], "entities": [{"text": "prediction strength (il)", "start_pos": 46, "end_pos": 70, "type": "METRIC", "confidence": 0.819709312915802}, {"text": "accuracy", "start_pos": 142, "end_pos": 150, "type": "METRIC", "confidence": 0.9455999135971069}]}, {"text": "We conclude that keeping full memory of types rather than tokens, and excluding minority ambiguities appear to be the only performance-preserving op-timi~tions of memory-based leaxning.", "labels": [], "entities": []}], "introductionContent": [{"text": "Memory-based learning of classification tasks is a branch of supervised machine learning in which the learning phase consists simply of storing all encountered instances from a training set in memory.", "labels": [], "entities": [{"text": "Memory-based learning of classification tasks", "start_pos": 0, "end_pos": 45, "type": "TASK", "confidence": 0.7283049046993255}]}, {"text": "Memory-based learning algorithms do not invest effort during learning in abstracting from the tr-lnlng data, such as eager-learning (e.g., decision-tree algorithms, rule-induction, or connectionist-learning algorithms,) do.", "labels": [], "entities": []}, {"text": "Rather, they defer investing effort until new instances axe presented.", "labels": [], "entities": []}, {"text": "On being presented with an instance, a memory-based *This research was done in the context of the \"Induction of Linguistic Knowledge\" research programme, partially supported by the Foundation for Language Speech and Logic (TSL), which is funded by the Netherlands Organization for Scientific Research (NWO).", "labels": [], "entities": [{"text": "Language Speech and Logic (TSL)", "start_pos": 196, "end_pos": 227, "type": "TASK", "confidence": 0.7131467972482953}]}, {"text": "Part of the first author's work was performed at the Department of Computer Science of the Unlversiteit Maastricht.", "labels": [], "entities": []}, {"text": "learning algorithm searches fora best-matching instance, or, more generically, a set of the k bestmatching instances in memory.", "labels": [], "entities": []}, {"text": "Having found such a set of h best-matching instances, the algorithm takes the (majority) class with which the instances in the set axe labeled to be the class of the new instance.", "labels": [], "entities": []}, {"text": "Pure memory-based learning algorithms implement the classic k-nearest neighbour algorithm; in different contexts, memory-based learning algorithms have also been named lazy, instance-based, exemplarbased, memory-based, case-based learning or reasoning () Memory-based learning has been demonstrated to yield accurate models of various natural language tasks such as grapheme-phoneme conversion, word stress assignment, part-of-speech tagging, and PP-attachment (Daelemans, Van den Bosch, and).", "labels": [], "entities": [{"text": "grapheme-phoneme conversion", "start_pos": 366, "end_pos": 393, "type": "TASK", "confidence": 0.7627115547657013}, {"text": "word stress assignment", "start_pos": 395, "end_pos": 417, "type": "TASK", "confidence": 0.6793629924456278}, {"text": "part-of-speech tagging", "start_pos": 419, "end_pos": 441, "type": "TASK", "confidence": 0.7240607440471649}]}, {"text": "Thus, when learning NLP tasks, the abstraction oeeurnng in decision trees (i.e., the explicit forgetting of information considered to be redundant) and in conneetionist networks (i.e., a non-symbolic encoding and decoding in relatively small numbers of connection van den weights) both hamper accurate generalisation of the learned knowledge to new material.", "labels": [], "entities": []}, {"text": "These findings appear to contrast with the general assumption behind eager learning, that data representing real-world classification tasks tends to contains (i) redundancy and (ii) exceptions: redundant data can be compressed, yielding smaller descriptions of the original data; some exceptions (e.g., lowfrequency exceptions) can (or should) be discarded since they are expected to be bad predictors for classhying new (test) material.", "labels": [], "entities": []}, {"text": "However, both redundancy and exeeptionality cannot be computed trivially; heuristic functions are generally used to estimate them (e.g., functions from ixLformation theory).", "labels": [], "entities": []}, {"text": "The lower generalization accuracies of both decision-tree and eonnectionist learning, compared to memory-based learning, on the abovementioned NLP tasks, suggest that these heuristic estimates may not be the best choice for learning NLP tasks.", "labels": [], "entities": []}, {"text": "It appears that in order to learn such tasks successfully, a learning algorithm should not forget (i.e., explicitly remove from memory) any information contained in the learning material: it should not abstract from the individual instances.", "labels": [], "entities": []}, {"text": "An obvious type of abstraction that is not harmful for generalisation accuracy (but that is not always acknowledged in implementations of memorybased learning) is the straightforward abstraction from tokens to types with frequency information.", "labels": [], "entities": [{"text": "generalisation", "start_pos": 55, "end_pos": 69, "type": "TASK", "confidence": 0.951602041721344}, {"text": "accuracy", "start_pos": 70, "end_pos": 78, "type": "METRIC", "confidence": 0.666995644569397}]}, {"text": "In general, data sets representing natural language tasks, when large enough, tend to contain considerable numbers of duplicate sequences mapping to the same output or class.", "labels": [], "entities": []}, {"text": "For example, in data representing word pronunciations, some sequences of letters, such as ing at the end of English words, occur hundreds of times, while each of the sequences is pronounced identically, viz.", "labels": [], "entities": []}, {"text": "Instead of storing all individual sequence tokens in memory, each set of identical tokens can be safely stored in memory as a single sequence type with frequency information, without loss of generalisation accuracy).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 206, "end_pos": 214, "type": "METRIC", "confidence": 0.9739298820495605}]}, {"text": "Thus, exceptionality should in someway express the unsuitability of a task instance type to be a best match (nearest neighbour) to new instances: it would be unwise to copy its associated classification to best-matching new instances.", "labels": [], "entities": []}, {"text": "In this paper, we investigate three criteria for estimating an instance type's exceptionality, and removing instance types estimated to be the most exceptional by each of these criteria.", "labels": [], "entities": []}, {"text": "The criteria investigated are 1.", "labels": [], "entities": []}, {"text": "typicality of instance types; 2.", "labels": [], "entities": []}, {"text": "class prediction strength of instance types; 3.", "labels": [], "entities": [{"text": "class prediction", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.715421125292778}]}, {"text": "fi-iendly-neighbourhood size of instance types; 4.", "labels": [], "entities": []}, {"text": "random (to provide a baseline experiment).", "labels": [], "entities": []}, {"text": "We base our experiments on a large data set of English word pronunciation.", "labels": [], "entities": [{"text": "English word pronunciation", "start_pos": 47, "end_pos": 73, "type": "TASK", "confidence": 0.5765918493270874}]}, {"text": "We briefly describe this data set, and the way it is converted into an instance base fit for memotT-based learning, in Section 2.", "labels": [], "entities": []}, {"text": "In Section 3 we describe the settings of our experiments and the memory-based learning algorithm IBI-Io with which the experiments are performed.", "labels": [], "entities": []}, {"text": "We then turn to describing the notions of typicality, class-prediction strength, and friendlyneighbourhood size, and the functions to estimate them, in Section 4.", "labels": [], "entities": []}, {"text": "Section 5 provides the experimental results.", "labels": [], "entities": []}, {"text": "In Section 6, we discuss the obtained results and formulate our conclusions.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 4: Four examples of nearest-neighbour rankings and their respective numbers of friendly neighbours  (fa). Each ranked nearest neighbour is identified by its match (o) or mismatch (\u00d7) with the target instance  the ranking is computed for, and a number denoting its distance to the target instance.", "labels": [], "entities": []}]}