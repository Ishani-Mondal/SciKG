{"title": [{"text": "II / Look-Back and Look-Ahead in the Conversion of Hidden Markov Models into Finite State Transducers", "labels": [], "entities": []}], "abstractContent": [{"text": "This paper describes the conversion of a Hidden Markov Model into a finite state transducer that closely approximates the behavior of the stochastic model.", "labels": [], "entities": []}, {"text": "In some cases the transducer is equivalent to the HMM.", "labels": [], "entities": []}, {"text": "This conversion is especially advantageous for part-of-speech tagging because the resulting transducer can be composed with other transducers that encode correction rules for the most frequent tagging errors.", "labels": [], "entities": [{"text": "part-of-speech tagging", "start_pos": 47, "end_pos": 69, "type": "TASK", "confidence": 0.717825174331665}]}, {"text": "The speed of tagging is also improved.", "labels": [], "entities": [{"text": "speed", "start_pos": 4, "end_pos": 9, "type": "METRIC", "confidence": 0.9876168370246887}, {"text": "tagging", "start_pos": 13, "end_pos": 20, "type": "TASK", "confidence": 0.9778043627738953}]}, {"text": "The described methods have been implemented and successfully tested.", "labels": [], "entities": []}], "introductionContent": [{"text": "This paper presents an algorithm 1 which approximates a Hidden Markov Model (HMM) by a finitestate transducer (FST).", "labels": [], "entities": []}, {"text": "We describe one application, namely part-of-speech tagging.", "labels": [], "entities": [{"text": "part-of-speech tagging", "start_pos": 36, "end_pos": 58, "type": "TASK", "confidence": 0.8099386692047119}]}, {"text": "Other potential applications maybe found in areas where both HMMs and finite-state technology are applied, such as speech recognition, etc.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 115, "end_pos": 133, "type": "TASK", "confidence": 0.855408638715744}]}, {"text": "The algorithm has been fully implemented.", "labels": [], "entities": []}, {"text": "An HMM used for tagging encodes, like a transducer, a relation between two languages.", "labels": [], "entities": []}, {"text": "One language contains sequences of ambiguity classes obtained by looking up in a lexicon all words of a sentence.", "labels": [], "entities": []}, {"text": "The other language contains sequences of tags obtained by statistically disambiguating the class sequences.", "labels": [], "entities": []}, {"text": "From the outside, an HMM tagger behaves like a sequential transducer that deterministically maps every class sequence to a tag sequence, e.g.: aThere are other (dillerent) algorithms for HMM to FST conversion: An unpublished one by Julian M. Kupiec and John T. Maxwell (p.c.), and n-type and stype approximation by.", "labels": [], "entities": [{"text": "HMM to FST conversion", "start_pos": 187, "end_pos": 208, "type": "TASK", "confidence": 0.7300728335976601}]}, {"text": "The main advantage of transforming an HMM is that the resulting transducer can be handled by finite state calculus.", "labels": [], "entities": []}, {"text": "Among others, it can be composed with transducers that encode: \u2022 correction rules for the most frequent tagging errors which are automatically generated or manually written, in order to significantly improve tagging accuracy -9 . These rules may include long-distance dependencies not handled by ttMM taggers, and can conveniently be expressed by the replace operator ().", "labels": [], "entities": [{"text": "correction", "start_pos": 65, "end_pos": 75, "type": "METRIC", "confidence": 0.9934068918228149}, {"text": "accuracy", "start_pos": 216, "end_pos": 224, "type": "METRIC", "confidence": 0.9313246607780457}]}, {"text": "\u2022 further steps of text analysis, e.g. light parsing or extraction of noun phrases or other phrases).", "labels": [], "entities": [{"text": "text analysis", "start_pos": 19, "end_pos": 32, "type": "TASK", "confidence": 0.7580346465110779}, {"text": "light parsing or extraction of noun phrases or other phrases", "start_pos": 39, "end_pos": 99, "type": "TASK", "confidence": 0.8414310663938522}]}, {"text": "These compositions enable complex text analysis to be performed by a single transducer.", "labels": [], "entities": [{"text": "text analysis", "start_pos": 34, "end_pos": 47, "type": "TASK", "confidence": 0.7299272418022156}]}, {"text": "The speed of tagging by an FST is up to six times higher than with the original HMM.", "labels": [], "entities": [{"text": "tagging", "start_pos": 13, "end_pos": 20, "type": "TASK", "confidence": 0.9644606709480286}, {"text": "FST", "start_pos": 27, "end_pos": 30, "type": "TASK", "confidence": 0.6136558055877686}]}, {"text": "The motivation for deriving the FST from an HMM is that the tIMM can be trained and converted with little manual effort.", "labels": [], "entities": [{"text": "FST", "start_pos": 32, "end_pos": 35, "type": "TASK", "confidence": 0.7522631287574768}]}, {"text": "An HMM transducer builds on the data (probability matrices) of the underlying HMM.", "labels": [], "entities": []}, {"text": "The accuracy of this data has an impact on the tagging accuracy of both the HMM itself and the derived transducer.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.9978381991386414}, {"text": "tagging", "start_pos": 47, "end_pos": 54, "type": "TASK", "confidence": 0.9544971585273743}, {"text": "accuracy", "start_pos": 55, "end_pos": 63, "type": "METRIC", "confidence": 0.9454326033592224}]}, {"text": "The training of the HMM can be done on either a tagged or untagged corpus, and is not a topic of this paper since it is exhaustively described in the literature ().", "labels": [], "entities": [{"text": "HMM", "start_pos": 20, "end_pos": 23, "type": "TASK", "confidence": 0.8676037192344666}]}, {"text": "An HMM can be identically represented by a weighted FST in a straightforward way.", "labels": [], "entities": []}, {"text": "We are, however, interested in non-weighted transducers.", "labels": [], "entities": []}], "datasetContent": [{"text": "This section compares different FSTs with each other and with the original ttMM.", "labels": [], "entities": [{"text": "FSTs", "start_pos": 32, "end_pos": 36, "type": "TASK", "confidence": 0.7568549513816833}]}, {"text": "As expected, the FSTs perform tagging faster than the HMM.", "labels": [], "entities": [{"text": "tagging", "start_pos": 30, "end_pos": 37, "type": "TASK", "confidence": 0.9739830493927002}]}, {"text": "Since all FSTs are approximations of HMMs, they show lower tagging accuracy than the ttMMs.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 67, "end_pos": 75, "type": "METRIC", "confidence": 0.9681563377380371}]}, {"text": "In the case of FSTs with fl > 1 and a = 1, this difference inaccuracy is negligible.", "labels": [], "entities": [{"text": "FSTs", "start_pos": 15, "end_pos": 19, "type": "TASK", "confidence": 0.722736120223999}]}, {"text": "Improvement inaccuracy can be expected since these FSTs can be composed with FSTs encoding correction rules for frequent errors (sec. 1).", "labels": [], "entities": [{"text": "FSTs", "start_pos": 51, "end_pos": 55, "type": "TASK", "confidence": 0.7243346571922302}]}, {"text": "For all tests below an English corpus, lexicon and guesser were used, which were originally annotated with 74 different tags.", "labels": [], "entities": [{"text": "English corpus", "start_pos": 23, "end_pos": 37, "type": "DATASET", "confidence": 0.8012510538101196}]}, {"text": "We automatically recoded the tags in order to reduce their number, i.e. in some cases more than one of the original tags were recoded into one and the same new tag.", "labels": [], "entities": []}, {"text": "We applied different recodings, thus obtaining English corpora, lexicons and guessers with reduced tag sets of 45, 36, 27, 18 and 9 tags respectively.", "labels": [], "entities": []}, {"text": "FSTs with fl= 2 and ~ = 1 and with fl= 1 and a = 2 were equivalent, in all cases where they could be computed.", "labels": [], "entities": []}, {"text": "compares different FSTs fora tag set of 36 tags.", "labels": [], "entities": []}, {"text": "The b-type FST with no look-back and no lookahead which is equivalent to an n0-type FST, shows the lowest tagging accuracy (b-FST ()3=0, a=0): 87.21%).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 114, "end_pos": 122, "type": "METRIC", "confidence": 0.964210569858551}]}, {"text": "It is also the smallest transducer (1 state and 181 arcs, as many as tag classes) and can be created faster than the other FSTs (6 sec.).", "labels": [], "entities": [{"text": "FSTs", "start_pos": 123, "end_pos": 127, "type": "DATASET", "confidence": 0.7153914570808411}]}, {"text": "The highest accuracy is obtained with a b-type FST with fl= 2 and a = 1 (b-FST (/3=2,~=1): 97.34 %) and with an s-type FST ( trained on 1 000 000 words (s+nl-FST (1M, F1): 97.33 %).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 12, "end_pos": 20, "type": "METRIC", "confidence": 0.999136745929718}]}, {"text": "In these two cases the difference inaccuracy with respect to the underlying ttMM (97.35 %) is negligible.", "labels": [], "entities": []}, {"text": "In this particular test, the s-type FST comes out ahead because it is considerably smaller than the b-type FST.", "labels": [], "entities": []}, {"text": "The size of a b-type FST increases with the size of the tag set and with the length of look-back plus look-ahead, ~+c~.", "labels": [], "entities": [{"text": "FST", "start_pos": 21, "end_pos": 24, "type": "TASK", "confidence": 0.8542755842208862}, {"text": "length", "start_pos": 77, "end_pos": 83, "type": "METRIC", "confidence": 0.9691392779350281}]}, {"text": "Accuracy improves with growing b-Type FSTs may produce ambiguous tagging resuits (sec. 2.4)'.", "labels": [], "entities": []}, {"text": "In such instances only the first result was retained (see. 3).", "labels": [], "entities": []}, {"text": "shows the tagging accuracy and the agreement of the tagging results with the results of the underlying HMM for different FSTs and tag sets of different sizes.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 18, "end_pos": 26, "type": "METRIC", "confidence": 0.9717338681221008}, {"text": "agreement", "start_pos": 35, "end_pos": 44, "type": "METRIC", "confidence": 0.9809853434562683}]}, {"text": "To get results that are almost equivalent to those of an HMM, a b-type FST needs at least a look-back of/5 = 2 and a look-ahead of a = 1 or vice versa.", "labels": [], "entities": [{"text": "FST", "start_pos": 71, "end_pos": 74, "type": "TASK", "confidence": 0.8476004004478455}]}, {"text": "For reasons of size, this kind of FST could only be computed for tag sets with 36 tags or less.", "labels": [], "entities": [{"text": "FST", "start_pos": 34, "end_pos": 37, "type": "TASK", "confidence": 0.8078034520149231}]}, {"text": "A b-type FST with/5 = 3 and a = 1 could only be computed for the tag set with 9 tags.", "labels": [], "entities": [{"text": "FST", "start_pos": 9, "end_pos": 12, "type": "TASK", "confidence": 0.6519116163253784}]}, {"text": "This FST gave exactly the same tagging results as the underlying HMM.", "labels": [], "entities": [{"text": "FST", "start_pos": 5, "end_pos": 8, "type": "TASK", "confidence": 0.6730902194976807}]}, {"text": "illustrates which of the b-type FSTs are sequential, i.e. always produce exactly one tagging result, and which of the FSTs are non-sequential.", "labels": [], "entities": []}, {"text": "For all tag sets, the FSTs with no look-back (/5 = 0) and/or no look-ahead (a = 0) behaved sequentially.", "labels": [], "entities": []}, {"text": "Here 100 % of the tagged sentences had only one result.", "labels": [], "entities": []}, {"text": "Most of the other FSTs (/5. o~ > 0) behaved non-sequentially.", "labels": [], "entities": [{"text": "FSTs", "start_pos": 18, "end_pos": 22, "type": "TASK", "confidence": 0.5477127432823181}]}, {"text": "For example, in the case of 27 tags withl3=l anda=l, 90.08%of the tagged sentences had one result, 9.46 % had two results, 0.23 % had tree results, etc.", "labels": [], "entities": []}, {"text": "Non-sequentiality decreases with growing lookback and look-ahead,/5+c~, and should completely disappear with sufficiently large/5+~.", "labels": [], "entities": []}, {"text": "Such b-type FSTs can, however, only be computed for small tag sets.", "labels": [], "entities": [{"text": "FSTs", "start_pos": 12, "end_pos": 16, "type": "TASK", "confidence": 0.9173583984375}]}, {"text": "We could compute this kind of FST only for the case of 9 tags with/5=3 and a=l.", "labels": [], "entities": [{"text": "FST", "start_pos": 30, "end_pos": 33, "type": "TASK", "confidence": 0.9012991189956665}]}, {"text": "The set of alternative tag sequences fora sentence, produced by a b-type FST with/5, a > 0, always contains the tag sequence that corresponds with the result of the underlying HMM.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Accuracy, speed, size and creation time of some HMM transducers", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9971737861633301}]}, {"text": " Table 2: Tagging accuracy and agreement of the FST tagging results with those  of the underlying HMM, for tag sets of different sizes", "labels": [], "entities": [{"text": "Tagging", "start_pos": 10, "end_pos": 17, "type": "TASK", "confidence": 0.9370790719985962}, {"text": "accuracy", "start_pos": 18, "end_pos": 26, "type": "METRIC", "confidence": 0.9860954880714417}, {"text": "agreement", "start_pos": 31, "end_pos": 40, "type": "METRIC", "confidence": 0.9790398478507996}, {"text": "FST tagging", "start_pos": 48, "end_pos": 59, "type": "TASK", "confidence": 0.7725655734539032}]}, {"text": " Table 3: Percentage of sentences with a par- ticular number of tagging results", "labels": [], "entities": []}]}