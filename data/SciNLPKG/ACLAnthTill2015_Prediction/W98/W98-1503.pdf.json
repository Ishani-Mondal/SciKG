{"title": [], "abstractContent": [{"text": "This paper describes a method for the automatic alignment of parallel texts at clause level.", "labels": [], "entities": [{"text": "automatic alignment of parallel texts", "start_pos": 38, "end_pos": 75, "type": "TASK", "confidence": 0.7436622023582459}]}, {"text": "The method features statistical techniques coupled with shallow linguistic processing.", "labels": [], "entities": []}, {"text": "It presupposes a parallel bilingual corpus and identifies alignments between the clauses of the source and target language sides of the corpus.", "labels": [], "entities": []}, {"text": "Parallel texts are first statistically aligned at sentence level and then tagged with their part-of-speech categories.", "labels": [], "entities": []}, {"text": "Regular grammars functioning on tags, recognize clauses on both sides of the parallel text.", "labels": [], "entities": []}, {"text": "A probabilistic model is applied next, operating on the basis of word occurrence and co-occurrence probabilities and character lengths.", "labels": [], "entities": []}, {"text": "Depending on sentence size, possible alignments arc fed into a dynamic progranuning framework or a simulated annealing system in order to find or approxim~te the best alignment.", "labels": [], "entities": []}, {"text": "1he method has been tested on a Small Eng~ lish-Greek corpus consisting of texts relevant to software systems and has produced promising results in terms of correctly identified clause alignments.", "labels": [], "entities": [{"text": "clause alignments", "start_pos": 178, "end_pos": 195, "type": "TASK", "confidence": 0.7193576395511627}]}], "introductionContent": [{"text": "The availability of large collections of texts in electronic fom1, has given rise to a wide range of applications aim~ ing at the elicitation of linguistic resources such as tTanslation dictionaries, transfer grammars and retTieval of translation examples, or even the building of fully-blown machine translation systems ().", "labels": [], "entities": []}, {"text": "The pmpose of this paper is to describe a technique for extracting translation correspondences at bellow sentence level by employing statistical techniques coupled with shallow linguistic processing catering for the segmentation of sentences into clauses.", "labels": [], "entities": [{"text": "pmpose", "start_pos": 4, "end_pos": 10, "type": "METRIC", "confidence": 0.9657720327377319}, {"text": "extracting translation correspondences", "start_pos": 56, "end_pos": 94, "type": "TASK", "confidence": 0.8745630582173666}, {"text": "segmentation of sentences into clauses", "start_pos": 216, "end_pos": 254, "type": "TASK", "confidence": 0.8191081166267395}]}, {"text": "Statistical processing has proved powerful for the exh\u00b7action of translation equivalences at sentence and intra-sentence level.", "labels": [], "entities": []}, {"text": "described a method based on the number of words contained in sentences.", "labels": [], "entities": []}, {"text": "The general idea is that the closer in length two sentences are, the most likely they are to align.", "labels": [], "entities": []}, {"text": "Moreover, certain anchor points and paragraph markers are considered.", "labels": [], "entities": []}, {"text": "Dynamic progra111111ing and HMMs are pipelined to produce alignments at sentence level.", "labels": [], "entities": []}, {"text": "The method has been applied to the Hansard-Corpus, achieving an accuracy of 96%-97%.", "labels": [], "entities": [{"text": "Hansard-Corpus", "start_pos": 35, "end_pos": 49, "type": "DATASET", "confidence": 0.9480648040771484}, {"text": "accuracy", "start_pos": 64, "end_pos": 72, "type": "METRIC", "confidence": 0.9993581175804138}]}, {"text": "proposed a method that relies on a simple statistical model of character lengths.", "labels": [], "entities": []}, {"text": "The model is based on the observation that longer sentences in one language tend to be translated into longer sentences in the other language while shorter ones tend to be translated into shorter ones.", "labels": [], "entities": []}, {"text": "A probabilistic score is assigned to each pair of proposed sentence pairs, and a dynamic programming framework calculates the most probable alignment.", "labels": [], "entities": []}, {"text": "Although the apparent efficacy of the Gale-Church algorithm is undeniable and validated on different pairs of languages, rt faces problems when handling complex alignments(l-0, 1-2, 2-2).", "labels": [], "entities": []}, {"text": "' argue that a small amount of linguistic information is necessary in order to overcome the inherited weaknesses of the purely statistical techniques.", "labels": [], "entities": []}, {"text": "They proposed using cognates, which are pairs of tokens of different languages sharing 11 0bvioUS phonological or orthographic and semantic properties, since these are likely to be used as mutual translations.", "labels": [], "entities": []}, {"text": "proposed a generic alignment scheme invoking surface linguistic information coupled with information about possible unit delimiters depending on the level at which alignment is sought.", "labels": [], "entities": []}, {"text": "Each unit, sentence, clause or phrase, is represented by the sum of its content part of speech (POS) tags.", "labels": [], "entities": []}, {"text": "The results are then fed into a dynamic programming framework that computes the optimum alignment of text units.", "labels": [], "entities": []}, {"text": "uses a probabilistic measure to estimate word similarity of two languages in the context of statistically-based machine translation.", "labels": [], "entities": [{"text": "statistically-based machine translation", "start_pos": 92, "end_pos": 131, "type": "TASK", "confidence": 0.672202338774999}]}, {"text": "present an algorithm for aligning bilingual texts on the basis of internal evidence.", "labels": [], "entities": []}, {"text": "Processing is pcrfom1ed in many iterations and each new iteration uses the results of the previous one in order to calculate more accurate word and sentence correspondences.", "labels": [], "entities": []}, {"text": "In each iteration, processing consists of calculating correspondences between sentences on the basis of their relative positions, and then calculating word correspondences on the basis of word co-occunences in related sentences.", "labels": [], "entities": []}, {"text": "The Dice coefficient is used as the similarity measure between words of two languages in an attempt to secure the conectness of the alignment of parallel texts at sentence level.", "labels": [], "entities": [{"text": "Dice coefficient", "start_pos": 4, "end_pos": 20, "type": "METRIC", "confidence": 0.8357614576816559}]}, {"text": "have used the same Dice coefficient to calculate the word similarity between Japanese-English parallel corpora.", "labels": [], "entities": [{"text": "Dice coefficient", "start_pos": 19, "end_pos": 35, "type": "METRIC", "confidence": 0.9335842132568359}]}, {"text": "Single word correspondences have also been investigated by) using a statistical evaluation of contingency tables. and describe methods for extracting single and multi-word equivalences based on a parallel corpus statistically aligned at sentence level and employing a similarity metric along the lines of the Dice coefficient with comparable performance.", "labels": [], "entities": [{"text": "Single word correspondences", "start_pos": 0, "end_pos": 27, "type": "TASK", "confidence": 0.6825069586435953}]}, {"text": "Collocational conespondences have been studied by and, in an attempt to find h\u00b7anslation patterns for continuous and discontinuous collocations in English and French.", "labels": [], "entities": []}, {"text": "Meaningful collocations are first extracted in the source language while their corresponding French ones are found by calculating the mutual information between instances of the English collocation and various single word candidates in English-French aligned corpora.", "labels": [], "entities": []}, {"text": "Recent work has broadened the scope identifying correspondences between word sequences.", "labels": [], "entities": []}, {"text": "proposes a method for extracting translation patterns of noun phrases from English-French parallel corpora.", "labels": [], "entities": [{"text": "extracting translation patterns of noun phrases from English-French parallel corpora", "start_pos": 22, "end_pos": 106, "type": "TASK", "confidence": 0.8949991524219513}]}, {"text": "The corpus is tagged at partof~spcech (POS) level and then finite-state recognizers specified by regular expressions defined in tenns of POS categories detect noun phrases on either side.", "labels": [], "entities": []}, {"text": "Probabilities of correspondences are then calculated using an iterative EM-like algorithm.", "labels": [], "entities": []}, {"text": "presuppose an ordinary bilingual dictimmy and non-parallel corpora, attempting to find bilingual conespondences in a Japanese-English setting at word, noun phrase and unknown word level.", "labels": [], "entities": []}, {"text": "Extending previous work, apply the Dice coefficient on word sequence correspondence extraction.", "labels": [], "entities": [{"text": "word sequence correspondence extraction", "start_pos": 55, "end_pos": 94, "type": "TASK", "confidence": 0.713621512055397}]}, {"text": "This paper describes a method for the automatic alignment of parallel texts at clause level.", "labels": [], "entities": [{"text": "automatic alignment of parallel texts", "start_pos": 38, "end_pos": 75, "type": "TASK", "confidence": 0.7436622023582459}]}, {"text": "Texts are first aligned at sentence level using statistical techniques.", "labels": [], "entities": []}, {"text": "Part-of-speech tagging takes place next annotating each word form with the appropriate part of speech.", "labels": [], "entities": [{"text": "Part-of-speech tagging", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.7860570251941681}]}, {"text": "Processing in this step and the next one is monolingual, so each language side of the text is treated independently of the other.", "labels": [], "entities": []}, {"text": "Surface syntactic analysis is performed next on the basis of regular grammars.", "labels": [], "entities": [{"text": "Surface syntactic analysis", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.7820039987564087}]}, {"text": "Shallow parsing results in the recognition of clauses.", "labels": [], "entities": [{"text": "Shallow parsing", "start_pos": 0, "end_pos": 15, "type": "TASK", "confidence": 0.5673178434371948}]}, {"text": "Statistical processing follows taking into account different sources of information, aiming at identifying intra-sentence alignments formed by the clauses of the parallel sentences of the bitext.", "labels": [], "entities": []}, {"text": "The 18 method caters for alignments of type 1-0, 1-1, 1-2, 2-1, and 2-2.", "labels": [], "entities": []}, {"text": "A first pass through the text computes occurrence and co-occunence probabilities for content words on both language sides.", "labels": [], "entities": []}, {"text": "A probabilistic score, expressing the probability that a clause (or a pair of clauses) of the source language is h\u00b7anslated into a clause (or a pair of clauses) of the target language, is computed on the basis of the previously calculated word probabilities, and a model of character lengths.", "labels": [], "entities": []}, {"text": "Possible clause alignments are examined by a dynamic programming framework deciding on the best alignment.", "labels": [], "entities": []}, {"text": "Avoiding combinatorial explosion requires that large sentences be channeled into a module that approximates the optimal alignment through simulated amrealing, operating in polynomial time.", "labels": [], "entities": []}, {"text": "EM iterative training caters for the estimation of the model's parameters, given the lack of hand-aligned training material.", "labels": [], "entities": []}, {"text": "The overview of the processing is pictured in.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": []}