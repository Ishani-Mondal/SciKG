{"title": [{"text": "!1 II II il II Syntaetico-Semantic Learning of Categorial Grammars", "labels": [], "entities": [{"text": "Syntaetico-Semantic Learning of Categorial Grammars", "start_pos": 15, "end_pos": 66, "type": "TASK", "confidence": 0.6744935035705566}]}], "abstractContent": [], "introductionContent": [{"text": "Natural language learning seems, from a formal point of view, an enigma.", "labels": [], "entities": [{"text": "Natural language learning", "start_pos": 0, "end_pos": 25, "type": "TASK", "confidence": 0.7263852953910828}]}, {"text": "As a matter of fact, every human being, given nearly exclusively positive examples (as psycholinguists have noticed) is able at the age of about five to master his/her mother tongue.", "labels": [], "entities": []}, {"text": "Though, no linguistically interesting class of formal languages is learnable with positive data in usual models and).", "labels": [], "entities": []}, {"text": "To solve this paradox, various solutions have been proposed.", "labels": [], "entities": []}, {"text": "Following the chomskian intuitions (Chomsky 65, 68), it can be admitted that natural languages belong to a restricted family and that the human mind includes an innate knowing of the structure of this class (Shinohara 90).", "labels": [], "entities": []}, {"text": "Another approach consists in putting structural, statistical or complexity constraints on the examples proposed to the learner, making his/her inferences easier (Sakakibara 92).", "labels": [], "entities": []}, {"text": "A particular family of research, more concerned with the cognitive relevance of its models, considers that in a natural, situations, examples are always provided with semantic and pragmatic information and tries to make profit of it.", "labels": [], "entities": []}, {"text": "This is the family our research belongs to.", "labels": [], "entities": []}, {"text": "But the property of meaningfulness of natural languages is computationally tractable only if we have at our disposal a theory that precisely articulates syntax and semantics.", "labels": [], "entities": []}, {"text": "The strongest possible articulation is known as the Fredge's principle of compositionality.", "labels": [], "entities": [{"text": "Fredge", "start_pos": 52, "end_pos": 58, "type": "DATASET", "confidence": 0.9482841491699219}]}, {"text": "This principle has acquired an explicit formulation with the works of Richard Montague (Dowry, and his inheritors.", "labels": [], "entities": [{"text": "Richard Montague", "start_pos": 70, "end_pos": 86, "type": "DATASET", "confidence": 0.8056345283985138}]}, {"text": "We will first briefly recall an adapted version of this syntaetico-semantie framework, based on a type of grammars called << classical categorial grammars, (or CCGs), and we will then show how it can been used in a formal theory of natural language learning.", "labels": [], "entities": []}], "datasetContent": [{"text": "The choices made in this model have theoretical backgrounds and consequences.", "labels": [], "entities": []}, {"text": "First, CCG seem to be particularly adapted to the learning process.", "labels": [], "entities": []}, {"text": "Recent researches have found conditions under which the syntax of these grammars is learnable.", "labels": [], "entities": []}, {"text": "But, in these frameworks, tree structures are provided as inputs to the learning algorithm : in our model, the semantic translation plays a close role but in a weaker and more cognitively relevant fashion.", "labels": [], "entities": []}, {"text": "Adriaans (92) also proposed a learning algorithm for categorial grammars, using both syntactic and semantic inputs, but he treated them separately : the semantic learning could only start when the syntactic learning was achieved, instead of helping it as we propose.", "labels": [], "entities": []}, {"text": "Previous models builtin the syntactieo-semantic spirit (Anderson 77, Hamburger & Wexler 75, Hill 83, Langley 82,) used more traditional syntax and semantic representations very close to syntactic structures (Pinker 79) : they failed to represent complex logical relations like quantification or Boolean operators.", "labels": [], "entities": []}, {"text": "Logical languages like IL are more powerful and a priori independent from linguistic structures.", "labels": [], "entities": []}, {"text": "In fact, our approach assumes that logic is the natural << language of the mind, in that situations perceived by our learner are supposed Syntactico-Semantic Learning to be automatically translated into logical formulas before being compared with linguistic expressions.", "labels": [], "entities": []}, {"text": "Fundamentally, what makes natural languages learnable in our model is the presupposition that there exists an isomorphism between the syntax of sentences and their semantics.", "labels": [], "entities": []}, {"text": "This strong principle of compositionality is contested by some linguists but remains an interesting approximation.", "labels": [], "entities": []}, {"text": "The ~< graph deformation condition, used in (Anderson 77) was a weaker version of it.", "labels": [], "entities": [{"text": "Anderson 77)", "start_pos": 45, "end_pos": 57, "type": "DATASET", "confidence": 0.880787173906962}]}, {"text": "Under this condition, the inputs provided to the learner are the leaves and root respectively of two isomorphic trees and what is to be reconstituted is the body of these trees, as displayed in.", "labels": [], "entities": []}, {"text": "But, as opposed to (Anderson 77), there is an asymmetry : the formalism chosen is adapted to language analysis but not to language generation.", "labels": [], "entities": [{"text": "language analysis", "start_pos": 93, "end_pos": 110, "type": "TASK", "confidence": 0.7078664153814316}, {"text": "language generation", "start_pos": 122, "end_pos": 141, "type": "TASK", "confidence": 0.7094122171401978}]}, {"text": "The efficiency of the algorithm seems to crucially rely on the complexity of the input relatively to the current hypothesis.", "labels": [], "entities": []}, {"text": "This complexity can be measured by the number of new words appearing in a sentence example.", "labels": [], "entities": []}, {"text": "If few new words are introduced in each new example, the number of hypotheses to explore will remain reasonable.", "labels": [], "entities": []}, {"text": "Else, the learning maybe too complicated.", "labels": [], "entities": []}, {"text": "Of course, this valuable intuition still needs to be formulated and proved in a more formal way.", "labels": [], "entities": []}, {"text": "It is not possible to develop here how to treat the cases when a word needs more than one category, but it remains possible to learn in this context.", "labels": [], "entities": []}, {"text": "The framework is still incomplete because we haven't chosen any learning model and we haven't proved the learnability of any language in it with our strategy.", "labels": [], "entities": []}, {"text": "An extended and more general version of the algorithm in, using Lambek grammars (Lambek 58), is being implemented and tested.", "labels": [], "entities": [{"text": "Lambek grammars (Lambek 58)", "start_pos": 64, "end_pos": 91, "type": "DATASET", "confidence": 0.806105375289917}]}, {"text": "But the approach seems original and interesting enough to be developed further.", "labels": [], "entities": []}], "tableCaptions": []}