{"title": [], "abstractContent": [{"text": "This paper describes experiments in Machine Learning for text classification using anew representation of text based on WordNet hypernyms.", "labels": [], "entities": [{"text": "text classification", "start_pos": 57, "end_pos": 76, "type": "TASK", "confidence": 0.7729681432247162}]}, {"text": "Six binary classification tasks of varying difficulty are defined, and the Ripper system is used to produce discrimination rules for each task using the new hypernym density representation.", "labels": [], "entities": []}, {"text": "Rules are also produced with the commonly used bag-of-words representation, incorporating no knowledge from WordNet.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 108, "end_pos": 115, "type": "DATASET", "confidence": 0.9730597734451294}]}, {"text": "Experiments show that for some of the more difficult tasks the hypernym density representation leads to significantly more accurate and more comprehensible rules.", "labels": [], "entities": []}], "introductionContent": [{"text": "The task of Supervised Machine Learning can be stated as follows: given a set of classification labels C, and set of training examples E, each of which has been assigned one of the class labels from C, the system must use E to form a hypothesis that can be ~used to predict the class labels of previously unseen examples of the same type.", "labels": [], "entities": [{"text": "Supervised Machine Learning", "start_pos": 12, "end_pos": 39, "type": "TASK", "confidence": 0.8151180942853292}]}, {"text": "In machine learning systems that classify text, E is a set of labeled documents from a corpus such as Reuters-21578.", "labels": [], "entities": [{"text": "Reuters-21578", "start_pos": 102, "end_pos": 115, "type": "DATASET", "confidence": 0.949487030506134}]}, {"text": "The labels can signify topic headings, writing styles, or judgements as to the documents' relevance.", "labels": [], "entities": []}, {"text": "Text classification systems are used in a variety of contexts, including e-mail and news filtering, personal information agents and assistants, information retrieval, and automatic indexing.", "labels": [], "entities": [{"text": "Text classification", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.71732497215271}, {"text": "e-mail and news filtering", "start_pos": 73, "end_pos": 98, "type": "TASK", "confidence": 0.7250320762395859}, {"text": "information retrieval", "start_pos": 144, "end_pos": 165, "type": "TASK", "confidence": 0.7993179857730865}]}, {"text": "Before a set of documents can be presented to a machine learning system, each document must be transformed into a feature vector.", "labels": [], "entities": []}, {"text": "Typically, each element of a feature vector represents a word from the corpus.", "labels": [], "entities": []}, {"text": "The feature values maybe binary, indicating presence or absence of the word in the document, or they maybe integers or real numbers indicating some measure of frequency of the word's appearance in the", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Information on the class~", "labels": [], "entities": []}, {"text": " Table 2: Comparison of percentage error rates over  l O-fold cross-validation for the normalizing  experiments. No statistically significant benefit or  harm is derived from any of these changes of  representation.", "labels": [], "entities": []}, {"text": " Table 3: Comparison of percentage error rates over  lO-fold cross-validation for the six data sets in the  study. Statistically significant improvements over  bag-of-words are shown in Italics.", "labels": [], "entities": []}]}