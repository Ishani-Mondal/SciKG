{"title": [{"text": "POS Tagging versus Classes in Language Modeling", "labels": [], "entities": [{"text": "POS Tagging", "start_pos": 0, "end_pos": 11, "type": "TASK", "confidence": 0.8284395039081573}, {"text": "Language Modeling", "start_pos": 30, "end_pos": 47, "type": "TASK", "confidence": 0.7303721606731415}]}], "abstractContent": [{"text": "Language models for speech recognition concen-Irate solely on recognizing the words that were spoken.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 20, "end_pos": 38, "type": "TASK", "confidence": 0.7142192423343658}]}, {"text": "In this paper, we advocate redefining the speech recognition problem so that its goal is to find both the best sequence of words and their POS tags, and thus incorporate POS tagging.", "labels": [], "entities": [{"text": "speech recognition problem", "start_pos": 42, "end_pos": 68, "type": "TASK", "confidence": 0.8267002105712891}, {"text": "POS tagging", "start_pos": 170, "end_pos": 181, "type": "TASK", "confidence": 0.7514061331748962}]}, {"text": "The use of POS tags allows more sophisticated generalizations than are afforded by using a class-based approach.", "labels": [], "entities": []}, {"text": "Furthermore , if we want to incorporate speech repair and intonational phrase modeling into the language model, using POS tags rather than classes gives better performance in this task.", "labels": [], "entities": [{"text": "speech repair", "start_pos": 40, "end_pos": 53, "type": "TASK", "confidence": 0.7728211581707001}, {"text": "phrase modeling", "start_pos": 71, "end_pos": 86, "type": "TASK", "confidence": 0.6829071640968323}]}], "introductionContent": [{"text": "For recognizing spontaneous speech, the acoustic signal is to weak to narrow down the number of word candidates.", "labels": [], "entities": [{"text": "recognizing spontaneous speech", "start_pos": 4, "end_pos": 34, "type": "TASK", "confidence": 0.8760112325350443}]}, {"text": "Hence, speech recognizers employ a language model that prunes out acoustic alternatives by taking into account the previous words that were recognized.", "labels": [], "entities": [{"text": "speech recognizers", "start_pos": 7, "end_pos": 25, "type": "TASK", "confidence": 0.7112712264060974}]}, {"text": "In doing this, the speech recognition problem is viewed as finding the most likely word sequence 12\u00a2 given the acoustic signal.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 19, "end_pos": 37, "type": "TASK", "confidence": 0.8162843585014343}]}, {"text": "if\" = argmwaX Pr(WIA ) We can rewrite the above using Bayes' rule.", "labels": [], "entities": [{"text": "argmwaX", "start_pos": 6, "end_pos": 13, "type": "METRIC", "confidence": 0.9537127017974854}]}, {"text": "14 r = argmax Pr(AIW) Pr(W) (2) w Pr(A) Since Pr(A) is independent of the choice of W, we simplify the above as follows.", "labels": [], "entities": [{"text": "argmax Pr(AIW) Pr", "start_pos": 7, "end_pos": 24, "type": "METRIC", "confidence": 0.9056758284568787}]}], "datasetContent": [{"text": "To make the best use of our limited data, we used a six-fold cross-validation procedure: each sixth of the data was tested using a model built from the remaining data.", "labels": [], "entities": []}, {"text": "Changes in speaker are marked in the word transcription with the special token <turn>.", "labels": [], "entities": []}, {"text": "We treat contractions, such as \"that'll\" and \"gonna\", as separate words, treating them as \"that\" and \"'ll'\" for the first example, and \"going\" and \"ta\" for the second.", "labels": [], "entities": []}, {"text": "1 We also changed all word fragments into the token <fragment>.", "labels": [], "entities": []}, {"text": "Since current speech recognition rates for spontaneous speech are quite low, we have run the experiments on the hand-collected transcripts.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 14, "end_pos": 32, "type": "TASK", "confidence": 0.6938325315713882}]}, {"text": "In searching for the best sequence of POS tags for the transcribed words, we follow the technique proposed by and only keep a small number of alternative paths by pruning the low probability paths after processing each word.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1 gives informa- tion about the corpus.", "labels": [], "entities": []}, {"text": " Table 3: Using Richer Context", "labels": [], "entities": []}, {"text": " Table 4: POS Tagging and Perplexity", "labels": [], "entities": [{"text": "POS Tagging", "start_pos": 10, "end_pos": 21, "type": "TASK", "confidence": 0.7086552232503891}]}, {"text": " Table 5: Detecting Intonational Phrase Boundaries", "labels": [], "entities": [{"text": "Detecting Intonational Phrase Boundaries", "start_pos": 10, "end_pos": 50, "type": "TASK", "confidence": 0.9037420451641083}]}, {"text": " Table 6: Detecting Speech Repairs", "labels": [], "entities": [{"text": "Detecting Speech Repairs", "start_pos": 10, "end_pos": 34, "type": "TASK", "confidence": 0.9040260513623556}]}]}