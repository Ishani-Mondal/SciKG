{"title": [], "abstractContent": [], "introductionContent": [{"text": "Since their renaissance in the mid-1980s, Artificial Neural Network (ANN) techniques have been successfully applied across abroad spectrum of problem domains such as pattern recognition and function approximation.", "labels": [], "entities": [{"text": "pattern recognition", "start_pos": 166, "end_pos": 185, "type": "TASK", "confidence": 0.8557242453098297}, {"text": "function approximation", "start_pos": 190, "end_pos": 212, "type": "TASK", "confidence": 0.8359054028987885}]}, {"text": "However despite these capabilities, to an end user an ANN is an arcane web of interconnected input, hidden, and output units.", "labels": [], "entities": []}, {"text": "Moreover an ANN solution manifests itself entirely as sets of numbers in the form of activation function parameters and weight vectors.", "labels": [], "entities": []}, {"text": "As such a trained ANN offers little or no insight into the process by which it has arrived at a given result nor, in general, the totality of \"knowledge\" actually embedded therein.", "labels": [], "entities": [{"text": "ANN", "start_pos": 18, "end_pos": 21, "type": "TASK", "confidence": 0.921904981136322}]}, {"text": "This lack of a capacity to provide a \"human comprehensible\" explanation is seen as a clear impediment to a more widespread acceptance of ANNs.", "labels": [], "entities": [{"text": "acceptance of ANNs", "start_pos": 123, "end_pos": 141, "type": "TASK", "confidence": 0.6133061746756235}]}, {"text": "In order to redress this situation, recently considerable effort has been directed towards providing ANNs with the requisite explanation capability.", "labels": [], "entities": [{"text": "ANNs", "start_pos": 101, "end_pos": 105, "type": "TASK", "confidence": 0.9372818470001221}]}, {"text": "In particular a number of mechanisms, procedures, and techniques have been proposed and developed to extract the knowledge embedded in a trained ANN as a set of symbolic rules which in effect mimic the behaviour of the ANN.", "labels": [], "entities": []}, {"text": "A recent survey conducted by offered an insight into the modus operandi of abroad cross-section of such techniques.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 2. The second column gives the total  number of transitions permitted by the FSA. The third  column gives the percent prediction score on the  training data. Best score is 60% which compares with  69% of the training data learned by the original Elman  network from which the hidden unit activations were  obtained. The total prediction score tends to increase  with the number of states.  The fourth column of", "labels": [], "entities": [{"text": "FSA", "start_pos": 84, "end_pos": 87, "type": "DATASET", "confidence": 0.8971800208091736}, {"text": "prediction", "start_pos": 125, "end_pos": 135, "type": "METRIC", "confidence": 0.8758898973464966}]}, {"text": " Table 2  Performance of FSA's prepared from k-means cluster", "labels": [], "entities": [{"text": "FSA", "start_pos": 25, "end_pos": 28, "type": "TASK", "confidence": 0.6558308601379395}]}, {"text": " Table 3  The effect of removing low frequency transitions from  an FSA having 10 states  % score  % total  % missing  on non-missing  score  transitions  transitions  45  53  1.7  53  23  52  10.3  51", "labels": [], "entities": [{"text": "FSA", "start_pos": 68, "end_pos": 71, "type": "DATASET", "confidence": 0.7852057218551636}]}, {"text": " Table 4  The effect of choice of'rescue' state on the  )efformance  Average  Rescue  word  state  position  of inputs  S2  1.36  $5  2.94  $8  3.30  $3  4.18  $7  4.37", "labels": [], "entities": [{"text": "efformance  Average  Rescue  word  state  position", "start_pos": 57, "end_pos": 107, "type": "METRIC", "confidence": 0.7732434223095576}]}]}