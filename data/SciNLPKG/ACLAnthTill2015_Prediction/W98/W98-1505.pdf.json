{"title": [{"text": "Valence Induction with a Head-Lexicalized PCFG", "labels": [], "entities": [{"text": "PCFG", "start_pos": 42, "end_pos": 46, "type": "DATASET", "confidence": 0.5539534091949463}]}], "abstractContent": [], "introductionContent": [{"text": "Either directly or indirectly, the lexicon fora natural language specifies complementation frames or valences for open-class words such as verbs and nouns.", "labels": [], "entities": []}, {"text": "Constructing a lexicon of complementation fram<:~s for larg<-'!", "labels": [], "entities": []}, {"text": "vocabularies constitutes a challenge of scale, with the further complication that frame usage, like vocabulary, varies with genre and undergoes ongoing: innovation in a living language.", "labels": [], "entities": []}, {"text": "This paper addresses this problem by means of a learning tech\u00b7\u00b7 niquc baswi on probabilistic lexicalized context free grammars and the expectation-maximi~\";ation (EM) algorithm.", "labels": [], "entities": []}, {"text": "Given a hand-written grammar and a text corpus, frequencies of ahead word accompanied by a frame are estimated using the inside-outside algorithm, and such frequencies are used to compute probability para.meters characterizing subcategorization.", "labels": [], "entities": []}, {"text": "The procedure can be iterated for improved models.", "labels": [], "entities": []}, {"text": "\\Nc show that the scheme is practical for large vocabularies and accurate enough to capture differences in usage, such as those characteristic of different domains.", "labels": [], "entities": []}], "datasetContent": [{"text": "We estimated a head-lexicalized PCFG from parts of the British National Corpus (BNC Consortium), using the grammar described in the first section and the estimation method of the previous section.", "labels": [], "entities": [{"text": "British National Corpus (BNC Consortium)", "start_pos": 55, "end_pos": 95, "type": "DATASET", "confidence": 0.9540993826729911}]}, {"text": "A bootstrapping method was used, in which first a non-lexicalized probabilistic model was used to collect lexicalized event counts.", "labels": [], "entities": []}, {"text": "On the next iteration, counts were estimated based on a lexicalized weighting of parses, as described in the previous section.", "labels": [], "entities": []}, {"text": "Analyses were restricted to those consistent with the part of speech tags specified in the BNC, which are produced with a tagger.", "labels": [], "entities": [{"text": "BNC", "start_pos": 91, "end_pos": 94, "type": "DATASET", "confidence": 0.9363921284675598}]}, {"text": "In each lexicalized iteration, event counts were collected over a contiguous 39 five million word segment of the corpus.", "labels": [], "entities": []}, {"text": "Parameters were re-cornputed in the way described above, and the procedure was iterated on the next contiguous five-million word segment.", "labels": [], "entities": []}, {"text": "Results from all iterations were pooled to form a single model estimated from 50M words.", "labels": [], "entities": []}, {"text": "illustrates lexical distributions in this model.", "labels": [], "entities": []}, {"text": "This training scheme allows the frame distributions for high-frequency words a chance to converge on their true distributions, whereas a single 50M word iteration would not.", "labels": [], "entities": []}, {"text": "The strategy derives from a variant generalized EM algorithm presented in.", "labels": [], "entities": []}, {"text": "Ina nutshell, reestimating the parameters during the course of a single training iteration will still lead to convergence On a maximum-likelihood estimate, provided certain conditions are met.", "labels": [], "entities": [{"text": "convergence", "start_pos": 110, "end_pos": 121, "type": "METRIC", "confidence": 0.9434057474136353}]}, {"text": "Foremost among these is the requirement that no parameter setting can be prematurely set to zero; this is met by our smoothing strategy.", "labels": [], "entities": []}, {"text": "This is not to say that precisely the same strategy, pursued across multiple iterations, would produce a maximum-likelihood estimate; it would not.", "labels": [], "entities": []}, {"text": "However, \"classical\" EM, requiring repeated iteration over the entire training set, is both relatively inefficient and infeasible given our present computational resources.", "labels": [], "entities": [{"text": "EM", "start_pos": 21, "end_pos": 23, "type": "TASK", "confidence": 0.9624332785606384}]}, {"text": "The comparison to frames specified in a dictionary we use was introduced by Brent and subsequently used by Manning, and.", "labels": [], "entities": []}, {"text": "The measure uses precision and recall to compare the set of induced frames to those in the standard.", "labels": [], "entities": [{"text": "precision", "start_pos": 17, "end_pos": 26, "type": "METRIC", "confidence": 0.999535322189331}, {"text": "recall", "start_pos": 31, "end_pos": 37, "type": "METRIC", "confidence": 0.9992850422859192}]}, {"text": "Precision is the percentage of frames that the system proposes that are correct (i.e. in the standard).", "labels": [], "entities": [{"text": "Precision", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.9935838580131531}]}, {"text": "Recall is the percentage of frames in the standard that the system duce measurements from our system, we must first reduce our distributions to set membership.", "labels": [], "entities": []}, {"text": "Brent proposed a stoehastic filter for this reduction, consisting of a set of per-frame probability cutoffs, which are applied independently of the lexical head.", "labels": [], "entities": []}, {"text": "Although though the independence assumption is certainly dubious, we have adopted this method, without change, except for the introduction of a heuristic for finding the frame cutoffs.", "labels": [], "entities": []}, {"text": "The key property of cutoffs is that they control the tradeoff of precision versus recall.", "labels": [], "entities": [{"text": "precision", "start_pos": 65, "end_pos": 74, "type": "METRIC", "confidence": 0.9985242486000061}, {"text": "recall", "start_pos": 82, "end_pos": 88, "type": "METRIC", "confidence": 0.9965102076530457}]}, {"text": "Raising the cutoff will generally produce a higher precision, but lower recall, and contrariwise.", "labels": [], "entities": [{"text": "precision", "start_pos": 51, "end_pos": 60, "type": "METRIC", "confidence": 0.9995872378349304}, {"text": "recall", "start_pos": 72, "end_pos": 78, "type": "METRIC", "confidence": 0.9997219443321228}]}, {"text": "As we are neutral about this tradeoff, we set the cutoffs at the crossover point!", "labels": [], "entities": []}, {"text": "where the difference in precision and recall changes sign.", "labels": [], "entities": [{"text": "precision", "start_pos": 24, "end_pos": 33, "type": "METRIC", "confidence": 0.9989645481109619}, {"text": "recall", "start_pos": 38, "end_pos": 44, "type": "METRIC", "confidence": 0.9989198446273804}]}, {"text": "This is not entirely deterministic, as the measures may cross more than once; in that case, we optimize for the best precision.", "labels": [], "entities": [{"text": "precision", "start_pos": 117, "end_pos": 126, "type": "METRIC", "confidence": 0.9975716471672058}]}, {"text": "For our dictionary, we used The Oxford Advanced Learner\u00b7's Dictionary (Hornby), also used by Ersan/Charniak and Manning.", "labels": [], "entities": [{"text": "Oxford Advanced Learner\u00b7's Dictionary (Hornby)", "start_pos": 32, "end_pos": 78, "type": "DATASET", "confidence": 0.9263899856143527}]}, {"text": "We reduced our frame set and the dictionary 1 S to a common set, mapping some frames and eliminating others.", "labels": [], "entities": []}, {"text": "For evaluation, we selected 200 verbs at random from among those that occurred more than 500 times in the training data; half were used to set the optimal cutoff parameters, and precision and recall were measured with the remainder.: Precision/recall broken down by frame.", "labels": [], "entities": [{"text": "precision", "start_pos": 178, "end_pos": 187, "type": "METRIC", "confidence": 0.9994780421257019}, {"text": "recall", "start_pos": 192, "end_pos": 198, "type": "METRIC", "confidence": 0.9985169768333435}, {"text": "Precision", "start_pos": 234, "end_pos": 243, "type": "METRIC", "confidence": 0.9089358448982239}, {"text": "recall", "start_pos": 244, "end_pos": 250, "type": "METRIC", "confidence": 0.9129152297973633}]}, {"text": "linked up with their complements because of interjections, complex conjunctions or ellipses, this includes frames such as SBAR and WH-complernents which are not included in the dnmk/phrase grammar.", "labels": [], "entities": []}, {"text": "While it would be possible in principle to extract these from the present word collocation statistics, we plan instead to pursue a solution involving extensions in the grammar.", "labels": [], "entities": []}, {"text": "A second major source of error is prepositional phrases.", "labels": [], "entities": []}, {"text": "The complementation model embodied in the PCFG does not distinguish complements from adjuncts, and therefore adjunct prepositional phrases area source of false positives.", "labels": [], "entities": [{"text": "PCFG", "start_pos": 42, "end_pos": 46, "type": "DATASET", "confidence": 0.9488836526870728}]}, {"text": "Thus the NP PP frame is scored as a false positive for the verb meet, because the OALD does not list the frame, although the combination appears often in the corpus data.", "labels": [], "entities": [{"text": "OALD", "start_pos": 82, "end_pos": 86, "type": "DATASET", "confidence": 0.8309614062309265}]}, {"text": "While such frames lead to a loss of precision in the dictionary evaluation, we do not necessarily consider them a flaw in the information learned by the system, since the argument/adjunct distinction is often tenuous, and adjuncts are in many cases lexically conditioned.", "labels": [], "entities": [{"text": "precision", "start_pos": 36, "end_pos": 45, "type": "METRIC", "confidence": 0.9984102249145508}]}, {"text": "Lastly, there are many false negatives for the particle frame and noun plus particle.", "labels": [], "entities": []}, {"text": "This is mainly due to disagreements between BNC particle tagging and particle markup in the OALD.", "labels": [], "entities": [{"text": "BNC particle tagging", "start_pos": 44, "end_pos": 64, "type": "TASK", "confidence": 0.7331485748291016}, {"text": "particle markup", "start_pos": 69, "end_pos": 84, "type": "TASK", "confidence": 0.7424620091915131}, {"text": "OALD", "start_pos": 92, "end_pos": 96, "type": "DATASET", "confidence": 0.9192277789115906}]}, {"text": "Despite these difficulties, the summary shown in table shows results that are on the whole favorable.", "labels": [], "entities": []}, {"text": "In comparison with other work with a comparable number of frames (Manning, Ersan/Charniak), the system is well ahead on recall and well behind on precision.", "labels": [], "entities": [{"text": "recall", "start_pos": 120, "end_pos": 126, "type": "METRIC", "confidence": 0.9981306195259094}, {"text": "precision", "start_pos": 146, "end_pos": 155, "type": "METRIC", "confidence": 0.9989312291145325}]}, {"text": "If one takes the sum of precision and recall to be the final performance indicator, than we are slightly ahead: 1.54 vs.  for Manning.", "labels": [], "entities": [{"text": "precision", "start_pos": 24, "end_pos": 33, "type": "METRIC", "confidence": 0.9977236390113831}, {"text": "recall", "start_pos": 38, "end_pos": 44, "type": "METRIC", "confidence": 0.9972311854362488}]}, {"text": "Briscoe and Carroll's work, with ten times as many target frames, is so different that the numbers maybe regarded as incomparable.", "labels": [], "entities": []}, {"text": "Obviously, precision and recall measured against a standard relies on the completeness and accuracy of that standard.", "labels": [], "entities": [{"text": "precision", "start_pos": 11, "end_pos": 20, "type": "METRIC", "confidence": 0.9993815422058105}, {"text": "recall", "start_pos": 25, "end_pos": 31, "type": "METRIC", "confidence": 0.9992828965187073}, {"text": "accuracy", "start_pos": 91, "end_pos": 99, "type": "METRIC", "confidence": 0.9970308542251587}]}, {"text": "In checking false positives, Ersan and Charniak found that the OALD was incomplete enough to have a serious impact on precision.", "labels": [], "entities": [{"text": "OALD", "start_pos": 63, "end_pos": 67, "type": "METRIC", "confidence": 0.8062273263931274}, {"text": "precision", "start_pos": 118, "end_pos": 127, "type": "METRIC", "confidence": 0.9978506565093994}]}, {"text": "Symmetrically, false negatives conflate deficiencies in the corpus with poor learning efficiency.", "labels": [], "entities": []}, {"text": "It is impossible to say based on table which of the systems is more efficient at learning.", "labels": [], "entities": []}, {"text": "While our system shows the best recall, this could be attributed to our having the best training data.", "labels": [], "entities": [{"text": "recall", "start_pos": 32, "end_pos": 38, "type": "METRIC", "confidence": 0.9994300007820129}]}, {"text": "Cha.rniak used 40M words of training data, comparable to our SOM, but his data was homogeneous, all taken from the Wall Street JournaL As we will show below, frame usage varies across genres, so the BNC, which includes texts from a wide variety of sources, shows more varied frame usage than the WSJ, and thus provides better dad for frame acquisition.", "labels": [], "entities": [{"text": "Wall Street JournaL", "start_pos": 115, "end_pos": 134, "type": "DATASET", "confidence": 0.9633804559707642}, {"text": "BNC", "start_pos": 199, "end_pos": 202, "type": "DATASET", "confidence": 0.8963890075683594}, {"text": "WSJ", "start_pos": 296, "end_pos": 299, "type": "DATASET", "confidence": 0.9336035251617432}, {"text": "frame acquisition", "start_pos": 334, "end_pos": 351, "type": "TASK", "confidence": 0.8445955216884613}]}, {"text": "The information-theoretic notion of cross entropy provides a detailed measure of the similarity of the acquired probabilistic lexicon to the distribution of frames actually exhibited in the corpus (which we call the empirical distribution).", "labels": [], "entities": []}, {"text": "The cross entropy of the estimated distribution q with the empirical distribution p obeys the identity where H is the usual entropy function and Dis the rel1etive entropy, or Kullback-Leibler distance.", "labels": [], "entities": [{"text": "Dis", "start_pos": 145, "end_pos": 148, "type": "METRIC", "confidence": 0.9887535572052002}]}, {"text": "The entropy of a distribution over frames can be conceptualized as the average number of bits required to designate a frame in an ideal code based on the given distribution.", "labels": [], "entities": []}, {"text": "In this context, entropy measures the complexity of the observed frame distribution.", "labels": [], "entities": []}, {"text": "The relative entropy is the penalty paid in bits when the frame is chosen according to the empirical distribution p, but the code is derived from the modeJls estimated distribution, q.", "labels": [], "entities": []}, {"text": "Relative entropy is always non-negative, and reaches zero only   distributions are identical.", "labels": [], "entities": []}, {"text": "Our goal, then, is to minimize the relative entropy.", "labels": [], "entities": []}, {"text": "For more in-depth discussion of entropy measures, see, or any introductory information theory text.", "labels": [], "entities": []}, {"text": "For relative entropy to be finite, the estimated distribution q must be non-zero whenever p is.", "labels": [], "entities": []}, {"text": "However, some observed frames are not present in the grammar, for one of two reasons.", "labels": [], "entities": []}, {"text": "Some well-known frames such as SBAR require high-level constructs not available in the chunk/phrase grammar and unusual/unorthodox frames turn up in the data, e.g. PAH.T PP PP.", "labels": [], "entities": []}, {"text": "Since the model lacks these frames, smoothing against the unlexicalized rules is insufficient.", "labels": [], "entities": []}, {"text": "Instead, for all the estimated distributions, we smooth against a Poisson distribution over cat~egories, which assigns non-zero probability to all frames, observed or not.", "labels": [], "entities": []}, {"text": "This allows us to spell out the unknown frame using a known finite alphabet, the grammar categories, while retaining a reasonable average length over frames.", "labels": [], "entities": []}, {"text": "For our entropy measurements) we selected three verbs, allow, reach 1 and suffer and extracted about 200 occurrences of each from portions of the BNC not used for training.", "labels": [], "entities": [{"text": "BNC", "start_pos": 146, "end_pos": 149, "type": "DATASET", "confidence": 0.9558430910110474}]}, {"text": "Half of each sample was drawn from \"imaginative\" text and the other half from the natural or applied sciences, as indicated by BNC text mark-up.", "labels": [], "entities": [{"text": "BNC text mark-up", "start_pos": 127, "end_pos": 143, "type": "DATASET", "confidence": 0.908599336942037}]}, {"text": "The true frame for each verb occurrence was marked by a human judge 3 . The empirical distribution was taken as the maximum-likelihood estimate from these frequencies.", "labels": [], "entities": []}, {"text": "indicate t.he observed frequencies and the entropy of the resulting distributions.", "labels": [], "entities": []}, {"text": "Alongside the observed frequencies, we indicate a set of estimated frequencies.", "labels": [], "entities": []}, {"text": "These were generated by taking the 50M word model described above, parsing the test sentences, and extracting the estimated frequencies.", "labels": [], "entities": []}, {"text": "The sum of estimated frequencies is gen-  erallv less than the observed frequencies due to tagging.errors, parse failures, and frequency assigned to frames not shown in the tables.", "labels": [], "entities": [{"text": "gen-  erallv", "start_pos": 36, "end_pos": 48, "type": "METRIC", "confidence": 0.8679240345954895}]}, {"text": "However, an eyeball inspection of the tables shows that the parser does a good job of reproducing the target distribution.", "labels": [], "entities": []}, {"text": "One striking feature in the tables is the variation across genre.", "labels": [], "entities": []}, {"text": "In particular, suffer used in the imaginative genre shows a very different distribution than suffer in the natural sciences.", "labels": [], "entities": []}, {"text": "A chi-squared test applied to each pair indicates that the samples come from distinct distributions (confidence> 95%).", "labels": [], "entities": []}, {"text": "The column labeled \"50M lex\" in provides a quantitative measure of the agreement between the 50M word combined model and the empirical distributions for the three verbs in two genres in the form of relative entropy.", "labels": [], "entities": []}, {"text": "The first column repeats the entropy of the data distributions.", "labels": [], "entities": []}, {"text": "For purposes of comparison, the second column indicates the relative entropy of one data distribution with the other data distribution filling the role of the estimated distribution (i.e. q) in t.he discussion above.", "labels": [], "entities": []}, {"text": "The relative entropy is lower when the estimated distribution is used for q than when the data distribution for the other genre is used for q in each case but one, where the figures are the same.", "labels": [], "entities": []}, {"text": "This suggests the combined model contains fairly good overall distributions.", "labels": [], "entities": []}, {"text": "To numerically evaluate whether the system was abl(', to learn the distribution exhibited in a given collection of sentences, we tuned the lexicon by parsing the test sentences for each genre separately with the 42: Frame relative entropy for three verbs in two genres.", "labels": [], "entities": []}, {"text": "The first column narnes the lexical head and genre, and the second the entropy (H) of the empirical distribution over frames, p.", "labels": [], "entities": [{"text": "entropy (H)", "start_pos": 71, "end_pos": 82, "type": "METRIC", "confidence": 0.9579431712627411}]}, {"text": "By empirical distribution we mean the relative frequencies from examples scored by a human judge.", "labels": [], "entities": []}, {"text": "Columns three through five give the relative entropy D(pjjq) for various related distributions.", "labels": [], "entities": [{"text": "relative entropy D(pjjq)", "start_pos": 36, "end_pos": 60, "type": "METRIC", "confidence": 0.7993240306774775}]}, {"text": "In column three, q is the empirical frame distribution for the same head 1 but with the complementary genre.", "labels": [], "entities": []}, {"text": "In column four q is the (genre-independent) distribution derived from the 50M word lexicalbed model.", "labels": [], "entities": []}, {"text": "Column five uses the unlexicalizecl frame distribution derived from the SOM model, i.e. a distribution insensitive to the head verb.", "labels": [], "entities": []}, {"text": "Lower relative entropy is better.", "labels": [], "entities": []}, {"text": "50IVI word model, extracting the frequencies, and estimating the distribution from these.", "labels": [], "entities": [{"text": "50IVI", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.936393678188324}]}, {"text": "The results are the column 4 labeled ''SOM lexicalized extraction 11 in 7.", "labels": [], "entities": [{"text": "SOM lexicalized extraction", "start_pos": 39, "end_pos": 65, "type": "TASK", "confidence": 0.783720850944519}]}, {"text": "The following columns give the same figures for frcqency extraction with other models.", "labels": [], "entities": [{"text": "frcqency extraction", "start_pos": 48, "end_pos": 67, "type": "TASK", "confidence": 0.7512865662574768}]}, {"text": "Extraction with the large lexicalized model gives the best results, and gives better relative entropy than the 50M lexicalilazed model itself (in column 2).", "labels": [], "entities": []}, {"text": "Notice that only the distributions estimated with the two 50M mo~lels are better than the 50M lexicalized model, though the unlexicalized one is only marginally better.", "labels": [], "entities": []}, {"text": "In this sense, only the 50M lexicalized parser proves to be a good enough parser for genre tuning.", "labels": [], "entities": [{"text": "genre tuning", "start_pos": 85, "end_pos": 97, "type": "TASK", "confidence": 0.704220786690712}]}, {"text": "Notice that with this model, tuning in no case gives worse relative entropy 1 and in five out of six cases give an improvement.", "labels": [], "entities": [{"text": "relative entropy 1", "start_pos": 59, "end_pos": 77, "type": "METRIC", "confidence": 0.7765392363071442}]}, {"text": "Notice also that relative entropy for the distributions obtained by tuning with the 50M model area good deal lower than the cross-genre figures from Table 6.", "labels": [], "entities": []}, {"text": "This suggests that if we wanted to have a good probabilistic lexicon for, say, the imaginative genre, we would be better off using the automatic extraction procedure on data drawn from that.", "labels": [], "entities": []}, {"text": "genre than using a perfect parser (or a lexicographer) on dat.a drawn from some other genre, such as the natural sciences.", "labels": [], "entities": []}, {"text": "This provides a calibration of the accuracy of the lexicalized parser 1 s estimates, and conversely demonstrates that words are not used in the same  with maximal probability; this is identified in a parse forest by iterating from terminal nodes, multiplying child probabilities and the local node weight at and-nodes (chart edges), and choosing a child with maximal probability at or-nodes (chart constituents).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 35, "end_pos": 43, "type": "METRIC", "confidence": 0.9992088675498962}]}, {"text": "give examples of maximal probability probability parses.", "labels": [], "entities": [{"text": "maximal probability probability parses", "start_pos": 17, "end_pos": 55, "type": "TASK", "confidence": 0.5603334605693817}]}, {"text": "Other optimality criteria can be defined.", "labels": [], "entities": []}, {"text": "Tlw structure on noun chunks is often highly ambiguous, because of bracketing and part of speech ambiguities among modifiers.", "labels": [], "entities": []}, {"text": "I<Dr many purposes, the internal structure of an noun chunk is irrelevant; one just wants to identify the chunk.", "labels": [], "entities": []}, {"text": "From this point of view, a probability estimate which considers just one analysis might underestimate the probability of a noun chunk.", "labels": [], "entities": []}, {"text": "In what we calla sum-max parse, probabilities are summed within chunks by the inside algorithm.", "labels": [], "entities": []}, {"text": "Above the chunk level, a highest-probability tree is computed, as described above.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: On the left: the eight largest parameters  in the lexical choice distribution describing modify- ing adjectives selected by satisfactory. On the right:  parallel information for the distribution describing  heads of objects of the verb address.  .", "labels": [], "entities": []}, {"text": " Table 2: Precision/recall broken down by frame.", "labels": [], "entities": [{"text": "Precision", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9641277194023132}, {"text": "recall", "start_pos": 20, "end_pos": 26, "type": "METRIC", "confidence": 0.9178207516670227}]}, {"text": " Table 3: Type precision/recall comparison. Some of  Manning's frames are parameterized for a preposi- tion.", "labels": [], "entities": [{"text": "precision", "start_pos": 15, "end_pos": 24, "type": "METRIC", "confidence": 0.7656992077827454}, {"text": "recall", "start_pos": 25, "end_pos": 31, "type": "METRIC", "confidence": 0.9607250094413757}]}, {"text": " Table 5: True and estimated frame frequencies for", "labels": [], "entities": []}]}