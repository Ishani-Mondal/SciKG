{"title": [{"text": "Exploring the utility of joint morphological and syntactic learning from child-directed speech", "labels": [], "entities": []}], "abstractContent": [{"text": "Children learn various levels of linguistic structure concurrently, yet most existing models of language acquisition deal with only a single level of structure, implicitly assuming a sequential learning process.", "labels": [], "entities": []}, {"text": "Developing models that learn multiple levels simultaneously can provide important insights into how these levels might interact synergistically during learning.", "labels": [], "entities": []}, {"text": "Here, we present a model that jointly induces syntactic categories and morphological segmentations by combining two well-known models for the individual tasks.", "labels": [], "entities": []}, {"text": "We test on child-directed utterances in English and Spanish and compare to single-task base-lines.", "labels": [], "entities": []}, {"text": "In the morphologically poorer language (English), the model improves morphological segmentation, while in the morphologically richer language (Spanish), it leads to better syntactic categorization.", "labels": [], "entities": []}, {"text": "These results provide further evidence that joint learning is useful, but also suggest that the benefits maybe different for typologically different languages.", "labels": [], "entities": []}], "introductionContent": [{"text": "Models of language acquisition seek to infer linguistic structure from data with minimal amounts of prior knowledge, in order to discover which characteristics of the input data are useful for learning, and thus potentially utilised by human learners.", "labels": [], "entities": [{"text": "language acquisition", "start_pos": 10, "end_pos": 30, "type": "TASK", "confidence": 0.7374275922775269}]}, {"text": "Most previous work has focused on learning individual aspects of linguistic structure.", "labels": [], "entities": []}, {"text": "However, children clearly learn multiple aspects in parallel, rather than sequentially, implying that models of language acquisition should also incorporate joint learning.", "labels": [], "entities": []}, {"text": "Joint models investigate the interaction between different levels of linguistic structure during learning.", "labels": [], "entities": []}, {"text": "These interactions are often (but not necessarily) synergistic, enabling better, more robust, learning by making use of cues from multiple sources.", "labels": [], "entities": []}, {"text": "Recent models using joint learning to model language acquisition have spanned various domains including phonology, word segmentation, syntax and semantics.", "labels": [], "entities": [{"text": "language acquisition", "start_pos": 44, "end_pos": 64, "type": "TASK", "confidence": 0.7392062842845917}, {"text": "word segmentation", "start_pos": 115, "end_pos": 132, "type": "TASK", "confidence": 0.7417867928743362}]}, {"text": "In this paper we examine the joint learning of syntactic categories and morphology, which are acquired by children at roughly the same age, implying possible interactions in the learning process.", "labels": [], "entities": []}, {"text": "Both morphology and word order depend on categorising words based on their morphosyntactic function.", "labels": [], "entities": []}, {"text": "However, previous models of syntactic category learning have relied principally on surrounding context, i.e., word order constraints, whereas models of morphology use word-internal cues.", "labels": [], "entities": [{"text": "syntactic category learning", "start_pos": 28, "end_pos": 55, "type": "TASK", "confidence": 0.7015154659748077}]}, {"text": "Our joint model integrates both sources of information, allowing the model to flexibly weigh them according to their utility.", "labels": [], "entities": []}, {"text": "Languages differ in the richness of their morphology and strictness of word order.", "labels": [], "entities": []}, {"text": "These characteristics appear to be (anti)correlated, with rich morphology co-occurring with free word order and vice versa.", "labels": [], "entities": []}, {"text": "The timecourse of acquisition is also influenced by language typology: learners of morphologically rich languages become productive in morphology earlier, suggesting that richer morphology maybe more salient for learners than impoverished morphology.", "labels": [], "entities": []}, {"text": "Sentence comprehension in children also shows cross-linguistic differences in the cues used to make sense of non-canonical sentence structure: learners of a morphologically rich language (Turkish) disregard word order in favour of morphology, whereas learners of English favour word order.", "labels": [], "entities": [{"text": "Sentence comprehension", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.9228640794754028}]}, {"text": "These interactions between morphology and word order suggest that a joint model will be better able to support the differences in cue strength (rich morphology versus strict word order), and thus be more language-general, than single-task models.", "labels": [], "entities": []}, {"text": "Both syntactic category and morphology induction have been the focus of much recent work.", "labels": [], "entities": [{"text": "syntactic category", "start_pos": 5, "end_pos": 23, "type": "TASK", "confidence": 0.7821136116981506}, {"text": "morphology induction", "start_pos": 28, "end_pos": 48, "type": "TASK", "confidence": 0.7626523971557617}]}, {"text": "for an overview of unsupervised morphology learning, likewise fora comparison of part of speech/syntactic category induction systems.)", "labels": [], "entities": []}, {"text": "However, given the tightly coupled nature of these two tasks, there has been surprisingly little work in joint learning of morphology and syntactic categories.", "labels": [], "entities": []}, {"text": "Systems for inducing syntactic categories often make use of morpheme-like features, such as word-final characters, or model words at the character-level), but do not include morphemes explicitly.", "labels": [], "entities": []}, {"text": "Other systems () use morphological segmentations learned by a separate morphology model as features in a pipeline approach.", "labels": [], "entities": []}, {"text": "Models of morphology induction generally operate over a lexicon, i.e. a list of word types, rather than token corpora).", "labels": [], "entities": [{"text": "morphology induction", "start_pos": 10, "end_pos": 30, "type": "TASK", "confidence": 0.8457554876804352}]}, {"text": "These models find morphological categories on the basis of wordinternal features, without taking syntactic context into account (which is of course not available in a lexicon). and Sirts and Alum\u00e4e (2012) present models that infer morphological segmentations and syntactic categories jointly, although do not evaluate the inferred syntactic categories.", "labels": [], "entities": []}, {"text": "Both make use of a word-type constraint which limits each word form to a single analysis (i.e., all instances of ducks are assigned to a single category and will have the same morpheme analysis, ignoring the gold standard distinction between a plural noun and third person singular verb).", "labels": [], "entities": []}, {"text": "This can make inference more tractable, and often increases performance, but does not respect the ambiguity inherent in natural language, both over syntactic categories and morphological analyses.", "labels": [], "entities": []}, {"text": "The degree of ambiguity is language dependent, so that even if a type-constraint is perhaps relatively unproblematic in English, it will pose problems in morphologically richer languages.", "labels": [], "entities": []}, {"text": "Furthermore, these two models make use of an array of heuristics that may not allow them to be easily generalisable across languages and datasets (e.g., likelihood scaling (, sequential suffix matching (.", "labels": [], "entities": [{"text": "sequential suffix matching", "start_pos": 175, "end_pos": 201, "type": "TASK", "confidence": 0.6643484036127726}]}, {"text": "In this paper, we present a joint model composed of two well-known individual models.", "labels": [], "entities": []}, {"text": "This allows us to cleanly investigate the effects of joint learning and its potential benefits over the single task models.", "labels": [], "entities": []}, {"text": "The simplicity of our models also allows us to avoid modelling and inference heuristics.", "labels": [], "entities": []}, {"text": "Previous models have used adult-directed written texts, which differs significantly from the type of language available to child learners.", "labels": [], "entities": []}, {"text": "We test our joint model on child-directed utterances in English (a morphologically poor language) and Spanish (with richer morphology) 1 . Our results indicate that our joint model is able to flexibly accommodate languages with differing levels of morphological richness.", "labels": [], "entities": []}, {"text": "The joint model matches the performance of single task models on both tasks, demonstrating that the additional complexity is not a problem (i.e., it does not add noise).", "labels": [], "entities": []}, {"text": "Moreover, the joint model improves performance significantly on the task corresponding to the language's weaker cue, indicating a transfer of information from the stronger cue.", "labels": [], "entities": []}, {"text": "The fact that the nature of this improvement varies by language provides evidence that joint learning can effectively accommodate typological diversity.", "labels": [], "entities": []}], "datasetContent": [{"text": "An important argument for joint learning is that it affords increased flexibility and robustness across a wider range of input data.", "labels": [], "entities": []}, {"text": "A model that relies on word order cannot learn syntactic categories from a morphologically complex language with free word order; likewise a model attempting to categorise words using morphology alone will fail on a language without morphology.", "labels": [], "entities": []}, {"text": "An effective joint model will be able to make use of the different cues in both language types in a flexible way.", "labels": [], "entities": []}, {"text": "In order to test the proposed model, we run two experiments on synthetic languages, which simulate languages in which either word order or morphology is the sole cue.", "labels": [], "entities": []}, {"text": "Most natural languages fall between these extremes, but these experiments show that our model can capture the full spectrum.", "labels": [], "entities": []}, {"text": "Language A is a strict word order language lacking morphology.", "labels": [], "entities": []}, {"text": "It has a vocabulary of 200 word types, split into four different categories.", "labels": [], "entities": []}, {"text": "The 50 word types in each category are created by combining four letters, with replacement, into four-letter words, with a different set of letters used in each category . Words within a category may thus share beginning or ending characters, which could be posited as stems or suffixes by the model, but since only 50 of 256 possible strings are used, there will be no strong evidence for consistent stem and suffixes (i.e. stems appearing with multiple suffixes and vice versa).", "labels": [], "entities": []}, {"text": "Each sentence in Language A consists of five words in one of twenty possible category sequences.", "labels": [], "entities": []}, {"text": "In these sequences, each category is either followed by itself or the next category (i.e. [2,2,2,3,4] is valid but [2,4,3,1,4] is not).", "labels": [], "entities": []}, {"text": "Word order is thus strongly constrained by category membership.", "labels": [], "entities": []}, {"text": "Language B has free word order, with category membership signalled by suffixes.", "labels": [], "entities": []}, {"text": "Words are cre- ated by the concatenation of a stem and a suffix, where the stems are the same as the words in language A (50 stems in each of four categories).", "labels": [], "entities": []}, {"text": "One of six category-specific suffixes is appended to each stem, resulting in 300 word types per category.", "labels": [], "entities": []}, {"text": "Each suffix is two letters long, created by combining three possible letters (the same letters used to create the stems), thus making mis-segmentation possible (for instance, up to three of the suffixes could have the same final letter).", "labels": [], "entities": []}, {"text": "Sentences are again five words long, but the sequence of categories is drawn at random, resulting in uniformly random word order.", "labels": [], "entities": []}, {"text": "See for example sentences in both languages.", "labels": [], "entities": []}, {"text": "We create a 5000 word corpus for each language, and run our model on these corpora.", "labels": [], "entities": []}, {"text": "Hyperparameters are set to the same values in both languages . We run the sampler on each dataset for 1000 iterations with simulated annealing.", "labels": [], "entities": []}, {"text": "In both cases, the correct solution is found by iteration 500.", "labels": [], "entities": []}, {"text": "shows that the morphology component continues to increase the log probability by increasing the number of tokens seated at a table.", "labels": [], "entities": []}, {"text": "Note that the correct solution in Language A involves learning a very peaked transition distribution as well as an even more extreme distribution over suffixes (where only the null suffix has high probability), whereas the same distributions in Language B are much flatter.", "labels": [], "entities": []}, {"text": "The fact that the same hyperparameter setting is able to correctly identify the two language extremes indicates that the model is robust to hyperparameter values.", "labels": [], "entities": []}, {"text": "These experiments demonstrate that our joint model is able to learn correctly even when only either morphology or word order is informative in a language.", "labels": [], "entities": []}, {"text": "We now turn to acquisition data from natural languages in which both morphology and word order are useful cues but to varying degrees.", "labels": [], "entities": []}, {"text": "Tags are evaluated using VM, as has become standard for this task (.", "labels": [], "entities": []}, {"text": "VM is a measure of the normalised cross-entropy between gold and proposed clusters; it ranges between 0 and 100, with higher scores being better.", "labels": [], "entities": [{"text": "VM", "start_pos": 0, "end_pos": 2, "type": "METRIC", "confidence": 0.47726839780807495}]}, {"text": "We also use VM to evaluate the morphological segmentation: all tokens with a common suffix are clustered together, and these clusters are compared against the gold suffix clusters . Using a clustering metric avoids the need to evaluate against a gold segmentation point (which the annotation lacks).", "labels": [], "entities": []}, {"text": "Tag membership is added to the non-null model suffixes, so that a final -s suffix found in tag 2 is distinguished from the same suffix found in tag 8 (creating suffixes -s-T8 and -s-T2), analogous to the gold annotation distinction between syncretic morphemes -PL and -3S.", "labels": [], "entities": []}, {"text": "Note that ceiling performance of our model on Suffix VM will be below 100, since our model cannot cluster allomorphs, which are represented by a single abstract morpheme in the gold standard.", "labels": [], "entities": [{"text": "Suffix VM", "start_pos": 46, "end_pos": 55, "type": "DATASET", "confidence": 0.8738507032394409}]}, {"text": "Hyperparameter values for the Pitman-Yor process were found using grid search on a development set (Section 10 of Eve and Section 8 of Ornat; these sections are removed from the dataset we report results on).", "labels": [], "entities": []}, {"text": "We use the values which give the best Suffix VM performance on the development data; however we stress that the development results did not vary greatly over a wide range of hyperparameter values, and only deteriorated significantly at extreme values of a.", "labels": [], "entities": []}, {"text": "There area number of other hyperparameters in the model which we set to fixed values.", "labels": [], "entities": []}, {"text": "The transition hyperparameter \u03b1 t is set to 0.1 in all models.", "labels": [], "entities": []}, {"text": "We set the hyperparameters for the stem and suffix distributions in the morphology base distribution P 0 to 0.001 for both \u03b1 sand \u03b1 f ; \u03b1 k over tags in the MORCLUSTERS model is set to 0.5.", "labels": [], "entities": [{"text": "MORCLUSTERS", "start_pos": 157, "end_pos": 168, "type": "METRIC", "confidence": 0.589647114276886}]}, {"text": "The number of possible stems and suffixes is given by the dataset: in the Eve dataset there are 5339 candidate stems and 6617 candidate suffixes; in the Ornat dataset these numbers are 8649 and 6598, respectively.", "labels": [], "entities": [{"text": "Eve dataset", "start_pos": 74, "end_pos": 85, "type": "DATASET", "confidence": 0.9408968091011047}, {"text": "Ornat dataset", "start_pos": 153, "end_pos": 166, "type": "DATASET", "confidence": 0.9424121677875519}]}, {"text": "The number of tags available to the model is set to the number of gold tags in the data.", "labels": [], "entities": []}, {"text": "Sampling is run for 5000 iterations with annealing.", "labels": [], "entities": [{"text": "Sampling", "start_pos": 0, "end_pos": 8, "type": "TASK", "confidence": 0.8882890939712524}]}, {"text": "Inspection of the posterior log-likelihood indicates that the models converge after about 1000 iterations.", "labels": [], "entities": []}, {"text": "We run inference overall models ten times and report the average performance.", "labels": [], "entities": []}, {"text": "Significance is reported using the non-parametric Wilcoxon ranksum test with a significance level of \u03c1 < 0.05.", "labels": [], "entities": [{"text": "Significance", "start_pos": 0, "end_pos": 12, "type": "METRIC", "confidence": 0.9780347347259521}, {"text": "significance level", "start_pos": 79, "end_pos": 97, "type": "METRIC", "confidence": 0.9697359502315521}]}], "tableCaptions": []}