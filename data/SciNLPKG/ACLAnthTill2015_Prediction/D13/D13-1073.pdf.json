{"title": [{"text": "Mining Scientific Terms and their Definitions: A Study of the ACL Anthology", "labels": [], "entities": [{"text": "Mining Scientific Terms and their Definitions", "start_pos": 0, "end_pos": 45, "type": "TASK", "confidence": 0.7402516901493073}]}], "abstractContent": [{"text": "This paper presents DefMiner, a supervised sequence labeling system that identifies scientific terms and their accompanying definitions.", "labels": [], "entities": []}, {"text": "DefMiner achieves 85% F 1 on a Wikipedia benchmark corpus, significantly improving the previous state-of-the-art by 8%.", "labels": [], "entities": [{"text": "F 1", "start_pos": 22, "end_pos": 25, "type": "METRIC", "confidence": 0.9921101927757263}, {"text": "Wikipedia benchmark corpus", "start_pos": 31, "end_pos": 57, "type": "DATASET", "confidence": 0.8759774565696716}]}, {"text": "We exploit DefMiner to process the ACL Anthology Reference Corpus (ARC)-a large, real-world digital library of scientific articles in computational linguistics.", "labels": [], "entities": [{"text": "ACL Anthology Reference Corpus (ARC)-", "start_pos": 35, "end_pos": 72, "type": "DATASET", "confidence": 0.9250279835292271}]}, {"text": "The resulting automatically-acquired glossary represents the terminology defined over several thousand individual research articles.", "labels": [], "entities": []}, {"text": "We highlight several interesting observations: more definitions are introduced for conference and workshop papers over the years and that multiword terms account for slightly less than half of all terms.", "labels": [], "entities": []}, {"text": "Obtaining a list of popular defined terms in a corpus of computational linguistics papers, we find that concepts can often be categorized into one of three categories: resources, methodologies and evaluation met-rics.", "labels": [], "entities": []}], "introductionContent": [{"text": "Technical terminology forms a key backbone in scientific communication.", "labels": [], "entities": []}, {"text": "By coining formalized terminology, scholars convey technical information precisely and compactly, augmenting the dissemination of scientific material.", "labels": [], "entities": []}, {"text": "Collectively, scholarly * This research is supported by the Singapore National Research Foundation under its International Research Centre @ Singapore Funding Initiative and administered by the IDM Programme compilation efforts result in reference sources such as printed dictionaries, ontologies and thesauri.", "labels": [], "entities": [{"text": "Singapore National Research Foundation", "start_pos": 60, "end_pos": 98, "type": "DATASET", "confidence": 0.8445426523685455}, {"text": "International Research Centre @ Singapore Funding Initiative", "start_pos": 109, "end_pos": 169, "type": "DATASET", "confidence": 0.8955417616026742}, {"text": "IDM Programme", "start_pos": 194, "end_pos": 207, "type": "DATASET", "confidence": 0.8868958652019501}]}, {"text": "While online versions are now common in many fields, these are still largely compiled manually, relying on costly human editorial effort.", "labels": [], "entities": []}, {"text": "This leads to resources that are often outdated or stale with respect to the current state-of-the-art.", "labels": [], "entities": []}, {"text": "Another indirect result of this leads to a second problem: lexical resources tend to be general, and may contain multiple definitions fora single term.", "labels": [], "entities": []}, {"text": "For example, the term \"CRF\" connotes \"Conditional Random Fields\" inmost modern computational linguistics literature; however, there are many definitions for this acronym in Wikipedia.", "labels": [], "entities": []}, {"text": "Because only one correct sense applies, readers may need to expend effort to identify the appropriate meaning of a term in context.", "labels": [], "entities": []}, {"text": "We address both issues in this work by automatically extracting terms and definitions directly from primary sources: scientific publications.", "labels": [], "entities": []}, {"text": "Since most new technical terms are introduced in scientific publications, our extraction process addresses the bottleneck of staleness.", "labels": [], "entities": []}, {"text": "Second, since science is organized into disciplines and sub-disciplines, we can exploit this inherent structure to gather contextual information about a term and its definition.", "labels": [], "entities": []}, {"text": "Aside from performance improvements, the key contributions of our work are in 1) recasting the problem as a sequence labeling task and exploring suitable learning architectures, 2) our proposal and validation of the the use of shallow parsing and dependency features to target definition extraction, and 3) analyzing the ACL Anthology Reference Corpus from statistical, chronological and lexical viewpoints.", "labels": [], "entities": [{"text": "sequence labeling task", "start_pos": 108, "end_pos": 130, "type": "TASK", "confidence": 0.7662136753400167}, {"text": "target definition extraction", "start_pos": 270, "end_pos": 298, "type": "TASK", "confidence": 0.7167666554450989}, {"text": "ACL Anthology Reference Corpus", "start_pos": 321, "end_pos": 351, "type": "DATASET", "confidence": 0.9443274140357971}]}], "datasetContent": [{"text": "We now assess the overall effectiveness of DefMiner, at both the word and sentence level.", "labels": [], "entities": []}, {"text": "Additionally, we want to ascertain the performance changes as we add features to an informed lexical baseline.", "labels": [], "entities": []}, {"text": "We not only benchmark DefMiner's performance over our own W00 collection, but also compare DefMiner against previous published work on the definition sentence identification task on the WCL (English Wikipedia) corpus.", "labels": [], "entities": [{"text": "W00 collection", "start_pos": 58, "end_pos": 72, "type": "DATASET", "confidence": 0.8874621391296387}, {"text": "definition sentence identification task", "start_pos": 139, "end_pos": 178, "type": "TASK", "confidence": 0.8124023526906967}, {"text": "WCL (English Wikipedia) corpus", "start_pos": 186, "end_pos": 216, "type": "DATASET", "confidence": 0.6432649890581766}]}, {"text": "For most of the related research reviewed, we could neither obtain their source code nor the corpora used in their work, making comparative evaluation difficult.", "labels": [], "entities": []}, {"text": "To the best of our knowledge, is the only attempt to extract definitions from the ACL ARC corpus, which is a superset of our W00 corpus.", "labels": [], "entities": [{"text": "ACL ARC corpus", "start_pos": 82, "end_pos": 96, "type": "DATASET", "confidence": 0.9183607498804728}, {"text": "W00 corpus", "start_pos": 125, "end_pos": 135, "type": "DATASET", "confidence": 0.9401830434799194}]}, {"text": "It would be desirable to have a direct comparison with their work, but their evaluation method is mainly based on human judges and their reported coverage of 90% is only fora sample, shortlist of domain terms they defined in advance.", "labels": [], "entities": [{"text": "coverage", "start_pos": 146, "end_pos": 154, "type": "METRIC", "confidence": 0.7233404517173767}]}, {"text": "To directly compare with the previous, more complex state-of-the-art system from (Navigli and Velardi, 2010), we evaluate DefMiner on the definition sentence detection task.", "labels": [], "entities": [{"text": "definition sentence detection task", "start_pos": 138, "end_pos": 172, "type": "TASK", "confidence": 0.8952480405569077}]}, {"text": "For the sentence-level evaluation, we calculate the P/R/F 1 score based on whether the sentence is a definition sentence.", "labels": [], "entities": [{"text": "P/R/F 1 score", "start_pos": 52, "end_pos": 65, "type": "METRIC", "confidence": 0.9349578874451774}]}, {"text": "We applied DefMiner on their whole WCL annotated corpus, reporting results in.", "labels": [], "entities": [{"text": "WCL annotated corpus", "start_pos": 35, "end_pos": 55, "type": "DATASET", "confidence": 0.7082526683807373}]}, {"text": "We randomized the definition and none-definition sentences in their corpus and applied 10-folds cross validation.", "labels": [], "entities": []}, {"text": "In each each iteration we used 90% of the sentences for training and 10% for testing.", "labels": [], "entities": []}, {"text": "Compared to their results reported in (Navigli and  Velardi, 2010), DefMiner improves overall F 1 by 8%.", "labels": [], "entities": [{"text": "F 1", "start_pos": 94, "end_pos": 97, "type": "METRIC", "confidence": 0.9884822070598602}]}, {"text": "While certainly less precise (precision of 92% versus 99%), recall is improved over their considerably more complex WCL-3 algorithm by almost 20%.", "labels": [], "entities": [{"text": "precision", "start_pos": 30, "end_pos": 39, "type": "METRIC", "confidence": 0.9993650317192078}, {"text": "recall", "start_pos": 60, "end_pos": 66, "type": "METRIC", "confidence": 0.9993811845779419}]}, {"text": "Even using just the simple heuristic of only classifying sentences that have identified terms as well as definitions as definition sentences, DefMiner serves to competitively identify definition sentences.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: 10-fold cross validation word-level performance over different system configurations on our W00 corpus.", "labels": [], "entities": [{"text": "W00 corpus", "start_pos": 102, "end_pos": 112, "type": "DATASET", "confidence": 0.9611965119838715}]}, {"text": " Table 4: Comparative performance over the WCL.", "labels": [], "entities": []}]}