{"title": [{"text": "Improving Pivot-Based Statistical Machine Translation Using Random Walk", "labels": [], "entities": [{"text": "Improving Pivot-Based Statistical Machine Translation", "start_pos": 0, "end_pos": 53, "type": "TASK", "confidence": 0.8605888962745667}]}], "abstractContent": [{"text": "This paper proposes a novel approach that utilizes a machine learning method to improve pivot-based statistical machine translation (SMT).", "labels": [], "entities": [{"text": "statistical machine translation (SMT)", "start_pos": 100, "end_pos": 137, "type": "TASK", "confidence": 0.7880133837461472}]}, {"text": "For language pairs with few bilingual data, a possible solution in pivot-based SMT using another language as a \"bridge\" to generate source-target translation.", "labels": [], "entities": [{"text": "SMT", "start_pos": 79, "end_pos": 82, "type": "TASK", "confidence": 0.827361524105072}]}, {"text": "However, one of the weaknesses is that some useful source-target translations cannot be generated if the corresponding source phrase and target phrase connect to different pivot phrases.", "labels": [], "entities": []}, {"text": "To alleviate the problem, we utilize Markov random walks to connect possible translation phrases between source and target language.", "labels": [], "entities": []}, {"text": "Experimental results on European Parliament data, spoken language data and web data show that our method leads to significant improvements on all the tasks over the baseline system.", "labels": [], "entities": [{"text": "European Parliament data", "start_pos": 24, "end_pos": 48, "type": "DATASET", "confidence": 0.9007792472839355}]}], "introductionContent": [{"text": "Statistical machine translation (SMT) uses bilingual corpora to build translation models.", "labels": [], "entities": [{"text": "Statistical machine translation (SMT)", "start_pos": 0, "end_pos": 37, "type": "TASK", "confidence": 0.8186090687910715}]}, {"text": "The amount and the quality of the bilingual data strongly affect the performance of SMT systems.", "labels": [], "entities": [{"text": "SMT", "start_pos": 84, "end_pos": 87, "type": "TASK", "confidence": 0.9966334700584412}]}, {"text": "For resource-rich language pairs, such as ChineseEnglish, it is easy to collect large amounts of bilingual corpus.", "labels": [], "entities": []}, {"text": "However, for resource-poor language pairs, such as Chinese-Spanish, it is difficult to build a high-performance SMT system with the small scale bilingual data available.", "labels": [], "entities": [{"text": "SMT", "start_pos": 112, "end_pos": 115, "type": "TASK", "confidence": 0.9953521490097046}]}, {"text": "The pivot language approach, which performs translation through a third language, provides a possible solution to the problem.", "labels": [], "entities": []}, {"text": "The triangulation method () is a representative work for pivot-based machine translation.", "labels": [], "entities": [{"text": "pivot-based machine translation", "start_pos": 57, "end_pos": 88, "type": "TASK", "confidence": 0.6275465389092764}]}, {"text": "With a triangulation pivot approach, a source-target phrase table can be obtained by combining the source-pivot phrase table and the pivot-target phrase table.", "labels": [], "entities": []}, {"text": "However, one of the weaknesses is that some corresponding source and target phrase pairs cannot be generated, because they are connected to different pivot phrases (.", "labels": [], "entities": []}, {"text": "As illustrated in, since there is no direct translation between \"\u5f88 \u53ef\u53e3 henkekou\" and \"really delicious\", the triangulation method is unable to establish a relation between \"\u5f88\u53ef\u53e3 henkekou\" and the two Spanish phrases.", "labels": [], "entities": []}, {"text": "To solve this problem, we apply a Markov random walk method to pivot-based SMT system.", "labels": [], "entities": [{"text": "SMT", "start_pos": 75, "end_pos": 78, "type": "TASK", "confidence": 0.9537470936775208}]}, {"text": "Random walk has been widely used.", "labels": [], "entities": [{"text": "Random walk", "start_pos": 0, "end_pos": 11, "type": "TASK", "confidence": 0.6990370452404022}]}, {"text": "For example, used random walk to discover potential relations between queries and documents for link analysis in information retrieval.", "labels": [], "entities": [{"text": "link analysis", "start_pos": 96, "end_pos": 109, "type": "TASK", "confidence": 0.7037981748580933}, {"text": "information retrieval", "start_pos": 113, "end_pos": 134, "type": "TASK", "confidence": 0.7423510253429413}]}, {"text": "Analogous to link analysis, the aim of pivot-based translation is to discover potential translations between source and target language via the pivot language.", "labels": [], "entities": [{"text": "link analysis", "start_pos": 13, "end_pos": 26, "type": "TASK", "confidence": 0.7647414803504944}]}, {"text": "The goal of this paper is to extend the previous triangulation approach by exploring implicit translation relations using random walk method.", "labels": [], "entities": []}, {"text": "We evaluated our approach in several translation tasks, including translations between European languages; Chinese-Spanish spoken language translation and Chinese-Japanese translation with English as the pivot language.", "labels": [], "entities": [{"text": "Chinese-Spanish spoken language translation", "start_pos": 107, "end_pos": 150, "type": "TASK", "confidence": 0.6327238827943802}, {"text": "Chinese-Japanese translation", "start_pos": 155, "end_pos": 183, "type": "TASK", "confidence": 0.7351885437965393}]}, {"text": "Experimental results show that our approach achieves significant improvements over the conventional pivot-based method, triangulation method.", "labels": [], "entities": []}, {"text": "The remainder of this paper is organized as follows.", "labels": [], "entities": []}, {"text": "In section 2, we describe the related work.", "labels": [], "entities": []}, {"text": "We review the triangulation method for pivotbased machine translation in section 3.", "labels": [], "entities": [{"text": "pivotbased machine translation", "start_pos": 39, "end_pos": 69, "type": "TASK", "confidence": 0.6270476679007212}]}, {"text": "Section 4 describes the random walk models.", "labels": [], "entities": []}, {"text": "In section 5 and section 6, we describe the experiments and analyze the performance, respectively.", "labels": [], "entities": []}, {"text": "Section 7 gives a conclusion of the paper.", "labels": [], "entities": []}], "datasetContent": [{"text": "In our experiments, the word alignment was obtained by GIZA++) and the heuristics \"grow-diag-final\" refinement rule.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 24, "end_pos": 38, "type": "TASK", "confidence": 0.6379330456256866}, {"text": "GIZA", "start_pos": 55, "end_pos": 59, "type": "METRIC", "confidence": 0.7231485247612}]}, {"text": "Our translation system is an in-house phrase-based system using a log-linear framework including a phrase translation model, a language model, a lexicalized reordering model, a word penalty model and a phrase penalty model, which is analogous to Moses ().", "labels": [], "entities": [{"text": "phrase translation", "start_pos": 99, "end_pos": 117, "type": "TASK", "confidence": 0.7862363159656525}]}, {"text": "The baseline system is the triangulation method based pivot approach (Wu and.", "labels": [], "entities": []}, {"text": "To evaluate the translation quality, we used BLEU () as our evaluation metric.", "labels": [], "entities": [{"text": "translation", "start_pos": 16, "end_pos": 27, "type": "TASK", "confidence": 0.959385335445404}, {"text": "BLEU", "start_pos": 45, "end_pos": 49, "type": "METRIC", "confidence": 0.9985975623130798}]}, {"text": "The statistical significance using 95% confidence intervals were measured with paired bootstrap resampling).   project.", "labels": [], "entities": []}, {"text": "We build 180 pivot translation systems 6 The baseline system was built following the traditional triangulation pivot approach.", "labels": [], "entities": []}, {"text": "lists the results on Europarl training data.", "labels": [], "entities": [{"text": "Europarl training data", "start_pos": 21, "end_pos": 43, "type": "DATASET", "confidence": 0.9826320012410482}]}, {"text": "Limited by (including 90 baseline systems and 90 random walk based systems) using 10 source/target languages and 1 pivot language (English).", "labels": [], "entities": []}, {"text": "the length of the paper, we only show the results on WMT08, the tendency of the results on WMT06 and WMT07 is similar to WMT08.", "labels": [], "entities": [{"text": "WMT08", "start_pos": 53, "end_pos": 58, "type": "DATASET", "confidence": 0.9675042033195496}, {"text": "WMT06", "start_pos": 91, "end_pos": 96, "type": "DATASET", "confidence": 0.9660781025886536}, {"text": "WMT07", "start_pos": 101, "end_pos": 106, "type": "DATASET", "confidence": 0.9160687327384949}, {"text": "WMT08", "start_pos": 121, "end_pos": 126, "type": "DATASET", "confidence": 0.966809093952179}]}, {"text": "Several observations can be made from the table.", "labels": [], "entities": []}, {"text": "1. In all 90 language pairs, our method achieves general improvements over the baseline system.", "labels": [], "entities": []}, {"text": "2. Among 90 language pairs, random walk based approach is significantly better than the baseline system in 75 language pairs.", "labels": [], "entities": []}, {"text": "3. The improvements of our approach are not equal in different translation directions.", "labels": [], "entities": []}, {"text": "The improvement ranges from 0.06 (it-es) to 1.21 (pt-da).", "labels": [], "entities": []}, {"text": "One possible reason is that the performance is related with the source and target language.", "labels": [], "entities": []}, {"text": "For example, when using Finnish as the target language, the improvement is significant over the baseline.", "labels": [], "entities": []}, {"text": "This maybe caused by the great divergence between Uralic language (Finnish) and IndoEuropean language (the other European language in Table4).", "labels": [], "entities": []}, {"text": "From the table we can find that the translation between languages in different language family is worse than that in some language family.", "labels": [], "entities": []}, {"text": "But our random walk approach can im- prove the performance of translations between different language families.", "labels": [], "entities": []}, {"text": "In addition to using English as the pivot language, we also try some other languages as the pivot language.", "labels": [], "entities": []}, {"text": "In this sub-section, experiments were carried out from translating Portuguese to Swedish via different pivot languages.", "labels": [], "entities": []}, {"text": "summarizes the BLEU% scores of different pivot language when translating from Portuguese to Swedish.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 15, "end_pos": 19, "type": "METRIC", "confidence": 0.998683512210846}]}, {"text": "Similar to, our approach still achieves general improvements over the baseline system even if the pivot language has been changed.", "labels": [], "entities": []}, {"text": "From the table we can see that for most of the pivot language, the random walk based approach gains more than 1 BLEU score over the baseline.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 112, "end_pos": 122, "type": "METRIC", "confidence": 0.9825460612773895}]}, {"text": "But when using Finnish as the pivot language, the improvement is only 0.02 BLEU scores on WMT08.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 75, "end_pos": 79, "type": "METRIC", "confidence": 0.9978888630867004}, {"text": "WMT08", "start_pos": 90, "end_pos": 95, "type": "DATASET", "confidence": 0.9778919816017151}]}, {"text": "This phenomenon shows that the pivot language can also influence the performance of random walk approach.", "labels": [], "entities": []}, {"text": "One possible reason for the poor performance of using Finnish as the pivot language is that Finnish belongs to Uralic language family, and the other languages belong to Indo-European family.", "labels": [], "entities": []}, {"text": "The divergence between different language families led to a poor performance.", "labels": [], "entities": []}, {"text": "Thus how to select a best pivot language is our future work.", "labels": [], "entities": []}, {"text": "The problem with random walk is that it will lead to a larger phrase table with noises.", "labels": [], "entities": []}, {"text": "In this sub-section, a pre-pruning (before random walk) and a post-pruning (after random walk) method were introduced to deal with this problem.", "labels": [], "entities": []}, {"text": "We used a naive pruning method which selects the top N phrase pairs in the phrase table.", "labels": [], "entities": []}, {"text": "In our experiments, we set N to 20.", "labels": [], "entities": []}, {"text": "For pre-pruning, we prune the SP phrase table and PT phrase table before applying random walks.", "labels": [], "entities": [{"text": "PT", "start_pos": 50, "end_pos": 52, "type": "METRIC", "confidence": 0.9408536553382874}]}, {"text": "Post-pruning means that we prune the ST phrase table after random walks.", "labels": [], "entities": []}, {"text": "For the baseline system, we also apply a pruning method before combine the SP and PT phrase table.", "labels": [], "entities": [{"text": "PT", "start_pos": 82, "end_pos": 84, "type": "METRIC", "confidence": 0.9699416160583496}]}, {"text": "We test our pruning method on pt-ensv translation task.", "labels": [], "entities": [{"text": "pt-ensv translation", "start_pos": 30, "end_pos": 49, "type": "TASK", "confidence": 0.40276069939136505}]}, {"text": "With a pre-and post-pruning method, the random walk approach is able to achieve further improvements.", "labels": [], "entities": []}, {"text": "Our approach achieved BLEU scores of 25.11, 24.69 and 24.34 on WMT06, WMT07 and WMT08 respectively, which is much better than the baseline and the random walk approach with pruning.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 22, "end_pos": 26, "type": "METRIC", "confidence": 0.999785840511322}, {"text": "WMT06", "start_pos": 63, "end_pos": 68, "type": "DATASET", "confidence": 0.9856842756271362}, {"text": "WMT07", "start_pos": 70, "end_pos": 75, "type": "DATASET", "confidence": 0.9521105289459229}, {"text": "WMT08", "start_pos": 80, "end_pos": 85, "type": "DATASET", "confidence": 0.9507958292961121}]}, {"text": "Moreover, the size of the phrase table is about half of the no-pruning method.", "labels": [], "entities": []}, {"text": "When adopting a post-pruning method, the performance of translation did not improved significantly over the pre-pruning, but the scale of the phrase table dropped to 69M, which is only about 2 times larger than the triangulation method.", "labels": [], "entities": [{"text": "translation", "start_pos": 56, "end_pos": 67, "type": "TASK", "confidence": 0.9619405269622803}]}, {"text": "Phrase table pruning is a key work to improve the performance of random walk.", "labels": [], "entities": [{"text": "Phrase table pruning", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.8144399325052897}]}, {"text": "We plan to explore more approaches for phrase table pruning.", "labels": [], "entities": [{"text": "phrase table pruning", "start_pos": 39, "end_pos": 59, "type": "TASK", "confidence": 0.9004199504852295}]}, {"text": "E.g. using significance test or monolingual key phrases (  The European languages show various degrees of similarity to one another.", "labels": [], "entities": []}, {"text": "In this sub-section, we consider translation from Chinese to Spanish with English as the pivot language.", "labels": [], "entities": [{"text": "translation from Chinese", "start_pos": 33, "end_pos": 57, "type": "TASK", "confidence": 0.8719379504521688}]}, {"text": "Chinese belongs to Sino-Tibetan Languages and English/Spanish belongs to Indo-European Languages, the gap between two languages is wide.", "labels": [], "entities": []}, {"text": "A pivot task was included in IWSLT 2008 in which the participants need to translate Chinese to Spanish via English.", "labels": [], "entities": [{"text": "IWSLT 2008", "start_pos": 29, "end_pos": 39, "type": "DATASET", "confidence": 0.8849455118179321}]}, {"text": "A Chinese-English and an English-Spanish data were supplied to carryout the experiments.", "labels": [], "entities": []}, {"text": "The entire training corpus was tokenized and lowercased.", "labels": [], "entities": []}, {"text": "summarize the training data and test data.", "labels": [], "entities": []}, {"text": "shows the similar tendency with  The setting with Europarl data is quite artificial as the training data for directly translating between source and target actually exists in the original data sets.", "labels": [], "entities": [{"text": "Europarl data", "start_pos": 50, "end_pos": 63, "type": "DATASET", "confidence": 0.9868008196353912}]}, {"text": "The IWSLT data set is too small to represent the real scenario.", "labels": [], "entities": [{"text": "IWSLT data set", "start_pos": 4, "end_pos": 18, "type": "DATASET", "confidence": 0.9284765720367432}]}, {"text": "Thus we try our experiment on a more realistic scenario: translating from Chinese to Japanese via English with web crawled data.", "labels": [], "entities": [{"text": "translating from Chinese to Japanese", "start_pos": 57, "end_pos": 93, "type": "TASK", "confidence": 0.8891663432121277}]}, {"text": "All the training data were crawled on the web.", "labels": [], "entities": []}, {"text": "The scale of Chinese-English and EnglishJapanese is 10 million respectively.", "labels": [], "entities": []}, {"text": "The test set was builtin house with 1,000 sentences and 4 references.", "labels": [], "entities": []}, {"text": "lists the results on web data.", "labels": [], "entities": []}, {"text": "From the table we can find that the random walk model can achieve an absolute improvement of 0.75 percentages points on BLEU over the baseline.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 120, "end_pos": 124, "type": "METRIC", "confidence": 0.9985634684562683}]}], "tableCaptions": [{"text": " Table 4.  The random walk models achieved BLEU% scores  32.09, which achieved an absolute improvement of  2.08 percentages points on BLEU over the base- line.", "labels": [], "entities": [{"text": "BLEU% scores", "start_pos": 43, "end_pos": 55, "type": "METRIC", "confidence": 0.9802614053090414}, {"text": "BLEU", "start_pos": 134, "end_pos": 138, "type": "METRIC", "confidence": 0.9979432225227356}]}, {"text": " Table 7: Training Data of IWSLT2008", "labels": [], "entities": [{"text": "Training Data of", "start_pos": 10, "end_pos": 26, "type": "DATASET", "confidence": 0.7860666314760844}, {"text": "IWSLT2008", "start_pos": 27, "end_pos": 36, "type": "DATASET", "confidence": 0.7578686475753784}]}]}