{"title": [{"text": "A Semantically Enhanced Approach to Determine Textual Similarity", "labels": [], "entities": [{"text": "Similarity", "start_pos": 54, "end_pos": 64, "type": "TASK", "confidence": 0.6252817511558533}]}], "abstractContent": [{"text": "This paper presents a novel approach to determine textual similarity.", "labels": [], "entities": [{"text": "determine textual similarity", "start_pos": 40, "end_pos": 68, "type": "TASK", "confidence": 0.66009920835495}]}, {"text": "A layered methodology to transform text into logic forms is proposed , and semantic features are derived from a logic prover.", "labels": [], "entities": []}, {"text": "Experimental results show that incorporating the semantic structure of sentences is beneficial.", "labels": [], "entities": []}, {"text": "When training data is unavailable, scores obtained from the logic prover in an unsupervised manner outperform supervised methods.", "labels": [], "entities": []}], "introductionContent": [{"text": "The task of Semantic Textual Similarity () measures the degree of semantic equivalence between two sentences.", "labels": [], "entities": [{"text": "Semantic Textual Similarity", "start_pos": 12, "end_pos": 39, "type": "TASK", "confidence": 0.7933637301127116}]}, {"text": "Unlike textual entailment (, textual similarity is symmetric, and unlike both textual entailment and paraphrasing), textual similarity is modeled using a graded score rather than a binary decision.", "labels": [], "entities": []}, {"text": "For example, sentence pair (1) below is very similar [5 out of 5], (2) is somewhat similar [3 out of 5] and is not similar at all [0 out of 5]: 1.", "labels": [], "entities": []}, {"text": "Someone is removing the scales from the fish.", "labels": [], "entities": []}, {"text": "A person is descaling a fish.", "labels": [], "entities": []}, {"text": "2. A woman is chopping an herb.", "labels": [], "entities": []}, {"text": "A man is finely chopping a green substance.", "labels": [], "entities": []}, {"text": "3. A cat is playing with a watermelon on a floor.", "labels": [], "entities": []}, {"text": "A man is pouring oil into a pan.", "labels": [], "entities": []}, {"text": "State-of-the-art systems to determine textual similarity) do not account for the semantic structure of sentences, and mostly rely on word pairings and knowledge derived from large corpora, e.g., Wikipedia.", "labels": [], "entities": []}, {"text": "Regardless of details, each word in sent 1 is paired with the word in sent 2 that is most similar according to some similarity measure.", "labels": [], "entities": []}, {"text": "Then, all similarities are added and normalized by the length of sent 1 to obtain the similarity score from sent 1 to sent 2 . The process is repeated to obtain the similarity score from sent 2 to sent 1 , and both scores are then averaged to determine the overall textual similarity.", "labels": [], "entities": [{"text": "similarity score", "start_pos": 86, "end_pos": 102, "type": "METRIC", "confidence": 0.9591516256332397}]}, {"text": "Several word-to-word similarity measures are often combined with other shallow features, e.g., n-gram overlap, syntactic dependencies, to obtain the final similarity score.", "labels": [], "entities": []}, {"text": "Consider sentences 1(a) A man is holding a leaf and 1(b) A monkey is fighting a man.", "labels": [], "entities": []}, {"text": "These two sentences are very dissimilar, the only commonality is the concept 'man'.", "labels": [], "entities": []}, {"text": "Any approach that blindly searches for the word in 1(b) that is the most similar to word 'man' in 1(a) will find 'man' from 1(b) to be a perfect match.", "labels": [], "entities": []}, {"text": "One of three content words is a match and thus the estimated similarity will be much higher than it actually is.", "labels": [], "entities": []}, {"text": "Consider now the semantic representations for sentences 1(a) and 1(b) in.", "labels": [], "entities": []}, {"text": "'man' plays the role of AGENT in 1(a), and THEME in 1(b).", "labels": [], "entities": [{"text": "AGENT", "start_pos": 24, "end_pos": 29, "type": "METRIC", "confidence": 0.9969539642333984}, {"text": "THEME", "start_pos": 43, "end_pos": 48, "type": "METRIC", "confidence": 0.9964209794998169}]}, {"text": "While in both sentences the word 'man' encodes the same concept, their semantic functions with respect to other concepts are different.", "labels": [], "entities": []}, {"text": "Intuitively, it seems reasonable to penalize the similarity score based on the role discrepancy.", "labels": [], "entities": [{"text": "similarity score", "start_pos": 49, "end_pos": 65, "type": "METRIC", "confidence": 0.9835169911384583}]}, {"text": "Figure 2: Semantic representations of 2(a) The man used a sword to slice a plastic bottle, 2(b) A man sliced a plastic bottle with a sword, 2(c) A woman is applying cosmetics to her face, 2(d) A woman is putting on makeup, 2(e) A woman is dancing in the rain, and 2(f) A woman dances in the rain outside.", "labels": [], "entities": []}, {"text": "Pairs (a, b), (c, d) and (e, f) are highly similar even though concepts and relations only match partially.", "labels": [], "entities": []}, {"text": "This paper proposes a novel approach to determine textual similarity.", "labels": [], "entities": [{"text": "determine textual similarity", "start_pos": 40, "end_pos": 68, "type": "TASK", "confidence": 0.6574968496958414}]}, {"text": "Semantic representations of sentences are exploited, syntactic features omitted and the only external resource used in WordNet.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 119, "end_pos": 126, "type": "DATASET", "confidence": 0.9598215222358704}]}, {"text": "The main novelties of our approach are: it (1) derives semantic features from a logic prover to be used in a machine learning framework; (2) uses three logic form transformations capturing different levels of knowledge; and (3) incorporates semantic representations extracted automatically.", "labels": [], "entities": []}], "datasetContent": [{"text": "Logic forms are derived from the output of stateof-the-art NLP tools developed previously and not tuned in anyway to the current task or corpora.", "labels": [], "entities": []}, {"text": "Our approach is not tied to any tool, set of named entities or relations.", "labels": [], "entities": []}, {"text": "Any other semantic representation could be used; the only required modification would be the LFT component) so that it accounts for the subtleties of the representation of choice.", "labels": [], "entities": []}, {"text": "The named entity recognizer extracts 35 finegrained types organized in a taxonomy (date, language, city, instrument, etc.) and was first developed fora question answering system ().", "labels": [], "entities": [{"text": "question answering", "start_pos": 152, "end_pos": 170, "type": "TASK", "confidence": 0.8349914848804474}]}, {"text": "The implementation uses publicly available gazetteers as well as machine learning.", "labels": [], "entities": []}, {"text": "Semantic relations are extracted with Polaris (), a semantic parser that given text extracts semantic relations.", "labels": [], "entities": []}, {"text": "Polaris is trained using FrameNet (), PropBank (), NomBank (), several SemEval corpora ( and in-house annotations.", "labels": [], "entities": [{"text": "Polaris", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.9408087134361267}, {"text": "PropBank", "start_pos": 38, "end_pos": 46, "type": "DATASET", "confidence": 0.938569962978363}]}], "tableCaptions": [{"text": " Table 3: Two logic forms and output of logic prover in both directions. For each predicate type (n, v, m, o, ne, sr)  and semantic relation type (AGENT, LOCATION, etc.) features indicate the total number of predicates, the number of  predicates dropped until a proof is found and ratio of the two counts ( t , d and r respectively). We omit the features for  predicate O and individual semantic relations because of space constraints.", "labels": [], "entities": [{"text": "AGENT", "start_pos": 147, "end_pos": 152, "type": "METRIC", "confidence": 0.980530858039856}, {"text": "LOCATION", "start_pos": 154, "end_pos": 162, "type": "METRIC", "confidence": 0.7249934673309326}]}, {"text": " Table 5: Correlations obtained with the test split using our approach (not dropping and dropping unbound predicates),  and results obtained by the top-3 performers and teams that included in their models features derived from the semantic  structure of sentences. Statistically significant differences in performance between our systems and LFT score Basic  not dropping unbound predicates are indicated with  *  (confidence 99%) and  \u2020 (confidence 95%).", "labels": [], "entities": []}]}