{"title": [{"text": "Exploiting language models for visual recognition", "labels": [], "entities": [{"text": "visual recognition", "start_pos": 31, "end_pos": 49, "type": "TASK", "confidence": 0.7858791351318359}]}], "abstractContent": [{"text": "The problem of learning language models from large text corpora has been widely studied within the computational linguistic community.", "labels": [], "entities": []}, {"text": "However, little is known about the performance of these language models when applied to the computer vision domain.", "labels": [], "entities": []}, {"text": "In this work, we compare representative models: a window-based model, a topic model, a distri-butional memory and a commonsense knowledge database, ConceptNet, in two visual recognition scenarios: human action recognition and object prediction.", "labels": [], "entities": [{"text": "human action recognition", "start_pos": 197, "end_pos": 221, "type": "TASK", "confidence": 0.6367323199907938}, {"text": "object prediction", "start_pos": 226, "end_pos": 243, "type": "TASK", "confidence": 0.7803606986999512}]}, {"text": "We examine whether the knowledge extracted from texts through these models are compatible to the knowledge represented in images.", "labels": [], "entities": []}, {"text": "We determine the usefulness of different language models in aiding the two visual recognition tasks.", "labels": [], "entities": [{"text": "visual recognition", "start_pos": 75, "end_pos": 93, "type": "TASK", "confidence": 0.7654384076595306}]}, {"text": "The study shows that the language models built from general text corpora can be used instead of expensive annotated images and even outperform the image model when testing on a big general dataset.", "labels": [], "entities": []}], "introductionContent": [{"text": "Computational linguistics have created many tools for automatic knowledge acquisition which have been successfully applied in many tasks inside the language domain, such as question answering, machine translation, semantic web, etc.", "labels": [], "entities": [{"text": "automatic knowledge acquisition", "start_pos": 54, "end_pos": 85, "type": "TASK", "confidence": 0.678994377454122}, {"text": "question answering", "start_pos": 173, "end_pos": 191, "type": "TASK", "confidence": 0.8578495681285858}, {"text": "machine translation", "start_pos": 193, "end_pos": 212, "type": "TASK", "confidence": 0.7771338820457458}]}, {"text": "In this paper we ask whether such knowledge generalizes to the observed reality outside the language domain, where we use well-known image datasets as a proxy for observed reality.", "labels": [], "entities": []}, {"text": "In particular, we aim to determine which language model yields knowledge that is most suitable for use in Computer Vision.", "labels": [], "entities": [{"text": "Computer Vision", "start_pos": 106, "end_pos": 121, "type": "TASK", "confidence": 0.8722155392169952}]}, {"text": "Therefore we test a variety of language models and a linguistically mined knowledge base within two computer vision scenarios: Human action recognition : Recognizing <subject, verb, object> triples based on objects (e.g., car, horse) and scenes (the place that the actions occur, e.g., countryside, forest, office) recognized in images.", "labels": [], "entities": [{"text": "Human action recognition", "start_pos": 127, "end_pos": 151, "type": "TASK", "confidence": 0.6561218102773031}, {"text": "Recognizing <subject, verb, object> triples based on objects (e.g., car, horse) and scenes (the place that the actions occur, e.g., countryside, forest, office) recognized in images", "start_pos": 154, "end_pos": 335, "type": "Description", "confidence": 0.7410775154829026}]}, {"text": "In this scenario, we only consider images with human actions so the \"human\" subject is always present.", "labels": [], "entities": []}, {"text": "Objects in context : Predicting the most likely identity of an object given its context as expressed in terms of co-occurring objects.", "labels": [], "entities": []}, {"text": "Computer vision can greatly benefit from natural language processing as learning from images requires a prohibitively expensive annotation effort.", "labels": [], "entities": [{"text": "Computer vision", "start_pos": 0, "end_pos": 15, "type": "TASK", "confidence": 0.7349894642829895}]}, {"text": "A major goal of natural language processing is to obtain general knowledge from text and in this paper we test which model provides the best knowledge for use in the visual domain.", "labels": [], "entities": [{"text": "natural language processing", "start_pos": 16, "end_pos": 43, "type": "TASK", "confidence": 0.6695238947868347}]}, {"text": "Within the two visual scenarios, we compare three state-of-the-art language models and a knowledge base: (1) A window-based model, which counts co-occurrence frequencies within a fixed window; (2) R-LDA, an extension of LDA that enables generation of joint probabilities; (3) TypeDM (, a strong Distributional Memory model; (4) ConceptNet, an automatically generated semantic graph containing concepts with their relations.", "labels": [], "entities": []}, {"text": "We test the language models in two ways: (1) We directly compare the statistics of the linguistic models with statistics extracted from the visual domain.", "labels": [], "entities": []}, {"text": "(2) We compare the linguistic models inside the two computer vision applications, leading to a direct estimation of their usefulness.", "labels": [], "entities": []}, {"text": "To summarize, our main research questions are: (1) Is the knowledge from language compatible with the knowledge from vision?", "labels": [], "entities": []}, {"text": "(2) Can the knowledge extracted from language help in computer vision scenarios?", "labels": [], "entities": []}], "datasetContent": [{"text": "For the objects in context scenario, we use the SUN object dataset (), which contains more than 16 thousand images, more than 79,000 objects whose locations are annotated using polygons.", "labels": [], "entities": [{"text": "SUN object dataset", "start_pos": 48, "end_pos": 66, "type": "DATASET", "confidence": 0.6346916755040487}]}, {"text": "The dataset has been annotated by various people who could choose their own object categories, leading to duplicate categories such as \"building\" and \"buildings\", \"person\" and \"person walking\".", "labels": [], "entities": []}, {"text": "Furthermore, for some images large parts are not annotated leading to an incomplete context.", "labels": [], "entities": []}, {"text": "We therefore cleaned the object categories (mapping from around 7,500 objects to over 700 unique object categories) and considered only images whose content was sufficiently annotated.", "labels": [], "entities": []}, {"text": "In our experiments, we used the predefined training and testing parts of the SUN dataset and obtained around 4,500 images for learning the object relations and 10,600 images for testing the object prediction.", "labels": [], "entities": [{"text": "SUN dataset", "start_pos": 77, "end_pos": 88, "type": "DATASET", "confidence": 0.8831601738929749}, {"text": "object prediction", "start_pos": 190, "end_pos": 207, "type": "TASK", "confidence": 0.7429706156253815}]}, {"text": "We obtain conditional probabilities P (o j |o i ) from frequency counts.", "labels": [], "entities": []}, {"text": "In this section we want to answer our two main research questions: (1) Is knowledge from language compatible with knowledge from vision?", "labels": [], "entities": []}, {"text": "(2) Can we use knowledge extracted from language in computer vision scenarios?", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Random R-LDA topics with the relations between Noun-Verb (first 2 columns) and between Noun-Noun (last  2 columns)", "labels": [], "entities": []}, {"text": " Table 4: X 2 distance for relations between verbs, objects,  scenes from different language models to image data", "labels": [], "entities": []}, {"text": " Table 5: Average rank over all images AR I of the human  action recognition using different settings: O gs , O rec  use only objects (gold standard and object recognizer);  S gs , S rec use only scenes, O gs S gs and O rec S rec integrate  both objects and scenes together", "labels": [], "entities": [{"text": "AR I", "start_pos": 39, "end_pos": 43, "type": "METRIC", "confidence": 0.9459612369537354}, {"text": "human  action recognition", "start_pos": 51, "end_pos": 76, "type": "TASK", "confidence": 0.6600488622983297}]}, {"text": " Table 6: Average rank over all images vs. actions of the  human action recognition using the O rec S rec setting", "labels": [], "entities": [{"text": "Average rank", "start_pos": 10, "end_pos": 22, "type": "METRIC", "confidence": 0.9574146270751953}, {"text": "human action recognition", "start_pos": 59, "end_pos": 83, "type": "TASK", "confidence": 0.7527661323547363}]}]}