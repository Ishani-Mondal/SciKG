{"title": [{"text": "Harvesting Parallel News Streams to Generate Paraphrases of Event Relations", "labels": [], "entities": [{"text": "Harvesting Parallel News Streams", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.8495461344718933}]}], "abstractContent": [{"text": "The distributional hypothesis, which states that words that occur in similar contexts tend to have similar meanings, has inspired several Web mining algorithms for paraphrasing semantically equivalent phrases.", "labels": [], "entities": []}, {"text": "Unfortunately , these methods have several drawbacks, such as confusing synonyms with antonyms and causes with effects.", "labels": [], "entities": []}, {"text": "This paper introduces three Temporal Correspondence Heuris-tics, that characterize regularities in parallel news streams, and shows how they maybe used to generate high precision paraphrases for event relations.", "labels": [], "entities": []}, {"text": "We encode the heuristics in a probabilistic graphical model to create the NEWSSPIKE algorithm for mining news streams.", "labels": [], "entities": []}, {"text": "We present experiments demonstrating that NEWSSPIKE significantly outper-forms several competitive baselines.", "labels": [], "entities": [{"text": "NEWSSPIKE", "start_pos": 42, "end_pos": 51, "type": "TASK", "confidence": 0.7401576042175293}]}, {"text": "In order to spur further research, we provide a large annotated corpus of timestamped news articles as well as the paraphrases produced by NEWSSPIKE.", "labels": [], "entities": [{"text": "NEWSSPIKE", "start_pos": 139, "end_pos": 148, "type": "DATASET", "confidence": 0.9516782760620117}]}], "introductionContent": [{"text": "Paraphrasing, the task of finding sets of semantically equivalent surface forms, is crucial to many natural language processing applications, including relation extraction, question answering, summarization () and machine translation).", "labels": [], "entities": [{"text": "relation extraction", "start_pos": 152, "end_pos": 171, "type": "TASK", "confidence": 0.8751401901245117}, {"text": "question answering", "start_pos": 173, "end_pos": 191, "type": "TASK", "confidence": 0.9071028530597687}, {"text": "summarization", "start_pos": 193, "end_pos": 206, "type": "TASK", "confidence": 0.9821731448173523}, {"text": "machine translation", "start_pos": 214, "end_pos": 233, "type": "TASK", "confidence": 0.799207329750061}]}, {"text": "While the benefits of paraphrasing have been demonstrated, creating a large-scale corpus of high precision paraphrases remains a challenge -especially for event relations.", "labels": [], "entities": [{"text": "paraphrasing", "start_pos": 22, "end_pos": 34, "type": "TASK", "confidence": 0.9659915566444397}, {"text": "event relations", "start_pos": 155, "end_pos": 170, "type": "TASK", "confidence": 0.763519287109375}]}, {"text": "Many researchers have considered generating paraphrases by mining the Web guided by the distributional hypothesis, which states that words occurring in similar contexts tend to have similar meanings.", "labels": [], "entities": []}, {"text": "For example, DIRT) and Resolver () identify synonymous relation phrases by the distributions of their arguments.", "labels": [], "entities": []}, {"text": "However, the distributional hypothesis has several drawbacks.", "labels": [], "entities": []}, {"text": "First, it can confuse antonyms with synonyms because antonymous phrases appear in similar contexts as often as synonymous phrases.", "labels": [], "entities": []}, {"text": "For the same reasons, it also often confuses causes with effects.", "labels": [], "entities": []}, {"text": "For example, DIRT reports that the closest phrase to fall is rise, and the closest phrase to shoot is kill.", "labels": [], "entities": [{"text": "DIRT", "start_pos": 13, "end_pos": 17, "type": "DATASET", "confidence": 0.8013628721237183}]}, {"text": "1 Second, the distributional hypothesis relies on statistics overlarge corpora to produce accurate similarity statistics.", "labels": [], "entities": []}, {"text": "It remains unclear how to accurately paraphrase less frequent relations with the distributional hypothesis.", "labels": [], "entities": []}, {"text": "Another common approach employs the use of parallel corpora.", "labels": [], "entities": []}, {"text": "News articles are an interesting target, because there often exist articles from different sources describing the same daily events.", "labels": [], "entities": []}, {"text": "This peculiar property allows the use of the temporal assumption, which assumes that phrases in articles published at the same time tend to have similar meanings.", "labels": [], "entities": []}, {"text": "For example, the approaches by and identify pairs of sentential paraphrases in similar articles that have appeared in the same period of time.", "labels": [], "entities": []}, {"text": "While these approaches use temporal information as a coarse filter in the data generation stage, they still largely rely on text metrics in the prediction stage.", "labels": [], "entities": []}, {"text": "This not only reduces precision, but also limits the discovery of paraphrases with dissimilar sur-face strings.", "labels": [], "entities": [{"text": "precision", "start_pos": 22, "end_pos": 31, "type": "METRIC", "confidence": 0.9987302422523499}]}, {"text": "The goal of our research is to develop a technique to generate paraphrases for large numbers of event relation with high precision, using only minimal human effort.", "labels": [], "entities": [{"text": "precision", "start_pos": 121, "end_pos": 130, "type": "METRIC", "confidence": 0.9899253845214844}]}, {"text": "The key to our approach is a joint cluster model using the temporal attributes of news streams, which allows us to identify semantic equivalence of event relation phrases with greater precision.", "labels": [], "entities": [{"text": "precision", "start_pos": 184, "end_pos": 193, "type": "METRIC", "confidence": 0.9899128675460815}]}, {"text": "In summary, this paper makes the following contributions: \u2022 We formulate a set of three temporal correspondence heuristics that characterize regularities over parallel news streams.", "labels": [], "entities": []}, {"text": "\u2022 We develop a novel program, NEWSSPIKE, based on a probabilistic graphical model that jointly encodes these heuristics.", "labels": [], "entities": [{"text": "NEWSSPIKE", "start_pos": 30, "end_pos": 39, "type": "DATASET", "confidence": 0.7301461100578308}]}, {"text": "We present inference and learning algorithms for our model.", "labels": [], "entities": []}, {"text": "\u2022 We present a series of detailed experiments demonstrating that NEWSSPIKE outperforms several competitive baselines, and show through ablation tests how each of the temporal heuristics affects performance.", "labels": [], "entities": []}, {"text": "\u2022 To spur further research on this topic, we provide both our generated paraphrase clusters and a corpus of 0.5M time-stamped news articles 2 , collected over a period of about 50 days from hundreds of news sources.", "labels": [], "entities": []}], "datasetContent": [{"text": "Since we were unable to find any elaborate timestamped, parallel, news corpus, we collected data using the following procedure: \u2022 Collect RSS news seeds, which contain the title, time-stamp, and abstract of the news items.", "labels": [], "entities": []}, {"text": "\u2022 Use these titles to query the Bing news search engine API and collect additional time-stamped news articles.", "labels": [], "entities": []}, {"text": "\u2022 Strip HTML tags from the news articles using Boilerpipe; keep only the title and first paragraph of each article.", "labels": [], "entities": [{"text": "Boilerpipe", "start_pos": 47, "end_pos": 57, "type": "DATASET", "confidence": 0.9350875020027161}]}, {"text": "\u2022 Extract shallow relation tuples using the OpenIE system).", "labels": [], "entities": []}, {"text": "We performed these steps everyday from January 1 to.", "labels": [], "entities": []}, {"text": "In total, we collected 546,713 news articles, for which 2.6 million extractions had 529 thousand unique relations.", "labels": [], "entities": []}, {"text": "We used several types of features for paraphrasing: 1) spike features obtained from time series; 2) tense features, such as whether two relation phrases are both in the present tense; 3) cause-effect features, such as whether two relation phrases often appear successively in the news articles; 4) text features, such as whether sentences are similar; 5) syntactic features, such as whether a relation phrase appears in a clausal complement; and 6) semantic features, such as whether a relation phrase contains negative words.", "labels": [], "entities": []}, {"text": "Text and semantic features are encoded using the relation factors of section 4.3.2.", "labels": [], "entities": []}, {"text": "For example, in, the factor \u03a6 Y 2 includes the textual similarity between the sentences containing the phrases \"step down\" and \"be chairman of\" respectively; it also includes the feature that the tense of \"step down\" (present) is different from the tense of \"be chairman of\" (past).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Comparison with methods using parallel news  corpora", "labels": [], "entities": []}, {"text": " Table 2: Comparison with methods using the distribu- tional hypothesis", "labels": [], "entities": []}]}