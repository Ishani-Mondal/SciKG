{"title": [{"text": "Scaling Semantic Parsers with On-the-fly Ontology Matching", "labels": [], "entities": [{"text": "Scaling Semantic Parsers", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.8318827549616495}, {"text": "Matching", "start_pos": 50, "end_pos": 58, "type": "TASK", "confidence": 0.7400179505348206}]}], "abstractContent": [{"text": "We consider the challenge of learning semantic parsers that scale to large, open-domain problems, such as question answering with Freebase.", "labels": [], "entities": [{"text": "learning semantic parsers", "start_pos": 29, "end_pos": 54, "type": "TASK", "confidence": 0.640692929426829}, {"text": "question answering", "start_pos": 106, "end_pos": 124, "type": "TASK", "confidence": 0.8261179327964783}]}, {"text": "In such settings, the sentences cover a wide variety of topics and include many phrases whose meaning is difficult to represent in a fixed target ontology.", "labels": [], "entities": []}, {"text": "For example , even simple phrases such as 'daugh-ter' and 'number of people living in' cannot be directly represented in Freebase, whose ontology instead encodes facts about gender , parenthood, and population.", "labels": [], "entities": [{"text": "Freebase", "start_pos": 121, "end_pos": 129, "type": "DATASET", "confidence": 0.9388279318809509}]}, {"text": "In this paper , we introduce anew semantic parsing approach that learns to resolve such ontological mismatches.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 34, "end_pos": 50, "type": "TASK", "confidence": 0.7152100503444672}]}, {"text": "The parser is learned from question-answer pairs, uses a probabilistic CCG to build linguistically motivated logical-form meaning representations, and includes an ontology matching model that adapts the output logical forms for each target ontology.", "labels": [], "entities": []}, {"text": "Experiments demonstrate state-of-the-art performance on two benchmark semantic parsing datasets, including a nine point accuracy improvement on a recent Freebase QA corpus.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 70, "end_pos": 86, "type": "TASK", "confidence": 0.7424210608005524}, {"text": "accuracy", "start_pos": 120, "end_pos": 128, "type": "METRIC", "confidence": 0.8624742031097412}, {"text": "Freebase QA corpus", "start_pos": 153, "end_pos": 171, "type": "DATASET", "confidence": 0.8717409571011862}]}], "introductionContent": [{"text": "Semantic parsers map sentences to formal representations of their underlying meaning.", "labels": [], "entities": []}, {"text": "Recently, algorithms have been developed to learn such parsers for many applications, including question answering (QA) (), relation extraction (, robot control (, interpreting instructions (, and generating programs.", "labels": [], "entities": [{"text": "question answering (QA)", "start_pos": 96, "end_pos": 119, "type": "TASK", "confidence": 0.8495548248291016}, {"text": "relation extraction", "start_pos": 124, "end_pos": 143, "type": "TASK", "confidence": 0.8821682929992676}]}, {"text": "In each case, the parser uses a predefined set of logical constants, or an ontology, to construct meaning representations.", "labels": [], "entities": []}, {"text": "In practice, the choice of ontology significantly impacts learning.", "labels": [], "entities": []}, {"text": "For example, consider the following questions (Q) and candidate meaning representations (MR): Q1: What is the population of Seattle?", "labels": [], "entities": [{"text": "MR", "start_pos": 89, "end_pos": 91, "type": "METRIC", "confidence": 0.8643316030502319}]}, {"text": "Q2: How many people live in Seattle?", "labels": [], "entities": []}, {"text": "MR1: \u03bbx.population(Seattle, x) MR2: count(\u03bbx.person(x) \u2227 live(x, Seattle)) A semantic parser might aim to construct MR1 for Q1 and MR2 for Q2; these pairings align constants (count, person, etc.) directly to phrases ('How many,' 'people,' etc.).", "labels": [], "entities": []}, {"text": "Unfortunately, few ontologies have sufficient coverage to support both meaning representations, for example many QA databases would only include the population relation required for MR1.", "labels": [], "entities": [{"text": "MR1", "start_pos": 182, "end_pos": 185, "type": "TASK", "confidence": 0.7924780249595642}]}, {"text": "Most existing approaches would, given this deficiency, simply aim to produce MR1 for Q2, thereby introducing significant lexical ambiguity that complicates learning.", "labels": [], "entities": [{"text": "MR1", "start_pos": 77, "end_pos": 80, "type": "TASK", "confidence": 0.7427972555160522}]}, {"text": "Such ontological mismatches become increasingly common as domain and language complexity increases.", "labels": [], "entities": []}, {"text": "In this paper, we introduce a semantic parsing approach that supports scalable, open-domain ontological reasoning.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 30, "end_pos": 46, "type": "TASK", "confidence": 0.7410516142845154}]}, {"text": "The parser first constructs a linguistically motivated domain-independent meaning representation.", "labels": [], "entities": []}, {"text": "For example, possibly producing MR1 for Q1 and MR2 for Q2 above.", "labels": [], "entities": [{"text": "MR1", "start_pos": 32, "end_pos": 35, "type": "METRIC", "confidence": 0.9487906098365784}]}, {"text": "It then uses a learned ontology matching model to transform this represen- How many people visit the public library of New York annually l0 : \u03bbx.eq(x, count(\u03bby.people(y) \u2227 \u2203e.visit(y, \u03b9z.public(z) \u2227 library(z) \u2227 of (z, new york), e) \u2227 annually(e))) y : \u03bbx.library.public library system.annual visits(x, new york public library) a : 13,554,002 x : What works did Mozart dedicate to Joseph Haydn l0 : \u03bbx.works(x) \u2227 \u2203e.dedicate(mozart, x, e) \u2227 to(haydn, e))) y : \u03bbx.dedicated work(x) \u2227 \u2203e.dedicated by(mozart, e) \u2227 dedication(x, e) \u2227 dedicated to(haydn, e))) a : {: Examples of sentences x, domain-independent underspecified logical forms l 0 , fully specified logical forms y, and answers a drawn from the Freebase domain.", "labels": [], "entities": []}, {"text": "tation for the target domain.", "labels": [], "entities": []}, {"text": "In our example, producing either MR1, MR2 or another more appropriate option, depending on the QA database schema.", "labels": [], "entities": []}, {"text": "This two stage approach enables parsing without any domain-dependent lexicon that pairs words with logical constants.", "labels": [], "entities": []}, {"text": "Instead, word meaning is filled in on-the-fly through ontology matching, enabling the parser to infer the meaning of previously unseen words and more easily transfer across domains.", "labels": [], "entities": []}, {"text": "shows the desired outputs for two example Freebase sentences.", "labels": [], "entities": []}, {"text": "The first parsing stage uses a probabilistic combinatory categorial grammar (CCG) to map sentences to new, underspecified logical-form meaning representations containing generic logical constants that are not tied to any specific ontology.", "labels": [], "entities": []}, {"text": "This approach enables us to share grammar structure across domains, instead of repeatedly re-learning different grammars for each target ontology.", "labels": [], "entities": []}, {"text": "The ontology-matching step considers a large number of type-equivalent domain-specific meanings.", "labels": [], "entities": []}, {"text": "It enables us to incorporate a number of cues, including the target ontology structure and lexical similarity between the names of the domain-independent and dependent constants, to construct the final logical forms.", "labels": [], "entities": []}, {"text": "During learning, we estimate a linear model over derivations that include all of the CCG parsing decisions and the choices for ontology matching.", "labels": [], "entities": [{"text": "CCG parsing", "start_pos": 85, "end_pos": 96, "type": "TASK", "confidence": 0.6664180159568787}, {"text": "ontology matching", "start_pos": 127, "end_pos": 144, "type": "TASK", "confidence": 0.7816995978355408}]}, {"text": "Following a number of recent approaches), we treat all intermediate decisions as latent and learn from data containing only easily gathered question answer pairs.", "labels": [], "entities": []}, {"text": "This approach aligns naturally with our two-stage parsing setup, where the final logical expression can be directly used to provide answers.", "labels": [], "entities": []}, {"text": "We report performance on two benchmark datasets: GeoQuery (Zelle and Mooney, 1996) and Freebase QA (FQ)).", "labels": [], "entities": [{"text": "Freebase QA (FQ))", "start_pos": 87, "end_pos": 104, "type": "DATASET", "confidence": 0.872438645362854}]}, {"text": "GeoQuery includes a geography database with a small ontology and questions with relatively complex, compositional structure.", "labels": [], "entities": [{"text": "GeoQuery", "start_pos": 0, "end_pos": 8, "type": "DATASET", "confidence": 0.9413118362426758}]}, {"text": "FQ includes questions to Freebase, a large community-authored database that spans many sub-domains.", "labels": [], "entities": [{"text": "FQ", "start_pos": 0, "end_pos": 2, "type": "DATASET", "confidence": 0.9589696526527405}, {"text": "Freebase", "start_pos": 25, "end_pos": 33, "type": "DATASET", "confidence": 0.9511796236038208}]}, {"text": "Experiments demonstrate state-of-the-art performance in both cases, including a nine point improvement in recall for the FQ test.", "labels": [], "entities": [{"text": "recall", "start_pos": 106, "end_pos": 112, "type": "METRIC", "confidence": 0.9997139573097229}, {"text": "FQ test", "start_pos": 121, "end_pos": 128, "type": "DATASET", "confidence": 0.634526401758194}]}], "datasetContent": [{"text": "Data We evaluate performance on the benchmark GeoQuery dataset, and a newly introduced Freebase Query (FQ) dataset.", "labels": [], "entities": [{"text": "GeoQuery dataset", "start_pos": 46, "end_pos": 62, "type": "DATASET", "confidence": 0.9241697788238525}, {"text": "Freebase Query (FQ) dataset", "start_pos": 87, "end_pos": 114, "type": "DATASET", "confidence": 0.7896047532558441}]}, {"text": "FQ contains 917 questions labeled with logical form meaning representations for querying Freebase.", "labels": [], "entities": [{"text": "FQ", "start_pos": 0, "end_pos": 2, "type": "DATASET", "confidence": 0.9607332944869995}, {"text": "Freebase", "start_pos": 89, "end_pos": 97, "type": "DATASET", "confidence": 0.7844879627227783}]}, {"text": "We gathered question answer labels by executing the logical forms against Freebase, and manually correcting any inconsistencies.", "labels": [], "entities": [{"text": "Freebase", "start_pos": 74, "end_pos": 82, "type": "DATASET", "confidence": 0.9839870929718018}]}, {"text": "Freebase) is a large, collaboratively authored database containing almost 40 million entities and two billion facts, covering more than 100 domains.", "labels": [], "entities": [{"text": "Freebase", "start_pos": 0, "end_pos": 8, "type": "DATASET", "confidence": 0.9586842656135559}]}, {"text": "We filter Freebase to cover the domains contained in the FQ dataset resulting in a database containing 18 million entities, 2072 relations, 635 types, 135 million facts and 81 domains, including for example film, sports, and business.", "labels": [], "entities": [{"text": "FQ dataset", "start_pos": 57, "end_pos": 67, "type": "DATASET", "confidence": 0.9832539856433868}]}, {"text": "We use this schema to define our target domain, allowing fora wider variety of queries than could be encoded with the 635 collapsed relations previously used to label the FQ data.", "labels": [], "entities": [{"text": "FQ data", "start_pos": 171, "end_pos": 178, "type": "DATASET", "confidence": 0.9459213316440582}]}, {"text": "We report two different experiments on the FQ data: test results on the existing 642/275 train/test split and domain adaptation results where the data is split three ways, partitioning the topics so that the logical meaning expressions do not share any symbols across folds.", "labels": [], "entities": [{"text": "FQ data", "start_pos": 43, "end_pos": 50, "type": "DATASET", "confidence": 0.9107323586940765}, {"text": "642/275 train/test split", "start_pos": 81, "end_pos": 105, "type": "DATASET", "confidence": 0.7831235442842756}, {"text": "domain adaptation", "start_pos": 110, "end_pos": 127, "type": "TASK", "confidence": 0.6762527823448181}]}, {"text": "We report on the standard 600/280 training/test split for GeoQuery.", "labels": [], "entities": [{"text": "GeoQuery", "start_pos": 58, "end_pos": 66, "type": "DATASET", "confidence": 0.873592734336853}]}, {"text": "Parameter Initialization and Training We initialize weights for \u03c6 np and \u03c6 direct to 10, and weights for \u03c6 stem and \u03c6 join to 5.", "labels": [], "entities": [{"text": "Parameter Initialization", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.7194157093763351}]}, {"text": "This promotes the use of entities and relations named in sentences.", "labels": [], "entities": []}, {"text": "We initialize weights for \u03c6 pp and \u03c6 emp to -1 to favour logical forms that have an interpretation in the knowledge base K.", "labels": [], "entities": []}, {"text": "All other feature weights are initialized to 0.", "labels": [], "entities": []}, {"text": "We run the training algorithm for one iteration on the Freebase data, at which point performance on the development set had converged.", "labels": [], "entities": [{"text": "Freebase data", "start_pos": 55, "end_pos": 68, "type": "DATASET", "confidence": 0.9928593039512634}]}, {"text": "This fast convergence is due to the very small number of matching parameters used (5 lexical features and 8 K features).", "labels": [], "entities": []}, {"text": "For GeoQuery, we include the larger domain specific feature set introduced in Section 7.2 and train for 10 iterations.", "labels": [], "entities": []}, {"text": "We set the pruning parameters from Section 6.1 as follows: k = 5 for Freebase, k = 30 for GeoQuery, N = 50, \u03c4 = 10.", "labels": [], "entities": [{"text": "Freebase", "start_pos": 69, "end_pos": 77, "type": "DATASET", "confidence": 0.9517022371292114}]}, {"text": "Comparison Systems We compare performance to state-of-the-art systems in both domains.", "labels": [], "entities": []}, {"text": "On GeoQuery, we report results from DCS (Liang et al., 2011) without special initialization (DCS) and with an small hand-engineered lexicon (DCS with L + ).", "labels": [], "entities": []}, {"text": "We also include results for the FUBL algorithm (), the CCG learning approach that is most closely related to our work.", "labels": [], "entities": [{"text": "FUBL", "start_pos": 32, "end_pos": 36, "type": "METRIC", "confidence": 0.968125581741333}]}, {"text": "On FQ, we compare to Cai and Yates (2013a) (CY13).", "labels": [], "entities": [{"text": "FQ", "start_pos": 3, "end_pos": 5, "type": "DATASET", "confidence": 0.848557710647583}, {"text": "CY13", "start_pos": 44, "end_pos": 48, "type": "DATASET", "confidence": 0.8315632343292236}]}, {"text": "Evaluation We evaluate by comparing the produced question answers to the labeled ones, with no partial credit.", "labels": [], "entities": []}, {"text": "Because the parser can fail to produce a complete query, we report recall, the percent of total questions answered correctly, and precision, the percentage of produced queries with correct answers.", "labels": [], "entities": [{"text": "recall", "start_pos": 67, "end_pos": 73, "type": "METRIC", "confidence": 0.9994603991508484}, {"text": "precision", "start_pos": 130, "end_pos": 139, "type": "METRIC", "confidence": 0.9996123909950256}]}, {"text": "CY13 and FUBL report fully correct logical forms, which is a close proxy to our numbers.", "labels": [], "entities": [{"text": "CY13", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.9732992053031921}, {"text": "FUBL", "start_pos": 9, "end_pos": 13, "type": "METRIC", "confidence": 0.8188907504081726}]}], "tableCaptions": []}