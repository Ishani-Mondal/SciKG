{"title": [{"text": "Combining PCFG-LA Models with Dual Decomposition: A Case Study with Function Labels and Binarization", "labels": [], "entities": []}], "abstractContent": [{"text": "It has recently been shown that different NLP models can be effectively combined using dual decomposition.", "labels": [], "entities": []}, {"text": "In this paper we demonstrate that PCFG-LA parsing models are suitable for combination in this way.", "labels": [], "entities": [{"text": "PCFG-LA parsing", "start_pos": 34, "end_pos": 49, "type": "TASK", "confidence": 0.6916747391223907}]}, {"text": "We experiment with the different models which result from alternative methods of extracting a grammar from a treebank (retaining or discarding function labels, left binarization versus right binarization) and achieve a labeled Parseval F-score of 92.4 on Wall Street Journal Section 23-this represents an absolute improvement of 0.7 and an error reduction rate of 7% over a strong PCFG-LA product-model base-line.", "labels": [], "entities": [{"text": "Parseval F-score", "start_pos": 227, "end_pos": 243, "type": "METRIC", "confidence": 0.8039790689945221}, {"text": "Wall Street Journal Section 23-this", "start_pos": 255, "end_pos": 290, "type": "DATASET", "confidence": 0.9709874749183655}, {"text": "error reduction rate", "start_pos": 340, "end_pos": 360, "type": "METRIC", "confidence": 0.9639997879664103}, {"text": "PCFG-LA product-model base-line", "start_pos": 381, "end_pos": 412, "type": "DATASET", "confidence": 0.9595323999722799}]}, {"text": "Although we experiment only with bina-rization and function labels in this study, there is much scope for applying this approach to other grammar extraction strategies.", "labels": [], "entities": [{"text": "grammar extraction", "start_pos": 138, "end_pos": 156, "type": "TASK", "confidence": 0.775113970041275}]}], "introductionContent": [{"text": "Because of the large amount of possibly contradictory information contained in a treebank, learning a phrase-structure-based parser implies making several choices regarding the prevalent annotations which have to be kept -or discarded -in order to guide the learning algorithm.", "labels": [], "entities": []}, {"text": "These choices, which include whether to keep function labels and empty nodes, how to binarize the trees and whether to alter the granularity of the tagset, are often motivated empirically by parsing performance rather than by the different aspects of the language they maybe able to capture.", "labels": [], "entities": []}, {"text": "Recently  framework for combining different types of NLP tasks or for building parsers from simple slave processes that only check partial well-formedness.", "labels": [], "entities": []}, {"text": "Here we propose to follow this idea, but with a different objective.", "labels": [], "entities": []}, {"text": "We want to mix different parsers trained on different versions of a treebank each of which makes some annotation choices in order to learn more specific or richer information.", "labels": [], "entities": []}, {"text": "We will use state-of-the-art unlexicalized probabilistic contextfree grammars with latent annotations (PCFG-LA) in order to compare our approach with a strong baseline of high-quality parses.", "labels": [], "entities": []}, {"text": "Dual Decomposition is used to mix several systems (between two and four) that may in turn be combinations of grammars, here products of PCFG-LAs (Petrov, 2010).", "labels": [], "entities": []}, {"text": "The systems being combined make different choices with regard to i) function labels and ii) grammar binarization.", "labels": [], "entities": []}, {"text": "Common sense would suggest that information in the form of function labels -syntactic labels such as SBJ and PRD and semantic labels such as TMP and LOC -might help in obtaining a fine-grained analysis.", "labels": [], "entities": []}, {"text": "On the other hand, the independence hypothe-sis on which CFGs rely and on which most popular parsers are based maybe too strong to learn the dependencies between functions across the parse trees.", "labels": [], "entities": []}, {"text": "Also, the number of parameters increases with the use of function labels and this can affect the learning process.", "labels": [], "entities": []}, {"text": "At first glance, binarization need not bean issue, as CFGs admit a binarized form recognizing exactly the same language.", "labels": [], "entities": []}, {"text": "But binarization can be associated with horizontal markovization and in this case the recognized language will differ.", "labels": [], "entities": []}, {"text": "Furthermore this can impose an unwanted emphasis on what frontier information is more relevant to learning (beginning or end of constituents).", "labels": [], "entities": []}, {"text": "In the toy example of, the original grammar consisting of a unique rule extracted from one tree only recognizes the string bcdef, while the grammar learned from the left binarized and markovized tree recognizes (among others) bcdef and bdcef and the grammar learned from the right binarized and markovized tree recognizes (among others) bcdef and bcedf.", "labels": [], "entities": []}, {"text": "We find that i) retaining the function labels in nonterminal categories loses its negative impact on parsing as the number of grammars increases in PCFG-LA product models, ii) the function labels themselves can be recovered with near state-of-the-artaccuracy, iii) combining grammars with and without function labels using dual decomposition is beneficial, iv) combining left and right-binarized grammars using dual decomposition also leads to better trees and, v) our best results (a Parseval labeled F-score of 92.4, a Stanford labeled attachment score (LAS) of 93.0 and a penn2malt unlabeled attachment score (UAS) of 94.3 on Section 23 of the Wall Street Journal) are obtained by combining three grammars which encode different function label/binarization decisions.", "labels": [], "entities": [{"text": "Stanford labeled attachment score (LAS)", "start_pos": 521, "end_pos": 560, "type": "METRIC", "confidence": 0.7779906264373234}, {"text": "penn2malt unlabeled attachment score (UAS)", "start_pos": 575, "end_pos": 617, "type": "METRIC", "confidence": 0.9321817074503217}, {"text": "Section 23 of the Wall Street Journal", "start_pos": 629, "end_pos": 666, "type": "DATASET", "confidence": 0.8083209821156093}]}, {"text": "The paper is organized as follows.", "labels": [], "entities": []}, {"text": "\u00a7 2 reviews related work.", "labels": [], "entities": []}, {"text": "\u00a7 3 presents approximate PCFG-LA parsers as linear models, while \u00a7 4 shows how we can use dual decomposition to derive an algorithm for combining these models.", "labels": [], "entities": []}, {"text": "Experimental results are presented and discussed in \u00a7 5.", "labels": [], "entities": []}], "datasetContent": [{"text": "We perform our experiments on the WSJ sections of the PTB with the usual split: sections 2 to 21 for training, section 23 for testing, and we run benchmarks on section 22.", "labels": [], "entities": [{"text": "WSJ sections of the PTB", "start_pos": 34, "end_pos": 57, "type": "DATASET", "confidence": 0.9210623502731323}]}, {"text": "evalb is used for evaluation.", "labels": [], "entities": []}, {"text": "We use the LORG parser modified with Algorithm 1.", "labels": [], "entities": []}, {"text": "All grammars are trained using 6 split/merge EM cycles.", "labels": [], "entities": []}, {"text": "For the handling of unknown words, we removed all words occurring once in the training set and replaced them by their morphological signature (.", "labels": [], "entities": []}, {"text": "Grammars for products are obtained by training with 16 random seeds for each setting.", "labels": [], "entities": []}, {"text": "We use the approximate al-gorithm MaxRule-Product (.", "labels": [], "entities": []}, {"text": "The basic settings area combination of the two following parameters: left or right binarization: we conjecture that this affects the quality of the parsers by impacting the recognition of left and right constituent frontiers.", "labels": [], "entities": []}, {"text": "We set vertical markovization to 1 (no parent annotation) and horizontal markovization to 0 (we drop all left/right annotations). with or without functional annotations: in particular when non-terminals are annotated with multiple functions, all are kept.", "labels": [], "entities": []}, {"text": "We also evaluate the quality of the function labels.", "labels": [], "entities": []}, {"text": "We compare the results obtained directly from the parser output with results obtained with Funtag, a state-of-the-art functional tagger that is applied on parser output, using a gold model trained on sections 02 to 21 of the WSJ (  The results are shown in.", "labels": [], "entities": [{"text": "WSJ", "start_pos": 225, "end_pos": 228, "type": "DATASET", "confidence": 0.9605883955955505}]}, {"text": "First, we can see that the parser output is always outperformed by Funtag.", "labels": [], "entities": [{"text": "Funtag", "start_pos": 67, "end_pos": 73, "type": "DATASET", "confidence": 0.9142703413963318}]}, {"text": "This is expected from a context-free parser that has a limited domain of locality with strong independence constraints, compared to a voted-SVM classifier that can rely on arbitrarily rich features.", "labels": [], "entities": []}, {"text": "Second, the quality of the Funtag prediction seems to be influenced by the fact that parser already handle functions and by the accuracy of the parser (Parseval F-score).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 128, "end_pos": 136, "type": "METRIC", "confidence": 0.9993917942047119}, {"text": "Parseval F-score", "start_pos": 152, "end_pos": 168, "type": "METRIC", "confidence": 0.8153221607208252}]}, {"text": "This is because we use a model trained on the gold reference and so the closer the parser output is from the reference, the better the prediction.", "labels": [], "entities": []}, {"text": "On the other hand, this is not the case with parser predicted functions, where the best system is the right-binarized product model with functions, with very similar performance obtained by the combinations consisting of 2 function parsers, settings DD Func and DD4.", "labels": [], "entities": []}, {"text": "This tends to indicate that the constraints we have set to define consistencies in c-parses, focusing on syntactical categories, do not help in retrieving better function labels.", "labels": [], "entities": []}, {"text": "This suggests some possible further improvements where parsers with functional annotations should be forced to agree on these too.", "labels": [], "entities": []}, {"text": "Dependency-based evaluation of phrase structure parser output has been used in recent years to provide a more rounded view on parser performance and to compare with direct dependency parsers.", "labels": [], "entities": [{"text": "Dependency-based evaluation of phrase structure parser output", "start_pos": 0, "end_pos": 61, "type": "TASK", "confidence": 0.5988266170024872}]}, {"text": "We evaluate our various parsing models on their ability to recover three types of dependencies: basic Stanford dependencies , LTH dependencies (Johansson and Nugues, We used the latest version at the time of writing, i.e. 3.20.  2007) and penn2malt dependencies.", "labels": [], "entities": []}, {"text": "The latter area simpler version of the LTH dependencies but are still used when reporting unlabeled attachment scores for dependency parsing.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 122, "end_pos": 140, "type": "TASK", "confidence": 0.7937435209751129}]}, {"text": "The results, shown in, mirror the constituency evaluation results in that the dual decomposition results tend to outperform the basic product model results, and combining three or four grammars using dual decomposition yields the highest scores.", "labels": [], "entities": []}, {"text": "The differences between the Func and No Func results highlight an important difference between the Stanford and LTH dependency schemes.", "labels": [], "entities": []}, {"text": "The tool used to produce Stanford dependencies has been designed to work with phrase structure trees that do not contain function labels.", "labels": [], "entities": []}, {"text": "In contrast, the LTH tool makes use of function label information in phrase structure trees.", "labels": [], "entities": []}, {"text": "Thus, their availability results in only a moderate improvement in LAS for the Stanford dependencies and a very striking improvement for the LTH dependencies.", "labels": [], "entities": [{"text": "LAS", "start_pos": 67, "end_pos": 70, "type": "METRIC", "confidence": 0.592745840549469}, {"text": "LTH dependencies", "start_pos": 141, "end_pos": 157, "type": "DATASET", "confidence": 0.8129712045192719}]}, {"text": "By retaining function labels during parsing, we have shown that LTH dependencies can be recovered with a high level of accuracy without having to resort to a post-parsing function labeling step.", "labels": [], "entities": [{"text": "parsing", "start_pos": 36, "end_pos": 43, "type": "TASK", "confidence": 0.9690582156181335}, {"text": "accuracy", "start_pos": 119, "end_pos": 127, "type": "METRIC", "confidence": 0.998087465763092}]}], "tableCaptions": [{"text": " Table 1. Un- surprisingly, the best configuration is the one com- bining the two best product systems (with right bi- narization) but all combined systems perform better  than their single components.", "labels": [], "entities": []}, {"text": " Table 1: Parse evaluation on development set.", "labels": [], "entities": []}, {"text": " Table 2: Rate of certificates of optimality on the dev set.", "labels": [], "entities": [{"text": "Rate", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9697020053863525}]}, {"text": " Table 3: Function labeling F1 on development set.", "labels": [], "entities": [{"text": "Function labeling", "start_pos": 10, "end_pos": 27, "type": "TASK", "confidence": 0.6551249623298645}, {"text": "F1", "start_pos": 28, "end_pos": 30, "type": "METRIC", "confidence": 0.9464576244354248}]}, {"text": " Table 3. First, we can  see that the parser output is always outperformed by  Funtag. This is expected from a context-free parser", "labels": [], "entities": [{"text": "Funtag", "start_pos": 79, "end_pos": 85, "type": "DATASET", "confidence": 0.9038284420967102}]}, {"text": " Table 4: Dependency accuracies on the dev set", "labels": [], "entities": []}, {"text": " Table 5: Test Set Results: Parseval F-score, penn2malt  UAS, Function Label Accuracy and Funtag Function La- bel Accuracy", "labels": [], "entities": [{"text": "Parseval", "start_pos": 28, "end_pos": 36, "type": "METRIC", "confidence": 0.8435759544372559}, {"text": "F-score", "start_pos": 37, "end_pos": 44, "type": "METRIC", "confidence": 0.510317325592041}, {"text": "penn2malt", "start_pos": 46, "end_pos": 55, "type": "METRIC", "confidence": 0.85162752866745}, {"text": "UAS", "start_pos": 57, "end_pos": 60, "type": "METRIC", "confidence": 0.700905978679657}, {"text": "Function Label Accuracy", "start_pos": 62, "end_pos": 85, "type": "METRIC", "confidence": 0.8788753946622213}, {"text": "Funtag Function La- bel Accuracy", "start_pos": 90, "end_pos": 122, "type": "METRIC", "confidence": 0.5825069298346838}]}]}