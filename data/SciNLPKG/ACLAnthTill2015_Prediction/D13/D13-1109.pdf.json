{"title": [{"text": "Monolingual Marginal Matching for Translation Model Adaptation", "labels": [], "entities": [{"text": "Monolingual Marginal Matching", "start_pos": 0, "end_pos": 29, "type": "TASK", "confidence": 0.7579844991366068}, {"text": "Translation Model Adaptation", "start_pos": 34, "end_pos": 62, "type": "TASK", "confidence": 0.8886980414390564}]}], "abstractContent": [{"text": "When using a machine translation (MT) model trained on OLD-domain parallel data to translate NEW-domain text, one major challenge is the large number of out-of-vocabulary (OOV) and new-translation-sense words.", "labels": [], "entities": [{"text": "machine translation (MT)", "start_pos": 13, "end_pos": 37, "type": "TASK", "confidence": 0.8465131044387817}]}, {"text": "We present a method to identify new translations of both known and unknown source language words that uses NEW-domain comparable document pairs.", "labels": [], "entities": []}, {"text": "Starting with a joint distribution of source-target word pairs derived from the OLD-domain parallel corpus, our method recovers anew joint distribution that matches the marginal distributions of the NEW-domain comparable document pairs, while minimizing the divergence from the OLD-domain distribution.", "labels": [], "entities": [{"text": "OLD-domain parallel corpus", "start_pos": 80, "end_pos": 106, "type": "DATASET", "confidence": 0.626078595717748}]}, {"text": "Adding learned translations to our French-English MT model results in gains of about 2 BLEU points over strong baselines.", "labels": [], "entities": [{"text": "MT", "start_pos": 50, "end_pos": 52, "type": "TASK", "confidence": 0.9223409295082092}, {"text": "BLEU", "start_pos": 87, "end_pos": 91, "type": "METRIC", "confidence": 0.999413251876831}]}], "introductionContent": [{"text": "When a statistical machine translation (SMT) model trained on OLD-domain (e.g. parliamentary proceedings) parallel text is used to translate text in a NEWdomain (e.g. medical or scientific), performance degrades drastically.", "labels": [], "entities": [{"text": "statistical machine translation (SMT)", "start_pos": 7, "end_pos": 44, "type": "TASK", "confidence": 0.7722913970549902}]}, {"text": "One of the major causes is the large number of NEW-domain words that are out-ofvocabulary (OOV) with respect to the OLD-domain text.", "labels": [], "entities": []}, {"text": "shows the OOV rate for text in several with respect to OLD-domain parliamentary proceedings.", "labels": [], "entities": [{"text": "OOV rate", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9796369671821594}]}, {"text": "Even more challenging are the difficult-to-detect new-translation-sense (NTS) words: French words that are present in both the OLD and NEW domains but that are translated differently in each domain.", "labels": [], "entities": []}, {"text": "For example, the French word enceinte is mostly translated in parliamentary proceedings as place, house, or chamber; in medical text, the translation is mostly pregnant; in scientific text, enclosures.", "labels": [], "entities": []}, {"text": "One potential remedy is to collect parallel data in the NEW-domain, from which we can train anew SMT model., for example, mine parallel text from comparable corpora.", "labels": [], "entities": [{"text": "NEW-domain", "start_pos": 56, "end_pos": 66, "type": "DATASET", "confidence": 0.8930097818374634}, {"text": "SMT", "start_pos": 97, "end_pos": 100, "type": "TASK", "confidence": 0.9839045405387878}]}, {"text": "Parallel sentences are informative but also rare: in the data released by, only 21% of the foreign sentences have a near-parallel counterpart in the English article.", "labels": [], "entities": []}, {"text": "1 Furthermore, these sentences do not capture all terms.", "labels": [], "entities": []}, {"text": "In that same dataset, we find that on average only 20% of foreign and 28% of English word types in a given article are represented in the parallel sentence pairs.", "labels": [], "entities": []}, {"text": "In this work, we seek to learn a joint distribu-tion of translation probabilities overall source and target word pairs in the NEW-domain.", "labels": [], "entities": [{"text": "NEW-domain", "start_pos": 126, "end_pos": 136, "type": "DATASET", "confidence": 0.9171428084373474}]}, {"text": "We begin with a maximum likelihood estimate of the joint based on a word aligned OLD-domain corpus and update this distribution using NEW-domain comparable data.", "labels": [], "entities": [{"text": "NEW-domain comparable data", "start_pos": 134, "end_pos": 160, "type": "DATASET", "confidence": 0.833773136138916}]}, {"text": "We define a model based on a single comparable corpus and then extend it to learn from document aligned comparable corpora with any number of comparable document pairs.", "labels": [], "entities": []}, {"text": "This approach allows us to identify translations for OOV words in the OLD-domain (e.g. French cisaillement and per\u00e7age, which translate as shear and drilling, in the scientific domain) as well as new translations for previously observed NTS words (e.g. enceinte translates as enclosures, not place, in the scientific domain).", "labels": [], "entities": []}, {"text": "In our MT experiments, we use the learned NEWdomain joint distribution to update our SMT model with translations of OOV and low frequency words; we leave the integration of new translations for NTS words to future work.", "labels": [], "entities": [{"text": "MT", "start_pos": 7, "end_pos": 9, "type": "TASK", "confidence": 0.9899072647094727}, {"text": "SMT", "start_pos": 85, "end_pos": 88, "type": "TASK", "confidence": 0.9933433532714844}]}, {"text": "Our approach crucially depends on finding comparable document pairs relevant to the NEW-domain.", "labels": [], "entities": [{"text": "NEW-domain", "start_pos": 84, "end_pos": 94, "type": "DATASET", "confidence": 0.870394229888916}]}, {"text": "Such pairs could be derived from a number of sources, with document pairings inferred from timestamps (e.g. news articles) or topics (inferred or manually labeled).", "labels": [], "entities": []}, {"text": "We use Wikipedia 2 as a source of comparable pairs.", "labels": [], "entities": []}, {"text": "So-called \"interwiki links\" (which link Wikipedia articles written on the same topic but in different languages) act as rough guidance that pages may contain similar information.", "labels": [], "entities": []}, {"text": "Our approach does not exploit any Wikipedia structure beyond this signal, and thus is portable to alternate sources of comparable articles, such as multilingual news articles covering the same event.", "labels": [], "entities": []}, {"text": "Our model also relies on the assumption that each comparable document pair describes generally the same concepts, though the order and structure of presentation may differ significantly.", "labels": [], "entities": []}, {"text": "The efficacy of this method likely depends on the degree of comparability of the data; exploring the correlation between comparability and MT performance is an interesting question for future work.", "labels": [], "entities": [{"text": "MT", "start_pos": 139, "end_pos": 141, "type": "TASK", "confidence": 0.974302351474762}]}], "datasetContent": [{"text": "We use French-English Hansard parliamentary proceedings as our OLD-domain parallel corpus.", "labels": [], "entities": [{"text": "Hansard parliamentary proceedings", "start_pos": 22, "end_pos": 55, "type": "DATASET", "confidence": 0.8211590647697449}]}, {"text": "With over 8 million parallel lines of text, it is one of the largest freely available parallel corpora for any lan- We could have, analogously, used the target language (English) side of the parallel corpus and measure overlap with the English Wikipedia documents, or even used both.", "labels": [], "entities": []}, {"text": "7 http://www.parl.gc.ca guage pair.", "labels": [], "entities": []}, {"text": "In order to simulate more typical data settings, we sample every 32nd line, using the resulting parallel corpus of 253, 387 lines and 5, 051, 016 tokens to train our baseline model.", "labels": [], "entities": []}, {"text": "We test our model using three NEW-domain corpora: (1) the EMEA medical corpus, (2) a corpus of scientific abstracts (Carpuat et al., 2013a), and (3) a corpus of translated movie subtitles.", "labels": [], "entities": [{"text": "EMEA medical corpus", "start_pos": 58, "end_pos": 77, "type": "DATASET", "confidence": 0.9281606276830038}]}, {"text": "We use development and test sets to tune and evaluate our MT models.", "labels": [], "entities": [{"text": "MT", "start_pos": 58, "end_pos": 60, "type": "TASK", "confidence": 0.9796839952468872}]}, {"text": "We use the NEW-domain parallel training corpora only for language modeling and for identifying NEW-domain-like comparable documents.", "labels": [], "entities": [{"text": "NEW-domain parallel training corpora", "start_pos": 11, "end_pos": 47, "type": "DATASET", "confidence": 0.7426439225673676}, {"text": "language modeling", "start_pos": 57, "end_pos": 74, "type": "TASK", "confidence": 0.7089515924453735}]}, {"text": "For each domain, we use the marginal matching method described in Section 3 to learn anew, domain-adapted joint distribution, p new k (s, t), overall French and English words.", "labels": [], "entities": []}, {"text": "We use the learned joint to compute conditional probabilities, p new k (t|s), for each French word sand rank English translations t accordingly.", "labels": [], "entities": []}, {"text": "First, we evaluate the learned joint directly using the distribution based on the wordaligned NEW-domain development set as a gold standard.", "labels": [], "entities": [{"text": "NEW-domain development set", "start_pos": 94, "end_pos": 120, "type": "DATASET", "confidence": 0.8392583529154459}]}, {"text": "Then, we perform end-to-end MT experiments.", "labels": [], "entities": [{"text": "MT", "start_pos": 28, "end_pos": 30, "type": "TASK", "confidence": 0.9895692467689514}]}, {"text": "We supplement phrase tables with translations for OOV and low frequency words (we experiment with training data frequencies less than 101, 11, and 1) and include p new k (t|s) and p new k (s|t) as new translation features for those supplemental translations.", "labels": [], "entities": []}, {"text": "For these new phrase pairs, we use the average lexicalized reordering values from the existing reordering tables.", "labels": [], "entities": []}, {"text": "For phrase pairs extracted bilingually, we use the bilingually estimated translation probabilities and uniform scores for the new translation features.", "labels": [], "entities": []}, {"text": "We experimented with using p new k (t|s) and p new k (s|t) to estimate additional lexical translation probabilities for the bilingually extracted phrase pairs but did not observe any gains (experimental details omitted due to space constraints).", "labels": [], "entities": []}, {"text": "We re-run tuning in all experiments.", "labels": [], "entities": []}, {"text": "We also perform oracle experiments in which we identify translations for French words in wordaligned development and test sets and append these translations to baseline phrase tables.", "labels": [], "entities": []}, {"text": "Before doing end-to-end MT experiments, we evaluate our learned joint distribution, p new k (s, t), by comparing it to the joint distribution taken from a word aligned NEW-domain parallel development set, p gold (s, t).", "labels": [], "entities": [{"text": "MT", "start_pos": 24, "end_pos": 26, "type": "TASK", "confidence": 0.9933280944824219}]}, {"text": "We call this evaluation semi-extrinsic because it involves neither end-to-end MT (our extrinsic task) nor an intrinsic evaluation based on our training objective (L1 norm).", "labels": [], "entities": [{"text": "MT", "start_pos": 78, "end_pos": 80, "type": "TASK", "confidence": 0.9113109111785889}]}, {"text": "We find it informative to evaluate the models using bilingual lexicon induction metrics before integrating our output into full MT.", "labels": [], "entities": [{"text": "MT", "start_pos": 128, "end_pos": 130, "type": "TASK", "confidence": 0.9259858727455139}]}, {"text": "That is, we do not compare the full joint distributions, but, rather, fora given French word, how our learned model ranks the word's most probable translation under the gold distribution.", "labels": [], "entities": []}, {"text": "In particular, because we are primarily concerned with learning translations for previously unseen words, we evaluate over OOV French word types.", "labels": [], "entities": [{"text": "learning translations for previously unseen words", "start_pos": 55, "end_pos": 104, "type": "TASK", "confidence": 0.7557689646879832}]}, {"text": "In some cases, the correct translation for OOV words is the identical string (e.g. na+, lycium).", "labels": [], "entities": []}, {"text": "Because it is trivial to produce these translations, we evaluate over the subset of OOV development set French words for which the correct translation is not the same string.", "labels": [], "entities": [{"text": "OOV development set French words", "start_pos": 84, "end_pos": 116, "type": "DATASET", "confidence": 0.8853925943374634}]}, {"text": "shows the mean reciprocal rank for the learned distribution, p new k (s, t), for each domains as a function of the number of comparable document pairs used in learning.", "labels": [], "entities": []}, {"text": "In all domains, the comparable document pairs are sorted according to their sim-  ilarity with the NEW-domain.", "labels": [], "entities": [{"text": "NEW-domain", "start_pos": 99, "end_pos": 109, "type": "DATASET", "confidence": 0.9442327618598938}]}, {"text": "also shows the performance of baseline models and our learner without the edit distance penalty.", "labels": [], "entities": []}, {"text": "For each source word s, the edit distance (ED) baseline ranks all English words tin our monolingual data by their edit distance with s.", "labels": [], "entities": [{"text": "edit distance (ED) baseline", "start_pos": 28, "end_pos": 55, "type": "METRIC", "confidence": 0.9228009184201559}]}, {"text": "The Canonical Correlation Analysis (CCA) baseline uses the approach of Daum\u00e9 III and Jagarlamudi (2011) and the top 25, 000 ranked document pairs as a comparable corpus.", "labels": [], "entities": [{"text": "Canonical Correlation Analysis (CCA)", "start_pos": 4, "end_pos": 40, "type": "TASK", "confidence": 0.7646587093671163}]}, {"text": "That model performs poorly largely because of sparse word context counts.", "labels": [], "entities": []}, {"text": "Interestingly, for Science and EMEA, the performance of our full model at 50, 000 document pairs is higher than the sum of the edit distance baseline and the model without the edit distance penalty, indicating that our approach effectively combines the marginal matching and edit distance signals.", "labels": [], "entities": [{"text": "EMEA", "start_pos": 31, "end_pos": 35, "type": "DATASET", "confidence": 0.8695666790008545}]}, {"text": "The learning curves for the three domains vary substantially.", "labels": [], "entities": []}, {"text": "For Science, learning is gradual and it appears that additional gains could be made by iterating over even more document pairs.", "labels": [], "entities": []}, {"text": "In contrast, the model learns quickly for the EMEA domain; performance is stable after 20, 000 document pairs.", "labels": [], "entities": []}, {"text": "Given these results and our experience with the two domains, we hypothesize that the difference is due to the fact that the Science data is much more heterogenous than the EMEA data.", "labels": [], "entities": [{"text": "Science data", "start_pos": 124, "end_pos": 136, "type": "DATASET", "confidence": 0.9749809503555298}, {"text": "EMEA data", "start_pos": 172, "end_pos": 181, "type": "DATASET", "confidence": 0.960353285074234}]}, {"text": "The Science data In particular, for each domain and each OOV French word, we ranked the set of all English words that appeared at least five times in the set of 50,000 most NEW-domain like Wikipedia pages.", "labels": [], "entities": [{"text": "Science data", "start_pos": 4, "end_pos": 16, "type": "DATASET", "confidence": 0.8774995505809784}, {"text": "OOV French word", "start_pos": 57, "end_pos": 72, "type": "DATASET", "confidence": 0.7449164787928263}]}, {"text": "Using a frequency threshold of five helped eliminate French words and improperly tokenized English words from the set of candidates.", "labels": [], "entities": []}, {"text": "includes physics, chemistry, and biology abstracts, among others.", "labels": [], "entities": []}, {"text": "The drug labels that makeup most of the EMEA data are more homogeneous.", "labels": [], "entities": [{"text": "EMEA data", "start_pos": 40, "end_pos": 49, "type": "DATASET", "confidence": 0.9175847768783569}]}, {"text": "In Section 6 we comment on the poor Subtitles performance, which persists in our MT experiments.", "labels": [], "entities": [{"text": "MT", "start_pos": 81, "end_pos": 83, "type": "TASK", "confidence": 0.9583657383918762}]}, {"text": "We experimented with making multiple learning passes over the document pairs and observed relatively small gains from doing so.", "labels": [], "entities": []}, {"text": "In all experiments, learning from some number of additional new document pairs resulted in higher semi-extrinsic performance gains than passing over document pairs which were already observed.", "labels": [], "entities": []}, {"text": "In the case of OOV words, it's clear that learning something about how to translate a previously unobserved French word is beneficial.", "labels": [], "entities": [{"text": "translate a previously unobserved French word", "start_pos": 74, "end_pos": 119, "type": "TASK", "confidence": 0.8413414855798086}]}, {"text": "However, our learning method also learns domain-specific newtranslation senses (NTS).", "labels": [], "entities": []}, {"text": "shows some examples of what the marginal matching method learns for different types of source words (OOVs, low frequency, and NTS).", "labels": [], "entities": []}, {"text": "By default, the Moses decoder copies OOV words directly into its translated output.", "labels": [], "entities": []}, {"text": "In some cases, this is correct (e.g. ensembles, blumeria, google).", "labels": [], "entities": []}, {"text": "In other cases, French words can be translated into English correctly by simply stripping accent marks off of the OOV word and then copying it to the output (e.g. cam\u00e9ra, \u00b4 el\u00e9ments, mol\u00e9cules).", "labels": [], "entities": []}, {"text": "In the Science and EMEA domains, we found that our baseline BLEU scores improved from 21.91 to 22.20 and 23.67 to 24.45, respectively, when we changed the default handling of OOVs to strip accents before copying into the output.", "labels": [], "entities": [{"text": "EMEA domains", "start_pos": 19, "end_pos": 31, "type": "DATASET", "confidence": 0.8066751956939697}, {"text": "BLEU", "start_pos": 60, "end_pos": 64, "type": "METRIC", "confidence": 0.9891930222511292}]}, {"text": "Interestingly, performance on the Subtitles domain text did not change at all with this baseline modification.", "labels": [], "entities": [{"text": "Subtitles domain text", "start_pos": 34, "end_pos": 55, "type": "DATASET", "confidence": 0.7350455125172933}]}, {"text": "This is likely due to the fact that there are fewer technical OOVs (the terms typically captured by this accent-stripping pattern) in the subtitles domain.", "labels": [], "entities": []}, {"text": "Throughout our experiments, we found it critical to retain correct 'freebie' OOV translations.", "labels": [], "entities": []}, {"text": "In the results presented below, including the baselines, we supplement phrase tables with anew candidate translation but also include accent-stripped identity, or 'freebie,' translations in the table for all OOV words.", "labels": [], "entities": []}, {"text": "We experimented with classifying French words as freebies or needing anew translation, but oracle experiments showed very little improvement (about 0.2 BLEU improvement in the Science domain), so instead we simply include both types of translations in the phrase tables.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 152, "end_pos": 156, "type": "METRIC", "confidence": 0.9993237257003784}]}, {"text": "In addition to the strip-accents baseline, we compare results with four other baselines.", "labels": [], "entities": []}, {"text": "First, we drop OOVs from the output translations.", "labels": [], "entities": [{"text": "OOVs", "start_pos": 15, "end_pos": 19, "type": "METRIC", "confidence": 0.8682904839515686}]}, {"text": "Second, like our semi-extrinsic baseline, we rank English words by their edit distance away from each French OOV word (ED baseline).", "labels": [], "entities": []}, {"text": "Third, we rank English words by their document-pair co-occurrence score with each French OOV word.", "labels": [], "entities": []}, {"text": "That is, for all words w, we compute D(w), the vector indicating the document pairs in which w occurs, over the set of 50,000 document-pairs which are most NEWdomain-like.", "labels": [], "entities": []}, {"text": "For French and English words sand t, if D(s) and D(t) are dissimilar, it is less likely (s, t) is a valid translation pair.", "labels": [], "entities": []}, {"text": "We weight D(w) entries with BM25 (.", "labels": [], "entities": [{"text": "BM25", "start_pos": 28, "end_pos": 32, "type": "METRIC", "confidence": 0.9486048221588135}]}, {"text": "For all French OOVs, we rank all English translations according to the cosine similarity between the pair of D(w) vectors.", "labels": [], "entities": []}, {"text": "The fourth baseline uses the CCA model to rank English words according to their distributional similarity with each French word.", "labels": [], "entities": []}, {"text": "For the CCA baseline comparison, we only learned translations using 25,000 Science-domain document pairs, rather than the full 50,000 and for all domains.", "labels": [], "entities": [{"text": "CCA baseline", "start_pos": 8, "end_pos": 20, "type": "DATASET", "confidence": 0.8890804648399353}]}, {"text": "However, it's unlikely that learning over more data would overcome the low performance observed so far.", "labels": [], "entities": []}, {"text": "For the final three baselines, we append French OOV words and their highest ranked English translation to the phrase table.", "labels": [], "entities": []}, {"text": "Along with each new translation pair, we include one new phrase table feature with the relevant translation score (edit distance, document similarity, or CCA distributional similarity).", "labels": [], "entities": [{"text": "CCA distributional similarity", "start_pos": 154, "end_pos": 183, "type": "METRIC", "confidence": 0.653692384560903}]}, {"text": "For all baselines other than drop-OOVs, we also include accent-stripped translation pairs with an additional indicator feature.", "labels": [], "entities": []}, {"text": "shows results appending the top ranked English translation for each OOV French word using each baseline method.", "labels": [], "entities": []}, {"text": "None of the alternate baselines outperform the simplest baseline on the subtitles data.", "labels": [], "entities": []}, {"text": "Using document pair co-occurrences is the strongest baseline for the Science and EMEA domains.", "labels": [], "entities": [{"text": "Science and EMEA domains", "start_pos": 69, "end_pos": 93, "type": "DATASET", "confidence": 0.8066045045852661}]}, {"text": "This confirms our intuition that taking advantage of document pair alignments is worthwhile.", "labels": [], "entities": [{"text": "document pair alignments", "start_pos": 53, "end_pos": 77, "type": "TASK", "confidence": 0.638948897520701}]}, {"text": "For Science and EMEA, supplementing a model with OOV translations learned through our marginal matching method drastically outperforms all base-OOVs translated correctly and incorrectly Input les r\u00e9sistances au cisaillement par poin\u00e7onnement ...", "labels": [], "entities": []}, {"text": "Ref the punching shear strengths...", "labels": [], "entities": []}, {"text": "Baseline the resistances in cisaillement by poinconnement ...", "labels": [], "entities": []}, {"text": "MM the resistances in shear reinforcement...", "labels": [], "entities": []}], "tableCaptions": []}