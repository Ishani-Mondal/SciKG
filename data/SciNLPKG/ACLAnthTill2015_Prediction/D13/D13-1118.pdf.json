{"title": [{"text": "Improvements to the Bayesian Topic N -gram Models", "labels": [], "entities": []}], "abstractContent": [{"text": "One of the language phenomena that n-gram language model fails to capture is the topic information of a given situation.", "labels": [], "entities": []}, {"text": "We advance the previous study of the Bayesian topic language model by Wallach (2006) in two directions: one, investigating new priors to alleviate the sparseness problem caused by dividing all n-grams into exclusive topics, and two, developing a novel Gibbs sampler that enables moving multiple n-grams across different documents to another topic.", "labels": [], "entities": []}, {"text": "Our blocked sampler can efficiently search for higher probability space even with higher order n-grams.", "labels": [], "entities": []}, {"text": "In terms of modeling assumption, we found it is effective to assign a topic to only some parts of a document .", "labels": [], "entities": []}], "introductionContent": [{"text": "N -gram language model is still ubiquitous in NLP, but due to its simplicity it fails to capture some important aspects of language, such as difference of word usage in different situations, sentence level syntactic correctness, and soon.", "labels": [], "entities": [{"text": "sentence level syntactic correctness", "start_pos": 191, "end_pos": 227, "type": "TASK", "confidence": 0.5800965651869774}]}, {"text": "Toward language model that can consider such a more global context, many extensions have been proposed from lexical pattern adaptation, e.g., adding cache or topic information (), to grammaticality aware models.", "labels": [], "entities": [{"text": "lexical pattern adaptation", "start_pos": 108, "end_pos": 134, "type": "TASK", "confidence": 0.753752867380778}]}, {"text": "Topic language models are important for use in e.g., unsupervised language model adaptation: we want a language model that can adapt to the domain or topic of the current situation (e.g., a document in SMT or a conversation in ASR) automatically and select the appropriate words using both topic and syntactic context.) is one such model, which generate each word based on local context and global topic information to capture the difference of lexical usage among different topics.", "labels": [], "entities": [{"text": "unsupervised language model adaptation", "start_pos": 53, "end_pos": 91, "type": "TASK", "confidence": 0.7248615920543671}, {"text": "SMT or a conversation in ASR", "start_pos": 202, "end_pos": 230, "type": "TASK", "confidence": 0.6498635609944662}]}, {"text": "However, Wallach's experiments were limited to bigrams, a toy setting for language models, and experiments with higher-order n-grams have not yet been sufficiently studied, which we investigate in this paper.", "labels": [], "entities": []}, {"text": "In particular, we point out the two fundamental problems caused when extending Wallach's model to a higher-order: sparseness caused by dividing all n-grams into exclusive topics, and local minima caused by the deep hierarchy of the model.", "labels": [], "entities": []}, {"text": "On resolving these problems, we make several contributions to both computational linguistics and machine learning.", "labels": [], "entities": [{"text": "computational linguistics", "start_pos": 67, "end_pos": 92, "type": "TASK", "confidence": 0.7263984382152557}]}, {"text": "To address the first problem, we investigate incorporating a global language model for ease of sparseness, along with some priors on a suffix tree to capture the difference of topicality for each context, which include an unsupervised extension of the doubly hierarchical Pitman-Yor language model, a Bayesian generative model for supervised language model adaptation.", "labels": [], "entities": [{"text": "supervised language model adaptation", "start_pos": 331, "end_pos": 367, "type": "TASK", "confidence": 0.6256225630640984}]}, {"text": "For the second inference problem, we develop a novel blocked Gibbs sampler.", "labels": [], "entities": []}, {"text": "When the number of topics is K and vocabulary size is V , n-gram topic model has O(KV n ) parameters, which grow exponentially ton, making the local minima problem even more severe.", "labels": [], "entities": [{"text": "O(KV n )", "start_pos": 81, "end_pos": 89, "type": "METRIC", "confidence": 0.8970482587814331}]}, {"text": "Our sampler resolves this problem by moving many customers in the hierarchical Chinese restaurant process at a time.", "labels": [], "entities": []}, {"text": "We evaluate various models by incremental calculation of test document perplexity on 3 types of corpora having different size and diversity.", "labels": [], "entities": []}, {"text": "By combining the proposed prior and the sampling method, our Bayesian model achieve much higher accuracies than the naive extension of and shows results competitive with the unigram rescaling (, which require huge computational cost at prediction, with much faster prediction time.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 2: Comparison of perplexity and the time require  for prediction (in seconds). The number of topics is fixed  to 100 on all topic-based models.", "labels": [], "entities": []}]}