{"title": [{"text": "Joint Learning of Phonetic Units and Word Pronunciations for ASR", "labels": [], "entities": [{"text": "ASR", "start_pos": 61, "end_pos": 64, "type": "TASK", "confidence": 0.9489956498146057}]}], "abstractContent": [{"text": "The creation of a pronunciation lexicon remains the most inefficient process in developing an Automatic Speech Recognizer (ASR).", "labels": [], "entities": [{"text": "Automatic Speech Recognizer (ASR)", "start_pos": 94, "end_pos": 127, "type": "TASK", "confidence": 0.7506650189558665}]}, {"text": "In this paper, we propose an unsupervised alternative-requiring no language-specific knowledge-to the conventional manual approach for creating pronunciation dictionaries.", "labels": [], "entities": []}, {"text": "We present a hierarchical Bayesian model, which jointly discovers the phonetic inventory and the Letter-to-Sound (L2S) mapping rules in a language using only transcribed data.", "labels": [], "entities": []}, {"text": "When tested on a corpus of spontaneous queries, the results demonstrate the superiority of the proposed joint learning scheme over its sequential counterpart, in which the latent phonetic inventory and L2S mappings are learned separately.", "labels": [], "entities": []}, {"text": "Furthermore, the recogniz-ers built with the automatically induced lexicon consistently outperform grapheme-based recognizers and even approach the performance of recognition systems trained using conventional supervised procedures.", "labels": [], "entities": []}], "introductionContent": [{"text": "Modern automatic speech recognizers require a few essential ingredients such as a signal representation of the speech signal, a search component, and typically a set of stochastic models that capture 1) the acoustic realizations of the basic sounds of a language, for example, phonemes, 2) the realization of words in terms of these sounds, and 3) how words are combined in spoken language.", "labels": [], "entities": [{"text": "automatic speech recognizers", "start_pos": 7, "end_pos": 35, "type": "TASK", "confidence": 0.706670343875885}]}, {"text": "When creating a speech recognizer fora new language the usual requirements are: first, a large speech corpus with word-level annotations; second, a pronunciation dictionary that essentially defines a phonetic inventory for the language as well as word-level pronunciations, and third, optional additional text data that can be used to train the language model.", "labels": [], "entities": []}, {"text": "Given these data and some decision about the signal representation, e.g., centi-second Mel-Frequency Cepstral Coefficients (MFCCs)) with various derivatives, as well as the nature of the acoustic and language model such as 3-state HMMs and n-grams, iterative training methods can be used to effectively learn the model parameters for the acoustic and language models.", "labels": [], "entities": []}, {"text": "Although the details of the components have changed through the years, this basic ASR formulation was well established by the late 1980's, and has not really changed much since then.", "labels": [], "entities": [{"text": "ASR formulation", "start_pos": 82, "end_pos": 97, "type": "TASK", "confidence": 0.8635494112968445}]}, {"text": "One of the interesting aspects of this formulation is the inherent dependence on the dictionary, which defines both the phonetic inventory of a language, and the pronunciations of all the words in the vocabulary.", "labels": [], "entities": []}, {"text": "The dictionary is arguably the cornerstone of a speech recognizer as it provides the essential transduction from sounds to words.", "labels": [], "entities": [{"text": "speech recognizer", "start_pos": 48, "end_pos": 65, "type": "TASK", "confidence": 0.7251651287078857}]}, {"text": "Unfortunately, the dependency on this resource is a significant impediment to the creation of speech recognizers for new languages, since they are typically created by experts, whereas annotated corpora can be relatively more easily created by native speakers of a language.", "labels": [], "entities": []}, {"text": "The existence of an expert-derived dictionary in the midst of stochastic speech recognition models is somewhat ironic, and it is natural to ask why it continues to receive special status after all these years.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 73, "end_pos": 91, "type": "TASK", "confidence": 0.6994027197360992}]}, {"text": "Why can we not learn the inventory of sounds of a language and associated word pronunciations automatically, much as we learn our acoustic model parameters?", "labels": [], "entities": []}, {"text": "If successful, we would move one step forward towards breaking the language barrier that limits us from having speech recognizers for all languages of the world, instead of the less than 2% that currently exist.", "labels": [], "entities": []}, {"text": "In this paper, we investigate the problem of inferring a pronunciation lexicon from an annotated corpus without exploiting any language-specific knowledge.", "labels": [], "entities": []}, {"text": "We formulate our approach as a hierarchical Bayesian model, which jointly discovers the acoustic inventory and the latent encoding scheme between the letters and the sounds of a language.", "labels": [], "entities": []}, {"text": "We evaluate the quality of the induced lexicon and acoustic model through a series of speech recognition experiments on a conversational weather query corpus ().", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 86, "end_pos": 104, "type": "TASK", "confidence": 0.7222660481929779}]}, {"text": "The results demonstrate that our model consistently generates close performance to recognizers that are trained with expertdefined phonetic inventory and lexicon.", "labels": [], "entities": []}, {"text": "Compared to grapheme-based recognizers, our model is capable of improving the Word Error Rates (WERs) by at least 15.3%.", "labels": [], "entities": [{"text": "Word Error Rates (WERs)", "start_pos": 78, "end_pos": 101, "type": "METRIC", "confidence": 0.8631595373153687}]}, {"text": "Finally, the joint learning framework proposed in this paper is proven to be much more effective than modeling the acoustic units and the letter-to-sound mappings separately, as shown in a 45% WER deduction our model achieves compared to a sequential approach.", "labels": [], "entities": [{"text": "WER", "start_pos": 193, "end_pos": 196, "type": "METRIC", "confidence": 0.750406801700592}]}], "datasetContent": [{"text": "To test the effectiveness of our model for joint learning phonetic units and word pronunciations from an annotated speech corpus, we construct speech recognizers out of the training results of our model.", "labels": [], "entities": [{"text": "joint learning phonetic units and word pronunciations", "start_pos": 43, "end_pos": 96, "type": "TASK", "confidence": 0.6433689636843545}]}, {"text": "The performance of the recognizers is evaluated and compared against three baselines: first, a graphemebased speech recognizer; second, a recognizer built by using an expert-crafted lexicon, which is referred to as an expert lexicon in the rest of the paper for simplicity; and third, a recognizer built by discovering the phonetic units and L2S pronunciation rules sequentially without using a lexicon.", "labels": [], "entities": []}, {"text": "In this section, we provide a detailed description of the experimental setup.: The values of the hyperparameters of our model.", "labels": [], "entities": []}, {"text": "We use a D to denote a D-dimensional vector with all entries being a.", "labels": [], "entities": []}, {"text": "*We follow the procedure reported in () to setup the HMM prior \u03b8 0 .  All the speech recognition experiments reported in this paper are performed on a weather query dataset, which consists of narrow-band, conversational telephone speech ().", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 78, "end_pos": 96, "type": "TASK", "confidence": 0.709400549530983}]}, {"text": "We follow the experimental setup of and split the corpus into a training set of 87,351 utterances, a dev set of 1,179 utterances and a test set of 3,497 utterances.", "labels": [], "entities": []}, {"text": "A subset of 10,000 utterances is randomly selected from the training set.", "labels": [], "entities": []}, {"text": "We use this subset of data for training our model to demonstrate that our model is able to discover the phonetic composition and the pronunciation rules of a language even from just a few hours of data.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 4: Pronunciation lists of the word Burma pro- duced by our model and refined by PMM after 1 and  2 iterations.", "labels": [], "entities": [{"text": "PMM", "start_pos": 87, "end_pos": 90, "type": "DATASET", "confidence": 0.6034778952598572}]}]}