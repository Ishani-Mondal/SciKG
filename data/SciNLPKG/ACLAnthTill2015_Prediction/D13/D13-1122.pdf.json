{"title": [{"text": "Exploiting Multiple Sources for Open-domain Hypernym Discovery", "labels": [], "entities": []}], "abstractContent": [{"text": "Hypernym discovery aims to extract such noun pairs that one noun is a hypernym of the other.", "labels": [], "entities": [{"text": "Hypernym discovery", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.8842021226882935}]}, {"text": "Most previous methods are based on lexical patterns but perform badly on open-domain data.", "labels": [], "entities": []}, {"text": "Other work extracts hypernym relations from encyclopedias but has limited coverage.", "labels": [], "entities": []}, {"text": "This paper proposes a simple yet effective distant supervision framework for Chi-nese open-domain hypernym discovery.", "labels": [], "entities": [{"text": "Chi-nese open-domain hypernym discovery", "start_pos": 77, "end_pos": 116, "type": "TASK", "confidence": 0.5620554089546204}]}, {"text": "Given an entity name, we try to discover its hy-pernyms by leveraging knowledge from multiple sources, i.e., search engine results, encyclopedias , and morphology of the entity name.", "labels": [], "entities": []}, {"text": "First, we extract candidate hypernyms from the above sources.", "labels": [], "entities": []}, {"text": "Then, we apply a statistical ranking model to select correct hypernyms.", "labels": [], "entities": []}, {"text": "A set of novel features is proposed for the ranking model.", "labels": [], "entities": []}, {"text": "We also present a heuristic strategy to build a large-scale noisy training data for the model without human annotation.", "labels": [], "entities": []}, {"text": "Experimental results demonstrate that our approach outperforms the state-of-the-art methods on a manually labeled test dataset.", "labels": [], "entities": []}], "introductionContent": [{"text": "Hypernym discovery is a task to extract such noun pairs that one noun is a hypernym of the other.", "labels": [], "entities": [{"text": "Hypernym discovery", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.8566722273826599}]}, {"text": "A noun H is a hypernym of another noun E if E is an instance or subclass of H.", "labels": [], "entities": []}, {"text": "In other word, H is a semantic class of E.", "labels": [], "entities": []}, {"text": "For instance, \"actor\" is a hypernym of \"Mel Gibson\"; \"dog\" is a hypernym of \"Caucasian sheepdog\"; \"medicine\" is a hypernym of \"Aspirin\".", "labels": [], "entities": []}, {"text": "Hypernym discovery is an important subtask of semantic relation extraction * Email correspondence. and has many applications in ontology construction, machine reading), question answering (, and soon.", "labels": [], "entities": [{"text": "Hypernym discovery", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.757371723651886}, {"text": "semantic relation extraction * Email correspondence.", "start_pos": 46, "end_pos": 98, "type": "TASK", "confidence": 0.6863904645045599}, {"text": "ontology construction", "start_pos": 128, "end_pos": 149, "type": "TASK", "confidence": 0.7977439165115356}, {"text": "machine reading", "start_pos": 151, "end_pos": 166, "type": "TASK", "confidence": 0.8045171499252319}, {"text": "question answering", "start_pos": 169, "end_pos": 187, "type": "TASK", "confidence": 0.9344519376754761}]}, {"text": "Some manually constructed thesauri such as WordNet can also provide some semantic relations such as hypernyms.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 43, "end_pos": 50, "type": "DATASET", "confidence": 0.9565673470497131}]}, {"text": "However, these thesauri are limited in its scope and domain, and manual construction is knowledge-intensive and time-consuming.", "labels": [], "entities": []}, {"text": "Therefore, many researchers try to automatically extract semantic relations or to construct taxonomies.", "labels": [], "entities": []}, {"text": "Most previous methods on automatic hypernym discovery are based on lexical patterns and suffer from the problem that such patterns can only cover a small part of complex linguistic circumstances).", "labels": [], "entities": [{"text": "hypernym discovery", "start_pos": 35, "end_pos": 53, "type": "TASK", "confidence": 0.7279479205608368}]}, {"text": "Other work tries to extract hypernym relations from large-scale encyclopedias like Wikipedia and achieves high precision).", "labels": [], "entities": [{"text": "precision", "start_pos": 111, "end_pos": 120, "type": "METRIC", "confidence": 0.9989036321640015}]}, {"text": "However, the coverage is limited since there exist many infrequent and new entities that are missing in encyclopedias (.", "labels": [], "entities": [{"text": "coverage", "start_pos": 13, "end_pos": 21, "type": "METRIC", "confidence": 0.9467626810073853}]}, {"text": "We made similar observation that more than a half of entities in our data set have no entries in the encyclopedias.", "labels": [], "entities": []}, {"text": "This paper proposes a simple yet effective distant supervision framework for Chinese open-domain hypernym discovery.", "labels": [], "entities": [{"text": "Chinese open-domain hypernym discovery", "start_pos": 77, "end_pos": 115, "type": "TASK", "confidence": 0.5707953199744225}]}, {"text": "Given an entity name, our goal is to discover its hypernyms by leveraging knowledge from multiple sources.", "labels": [], "entities": []}, {"text": "Considering the case where a person wants to know the meaning of an unknown entity, he/she may search it in a search engine and then finds out the answer after going through the search results.", "labels": [], "entities": []}, {"text": "Furthermore, if he/she finds an entry about the entity in an authentic website, such as Wikipedia, the information will help him/her under-stand the entity.", "labels": [], "entities": []}, {"text": "Also, the morphology of the entity name can provide supplementary information.", "labels": [], "entities": []}, {"text": "In this paper, we imitate the process.", "labels": [], "entities": []}, {"text": "The evidences from the above sources are integrated in our hypernym discovery model.", "labels": [], "entities": [{"text": "hypernym discovery", "start_pos": 59, "end_pos": 77, "type": "TASK", "confidence": 0.6579222530126572}]}, {"text": "Our approach is composed of two major steps: hypernym candidate extraction and ranking.", "labels": [], "entities": [{"text": "hypernym candidate extraction", "start_pos": 45, "end_pos": 74, "type": "TASK", "confidence": 0.6125683387120565}]}, {"text": "In the first step, we collect hypernym candidates from multiple sources.", "labels": [], "entities": []}, {"text": "Given an entity name, we search it in a search engine and extract high-frequency nouns as its main candidate hypernyms from the search results.", "labels": [], "entities": []}, {"text": "We also collect the category tags for the entity from two Chinese encyclopedias and the headword of the entity as the candidates.", "labels": [], "entities": []}, {"text": "In the second step, we identify correct hypernyms from the candidates.", "labels": [], "entities": []}, {"text": "We view this task as a ranking problem and propose a set of effective features to build a statistical ranking model.", "labels": [], "entities": []}, {"text": "For the parameter learning of the model, we also present a heuristic strategy to build a large-scale noisy training data without human annotation.", "labels": [], "entities": []}, {"text": "Our contributions are as follows: \u2022 We are the first to discover hypernym for Chinese open-domain entities by exploiting multiple sources.", "labels": [], "entities": []}, {"text": "The evidences from different sources can authenticate and complement each other to improve both precision and recall.", "labels": [], "entities": [{"text": "precision", "start_pos": 96, "end_pos": 105, "type": "METRIC", "confidence": 0.9994077682495117}, {"text": "recall", "start_pos": 110, "end_pos": 116, "type": "METRIC", "confidence": 0.9983707070350647}]}, {"text": "\u2022 We manually annotate a dataset containing 1,879 Chinese entities and their hypernyms, which will be made publicly available.", "labels": [], "entities": []}, {"text": "To the best of our knowledge, this is the first dataset for Chinese hypernyms.", "labels": [], "entities": []}, {"text": "\u2022 We propose a set of novel and effective features for hypernym ranking.", "labels": [], "entities": [{"text": "hypernym ranking", "start_pos": 55, "end_pos": 71, "type": "TASK", "confidence": 0.89223912358284}]}, {"text": "Experimental results show that our method achieves the best performance.", "labels": [], "entities": []}, {"text": "Furthermore, our approach can be easily ported from Chinese to English and other languages, except that a few language dependent features need to be changed.", "labels": [], "entities": []}, {"text": "The remainder of the paper is organized as follows: Section 2 discusses the related work.", "labels": [], "entities": []}, {"text": "Section 3 introduces our method in detail.", "labels": [], "entities": []}, {"text": "Section 4 describes the experimental setup.", "labels": [], "entities": []}, {"text": "Section 5 shows the experimental results.", "labels": [], "entities": []}, {"text": "Conclusion and future work are presented in Section 6.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this work, we use Baidu 3 search engine, the most popular search engine for Chinese, and get the top 100 search results for each entity.", "labels": [], "entities": [{"text": "Baidu 3 search engine", "start_pos": 21, "end_pos": 42, "type": "DATASET", "confidence": 0.849450409412384}]}, {"text": "The Chinese segmentation, POS tagging and dependency parsing is provided by an open-source Chinese language processing platform LTP 4 (Che et al., 2010).", "labels": [], "entities": [{"text": "Chinese segmentation", "start_pos": 4, "end_pos": 24, "type": "TASK", "confidence": 0.5218017846345901}, {"text": "POS tagging", "start_pos": 26, "end_pos": 37, "type": "TASK", "confidence": 0.8330696821212769}, {"text": "dependency parsing", "start_pos": 42, "end_pos": 60, "type": "TASK", "confidence": 0.6978817880153656}]}, {"text": "In our experiments, we prepare open-domain entities from dictionaries in wide domains, which are published by a Chinese input method editor software Sogou Pinyin 5 . The domains include biology, healthcare, food, movie, industry, and soon.", "labels": [], "entities": []}, {"text": "We sample 1,879 entities from these domain dictionaries and randomly split them into 1/5 for development and 4/5 for test.", "labels": [], "entities": []}, {"text": "We find that only 865 (46.04%) entities exist in Baidubaike or Hudongbaike.", "labels": [], "entities": [{"text": "Baidubaike", "start_pos": 49, "end_pos": 59, "type": "DATASET", "confidence": 0.9547441005706787}, {"text": "Hudongbaike", "start_pos": 63, "end_pos": 74, "type": "DATASET", "confidence": 0.7809375524520874}]}, {"text": "Then we extract candidate hypernyms for the entities and ask two annotators to judge each hypernym relation pair true or false manually.", "labels": [], "entities": []}, {"text": "A pair (E, H) is annotated as true if the annotators judge \"E is a (or a kind of) H\" is true.", "labels": [], "entities": []}, {"text": "Finally, we get 12.53 candidate hypernyms for each entity on average in which about 2.09 hypernyms are correct.", "labels": [], "entities": []}, {"text": "4,330 hypernym relation pairs are judged by both the annotators.", "labels": [], "entities": []}, {"text": "We measure the agreement of the judges using the Kappa coefficient  The evaluation metrics for our task include: Coverage Rate: We evaluate coverage rate of the candidate hypernyms.", "labels": [], "entities": [{"text": "agreement", "start_pos": 15, "end_pos": 24, "type": "METRIC", "confidence": 0.977286159992218}, {"text": "Coverage Rate", "start_pos": 113, "end_pos": 126, "type": "METRIC", "confidence": 0.8827760219573975}, {"text": "coverage rate", "start_pos": 140, "end_pos": 153, "type": "METRIC", "confidence": 0.9655331671237946}]}, {"text": "Coverage rate is the number of entities for which at least one correct hypernym is found divided by the total number of all entities.", "labels": [], "entities": [{"text": "Coverage rate", "start_pos": 0, "end_pos": 13, "type": "METRIC", "confidence": 0.9666494727134705}]}, {"text": "Precision@1: Our method returns a ranked list of hypernyms for each entity.", "labels": [], "entities": [{"text": "Precision", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.9298713207244873}]}, {"text": "We evaluate precision of top-1 hypernyms (the most probable ones) in the ranked lists, which is the number of correct top-1 hypernyms divided by the number of all entities.", "labels": [], "entities": [{"text": "precision", "start_pos": 12, "end_pos": 21, "type": "METRIC", "confidence": 0.9986054301261902}]}, {"text": "R-precision: It is equivalent to Precision@R where R is the total number of candidates labeled as true hypernyms of an entity.", "labels": [], "entities": []}, {"text": "Precision, Recall, and F-score: Besides, we can convert our ranking models to classification models by setting thresholds.", "labels": [], "entities": [{"text": "Precision", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.994670569896698}, {"text": "Recall", "start_pos": 11, "end_pos": 17, "type": "METRIC", "confidence": 0.9958382844924927}, {"text": "F-score", "start_pos": 23, "end_pos": 30, "type": "METRIC", "confidence": 0.9994124174118042}]}, {"text": "Varying the thresholds, we can get different precisions, recalls, and F-scores.", "labels": [], "entities": [{"text": "precisions", "start_pos": 45, "end_pos": 55, "type": "METRIC", "confidence": 0.9994833469390869}, {"text": "recalls", "start_pos": 57, "end_pos": 64, "type": "METRIC", "confidence": 0.9974936246871948}, {"text": "F-scores", "start_pos": 70, "end_pos": 78, "type": "METRIC", "confidence": 0.997114896774292}]}], "tableCaptions": [{"text": " Table 1: The features for ranking", "labels": [], "entities": []}, {"text": " Table 1. We illustrate them in detail in the following.", "labels": [], "entities": []}, {"text": " Table 3: The evaluation data", "labels": [], "entities": []}, {"text": " Table 4: Coverage evaluation of the candidate hypernym  extraction", "labels": [], "entities": [{"text": "Coverage evaluation", "start_pos": 10, "end_pos": 29, "type": "TASK", "confidence": 0.9504534602165222}]}, {"text": " Table 5: Precision@1 and R-Precision results on the test set. Here the present entities mean the entities existing in the  encyclopedias. The absent entities mean the ones not existing in the encyclopedias.", "labels": [], "entities": [{"text": "Precision", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9842735528945923}, {"text": "R-Precision", "start_pos": 26, "end_pos": 37, "type": "METRIC", "confidence": 0.9558261036872864}]}, {"text": " Table 7: Summary of maximum F-score on the test set", "labels": [], "entities": [{"text": "Summary", "start_pos": 10, "end_pos": 17, "type": "METRIC", "confidence": 0.8542711734771729}, {"text": "F-score", "start_pos": 29, "end_pos": 36, "type": "METRIC", "confidence": 0.9786648154258728}]}, {"text": " Table 8: Performance of LR models with different fea- tures on the test set", "labels": [], "entities": []}, {"text": " Table 10: Examples of entity-hypernym pairs extracted by M LR", "labels": [], "entities": []}, {"text": " Table 9: Performance of M LR in various domains", "labels": [], "entities": [{"text": "M LR", "start_pos": 25, "end_pos": 29, "type": "TASK", "confidence": 0.8734378516674042}]}]}