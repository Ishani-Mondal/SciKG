{"title": [{"text": "Joint Bootstrapping of Corpus Annotations and Entity Types", "labels": [], "entities": [{"text": "Joint Bootstrapping of Corpus Annotations and Entity Types", "start_pos": 0, "end_pos": 58, "type": "TASK", "confidence": 0.55481181666255}]}], "abstractContent": [{"text": "Web search can be enhanced in powerful ways if token spans in Web text are annotated with disambiguated entities from large catalogs like Freebase.", "labels": [], "entities": [{"text": "Web search", "start_pos": 0, "end_pos": 10, "type": "TASK", "confidence": 0.7048456817865372}]}, {"text": "Entity anno-tators need to be trained on sample mention snippets.", "labels": [], "entities": []}, {"text": "Wikipedia entities and annotated pages offer high-quality labeled data for training and evaluation.", "labels": [], "entities": []}, {"text": "Unfortunately, Wikipedia features only one-ninth the number of entities as Freebase, and these area highly biased sample of well-connected, frequently mentioned \"head\" entities.", "labels": [], "entities": []}, {"text": "To bring hope to \"tail\" entities, we broaden our goal to a second task: assigning types to entities in Freebase but not Wikipedia.", "labels": [], "entities": []}, {"text": "The two tasks are synergistic: knowing the types of unfamiliar entities helps disambiguate mentions, and words in mention contexts help assign types to entities.", "labels": [], "entities": []}, {"text": "We present TMI, a bipartite graphical model for joint type-mention inference.", "labels": [], "entities": [{"text": "joint type-mention inference", "start_pos": 48, "end_pos": 76, "type": "TASK", "confidence": 0.6327467262744904}]}, {"text": "TMI attempts no schema integration or entity resolution, but exploits the above-mentioned synergy.", "labels": [], "entities": [{"text": "TMI", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.650870680809021}, {"text": "entity resolution", "start_pos": 38, "end_pos": 55, "type": "TASK", "confidence": 0.7243169844150543}]}, {"text": "In experiments involving 780,000 people in Wikipedia, 2.3 million people in Free-base, 700 million Web pages, and over 20 professional editors, TMI shows considerable annotation accuracy improvement (e.g., 70%) compared to baselines (e.g., 46%), especially for \"tail\" and emerging entities.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 178, "end_pos": 186, "type": "METRIC", "confidence": 0.7800630331039429}]}, {"text": "We also compare with Google's recent annotations of the same corpus with Freebase entities, and report considerable improvements within the people domain.", "labels": [], "entities": []}], "introductionContent": [{"text": "Thanks to automatic information extraction and semantic Web efforts, keyword search over unstructured Web text is rapidly evolving toward entityand type-oriented queries () over semi-structured databases such as Wikipedia, Freebase, and other forms of Linked Data.", "labels": [], "entities": [{"text": "information extraction", "start_pos": 20, "end_pos": 42, "type": "TASK", "confidence": 0.7361968755722046}, {"text": "keyword search", "start_pos": 69, "end_pos": 83, "type": "TASK", "confidence": 0.7882378399372101}]}, {"text": "A key enabling component for such enhanced search capability is a type and entity catalog.", "labels": [], "entities": []}, {"text": "This includes a directed acyclic graph of types under the subTypeOf relation between types, and entities attached to one or more types via instanceOf edges.", "labels": [], "entities": []}, {"text": "* soumen@cse.iitb.ac.in) provides such a catalog by unifying Wikipedia and WordNet, followed by some cleanup.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 75, "end_pos": 82, "type": "DATASET", "confidence": 0.7576754093170166}]}, {"text": "Another enabling component is an annotated corpus in which token spans (e.g., the word \"Albert\") are identified as a mention of an entity (e.g., the Physicist Einstein).", "labels": [], "entities": []}, {"text": "Equipped with suitable indices, a catalog and an annotated corpus let us find \"scientists who played some musical instrument\", and answer many other powerful classes of queries (.", "labels": [], "entities": []}, {"text": "Consequently, accurate corpus annotation has been intensely investigated).", "labels": [], "entities": [{"text": "corpus annotation", "start_pos": 23, "end_pos": 40, "type": "TASK", "confidence": 0.641547754406929}]}, {"text": "With two exceptions () that we discuss later, public-domain corpus annotation work has almost exclusively used Wikipedia and derivatives, partly because Wikipedia provides not only a standardized space of entities, but also reliably labeled mention text within its own documents, which can be used to train machine learning algorithms for entity disambiguation.", "labels": [], "entities": [{"text": "entity disambiguation", "start_pos": 339, "end_pos": 360, "type": "TASK", "confidence": 0.7481894791126251}]}, {"text": "However, the high quality of Wikipedia comes at the cost of low entity coverage (4.2 million) and bias toward often-mentioned, richly-connected \"head\" entities.", "labels": [], "entities": []}, {"text": "Hereafter, Wikipedia entities are called W . Freebase has fewer editorial controls, but has at least nine times as many entities.", "labels": [], "entities": []}, {"text": "This is particularly perceptible for people entities: one needs to be relatively famous to be featured on Wikipedia, but Freebase is less selective.", "labels": [], "entities": []}, {"text": "Hereafter, Freebase entities are called F . As in any heavy-tailed distribution, even relatively obsecure entities from F \\ Ware collectively mentioned a great many times on the Web, and including them in Web annotation is critical, if entityoriented search is to impact the vast number of tail queries submitted to Web search engines.", "labels": [], "entities": []}, {"text": "Primary goal -corpus annotation: We have thus established a pressing need to bootstrap from a small entity catalog W (such as Wikipedia entities), and a small reference corpus C W (e.g., Wikipedia text) reliably annotated with entities from W , to a much larger catalog F (e.g.,, and an opendomain large payload corpus C (e.g., the Web).", "labels": [], "entities": []}, {"text": "We can and will use entities in F \u2229 W 1 in the bootstrapping process, but the real challenge is to annotate C with mentions m of entities in F \\ W . Unlike for F \u2229 W , we have no training mentions for F \\ W . Therefore, the main disambiguation signal is from the immediate entity neighborhood N (e) of the candidate entity e in the Freebase graph.", "labels": [], "entities": [{"text": "Freebase graph", "start_pos": 332, "end_pos": 346, "type": "DATASET", "confidence": 0.9734303057193756}]}, {"text": "I.e., if m also reliably mentions some entity in N (e), then e becomes a stronger candidate.", "labels": [], "entities": []}, {"text": "Unfortunately, for many \"tail\" entities e \u2208 F \\ W , N (e) is sparse.", "labels": [], "entities": []}, {"text": "Is there hope for annotating the Web with tail entities?", "labels": [], "entities": []}, {"text": "Here, we achieve enhanced accuracy for the primary annotation goal by extending it with a related secondary goal.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 26, "end_pos": 34, "type": "METRIC", "confidence": 0.9992384910583496}]}, {"text": "Secondary goal -entity typing: If we had available a suitable type catalog T with associated entities in W , which in turn have known textual mentions, we can build models of contexts referring to types like chemists, sportspeople and politicans.", "labels": [], "entities": []}, {"text": "When faced with people called John Williams in F \\W , we may first try to associate them with types.", "labels": [], "entities": []}, {"text": "This can then help disambiguate mentions to specific instances of John Williams in F \\ W . In principle, useful information may also flow in the reverse direction: words in mention contexts may help assign types to entities in F \\ W . For reasons to be made clear, we choose YAGO ( as the type catalog T accompanying entities in W . Our contributions: We present TMI, a bootstrapping system for solving the two tasks jointly.", "labels": [], "entities": [{"text": "YAGO", "start_pos": 275, "end_pos": 279, "type": "METRIC", "confidence": 0.8848960995674133}]}, {"text": "Apart from matches between the context of m and entity names in N (e), TMI combines and balances evidence from two other sources to decide if e is mentioned at token span m, and has type t: \u2022 a language model for the context in which entities of type tare usually mentioned With F =Freebase and W =Wikipedia, F \u2229W \u2248 W but not quite; W \\ F is small but non-empty.", "labels": [], "entities": []}, {"text": "\u2022 correlations between t and certain path features generated from N (e).", "labels": [], "entities": []}, {"text": "TMI uses a novel probabilistic graphical model formulation to integrate these signals.", "labels": [], "entities": [{"text": "TMI", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.615230917930603}]}, {"text": "We give a detailed account of our design of node and edge potentials, and a natural reject option (recall/precision tradeoff).", "labels": [], "entities": [{"text": "recall", "start_pos": 99, "end_pos": 105, "type": "METRIC", "confidence": 0.9986838698387146}, {"text": "precision", "start_pos": 106, "end_pos": 115, "type": "METRIC", "confidence": 0.6250460743904114}]}, {"text": "We report on extensive experiments using YAGO types, Wikipedia entities and text, Freebase entities, and text from ClueWeb12 2 , a 700-millionpage Web corpus.", "labels": [], "entities": []}, {"text": "We focus on all people entities in Wikipedia and Freebase, and provide three kinds of evaluation.", "labels": [], "entities": [{"text": "Freebase", "start_pos": 49, "end_pos": 57, "type": "DATASET", "confidence": 0.822847306728363}]}, {"text": "First, we evaluate TMI on over 1100 entities in F \u2229 W and 5500 snippets from Wikipedia text, where it visibly improves upon baselines and a recently proposed alternative method ().", "labels": [], "entities": [{"text": "TMI", "start_pos": 19, "end_pos": 22, "type": "TASK", "confidence": 0.7951317429542542}]}, {"text": "Second, we resort to extensive manual evaluation of annotation on ClueWeb12 Web text with Freebase entities, by professional editors at a commercial search company.", "labels": [], "entities": []}, {"text": "TMI again clearly outperforms strong baselines, doing particularly well for nascent or tail entities.", "labels": [], "entities": [{"text": "TMI", "start_pos": 0, "end_pos": 3, "type": "TASK", "confidence": 0.7469903230667114}]}, {"text": "TMI improves per-snippet accuracy, for some classes of entities, from 46% to 70%, and pooled F1 score from 66% to 73%.", "labels": [], "entities": [{"text": "TMI", "start_pos": 0, "end_pos": 3, "type": "METRIC", "confidence": 0.7852872014045715}, {"text": "accuracy", "start_pos": 25, "end_pos": 33, "type": "METRIC", "confidence": 0.9755395650863647}, {"text": "F1 score", "start_pos": 93, "end_pos": 101, "type": "METRIC", "confidence": 0.9536200761795044}]}, {"text": "Third, we compare TMI annotations with Google's FACC1 () annotations restricted to people; TMI is significantly better.", "labels": [], "entities": [{"text": "FACC1", "start_pos": 48, "end_pos": 53, "type": "DATASET", "confidence": 0.8154140710830688}, {"text": "TMI", "start_pos": 91, "end_pos": 94, "type": "METRIC", "confidence": 0.7949109077453613}]}, {"text": "Our annotations and related data can be downloaded from http://www.cse.iit b.ac.in/ \u02dc soumen/doc/CSAW/.", "labels": [], "entities": []}, {"text": "To our knowledge, this is among the first reports on extensive human evaluation of machine annotation for F \\ W on a large Web corpus.", "labels": [], "entities": []}], "datasetContent": [{"text": "We focus our experiments on one broad type of entities, people, that is more challenging for disambiguators than typical showcase examples of distinguishing (Steve) Jobs from employment and Apple Inc. from fruit.", "labels": [], "entities": []}, {"text": "We report on three sets of experiments.", "labels": [], "entities": []}, {"text": "In \u00a75.1, we restrict to entities from F \u2229 W and Wikipedia text, for which ground truth annotation is available.", "labels": [], "entities": []}, {"text": "In \u00a75.2, we evaluate TMI and baselines on ClueWeb12 and entities from Freebase, not limited to Wikipedia.", "labels": [], "entities": []}, {"text": "In \u00a75.3, we compare TMI with Google's recently published FACC1 annotations ().", "labels": [], "entities": [{"text": "FACC1", "start_pos": 57, "end_pos": 62, "type": "DATASET", "confidence": 0.7655538320541382}]}], "tableCaptions": []}