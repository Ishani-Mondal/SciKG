{"title": [{"text": "Centering Similarity Measures to Reduce Hubs", "labels": [], "entities": [{"text": "Centering Similarity Measures", "start_pos": 0, "end_pos": 29, "type": "TASK", "confidence": 0.8341202139854431}, {"text": "Hubs", "start_pos": 40, "end_pos": 44, "type": "TASK", "confidence": 0.4490068852901459}]}], "abstractContent": [{"text": "The performance of nearest neighbor methods is degraded by the presence of hubs, i.e., objects in the dataset that are similar to many other objects.", "labels": [], "entities": []}, {"text": "In this paper, we show that the classical method of centering, the transformation that shifts the origin of the space to the data centroid, provides an effective way to reduce hubs.", "labels": [], "entities": []}, {"text": "We show analytically why hubs emerge and why they are suppressed by centering , under a simple probabilistic model of data.", "labels": [], "entities": []}, {"text": "To further reduce hubs, we also move the origin more aggressively towards hubs, through weighted centering.", "labels": [], "entities": []}, {"text": "Our experimental results show that (weighted) centering is effective for natural language data; it improves the performance of the k-nearest neighbor classi-fiers considerably in word sense disambigua-tion and document classification tasks.", "labels": [], "entities": [{"text": "word sense disambigua-tion", "start_pos": 179, "end_pos": 205, "type": "TASK", "confidence": 0.646399974822998}, {"text": "document classification tasks", "start_pos": 210, "end_pos": 239, "type": "TASK", "confidence": 0.8105144302050272}]}], "introductionContent": [], "datasetContent": [{"text": "We did the same analysis as Sections 5.1.2-5.1.3 to areal dataset with multiple-cluster structure: the Reuters Transcribed dataset.", "labels": [], "entities": [{"text": "Reuters Transcribed dataset", "start_pos": 103, "end_pos": 130, "type": "DATASET", "confidence": 0.9709540009498596}]}, {"text": "This multi-class document classification dataset hasten classes, and each class roughly forms a cluster.", "labels": [], "entities": []}, {"text": "We will also use this dataset in an experiment in Section 7.2.", "labels": [], "entities": [{"text": "Section 7.2", "start_pos": 50, "end_pos": 61, "type": "DATASET", "confidence": 0.8313327133655548}]}, {"text": "The results are shown in.", "labels": [], "entities": []}, {"text": "We can observe the same trends as we saw in for the synthetic data: positive correlation between hubness (N 10 ) and inner product with the data centroid before centering; hubs appearing in the nearest neighbors of many objects of different classes; and both are reduced after centering.", "labels": [], "entities": []}, {"text": "The ratio of the height of black bars to that of all bars in(c) is 38.4% before centering, whereas it improves to 41.0% after centering).", "labels": [], "entities": []}, {"text": "We evaluated the effect of centering in two natural language tasks: word sense disambiguation (WSD) and document classification.", "labels": [], "entities": [{"text": "word sense disambiguation (WSD)", "start_pos": 68, "end_pos": 99, "type": "TASK", "confidence": 0.7917565703392029}, {"text": "document classification", "start_pos": 104, "end_pos": 127, "type": "TASK", "confidence": 0.7861765921115875}]}, {"text": "We are interested in whether hubs are actually reduced after centering, and whether the performance of kNN classification is improved.", "labels": [], "entities": [{"text": "kNN classification", "start_pos": 103, "end_pos": 121, "type": "TASK", "confidence": 0.6647106558084488}]}, {"text": "Throughout this section, K denotes cosine similarity matrix; i.e., inner product of feature vectors normalized to unit length; K cent denotes the centered similarity matrix computed by Eq.", "labels": [], "entities": []}, {"text": "(3) from K; K weighted denotes its hubness weighted variant given by Eq.", "labels": [], "entities": []}, {"text": "Depending on context, these symbols are also used to denote kNN classifiers using respective similarity measures.", "labels": [], "entities": []}, {"text": "For comparison, we also tested two recently proposed approaches to hub reduction: transformation of the base similarity measure (in our case, K) by Mutual Proximity (Schnitzer et al., 2012) 2 , and the one () based on graph Laplacian kernels.", "labels": [], "entities": [{"text": "hub reduction", "start_pos": 67, "end_pos": 80, "type": "TASK", "confidence": 0.712746188044548}]}, {"text": "Since the Laplacian kernels are defined for graph nodes, we computed them by taking the cosine similarity matrix K as the weighted adjacency (affinity) matrix of a graph.", "labels": [], "entities": [{"text": "cosine similarity matrix K", "start_pos": 88, "end_pos": 114, "type": "METRIC", "confidence": 0.723422959446907}, {"text": "weighted adjacency (affinity) matrix", "start_pos": 122, "end_pos": 158, "type": "METRIC", "confidence": 0.6879709909359614}]}, {"text": "For Laplacian kernels, we computed both the regularized Laplacian kernel) with several parameter values, as well as the commute-time kernel), but present only the best results among these kernels.", "labels": [], "entities": []}, {"text": "Two multiclass document classification datasets were used: Reuters Transcribed and Mini Newsgroups, distributed at http://archive.ics.uci.edu/ml/.", "labels": [], "entities": [{"text": "Reuters Transcribed and Mini Newsgroups", "start_pos": 59, "end_pos": 98, "type": "DATASET", "confidence": 0.8237857103347779}]}, {"text": "The properties of the datasets are summarized in Table 2.", "labels": [], "entities": []}, {"text": "The performance was evaluated by the F1 score (equivalent to accuracy in this task) of prediction using leave-one-out cross validation, due to the limited number of documents.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 37, "end_pos": 45, "type": "METRIC", "confidence": 0.9824538230895996}, {"text": "accuracy", "start_pos": 61, "end_pos": 69, "type": "METRIC", "confidence": 0.9988528490066528}, {"text": "prediction", "start_pos": 87, "end_pos": 97, "type": "TASK", "confidence": 0.9502665400505066}]}], "tableCaptions": [{"text": " Table 1: WSD results: Macro-averaged F1 score (points)", "labels": [], "entities": [{"text": "WSD", "start_pos": 10, "end_pos": 13, "type": "TASK", "confidence": 0.7267730832099915}, {"text": "F1 score", "start_pos": 38, "end_pos": 46, "type": "METRIC", "confidence": 0.9780312776565552}]}, {"text": " Table 2: Document classification datasets: Number of  classes, data size, and number of features.", "labels": [], "entities": [{"text": "Document classification", "start_pos": 10, "end_pos": 33, "type": "TASK", "confidence": 0.7997676730155945}]}, {"text": " Table 3: Document classification results: F1 score (%)  (larger is better) and skewness of the N 10 distribution for  each similarity measure (smaller is better).", "labels": [], "entities": [{"text": "Document classification", "start_pos": 10, "end_pos": 33, "type": "TASK", "confidence": 0.8316771984100342}, {"text": "F1 score", "start_pos": 43, "end_pos": 51, "type": "METRIC", "confidence": 0.9826364517211914}]}]}