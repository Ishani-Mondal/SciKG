{"title": [{"text": "Optimal Incremental Parsing via Best-First Dynamic Programming *", "labels": [], "entities": [{"text": "Optimal Incremental Parsing", "start_pos": 0, "end_pos": 27, "type": "TASK", "confidence": 0.6993133723735809}]}], "abstractContent": [{"text": "We present the first provably optimal polynomial time dynamic programming (DP) algorithm for best-first shift-reduce parsing, which applies the DP idea of Huang and Sagae (2010) to the best-first parser of Sagae and Lavie (2006) in a non-trivial way, reducing the complexity of the latter from exponential to polynomial.", "labels": [], "entities": []}, {"text": "We prove the correctness of our algorithm rigorously.", "labels": [], "entities": []}, {"text": "Experiments confirm that DP leads to a significant speedup on a probablistic best-first shift-reduce parser, and makes exact search under such a model tractable for the first time.", "labels": [], "entities": []}], "introductionContent": [{"text": "Best-first parsing, such as A* parsing, makes constituent parsing efficient, especially for bottom-up CKY style parsing.", "labels": [], "entities": [{"text": "A* parsing", "start_pos": 28, "end_pos": 38, "type": "TASK", "confidence": 0.5281083683172861}, {"text": "constituent parsing", "start_pos": 46, "end_pos": 65, "type": "TASK", "confidence": 0.6190347820520401}, {"text": "CKY style parsing", "start_pos": 102, "end_pos": 119, "type": "TASK", "confidence": 0.4944828748703003}]}, {"text": "Traditional CKY parsing performs cubic time exact search over an exponentially large space.", "labels": [], "entities": [{"text": "CKY parsing", "start_pos": 12, "end_pos": 23, "type": "TASK", "confidence": 0.6448980569839478}]}, {"text": "Best-first parsing significantly speeds up by always preferring to explore states with higher probabilities.", "labels": [], "entities": [{"text": "Best-first parsing", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.4605835825204849}]}, {"text": "In terms of incremental parsing, is the first work to extend best-first search to shift-reduce constituent parsing.", "labels": [], "entities": [{"text": "shift-reduce constituent parsing", "start_pos": 82, "end_pos": 114, "type": "TASK", "confidence": 0.6369238495826721}]}, {"text": "Unlike other very fast greedy parsers that produce suboptimal results, this best-first parser still guarantees optimality but requires exponential time for very long sentences in the worst case, which is intractable in practice.", "labels": [], "entities": []}, {"text": "Because it needs to explore an exponentially large space in the worst case, a bounded priority queue becomes necessary to ensure limited parsing time.", "labels": [], "entities": [{"text": "parsing", "start_pos": 137, "end_pos": 144, "type": "TASK", "confidence": 0.9613458514213562}]}, {"text": "On the other hand, explore the idea of dynamic programming, which is originated in bottom-up constituent parsing algorithms like, but in a beam-based non best-first parser.", "labels": [], "entities": []}, {"text": "In each beam step, they enable state merging in a style similar to the dynamic programming in bottom-up constituent parsing, based on an equivalence relation defined upon feature values.", "labels": [], "entities": []}, {"text": "Although in theory they successfully reduced the underlying deductive system to polynomial time complexity, their merging method is limited in that the state merging is only between two states in the same beam step.", "labels": [], "entities": []}, {"text": "This significantly reduces the number of possible merges, because: 1) there are only a very limited number of states in the beam at the same time; 2) a lot of states in the beam with different steps cannot be merged.", "labels": [], "entities": []}, {"text": "We instead propose to combine the idea of dynamic programming with the best-first search framework, and apply it in shift-reduce dependency parsing.", "labels": [], "entities": [{"text": "shift-reduce dependency parsing", "start_pos": 116, "end_pos": 147, "type": "TASK", "confidence": 0.6041355729103088}]}, {"text": "We merge states with the same features set globally to further reduce the number of possible states in the search graph.", "labels": [], "entities": []}, {"text": "Thus, our DP best-first algorithm is significantly faster than non-DP best-first parsing, and, more importantly, it has a polynomial time complexity even in the worst case.", "labels": [], "entities": []}, {"text": "We make the following contributions: \u2022 theoretically, we formally prove that our DP best-first parsing reaches optimality with polynomial time complexity.", "labels": [], "entities": [{"text": "DP best-first parsing", "start_pos": 81, "end_pos": 102, "type": "TASK", "confidence": 0.6022492349147797}]}, {"text": "This is the first time that exact search under such a probabilistic model becomes tractable.", "labels": [], "entities": []}, {"text": "\u2022 more interestingly, we reveal that our dynamic programming over shift-reduce parsing is in parallel with the bottom-up parsers, except that we have an extra order constraint given by the shift action to enforce left to right generation of input w 0 . .", "labels": [], "entities": []}, {"text": "w n\u22121 axiom 0 : 0, : 0 sh : j, S : c + 1 : j + 1, S|w j : c + sc sh (j, S) j < n re : j, S|s 1 |s 0 : c + 1 : j, S|s 1 s 0 : c + sc re (j, S|s 1 |s 0 ) re : j, S|s 1 |s 0 : c + 1 : j, S|s 1 s 0 : c + sc re (j, S|s 1 |s 0 ) Figure 1: Deductive system of basic non-DP shift-reduce parsing.", "labels": [], "entities": []}, {"text": "Here is the step index (for beam search), S is the stack, c is the score of the precedent, and sc a (x) is the score of action a from derivation x.", "labels": [], "entities": [{"text": "beam search", "start_pos": 28, "end_pos": 39, "type": "TASK", "confidence": 0.9182129204273224}]}, {"text": "See for the DP version.", "labels": [], "entities": []}, {"text": "partial trees, which is analogous to Earley.", "labels": [], "entities": []}, {"text": "\u2022 practically, our DP best-first parser is only \u223c2 times slower than a pure greedy parser, but is guaranteed to reach optimality.", "labels": [], "entities": []}, {"text": "In particular, it is \u223c20 times faster than a non-DP best-first parser.", "labels": [], "entities": []}, {"text": "With inexact search of bounded priority queue size, DP best-first search can reach optimality with a significantly smaller priority queue size bound, compared to non-DP bestfirst parser.", "labels": [], "entities": []}, {"text": "Our system is based on a MaxEnt model to meet the requirement from best-first search.", "labels": [], "entities": []}, {"text": "We observe that this locally trained model is not as strong as global models like structured perceptron.", "labels": [], "entities": []}, {"text": "With that being said, our algorithm shows its own merits in both theory and practice.", "labels": [], "entities": []}, {"text": "To find a better model for best-first search would bean interesting topic for future work.", "labels": [], "entities": []}], "datasetContent": [{"text": "In experiments we compare our DP best-first parsing with non-DP best-first parsing, pure greedy parsing, and beam parser of.", "labels": [], "entities": [{"text": "DP best-first parsing", "start_pos": 30, "end_pos": 51, "type": "TASK", "confidence": 0.5142700672149658}]}, {"text": "Our underlying MaxEnt model is trained on the Penn Treebank (PTB) following the standard split: Sections 02-21 as the training set and Section 22 as the held-out set.", "labels": [], "entities": [{"text": "Penn Treebank (PTB)", "start_pos": 46, "end_pos": 65, "type": "DATASET", "confidence": 0.9808343648910522}]}, {"text": "We collect gold actions at different parsing configurations as positive examples from gold parses in PTB to train the MaxEnt model.", "labels": [], "entities": [{"text": "PTB", "start_pos": 101, "end_pos": 104, "type": "DATASET", "confidence": 0.9664908647537231}]}, {"text": "We use the feature set of.", "labels": [], "entities": []}, {"text": "Furthermore, we reimplemented the beam parser with DP of Huang and Sagae (2010) for comparison.", "labels": [], "entities": [{"text": "beam parser", "start_pos": 34, "end_pos": 45, "type": "TASK", "confidence": 0.6949726194143295}, {"text": "DP", "start_pos": 51, "end_pos": 53, "type": "METRIC", "confidence": 0.9916172623634338}]}, {"text": "The result of our implementation is consistent with theirs.", "labels": [], "entities": []}, {"text": "We reach 92.39% accuracy with structured perceptron.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 16, "end_pos": 24, "type": "METRIC", "confidence": 0.9996114373207092}]}, {"text": "However, in experiments we still use MaxEnt to make the comparison fair.", "labels": [], "entities": [{"text": "MaxEnt", "start_pos": 37, "end_pos": 43, "type": "DATASET", "confidence": 0.762350857257843}]}, {"text": "To compare the performance we measure two sets of criteria: 1) the internal criteria consist of the model score of the parsing result, and the number of states explored; 2) the external criteria consist of the unlabeled accuracy of the parsing result, and the parsing time.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 220, "end_pos": 228, "type": "METRIC", "confidence": 0.9893172383308411}, {"text": "parsing", "start_pos": 260, "end_pos": 267, "type": "TASK", "confidence": 0.964495062828064}]}, {"text": "We perform our experiments on a computer with two 3.1GHz 8-core CPUs (16 processors in total) and 64GB RAM.", "labels": [], "entities": []}, {"text": "Our implementation is in Python.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Dynamic programming best-first parsing reach  optimality faster. *: for beam search we use beam size of  8. (All above results are averaged over the held-out set.)", "labels": [], "entities": [{"text": "beam search", "start_pos": 82, "end_pos": 93, "type": "TASK", "confidence": 0.9028920233249664}]}]}