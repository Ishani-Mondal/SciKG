{"title": [{"text": "Prior Disambiguation of Word Tensors for Constructing Sentence Vectors", "labels": [], "entities": [{"text": "Prior Disambiguation of Word Tensors", "start_pos": 0, "end_pos": 36, "type": "TASK", "confidence": 0.7860903024673462}]}], "abstractContent": [{"text": "Recent work has shown that compositional-distributional models using element-wise operations on contextual word vectors benefit from the introduction of a prior disambigua-tion step.", "labels": [], "entities": []}, {"text": "The purpose of this paper is to generalise these ideas to tensor-based models, where relational words such as verbs and adjectives are represented by linear maps (higher order tensors) acting on a number of arguments (vectors).", "labels": [], "entities": []}, {"text": "We propose disambiguation algorithms fora number of tensor-based models , which we then test on a variety of tasks.", "labels": [], "entities": []}, {"text": "The results show that disambiguation can provide better compositional representation even for the case of tensor-based models.", "labels": [], "entities": []}, {"text": "Furthermore , we confirm previous findings regarding the positive effect of disambiguation on vector mixture models, and we compare the effectiveness of the two approaches.", "labels": [], "entities": []}], "introductionContent": [{"text": "Distributional models of meaning have been proved extremely useful fora number of natural language processing tasks, ranging from thesaurus extraction) to topic modelling) and information retrieval (, to name just a few.", "labels": [], "entities": [{"text": "thesaurus extraction", "start_pos": 130, "end_pos": 150, "type": "TASK", "confidence": 0.7512229084968567}, {"text": "topic modelling", "start_pos": 155, "end_pos": 170, "type": "TASK", "confidence": 0.803900420665741}, {"text": "information retrieval", "start_pos": 176, "end_pos": 197, "type": "TASK", "confidence": 0.8350598216056824}]}, {"text": "These models are based on the distributional hypothesis of, which states that the meaning of a word depends on its context.", "labels": [], "entities": []}, {"text": "This idea allows the words to be represented by vectors of statistics collected from a sufficiently large corpus of text; each element of the vector reflects how many times a word co-occurs in the same context with another word of the vocabulary.", "labels": [], "entities": []}, {"text": "However, due to the generative power of natural language, which is able to produce infinite new structures from a finite set of resources (words), no text corpus, regardless of its size, can provide reliable distributional representations for anything longer than single words or perhaps very short phrases consisting of two words; in other words, this technique cannot scale up to the phrase or sentence level.", "labels": [], "entities": []}, {"text": "Much research activity has been recently dedicated to provide a solution to this problem: although the direct construction of a sentence vector is not possible, we might still be able to synthetically create such a vectorial representation by somehow composing the vectors of the words that comprise the sentence.", "labels": [], "entities": []}, {"text": "Towards this goal, researchers have employed a variety of approaches that roughly fall into two general categories.", "labels": [], "entities": []}, {"text": "Following an influential work, the models in the first category compute a sentence vector as a mixture of the original word vectors, using simple operations such as element-wise multiplication and addition; we refer to these models as vector mixtures.", "labels": [], "entities": []}, {"text": "The main characteristic of these models is that they do not distinguish between the type-logical identities of the different words: an intransitive verb, for example, is of the same order as its subject (a noun), and both will contribute equally to the composite sentence vector.", "labels": [], "entities": []}, {"text": "However, this symmetric treatment of composition seems unjustified from a formal semantics point of view.", "labels": [], "entities": []}, {"text": "Words with special meanings, such as verbs and adjectives, are usually seen as functions acting on, hence modifying, a number of arguments rather than lexical units of the same order as them; an adjective, for example, is a function that returns a modified version of its input noun.", "labels": [], "entities": []}, {"text": "Inspired from this more-aligned-to-formal-semantics view, a second research direction aims to represent relational words as linear maps (tensors of various orders) that can be applied to one or more arguments., for example, model adjectives as matrices which, when matrixmultiplied with a noun vector, will produce a vectorial representation of the specific adjective-noun compound.", "labels": [], "entities": []}, {"text": "The notion of a framework where relational words are entities living in vector spaces of higher order than nouns, which are simple vectors, has been formalized by in the context of the abstract mathematical framework of compact closed categories.", "labels": [], "entities": []}, {"text": "We refer to this class of models as tensor-based.", "labels": [], "entities": []}, {"text": "Regardless of the way they approach the representation of relational words and their composition operation, however, most current compositionaldistributional models do share a common feature: they all rely on ambiguous vector representations, where all the senses of a polysemous word, such as the verb 'file' (which can mean register or smooth), are merged into the same vector or tensor.", "labels": [], "entities": []}, {"text": "At least for the vector mixture approach, this practice has been proved suboptimal: and test a number of simple multiplicative and additive models using disambiguated vector representations on various tasks, showing that the introduction of a disambiguation step prior to actual composition can indeed increase the quality of the composite vectors.", "labels": [], "entities": []}, {"text": "However, the fact that disambiguation can be beneficial for models based on vector mixtures is not very surprising.", "labels": [], "entities": []}, {"text": "Both additive and multiplicative compositions are but a kind of average of the vectors of the words in the sentence, hence can directly benefit from the provision of more accurate starting points.", "labels": [], "entities": []}, {"text": "Perhaps a more interesting question, and one that the current paper aims to address, is to what extent disambiguation can also provide benefits for tensor-based approaches, which in general constitute more powerful models for natural language (see discussion in Section 2).", "labels": [], "entities": []}, {"text": "Specifically, this paper aims to: (a) propose disambiguation algorithms fora number of tensorbased distributional models; (b) examine the effect of disambiguation on tensors for relational words; and (c) meaningfully compare the effectiveness of tensor-based against vector mixture models in a number of tasks.", "labels": [], "entities": []}, {"text": "Based on the generic procedure of Sch\u00fctze (1998), we propose algorithms fora number of tensor-based models, where the composition is modelled as the application of linear maps (tensor contractions).", "labels": [], "entities": []}, {"text": "Following and many others, we test our models on two disambiguation tasks similar to that of, and on the phrase similarity task introduced in.", "labels": [], "entities": [{"text": "phrase similarity", "start_pos": 105, "end_pos": 122, "type": "TASK", "confidence": 0.7455636560916901}]}, {"text": "In almost every case, the results show that disambiguation can make a great difference in the case of tensor-based models; they also reconfirm previous findings regarding the effectiveness of the method for simple vector mixture models.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section we will test the effect of disambiguation on the models of Section 5 in a variety of tasks.", "labels": [], "entities": []}, {"text": "Due to the significant methodological differences of the linear regression model from the other approaches and the variety of its set of parameters, we decided that it would be better if this was left as the subject of a distinct work.", "labels": [], "entities": []}, {"text": "Experimental setting We train our vectors using ukWaC), a corpus of English text with 2 billion words (100m sentences).", "labels": [], "entities": [{"text": "ukWaC", "start_pos": 48, "end_pos": 53, "type": "DATASET", "confidence": 0.9352784752845764}]}, {"text": "We use 2000 dimensions, with weights calculated as the ratio of the probability of the context word given the target word to the probability of the context word overall.", "labels": [], "entities": []}, {"text": "The context here is a 5-word window on both sides of the target word.", "labels": [], "entities": []}, {"text": "The vectors are disambiguated both syntactically and semantically: first, separate vectors have been created for different syntactic usages of the same word in the corpus; for example, the word 'book' has two vectors, one for its noun sense and one for its verb sense.", "labels": [], "entities": []}, {"text": "Furthermore, each word is semantically disambiguated according to the method of Section 6.", "labels": [], "entities": []}, {"text": "Models We compare the tensor-based models of Section 5 with the multiplicative and additive models of, reporting results for both ambiguous and disambiguated versions.", "labels": [], "entities": []}, {"text": "For all the disambiguated models, the best sense for each word in the sentence or phrase is first selected by applying the procedure of Section 6 and Equation 9.", "labels": [], "entities": []}, {"text": "If the model is based on a vector mixture, the sense vectors corresponding to these senses are multiplied or added to form the composite representation for the sentence or phrase.", "labels": [], "entities": []}, {"text": "For the tensor-based models, the composite meanings are calculated ac-cording to the equations of Section 5, using verb tensors created by the procedures of Section 7.", "labels": [], "entities": []}, {"text": "The semantic similarity of two phrases or sentences is measured as the cosine distance between their composite vectors.", "labels": [], "entities": []}, {"text": "For models that return a matrix (e.g. Relational, Kronecker), the distance is based on the Frobenius inner product.", "labels": [], "entities": []}, {"text": "Implementation details Our code is mainly written in Python and C++, and for the actual clustering step we use the Python interface of the efficient FASTCLUSTER library.", "labels": [], "entities": [{"text": "Implementation", "start_pos": 0, "end_pos": 14, "type": "METRIC", "confidence": 0.894583523273468}]}, {"text": "Ina shared 24-core Xeon machine with 72 GB of memory, and with a fair amount of parallelism applied, the average processing time per word was about 4 minutes; this is roughly translated to 12-13 hours of training on average per dataset.", "labels": [], "entities": [{"text": "Ina", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.9223605394363403}]}], "tableCaptions": [{"text": " Table 1: Results for the G&S dataset.", "labels": [], "entities": [{"text": "G&S dataset", "start_pos": 26, "end_pos": 37, "type": "DATASET", "confidence": 0.881890058517456}]}, {"text": " Table 2: Results for the Kartsaklis et al. dataset.", "labels": [], "entities": [{"text": "Kartsaklis et al. dataset", "start_pos": 26, "end_pos": 51, "type": "DATASET", "confidence": 0.6350337117910385}]}, {"text": " Table 3: Results for the original M&L task.", "labels": [], "entities": [{"text": "M&L task", "start_pos": 35, "end_pos": 43, "type": "TASK", "confidence": 0.9011145383119583}]}, {"text": " Table 4: Transitive version of M&L task.", "labels": [], "entities": [{"text": "M&L task", "start_pos": 32, "end_pos": 40, "type": "TASK", "confidence": 0.8057318478822708}]}]}