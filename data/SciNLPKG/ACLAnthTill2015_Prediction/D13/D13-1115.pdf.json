{"title": [{"text": "A Multimodal LDA Model Integrating Textual, Cognitive and Visual Modalities", "labels": [], "entities": []}], "abstractContent": [{"text": "Recent investigations into grounded models of language have shown that holistic views of language and perception can provide higher performance than independent views.", "labels": [], "entities": []}, {"text": "In this work, we improve a two-dimensional multi-modal version of Latent Dirichlet Allocation (Andrews et al., 2009) in various ways.", "labels": [], "entities": []}, {"text": "(1) We outperform text-only models in two different evaluations, and demonstrate that low-level visual features are directly compatible with the existing model.", "labels": [], "entities": []}, {"text": "(2) We present a novel way to integrate visual features into the LDA model using unsupervised clusters of images.", "labels": [], "entities": []}, {"text": "The clusters are directly interpretable and improve on our evaluation tasks.", "labels": [], "entities": []}, {"text": "(3) We provide two novel ways to extend the bimodal models to support three or more modalities.", "labels": [], "entities": []}, {"text": "We find that the three-, four-, and five-dimensional models significantly outperform models using only one or two modalities, and that nontex-tual modalities each provide separate, disjoint knowledge that cannot be forced into a shared, latent structure.", "labels": [], "entities": []}], "introductionContent": [{"text": "In recent years, an increasing body of work has been devoted to multimodal or \"grounded\" models of language where semantic representations of words are extended to include perceptual information.", "labels": [], "entities": []}, {"text": "The underlying hypothesis is that the meanings of words are explicitly tied to our perception and understanding of the world around us, and textual-information alone is insufficient fora complete understanding of language.", "labels": [], "entities": []}, {"text": "The language grounding problem has come in many different flavors with just as many different approaches.", "labels": [], "entities": []}, {"text": "Some approaches apply semantic parsing, where words and sentences are mapped to logical structure meaning.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 22, "end_pos": 38, "type": "TASK", "confidence": 0.7341645658016205}]}, {"text": "Others provide automatic mappings of natural language instructions to executable actions, such as interpreting navigation directions ( or robot commands.", "labels": [], "entities": [{"text": "interpreting navigation directions", "start_pos": 98, "end_pos": 132, "type": "TASK", "confidence": 0.8993123571077982}]}, {"text": "Some efforts have tackled tasks such as automatic image caption generation), text illustration (), or automatic location identification of Twitter users).", "labels": [], "entities": [{"text": "automatic image caption generation", "start_pos": 40, "end_pos": 74, "type": "TASK", "confidence": 0.7073104679584503}, {"text": "text illustration", "start_pos": 77, "end_pos": 94, "type": "TASK", "confidence": 0.8340851962566376}, {"text": "automatic location identification of Twitter users", "start_pos": 102, "end_pos": 152, "type": "TASK", "confidence": 0.757219543059667}]}, {"text": "Another line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information (.", "labels": [], "entities": []}, {"text": "Although these approaches have differed in model definition, the general goal in this line of research has been to enhance word meaning with perceptual information in order to address one of the most common criticisms of distributional semantics: that the \"meaning of words is entirely given by other words\" (.", "labels": [], "entities": []}, {"text": "In this paper, we explore various ways to integrate new perceptual information through novel computational modeling of this grounded knowledge into a multimodal distributional model of word meaning.", "labels": [], "entities": []}, {"text": "The model we rely on was originally developed by and is based on a generalization of Latent Dirichlet Allocation.", "labels": [], "entities": []}, {"text": "This model has previously been shown to provide excellent performance on multiple tasks, including prediction of association norms, word substitution errors, semantic inferences, and word similarity (.", "labels": [], "entities": [{"text": "prediction of association norms", "start_pos": 99, "end_pos": 130, "type": "TASK", "confidence": 0.8191758841276169}]}, {"text": "While prior work has used the model only with feature norms and visual attributes, we show that low-level image features are directly compatible with the model and provide improved representations of word meaning.", "labels": [], "entities": []}, {"text": "We also show how simple, unsupervised clusters of images can act as a semantically useful and qualitatively interesting set of features.", "labels": [], "entities": []}, {"text": "Finally, we describe two ways to extend the model by incorporating three or more modalities.", "labels": [], "entities": []}, {"text": "We find that each modality provides useful but disjoint information for describing word meaning, and that a hybrid integration of multiple modalities provides significant improvements in the representations of word meaning.", "labels": [], "entities": []}, {"text": "We release both our code and data to the community for future research.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluate each of our models with two data sets: a set of compositionality ratings fora number of German noun-noun compounds, and the same association norm data set used as one of our training modalities in some settings.", "labels": [], "entities": []}, {"text": "Compositionality Ratings is a data set of compositionality ratings originally collected by von der.", "labels": [], "entities": []}, {"text": "The data set consists of 450 concrete, depictable German noun compounds along with compositionality ratings with regard to their constituents.", "labels": [], "entities": []}, {"text": "For each compound, 30 native German speakers are asked to rate how related the meaning of the compound is to each of its constituents on a scale from 1 (highly opaque; entirely noncompositional) to 7 (highly transparent; very compositional).", "labels": [], "entities": []}, {"text": "The mean of the 30 judgments is taken as the gold compositionality rating for each of the compound-constituent pairs.", "labels": [], "entities": []}, {"text": "For example, Ahornblatt 'maple leaf' is rated highly transparent with respect to its constituents, Ahorn 'maple' and Blatt 'leaf', but L\u00f6wenzahn 'dandelion' is rated noncompositional with respect to its constituents, L\u00f6we 'lion' and Zahn 'tooth'.", "labels": [], "entities": []}, {"text": "We use a subset of the original data, comprising of all two-part noun-noun compounds and their constituents.", "labels": [], "entities": []}, {"text": "This data set consists of 488 compositionality ratings (244 compound-head and 244 compound-modifier ratings) for 571 words.", "labels": [], "entities": []}, {"text": "309 of the targets have images (the entire image data set); 563 have feature norms; and all 571 of have association norms.", "labels": [], "entities": []}, {"text": "In order to predict compositionality, for each compound-constituent pair (w compound , w constituent ), we compute negative symmetric KL divergence between the two words' topic distributions, where symmetric KL divergence is defined as and KL divergence is defined as The values of \u2212sKL for all compoundconstituent word pairs are correlated with the human judgments of compositionality using Spearman's \u03c1, a rank-order correlation coefficient.", "labels": [], "entities": []}, {"text": "Note that, since KL divergence is a measure of dissimilarity, we use negative symmetric KL divergence so that our \u03c1 correlation coefficient is positive.", "labels": [], "entities": [{"text": "KL divergence", "start_pos": 17, "end_pos": 30, "type": "TASK", "confidence": 0.6812704801559448}, {"text": "\u03c1 correlation coefficient", "start_pos": 114, "end_pos": 139, "type": "METRIC", "confidence": 0.9078200260798136}]}, {"text": "For example, we compute both \u2212sKL(Ahornblatt, Ahorn) and \u2212sKL(Ahornblatt, Blatt), and soon for all 488 compound-constituent pairs, and then correlate these values with the human judgments.", "labels": [], "entities": []}, {"text": "Additionally, we also evaluate using the Association Norms data set described in Section 3.", "labels": [], "entities": [{"text": "Association Norms data set", "start_pos": 41, "end_pos": 67, "type": "DATASET", "confidence": 0.7341724410653114}]}, {"text": "Since it is not sensible to evaluate association norm prediction when they are also used as training data, we omit this evaluation for this modality.", "labels": [], "entities": [{"text": "association norm prediction", "start_pos": 37, "end_pos": 64, "type": "TASK", "confidence": 0.670637716849645}]}, {"text": "Following, we measure association norm prediction as an average of percentile ranks.", "labels": [], "entities": [{"text": "association norm prediction", "start_pos": 22, "end_pos": 49, "type": "TASK", "confidence": 0.649535725514094}]}, {"text": "For all possible pairs of words in our vocabulary, we compute the negative symmetric KL divergence between the two words.", "labels": [], "entities": []}, {"text": "We then compute the percentile ranks of similarity for each word pair, e.g., \"cat\" is more similar to \"dog\" than 97.3% of the rest of the vocabulary.", "labels": [], "entities": [{"text": "similarity", "start_pos": 40, "end_pos": 50, "type": "METRIC", "confidence": 0.9423354268074036}]}, {"text": "We report the weighted mean percentile ranks for all cue-association pairs, i.e., if a cue-association is given more than once, it is counted more than once.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1:  Average rank correlations between  \u2212sKL(w compound , w constituent ) and our Composi- tionality gold standard. The Hybrid models are the  concatenation of the corresponding Bimodal mLDA  models. Stars indicate statistical significance compared  to the text-only setting at the .05, .01 and .001 levels  using a two-tailed t-test.", "labels": [], "entities": [{"text": "Composi- tionality gold standard", "start_pos": 88, "end_pos": 120, "type": "DATASET", "confidence": 0.6075281441211701}]}, {"text": " Table 2: Average predicted rank similarity between cue  words and their associates. Stars indicate statistical sig- nificance compared to the text-only modality, with gray  stars indicating the model is statistically worse than the  text model. The Hybrid models are the concatenation of  the corresponding Bimodal mLDA models.", "labels": [], "entities": []}]}