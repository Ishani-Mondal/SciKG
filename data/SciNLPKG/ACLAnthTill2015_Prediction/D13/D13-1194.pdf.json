{"title": [{"text": "Detection of Product Comparisons -How Far Does an Out-of-the-box Semantic Role Labeling System Take You?", "labels": [], "entities": [{"text": "Detection of Product Comparisons", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.8853500783443451}, {"text": "Semantic Role Labeling", "start_pos": 65, "end_pos": 87, "type": "TASK", "confidence": 0.6582384705543518}]}], "abstractContent": [{"text": "This short paper presents a pilot study investigating the training of a standard Semantic Role Labeling (SRL) system on product reviews for the new task of detecting comparisons.", "labels": [], "entities": [{"text": "Semantic Role Labeling (SRL)", "start_pos": 81, "end_pos": 109, "type": "TASK", "confidence": 0.7710292339324951}, {"text": "detecting comparisons", "start_pos": 156, "end_pos": 177, "type": "TASK", "confidence": 0.8832410871982574}]}, {"text": "An (opinionated) comparison consists of a comparative \"predicate\" and up to three \"arguments\": the entity evaluated positively , the entity evaluated negatively, and the aspect under which the comparison is made.", "labels": [], "entities": []}, {"text": "In user-generated product reviews, the \"predi-cate\" and \"arguments\" are expressed in highly heterogeneous ways; but since the elements are textually annotated in existing datasets, SRL is technically applicable.", "labels": [], "entities": [{"text": "SRL", "start_pos": 181, "end_pos": 184, "type": "TASK", "confidence": 0.8327832221984863}]}, {"text": "We address the interesting question how well training an out-of-the-box SRL model works for English data.", "labels": [], "entities": []}, {"text": "We observe that even without any feature engineering or other major adaptions to our task, the system outperforms a reasonable heuristic baseline in all steps (predicate identification, argument identification and argument classification) and in three different datasets.", "labels": [], "entities": [{"text": "predicate identification", "start_pos": 160, "end_pos": 184, "type": "TASK", "confidence": 0.7319436371326447}, {"text": "argument identification", "start_pos": 186, "end_pos": 209, "type": "TASK", "confidence": 0.6766783595085144}, {"text": "argument classification", "start_pos": 214, "end_pos": 237, "type": "TASK", "confidence": 0.7313120663166046}]}], "introductionContent": [{"text": "Sentiment analysis deals with the task of determining the polarity of an opinionated document or a sentence, in product reviews typically with regard to some target product.", "labels": [], "entities": [{"text": "Sentiment analysis", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.9614555239677429}]}, {"text": "A common way to express sentiment about some product is by comparing it to a different product.", "labels": [], "entities": []}, {"text": "In the corpus data we use, around 10% of sentences contain at least one comparison.", "labels": [], "entities": []}, {"text": "Here are some examples of comparison sentences from our corpus: Note that our definition of comparisons is broader than the linguistic category of comparative sentences, which only includes sentences that contain a comparative adjective or adverb.", "labels": [], "entities": []}, {"text": "For our work, we consider comparisons expressed by any Part of Speech (POS).", "labels": [], "entities": []}, {"text": "A comparison contains several parts that must be identified in order to get meaningful information.", "labels": [], "entities": []}, {"text": "We call the word or phrase that is used to express the comparison (\"better\", \"beats\", . .", "labels": [], "entities": []}, {"text": "A comparison involves two entities, one or both of them maybe implicit.", "labels": [], "entities": []}, {"text": "In our data, most of the entities are products, e.g., the two cameras \"D70\" and \"EOS 300D\" in sentence 1b.", "labels": [], "entities": []}, {"text": "In graded comparisons, entity+ (E+) is the entity that is being evaluated positively, entity-(E-) the entity evaluated negatively.", "labels": [], "entities": []}, {"text": "In many sentences one attribute or part of a product is being compared, like \"image sensor\" in sentence 1d.", "labels": [], "entities": []}, {"text": "We call this the aspect (A).", "labels": [], "entities": []}, {"text": "The task we want to solve fora given comparison sentence is to detect the comparative predicate, the entities that are involved and the aspect that is being compared.", "labels": [], "entities": []}, {"text": "We borrow our methodology from Semantic Role Labeling (SRL).", "labels": [], "entities": [{"text": "Semantic Role Labeling (SRL)", "start_pos": 31, "end_pos": 59, "type": "TASK", "confidence": 0.7801476120948792}]}, {"text": "In SRL, events are expressed by predicates and participants of these events are expressed by arguments that fill different semantic roles.", "labels": [], "entities": [{"text": "SRL", "start_pos": 3, "end_pos": 6, "type": "TASK", "confidence": 0.9768735766410828}]}, {"text": "Adapted to the problem of detecting comparisons, the events we are interested in are comparative predicates and the arguments are the two entities and the aspect that is being compared.", "labels": [], "entities": []}, {"text": "Due to the diversity of possible ways of expressing comparisons, the \"predicates\" and \"arguments\" in this task are more heterogeneous categories than in standard SRL based on PropBank and NomBank annotations.", "labels": [], "entities": [{"text": "PropBank", "start_pos": 175, "end_pos": 183, "type": "DATASET", "confidence": 0.9419072270393372}]}, {"text": "Moreoever, the existing labeled datasets are based on an annotation methodology which gave the annotators a lot of freedom in deciding on the linguistic anchoring of the \"predicate\" and \"arguments\".", "labels": [], "entities": []}, {"text": "This adds to the heterogeneity of the observed constructions and makes it even more interesting to ask the question how far an out-of-thebox SRL model can take you.", "labels": [], "entities": []}, {"text": "In this work, we re-train an existing SRL system) on product review data labeled with comparative predicates and arguments.", "labels": [], "entities": []}, {"text": "We show that we can get reasonable results without any feature engineering or other major adaptions.", "labels": [], "entities": []}, {"text": "This is an encouraging result fora linguistically grounded modeling approach to comparison detection.", "labels": [], "entities": [{"text": "comparison detection", "start_pos": 80, "end_pos": 100, "type": "TASK", "confidence": 0.9740250408649445}]}], "datasetContent": [{"text": "We use the JDPA corpus 2 by J. for our experiments.", "labels": [], "entities": [{"text": "JDPA corpus 2", "start_pos": 11, "end_pos": 24, "type": "DATASET", "confidence": 0.9673747221628824}]}, {"text": "It contains blog posts about cameras and cars.", "labels": [], "entities": []}, {"text": "We use the annotation class \"Comparison\" that has four annotation slots.", "labels": [], "entities": []}, {"text": "We convert the \"more\" slot to entity+, the \"less\" slot to entity-and the \"dimension\" slot to the aspect.", "labels": [], "entities": []}, {"text": "For now, we ignore the \"same\" slot which indicates if the two mentions are ranked as equal.", "labels": [], "entities": []}, {"text": "We have also tested our approach on the dataset used in (Jindal and  parisons annotated as types 1 to 3 (ignoring type 4, non-gradable comparisons).", "labels": [], "entities": []}, {"text": "In this dataset (J&L), entities are annotated as entity 1 or entity 2 depending on their position before or after the predicate.", "labels": [], "entities": []}, {"text": "We keep this annotation and train our system to assign these labels.", "labels": [], "entities": []}, {"text": "We do sentence segmentation and tokenization with the Stanford Core NLP 4 . Annotations are mapped to the extracted tokens.", "labels": [], "entities": [{"text": "sentence segmentation", "start_pos": 6, "end_pos": 27, "type": "TASK", "confidence": 0.7343789637088776}, {"text": "Stanford Core NLP 4", "start_pos": 54, "end_pos": 73, "type": "DATASET", "confidence": 0.9592930823564529}]}, {"text": "We ignore annotations that do not correspond to complete tokens.", "labels": [], "entities": []}, {"text": "In the JDPA corpus, if an annotated argument is outside the current sentence, we follow the coreference chain to find a coreferent annotation in the same sentence.", "labels": [], "entities": [{"text": "JDPA corpus", "start_pos": 7, "end_pos": 18, "type": "DATASET", "confidence": 0.9307054579257965}]}, {"text": "If this is not successful, the argument is ignored.", "labels": [], "entities": []}, {"text": "We extract all sentences where we found at least one comparative predicate as our dataset.", "labels": [], "entities": []}, {"text": "shows some statistics of the data.", "labels": [], "entities": []}, {"text": "We evaluate on each dataset separately using 5-fold cross-validation.", "labels": [], "entities": []}, {"text": "We report precision (P), recall (R), F1-measure (F1), and for argument classification macro averaged F1-measure (F1 m ) over the three arguments.", "labels": [], "entities": [{"text": "precision (P)", "start_pos": 10, "end_pos": 23, "type": "METRIC", "confidence": 0.931811735033989}, {"text": "recall (R)", "start_pos": 25, "end_pos": 35, "type": "METRIC", "confidence": 0.937002569437027}, {"text": "F1-measure (F1)", "start_pos": 37, "end_pos": 52, "type": "METRIC", "confidence": 0.8286637961864471}, {"text": "argument classification", "start_pos": 62, "end_pos": 85, "type": "TASK", "confidence": 0.701541543006897}, {"text": "F1-measure (F1 m )", "start_pos": 101, "end_pos": 119, "type": "METRIC", "confidence": 0.8208727478981018}]}, {"text": "Bold numbers denote the best result in each column and dataset.", "labels": [], "entities": []}, {"text": "We mark a F1-measure result with * if it is significantly higher than all previous lines.", "labels": [], "entities": [{"text": "F1-measure", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9986796975135803}]}, {"text": "We have implemented two baselines based on previous work.", "labels": [], "entities": []}, {"text": "The simplest baseline, BL POS classifies all tokens with a comparative POS ('JJR', 'JJS', 'RBR', 'RBS') as predicates.", "labels": [], "entities": []}, {"text": "A more sophisticated baseline, BL Keyphrases, uses a list of about 80 manually comcomparable to the results reported there.", "labels": [], "entities": [{"text": "BL Keyphrases", "start_pos": 31, "end_pos": 44, "type": "DATASET", "confidence": 0.6345508098602295}]}, {"text": "http://nlp.stanford.edu/software/ corenlp.shtml 5 Statistically significant at p < .05 using the approximate randomization test     piled comparative keyphrases from (Jindal and) in addition to the POS tags.", "labels": [], "entities": []}, {"text": "shows the result of our experiments.", "labels": [], "entities": []}, {"text": "Our method significantly outperforms both baselines in all datasets.", "labels": [], "entities": []}, {"text": "The generally low recall values are mainly a result of the wide variety of predicates that are used to express comparisons (see Discussion).", "labels": [], "entities": [{"text": "recall", "start_pos": 18, "end_pos": 24, "type": "METRIC", "confidence": 0.9987793564796448}]}, {"text": "To get results indepent of the errors introduced by the relatively low performance on predicate identification, we use annotated predicates (gold predicates) as a starting point for the argument experiments.", "labels": [], "entities": [{"text": "predicate identification", "start_pos": 86, "end_pos": 110, "type": "TASK", "confidence": 0.9111764132976532}]}, {"text": "All results drop about 10% when system predicates are used.", "labels": [], "entities": []}, {"text": "As a baseline (BL) for argument identification and classification, we use some heuristics based on the characteristics of our data.", "labels": [], "entities": [{"text": "BL", "start_pos": 15, "end_pos": 17, "type": "METRIC", "confidence": 0.9458070993423462}, {"text": "argument identification and classification", "start_pos": 23, "end_pos": 65, "type": "TASK", "confidence": 0.7240052223205566}]}, {"text": "Most entities are (pro)nouns and most predicates are positive, so we classify the first noun or pronoun before the predicate as entity+ (entity 1 for J&L) and the first noun or pronoun after the predicate as a entity-(entity 2).", "labels": [], "entities": []}, {"text": "If the predicate is a comparative adjective, we classify the predicate itself as aspect, because this type of annotation is very frequent in the JDPA data.", "labels": [], "entities": [{"text": "JDPA data", "start_pos": 145, "end_pos": 154, "type": "DATASET", "confidence": 0.9646172821521759}]}, {"text": "For other predicates except nouns and verbs, we classify the direct head of the predicate as aspect.", "labels": [], "entities": []}, {"text": "shows the results for argument identification, the results for argument classification can be seen in.", "labels": [], "entities": [{"text": "argument identification", "start_pos": 22, "end_pos": 45, "type": "TASK", "confidence": 0.8475425541400909}, {"text": "argument classification", "start_pos": 63, "end_pos": 86, "type": "TASK", "confidence": 0.791886955499649}]}, {"text": "Our system outperforms the baseline for all datasets.", "labels": [], "entities": []}, {"text": "The differences are significant except for the cameras dataset.", "labels": [], "entities": [{"text": "cameras dataset", "start_pos": 47, "end_pos": 62, "type": "DATASET", "confidence": 0.7892860770225525}]}, {"text": "In general, the numbers are low.", "labels": [], "entities": []}, {"text": "We will discuss some reasons for this in the next section.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Statistics about the datasets", "labels": [], "entities": []}, {"text": " Table 2: Results predicate identification", "labels": [], "entities": [{"text": "Results predicate", "start_pos": 10, "end_pos": 27, "type": "TASK", "confidence": 0.5465944707393646}]}, {"text": " Table 3: Results argument identification (gold predicates)", "labels": [], "entities": [{"text": "Results argument identification", "start_pos": 10, "end_pos": 41, "type": "TASK", "confidence": 0.7069843411445618}]}, {"text": " Table 4: Results argument classification (gold predicates)", "labels": [], "entities": []}]}