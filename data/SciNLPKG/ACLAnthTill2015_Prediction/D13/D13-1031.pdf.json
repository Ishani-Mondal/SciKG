{"title": [{"text": "Exploring Representations from Unlabeled Data with Co-training for Chinese Word Segmentation", "labels": [], "entities": [{"text": "Chinese Word Segmentation", "start_pos": 67, "end_pos": 92, "type": "TASK", "confidence": 0.5815931260585785}]}], "abstractContent": [{"text": "Nowadays supervised sequence labeling models can reach competitive performance on the task of Chinese word segmenta-tion.", "labels": [], "entities": []}, {"text": "However, the ability of these models is restricted by the availability of annotated data and the design of features.", "labels": [], "entities": []}, {"text": "We propose a scalable semi-supervised feature engineering approach.", "labels": [], "entities": []}, {"text": "In contrast to previous works using pre-defined task-specific features with fixed values, we dynamically extract representations of label distributions from both an in-domain corpus and an out-of-domain corpus.", "labels": [], "entities": []}, {"text": "We update the representation values with a semi-supervised approach.", "labels": [], "entities": []}, {"text": "Experiments on the benchmark datasets show that our approach achieve good results and reach an f-score of 0.961.", "labels": [], "entities": [{"text": "f-score", "start_pos": 95, "end_pos": 102, "type": "METRIC", "confidence": 0.9982560276985168}]}, {"text": "The feature engineering approach proposed here is a general iterative semi-supervised method and not limited to the word segmentation task.", "labels": [], "entities": [{"text": "word segmentation task", "start_pos": 116, "end_pos": 138, "type": "TASK", "confidence": 0.8068820834159851}]}], "introductionContent": [{"text": "Chinese is a language without natural word delimiters.", "labels": [], "entities": []}, {"text": "Therefore, Chinese Word Segmentation (CWS) is an essential task required by further language processing.", "labels": [], "entities": [{"text": "Chinese Word Segmentation (CWS)", "start_pos": 11, "end_pos": 42, "type": "TASK", "confidence": 0.7398790816466013}]}, {"text": "Previous research shows that sequence labeling models trained on labeled data can reach competitive accuracy on the CWS task, and supervised models are more accurate than unsupervised models).", "labels": [], "entities": [{"text": "sequence labeling", "start_pos": 29, "end_pos": 46, "type": "TASK", "confidence": 0.6775836050510406}, {"text": "accuracy", "start_pos": 100, "end_pos": 108, "type": "METRIC", "confidence": 0.9943958520889282}]}, {"text": "However, the resource of manually labeled training corpora is limited.", "labels": [], "entities": []}, {"text": "Therefore, semi-supervised learning has become one * Corresponding author of the most natural forms of training for CWS.", "labels": [], "entities": [{"text": "CWS", "start_pos": 116, "end_pos": 119, "type": "TASK", "confidence": 0.9166936874389648}]}, {"text": "Traditional semi-supervised methods focus on adding new unlabeled instances to the training set by a given criterion.", "labels": [], "entities": []}, {"text": "The possible mislabeled instances, which are introduced from the automatically labeled raw data, can hurt the performance and not easy to exclude by setting a sound selecting criterion.", "labels": [], "entities": []}, {"text": "In this paper, we propose a simple and scalable semi-supervised strategy that works by providing semi-supervision at the level of representation.", "labels": [], "entities": []}, {"text": "Previous works mainly assume that context features are helpful to decide the potential label of a character.", "labels": [], "entities": []}, {"text": "However, when some of the context features do not appear in the training corpus, this assumption may fail.", "labels": [], "entities": []}, {"text": "An example is shown in table 1.", "labels": [], "entities": []}, {"text": "Although the context of \"\u6c34\" and \"\u7bee\" is totally different, they share a homogeneous structure as \"verb-noun\".", "labels": [], "entities": []}, {"text": "A much better way is to map the context information to a kind of representation.", "labels": [], "entities": []}, {"text": "More precisely, the mapping should let the similar contexts map to similar representations, while let the distinct contexts map to distinct representations.: Example of the context of \"\u6c34\" in \"\u5403\u6c34 \u679c (Eat fruits)\" and the context of \"\u7bee\" in \"\u6253\u7bee\u7403 (Play basketball)\" We use the label distribution information that is extracted from the unlabeled corpus as this representation to enhance the supervised model.", "labels": [], "entities": []}, {"text": "We add \"pseudo-labels\" by tagging the unlabeled data with the trained model on the training corpus.", "labels": [], "entities": []}, {"text": "These \"pseudo-labels\" are not accurate enough.", "labels": [], "entities": []}, {"text": "Therefore, we use the label distribution, which is much more accurate.", "labels": [], "entities": []}, {"text": "To accurately calculate the precise label distribution, we use a framework similar to the cotraining algorithm to adjust the feature values iteratively.", "labels": [], "entities": []}, {"text": "Generally speaking, unlabeled data can be classified as in-domain data and out-ofdomain data.", "labels": [], "entities": []}, {"text": "In previous works these two kinds of unlabeled data are used separately for different purposes.", "labels": [], "entities": []}, {"text": "In-domain data is mainly used to solve the problem of data sparseness).", "labels": [], "entities": []}, {"text": "On the other hand, out-of domain data is used for domain adaptation.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 50, "end_pos": 67, "type": "TASK", "confidence": 0.8427274823188782}]}, {"text": "In our work, we use in-domain and out-of-domain data together to adjust the labels of the unlabeled corpus.", "labels": [], "entities": []}, {"text": "We evaluate the performance of CWS on the benchmark dataset of Peking University in the second International Chinese Word Segmentation Bakeoff.", "labels": [], "entities": [{"text": "International Chinese Word Segmentation Bakeoff", "start_pos": 95, "end_pos": 142, "type": "TASK", "confidence": 0.7677475094795227}]}, {"text": "Experiment results show that our approach yields improvements compared with the state-of-art systems.", "labels": [], "entities": []}, {"text": "Even when the labeled data is insufficient, our methods can still work better than traditional methods.", "labels": [], "entities": []}, {"text": "Compared to the baseline CWS model, which has already achieved an f-score above 0.95, we further reduce the error rate by 15%.", "labels": [], "entities": [{"text": "f-score", "start_pos": 66, "end_pos": 73, "type": "METRIC", "confidence": 0.978118896484375}, {"text": "error rate", "start_pos": 108, "end_pos": 118, "type": "METRIC", "confidence": 0.9903292655944824}]}, {"text": "Our method is not limited to word segmentation.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 29, "end_pos": 46, "type": "TASK", "confidence": 0.7380392402410507}]}, {"text": "It is also applicable to other problems which can be solved by sequence labeling models.", "labels": [], "entities": []}, {"text": "We also applied our method to the Chinese Named Entity Recognition task, and also achieved better results compared to traditional methods.", "labels": [], "entities": [{"text": "Chinese Named Entity Recognition task", "start_pos": 34, "end_pos": 71, "type": "TASK", "confidence": 0.770262348651886}]}, {"text": "The main contributions of our work are as follows: \u2022 We proposed a general method to utilize the label distribution given text contexts as representations in a semi-supervised framework.", "labels": [], "entities": []}, {"text": "We let the co-training process adjust the representation values from label distribution instead of using manually predefined feature templates.", "labels": [], "entities": []}, {"text": "\u2022 Compared with previous work, our method achieved anew state-of-art accuracy on the CWS task as well as on the NER task.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 69, "end_pos": 77, "type": "METRIC", "confidence": 0.9994434714317322}, {"text": "NER task", "start_pos": 112, "end_pos": 120, "type": "TASK", "confidence": 0.8060430288314819}]}, {"text": "The remaining part of this paper is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 describes the details of the problem and our algorithm.", "labels": [], "entities": []}, {"text": "Section 3 describes the experiment and presents the results.", "labels": [], "entities": []}, {"text": "Section 4 reviews the related work.", "labels": [], "entities": []}, {"text": "Section 5 concludes this paper.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 4: Details of the PKU data", "labels": [], "entities": [{"text": "PKU data", "start_pos": 25, "end_pos": 33, "type": "DATASET", "confidence": 0.9515224993228912}]}, {"text": " Table 5: Details of the unlabeled data.", "labels": [], "entities": []}, {"text": " Table 6: Segmentation results on test data with  different feature combinations. The symbol \"+\"  means this feature configuration contains features set  containing the baseline features and all features after  '+'. The size of unlabeled data is fixed as 5 million  characters.", "labels": [], "entities": [{"text": "Segmentation", "start_pos": 10, "end_pos": 22, "type": "TASK", "confidence": 0.9609156250953674}]}, {"text": " Table 7: Comparison of results when changing the  size of unlabeled data. (0.5 million, 1 million and 5  million characters).", "labels": [], "entities": []}, {"text": " Table 8: Comparison of f-scores when changing the  size of labeled data. (1/10, 1/4, 1/2 and all labeled  data. The size of unlabeled data is fixed as 5 million  characters.)", "labels": [], "entities": []}, {"text": " Table 10: Comparison of our approach with the state-of-art systems", "labels": [], "entities": []}]}