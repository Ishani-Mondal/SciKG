{"title": [], "abstractContent": [{"text": "We introduce an extended naive Bayes model for word sense induction (WSI) and apply it to a WSI task.", "labels": [], "entities": [{"text": "word sense induction (WSI)", "start_pos": 47, "end_pos": 73, "type": "TASK", "confidence": 0.8391992151737213}, {"text": "WSI task", "start_pos": 92, "end_pos": 100, "type": "TASK", "confidence": 0.7563370764255524}]}, {"text": "The extended model incorporates the idea the words closer to the target word are more relevant in predicting its sense.", "labels": [], "entities": [{"text": "predicting its sense", "start_pos": 98, "end_pos": 118, "type": "TASK", "confidence": 0.8592645724614462}]}, {"text": "The proposed model is very simple yet effective when evaluated on SemEval-2010 WSI data.", "labels": [], "entities": [{"text": "SemEval-2010 WSI data", "start_pos": 66, "end_pos": 87, "type": "DATASET", "confidence": 0.793238361676534}]}], "introductionContent": [{"text": "The task of word sense induction (WSI) is to find clusters of tokens of an ambiguous word in an unlabeled corpus that have the same sense.", "labels": [], "entities": [{"text": "word sense induction (WSI)", "start_pos": 12, "end_pos": 38, "type": "TASK", "confidence": 0.8500833610693613}]}, {"text": "For instance, given a target word \"crane,\" a good WSI system should find a cluster of tokens referring to avian cranes and another referring to mechanical cranes.", "labels": [], "entities": []}, {"text": "We believe that neighboring words contain enough information that these clusters can be found from plain texts.", "labels": [], "entities": []}, {"text": "WSI is related to word sense disambiguation (WSD).", "labels": [], "entities": [{"text": "WSI", "start_pos": 0, "end_pos": 3, "type": "TASK", "confidence": 0.8291397094726562}, {"text": "word sense disambiguation (WSD)", "start_pos": 18, "end_pos": 49, "type": "TASK", "confidence": 0.760163202881813}]}, {"text": "Ina WSD task, a system learns a sense classifier in a supervised manner from a sense-labeled corpus.", "labels": [], "entities": []}, {"text": "The performance of the learned classifier is measured on some unseen data.", "labels": [], "entities": []}, {"text": "WSD systems perform better than WSI systems, but building labeled data can be prohibitively expensive.", "labels": [], "entities": []}, {"text": "In addition, WSD systems are not suitable for newly created words, new senses of existing words, or domainspecific words.", "labels": [], "entities": [{"text": "WSD", "start_pos": 13, "end_pos": 16, "type": "TASK", "confidence": 0.946357786655426}]}, {"text": "On the other hand, WSI systems can learn new senses of words directly from texts because these programs do not rely on a predefined set of senses.", "labels": [], "entities": []}, {"text": "In Section 2 we describe relevant previous work.", "labels": [], "entities": []}, {"text": "In Section 3 and 4 we introduce the naive Bayes model for WSI and inference schemes for the model.", "labels": [], "entities": [{"text": "WSI", "start_pos": 58, "end_pos": 61, "type": "TASK", "confidence": 0.5618778467178345}]}, {"text": "In Section 5 we evaluate the model on SemEval-2010 data.", "labels": [], "entities": [{"text": "SemEval-2010 data", "start_pos": 38, "end_pos": 55, "type": "DATASET", "confidence": 0.8526028990745544}]}, {"text": "In Section 6 we conclude.", "labels": [], "entities": []}], "datasetContent": [{"text": "We compare our system to other WSI systems and discuss two metrics for unsupervised evaluation (VMeasure, paired F-Score) and one metric for supervised evaluation (supervised recall).", "labels": [], "entities": [{"text": "F-Score", "start_pos": 113, "end_pos": 120, "type": "METRIC", "confidence": 0.6677883267402649}, {"text": "recall", "start_pos": 175, "end_pos": 181, "type": "METRIC", "confidence": 0.8520275950431824}]}, {"text": "We refer to the true group of tokens as a gold class and to an induced group of tokens as a cluster.", "labels": [], "entities": []}, {"text": "We refer to the model learned with the sampler and EM as NB, and to the model learned with EM only as NB0.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Details of SemEval-2010 data", "labels": [], "entities": [{"text": "SemEval-2010", "start_pos": 21, "end_pos": 33, "type": "TASK", "confidence": 0.8323860168457031}]}, {"text": " Table 2: Performance of the model with various parameters: supervised recall on the trial data. The best value from  each row is bold-faced. The scores are averaged over 100 runs.", "labels": [], "entities": [{"text": "recall", "start_pos": 71, "end_pos": 77, "type": "METRIC", "confidence": 0.9714735746383667}]}, {"text": " Table 3: Unsupervised evaluation: V-Measure", "labels": [], "entities": [{"text": "V-Measure", "start_pos": 35, "end_pos": 44, "type": "TASK", "confidence": 0.7888369560241699}]}, {"text": " Table 4: Unsupervised evaluation: paired F-Score", "labels": [], "entities": [{"text": "paired", "start_pos": 35, "end_pos": 41, "type": "METRIC", "confidence": 0.9941598176956177}, {"text": "F-Score", "start_pos": 42, "end_pos": 49, "type": "METRIC", "confidence": 0.7948875427246094}]}, {"text": " Table 5: Supervised evaluation: supervised recall, 80%  mapping and 20% evaluation", "labels": [], "entities": [{"text": "recall", "start_pos": 44, "end_pos": 50, "type": "METRIC", "confidence": 0.9858761429786682}]}]}