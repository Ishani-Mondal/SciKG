{"title": [{"text": "Optimal Beam Search for Machine Translation", "labels": [], "entities": [{"text": "Machine Translation", "start_pos": 24, "end_pos": 43, "type": "TASK", "confidence": 0.7701112031936646}]}], "abstractContent": [{"text": "Beam search is a fast and empirically effective method for translation decoding, but it lacks formal guarantees about search error.", "labels": [], "entities": [{"text": "Beam search", "start_pos": 0, "end_pos": 11, "type": "TASK", "confidence": 0.9039202630519867}, {"text": "translation decoding", "start_pos": 59, "end_pos": 79, "type": "TASK", "confidence": 0.9594716727733612}]}, {"text": "We develop anew decoding algorithm that combines the speed of beam search with the optimal certificate property of Lagrangian relaxation, and apply it to phrase-and syntax-based translation decoding.", "labels": [], "entities": [{"text": "phrase-and syntax-based translation decoding", "start_pos": 154, "end_pos": 198, "type": "TASK", "confidence": 0.7294486835598946}]}, {"text": "The new method is efficient, utilizes standard MT algorithms, and returns an exact solution on the majority of translation examples in our test data.", "labels": [], "entities": [{"text": "MT", "start_pos": 47, "end_pos": 49, "type": "TASK", "confidence": 0.9734638929367065}]}, {"text": "The algorithm is 3.5 times faster than an optimized incremen-tal constraint-based decoder for phrase-based translation and 4 times faster for syntax-based translation.", "labels": [], "entities": [{"text": "phrase-based translation", "start_pos": 94, "end_pos": 118, "type": "TASK", "confidence": 0.7637883424758911}, {"text": "syntax-based translation", "start_pos": 142, "end_pos": 166, "type": "TASK", "confidence": 0.7215446531772614}]}], "introductionContent": [{"text": "Beam search ( and cube pruning have become the de facto decoding algorithms for phrase-and syntax-based translation.", "labels": [], "entities": [{"text": "Beam search", "start_pos": 0, "end_pos": 11, "type": "TASK", "confidence": 0.7958993315696716}, {"text": "phrase-and syntax-based translation", "start_pos": 80, "end_pos": 115, "type": "TASK", "confidence": 0.695622444152832}]}, {"text": "The algorithms are central to large-scale machine translation systems due to their efficiency and tendency to produce high-quality translations.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 42, "end_pos": 61, "type": "TASK", "confidence": 0.7234620153903961}]}, {"text": "However despite practical effectiveness, neither algorithm provides any bound on possible decoding error.", "labels": [], "entities": []}, {"text": "In this work we present a variant of beam search decoding for phrase-and syntax-based translation.", "labels": [], "entities": [{"text": "phrase-and syntax-based translation", "start_pos": 62, "end_pos": 97, "type": "TASK", "confidence": 0.6704320510228475}]}, {"text": "The motivation is to exploit the effectiveness and efficiency of beam search, but still maintain formal guarantees.", "labels": [], "entities": [{"text": "beam search", "start_pos": 65, "end_pos": 76, "type": "TASK", "confidence": 0.920277327299118}]}, {"text": "The algorithm has the following benefits: \u2022 In theory, it can provide a certificate of optimality; in practice, we show that it produces optimal hypotheses, with certificates of optimality, on the vast majority of examples.", "labels": [], "entities": []}, {"text": "\u2022 It utilizes well-studied algorithms and extends off-the-shelf beam search decoders.", "labels": [], "entities": []}, {"text": "\u2022 Empirically it is very fast, results show that it is 3.5 times faster than an optimized incremental constraint-based solver.", "labels": [], "entities": []}, {"text": "While our focus is on fast decoding for machine translation, the algorithm we present can be applied to a variety of dynamic programming-based decoding problems.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 40, "end_pos": 59, "type": "TASK", "confidence": 0.7739886343479156}]}, {"text": "The method only relies on having a constrained beam search algorithm and a fast unconstrained search algorithm.", "labels": [], "entities": []}, {"text": "Similar algorithms exist for many NLP tasks.", "labels": [], "entities": []}, {"text": "We begin in Section 2 by describing constrained hypergraph search and showing how it generalizes translation decoding.", "labels": [], "entities": []}, {"text": "Section 3 introduces a variant of beam search that is, in theory, able to produce a certificate of optimality.", "labels": [], "entities": [{"text": "beam search", "start_pos": 34, "end_pos": 45, "type": "TASK", "confidence": 0.9246545732021332}]}, {"text": "Section 4 shows how to improve the effectiveness of beam search by using weights derived from Lagrangian relaxation.", "labels": [], "entities": [{"text": "beam search", "start_pos": 52, "end_pos": 63, "type": "TASK", "confidence": 0.9569565057754517}]}, {"text": "Section 5 puts everything together to derive a fast beam search algorithm that is often optimal in practice.", "labels": [], "entities": [{"text": "beam search", "start_pos": 52, "end_pos": 63, "type": "TASK", "confidence": 0.8782701194286346}]}, {"text": "Experiments compare the new algorithm with several variants of beam search, cube pruning, A * search, and relaxation-based decoders on two translation tasks.", "labels": [], "entities": []}, {"text": "The optimal beam search algorithm is able to find exact solutions with certificates of optimality on 99% of translation examples, significantly more than other baselines.", "labels": [], "entities": [{"text": "optimal beam search", "start_pos": 4, "end_pos": 23, "type": "TASK", "confidence": 0.6463462511698405}]}, {"text": "Additionally the optimal beam search algorithm is much faster than other exact methods.", "labels": [], "entities": [{"text": "beam search", "start_pos": 25, "end_pos": 36, "type": "TASK", "confidence": 0.8848151862621307}]}], "datasetContent": [{"text": "is seven times faster than the decoder of Chang and Collins (2011) and 3.5 times faster then our reimplementation, LR-TIGHT.", "labels": [], "entities": [{"text": "LR-TIGHT", "start_pos": 115, "end_pos": 123, "type": "METRIC", "confidence": 0.5522472262382507}]}, {"text": "ASTAR performs poorly, taking lots of time on difficult sentences.", "labels": [], "entities": [{"text": "ASTAR", "start_pos": 0, "end_pos": 5, "type": "TASK", "confidence": 0.8974708318710327}]}, {"text": "BEAM runs quickly, but rarely finds an exact solution.", "labels": [], "entities": []}, {"text": "MOSES without gap constraints is also fast, but less exact than OPTBEAM and unable to produce certificates.", "labels": [], "entities": [{"text": "MOSES", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.6675876975059509}, {"text": "OPTBEAM", "start_pos": 64, "end_pos": 71, "type": "DATASET", "confidence": 0.7948582172393799}]}, {"text": "OPTBEAM finds a certificate on 98.8% of solutions with an average time of 1.75 seconds per sentence, and is four times faster than LR-TIGHT.", "labels": [], "entities": [{"text": "OPTBEAM", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.5292487740516663}, {"text": "LR-TIGHT", "start_pos": 131, "end_pos": 139, "type": "METRIC", "confidence": 0.5473710894584656}]}, {"text": "CUBE (100) is an order of magnitude faster, but is rarely exact on longer sentences.", "labels": [], "entities": [{"text": "CUBE (100)", "start_pos": 0, "end_pos": 10, "type": "METRIC", "confidence": 0.7832664102315903}, {"text": "exact", "start_pos": 58, "end_pos": 63, "type": "METRIC", "confidence": 0.9598245024681091}]}, {"text": "CUBE (1000) finds more exact solutions, but is comparable in speed to optimal beam search.", "labels": [], "entities": []}, {"text": "BEAM performs better than in the phrasebased model, but is not much faster than OPTBEAM..2 shows the relationship between beam search optimality and duality gap.", "labels": [], "entities": [{"text": "BEAM", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.9833906292915344}]}, {"text": "Graph (a) shows how a handful of LR rounds can significantly tighten the upper bound score of many sentences.", "labels": [], "entities": [{"text": "upper bound score", "start_pos": 73, "end_pos": 90, "type": "METRIC", "confidence": 0.7985811829566956}]}, {"text": "Graph (b) shows how beam search is more likely to find optimal solutions with tighter bounds.", "labels": [], "entities": [{"text": "beam search", "start_pos": 20, "end_pos": 31, "type": "TASK", "confidence": 0.9148158729076385}]}, {"text": "BEAM effectively uses 0 rounds of LR, which may explain why it finds so few optimal solutions compared to OPTBEAM.  inates.", "labels": [], "entities": []}, {"text": "If not for this cost, OPTBEAM might be comparable in speed to MOSES (1000).", "labels": [], "entities": [{"text": "OPTBEAM", "start_pos": 22, "end_pos": 29, "type": "METRIC", "confidence": 0.6313625574111938}, {"text": "speed", "start_pos": 53, "end_pos": 58, "type": "METRIC", "confidence": 0.985081136226654}, {"text": "MOSES", "start_pos": 62, "end_pos": 67, "type": "METRIC", "confidence": 0.7452505230903625}]}], "tableCaptions": [{"text": " Table 1: Experimental results for translation experiments. Column time is the mean time per sentence in seconds,  cert is the percentage of sentences solved with a certificate of optimality, exact is the percentage of sentences solved  exactly, i.e. \u03b8 x + \u03c4 = \u03b8 x  *  + \u03c4 . Results are grouped by sentence length (group 1-10 is omitted for space).", "labels": [], "entities": [{"text": "translation", "start_pos": 35, "end_pos": 46, "type": "TASK", "confidence": 0.9657421112060547}, {"text": "exact", "start_pos": 192, "end_pos": 197, "type": "METRIC", "confidence": 0.9752324223518372}]}]}