{"title": [{"text": "Overcoming the Lack of Parallel Data in Sentence Compression", "labels": [], "entities": []}], "abstractContent": [{"text": "A major challenge in supervised sentence compression is making use of rich feature representations because of very scarce parallel data.", "labels": [], "entities": [{"text": "supervised sentence compression", "start_pos": 21, "end_pos": 52, "type": "TASK", "confidence": 0.6031429270903269}]}, {"text": "We address this problem and present a method to automatically build a compression corpus with hundreds of thousands of instances on which deletion-based algorithms can be trained.", "labels": [], "entities": []}, {"text": "In our corpus, the syntactic trees of the compressions are subtrees of their uncompressed counterparts, and hence supervised systems which require a structural alignment between the input and output can be successfully trained.", "labels": [], "entities": []}, {"text": "We also extend an existing unsupervised compression method with a learning module.", "labels": [], "entities": []}, {"text": "The new system uses struc-tured prediction to learn from lexical, syntactic and other features.", "labels": [], "entities": [{"text": "struc-tured prediction", "start_pos": 20, "end_pos": 42, "type": "TASK", "confidence": 0.6701771765947342}]}, {"text": "An evaluation with human raters shows that the presented data harvesting method indeed produces a parallel corpus of high quality.", "labels": [], "entities": []}, {"text": "Also, the supervised system trained on this corpus gets high scores both from human raters and in an automatic evaluation setting, significantly outperforming a strong baseline.", "labels": [], "entities": []}], "introductionContent": [], "datasetContent": [{"text": "From the corpus of 250K tuples we used 100K to get pairs of extracted headlines and sentences for training (on the development set we did not observe much improvement from using more training data), 250 for development and the rest for testing.", "labels": [], "entities": []}, {"text": "We ran the learning algorithm for 20 iterations, checking the performance on the development set.", "labels": [], "entities": []}, {"text": "Features which applied to less than 20 edges were pruned, the size of the feature set is about 28K.", "labels": [], "entities": []}, {"text": "50 pairs of original headlines and sentences (different from the data validation set in Sec.", "labels": [], "entities": []}, {"text": "4) were randomly selected for an evaluation with humans from the test data.", "labels": [], "entities": []}, {"text": "As in the data quality validation experiment, we asked raters to assess the readability and informativeness of proposed compressions for the unsupervised system, our system and humanwritten headlines.", "labels": [], "entities": []}, {"text": "The latter provide us with upper bounds on the evaluation criteria.", "labels": [], "entities": []}, {"text": "Three ratings per item per parameter were collected.", "labels": [], "entities": []}, {"text": "To get comparable results, the unsupervised and our systems used the same compression rate: for both, the requested maximum length was set to the length of the headline.", "labels": [], "entities": []}, {"text": "The results indicate that the trained model significantly outperforms the unsupervised system, getting particularly good marks for readability.", "labels": [], "entities": []}, {"text": "The difference in readability between our system and original headlines is not statistically significant.", "labels": [], "entities": []}, {"text": "Note that: Results for the systems and original headline: \u2020 and \u2021 stand for significantly better than Unsupervised and Our system at 95% confidence, respectively the unsupervised baseline is also capable of generating readable compressions but does a much poorer job in selecting most important information.", "labels": [], "entities": []}, {"text": "Our trained model successfully learned to optimize both scores.", "labels": [], "entities": []}, {"text": "We refer the reader to Appendix for input and compression examples.", "labels": [], "entities": []}, {"text": "Note that the ratings for the human-written headlines in this experiment are slightly different from the ratings in the data validation experiment because a different data sample was used.", "labels": [], "entities": []}, {"text": "Our automatic evaluation had the goal of explicitly addressing two relevant questions related to our claims about (1) the benefits of having a large parallel corpus and (2) employing a supervised approach with a rich feature representation.", "labels": [], "entities": []}, {"text": "1. Our primary motivation for collecting parallel data has been that having access to sparse lexical features, which considerably increase the feature space, would benefit compression systems.", "labels": [], "entities": []}, {"text": "But is it really the case for sentence compression?", "labels": [], "entities": [{"text": "sentence compression", "start_pos": 30, "end_pos": 50, "type": "TASK", "confidence": 0.772646963596344}]}, {"text": "Can a comparable performance be achieved with a closed, moderately sized set of dense, non-lexical features?", "labels": [], "entities": []}, {"text": "If yes, then a large compression corpus is probably not needed.", "labels": [], "entities": []}, {"text": "Furthermore, to demonstrate that a large corpus is not only sufficient but also necessary to learn weights for thousands of features, we need to compare the performance of the system when trained on the full data set and a small portion of it.", "labels": [], "entities": []}, {"text": "2. The syntactic and informativeness scores in Eq.", "labels": [], "entities": []}, {"text": "(3) were calculated over millions of news articles and do provide us with meaninful statistics (see Sec. 2).", "labels": [], "entities": []}, {"text": "Is there any benefit in replacing those scores with weights learned for their feature counterparts?", "labels": [], "entities": []}, {"text": "Recall that one of our feature types in is the concatenation of lemma(h) (parent lemma) and label(e) which relies on the same information as w synt = P (label(e)|lemma(h)).", "labels": [], "entities": []}, {"text": "The feature counterpart of w info defined in Eq. is lemma(n)-the lemma of the node to which edge points.", "labels": [], "entities": [{"text": "Eq.", "start_pos": 45, "end_pos": 48, "type": "DATASET", "confidence": 0.9334540367126465}]}, {"text": "How would the supervised system perform against the unsupervised one, if it only extracted features of these two types?", "labels": [], "entities": []}, {"text": "To answer these questions, we sampled 1,000 tuples from the unused test data and measured F1 score () by comparing the trees of the generated compression and the \"correct\", extracted headline.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 90, "end_pos": 98, "type": "METRIC", "confidence": 0.9912348687648773}]}, {"text": "The systems we compared are the unsupervised baseline (UNSUP.", "labels": [], "entities": [{"text": "UNSUP", "start_pos": 55, "end_pos": 60, "type": "DATASET", "confidence": 0.8347175121307373}]}, {"text": "SYSTEM) and the supervised model trained on three kinds of feature sets: (1) SYNT-INFO FEATURES, corresponding to the supervised training of the unsupervised baseline model (i.e., lemma(h)-label(e) and lemma(n)); (2) NON-LEX FEATURES, corresponding to a dense, non-lexical feature representation (i.e., all the feature types from excluding the three involving lemmas); (3) ALL FEATURES (same as OUR SYSTEM).", "labels": [], "entities": [{"text": "FEATURES", "start_pos": 87, "end_pos": 95, "type": "METRIC", "confidence": 0.49049320816993713}, {"text": "ALL", "start_pos": 373, "end_pos": 376, "type": "METRIC", "confidence": 0.9936145544052124}, {"text": "FEATURES", "start_pos": 377, "end_pos": 385, "type": "METRIC", "confidence": 0.6417099237442017}]}, {"text": "Additionally, we trained the system on 10% of the data-10K as opposed to 100K tuples, ALL FEATURES (10K)-for 20 iterations ignoring features which applied to less than three edges . As before, the same compression rate was used for all the systems.", "labels": [], "entities": [{"text": "ALL", "start_pos": 86, "end_pos": 89, "type": "METRIC", "confidence": 0.9952577948570251}, {"text": "FEATURES", "start_pos": 90, "end_pos": 98, "type": "METRIC", "confidence": 0.6509655117988586}]}, {"text": "The results are summarized in  Clearly, having more features, lexicalized and unlexicalized, is important: there is a significant im- Recall from the beginning of the section that for the full (100K) training set the threshold was set to 20 with no tuning.", "labels": [], "entities": [{"text": "im- Recall", "start_pos": 130, "end_pos": 140, "type": "METRIC", "confidence": 0.8522625168164571}]}, {"text": "For the 10K training set, we tried values of two, three, five and varied the number of iterations.", "labels": [], "entities": [{"text": "10K training set", "start_pos": 8, "end_pos": 24, "type": "DATASET", "confidence": 0.639030396938324}]}, {"text": "The result we report is the highest we could get for 10K.", "labels": [], "entities": []}, {"text": "provement in going beyond the closed set of 330 non-lexical features to all, from 79.6 to 84.3 points.", "labels": [], "entities": []}, {"text": "Moreover, successful training requires a large corpus since the performance of the system degrades if only 10K training instances are used.", "labels": [], "entities": []}, {"text": "Note that this number already exceeds all the existing compression corpora taken together.", "labels": [], "entities": []}, {"text": "Hence, sparse lexical features are useful for compression and a large parallel corpus is a requirement for successful supervised training.", "labels": [], "entities": []}, {"text": "Concerning our second question, learning feature weights from the data produces significantly better results than the hand-crafted way of making use of the same information, even if a much larger data set is used to collect statistics.", "labels": [], "entities": []}, {"text": "We observed a dramatic increase from 52.3 to 75.0 points.", "labels": [], "entities": []}, {"text": "Thus, we may conclude that training with dense and sparse features directly from data definitely improves the performance of the dependency pruning system.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 4: Results for the systems and original headline:  \u2020  and  \u2021 stand for significantly better than Unsupervised and  Our system at 95% confidence, respectively", "labels": [], "entities": [{"text": "headline", "start_pos": 47, "end_pos": 55, "type": "METRIC", "confidence": 0.9414655566215515}, {"text": "\u2020", "start_pos": 58, "end_pos": 59, "type": "METRIC", "confidence": 0.96292644739151}]}, {"text": " Table 5: Results for the unsupervised baseline and the  supervised system trained on three kinds of feature sets", "labels": [], "entities": []}]}