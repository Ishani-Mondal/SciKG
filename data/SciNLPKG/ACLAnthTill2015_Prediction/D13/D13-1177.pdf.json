{"title": [{"text": "Learning Biological Processes with Global Constraints", "labels": [], "entities": []}], "abstractContent": [{"text": "Biological processes are complex phenomena involving a series of events that are related to one another through various relationships.", "labels": [], "entities": []}, {"text": "Systems that can understand and reason over biological processes would dramatically improve the performance of semantic applications involving inference such as question answering (QA)-specifically \"How?\" and \"Why?\" questions.", "labels": [], "entities": [{"text": "question answering (QA)-", "start_pos": 161, "end_pos": 185, "type": "TASK", "confidence": 0.8323330819606781}]}, {"text": "In this paper, we present the task of process extraction, in which events within a process and the relations between the events are automatically extracted from text.", "labels": [], "entities": [{"text": "process extraction", "start_pos": 38, "end_pos": 56, "type": "TASK", "confidence": 0.7850448489189148}]}, {"text": "We represent processes by graphs whose edges describe a set of temporal, causal and co-reference event-event relations, and characterize the structural properties of these graphs (e.g., the graphs are connected).", "labels": [], "entities": []}, {"text": "Then, we present a method for extracting relations between the events, which exploits these structural properties by performing joint inference over the set of extracted relations.", "labels": [], "entities": []}, {"text": "On a novel dataset containing 148 descriptions of biological processes (released with this paper), we show significant improvement comparing to baselines that disregard process structure.", "labels": [], "entities": []}], "introductionContent": [{"text": "A process is defined as a series of inter-related events that involve multiple entities and lead to an end result.", "labels": [], "entities": []}, {"text": "Product manufacturing, economical developments, and various phenomena in life and social sciences can all be viewed as types of processes.", "labels": [], "entities": []}, {"text": "Processes are complicated objects; consider for example the biological process of ATP synthesis described in.", "labels": [], "entities": [{"text": "ATP synthesis", "start_pos": 82, "end_pos": 95, "type": "TASK", "confidence": 0.7482163310050964}]}, {"text": "This process involves 12 entities and 8 events.", "labels": [], "entities": []}, {"text": "Additionally, it describes relations between events and entities, and the relationship between events (e.g., the second occurrence of the event 'enter', causes the event 'changing').", "labels": [], "entities": []}, {"text": "* Both authors equally contributed to the paper Automatically extracting the structure of processes from text is crucial for applications that require reasoning, such as non-factoid QA.", "labels": [], "entities": [{"text": "Automatically extracting the structure of processes from text", "start_pos": 48, "end_pos": 109, "type": "TASK", "confidence": 0.8398351445794106}]}, {"text": "For instance, answering a question on ATP synthesis, such as \"How do H+ ions contribute to the production of ATP?\" requires a structure that links H+ ions (, sentence 1) to ATP, sentence 4) through a sequence of intermediate events.", "labels": [], "entities": []}, {"text": "Such \"How?\" questions are common on FAQ websites ( , which further supports the importance of process extraction.", "labels": [], "entities": [{"text": "process extraction", "start_pos": 94, "end_pos": 112, "type": "TASK", "confidence": 0.7551571428775787}]}, {"text": "Process extraction is related to two recent lines of work in Information Extraction -event extraction and timeline construction.", "labels": [], "entities": [{"text": "Process extraction", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.7509743571281433}, {"text": "Information Extraction -event extraction", "start_pos": 61, "end_pos": 101, "type": "TASK", "confidence": 0.7263869404792785}, {"text": "timeline construction", "start_pos": 106, "end_pos": 127, "type": "TASK", "confidence": 0.8149514496326447}]}, {"text": "Traditional event extraction focuses on identifying a closed set of events within a single sentence.", "labels": [], "entities": [{"text": "event extraction", "start_pos": 12, "end_pos": 28, "type": "TASK", "confidence": 0.7294335067272186}]}, {"text": "For example, the BioNLP 2009 and 2011 shared tasks () consider nine events types related to proteins.", "labels": [], "entities": [{"text": "BioNLP 2009", "start_pos": 17, "end_pos": 28, "type": "DATASET", "confidence": 0.8441847264766693}]}, {"text": "In practice, events are currently almost always extracted from a single sentence.", "labels": [], "entities": []}, {"text": "Process extraction, on the other hand, is centered around discovering relations between events that span multiple sentences.", "labels": [], "entities": [{"text": "Process extraction", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.7454869449138641}]}, {"text": "The set of possible event types in process extraction is also much larger.", "labels": [], "entities": [{"text": "process extraction", "start_pos": 35, "end_pos": 53, "type": "TASK", "confidence": 0.777596652507782}]}, {"text": "Timeline construction involves identifying temporal relations between events (, and is thus related to process extraction as both focus on event-event relations spanning multiple sentences.", "labels": [], "entities": [{"text": "Timeline construction", "start_pos": 0, "end_pos": 21, "type": "TASK", "confidence": 0.6705186665058136}, {"text": "process extraction", "start_pos": 103, "end_pos": 121, "type": "TASK", "confidence": 0.8041766881942749}]}, {"text": "However, events in processes are tightly coupled in ways that go beyond simple temporal ordering, and these dependencies are central for the process extraction task.", "labels": [], "entities": [{"text": "process extraction", "start_pos": 141, "end_pos": 159, "type": "TASK", "confidence": 0.7625040113925934}]}, {"text": "Hence, capturing process structure requires modeling a larger set of relations that includes temporal, causal and co-reference relations.", "labels": [], "entities": []}, {"text": "In this paper, we formally define the task of process extraction and present automatic extraction methods.", "labels": [], "entities": [{"text": "process extraction", "start_pos": 46, "end_pos": 64, "type": "TASK", "confidence": 0.7822476923465729}]}, {"text": "Our approach handles an open set of event types and works over multiple sentences, extracting a rich set of event-event relations.", "labels": [], "entities": []}, {"text": "Furthermore, H+ ions flowing down their gradient enter a half channel in a stator, which is anchored in the membrane.", "labels": [], "entities": []}, {"text": "H+ ions enter binding sites within a rotor, changing the shape of each subunit so that the rotor spins within the membrane.", "labels": [], "entities": []}, {"text": "Spinning of the rotor causes an internal rod to spin as well.", "labels": [], "entities": []}, {"text": "Turning of the rod activates catalytic sites in the knob that can produce ATP from ADP and P_i.", "labels": [], "entities": []}, {"text": "we characterize a set of global properties of process structure that can be utilized during process extraction.", "labels": [], "entities": []}, {"text": "For example, all events in a process are somehow connected to one another.", "labels": [], "entities": []}, {"text": "Also, processes usually exhibit a \"chain-like\" structure reflecting process progression overtime.", "labels": [], "entities": []}, {"text": "We show that incorporating such global properties into our model and performing joint inference over the extracted relations significantly improves the quality of process structures predicted.", "labels": [], "entities": []}, {"text": "We conduct experiments on a novel dataset of process descriptions from the textbook \"Biology\") that were annotated by trained biologists.", "labels": [], "entities": []}, {"text": "Our method does not require any domain-specific knowledge and can be easily adapted to non-biology domains.", "labels": [], "entities": []}, {"text": "The main contributions of this paper are: 1.", "labels": [], "entities": []}, {"text": "We define process extraction and characterize processes' structural properties.", "labels": [], "entities": [{"text": "process extraction", "start_pos": 10, "end_pos": 28, "type": "TASK", "confidence": 0.7932374179363251}]}, {"text": "2. We model global structural properties in processes and demonstrate significant improvement in extraction accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 108, "end_pos": 116, "type": "METRIC", "confidence": 0.9962437152862549}]}, {"text": "3. We publicly release a novel data set of 148 fully annotated biological process descriptions along with the source code for our system.", "labels": [], "entities": []}, {"text": "The dataset and code can be downloaded from http://nlp.stanford.edu/ software/bioprocess/.", "labels": [], "entities": []}], "datasetContent": [{"text": "We define a process description as a paragraph or sequence of tokens x = {x 1 , . .", "labels": [], "entities": []}, {"text": "x |x| } that describes a series of events related by temporal and/or causal relations.", "labels": [], "entities": []}, {"text": "For example, in ATP synthesis), the event of rotor spinning causes the event where an internal rod spins.", "labels": [], "entities": [{"text": "ATP synthesis", "start_pos": 16, "end_pos": 29, "type": "TASK", "confidence": 0.7672537863254547}]}, {"text": "We model the events within a process and their relations by a directed graph P = (V, E), where the nodes V = {1, . .", "labels": [], "entities": []}, {"text": ", |V |} represent event mentions and labeled edges E correspond to event-event relations.", "labels": [], "entities": []}, {"text": "An event mention v \u2208 V is defined by a trigger t v , which is a span of words xi , x i+1 , . .", "labels": [], "entities": []}, {"text": ", x j ; and by a set of argument mentions Av , where each argument mention av \u2208 Av is also a span of words labeled by a semantic role l taken from a set L.", "labels": [], "entities": []}, {"text": "For example, in the last event mention of ATP synthesis, t v = produce, and one of the argument mentions is av = (ATP, RESULT).", "labels": [], "entities": [{"text": "RESULT", "start_pos": 119, "end_pos": 125, "type": "METRIC", "confidence": 0.9939669370651245}]}, {"text": "A labeled edge (u, v, r) in the graph describes a relation r \u2208 R between the event mentions u and v.", "labels": [], "entities": []}, {"text": "The task of process extraction is to extract the graph P from the text x.", "labels": [], "entities": [{"text": "process extraction", "start_pos": 12, "end_pos": 30, "type": "TASK", "confidence": 0.7905279695987701}]}, {"text": "A natural way to breakdown process extraction into sub-parts is to first perform semantic role labeling (SRL), that is, identify triggers and predict argument mentions with their semantic role, and then extract event-event relations between pairs of event mentions.", "labels": [], "entities": [{"text": "semantic role labeling (SRL)", "start_pos": 81, "end_pos": 109, "type": "TASK", "confidence": 0.8118098874886831}]}, {"text": "In this paper, we focus on the second step, where given a set of event triggers T , we find all event-event relations, where a trigger represents the entire event.", "labels": [], "entities": []}, {"text": "For completeness, we now describe the semantic roles L used in our dataset, and then present the set of event-event relations R.", "labels": [], "entities": []}, {"text": "The set L contains standard semantic roles such as AGENT, THEME, ORIGIN, DESTINATION and LO-CATION.", "labels": [], "entities": [{"text": "AGENT", "start_pos": 51, "end_pos": 56, "type": "METRIC", "confidence": 0.9666365385055542}, {"text": "THEME", "start_pos": 58, "end_pos": 63, "type": "METRIC", "confidence": 0.8634841442108154}, {"text": "ORIGIN", "start_pos": 65, "end_pos": 71, "type": "METRIC", "confidence": 0.9446417689323425}, {"text": "DESTINATION", "start_pos": 73, "end_pos": 84, "type": "METRIC", "confidence": 0.751151442527771}, {"text": "LO-CATION", "start_pos": 89, "end_pos": 98, "type": "METRIC", "confidence": 0.9645434021949768}]}, {"text": "Two additional semantic roles were employed that are relevant for biological text: RESULT corresponds to an entity that is the result of an event, and RAW-MATERIAL describes an entity that is used or consumed during an event.", "labels": [], "entities": [{"text": "RESULT", "start_pos": 83, "end_pos": 89, "type": "METRIC", "confidence": 0.9959136843681335}, {"text": "RAW-MATERIAL", "start_pos": 151, "end_pos": 163, "type": "METRIC", "confidence": 0.7585625648498535}]}, {"text": "For example, the last event 'produce' in, has 'ATP' as the RE-SULT, and 'ADP' as the RAW-MATERIAL.", "labels": [], "entities": [{"text": "ATP", "start_pos": 47, "end_pos": 50, "type": "METRIC", "confidence": 0.8397688269615173}, {"text": "RE-SULT", "start_pos": 59, "end_pos": 66, "type": "METRIC", "confidence": 0.9941906929016113}, {"text": "ADP", "start_pos": 73, "end_pos": 76, "type": "METRIC", "confidence": 0.9577807784080505}, {"text": "RAW-MATERIAL", "start_pos": 85, "end_pos": 97, "type": "DATASET", "confidence": 0.7149918675422668}]}, {"text": "The event-event relation set R contains the following (assuming a labeled edge (u, v, r)): 1.", "labels": [], "entities": []}, {"text": "PREV denotes that u is an event immediately before v.", "labels": [], "entities": [{"text": "PREV", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.9386689066886902}]}, {"text": "Thus, the edges (u, v, PREV) and (v, w, PREV), preclude the edge (u, w, PREV).", "labels": [], "entities": [{"text": "PREV", "start_pos": 23, "end_pos": 27, "type": "METRIC", "confidence": 0.8713892102241516}, {"text": "PREV", "start_pos": 40, "end_pos": 44, "type": "METRIC", "confidence": 0.8669218420982361}]}, {"text": "For example, in \"When a photon strikes . .", "labels": [], "entities": []}, {"text": "\", there is no edge (strikes, reaches, PREV) due to the intervening event 'passed'.", "labels": [], "entities": [{"text": "PREV", "start_pos": 39, "end_pos": 43, "type": "METRIC", "confidence": 0.9834116697311401}]}, {"text": "2. COTEMP denotes that events u and v overlap in time (e.g., the first two event mentions flowing and enter in).", "labels": [], "entities": [{"text": "COTEMP", "start_pos": 3, "end_pos": 9, "type": "METRIC", "confidence": 0.9749802350997925}]}, {"text": "3. SUPER denotes that event u includes event v.", "labels": [], "entities": [{"text": "SUPER", "start_pos": 3, "end_pos": 8, "type": "METRIC", "confidence": 0.9660002589225769}]}, {"text": "For instance, in \"During DNA replication, DNA polymerases proofread each nucleotide.", "labels": [], "entities": [{"text": "DNA replication", "start_pos": 25, "end_pos": 40, "type": "TASK", "confidence": 0.6845449507236481}]}, {"text": "\" there is an edge (DNA replication, proofread, SUPER).", "labels": [], "entities": [{"text": "DNA replication", "start_pos": 20, "end_pos": 35, "type": "TASK", "confidence": 0.6254556626081467}, {"text": "SUPER", "start_pos": 48, "end_pos": 53, "type": "METRIC", "confidence": 0.9522156715393066}]}, {"text": "4. CAUSES denotes that event u causes event v (e.g., the relation between changing and spins in sentence 2 of).", "labels": [], "entities": [{"text": "CAUSES", "start_pos": 3, "end_pos": 9, "type": "METRIC", "confidence": 0.9275619983673096}]}, {"text": "5. ENABLES denotes that event u creates preconditions that allow event v to take place.", "labels": [], "entities": [{"text": "ENABLES", "start_pos": 3, "end_pos": 10, "type": "METRIC", "confidence": 0.940576434135437}]}, {"text": "For example, the description \".", "labels": [], "entities": []}, {"text": "cause cancer cells to lose attachments to neighboring cells.", "labels": [], "entities": []}, {"text": ", allowing them to spread into nearby tissues\" has the edge (lose, spread, ENABLES).", "labels": [], "entities": [{"text": "ENABLES", "start_pos": 75, "end_pos": 82, "type": "METRIC", "confidence": 0.983655571937561}]}, {"text": "An intuitive way to think about the difference between Causes and Enables is the following: if u causes v this means that if u happens, then v happens.", "labels": [], "entities": []}, {"text": "If u enables v, then if u does not happen, then v does not happen.", "labels": [], "entities": []}, {"text": "6. SAME denotes that u and v both refer to the same event (spins and Spinning in).", "labels": [], "entities": [{"text": "SAME", "start_pos": 3, "end_pos": 7, "type": "METRIC", "confidence": 0.9880126714706421}]}, {"text": "Early work on temporal logic contained more temporal relations than are used in our relation set R.", "labels": [], "entities": [{"text": "temporal logic", "start_pos": 14, "end_pos": 28, "type": "TASK", "confidence": 0.7356614172458649}]}, {"text": "We chose a relation set R that captures the essential aspects of temporal relations between events in a process, while keeping the annotation as simple as possible.", "labels": [], "entities": []}, {"text": "For instance, we include the SUPER relation that appears in temporal annotations such as the Timebank corpus () and Allen's work, but in practice was not considered by many temporal ordering systems).", "labels": [], "entities": [{"text": "SUPER", "start_pos": 29, "end_pos": 34, "type": "METRIC", "confidence": 0.8020541071891785}, {"text": "Timebank corpus", "start_pos": 93, "end_pos": 108, "type": "DATASET", "confidence": 0.9795736968517303}]}, {"text": "Importantly, our relation set also includes the relations CAUSES and ENABLES, which are fundamental to modeling processes and go beyond simple temporal ordering.", "labels": [], "entities": [{"text": "ENABLES", "start_pos": 69, "end_pos": 76, "type": "METRIC", "confidence": 0.9700624346733093}]}, {"text": "We also added event coreference (SAME) to R. used event coreference information in a temporal ordering task to modify probabilities provided by pairwise classifiers prior to joint inference.", "labels": [], "entities": [{"text": "R.", "start_pos": 42, "end_pos": 44, "type": "TASK", "confidence": 0.9642443060874939}]}, {"text": "In this paper, we simply treat SAME as another event-event relation, which allows us to easily perform joint inference and employ structural constraints that combine both coreference and temporal relations simultaneously.", "labels": [], "entities": []}, {"text": "For example, if u and v are the same event, then there can exist now, such that u is before w, but v is after w (see Section 3.3) We annotated 148 process descriptions based on the aforementioned definitions.", "labels": [], "entities": []}, {"text": "Further details on annotation and data set statistics are provided in Section 4 and.", "labels": [], "entities": []}, {"text": "Structural properties of processes Coherent processes exhibit many structural properties.", "labels": [], "entities": []}, {"text": "For example, two argument mentions related to the same event cannot overlap -a constraint that has been used in the past in SRL (.", "labels": [], "entities": [{"text": "SRL", "start_pos": 124, "end_pos": 127, "type": "TASK", "confidence": 0.832389771938324}]}, {"text": "In this paper we focus on three main structural properties of the graph P.", "labels": [], "entities": []}, {"text": "First, in a coherent process, all events mentioned are related to one another, and hence the graph P must be connected.", "labels": [], "entities": []}, {"text": "Second, processes tend to have a \"chain-like\" structure where one event follows another, and thus we expect  nodes' degree to generally be \u2264 2.", "labels": [], "entities": []}, {"text": "Indeed, 90% of event mentions have degree \u2264 2, as demonstrated by the Gold column of.", "labels": [], "entities": [{"text": "degree", "start_pos": 35, "end_pos": 41, "type": "METRIC", "confidence": 0.9757810235023499}, {"text": "Gold", "start_pos": 70, "end_pos": 74, "type": "METRIC", "confidence": 0.935506284236908}]}, {"text": "Last, if we consider relations between all possible triples of events in a process, clearly some configurations are impossible, while others are common (illustrated in).", "labels": [], "entities": []}, {"text": "In Section 3.3, we show that modeling these properties using a joint inference framework improves the quality of process extraction significantly.", "labels": [], "entities": []}, {"text": "We extracted 148 process descriptions by going through chapters from the textbook \"Biology\" and marking any contiguous sequence of sentences that describes a process, i.e., a series of events that lead towards some objective.", "labels": [], "entities": []}, {"text": "Then, each process description was annotated by a biologist.", "labels": [], "entities": []}, {"text": "The annotator was first presented with annotation guidelines and annotated 20 descriptions.", "labels": [], "entities": []}, {"text": "The annotations were then discussed with the authors, after which all process descriptions were annotated.", "labels": [], "entities": []}, {"text": "After training a second biologist, we measured inter-annotator agreement \u03ba = 0.69, on 30 random process descriptions.", "labels": [], "entities": []}, {"text": "Process descriptions were parsed with Stanford constituency and dependency parsers), and 35 process descriptions were set aside as a test set (number of training set trigger pairs: 1932, number of test set trigger pairs: 906).", "labels": [], "entities": []}, {"text": "We performed 10-fold cross validation over the training set for feature selection and tuning of constraint parameters.", "labels": [], "entities": [{"text": "feature selection", "start_pos": 64, "end_pos": 81, "type": "TASK", "confidence": 0.6541408151388168}]}, {"text": "For each constraint type (connectivity, chain-structure, and five triad constraints) we introduced a parameter and tuned the seven parameters by coordinatewise ascent, where for hard constraints a binary parameter controls whether the constraint is used, and for soft constraints we attempted 10 different reward/penalty values.", "labels": [], "entities": []}, {"text": "For our global model we defined \u03b8 ijr = log p ijr , where p ijr is the probability at edge (t i , t j ) for label r, given by the pairwise classifier.", "labels": [], "entities": []}, {"text": "We test the following systems: (a) All-Prev: Since the most common process structure was chain-like, we simply predict PREV for every two adjacent triggers in text.", "labels": [], "entities": [{"text": "PREV", "start_pos": 119, "end_pos": 123, "type": "METRIC", "confidence": 0.9366567730903625}]}, {"text": "(b) Local base : A pairwise classifier with features from previous work (Section 3.1) (c) Local:  A pairwise classifier with all features (Section 3.2) (d) Chain: For every two adjacent triggers, choose the non-NONE relation with highest probability according to Local.", "labels": [], "entities": []}, {"text": "This baseline heuristically combines our structural assumptions with the pairwise classifier.", "labels": [], "entities": []}, {"text": "We deterministically choose a connected chain structure, and then use the classifier to label the edges.", "labels": [], "entities": []}, {"text": "(e) Global: Our full model that uses ILP inference.", "labels": [], "entities": []}, {"text": "To evaluate system performance we compare the set of predictions on all trigger pairs to the gold standard annotations and compute micro-averaged precision, recall and F 1 . We perform two types of evaluations: (a) Full: evaluation on our full set of 11 relations (b) Temporal: Evaluation on temporal relations only, by collapsing PREV, CAUSES, and EN-ABLES to a single category and similarly for NEXT, CAUSED, and ENABLED (inter-annotator agreement \u03ba = 0.75).", "labels": [], "entities": [{"text": "precision", "start_pos": 146, "end_pos": 155, "type": "METRIC", "confidence": 0.9787569642066956}, {"text": "recall", "start_pos": 157, "end_pos": 163, "type": "METRIC", "confidence": 0.999643087387085}, {"text": "F 1", "start_pos": 168, "end_pos": 171, "type": "METRIC", "confidence": 0.9946716129779816}]}, {"text": "We computed statistical significance of our results with the paired bootstrap resampling method of 2000 iterations, where the units resampled are trigger-triggerrelation triples.", "labels": [], "entities": []}, {"text": "presents performance of all systems.", "labels": [], "entities": []}, {"text": "We see that using global constraints improves performance almost invariably on all measures in both full and temporal evaluations.", "labels": [], "entities": []}, {"text": "Particularly, in the full evaluation Global improves recall by 12% and overall F 1 improves significantly by 3.7 points against Local (p < 0.01).", "labels": [], "entities": [{"text": "recall", "start_pos": 53, "end_pos": 59, "type": "METRIC", "confidence": 0.9997830986976624}, {"text": "F 1", "start_pos": 79, "end_pos": 82, "type": "METRIC", "confidence": 0.9901230931282043}]}, {"text": "Recall improvement suggests that modeling connectivity allowed Global to add correct relations in cases where some events were not connected to one another.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Process statistics over 148 process descriptions.  NONE is used to indicate no relation.", "labels": [], "entities": [{"text": "NONE", "start_pos": 61, "end_pos": 65, "type": "METRIC", "confidence": 0.9754089713096619}]}, {"text": " Table 2: Node degree distribution for event mentions on  the training set. Predictions for the Local and Global  models were obtained using 10-fold cross validation.", "labels": [], "entities": []}, {"text": " Table 4: Test set results on all experiments. Best number  in each column is bolded.  \u2020 and  \u2021 denote statistical signif- icance (p < 0.01) against Local base and Local baselines,  respectively.", "labels": [], "entities": []}, {"text": " Table 5: Order by which constraint parameters were set  using coordinate ascent on the development set. For each  parameter, the value chosen and F 1 score after including  the constraint are provided. Negative values correspond  to penalties, positive values to rewards, and a value of \u221e  indicates a hard constraint.", "labels": [], "entities": [{"text": "F 1 score", "start_pos": 147, "end_pos": 156, "type": "METRIC", "confidence": 0.9696155587832133}]}]}