{"title": [{"text": "Grounding Strategic Conversation: Using negotiation dialogues to predict trades in a win-lose game", "labels": [], "entities": []}], "abstractContent": [{"text": "This paper describes a method that predicts which trades players execute during a win-lose game.", "labels": [], "entities": []}, {"text": "Our method uses data collected from chat negotiations of the game The Settlers of Catan and exploits the conversation to construct dynamically a partial model of each player's preferences.", "labels": [], "entities": [{"text": "The Settlers of Catan", "start_pos": 66, "end_pos": 87, "type": "DATASET", "confidence": 0.7010325044393539}]}, {"text": "This in turn yields equilibrium trading moves via principles from game theory.", "labels": [], "entities": []}, {"text": "We compare our method against four baselines and show that tracking how preferences evolve through the dialogue and reasoning about equilibrium moves are both crucial to success.", "labels": [], "entities": []}], "introductionContent": [{"text": "Rational agents act so as to maximise their expected utilities-an optimal trade off between what they prefer and what they believe they can achieve.", "labels": [], "entities": []}, {"text": "Solving a game problem involves finding equilibrium strategies: an optimal action for each player that maximises his expected utility, assuming that the other players perform their specified action).", "labels": [], "entities": []}, {"text": "Calculating equilibria thus requires knowledge of the other players' preferences but almost all bargaining games occur under the handicap of imperfect information about this.", "labels": [], "entities": []}, {"text": "Players therefore try to extract their opponents' preferences from what they say, likewise revealing their own preferences in their own utterances.", "labels": [], "entities": []}, {"text": "These elicited preferences guide an agent's decisions, like choosing to make such and such a bargain with such and such a person.", "labels": [], "entities": []}, {"text": "Tracking preferences through dialogue is thus crucial for analyzing the agents' strategic reasoning in real game scenarios.", "labels": [], "entities": []}, {"text": "In this paper, we design a model that maps what people say in a win-lose game into a prediction of exactly which players, if any, trade with each other, and exactly what resources they exchange.", "labels": [], "entities": []}, {"text": "We use both statistics and logic: we use a corpus of negotiation dialogues to learn classifiers that map each utterance to its speech act and to other acts pertinent to bargaining; and we develop a symbolic algorithm that, from the classifiers' output, dynamically constructs a model of each player's preferences as the conversation proceeds (for instance, the preference to receive a certain resource, or to accept a certain trade).", "labels": [], "entities": []}, {"text": "This preference model uses CP-nets (), a representation of preferences for which algorithms for computing equilibrium strategies exist.", "labels": [], "entities": []}, {"text": "We adapt those algorithms to predict the trades executed in the game.", "labels": [], "entities": []}, {"text": "The algorithm for construcing CP-nets uses only the output of our classifiers, which in turn rely entirely on shallow features in the raw text and robust parsers.", "labels": [], "entities": []}, {"text": "Together they provide an end to end model, from raw text to a prediction of which trade, if any, occurred.", "labels": [], "entities": []}, {"text": "We evaluate the various components of this (pipeline) algorithm separately, as well as the end to end model.", "labels": [], "entities": []}, {"text": "Our study exploits a corpus of negotiation dialogues from an online version of the win lose game The Settlers of Catan.", "labels": [], "entities": []}, {"text": "Sections 2 and 3 describe the corpus and its annotation.", "labels": [], "entities": []}, {"text": "Section 4 introduces our method for constructing the agents' preferences from the dialogues.", "labels": [], "entities": []}, {"text": "We use this in Section 5 to predict whether a trade is executed as a result of the players' negotiations, and if so we predict who took part in the trade, and what they exchanged.", "labels": [], "entities": []}, {"text": "Our method shows promising results, beating baselines that don't adequately track or reason about preferences.", "labels": [], "entities": []}, {"text": "We compare our model to related work in Section 6 and point to future work in Section 7.", "labels": [], "entities": [{"text": "Section 6", "start_pos": 40, "end_pos": 49, "type": "DATASET", "confidence": 0.9020053744316101}]}], "datasetContent": [{"text": "We compare our model against four baselines.", "labels": [], "entities": []}, {"text": "Since none of these baselines support reasoning about equilibrium moves, they all rely on the presence of an Accept act to predict there was a trade, and its absence to predict there wasn't.", "labels": [], "entities": []}, {"text": "The baselines differ, however, in how they identify the trading partners and resources in an executed trade.", "labels": [], "entities": []}, {"text": "The first baseline predicts a trade according to the first Offer and the last person to Accept, and if the Offer doesn't specify one of the resources then it is chosen randomly (similar random choices complete all partial predictions in all the models we consider here): e.g., for this would predict that Kittles gave clay to Rainbow (which is incorrect) in exchange for something that's chosen randomly (which will probably be incorrect).", "labels": [], "entities": [{"text": "Accept", "start_pos": 88, "end_pos": 94, "type": "METRIC", "confidence": 0.9774341583251953}]}, {"text": "The second baseline uses the last Offer and the last person to Accept: e.g., for this predicts that Kittles gave ore to Rainbow (correct) for something random (probably incorrect).", "labels": [], "entities": [{"text": "Accept", "start_pos": 63, "end_pos": 69, "type": "METRIC", "confidence": 0.9884582161903381}]}, {"text": "The third baseline uses the last Offer or Counteroffer, whichever is latest, and the last person to Accept: e.g., for this correctly predicts that Kittles gave ore to Rainbow in exchange for wheat.", "labels": [], "entities": [{"text": "Accept", "start_pos": 100, "end_pos": 106, "type": "METRIC", "confidence": 0.9963101744651794}]}, {"text": "And the fourth baseline, uses default unification between the prior Offers or Counteroffers and the current one to resolve any of the current offer's elided parts and to replace specific values in prior offers with conflicting specific values in the current offer.", "labels": [], "entities": []}, {"text": "One then takes the executed trade to be the result of this unification process at the point where the last Accept occurs.", "labels": [], "entities": [{"text": "Accept", "start_pos": 107, "end_pos": 113, "type": "METRIC", "confidence": 0.9659440517425537}]}, {"text": "This makes the same predictions as the third baseline for, but outperforms it in the corpus example (1) by predicting the correct and complete trade (i.e., Rainbow gave Kittles sheep for wheat, rather than for something random): (1) Rainbow: i need clay ore or wheat Kittles: i got wheat Rainbow: i cn giv sheep Kittles: ok We performed the evaluation on the data presented in Sections 3 and 4: 254 dialogues in total since we ignore dialogues that contain only Others.", "labels": [], "entities": []}, {"text": "90 of these dialogues end with a trade being executed and 2 of them end with 2 trades.", "labels": [], "entities": []}, {"text": "A random baseline would give 1.6% accuracy (given the 61 possible trading actions) and a frequency baseline (always choose no trade) gives 64.1% accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 34, "end_pos": 42, "type": "METRIC", "confidence": 0.9991057515144348}, {"text": "accuracy", "start_pos": 145, "end_pos": 153, "type": "METRIC", "confidence": 0.9972119927406311}]}, {"text": "presents the accuracy figures for all the models when calculated from the gold standard labels rather than the classifiers' predicted labels from Section 4, so that we can compare the models in isolation of the classifiers' errors.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 13, "end_pos": 21, "type": "METRIC", "confidence": 0.9995684027671814}]}, {"text": "McNemar's test shows that our model significantly outperforms all the baselines (p < 0.05).", "labels": [], "entities": []}, {"text": "A predicted trade counts as correct only if it specifies the right participants and the correct type of resources offered and received (we ignore their quantity).", "labels": [], "entities": []}, {"text": "True Positives (TP) are thus examples where the model correctly predicts not only that a trade happened, but also the correct partners and resources; Wrong Positives (WP), on the other hand, constitute a correct prediction that there was a trade but errors on the partners and/or resources involved (so WPs undermine accuracy).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 317, "end_pos": 325, "type": "METRIC", "confidence": 0.9984379410743713}]}, {"text": "True Negatives (TN) are examples where the model correctly predicts there was no trade (so TPs and TNs contribute to accuracy).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 117, "end_pos": 125, "type": "METRIC", "confidence": 0.9989153146743774}]}, {"text": "False Positives (FP) and False Negatives (FN) are respectively incorrect predictions that there was a trade, or that there was no trade.", "labels": [], "entities": [{"text": "False Positives (FP)", "start_pos": 0, "end_pos": 20, "type": "METRIC", "confidence": 0.8566236138343811}, {"text": "False Negatives (FN)", "start_pos": 25, "end_pos": 45, "type": "METRIC", "confidence": 0.9613720774650574}]}, {"text": "While does not reflect this, the first three baselines tend to predict incomplete information about the trade even when what they do predict is correct: that is, they predict the correct addressee and the owner but resort to random choice fora resource that's missing from the Offer or Counteroffer that predicts which trade occurred.", "labels": [], "entities": []}, {"text": "For the first baseline 34 examples are like this; for the second and third baselines it's 32.", "labels": [], "entities": []}, {"text": "In contrast, this problem occurs only once with the fourth baseline, and all the trades predicted by our method are complete, making random choice unnecessary.", "labels": [], "entities": []}, {"text": "Moreover, the first three baselines often make incorrect predictions about the addressee or resources exchanged because in contrast to our model and the fourth baseline, they don't track how potential trades evolve through a sequence of offers and counteroffers.", "labels": [], "entities": []}, {"text": "Even though the fourth baseline, which uses default unification to track the content of the current offer, is smart and gives good results, it has statistically significant lower accuracy than our model.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 179, "end_pos": 187, "type": "METRIC", "confidence": 0.9988495111465454}]}, {"text": "One major problem with the fourth baseline is that, in contrast to our model, it does not track each player's attitude towards the current offer.", "labels": [], "entities": []}, {"text": "Instead, like all our baselines, it relies on the presence of an Accept act to predict that there's a trade.", "labels": [], "entities": []}, {"text": "2 But several corpus examples are like (2), in which a trade is executed but there's no Accept act, thus yielding a False Negative (FN) for all four baselines: (2) Joel: anyone have sheep or wheat Cardlinger: neither :( Joel: will give clay or ore Euan: not just now Jon: got a wheat fora clay (Joel gives clay to Jon and receives wheat) So overall, our analysis shows that using CP-nets significantly outperforms all baselines that don't model how preferences evolve in the dialogue, and error analysis yields evidence that our model outperforms the fourth baseline because our model supports reasoning about player preferences, rational behavior and equilibrium strategies.", "labels": [], "entities": [{"text": "False Negative (FN)", "start_pos": 116, "end_pos": 135, "type": "METRIC", "confidence": 0.9619610667228699}]}, {"text": "presents the results for the end to end evaluation, where trade predictions are made from the classifiers' output from Section 4 rather than the gold standard labels.", "labels": [], "entities": []}, {"text": "As expected, performance decreases due to the classifiers' errors, mainly on the type of resources (Givable, etc.).", "labels": [], "entities": []}, {"text": "But our method still significantly outperforms all the baselines with an accuracy of 73.4% when the baselines obtain values between 60.9% and 68.4%.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 73, "end_pos": 81, "type": "METRIC", "confidence": 0.9995741248130798}]}], "tableCaptions": [{"text": " Table 2: Results for dialogue act classification.", "labels": [], "entities": [{"text": "dialogue act classification", "start_pos": 22, "end_pos": 49, "type": "TASK", "confidence": 0.8859802683194479}]}, {"text": " Table 3: Results for resource type classification.", "labels": [], "entities": [{"text": "resource type classification", "start_pos": 22, "end_pos": 50, "type": "TASK", "confidence": 0.7330176532268524}]}, {"text": " Table 4: Results for trade prediction. TP, FP, FN, TN  and WP are the True and False Positives, False and True  Negatives and Wrong Positives.", "labels": [], "entities": [{"text": "trade prediction", "start_pos": 22, "end_pos": 38, "type": "TASK", "confidence": 0.7402735650539398}]}, {"text": " Table 5: Results for the end to end trade prediction.", "labels": [], "entities": [{"text": "end to end trade prediction", "start_pos": 26, "end_pos": 53, "type": "TASK", "confidence": 0.5470185339450836}]}]}