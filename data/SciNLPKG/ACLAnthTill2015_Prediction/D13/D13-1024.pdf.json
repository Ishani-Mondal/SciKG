{"title": [{"text": "Structured Penalties for Log-linear Language Models", "labels": [], "entities": [{"text": "Log-linear Language Models", "start_pos": 25, "end_pos": 51, "type": "TASK", "confidence": 0.7535862326622009}]}], "abstractContent": [{"text": "Language models can be formalized as log-linear regression models where the input features represent previously observed contexts up to a certain length m.", "labels": [], "entities": []}, {"text": "The complexity of existing algorithms to learn the parameters by maximum likelihood scale linearly in nd, where n is the length of the training corpus and dis the number of observed features.", "labels": [], "entities": []}, {"text": "We present a model that grows logarithmically ind, making it possible to efficiently leverage longer contexts.", "labels": [], "entities": []}, {"text": "We account for the sequential structure of natural language using tree-structured penalized objectives to avoid over-fitting and achieve better generalization.", "labels": [], "entities": []}], "introductionContent": [{"text": "Language models are crucial parts of advanced natural language processing pipelines, such as speech recognition, machine translation (, or information retrieval (.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 93, "end_pos": 111, "type": "TASK", "confidence": 0.7401333749294281}, {"text": "machine translation", "start_pos": 113, "end_pos": 132, "type": "TASK", "confidence": 0.7765347361564636}, {"text": "information retrieval", "start_pos": 139, "end_pos": 160, "type": "TASK", "confidence": 0.7508149445056915}]}, {"text": "When a sequence of symbols is observed, a language model predicts the probability of occurrence of the next symbol in the sequence.", "labels": [], "entities": []}, {"text": "Models based on so-called back-off smoothing have shown good predictive power).", "labels": [], "entities": []}, {"text": "In particular, Kneser-Ney (KN) and its variants are still achieving state-of-the-art results for more than a decade after they were originally proposed.", "labels": [], "entities": []}, {"text": "Smoothing methods are in fact clever heuristics that require tuning parameters in an ad-hoc fashion.", "labels": [], "entities": []}, {"text": "Hence, more principled ways of learning language models have been proposed based on maximum entropy) or conditional random fields), or by adopting a Bayesian approach (.", "labels": [], "entities": []}, {"text": "In this paper, we focus on penalized maximum likelihood estimation in log-linear models.", "labels": [], "entities": [{"text": "penalized maximum likelihood estimation", "start_pos": 27, "end_pos": 66, "type": "TASK", "confidence": 0.44386544078588486}]}, {"text": "In contrast to language models based on unstructured norms such as 2 (quadratic penalties) or 1 (absolute discounting), we use tree-structured norms ().", "labels": [], "entities": []}, {"text": "Structured penalties have been successfully applied to various NLP tasks, including chunking and named entity recognition), but not language modelling.", "labels": [], "entities": [{"text": "chunking and named entity recognition", "start_pos": 84, "end_pos": 121, "type": "TASK", "confidence": 0.5918137311935425}, {"text": "language modelling", "start_pos": 132, "end_pos": 150, "type": "TASK", "confidence": 0.7666135430335999}]}, {"text": "Such penalties are particularly well-suited to this problem as they mimic the nested nature of word contexts.", "labels": [], "entities": []}, {"text": "However, existing optimizing techniques are not scalable for large contexts m.", "labels": [], "entities": []}, {"text": "In this work, we show that structured tree norms provide an efficient framework for language modelling.", "labels": [], "entities": [{"text": "language modelling", "start_pos": 84, "end_pos": 102, "type": "TASK", "confidence": 0.7616881132125854}]}, {"text": "For a special case of these tree norms, we obtain an memory-efficient learning algorithm for log-linear language models.", "labels": [], "entities": []}, {"text": "Furthermore, we aslo give the first efficient learning algorithm for structured \u221e tree norms with a complexity nearly linear in the number of training samples.", "labels": [], "entities": []}, {"text": "This leads to a memory-efficient and time-efficient learning algorithm for generalized linear language models.", "labels": [], "entities": []}, {"text": "The paper is organized as follows.", "labels": [], "entities": []}, {"text": "The model and other preliminary material is introduced in Section 2.", "labels": [], "entities": []}, {"text": "In Section 3, we review unstructured penalties that were proposed earlier.", "labels": [], "entities": []}, {"text": "Next, we propose structured penalties and compare their memory and time requirements.", "labels": [], "entities": [{"text": "memory", "start_pos": 56, "end_pos": 62, "type": "METRIC", "confidence": 0.9720706343650818}]}, {"text": "We summarize the characteristics of the proposed algorithms in Section 5 and experimentally validate our findings in Section 6.", "labels": [], "entities": []}, {"text": "Figure 1: Example of uncollapsed (trie) and corresponding collapsed (tree) structured vectors and proximal operators applied to them.", "labels": [], "entities": []}, {"text": "Weight values are written inside the node.", "labels": [], "entities": [{"text": "Weight", "start_pos": 0, "end_pos": 6, "type": "METRIC", "confidence": 0.9121060371398926}]}, {"text": "Subfigure (a) shows the complete trie Sand Subfigure (b) shows the corresponding collapsed tree T . The number in the brackets shows the number of nodes collapsed.", "labels": [], "entities": []}, {"text": "Subfigure (c) shows vector after proximal projection for T 2 -norm (which cannot be collapsed), and Subfigure (d) that of T \u221e -norm proximal projection which can be collapsed.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we demonstrate empirically the properties of the algorithms summarized in.", "labels": [], "entities": []}, {"text": "We consider four distinct subsets of the Associated Press News (AP-news) text corpus with train-test sizes of 100K-20K for our experiments.", "labels": [], "entities": [{"text": "Associated Press News (AP-news) text corpus", "start_pos": 41, "end_pos": 84, "type": "DATASET", "confidence": 0.8881177455186844}]}, {"text": "The corpus was preprocessed as described in ( by replacing proper nouns, numbers and rare words with special symbols \"proper noun\", \"#n\" and \"unknown\" respectively.", "labels": [], "entities": []}, {"text": "Punctuation marks are retained which are treated like other normal words.", "labels": [], "entities": [{"text": "Punctuation", "start_pos": 0, "end_pos": 11, "type": "METRIC", "confidence": 0.9448389410972595}]}, {"text": "Vocabulary size for each of the training subsets was around 8,500 words.", "labels": [], "entities": [{"text": "Vocabulary size", "start_pos": 0, "end_pos": 15, "type": "METRIC", "confidence": 0.924608588218689}]}, {"text": "The model was reset at the start of each sentence, meaning that a word in any given sentence does not depend on any word in the previous sentence.", "labels": [], "entities": []}, {"text": "The regularization parameter \u03bb is chosen for each model by cross-validation on a smaller subset of data.", "labels": [], "entities": []}, {"text": "Models are fitted to training sequence of 30K words for different values of \u03bb and validated against a sequence of 10K words to choose \u03bb.", "labels": [], "entities": []}, {"text": "We quantitatively evaluate the proposed model using perplexity, which is computed as follows: , where n V = i I(y i \u2208 V ).", "labels": [], "entities": []}, {"text": "Performance is measured for varying depth of the suffix trie with different penalties.", "labels": [], "entities": []}, {"text": "Interpolated Kneser-Ney results were computed using the openly available SRILM toolkit. and rand-pivot-col is the same applied with the nodes collapsed.", "labels": [], "entities": [{"text": "SRILM toolkit.", "start_pos": 73, "end_pos": 87, "type": "DATASET", "confidence": 0.8780624270439148}]}, {"text": "The k-best heap is the method described in Algorithm 4. that taking the tree-structure into account is beneficial.", "labels": [], "entities": []}, {"text": "Moreover, the log-linear language model with T 2 penalty performs similar to interpolated KneserNey.", "labels": [], "entities": []}, {"text": "The T \u221e -norm outperforms all other models at order 5, but taking the structure into account does not prevent a degradation of the performance at higher orders, unlike T 2 . This means that a single regularization for all model orders is still inappropriate.", "labels": [], "entities": []}, {"text": "To investigate this further, we adjust the penalties by choosing an exponential decrease of weights varying as \u03b1 m fora feature at depth min the suffix tree.", "labels": [], "entities": []}, {"text": "Parameter \u03b1 was tuned on a smaller validation set.", "labels": [], "entities": [{"text": "Parameter \u03b1", "start_pos": 0, "end_pos": 11, "type": "METRIC", "confidence": 0.9546903967857361}]}, {"text": "The best performing values for these weighted models w 2 2 , w 1 , w T 2 and w T \u221e are 0.5, 0.7, 1.1 and 0.85 respectively.", "labels": [], "entities": []}, {"text": "The weighting scheme further appropriates the regularization at various levels to suit the problem's structure.", "labels": [], "entities": []}, {"text": "Perplexity plots for weighted models are shown in(b).", "labels": [], "entities": []}, {"text": "While w 1 improves at larger depths, it fails to compare to others showing that the problem does not admit sparse solutions.", "labels": [], "entities": []}, {"text": "Weighted 2 2 improves considerably and performs comparably to the unweighted tree-structured norms.", "labels": [], "entities": []}, {"text": "However, the introduction of weighted features prevents us from using the suffix tree representation, making these models inefficient in terms of memory.", "labels": [], "entities": []}, {"text": "Weighted T \u221e is corrected for overfitting at larger depths and w T 2 gains more than others.", "labels": [], "entities": [{"text": "Weighted T \u221e", "start_pos": 0, "end_pos": 12, "type": "METRIC", "confidence": 0.8714843392372131}]}, {"text": "Optimal values for \u03b1 are fractional for all norms except w T 2 -norm showing that the unweighted model T 2 -norm was over-penalizing features at larger depths, while that of others were under-penalizing them.", "labels": [], "entities": []}, {"text": "Interestingly, perplexity improves up to about 9-grams with w T 2 penalty for the data set we considered, indicating that there is more to gain from longer dependencies in natural language sentences than what is currently believed.(c) compares model complexity measured by the number of parameters for weighted models using structured penalties.", "labels": [], "entities": []}, {"text": "The T 2 penalty is applied on trie-structured vectors, which grows roughly at a linear rate with increasing model order.", "labels": [], "entities": [{"text": "T 2 penalty", "start_pos": 4, "end_pos": 15, "type": "METRIC", "confidence": 0.8729558984438578}]}, {"text": "This is similar to Kneser-Ney.", "labels": [], "entities": []}, {"text": "However, the number of parameters for thew T \u221e penalty grows logarithmically with the model order.", "labels": [], "entities": []}, {"text": "This is due to the fact that it operates on the suffix tree-structured vectors instead of the suffix trie-structured vectors.", "labels": [], "entities": []}, {"text": "These results are valid for, both, weighted and unweighted penalties.", "labels": [], "entities": []}, {"text": "Next, we compare the average time taken per iteration for different implementations of the T \u221e proximal step.", "labels": [], "entities": []}, {"text": "shows this time against increasing depth of the language model order for random pivoting method with and without the collapsing of parameters at different constant value nonbranching paths.", "labels": [], "entities": []}, {"text": "The trend in this plot resembles that of the number of parameters in.", "labels": [], "entities": []}, {"text": "This shows that the complexity of the full proximal step is sublinear when accounting for the suffix tree data structure.(b) plots time per iteration random pivoting and k-best heap against the varying size of training sequence.", "labels": [], "entities": []}, {"text": "The two algorithms are operating directly on the suffix tree.", "labels": [], "entities": []}, {"text": "It can be observed that the heap-based method are superior with increasing size of training data.", "labels": [], "entities": []}], "tableCaptions": []}