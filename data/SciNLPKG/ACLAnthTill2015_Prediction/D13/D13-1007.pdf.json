{"title": [{"text": "A Log-Linear Model for Unsupervised Text Normalization", "labels": [], "entities": [{"text": "Unsupervised Text Normalization", "start_pos": 23, "end_pos": 54, "type": "TASK", "confidence": 0.6670716603597006}]}], "abstractContent": [{"text": "We present a unified unsupervised statistical model for text normalization.", "labels": [], "entities": [{"text": "text normalization", "start_pos": 56, "end_pos": 74, "type": "TASK", "confidence": 0.8114600777626038}]}, {"text": "The relationship between standard and non-standard tokens is characterized by a log-linear model, permitting arbitrary features.", "labels": [], "entities": []}, {"text": "The weights of these features are trained in a maximum-likelihood framework, employing a novel sequential Monte Carlo training algorithm to overcome the large label space, which would be impractical for traditional dynamic programming solutions.", "labels": [], "entities": []}, {"text": "This model is implemented in a normalization system called UNLOL, which achieves the best known results on two normalization datasets, outper-forming more complex systems.", "labels": [], "entities": [{"text": "UNLOL", "start_pos": 59, "end_pos": 64, "type": "DATASET", "confidence": 0.8995729684829712}]}, {"text": "We use the output of UNLOL to automatically normalize a large corpus of social media text, revealing a set of coherent orthographic styles that underlie online language variation.", "labels": [], "entities": [{"text": "UNLOL", "start_pos": 21, "end_pos": 26, "type": "DATASET", "confidence": 0.871870219707489}]}], "introductionContent": [{"text": "Social media language can differ substantially from other written text.", "labels": [], "entities": []}, {"text": "Many of the attempts to characterize and overcome this variation have focused on normalization: transforming social media language into text that better matches standard datasets).", "labels": [], "entities": []}, {"text": "Because there is little available training data, and because social media language changes rapidly, fully supervised training is generally not considered appropriate for this task.", "labels": [], "entities": []}, {"text": "However, due to the extremely high-dimensional output space -arbitrary sequences of words across the vocabulary -it is a very challenging problem for unsupervised learning.", "labels": [], "entities": []}, {"text": "Perhaps it is for these reasons that the most successful systems are pipeline architectures that cobble together a diverse array of techniques and resources, including statistical language models, dependency parsers, string edit distances, off-the-shelf spellcheckers, and curated slang dictionaries (.", "labels": [], "entities": []}, {"text": "We propose a different approach, performing normalization in a maximum-likelihood framework.", "labels": [], "entities": []}, {"text": "There are two main sources of information to be exploited: local context, and surface similarity between the observed strings and normalization candidates.", "labels": [], "entities": []}, {"text": "We treat the local context using standard language modeling techniques; we treat string similarity with a log-linear model that includes features for both surface similarity and word-word pairs.", "labels": [], "entities": []}, {"text": "Because labeled examples of normalized text are not available, this model cannot be trained in the standard supervised fashion.", "labels": [], "entities": []}, {"text": "Nor can we apply dynamic programming techniques for unsupervised training of locally-normalized conditional models, as their complexity is quadratic in the size of label space; in normalization, the label space is the vocabulary itself, with at least 10 4 elements.", "labels": [], "entities": []}, {"text": "Instead, we present anew training approach using Monte Carlo techniques to compute an approximate gradient on the feature weights.", "labels": [], "entities": []}, {"text": "This training method maybe applicable in other unsupervised learning problems with a large label space.", "labels": [], "entities": []}, {"text": "This model is implemented in a normalization system called UNLOL (unsupervised normalization in a LOg-Linear model).", "labels": [], "entities": [{"text": "UNLOL", "start_pos": 59, "end_pos": 64, "type": "DATASET", "confidence": 0.7097576260566711}]}, {"text": "It is a lightweight proba-bilistic approach, relying only on a language model for the target domain; it can be adapted to new corpora text or new domains easily and quickly.", "labels": [], "entities": []}, {"text": "Our evaluations show that UNLOL outperforms the state-of-the-art on standard normalization datasets.", "labels": [], "entities": []}, {"text": "In addition, we demonstrate the linguistic insights that can be obtained from normalization, using UNLOL to identify classes of orthographic transformations that form coherent linguistic styles.", "labels": [], "entities": [{"text": "UNLOL", "start_pos": 99, "end_pos": 104, "type": "DATASET", "confidence": 0.8403454422950745}]}], "datasetContent": [{"text": "Datasets We use two existing labeled Twitter datasets to evaluate our approach.", "labels": [], "entities": []}, {"text": "The first dataset -which we call LWWL11, based on the names of its authors Liu et al.", "labels": [], "entities": []}, {"text": "(2011) -contains 3,802 individual \"nonstandard\" words (i.e., words that are not in the target vocabulary) and their normalized forms.", "labels": [], "entities": []}, {"text": "The rest of the message in which the words is appear is not available.", "labels": [], "entities": []}, {"text": "As this corpus does not provide linguistic context, its decoding must use a unigram target language model.", "labels": [], "entities": []}, {"text": "The second dataset -which is called LexNorm1.1 by its authors Han and Baldwin (2011) -contains 549 complete tweets with 1,184 nonstandard tokens (558 unique word types).", "labels": [], "entities": [{"text": "LexNorm1.1", "start_pos": 36, "end_pos": 46, "type": "DATASET", "confidence": 0.9502484202384949}]}, {"text": "Precision Recall F-measure (  In this corpus, we can decode with a trigram language model.", "labels": [], "entities": [{"text": "Precision Recall F-measure", "start_pos": 0, "end_pos": 26, "type": "METRIC", "confidence": 0.6560011307398478}]}, {"text": "Close analysis of LexNorm1.1 revealed some inconsistencies in annotation (for example, y'all and 2 are sometimes normalized to you and to, but are left unnormalized in other cases).", "labels": [], "entities": [{"text": "LexNorm1.1", "start_pos": 18, "end_pos": 28, "type": "DATASET", "confidence": 0.973681628704071}]}, {"text": "In addition, several annotations disagree with existing resources on internet language and dialectal English.", "labels": [], "entities": []}, {"text": "For example, smh is normalized to somehow in LexNorm1.1, but internetslang.com and urbandictionary.com assert that it stands for shake my head, and this is evident from examples such as smh at this girl.", "labels": [], "entities": [{"text": "LexNorm1.1", "start_pos": 45, "end_pos": 55, "type": "DATASET", "confidence": 0.956916868686676}]}, {"text": "Similarly, finna is normalized to finally in LexNorm1.1, but from the literature on African American English, it corresponds to fixing to (e.g., i'm finna go home).", "labels": [], "entities": [{"text": "LexNorm1.1", "start_pos": 45, "end_pos": 55, "type": "DATASET", "confidence": 0.9774272441864014}]}, {"text": "To address these issues, we have produced anew version of this dataset, which we call LexNorm1.2 (after consulting with the creators of LexNorm1.1).", "labels": [], "entities": [{"text": "LexNorm1.2", "start_pos": 86, "end_pos": 96, "type": "DATASET", "confidence": 0.9705105423927307}, {"text": "LexNorm1.1", "start_pos": 136, "end_pos": 146, "type": "DATASET", "confidence": 0.9470967054367065}]}, {"text": "LexNorm1.2 differs from version 1.1 in the annotations for 172 of the 2140 OOV words.", "labels": [], "entities": [{"text": "LexNorm1.2", "start_pos": 0, "end_pos": 10, "type": "DATASET", "confidence": 0.9655008316040039}]}, {"text": "We evaluate on LexNorm1.1 to compare with prior work, but we also present results on LexNorm1.2 in the hope that it will become standard in future work on normalization in English.", "labels": [], "entities": [{"text": "LexNorm1.1", "start_pos": 15, "end_pos": 25, "type": "DATASET", "confidence": 0.9771990776062012}, {"text": "LexNorm1.2", "start_pos": 85, "end_pos": 95, "type": "DATASET", "confidence": 0.9669636487960815}, {"text": "normalization", "start_pos": 155, "end_pos": 168, "type": "TASK", "confidence": 0.9688600301742554}]}, {"text": "The dataset is available at http://www.cc.gatech.edu/ ~jeisenst/lexnorm.v1.2.tgz.", "labels": [], "entities": []}, {"text": "To obtain unlabeled training data, we randomly sample 50 tweets from the Edinburgh Twitter corpus Petrovi\u00b4c for each OOV word.", "labels": [], "entities": [{"text": "Edinburgh Twitter corpus Petrovi\u00b4c", "start_pos": 73, "end_pos": 107, "type": "DATASET", "confidence": 0.9367480874061584}]}, {"text": "Some OOV words appear less than 50 times in the corpus, so we obtained more training tweets for them through the Twitter search API.", "labels": [], "entities": []}, {"text": "Metrics Prior work on these datasets has assumed perfect detection of words requiring normalization, and has focused on finding the correct normalization for these.", "labels": [], "entities": []}, {"text": "Recall has been defined as the proportion of words requiring normalization which are normalized correctly; precision is defined as the proportion of normalizations which are correct.", "labels": [], "entities": [{"text": "Recall", "start_pos": 0, "end_pos": 6, "type": "METRIC", "confidence": 0.937713086605072}, {"text": "precision", "start_pos": 107, "end_pos": 116, "type": "METRIC", "confidence": 0.9993821382522583}]}, {"text": "Results We run our training algorithm for two iterations (pass the training data twice).", "labels": [], "entities": []}, {"text": "The results are presented in.", "labels": [], "entities": []}, {"text": "Our system, UNLOL, achieves the highest published F-measure on both datasets.", "labels": [], "entities": [{"text": "UNLOL", "start_pos": 12, "end_pos": 17, "type": "DATASET", "confidence": 0.9223071932792664}, {"text": "F-measure", "start_pos": 50, "end_pos": 59, "type": "METRIC", "confidence": 0.965213418006897}]}, {"text": "Performance on LexNorm1.2 is very similar to LexNorm1.1, despite the fact that roughly 8% of the examples were relabeled.", "labels": [], "entities": [{"text": "LexNorm1.2", "start_pos": 15, "end_pos": 25, "type": "DATASET", "confidence": 0.981106162071228}, {"text": "LexNorm1.1", "start_pos": 45, "end_pos": 55, "type": "DATASET", "confidence": 0.9713564515113831}]}, {"text": "In the normalization task that we consider, the tokens to be normalized are specified in advance.", "labels": [], "entities": [{"text": "normalization task", "start_pos": 7, "end_pos": 25, "type": "TASK", "confidence": 0.9179005920886993}]}, {"text": "This is the same task specification as in the prior work against which we compare.", "labels": [], "entities": []}, {"text": "At test time, our system attempts normalizes all such tokens; every error is thus both a false positive and false negative, so precision equals to recall for this task; this is also true for Han and Baldwin (2011) and.", "labels": [], "entities": [{"text": "precision", "start_pos": 127, "end_pos": 136, "type": "METRIC", "confidence": 0.9993171691894531}, {"text": "recall", "start_pos": 147, "end_pos": 153, "type": "METRIC", "confidence": 0.9990555644035339}]}, {"text": "It is possible to trade recall for precision by refusing to normalize words when the system's confidence falls below a threshold.", "labels": [], "entities": [{"text": "recall", "start_pos": 24, "end_pos": 30, "type": "METRIC", "confidence": 0.9986149072647095}, {"text": "precision", "start_pos": 35, "end_pos": 44, "type": "METRIC", "confidence": 0.9988797307014465}]}, {"text": "A good setting of this threshold can improve the F-measure, but we did not report these results because we have no development set for parameter tuning.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 49, "end_pos": 58, "type": "METRIC", "confidence": 0.9975830316543579}]}, {"text": "Regularization One potential concern is that the number of non-zero feature weights will continually increase until the memory cost becomes overwhelming.", "labels": [], "entities": [{"text": "Regularization", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.9535980820655823}]}, {"text": "Although we did not run up against mem-  Figure 1: Effect of L1 regularization on the F-measure and the number of features with non-zero weights ory limitations in the experiments producing the results in, this issue can be addressed through the application of L1 regularization, which produces sparse weight vectors by adding a penalty of \u03bb||\u03b8|| 1 to the log-likelihood.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 86, "end_pos": 95, "type": "METRIC", "confidence": 0.8732503652572632}]}, {"text": "We perform online optimization of the L1-regularized log-likelihood by applying the truncated gradient method (.", "labels": [], "entities": []}, {"text": "We use an exponential decreasing learning rate \u03b7 k = \u03b7 0 \u03b1 k/N , where k is the iteration counter and N is the size of training data.", "labels": [], "entities": []}, {"text": "We set \u03b7 0 = 1 and \u03b1 = 0.5.", "labels": [], "entities": []}, {"text": "Experiments were run until 300,000 training instances were observed, with a final learning rate of less than 1/32.", "labels": [], "entities": [{"text": "learning rate", "start_pos": 82, "end_pos": 95, "type": "METRIC", "confidence": 0.8476960062980652}]}, {"text": "As shown in, a small amount of regularization can dramatically decrease the number of active features without harming performance.", "labels": [], "entities": []}], "tableCaptions": []}