{"title": [{"text": "Pair Language Models for Deriving Alternative Pronunciations and Spellings from Pronunciation Dictionaries", "labels": [], "entities": [{"text": "Deriving Alternative Pronunciations and Spellings from Pronunciation Dictionaries", "start_pos": 25, "end_pos": 106, "type": "TASK", "confidence": 0.7258023321628571}]}], "abstractContent": [{"text": "Pronunciation dictionaries provide a readily available parallel corpus for learning to trans-duce between character strings and phoneme strings or vice versa.", "labels": [], "entities": []}, {"text": "Translation models can be used to derive character-level paraphrases on either side of this transduction, allowing for the automatic derivation of alternative pronunciations or spellings.", "labels": [], "entities": []}, {"text": "We examine finite-state and SMT-based methods for these related tasks, and demonstrate that the tasks have different characteristics-finding alternative spellings is harder than alternative pronunciations and benefits from round-trip algorithms when the other does not.", "labels": [], "entities": [{"text": "SMT-based", "start_pos": 28, "end_pos": 37, "type": "TASK", "confidence": 0.9786705374717712}]}, {"text": "We also show that we can increase accuracy by modeling syllable stress.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 34, "end_pos": 42, "type": "METRIC", "confidence": 0.9986913800239563}]}], "introductionContent": [{"text": "Robust processing of speech and language requires dealing with variation in language production, either in terms of pronunciation in the spoken domain or spelling in the written domain.", "labels": [], "entities": [{"text": "Robust processing of speech and language", "start_pos": 0, "end_pos": 40, "type": "TASK", "confidence": 0.8782233496507009}]}, {"text": "Predicting the intended words of an acoustic or textual sequence is an important recognition task, often required for downstream processing such as spoken language understanding or knowledge extraction.", "labels": [], "entities": [{"text": "Predicting the intended words of an acoustic or textual sequence", "start_pos": 0, "end_pos": 64, "type": "TASK", "confidence": 0.8225631475448608}, {"text": "spoken language understanding", "start_pos": 148, "end_pos": 177, "type": "TASK", "confidence": 0.6892284949620565}, {"text": "knowledge extraction", "start_pos": 181, "end_pos": 201, "type": "TASK", "confidence": 0.7412006109952927}]}, {"text": "Informal text genres, such as those found in social media, share some characteristics with speech; in fact such text is often informed by pronunciation variation.", "labels": [], "entities": []}, {"text": "For example, consider the following tweet: He aint gotta question my loyalty, cuz he knw wen sh!t get real.", "labels": [], "entities": []}, {"text": "where several tokens (e.g. \"cuz\", \"wen\") represent spelling alternations related to pronunciation.", "labels": [], "entities": []}, {"text": "Work in text normalization and spelling correction -e.g.,; has included pronunciation information to improve recognition of the intended word, via grapheme to phoneme (g2p) conversion modeling derived from pronunciation dictionaries.", "labels": [], "entities": [{"text": "text normalization", "start_pos": 8, "end_pos": 26, "type": "TASK", "confidence": 0.764450877904892}, {"text": "spelling correction", "start_pos": 31, "end_pos": 50, "type": "TASK", "confidence": 0.8111370801925659}]}, {"text": "Pronunciation dictionaries provide natural parallel corpora, with strings of characters paired to strings of phones.", "labels": [], "entities": []}, {"text": "Thus, standard lexicons have been used in recent years with machine translation systems such as Moses (, to train g2p systems.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 60, "end_pos": 79, "type": "TASK", "confidence": 0.7158663123846054}]}, {"text": "Further, other algorithms using such dictionaries also use translation phrase tables, but not for translation tasks.", "labels": [], "entities": [{"text": "translation tasks", "start_pos": 98, "end_pos": 115, "type": "TASK", "confidence": 0.9035624265670776}]}, {"text": "For example, data-driven paraphrasing methods) use translation phrase-tables as a \"pivot\" to learn sets of phrases which translated to the same target phrase.", "labels": [], "entities": []}, {"text": "Ina similar manner, with a pronunciation dictionary instead of a phrsetable, pivoting can be used to learn alternative pronunciations (, i.e., direct phoneme-to-phoneme (p2p) \"translation\" systems that yield alternative pronunciations.", "labels": [], "entities": []}, {"text": "Alternatively, round-trip translation could be used, e.g., to map from letter strings to phone strings in one step, then from the resulting phone strings to letter strings in a second step, as the means to find alternative spellings (.", "labels": [], "entities": []}, {"text": "In this study, we explore dictionary-derived models to find either alternative pronunciations or alternative spellings, using either direct (p2p or g2g) or round-trip algorithms (p2g2p or g2p2g).", "labels": [], "entities": []}, {"text": "We compare methods based on weighted finite-state transducers (WFST) with phrase-based models trained with Moses.", "labels": [], "entities": []}, {"text": "Our main interest is to evaluate Karanasou and methods -shown to be useful for deriving alternative pronunciations -for deriving alternative spellings, and thus to determine the relative difficulty of these two tasks.", "labels": [], "entities": [{"text": "deriving alternative spellings", "start_pos": 120, "end_pos": 150, "type": "TASK", "confidence": 0.885513703028361}]}, {"text": "We also examine when, if ever, round-trip processing yields benefits over direct transduction.", "labels": [], "entities": []}, {"text": "Our results indicate that real alternative pronunciations are substantially easier to find than real alternative spellings, partic-ularly when pronunciation features such as syllable stress are available.", "labels": [], "entities": []}, {"text": "Second, round trip translation yields no gain (and some loss) over direct transduction for finding alternative pronunciations, yet yields some modest gains for finding alternative spellings.", "labels": [], "entities": []}, {"text": "Further, WFST methods perform as well as or better than Moses trained models.", "labels": [], "entities": []}, {"text": "Finally, combining the methods yields further gains, indicating that the models are learning complementary sets of patterns.", "labels": [], "entities": []}, {"text": "The primary contribution of this work is to introduce a competitive method of building and using pair language model WFSTs for generating alternative spellings and pronunciations which reflect real-world variability.", "labels": [], "entities": []}, {"text": "This could improve results for downstream processes, e.g., epidemiological studies ( or sentiment analysis () derived from social media text.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 88, "end_pos": 106, "type": "TASK", "confidence": 0.9173462986946106}]}, {"text": "Further, we present a controlled comparison between the two tasks, and demonstrate that they differ in terms of task difficulty", "labels": [], "entities": []}], "datasetContent": [{"text": "For evaluation purposes, we reserved a set of 1000 test homophone sets and 1000 test homograph sets, as described in Section 3.1.", "labels": [], "entities": []}, {"text": "From each set, we generate alternatives from the longest set member (ties broken alphabetically) and examine the resulting kbest list for presence of other members of the set.", "labels": [], "entities": []}, {"text": "Note that the input string itself is not a target, and, before evaluation, is removed from the k-best list.", "labels": [], "entities": []}, {"text": "Recall is the proportion of the k-best list returned by the system: Results for generating alternative pronunciations are listed in; those for generating alternative spellings are in.", "labels": [], "entities": []}, {"text": "For alternative spellings, we also present results that combine the outputs of direct, round-trip (no stress) and Moses into a single list using a simple ranked voting scheme (simple Borda count).", "labels": [], "entities": [{"text": "Borda count", "start_pos": 183, "end_pos": 194, "type": "METRIC", "confidence": 0.9635203778743744}]}, {"text": "A noteworthy result is the apparent usefulness of stress modeling for predicting pronunciation variation using WFSTs with the direct method; this is: Recall for generating alternative pronunciations seen in the first two data columns of 1.", "labels": [], "entities": [{"text": "predicting pronunciation variation", "start_pos": 70, "end_pos": 104, "type": "TASK", "confidence": 0.8394138018290201}, {"text": "WFSTs", "start_pos": 111, "end_pos": 116, "type": "DATASET", "confidence": 0.8488278985023499}, {"text": "Recall", "start_pos": 150, "end_pos": 156, "type": "METRIC", "confidence": 0.9509341716766357}]}, {"text": "This suggests that stress has an effect on phoneme alteration, something we discuss in more detail in Section 5.", "labels": [], "entities": [{"text": "phoneme alteration", "start_pos": 43, "end_pos": 61, "type": "TASK", "confidence": 0.7563362121582031}]}, {"text": "However, while providing a large gain in the p2p condition, pronunciation modeling gives small or negative effects elsewhere.", "labels": [], "entities": [{"text": "pronunciation modeling", "start_pos": 60, "end_pos": 82, "type": "TASK", "confidence": 0.9308768212795258}]}, {"text": "In the round trip methods, the effects of stress are lost: stress has little influence of how a particular phoneme is spelled.", "labels": [], "entities": []}, {"text": "Thus, graphemes do not retain much stress information, hence any pass through the orthographic domain will shed it.", "labels": [], "entities": []}, {"text": "Recall is higher for alternative pronunciations than for alternative spellings.", "labels": [], "entities": [{"text": "Recall", "start_pos": 0, "end_pos": 6, "type": "METRIC", "confidence": 0.9953553676605225}]}, {"text": "One reason for this is that spellings in our test set average eight letters, whereas the pronunciations average around five phonemes.", "labels": [], "entities": []}, {"text": "Furthermore, the average Levenshtein distance between original spellings and their target alternatives, is 2.6, while for pronunciations, it is 2.2.", "labels": [], "entities": [{"text": "Levenshtein distance", "start_pos": 25, "end_pos": 45, "type": "METRIC", "confidence": 0.895079493522644}]}, {"text": "Combining these factors, we see that, for spellings, more edit operations are required, and there are more symbols to which to apply them.", "labels": [], "entities": [{"text": "spellings", "start_pos": 42, "end_pos": 51, "type": "TASK", "confidence": 0.9707831144332886}]}, {"text": "Therefore, for spellings, there are more incorrect candidates.", "labels": [], "entities": [{"text": "spellings", "start_pos": 15, "end_pos": 24, "type": "TASK", "confidence": 0.9499844908714294}]}, {"text": "The results also show gains resulting from the roundtrip method when applied to finding alternative spellings, but no such gains when roundtrip methods are applied to alternative pronunciations.", "labels": [], "entities": []}, {"text": "Suppose, when seeking alternatives for some spelling, we alter grapheme g 1 tog 2 . With a direct method, we must have instances of g 1 mapping tog 2 in the training set.", "labels": [], "entities": []}, {"text": "The roundtrip method, however, is less constrained: there must exist some phoneme p 1 in the training set such that g 1 maps top 1 , and p 1 maps tog 2 ; thus, the set of possible alternations at testing are {g 1 \u2192 p 1 } \u00d7 {p 1 \u2192 g 2 }.", "labels": [], "entities": []}, {"text": "This argument also applies to finding alternative pronunciations.", "labels": [], "entities": []}, {"text": "Thus the roundtrip method offers more possible mappings.", "labels": [], "entities": []}, {"text": "These extra possible mappings maybe helpful or harmful, depending on how likely they are compared to the possible mappings they displace.", "labels": [], "entities": []}, {"text": "Why are they helpful for alternative spellings, but not for al-  ternative pronunciations?", "labels": [], "entities": [{"text": "al-  ternative pronunciations", "start_pos": 60, "end_pos": 89, "type": "TASK", "confidence": 0.6934960708022118}]}, {"text": "We discuss one possible explanation in Section 5.", "labels": [], "entities": []}, {"text": "Comparing Moses to the pair language model methods, Moses does slightly better for smaller n (n = 1, 3), and slightly worse for larger n (n = 10).", "labels": [], "entities": []}, {"text": "Our only partial explanation for this is that Moses does well at weighing alternatives but, possibly, does not generate a large number of viable alternatives.", "labels": [], "entities": []}, {"text": "System combination yields solid gains in finding alternative spellings, demonstrating that these different systems are coming up with diverse options.", "labels": [], "entities": []}, {"text": "Finally, we note that many of the false positive pronunciations given by the WFST system are plausibly correct although they are not included in the CMU dictionary.", "labels": [], "entities": [{"text": "WFST system", "start_pos": 77, "end_pos": 88, "type": "DATASET", "confidence": 0.8763425946235657}, {"text": "CMU dictionary", "start_pos": 149, "end_pos": 163, "type": "DATASET", "confidence": 0.9697329699993134}]}, {"text": "For example, for the spelling, adequate, the CMU dictionary provides two pronunciations: /aed@kw@t/ and /aed@kwet/.", "labels": [], "entities": [{"text": "CMU dictionary", "start_pos": 45, "end_pos": 59, "type": "DATASET", "confidence": 0.9745544791221619}]}, {"text": "Meanwhile, the p2p WFST system (with stress modeling) produces /aed@kwIt/.", "labels": [], "entities": []}, {"text": "This suggests that we can learn from CMU dictionary to predict actual pronunciations that CMU dictionary does not itself list.", "labels": [], "entities": [{"text": "CMU dictionary", "start_pos": 37, "end_pos": 51, "type": "DATASET", "confidence": 0.9494287371635437}, {"text": "CMU dictionary", "start_pos": 90, "end_pos": 104, "type": "DATASET", "confidence": 0.9603685438632965}]}], "tableCaptions": [{"text": " Table 1: Recall for generating alternative pronunciations", "labels": [], "entities": [{"text": "Recall", "start_pos": 10, "end_pos": 16, "type": "METRIC", "confidence": 0.5652646422386169}]}, {"text": " Table 2: Recall for generating alternative spellings", "labels": [], "entities": [{"text": "Recall", "start_pos": 10, "end_pos": 16, "type": "TASK", "confidence": 0.5099427700042725}]}]}