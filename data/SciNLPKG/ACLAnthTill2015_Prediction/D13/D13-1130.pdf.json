{"title": [{"text": "Leveraging lexical cohesion and disruption for topic segmentation Anca S \u00b8 imon Universit\u00e9 de Rennes 1 IRISA & INRIA Rennes", "labels": [], "entities": [{"text": "topic segmentation", "start_pos": 47, "end_pos": 65, "type": "TASK", "confidence": 0.7729527950286865}, {"text": "Anca S \u00b8 imon Universit\u00e9 de Rennes 1 IRISA", "start_pos": 66, "end_pos": 108, "type": "DATASET", "confidence": 0.7396731310420566}, {"text": "INRIA", "start_pos": 111, "end_pos": 116, "type": "DATASET", "confidence": 0.5421393513679504}, {"text": "Rennes", "start_pos": 117, "end_pos": 123, "type": "DATASET", "confidence": 0.7431910634040833}]}], "abstractContent": [{"text": "Topic segmentation classically relies on one of two criteria, either finding areas with coherent vocabulary use or detecting discontinu-ities.", "labels": [], "entities": [{"text": "Topic segmentation", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.8537474274635315}]}, {"text": "In this paper, we propose a segmenta-tion criterion combining both lexical cohesion and disruption, enabling a trade-off between the two.", "labels": [], "entities": []}, {"text": "We provide the mathematical formulation of the criterion and an efficient graph based decoding algorithm for topic segmenta-tion.", "labels": [], "entities": []}, {"text": "Experimental results on standard textual data sets and on a more challenging corpus of automatically transcribed broadcast news shows demonstrate the benefit of such a combination.", "labels": [], "entities": []}, {"text": "Gains were observed in all conditions , with segments of either regular or varying length and abrupt or smooth topic shifts.", "labels": [], "entities": [{"text": "Gains", "start_pos": 0, "end_pos": 5, "type": "METRIC", "confidence": 0.9236274361610413}]}, {"text": "Long segments benefit more than short segments.", "labels": [], "entities": []}, {"text": "However the algorithm has proven robust on automatic transcripts with short segments and limited vocabulary reoccurrences.", "labels": [], "entities": []}], "introductionContent": [{"text": "Topic segmentation consists in evidentiating the semantic structure of a document: Algorithms developed for this task aim at automatically detecting frontiers which define topically coherent segments in a text.", "labels": [], "entities": [{"text": "Topic segmentation", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.7976985275745392}, {"text": "evidentiating the semantic structure of a document", "start_pos": 31, "end_pos": 81, "type": "TASK", "confidence": 0.8449381760188511}]}, {"text": "Various methods for topic segmentation of textual data are described in the literature, e.g.,), most of them relying on the notion of lexical cohesion, i.e., identifying segments with a consistent use of vocabulary, either based on words or on semantic relations between words.", "labels": [], "entities": [{"text": "topic segmentation of textual", "start_pos": 20, "end_pos": 49, "type": "TASK", "confidence": 0.8150314539670944}]}, {"text": "Reoccurrences of words or related words and lexical chains are two popular methods to evidence lexical cohesion.", "labels": [], "entities": []}, {"text": "This general principle of lexical cohesion is further exploited for topic segmentation with two radically different strategies.", "labels": [], "entities": [{"text": "topic segmentation", "start_pos": 68, "end_pos": 86, "type": "TASK", "confidence": 0.8652765452861786}]}, {"text": "On the one hand, a measure of the lexical cohesion can be used to determine coherent segments).", "labels": [], "entities": []}, {"text": "On the other hand, shifts in the use of vocabulary can be searched for to directly identify the segment frontiers by measuring the lexical disruption.", "labels": [], "entities": []}, {"text": "Techniques based on the first strategy yield more accurate segmentation results, but face a problem of over-segmentation which can, up to now, only be solved by providing prior information regarding the distribution of segment length or the expected number of segments.", "labels": [], "entities": [{"text": "segmentation", "start_pos": 59, "end_pos": 71, "type": "TASK", "confidence": 0.9570857882499695}]}, {"text": "In this paper, we propose a segmentation criterion combining both cohesion and disruption along with the corresponding algorithm for topic segmentation.", "labels": [], "entities": [{"text": "topic segmentation", "start_pos": 133, "end_pos": 151, "type": "TASK", "confidence": 0.7499930262565613}]}, {"text": "Such a criterion ensures a coherent use of vocabulary within each resulting segment, as well as a significant difference of vocabulary between neighboring segments.", "labels": [], "entities": []}, {"text": "Moreover, the combination of these two strategies enables regularizing the number of segments found without resorting to prior knowledge.", "labels": [], "entities": []}, {"text": "This piece of work uses the algorithm of as a starting point, a versatile and performing topic segmentation algorithm cast in a statistical framework.", "labels": [], "entities": [{"text": "topic segmentation algorithm", "start_pos": 89, "end_pos": 117, "type": "TASK", "confidence": 0.7855263551076254}]}, {"text": "Among the benefits of this algorithm are its independency to any particular domain and its ability to cope with thematic segments of highly varying lengths, two interesting features to obtain a generic solution to the problem of topic segmentation.", "labels": [], "entities": [{"text": "topic segmentation", "start_pos": 229, "end_pos": 247, "type": "TASK", "confidence": 0.7224848568439484}]}, {"text": "Moreover, the algorithm has proven to be up to the state of the art in several studies, with no need of a priori information about the number of segments (contrary to algorithms in () that can attain a higher segmentation accuracy).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 222, "end_pos": 230, "type": "METRIC", "confidence": 0.8636721968650818}]}, {"text": "It also provides an efficient graph based implementation of which we take advantage.", "labels": [], "entities": []}, {"text": "To account both for cohesion and disruption, we extend the formalism of Isahara and Utiyama using a Markovian assumption between segments in place of the independence assumption of the original algorithm.", "labels": [], "entities": []}, {"text": "Keeping unchanged their probabilistic measure of lexical cohesion, the Markovian assumption enables to introduce the disruption between two consecutive segments.", "labels": [], "entities": []}, {"text": "We propose an extended graph based decoding strategy, which is both optimal and efficient, exploiting the notion of generalized segment model or semi hidden Markov models.", "labels": [], "entities": []}, {"text": "Tests are performed on standard textual data sets and on a more challenging corpus of automatically transcribed broadcast news shows.", "labels": [], "entities": []}, {"text": "The seminal idea of this paper was partially published in) in the French language.", "labels": [], "entities": []}, {"text": "The current paper significantly elaborates on the latter, with a more detailed description of the algorithm and additional contrastive experiments including more data sets.", "labels": [], "entities": []}, {"text": "In particular, new experiments clearly demonstrate the benefit of the method in a realistic setting with statistically significant gains.", "labels": [], "entities": []}, {"text": "The organization of the article is as follows.", "labels": [], "entities": []}, {"text": "Existing work on topic segmentation is presented in Section 2, emphasizing the motivations of the model we propose.", "labels": [], "entities": [{"text": "topic segmentation", "start_pos": 17, "end_pos": 35, "type": "TASK", "confidence": 0.7277100533246994}]}, {"text": "Section 3 details the baseline method of Utiyama and Isahara before introducing our algorithm.", "labels": [], "entities": []}, {"text": "Experimental protocol and results are given in Section 4.", "labels": [], "entities": []}, {"text": "Section 5 summarizes the finding and concludes with a discussion of future work.", "labels": [], "entities": []}], "datasetContent": [{"text": "Experiments are performed on three distinct corpora which exhibit different characteristics, two containing textual data and one spoken data.", "labels": [], "entities": []}, {"text": "We first present the corpora before presenting and discussing results on each.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Number of documents in Choi's corpus (Choi,  2000).", "labels": [], "entities": [{"text": "Choi's corpus (Choi,  2000)", "start_pos": 33, "end_pos": 60, "type": "DATASET", "confidence": 0.8973027765750885}]}]}