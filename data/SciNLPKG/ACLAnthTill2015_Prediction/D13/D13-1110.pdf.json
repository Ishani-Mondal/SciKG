{"title": [{"text": "Efficient Left-to-Right Hierarchical Phrase-based Translation with Improved Reordering", "labels": [], "entities": [{"text": "Hierarchical Phrase-based Translation", "start_pos": 24, "end_pos": 61, "type": "TASK", "confidence": 0.6205528477827708}]}], "abstractContent": [{"text": "Left-to-right (LR) decoding (Watanabe et al., 2006b) is a promising decoding algorithm for hierarchical phrase-based translation (Hiero).", "labels": [], "entities": [{"text": "hierarchical phrase-based translation", "start_pos": 91, "end_pos": 128, "type": "TASK", "confidence": 0.6038316686948141}, {"text": "Hiero)", "start_pos": 130, "end_pos": 136, "type": "DATASET", "confidence": 0.8537239730358124}]}, {"text": "It generates the target sentence by extending the hypotheses only on the right edge.", "labels": [], "entities": []}, {"text": "LR decoding has complexity O(n 2 b) for input of n words and beam size b, compared to O(n 3) for the CKY algorithm.", "labels": [], "entities": [{"text": "O", "start_pos": 27, "end_pos": 28, "type": "METRIC", "confidence": 0.6833708882331848}, {"text": "O", "start_pos": 86, "end_pos": 87, "type": "METRIC", "confidence": 0.9837895035743713}]}, {"text": "It requires a single language model (LM) history for each target hypothesis rather than two LM histories per hypothesis as in CKY.", "labels": [], "entities": []}, {"text": "In this paper we present an augmented LR decoding algorithm that builds on the original algorithm in (Watanabe et al., 2006b).", "labels": [], "entities": []}, {"text": "Unlike that algorithm, using experiments over multiple language pairs we show two new results: our LR decoding algorithm provides demonstrably more efficient decoding than CKY Hiero, four times faster; and by introducing new distortion and reordering features for LR decoding, it maintains the same translation quality (as in BLEU scores) obtained phrase-based and CKY Hiero with the same translation model.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 326, "end_pos": 330, "type": "METRIC", "confidence": 0.9963764548301697}]}], "introductionContent": [{"text": "Hiero) models translation using a lexicalized synchronous context-free grammar (SCFG) extracted from word aligned bitexts.", "labels": [], "entities": [{"text": "Hiero)", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.9339596629142761}]}, {"text": "Typically, CKY-style decoding is used for Hiero with time complexity O(n 3 ) for source input with n words.", "labels": [], "entities": [{"text": "Hiero", "start_pos": 42, "end_pos": 47, "type": "DATASET", "confidence": 0.7941399216651917}]}, {"text": "Scoring the target language output using a language model within CKY-style decoding requires two histories per hypothesis, one on the left edge of each span and one on the right, due to the fact that the target side is not generated in left to right order, but rather built bottom-up from sub-spans.", "labels": [], "entities": []}, {"text": "This leads to complex problems in efficient language model integration and requires state reduction techniques.", "labels": [], "entities": [{"text": "language model integration", "start_pos": 44, "end_pos": 70, "type": "TASK", "confidence": 0.6806112130482992}]}, {"text": "The size of a Hiero SCFG grammar is typically larger than phrase-based models extracted from the same data creating challenges in rule extraction and decoding time especially for larger datasets ).", "labels": [], "entities": [{"text": "Hiero SCFG grammar", "start_pos": 14, "end_pos": 32, "type": "DATASET", "confidence": 0.7089032729466757}, {"text": "rule extraction", "start_pos": 130, "end_pos": 145, "type": "TASK", "confidence": 0.7607724666595459}]}, {"text": "In contrast, the LR-decoding algorithm could avoid these shortcomings such as faster time complexity, reduction in the grammar size and the simplified left-to-right language model scoring.", "labels": [], "entities": []}, {"text": "It means LR decoding has the potential to replace CKY decoding for Hiero.", "labels": [], "entities": [{"text": "Hiero", "start_pos": 67, "end_pos": 72, "type": "DATASET", "confidence": 0.9325467944145203}]}, {"text": "Despite these attractive properties, we show that the original LR-Hiero decoding proposed by () does not perform to the same level of the standard CKY Hiero with cube pruning (see).", "labels": [], "entities": [{"text": "CKY Hiero", "start_pos": 147, "end_pos": 156, "type": "DATASET", "confidence": 0.9469753503799438}]}, {"text": "In addition, the current LR decoding algorithm does not obtain BLEU scores comparable to phrase-based or CKYbased Hiero models for different language pairs (see).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 63, "end_pos": 67, "type": "METRIC", "confidence": 0.9990098476409912}]}, {"text": "In this paper we propose modifications to the LR decoding algorithm that addresses these limitations and provides, for the first time, a true alternative to the standard CKY Hiero algorithm that uses left-to-right decoding.", "labels": [], "entities": [{"text": "CKY Hiero algorithm", "start_pos": 170, "end_pos": 189, "type": "DATASET", "confidence": 0.8790757060050964}]}, {"text": "We introduce anew extended version of the LR decoding algorithm presented in () which is demonstrably more efficient than the CKY Hiero algorithm.", "labels": [], "entities": [{"text": "CKY Hiero algorithm", "start_pos": 126, "end_pos": 145, "type": "DATASET", "confidence": 0.8638914624849955}]}, {"text": "We measure the efficiency of the LR Hiero decoder in away that is independent of the choice of system and programming language by measuring the number of language model queries.", "labels": [], "entities": [{"text": "LR Hiero decoder", "start_pos": 33, "end_pos": 49, "type": "DATASET", "confidence": 0.8130422035853068}]}, {"text": "Although more efficient, the new LR decoding algorithm suffered from lower BLEU scores compared to CKY Hiero.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 75, "end_pos": 79, "type": "METRIC", "confidence": 0.9996953010559082}, {"text": "CKY Hiero", "start_pos": 99, "end_pos": 108, "type": "DATASET", "confidence": 0.8753353953361511}]}, {"text": "Our analysis of left to right decoding showed that it has more potential for search errors due to early pruning of good hypotheses.", "labels": [], "entities": []}, {"text": "This is unlike bottom-up decoding (CKY) which keeps best hypotheses for each span.", "labels": [], "entities": []}, {"text": "To address this issue, we introduce two novel features into the Hiero SMT model that deal with reordering and distortion.", "labels": [], "entities": [{"text": "Hiero SMT", "start_pos": 64, "end_pos": 73, "type": "TASK", "confidence": 0.6712611615657806}]}, {"text": "Our experiments show that LR decoding with these features using prefix lexi-calized target side rules equals the scores obtained by CKY decoding with prefix lexicalized target side rules and phrase-based translation system.", "labels": [], "entities": [{"text": "phrase-based translation", "start_pos": 191, "end_pos": 215, "type": "TASK", "confidence": 0.6731970310211182}]}, {"text": "It performs four times fewer language model queries on average, compare to CKY Hiero decoding with unrestricted Hiero rules: 6466.7 LM queries for CKY Hiero (with cube pruning) compared to 1500.45 LM queries in LR Hiero (with cube pruning).", "labels": [], "entities": [{"text": "CKY Hiero", "start_pos": 147, "end_pos": 156, "type": "DATASET", "confidence": 0.8760227262973785}]}, {"text": "While translation quality suffers by only about 0.67 in BLEU score on average, across two different language pairs.", "labels": [], "entities": [{"text": "translation", "start_pos": 6, "end_pos": 17, "type": "TASK", "confidence": 0.9628871083259583}, {"text": "BLEU score", "start_pos": 56, "end_pos": 66, "type": "METRIC", "confidence": 0.9776257276535034}]}], "datasetContent": [{"text": "We conduct different types of experiments to evaluate LR-Hiero decoding developed by cube pruning and integrating new features into LR-Hiero system for two language pairs: German-English (de-en) and Czech-English (cs-en).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Corpus statistics in number of sentences", "labels": [], "entities": []}, {"text": " Table 2: Model sizes (millions of rules). We do not count  glue rules for LR-Hiero which are created at runtime as  needed.", "labels": [], "entities": []}, {"text": " Table 4: BLEU scores. The rows are grouped such that  each group use the same model. The last row in part 2 of  table shows LR-Hiero+CP using our new features in ad- dition to the baseline Watanabe features (line LR-Hiero  baseline). The last part shows CKY Hiero using new re- ordering features. The reordering features used are d p , d g  and r . LR-Hiero+CP has a beam size of 500 while LR- Hiero has a beam size of 1000, c.f. with the LM calls  shown in Table 3.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9988574981689453}]}]}