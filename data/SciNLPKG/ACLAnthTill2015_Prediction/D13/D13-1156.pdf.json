{"title": [{"text": "Fast Joint Compression and Summarization via Graph Cuts", "labels": [], "entities": [{"text": "Summarization", "start_pos": 27, "end_pos": 40, "type": "TASK", "confidence": 0.915668249130249}, {"text": "Graph Cuts", "start_pos": 45, "end_pos": 55, "type": "TASK", "confidence": 0.574165090918541}]}], "abstractContent": [{"text": "Extractive summarization typically uses sentences as summarization units.", "labels": [], "entities": [{"text": "Extractive summarization", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.9138015806674957}]}, {"text": "In contrast, joint compression and summarization can use smaller units such as words and phrases, resulting in summaries containing more information.", "labels": [], "entities": [{"text": "joint compression", "start_pos": 13, "end_pos": 30, "type": "TASK", "confidence": 0.7597878873348236}, {"text": "summarization", "start_pos": 35, "end_pos": 48, "type": "TASK", "confidence": 0.9816584587097168}]}, {"text": "The goal of compressive summariza-tion is to find a subset of words that maximize the total score of concepts and cutting dependency arcs under the grammar constraints and summary length constraint.", "labels": [], "entities": []}, {"text": "We propose an efficient decoding algorithm for fast compressive summarization using graph cuts.", "labels": [], "entities": [{"text": "summarization", "start_pos": 64, "end_pos": 77, "type": "TASK", "confidence": 0.8286117911338806}]}, {"text": "Our approach first relaxes the length constraint using Lagrangian relaxation.", "labels": [], "entities": [{"text": "length", "start_pos": 31, "end_pos": 37, "type": "METRIC", "confidence": 0.9711962938308716}]}, {"text": "Then we propose to bound the relaxed objective function by the supermodular binary quadratic programming problem, which can be solved efficiently using graph max-flow/min-cut.", "labels": [], "entities": []}, {"text": "Since finding the tightest lower bound suffers from local optimality, we use convex relaxation for initialization.", "labels": [], "entities": []}, {"text": "Experimental results on TAC2008 dataset demonstrate our method achieves competitive ROUGE score and has good readability, while is much faster than the integer linear programming (ILP) method.", "labels": [], "entities": [{"text": "TAC2008 dataset", "start_pos": 24, "end_pos": 39, "type": "DATASET", "confidence": 0.9459048211574554}, {"text": "ROUGE score", "start_pos": 84, "end_pos": 95, "type": "METRIC", "confidence": 0.9828038811683655}]}], "introductionContent": [{"text": "Automatic multi-document summarization helps readers get the most important information from large amounts of texts.", "labels": [], "entities": [{"text": "multi-document summarization", "start_pos": 10, "end_pos": 38, "type": "TASK", "confidence": 0.6511400043964386}]}, {"text": "Summarization techniques can be roughly divided into two categories: extractive and abstractive.", "labels": [], "entities": [{"text": "Summarization", "start_pos": 0, "end_pos": 13, "type": "TASK", "confidence": 0.9659241437911987}]}, {"text": "Extractive summarization casts the summarization task as a sentence selection problem: identifying important summary sentences from one or multiple documents.", "labels": [], "entities": [{"text": "Extractive summarization", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.8700556755065918}, {"text": "summarization task", "start_pos": 35, "end_pos": 53, "type": "TASK", "confidence": 0.894023984670639}]}, {"text": "Many methods have been developed in the past decades, including supervised approaches that use classifiers to predict summary sentences, graph based approaches to rank the sentences, and recent global optimization methods such as integer linear programming () (ILP) and submodular maximization methods ().", "labels": [], "entities": []}, {"text": "Though extractive summarization is popular because of its simplicity and high readability, it has limitations in that it selects each sentence as a whole, and thus may miss informative partial sentences.", "labels": [], "entities": [{"text": "extractive summarization", "start_pos": 7, "end_pos": 31, "type": "TASK", "confidence": 0.7363711297512054}]}, {"text": "To improve the informativeness, joint compression and summarization was proposed), which uses words as summarization units, unlike extractive summarization where each sentence is a basic undecomposable unit.", "labels": [], "entities": [{"text": "joint compression", "start_pos": 32, "end_pos": 49, "type": "TASK", "confidence": 0.7000162303447723}, {"text": "summarization", "start_pos": 54, "end_pos": 67, "type": "TASK", "confidence": 0.9712893962860107}]}, {"text": "To achieve better readability, manually defined grammar constraints or automatically learned models based on syntax trees are added during the summarization process.", "labels": [], "entities": []}, {"text": "Up to now, the state of the art compressive systems are based on integer linear programming (ILP).", "labels": [], "entities": []}, {"text": "Because ILP suffers from exponential complexity, word-based compression summarization is an order of magnitude slower than sentence-based extraction.", "labels": [], "entities": [{"text": "word-based compression summarization", "start_pos": 49, "end_pos": 85, "type": "TASK", "confidence": 0.8163053592046102}, {"text": "sentence-based extraction", "start_pos": 123, "end_pos": 148, "type": "TASK", "confidence": 0.7174824327230453}]}, {"text": "One common way to solve an ILP problem is to use its LP relaxation and round the results.", "labels": [], "entities": [{"text": "LP relaxation", "start_pos": 53, "end_pos": 66, "type": "METRIC", "confidence": 0.9763727188110352}]}, {"text": "However found that LP relaxation gave poor results, finding unacceptably suboptimal solutions.", "labels": [], "entities": []}, {"text": "For speedup, they proposed a two stage method where they performed some sentence selection in the first step to reduce the number of candidates.", "labels": [], "entities": [{"text": "sentence selection", "start_pos": 72, "end_pos": 90, "type": "TASK", "confidence": 0.6954094618558884}]}, {"text": "Despite their empirical success, such a pruning approach has its inherent problem in that it may eliminate correct sentences in the first step.", "labels": [], "entities": []}, {"text": "Recently, proposed a fast joint decoding algorithm based on dual decomposition.", "labels": [], "entities": []}, {"text": "For fast convergence, they added quadratic penalty terms to alleviate the learning rate problem.", "labels": [], "entities": []}, {"text": "In this paper, we propose an efficient decoding algorithm for fast ILP based compressive summarization using graph cuts.", "labels": [], "entities": [{"text": "ILP based compressive summarization", "start_pos": 67, "end_pos": 102, "type": "TASK", "confidence": 0.6464421600103378}]}, {"text": "Our assumption is that all concepts are word n-grams and non-negatively scored.", "labels": [], "entities": []}, {"text": "The rationale for the non-negativity assumption is straightforward: the score of a concept reflects its informativeness, hence should be non-negative.", "labels": [], "entities": []}, {"text": "Given a set of documents, each word is associated with a binary variable, indicating whether the word is selected in the summary.", "labels": [], "entities": []}, {"text": "Our idea is to approximate the ILP as a binary quadratic programming problem where coefficients of all quadratic terms are nonnegative.", "labels": [], "entities": []}, {"text": "It is well known that such binary quadratic function is supermodular, and its maximum can be solved efficiently using graph max-flow/min-cut.", "labels": [], "entities": []}, {"text": "Hence the key is to find the coefficients of the supermodular binary quadratic function (SBQF) so that its maximum is close to the optimal ILP objective function.", "labels": [], "entities": []}, {"text": "Our solution consists of 3 steps.", "labels": [], "entities": []}, {"text": "First, we show that the subtree deletion model and grammar constraints can be eliminated by adding SBQFs to the objective function.", "labels": [], "entities": []}, {"text": "Second, we relax the summary length constraint using Lagrangian relaxation.", "labels": [], "entities": [{"text": "summary length constraint", "start_pos": 21, "end_pos": 46, "type": "METRIC", "confidence": 0.6620491743087769}]}, {"text": "Third, we propose a family of SBQFs that are lower bounds of the ILP objective function.", "labels": [], "entities": []}, {"text": "Since finding the tightest lower bound suffers from local optimality, we choose to use convex relaxation for initialization.", "labels": [], "entities": []}, {"text": "To demonstrate our technique, we conduct experiments on Text Analysis Conference (TAC) datasets using the same train/test splits as previous work).", "labels": [], "entities": [{"text": "Text Analysis Conference (TAC)", "start_pos": 56, "end_pos": 86, "type": "TASK", "confidence": 0.8031351268291473}]}, {"text": "We compare our approach with the state-of-the-art ILP based approach in terms of summary quality (ROUGE scores and sentence quality) and speed.", "labels": [], "entities": [{"text": "ROUGE scores", "start_pos": 98, "end_pos": 110, "type": "METRIC", "confidence": 0.9398351609706879}, {"text": "speed", "start_pos": 137, "end_pos": 142, "type": "METRIC", "confidence": 0.99310302734375}]}, {"text": "Experimental results show that our proposed method achieves competitive performance with ILP, while about 100 times faster.", "labels": [], "entities": []}], "datasetContent": [{"text": "Due to the lack of training data for compressive summarization, we learn the subtree deletion model and the concept model separately.", "labels": [], "entities": [{"text": "compressive summarization", "start_pos": 37, "end_pos": 62, "type": "TASK", "confidence": 0.5322536826133728}]}, {"text": "Specifically, the sentence compression dataset) (referred as CL08) is used for subtree deletion model training (\u03b8 arc ).", "labels": [], "entities": [{"text": "subtree deletion model training", "start_pos": 79, "end_pos": 110, "type": "TASK", "confidence": 0.748799629509449}]}, {"text": "A sentence pair in the corpus is kept for training the subtree deletion model if the compressed sentence can be derived by deleting subtrees from the parse tree of the original sentence.", "labels": [], "entities": []}, {"text": "There are 3, 178 out of 5, 739 such pairs.", "labels": [], "entities": []}, {"text": "The concept model (\u03b8 concept ) is learned from the TAC2009 dataset.", "labels": [], "entities": [{"text": "TAC2009 dataset", "start_pos": 51, "end_pos": 66, "type": "DATASET", "confidence": 0.9745581448078156}]}, {"text": "We create the oracle extractive summaries with the maximal bigram recall as the reference summary.", "labels": [], "entities": [{"text": "maximal bigram recall", "start_pos": 51, "end_pos": 72, "type": "METRIC", "confidence": 0.7429988582928976}]}, {"text": "TAC2010 data is used as has the descriptions of all the data used.", "labels": [], "entities": [{"text": "TAC2010 data", "start_pos": 0, "end_pos": 12, "type": "DATASET", "confidence": 0.9349128305912018}]}, {"text": "We choose averaged perceptron for fast training.", "labels": [], "entities": []}, {"text": "The number of iterations is tuned on the development data.", "labels": [], "entities": []}, {"text": "Remind that our algorithm is based on the assumption that scores of concepts are non-negative, \u2200j, w j \u2265 0.", "labels": [], "entities": []}, {"text": "We assume that feature vector f concept is non-negative (e.g., term frequency, n-gram features), then \u03b8 concept \u2265 0 is required to guarantee the non-negativity of w j . Therefore, we project \u03b8 concept onto the non-negative space after each iteration.", "labels": [], "entities": []}, {"text": "Since training is offline, we use ILP based exact inference for accurate learning.", "labels": [], "entities": []}, {"text": "To control the contributions of the concept model and the subtree deletion model, we introduce a parameter \u00b5, and modify the original maximization problem (Problem 2) to: We tune \u00b5 on TAC2010 dataset.", "labels": [], "entities": [{"text": "TAC2010 dataset", "start_pos": 184, "end_pos": 199, "type": "DATASET", "confidence": 0.9613757431507111}]}, {"text": "For max-flow/mincut, in our experiments, we implemented the improved shortest augmented path (SAP) method.", "labels": [], "entities": []}, {"text": "For performance measure of the summaries, we use both ROUGE and linguistic quality.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 54, "end_pos": 59, "type": "METRIC", "confidence": 0.9952634572982788}]}, {"text": "ROUGE has been widely used for summarization performance and can measure the informativeness of the summaries (content match between system and reference summaries).", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 0, "end_pos": 5, "type": "METRIC", "confidence": 0.9217820763587952}, {"text": "summarization", "start_pos": 31, "end_pos": 44, "type": "TASK", "confidence": 0.9920544624328613}]}, {"text": "Since joint compression and summarization tends to pick isolated words to maximize the information coverage in the system generated summaries, it may have poor readability.", "labels": [], "entities": [{"text": "joint compression", "start_pos": 6, "end_pos": 23, "type": "TASK", "confidence": 0.7022147476673126}, {"text": "summarization", "start_pos": 28, "end_pos": 41, "type": "TASK", "confidence": 0.9806488156318665}]}, {"text": "Therefore we conduct human evaluation for the linguis-tic quality for various systems.", "labels": [], "entities": []}, {"text": "The linguistic quality consists of two parts.", "labels": [], "entities": []}, {"text": "One evaluates the grammar quality within a sentence.", "labels": [], "entities": []}, {"text": "Annotators marked if a compressed sentence is grammatically correct.", "labels": [], "entities": []}, {"text": "Typical grammar errors include lack of verb or subordinate clause.", "labels": [], "entities": []}, {"text": "The other evaluates the coherence between sentences, including the order of sentences and irrelevant sentences.", "labels": [], "entities": []}, {"text": "For compressive summaries, we removed the sentences with grammar errors when evaluating coherence.", "labels": [], "entities": []}, {"text": "The overall linguistic quality score is the combined score of the percentage of grammatically correct sentences and the correct ordering of the summary sentences.", "labels": [], "entities": []}, {"text": "The score is scaled and ranges from 1 (bad) to 10 (good).", "labels": [], "entities": []}, {"text": "We also list the performance of some state-of-theart systems, including the two ICSI systems (, the compressive summarization system of, the multi-aspect ILP system of Woodsend and Lapata (2012)(WL'12) and the dual decomposition based system (Almeida and Martins, 2013) (AM'13).", "labels": [], "entities": [{"text": "Woodsend and Lapata (2012)(WL'12", "start_pos": 168, "end_pos": 200, "type": "DATASET", "confidence": 0.7921059642519269}, {"text": "AM'13", "start_pos": 271, "end_pos": 276, "type": "DATASET", "confidence": 0.9571846127510071}]}, {"text": "Note that for these referred systems since the linguistic quality results are not comparable due to different judgment methods.", "labels": [], "entities": []}, {"text": "For our graph-cut based method, to study the tradeoff between the readability of the summary and the ROUGE scores, we present two versions for this method: one uses all the constraints (C0-C3), the other does not use C0.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 101, "end_pos": 106, "type": "METRIC", "confidence": 0.9915738701820374}]}, {"text": "We can see that our proposed method balanced speed and quality.", "labels": [], "entities": [{"text": "speed", "start_pos": 45, "end_pos": 50, "type": "METRIC", "confidence": 0.9812984466552734}]}, {"text": "Compared with ILP, we achieved competitive ROUGE scores, but with about 100x speedup.", "labels": [], "entities": [{"text": "ILP", "start_pos": 14, "end_pos": 17, "type": "DATASET", "confidence": 0.8273998498916626}, {"text": "ROUGE", "start_pos": 43, "end_pos": 48, "type": "METRIC", "confidence": 0.9944908618927002}]}, {"text": "Our method is also faster than the 2-step ILP system.", "labels": [], "entities": []}, {"text": "We also tried another state-of-the-art LP solver, Gurobi version 5.5 3 , it achieves 0.17 seconds per topic, much faster than GLPK, but stil-  l slower than ours.", "labels": [], "entities": [{"text": "solver", "start_pos": 42, "end_pos": 48, "type": "TASK", "confidence": 0.6313912272453308}, {"text": "GLPK", "start_pos": 126, "end_pos": 130, "type": "DATASET", "confidence": 0.7964968085289001}]}, {"text": "Regarding the grammar constraints used in our system, from the two results for our graph-cut based method, we can see that adding constraint C0 significantly decreases the R-2 score but improves the language quality.", "labels": [], "entities": []}, {"text": "This shows that word-based joint compression and summarization can improve ROUGE score; however, we need to keep in mind about linguistic quality and find a tradeoff between the ROUGE score and the linguistic quality.", "labels": [], "entities": [{"text": "word-based joint compression", "start_pos": 16, "end_pos": 44, "type": "TASK", "confidence": 0.6166068613529205}, {"text": "summarization", "start_pos": 49, "end_pos": 62, "type": "TASK", "confidence": 0.9502658843994141}, {"text": "ROUGE score", "start_pos": 75, "end_pos": 86, "type": "METRIC", "confidence": 0.9460930526256561}, {"text": "ROUGE score", "start_pos": 178, "end_pos": 189, "type": "METRIC", "confidence": 0.939298003911972}]}, {"text": "trained their model on extra corpora using multi-task learning, and achieved better results than ours.", "labels": [], "entities": []}, {"text": "The results of our system and theirs showed that Lagrangian relaxation based method combined with combinatorial optimization algorithms such as dynamic programming or minimum cut can exploit the inner structure of problems and achieve significant speedup over ILP.", "labels": [], "entities": []}, {"text": "Four example summaries produced by our system are shown below.", "labels": [], "entities": []}, {"text": "Words in gray are not selected in the summary.", "labels": [], "entities": []}, {"text": "India's space agency is ready to send a man to space within seven years if the government gives the nod, while preparations have lready begun for the launch of an unmanned lunar mission, atop official said.", "labels": [], "entities": []}, {"text": "India will launch more missions to the moon if its maiden unmanned spacecraft Chandrayaan-1, slated to be launched by 2008, is successful atop space fficial said Tuesday.", "labels": [], "entities": [{"text": "Chandrayaan-1", "start_pos": 78, "end_pos": 91, "type": "DATASET", "confidence": 0.9028643369674683}]}, {"text": "The United States, the European Space Agency, China, Japan and India are all planning lunar missions during the ext decade.India is \"a step ahead\" of China in satellite technology and can surpass Beijing in space research by tapping the talent of its huge pool of young scientists, India's space research chief said Monday.", "labels": [], "entities": []}, {"text": "The space agencies of India and France signed an agreement on Friday to cooperate in launching a satellite in four years that will help make climate predictions more accurate.", "labels": [], "entities": []}, {"text": "The Indian Space Research Organization (ISRO) has short-listed experiments from five nations including the United States, Britain and Germany, fora slot on India's unmanned moon mission Chandrayaan-1 to be undertaken by 2006-2007, the Press Trust of India (PTI) reported Monday.", "labels": [], "entities": [{"text": "Press Trust of India (PTI) reported Monday", "start_pos": 235, "end_pos": 277, "type": "DATASET", "confidence": 0.718294706609514}]}, {"text": "A three-member Afghan delegation is in Bangalore seeking help to setup a high-tech telemedicine facility in 10 Afghan cities linked via Indian satellites, Indo-Asian News Service reported Saturday.", "labels": [], "entities": [{"text": "Indo-Asian News Service reported", "start_pos": 155, "end_pos": 187, "type": "DATASET", "confidence": 0.8294680640101433}]}, {"text": "A woman was killed in Mississippi when a tree crashed on her car, becoming the 11th fatality blamed on the powerful Hurricane Katrina that slammed the US Gulf coast after pounding Florida, local TV reported Monday.", "labels": [], "entities": []}, {"text": "The bill for the Hurricane Katrina disaster effort has so far reached 2.87 billion dollars, federal officials said on Tuesday.", "labels": [], "entities": []}, {"text": "The official death toll from Hurricane Katrina has risen to 118 people in and around the swamped city of New Orleans, officials said Thursday.", "labels": [], "entities": []}, {"text": "The Foreign Ministry on Friday reported the first confirmed death of a Guatemalan due to Hurricane Katrina in the United States.", "labels": [], "entities": []}, {"text": "The Ugandan government has pledged 200,000 US dollars toward relief and rebuilding efforts in the aftermath of Hurricane Katrina, local press reported on Friday.", "labels": [], "entities": []}, {"text": "Swiss Reinsurance Co., the world's second largest reinsurance company on Monday doubled to 40 billion US dollars its initial estimate of the global insured losses caused by Hurricane Katrina in the United States.", "labels": [], "entities": [{"text": "Swiss Reinsurance Co.", "start_pos": 0, "end_pos": 21, "type": "DATASET", "confidence": 0.9567447702089945}]}, {"text": "The A380 'superjumbo', which will be presented to the world in a lavish ceremony in southern France on Tuesday, will be profitable from 2008, its maker Airbus told the French financial newspaper La Tribune.", "labels": [], "entities": []}, {"text": "The A380 will takeover from the Boeing 747 as the biggest jet in the skies.", "labels": [], "entities": []}, {"text": "An association of residents living near Paris's Charles-de-Gaulle airport on Wednesday denounced the noise pollution generated by the giant Airbus A380, after the new airliner's maiden flight.", "labels": [], "entities": []}, {"text": "One problem that Airbus is encountering with its new A380 is that the craft pushes the envelope on the maximum size of a commercial airplane.", "labels": [], "entities": []}, {"text": "With a whisper more than a roar, the largest passenger airliner ever built, the Airbus 380, took off on its maiden flight Wednesday.", "labels": [], "entities": []}, {"text": "\"When she came in, she was in good spirits,\" a prison staffer told the New York Daily News.", "labels": [], "entities": [{"text": "New York Daily News", "start_pos": 71, "end_pos": 90, "type": "DATASET", "confidence": 0.8047819137573242}]}, {"text": "Martha Stewart, the American celebrity homemaker who had her own cooking and home improvement TV show, reported to a federal prison in Alderson, West Virginia, on Friday to serve a five-month sentence for lying about a stock sale.", "labels": [], "entities": []}, {"text": "Home fashion guru Martha Stewart said on Friday that she has adjusted to prison life and is keeping busy behind bars since reporting a week ago to a federal penal camp in West Virginia, where she is serving a five-month sentence for lying about a stock sale.", "labels": [], "entities": []}, {"text": "The lawyer said he did not know what she is writing, but Stewart has suggested since her conviction that she might write a book about her recent experience with the legal system.", "labels": [], "entities": []}, {"text": "Walter Dellinger, the lawyer leading the appeal, said on NBC's \"Today\" that Stewart is exploring \"innovative ways to do microwave cooking\" The lawyer said he did not know with her fellow inmates.", "labels": [], "entities": [{"text": "NBC's \"Today\"", "start_pos": 57, "end_pos": 70, "type": "DATASET", "confidence": 0.7268243908882142}]}, {"text": "As Martha Stewart arrives at the red-brick federal prison in Alderson, W. Va., on Friday to begin a five-month sentence, the company she founded is focused both on life without her and on life once she returns.", "labels": [], "entities": []}, {"text": "In most cases, the removed phrases do not hurt the readability of the summaries.", "labels": [], "entities": []}, {"text": "The errors are mainly caused by the break of sub-clauses or main clauses that are separated by commas, for example, the fourth sentence in the last summary, The lawyer said he did not know what she is writing.", "labels": [], "entities": []}, {"text": "The compressed sentence is grammatically correct, but semantically incomplete.", "labels": [], "entities": []}, {"text": "Other errors are due to the lack of verb, subject, or object, or incorrect removal of PP, such as the last sentence of the last summary.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Corpus statistics. Training data consist of  two parts, TAC2009 for learning the concept mod- el, CL08 (Clarke and Lapata, 2008) for learning the  subtree deletion model.", "labels": [], "entities": [{"text": "TAC2009", "start_pos": 66, "end_pos": 73, "type": "METRIC", "confidence": 0.9357799887657166}]}, {"text": " Table 2: Experimental results on developmen- t dataset. R-2 and LQ are short for ROUGE-2 score  multiplied by 100, and linguistic quality respective- ly.", "labels": [], "entities": [{"text": "developmen- t dataset", "start_pos": 34, "end_pos": 55, "type": "DATASET", "confidence": 0.5966412723064423}, {"text": "R-2", "start_pos": 57, "end_pos": 60, "type": "METRIC", "confidence": 0.9208998084068298}, {"text": "ROUGE-2 score  multiplied", "start_pos": 82, "end_pos": 107, "type": "METRIC", "confidence": 0.9567155440648397}]}, {"text": " Table 3: Experimental results on TAC2008 dataset.  Columns 2-5 are scores of ROUGE-2, ROUGE- SU4, linguistic quality, and speed (seconds per top- ic). ROUGE-2 and ROUGE-SU4 scores are multi- plied by 100. All the experiments are conducted on  the platform Intel Core i5-2500 CPU 3.30GHz.  \u2020  numbers are not directly comparable due to differ- ent annotations or platforms. + extra resources are  used.", "labels": [], "entities": [{"text": "TAC2008 dataset", "start_pos": 34, "end_pos": 49, "type": "DATASET", "confidence": 0.929842621088028}, {"text": "speed", "start_pos": 123, "end_pos": 128, "type": "METRIC", "confidence": 0.9907454252243042}]}]}