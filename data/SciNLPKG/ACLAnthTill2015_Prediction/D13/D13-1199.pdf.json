{"title": [{"text": "Identifying Manipulated Offerings on Review Portals", "labels": [], "entities": [{"text": "Identifying Manipulated Offerings on Review Portals", "start_pos": 0, "end_pos": 51, "type": "TASK", "confidence": 0.8514645397663116}]}], "abstractContent": [{"text": "Recent work has developed supervised methods for detecting deceptive opinion spam-fake reviews written to sound authentic and deliberately mislead readers.", "labels": [], "entities": [{"text": "detecting deceptive opinion spam-fake reviews written", "start_pos": 49, "end_pos": 102, "type": "TASK", "confidence": 0.7915944457054138}]}, {"text": "And whereas past work has focused on identifying individual fake reviews, this paper aims to identify offerings (e.g., hotels) that contain fake reviews.", "labels": [], "entities": []}, {"text": "We introduce a semi-supervised man-ifold ranking algorithm for this task, which relies on a small set of labeled individual reviews for training.", "labels": [], "entities": []}, {"text": "Then, in the absence of gold standard labels (at an offering level), we introduce a novel evaluation procedure that ranks artificial instances of real offerings , where each artificial offering contains a known number of injected deceptive reviews.", "labels": [], "entities": []}, {"text": "Experiments on a novel dataset of hotel reviews show that the proposed method outper-forms state-of-art learning baselines.", "labels": [], "entities": []}], "introductionContent": [{"text": "Consumers increasingly rely on user-generated online reviews when making purchase decisions.", "labels": [], "entities": []}, {"text": "Unfortunately, the ease of posting content to the Web, potentially anonymously, combined with the public's trust and growing reliance on opinions and other information found online, create opportunities and incentives for unscrupulous businesses to post deceptive opinion spam-fraudulent or fictitious reviews that are deliberately written to sound authentic, in order to deceive the reader ().", "labels": [], "entities": []}, {"text": "Unlike other kinds of spam, such as Web () and e-mail spam (), recent work has found that deceptive opinion spam is neither easily ignored nor easily identified by human readers ().", "labels": [], "entities": []}, {"text": "Accordingly, there is growing interest in developing automatic, usually learning-based, methods to help users identify deceptive opinion spam (see Section 2).", "labels": [], "entities": []}, {"text": "Even in fully-supervised settings, however, automatic methods are imperfect at identifying individual deceptive reviews, and erroneously labeling genuine reviews as deceptive may frustrate and alienate honest reviewers.", "labels": [], "entities": []}, {"text": "An alternative approach, not yet considered in previous work, is to instead identify those product or service offerings where fake reviews appear with high probability.", "labels": [], "entities": []}, {"text": "For example, a hotel manager may post fake positive reviews to promote their own hotel, or fake negative reviews to demote a competitor's hotel.", "labels": [], "entities": []}, {"text": "In both cases, rather than identifying these deceptive reviews individually, it maybe preferable to identify the manipulated offering (i.e., the hotel) so that review portal operators, such as TripAdvisor or Yelp, can further investigate the situation without alienating users.", "labels": [], "entities": [{"text": "Yelp", "start_pos": 208, "end_pos": 212, "type": "DATASET", "confidence": 0.8306209444999695}]}, {"text": "Accordingly, this paper addresses the novel task of identifying manipulated offerings, which we frame as a ranking problem, where the goal is to rank offerings by the proportion of their reviews that are believed to be deceptive.", "labels": [], "entities": []}, {"text": "We propose a novel threelayer graph model, based on manifold ranking (), to jointly model deceptive language at the offering-, review-and term-level.", "labels": [], "entities": []}, {"text": "In particular, rather than treating reviews within the same offering as independent units, there is a reinforcing relationship between offerings and reviews.", "labels": [], "entities": []}, {"text": "Our manifold ranking approach is semisupervised in that it requires no supervisory information at the offering level; rather, it requires only a small amount of labeled data at a review level.", "labels": [], "entities": []}, {"text": "Intuitively, and as depicted in for hotel offerings, we represent hotels, reviews and terms as nodes in a graph, where each hotel is connected to its reviews, and each review, in turn, is connected to the terms used within it.", "labels": [], "entities": []}, {"text": "The influence of labeled data is propagated along the graph to unlabeled data, such that a hotel is considered more deceptive if it is heavily linked with other deceptive reviews, and a review, in turn, is more deceptive if it is generated by a deceptive hotel.", "labels": [], "entities": []}, {"text": "The success of our semi-supervised approach further depends on the ability to learn patterns of truthful and deceptive reviews that generalize across reviews of different offerings.", "labels": [], "entities": []}, {"text": "This is challenging, because reviews often contain offering-specific vocabulary.", "labels": [], "entities": []}, {"text": "For example, reviews of hotels in Los Angeles are more likely to include keywords such as \"beach\", \"sea\", \"sunshine\" or \"LA\", while reviews of Juneau hotels may contain \"glacier\", \"Juneau\", \"bear\" or \"aurora borealis.\"", "labels": [], "entities": []}, {"text": "A hotel review might also mention the hotel's restaurant or bar by name.", "labels": [], "entities": []}, {"text": "Unfortunately, it is unclear how important (or detrimental) offering-specific features are when deciding whether a review is fake.", "labels": [], "entities": []}, {"text": "Accordingly, we propose a dimensionality-reduction approach, based on Latent Dirichlet Allocation (LDA) (, to obtain a vector representation of reviews for the ranking algorithm that generalizes across reviews of different offerings.", "labels": [], "entities": [{"text": "Latent Dirichlet Allocation (LDA)", "start_pos": 70, "end_pos": 103, "type": "METRIC", "confidence": 0.9247239232063293}]}, {"text": "Specifically, we train an LDA-based topic model to view each review as a mixture of aspect-, city-, hotel-and review-specific topics (see Section 6).", "labels": [], "entities": []}, {"text": "We then reduce the dimensionality of our data (i.e., labeled and unlabeled reviews) by replacing each review term vector with a vector that corresponds to its term distribution over just its aspect-specific topics, i.e., excluding city-, hotel-and review-specific topics.", "labels": [], "entities": []}, {"text": "We find that, compared to models trained either on the full vocabulary, or trained on standard LDA document-topic vectors, this representation allows our models to generalize better across reviews of different offerings.", "labels": [], "entities": []}, {"text": "We evaluate our approach on the task of identifying (ranking) manipulated hotels.", "labels": [], "entities": []}, {"text": "In particular, in the absence of gold standard offering-level labels, we introduce a novel evaluation procedure for this task, in which we rank numerous versions of each hotel, where each hotel version contains a different number of injected, known deceptive reviews.", "labels": [], "entities": []}, {"text": "Thus, we expect hotel versions with larger proportions of deceptive reviews to be ranked higher than those with smaller proportions.", "labels": [], "entities": []}, {"text": "For labeled training data, we use the Ott et al.", "labels": [], "entities": []}, {"text": "(2011) dataset of 800 positive (5-star) reviews of 20 Chicago hotels (400 deceptive and 400 truthful).", "labels": [], "entities": []}, {"text": "For evaluation, we construct anew FOUR-CITIES dataset, containing 40 deceptive and 40 truthful reviews for each of eight hotels in four different cities (640 reviews total), following the procedure outlined in.", "labels": [], "entities": [{"text": "FOUR-CITIES", "start_pos": 34, "end_pos": 45, "type": "METRIC", "confidence": 0.9959143996238708}]}, {"text": "We find that our manifold ranking approach outperforms several state-of-theart learning baselines on this task, including transductive Support Vector Regression.", "labels": [], "entities": []}, {"text": "We additionally apply our approach to a large-scale collection of real-world reviews from TripAdvisor and explore the resulting ranking.", "labels": [], "entities": []}, {"text": "In the sections below, we discuss related work (Section 2) and describe the datasets used in this work (Section 3), the dimensionality-reduction approach for representing reviews (Section 4), and the semi-supervised manifold ranking approach (Section 5).", "labels": [], "entities": []}, {"text": "We then evaluate the methods quantitatively (Sections 6 and 7) and qualitatively (Section 8).", "labels": [], "entities": []}], "datasetContent": [{"text": "In this paper, we train all of our models using the CHICAGO dataset of Ott et al (2011), which contains 20 deceptive and 20 truthful reviews from each of 20 Chicago hotels (800 reviews total).", "labels": [], "entities": [{"text": "CHICAGO dataset of Ott et al (2011)", "start_pos": 52, "end_pos": 87, "type": "DATASET", "confidence": 0.9204214347733392}]}, {"text": "This dataset is Approaches for identifying individual fake reviews maybe applied to our task, for example, by averaging the review-level predictions for an offering.", "labels": [], "entities": [{"text": "Approaches", "start_pos": 16, "end_pos": 26, "type": "METRIC", "confidence": 0.9755187630653381}]}, {"text": "This averaging approach is one of our baselines in Section 7.", "labels": [], "entities": []}, {"text": "unique in that it contains known (gold standard) deceptive reviews, solicited through Amazon Mechanical Turk, and is publicly-available.", "labels": [], "entities": [{"text": "Amazon Mechanical Turk", "start_pos": 86, "end_pos": 108, "type": "DATASET", "confidence": 0.8640878001848856}]}, {"text": "Unfortunately, the CHICAGO dataset is limited, both in size (800 reviews) and scope, in that it only contains reviews of hotels in one city: Chicago.", "labels": [], "entities": [{"text": "CHICAGO dataset", "start_pos": 19, "end_pos": 34, "type": "DATASET", "confidence": 0.8461316525936127}]}, {"text": "Accordingly, in order to perform a more realistic evaluation for our task, we construct anew dataset, FOUR-CITIES, that contains 40 deceptive and 40 truthful reviews from each of eight hotels in four different cities (640 reviews total).", "labels": [], "entities": [{"text": "FOUR-CITIES", "start_pos": 102, "end_pos": 113, "type": "METRIC", "confidence": 0.9967954754829407}]}, {"text": "We build the FOUR-CITIES dataset using the same procedure as Ott et al (2011), by creating and dividing 320 Mechanical Turk jobs, called HumanIntelligence Tasks (HITs), evenly across eight of the most popular hotels in our four chosen cities (see Table 1).", "labels": [], "entities": [{"text": "FOUR-CITIES dataset", "start_pos": 13, "end_pos": 32, "type": "DATASET", "confidence": 0.8330771327018738}]}, {"text": "Each HIT presents a worker with the name of a hotel and a link to the hotel's website.", "labels": [], "entities": []}, {"text": "Workers are asked to imagine that they work for the marketing department of the hotel and that their boss has asked them to write a fake positive review, as if they were a customer, to be posted on a travel review website.", "labels": [], "entities": []}, {"text": "Each worker is allowed to submit a single review, and is paid $1 for an acceptable submission.", "labels": [], "entities": []}, {"text": "Finally, we augment our deceptive FOUR-CITIES reviews with a matching set of truthful reviews from TripAdvisor by randomly sampling 40 positive (5-star) reviews for each of the eight chosen hotels.", "labels": [], "entities": [{"text": "FOUR-CITIES", "start_pos": 34, "end_pos": 45, "type": "METRIC", "confidence": 0.9741670489311218}]}, {"text": "While we cannot know for sure that the sampled reviews are truthful, previous work has suggested that rates of deception among popular hotels is likely to below ().", "labels": [], "entities": []}, {"text": "We now present qualitative evaluations for the RLDA topic model and the manifold ranking model.", "labels": [], "entities": [{"text": "RLDA topic", "start_pos": 47, "end_pos": 57, "type": "TASK", "confidence": 0.547222912311554}]}, {"text": "gives the top words for four aspect topics and four city-specific topics in the RLDA topic model; gives the highest and lowest ranking term weights in our three-layer manifold model.", "labels": [], "entities": [{"text": "RLDA topic model", "start_pos": 80, "end_pos": 96, "type": "DATASET", "confidence": 0.7790109713872274}]}, {"text": "By comparing the first row of topics in, corresponding to aspect topics, to the top words in, we observe that the learned topics relate to truthful and deceptive classes.", "labels": [], "entities": []}, {"text": "For example, Topics 1 and 4 share many terms with the top truthful terms in the manifold model, e.g., spatial terms, such as location, floor and block, and punctuation, such as (, ), and $.", "labels": [], "entities": []}, {"text": "Similarly, Topics 2 and 7 share many terms with the top deceptive terms in the manifold model, e.g., hotel, husband, wife, amazing, experience and recommend.", "labels": [], "entities": []}, {"text": "This makes sense, since topic models have been shown to produce discriminative topics on   this data in previous work ().", "labels": [], "entities": []}, {"text": "With respect to the second row in, containing top words from city-specific topics, we observe that each topic does contain primarily cityspecific information.", "labels": [], "entities": []}, {"text": "This helps to explain why removing terms associated with these topics resulted in a better vector representation for reviews.", "labels": [], "entities": []}, {"text": "Finally, we apply our ranking model to a large-scale collection of realworld reviews from TripAdvisor.", "labels": [], "entities": [{"text": "TripAdvisor", "start_pos": 90, "end_pos": 101, "type": "DATASET", "confidence": 0.8682613372802734}]}, {"text": "We crawl 878,561 reviews from 3,945 hotels in 25 US cities from TripAdvisor excluding all non-5-star reviews and removing hotels with fewer than 100 reviews.", "labels": [], "entities": []}, {"text": "In the end, we collect 244,810 reviews from 838 hotels.", "labels": [], "entities": []}, {"text": "We apply our manifold ranking model and rank all 838 hotels.", "labels": [], "entities": []}, {"text": "First, we present a histogram of the resulting manifold ranking scores in.", "labels": [], "entities": []}, {"text": "We observe that the distribution reaches a peak around 0.04, which in our quantitative evaluation (Section 7) corresponded to a hotel with 34 truthful and 6 deceptive reviews.", "labels": [], "entities": []}, {"text": "These results suggest that the majority of reviews in TripAdvisor are truthful, inline with previous findings by.", "labels": [], "entities": [{"text": "TripAdvisor", "start_pos": 54, "end_pos": 65, "type": "DATASET", "confidence": 0.9653621315956116}]}, {"text": "Next, we note that previous work has hypothesized that deceptive reviews are more likely to be posted by first-time review writers, or singleton reviewers ().", "labels": [], "entities": []}, {"text": "Accordingly, if this hypothesis were valid, then manipulated hotels would have an above-average proportion of singleton reviews.", "labels": [], "entities": []}, {"text": "shows a histogram of the average proportion of singleton reviews, as a function of the ranking scores produced by our model.", "labels": [], "entities": []}, {"text": "Noting that lower scores correspond to a higher predicted proportion of deceptive reviews, we observe that hotels that are ranked as being more deceptive by our model have much higher proportions of singleton reviews, on average, compared to hotels ranked as less deceptive.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Binary classification results showing that n-gram features overfit to the CHICAGO training data. Results  correspond to evaluation on reviews for the two Chicago hotels from FOUR-CITIES and non-Chicago FOUR-CITIES  reviews (six hotels).", "labels": [], "entities": [{"text": "CHICAGO training data", "start_pos": 84, "end_pos": 105, "type": "DATASET", "confidence": 0.8609486023585001}, {"text": "FOUR-CITIES", "start_pos": 184, "end_pos": 195, "type": "METRIC", "confidence": 0.5968703627586365}]}]}