{"title": [{"text": "Elephant: Sequence Labeling for Word and Sentence Segmentation", "labels": [], "entities": [{"text": "Sequence Labeling", "start_pos": 10, "end_pos": 27, "type": "TASK", "confidence": 0.8745798766613007}, {"text": "Word and Sentence Segmentation", "start_pos": 32, "end_pos": 62, "type": "TASK", "confidence": 0.5795734599232674}]}], "abstractContent": [{"text": "Tokenization is widely regarded as a solved problem due to the high accuracy that rule-based tokenizers achieve.", "labels": [], "entities": [{"text": "Tokenization", "start_pos": 0, "end_pos": 12, "type": "TASK", "confidence": 0.919457733631134}, {"text": "accuracy", "start_pos": 68, "end_pos": 76, "type": "METRIC", "confidence": 0.9989519119262695}]}, {"text": "But rule-based tokenizers are hard to maintain and their rules language specific.", "labels": [], "entities": []}, {"text": "We show that high-accuracy word and sentence segmentation can be achieved by using supervised sequence labeling on the character level combined with unsupervised feature learning.", "labels": [], "entities": [{"text": "word and sentence segmentation", "start_pos": 27, "end_pos": 57, "type": "TASK", "confidence": 0.6176630854606628}]}, {"text": "We evaluated our method on three languages and obtained error rates of 0.27 \u2030 (English), 0.35 \u2030 (Dutch) and 0.76 \u2030 (Italian) for our best models .", "labels": [], "entities": [{"text": "error", "start_pos": 56, "end_pos": 61, "type": "METRIC", "confidence": 0.9909114241600037}]}], "introductionContent": [], "datasetContent": [{"text": "In our experiments we use three datasets to compare our method for different languages and for different domains: manually checked English newswire texts taken from the Groningen Meaning Bank, GMB (), Dutch newswire texts, comprising two days from January 2000 extracted from the Twente News Corpus,, and a random sample of Italian texts from the PAIS`APAIS` PAIS`A corpus ().", "labels": [], "entities": [{"text": "Groningen Meaning Bank", "start_pos": 169, "end_pos": 191, "type": "DATASET", "confidence": 0.6647130250930786}, {"text": "GMB", "start_pos": 193, "end_pos": 196, "type": "DATASET", "confidence": 0.5620484352111816}, {"text": "Twente News Corpus", "start_pos": 280, "end_pos": 298, "type": "DATASET", "confidence": 0.7771920561790466}, {"text": "PAIS`APAIS` PAIS`A corpus", "start_pos": 347, "end_pos": 372, "type": "DATASET", "confidence": 0.8926983028650284}]}, {"text": "The data was converted into IOB format by inferring an alignment between the raw text and the segmented text.", "labels": [], "entities": []}, {"text": "In order to evaluate the quality of the tokenization produced by our models we conducted several experiments with different combinations of features and context sizes.", "labels": [], "entities": []}, {"text": "For these tests, the models are trained on an 80% portion of the data sets and tested on a 10% development set.", "labels": [], "entities": []}, {"text": "Final results are obtained on a 10% test set.", "labels": [], "entities": []}, {"text": "We report both absolute number of errors and error rates per thousand (\u2030).", "labels": [], "entities": [{"text": "absolute number of errors", "start_pos": 15, "end_pos": 40, "type": "METRIC", "confidence": 0.9108756184577942}, {"text": "error rates", "start_pos": 45, "end_pos": 56, "type": "METRIC", "confidence": 0.9840347170829773}]}], "tableCaptions": [{"text": " Table 2: Error rates obtained with different feature sets.  Cat stands for Unicode category, Code for Unicode char- acter code, and Cat-Code for a union of these features.", "labels": [], "entities": []}, {"text": " Table 4.2 we show the results for symmetri- cal windows ranging in size from 1 to 13.", "labels": [], "entities": []}, {"text": " Table 3: Using different context window sizes.", "labels": [], "entities": []}, {"text": " Table 6: Confusion matrix for Dutch development set.", "labels": [], "entities": [{"text": "Dutch development set", "start_pos": 31, "end_pos": 52, "type": "DATASET", "confidence": 0.8558398683865865}]}]}