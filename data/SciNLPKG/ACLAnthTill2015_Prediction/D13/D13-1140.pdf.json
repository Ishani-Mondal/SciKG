{"title": [{"text": "Decoding with Large-Scale Neural Language Models Improves Translation", "labels": [], "entities": [{"text": "Translation", "start_pos": 58, "end_pos": 69, "type": "TASK", "confidence": 0.7799180150032043}]}], "abstractContent": [{"text": "We explore the application of neural language models to machine translation.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 56, "end_pos": 75, "type": "TASK", "confidence": 0.7820975184440613}]}, {"text": "We develop anew model that combines the neural proba-bilistic language model of Bengio et al., rectified linear units, and noise-contrastive estimation , and we incorporate it into a machine translation system both by reranking k-best lists and by direct integration into the decoder.", "labels": [], "entities": []}, {"text": "Our large-scale, large-vocabulary experiments across four language pairs show that our neu-ral language model improves translation quality by up to 1.1 Bleu.", "labels": [], "entities": [{"text": "Bleu", "start_pos": 152, "end_pos": 156, "type": "METRIC", "confidence": 0.982553243637085}]}], "introductionContent": [{"text": "Machine translation (MT) systems rely upon language models (LMs) during decoding to ensure fluent output in the target language.", "labels": [], "entities": [{"text": "Machine translation (MT)", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.8961633563041687}]}, {"text": "Typically, these LMs are n-gram models over discrete representations of words.", "labels": [], "entities": []}, {"text": "Such models are susceptible to data sparsity-that is, the probability of an n-gram observed only few times is difficult to estimate reliably, because these models do not use any information about similarities between words.", "labels": [], "entities": []}, {"text": "To address this issue, propose distributed word representations, in which each word is represented as a real-valued vector in a high-dimensional feature space.", "labels": [], "entities": []}, {"text": "introduce a feed-forward neural probabilistic LM (NPLM) that operates over these distributed representations.", "labels": [], "entities": []}, {"text": "During training, the NPLM learns both a distributed representation for each word in the vocabulary and an n-gram probability distribution over words in terms of these distributed representations.", "labels": [], "entities": []}, {"text": "Although neural LMs have begun to rival or even surpass traditional n-gram LMs (), they have not yet been widely adopted in large-vocabulary applications such as MT, because standard maximum likelihood estimation (MLE) requires repeated summations overall words in the vocabulary.", "labels": [], "entities": [{"text": "MT", "start_pos": 162, "end_pos": 164, "type": "TASK", "confidence": 0.9678424596786499}]}, {"text": "A variety of strategies have been proposed to combat this issue, many of which require severe restrictions on the size of the network or the size of the data.", "labels": [], "entities": []}, {"text": "In this work, we extend the NPLM of in two ways.", "labels": [], "entities": []}, {"text": "First, we use rectified linear units, whose activations are cheaper to compute than sigmoid or tanh units.", "labels": [], "entities": []}, {"text": "There is also evidence that deep neural networks with rectified linear units can be trained successfully without pre-training (.", "labels": [], "entities": []}, {"text": "Second, we train using noise-contrastive estimation or NCE (), which does not require repeated summations over the whole vocabulary.", "labels": [], "entities": [{"text": "NCE", "start_pos": 55, "end_pos": 58, "type": "METRIC", "confidence": 0.8811120390892029}]}, {"text": "This enables us to efficiently build NPLMs on a larger scale than would be possible otherwise.", "labels": [], "entities": []}, {"text": "We then apply this LM to MT in two ways.", "labels": [], "entities": [{"text": "MT", "start_pos": 25, "end_pos": 27, "type": "TASK", "confidence": 0.8660653829574585}]}, {"text": "First, we use it to rerank the k-best output of a hierarchical phrase-based decoder.", "labels": [], "entities": []}, {"text": "Second, we integrate it directly into the decoder, allowing the neural LM to more strongly influence the model.", "labels": [], "entities": []}, {"text": "We achieve gains of up to 0.6 Bleu translating French, German, and Spanish to English, and up to 1.1 Bleu on Chinese-English translation.: Neural probabilistic language model ().", "labels": [], "entities": [{"text": "Bleu", "start_pos": 30, "end_pos": 34, "type": "METRIC", "confidence": 0.9905559420585632}, {"text": "Bleu", "start_pos": 101, "end_pos": 105, "type": "METRIC", "confidence": 0.9862937927246094}]}], "datasetContent": [{"text": "We ran experiments on four language pairs -Chinese to English and French, German, and Spanish to English -using a hierarchical phrase-based MT system and GIZA++ for word alignments.", "labels": [], "entities": [{"text": "word alignments", "start_pos": 165, "end_pos": 180, "type": "TASK", "confidence": 0.7454642057418823}]}, {"text": "For all experiments, we used four LMs.", "labels": [], "entities": []}, {"text": "The baselines used conventional 5-gram LMs, estimated with modified Kneser-Ney smoothing) on the English side of the bitext and the 329M-word Xinhua portion of English Gigaword (LDC2011T07).", "labels": [], "entities": [{"text": "English Gigaword", "start_pos": 160, "end_pos": 176, "type": "DATASET", "confidence": 0.8457982242107391}]}, {"text": "Against these baselines, we tested systems that included the two conventional LMs as well as two 5-gram NPLMs trained on the same datasets.", "labels": [], "entities": []}, {"text": "The Europarl bitext NPLMs had a vocabulary size of 50k, while the other NPLMs had a vocabulary size of 100k.", "labels": [], "entities": [{"text": "Europarl bitext NPLMs", "start_pos": 4, "end_pos": 25, "type": "DATASET", "confidence": 0.9343441526095072}]}, {"text": "We used 150 dimensions for word embeddings, 750 units in hidden layer h 1 , and 150 units in hidden layer h 2 . We initialized the network parameters uniformly from (\u22120.01, 0.01) and the output biases to \u2212 log |V|, and optimized them by 10 epochs of stochastic gradient ascent, using minibatches of size 1000 and a learning rate of 1.", "labels": [], "entities": []}, {"text": "We drew 100 noise samples per training example from the unigram distribution, using the alias method for efficiency (.", "labels": [], "entities": []}, {"text": "We trained the discriminative models with MERT and the discriminative rerankers on 1000-best lists with MERT.", "labels": [], "entities": [{"text": "MERT", "start_pos": 42, "end_pos": 46, "type": "DATASET", "confidence": 0.7912495732307434}, {"text": "MERT", "start_pos": 104, "end_pos": 108, "type": "DATASET", "confidence": 0.9527940154075623}]}, {"text": "Except where noted, we ran MERT three times and report the average score.", "labels": [], "entities": [{"text": "MERT", "start_pos": 27, "end_pos": 31, "type": "METRIC", "confidence": 0.3630932569503784}, {"text": "average score", "start_pos": 59, "end_pos": 72, "type": "METRIC", "confidence": 0.9677017033100128}]}, {"text": "We evaluated using case-insensitive NIST Bleu.", "labels": [], "entities": [{"text": "NIST Bleu", "start_pos": 36, "end_pos": 45, "type": "DATASET", "confidence": 0.8427749872207642}]}], "tableCaptions": [{"text": " Table 1: Results for Chinese-English experiments, with- out neural LM (baseline) and with neural LM for rerank- ing and integrated decoding. Reranking with the neural  LM improves translation quality, while integrating it into  the decoder improves even more.", "labels": [], "entities": []}, {"text": " Table 2: Results for Europarl MT experiments, without  neural LM (baseline) and with neural LM for reranking  and integrated decoding. The neural LM gives improve- ments across three different language pairs. Superscript 2  indicates a score averaged between two runs; all other  scores were averaged over three runs.", "labels": [], "entities": [{"text": "Europarl MT", "start_pos": 22, "end_pos": 33, "type": "DATASET", "confidence": 0.8057030141353607}]}]}