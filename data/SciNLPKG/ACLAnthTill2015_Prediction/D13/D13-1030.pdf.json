{"title": [{"text": "Interpreting Anaphoric Shell Nouns using Antecedents of Cataphoric Shell Nouns as Training Data", "labels": [], "entities": [{"text": "Interpreting Anaphoric Shell Nouns", "start_pos": 0, "end_pos": 34, "type": "TASK", "confidence": 0.7875897139310837}]}], "abstractContent": [{"text": "Interpreting anaphoric shell nouns (ASNs) such as this issue and this fact is essential to understanding virtually any substantial natural language text.", "labels": [], "entities": [{"text": "Interpreting anaphoric shell nouns (ASNs)", "start_pos": 0, "end_pos": 41, "type": "TASK", "confidence": 0.758065232208797}]}, {"text": "One obstacle in developing methods for automatically interpreting ASNs is the lack of annotated data.", "labels": [], "entities": [{"text": "interpreting ASNs", "start_pos": 53, "end_pos": 70, "type": "TASK", "confidence": 0.788110762834549}]}, {"text": "We tackle this challenge by exploiting cataphoric shell nouns (CSNs) whose construction makes them particularly easy to interpret (e.g., the fact that X).", "labels": [], "entities": []}, {"text": "We propose an approach that uses automatically extracted antecedents of CSNs as training data to interpret ASNs.", "labels": [], "entities": [{"text": "interpret ASNs", "start_pos": 97, "end_pos": 111, "type": "TASK", "confidence": 0.6247105002403259}]}, {"text": "We achieve precisions in the range of 0.35 (baseline = 0.21) to 0.72 (baseline = 0.44), depending upon the shell noun.", "labels": [], "entities": [{"text": "precisions", "start_pos": 11, "end_pos": 21, "type": "METRIC", "confidence": 0.9992994070053101}]}], "introductionContent": [{"text": "Anaphors such as this fact and this issue encapsulate complex abstract entities such as propositions, facts, and events.", "labels": [], "entities": []}, {"text": "An example is shown below.", "labels": [], "entities": []}, {"text": "(1) Here is another bit of advice: Environmental Defense, a national advocacy group, notes that \"Mowing the lawn with a gas mower produces as much pollution in half an hour as driving a car 172 miles.\"", "labels": [], "entities": []}, {"text": "This fact may help to explain the recent surge in the sales of the good oldfashioned push mowers or the battery-powered mowers.", "labels": [], "entities": []}, {"text": "Here, the anaphor this fact is interpreted with the help of the clausal antecedent marked in bold.", "labels": [], "entities": []}, {"text": "The antecedent here is complex because it involves a number of entities and events (e.g., mowing the lawn, a gas mower) and relationships between them, and is abstract because the antecedent itself is not a purely physical entity.", "labels": [], "entities": []}, {"text": "The distinguishing property of these anaphors is that they contain semantically rich abstract nouns (e.g., fact in) which characterize and label their corresponding antecedents.", "labels": [], "entities": []}, {"text": "Linguists and philosophers have studied such abstract nouns for decades.", "labels": [], "entities": []}, {"text": "Our work is inspired by one such study, namely that of.", "labels": [], "entities": []}, {"text": "Following Schmid, we refer to these abstract nouns as shell nouns, as they serve as conceptual shells for complex chunks of information.", "labels": [], "entities": []}, {"text": "Accordingly, we refer to the anaphoric occurrences of shell nouns (e.g., this fact in (1)) as anaphoric shell nouns (ASNs).", "labels": [], "entities": []}, {"text": "An important reason for studying ASNs is their ubiquity in all kinds of text.", "labels": [], "entities": [{"text": "ASNs", "start_pos": 33, "end_pos": 37, "type": "TASK", "confidence": 0.9505423903465271}]}, {"text": "observed that shell nouns such as fact, idea, point, and problem were among the 100 most frequently occurring nouns in a corpus of 225 million words of British English.", "labels": [], "entities": []}, {"text": "Moreover, ASNs can play several roles in organizing a discourse such as encapsulation of complex information, cohesion, and topic boundary marking.", "labels": [], "entities": [{"text": "topic boundary marking", "start_pos": 124, "end_pos": 146, "type": "TASK", "confidence": 0.6332662800947825}]}, {"text": "So correct interpretation of ASNs can bean important step for correct interpretation of a discourse, and in a number of NLP applications such as text summarization, information extraction, and non-factoid question answering.", "labels": [], "entities": [{"text": "correct interpretation of ASNs", "start_pos": 3, "end_pos": 33, "type": "TASK", "confidence": 0.693510428071022}, {"text": "text summarization", "start_pos": 145, "end_pos": 163, "type": "TASK", "confidence": 0.7609039545059204}, {"text": "information extraction", "start_pos": 165, "end_pos": 187, "type": "TASK", "confidence": 0.8645609617233276}, {"text": "question answering", "start_pos": 205, "end_pos": 223, "type": "TASK", "confidence": 0.7345593571662903}]}, {"text": "Despite their importance, ASNs have not received much attention in Computational Linguistics, and research in this field remains in its earliest stages.", "labels": [], "entities": [{"text": "ASNs", "start_pos": 26, "end_pos": 30, "type": "TASK", "confidence": 0.9781550765037537}, {"text": "Computational Linguistics", "start_pos": 67, "end_pos": 92, "type": "TASK", "confidence": 0.8302681744098663}]}, {"text": "At present, the major obstacle is that there is very little annotated data available that could be used to train a supervised machine learning system for robustly interpreting these anaphors, and manual annotation is an expensive and time-consuming task.", "labels": [], "entities": []}, {"text": "We tackle this challenge by exploiting a category of examples, as shown in (2), whose construction is particularly easy to interpret.", "labels": [], "entities": []}, {"text": "(2) Congress has focused almost solely on the fact that special education is expensive -and that it takes away money from regular education.", "labels": [], "entities": []}, {"text": "Here, in contrast with (1), the fact is not anaphoric in the traditional sense, but is an easy case of a forward-looking anaphor -a cataphor.", "labels": [], "entities": []}, {"text": "While the resolution process of this fact in is quite challenging as it requires the use of semantics and world knowledge, it is fairly easy to interpret the fact in (2) based on the syntactic structure alone.", "labels": [], "entities": []}, {"text": "We refer to these easy-to-interpret cataphoric occurrences of shell nouns as cataphoric shell nouns (CSNs).", "labels": [], "entities": []}, {"text": "The interpretation of both ASNs and CSNs will be referred to as antecedent.", "labels": [], "entities": []}, {"text": "The antecedent of the fact in (2) is given in the post-nominal that clause.", "labels": [], "entities": []}, {"text": "We use the term shell concept to refer to the general notion of a shell noun, i.e., the semantic type of the antecedent.", "labels": [], "entities": []}, {"text": "For example, the notion of an issue is an important problem which requires a solution.", "labels": [], "entities": []}, {"text": "In this work, we propose an approach to interpret ASNs that exploits unlabelled but easy-to-interpret CSN examples to extract characteristic features associated with the antecedent of different shell concepts.", "labels": [], "entities": []}, {"text": "We evaluate our approach using crowdsourcing.", "labels": [], "entities": []}, {"text": "Our results show that these unlabelled CSN examples provide useful linguistic properties that help in interpreting ASNs.", "labels": [], "entities": [{"text": "interpreting ASNs", "start_pos": 102, "end_pos": 119, "type": "TASK", "confidence": 0.8170827627182007}]}], "datasetContent": [{"text": "We evaluate the ranked candidates of ASN instances using crowdsourcing.", "labels": [], "entities": []}, {"text": "Interface We chose to use CrowdFlower 9 as our crowdsourcing interface because of its integrated quality-control mechanism.", "labels": [], "entities": []}, {"text": "For instance, it throws gold questions randomly at the workers and the workers who do not answer them correctly are not allowed to continue.", "labels": [], "entities": []}, {"text": "We presented to the crowd evaluators the ASN instances from the ASN corpus.", "labels": [], "entities": [{"text": "ASN corpus", "start_pos": 64, "end_pos": 74, "type": "DATASET", "confidence": 0.7498611211776733}]}, {"text": "Recall that each ASN instance is made up of the paragraph containing the ASN and two preceding paragraphs as context.", "labels": [], "entities": []}, {"text": "We displayed the first 10 highly-ranked candidates (ordered randomly) given by our testing phase and asked the evaluators to choose the best answer that represents the ASN antecedent.", "labels": [], "entities": []}, {"text": "We encouraged the evaluators to select None when they did not agree with any of the displayed answers.", "labels": [], "entities": []}, {"text": "We also asked them how satisfied they were with the displayed answers.", "labels": [], "entities": []}, {"text": "We provided them with three options: unsatisfied, satisfied, and partially satisfied.", "labels": [], "entities": []}, {"text": "Our job contained 2,323 evaluation units.", "labels": [], "entities": []}, {"text": "We asked for 8 judgements per instance and paid 6 cents per evaluation unit.", "labels": [], "entities": []}, {"text": "As we were interested in the verdict of native speakers of English, we limited the allowed demographic region to Englishspeaking countries.", "labels": [], "entities": []}, {"text": "Results Among the 2,323 ASN instances, 96% of them were labelled as satisfied, 3% as partially satisfied and 1% as unsatisfied.", "labels": [], "entities": []}, {"text": "Only 2% of the instances were labelled as None.", "labels": [], "entities": []}, {"text": "As expected, evaluators were unsatisfied or partially satisfied with the options of these instances.", "labels": [], "entities": []}, {"text": "These results suggest that our resolution models trained on automatically extracted antecedents of CSNs bring the relevant candidates of ASN antecedents to the top, i.e., within first 10 highly-ranked candidates.", "labels": [], "entities": []}, {"text": "This itself is a positive result given the large search space of ASN antecedent candidates (more than 55 candidates on average).", "labels": [], "entities": [{"text": "ASN antecedent", "start_pos": 65, "end_pos": 79, "type": "TASK", "confidence": 0.7304847836494446}]}, {"text": "Among the evaluation units, more than half of the evaluators agreed on an answer for 1,810 units.", "labels": [], "entities": []}, {"text": "We used these instances for further analysis.", "labels": [], "entities": []}, {"text": "To examine which CSN antecedent features are relevant in identifying ASN antecedents, we carried out ablation experiments with all feature class combinations.", "labels": [], "entities": []}, {"text": "We compared the rankings given by our ranker to the crowd's answer using precision at n (P@n).", "labels": [], "entities": [{"text": "precision", "start_pos": 73, "end_pos": 82, "type": "METRIC", "confidence": 0.9981514811515808}]}, {"text": "10 More specifically, we count the number of instances where the crowd's answers occur within our ranker's first n choices.", "labels": [], "entities": []}, {"text": "P@n then is this count divided by the total number of instances.", "labels": [], "entities": []}, {"text": "Note that P@1 is equivalent to the standard precision.", "labels": [], "entities": [{"text": "P@1", "start_pos": 10, "end_pos": 13, "type": "METRIC", "confidence": 0.9215103983879089}, {"text": "precision", "start_pos": 44, "end_pos": 53, "type": "METRIC", "confidence": 0.9388265013694763}]}, {"text": "We compared our results against two baselines: preceding sentence and chance.", "labels": [], "entities": []}, {"text": "The preceding sentence baseline chooses the previous sentence as the correct antecedent.", "labels": [], "entities": []}, {"text": "The chance baseline chooses a candidate from a uniform random distribution over the set of 10 top-ranked candidates.", "labels": [], "entities": []}, {"text": "The results are shown in.", "labels": [], "entities": []}, {"text": "Although different feature combinations gave the best results for different shell nouns, the features that occur frequently in many best-performing combinations were embedding level (E), lexical (LX), and subordinating conjunction (SC) features.", "labels": [], "entities": []}, {"text": "The SC features were particularly effective for issue and question, where we expected patterns such as whether X.", "labels": [], "entities": []}, {"text": "Surprisingly, the syntactic type features (S) did not show up very often in the best-performing feature combinations, suggesting that the ASN antecedents had a greater variety of syntactic types than what was available in our CSN training data.", "labels": [], "entities": []}, {"text": "The context features (C) did not appear in any of the best-performing feature combinations.", "labels": [], "entities": []}, {"text": "In fact, they resulted in a sharp decline in the precision.", "labels": [], "entities": [{"text": "precision", "start_pos": 49, "end_pos": 58, "type": "METRIC", "confidence": 0.99795001745224}]}, {"text": "For instance, for question, adding the context features to the best-performing combination {E,SC,V,L,LX} resulted in a drop of 16 percentage points.", "labels": [], "entities": []}, {"text": "This result was not surprising because although the antecedents of ASNs and CSNs share similar properties such as common words, we know that their context is generally different.", "labels": [], "entities": []}, {"text": "We did not observe specific features associated with Schmid's semantic categories.", "labels": [], "entities": []}, {"text": "An exception was the E features which were particularly effective for the factual nouns fact and reason: the results with them alone gave high precision (0.68 for fact and 0.72 for reason).", "labels": [], "entities": [{"text": "precision", "start_pos": 143, "end_pos": 152, "type": "METRIC", "confidence": 0.9990047812461853}]}, {"text": "That said, the E features were present inmost of the best-performing combinations even for the shell nouns in other semantic categories.: Evaluation of our ranker for antecedents of six ASNs.", "labels": [], "entities": []}, {"text": "For each noun we show the three best-performing feature combinations.", "labels": [], "entities": []}, {"text": "P@n is the precision at rank n (P@1 = standard precision).", "labels": [], "entities": [{"text": "precision", "start_pos": 11, "end_pos": 20, "type": "METRIC", "confidence": 0.9981101751327515}, {"text": "standard precision", "start_pos": 38, "end_pos": 56, "type": "METRIC", "confidence": 0.6478556096553802}]}, {"text": "Boldface indicates the best in the column.", "labels": [], "entities": []}, {"text": "PSbaseline = preceding sentence baseline.", "labels": [], "entities": []}, {"text": "The P@1 results significantly higher than PSbaseline are marked with * (two-sample \u03c7 2 test: p < 0.05).", "labels": [], "entities": [{"text": "PSbaseline", "start_pos": 42, "end_pos": 52, "type": "DATASET", "confidence": 0.779028594493866}]}, {"text": "The chance baseline results were 0.1, 0.2, 0.3, and 0.4 for P@1, P@2, P@3, and P@4, respectively.", "labels": [], "entities": [{"text": "chance baseline", "start_pos": 4, "end_pos": 19, "type": "METRIC", "confidence": 0.9583164751529694}]}, {"text": "The only previous work with which our results could be compared is that of.", "labels": [], "entities": []}, {"text": "The work reports precision in the range of 0.41 to 0.61 in resolving this issue anaphora in the Medline domain.", "labels": [], "entities": [{"text": "precision", "start_pos": 17, "end_pos": 26, "type": "METRIC", "confidence": 0.9995273351669312}, {"text": "Medline domain", "start_pos": 96, "end_pos": 110, "type": "DATASET", "confidence": 0.9548170566558838}]}, {"text": "In our case, for this issue instances from the NYT corpus, we achieved precision in the range of 0.40 to 0.47.", "labels": [], "entities": [{"text": "NYT corpus", "start_pos": 47, "end_pos": 57, "type": "DATASET", "confidence": 0.9538443386554718}, {"text": "precision", "start_pos": 71, "end_pos": 80, "type": "METRIC", "confidence": 0.9994860887527466}]}, {"text": "Furthermore, we applied our models to resolve this issue instances from work.", "labels": [], "entities": []}, {"text": "Even with models trained on automatically labelled CSN antecedents, we achieved similar results to Kolhatkar and Hirst's results: P@1 of 0.45, P@2 of 0.59, P@3 of 0.65, and P@4 of 0.67.", "labels": [], "entities": []}, {"text": "These results show the domain robustness of our methods with respect to the shell noun issue.", "labels": [], "entities": []}, {"text": "Recall that looked at only very specific cases of this issue and used manually annotated data (Section 2), as opposed to the automatically extracted CSN antecedent data we use.", "labels": [], "entities": []}, {"text": "We thank an anonymous reviewer for suggesting this to us.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 4: Evaluation of our ranker for antecedents of six ASNs. For each noun we show the three best-performing  feature combinations. P@n is the precision at rank n (P@1 = standard precision). Boldface indicates the best in the  column. PSbaseline = preceding sentence baseline. The P@1 results significantly higher than PSbaseline are marked  with  *  (two-sample \u03c7 2 test: p < 0.05). The chance baseline results were 0.1, 0.2, 0.3, and 0.4 for P@1, P@2, P@3,  and P@4, respectively.", "labels": [], "entities": [{"text": "precision", "start_pos": 146, "end_pos": 155, "type": "METRIC", "confidence": 0.9977450370788574}]}]}