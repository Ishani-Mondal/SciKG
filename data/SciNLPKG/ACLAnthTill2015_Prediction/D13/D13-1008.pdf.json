{"title": [], "abstractContent": [{"text": "Compared to the edited genres that have played a central role in NLP research, mi-croblog texts use a more informal register with nonstandard lexical items, abbreviations, and free orthographic variation.", "labels": [], "entities": []}, {"text": "When confronted with such input, conventional text analysis tools often perform poorly.", "labels": [], "entities": [{"text": "text analysis", "start_pos": 46, "end_pos": 59, "type": "TASK", "confidence": 0.777772456407547}]}, {"text": "Normalization-replacing orthographically or lexically id-iosyncratic forms with more standard variants-can improve performance.", "labels": [], "entities": []}, {"text": "We propose a method for learning normalization rules from machine translations of a parallel corpus of microblog messages.", "labels": [], "entities": [{"text": "learning normalization rules from machine translations of a parallel corpus of microblog messages", "start_pos": 24, "end_pos": 121, "type": "TASK", "confidence": 0.8133114255391635}]}, {"text": "To validate the utility of our approach, we evaluate extrinsically, showing that normalizing English tweets and then translating improves translation quality (com-pared to translating unnormalized text) using three standard web translation services as well as a phrase-based translation system trained on parallel microblog data.", "labels": [], "entities": [{"text": "phrase-based translation", "start_pos": 262, "end_pos": 286, "type": "TASK", "confidence": 0.6757443249225616}]}], "introductionContent": [{"text": "Microblogs such as Twitter, Sina Weibo (a popular Chinese microblog service) and Facebook have received increasing attention in diverse research communities (.", "labels": [], "entities": []}, {"text": "In contrast to traditional text domains that use carefully controlled, standardized language, microblog content is often informal, with less adherence to conventions regarding punctuation, spelling, and style, and with a higher proportion of dialect or pronouciation-derived orthography.", "labels": [], "entities": []}, {"text": "While this diversity itself is an important resource for studying, e.g., sociolinguistic variation, it poses challenges to NLP applications developed for more formal domains.", "labels": [], "entities": []}, {"text": "If retaining variation due to sociolinguistic or phonological factors is not crucial, text normalization can improve performance on downstream tasks ( \u00a72).", "labels": [], "entities": [{"text": "text normalization", "start_pos": 86, "end_pos": 104, "type": "TASK", "confidence": 0.8036236763000488}]}, {"text": "This paper introduces a data-driven approach to learning normalization rules by conceiving of normalization as a kind of paraphrasing and taking inspiration from the bilingual pivot approach to paraphrase detection () and the observation that translation is an inherently \"simplifying\" process.", "labels": [], "entities": [{"text": "paraphrase detection", "start_pos": 194, "end_pos": 214, "type": "TASK", "confidence": 0.8570432066917419}]}, {"text": "Starting from a parallel corpus of microblog messages consisting of English paired with several other languages (, we use standard web machine translation systems to re-translate the non-English segment, producing English original, English MT pairs ( \u00a73).", "labels": [], "entities": []}, {"text": "These are our normalization examples, with MT output playing the role of normalized English.", "labels": [], "entities": [{"text": "MT", "start_pos": 43, "end_pos": 45, "type": "TASK", "confidence": 0.922902524471283}]}, {"text": "Several techniques for identifying high-precision normalization rules are proposed, and we introduce a character-based normalization model to account for predictable character-level processes, like repetition and substitution ( \u00a74).", "labels": [], "entities": []}, {"text": "We then describe our decoding procedure ( \u00a75) and show that our normalization model improve translation quality for EnglishChinese microblog translation ( \u00a76).", "labels": [], "entities": [{"text": "EnglishChinese microblog translation", "start_pos": 116, "end_pos": 152, "type": "TASK", "confidence": 0.7905082305272421}]}], "datasetContent": [{"text": "We evaluate our normalization model intrinsically by testing whether our normalizations more closely resemble standardized data, and then extrinsically by testing whether we can improve the translation quality of in-house as well as online Machine Translation systems by normalizing the input.", "labels": [], "entities": [{"text": "Machine Translation", "start_pos": 240, "end_pos": 259, "type": "TASK", "confidence": 0.7147375196218491}]}], "tableCaptions": [{"text": " Table 4: Fragment of the phrase normalization model  built, for each original phrase o, we present the top-3 nor- malized forms ranked by f (n | o).", "labels": [], "entities": [{"text": "phrase normalization", "start_pos": 26, "end_pos": 46, "type": "TASK", "confidence": 0.7513558864593506}]}, {"text": " Table 6: Normalization and MT Results. Rows denote different normalizations, and columns different translation  systems, except the first column (Norm), which denotes the normalization experiment. Cells display the BLEU score  of that experiment.  Moses  Moses  Condition  Norm (News) (News+Weibo) Online A Online B Online C  baseline  19.90  15.10  24.37  20.09  17.89  18.79  norm+phrase  21.96  15.69  24.29  20.50  18.13  18.93  norm+phrase+char  22.39  15.87  24.40  20.61  18.22  19.08  norm+phrase+char+mono 22.91  15.94  24.46  20.78  18.37  19.21", "labels": [], "entities": [{"text": "MT", "start_pos": 28, "end_pos": 30, "type": "TASK", "confidence": 0.9882121682167053}, {"text": "BLEU score", "start_pos": 216, "end_pos": 226, "type": "METRIC", "confidence": 0.9764358997344971}, {"text": "News+Weibo) Online A Online B Online C  baseline", "start_pos": 287, "end_pos": 335, "type": "DATASET", "confidence": 0.8209704052318226}]}]}