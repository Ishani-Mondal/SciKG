{"title": [{"text": "Automatic Domain Partitioning for Multi-Domain Learning", "labels": [], "entities": [{"text": "Automatic Domain Partitioning", "start_pos": 0, "end_pos": 29, "type": "TASK", "confidence": 0.5731417735417684}]}], "abstractContent": [{"text": "Multi-Domain learning (MDL) assumes that the domain labels in the dataset are known.", "labels": [], "entities": [{"text": "Multi-Domain learning (MDL)", "start_pos": 0, "end_pos": 27, "type": "TASK", "confidence": 0.8015728712081909}]}, {"text": "However, when there are multiple metadata attributes available, it is not always straightforward to select a single best attribute for domain partition, and it is possible that combining more than one metadata attributes (includ-ing continuous attributes) can lead to better MDL performance.", "labels": [], "entities": []}, {"text": "In this work, we propose an automatic domain partitioning approach that aims at providing better domain identities for MDL.", "labels": [], "entities": [{"text": "domain partitioning", "start_pos": 38, "end_pos": 57, "type": "TASK", "confidence": 0.6855785101652145}]}, {"text": "We use a supervised clustering approach that learns the domain distance between data instances , and then cluster the data into better domains for MDL.", "labels": [], "entities": []}, {"text": "Our experiment on real multi-domain datasets shows that using our automatically generated domain partition improves over popular MDL methods.", "labels": [], "entities": []}], "introductionContent": [{"text": "Instead of assuming data are i.i.d, Multi-domain learning (MDL) methods assumes that data come from several domains and make use of domain labels to improve modeling performance.", "labels": [], "entities": []}, {"text": "The motivation of using MDL is that datasets from different domains could be different, in two ways.", "labels": [], "entities": []}, {"text": "First, the feature distribution p(x) could be domain specific, meaning that the importance of each feature is different across domains.", "labels": [], "entities": []}, {"text": "Second, the distribution of label Y given X, p(y|x), of different domains could be different.", "labels": [], "entities": []}, {"text": "These differences could create problems for traditional machine learning methods: models learned from one domain might not be generalizable to other domains.", "labels": [], "entities": []}, {"text": "One common assumption of MDL methods is that the domain identities are pre-defined.", "labels": [], "entities": []}, {"text": "For example, in the multi-domain Amazon product review dataset, the product categories are typically used as the domain identities.", "labels": [], "entities": [{"text": "Amazon product review dataset", "start_pos": 33, "end_pos": 62, "type": "DATASET", "confidence": 0.5331783220171928}]}, {"text": "However, a question raised by is that, in real-world data sets, there could be many ways to split data into domains, and it is hard to decide which one to use.", "labels": [], "entities": []}, {"text": "Consider the Amazon product reviews, where we have multiple attributes attached to each review: for example, product category, reviewer location, price, and number of feedback.", "labels": [], "entities": []}, {"text": "Which attribute is the most informative domain label?", "labels": [], "entities": []}, {"text": "Or we should use all of these meta-data and partition the data into many small domains?", "labels": [], "entities": []}, {"text": "In this paper, we investigate the problem of automatic domain partitioning.", "labels": [], "entities": [{"text": "automatic domain partitioning", "start_pos": 45, "end_pos": 74, "type": "TASK", "confidence": 0.5876937111218771}]}, {"text": "We propose an empirical domain difference testing method to examine whether two groups of data are i.i.d, or generated from different distributions, and how different they are.", "labels": [], "entities": []}, {"text": "Using this approach, we generate data pairs that belong to the same distribution, and data pairs that should be partitioned into different domains.", "labels": [], "entities": []}, {"text": "These pairs are then used as training data fora supervised clustering algorithm, which automatically partitions the dataset into several domains.", "labels": [], "entities": []}, {"text": "In the evaluation, we show that our automatically-partitioned domains improve the performances of two popular MDL methods on real sentiment analysis data sets.", "labels": [], "entities": []}, {"text": "Note that proposed a MultiAttribute Multi-Domain learning (MAMD) method, which also exploited multiple dimensions of meta-data and provided extensions to two traditional MDL methods.", "labels": [], "entities": []}, {"text": "However, extensions to the MAMD setting may not be trivial for every MDL algorithm, while our method serves as a pre-processing step and can be easily used for all MDL approaches.", "labels": [], "entities": []}, {"text": "In addition to this, MAMD only works with categorical metadata, and cannot fully utilize information in the form of continuous metadata values.", "labels": [], "entities": [{"text": "MAMD", "start_pos": 21, "end_pos": 25, "type": "TASK", "confidence": 0.8068084120750427}]}], "datasetContent": [{"text": "Datasets To evaluate our methods, we used two subsets of Amazon review corpus (Jindal and Liu, 2008), which originally contain 5.8 million reviews with a variety of metadata about products and users.", "labels": [], "entities": [{"text": "Amazon review corpus", "start_pos": 57, "end_pos": 77, "type": "DATASET", "confidence": 0.9378673632939657}]}, {"text": "The first subset (BOOK) contains 20,000 reviews on books published by eleven most popular publishers, while the second (PROD) is reviews about products within seven most common product categories.", "labels": [], "entities": [{"text": "BOOK", "start_pos": 18, "end_pos": 22, "type": "METRIC", "confidence": 0.992877185344696}]}, {"text": "We randomly split each dataset into training and testing sets with equal size.", "labels": [], "entities": []}, {"text": "The task is to predict a positive or negative label for each review.", "labels": [], "entities": []}, {"text": "Case insensitive unigrams excluding stop words are used as features, and all features appear less than 500 times are removed for efficient experiment processing.", "labels": [], "entities": []}, {"text": "Reviews of 4 or 5 stars are considered positive and 1 or 2 stars are considered negative, while 3 stars reviews are excluded.", "labels": [], "entities": []}, {"text": "Each review has multiple metadata such as book's publisher, product's type, user's state location, product price, review year, and number of other user feedback.", "labels": [], "entities": []}, {"text": "Reviews with missing metadata are filtered out.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Overall accuracies on PROD and BOOK  datasets. ADP results that are statistically significantly  better than MAMD are marked with , and better than 1- Best and Random are indicated by  \u2021 and  *  respectively,  using a paired t-test, with p < 0.05.", "labels": [], "entities": [{"text": "accuracies", "start_pos": 18, "end_pos": 28, "type": "METRIC", "confidence": 0.9863972067832947}, {"text": "PROD and BOOK  datasets", "start_pos": 32, "end_pos": 55, "type": "DATASET", "confidence": 0.6864227876067162}]}]}