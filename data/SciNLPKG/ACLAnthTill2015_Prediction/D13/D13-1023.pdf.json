{"title": [{"text": "An Efficient Language Model Using Double-Array Structures", "labels": [], "entities": []}], "abstractContent": [{"text": "N gram language models tend to increase in size with inflating the corpus size, and consume considerable resources.", "labels": [], "entities": []}, {"text": "In this paper , we propose an efficient method for implementing ngram models based on double-array structures.", "labels": [], "entities": []}, {"text": "First, we propose a method for representing backwards suffix trees using double-array structures and demonstrate its efficiency.", "labels": [], "entities": []}, {"text": "Next, we propose two optimization methods for improving the efficiency of data representation in the double-array structures.", "labels": [], "entities": []}, {"text": "Embedding probabilities into unused spaces in double-array structures reduces the model size.", "labels": [], "entities": []}, {"text": "Moreover, tuning the word IDs in the language model makes the model smaller and faster.", "labels": [], "entities": []}, {"text": "We also show that our method can be used for building large language models using the division method.", "labels": [], "entities": []}, {"text": "Lastly, we show that our method outperforms methods based on recent related works from the viewpoints of model size and query speed when both optimization methods are used.", "labels": [], "entities": []}], "introductionContent": [{"text": "N gram language models (F. are widely used as probabilistic models of sentence in natural language processing.", "labels": [], "entities": []}, {"text": "The wide use of the Internet has entailed a dramatic increase in size of the available corpora, which can be harnessed to obtain a significant improvement in model quality.", "labels": [], "entities": []}, {"text": "In particular, have shown that the performance of statistical machine translation systems is monotonically improved with the increasing size of training corpora for the language model.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 50, "end_pos": 81, "type": "TASK", "confidence": 0.6269770165284475}]}, {"text": "However, models using larger corpora also consume more resources.", "labels": [], "entities": []}, {"text": "In recent years, many methods for improving the efficiency of language models have been proposed to tackle this problem ().", "labels": [], "entities": []}, {"text": "Such methods not only reduce the required memory size but also raise query speed.", "labels": [], "entities": []}, {"text": "In this paper, we propose the double-array language model (DALM) which uses double-array structures.", "labels": [], "entities": []}, {"text": "Double-array structures are widely used in text processing, especially for Japanese.", "labels": [], "entities": [{"text": "text processing", "start_pos": 43, "end_pos": 58, "type": "TASK", "confidence": 0.7386237680912018}]}, {"text": "They are known to provide a compact representation of tries and fast transitions between trie nodes.", "labels": [], "entities": []}, {"text": "The ability to store and manipulate tries efficiently is expected to increase the performance of language models (i.e., improving query speed and reducing the model size in terms of memory) because tries are one of the most common representations of data structures in language models.", "labels": [], "entities": []}, {"text": "We use double-array structures to implement a language model since we can utilize their speed and compactness when querying the model about an ngram.", "labels": [], "entities": []}, {"text": "In order to utilize of double-array structures as language models, we modify them to be able to store probabilities and backoff weights.", "labels": [], "entities": []}, {"text": "We also propose two optimization methods: embedding and ordering.", "labels": [], "entities": []}, {"text": "These methods reduce model size and increase query speed.", "labels": [], "entities": []}, {"text": "Embedding is an efficient method for storing ngram probabilities and backoff weights, whereby we find vacant spaces in the double-array language model structure and populate them with language model information, such as probabilities and backoff weights.", "labels": [], "entities": []}, {"text": "Ordering is a method for compacting the double-array structure.", "labels": [], "entities": []}, {"text": "DALM uses word IDs for all words of the ngram, and ordering assigns a word ID to each word to reduce the model size.", "labels": [], "entities": [{"text": "DALM", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.824665367603302}]}, {"text": "These two optimization methods can be used simultaneously and are also expected to work well.", "labels": [], "entities": []}, {"text": "In our experiments, we use a language model based on corpora of the NTCIR patent retrieval task.", "labels": [], "entities": [{"text": "NTCIR patent retrieval task", "start_pos": 68, "end_pos": 95, "type": "TASK", "confidence": 0.6980972290039062}]}, {"text": "The model size is 31 GB in the ARPA file format.", "labels": [], "entities": [{"text": "ARPA file format", "start_pos": 31, "end_pos": 47, "type": "DATASET", "confidence": 0.9443510969479879}]}, {"text": "We conducted experiments focusing on query speed and model size.", "labels": [], "entities": []}, {"text": "The results indicate that when the abovementioned optimization methods are used together, DALM outperforms state-ofthe-art methods on those points.", "labels": [], "entities": []}], "datasetContent": [{"text": "To compare the performance of DALM with other methods, we conduct experiments on two ngram models built from small and large training corpora.", "labels": [], "entities": []}, {"text": "shows the specifications of the model.", "labels": [], "entities": []}, {"text": "Training data are extracted from the Publication of unexamined Japanese patent applications, which is distributed with the NTCIR 3,4,5,6 patent retrieval task.", "labels": [], "entities": [{"text": "NTCIR 3,4,5,6 patent retrieval", "start_pos": 123, "end_pos": 153, "type": "TASK", "confidence": 0.7271134704351425}]}, {"text": "We used data for the period from  Our experiments were performed from the viewpoints of speed and model size.", "labels": [], "entities": [{"text": "speed", "start_pos": 88, "end_pos": 93, "type": "METRIC", "confidence": 0.995206892490387}]}, {"text": "We executed each program twice, and the results of the second run were taken as the final performance.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Comparison between tuned and non-tuned  double-array structures.", "labels": [], "entities": []}, {"text": " Table 3: Comparison between divided and original  double-array structures.", "labels": [], "entities": []}, {"text": " Table 4: Comparison between DALM and other methods.", "labels": [], "entities": []}]}