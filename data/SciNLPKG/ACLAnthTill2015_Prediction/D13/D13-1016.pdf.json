{"title": [{"text": "Learning Latent Word Representations for Domain Adaptation using Supervised Word Clustering", "labels": [], "entities": [{"text": "Learning Latent Word Representations", "start_pos": 0, "end_pos": 36, "type": "TASK", "confidence": 0.540635496377945}, {"text": "Domain Adaptation", "start_pos": 41, "end_pos": 58, "type": "TASK", "confidence": 0.7201191037893295}]}], "abstractContent": [{"text": "Domain adaptation has been popularly studied on exploiting labeled information from a source domain to learn a prediction model in a target domain.", "labels": [], "entities": [{"text": "Domain adaptation", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.7817286550998688}]}, {"text": "In this paper, we develop a novel representation learning approach to address domain adaptation for text classification with automatically induced discriminative latent features, which are generalizable across domains while informative to the prediction task.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 78, "end_pos": 95, "type": "TASK", "confidence": 0.7455922961235046}, {"text": "text classification", "start_pos": 100, "end_pos": 119, "type": "TASK", "confidence": 0.7296654134988785}]}, {"text": "Specifically, we propose a hierarchical multinomial Naive Bayes model with latent variables to conduct supervised word clustering on labeled documents from both source and target domains, and then use the produced cluster distribution of each word as its latent feature representation for domain adaptation.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 289, "end_pos": 306, "type": "TASK", "confidence": 0.7484035193920135}]}, {"text": "We train this latent graphical model using a simple expectation-maximization (EM) algorithm.", "labels": [], "entities": []}, {"text": "We empirically evaluate the proposed method with both cross-domain document categorization tasks on Reuters-21578 dataset and cross-domain sentiment classification tasks on Amazon product review dataset.", "labels": [], "entities": [{"text": "Reuters-21578 dataset", "start_pos": 100, "end_pos": 121, "type": "DATASET", "confidence": 0.9826002418994904}, {"text": "cross-domain sentiment classification", "start_pos": 126, "end_pos": 163, "type": "TASK", "confidence": 0.6439132988452911}, {"text": "Amazon product review dataset", "start_pos": 173, "end_pos": 202, "type": "DATASET", "confidence": 0.9475743621587753}]}, {"text": "The experimental results demonstrate that our proposed approach achieves superior performance compared with alternative methods.", "labels": [], "entities": []}], "introductionContent": [{"text": "Supervised prediction models typically require a large amount of labeled data for training.", "labels": [], "entities": []}, {"text": "However, manually collecting data annotations is expensive in many real-world applications such as document categorization or sentiment classification.", "labels": [], "entities": [{"text": "sentiment classification", "start_pos": 126, "end_pos": 150, "type": "TASK", "confidence": 0.9388211071491241}]}, {"text": "Recently, domain adaptation has been proposed to exploit existing labeled data in a related source domain to assist the prediction model training in the target domain.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 10, "end_pos": 27, "type": "TASK", "confidence": 0.7763426005840302}, {"text": "prediction model training", "start_pos": 120, "end_pos": 145, "type": "TASK", "confidence": 0.8929421106974283}]}, {"text": "As an effective tool to reduce annotation effort, domain adaptation has achieved success in various crossdomain natural language processing (NLP) systems such as document categorization, sentiment classification (), email spam detection (, and a number of other NLP tasks.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 50, "end_pos": 67, "type": "TASK", "confidence": 0.7520670592784882}, {"text": "document categorization", "start_pos": 162, "end_pos": 185, "type": "TASK", "confidence": 0.7578449547290802}, {"text": "sentiment classification", "start_pos": 187, "end_pos": 211, "type": "TASK", "confidence": 0.9248780310153961}, {"text": "email spam detection", "start_pos": 216, "end_pos": 236, "type": "TASK", "confidence": 0.7096591194470724}]}, {"text": "One primary challenge of domain adaptation lies in the distribution divergence of the two domains in the original feature representation space.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 25, "end_pos": 42, "type": "TASK", "confidence": 0.7189500778913498}]}, {"text": "For example, documents about books may contain very different high-frequency words and discriminative words from documents about kitchen.", "labels": [], "entities": []}, {"text": "A good crossdomain feature representation thus has been viewed as critical for bridging the domain divergence gap and facilitating domain adaptation in the NLP area).", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 131, "end_pos": 148, "type": "TASK", "confidence": 0.7056990414857864}]}, {"text": "Many domain adaptation works have been proposed to learn new cross-domain feature representations ().", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 5, "end_pos": 22, "type": "TASK", "confidence": 0.7613880932331085}]}, {"text": "Though demonstrated good performance on certain problems, these works mostly induce new feature representations in an unsupervised way, without taking the valuable label information into account.", "labels": [], "entities": []}, {"text": "In this work, we present a novel supervised representation learning approach to discover a latent representation of words which is not only generalizable across domains but also informative to the classification task.", "labels": [], "entities": []}, {"text": "Specifically, we propose a hier-archical multinomial Naive Bayes model with latent word cluster variables to perform supervised word clustering on labeled documents from both domains.", "labels": [], "entities": []}, {"text": "Our model directly models the relationships between the observed document label variables and the latent word cluster variables.", "labels": [], "entities": []}, {"text": "The induced cluster representation of each word thus will be informative for the classification labels, and hence discriminative for the target classification task.", "labels": [], "entities": []}, {"text": "We train this directed graphical model using an expectationmaximization (EM) algorithm, which maximizes the log-likelihood of the observations of labeled documents.", "labels": [], "entities": [{"text": "expectationmaximization (EM)", "start_pos": 48, "end_pos": 76, "type": "METRIC", "confidence": 0.7370673716068268}]}, {"text": "The induced cluster distribution of each word can then be used as its generalizable representation to construct new cluster-based representation of each document.", "labels": [], "entities": []}, {"text": "For domain adaptation, we train a supervised learning system with labeled data from both domains in the new representation space and apply it to categorize test documents in the target domain.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 4, "end_pos": 21, "type": "TASK", "confidence": 0.7602606117725372}]}, {"text": "In order to evaluate the proposed technique, we conduct extensive experiments on the Reuters-21578 dataset for cross-domain document categorization and on Amazon product review dataset for cross-domain sentiment classification.", "labels": [], "entities": [{"text": "Reuters-21578 dataset", "start_pos": 85, "end_pos": 106, "type": "DATASET", "confidence": 0.9850023090839386}, {"text": "cross-domain document categorization", "start_pos": 111, "end_pos": 147, "type": "TASK", "confidence": 0.7028210163116455}, {"text": "Amazon product review dataset", "start_pos": 155, "end_pos": 184, "type": "DATASET", "confidence": 0.9295764863491058}, {"text": "cross-domain sentiment classification", "start_pos": 189, "end_pos": 226, "type": "TASK", "confidence": 0.8154875636100769}]}, {"text": "The experimental results show the proposed approach can produce more effective representations than the comparison domain adaptation methods.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluate the proposed approach with experiments on cross domain document categorization of Reuters data and cross domain sentiment classification of Amazon product reviews, comparing to a number of baseline and existing domain adaptation methods.", "labels": [], "entities": [{"text": "cross domain document categorization", "start_pos": 54, "end_pos": 90, "type": "TASK", "confidence": 0.5837968289852142}, {"text": "Reuters data", "start_pos": 94, "end_pos": 106, "type": "DATASET", "confidence": 0.79163658618927}, {"text": "cross domain sentiment classification", "start_pos": 111, "end_pos": 148, "type": "TASK", "confidence": 0.6647090092301369}]}, {"text": "In this section, we report the experimental setting and results on these two data sets.", "labels": [], "entities": []}, {"text": "We used the popularly studied Reuters-21578 dataset, which contains three crossdomain document categorization tasks, Orgs vs People, Orgs vs Places, People vs Places.", "labels": [], "entities": [{"text": "Reuters-21578 dataset", "start_pos": 30, "end_pos": 51, "type": "DATASET", "confidence": 0.9808992743492126}]}, {"text": "The source and target domains of each task contain documents sampled from different non-overlapping subcategories.", "labels": [], "entities": []}, {"text": "From example, the task of Orgs vs People assigns a document into one of the two top categories, and the source domain documents and the target domain documents are sampled from different subcategories of Orgs and People.", "labels": [], "entities": []}, {"text": "There are 1237 source documents and 1208 target documents for the task of Orgs vs People, 1016 source documents and 1043 target documents for the task of Orgs vs Places, and 1077 source documents and 1077 target documents for the task ofPeople vs Places.", "labels": [], "entities": []}, {"text": "For each task, we built a unigram vocabulary based on all the documents from the two domains and represented each document as a feature vector containing term frequency values.", "labels": [], "entities": []}, {"text": "For each of the three cross-domain document categorization tasks on Reuters-21578 dataset, we used all the source documents as labeled training data while randomly selecting 100 target documents as labeled training data and setting the rest as unlabeled test data.", "labels": [], "entities": [{"text": "Reuters-21578 dataset", "start_pos": 68, "end_pos": 89, "type": "DATASET", "confidence": 0.9842537939548492}]}, {"text": "For the BOW baseline method, we used the term-frequency features.", "labels": [], "entities": [{"text": "BOW baseline", "start_pos": 8, "end_pos": 20, "type": "DATASET", "confidence": 0.6622669100761414}]}, {"text": "The other five approaches are based on representation learning, and we selected the dimension size of the representation learning, i.e., the cluster number in our proposed approach, from {5, 10, 20, 50, 100} according to the average classification results over 3 runs on the task of Orgs vs People.", "labels": [], "entities": []}, {"text": "The dimension sizes of the induced representations for the five approaches, PLSA, FDLDA, SCL, CPSP and SWC are 20, 20, 100, 100 and 20 respectively.", "labels": [], "entities": [{"text": "FDLDA", "start_pos": 82, "end_pos": 87, "type": "METRIC", "confidence": 0.5922033786773682}]}, {"text": "We then repeated each experiment 10 times on each task with different random selections of the 100 labeled target documents to compare the six comparison approaches.", "labels": [], "entities": []}, {"text": "The average classification results in terms of accuracy and standard deviations are reported in.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 47, "end_pos": 55, "type": "METRIC", "confidence": 0.9996583461761475}]}, {"text": "We can see that by simply combining labeled documents from the two domains without adaptation, the BOW method performs poorly across the three tasks.", "labels": [], "entities": [{"text": "BOW", "start_pos": 99, "end_pos": 102, "type": "METRIC", "confidence": 0.8325067758560181}]}, {"text": "The PLSA method outperforms the BOW method overall the three tasks with small improvements.", "labels": [], "entities": []}, {"text": "The supervised word clustering method FDLDA, though performing slightly better than the unsupervised clustering method PLSA, produces poor performance comparing to the proposed SWC method.", "labels": [], "entities": [{"text": "word clustering", "start_pos": 15, "end_pos": 30, "type": "TASK", "confidence": 0.6702338010072708}, {"text": "FDLDA", "start_pos": 38, "end_pos": 43, "type": "DATASET", "confidence": 0.8208084106445312}]}, {"text": "One possible reason is that the FDLDA model is not specialized for supervised word clustering, and it uses a logistic regression model to predict the labels from the word topics, while the final soft word clustering is computed from the learned distribution p(z) and p(w|z).", "labels": [], "entities": [{"text": "FDLDA", "start_pos": 32, "end_pos": 37, "type": "DATASET", "confidence": 0.6200518012046814}, {"text": "word clustering", "start_pos": 78, "end_pos": 93, "type": "TASK", "confidence": 0.7288268357515335}]}, {"text": "That is, in the FDLDA model the labels only influence the word clusterings indirectly and hence its influence can be much smaller than the influence of labels as direct parent variables of the word cluster variables in the SWC model.", "labels": [], "entities": [{"text": "FDLDA", "start_pos": 16, "end_pos": 21, "type": "DATASET", "confidence": 0.8036799430847168}]}, {"text": "The two domain adaptation approaches, SCL and CPSP, both produce significant improvements over BOW, PLSA and FDLDA on the two tasks of Orgs vs People and Orgs vs Places, while the CPSP method produces slightly inferior performance than PLSA and FDLDA on the task of People vs Places.", "labels": [], "entities": [{"text": "BOW", "start_pos": 95, "end_pos": 98, "type": "METRIC", "confidence": 0.9880049824714661}, {"text": "FDLDA", "start_pos": 109, "end_pos": 114, "type": "METRIC", "confidence": 0.9641228318214417}]}, {"text": "The proposed method SWC on the other hand consistently and significantly outperforms all the other comparison methods across all the three tasks.", "labels": [], "entities": [{"text": "SWC", "start_pos": 20, "end_pos": 23, "type": "TASK", "confidence": 0.7336834073066711}]}, {"text": "We also studied the sensitivity of the proposed approach with respect to the number of clusters, i.e., the dimension size of the learned representation.", "labels": [], "entities": []}, {"text": "We experimented with a set of different values m \u2208 {5, 10, 20, 50, 100} as the number of clusters.", "labels": [], "entities": []}, {"text": "For each m value, we used the same experimental setting as above and repeated the experiments 10 times to obtain the average comparison results.", "labels": [], "entities": []}, {"text": "The classification accuracy results on the three tasks are reported in.", "labels": [], "entities": [{"text": "classification", "start_pos": 4, "end_pos": 18, "type": "TASK", "confidence": 0.9034997820854187}, {"text": "accuracy", "start_pos": 19, "end_pos": 27, "type": "METRIC", "confidence": 0.9607944488525391}]}, {"text": "We can see that the proposed method is not very sensitive to the number of clusters, across the set of increasing values we considered, and its performance becomes very stable after the cluster number reaches 20.", "labels": [], "entities": []}, {"text": "We conducted cross-domain sentiment classification on the widely used Amazon product reviews, which contains review documents distributed in four categories: Books(B), DVD(D), Electronics(E) and Kitchen(K).", "labels": [], "entities": [{"text": "cross-domain sentiment classification", "start_pos": 13, "end_pos": 50, "type": "TASK", "confidence": 0.7226475675900778}, {"text": "Amazon product reviews", "start_pos": 70, "end_pos": 92, "type": "DATASET", "confidence": 0.8690081437428793}]}, {"text": "Each category contains 1000 positive and 1000 negative reviews.", "labels": [], "entities": []}, {"text": "We constructed 12 cross-domain sentiment classification tasks, one for each source-target domain pair, B2D, B2E, B2K, D2B, D2E, D2K, E2B, E2D, E2K, K2B, K2D, K2E.", "labels": [], "entities": [{"text": "cross-domain sentiment classification", "start_pos": 18, "end_pos": 55, "type": "TASK", "confidence": 0.7059787511825562}]}, {"text": "For example, the task B2D means that we use the Books reviews as the source domain and the DVD reviews as the target domain.", "labels": [], "entities": [{"text": "B2D", "start_pos": 22, "end_pos": 25, "type": "METRIC", "confidence": 0.9685162901878357}, {"text": "Books reviews", "start_pos": 48, "end_pos": 61, "type": "DATASET", "confidence": 0.9800808429718018}, {"text": "DVD reviews", "start_pos": 91, "end_pos": 102, "type": "DATASET", "confidence": 0.8998692333698273}]}, {"text": "For each pair of domains, we built a vocabulary with both unigram and bigram features extracted from all the documents of the two domains, and then represented each review document as a feature vector with term frequency values.", "labels": [], "entities": []}, {"text": "For each of the twelve cross-domain sentiment classification tasks on Amazon product reviews, we used all the source reviews as labeled data and randomly selected 100 target reviews as labeled data while treating the rest as unlabeled test data.", "labels": [], "entities": [{"text": "cross-domain sentiment classification tasks", "start_pos": 23, "end_pos": 66, "type": "TASK", "confidence": 0.7130536660552025}]}, {"text": "For the baseline method BOW, we used binary indicator values as features, which has been shown to work better than the term-frequency features for sentiment classification tasks ().", "labels": [], "entities": [{"text": "BOW", "start_pos": 24, "end_pos": 27, "type": "METRIC", "confidence": 0.5540839433670044}, {"text": "sentiment classification tasks", "start_pos": 147, "end_pos": 177, "type": "TASK", "confidence": 0.9279384215672811}]}, {"text": "For all the other representation learning based methods, we selected the dimension size of learned representation according to the average results over 3 runs on the B2D task.", "labels": [], "entities": []}, {"text": "The dimension sizes selected for the methods PLSA, FDLDA, SCL, CPSP, and SWC are 10, 50, 50, 100 and 10, respectively.", "labels": [], "entities": [{"text": "FDLDA", "start_pos": 51, "end_pos": 56, "type": "METRIC", "confidence": 0.5146840214729309}]}, {"text": "We then repeated each experiment 10 times based on different random selections of 100 labeled reviews from the target domain to compare the six methods on the twelve tasks.", "labels": [], "entities": []}, {"text": "The average classification results are reported in.", "labels": [], "entities": []}, {"text": "We can see that the PLSA and FDLDA methods do not show much advantage over the baseline method BOW.", "labels": [], "entities": [{"text": "FDLDA", "start_pos": 29, "end_pos": 34, "type": "METRIC", "confidence": 0.9618064761161804}, {"text": "BOW", "start_pos": 95, "end_pos": 98, "type": "METRIC", "confidence": 0.926633894443512}]}, {"text": "CPSP performs better than PLSA and BOW on many of the twelve tasks, but with small advantages, while SCL outperforms CPSP on most tasks.", "labels": [], "entities": [{"text": "BOW", "start_pos": 35, "end_pos": 38, "type": "METRIC", "confidence": 0.9741066694259644}]}, {"text": "The proposed method SWC however demonstrates a clear advantage overall the other methods and produces the best results on all the twelve tasks.", "labels": [], "entities": []}, {"text": "We also conducted sensitivity analysis over the proposed approach regarding the number of clusters on the twelve cross-domain sentiment classification tasks, by testing a set of cluster number values m = {5, 10, 20, 50, 100}.", "labels": [], "entities": [{"text": "cross-domain sentiment classification tasks", "start_pos": 113, "end_pos": 156, "type": "TASK", "confidence": 0.739935614168644}]}, {"text": "The average results are plotted in.", "labels": [], "entities": []}, {"text": "Similar as before, we can seethe proposed approach has stable performance across the set of different cluster numbers.", "labels": [], "entities": []}, {"text": "Moreover, these results also clearly show that domain adaptation is not asymmetric process, as we can see it is easier to conduct domain adaptation from the source domain Books to the target domain Kitchen (with an accuracy around 82%), but it is more difficult to make domain adaptation from the source domain Kitchen to the target domain Books (with an ac-   curacy around 75%).", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 47, "end_pos": 64, "type": "TASK", "confidence": 0.8711908161640167}, {"text": "domain adaptation", "start_pos": 130, "end_pos": 147, "type": "TASK", "confidence": 0.7296846508979797}, {"text": "accuracy", "start_pos": 215, "end_pos": 223, "type": "METRIC", "confidence": 0.9974047541618347}, {"text": "domain adaptation", "start_pos": 270, "end_pos": 287, "type": "TASK", "confidence": 0.7018851637840271}, {"text": "Books", "start_pos": 340, "end_pos": 345, "type": "DATASET", "confidence": 0.9623253345489502}, {"text": "ac-   curacy", "start_pos": 355, "end_pos": 367, "type": "METRIC", "confidence": 0.955709179242452}]}, {"text": "It also shows that the degree of relatedness of the two domains is an important factor for the effectiveness of knowledge adaptation.", "labels": [], "entities": [{"text": "knowledge adaptation", "start_pos": 112, "end_pos": 132, "type": "TASK", "confidence": 0.8127616941928864}]}, {"text": "For example, one can see that it is much easier to conduct domain adaptation from Kitchen to Electronics (with an accuracy around 84%) than from Kitchen to Books (with an accuracy around 75%), as Kitchen is more closely related to Electronics than Books.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 59, "end_pos": 76, "type": "TASK", "confidence": 0.7291908860206604}, {"text": "accuracy", "start_pos": 114, "end_pos": 122, "type": "METRIC", "confidence": 0.9955224990844727}, {"text": "accuracy", "start_pos": 171, "end_pos": 179, "type": "METRIC", "confidence": 0.9746866822242737}]}], "tableCaptions": [{"text": " Table 1: Average results (accuracy\u00b1standard deviation) for three cross-domain document categorization tasks on  Reuters-21578 dataset.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 27, "end_pos": 35, "type": "METRIC", "confidence": 0.9979346990585327}, {"text": "Reuters-21578 dataset", "start_pos": 113, "end_pos": 134, "type": "DATASET", "confidence": 0.9854588210582733}]}, {"text": " Table 2: Average results (accuracy\u00b1standard deviation) for twelve cross-domain sentiment classification tasks on  Amazon product reviews.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 27, "end_pos": 35, "type": "METRIC", "confidence": 0.998540997505188}, {"text": "cross-domain sentiment classification tasks", "start_pos": 67, "end_pos": 110, "type": "TASK", "confidence": 0.7412908971309662}]}]}