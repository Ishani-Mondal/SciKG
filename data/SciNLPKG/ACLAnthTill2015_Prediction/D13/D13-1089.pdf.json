{"title": [{"text": "Scaling to Large 3 Data: An efficient and effective method to compute Distributional Thesauri", "labels": [], "entities": []}], "abstractContent": [{"text": "We introduce anew highly scalable approach for computing Distributional Thesauri (DTs).", "labels": [], "entities": []}, {"text": "By employing pruning techniques and a distributed framework, we make the computation for very large corpora feasible on comparably small computational resources.", "labels": [], "entities": []}, {"text": "We demonstrate this by releasing a DT for the whole vocabulary of Google Books syntactic n-grams.", "labels": [], "entities": [{"text": "DT", "start_pos": 35, "end_pos": 37, "type": "METRIC", "confidence": 0.778760552406311}]}, {"text": "Evaluating against lexical resources using two measures, we show that our approach produces higher quality DTs than previous approaches , and is thus preferable in terms of speed and quality for large corpora.", "labels": [], "entities": [{"text": "speed", "start_pos": 173, "end_pos": 178, "type": "METRIC", "confidence": 0.9853041768074036}]}], "introductionContent": [{"text": "Using larger data to estimate models for machine learning applications as well as for applications of Natural Language Processing (NLP) has repeatedly shown to be advantageous, see e.g. (. In this work, we tackle the influence of corpus size for building a distributional thesaurus.", "labels": [], "entities": []}, {"text": "Especially, we shed light on the interaction of similarity measures and corpus size, as well as aspects of scalability.", "labels": [], "entities": []}, {"text": "We shortly introduce the JoBimText framework for distributional semantics and show its scalability for large corpora.", "labels": [], "entities": []}, {"text": "For the computation of the data we follow the MapReduce () paradigm.", "labels": [], "entities": []}, {"text": "The computation of similarities between terms becomes challenging on large corpora, as both the numbers of terms to be compared and the number of context features increases.", "labels": [], "entities": []}, {"text": "This makes standard similarity calculations as proposed in) computationally infeasible.", "labels": [], "entities": []}, {"text": "These approaches first calculate an information measure between each word and the according context and then calculate the similarity between all words, based on the information measure for all shared contexts.", "labels": [], "entities": []}], "datasetContent": [{"text": "The evaluation is performed using a recent dump of English Wikipedia, containing 36 million sentences and a newspaper corpus, compiled from 120 million sentences (about 2 Gigawords) from Leipzig Corpora Collection () and the Gigaword corpus (Parker et al., 2011).", "labels": [], "entities": [{"text": "Leipzig Corpora Collection", "start_pos": 187, "end_pos": 213, "type": "DATASET", "confidence": 0.9575009147326151}, {"text": "Gigaword corpus", "start_pos": 225, "end_pos": 240, "type": "DATASET", "confidence": 0.8892808854579926}]}, {"text": "The DTs are based on collapsed dependencies from the Stanford Parser () in the holing operation.", "labels": [], "entities": [{"text": "Stanford Parser", "start_pos": 53, "end_pos": 68, "type": "DATASET", "confidence": 0.888657808303833}]}, {"text": "For all DTs we use the pruning parameters s=0, p=1000 and w=1000.", "labels": [], "entities": []}, {"text": "Ina final evaluation, we use the syntactic n-grams built from Google Books (.", "labels": [], "entities": []}, {"text": "To show the impact of corpus size, we downsampled our corpora to 10 million, 1 million and 100,000 sentences.", "labels": [], "entities": []}, {"text": "We compare our results against DTs calculated using Lin's (Lin, 1998) measure and the best measure proposed by (see Table 1).", "labels": [], "entities": []}, {"text": "Our evaluation is performed using the same 1000 frequent and 1000 infrequent nouns as previously employed by.", "labels": [], "entities": []}, {"text": "We create a gold standard, by extracting reasonable entries of these 2000 nouns using Roget's 1911 thesaurus, Moby Thesaurus, Merriam Webster's Thesaurus, the Big Huge Thesaurus and the OpenOffice Thesaurus and employ the inverse ranking measure to evaluate the DTs.", "labels": [], "entities": [{"text": "Merriam Webster's Thesaurus", "start_pos": 126, "end_pos": 153, "type": "DATASET", "confidence": 0.7712301164865494}]}, {"text": "Furthermore, we introduce a WordNet-based method.", "labels": [], "entities": []}, {"text": "To calculate the similarity between two terms, we use the WordNet::Similarity path) measure.", "labels": [], "entities": [{"text": "WordNet::Similarity path) measure", "start_pos": 58, "end_pos": 91, "type": "DATASET", "confidence": 0.7558271629469735}]}, {"text": "While its absolute scores are hard to interpret due to inhomogenity in the granularity of WordNet, they are well-suited for relative comparison.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 90, "end_pos": 97, "type": "DATASET", "confidence": 0.969176709651947}]}, {"text": "The score between two terms is inversely proportional to the shortest path between all the synsets of both terms.", "labels": [], "entities": []}, {"text": "The highest possible score is one, if two terms share a synset.", "labels": [], "entities": []}, {"text": "We compare the average score of the top five (or ten) entries in the DT for each of the 2000 selected words for our comparison.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Comparing results for different corpora.", "labels": [], "entities": []}]}