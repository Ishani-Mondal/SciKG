{"title": [{"text": "A temporal model of text periodicities using Gaussian Processes", "labels": [], "entities": [{"text": "text periodicities", "start_pos": 20, "end_pos": 38, "type": "TASK", "confidence": 0.7589591145515442}]}], "abstractContent": [{"text": "Temporal variations of text are usually ignored in NLP applications.", "labels": [], "entities": []}, {"text": "However, text use changes with time, which can affect many applications.", "labels": [], "entities": []}, {"text": "In this paper we model periodic distributions of words overtime.", "labels": [], "entities": []}, {"text": "Focus-ing on hashtag frequency in Twitter, we first automatically identify the periodic patterns.", "labels": [], "entities": []}, {"text": "We use this for regression in order to forecast the volume of a hashtag based on past data.", "labels": [], "entities": []}, {"text": "We use Gaussian Processes, a state-of-the-art bayesian non-parametric model, with a novel periodic kernel.", "labels": [], "entities": []}, {"text": "We demonstrate this in a text classification setting, assigning the tweet hashtag based on the rest of its text.", "labels": [], "entities": [{"text": "text classification", "start_pos": 25, "end_pos": 44, "type": "TASK", "confidence": 0.7206448167562485}]}, {"text": "This method shows significant improvements over competitive baselines.", "labels": [], "entities": []}], "introductionContent": [{"text": "Temporal changes in text corpora are central to our understanding of many linguistic and social phenomena.", "labels": [], "entities": []}, {"text": "Social Media platforms and the digitalization of libraries provides avast body of timestamped data.", "labels": [], "entities": []}, {"text": "This allows studying of the complex temporal patterns exhibited by text usage including highly non-stationary distributions and periodicities.", "labels": [], "entities": []}, {"text": "However, temporal effects have been mostly ignored by previous work on text analysis or at best dealt with by making strong assumptions such as smoothly varying parameters with time) or modelled using a simple uni-modal distribution ().", "labels": [], "entities": [{"text": "text analysis", "start_pos": 71, "end_pos": 84, "type": "TASK", "confidence": 0.8168506026268005}]}, {"text": "This paper develops a temporal model for classifying microblog posts which explicitly incorporates multimodal periodic behaviours using Gaussian Processes (GPs).", "labels": [], "entities": [{"text": "classifying microblog posts", "start_pos": 41, "end_pos": 68, "type": "TASK", "confidence": 0.8409421245257059}]}, {"text": "We expect text usage to follow multiple periodicities at different scales.", "labels": [], "entities": []}, {"text": "For example, people on Social Media might talk about different topics during and after work on weekdays, talk every Friday about the weekend ahead, or comment about their favorite weekly TV show during its airtime.", "labels": [], "entities": []}, {"text": "Given this, text frequencies will display periodic patterns.", "labels": [], "entities": []}, {"text": "This applies to other text related quantities like cooccurrence values or topic distributions overtime, as well as applications outside NLP like user behaviour.", "labels": [], "entities": []}, {"text": "Modelling temporal patterns and periodicities can be useful to tasks like text classification.", "labels": [], "entities": [{"text": "text classification", "start_pos": 74, "end_pos": 93, "type": "TASK", "confidence": 0.8600085973739624}]}, {"text": "For example a tweet containing 'music' is normally attributed to a general hashtag about music like #np (now playing).", "labels": [], "entities": []}, {"text": "However, knowing time, if it occurs during the (weekly periodic) airtime of 'American Idol' it is more likely for it to belong to #americanidol or if its mentioned in the days building up to the Video Music Awards to be assigned to #VMA.", "labels": [], "entities": [{"text": "VMA", "start_pos": 233, "end_pos": 236, "type": "DATASET", "confidence": 0.8490408658981323}]}, {"text": "In NLP, temporal models have treated time in overly simplistic ways and without regard to periodicities.", "labels": [], "entities": []}, {"text": "We propose a model that first broadly identifies several types of temporal patterns: a) periodic, b) constant in time, c) falling out of use after enjoying a brief spell of popularity (e.g. internet memes, news).", "labels": [], "entities": []}, {"text": "This is performed automatically only using training data and makes no assumptions on the existence or the length of the periods we aim to model.", "labels": [], "entities": []}, {"text": "We demonstrate the approach by modelling frequencies of hashtag occurrences in Twitter.", "labels": [], "entities": []}, {"text": "Hashtags are user-generated labels included in tweets by their authors in order to assign them to a conversation and can be considered as a proxy for topics.", "labels": [], "entities": []}, {"text": "To this end, we make use of Gaussian Processes (GP)), a Bayesian non-parametric model for regression.", "labels": [], "entities": []}, {"text": "Using the Bayesian evidence we automatically perform model selection to classify temporal patterns.", "labels": [], "entities": []}, {"text": "We aim to use the most suitable model for extrapolation, i.e. predicting future values from past observations.", "labels": [], "entities": []}, {"text": "The GP is fully defined by the covariance structure assumed between the observed points, and its hyperparameters, which can be automatically learned from data.", "labels": [], "entities": []}, {"text": "We also introduce anew kernel suitable to model the periodic behaviour we observe in text: periods of low frequency followed by bursts at regular time intervals.", "labels": [], "entities": []}, {"text": "We demonstrate that the GP approach is more general and gives better results than frequentist models (e.g. autoregressive models) because it incorporates uncertainty explicitly and elegantly, in addition to automatic model selection and parameter fitting.", "labels": [], "entities": []}, {"text": "To demonstrate the practical importance of our approach, we use our GP prediction as a prior in a Na\u00a8\u0131veNa\u00a8\u0131ve Bayes model for text classification showing improvements over baselines which do not account for temporal periodicities.", "labels": [], "entities": [{"text": "GP prediction", "start_pos": 68, "end_pos": 81, "type": "METRIC", "confidence": 0.7702437043190002}, {"text": "text classification", "start_pos": 127, "end_pos": 146, "type": "TASK", "confidence": 0.7972539365291595}]}, {"text": "Our approach extends to more general uses, e.g. to discriminative text regression and classification.", "labels": [], "entities": [{"text": "discriminative text regression", "start_pos": 51, "end_pos": 81, "type": "TASK", "confidence": 0.5906959672768911}]}, {"text": "More broadly, we aim to establish GPs as a state-of-the-art model for regression and classification in NLP.", "labels": [], "entities": []}, {"text": "To our knowledge, this is the first paper to use GP regression for forecasting and model selection within a NLP task.", "labels": [], "entities": [{"text": "forecasting", "start_pos": 67, "end_pos": 78, "type": "TASK", "confidence": 0.9779579043388367}, {"text": "model selection", "start_pos": 83, "end_pos": 98, "type": "TASK", "confidence": 0.6693893820047379}]}, {"text": "All the hashtag time series data and the implementation of the PS kernel in the popular opensource Gaussian Processes packages GPML 1 and GPy 2 are available on the author's website 3 .", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Negative Log Marginal Likelihood (NLML)  shows the best fitted model for the time series in", "labels": [], "entities": [{"text": "Negative Log Marginal Likelihood", "start_pos": 10, "end_pos": 42, "type": "TASK", "confidence": 0.6592372059822083}]}, {"text": " Table 2: NRMSE shows the best performance for forecasting and NLML shows the best model for all the regressions  in", "labels": [], "entities": [{"text": "NRMSE", "start_pos": 10, "end_pos": 15, "type": "DATASET", "confidence": 0.6536526679992676}, {"text": "forecasting", "start_pos": 47, "end_pos": 58, "type": "TASK", "confidence": 0.9645105600357056}]}, {"text": " Table 3: Sample hashtags for each category. The last line  shows the total number of hashtags of each type.", "labels": [], "entities": []}, {"text": " Table 5: Example of tweet classification using the Na\u00a8\u0131veNa\u00a8\u0131ve Bayes model with the two different priors (E -empirical, P - GP forecast). Rank shows the rank in probability of the correct class (hashtag) under the model. Time is G.M.T.", "labels": [], "entities": [{"text": "tweet classification", "start_pos": 21, "end_pos": 41, "type": "TASK", "confidence": 0.7876063585281372}, {"text": "P - GP forecast", "start_pos": 122, "end_pos": 137, "type": "METRIC", "confidence": 0.7743143439292908}, {"text": "Time", "start_pos": 223, "end_pos": 227, "type": "METRIC", "confidence": 0.9183634519577026}, {"text": "G.M.T", "start_pos": 231, "end_pos": 236, "type": "METRIC", "confidence": 0.9186647534370422}]}]}