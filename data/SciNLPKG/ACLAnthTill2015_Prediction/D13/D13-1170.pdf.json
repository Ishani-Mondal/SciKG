{"title": [{"text": "Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank", "labels": [], "entities": []}], "abstractContent": [{"text": "Semantic word spaces have been very useful but cannot express the meaning of longer phrases in a principled way.", "labels": [], "entities": []}, {"text": "Further progress towards understanding compositionality in tasks such as sentiment detection requires richer supervised training and evaluation resources and more powerful models of composition.", "labels": [], "entities": [{"text": "sentiment detection", "start_pos": 73, "end_pos": 92, "type": "TASK", "confidence": 0.9640273153781891}]}, {"text": "To remedy this, we introduce a Sentiment Treebank.", "labels": [], "entities": [{"text": "Sentiment Treebank", "start_pos": 31, "end_pos": 49, "type": "DATASET", "confidence": 0.7697974443435669}]}, {"text": "It includes fine grained sentiment labels for 215,154 phrases in the parse trees of 11,855 sentences and presents new challenges for sentiment composition-ality.", "labels": [], "entities": []}, {"text": "To address them, we introduce the Recursive Neural Tensor Network.", "labels": [], "entities": [{"text": "Recursive Neural Tensor", "start_pos": 34, "end_pos": 57, "type": "TASK", "confidence": 0.6100673675537109}]}, {"text": "When trained on the new treebank, this model out-performs all previous methods on several met-rics.", "labels": [], "entities": []}, {"text": "It pushes the state of the art in single sentence positive/negative classification from 80% up to 85.4%.", "labels": [], "entities": [{"text": "single sentence positive/negative classification", "start_pos": 34, "end_pos": 82, "type": "TASK", "confidence": 0.636182482043902}]}, {"text": "The accuracy of predicting fine-grained sentiment labels for all phrases reaches 80.7%, an improvement of 9.7% over bag of features baselines.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.9995142221450806}, {"text": "predicting fine-grained sentiment labels", "start_pos": 16, "end_pos": 56, "type": "TASK", "confidence": 0.8617086857557297}]}, {"text": "Lastly, it is the only model that can accurately capture the effects of negation and its scope at various tree levels for both positive and negative phrases.", "labels": [], "entities": []}], "introductionContent": [{"text": "Semantic vector spaces for single words have been widely used as features).", "labels": [], "entities": []}, {"text": "Because they cannot capture the meaning of longer phrases properly, compositionality in semantic vector spaces has recently received a lot of attention (.", "labels": [], "entities": []}, {"text": "However, progress is held back by the current lack of large and labeled compositionality resources and Figure 1: Example of the Recursive Neural Tensor Network accurately predicting 5 sentiment classes, very negative to very positive (--, -, 0, +, + +), at every node of a parse tree and capturing the negation and its scope in this sentence.", "labels": [], "entities": []}, {"text": "models to accurately capture the underlying phenomena presented in such data.", "labels": [], "entities": []}, {"text": "To address this need, we introduce the Stanford Sentiment Treebank and a powerful Recursive Neural Tensor Network that can accurately predict the compositional semantic effects present in this new corpus.", "labels": [], "entities": [{"text": "Stanford Sentiment Treebank", "start_pos": 39, "end_pos": 66, "type": "DATASET", "confidence": 0.8806507786115011}]}, {"text": "The Stanford Sentiment Treebank is the first corpus with fully labeled parse trees that allows fora complete analysis of the compositional effects of sentiment in language.", "labels": [], "entities": [{"text": "Stanford Sentiment Treebank", "start_pos": 4, "end_pos": 31, "type": "DATASET", "confidence": 0.804974377155304}]}, {"text": "The corpus is based on the dataset introduced by Pang and and consists of 11,855 single sentences extracted from movie reviews.", "labels": [], "entities": [{"text": "Pang", "start_pos": 49, "end_pos": 53, "type": "DATASET", "confidence": 0.9498813152313232}]}, {"text": "It was parsed with the Stanford parser ( and includes a total of 215,154 unique phrases from those parse trees, each annotated by 3 human judges.", "labels": [], "entities": [{"text": "Stanford parser", "start_pos": 23, "end_pos": 38, "type": "DATASET", "confidence": 0.9162553250789642}]}, {"text": "This new dataset allows us to analyze the intricacies of sentiment and to capture complex linguistic phenomena.", "labels": [], "entities": []}, {"text": "shows one of the many examples with clear compositional structure.", "labels": [], "entities": []}, {"text": "The granularity and size of this dataset will enable the community to train compositional models that are based on supervised and structured machine learning techniques.", "labels": [], "entities": []}, {"text": "While there are several datasets with document and chunk labels available, there is a need to better capture sentiment from short comments, such as Twitter data, which provide less overall signal per document.", "labels": [], "entities": []}, {"text": "In order to capture the compositional effects with higher accuracy, we propose anew model called the Recursive Neural Tensor Network (RNTN).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 58, "end_pos": 66, "type": "METRIC", "confidence": 0.9937946796417236}]}, {"text": "Recursive Neural Tensor Networks take as input phrases of any length.", "labels": [], "entities": []}, {"text": "They represent a phrase through word vectors and a parse tree and then compute vectors for higher nodes in the tree using the same tensor-based composition function.", "labels": [], "entities": []}, {"text": "We compare to several supervised, compositional models such as standard recursive neural networks (RNN)), matrix-vector RNNs ( , and baselines such as neural networks that ignore word order, Naive Bayes (NB), bi-gram NB and SVM.", "labels": [], "entities": []}, {"text": "All models get a significant boost when trained with the new dataset but the RNTN obtains the highest performance with 80.7% accuracy when predicting finegrained sentiment for all nodes.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 125, "end_pos": 133, "type": "METRIC", "confidence": 0.9990382194519043}]}, {"text": "Lastly, we use a test set of positive and negative sentences and their respective negations to show that, unlike bag of words models, the RNTN accurately captures the sentiment change and scope of negation.", "labels": [], "entities": []}, {"text": "RNTNs also learn that sentiment of phrases following the contrastive conjunction 'but' dominates.", "labels": [], "entities": []}, {"text": "The complete training and testing code, a live demo and the Stanford Sentiment Treebank dataset are available at http://nlp.stanford.edu/ sentiment.", "labels": [], "entities": [{"text": "Stanford Sentiment Treebank dataset", "start_pos": 60, "end_pos": 95, "type": "DATASET", "confidence": 0.9377739280462265}]}], "datasetContent": [{"text": "We include two types of analyses.", "labels": [], "entities": []}, {"text": "The first type includes several large quantitative evaluations on the test set.", "labels": [], "entities": []}, {"text": "The second type focuses on two linguistic phenomena that are important in sentiment.", "labels": [], "entities": []}, {"text": "For all models, we use the dev set and crossvalidate over regularization of the weights, word vector size as well as learning rate and minibatch size for AdaGrad.", "labels": [], "entities": []}, {"text": "Optimal performance for all models was achieved at word vector sizes between 25 and 35 dimensions and batch sizes between 20 and 30.", "labels": [], "entities": []}, {"text": "Performance decreased at larger or smaller vector and batch sizes.", "labels": [], "entities": []}, {"text": "This indicates that the RNTN does not outperform the standard RNN due to simply having more parameters.", "labels": [], "entities": []}, {"text": "The MV-RNN has orders of magnitudes more parameters than any other model due to the word matrices.", "labels": [], "entities": []}, {"text": "The RNTN would usually achieve its best performance on the dev set after training for 3 -5 hours.", "labels": [], "entities": [{"text": "RNTN", "start_pos": 4, "end_pos": 8, "type": "DATASET", "confidence": 0.7562892436981201}]}, {"text": "Initial experiments: Accuracy for fine grained (5-class) and binary predictions at the sentence level (root) and for all nodes.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 21, "end_pos": 29, "type": "METRIC", "confidence": 0.9886242151260376}]}, {"text": "showed that the recursive models worked significantly worse (over 5% drop in accuracy) when no nonlinearity was used.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 77, "end_pos": 85, "type": "METRIC", "confidence": 0.9993051290512085}]}, {"text": "We use f = tanh in all experiments.", "labels": [], "entities": []}, {"text": "We compare to commonly used methods that use bag of words features with Naive Bayes and SVMs, as well as Naive Bayes with bag of bigram features.", "labels": [], "entities": []}, {"text": "We abbreviate these with NB, SVM and biNB.", "labels": [], "entities": []}, {"text": "We also compare to a model that averages neural word vectors and ignores word order (VecAvg).", "labels": [], "entities": []}, {"text": "The sentences in the treebank were split into a train (8544), dev (1101) and test splits (2210) and these splits are made available with the data release.", "labels": [], "entities": []}, {"text": "We also analyze performance on only positive and negative sentences, ignoring the neutral class.", "labels": [], "entities": []}, {"text": "This filters about 20% of the data with the three sets having 6920/872/1821 sentences.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Accuracy for fine grained (5-class) and binary  predictions at the sentence level (root) and for all nodes.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9974791407585144}]}, {"text": " Table 2: Accuracy of negation detection. Negated posi- tive is measured as correct sentiment inversions. Negated  negative is measured as increases in positive activations.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9872469902038574}, {"text": "negation detection", "start_pos": 22, "end_pos": 40, "type": "TASK", "confidence": 0.9823166728019714}]}]}