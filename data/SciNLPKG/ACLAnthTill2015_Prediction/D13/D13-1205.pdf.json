{"title": [{"text": "Cross-Lingual Discriminative Learning of Sequence Models with Posterior Regularization", "labels": [], "entities": []}], "abstractContent": [{"text": "We present a framework for cross-lingual transfer of sequence information from a resource-rich source language to a resource-impoverished target language that incorporates soft constraints via posterior regularization.", "labels": [], "entities": [{"text": "cross-lingual transfer of sequence information", "start_pos": 27, "end_pos": 73, "type": "TASK", "confidence": 0.7725291669368743}]}, {"text": "To this end, we use automatically word aligned bitext between the source and target language pair, and learn a discriminative conditional random field model on the target side.", "labels": [], "entities": []}, {"text": "Our posterior regularization constraints are derived from simple intuitions about the task at hand and from cross-lingual alignment information.", "labels": [], "entities": []}, {"text": "We show improvements over strong baselines for two tasks: part-of-speech tagging and named-entity segmentation.", "labels": [], "entities": [{"text": "part-of-speech tagging", "start_pos": 58, "end_pos": 80, "type": "TASK", "confidence": 0.7862842977046967}, {"text": "named-entity segmentation", "start_pos": 85, "end_pos": 110, "type": "TASK", "confidence": 0.7117897272109985}]}], "introductionContent": [{"text": "Supervised systems for NLP tasks are available fora handful of languages.", "labels": [], "entities": [{"text": "NLP tasks", "start_pos": 23, "end_pos": 32, "type": "TASK", "confidence": 0.9060762822628021}]}, {"text": "These systems achieve high accuracy for many applications; a variety of robust algorithms to train them from labeled data have been developed.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 27, "end_pos": 35, "type": "METRIC", "confidence": 0.9964021444320679}]}, {"text": "Here, we focus on learning sequence models for the languages that lack annotated resources.", "labels": [], "entities": []}, {"text": "For a given resource-poor target language of interest, we assume that parallel data with a resource-rich source language exists.", "labels": [], "entities": []}, {"text": "With the help of this bitext and a supervised system in the source language, we infer constraints over the label distribution in the target language, and train a discriminative model using posterior regularization (.", "labels": [], "entities": []}, {"text": "Cross-lingual learning of structured prediction models via parallel data has been applied for several natural language processing problems, including partof-speech (POS) tagging), syntactic parsing () and named-entity recognition ().", "labels": [], "entities": [{"text": "partof-speech (POS) tagging", "start_pos": 150, "end_pos": 177, "type": "TASK", "confidence": 0.6133791923522949}, {"text": "syntactic parsing", "start_pos": 180, "end_pos": 197, "type": "TASK", "confidence": 0.7488790452480316}, {"text": "named-entity recognition", "start_pos": 205, "end_pos": 229, "type": "TASK", "confidence": 0.749888688325882}]}, {"text": "These methods are useful in several ways.", "labels": [], "entities": []}, {"text": "First, they help in fast prototyping of natural language systems for new languages that do not boast human annotations.", "labels": [], "entities": []}, {"text": "Second, the output of such systems could be used to bootstrap more extensive human annotation projects).", "labels": [], "entities": []}, {"text": "Finally, they are significantly more accurate than purely unsupervised systems.", "labels": [], "entities": []}, {"text": "Recently, presented a technique for coupling token constraints derived from projected cross-lingual information and type constraints derived from noisy tag dictionaries to learn POS taggers.", "labels": [], "entities": [{"text": "POS taggers", "start_pos": 178, "end_pos": 189, "type": "TASK", "confidence": 0.6765367686748505}]}, {"text": "Although this technique resulted in state-ofthe-art weakly supervised taggers, the authors used a heuristic to combine the aforementioned two sources of constraints: the dictionary constraints pruned the tagger's search space, and the intersected token-level projections were treated as hard observations.", "labels": [], "entities": []}, {"text": "On the other hand, presented a framework for learning weakly-supervised systems (in their case, dependency parsers) that incorporated alignment-based information too, but used the crosslingual information only as soft constraints, via posterior regularization.", "labels": [], "entities": []}, {"text": "The advantage of this framework lay in the fact that the projections were only trusted to a certain degree, determined by a strength hyperparameter, which unfortunately the authors did not have an elegant way to tune.", "labels": [], "entities": []}, {"text": "In this paper, we exploit the better aspects of these two lines of work: first, we extend the framework of T\u00e4ckstr\u00f6m et al. by treating the alignment-based projections only as soft constraints (see \u00a73.4); second, we choose the constraint strength by utilizing the tag ambiguity of tokens fora given resource-poor language (see \u00a76.1).", "labels": [], "entities": []}, {"text": "Other than validating our framework on part-ofspeech tagging, we experiment on named-entity segmentation in a cross-lingual framework.", "labels": [], "entities": [{"text": "part-ofspeech tagging", "start_pos": 39, "end_pos": 60, "type": "TASK", "confidence": 0.6728402376174927}]}, {"text": "For this task, we present a novel method to perform highprecision phrase-level entity transfer ( \u00a75.2.2); we also provide ways to balance precision and recall with posterior regularization ( \u00a76.2) by incorporating intuitive soft constraints during learning.", "labels": [], "entities": [{"text": "phrase-level entity transfer", "start_pos": 66, "end_pos": 94, "type": "TASK", "confidence": 0.5994950632254282}, {"text": "precision", "start_pos": 138, "end_pos": 147, "type": "METRIC", "confidence": 0.9978506565093994}, {"text": "recall", "start_pos": 152, "end_pos": 158, "type": "METRIC", "confidence": 0.9719879627227783}]}, {"text": "We measure performance on standard benchmark datasets for both of these tasks, and report improvements over state-of-the-art baselines.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we provide details about taskspecific implementations of the supervised sourceside model and the word-alignment filtering techniques (steps 2 and 3 in Algorithm 1 respectively); we also briefly describe the setup of the cross-lingual experiments for each task.", "labels": [], "entities": [{"text": "word-alignment filtering", "start_pos": 114, "end_pos": 138, "type": "TASK", "confidence": 0.760593444108963}]}, {"text": "We use a CRF with the same feature set and BIO encoding for the cross-lingual models as the sourceside NER model.", "labels": [], "entities": [{"text": "BIO encoding", "start_pos": 43, "end_pos": 55, "type": "METRIC", "confidence": 0.982047826051712}]}, {"text": "We compare our approach (\"PR\" in) to a baseline (\"BASE\" in) which treats the projected annotations as fully observed.", "labels": [], "entities": [{"text": "BASE", "start_pos": 50, "end_pos": 54, "type": "METRIC", "confidence": 0.9974455833435059}]}, {"text": "The PR model treats the projected NE spans of a sentence as observed, and allows all labels on the remaining tokens.", "labels": [], "entities": []}, {"text": "Since the \"O\" tag is never seen, an unconstrained model would learn to never predict it.", "labels": [], "entities": []}, {"text": "We add two features that fire when the current word is tagged \"O\": a bias feature and a feature that fires when the automatic POS tag is a proper noun.", "labels": [], "entities": []}, {"text": "We setup Q so the desired expectations are at least 0.98 and at most 0.1 for these constraint features respectively.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: POS tagging results. BASE represents the best  model of T\u00e4ckstr\u00f6m et al. (2013). PR is a system with  the same features but with relaxed constraints. BASE+  and PR+ add additional model features (see  \u00a75.2.3).  *  in- dicates improvements over the previous state of the art  (BASE), and bold values indicate the best score for a lan- guage. \"Avg\" indicates averaged results for all 17 lan- guages, while \"-zh-ar\" shows averaged results without  Chinese and Arabic.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 10, "end_pos": 21, "type": "TASK", "confidence": 0.7309858500957489}, {"text": "BASE", "start_pos": 31, "end_pos": 35, "type": "METRIC", "confidence": 0.9020341038703918}, {"text": "BASE", "start_pos": 160, "end_pos": 164, "type": "METRIC", "confidence": 0.891558051109314}, {"text": "BASE", "start_pos": 286, "end_pos": 290, "type": "METRIC", "confidence": 0.9178231358528137}, {"text": "Avg", "start_pos": 352, "end_pos": 355, "type": "METRIC", "confidence": 0.9738908410072327}]}, {"text": " Table 2: Result for the named-entity segmentation exper- iments. The highest score in each category is shown in  bold. Note that \"No Filtering\" still discards sentences with  no projected entities.", "labels": [], "entities": []}]}