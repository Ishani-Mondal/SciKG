{"title": [{"text": "Event Schema Induction with a Probabilistic Entity-Driven Model", "labels": [], "entities": []}], "abstractContent": [{"text": "Event schema induction is the task of learning high-level representations of complex events (e.g., a bombing) and their entity roles (e.g., perpetrator and victim) from unlabeled text.", "labels": [], "entities": [{"text": "Event schema induction is the task of learning high-level representations of complex events (e.g., a bombing) and their entity roles (e.g., perpetrator and victim) from unlabeled text", "start_pos": 0, "end_pos": 183, "type": "Description", "confidence": 0.7166155143217607}]}, {"text": "Event schemas have important connections to early NLP research on frames and scripts, as well as modern applications like template extraction.", "labels": [], "entities": [{"text": "template extraction", "start_pos": 122, "end_pos": 141, "type": "TASK", "confidence": 0.7756693959236145}]}, {"text": "Recent research suggests event schemas can be learned from raw text.", "labels": [], "entities": []}, {"text": "Inspired by a pipelined learner based on named entity coreference, this paper presents the first generative model for schema induction that integrates coreference chains into learning.", "labels": [], "entities": [{"text": "schema induction", "start_pos": 118, "end_pos": 134, "type": "TASK", "confidence": 0.7598952054977417}]}, {"text": "Our generative model is conceptually simpler than the pipelined approach and requires far less training data.", "labels": [], "entities": []}, {"text": "It also provides an interesting contrast with a recent HMM-based model.", "labels": [], "entities": []}, {"text": "We evaluate on a common dataset for template schema extraction.", "labels": [], "entities": [{"text": "template schema extraction", "start_pos": 36, "end_pos": 62, "type": "TASK", "confidence": 0.6581242283185323}]}, {"text": "Our generative model matches the pipeline's performance, and out-performs the HMM by 7 F1 points (20%).", "labels": [], "entities": [{"text": "F1", "start_pos": 87, "end_pos": 89, "type": "METRIC", "confidence": 0.9965413212776184}]}], "introductionContent": [{"text": "Early research in language understanding focused on high-level semantic representations to drive their models.", "labels": [], "entities": [{"text": "language understanding", "start_pos": 18, "end_pos": 40, "type": "TASK", "confidence": 0.7415177971124649}]}, {"text": "Many proposals, such as frames and scripts, used rich event schemas to model the situations described in text.", "labels": [], "entities": []}, {"text": "While the field has since focused on more shallow approaches, recent work on schema induction shows that event schemas might be learnable from raw text.", "labels": [], "entities": [{"text": "schema induction", "start_pos": 77, "end_pos": 93, "type": "TASK", "confidence": 0.9277453720569611}]}, {"text": "This paper continues the trend, addressing the question, can event schemas be induced from raw text without prior knowledge?", "labels": [], "entities": []}, {"text": "We present anew generative model for event schemas, and it produces state-of-the-art induction results, including a 7 F1 point gain over a different generative proposal developed in parallel with this work.", "labels": [], "entities": [{"text": "F1 point gain", "start_pos": 118, "end_pos": 131, "type": "METRIC", "confidence": 0.9388005336125692}]}, {"text": "Event schemas are unique from most work in information extraction (IE).", "labels": [], "entities": [{"text": "information extraction (IE)", "start_pos": 43, "end_pos": 70, "type": "TASK", "confidence": 0.8694543361663818}]}, {"text": "Current relation discovery () focuses on atomic facts and relations.", "labels": [], "entities": [{"text": "relation discovery", "start_pos": 8, "end_pos": 26, "type": "TASK", "confidence": 0.7307903617620468}]}, {"text": "Event schemas build relations into coherent event structures, often called templates in IE.", "labels": [], "entities": []}, {"text": "For instance, an election template jointly connects that obama won a presidential election with romney was the defeated, the election occurred in 2012, and the popular vote was 50-48.", "labels": [], "entities": []}, {"text": "The entities in these relations fill specific semantic roles, as in this template schema: Template Schema for Elections (events: nominate, vote, elect, win, declare, concede) Date: Timestamp Winner: Person Loser: Person Position: Occupation Vote: Number Traditionally, template extractors assume foreknowledge of the event schemas.", "labels": [], "entities": []}, {"text": "They know a Winner exists, and research focuses on supervised learning to extract winners from text.", "labels": [], "entities": []}, {"text": "This paper focuses on the other side of the supervision spectrum.", "labels": [], "entities": []}, {"text": "The learner receives no human input, and it first induces a schema before extracting instances of it.", "labels": [], "entities": []}, {"text": "Our proposed model contributes to a growing line of research in schema induction.", "labels": [], "entities": [{"text": "schema induction", "start_pos": 64, "end_pos": 80, "type": "TASK", "confidence": 0.9012602865695953}]}, {"text": "The majority of previous work relies on ad-hoc clustering algorithms ().", "labels": [], "entities": []}, {"text": "Chambers and Jurafsky is a pipelined approach, learning events first, and later learning syntactic patterns as fillers.", "labels": [], "entities": []}, {"text": "It requires several ad-hoc metrics and parameters, and it lacks the benefits of a formal model.", "labels": [], "entities": []}, {"text": "However, central to their algorithm is the use of coreferring entity mentions to knit events and entities together into an event schema.", "labels": [], "entities": []}, {"text": "We adapt this entity-driven approach to a single model that requires fewer parameters and far less training data.", "labels": [], "entities": []}, {"text": "Further, experiments show stateof-the-art performance.", "labels": [], "entities": []}, {"text": "Other research conducted at the time of this paper also proposes a generative model for schema induction ().", "labels": [], "entities": [{"text": "schema induction", "start_pos": 88, "end_pos": 104, "type": "TASK", "confidence": 0.8655502498149872}]}, {"text": "Theirs is not entitybased, but instead uses a sequence model (HMMbased) of verb clauses.", "labels": [], "entities": []}, {"text": "These two papers thus provide a unique opportunity to compare two very different views of document structure.", "labels": [], "entities": []}, {"text": "One is entitydriven, modeling an entity's role by its coreference chain.", "labels": [], "entities": []}, {"text": "The other is clause-driven, classifying individual clauses based on text sequence.", "labels": [], "entities": []}, {"text": "Each model makes unique assumptions, providing an interesting contrast.", "labels": [], "entities": []}, {"text": "Our entity model outperforms by 7 F1 points on a common extraction task.", "labels": [], "entities": [{"text": "F1", "start_pos": 34, "end_pos": 36, "type": "METRIC", "confidence": 0.9941372871398926}]}, {"text": "The rest of the paper describes in detail our main contributions: (1) the first entity-based generative model for schema induction, (2) a direct pipeline/formal model comparison, (3) results improving state-of-the-art performance by 20%, and (4) schema induction from the smallest amount of training data to date.", "labels": [], "entities": [{"text": "schema induction", "start_pos": 114, "end_pos": 130, "type": "TASK", "confidence": 0.8544715344905853}, {"text": "schema induction", "start_pos": 246, "end_pos": 262, "type": "TASK", "confidence": 0.9558840990066528}]}], "datasetContent": [{"text": "The corpus from the Message Understanding Conference (MUC-4) serves as the challenge text, and will ground discussion of our model.", "labels": [], "entities": [{"text": "Message Understanding Conference (MUC-4)", "start_pos": 20, "end_pos": 60, "type": "TASK", "confidence": 0.720266138513883}]}, {"text": "MUC-4 is also used by the closest previous work.", "labels": [], "entities": [{"text": "MUC-4", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.9711908102035522}]}, {"text": "It contains Latin American newswire about terrorism events, and it provides a set of hand-constructed event schemas that are traditionally called template schemas.", "labels": [], "entities": []}, {"text": "It also maps labeled templates to the text, providing a dataset for template extraction evaluations.", "labels": [], "entities": [{"text": "template extraction evaluations", "start_pos": 68, "end_pos": 99, "type": "TASK", "confidence": 0.7873822053273519}]}, {"text": "Until very recently, only extraction has been evaluated.", "labels": [], "entities": []}, {"text": "We too evaluate our model through extraction, but we also compare our learned schemas to the hand-created template schemas.", "labels": [], "entities": []}, {"text": "An example of a filled in MUC-4 template is given in.", "labels": [], "entities": [{"text": "MUC-4 template", "start_pos": 26, "end_pos": 40, "type": "DATASET", "confidence": 0.8202913999557495}]}, {"text": "The MUC-4 corpus defines six template types: Attack, Kidnapping, Bombing, Arson, Robbery, and Forced Work Stoppage.", "labels": [], "entities": [{"text": "MUC-4 corpus", "start_pos": 4, "end_pos": 16, "type": "DATASET", "confidence": 0.9646111130714417}]}, {"text": "Documents are often labeled with more than one template and type.", "labels": [], "entities": []}, {"text": "Many include multiple events at different times in different locations.", "labels": [], "entities": []}, {"text": "The corpus is particularly challenging because template schemas are inter-mixed and entities can play multiple roles across instances.", "labels": [], "entities": []}, {"text": "The training corpus contains 1300 documents, 733 of which are labeled with at least one schema.", "labels": [], "entities": []}, {"text": "567 documents are not labeled with any schemas.", "labels": [], "entities": []}, {"text": "These unlabeled documents are articles that report on non-specific political events and speeches.", "labels": [], "entities": []}, {"text": "They make the corpus particularly challenging.", "labels": [], "entities": []}, {"text": "The development and test sets each contain 200 documents.", "labels": [], "entities": []}, {"text": "Evaluating on MUC-4 has a diverse history that complicates comparison.", "labels": [], "entities": [{"text": "Evaluating on MUC-4", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.6219546496868134}]}, {"text": "The following balances comparison against previous work and enables future comparison to our results.", "labels": [], "entities": []}, {"text": "We use the Stanford CoreNLP toolkit for text processing and parsing.", "labels": [], "entities": [{"text": "Stanford CoreNLP toolkit", "start_pos": 11, "end_pos": 35, "type": "DATASET", "confidence": 0.9389397104581197}, {"text": "text processing", "start_pos": 40, "end_pos": 55, "type": "TASK", "confidence": 0.8211395740509033}]}, {"text": "We developed the models on the 1300 document MUC-4 training set.", "labels": [], "entities": [{"text": "1300 document MUC-4 training set", "start_pos": 31, "end_pos": 63, "type": "DATASET", "confidence": 0.7222148835659027}]}, {"text": "We then learned once on the entire 1700 training/dev/test set, and report extraction numbers from the inferred labels on the 200 document test set.", "labels": [], "entities": [{"text": "1700 training/dev/test set", "start_pos": 35, "end_pos": 61, "type": "DATASET", "confidence": 0.6232630695615496}, {"text": "200 document test set", "start_pos": 125, "end_pos": 146, "type": "DATASET", "confidence": 0.7686282396316528}]}, {"text": "Each experiment was repeated 10 times.", "labels": [], "entities": []}, {"text": "Reported numbers are averaged across these runs.", "labels": [], "entities": [{"text": "Reported", "start_pos": 0, "end_pos": 8, "type": "METRIC", "confidence": 0.9432618618011475}]}, {"text": "There are two structure variables for the model: the number of schema types and the number of slots under each type.", "labels": [], "entities": []}, {"text": "We searched for the optimal values on the training set before evaluating on test.", "labels": [], "entities": []}, {"text": "The hyperparameters for all evaluations were set to \u03b1 = \u03b7 = \u00b5 = \u03bd = 1, \u03d5 = .1 based on a grid search.", "labels": [], "entities": []}, {"text": "We now present the full extraction experiment that is traditionally used for evaluating MUC-4 performance.", "labels": [], "entities": [{"text": "MUC-4", "start_pos": 88, "end_pos": 93, "type": "TASK", "confidence": 0.8127753734588623}]}, {"text": "Although our learned schemas closely match gold schemas, extraction depends on how well the model can extract from diverse lexical contexts.", "labels": [], "entities": []}, {"text": "We ran inference on the full training and test sets, and used the inferred labels as schema labels.", "labels": [], "entities": []}, {"text": "These labels were mapped and evaluated against the gold MUC-4 labels as discussed in Section 5.", "labels": [], "entities": [{"text": "MUC-4 labels", "start_pos": 56, "end_pos": 68, "type": "DATASET", "confidence": 0.759790301322937}]}, {"text": "Performance is compared to two state-of-the-art induction systems.", "labels": [], "entities": []}, {"text": "Since these previous two models used different methods to map their learned schemas, we compare separately.", "labels": [], "entities": []}, {"text": "shows the template mapping evaluation with Chambers and Jurafsky (C&J).", "labels": [], "entities": []}, {"text": "shows the slot-only mapping evaluation with Cheung et al.", "labels": [], "entities": []}, {"text": "Our model achieves an F1 score comparable to C&J, and 20% higher than Cheung et al.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 22, "end_pos": 30, "type": "METRIC", "confidence": 0.988114207983017}]}, {"text": "Part of the greater increase over Cheung et al. is the mapping difference.", "labels": [], "entities": []}, {"text": "For each MUC-4 type, such as bombing, any four learned slots can map to the four MUC-4 bombing slots.", "labels": [], "entities": [{"text": "bombing", "start_pos": 29, "end_pos": 36, "type": "TASK", "confidence": 0.9671297669410706}]}, {"text": "There is no constraint that the learned slots must come from the same schema type.", "labels": [], "entities": []}, {"text": "The more strict template mapping ensures that entire schema types are mapped together, and it reduces our performance from .41 to .33.", "labels": [], "entities": []}, {"text": "Prec Recall F1 . shows that the flat relation model (no latent type variables t) is inferior to the full schema model.", "labels": [], "entities": [{"text": "F1", "start_pos": 12, "end_pos": 14, "type": "METRIC", "confidence": 0.6264558434486389}]}, {"text": "F1 drops 20% without the explicit modeling of both schema types t and their entity slots s.", "labels": [], "entities": [{"text": "F1", "start_pos": 0, "end_pos": 2, "type": "METRIC", "confidence": 0.9932379722595215}]}, {"text": "The entity features Fe are less important.", "labels": [], "entities": []}, {"text": "Experiments without them show a slight drop in performance (2 F1 points), small enough that they could be removed for efficiency.", "labels": [], "entities": [{"text": "F1 points)", "start_pos": 62, "end_pos": 72, "type": "METRIC", "confidence": 0.9732447266578674}]}, {"text": "However, it is extremely useful to learn slots with NER labels like Person or Location.", "labels": [], "entities": []}, {"text": "Finally, we experimented without the subject/object constraint (Section 4.4).", "labels": [], "entities": []}, {"text": "Performance drops 5-10% depending on the number of schemas learned.", "labels": [], "entities": []}, {"text": "Anecdotally, it merges too many schema slots that should be separate.", "labels": [], "entities": []}, {"text": "We recommend using this constraint as it has little impact on CPU time.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: MUC-4 extraction with template mapping. A  learned schema first maps to a gold MUC template.  Learned slots can then only map to slots in that template.", "labels": [], "entities": [{"text": "MUC-4 extraction", "start_pos": 10, "end_pos": 26, "type": "TASK", "confidence": 0.9080440402030945}]}, {"text": " Table 2: MUC-4 extraction with slot-only mapping. Any  learned slot is allowed to map to any gold slot.", "labels": [], "entities": [{"text": "MUC-4 extraction", "start_pos": 10, "end_pos": 26, "type": "TASK", "confidence": 0.8065169453620911}]}, {"text": " Table 3: Results for each MUC-4 template slot using the  template-mapping evaluation.", "labels": [], "entities": [{"text": "MUC-4 template slot", "start_pos": 27, "end_pos": 46, "type": "DATASET", "confidence": 0.7116623918215433}]}, {"text": " Table 4: Full MUC-4 extraction with gold document clas- sification. These results ignore false positives extracted  from \"irrelevant\" documents in the test set.", "labels": [], "entities": [{"text": "MUC-4 extraction", "start_pos": 15, "end_pos": 31, "type": "TASK", "confidence": 0.8395593762397766}]}]}