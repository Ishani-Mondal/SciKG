{"title": [{"text": "Efficient Collective Entity Linking with Stacking", "labels": [], "entities": [{"text": "Efficient Collective Entity Linking", "start_pos": 0, "end_pos": 35, "type": "TASK", "confidence": 0.5181092098355293}, {"text": "Stacking", "start_pos": 41, "end_pos": 49, "type": "TASK", "confidence": 0.4686160683631897}]}], "abstractContent": [{"text": "Entity disambiguation works by linking ambiguous mentions in text to their corresponding real-world entities in knowledge base.", "labels": [], "entities": [{"text": "Entity disambiguation", "start_pos": 0, "end_pos": 21, "type": "TASK", "confidence": 0.8223896324634552}]}, {"text": "Recent collective disambiguation methods enforce coherence among contextual decisions at the cost of non-trivial inference processes.", "labels": [], "entities": []}, {"text": "We propose a fast collective disambiguation approach based on stacking.", "labels": [], "entities": []}, {"text": "First, we train a local predictor g 0 with learning to rank as base learner, to generate initial ranking list of candidates.", "labels": [], "entities": []}, {"text": "Second, top k candidates of related instances are searched for constructing expressive global coherence features.", "labels": [], "entities": []}, {"text": "A global pre-dictor g 1 is trained in the augmented feature space and stacking is employed to tackle the train/test mismatch problem.", "labels": [], "entities": [{"text": "stacking", "start_pos": 70, "end_pos": 78, "type": "TASK", "confidence": 0.9531581997871399}]}, {"text": "The proposed method is fast and easy to implement.", "labels": [], "entities": []}, {"text": "Experiments show its effectiveness over various algorithms on several public datasets.", "labels": [], "entities": []}, {"text": "By learning a rich semantic relatedness measure between entity categories and context document, performance is further improved.", "labels": [], "entities": []}], "introductionContent": [{"text": "When extracting knowledge from natural language text into a machine readable format, ambiguous names must be resolved in order to tell which realworld entity the name refers to.", "labels": [], "entities": []}, {"text": "The task of linking names to knowledge base is known as entity linking or disambiguation).", "labels": [], "entities": [{"text": "entity linking or disambiguation", "start_pos": 56, "end_pos": 88, "type": "TASK", "confidence": 0.7942285686731339}]}, {"text": "The resulting text is populated with semantic rich links to knowledge base like Wikipedia, and ready for various downstream NLP applications.", "labels": [], "entities": []}, {"text": "Previous researches have proposed several kinds of effective approaches for this problem.", "labels": [], "entities": []}, {"text": "Learning to rank (L2R) approaches use hand-crafted features f (d, e) to describe the similarity or dissimilarity between contextual document d and entity definition e.", "labels": [], "entities": []}, {"text": "L2R approaches are very flexible and expressive.", "labels": [], "entities": []}, {"text": "Features like name matching, context similarity ( and category context correlation () can be incorporated with ease.", "labels": [], "entities": [{"text": "name matching", "start_pos": 14, "end_pos": 27, "type": "TASK", "confidence": 0.8541801273822784}, {"text": "category context correlation", "start_pos": 54, "end_pos": 82, "type": "TASK", "confidence": 0.5631209115187327}]}, {"text": "Nevertheless, decisions are made independently and inconsistent results are found from time to time.", "labels": [], "entities": []}, {"text": "Collective approaches utilize dependencies between different decisions and resolve all ambiguous mentions within the same context simultaneously).", "labels": [], "entities": []}, {"text": "Collective approaches can improve performance when local evidence is not confident enough.", "labels": [], "entities": []}, {"text": "They often utilize semantic relations across different mentions, and is why they are called global approaches, while L2R methods fall into local approaches).", "labels": [], "entities": []}, {"text": "However, collective inference processes are often expensive and involve an exponential search space.", "labels": [], "entities": []}, {"text": "We propose a collective entity linking method based on stacking.", "labels": [], "entities": []}, {"text": "Stacked generalization) is a powerful meta learning algorithm that uses two levels of learners.", "labels": [], "entities": [{"text": "Stacked generalization", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.7710169851779938}]}, {"text": "The predictions of the first learner are taken as augmented features for the second learner.", "labels": [], "entities": []}, {"text": "The nice property of stacking is that it does not restrict the form of the base learner.", "labels": [], "entities": [{"text": "stacking", "start_pos": 21, "end_pos": 29, "type": "TASK", "confidence": 0.9811268448829651}]}, {"text": "In this paper, our base learner, an L2R ranker, is first employed to generate a ranking list of candidates.", "labels": [], "entities": []}, {"text": "At the next level, we search for semantic coherent entities from the top k candidates of neighboring mentions.", "labels": [], "entities": []}, {"text": "The second learner is trained on the augmented feature space to enforce semantic coherence.", "labels": [], "entities": []}, {"text": "Stacking is employed to handle train/test mismatch problem.", "labels": [], "entities": []}, {"text": "Compared with existing collective methods, the inference process of our method is much faster because of the simple form of its base learner.", "labels": [], "entities": []}, {"text": "Wikipedians annotate each entity with categories which provide another source of valuable semantic information.", "labels": [], "entities": []}, {"text": "() propose to generalize beyond context-entity correlation s(d, e) with word-category correlation s(w, c).", "labels": [], "entities": []}, {"text": "However, this method works at word level, and does not scale well to large number of categories.", "labels": [], "entities": []}, {"text": "We explore a representation learning technique to learn the category-context association in latent semantic space, which scales much better to large knowledge base.", "labels": [], "entities": []}, {"text": "Our contributions are as follows: We propose a fast and accurate stacking-based collective entity linking method, which combines the benefits of both coherence modeling of collective approaches and expressivity of L2R methods.", "labels": [], "entities": [{"text": "stacking-based collective entity linking", "start_pos": 65, "end_pos": 105, "type": "TASK", "confidence": 0.8480145782232285}]}, {"text": "We show an effective usage of ranking list as global features, which is a key improvement for the global predictor.", "labels": [], "entities": []}, {"text": "(2) To overcome problems of scalability and shallow word-level comparison, we learn the categorycontext correlation with recent advances of representation learning, and show that this extra semantic information indeed helps improve entity linking performance.", "labels": [], "entities": [{"text": "representation learning", "start_pos": 140, "end_pos": 163, "type": "TASK", "confidence": 0.8945175409317017}, {"text": "entity linking", "start_pos": 232, "end_pos": 246, "type": "TASK", "confidence": 0.740755558013916}]}], "datasetContent": [{"text": "Previous researches have used diverse datasets for evaluation, which makes it hard for comparison with others' approaches.", "labels": [], "entities": []}, {"text": "TAC-KBP has several years of data for evaluating entity linking system, but is not well suited for evaluating collective approaches.", "labels": [], "entities": [{"text": "TAC-KBP", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.8662713766098022}]}, {"text": "Recently,) annotated a clean and much larger dataset AIDA 1 for collective approaches evaluation based on) also refined previous work and contribute four publicly available datasets 2 . Thanks to their great works, we have enough data to evaluate against.", "labels": [], "entities": [{"text": "AIDA", "start_pos": 53, "end_pos": 57, "type": "DATASET", "confidence": 0.8501400947570801}]}, {"text": "According to the setting of, we split the AIDA dataset for train/development/test with 946/216/231 documents.", "labels": [], "entities": [{"text": "AIDA dataset", "start_pos": 42, "end_pos": 54, "type": "DATASET", "confidence": 0.9641391634941101}]}, {"text": "We train a separate model on the Wikipedia training set for evaluating ACE/QUAINT/WIKI dataset.", "labels": [], "entities": [{"text": "Wikipedia training set", "start_pos": 33, "end_pos": 55, "type": "DATASET", "confidence": 0.8985986510912577}, {"text": "ACE/QUAINT/WIKI dataset", "start_pos": 71, "end_pos": 94, "type": "DATASET", "confidence": 0.6670083403587341}]}, {"text": "gives a brief overview of the datasets used.", "labels": [], "entities": []}, {"text": "For knowledge base, we use the Wikipedia XML dump 3 to extract over 3.3 million entities.", "labels": [], "entities": [{"text": "Wikipedia XML dump 3", "start_pos": 31, "end_pos": 51, "type": "DATASET", "confidence": 0.9426827728748322}]}, {"text": "We use annotation from Wikipedia to build a name dictionary from mention string m to entity e for candidate generation, including redirects, disambiguation pages and hyperlinks, follows the approach of.", "labels": [], "entities": [{"text": "candidate generation", "start_pos": 98, "end_pos": 118, "type": "TASK", "confidence": 0.7267023622989655}]}, {"text": "For candidate generation, we keep the top 30 candidates by popularity (Tbl. 1).", "labels": [], "entities": [{"text": "candidate generation", "start_pos": 4, "end_pos": 24, "type": "TASK", "confidence": 0.7883711159229279}]}, {"text": "Note that our name dictionary is different from) and has a much higher recall.", "labels": [], "entities": [{"text": "recall", "start_pos": 71, "end_pos": 77, "type": "METRIC", "confidence": 0.9992732405662537}]}, {"text": "Since (Ratinov et al., 2011) evaluate on \"solvable\" mentions and we have noway to recover those mentions, we re-implement their global features and the final scores are not directly comparable to theirs.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Number of mentions in each dataset. \"identi- fied\" means the mention exists in our name dictionary  and \"solvable\" means the true entity are among the top 30  candidates by popularity. Number in parenthesis shows  the results of (Ratinov et al., 2011).", "labels": [], "entities": []}, {"text": " Table 3: Top k recall for local predictor g 0 .", "labels": [], "entities": [{"text": "recall", "start_pos": 16, "end_pos": 22, "type": "METRIC", "confidence": 0.7797684073448181}]}, {"text": " Table 4: Performance on AIDA dataset. Maximal value in each group are highlighted with bold font. top k means up  to k candidates are used for searching related instances with relational template.", "labels": [], "entities": [{"text": "AIDA dataset", "start_pos": 25, "end_pos": 37, "type": "DATASET", "confidence": 0.9332834184169769}]}, {"text": " Table 5: Evaluation on ACE/AQUAINT/WIKI datasets.", "labels": [], "entities": [{"text": "ACE/AQUAINT/WIKI datasets", "start_pos": 24, "end_pos": 49, "type": "DATASET", "confidence": 0.7409723997116089}]}]}