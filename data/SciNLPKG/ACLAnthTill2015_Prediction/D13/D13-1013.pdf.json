{"title": [{"text": "Joint Parsing and Disfluency Detection in Linear Time", "labels": [], "entities": [{"text": "Joint Parsing and Disfluency Detection", "start_pos": 0, "end_pos": 38, "type": "TASK", "confidence": 0.6842275083065033}]}], "abstractContent": [{"text": "We introduce a novel method to jointly parse and detect disfluencies in spoken utterances.", "labels": [], "entities": [{"text": "parse and detect disfluencies in spoken utterances", "start_pos": 39, "end_pos": 89, "type": "TASK", "confidence": 0.6476422037397113}]}, {"text": "Our model can use arbitrary features for parsing sentences and adapt itself with out-of-domain data.", "labels": [], "entities": []}, {"text": "We show that our method, based on transition-based parsing, performs at a high level of accuracy for both the parsing and disfluency detection tasks.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 88, "end_pos": 96, "type": "METRIC", "confidence": 0.9992771744728088}, {"text": "parsing and disfluency detection", "start_pos": 110, "end_pos": 142, "type": "TASK", "confidence": 0.677630640566349}]}, {"text": "Additionally, our method is the fastest for the joint task, running in linear time.", "labels": [], "entities": []}], "introductionContent": [{"text": "Detecting disfluencies in spontaneous speech has been widely studied by researchers in different communities including natural language processing (e.g.), speech processing (e.g.) and psycholinguistics (e.g.).", "labels": [], "entities": [{"text": "Detecting disfluencies in spontaneous speech", "start_pos": 0, "end_pos": 44, "type": "TASK", "confidence": 0.8672364354133606}, {"text": "speech processing", "start_pos": 155, "end_pos": 172, "type": "TASK", "confidence": 0.7630155682563782}]}, {"text": "While the percentage of spoken words which are disfluent is typically not more than ten percent (), this additional \"noise\" makes it much harder for spoken language systems to predict the correct structure of the sentence.", "labels": [], "entities": []}, {"text": "Disfluencies can be filled pauses (e.g. \"uh\", \"um\", \"huh\"), discourse markers (e.g. \"you know\", \"I mean\") or edited words which are repeated or corrected by the speaker.", "labels": [], "entities": []}, {"text": "For example, in the following sentence, an edited phrase or reparandum interval (\"to Boston\") occurs with its repair (\"to Denver\"), a filled pause (\"uh\") and discourse marker (\"I * The first author worked on this project while he was a research intern in CoreNL research group, NLU lab, Nuance Communications, Sunnyvale, CA.", "labels": [], "entities": []}, {"text": "mean\" Filled pauses and discourse markers are to some extent a fixed and closed set.", "labels": [], "entities": []}, {"text": "The main challenge in finding disfluencies is the case where the edited phrase is neither a rough copy of its repair or has any repair phrase (i.e. discarded edited phrase).", "labels": [], "entities": []}, {"text": "Hence, in previous work, researchers report their method performance on detecting edited phrases (reparandum).", "labels": [], "entities": []}, {"text": "In contrast to most previous work which focuses solely on either detection or on parsing, we introduce a novel framework for jointly parsing sentences with disfluencies.", "labels": [], "entities": []}, {"text": "To our knowledge, our work is the first model that is based on joint dependency and disfluency detection.", "labels": [], "entities": [{"text": "disfluency detection", "start_pos": 84, "end_pos": 104, "type": "TASK", "confidence": 0.7221362739801407}]}, {"text": "We show that our model is robust enough to detect disfluencies with high accuracy, while still maintaining a high level of dependency parsing accuracy that approaches the upper bound.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 73, "end_pos": 81, "type": "METRIC", "confidence": 0.9971728324890137}, {"text": "dependency parsing", "start_pos": 123, "end_pos": 141, "type": "TASK", "confidence": 0.6433031558990479}, {"text": "accuracy", "start_pos": 142, "end_pos": 150, "type": "METRIC", "confidence": 0.8873686194419861}]}, {"text": "Additionally, our model outperforms prior work on joint parsing and disfluency detection on the disfluency detection task, and improves upon this prior work by running in linear time complexity.", "labels": [], "entities": [{"text": "joint parsing", "start_pos": 50, "end_pos": 63, "type": "TASK", "confidence": 0.5702520161867142}, {"text": "disfluency detection", "start_pos": 68, "end_pos": 88, "type": "TASK", "confidence": 0.7104926705360413}, {"text": "disfluency detection task", "start_pos": 96, "end_pos": 121, "type": "TASK", "confidence": 0.7875116666158041}]}, {"text": "The remainder of this paper is as follows.", "labels": [], "entities": []}, {"text": "In \u00a72, we overview some the previous work on disfluency detection.", "labels": [], "entities": [{"text": "disfluency detection", "start_pos": 45, "end_pos": 65, "type": "TASK", "confidence": 0.9267866015434265}]}, {"text": "\u00a73 describes our model.", "labels": [], "entities": []}, {"text": "Experiments are described in \u00a74 and Conclusions are made in \u00a75.", "labels": [], "entities": [{"text": "Conclusions", "start_pos": 36, "end_pos": 47, "type": "METRIC", "confidence": 0.9830742478370667}]}], "datasetContent": [{"text": "For our experiments, we use the Switchboard corpus () with the same train/dev/test split as.", "labels": [], "entities": [{"text": "Switchboard corpus", "start_pos": 32, "end_pos": 50, "type": "DATASET", "confidence": 0.8044097721576691}]}, {"text": "As in that work, incomplete words and punctuations are removed from data (except that we do not remove incomplete words that are not disfluent 4 ) and all words are turned into lower-case.", "labels": [], "entities": []}, {"text": "The main difference with previous work is that we use Switchboard mrg files for training and testing our model (since they contain parse trees) instead of the more commonly used Swithboard dps text files.", "labels": [], "entities": []}, {"text": "Mrg files area subset of dps files with about more than half of their size.", "labels": [], "entities": []}, {"text": "Unfortunately, the disfluencies marked in the dps files are not exactly the same as those marked in the corresponding mrg files.", "labels": [], "entities": []}, {"text": "Hence, our result is not completely comparable to previous work except for (.", "labels": [], "entities": []}, {"text": "We use Tsurgeon () for extracting sentences from mrg files and use the Penn2Malt tool to convert them to dependencies.", "labels": [], "entities": [{"text": "Penn2Malt", "start_pos": 71, "end_pos": 80, "type": "DATASET", "confidence": 0.9588238596916199}]}, {"text": "Afterwards, we provide dependency trees with disfluent words being the dependent of nothing.", "labels": [], "entities": []}, {"text": "Learning For the first classifier, we use averaged structured Perceptron (AP)) with a minor modification.", "labels": [], "entities": []}, {"text": "Since the first classifier data is heavily biased towards the \"regular label\", we modify the weight updates in the original algorithm to 2 (original is 1) for the cases where a \"reparandum\" is wrongly recognized as another label.", "labels": [], "entities": []}, {"text": "We call the modified version \"weighted averaged Perceptron (WAP)\".", "labels": [], "entities": []}, {"text": "We see that this simple modification improves the model accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 56, "end_pos": 64, "type": "METRIC", "confidence": 0.9978783130645752}]}, {"text": "For the second classifier (parser), we use the original averaged structured Perceptron algorithm.", "labels": [], "entities": []}, {"text": "We report results on both AP and WAP versions of the parser.", "labels": [], "entities": [{"text": "AP", "start_pos": 26, "end_pos": 28, "type": "METRIC", "confidence": 0.6719502806663513}]}, {"text": "Features Since for every state in the parser configuration, there are many candidates for being disfluent; we use local features as well as global features for the first classifier.", "labels": [], "entities": []}, {"text": "Global features are mostly useful for discriminating between the four actions and local features are mostly useful for choosing a phrase as a candidate for being a disfluent phrase.", "labels": [], "entities": []}, {"text": "The features are described in.", "labels": [], "entities": []}, {"text": "For the second classifier, we use the same features as (Zhang and Nivre, 2011, Table 1) except that we train our  8 We use the second version of the code: http://code.", "labels": [], "entities": []}, {"text": "google.com/p/disfluency-detection/.", "labels": [], "entities": []}, {"text": "Results from the first version are 81.4 and 82.1 for the default and optimized settings.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Parsing results. UB = upperbound (parsing clean  sentences), LB = lowerbound (parsing disfluent sentences  without disfluency correction). UAS is unlabeled attach- ment score (accuracy), Pr. is precision, Rec. is recall and  F1 is f-score.", "labels": [], "entities": [{"text": "UAS", "start_pos": 149, "end_pos": 152, "type": "METRIC", "confidence": 0.9864540696144104}, {"text": "attach- ment score", "start_pos": 166, "end_pos": 184, "type": "METRIC", "confidence": 0.6787485331296921}, {"text": "accuracy", "start_pos": 186, "end_pos": 194, "type": "METRIC", "confidence": 0.8338760733604431}, {"text": "precision", "start_pos": 204, "end_pos": 213, "type": "METRIC", "confidence": 0.9993628859519958}, {"text": "Rec.", "start_pos": 215, "end_pos": 219, "type": "METRIC", "confidence": 0.9658551812171936}, {"text": "recall", "start_pos": 223, "end_pos": 229, "type": "METRIC", "confidence": 0.9968968629837036}, {"text": "F1", "start_pos": 235, "end_pos": 237, "type": "METRIC", "confidence": 0.9978245496749878}, {"text": "f-score", "start_pos": 241, "end_pos": 248, "type": "METRIC", "confidence": 0.9431724548339844}]}, {"text": " Table 2: Disfluency results. Pr. is precision, Rec. is recall  and F1 is f-score. KL = (Kahn et al., 2005), LJ = (Lease  and Johnson, 2006), MS = (Miller and Schuler, 2008) and  QL = (Qian and Liu, 2013).", "labels": [], "entities": [{"text": "precision", "start_pos": 37, "end_pos": 46, "type": "METRIC", "confidence": 0.9993934631347656}, {"text": "Rec.", "start_pos": 48, "end_pos": 52, "type": "METRIC", "confidence": 0.9636242985725403}, {"text": "recall", "start_pos": 56, "end_pos": 62, "type": "METRIC", "confidence": 0.9979020357131958}, {"text": "F1", "start_pos": 68, "end_pos": 70, "type": "METRIC", "confidence": 0.9952334761619568}, {"text": "f-score", "start_pos": 74, "end_pos": 81, "type": "METRIC", "confidence": 0.9383827447891235}, {"text": "KL", "start_pos": 83, "end_pos": 85, "type": "METRIC", "confidence": 0.9887308478355408}, {"text": "MS", "start_pos": 142, "end_pos": 144, "type": "METRIC", "confidence": 0.9551934003829956}, {"text": "QL", "start_pos": 179, "end_pos": 181, "type": "METRIC", "confidence": 0.9966192245483398}]}]}