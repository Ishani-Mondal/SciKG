{"title": [{"text": "Unsupervised Spectral Learning of WCFG as Low-rank Matrix Completion", "labels": [], "entities": []}], "abstractContent": [{"text": "We derive a spectral method for unsupervised learning of Weighted Context Free Grammars.", "labels": [], "entities": []}, {"text": "We frame WCFG induction as finding a Han-kel matrix that has low rank and is linearly constrained to represent a function computed by inside-outside recursions.", "labels": [], "entities": [{"text": "WCFG induction", "start_pos": 9, "end_pos": 23, "type": "TASK", "confidence": 0.8596152365207672}]}, {"text": "The proposed algorithm picks the grammar that agrees with a sample and is the simplest with respect to the nuclear norm of the Hankel matrix.", "labels": [], "entities": []}], "introductionContent": [{"text": "Weighted Context Free Grammars (WCFG) define an important class of languages.", "labels": [], "entities": [{"text": "Weighted Context Free Grammars (WCFG)", "start_pos": 0, "end_pos": 37, "type": "TASK", "confidence": 0.6983031460217067}]}, {"text": "Their expressivity makes them good candidates for modeling a wide range of natural language phenomena.", "labels": [], "entities": []}, {"text": "This expressivity comes at a cost: unsupervised learning of WCFG seems to be a particularly hard task.", "labels": [], "entities": []}, {"text": "And while it is a well-studied problem, it is still to a great extent unsolved.", "labels": [], "entities": []}, {"text": "Several methods for unsupervised learning of WCFG have been proposed.", "labels": [], "entities": []}, {"text": "Some rely on heuristics that are used to build incrementally an approximation of the unknown grammar (;.", "labels": [], "entities": []}, {"text": "Other methods are based on maximum likelihood estimation, searching for the grammar that has the largest posterior given the training corpus).", "labels": [], "entities": []}, {"text": "Several Bayesian inference approaches have also been proposed.", "labels": [], "entities": []}, {"text": "These approaches perform parameter estimation by exploiting Markov sampling techniques.", "labels": [], "entities": [{"text": "parameter estimation", "start_pos": 25, "end_pos": 45, "type": "TASK", "confidence": 0.7304977178573608}]}, {"text": "Recently, for the related problem of unsupervised dependency parsing, proposed anew way of framing the max-likelihood estimation.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 50, "end_pos": 68, "type": "TASK", "confidence": 0.7269665598869324}]}, {"text": "In their formulation the problem is expressed as an integer quadratic program subject to non-linear constraints.", "labels": [], "entities": []}, {"text": "They exploit techniques from mathematical programming to solve the resulting optimization.", "labels": [], "entities": []}, {"text": "In spirit, the work by is probably the most similar to our approach since both approaches share an algebraic view of the problem.", "labels": [], "entities": []}, {"text": "In his case the key idea is to work with an algebraic representation of a WCFG.", "labels": [], "entities": []}, {"text": "The problem of recovering the constituents of the grammar is reduced to the problem of identifying its syntactic congruence.", "labels": [], "entities": []}, {"text": "In the last years, multiple spectral learning algorithms have been proposed fora wide range of models (.", "labels": [], "entities": []}, {"text": "Since the spectral approach provides a good thinking tool to reason about distributions over \u03a3 * , the question of whether they can be used for unsupervised learning of WCFG seems natural.", "labels": [], "entities": []}, {"text": "Still, while spectral algorithms for unsupervised learning of languages can learn regular languages, tree languages and simple dependency grammars, the frontier to WCFG seems hard to reach.", "labels": [], "entities": []}, {"text": "In fact, the most recent theoretical results on spectral learning of WCFG do not seem to be very encouraging.", "labels": [], "entities": [{"text": "spectral learning", "start_pos": 48, "end_pos": 65, "type": "TASK", "confidence": 0.8348553478717804}]}, {"text": "Recently, showed that the problem of recovering the joint distribution over PCFG derivations and their yields is not identifiable.", "labels": [], "entities": []}, {"text": "Although, for some simple grammar subclasses (e.g. independent left and right children), identification in the weaker sense (over the yields of the grammar) implies strong identification (e.g. over joint distribution of yields and derivations).", "labels": [], "entities": []}, {"text": "In their paper, they propose a spectral algorithm based on a generalization of the method of moments for these restricted subclasses.", "labels": [], "entities": []}, {"text": "Thus one open direction for spectral research consists on defining subclasses of context free languages that can be learned (in the strong sense) from observations of yields.", "labels": [], "entities": []}, {"text": "Yet, an alternative research direction is to consider learnability in the weaker sense.", "labels": [], "entities": []}, {"text": "In this paper we take the second road, and focus on the problem of approximating the distribution over yields generated by a WCFG.", "labels": [], "entities": []}, {"text": "Our main contribution is to present a spectral algorithm for unsupervised learning of WCFG.", "labels": [], "entities": [{"text": "WCFG", "start_pos": 86, "end_pos": 90, "type": "DATASET", "confidence": 0.8377419114112854}]}, {"text": "Following ideas from , the algorithm is framed as a convex optimization where we search fora low-rank matrix satisfying two types of constraints: (1) Constraints derived from observable statistics over yields; and (2) Constraints derived from certain recurrence relations satisfied by a WCFG.", "labels": [], "entities": []}, {"text": "Our derivations of the learning algorithm illustrate the main ingredients behind the spectral approach to learning functions over \u03a3 * which are: (1) to exploit the recurrence relations satisfied by the target family of functions and (2) provide algebraic formulations of these relations.", "labels": [], "entities": []}, {"text": "We alert the reader that although we are able to frame the problem as a convex optimization, the number of variables involved is quite large and prohibits a practical implementation of the method on a realistic scenario.", "labels": [], "entities": []}, {"text": "The experiments we present should be regarded as examples designed to illustrate the behavior of the method.", "labels": [], "entities": []}, {"text": "More research is needed to make the optimization more efficient, and we are optimistic that such improvements can be achieved by exploiting problem-specific properties of the optimization.", "labels": [], "entities": []}, {"text": "Regardless of this, ours is a novel way of framing the grammatical inference problem.", "labels": [], "entities": [{"text": "framing the grammatical inference problem", "start_pos": 43, "end_pos": 84, "type": "TASK", "confidence": 0.7293697774410248}]}, {"text": "The rest of the paper is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 gives preliminaries on WCFG and the type of functions we will learn.", "labels": [], "entities": [{"text": "WCFG", "start_pos": 33, "end_pos": 37, "type": "DATASET", "confidence": 0.6403980851173401}]}, {"text": "Section 3 establishes that spectral methods can learn a WCFG from a Hankel matrix containing statistics about context-free cuts.", "labels": [], "entities": []}, {"text": "Section 4 presents the unsupervised algorithm, where we formulate grammar induction as a lowrank optimization.", "labels": [], "entities": [{"text": "formulate grammar induction", "start_pos": 56, "end_pos": 83, "type": "TASK", "confidence": 0.7025942305723826}]}, {"text": "Section 5 presents experiments, and finally we conclude the paper.", "labels": [], "entities": []}, {"text": "Notation Let \u03a3 bean alphabet.", "labels": [], "entities": []}, {"text": "We use \u03c3 to denote an arbitrary symbol in \u03a3.", "labels": [], "entities": []}, {"text": "The set of all finite strings over \u03a3 is denoted by \u03a3 , where we write \u03bb for the empty string.", "labels": [], "entities": []}, {"text": "We also use the set \u03a3 + = \u03a3 \\ {\u03bb}.", "labels": [], "entities": []}, {"text": "We use bold letters to represent column vectors v and matrices M . We use In to denote the ndimensional identity matrix.", "labels": [], "entities": []}, {"text": "We use M + to denote the Moore-Penrose pseudoinverse of some matrix M . M \u2297 M is the Kronecker product between matrices M \u2208 R m\u00d7n and M \u2208 R p\u00d7q resulting in a matrix in R mp\u00d7nq . The rest of notation will be given as needed.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section we describe some experiments with the learning algorithms for WCFG.", "labels": [], "entities": []}, {"text": "Our goal is to verify that the algorithms can learn some basic context-free languages, and to study the possibility of using them on real data.", "labels": [], "entities": []}, {"text": "We performed experiments on synthetic data, obtained by choosing a PCFG with random parameters (\u2208), with a normalization step in order to get a probability distribution.", "labels": [], "entities": []}, {"text": "We built the Hankel matrix from the inside basis {(x)} x\u2208\u03a3 and outside basis {{\u03bb; \u03bb} \u222a {{x; \u03bb, \u03bb; x} x\u2208\u03a3 . The composed insides for the operator matrix are thus {(x, y)} x,y\u2208\u03a3 . The matrix in the optimizer has the following structure The constraints we use are: We use p S to denote the empirical distribution.", "labels": [], "entities": []}, {"text": "Those are simplified versions of the Hankel, inside, outside and observation constraints.", "labels": [], "entities": [{"text": "Hankel", "start_pos": 37, "end_pos": 43, "type": "DATASET", "confidence": 0.7975671887397766}]}, {"text": "The set O is built from the following remarks: (1) (xy) = (x, y) and (2) (xyz) = (xy, z) + (x, yz).", "labels": [], "entities": []}, {"text": "The method uses statistics for sequences up to length 3.", "labels": [], "entities": []}, {"text": "The algorithm we use for the unsupervised spectral method is a simplified version: we use alternatively a hard projection on the constraints (by projecting iteratively on each constraint), and a thresholding-shrinkage operation for the target dimension.", "labels": [], "entities": []}, {"text": "We use the same trick as FISTA for the update.", "labels": [], "entities": [{"text": "FISTA", "start_pos": 25, "end_pos": 30, "type": "DATASET", "confidence": 0.596464216709137}]}, {"text": "We finally use the regular spectral method on this matrix to get our model.", "labels": [], "entities": []}, {"text": "We compare this method with an unsupervised EM, and also with supervised versions of spectral method and EM.", "labels": [], "entities": []}, {"text": "We compare the accuracy of the different models in terms of KL-divergence for sequences up to length 10.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 15, "end_pos": 23, "type": "METRIC", "confidence": 0.9994958639144897}]}, {"text": "We run 50 optimization steps for the unsupervised spectral method, and 200 iterations for the EM methods.", "labels": [], "entities": []}, {"text": "shows the results, corresponding the the geometric mean over 50 experiments on random targets of 2 symbols and 2 states.", "labels": [], "entities": []}, {"text": "For sample size greater than 10 5 , the unsupervised spectral method seems to provide better solutions than both EM and supervised EM.", "labels": [], "entities": []}, {"text": "The solution, in terms of KL-divergence, is comparable to the one obtained with the supervised spectral method.", "labels": [], "entities": []}, {"text": "The computation time of unsupervised spectral method is almost constant w.r.t. the sample size, around 1.67s, while computation time of unsupervised EM (resp. supervised EM) is 6.10 3 s (resp.", "labels": [], "entities": []}, {"text": "2.10 4 s) for sample size 10 6 . presents learnings curve for random targets with 3 states and 6 symbols.", "labels": [], "entities": []}, {"text": "One can see that, for big sample sizes (10 9 ), the unsupervised spectral method is losing accuracy compared to the supervised method.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 91, "end_pos": 99, "type": "METRIC", "confidence": 0.9991264939308167}]}, {"text": "This is due to alack of information, and could be overcome by considering a greater basis (e.g. inside sequences up to length 2 or 3).", "labels": [], "entities": []}, {"text": "Now we present some preliminar tests using natural language data.", "labels": [], "entities": []}, {"text": "For these tests, we used the WSJ10 subset of the Penn Treebank, as.", "labels": [], "entities": [{"text": "WSJ10 subset", "start_pos": 29, "end_pos": 41, "type": "DATASET", "confidence": 0.9644558429718018}, {"text": "Penn Treebank", "start_pos": 49, "end_pos": 62, "type": "DATASET", "confidence": 0.8595148026943207}]}, {"text": "This dataset consists of the sentences of length \u2264 10 after filtering punctuation and currency.", "labels": [], "entities": []}, {"text": "We removed lexical items and mapped the POS tags to the Universal Part-of-Speech Tagset (, reducing the alphabet to a set of 11 symbols.", "labels": [], "entities": []}, {"text": "shows the size of the problem for different basis sizes.", "labels": [], "entities": []}, {"text": "As described in the previous subsection for the unsupervised case, we obtain the basis by taking the most frequent observed substrings and contexts.", "labels": [], "entities": []}, {"text": "We then compute all yields that can be generated with this basis, and close the basis to include all possible insides and outsides with operations completions, such that we create a Hankel as described in Section 4.1.", "labels": [], "entities": []}, {"text": "shows, for each base, the size of H we induce, the number of observable constraints (i.e. sentences we train from), and the number of inside-outside constraints.", "labels": [], "entities": []}, {"text": "With the current implementation of the optimizer we were only able to run the unsupervised learning for small basis sizes.", "labels": [], "entities": []}, {"text": "shows the expected L 1 on training data.", "labels": [], "entities": [{"text": "L 1", "start_pos": 19, "end_pos": 22, "type": "METRIC", "confidence": 0.9846006333827972}]}, {"text": "For a fixed basis, as we increase the number of states we see that the error decreases, showing that the method is inducing a Hankel matrix that explains the observable statistics.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Problem sizes for the WSJ10 training corpus.", "labels": [], "entities": [{"text": "WSJ10 training corpus", "start_pos": 32, "end_pos": 53, "type": "DATASET", "confidence": 0.8757511973381042}]}, {"text": " Table 2: Experiments with the unsupervised spectral  method on the WSJ10 corpus. Results are in terms of  expected L 1 on the training set, for different basis and  numbers of states.", "labels": [], "entities": [{"text": "WSJ10 corpus", "start_pos": 68, "end_pos": 80, "type": "DATASET", "confidence": 0.9947434961795807}]}]}