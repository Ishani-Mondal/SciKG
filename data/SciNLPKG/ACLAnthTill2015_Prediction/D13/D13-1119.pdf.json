{"title": [{"text": "An Empirical Study Of Semi-Supervised Chinese Word Segmentation Using Co-Training", "labels": [], "entities": [{"text": "Chinese Word Segmentation", "start_pos": 38, "end_pos": 63, "type": "TASK", "confidence": 0.5739842057228088}]}], "abstractContent": [{"text": "In this paper we report an empirical study on semi-supervised Chinese word segmenta-tion using co-training.", "labels": [], "entities": []}, {"text": "We utilize two seg-menters: 1) a word-based segmenter lever-aging a word-level language model, and 2) a character-based segmenter using character-level features within a CRF-based sequence labeler.", "labels": [], "entities": []}, {"text": "These two segmenters are initially trained with a small amount of segmented data, and then iteratively improve each other using the large amount of unlabelled data.", "labels": [], "entities": []}, {"text": "Our experimental results show that co-training captures 20% and 31% of the performance improvement achieved by supervised training with an order of magnitude more data for the SIGHAN Bakeoff 2005 PKU and CU corpora respectively.", "labels": [], "entities": [{"text": "SIGHAN Bakeoff 2005 PKU and CU corpora", "start_pos": 176, "end_pos": 214, "type": "DATASET", "confidence": 0.7745899387768337}]}], "introductionContent": [{"text": "In the literature there exist two general models for supervised Chinese word segmentation, the wordbased approach and the character-based approach.", "labels": [], "entities": [{"text": "supervised Chinese word segmentation", "start_pos": 53, "end_pos": 89, "type": "TASK", "confidence": 0.6129191219806671}]}, {"text": "The word-based approach searches for all possible segmentations, usually created using a dictionary, for the optimal one that maximizes a certain utility.", "labels": [], "entities": []}, {"text": "The character-based approach treats segmentation as a character sequence labeling problem, indicating whether a character is located at the boundary of a word.", "labels": [], "entities": [{"text": "character sequence labeling", "start_pos": 54, "end_pos": 81, "type": "TASK", "confidence": 0.6923739512761434}]}, {"text": "Typically the word-based approach uses word level features, such as word n-grams and word length; while the character-based approach uses character level information, such as character ngrams.", "labels": [], "entities": []}, {"text": "Both approaches have their own advantages and disadvantages, and there has been some research in combining the two approaches to improve the performance of supervised word segmentation.", "labels": [], "entities": [{"text": "supervised word segmentation", "start_pos": 156, "end_pos": 184, "type": "TASK", "confidence": 0.6307215392589569}]}, {"text": "In this research we are trying to take advantage of the word-based and the character-based approaches in the semi-supervised setting for Chinese word segmentation, where there is only a limited amount of human-segmented data available, but there exists a relatively large amount of in-domain unsegmented data.", "labels": [], "entities": [{"text": "Chinese word segmentation", "start_pos": 137, "end_pos": 162, "type": "TASK", "confidence": 0.5924926102161407}]}, {"text": "The goal is to make use of the indomain unsegmented data to improve the ultimate performance of word segmentation.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 96, "end_pos": 113, "type": "TASK", "confidence": 0.747948557138443}]}, {"text": "According to , \"the two approaches [word-based and character-based approaches] are either based on a particular view of segmentation.\"", "labels": [], "entities": []}, {"text": "This naturally motivates the use of co-training, which utilizes two models trained on different views of the input labeled data which then iteratively educate each other with the unlabelled data.", "labels": [], "entities": []}, {"text": "At the end of the cotraining iterations, the initially weak models achieve improved performance.", "labels": [], "entities": []}, {"text": "Co-training has been successfully applied in many natural language processing tasks.", "labels": [], "entities": [{"text": "natural language processing tasks", "start_pos": 50, "end_pos": 83, "type": "TASK", "confidence": 0.6961049512028694}]}, {"text": "In this paper we describe an empirical study of applying co-training to semi-supervised Chinese word segmentation.", "labels": [], "entities": [{"text": "semi-supervised Chinese word segmentation", "start_pos": 72, "end_pos": 113, "type": "TASK", "confidence": 0.6182250082492828}]}, {"text": "Our experimental results show that co-training captures 20% and 31% of the performance improvement achieved by supervised training with an order of magnitude more data for the SIGHAN Bakeoff 2005 PKU and CU corpora respectively.", "labels": [], "entities": [{"text": "SIGHAN Bakeoff 2005 PKU and CU corpora", "start_pos": 176, "end_pos": 214, "type": "DATASET", "confidence": 0.7745899387768337}]}, {"text": "In section 2 we review the two supervised approaches and co-training algorithm in more detail.", "labels": [], "entities": []}, {"text": "In section 3 we describe our implementation of the co-training word segmentation.", "labels": [], "entities": [{"text": "co-training word segmentation", "start_pos": 51, "end_pos": 80, "type": "TASK", "confidence": 0.636099616686503}]}, {"text": "In section 4 we de-", "labels": [], "entities": []}], "datasetContent": [{"text": "We conduct a set of experiments to evaluate the performance of our co-training on semi-supervised Chinese word segmentation.", "labels": [], "entities": [{"text": "semi-supervised Chinese word segmentation", "start_pos": 82, "end_pos": 123, "type": "TASK", "confidence": 0.6111475899815559}]}, {"text": "Two corpora, the PKU corpus and the CU corpus, from the SIGhan Bakeoff 2005 are used.", "labels": [], "entities": [{"text": "PKU corpus", "start_pos": 17, "end_pos": 27, "type": "DATASET", "confidence": 0.9607285559177399}, {"text": "CU corpus", "start_pos": 36, "end_pos": 45, "type": "DATASET", "confidence": 0.8747667074203491}, {"text": "SIGhan Bakeoff 2005", "start_pos": 56, "end_pos": 75, "type": "DATASET", "confidence": 0.8861597577730814}]}, {"text": "The PKU corpus contains texts of simplified Chinese characters, which include 19056 sentences in the training data and 1945 sentences in the testing data.", "labels": [], "entities": [{"text": "PKU corpus", "start_pos": 4, "end_pos": 14, "type": "DATASET", "confidence": 0.9635683596134186}]}, {"text": "The CU corpus contains texts of traditional Chinese characters, which include 53019 sentences in the training data and 1493 sentences in the testing data.", "labels": [], "entities": [{"text": "CU corpus", "start_pos": 4, "end_pos": 13, "type": "DATASET", "confidence": 0.9678730070590973}]}, {"text": "The training data in each corpus is randomly split into 10 subsets.", "labels": [], "entities": []}, {"text": "In each run one set is used as the labelled data S, and the other nine sets are combined and used as the unlabelled data U with segmentations removed.", "labels": [], "entities": []}, {"text": "That is, 10% of the training data is used as segmented data, and 90% are used as unsegmented data in our semi-supervised training.", "labels": [], "entities": []}, {"text": "This setup resembles our semi-supervised application, where there is only a small limited amount of segmented data but a relatively large amount of in-domain unsegmented data available.", "labels": [], "entities": []}, {"text": "The final trained character-based and word-based segmenters from co-training are then evaluated on the testing data.", "labels": [], "entities": []}, {"text": "Results we report in this paper are the average of the 10 runs.", "labels": [], "entities": []}, {"text": "F-measure is used as the performance measurement.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.9540905952453613}]}, {"text": "A 99% confidence interval is calculated as \u00b12.56 \u221a p(1 \u2212 F )/N for statistical significance evaluation, where F is the F-measure and N is the number of words.", "labels": [], "entities": [{"text": "statistical significance evaluation", "start_pos": 67, "end_pos": 102, "type": "TASK", "confidence": 0.8048175772031149}]}, {"text": "Subsequent assertions in this paper about statistical significance indicate whether or not the p-value in question exceeds 1%.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Absolute Gain and Relative Gain", "labels": [], "entities": [{"text": "Absolute", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.961624264717102}, {"text": "Relative Gain", "start_pos": 28, "end_pos": 41, "type": "TASK", "confidence": 0.7043029069900513}]}, {"text": " Table 4: Results of feature combination", "labels": [], "entities": [{"text": "feature combination", "start_pos": 21, "end_pos": 40, "type": "TASK", "confidence": 0.7270952612161636}]}]}