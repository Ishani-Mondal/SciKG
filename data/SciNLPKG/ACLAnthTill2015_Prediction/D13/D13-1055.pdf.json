{"title": [{"text": "Automatically Classifying Edit Categories in Wikipedia Revisions", "labels": [], "entities": [{"text": "Automatically Classifying Edit Categories in Wikipedia Revisions", "start_pos": 0, "end_pos": 64, "type": "TASK", "confidence": 0.5748190070901599}]}], "abstractContent": [{"text": "In this paper, we analyze a novel set of features for the task of automatic edit category classification.", "labels": [], "entities": [{"text": "automatic edit category classification", "start_pos": 66, "end_pos": 104, "type": "TASK", "confidence": 0.58497504144907}]}, {"text": "Edit category classification assigns categories such as spelling error correction , paraphrase or vandalism to edits in a document.", "labels": [], "entities": [{"text": "Edit category classification", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.662287304798762}, {"text": "spelling error correction", "start_pos": 56, "end_pos": 81, "type": "TASK", "confidence": 0.6170550386110941}]}, {"text": "Our features are based on differences between two versions of a document including metadata, textual and language properties and markup.", "labels": [], "entities": []}, {"text": "Ina supervised machine learning experiment, we achieve a micro-averaged F1 score of .62 on a corpus of edits from the En-glish Wikipedia.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 72, "end_pos": 80, "type": "METRIC", "confidence": 0.9680474698543549}, {"text": "En-glish Wikipedia", "start_pos": 118, "end_pos": 136, "type": "DATASET", "confidence": 0.8819530010223389}]}, {"text": "In this corpus, each edit has been multi-labeled according to a 21-category taxonomy.", "labels": [], "entities": []}, {"text": "A model trained on the same data achieves state-of-the-art performance on the related task of fluency edit classification.", "labels": [], "entities": [{"text": "fluency edit classification", "start_pos": 94, "end_pos": 121, "type": "TASK", "confidence": 0.808494508266449}]}, {"text": "We apply pattern mining to automatically labeled edits in the revision histories of different Wikipedia articles.", "labels": [], "entities": [{"text": "pattern mining", "start_pos": 9, "end_pos": 23, "type": "TASK", "confidence": 0.7046821564435959}]}, {"text": "Our results suggest that high-quality articles show a higher degree of homogeneity with respect to their collaboration patterns as compared to random articles.", "labels": [], "entities": []}], "introductionContent": [{"text": "Due to its ever-evolving and collaboratively built content, Wikipedia has been the subject of many NLP studies.", "labels": [], "entities": []}, {"text": "While the number of newly created articles in the online encyclopedia declined in the last few years (, the number of edits in existing articles is rather stable.", "labels": [], "entities": []}, {"text": "It is reasonable to assume that the latter will not change in the near future.", "labels": [], "entities": []}, {"text": "One of the major reasons for the popularity of Wikipedia is its up-to-dateness (, which in turn requires constant editing activity.", "labels": [], "entities": []}, {"text": "Wikipedia's revision history stores all changes made to any page in the encyclopedia in separate revisions.", "labels": [], "entities": [{"text": "Wikipedia's revision history", "start_pos": 0, "end_pos": 28, "type": "DATASET", "confidence": 0.802447646856308}]}, {"text": "Previous studies have exploited revision history data in tasks such as preposition error correction (, spelling error correction or paraphrasing.", "labels": [], "entities": [{"text": "preposition error correction", "start_pos": 71, "end_pos": 99, "type": "TASK", "confidence": 0.5916345417499542}, {"text": "spelling error correction", "start_pos": 103, "end_pos": 128, "type": "TASK", "confidence": 0.624012420574824}]}, {"text": "However, they all use different approaches to extract the information needed for their task.", "labels": [], "entities": []}, {"text": "outline several applications benefiting from revision history data.", "labels": [], "entities": []}, {"text": "They argue fora unified approach to extract and classify edits from revision histories based on a predefined edit category taxonomy.", "labels": [], "entities": [{"text": "classify edits from revision histories", "start_pos": 48, "end_pos": 86, "type": "TASK", "confidence": 0.7436692357063294}]}, {"text": "In this work, we show how the extraction and automatic multi-label classification of any edit in Wikipedia can be handled with a single approach.", "labels": [], "entities": [{"text": "multi-label classification of any edit in Wikipedia", "start_pos": 55, "end_pos": 106, "type": "TASK", "confidence": 0.7792464452130454}]}, {"text": "Therefore, we use the 21-category edit classification taxonomy developed in previous work.", "labels": [], "entities": [{"text": "21-category edit classification", "start_pos": 22, "end_pos": 53, "type": "TASK", "confidence": 0.6046719749768575}]}, {"text": "This taxonomy enables a finegrained analysis of edit activity in revision histories.", "labels": [], "entities": [{"text": "revision histories", "start_pos": 65, "end_pos": 83, "type": "TASK", "confidence": 0.867941677570343}]}, {"text": "We present the results from an automatic classification experiment, based on an annotated corpus of edits in the English Wikipedia.", "labels": [], "entities": [{"text": "English Wikipedia", "start_pos": 113, "end_pos": 130, "type": "DATASET", "confidence": 0.9135730564594269}]}, {"text": "Additional information necessary to reproduce our results, including word lists and training, development and test data, is released online.", "labels": [], "entities": []}, {"text": "To the best of our knowledge, this is the first approach allowing to classify each single edit in Wikipedia into one or more of 21 different edit categories using a supervised machine learning approach.", "labels": [], "entities": []}, {"text": "We define our task as edit category classification.", "labels": [], "entities": [{"text": "edit category classification", "start_pos": 22, "end_pos": 50, "type": "TASK", "confidence": 0.8027994831403097}]}, {"text": "An edit is a coherent, local change which modifies a document and which can be related to certain metadata (e.g. its author, timestamp etc.).", "labels": [], "entities": []}, {"text": "In edit category classification, we aim to detect all n edits e k v\u22121,v with 0 \u2264 k < n in adjacent versions r v\u22121 , r v of a document (we refer to the older revision as r v\u22121 and to the newer as r v ) and assign each of them to one or more edit categories.", "labels": [], "entities": [{"text": "edit category classification", "start_pos": 3, "end_pos": 31, "type": "TASK", "confidence": 0.7128675282001495}]}, {"text": "There exist at least two main applications of edit category classification: First, a fine-grained classification of edits in collaboratively created documents such as Wikipedia articles, scientific papers or research proposals, would help us to better understand the collaborative writing process.", "labels": [], "entities": [{"text": "edit category classification", "start_pos": 46, "end_pos": 74, "type": "TASK", "confidence": 0.6880555947621664}, {"text": "fine-grained classification of edits in collaboratively created documents such as Wikipedia articles, scientific papers or research proposals", "start_pos": 85, "end_pos": 226, "type": "TASK", "confidence": 0.6852631866931915}]}, {"text": "This includes answers to questions about the kind of contribution of individual authors (Who has added substantial contents?, Who has improved stylistic issues?) and about the kind of collaboration which characterizes different articles ().", "labels": [], "entities": []}, {"text": "Second, automatic classification of edits generates huge amounts of training data for the above mentioned NLP systems.", "labels": [], "entities": [{"text": "classification of edits", "start_pos": 18, "end_pos": 41, "type": "TASK", "confidence": 0.7578489780426025}]}, {"text": "Edit category classification is related to the better known task of document pair classification.", "labels": [], "entities": [{"text": "Edit category classification", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.748572826385498}, {"text": "document pair classification", "start_pos": 68, "end_pos": 96, "type": "TASK", "confidence": 0.6795530120531718}]}, {"text": "In document pair classification, a pair of documents has to be assigned to one or more categories (e.g. paraphrase/non-paraphrase, plagiarism/nonplagiarism).", "labels": [], "entities": [{"text": "document pair classification", "start_pos": 3, "end_pos": 31, "type": "TASK", "confidence": 0.7415807247161865}]}, {"text": "Here, the document maybe a very short text, such as a sentence or a single word.", "labels": [], "entities": []}, {"text": "Applications of document pair classification include plagiarism detection), paraphrase detection () or text similarity detection.", "labels": [], "entities": [{"text": "document pair classification", "start_pos": 16, "end_pos": 44, "type": "TASK", "confidence": 0.7306290964285532}, {"text": "plagiarism detection", "start_pos": 53, "end_pos": 73, "type": "TASK", "confidence": 0.7785309553146362}, {"text": "paraphrase detection", "start_pos": 76, "end_pos": 96, "type": "TASK", "confidence": 0.7813016772270203}, {"text": "text similarity detection", "start_pos": 103, "end_pos": 128, "type": "TASK", "confidence": 0.7712016900380453}]}, {"text": "In edit category classification, we also have two documents.", "labels": [], "entities": [{"text": "edit category classification", "start_pos": 3, "end_pos": 31, "type": "TASK", "confidence": 0.7610600193341573}]}, {"text": "However, these documents are different versions of the same text.", "labels": [], "entities": []}, {"text": "This scenario implies certain characteristics fora well-designed feature set as we will demonstrate in this study.", "labels": [], "entities": []}, {"text": "The main contributions of this paper are: First, we introduce a novel feature set for edit category classification.", "labels": [], "entities": [{"text": "edit category classification", "start_pos": 86, "end_pos": 114, "type": "TASK", "confidence": 0.8005855679512024}]}, {"text": "Second, we evaluate the performance of this feature set on different tasks within a corpus of Wikipedia edits.", "labels": [], "entities": []}, {"text": "We propose the new task of edit category classification and show that our model is able to classify edits from a 21-category taxonomy.", "labels": [], "entities": [{"text": "edit category classification", "start_pos": 27, "end_pos": 55, "type": "TASK", "confidence": 0.7670717040697733}]}, {"text": "Furthermore, our model achieves state-of-theart performance in a fluency edit classification task.", "labels": [], "entities": [{"text": "fluency edit classification task", "start_pos": 65, "end_pos": 97, "type": "TASK", "confidence": 0.8016751855611801}]}, {"text": "Third, we analyze collaboration patterns based on edit categories on two subsets of Wikipedia articles, namely featured and non-featured articles.", "labels": [], "entities": []}, {"text": "We detect correlations between collaboration patterns and high-quality articles.", "labels": [], "entities": []}, {"text": "This is demonstrated by the fact that featured articles have a higher degree of homogeneity with respect to their collaboration patterns as compared to random articles.", "labels": [], "entities": []}, {"text": "The rest of this paper is structured as follows.", "labels": [], "entities": []}, {"text": "In Section 2, we motivate our experiments based on previous work.", "labels": [], "entities": []}, {"text": "Section 3 explains our training data and the features we use for the machine learning experiments.", "labels": [], "entities": []}, {"text": "In Section 4, we present and discuss the results of our experiments.", "labels": [], "entities": []}, {"text": "We also demonstrate an application of our classifier model in Section 5 by mining frequent collaboration patterns in the revision histories of different articles.", "labels": [], "entities": []}, {"text": "Finally, we draw a conclusion in Section 6.", "labels": [], "entities": []}], "datasetContent": [{"text": "We extract features with the help of ClearTK (: Overall classification results with 3 multi-label classifiers and a C4.5 decision tree base classifier, as compared to random and majority category baselines.", "labels": [], "entities": [{"text": "ClearTK", "start_pos": 37, "end_pos": 44, "type": "DATASET", "confidence": 0.7998969554901123}]}, {"text": "Multi-label classification problems are solved by either transforming the multi-label classification task into one or more single-label classification tasks (problem transformation method) or by adapting single-label classification algorithms (algorithm adaption method).", "labels": [], "entities": [{"text": "Multi-label classification", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.775824636220932}, {"text": "multi-label classification task", "start_pos": 74, "end_pos": 105, "type": "TASK", "confidence": 0.7829612294832865}]}, {"text": "Several algorithms have been developed on top of the former methods and use ensembles of such classifiers (ensemble methods).", "labels": [], "entities": []}, {"text": "We applied the Binary Relevance approach (BR), a simple transformation method which converts the multi-label problem into |C| binary single-label problems, where |C| is the number of categories.", "labels": [], "entities": [{"text": "Binary Relevance approach (BR", "start_pos": 15, "end_pos": 44, "type": "METRIC", "confidence": 0.5582977831363678}]}, {"text": "Hence, this method trains a classifier for each category in the corpus (one-against-all).", "labels": [], "entities": []}, {"text": "It is the most straightforward approach when dealing with multi-labeled data.", "labels": [], "entities": []}, {"text": "However, it does not consider possible relationships or dependencies between categories.", "labels": [], "entities": []}, {"text": "Therefore, we tested two more sophisticated methods.", "labels": [], "entities": []}, {"text": "Hierarchy of multi-label classifiers HOMER () is a problem transformation method.", "labels": [], "entities": [{"text": "Hierarchy of multi-label classifiers HOMER", "start_pos": 0, "end_pos": 42, "type": "TASK", "confidence": 0.5419065415859222}, {"text": "problem transformation", "start_pos": 51, "end_pos": 73, "type": "TASK", "confidence": 0.7463542520999908}]}, {"text": "It accounts for possibly hierarchical relationships among categories by dividing the overall category set into a tree-like structure with nodes of small category sets of size k and leaves of single categories.", "labels": [], "entities": []}, {"text": "Subsequently, a multi-label classifier is applied to each node in the tree.", "labels": [], "entities": []}, {"text": "Random k-labelsets RAKEL () is an ensemble method, which randomly chooses l typically small subsets with k categories from the overall set of categories.", "labels": [], "entities": [{"text": "RAKEL", "start_pos": 19, "end_pos": 24, "type": "TASK", "confidence": 0.8736675381660461}]}, {"text": "Subsequently, all k-labelsets which are found in the multi-labeled data set are converted into new categories in a single-labeled data set using the la-bel powerset transformation (.", "labels": [], "entities": []}, {"text": "HOMER and BR are among the multi-label classifiers, which recommend as benchmark methods.", "labels": [], "entities": [{"text": "HOMER", "start_pos": 0, "end_pos": 5, "type": "METRIC", "confidence": 0.7855187654495239}, {"text": "BR", "start_pos": 10, "end_pos": 12, "type": "METRIC", "confidence": 0.9960786700248718}]}, {"text": "As underlying single-label classification algorithm, we used a C4.5 decision tree classifier, as decision tree classifiers yield state-of-the-art performance in the related work.", "labels": [], "entities": [{"text": "single-label classification", "start_pos": 14, "end_pos": 41, "type": "TASK", "confidence": 0.7060861885547638}]}, {"text": "Multi-label Evaluation We denote the set of relevant categories for each edit e i \u2208 E as y i \u2208 C and the set of predicted categories as h(e i ).", "labels": [], "entities": []}, {"text": "Evaluation measures for multi-label classification systems are based on either bipartitions or rankings.", "labels": [], "entities": [{"text": "multi-label classification", "start_pos": 24, "end_pos": 50, "type": "TASK", "confidence": 0.7122816145420074}]}, {"text": "Among the former, we report examplebased (weighting each edit equally) and label-based (weighting each edit category equally) measures.", "labels": [], "entities": []}, {"text": "The accuracy of a multi-label classifier is defined as 1 |h(e i )\u222ay i | , which corresponds to the Jaccard similarity of h(e i ) and y i averaged overall edits.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.9993439316749573}]}, {"text": "We report subset accuracy (exact match), calculated as 1 , and F1 as 1 |h(e i )|+|y i | . For the label-based measures, we report macroand micro-averaged F1 scores.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 17, "end_pos": 25, "type": "METRIC", "confidence": 0.8256686925888062}, {"text": "exact match)", "start_pos": 27, "end_pos": 39, "type": "METRIC", "confidence": 0.925935427347819}, {"text": "F1", "start_pos": 63, "end_pos": 65, "type": "METRIC", "confidence": 0.9997373223304749}, {"text": "F1", "start_pos": 154, "end_pos": 156, "type": "METRIC", "confidence": 0.9253150224685669}]}, {"text": "As a ranking-based measure, we report one error, which is defined as expr is true and expr = 0 otherwise.", "labels": [], "entities": []}, {"text": "f (e i , c) denotes the rank of category c \u2208 C as predicted by the classifier.", "labels": [], "entities": []}, {"text": "The one error measure evaluates the number of edits where the highest ranked category in the predictions is not in the set of relevant categories.", "labels": [], "entities": []}, {"text": "It becomes smaller when the performance of the classifier increases.", "labels": [], "entities": []}, {"text": "shows the overall classification scores.", "labels": [], "entities": []}, {"text": "We calculated a random baseline, which multi-labels edits at random considering the label powerset frequencies it has learned from the training data.", "labels": [], "entities": []}, {"text": "Furthermore, we calculated a majority category baseline, which labels all edits with the most frequent edit category in the training data.", "labels": [], "entities": []}, {"text": "In, we list the results for each category, together with the average pair-wise inter-rater agreement (F1 scores).", "labels": [], "entities": [{"text": "pair-wise inter-rater agreement", "start_pos": 69, "end_pos": 100, "type": "METRIC", "confidence": 0.6354182263215383}, {"text": "F1 scores)", "start_pos": 102, "end_pos": 112, "type": "METRIC", "confidence": 0.9307233293851217}]}, {"text": "The F1 scores are calculated based on the study we carried out in.", "labels": [], "entities": [{"text": "F1", "start_pos": 4, "end_pos": 6, "type": "METRIC", "confidence": 0.9995506405830383}]}, {"text": "Parameters and Feature selection All parameters have been adjusted on the development set using the RAKEL classifier, aiming to optimize accuracy.", "labels": [], "entities": [{"text": "Parameters", "start_pos": 0, "end_pos": 10, "type": "METRIC", "confidence": 0.9665665626525879}, {"text": "accuracy", "start_pos": 137, "end_pos": 145, "type": "METRIC", "confidence": 0.9971314668655396}]}, {"text": "With respect to the n-gram features, we tested values for n = 1, 2 and 3.", "labels": [], "entities": []}, {"text": "For comment n-grams, unigrams turned out to yield the best overall performance, and bigrams for character and token ngrams.", "labels": [], "entities": []}, {"text": "The word and character n-gram spaces are limited to the 500 most frequent items, the comment n-gram space is limited to the 1,500 most frequent items.", "labels": [], "entities": []}, {"text": "To transform ranked output into bipartitions, it is necessary to set a threshold.", "labels": [], "entities": []}, {"text": "This threshold is reported in and has been optimized for each classifier with respect to label cardinality (average number of labels assigned to edits) on the development set.", "labels": [], "entities": []}, {"text": "Since most of the traditional feature selection methods cannot be applied directly to multilabeled data, we used the label powerset approach to transform the multi-labeled data into single-labeled data and subsequently applied \u03c7 2 . Feature reduction to the highest-ranked features clearly improved the classifier performance on the development set.", "labels": [], "entities": []}, {"text": "We therefore limited the feature space to the 150 highest-ranked features in our experiments.", "labels": [], "entities": []}, {"text": "For the RAKEL classifier, we set l = 42 (twice the size of the category set) and k = 3.", "labels": [], "entities": [{"text": "RAKEL classifier", "start_pos": 8, "end_pos": 24, "type": "TASK", "confidence": 0.6079478859901428}]}, {"text": "In HOMER, we used BR as transformation method, random distribution of categories to the children nodes and k = 3.", "labels": [], "entities": [{"text": "HOMER", "start_pos": 3, "end_pos": 8, "type": "DATASET", "confidence": 0.8517983555793762}, {"text": "BR", "start_pos": 18, "end_pos": 20, "type": "METRIC", "confidence": 0.9981836676597595}]}, {"text": "For all other classifier parameters, we used the default settings as configured in Meka respective Mulan.", "labels": [], "entities": [{"text": "Meka respective Mulan", "start_pos": 83, "end_pos": 104, "type": "DATASET", "confidence": 0.8724567890167236}]}], "tableCaptions": [{"text": " Table 2: Statistics of the training, test and development  set. Cardinality is the average number of edit categories  assigned to an edit.", "labels": [], "entities": [{"text": "Cardinality", "start_pos": 65, "end_pos": 76, "type": "METRIC", "confidence": 0.9690009355545044}]}, {"text": " Table 3: Overall classification results with 3 multi-label  classifiers and a C4.5 decision tree base classifier, as com- pared to random and majority category baselines.", "labels": [], "entities": []}, {"text": " Table 1. The number of edits labeled with each category in the test set is given in brackets. The FILE-M  and TEMPLATE-M categories are omitted in this", "labels": [], "entities": [{"text": "FILE-M", "start_pos": 99, "end_pos": 105, "type": "METRIC", "confidence": 0.9685561656951904}, {"text": "TEMPLATE-M", "start_pos": 111, "end_pos": 121, "type": "METRIC", "confidence": 0.8470622301101685}]}]}