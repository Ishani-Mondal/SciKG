{"title": [{"text": "Modeling and Learning Semantic Co-Compositionality through Prototype Projections and Neural Networks", "labels": [], "entities": []}], "abstractContent": [{"text": "We present a novel vector space model for semantic co-compositionality.", "labels": [], "entities": []}, {"text": "Inspired by Gen-erative Lexicon Theory (Pustejovsky, 1995), our goal is a compositional model where both predicate and argument are allowed to modify each others' meaning representations while generating the overall semantics.", "labels": [], "entities": []}, {"text": "This readily addresses some major challenges with current vector space models, notably the pol-ysemy issue and the use of one representation per word type.", "labels": [], "entities": []}, {"text": "We implement co-compositionality using prototype projections on predicates/arguments and show that this is effective in adapting their word representations.", "labels": [], "entities": []}, {"text": "We further cast the model as a neural network and propose an unsupervised algorithm to jointly train word representations with co-compositionality.", "labels": [], "entities": []}, {"text": "The model achieves the best result to date (\u03c1 = 0.47) on the semantic similarity task of transitive verbs (Grefenstette and Sadrzadeh, 2011).", "labels": [], "entities": []}], "introductionContent": [{"text": "Vector space models of words have been very successful in capturing the semantic and syntactic characteristics of individual lexical items.", "labels": [], "entities": []}, {"text": "Much research has addressed the question of how to construct individual word representations, for example distributional models) and neural models: Here, we capture the semantics of run in run company by projecting the original word representation of run to the prototype space of company (and vice versa).", "labels": [], "entities": []}, {"text": "Recently, modeling of semantic compositionality in vector space has emerged as another important line of research ().", "labels": [], "entities": [{"text": "modeling of semantic compositionality", "start_pos": 10, "end_pos": 47, "type": "TASK", "confidence": 0.6480546817183495}]}, {"text": "The goal is to formulate how individual word representations ought to be combined to achieve phrasal or sentential semantics.", "labels": [], "entities": []}, {"text": "The main questions for semantic compositionality that we are concerned with are: (1) how can polysemy be handled by a single vector representation per word type, learned by either a distributional or neural model, and (2) how does composition resolve these ambiguities.", "labels": [], "entities": [{"text": "semantic compositionality", "start_pos": 23, "end_pos": 48, "type": "TASK", "confidence": 0.7142291814088821}]}, {"text": "To this end, we are inspired by the idea of type coercion and co-compositionality in Generative Lexicon Theory.", "labels": [], "entities": [{"text": "Generative Lexicon Theory", "start_pos": 85, "end_pos": 110, "type": "TASK", "confidence": 0.7147265672683716}]}, {"text": "Co-compositionality advocates that instead of a predicate-argument view of composition, both predicate and argument influence/coerce each other to generate the overall meaning.", "labels": [], "entities": []}, {"text": "For example, consider a polysemous word like run: \u2022 (a) He runs the company.", "labels": [], "entities": []}, {"text": "\u2022 (b) He runs the marathon.", "labels": [], "entities": []}, {"text": "Run may have several senses, but the prototypical verbs that select for company differ from those that select for marathon, and thus the ambiguity at the word level is resolved at the sentence level.", "labels": [], "entities": []}, {"text": "The same is true for the other direction, where the predicate also coerces meaning to the argument to fit expectation.", "labels": [], "entities": []}, {"text": "We believe that models for semantic composition ought to incorporate elements of cocompositionality.", "labels": [], "entities": [{"text": "semantic composition", "start_pos": 27, "end_pos": 47, "type": "TASK", "confidence": 0.7088846415281296}]}, {"text": "We propose such a model here, using what we call prototype projections.", "labels": [], "entities": []}, {"text": "For each predicate, we transform its vector representation by projecting it into a latent space that is prototypical of its argument.", "labels": [], "entities": []}, {"text": "This projection is performed analogously for each argument as well, and the final meaning is computed by composition of these transformed vectors).", "labels": [], "entities": []}, {"text": "In addition, the model is cast as a neural network where word representations could be re-trained or fine-tuned.", "labels": [], "entities": []}, {"text": "Our contributions are two-fold: 1.", "labels": [], "entities": []}, {"text": "We propose a novel model for semantic cocompositionality.", "labels": [], "entities": []}, {"text": "This model, based on prototype projections, is easy to implement and achieves state-of-the-art performance in the sentence similarity dataset developed by Grefenstette and Sadrzadeh (2011).", "labels": [], "entities": []}], "datasetContent": [{"text": "In order to evaluate the performance of our new co-compositional model with prototype projection and word representation learning algorithm, we make use of the disambiguation task of transitive sentences developed by.", "labels": [], "entities": [{"text": "word representation learning", "start_pos": 101, "end_pos": 129, "type": "TASK", "confidence": 0.7424851556619009}]}, {"text": "This is an extension of the two words phrase similarity task defined in, and constructed according to similar guidelines.", "labels": [], "entities": [{"text": "phrase similarity task", "start_pos": 38, "end_pos": 60, "type": "TASK", "confidence": 0.7685282925764719}]}, {"text": "The dataset consists of similarity judgments between a landmark verb and a triple consisting of a transitive target verb, subject and object extracted from the BNC corpus.", "labels": [], "entities": [{"text": "BNC corpus", "start_pos": 160, "end_pos": 170, "type": "DATASET", "confidence": 0.9506825804710388}]}, {"text": "Human judges give scores between 1 to 7, with higher scores implying higher semantic similarity.", "labels": [], "entities": []}, {"text": "For example, shows some examples from the data: we see that the verb meet with subject system and object criterion is judged similar to the landmark verb satisfy but not visit.", "labels": [], "entities": []}, {"text": "The dataset contains a total of 2500 similarity judgements, provided by 25 participants.", "labels": [], "entities": [{"text": "similarity", "start_pos": 37, "end_pos": 47, "type": "METRIC", "confidence": 0.9748729467391968}]}, {"text": "The task is to have the model produce a score for each pair of landmark verb and verb-subject-object triple.", "labels": [], "entities": []}, {"text": "Models are evaluated by computing the Spearman's \u03c1 correlation between its similarity scores and that of the human judgments.", "labels": [], "entities": [{"text": "Spearman's \u03c1 correlation", "start_pos": 38, "end_pos": 62, "type": "METRIC", "confidence": 0.6450930461287498}, {"text": "similarity scores", "start_pos": 75, "end_pos": 92, "type": "METRIC", "confidence": 0.8921257555484772}]}, {"text": "verb subj obj landmark sim meet system criterion satisfy 6 meet system criterion visit 1 write student name spell 7 write student paper spell 2: Examples from the disambiguation task developed by.", "labels": [], "entities": []}, {"text": "Human judges give scores between 1 to 7, with higher scores implying higher semantic similarity.", "labels": [], "entities": []}, {"text": "Verb meet with subject system and object criterion is judged similar to the landmark verb satisfy but not visit.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Examples of verb-object pairs. Original verb and landmark verb similarity, prototype projected verb and  landmark verb similarity, as measure by cosine using Collobert and Weston's word embeddings. Meet has a abstract  meaning itself, but after prototype projection with matrix constructed by word vectors of W (VerbOf, criterion), meet  is more close to meaning of satisfy.", "labels": [], "entities": []}, {"text": " Table 5: Variants of the full co-compositional model,  based on how subject, verb, and object vector repre- sentations are included. prpj indicates that prototype  projection is used. + indicates that the vector is added  without projection first. Blank indicates that the vector is  not used in the final compositional score.", "labels": [], "entities": [{"text": "Blank", "start_pos": 249, "end_pos": 254, "type": "METRIC", "confidence": 0.9983452558517456}]}]}