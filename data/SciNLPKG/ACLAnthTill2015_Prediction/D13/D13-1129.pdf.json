{"title": [{"text": "Semi-supervised Feature Transformation for Dependency Parsing", "labels": [], "entities": [{"text": "Feature Transformation", "start_pos": 16, "end_pos": 38, "type": "TASK", "confidence": 0.6303988099098206}, {"text": "Dependency Parsing", "start_pos": 43, "end_pos": 61, "type": "TASK", "confidence": 0.7216652929782867}]}], "abstractContent": [{"text": "In current dependency parsing models, conventional features (i.e. base features) defined over surface words and part-of-speech tags in a relatively high-dimensional feature space may suffer from the data sparseness problem and thus exhibit less discriminative power on unseen data.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 11, "end_pos": 29, "type": "TASK", "confidence": 0.6918891370296478}]}, {"text": "In this paper, we propose a novel semi-supervised approach to addressing the problem by transforming the base features into high-level features (i.e. meta features) with the help of a large amount of automatically parsed data.", "labels": [], "entities": []}, {"text": "The meta features are used together with base features in our final parser.", "labels": [], "entities": []}, {"text": "Our studies indicate that our proposed approach is very effective in processing unseen data and features.", "labels": [], "entities": []}, {"text": "Experiments on Chi-nese and English data sets show that the final parser achieves the best-reported accuracy on the Chinese data and comparable accuracy with the best known parsers on the English data.", "labels": [], "entities": [{"text": "English data sets", "start_pos": 28, "end_pos": 45, "type": "DATASET", "confidence": 0.8312707940737406}, {"text": "accuracy", "start_pos": 100, "end_pos": 108, "type": "METRIC", "confidence": 0.9973604083061218}, {"text": "accuracy", "start_pos": 144, "end_pos": 152, "type": "METRIC", "confidence": 0.9988589286804199}]}], "introductionContent": [{"text": "In recent years, supervised learning models have achieved lots of progress in the dependency parsing task, as can be found in the CoNLL shared tasks (.", "labels": [], "entities": [{"text": "dependency parsing task", "start_pos": 82, "end_pos": 105, "type": "TASK", "confidence": 0.8359447320302328}]}, {"text": "The supervised models take annotated data as training data, utilize features defined over surface words, part-of-speech tags, and dependency trees, and learn the preference of features via adjusting feature weights.", "labels": [], "entities": []}, {"text": "In the supervised learning scenarios, many previous studies explore rich feature representation that leads to significant improvements. and define secondorder features over two adjacent arcs in secondorder graph-based models.", "labels": [], "entities": []}, {"text": "use third-order features in a third-order graph-based model.", "labels": [], "entities": []}, {"text": "considers information of more surrounding words for the graph-based models, while Zhang and Nivre (2011) define a set of rich features including the word valency and the third-order context features for transition-based models.", "labels": [], "entities": []}, {"text": "All these models utilize richer and more complex feature representations and achieve better performance than the earlier models that utilize the simpler features).", "labels": [], "entities": []}, {"text": "However, the richer feature representations result in a high-dimensional feature space.", "labels": [], "entities": []}, {"text": "Features in such a space may suffer from the data sparseness problem and thus have less discriminative power on unseen data.", "labels": [], "entities": []}, {"text": "If input sentences contain unknown features that are not included in training data, the parsers can usually give lower accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 119, "end_pos": 127, "type": "METRIC", "confidence": 0.995731770992279}]}, {"text": "Several methods have been proposed to alleviate this problem by using large amounts of unannotated data, ranging from self-training and co-training) to more complex methods that collect statistical information from unannotated sentences and use them as additional features (.", "labels": [], "entities": []}, {"text": "In this paper, we propose an alternative approach to semi-supervised dependency parsing via feature transformation ().", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 69, "end_pos": 87, "type": "TASK", "confidence": 0.6597670614719391}]}, {"text": "More specifically, we transform base features to a higherlevel space.", "labels": [], "entities": []}, {"text": "The base features defined over surface words, part-of-speech tags, and dependency trees are high dimensional and have been explored in the above previous studies.", "labels": [], "entities": []}, {"text": "The higher-level features, which we call meta features, are low dimensional, and newly defined in this paper.", "labels": [], "entities": []}, {"text": "The key idea behind is that we build connections between known and unknown base features via the meta features.", "labels": [], "entities": []}, {"text": "From another viewpoint, we can also interpret the meta features as away of doing feature smoothing.", "labels": [], "entities": []}, {"text": "Our feature transfer method is simpler than that of Ando and, which is based on splitting the original problem into multiple auxiliary problems.", "labels": [], "entities": [{"text": "feature transfer", "start_pos": 4, "end_pos": 20, "type": "TASK", "confidence": 0.704871729016304}]}, {"text": "In our approach, the base features are grouped and each group relates to a meta feature.", "labels": [], "entities": []}, {"text": "In the first step, we use a baseline parser to parse a large amount of unannotated sentences.", "labels": [], "entities": []}, {"text": "Then we collect the base features from the parse trees.", "labels": [], "entities": []}, {"text": "The collected features are transformed into predefined discrete values via a transformation function.", "labels": [], "entities": []}, {"text": "Based on the transformed values, we define a set of meta features.", "labels": [], "entities": []}, {"text": "Finally, the meta features are incorporated directly into parsing models.", "labels": [], "entities": []}, {"text": "To demonstrate the effectiveness of the proposed approach, we apply it to the graph-based parsing models . We conduct experiments on the standard data split of the Penn English Treebank () and the Chinese Treebank Version 5.1 ().", "labels": [], "entities": [{"text": "Penn English Treebank", "start_pos": 164, "end_pos": 185, "type": "DATASET", "confidence": 0.982030193010966}, {"text": "Chinese Treebank Version 5.1", "start_pos": 197, "end_pos": 225, "type": "DATASET", "confidence": 0.9717910587787628}]}, {"text": "The results indicate that the approach significantly improves the accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 66, "end_pos": 74, "type": "METRIC", "confidence": 0.9987446069717407}]}, {"text": "In summary, we make the following contributions: \u2022 We define a simple yet useful transformation function to transform base features to meta features automatically.", "labels": [], "entities": []}, {"text": "The meta features build connections between known and unknown base features, and relieve the data sparseness problem.", "labels": [], "entities": []}, {"text": "\u2022 Compared to the base features, the number of meta features is remarkably small.", "labels": [], "entities": []}, {"text": "\u2022 We build semi-supervised dependency parsers that achieve the best accuracy on the Chinese data and comparable accuracy with the best known systems on the English data.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 68, "end_pos": 76, "type": "METRIC", "confidence": 0.9979052543640137}, {"text": "Chinese data", "start_pos": 84, "end_pos": 96, "type": "DATASET", "confidence": 0.7452307939529419}, {"text": "accuracy", "start_pos": 112, "end_pos": 120, "type": "METRIC", "confidence": 0.9989650249481201}]}, {"text": "The rest of this paper is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 introduces the graph-based parsing model.", "labels": [], "entities": []}, {"text": "Section 3 describes the meta features and meta parser.", "labels": [], "entities": []}, {"text": "Section 4 describes the experiment settings and reports the experimental results on English and Chinese data sets.", "labels": [], "entities": [{"text": "English and Chinese data sets", "start_pos": 84, "end_pos": 113, "type": "DATASET", "confidence": 0.7068851113319397}]}, {"text": "Section 5 discusses related work.", "labels": [], "entities": []}, {"text": "Finally, in Section 6 we summarize the proposed approach.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluated the effect of the meta features for the graph-based parsers on English and Chinese data.", "labels": [], "entities": []}, {"text": "In our experiments, we used the Penn Treebank (PTB) () for English and the Chinese Treebank version 5.1 (CTB5) () for Chinese.", "labels": [], "entities": [{"text": "Penn Treebank (PTB)", "start_pos": 32, "end_pos": 51, "type": "DATASET", "confidence": 0.9673721551895141}, {"text": "Chinese Treebank version 5.1 (CTB5)", "start_pos": 75, "end_pos": 110, "type": "DATASET", "confidence": 0.9336976153509957}]}, {"text": "The tool \"Penn2Malt\" 1 was used 1 http://w3.msi.vxu.se/\u02dcnivre/research/Penn2Malt.html to convert the data into dependency structures with the English head rules of and the Chinese head rules of.", "labels": [], "entities": [{"text": "Penn2Malt", "start_pos": 10, "end_pos": 19, "type": "DATASET", "confidence": 0.9394727945327759}]}, {"text": "We followed the standard data splits as shown in.", "labels": [], "entities": []}, {"text": "Following the work of, we used a tagger trained on training data to provide part-of-speech (POS) tags for the development and test sets, and used 10-way jackknifing to generate part-of-speech tags for the training set.", "labels": [], "entities": []}, {"text": "We used the MXPOST (Ratnaparkhi, 1996) tagger for English and the CRF-based tagger for Chinese.", "labels": [], "entities": [{"text": "MXPOST (Ratnaparkhi, 1996) tagger", "start_pos": 12, "end_pos": 45, "type": "DATASET", "confidence": 0.916573064667838}]}, {"text": "We used gold standard segmentation in the CTB5.", "labels": [], "entities": [{"text": "CTB5", "start_pos": 42, "end_pos": 46, "type": "DATASET", "confidence": 0.9834363460540771}]}, {"text": "The data partition of Chinese were chosen to match previous work (  For the unannotated data in English, we used the BLLIP WSJ corpus () containing about 43 million words.", "labels": [], "entities": [{"text": "BLLIP WSJ corpus", "start_pos": 117, "end_pos": 133, "type": "DATASET", "confidence": 0.709666113058726}]}, {"text": "We used the MXPOST tagger trained on the training data to assign part-ofspeech tags and used the Baseline parser to process the sentences of the Brown corpus.", "labels": [], "entities": [{"text": "Brown corpus", "start_pos": 145, "end_pos": 157, "type": "DATASET", "confidence": 0.9397633671760559}]}, {"text": "For the unannotated data in Chinese, we used the Xinhua portion of Chinese Gigaword 3 Version 2.0 (LDC2009T14), which has approximately 311 million words.", "labels": [], "entities": [{"text": "Xinhua portion of Chinese Gigaword 3 Version 2.0 (LDC2009T14)", "start_pos": 49, "end_pos": 110, "type": "DATASET", "confidence": 0.845031662420793}]}, {"text": "We used the MMA system () trained on the training data to perform word segmentation and POS tagging and used the Baseline parser to parse the sentences in the Gigaword data.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 66, "end_pos": 83, "type": "TASK", "confidence": 0.758091926574707}, {"text": "POS tagging", "start_pos": 88, "end_pos": 99, "type": "TASK", "confidence": 0.7988560795783997}, {"text": "Gigaword data", "start_pos": 159, "end_pos": 172, "type": "DATASET", "confidence": 0.9452061653137207}]}, {"text": "In collecting the base features, we removed the features which occur only once in the English data and less than four times in the Chinese data.", "labels": [], "entities": [{"text": "English data", "start_pos": 86, "end_pos": 98, "type": "DATASET", "confidence": 0.7519352734088898}]}, {"text": "The feature occurrences of onetime and four times are based on the development data performance.", "labels": [], "entities": []}, {"text": "We measured the parser quality by the unlabeled attachment score (UAS), i.e., the percentage of to-kens (excluding all punctuation tokens) with the correct HEAD.", "labels": [], "entities": [{"text": "unlabeled attachment score (UAS)", "start_pos": 38, "end_pos": 70, "type": "METRIC", "confidence": 0.7800005525350571}, {"text": "HEAD", "start_pos": 156, "end_pos": 160, "type": "METRIC", "confidence": 0.8685056567192078}]}, {"text": "We also reported the scores on complete dependency trees evaluation (COMP).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: Standard data splits", "labels": [], "entities": []}, {"text": " Table 5: Effect of different categories of meta features", "labels": [], "entities": []}, {"text": " Table 6: Numbers of meta features", "labels": [], "entities": []}, {"text": " Table 7: Main results on English", "labels": [], "entities": [{"text": "English", "start_pos": 26, "end_pos": 33, "type": "DATASET", "confidence": 0.9255369901657104}]}, {"text": " Table 8. As in the ex- periment on English, the meta parser outperformed  the baseline. We obtained an absolute improvement  of 2.07 points (UAS). The improvement was signif- icant in", "labels": [], "entities": [{"text": "UAS)", "start_pos": 142, "end_pos": 146, "type": "METRIC", "confidence": 0.9793983995914459}]}, {"text": " Table 9: Effect of different sizes of auto-parsed data", "labels": [], "entities": []}, {"text": " Table 10: Relevant results for English. Sup denotes the  supervised parsers, Semi denotes the parsers with semi- supervised methods.", "labels": [], "entities": []}, {"text": " Table 11: Relevant results for Chinese", "labels": [], "entities": [{"text": "Relevant", "start_pos": 11, "end_pos": 19, "type": "METRIC", "confidence": 0.9856852293014526}]}]}