{"title": [{"text": "Improving Statistical Machine Translation with Word Class Models", "labels": [], "entities": [{"text": "Improving Statistical Machine Translation", "start_pos": 0, "end_pos": 41, "type": "TASK", "confidence": 0.8887201249599457}]}], "abstractContent": [{"text": "Automatically clustering words from a mono-lingual or bilingual training corpus into classes is a widely used technique in statistical natural language processing.", "labels": [], "entities": [{"text": "Automatically clustering words from a mono-lingual or bilingual training", "start_pos": 0, "end_pos": 72, "type": "TASK", "confidence": 0.7315274675687155}, {"text": "statistical natural language processing", "start_pos": 123, "end_pos": 162, "type": "TASK", "confidence": 0.7206532061100006}]}, {"text": "We present a very simple and easy to implement method for using these word classes to improve translation quality.", "labels": [], "entities": []}, {"text": "It can be applied across different machine translation paradigms and with arbitrary types of models.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 35, "end_pos": 54, "type": "TASK", "confidence": 0.7086292207241058}]}, {"text": "We show its efficacy on a small German\u2192English and a larger French\u2192German translation task with both standard phrase-based and hierarchical phrase-based translation systems fora common set of models.", "labels": [], "entities": []}, {"text": "Our results show that with word class models, the baseline can be improved by up to 1.4% BLEU and 1.0% TER on the French\u2192German task and 0.3% BLEU and 1.1% TER on the German\u2192English task.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 89, "end_pos": 93, "type": "METRIC", "confidence": 0.9987658262252808}, {"text": "TER", "start_pos": 103, "end_pos": 106, "type": "METRIC", "confidence": 0.9880367517471313}, {"text": "BLEU", "start_pos": 142, "end_pos": 146, "type": "METRIC", "confidence": 0.9985254406929016}, {"text": "TER", "start_pos": 156, "end_pos": 159, "type": "METRIC", "confidence": 0.9822673201560974}]}], "introductionContent": [{"text": "Data sparsity is one of the major problems for statistical learning methods in natural language processing (NLP) today.", "labels": [], "entities": [{"text": "natural language processing (NLP)", "start_pos": 79, "end_pos": 112, "type": "TASK", "confidence": 0.7671130100886027}]}, {"text": "Even with the huge training data sets available in some tasks, for many phenomena that need to be modeled only few training instances can be observed.", "labels": [], "entities": []}, {"text": "This is partly due to the large vocabularies of natural languages.", "labels": [], "entities": []}, {"text": "One possiblity to reduce the sparsity for model estimation is to reduce the vocabulary size.", "labels": [], "entities": [{"text": "model estimation", "start_pos": 42, "end_pos": 58, "type": "TASK", "confidence": 0.6563906073570251}]}, {"text": "By clustering the vocabulary into a fixed number of word classes, it is possible to train models that are less prone to sparsity issues.", "labels": [], "entities": []}, {"text": "This work investigates the performance of standard models used in statistical machine translation when they are trained on automatically learned word classes rather than the actual word identities.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 66, "end_pos": 97, "type": "TASK", "confidence": 0.6473779181639353}]}, {"text": "In the popular tooklit GIZA++, word classes are an essential ingredient to model alignment probabilities with the HMM or IBM translation models.", "labels": [], "entities": []}, {"text": "It contains the mkcls tool, which can automatically cluster the vocabulary into classes.", "labels": [], "entities": []}, {"text": "Using this tool, we propose to re-parameterize the standard models used in statistical machine translation (SMT), which are usually conditioned on word identities rather than word classes.", "labels": [], "entities": [{"text": "statistical machine translation (SMT)", "start_pos": 75, "end_pos": 112, "type": "TASK", "confidence": 0.7858885029951731}]}, {"text": "The idea is that this should lead to a smoother distribution, which is more reliable due to less sparsity.", "labels": [], "entities": []}, {"text": "Here, we focus on the phrase-based and lexical channel models in both directions, simple count models identifying frequency thresholds, lexicalized reordering models and an n-gram language model.", "labels": [], "entities": []}, {"text": "Although our results show that it is not a good idea to replace the original models, we argue that adding them to the log-linear feature combination can improve translation quality.", "labels": [], "entities": [{"text": "translation", "start_pos": 161, "end_pos": 172, "type": "TASK", "confidence": 0.9631462693214417}]}, {"text": "They can easily be computed for different translation paradigms and arbitrary models.", "labels": [], "entities": []}, {"text": "Training and decoding is possible without or with only little change to the code base.", "labels": [], "entities": []}, {"text": "Our experiments are conducted on a mediumsized French\u2192German task and a small German\u2192English task and with both phrasebased and hierarchical phrase-based translation decoders.", "labels": [], "entities": []}, {"text": "By using word class models, we can improve our respective baselines by 1.4% BLEU and 1.0% TER on the French\u2192German task and 0.3% BLEU and 1.1% TER on the German\u2192English task.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 76, "end_pos": 80, "type": "METRIC", "confidence": 0.9987290501594543}, {"text": "TER", "start_pos": 90, "end_pos": 93, "type": "METRIC", "confidence": 0.9896348118782043}, {"text": "BLEU", "start_pos": 129, "end_pos": 133, "type": "METRIC", "confidence": 0.9986042380332947}, {"text": "TER", "start_pos": 143, "end_pos": 146, "type": "METRIC", "confidence": 0.965732216835022}]}, {"text": "Training an additional language model for trans-lation based on word classes has been proposed in.", "labels": [], "entities": []}, {"text": "In addition to the reduced sparsity, an advantage of the smaller vocabulary is that longer n-gram context can be modeled efficiently.", "labels": [], "entities": []}, {"text": "Mathematically, our idea is equivalent to a special case of the Factored Translation Models proposed by.", "labels": [], "entities": []}, {"text": "We will go into more detail in Section 4.", "labels": [], "entities": []}, {"text": "Also related to our proposes to parameterize a hierarchical reordering model with sparse features that are conditioned on word classes trained with mkcls.", "labels": [], "entities": []}, {"text": "However, the features are trained with MIRA rather than estimated by relative frequencies.", "labels": [], "entities": [{"text": "MIRA", "start_pos": 39, "end_pos": 43, "type": "METRIC", "confidence": 0.9790611863136292}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 2: BLEU and TER results on the French\u2192German  task. Results marked with  \u2021 are statistically significant  with 95% confidence, results marked with  \u2020 with 90%  confidence. -X +wcX denote the systems, where the  model X in the baseline is replaced by its word class  counterpart. The 7-gram word class LM is denoted  as wcLM", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9989253878593445}, {"text": "TER", "start_pos": 19, "end_pos": 22, "type": "METRIC", "confidence": 0.9967899918556213}]}, {"text": " Table 2. In a first set of experiments we replaced one  of the standard TM, LM and HRM models by the  same model based on word classes. Unsurprisingly,  this degrades performance with different levels of  severity. The strongest degradation can be seen  when replacing the TM, while replacing the HRM  only leads to a small drop in performance. However,  when the word class models are added as additional  features to the baseline, we observe improvements.  The wcTM yields 0.3% BLEU and 0.5% TER on  test. By adding the 4-gram wcLM, we get another  0.3% BLEU and the wcHRM shows further improve- ments of 0.5% BLEU and 0.2% TER. Extending the  context length of the wcLM to 7-grams gives an ad- ditional boost, reaching a total gain over the baseline  of 1.4% BLEU and 1.0% TER. Using 200 classes  instead of 100 seems to perform slightly better on  test, but with 500 classes, translation quality de- grades again.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 481, "end_pos": 485, "type": "METRIC", "confidence": 0.9962677359580994}, {"text": "TER", "start_pos": 495, "end_pos": 498, "type": "METRIC", "confidence": 0.975809633731842}, {"text": "BLEU", "start_pos": 557, "end_pos": 561, "type": "METRIC", "confidence": 0.9774322509765625}, {"text": "BLEU", "start_pos": 613, "end_pos": 617, "type": "METRIC", "confidence": 0.9822787046432495}, {"text": "BLEU", "start_pos": 763, "end_pos": 767, "type": "METRIC", "confidence": 0.9685271978378296}]}, {"text": " Table 3: BLEU and TER results on the German\u2192English  task. Results marked with  \u2021 are statistically significant  with 95% confidence, results marked with  \u2020 with 90%  confidence.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9987179040908813}, {"text": "TER", "start_pos": 19, "end_pos": 22, "type": "METRIC", "confidence": 0.9971181154251099}]}]}