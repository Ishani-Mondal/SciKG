{"title": [{"text": "Feature Noising for Log-linear Structured Prediction", "labels": [], "entities": [{"text": "Log-linear Structured Prediction", "start_pos": 20, "end_pos": 52, "type": "TASK", "confidence": 0.6893428166707357}]}], "abstractContent": [{"text": "NLP models have many and sparse features, and regularization is key for balancing model overfitting versus underfitting.", "labels": [], "entities": []}, {"text": "A recently re-popularized form of regularization is to generate fake training data by repeatedly adding noise to real data.", "labels": [], "entities": [{"text": "regularization", "start_pos": 34, "end_pos": 48, "type": "TASK", "confidence": 0.9677863717079163}]}, {"text": "We reinterpret this noising as an explicit regularizer, and approximate it with a second-order formula that can be used during training without actually generating fake data.", "labels": [], "entities": []}, {"text": "We show how to apply this method to structured prediction using multinomial logistic regression and linear-chain CRFs.", "labels": [], "entities": [{"text": "structured prediction", "start_pos": 36, "end_pos": 57, "type": "TASK", "confidence": 0.7984130680561066}]}, {"text": "We tackle the key challenge of developing a dynamic program to compute the gradient of the regularizer efficiently.", "labels": [], "entities": []}, {"text": "The regularizer is a sum over inputs, so we can estimate it more accurately via a semi-supervised or transduc-tive extension.", "labels": [], "entities": []}, {"text": "Applied to text classification and NER, our method provides a >1% absolute performance gain overuse of standard L 2 regularization.", "labels": [], "entities": [{"text": "text classification", "start_pos": 11, "end_pos": 30, "type": "TASK", "confidence": 0.833322674036026}, {"text": "NER", "start_pos": 35, "end_pos": 38, "type": "TASK", "confidence": 0.9492860436439514}]}], "introductionContent": [{"text": "NLP models often have millions of mainly sparsely attested features.", "labels": [], "entities": []}, {"text": "As a result, balancing overfitting versus underfitting through good weight regularization remains a key issue for achieving optimal performance.", "labels": [], "entities": []}, {"text": "Traditionally, L 2 or L 1 regularization is employed, but these simple types of regularization penalize all features in a uniform way without taking into account the properties of the actual model.", "labels": [], "entities": []}, {"text": "An alternative approach to regularization is to generate fake training data by adding random noise to the input features of the original training data.", "labels": [], "entities": [{"text": "regularization", "start_pos": 27, "end_pos": 41, "type": "TASK", "confidence": 0.979583203792572}]}, {"text": "Intuitively, this can bethought of as simulating miss- * Both authors contributed equally to the paper ing features, whether due to typos or use of a previously unseen synonym.", "labels": [], "entities": []}, {"text": "The effectiveness of this technique is well-known in machine learning (;), but working directly with many corrupted copies of a dataset can be computationally prohibitive.", "labels": [], "entities": []}, {"text": "Fortunately, feature noising ideas often lead to tractable deterministic objectives that can be optimized directly.", "labels": [], "entities": []}, {"text": "Sometimes, training with corrupted features reduces to a special form of regularization.", "labels": [], "entities": []}, {"text": "For example, showed that training with features that have been corrupted with additive Gaussian noise is equivalent to a form of L 2 regularization in the low noise limit.", "labels": [], "entities": []}, {"text": "In other cases it is possible to develop anew objective function by marginalizing over the artificial noise ().", "labels": [], "entities": []}, {"text": "The central contribution of this paper is to show how to efficiently simulate training with artificially noised features in the context of log-linear structured prediction, without actually having to generate noised data.", "labels": [], "entities": []}, {"text": "We focus on dropout noise (), a recently popularized form of artificial feature noise where a random subset of features is omitted independently for each training example.", "labels": [], "entities": []}, {"text": "Dropout and its variants have been shown to outperform L 2 regularization on various tasks;.", "labels": [], "entities": []}, {"text": "Dropout is is similar in spirit to feature bagging in the deliberate removal of features, but performs the removal in a preset way rather than randomly ().", "labels": [], "entities": []}, {"text": "Our approach is based on a second-order approximation to feature noising developed among others by and, which allows us to convert dropout noise into a form of adaptive regularization.", "labels": [], "entities": []}, {"text": "This method is suitable for structured prediction in log-linear models where second derivatives are computable.", "labels": [], "entities": [{"text": "structured prediction", "start_pos": 28, "end_pos": 49, "type": "TASK", "confidence": 0.8456045389175415}]}, {"text": "In particular, it can be used for multiclass classification with maximum entropy models (a.k.a., softmax or multinomial logistic regression) and for the sequence models that are ubiquitous in NLP, via linear chain Conditional Random Fields (CRFs).", "labels": [], "entities": [{"text": "multiclass classification", "start_pos": 34, "end_pos": 59, "type": "TASK", "confidence": 0.7732861042022705}]}, {"text": "For linear chain CRFs, we additionally show how we can use a noising scheme that takes advantage of the clique structure so that the resulting noising regularizer can be computed in terms of the pairwise marginals.", "labels": [], "entities": []}, {"text": "A simple forward-backward-type dynamic program can then be used to compute the gradient tractably.", "labels": [], "entities": []}, {"text": "For ease of implementation and scalability to semi-supervised learning, we also outline an even faster approximation to the regularizer.", "labels": [], "entities": []}, {"text": "The general approach also works in other clique structures in addition to the linear chain when the clique marginals can be computed efficiently.", "labels": [], "entities": []}, {"text": "Finally, we extend feature noising for structured prediction to a transductive or semi-supervised setting.", "labels": [], "entities": [{"text": "structured prediction", "start_pos": 39, "end_pos": 60, "type": "TASK", "confidence": 0.6610884815454483}]}, {"text": "The regularizer induced by feature noising is label-independent for log-linear models, and so we can use unlabeled data to learn a better regularizer.", "labels": [], "entities": []}, {"text": "NLP sequence labeling tasks are especially well suited to a semi-supervised approach, as input features are numerous but sparse, and labeled data is expensive to obtain but unlabeled data is abundant ().", "labels": [], "entities": [{"text": "NLP sequence labeling", "start_pos": 0, "end_pos": 21, "type": "TASK", "confidence": 0.6794984936714172}]}, {"text": "showed that semi-supervised dropout training for logistic regression captures a similar intuition to techniques such as entropy regularization () and transductive SVMs, which encourage confident predictions on the unlabeled data.", "labels": [], "entities": []}, {"text": "Semisupervised dropout has the advantage of only using the predicted label probabilities on the unlabeled data to modulate an L 2 regularizer, rather than requiring more heavy-handed modeling of the unlabeled data as in entropy regularization or expectation regularization.", "labels": [], "entities": []}, {"text": "In experimental results, we show that simulated feature noising gives more than a 1% absolute boost f (y t , y t+1 ): An illustration of dropout feature noising in linear-chain CRFs with only transition features and node features.", "labels": [], "entities": []}, {"text": "The green squares are node features f (y t , x t ), and the orange squares are edge features f (y t\u22121 , y t ).", "labels": [], "entities": []}, {"text": "Conceptually, given a training example, we sample some features to ignore (generate fake data) and make a parameter update.", "labels": [], "entities": []}, {"text": "Our goal is to train with a roughly equivalent objective, without actually sampling.", "labels": [], "entities": []}, {"text": "in performance over L 2 regularization, on both text classification and an NER sequence labeling task.", "labels": [], "entities": [{"text": "text classification", "start_pos": 48, "end_pos": 67, "type": "TASK", "confidence": 0.8083668351173401}, {"text": "NER sequence labeling task", "start_pos": 75, "end_pos": 101, "type": "TASK", "confidence": 0.8237348943948746}]}], "datasetContent": [{"text": "We show experimental results on the CoNLL-2003 Named Entity Recognition (NER) task, the SANCL Part-of-speech (POS) tagging task, and several document classification tasks.", "labels": [], "entities": [{"text": "CoNLL-2003 Named Entity Recognition (NER) task", "start_pos": 36, "end_pos": 82, "type": "TASK", "confidence": 0.7720311246812344}, {"text": "SANCL Part-of-speech (POS) tagging task", "start_pos": 88, "end_pos": 127, "type": "TASK", "confidence": 0.6818429401942662}, {"text": "document classification", "start_pos": 141, "end_pos": 164, "type": "TASK", "confidence": 0.8023070693016052}]}, {"text": "The datasets used are described in.", "labels": [], "entities": []}, {"text": "We used standard splits whenever available; otherwise we split the data at random into a test set and a train set of equal sizes (RCV1 4 , TDT2).", "labels": [], "entities": []}, {"text": "CoNLL has a development set of size 51578, which we used to tune regularization parameters.", "labels": [], "entities": [{"text": "CoNLL", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.949491024017334}]}, {"text": "The SANCL test set is divided into 3 genres, namely answers, newsgroups, and reviews, each of which has a corresponding development set.", "labels": [], "entities": [{"text": "SANCL test set", "start_pos": 4, "end_pos": 18, "type": "DATASET", "confidence": 0.7940936187903086}]}, {"text": "3  We evaluate the quadratic dropout regularizer in linear-chain CRFs on two sequence tagging tasks: the CoNLL 2003 NER shared task and the SANCL 2012 POS tagging task . The standard CoNLL-2003 English shared task benchmark dataset) is a collection of documents from Reuters newswire articles, annotated with four entity types: Person, Location, Organization, and Miscellaneous.", "labels": [], "entities": [{"text": "CoNLL 2003 NER shared task", "start_pos": 105, "end_pos": 131, "type": "DATASET", "confidence": 0.9268515348434448}, {"text": "SANCL 2012 POS tagging task", "start_pos": 140, "end_pos": 167, "type": "TASK", "confidence": 0.7052817165851593}, {"text": "CoNLL-2003 English shared task benchmark dataset", "start_pos": 183, "end_pos": 231, "type": "DATASET", "confidence": 0.7606451908747355}]}, {"text": "We predicted the label sequence Y = {LOC, MISC, ORG, PER, O} T without considering the BIO tags.", "labels": [], "entities": [{"text": "LOC", "start_pos": 37, "end_pos": 40, "type": "METRIC", "confidence": 0.9062303304672241}, {"text": "MISC", "start_pos": 42, "end_pos": 46, "type": "METRIC", "confidence": 0.8767499923706055}, {"text": "ORG", "start_pos": 48, "end_pos": 51, "type": "METRIC", "confidence": 0.9387909770011902}, {"text": "PER, O} T", "start_pos": 53, "end_pos": 62, "type": "METRIC", "confidence": 0.7778929829597473}]}, {"text": "For training the CRF model, we used a comprehensive set of features from that gives state-of-the-art results on this task.", "labels": [], "entities": []}, {"text": "A total number of 437906 features were generated on the CoNLL-2003 training dataset.", "labels": [], "entities": [{"text": "CoNLL-2003 training dataset", "start_pos": 56, "end_pos": 83, "type": "DATASET", "confidence": 0.9672813812891642}]}, {"text": "The most important features are: \u2022 The word, word shape, and letter n-grams (up to 6gram) at current position \u2022 The prediction, word, and word shape of the previous and next position \u2022 Previous word shape in conjunction with current word shape \u2022 Disjunctive word set of the previous and next 4 positions \u2022 Capitalization pattern in a 3 word window \u2022 Previous two words in conjunction with the word shape of the previous word \u2022 The current word matched against a list of name titles (e.g., Mr., Mrs.) The F \u03b2=1 results are summarized in.", "labels": [], "entities": [{"text": "F \u03b2=1", "start_pos": 504, "end_pos": 509, "type": "METRIC", "confidence": 0.9776427447795868}]}, {"text": "We obtain a 1.6% and 1.1% absolute gain on the test and dev set, respectively.", "labels": [], "entities": []}, {"text": "Detailed results are broken down by precision and recall for each tag and are shown in.", "labels": [], "entities": [{"text": "precision", "start_pos": 36, "end_pos": 45, "type": "METRIC", "confidence": 0.9995473027229309}, {"text": "recall", "start_pos": 50, "end_pos": 56, "type": "METRIC", "confidence": 0.9992790818214417}]}, {"text": "These improvements are significant at the 0.1% level according to the paired bootstrap resampling method of 2000 iterations   We obtained a small but consistent improvement using the quadratic dropout regularizer in (14) over the L 2 -regularized CRFs baseline.", "labels": [], "entities": []}, {"text": "Although the difference on SANCL is small, the performance differences on the test sets of reviews and newsgroups are statistically significant at the 0.1% level.", "labels": [], "entities": [{"text": "SANCL", "start_pos": 27, "end_pos": 32, "type": "TASK", "confidence": 0.5066022276878357}]}, {"text": "This is also interesting because here is a situation where the features are extremely sparse, L 2 regularization gave no improvement, and where regularization overall matters less.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Description of datasets. q: average number  of non-zero features per example, d: total number  of features, K: number of classes to predict, N train :  number of training examples, N test : number of test  examples.", "labels": [], "entities": []}, {"text": " Table 1. We used standard splits when- ever available; otherwise we split the data at ran- dom into a test set and a train set of equal sizes  (RCV1 4 , TDT2). CoNLL has a development set  of size 51578, which we used to tune regulariza- tion parameters. The SANCL test set is divided into  3 genres, namely answers, newsgroups, and  reviews, each of which has a corresponding de- velopment set. 3", "labels": [], "entities": [{"text": "SANCL test set", "start_pos": 260, "end_pos": 274, "type": "DATASET", "confidence": 0.7856340209643046}]}, {"text": " Table 2: Classification performance and transduc- tive learning results on some standard datasets.  None: use no regularization, Drop: quadratic ap- proximation to the dropout noise (8), +Test: also use  the test set to estimate the noising regularizer (11).", "labels": [], "entities": [{"text": "quadratic ap- proximation", "start_pos": 136, "end_pos": 161, "type": "METRIC", "confidence": 0.773662880063057}]}, {"text": " Table 3: Semisupervised learning results on some  standard datasets. A third (33%) of the full dataset  was used for training, a third for testing, and the rest  as unlabeled.", "labels": [], "entities": []}, {"text": " Table 4. We  obtain a 1.6% and 1.1% absolute gain on the test  and dev set, respectively. Detailed results are bro- ken down by precision and recall for each tag and are  shown in", "labels": [], "entities": [{"text": "precision", "start_pos": 129, "end_pos": 138, "type": "METRIC", "confidence": 0.9993060827255249}, {"text": "recall", "start_pos": 143, "end_pos": 149, "type": "METRIC", "confidence": 0.9992701411247253}]}, {"text": " Table 6. These improvements are signifi- cant at the 0.1% level according to the paired boot- strap resampling method of 2000 iterations", "labels": [], "entities": []}, {"text": " Table 4: CoNLL summary of results. None: no reg- ularization, Drop: quadratic dropout regularization  (14) described in this paper.", "labels": [], "entities": []}, {"text": " Table 5: SANCL POS tagging F \u03b2=1 scores for the 3  official evaluation sets.", "labels": [], "entities": [{"text": "SANCL POS tagging F \u03b2", "start_pos": 10, "end_pos": 31, "type": "METRIC", "confidence": 0.6755753278732299}]}, {"text": " Table 6: CoNLL NER results broken down by tags and by precision, recall, and F \u03b2=1 . Top: development  set, bottom: test set performance.", "labels": [], "entities": [{"text": "CoNLL NER", "start_pos": 10, "end_pos": 19, "type": "TASK", "confidence": 0.38302479684352875}, {"text": "precision", "start_pos": 55, "end_pos": 64, "type": "METRIC", "confidence": 0.9995816349983215}, {"text": "recall", "start_pos": 66, "end_pos": 72, "type": "METRIC", "confidence": 0.9989513158798218}, {"text": "F \u03b2", "start_pos": 78, "end_pos": 81, "type": "METRIC", "confidence": 0.9884793162345886}]}]}