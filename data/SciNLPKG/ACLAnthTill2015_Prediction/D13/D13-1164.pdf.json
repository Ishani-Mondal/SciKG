{"title": [{"text": "A Convex Alternative to IBM Model 2", "labels": [], "entities": []}], "abstractContent": [{"text": "The IBM translation models have been hugely influential in statistical machine translation; they are the basis of the alignment models used in modern translation systems.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 59, "end_pos": 90, "type": "TASK", "confidence": 0.700284073750178}]}, {"text": "Excluding IBM Model 1, the IBM translation models , and practically all variants proposed in the literature, have relied on the optimization of likelihood functions or similar functions that are non-convex, and hence have multiple local optima.", "labels": [], "entities": [{"text": "IBM translation", "start_pos": 27, "end_pos": 42, "type": "TASK", "confidence": 0.4638177454471588}]}, {"text": "In this paper we introduce a convex relaxation of IBM Model 2, and describe an optimization algorithm for the relaxation based on a subgradient method combined with exponentiated-gradient updates.", "labels": [], "entities": [{"text": "IBM Model 2", "start_pos": 50, "end_pos": 61, "type": "DATASET", "confidence": 0.9608670870463053}]}, {"text": "Our approach gives the same level of alignment accuracy as IBM Model 2.", "labels": [], "entities": [{"text": "alignment", "start_pos": 37, "end_pos": 46, "type": "TASK", "confidence": 0.8730794787406921}, {"text": "accuracy", "start_pos": 47, "end_pos": 55, "type": "METRIC", "confidence": 0.9834943413734436}]}], "introductionContent": [{"text": "The IBM translation models ( have been tremendously important in statistical machine translation (SMT).", "labels": [], "entities": [{"text": "IBM translation", "start_pos": 4, "end_pos": 19, "type": "TASK", "confidence": 0.4914597272872925}, {"text": "statistical machine translation (SMT)", "start_pos": 65, "end_pos": 102, "type": "TASK", "confidence": 0.8179675936698914}]}, {"text": "The IBM models were the first generation of SMT systems; in recent work, they play a central role in deriving alignments used within many modern SMT approaches, for example phrase-based translation models and syntax-based translation systems (e.g.,).", "labels": [], "entities": [{"text": "SMT", "start_pos": 44, "end_pos": 47, "type": "TASK", "confidence": 0.9912461638450623}, {"text": "SMT", "start_pos": 145, "end_pos": 148, "type": "TASK", "confidence": 0.9804865717887878}, {"text": "phrase-based translation", "start_pos": 173, "end_pos": 197, "type": "TASK", "confidence": 0.6277740001678467}]}, {"text": "Since the original IBM paper, there has been a large amount of research exploring the original IBM models and modern variants (e.g.,;).", "labels": [], "entities": []}, {"text": "Excluding IBM Model 1, the IBM translation models, and practically all variants proposed in the literature, have relied on the optimization of likelihood functions or similar functions that are nonconvex.", "labels": [], "entities": [{"text": "IBM translation", "start_pos": 27, "end_pos": 42, "type": "TASK", "confidence": 0.4683101177215576}]}, {"text": "Unfortunately, non-convex objective functions have multiple local optima, and finding a global optimum of a non-convex function is typically a computationally intractible problem.", "labels": [], "entities": []}, {"text": "Typically, an EM algorithm is used, which often runs in a reasonable amount of time, but with no guarantees of finding a global optima (or for that matter, even a near-optimal solution).", "labels": [], "entities": []}, {"text": "In this paper we make the following contributions: \u2022 We introduce a convex relaxation of IBM Model 2.", "labels": [], "entities": [{"text": "IBM Model 2", "start_pos": 89, "end_pos": 100, "type": "DATASET", "confidence": 0.9551342924435934}]}, {"text": "At a very high level, the relaxation is derived by replacing the product t(f j |e i ) \u00d7 d(i|j) with a relaxation that is commonly used in the linear programming literature (e.g., see).", "labels": [], "entities": []}, {"text": "(Here t(f |e) are the translation parameters of the model, and d(i|j) are the distortion parameters; the product is non-linear, effectively introducing nonconvexity into the problem.)", "labels": [], "entities": []}, {"text": "\u2022 We describe an optimization algorithm for the relaxed objective, based on a combination of stochastic subgradient methods with the exponentiated-gradient (EG) algorithm).", "labels": [], "entities": []}, {"text": "\u2022 We describe experiments with the method on standard alignment datasets, showing that the EG algorithm converges in only a few passes over the data, and that our method achieves accuracies that are very similar to those of IBM Model 2.", "labels": [], "entities": []}, {"text": "Framing the unsupervised learning of alignment models as a convex optimization problem, with guaranteed convergence to a global optimum, has several clear advantages.", "labels": [], "entities": []}, {"text": "First, the method is easier to analyze, as the objective function is being truly maximized.", "labels": [], "entities": []}, {"text": "Second, there is no need for initialization heuristics with the approach, given that the method will always converge to a global optimum.", "labels": [], "entities": []}, {"text": "Finally, we expect that our convexitybased approach may facilitate the further development of more convex models.", "labels": [], "entities": []}, {"text": "There has been a rich interplay between convex and non-convex methods in machine learning: as one example consider the literature on classification problems, with early work on the perceptron (linear/convex), then work on neural networks with back-propagation (nonlinear/non-convex), then the introduction of support vector machines (non-linear/convex), and finally recent work on deep belief networks (non-linear/nonconvex).", "labels": [], "entities": []}, {"text": "In view of these developments, the lack of convex methods in translation alignment models has been noticeable, and we hope that our work will open up new directions and lead to further progress in this area.", "labels": [], "entities": [{"text": "translation alignment", "start_pos": 61, "end_pos": 82, "type": "TASK", "confidence": 0.9688248634338379}]}, {"text": "Throughout this paper, for any integer N , we use [N ] to denote {1 . .", "labels": [], "entities": []}, {"text": "N } and [N ] 0 to denote {0 . .", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section we describe experiments using the I2CR-2 optimization problem combined with the fork = 1 . .", "labels": [], "entities": [{"text": "I2CR-2 optimization", "start_pos": 50, "end_pos": 69, "type": "TASK", "confidence": 0.6208280324935913}, {"text": "fork", "start_pos": 96, "end_pos": 100, "type": "METRIC", "confidence": 0.9582750797271729}]}, {"text": "n, D(e) fore \u2208 E as in Section 3.", "labels": [], "entities": []}, {"text": "An integer B specifying the batch size.", "labels": [], "entities": []}, {"text": "An integer S specifying the number of passes over the data.", "labels": [], "entities": []}, {"text": "A step size \u03b3 > 0.", "labels": [], "entities": []}, {"text": "A parameter \u03bb > 0 used in the definition of log . 2: Parameters: \u2022 A parameter t(f |e) for each e \u2208 E, f \u2208 D(e).", "labels": [], "entities": []}, {"text": "\u2022 Randomly partition [n] into subsets T 1 . .", "labels": [], "entities": []}, {"text": "T K of size B where K = n/B.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Results on the Hansards data for IBM Model 2  and the I2CR-2 method.", "labels": [], "entities": [{"text": "Hansards data", "start_pos": 25, "end_pos": 38, "type": "DATASET", "confidence": 0.9315820038318634}, {"text": "IBM Model 2", "start_pos": 43, "end_pos": 54, "type": "DATASET", "confidence": 0.8657172918319702}]}, {"text": " Table 2: Results on the English-Romanian data for IBM  Model 2 and the I2CR-2 method.", "labels": [], "entities": [{"text": "IBM  Model 2", "start_pos": 51, "end_pos": 63, "type": "DATASET", "confidence": 0.9053350488344828}]}, {"text": " Table 3: Objective values for the EG algorithm opti- mization of I2CR-2 at each iteration. \"EF Objective\"  corresponds to training a model with t(e|f ) parameters,  \"FE Objective\" corresponds to the reverse direction, with  t(f |e) parameters. Iteration 0 corresponds to the objec- tive value under the initial, uniform parameter values.", "labels": [], "entities": [{"text": "FE", "start_pos": 167, "end_pos": 169, "type": "METRIC", "confidence": 0.9727757573127747}]}]}