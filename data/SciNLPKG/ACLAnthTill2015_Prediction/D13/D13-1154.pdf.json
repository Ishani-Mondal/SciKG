{"title": [{"text": "Using crowdsourcing to get representations based on regular expressions", "labels": [], "entities": []}], "abstractContent": [{"text": "Often the bottleneck in document classification is finding good representations that zoom in on the most important aspects of the documents.", "labels": [], "entities": [{"text": "document classification", "start_pos": 24, "end_pos": 47, "type": "TASK", "confidence": 0.7095780521631241}]}, {"text": "Most research uses n-gram representations , but relevant features often occur discontinuously, e.g., not..", "labels": [], "entities": []}, {"text": "In this paper we present experiments getting experts to provide regular expressions , as well as crowdsourced annotation tasks from which regular expressions can be derived.", "labels": [], "entities": []}, {"text": "Somewhat surprisingly, it turns out that these crowdsourced feature combinations outperform automatic feature combination methods, as well as expert features, by a very large margin and reduce error by 24-41% over n-gram representations.", "labels": [], "entities": [{"text": "error", "start_pos": 193, "end_pos": 198, "type": "METRIC", "confidence": 0.989407479763031}]}], "introductionContent": [{"text": "Finding good representations of classification problems is often glossed over in the literature.", "labels": [], "entities": []}, {"text": "Several authors have emphasized the need to pay more attention to finding such representations, but in document classification most research still uses n-gram representations.", "labels": [], "entities": [{"text": "document classification", "start_pos": 103, "end_pos": 126, "type": "TASK", "confidence": 0.7101474553346634}]}, {"text": "This paper considers two document classification problems where such representations seem inadequate.", "labels": [], "entities": [{"text": "document classification", "start_pos": 25, "end_pos": 48, "type": "TASK", "confidence": 0.7034248858690262}]}, {"text": "The problems are answer scoring (), on data from stackoverflow.com, and multi-attribute sentiment analysis.", "labels": [], "entities": [{"text": "answer scoring", "start_pos": 17, "end_pos": 31, "type": "TASK", "confidence": 0.9030236005783081}, {"text": "multi-attribute sentiment analysis", "start_pos": 72, "end_pos": 106, "type": "TASK", "confidence": 0.7758373220761617}]}, {"text": "We argue that in order to adequately represent such problems we need discontinuous features, i.e., regular expressions.", "labels": [], "entities": []}, {"text": "The problem with using regular expressions as features is of course that even with a finite vocabulary we can generate infinitely many regular expressions that match our documents.", "labels": [], "entities": []}, {"text": "We suggest to use expert knowledge or crowdsourcing in the loop.", "labels": [], "entities": []}, {"text": "In particular we present experiments where standard representations are augmented with features from a few hours of manual work, by machine learning experts or by turkers.", "labels": [], "entities": []}, {"text": "Somewhat surprisingly, we find that features derived from crowdsourced annotation tasks lead to the best results across the three datasets.", "labels": [], "entities": []}, {"text": "While crowdsourcing of annotation tasks has become increasing popular in NLP, this is, to the best of our knowledge, the first attempt to crowdsource the problem of finding good representations.", "labels": [], "entities": []}], "datasetContent": [{"text": "Data The three datasets used in our experiments come from two sources, namely stackoverflow.com and ratebeer.com.", "labels": [], "entities": []}, {"text": "The two beer review datasets (TASTE and APPEARANCE) are described in and available for download.", "labels": [], "entities": [{"text": "beer review datasets", "start_pos": 8, "end_pos": 28, "type": "DATASET", "confidence": 0.9084088206291199}, {"text": "TASTE", "start_pos": 30, "end_pos": 35, "type": "METRIC", "confidence": 0.9730224013328552}, {"text": "APPEARANCE", "start_pos": 40, "end_pos": 50, "type": "METRIC", "confidence": 0.8779072165489197}]}, {"text": "Each input example is an unstructured review text, and the associated label is the score assigned to taste or appearance by the reviewer.", "labels": [], "entities": []}, {"text": "We randomly sample about 152k data points, as well as 500 examples for experiments with experts and turks.", "labels": [], "entities": []}, {"text": "We extracted the STACKOVERFLOW dataset from a publicly available data dump, 2 , and we briefly describe our sampling process here.", "labels": [], "entities": [{"text": "STACKOVERFLOW dataset from a publicly available data dump", "start_pos": 17, "end_pos": 74, "type": "DATASET", "confidence": 0.756741788238287}]}, {"text": "We select pairs of answers, where one is ranked higher than the other by stackoverflow.com users.", "labels": [], "entities": []}, {"text": "Obviously the answers submitted first have a better chance of being ranked highly, so we also require that the highest ranked answer was submitted last.", "labels": [], "entities": []}, {"text": "From this set of answer pairs, we randomly sample 97,519 pairs, as well as 500 examples for our experiments with experts and turks.", "labels": [], "entities": []}, {"text": "Our experiments are classification experiments using the same learning algorithm in all experiments, namely L 1 -regularized logistic regression.", "labels": [], "entities": []}, {"text": "We don't set any parameters The only differences between our systems are in the feature sets.", "labels": [], "entities": []}, {"text": "Results are from 5-fold cross-validation.", "labels": [], "entities": []}, {"text": "The four feature sets are described below: BoW, HI, Exp and AMT.", "labels": [], "entities": [{"text": "BoW", "start_pos": 43, "end_pos": 46, "type": "DATASET", "confidence": 0.5459473133087158}]}, {"text": "For motivating using regular expressions, consider the following sentence from a review of John Harvard's Grand Cru: (1) Could have been more flavorful.", "labels": [], "entities": [{"text": "Harvard's Grand Cru", "start_pos": 96, "end_pos": 115, "type": "DATASET", "confidence": 0.8964900821447372}]}, {"text": "The only word carrying direct sentiment in this sentence is flavorful, which is positive, but the sentence is a negative evaluation of the Grand Cru's 1 http://snap.stanford.edu/data/web-RateBeer.html 2 http://www.clearbits.net/torrents taste.", "labels": [], "entities": [{"text": "Grand Cru's", "start_pos": 139, "end_pos": 150, "type": "DATASET", "confidence": 0.9680450161298116}]}, {"text": "The trigram been more flavorful seems negative at first, but in the context of negation or in a comparative, it can become positive again.", "labels": [], "entities": []}, {"text": "However, note that this trigram may occur discontinuously, e.g., in been less watery and more flavorful.", "labels": [], "entities": []}, {"text": "In order to match such occurrences, we need simple regular expressions, e.g.,: been.", "labels": [], "entities": []}, {"text": "* flavorful This is exactly the kind of regular expressions we asked experts to submit, and that we derived from the crowdsourced annotation tasks.", "labels": [], "entities": []}, {"text": "Note that the sentence says nothing about the beer's appearance, so this feature is only relevant in TASTE, not in APPEARANCE.", "labels": [], "entities": [{"text": "TASTE", "start_pos": 101, "end_pos": 106, "type": "METRIC", "confidence": 0.6472094058990479}, {"text": "APPEARANCE", "start_pos": 115, "end_pos": 125, "type": "DATASET", "confidence": 0.8415960669517517}]}, {"text": "BoW and BoW+HI Our most simple baseline approach is a bag-of-words model of unigram features (BoW).", "labels": [], "entities": [{"text": "BoW", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.8973154425621033}]}, {"text": "We lower-case our data, but leave in stop words.", "labels": [], "entities": []}, {"text": "We also introduce a semantically enriched unigram model (BoW)+HI, wherein addition to representing what words occur in a text, we also represent what Harvard Inquirer (HI) 3 word classes occur in it.", "labels": [], "entities": []}, {"text": "The HI classes are used to generate features from the crowdsourced annotation tasks, so the semantically enriched unigram model is an important baseline in our experiments below.", "labels": [], "entities": []}, {"text": "BoW+Exp In order to collect regular expressions from experts, we setup a web interface for querying held-out portions of the datasets with regular expressions that reports how occurrences of the submitted regular expressions correlate with class.", "labels": [], "entities": [{"text": "BoW+Exp", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.8769685626029968}]}, {"text": "We used the Python re syntax for regular expressions after augmenting word forms with POS and semantic classes from the HI.", "labels": [], "entities": []}, {"text": "Few of the experts made use of the POS tags, but many regular expressions included references to HI classes.", "labels": [], "entities": []}, {"text": "Regular expressions submitted by participants were visible to other participants during the experiment, and participants were allowed to work together.", "labels": [], "entities": []}, {"text": "Participants had 15 minutes to familiarize themselves with the syntax used in the experiments.", "labels": [], "entities": []}, {"text": "Each query was executed in 2-30 seconds.", "labels": [], "entities": []}, {"text": "Seven researchers and graduate students spent five effective hours querying the datasets with regular expressions.", "labels": [], "entities": []}, {"text": "In particular, they spent three hours on the Stack Exchange dataset, and one hour on each of the two RateBeer datasets.", "labels": [], "entities": [{"text": "Stack Exchange dataset", "start_pos": 45, "end_pos": 67, "type": "DATASET", "confidence": 0.9333368539810181}, {"text": "RateBeer datasets", "start_pos": 101, "end_pos": 118, "type": "DATASET", "confidence": 0.9704014956951141}]}, {"text": "One had to leave an hour early.", "labels": [], "entities": []}, {"text": "So, in total, we spent 20 person hours on Stack Exchange, and seven person hours on each of the RateBeer datasets.", "labels": [], "entities": [{"text": "Stack Exchange", "start_pos": 42, "end_pos": 56, "type": "DATASET", "confidence": 0.9284571409225464}, {"text": "RateBeer datasets", "start_pos": 96, "end_pos": 113, "type": "DATASET", "confidence": 0.9773356020450592}]}, {"text": "In the five hours, we collected 1,156 regular expressions for the STACKOVERFLOW dataset, and about 650 regular expressions for each of the two RateBeer datasets.", "labels": [], "entities": [{"text": "STACKOVERFLOW dataset", "start_pos": 66, "end_pos": 87, "type": "DATASET", "confidence": 0.8392654061317444}, {"text": "RateBeer datasets", "start_pos": 143, "end_pos": 160, "type": "DATASET", "confidence": 0.9666688144207001}]}, {"text": "Exp refers to these sets of regular expressions.", "labels": [], "entities": []}, {"text": "In our experiments below we concatenate these with the BoW features to form BoW+Exp.", "labels": [], "entities": [{"text": "BoW", "start_pos": 55, "end_pos": 58, "type": "DATASET", "confidence": 0.8316185474395752}, {"text": "BoW+Exp", "start_pos": 76, "end_pos": 83, "type": "DATASET", "confidence": 0.8738488554954529}]}, {"text": "BoW+AMT For each dataset, we also had 500 heldout examples annotated by three turkers each, using Amazon Mechanical Turk, 4 obtaining 1,500 HITs for each dataset.", "labels": [], "entities": [{"text": "BoW", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.8874792456626892}, {"text": "AMT", "start_pos": 4, "end_pos": 7, "type": "METRIC", "confidence": 0.8000567555427551}, {"text": "Amazon Mechanical Turk", "start_pos": 98, "end_pos": 120, "type": "DATASET", "confidence": 0.9282921950022379}, {"text": "HITs", "start_pos": 140, "end_pos": 144, "type": "METRIC", "confidence": 0.9831936359405518}]}, {"text": "The annotators were presented with each text, a review or an answer, twice: once as running text, once word-by-word with bullets to tick off words.", "labels": [], "entities": []}, {"text": "The annotators were instructed to tick off words or phrases that they found predictive of the text's sentiment or answer quality.", "labels": [], "entities": []}, {"text": "They were not informed about the class of the text.", "labels": [], "entities": []}, {"text": "We chose this annotation task, because it is relatively easy for annotators to mark spans of text with a particular attribute.", "labels": [], "entities": []}, {"text": "This set-up has been used in other applications, including NER () and error detection ().", "labels": [], "entities": [{"text": "NER", "start_pos": 59, "end_pos": 62, "type": "TASK", "confidence": 0.8987382650375366}, {"text": "error detection", "start_pos": 70, "end_pos": 85, "type": "TASK", "confidence": 0.6597170233726501}]}, {"text": "The annotators were constrained to tick off at least three words, including one closed class item (closed class items were colored differently).", "labels": [], "entities": []}, {"text": "Finally, we only used annotators with a track record of providing high-quality annotations in previous tasks.", "labels": [], "entities": []}, {"text": "It was clear from the average time spent by annotators that annotating STACK-OVERFLOW was harder than annotating the Ratebeer datasets.", "labels": [], "entities": [{"text": "STACK-OVERFLOW", "start_pos": 71, "end_pos": 85, "type": "DATASET", "confidence": 0.46364542841911316}, {"text": "Ratebeer datasets", "start_pos": 117, "end_pos": 134, "type": "DATASET", "confidence": 0.9923302829265594}]}, {"text": "The average time spent on a Ratebeer HIT was 44s, while for STACKOVERFLOW it was 3m:8s.", "labels": [], "entities": [{"text": "Ratebeer HIT", "start_pos": 28, "end_pos": 40, "type": "DATASET", "confidence": 0.9036951959133148}, {"text": "STACKOVERFLOW", "start_pos": 60, "end_pos": 73, "type": "DATASET", "confidence": 0.4366942346096039}]}, {"text": "The mean number of words ticked off  was between 5.6 and 7, with more words ticked off in STACKOVERFLOW.", "labels": [], "entities": [{"text": "mean", "start_pos": 4, "end_pos": 8, "type": "METRIC", "confidence": 0.9641972780227661}, {"text": "STACKOVERFLOW", "start_pos": 90, "end_pos": 103, "type": "METRIC", "confidence": 0.8573276996612549}]}, {"text": "The maximum number of words ticked off by an annotator was 41.", "labels": [], "entities": []}, {"text": "We spent $292.5 on the annotations, including atrial round.", "labels": [], "entities": []}, {"text": "This was supposed to match, roughly, the cost of the experts consulted for BoW+Exp.", "labels": [], "entities": [{"text": "BoW+Exp.", "start_pos": 75, "end_pos": 83, "type": "DATASET", "confidence": 0.9537720382213593}]}, {"text": "The features generated from the annotations were constructed as follows: We use a sliding window of size 3 to extract trigrams over the possibly discontinuous words ticked off by the annotators.", "labels": [], "entities": []}, {"text": "These trigrams were converted into regular expressions by placing Kleene stars between the words.", "labels": [], "entities": []}, {"text": "This gives us a manually selected subset of skip trigrams.", "labels": [], "entities": []}, {"text": "For each skip trigram, we add copies with one or more words replaced by one of their HI classes.", "labels": [], "entities": []}, {"text": "Feature combinations This subsection introduces some harder baselines for our experiments, considered in Experiment #2.", "labels": [], "entities": []}, {"text": "The simplest possible way of combining unigram features is by considering ngram models.", "labels": [], "entities": []}, {"text": "An n-gram extracts features from a sliding window (of size n) over the text.", "labels": [], "entities": []}, {"text": "We call this model BoW(N = n).", "labels": [], "entities": [{"text": "BoW", "start_pos": 19, "end_pos": 22, "type": "METRIC", "confidence": 0.49276548624038696}]}, {"text": "Our BoW(N = 1) model takes word forms as features, and there are obviously more advanced ways of automatically combining such features.", "labels": [], "entities": []}, {"text": "Kernel representations We experimented with applying an approximate feature map for the additive \u03c7 2 -kernel.", "labels": [], "entities": []}, {"text": "We used two sample steps, resulting in 4N + 1 features.", "labels": [], "entities": []}, {"text": "See Vedaldi and Zimmerman (2011) for details.", "labels": [], "entities": []}, {"text": "Deep features We also ran denoising autoencoders (, previously applied to a wide range of NLP tasks, with 2N nodes in the middle layer to obtain a deep representation of our datasets from \u03c7 2 -BoW input.", "labels": [], "entities": []}, {"text": "The network was trained for 15 epochs.", "labels": [], "entities": []}, {"text": "We set the drop-out rate to 0.0 and 0.3.", "labels": [], "entities": []}, {"text": "Summary of feature sets The feature sets -BoW, Exp and AMT -are very different.", "labels": [], "entities": [{"text": "BoW", "start_pos": 42, "end_pos": 45, "type": "DATASET", "confidence": 0.7864763736724854}]}, {"text": "Their characteristics are presented in.", "labels": [], "entities": []}, {"text": "P (1) is the class distribution, e.g., the prior probability of positive class.", "labels": [], "entities": [{"text": "P", "start_pos": 0, "end_pos": 1, "type": "METRIC", "confidence": 0.916299045085907}]}, {"text": "n is the number of data points, m the number of features.", "labels": [], "entities": []}, {"text": "Finally, \u00b5 x is the average density of data points.", "labels": [], "entities": []}, {"text": "One observation is of course that the expert feature set Exp is much smaller than BoW and AMT, but note also that the expert features fire about 150 times more often on average than the BoW features.", "labels": [], "entities": [{"text": "Exp", "start_pos": 57, "end_pos": 60, "type": "DATASET", "confidence": 0.5906171202659607}, {"text": "BoW", "start_pos": 82, "end_pos": 85, "type": "DATASET", "confidence": 0.9808313250541687}, {"text": "AMT", "start_pos": 90, "end_pos": 93, "type": "DATASET", "confidence": 0.8258770108222961}, {"text": "BoW", "start_pos": 186, "end_pos": 189, "type": "DATASET", "confidence": 0.9364755749702454}]}, {"text": "HI is only a small set of additional features.", "labels": [], "entities": [{"text": "HI", "start_pos": 0, "end_pos": 2, "type": "DATASET", "confidence": 0.6958526968955994}]}], "tableCaptions": [{"text": " Table 1: Characteristics of the n \u00d7 m data sets", "labels": [], "entities": []}, {"text": " Table 2: Results using all features", "labels": [], "entities": []}, {"text": " Table 1. P (1) is the class dis- tribution, e.g., the prior probability of positive class.  n is the number of data points, m the number of  features. Finally, \u00b5 x is the average density of data  points. One observation is of course that the expert  feature set Exp is much smaller than BoW and AMT,  but note also that the expert features fire about 150  times more often on average than the BoW features.  HI is only a small set of additional features.", "labels": [], "entities": [{"text": "BoW", "start_pos": 288, "end_pos": 291, "type": "DATASET", "confidence": 0.9729184508323669}, {"text": "AMT", "start_pos": 296, "end_pos": 299, "type": "DATASET", "confidence": 0.7320108413696289}, {"text": "BoW", "start_pos": 394, "end_pos": 397, "type": "DATASET", "confidence": 0.9687839150428772}, {"text": "HI", "start_pos": 409, "end_pos": 411, "type": "METRIC", "confidence": 0.6868559122085571}]}]}