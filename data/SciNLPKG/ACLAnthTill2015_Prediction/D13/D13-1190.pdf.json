{"title": [], "abstractContent": [{"text": "This paper presents an approach for detecting promotional content in Wikipedia.", "labels": [], "entities": [{"text": "detecting promotional content in Wikipedia", "start_pos": 36, "end_pos": 78, "type": "TASK", "confidence": 0.8071963787078857}]}, {"text": "By incorporating stylometric features, including features based on n-gram and PCFG language models, we demonstrate improved accuracy at identifying promotional articles, compared to using only lexical information and meta-features.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 124, "end_pos": 132, "type": "METRIC", "confidence": 0.9977478384971619}]}], "introductionContent": [{"text": "Wikipedia is a free, collaboratively edited encyclopedia.", "labels": [], "entities": [{"text": "Wikipedia", "start_pos": 0, "end_pos": 9, "type": "DATASET", "confidence": 0.9342619180679321}]}, {"text": "Since normally anyone can create and edit pages, some articles are written in a promotional tone, violating Wikipedia's policy requiring a neutral viewpoint.", "labels": [], "entities": []}, {"text": "Currently, such articles are identified manually and tagged with an appropriate Cleanup message 1 by Wikipedia editors.", "labels": [], "entities": []}, {"text": "Given the scale and rate of growth of Wikipedia, it is infeasible to manually identify all such articles.", "labels": [], "entities": []}, {"text": "Hence, we present an approach to automatically detect promotional articles.", "labels": [], "entities": []}, {"text": "Related work in quality flaw detection in Wikipedia () has relied on meta-features based on edit history, Wikipedia links, structural features and counts of words, sentences and paragraphs.", "labels": [], "entities": [{"text": "quality flaw detection", "start_pos": 16, "end_pos": 38, "type": "TASK", "confidence": 0.7553680340449015}]}, {"text": "However, we hypothesize that there are subtle differences in the linguistic style that distinguish promotional tone, which we attempt to capture using stylometric features, particularly deeper syntactic features.", "labels": [], "entities": []}, {"text": "We model the style of promotional and normal articles using language models http://en.wikipedia.org/wiki/Wikipedia: Template_messages/Cleanup based on both n-grams and Probabilistic Context Free Grammars (PCFGs).", "labels": [], "entities": []}, {"text": "We show that using such stylometric features improves over using only shallow lexical and meta-features.", "labels": [], "entities": []}], "datasetContent": [{"text": "We extracted a set of about 13,000 articles from English Wikipedia's category, \"Category:All arti- such as peacock terms ('legendary', 'acclaimed', 'world-class'), weasel terms ('many scholars state', 'it is believed/regarded', 'many are of the opinion', 'most feel', 'experts declare', 'it is often reported') , editorializing terms ('without a doubt', 'of course', 'essentially') Percentage of easy words, difficult words (DaleChall List), long words and stop words Overall Sentiment Score based on SentiWordNet 4: Content Features of a Wikipedia Article cles with a promotional tone\" as a set of positive examples.", "labels": [], "entities": [{"text": "DaleChall List", "start_pos": 425, "end_pos": 439, "type": "DATASET", "confidence": 0.9459213018417358}]}, {"text": "We extracted a set of 26,000 untagged articles to form a noisy set of negative examples, which may contain some promotional articles that have not yet been tagged by Wikipedia editors.", "labels": [], "entities": []}, {"text": "To counter this noise, we repeated the experiment using Wikipedia's Featured Articles and Good Articles (approx. 11,000) as a set of clean negative examples.", "labels": [], "entities": []}, {"text": "We used 70% of the articles in each category to train language models for each of the three categories (promotional articles, featured/good articles, untagged articles), and used the remaining 30% to evaluate classifier performance using 10-fold crossvalidation.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 6: Performance (Precision(P), Recall(R), F1 score, AUC) of the classifier in the two settings", "labels": [], "entities": [{"text": "Precision(P)", "start_pos": 23, "end_pos": 35, "type": "METRIC", "confidence": 0.9493076503276825}, {"text": "Recall(R)", "start_pos": 37, "end_pos": 46, "type": "METRIC", "confidence": 0.96242356300354}, {"text": "F1 score", "start_pos": 48, "end_pos": 56, "type": "METRIC", "confidence": 0.9912542998790741}, {"text": "AUC", "start_pos": 58, "end_pos": 61, "type": "METRIC", "confidence": 0.9876132011413574}]}]}