{"title": [{"text": "Using Paraphrases and Lexical Semantics to Improve the Accuracy and the Robustness of Supervised Models in Situated Dialogue Systems", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 55, "end_pos": 63, "type": "METRIC", "confidence": 0.9477111101150513}]}], "abstractContent": [{"text": "This paper explores to what extent lemmati-sation, lexical resources, distributional semantics and paraphrases can increase the accuracy of supervised models for dialogue management.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 128, "end_pos": 136, "type": "METRIC", "confidence": 0.9971502423286438}, {"text": "dialogue management", "start_pos": 162, "end_pos": 181, "type": "TASK", "confidence": 0.8286046385765076}]}, {"text": "The results suggest that each of these factors can help improve performance but that the impact will vary depending on their combination and on the evaluation mode.", "labels": [], "entities": []}], "introductionContent": [{"text": "One strand of work in dialog research targets the rapid prototyping of virtual humans capable of conducting a conversation with humans in the context of a virtual world.", "labels": [], "entities": []}, {"text": "In particular, question answering (QA) characters can respond to a restricted set of topics after training on a set of dialogs whose utterances are annotated with dialogue acts (.", "labels": [], "entities": [{"text": "question answering (QA)", "start_pos": 15, "end_pos": 38, "type": "TASK", "confidence": 0.8418165802955627}]}, {"text": "As argued in (, the size of the training corpus is a major factor in allowing QA characters that are both robust and accurate.", "labels": [], "entities": []}, {"text": "In addition, the training corpus should arguably be of good quality in that (i) it should contain the various ways of expressing the same content (paraphrases) and (ii) the data should not be skewed.", "labels": [], "entities": []}, {"text": "In sum, the ideal training data should be large (more data is better data) ; balanced (similar amount of data for each class targeted by the classifier) and varied (it should encompass the largest possible number of paraphrases and synonyms for the utterances of each class).", "labels": [], "entities": []}, {"text": "In this paper, we explore different ways of improving and complementing the training data of a supervised QA character.", "labels": [], "entities": []}, {"text": "We expand the size and the quality (less skewed data) of the training corpus using paraphrase generation techniques.", "labels": [], "entities": [{"text": "paraphrase generation", "start_pos": 83, "end_pos": 104, "type": "TASK", "confidence": 0.8496389985084534}]}, {"text": "We compare the performance obtained on lemmatised vs. non lemmatised data.", "labels": [], "entities": []}, {"text": "And we investigate how various resources (synonym dictionaries, WordNet, distributional neighbours) can be used to handle unseen words at run time.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 64, "end_pos": 71, "type": "DATASET", "confidence": 0.9493009448051453}]}], "datasetContent": [{"text": "We run our experiments on a dialog engine developed fora serious game called Mission Plastechnologie.", "labels": [], "entities": []}, {"text": "In this game, the player must interact with different virtual humans through a sequence of 12 subdialogs, each of them occurring in a different part of the virtual world.", "labels": [], "entities": []}, {"text": "The training corpus consists of around 1250 Human-Human dialogues which were manually annotated with dialog moves.", "labels": [], "entities": []}, {"text": "As the following dialog excerpt illustrates, the dialogs are conducted in French and each dialog turn is manually annotated using a set of 28 dialog acts.", "labels": [], "entities": []}, {"text": "For a more detailed presentation of the training corpus and of the annotation scheme, the reader is referred to dialog : 01_dialogDirecteur-Tue Jun 14 11 :04 :23 2011 >M.Jasper : Bonjour, je suis M.Jasper le directeur.", "labels": [], "entities": []}, {"text": "|| greet Dialog Systems For our experiments, we use a hybrid dialog system similar to that described in (Rojas.", "labels": [], "entities": []}, {"text": "This system combines a classifier for interpreting the players utterances with an information state dialog manager which selects an appropriate system response based on the dialog move assigned by the classifier to the user turn.", "labels": [], "entities": []}, {"text": "The classifier is a logistic regression classifier 1 which was trained for each subdialog in the game.", "labels": [], "entities": []}, {"text": "The features used for training are the set of content words which are associated with a given dialog move and which remain after TF*IDF 2 filtering.", "labels": [], "entities": [{"text": "TF*IDF 2 filtering", "start_pos": 129, "end_pos": 147, "type": "DATASET", "confidence": 0.627620804309845}]}, {"text": "Note that in this experiment, we do not use contextual features such as the dialog acts labeling the previous turns.", "labels": [], "entities": []}, {"text": "There are two reasons for this.", "labels": [], "entities": []}, {"text": "First, we want to focus on the impact of synonym handling, paraphrasing and lemmatisation on dialog management.", "labels": [], "entities": [{"text": "synonym handling", "start_pos": 41, "end_pos": 57, "type": "TASK", "confidence": 0.8969444930553436}, {"text": "dialog management", "start_pos": 93, "end_pos": 110, "type": "TASK", "confidence": 0.8853508532047272}]}, {"text": "Removing contextual features allows us to focus on how content features (content words) can be improved by these mechanisms.", "labels": [], "entities": []}, {"text": "Second, when evaluating on the H-C corpus (see below), contextual features are often incorrect (because the system might incorrectly interpret and thus label a user turn).", "labels": [], "entities": []}, {"text": "Excluding contextual features from training allows fora fair comparison between the H-H and the H-C evaluation.", "labels": [], "entities": []}, {"text": "Test Data and Evaluation Metrics We use accu- 1.", "labels": [], "entities": [{"text": "accu- 1", "start_pos": 40, "end_pos": 47, "type": "METRIC", "confidence": 0.9565529028574625}]}, {"text": "We used MALLET) for the LR classifier with L1 Regularisation.", "labels": [], "entities": [{"text": "MALLET", "start_pos": 8, "end_pos": 14, "type": "METRIC", "confidence": 0.9821719527244568}]}, {"text": "2. TF*IDF = Term Frequency*Inverse Document Frequency racy (the number of correct classifications divided by the number of instances in the testset) to measure performance and we carryout two types of evaluation.", "labels": [], "entities": [{"text": "TF", "start_pos": 3, "end_pos": 5, "type": "METRIC", "confidence": 0.9577210545539856}, {"text": "IDF", "start_pos": 6, "end_pos": 9, "type": "METRIC", "confidence": 0.5819539427757263}, {"text": "Term Frequency*Inverse Document Frequency racy", "start_pos": 12, "end_pos": 58, "type": "METRIC", "confidence": 0.7243180445262364}]}, {"text": "On the one hand, we use 10-fold crossvalidation on the EmoSpeech corpus (H-H data).", "labels": [], "entities": [{"text": "EmoSpeech corpus (H-H data", "start_pos": 55, "end_pos": 81, "type": "DATASET", "confidence": 0.7927509069442749}]}, {"text": "On the other hand, we report accuracy on a corpus of 550 Human-Computer (H-C) dialogues obtained by having 22 subjects play the game against the QA character trained on the H-H corpus.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 29, "end_pos": 37, "type": "METRIC", "confidence": 0.9996504783630371}]}, {"text": "As we shall see below, performance decreases in this second evaluation suggesting that subjects produce different turns when playing with a computer than with a human thereby inducing a weak out-of-domain effect and negatively impacting classification.", "labels": [], "entities": []}, {"text": "Evaluation on the H-H corpus therefore gives a measure of how well the techniques explored help improving the dialog engine when used in areal life setting.", "labels": [], "entities": [{"text": "H-H corpus", "start_pos": 18, "end_pos": 28, "type": "DATASET", "confidence": 0.7363450825214386}]}, {"text": "Correspondingly, we use two different tests for measuring statistical significance.", "labels": [], "entities": []}, {"text": "In the H-H evaluation, significance is computed using the Wilcoxon signed rank test because data are dependent and are not assumed to be normally distributed.", "labels": [], "entities": [{"text": "significance", "start_pos": 23, "end_pos": 35, "type": "METRIC", "confidence": 0.9531050324440002}]}, {"text": "When building the testset we took care of not including paraphrases of utterances in the training partition (for each paraphrase generated automatically we keep track of the original utterance), however utterances in both datasets might be generated by the same subject, since a subject completed 12 distinct dialogues during the game.", "labels": [], "entities": []}, {"text": "Conversely, in the H-C evaluation, training (H-H data) and test (H-C data) sets were collected under different conditions with different subjects therefore significance was computed using the McNemar sign-test).", "labels": [], "entities": [{"text": "McNemar sign-test", "start_pos": 192, "end_pos": 209, "type": "DATASET", "confidence": 0.7513662278652191}]}], "tableCaptions": []}