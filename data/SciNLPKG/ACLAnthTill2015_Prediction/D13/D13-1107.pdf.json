{"title": [{"text": "Multi-domain Adaptation for SMT Using Multi-task Learning *", "labels": [], "entities": [{"text": "Multi-domain Adaptation", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.6334259957075119}, {"text": "SMT", "start_pos": 28, "end_pos": 31, "type": "TASK", "confidence": 0.9856692552566528}]}], "abstractContent": [{"text": "Domain adaptation for SMT usually adapts models to an individual specific domain.", "labels": [], "entities": [{"text": "Domain adaptation", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.7711122035980225}, {"text": "SMT", "start_pos": 22, "end_pos": 25, "type": "TASK", "confidence": 0.9921108484268188}]}, {"text": "However, it often lacks some correlation among different domains where common knowledge could be shared to improve the overall translation quality.", "labels": [], "entities": []}, {"text": "In this paper, we propose a novel multi-domain adaptation approach for SMT using Multi-Task Learning (MTL), with in-domain models tailored for each specific domain and a general-domain model shared by different domains.", "labels": [], "entities": [{"text": "SMT", "start_pos": 71, "end_pos": 74, "type": "TASK", "confidence": 0.994490385055542}]}, {"text": "The parameters of these models are tuned jointly via MTL so that they can learn general knowledge more accurately and exploit domain knowledge better.", "labels": [], "entities": [{"text": "MTL", "start_pos": 53, "end_pos": 56, "type": "DATASET", "confidence": 0.7000005841255188}]}, {"text": "Our experiments on a large-scale English-to-Chinese translation task validate that the MTL-based adaptation approach significantly and consistently improves the translation quality compared to a non-adapted baseline.", "labels": [], "entities": [{"text": "English-to-Chinese translation task", "start_pos": 33, "end_pos": 68, "type": "TASK", "confidence": 0.697094718615214}, {"text": "MTL-based adaptation", "start_pos": 87, "end_pos": 107, "type": "TASK", "confidence": 0.8706552982330322}]}, {"text": "Furthermore, it also outperforms the individual adaptation of each specific domain.", "labels": [], "entities": []}], "introductionContent": [{"text": "Domain adaptation is an active topic in statistical machine learning and aims to alleviate the domain mismatch between training and testing data.", "labels": [], "entities": [{"text": "Domain adaptation", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.8013036549091339}, {"text": "statistical machine learning", "start_pos": 40, "end_pos": 68, "type": "TASK", "confidence": 0.6640467345714569}]}, {"text": "Like many machine learning tasks, Statistical Machine Translation (SMT) assumes that the data distributions of training and testing domains are similar.", "labels": [], "entities": [{"text": "Statistical Machine Translation (SMT)", "start_pos": 34, "end_pos": 71, "type": "TASK", "confidence": 0.8815974990526835}]}, {"text": "However, this assumption does not hold for real world SMT systems since training data for SMT models may come from a variety of domains.", "labels": [], "entities": [{"text": "SMT", "start_pos": 54, "end_pos": 57, "type": "TASK", "confidence": 0.9850513935089111}, {"text": "SMT", "start_pos": 90, "end_pos": 93, "type": "TASK", "confidence": 0.9911052584648132}]}, {"text": "The translation quality is often unsatisfactory when * This work was done while the first and second authors were visiting Microsoft Research Asia.", "labels": [], "entities": [{"text": "translation", "start_pos": 4, "end_pos": 15, "type": "TASK", "confidence": 0.945286750793457}]}, {"text": "translating texts from a specific domain using a general model that is trained over a hotchpotch of bilingual corpora.", "labels": [], "entities": []}, {"text": "Therefore, domain adaptation is crucial for SMT systems to achieve better performance.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 11, "end_pos": 28, "type": "TASK", "confidence": 0.799701601266861}, {"text": "SMT", "start_pos": 44, "end_pos": 47, "type": "TASK", "confidence": 0.9956556558609009}]}, {"text": "Previous research on domain adaptation for SMT includes data selection and weighting), mixture models, and semi-supervised transductive learning (, etc.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 21, "end_pos": 38, "type": "TASK", "confidence": 0.7126136720180511}, {"text": "SMT", "start_pos": 43, "end_pos": 46, "type": "TASK", "confidence": 0.9857930541038513}, {"text": "data selection and weighting", "start_pos": 56, "end_pos": 84, "type": "TASK", "confidence": 0.6957879662513733}]}, {"text": "Most of these methods adapt SMT models to a specific domain according to testing data and have achieved good performance.", "labels": [], "entities": [{"text": "SMT", "start_pos": 28, "end_pos": 31, "type": "TASK", "confidence": 0.994998574256897}]}, {"text": "It is natural that real world SMT systems should adapt the models to multiple domains because the input maybe heterogeneous, so that the overall translation quality can be improved.", "labels": [], "entities": [{"text": "SMT", "start_pos": 30, "end_pos": 33, "type": "TASK", "confidence": 0.9896929860115051}]}, {"text": "Although we can easily apply these methods to multiple domains individually, it is difficult to use the common knowledge across different domains.", "labels": [], "entities": []}, {"text": "To leverage the common knowledge, we need to devise a multi-domain adaptation approach that jointly adapts the SMT models.", "labels": [], "entities": [{"text": "SMT", "start_pos": 111, "end_pos": 114, "type": "TASK", "confidence": 0.9843981862068176}]}, {"text": "Multi-domain adaptation has been proved quite effective in sentiment analysis) and web ranking, where the commonalities and differences across multiple domains are explicitly addressed by.", "labels": [], "entities": [{"text": "Multi-domain adaptation", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.7523566484451294}, {"text": "sentiment analysis", "start_pos": 59, "end_pos": 77, "type": "TASK", "confidence": 0.9570489227771759}, {"text": "web ranking", "start_pos": 83, "end_pos": 94, "type": "TASK", "confidence": 0.7627347409725189}]}, {"text": "MTL is an approach that learns one target problem with other related problems at the same time, using a shared feature representation.", "labels": [], "entities": [{"text": "MTL", "start_pos": 0, "end_pos": 3, "type": "TASK", "confidence": 0.9003265500068665}]}, {"text": "The key advantage of MTL is to enable implicit data sharing and regularization.", "labels": [], "entities": [{"text": "MTL", "start_pos": 21, "end_pos": 24, "type": "TASK", "confidence": 0.911163330078125}, {"text": "data sharing", "start_pos": 47, "end_pos": 59, "type": "TASK", "confidence": 0.6919223219156265}]}, {"text": "Therefore, it often leads to a better model for each task.", "labels": [], "entities": []}, {"text": "Analogously, we expect that the overall translation quality can be further improved by using an MTL-based  multi-domain adaptation approach.", "labels": [], "entities": [{"text": "MTL-based  multi-domain adaptation", "start_pos": 96, "end_pos": 130, "type": "TASK", "confidence": 0.730649491151174}]}, {"text": "In this paper, we use MTL to jointly adapt SMT models to multiple domains.", "labels": [], "entities": [{"text": "MTL", "start_pos": 22, "end_pos": 25, "type": "TASK", "confidence": 0.7956399917602539}, {"text": "SMT", "start_pos": 43, "end_pos": 46, "type": "TASK", "confidence": 0.9932233095169067}]}, {"text": "Specifically, we develop multiple SMT systems based on mixture models, where each system is tailored for one specific domain with an in-domain Translation and an in-domain Language Model (LM).", "labels": [], "entities": [{"text": "SMT", "start_pos": 34, "end_pos": 37, "type": "TASK", "confidence": 0.9927704930305481}]}, {"text": "Meanwhile, all the systems share a same general-domain TM and LM.", "labels": [], "entities": []}, {"text": "These SMT systems are considered as several related tasks with a shared feature representation, which fits well into a unified MTL framework.", "labels": [], "entities": [{"text": "SMT", "start_pos": 6, "end_pos": 9, "type": "TASK", "confidence": 0.987917959690094}]}, {"text": "With the MTL-based joint tuning, general knowledge can be better learned by the generaldomain models, while domain knowledge can be better exploited by the in-domain models as well.", "labels": [], "entities": []}, {"text": "By using a distributed stochastic learning approach, we can estimate the feature weights of multiple SMT systems at the same time.", "labels": [], "entities": [{"text": "SMT", "start_pos": 101, "end_pos": 104, "type": "TASK", "confidence": 0.9814698696136475}]}, {"text": "Furthermore, we modify the algorithm to treat in-domain and general-domain features separately, which brings regularization to multiple SMT systems in an efficient way.", "labels": [], "entities": [{"text": "SMT", "start_pos": 136, "end_pos": 139, "type": "TASK", "confidence": 0.9782861471176147}]}, {"text": "Experimental results have shown that our method can significantly improve the translation quality on multiple domains over a nonadapted baseline.", "labels": [], "entities": []}, {"text": "Moreover, the MTL-based adaptation also outperforms the conventional individual adaptation approach towards each domain.", "labels": [], "entities": [{"text": "MTL-based adaptation", "start_pos": 14, "end_pos": 34, "type": "TASK", "confidence": 0.9491441547870636}]}, {"text": "The rest of the paper is organized as follows: The proposed approach is explained in Section 2.", "labels": [], "entities": []}, {"text": "Experimental results are presented in Section 3.", "labels": [], "entities": []}, {"text": "Section 4 introduces some related work.", "labels": [], "entities": []}, {"text": "Section 5 concludes the paper and suggests future research directions.", "labels": [], "entities": []}, {"text": "gives an example with N pre-defined domains to illustrate the main idea.", "labels": [], "entities": []}, {"text": "There are three steps in the training phase.", "labels": [], "entities": []}, {"text": "First, in-domain training data is selected according to the pre-defined domains (Section 2.1).", "labels": [], "entities": []}, {"text": "Second, in-domain models and general-domain models are trained to develop the domain-specific SMT systems (Section 2.2).", "labels": [], "entities": [{"text": "SMT", "start_pos": 94, "end_pos": 97, "type": "TASK", "confidence": 0.9006640315055847}]}, {"text": "Third, multiple domain-specific SMT systems are tuned jointly by using an MTL-based approach (Section 2.3).", "labels": [], "entities": [{"text": "SMT", "start_pos": 32, "end_pos": 35, "type": "TASK", "confidence": 0.9306575059890747}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 3: End-to-end experimental results (BLEU4%) with large-scale training data (p < 0.05). \"", "labels": [], "entities": [{"text": "BLEU4", "start_pos": 43, "end_pos": 48, "type": "METRIC", "confidence": 0.9990307092666626}]}]}