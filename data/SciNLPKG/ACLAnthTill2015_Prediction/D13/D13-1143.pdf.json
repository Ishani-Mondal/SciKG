{"title": [{"text": "Dependency language models for sentence completion", "labels": [], "entities": [{"text": "sentence completion", "start_pos": 31, "end_pos": 50, "type": "TASK", "confidence": 0.7333459854125977}]}], "abstractContent": [{"text": "Sentence completion is a challenging semantic modeling task in which models must choose the most appropriate word from a given set to complete a sentence.", "labels": [], "entities": [{"text": "Sentence completion", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.9304450154304504}]}, {"text": "Although a variety of language models have been applied to this task in previous work, none of the existing approaches incorporate syntactic information.", "labels": [], "entities": []}, {"text": "In this paper we propose to tackle this task using a pair of simple language models in which the probability of a sentence is estimated as the probability of the lexicalisa-tion of a given syntactic dependency tree.", "labels": [], "entities": []}, {"text": "We apply our approach to the Microsoft Research Sentence Completion Challenge and show that it improves on n-gram language models by 8.7 percentage points, achieving the highest accuracy reported to date apart from neural language models that are more complex and expensive to train.", "labels": [], "entities": [{"text": "Microsoft Research Sentence Completion Challenge", "start_pos": 29, "end_pos": 77, "type": "TASK", "confidence": 0.7097925543785095}, {"text": "accuracy", "start_pos": 178, "end_pos": 186, "type": "METRIC", "confidence": 0.9967856407165527}]}], "introductionContent": [{"text": "The verbal reasoning sections of standardised tests such as the Scholastic Aptitude Test (SAT) feature problems where a partially complete sentence is given and the candidate must choose the word or phrase from a list of options which completes the sentence in a logically consistent way.", "labels": [], "entities": []}, {"text": "Sentence completion is a challenging semantic modelling problem.", "labels": [], "entities": [{"text": "Sentence completion", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.9376583695411682}]}, {"text": "Systematic approaches for solving such problems require models that can judge the global coherence of sentences.", "labels": [], "entities": []}, {"text": "Such measures of global coherence may prove to be useful in various applications, including machine translation and natural language generation ( ).", "labels": [], "entities": [{"text": "machine translation", "start_pos": 92, "end_pos": 111, "type": "TASK", "confidence": 0.8281142711639404}, {"text": "natural language generation", "start_pos": 116, "end_pos": 143, "type": "TASK", "confidence": 0.6738252739111582}]}, {"text": "Most approaches to sentence completion employ language models which use a window of immediate context around the missing word and choose the word that results in the completed sentence with the highest probability (.", "labels": [], "entities": [{"text": "sentence completion", "start_pos": 19, "end_pos": 38, "type": "TASK", "confidence": 0.7541162669658661}]}, {"text": "However, such language models may fail to identify sentences that are locally coherent but are improbable due to long-range syntactic/semantic dependencies.", "labels": [], "entities": []}, {"text": "Consider, for example, completing the sentence I saw a tiger which was really very ... with either fierce or talkative.", "labels": [], "entities": []}, {"text": "A language model relying on up to five words of immediate context would ignore the crucial dependency between the missing word and the noun tiger.", "labels": [], "entities": []}, {"text": "In this paper we tackle sentence completion using language models based on dependency grammar.", "labels": [], "entities": [{"text": "sentence completion", "start_pos": 24, "end_pos": 43, "type": "TASK", "confidence": 0.8636361360549927}]}, {"text": "These models are similar to standard n-gram language models, but instead of using the linear ordering of the words in the sentence, they generate words along paths in the dependency tree of the sentence.", "labels": [], "entities": []}, {"text": "Unlike other approaches incorporating syntax into language models (e.g.,), our models are relatively easy to train and estimate, and can exploit standard smoothing methods.", "labels": [], "entities": []}, {"text": "We apply them to the Microsoft Research Sentence Completion Challenge (  and show an improvement of 8.7 points inaccuracy over ngram models, giving the best results to date for any method apart from the more computationally demanding neural language models.", "labels": [], "entities": [{"text": "Microsoft Research Sentence Completion Challenge", "start_pos": 21, "end_pos": 69, "type": "TASK", "confidence": 0.6333952188491822}]}], "datasetContent": [{"text": "We carried out experiments using the Microsoft Research Sentence (MSR) Completion Challenge ( . This consists of a set of 1,040 sentence completion problems taken from five of the Sherlock Holmes novels by Arthur Conan Doyle.", "labels": [], "entities": [{"text": "sentence completion problems taken from five of the Sherlock Holmes novels by Arthur Conan Doyle", "start_pos": 128, "end_pos": 224, "type": "TASK", "confidence": 0.6497176984945933}]}, {"text": "Each problem consists of a sentence in which one word has been removed and replaced with a blank and a set of 5 candidate words to complete the sentence.", "labels": [], "entities": []}, {"text": "The task is to choose the candidate word which, when inserted into the blank, gives the most probable complete sentence.", "labels": [], "entities": []}, {"text": "The set of candidates consists of the original word and 4 imposter words with similar distributional statistics.", "labels": [], "entities": []}, {"text": "Human judges were tasked with choosing imposter words which would lead to grammatically correct sentences and such that, with some thought, the correct answer should be unambiguous.", "labels": [], "entities": []}, {"text": "The training data set consists of 522 19th century novels from Project Gutenberg.", "labels": [], "entities": [{"text": "training data set", "start_pos": 4, "end_pos": 21, "type": "DATASET", "confidence": 0.781365712483724}]}, {"text": "We parsed the training data using the Nivre arc-eager deterministic dependency parsing algorithm () as implemented in MaltParser ().", "labels": [], "entities": [{"text": "deterministic dependency parsing", "start_pos": 54, "end_pos": 86, "type": "TASK", "confidence": 0.5887370109558105}]}, {"text": "We trained order N labelled and unabelled dependency).", "labels": [], "entities": []}, {"text": "To test a given language model, we calculated the scores it assigned to each candidate sentence and chose the completion with the highest score.", "labels": [], "entities": []}, {"text": "For the dependency language models we parsed the sentence with each of the 5 possible completions and calculated the probability in each case.", "labels": [], "entities": []}, {"text": "illustrates an example of this process for the order 3 unlabelled model.", "labels": [], "entities": []}, {"text": "margin for all orders considered.", "labels": [], "entities": [{"text": "margin", "start_pos": 0, "end_pos": 6, "type": "METRIC", "confidence": 0.9761725664138794}]}, {"text": "The best result was achieved by the order 4 labelled dependency model which is 8.7 points inaccuracy better than the best ngram model.", "labels": [], "entities": []}, {"text": "Furthermore, the labelled dependency models outperformed their unlabelled counterparts for every order except 2.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Summary of results for Sentence Completion", "labels": [], "entities": [{"text": "Sentence Completion", "start_pos": 33, "end_pos": 52, "type": "TASK", "confidence": 0.9831957817077637}]}, {"text": " Table 2: Comparison against previous results", "labels": [], "entities": []}]}