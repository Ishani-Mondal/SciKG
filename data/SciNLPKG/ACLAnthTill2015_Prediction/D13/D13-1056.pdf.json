{"title": [], "abstractContent": [{"text": "We introduce a novel discriminative model for phrase-based monolingual alignment using a semi-Markov CRF.", "labels": [], "entities": [{"text": "phrase-based monolingual alignment", "start_pos": 46, "end_pos": 80, "type": "TASK", "confidence": 0.6456856727600098}]}, {"text": "Our model achieves state-of-the-art alignment accuracy on two phrase-based alignment datasets (RTE and paraphrase), while doing significantly better than other strong baselines in both non-identical alignment and phrase-only alignment.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 46, "end_pos": 54, "type": "METRIC", "confidence": 0.9805482625961304}]}, {"text": "Additional experiments highlight the potential benefit of our alignment model to RTE, paraphrase identification and question answering, where even a naive application of our model's alignment score approaches the state of the art.", "labels": [], "entities": [{"text": "RTE", "start_pos": 81, "end_pos": 84, "type": "TASK", "confidence": 0.962804913520813}, {"text": "paraphrase identification", "start_pos": 86, "end_pos": 111, "type": "TASK", "confidence": 0.9549273550510406}, {"text": "question answering", "start_pos": 116, "end_pos": 134, "type": "TASK", "confidence": 0.8926507532596588}]}], "introductionContent": [{"text": "Various NLP tasks can be treated as an alignment problem: machine translation (aligning words in one language with words in another language), question answering (aligning question words with the answer phrase), textual entailment recognition (aligning premise with hypothesis), paraphrase detection (aligning semantically equivalent words), etc.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 58, "end_pos": 77, "type": "TASK", "confidence": 0.7876870334148407}, {"text": "question answering", "start_pos": 143, "end_pos": 161, "type": "TASK", "confidence": 0.8217467367649078}, {"text": "textual entailment recognition", "start_pos": 212, "end_pos": 242, "type": "TASK", "confidence": 0.7479712168375651}, {"text": "paraphrase detection", "start_pos": 279, "end_pos": 299, "type": "TASK", "confidence": 0.9106842875480652}]}, {"text": "Even though most of these tasks involve only a single language, alignment research has primarily focused on the bilingual setting (i.e., machine translation) rather than monolingual.", "labels": [], "entities": [{"text": "alignment", "start_pos": 64, "end_pos": 73, "type": "TASK", "confidence": 0.9698193669319153}, {"text": "machine translation)", "start_pos": 137, "end_pos": 157, "type": "TASK", "confidence": 0.7864532371362051}]}, {"text": "Moreover, most work has considered token-based approaches over phrase-based.", "labels": [], "entities": []}, {"text": "Here we seek to address this imbalance by proposing better phrase-based models for monolingual word alignment.", "labels": [], "entities": [{"text": "monolingual word alignment", "start_pos": 83, "end_pos": 109, "type": "TASK", "confidence": 0.6554147104422251}]}, {"text": "Most token-based alignment models can extrinsically handle phrase-based alignment to some extent.", "labels": [], "entities": [{"text": "phrase-based alignment", "start_pos": 59, "end_pos": 81, "type": "TASK", "confidence": 0.6972495913505554}]}, {"text": "For instance, in the case of NYC aligning to New York City, the single source word NYC may align three times separately to the target words: NYC\u2194New, NYC\u2194York, NYC\u2194City.", "labels": [], "entities": []}, {"text": "Orin the case of identical alignment, New York City aligning to New York City is simply New\u2194New, York\u2194York, City\u2194City.", "labels": [], "entities": []}, {"text": "However, it is not as clear how to token-align New York (as a city) with New York City.", "labels": [], "entities": []}, {"text": "The problem is more prominent when aligning phrasal paraphrases or multiword expressions, such as pass away and kick the bucket.", "labels": [], "entities": []}, {"text": "This suggests an intrinsically phrase-based alignment model.", "labels": [], "entities": []}, {"text": "The token aligner jacana-align ( has achieved state-of-the-art result on the task of monolingual alignment, based on previous work of.", "labels": [], "entities": []}, {"text": "It employs a Conditional Random Field () to align tokens from the source sentence to tokens in the target sentence, by treating source tokens as \"observation\" and target tokens as \"hidden states\".", "labels": [], "entities": []}, {"text": "However, it is not designed to handle phrase-based alignment, largely due to the Markov nature of the underlying model: a state can only span one token each time, making it unable to align multiple consecutive tokens (i.e. a phrase).", "labels": [], "entities": [{"text": "phrase-based alignment", "start_pos": 38, "end_pos": 60, "type": "TASK", "confidence": 0.7312643527984619}]}, {"text": "We extend this model by introducing semiMarkov states for phrase-based alignment: a state can instead span multiple consecutive time steps, thus aligning phrases on the source side.", "labels": [], "entities": [{"text": "phrase-based alignment", "start_pos": 58, "end_pos": 80, "type": "TASK", "confidence": 0.7582074701786041}]}, {"text": "Also, we merge phrases on the target side to phrasal states, allowing the model to align phrases on the target side as well.", "labels": [], "entities": []}, {"text": "We evaluate the resulting semi-Markov CRF model on the task of phrase-based alignment, and then show a basic application in the NLP tasks of recognizing textual entailment, paraphrase identification, and question answering sentence ranking.", "labels": [], "entities": [{"text": "phrase-based alignment", "start_pos": 63, "end_pos": 85, "type": "TASK", "confidence": 0.7756587564945221}, {"text": "recognizing textual entailment", "start_pos": 141, "end_pos": 171, "type": "TASK", "confidence": 0.7767971555391947}, {"text": "paraphrase identification", "start_pos": 173, "end_pos": 198, "type": "TASK", "confidence": 0.9029164016246796}, {"text": "question answering sentence ranking", "start_pos": 204, "end_pos": 239, "type": "TASK", "confidence": 0.8467822819948196}]}, {"text": "The final phrase-based aligner is open-source.", "labels": [], "entities": []}], "datasetContent": [{"text": "MacCartney et al. and showed that the traditional MT bilingual aligner GIZA++ ( presented weak results on the task of monolingual alignment.", "labels": [], "entities": [{"text": "MT bilingual aligner GIZA", "start_pos": 50, "end_pos": 75, "type": "TASK", "confidence": 0.7708229124546051}]}, {"text": "Thus we instead used four other strong baselines: Meteor (Denkowski and Lavie, 2011): a system for evaluating machine translation by aligning MT output with reference sentences.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 110, "end_pos": 129, "type": "TASK", "confidence": 0.7103146463632584}]}, {"text": "It is designed for the task of monolingual alignment and supports phrasal alignment.", "labels": [], "entities": [{"text": "monolingual alignment", "start_pos": 31, "end_pos": 52, "type": "TASK", "confidence": 0.6569014191627502}, {"text": "phrasal alignment", "start_pos": 66, "end_pos": 83, "type": "TASK", "confidence": 0.6634545177221298}]}, {"text": "We used version 1.4 and default weights to optimize by maximum accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 63, "end_pos": 71, "type": "METRIC", "confidence": 0.9984110593795776}]}, {"text": "MANLI-constraint (Thadani and McKeown, 2011): a re-implemented MANLI system with ILPpowered decoding for speed and hard syntactic constraints to boost exact match rate, with reported numbers on MSR06.", "labels": [], "entities": [{"text": "MANLI-constraint", "start_pos": 0, "end_pos": 16, "type": "DATASET", "confidence": 0.9290978312492371}, {"text": "MSR06", "start_pos": 194, "end_pos": 199, "type": "DATASET", "confidence": 0.9637746810913086}]}, {"text": "MANLI-joint (: an improved version of MANLI-constraint that not only models phrasal alignments, but also alignments between dependency arcs, with reported numbers on the original Edinburgh paraphrase corpus.", "labels": [], "entities": [{"text": "MANLI-joint", "start_pos": 0, "end_pos": 11, "type": "DATASET", "confidence": 0.9221825003623962}, {"text": "MANLI-constraint", "start_pos": 38, "end_pos": 54, "type": "DATASET", "confidence": 0.821682870388031}, {"text": "Edinburgh paraphrase corpus", "start_pos": 179, "end_pos": 206, "type": "DATASET", "confidence": 0.9564431508382162}]}, {"text": "jacana-token (Yao et al., 2013a): a tokenbased aligner with state-of-the-art performance on MSR06.", "labels": [], "entities": [{"text": "MSR06", "start_pos": 92, "end_pos": 97, "type": "DATASET", "confidence": 0.9545226097106934}]}, {"text": "Note that the jacana-token aligner is open-source, so we were able to re-train it with exactly the same feature set used by our phrase-based model.", "labels": [], "entities": []}, {"text": "This allows a fair comparison of model performance (token-based vs. phrase-based).", "labels": [], "entities": []}, {"text": "The MANLI* systems are not available, thus we only reported their numbers from published papers.", "labels": [], "entities": [{"text": "MANLI*", "start_pos": 4, "end_pos": 10, "type": "DATASET", "confidence": 0.7697288691997528}]}, {"text": "The standard evaluation metrics for alignments are precision (P), recall (R), F 1 , and exact matching 6 a few examples: two Atlanta-based companies\u2194two Atlanta companies, the UK\u2194the UK, the 17-year-old\u2194the teenager, was held\u2194was held.", "labels": [], "entities": [{"text": "precision (P)", "start_pos": 51, "end_pos": 64, "type": "METRIC", "confidence": 0.9409601241350174}, {"text": "recall (R)", "start_pos": 66, "end_pos": 76, "type": "METRIC", "confidence": 0.9587632715702057}, {"text": "F 1", "start_pos": 78, "end_pos": 81, "type": "METRIC", "confidence": 0.991763710975647}, {"text": "exact matching", "start_pos": 88, "end_pos": 102, "type": "METRIC", "confidence": 0.9554141163825989}]}, {"text": "rate (E) based on either tokens (two tokens are considered aligned iff they are aligned) or phrases (two tokens are considered aligned iff they are contained within phrases that are aligned).", "labels": [], "entities": [{"text": "rate (E)", "start_pos": 0, "end_pos": 8, "type": "METRIC", "confidence": 0.7565174698829651}]}, {"text": "Following, we only report the results based on token alignments (which allows a partial credit if their containing phrases are not aligned), even for the phrase-based alignment task.", "labels": [], "entities": [{"text": "phrase-based alignment task", "start_pos": 154, "end_pos": 181, "type": "TASK", "confidence": 0.7657573223114014}]}, {"text": "The reasoning is that if a phrase-based aligner is already doing better than a token aligner in terms of token alignment scores, then the difference in terms of phrase alignment scores will be even larger.", "labels": [], "entities": []}, {"text": "Thus showing the superiority of token alignment scores is sufficient.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Statistics of the two manually aligned cor- pora, divided into training and test in sentence pairs.  The length column shows average lengths of source  and target sentences in a pair. %align. is the per- centage of aligned tokens.", "labels": [], "entities": []}, {"text": " Table 3: Results on original (mostly token) and phrasal", "labels": [], "entities": []}, {"text": " Table 4: Same results on the phrasal Edinburgh++ cor-", "labels": [], "entities": [{"text": "phrasal Edinburgh++ cor-", "start_pos": 30, "end_pos": 54, "type": "METRIC", "confidence": 0.862856638431549}]}, {"text": " Table 5: Results (Accuracy, Precision, Recall, Mean  Average Precision, Mean Reciprocal Rank) on the  tasks of RTE, PP and QA.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 19, "end_pos": 27, "type": "METRIC", "confidence": 0.9987512826919556}, {"text": "Precision", "start_pos": 29, "end_pos": 38, "type": "METRIC", "confidence": 0.9555612206459045}, {"text": "Recall", "start_pos": 40, "end_pos": 46, "type": "METRIC", "confidence": 0.9790275692939758}, {"text": "Mean  Average Precision", "start_pos": 48, "end_pos": 71, "type": "METRIC", "confidence": 0.9634651939074198}, {"text": "Mean Reciprocal Rank", "start_pos": 73, "end_pos": 93, "type": "METRIC", "confidence": 0.9160101016362509}, {"text": "RTE", "start_pos": 112, "end_pos": 115, "type": "TASK", "confidence": 0.6699857115745544}]}]}