{"title": [{"text": "Generating Coherent Event Schemas at Scale", "labels": [], "entities": [{"text": "Generating Coherent Event Schemas", "start_pos": 0, "end_pos": 33, "type": "TASK", "confidence": 0.6312248632311821}]}], "abstractContent": [{"text": "Chambers and Jurafsky (2009) demonstrated that event schemas can be automatically induced from text corpora.", "labels": [], "entities": []}, {"text": "However, our analysis of their schemas identifies several weaknesses , e.g., some schemas lack a common topic and distinct roles are incorrectly mixed into a single actor.", "labels": [], "entities": []}, {"text": "It is due in part to their pair-wise representation that treats subject-verb independently from verb-object.", "labels": [], "entities": []}, {"text": "This often leads to subject-verb-object triples that are not meaningful in the real-world.", "labels": [], "entities": []}, {"text": "We present a novel approach to inducing open-domain event schemas that overcomes these limitations.", "labels": [], "entities": []}, {"text": "Our approach uses co-occurrence statistics of semantically typed re-lational triples, which we call Rel-grams (re-lational n-grams).", "labels": [], "entities": []}, {"text": "Ina human evaluation, our schemas outperform Chambers's schemas by wide margins on several evaluation criteria.", "labels": [], "entities": []}, {"text": "Both Rel-grams and event schemas are freely available to the research community.", "labels": [], "entities": []}], "introductionContent": [{"text": "Event schemas (also known as templates or frames) have been widely used in information extraction.", "labels": [], "entities": [{"text": "information extraction", "start_pos": 75, "end_pos": 97, "type": "TASK", "confidence": 0.8944367170333862}]}, {"text": "An event schema is a set of actors (also known as slots) that play different roles in an event, such as the perpetrator, victim, and instrument in a bombing event.", "labels": [], "entities": [{"text": "An event schema is a set of actors (also known as slots) that play different roles in an event, such as the perpetrator, victim, and instrument in a bombing event", "start_pos": 0, "end_pos": 162, "type": "Description", "confidence": 0.7802436675344194}]}, {"text": "They provide essential guidance in extracting information related to events from free text, and can also aid in other NLP tasks, such as coreference), summarization, and inference about temporal ordering and causality.", "labels": [], "entities": [{"text": "extracting information related to events from free text", "start_pos": 35, "end_pos": 90, "type": "TASK", "confidence": 0.8201591446995735}, {"text": "summarization", "start_pos": 151, "end_pos": 164, "type": "TASK", "confidence": 0.9941851496696472}]}], "datasetContent": [{"text": "We present experiments to explore two main questions: How well do Rel-grams capture real world knowledge, and what is the quality of event schemas built using Rel-grams.", "labels": [], "entities": []}, {"text": "In our schema evaluation, we are interested in assessing how well the schemas correspond to common-sense knowledge about real world events.", "labels": [], "entities": []}, {"text": "To this end, we focus on three measures, topical coherence, tuple validity, and actor coherence.", "labels": [], "entities": []}, {"text": "A good schema must be topically coherent, i.e., the relations and actors should relate to some real world topic or event.", "labels": [], "entities": []}, {"text": "The tuples that comprise a schema should be valid assertions that make sense in the real world.", "labels": [], "entities": []}, {"text": "Finally, each actor in the schema should belong to a cohesive set that plays a consistent role in the relations.", "labels": [], "entities": []}, {"text": "Since there are no good automated ways to make such judgments, we perform a human evaluation using workers from Amazon's Mechanical Turk (AMT).", "labels": [], "entities": [{"text": "Amazon's Mechanical Turk (AMT)", "start_pos": 112, "end_pos": 142, "type": "DATASET", "confidence": 0.8875161579677037}]}, {"text": "We compare Rel-grams schemas against the stateof-the-art narrative schemas released by  schemas are less expressive than ours -they do not associate types with actors and each schema has a constant pre-specified number of relations.", "labels": [], "entities": []}, {"text": "For a fair comparison we use a similarly expressive version of our schemas that strips off argument types and has the same number of relations per schema (six) as their highest quality output set.", "labels": [], "entities": []}, {"text": "We created two tasks for AMT annotators.", "labels": [], "entities": [{"text": "AMT annotators", "start_pos": 25, "end_pos": 39, "type": "TASK", "confidence": 0.8761337399482727}]}, {"text": "The first task tests the coherence and validity of relations in a schema and the second does the same for the schema actors.", "labels": [], "entities": [{"text": "validity", "start_pos": 39, "end_pos": 47, "type": "METRIC", "confidence": 0.9632685780525208}]}, {"text": "In order to make the tasks understandable to unskilled AMT workers, we followed the accepted practice of presenting them with grounded instances of the schemas (), e.g., instantiating a schema with a specific argument instead of showing the various possibilities for an actor.", "labels": [], "entities": [{"text": "AMT", "start_pos": 55, "end_pos": 58, "type": "TASK", "confidence": 0.9793828725814819}]}, {"text": "First, we collect the information in schemas as a set of tuples: S = {T 1 , T 2 , \u00b7 \u00b7 \u00b7 , T n }, where each tuple is of the form T : (X, Rel, Y ), which conveys a relationship Rel between actors X and Y . Each actor is represented by its highest frequency examples (instances).", "labels": [], "entities": []}, {"text": "shows examples of schemas from Chambers and Rel-grams represented in this format.", "labels": [], "entities": []}, {"text": "Then, we create grounded tuples by randomly sampling from top instances for each actor.", "labels": [], "entities": []}, {"text": "Task I: Topical Coherence To test whether the relations in a schema form a coherent topic or event, we presented the AMT annotators with a schema as a set of grounded tuples, showing each relation in the schema, but randomly selecting one of the top 5 instances from each actor.", "labels": [], "entities": []}, {"text": "We generated five such nchamber/data/schemas/acl09 instantiations for each schema.", "labels": [], "entities": []}, {"text": "An example instantiation is shown in.", "labels": [], "entities": []}, {"text": "We ask three kinds of questions on each grounded schema: (1) is each of the grounded tuples valid (i.e. meaningful in the real world); (2) do the majority of relations form a coherent topic; and (3) does each tuple belong to the common topic.", "labels": [], "entities": []}, {"text": "Similar to previous AMT studies we get judgments from multiple (five) annotators on each task and use the majority labels (.", "labels": [], "entities": [{"text": "AMT", "start_pos": 20, "end_pos": 23, "type": "TASK", "confidence": 0.9701382517814636}]}, {"text": "Our instructions specified that the annotators should ignore grammar and focus on whether a tuple maybe interpreted as areal world statement.", "labels": [], "entities": []}, {"text": "For example, the first tuple in R1 in is a valid statement -\"a bomb exploded in a city\", but the tuples in C1 \"a blast exploded a child\", \"a child detonated a blast\", and \"a child planted a blast\" don't make sense.", "labels": [], "entities": []}, {"text": "Task II: Actor Coherence To test whether the instances of an actor form a coherent set, we held the relation and one actor fixed and presented the AMT annotators with the top 5 instances for the other actor.", "labels": [], "entities": []}, {"text": "The first example R11 in holds the relation \"explode in\" fixed, and A2 is grounded to the randomly selected instance \"city\".", "labels": [], "entities": [{"text": "A2", "start_pos": 68, "end_pos": 70, "type": "METRIC", "confidence": 0.978278636932373}]}, {"text": "We present grounded tuples by varying A1 and ask annotators to judge whether these instances form a coherent topic and whether each instance belongs to that common topic.", "labels": [], "entities": []}, {"text": "As with Task I, we create five random instantiations for each schema.", "labels": [], "entities": []}, {"text": "Fits Role compares the percentage of top instances that fit the specified role for the tested actors.", "labels": [], "entities": []}, {"text": "All differences are statistically significant with a p-value < 0.01.", "labels": [], "entities": []}], "tableCaptions": []}