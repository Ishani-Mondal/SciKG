{"title": [], "abstractContent": [{"text": "We propose a novel approach to sentiment analysis fora low resource setting.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 31, "end_pos": 49, "type": "TASK", "confidence": 0.9732790887355804}]}, {"text": "The intuition behind this work is that sentiment expressed towards an entity, targeted sentiment , maybe viewed as a span of sentiment expressed across the entity.", "labels": [], "entities": []}, {"text": "This representation allows us to model sentiment detection as a sequence tagging problem, jointly discovering people and organizations along with whether there is sentiment directed towards them.", "labels": [], "entities": [{"text": "sentiment detection", "start_pos": 39, "end_pos": 58, "type": "TASK", "confidence": 0.9046939611434937}]}, {"text": "We compare performance in both Spanish and English on microblog data, using only a sentiment lexicon as an external resource.", "labels": [], "entities": []}, {"text": "By leveraging linguistically-informed features within conditional random fields (CRFs) trained to minimize empirical risk, our best models in Spanish significantly outperform a strong baseline, and reach around 90% accuracy on the combined task of named entity recognition and sentiment prediction.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 215, "end_pos": 223, "type": "METRIC", "confidence": 0.9991685152053833}, {"text": "named entity recognition", "start_pos": 248, "end_pos": 272, "type": "TASK", "confidence": 0.636390378077825}, {"text": "sentiment prediction", "start_pos": 277, "end_pos": 297, "type": "TASK", "confidence": 0.9406243562698364}]}, {"text": "Our models in English, trained on a much smaller dataset, are not yet statistically significant against their baselines.", "labels": [], "entities": []}], "introductionContent": [{"text": "Sentiment analysis is a multi-faceted problem.", "labels": [], "entities": [{"text": "Sentiment analysis", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.9426504671573639}]}, {"text": "Determining when a positive or negative sentiment is being expressed is a large part of the challenge, but identifying other attributes, such as the target of the sentiment, is also crucial if the ultimate goal is to pinpoint and extract opinions.", "labels": [], "entities": []}, {"text": "Consider the examples below, all of which contain a positive sentiment: (1) So happy that Kentucky lost to Tennessee!", "labels": [], "entities": []}, {"text": "(2) Kentucky versus Kansas I can hardly wait...", "labels": [], "entities": [{"text": "Kentucky versus Kansas", "start_pos": 4, "end_pos": 26, "type": "DATASET", "confidence": 0.7810868819554647}]}, {"text": "The entities in these examples are college basketball teams, and the events referred to are games.", "labels": [], "entities": []}, {"text": "In (1), although there is a positive sentiment, the target of the sentiment is an event (Kentucky losing to Tennessee).", "labels": [], "entities": []}, {"text": "However, from the positive sentiment toward this event, we can infer that the speaker has a negative sentiment toward Kentucky and a positive sentiment toward Tennessee.", "labels": [], "entities": [{"text": "Kentucky", "start_pos": 118, "end_pos": 126, "type": "DATASET", "confidence": 0.5131691098213196}]}, {"text": "In (2), the positive sentiment is toward a future event, but we are not given enough information to infer a sentiment toward the mentioned entities.", "labels": [], "entities": []}, {"text": "In (3), Kentucky is the direct target of the positive sentiment.", "labels": [], "entities": [{"text": "Kentucky", "start_pos": 8, "end_pos": 16, "type": "DATASET", "confidence": 0.6511988639831543}]}, {"text": "We can also infer a positive sentiment toward Douglas's Syracuse teams, and even toward Douglas himself.", "labels": [], "entities": []}, {"text": "These examples illustrate the importance of the target when interpreting sentiment in context.", "labels": [], "entities": [{"text": "interpreting sentiment in context", "start_pos": 60, "end_pos": 93, "type": "TASK", "confidence": 0.9052586555480957}]}, {"text": "If we are looking for sentiments toward Kentucky, for example, we would want to identify (1) as negative, (2) as neutral (no sentiment) and (3) as positive.", "labels": [], "entities": [{"text": "Kentucky", "start_pos": 40, "end_pos": 48, "type": "DATASET", "confidence": 0.9000316262245178}]}, {"text": "However, if we are looking for sentiment toward Tennessee, we would want to identify (1) as positive, and (2) and (3) as neutral.", "labels": [], "entities": []}, {"text": "The expression of these and other kinds of sentiment can be understood as involving three items: (1) An experiencer Research in sentiment analysis often focuses on (2), predicting overall sentiment polarity.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 128, "end_pos": 146, "type": "TASK", "confidence": 0.9111630916595459}, {"text": "predicting overall sentiment polarity", "start_pos": 169, "end_pos": 206, "type": "TASK", "confidence": 0.7321855947375298}]}, {"text": "Recent work has begun to combine (2) with (3), examining how to automatically predict the sentiment polarity expressed towards a target entity ) fora fixed set of targets.", "labels": [], "entities": []}, {"text": "This topic-dependent sentiment classification requires that the target entity be given, and returns statements expressing sentiment towards the given entity.", "labels": [], "entities": [{"text": "topic-dependent sentiment classification", "start_pos": 5, "end_pos": 45, "type": "TASK", "confidence": 0.7127750118573507}]}, {"text": "In this paper, we take a step towards open-domain, targeted sentiment analysis by investigating how to detect both the named entity and the sentiment expressed toward it.", "labels": [], "entities": [{"text": "open-domain, targeted sentiment analysis", "start_pos": 38, "end_pos": 78, "type": "TASK", "confidence": 0.7619057536125183}]}, {"text": "We observe that sentiment expressed towards a target entity maybe possible to learn in a graphical model along the span of the entity itself: Similar to how named entity recognition (NER) learns labels along the span of each word in an entity name, sentiment maybe expressed along the entity as well.", "labels": [], "entities": [{"text": "named entity recognition (NER)", "start_pos": 157, "end_pos": 187, "type": "TASK", "confidence": 0.8425847788651785}]}, {"text": "A small example is shown in Figure 1.", "labels": [], "entities": []}, {"text": "We focus on people and organizations (volitional named entities), which are the primary targets of sentiment in our microblog data (see).", "labels": [], "entities": []}, {"text": "Both NER and opinion expression extraction have achieved impressive results using conditional random fields (CRFs) () to define the conditional probability of entity categories.", "labels": [], "entities": [{"text": "NER", "start_pos": 5, "end_pos": 8, "type": "TASK", "confidence": 0.974818766117096}, {"text": "opinion expression extraction", "start_pos": 13, "end_pos": 42, "type": "TASK", "confidence": 0.7065476179122925}]}, {"text": "We develop such models to jointly predict the NE and the sentiment expressed towards it using minimum risk training).", "labels": [], "entities": [{"text": "NE", "start_pos": 46, "end_pos": 48, "type": "TASK", "confidence": 0.7953281998634338}]}, {"text": "We learn our models on informal Spanish and English language taken from the social network Twitter, 1 where the language variety makes NLP particularly challenging (see.", "labels": [], "entities": []}, {"text": "Our ultimate goal is to develop models that will be useful for low resource languages, where a sentiment lexicon maybe known or bootstrapped, but more sophisticated linguistic tools may not be readily available.", "labels": [], "entities": []}, {"text": "We therefore do not rely on an external part-of-speech tagger or parser, which are often used for features in fine-grained sentiment analysis; such tools are not available in many languages, and if they are, are not usually adapted for noisy social media.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 123, "end_pos": 141, "type": "TASK", "confidence": 0.7841153740882874}]}, {"text": "Instead, we use information from sentiment lexicons and some simple hand-written features, and otherwise use only features of the word that can be 1 www.twitter.com @[user] le dijo erralo muy por lo bajo jaja un grande juancito grandes amigos mios @[user] he told him it was very on the dl haha a great juancito great friends of mine @[user] buenos d\u00edas Profe!!", "labels": [], "entities": []}, {"text": "Nos quedamos accidentados otra vez en la carretera vieja guarenas echando gasoil, estamos a la interperie @[user] good morning, Prof!!", "labels": [], "entities": []}, {"text": "We were wrecked again on the old guarenas highway while getting diesel, we're out in the open Sin\u00e1nimoSin\u00b4Sin\u00e1nimo de ofender a los Militares, que realmente se merecen ese aumento y m\u00e1s.", "labels": [], "entities": []}, {"text": "Pero, d\u00f3nde queda la misma recompensa para M\u00e9dicos.", "labels": [], "entities": []}, {"text": "I do not intend to offend the military in the slightest, they truly deserve the raise and more.", "labels": [], "entities": []}, {"text": "However, I'm wondering whether doctors will ever receive a similar compensation.: Messages on Twitter use a wide range of formality, style, and errors, which makes extracting information particularly difficult.", "labels": [], "entities": []}, {"text": "Examples from Spanish (screen names anonymized), with approximate translations in English.", "labels": [], "entities": []}, {"text": "These include features based on unsupervised word tags (Brown clusters) and a method that automatically syllabifies a word based on the orthography of the language.", "labels": [], "entities": []}, {"text": "All tools and code used for this research are released with this paper.", "labels": [], "entities": []}], "datasetContent": [{"text": "We are interested in both PERSON and ORGANIZA-TION entities, and evaluate these in the collapsed category VOLITIONAL.", "labels": [], "entities": [{"text": "ORGANIZA-TION", "start_pos": 37, "end_pos": 50, "type": "METRIC", "confidence": 0.9025500416755676}, {"text": "VOLITIONAL", "start_pos": 106, "end_pos": 116, "type": "METRIC", "confidence": 0.842706561088562}]}, {"text": "This suggests that the data maybe pre-processed to label all volitional entities as VOLITIONAL NEs, or the models maybe learned with the traditional named entities in place, and post-SURFACE FEATURES binned word length, message length, and sentence position; Jerboa features; word identity; word lengthening; punctuation characters, has digit; has dash; is lowercase; is 3 or 4 letters; first letter capitalized; more than one letter capitalized, etc.", "labels": [], "entities": [{"text": "FEATURES binned word length", "start_pos": 191, "end_pos": 218, "type": "METRIC", "confidence": 0.7995032668113708}, {"text": "word lengthening", "start_pos": 291, "end_pos": 307, "type": "TASK", "confidence": 0.6558425724506378}]}, {"text": "LINGUISTIC FEATURES function words; can syllabify; curse words; laugh words; words for good, bad, no, my; slang words; abbreviations; intensifiers; subjective suffixes and prefixes (such as diminutive forms); common verb endings; common noun endings BROWN CLUSTERING FEATURES cluster at length 3; cluster at length 5 SENTIMENT FEATURES is sentiment-bearing word; prior sentiment polarity: Features used in model.", "labels": [], "entities": [{"text": "LINGUISTIC FEATURES function words; can syllabify; curse words; laugh words; words for good, bad, no, my; slang words; abbreviations; intensifiers; subjective suffixes and prefixes (such as diminutive forms)", "start_pos": 0, "end_pos": 207, "type": "Description", "confidence": 0.8048591940868192}, {"text": "BROWN CLUSTERING FEATURES cluster", "start_pos": 250, "end_pos": 283, "type": "METRIC", "confidence": 0.8546442687511444}, {"text": "SENTIMENT FEATURES", "start_pos": 317, "end_pos": 335, "type": "METRIC", "confidence": 0.6352892220020294}]}, {"text": "processed to identify those that are VOLITIONAL.", "labels": [], "entities": [{"text": "VOLITIONAL", "start_pos": 37, "end_pos": 47, "type": "METRIC", "confidence": 0.7893141508102417}]}, {"text": "We explored results using both methods, and found that training models on VOLITIONAL tags yielded the best performance overall; we report numbers for this approach below.", "labels": [], "entities": []}, {"text": "We compare against a baseline (BASE-NS) where we use our volitional entity labels and assign no sentiment directed towards the entity (the majority case).", "labels": [], "entities": [{"text": "BASE-NS", "start_pos": 31, "end_pos": 38, "type": "METRIC", "confidence": 0.9406344294548035}]}, {"text": "This is a strong baseline to isolate how our methods perform specifically for the task of identifying sentiment targeted at an entity.", "labels": [], "entities": [{"text": "identifying sentiment targeted at an entity", "start_pos": 90, "end_pos": 133, "type": "TASK", "confidence": 0.8566431701183319}]}, {"text": "We report on precision, recall, and sensitivity for the tasks of NER and targeted subjectivity/sentiment prediction in isolation; and we report on accuracy for the targeted subjectivity and targeted sentiment models.", "labels": [], "entities": [{"text": "precision", "start_pos": 13, "end_pos": 22, "type": "METRIC", "confidence": 0.9995928406715393}, {"text": "recall", "start_pos": 24, "end_pos": 30, "type": "METRIC", "confidence": 0.9988824725151062}, {"text": "sensitivity", "start_pos": 36, "end_pos": 47, "type": "METRIC", "confidence": 0.9919365644454956}, {"text": "NER and targeted subjectivity/sentiment prediction", "start_pos": 65, "end_pos": 115, "type": "TASK", "confidence": 0.6088893243244716}, {"text": "accuracy", "start_pos": 147, "end_pos": 155, "type": "METRIC", "confidence": 0.9992977380752563}]}, {"text": "For sentiment, a true positive is an instance where the label has sentiment, and a true negative is an instance where the label has no sentiment (neutral).", "labels": [], "entities": []}, {"text": "For NER, a true positive is an instance where the label is a B-or I-label; a true negative is an instance where the label is O.", "labels": [], "entities": [{"text": "NER", "start_pos": 4, "end_pos": 7, "type": "TASK", "confidence": 0.8704569339752197}]}, {"text": "The three systems are evaluated against one another for NER, subjectivity (entity has/does not have sentiment expressed towards it), and sentiment (positive/negative/no sentiment) using paired t-tests across folds, with a Bonferroni correction to set \u03b1 to 0.02.", "labels": [], "entities": [{"text": "NER", "start_pos": 56, "end_pos": 59, "type": "TASK", "confidence": 0.8056981563568115}]}, {"text": "NER We include results for the isolated task of volitional named entity recognition in.", "labels": [], "entities": [{"text": "NER", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.9053490161895752}, {"text": "volitional named entity recognition", "start_pos": 48, "end_pos": 83, "type": "TASK", "confidence": 0.6750239282846451}]}, {"text": "In both Spanish and English, all three models are roughly comparable for precision, recall, and specificity.", "labels": [], "entities": [{"text": "precision", "start_pos": 73, "end_pos": 82, "type": "METRIC", "confidence": 0.9996833801269531}, {"text": "recall", "start_pos": 84, "end_pos": 90, "type": "METRIC", "confidence": 0.9994070529937744}]}, {"text": "The task of finding O tags -spans that are not named entities -works especially well (NE spec).", "labels": [], "entities": []}, {"text": "Common: Average precision, recall, and specificity for volitional entity NER (in %).", "labels": [], "entities": [{"text": "precision", "start_pos": 16, "end_pos": 25, "type": "METRIC", "confidence": 0.9985936284065247}, {"text": "recall", "start_pos": 27, "end_pos": 33, "type": "METRIC", "confidence": 0.9995542168617249}, {"text": "volitional entity NER", "start_pos": 55, "end_pos": 76, "type": "TASK", "confidence": 0.4252207080523173}]}, {"text": "mistakes include confusing B-labels with I-labels.", "labels": [], "entities": []}, {"text": "Subjectivity and Sentiment shows results for the isolated task of predicting the presence of sentiment about a volitional entity.", "labels": [], "entities": [{"text": "predicting the presence of sentiment about a volitional entity", "start_pos": 66, "end_pos": 128, "type": "TASK", "confidence": 0.8204587764210172}]}, {"text": "In Spanish, the pipeline models (PIPE) perform optimally for subjectivity recall (Subj rec), and significantly above the COLL models (p<.001).", "labels": [], "entities": [{"text": "recall", "start_pos": 74, "end_pos": 80, "type": "METRIC", "confidence": 0.9202680587768555}]}, {"text": "Precision and specificity are comparable across models.", "labels": [], "entities": [{"text": "Precision", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.9911623001098633}]}, {"text": "In English as in Spanish, the collapsed model is particularly poor at subjectivity recall.", "labels": [], "entities": [{"text": "recall", "start_pos": 83, "end_pos": 89, "type": "METRIC", "confidence": 0.9772193431854248}]}, {"text": "As discussed in Section 2, the subtask of predicting whether subjectivity is expressed towards an entity is comparable to the main task of , and so we compare our approach here.", "labels": [], "entities": [{"text": "predicting whether subjectivity", "start_pos": 42, "end_pos": 73, "type": "TASK", "confidence": 0.8409464756647745}]}, {"text": "The Jiang et al. study is similar to the current study in that they aim to detect targeted sentiment, but it differs from the current study in that they focus exclusively on subjectivity towards five manually selected entities: {Obama, Google, iPad, Lakers, Lady Gaga}.", "labels": [], "entities": []}, {"text": "They also evaluate on artificially balanced evaluation data, and evaluate sentiment polarity (positive/negative) separately from subjectivity (has/does not have sentiment).", "labels": [], "entities": []}, {"text": "Our dataset includes any entity labeled as PERSON or ORGANIZATION, and is not balanced (most targets have no sentiment expressed towards them; see), thus we can only roughly compare against their approach.", "labels": [], "entities": [{"text": "PERSON", "start_pos": 43, "end_pos": 49, "type": "METRIC", "confidence": 0.9434943199157715}, {"text": "ORGANIZATION", "start_pos": 53, "end_pos": 65, "type": "METRIC", "confidence": 0.868471622467041}]}, {"text": "Lakers and Lady Gaga are rare in our collection (appearing less than 3 times), and so we updated the comparison set prior to evaluation to: {Obama, Google, iPad, BBC, Tebow}.", "labels": [], "entities": [{"text": "BBC", "start_pos": 162, "end_pos": 165, "type": "DATASET", "confidence": 0.9496229290962219}, {"text": "Tebow", "start_pos": 167, "end_pos": 172, "type": "DATASET", "confidence": 0.6980189085006714}]}, {"text": "On this set, a baseline that always guesses no sentiment reaches an accuracy of 66.9%, compared to Jiang et al.'s 65.5% accuracy on a balanced set (not strictly comparable, but provided for reference).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 68, "end_pos": 76, "type": "METRIC", "confidence": 0.9993979930877686}, {"text": "accuracy", "start_pos": 120, "end_pos": 128, "type": "METRIC", "confidence": 0.9928930401802063}]}, {"text": "The JOINT models reach an accuracy of 71.04% on this set, demonstrating this approach as potentially useful for topicdependent targeted sentiment.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 26, "end_pos": 34, "type": "METRIC", "confidence": 0.9992828965187073}, {"text": "topicdependent targeted sentiment", "start_pos": 112, "end_pos": 145, "type": "TASK", "confidence": 0.8727517127990723}]}, {"text": "shows results for the task of predicting the polarity of the sentiment expressed about an entity.", "labels": [], "entities": []}, {"text": "In Spanish, the PIPE models significantly out-   perform the COLL models on sentiment recall, and the JOINT models on sentiment precision (p<.01).", "labels": [], "entities": [{"text": "PIPE", "start_pos": 16, "end_pos": 20, "type": "METRIC", "confidence": 0.6569499969482422}, {"text": "COLL", "start_pos": 61, "end_pos": 65, "type": "METRIC", "confidence": 0.8320093154907227}, {"text": "recall", "start_pos": 86, "end_pos": 92, "type": "METRIC", "confidence": 0.7852519154548645}, {"text": "JOINT", "start_pos": 102, "end_pos": 107, "type": "METRIC", "confidence": 0.708424985408783}, {"text": "precision", "start_pos": 128, "end_pos": 137, "type": "METRIC", "confidence": 0.8268648982048035}]}, {"text": "In English, PIPE significantly outperforms JOINT on precision (p<.001).", "labels": [], "entities": [{"text": "PIPE", "start_pos": 12, "end_pos": 16, "type": "METRIC", "confidence": 0.9342208504676819}, {"text": "JOINT", "start_pos": 43, "end_pos": 48, "type": "METRIC", "confidence": 0.5498760938644409}, {"text": "precision", "start_pos": 52, "end_pos": 61, "type": "METRIC", "confidence": 0.9966976642608643}]}], "tableCaptions": [{"text": " Table 1: Distribution of named entities in our Spanish  Twitter corpus. Targeted sentiment percentages are based  on expert annotations from a random sample of 10 (or  all) of of each entity. Most entities are not sentiment tar- gets (NEUTRAL). PERSON and ORGANIZATION are most  frequent, and among the top recipients of sentiment.", "labels": [], "entities": [{"text": "Spanish  Twitter corpus", "start_pos": 48, "end_pos": 71, "type": "DATASET", "confidence": 0.8064425190289816}, {"text": "PERSON", "start_pos": 246, "end_pos": 252, "type": "METRIC", "confidence": 0.8661518692970276}, {"text": "ORGANIZATION", "start_pos": 257, "end_pos": 269, "type": "METRIC", "confidence": 0.95612633228302}]}, {"text": " Table 2: Number of targeted sentiment instances where  at least two of the three annotators (Majority) agreed.", "labels": [], "entities": []}, {"text": " Table 6: Average precision, recall, and specificity for vo- litional entity NER (in %).", "labels": [], "entities": [{"text": "precision", "start_pos": 18, "end_pos": 27, "type": "METRIC", "confidence": 0.9911250472068787}, {"text": "recall", "start_pos": 29, "end_pos": 35, "type": "METRIC", "confidence": 0.9992365837097168}, {"text": "vo- litional entity NER", "start_pos": 57, "end_pos": 80, "type": "TASK", "confidence": 0.5040535688400268}]}, {"text": " Table 7: Average precision, recall, and specificity (in %)  for subjectivity prediction (has/does not have sentiment)  along the target entity.", "labels": [], "entities": [{"text": "precision", "start_pos": 18, "end_pos": 27, "type": "METRIC", "confidence": 0.9946394562721252}, {"text": "recall", "start_pos": 29, "end_pos": 35, "type": "METRIC", "confidence": 0.9992578625679016}, {"text": "subjectivity prediction", "start_pos": 65, "end_pos": 88, "type": "TASK", "confidence": 0.677089512348175}]}, {"text": " Table 8: Average precision, recall, and specificity (in %)  for sentiment prediction (positive/negative/no sentiment)  along the target entity.", "labels": [], "entities": [{"text": "precision", "start_pos": 18, "end_pos": 27, "type": "METRIC", "confidence": 0.9955256581306458}, {"text": "recall", "start_pos": 29, "end_pos": 35, "type": "METRIC", "confidence": 0.9994052648544312}, {"text": "sentiment prediction", "start_pos": 65, "end_pos": 85, "type": "TASK", "confidence": 0.9076245129108429}]}, {"text": " Table 9: Average accuracy on Targeted Subjectivity Pre- diction: Identifying volitional entities and whether they  are a sentiment target. In the core task, Acc-Bsent, the  best model in Spanish is JOINT, significantly outperform- ing the baseline. In English, the best model (PIPE) does  not significantly improve over its baseline.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 18, "end_pos": 26, "type": "METRIC", "confidence": 0.9984226226806641}, {"text": "Acc-Bsent", "start_pos": 158, "end_pos": 167, "type": "METRIC", "confidence": 0.8984513282775879}, {"text": "JOINT", "start_pos": 199, "end_pos": 204, "type": "METRIC", "confidence": 0.7385174632072449}, {"text": "PIPE)", "start_pos": 278, "end_pos": 283, "type": "METRIC", "confidence": 0.9406924843788147}]}, {"text": " Table 10: Average accuracy on Targeted Sentiment Pre- diction: Identifying volitional entities and the polarity  of the sentiment expressed towards them. The Spanish  JOINT models significantly improve over their baseline  for the core task. In English, no models outperform their  baseline.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 19, "end_pos": 27, "type": "METRIC", "confidence": 0.9992021918296814}]}, {"text": " Table 12. Mistakes  are often made by confusing B-labels (the start of", "labels": [], "entities": [{"text": "Mistakes", "start_pos": 11, "end_pos": 19, "type": "TASK", "confidence": 0.9720760583877563}]}, {"text": " Table 12: Predicted vs. observed values for a joint model.  (a) For named entities, most common confusions were  between B-VOLITIONAL and O labels. (b) For sentiment,  most common mistakes were to predict that a positive  sentiment was neutral (no sentiment), and that a neutral  sentiment was negative.", "labels": [], "entities": []}]}