{"title": [{"text": "Anchor Graph: Global Reordering Contexts for Statistical Machine Translation", "labels": [], "entities": [{"text": "Global Reordering Contexts", "start_pos": 14, "end_pos": 40, "type": "TASK", "confidence": 0.7338514029979706}, {"text": "Statistical Machine Translation", "start_pos": 45, "end_pos": 76, "type": "TASK", "confidence": 0.7938921848932902}]}], "abstractContent": [{"text": "Reordering poses one of the greatest challenges in Statistical Machine Translation research as the key contextual information may well be beyond the confine of translation units.", "labels": [], "entities": [{"text": "Statistical Machine Translation research", "start_pos": 51, "end_pos": 91, "type": "TASK", "confidence": 0.8873347193002701}]}, {"text": "We present the \"Anchor Graph\" (AG) model where we use a graph structure to model global contextual information that is crucial for reordering.", "labels": [], "entities": []}, {"text": "The key ingredient of our AG model is the edges that capture the relationship between the reordering around a set of selected translation units, which we refer to as anchors.", "labels": [], "entities": []}, {"text": "As the edges link anchors that may span multiple translation units at decoding time, our AG model effectively encodes global contextual information that is previously absent.", "labels": [], "entities": []}, {"text": "We integrate our proposed model into a state-of-the-art translation system and demonstrate the efficacy of our proposal in a large-scale Chinese-to-English translation task.", "labels": [], "entities": [{"text": "Chinese-to-English translation task", "start_pos": 137, "end_pos": 172, "type": "TASK", "confidence": 0.7407402098178864}]}], "introductionContent": [{"text": "Reordering remains one of the greatest challenges in Statistical Machine Translation (SMT) research as the key contextual information may span across multiple translation units.", "labels": [], "entities": [{"text": "Statistical Machine Translation (SMT)", "start_pos": 53, "end_pos": 90, "type": "TASK", "confidence": 0.8575309614340464}]}, {"text": "Unfortunately, previous approaches fall short in capturing such cross-unit contextual information that could be critical in reordering.", "labels": [], "entities": []}, {"text": "For example, state-of-the-art translation models, such as Hiero () or Moses (, are good at capturing local reordering within the confine of a translation unit, but their formulation is approximately a simple unigram model over derivation (a sequence of the application of translation units) with some aid from target language models.", "labels": [], "entities": []}, {"text": "Moving to a higher order formulation (say to a bigram model) is highly impractical for several reasons: 1) it has to deal with a severe sparsity issue as the size of the unigram model is already huge; and 2) it has to deal with a spurious ambiguity issue which allows multiple derivations of a sentence pair to have radically different model scores.", "labels": [], "entities": []}, {"text": "In this paper, we develop \"Anchor Graph\" (AG) where we use a graph structure to capture global contexts that are crucial for translation.", "labels": [], "entities": [{"text": "translation", "start_pos": 125, "end_pos": 136, "type": "TASK", "confidence": 0.964961588382721}]}, {"text": "To circumvent the sparsity issue, we design our model to rely only on contexts from a set of selected translation units, particularly those that appear frequently with important reordering patterns.", "labels": [], "entities": []}, {"text": "We refer to the units in this special set as anchors where they act as vertices in the graph.", "labels": [], "entities": []}, {"text": "To address the spurious ambiguity issue, we insist on computing the model score for every anchors in the derivation, including those that appear inside larger translation units, as such our AG model gives the same score to the derivations that share the same reordering pattern.", "labels": [], "entities": []}, {"text": "In AG model, the actual reordering is modeled by the edges, or more specifically, by the edges' labels where different reordering around the anchors would correspond to a different label.", "labels": [], "entities": []}, {"text": "As detailed later, we consider two distinct set of labels, namely dominance and precedence, reflecting the two dominant views about reordering in literature, i.e. the first one that views reordering as a linear operation over a sequence and the second one that views reordering as a recursive operation over nodes in a tree structure The former is prevalent in phrase-based context, while the latter in hierarchical phrase-based and syntax-based context.", "labels": [], "entities": []}, {"text": "More concretely, the dominance looks at the anchors' relative positions in the translated sentence, while the precedence looks at the anchors' relative positions in a latent structure, induced via a novel synchronous grammar: Anchorcentric, Lexicalized Synchronous Grammar.", "labels": [], "entities": []}, {"text": "From these two sets of labels, we develop two probabilistic models, namely the dominance and the orientation models.", "labels": [], "entities": []}, {"text": "As the edges of AG link pairs of anchors that may appear in multiple translation units, our AG models are able to capture high order contextual information that is previously absent.", "labels": [], "entities": []}, {"text": "Furthermore, the parameters of these models are estimated in an unsupervised manner without linguistic supervision.", "labels": [], "entities": []}, {"text": "More importantly, our experimental results demonstrate the efficacy of our proposed AGbased models, which we integrate into a state-of-theart syntax-based translation system, in a large scale Chinese-to-English translation task.", "labels": [], "entities": [{"text": "Chinese-to-English translation task", "start_pos": 192, "end_pos": 227, "type": "TASK", "confidence": 0.6991759737332662}]}, {"text": "We would like to emphasize that although we use a syntax-based translation system in our experiments, in principle, our approach is applicable to other translation models as it is agnostic to the translation units.", "labels": [], "entities": []}], "datasetContent": [{"text": "Our baseline systems is a state-of-the-art string-todependency system).", "labels": [], "entities": []}, {"text": "The system is trained on 10 million parallel sentences that are available to the Phase 1 of the DARPA BOLT Chinese-English MT task.", "labels": [], "entities": [{"text": "DARPA BOLT Chinese-English MT task", "start_pos": 96, "end_pos": 130, "type": "TASK", "confidence": 0.6369869768619537}]}, {"text": "The training corpora include a mixed genre of newswire, weblog, broadcast news, broadcast conversation, discussion forums and comes from various sources such as LDC, HK Law, HK Hansard and UN data.", "labels": [], "entities": [{"text": "LDC", "start_pos": 161, "end_pos": 164, "type": "DATASET", "confidence": 0.9400256872177124}, {"text": "HK Law", "start_pos": 166, "end_pos": 172, "type": "DATASET", "confidence": 0.9253550171852112}, {"text": "HK Hansard and UN data", "start_pos": 174, "end_pos": 196, "type": "DATASET", "confidence": 0.7204296827316284}]}, {"text": "In total, our baseline model employs more than 50 features, including from our proposed dominance and orientation models.", "labels": [], "entities": []}, {"text": "In addition to the standard  The NIST MT08 results on newswire (nw), weblog (wb) and combined genres.", "labels": [], "entities": [{"text": "NIST MT08", "start_pos": 33, "end_pos": 42, "type": "DATASET", "confidence": 0.8642319440841675}]}, {"text": "S2D is the baseline stringto-dependency system (line 1).", "labels": [], "entities": []}, {"text": "Lines 2-7 shows the results of the dominance model with O = 1 \u2212 6.", "labels": [], "entities": [{"text": "O", "start_pos": 56, "end_pos": 57, "type": "METRIC", "confidence": 0.995197594165802}]}, {"text": "Line 8 shows result on adding ori to the baseline.", "labels": [], "entities": []}, {"text": "Lines 9-13 shows the results of the orientation complemented with the dominance model with varying O.", "labels": [], "entities": [{"text": "O", "start_pos": 99, "end_pos": 100, "type": "METRIC", "confidence": 0.9844213724136353}]}, {"text": "The best BLEU, TER and Comb on each genre of the first set are in italic while those of the second set are in bold.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 9, "end_pos": 13, "type": "METRIC", "confidence": 0.999529242515564}, {"text": "TER", "start_pos": 15, "end_pos": 18, "type": "METRIC", "confidence": 0.9974135756492615}, {"text": "Comb", "start_pos": 23, "end_pos": 27, "type": "METRIC", "confidence": 0.9513862133026123}]}, {"text": "For BLEU, higher scores are better, while for TER and Comb, lower scores are better.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 4, "end_pos": 8, "type": "METRIC", "confidence": 0.9986525177955627}, {"text": "TER", "start_pos": 46, "end_pos": 49, "type": "METRIC", "confidence": 0.9828572273254395}]}, {"text": "features such as translation probabilities, we incorporate features that are found useful for developing a state-of-the-art baseline, such as the provenance features ).", "labels": [], "entities": [{"text": "translation probabilities", "start_pos": 17, "end_pos": 42, "type": "TASK", "confidence": 0.8732213973999023}]}, {"text": "We use a 6-gram language model, which was trained on 10 billion English words from multiple corpora, including the English side of our parallel corpus plus other corpora such as Gigaword (LDC2011T07) and Google News.", "labels": [], "entities": [{"text": "Gigaword (LDC2011T07)", "start_pos": 178, "end_pos": 199, "type": "DATASET", "confidence": 0.8471829295158386}, {"text": "Google News", "start_pos": 204, "end_pos": 215, "type": "DATASET", "confidence": 0.815407395362854}]}, {"text": "We also train a class-based language model (Chen, 2009) on two million English sentences selected from the parallel corpus.", "labels": [], "entities": []}, {"text": "As for our string-todependency system, we train 3-gram models for left and right dependencies and unigram for head using the target side of the parallel corpus.", "labels": [], "entities": []}, {"text": "To train our models, we select a set of 5 million sentence pairs.", "labels": [], "entities": []}, {"text": "For the tuning and development sets, we set aside 1275 and 1239 sentences selected from LDC2010E30 corpus.", "labels": [], "entities": [{"text": "LDC2010E30 corpus", "start_pos": 88, "end_pos": 105, "type": "DATASET", "confidence": 0.9735154509544373}]}, {"text": "We tune the feature weights with PRO () to minimize (TER-BLEU)/2 metric.", "labels": [], "entities": [{"text": "PRO", "start_pos": 33, "end_pos": 36, "type": "METRIC", "confidence": 0.978080153465271}, {"text": "TER-BLEU)/2 metric", "start_pos": 53, "end_pos": 71, "type": "METRIC", "confidence": 0.9674543291330338}]}, {"text": "As for the blind test set, we report the performance on the NIST MT08 evaluation set, which consists of 691 sentences from newswire and 666 sentences from weblog.", "labels": [], "entities": [{"text": "NIST MT08 evaluation set", "start_pos": 60, "end_pos": 84, "type": "DATASET", "confidence": 0.920654758810997}]}, {"text": "We pick the weights that produce the highest development set scores to decode the test set.", "labels": [], "entities": []}, {"text": "We perform two sets of experiments.", "labels": [], "entities": []}, {"text": "The first set looks at the contribution of the dominance model with varying values of o.", "labels": [], "entities": []}, {"text": "The second one looks at the combination of the dominance model and the orientation model.", "labels": [], "entities": []}, {"text": "summarizes the experimental results on NIST MT08 sets, categorized by genres.", "labels": [], "entities": [{"text": "NIST MT08 sets", "start_pos": 39, "end_pos": 53, "type": "DATASET", "confidence": 0.8475606441497803}]}, {"text": "We report the results on newswire genre in columns a-c, those on weblog genre in column d-f, and those on mixed genre in column g-i.", "labels": [], "entities": []}, {"text": "The performance of our baseline string-to-dependency syntaxbased SMT is shown in the first line.", "labels": [], "entities": [{"text": "SMT", "start_pos": 65, "end_pos": 68, "type": "TASK", "confidence": 0.8438800573348999}]}, {"text": "Lines 2-7 in show the results of our first set of experiments, starting from the result of dom 1 , which looks at only at pairs of adjacent anchors, to the result of dom 6 , which looks at pairs of anchors that are at most 5 anchors away.", "labels": [], "entities": []}, {"text": "As shown inline 2, our dominance model provides a nice improvement of around 0.5 point over the baseline even if it only looks at restricted context.", "labels": [], "entities": []}, {"text": "Increasing the order of our dominance model provides an additional gain.", "labels": [], "entities": []}, {"text": "However, the gain is more pronounced in the weblog genre (up to around 1 BLEU point) than in the newswire genre.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 73, "end_pos": 77, "type": "METRIC", "confidence": 0.996863842010498}]}, {"text": "We conjecture that this maybe the artifact of our tune set, which comes from the weblog genre.", "labels": [], "entities": []}, {"text": "We stop at dom 6 because we observe that the weight of the feature score that corresponds to the maximum order (o = 6) has a negative sign, which often indicates a high correlation between the new features and existing ones.", "labels": [], "entities": []}, {"text": "Lines 8-13 in shows the results of our second set of experiments.", "labels": [], "entities": []}, {"text": "Line 8 shows the result of adding the orientation model (ori) to the baseline system.", "labels": [], "entities": []}, {"text": "As shown, integrating ori shows a significant gain.", "labels": [], "entities": []}, {"text": "On top of which, we then integrate dom 1 to dom 5 . We see a very encouraging result as adding the dominance model increases the performance further, consistently over different value of o.", "labels": [], "entities": []}, {"text": "This suggests that the dominance model is complementary to the orientation model.", "labels": [], "entities": []}, {"text": "Our best result provides more than 1 BP improvement and 1 TER reduction consistently over different genres.", "labels": [], "entities": [{"text": "BP improvement", "start_pos": 37, "end_pos": 51, "type": "METRIC", "confidence": 0.987751692533493}, {"text": "TER reduction", "start_pos": 58, "end_pos": 71, "type": "METRIC", "confidence": 0.9696890413761139}]}, {"text": "We see this result as confirming our intuition that the global contextual information provided by our AG model can significantly improve the performance of SMT even in a state-of-the-art system.", "labels": [], "entities": [{"text": "SMT", "start_pos": 156, "end_pos": 159, "type": "TASK", "confidence": 0.9956632256507874}]}], "tableCaptions": []}