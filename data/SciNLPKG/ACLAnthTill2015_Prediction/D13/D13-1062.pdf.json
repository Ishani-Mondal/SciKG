{"title": [{"text": "Joint Chinese Word Segmentation and POS Tagging on Heterogeneous Annotated Corpora with Multiple Task Learning", "labels": [], "entities": [{"text": "Chinese Word Segmentation", "start_pos": 6, "end_pos": 31, "type": "TASK", "confidence": 0.6013981799284617}, {"text": "POS Tagging", "start_pos": 36, "end_pos": 47, "type": "TASK", "confidence": 0.7780351936817169}]}], "abstractContent": [{"text": "Chinese word segmentation and part-of-speech tagging (S&T) are fundamental steps for more advanced Chinese language processing tasks.", "labels": [], "entities": [{"text": "Chinese word segmentation", "start_pos": 0, "end_pos": 25, "type": "TASK", "confidence": 0.5670036375522614}, {"text": "part-of-speech tagging (S&T)", "start_pos": 30, "end_pos": 58, "type": "TASK", "confidence": 0.8581928738525936}, {"text": "Chinese language processing", "start_pos": 99, "end_pos": 126, "type": "TASK", "confidence": 0.6348526179790497}]}, {"text": "Recently, it has attracted more and more research interests to exploit heterogeneous annotation corpora for Chinese S&T.", "labels": [], "entities": [{"text": "Chinese S&T", "start_pos": 108, "end_pos": 119, "type": "TASK", "confidence": 0.5039040222764015}]}, {"text": "In this paper, we propose a unified model for Chinese S&T with heterogeneous annotation corpora.", "labels": [], "entities": [{"text": "S&T", "start_pos": 54, "end_pos": 57, "type": "TASK", "confidence": 0.6058173576990763}]}, {"text": "We first automatically construct a loose and uncertain mapping between two representative heterogeneous corpora, Penn Chinese Treebank (CTB) and PKU's Peo-ple's Daily (PPD).", "labels": [], "entities": [{"text": "Penn Chinese Treebank (CTB)", "start_pos": 113, "end_pos": 140, "type": "DATASET", "confidence": 0.9723478257656097}, {"text": "PKU's Peo-ple's Daily (PPD)", "start_pos": 145, "end_pos": 172, "type": "DATASET", "confidence": 0.8989862203598022}]}, {"text": "Then we regard the Chinese S&T with heterogeneous corpora as two \"related\" tasks and train our model on two heterogeneous corpora simultaneously.", "labels": [], "entities": []}, {"text": "Experiments show that our method can boost the performances of both of the heterogeneous corpora by using the shared information, and achieves significant improvements over the state-of-the-art methods .", "labels": [], "entities": []}], "introductionContent": [{"text": "Currently, most of statistical natural language processing (NLP) systems rely heavily on manually annotated resources to train their statistical models.", "labels": [], "entities": [{"text": "statistical natural language processing (NLP)", "start_pos": 19, "end_pos": 64, "type": "TASK", "confidence": 0.7538661871637616}]}, {"text": "The more of the data scale, the better the performance will be.", "labels": [], "entities": []}, {"text": "However, the costs are extremely expensive to build the large scale resources for some NLP tasks.", "labels": [], "entities": []}, {"text": "Even worse, the existing resources are often incompatible even fora same task and the annotation guidelines are usually different for different projects, since there are many underlying linguistic theories which explain the same language with different perspectives.", "labels": [], "entities": []}, {"text": "As a result, there often exist multiple heterogeneous annotated corpora fora same task with vastly different and incompatible annotation philosophies.", "labels": [], "entities": []}, {"text": "These heterogeneous resources are waste on some level if we cannot fully exploit them.", "labels": [], "entities": []}, {"text": "However, though most of statistical NLP methods are not bound to specific annotation standards, almost all of them cannot deal simultaneously with the training data with different and incompatible annotation.", "labels": [], "entities": []}, {"text": "The co-existence of heterogeneous annotation data therefore presents anew challenge to utilize these resources.", "labels": [], "entities": []}, {"text": "The problem of incompatible annotation standards is very serious for many tasks in NLP, especially for Chinese word segmentation and part-of-speech (POS) tagging (Chinese S&T).", "labels": [], "entities": [{"text": "Chinese word segmentation", "start_pos": 103, "end_pos": 128, "type": "TASK", "confidence": 0.5991937418778738}, {"text": "part-of-speech (POS) tagging (Chinese S&T)", "start_pos": 133, "end_pos": 175, "type": "TASK", "confidence": 0.6976489424705505}]}, {"text": "In Chinese S&T, the annotation standards are often incompatible for two main reasons.", "labels": [], "entities": [{"text": "S&T", "start_pos": 11, "end_pos": 14, "type": "TASK", "confidence": 0.52873362104098}]}, {"text": "One is that there is no widely accepted segmentation standard due to the lack of a clear definition of Chinese words.", "labels": [], "entities": []}, {"text": "Another is that there are no morphology for Chinese word so that there are many ambiguities to tag the parts-of-speech for Chinese word.", "labels": [], "entities": []}, {"text": "For example, the two commonlyused corpora, PKU's People's Daily (PPD) () and Penn Chinese Treebank (CTB)), use very different segmentation and POS tagging standards.", "labels": [], "entities": [{"text": "PKU's People's Daily (PPD)", "start_pos": 43, "end_pos": 69, "type": "DATASET", "confidence": 0.9632661566138268}, {"text": "Penn Chinese Treebank (CTB))", "start_pos": 77, "end_pos": 105, "type": "DATASET", "confidence": 0.9692615469296774}, {"text": "POS tagging", "start_pos": 143, "end_pos": 154, "type": "TASK", "confidence": 0.7115480601787567}]}, {"text": "For example, in, it is very different to annotate the sentence \"\u5218\u7fd4\u8fdb\u5165\u4e2d\u56fd\u533a\u603b \u51b3\u8d5b (Liu Xiang reaches the national final in China)\" with guidelines of CTB and PDD.", "labels": [], "entities": [{"text": "CTB", "start_pos": 144, "end_pos": 147, "type": "DATASET", "confidence": 0.9534111618995667}, {"text": "PDD", "start_pos": 152, "end_pos": 155, "type": "DATASET", "confidence": 0.825287938117981}]}, {"text": "PDD breaks some phrases, which are single words in: Incompatible word segmentation and POS tagging standards between CTB and PDD CTB, into two words.", "labels": [], "entities": [{"text": "Incompatible word segmentation", "start_pos": 52, "end_pos": 82, "type": "TASK", "confidence": 0.5611778000990549}, {"text": "POS tagging", "start_pos": 87, "end_pos": 98, "type": "TASK", "confidence": 0.6745903342962265}, {"text": "PDD CTB", "start_pos": 125, "end_pos": 132, "type": "DATASET", "confidence": 0.8650590479373932}]}, {"text": "The POS tagsets are also significantly different.", "labels": [], "entities": [{"text": "POS tagsets", "start_pos": 4, "end_pos": 15, "type": "DATASET", "confidence": 0.8445444703102112}]}, {"text": "For example, PDD gives diverse tags \"n\" and \"vn\" for the noun, while CTB just gives \"NN\".", "labels": [], "entities": [{"text": "PDD", "start_pos": 13, "end_pos": 16, "type": "DATASET", "confidence": 0.8467978239059448}, {"text": "CTB", "start_pos": 69, "end_pos": 72, "type": "DATASET", "confidence": 0.9198227524757385}]}, {"text": "For proper names, they maybe tagged as \"nr\", \"ns\", etc in PDD, while they are just tagged as \"NR\" in CTB.", "labels": [], "entities": [{"text": "CTB", "start_pos": 101, "end_pos": 104, "type": "DATASET", "confidence": 0.9678342938423157}]}, {"text": "Recently, it has attracted more and more research interests to exploit heterogeneous annotation data for Chinese word segmentation and POS tagging.", "labels": [], "entities": [{"text": "Chinese word segmentation", "start_pos": 105, "end_pos": 130, "type": "TASK", "confidence": 0.5802231828371683}, {"text": "POS tagging", "start_pos": 135, "end_pos": 146, "type": "TASK", "confidence": 0.7653937041759491}]}, {"text": "() presented a preliminary study for the annotation adaptation topic.", "labels": [], "entities": [{"text": "annotation adaptation", "start_pos": 41, "end_pos": 62, "type": "TASK", "confidence": 0.7642228305339813}]}, {"text": "() proposed a structure-based stacking model to fully utilize heterogeneous word structures.", "labels": [], "entities": []}, {"text": "They also reported that there is no one-to-one mapping between the heterogeneous word classification and the mapping between heterogeneous tags is very uncertain.", "labels": [], "entities": [{"text": "word classification", "start_pos": 81, "end_pos": 100, "type": "TASK", "confidence": 0.711744412779808}]}, {"text": "These methods usually have a two-step process.", "labels": [], "entities": []}, {"text": "The first step is to train the preliminary taggers on heterogeneous annotations.", "labels": [], "entities": []}, {"text": "The second step is to train the final taggers by using the outputs of the preliminary taggers as features.", "labels": [], "entities": []}, {"text": "We call these methods as \"pipelinebased\" methods.", "labels": [], "entities": []}, {"text": "In this paper, we propose a method for joint Chinese word segmentation and POS tagging with heterogeneous annotation corpora.", "labels": [], "entities": [{"text": "Chinese word segmentation", "start_pos": 45, "end_pos": 70, "type": "TASK", "confidence": 0.561960905790329}, {"text": "POS tagging", "start_pos": 75, "end_pos": 86, "type": "TASK", "confidence": 0.7715879678726196}]}, {"text": "We regard the Chinese S&T with heterogeneous corpora as two \"related\" tasks which can improve the performance of each other.", "labels": [], "entities": []}, {"text": "Since it is impossible to establish an exact mapping between two annotations, we first automatically construct a loose and uncertain mapping the heterogeneous tagsets of CTB and PPD.", "labels": [], "entities": [{"text": "CTB", "start_pos": 170, "end_pos": 173, "type": "DATASET", "confidence": 0.9255679845809937}]}, {"text": "Thus we can tag a sentence in one style with the help of the \"related\" information in another heterogeneous style.", "labels": [], "entities": []}, {"text": "The proposed method can improve the performances of joint Chinese S&T on both corpora by using the shared information of each other, which is proven effective by experiments.", "labels": [], "entities": []}, {"text": "There are three main contributions of our model: \u2022 First, we regard these two joint S&T tasks on different corpora as two related tasks which have interdependent and peer relationship.", "labels": [], "entities": []}, {"text": "\u2022 Second, different to the pipeline-based methods, our model can be trained simultaneously on the heterogeneous corpora.", "labels": [], "entities": []}, {"text": "Thus, it can also produce two different styles of POS tags.", "labels": [], "entities": []}, {"text": "\u2022 Third, our model do not depend on the exactly correct mappings between the two heterogeneous tagsets.", "labels": [], "entities": []}, {"text": "The correct mapping relations can be automatically builtin training phase.", "labels": [], "entities": []}, {"text": "The rest of the paper is organized as follows: We first introduce the related works in section 2 and describe the background of character-based method for joint Chinese S&T in section 3.", "labels": [], "entities": [{"text": "S&T", "start_pos": 169, "end_pos": 172, "type": "TASK", "confidence": 0.5616908470789591}]}, {"text": "Section 4 presents an automatic method to build the loose mapping function.", "labels": [], "entities": []}, {"text": "Then we propose our method on heterogeneous corpora in 5 and 6.", "labels": [], "entities": []}, {"text": "The experimental results are given in section 7.", "labels": [], "entities": []}, {"text": "Finally, we conclude our work in section 8.", "labels": [], "entities": []}], "datasetContent": [{"text": "We use the two representative corpora mentioned above, Penn Chinese Treebank (CTB) and PKU's People's Daily (PPD) in our experiments.", "labels": [], "entities": [{"text": "Penn Chinese Treebank (CTB)", "start_pos": 55, "end_pos": 82, "type": "DATASET", "confidence": 0.9682746032873789}, {"text": "PKU's People's Daily (PPD)", "start_pos": 87, "end_pos": 113, "type": "DATASET", "confidence": 0.9432417899370193}]}, {"text": "To better comparison with the previous works, we use two commonly used criterions to partition CTB dataset into the train and test sets.", "labels": [], "entities": [{"text": "CTB dataset", "start_pos": 95, "end_pos": 106, "type": "DATASET", "confidence": 0.9066450893878937}]}, {"text": "\u2022 One is the partition criterion used in) for CTB 5.0.", "labels": [], "entities": [{"text": "CTB 5.0", "start_pos": 46, "end_pos": 53, "type": "DATASET", "confidence": 0.9366320371627808}]}, {"text": "\u2022 Another is the CTB dataset from the POS tagging task of the Fourth International Chinese Language Processing Bakeoff (SIGHAN Bakeoff 2008)(Jin and Chen, 2008).", "labels": [], "entities": [{"text": "CTB dataset", "start_pos": 17, "end_pos": 28, "type": "DATASET", "confidence": 0.8731304705142975}, {"text": "POS tagging task", "start_pos": 38, "end_pos": 54, "type": "TASK", "confidence": 0.8493521610895792}, {"text": "International Chinese Language Processing Bakeoff (SIGHAN Bakeoff 2008)(Jin and Chen, 2008)", "start_pos": 69, "end_pos": 160, "type": "DATASET", "confidence": 0.7087061330676079}]}, {"text": "For the PPD dataset, we use the PKU dataset from SIGHAN Bakeoff 2008.", "labels": [], "entities": [{"text": "PPD dataset", "start_pos": 8, "end_pos": 19, "type": "DATASET", "confidence": 0.7496309876441956}, {"text": "PKU dataset from SIGHAN Bakeoff 2008", "start_pos": 32, "end_pos": 68, "type": "DATASET", "confidence": 0.9381077190240225}]}, {"text": "The details of all datasets are shown in.", "labels": [], "entities": []}, {"text": "Our experiment on these datasets may lead to a fair comparison of our system and the related works.", "labels": [], "entities": []}, {"text": "The experiment results on the heterogeneous corpora CTB-5 + PPD are shown in.", "labels": [], "entities": []}, {"text": "Our method obtains an error reductions of 24.08% and 90.8% over the baseline on CTB-5 and PDD respectively.", "labels": [], "entities": [{"text": "error", "start_pos": 22, "end_pos": 27, "type": "METRIC", "confidence": 0.9826340079307556}, {"text": "CTB-5", "start_pos": 80, "end_pos": 85, "type": "DATASET", "confidence": 0.9218354821205139}, {"text": "PDD", "start_pos": 90, "end_pos": 93, "type": "DATASET", "confidence": 0.8045052886009216}]}, {"text": "Our method also gives better performance than the pipeline-based methods on heterogeneous corpora, such as ( and).", "labels": [], "entities": []}, {"text": "The reason is that our model can utilize the information of both corpora effectively, which can boost the performance of each other.", "labels": [], "entities": []}, {"text": "Although the loose mapping function are bidirectional between two annotation tagsets, we may also use unidirectional mapping.", "labels": [], "entities": []}, {"text": "Therefore, we also evaluate the performance when we use unidirectional mapping.", "labels": [], "entities": []}, {"text": "We just use the mapping function \u03c8 PDD\u2192CTB , which means we obtain the PDD-style output without the information from CTB in tagging stage.", "labels": [], "entities": []}, {"text": "Thus, in training stage, there are no updates for the weights of CTB-features for the instances from PDD corpus, while instances from CTB corpus can result to updates for PDD-features.", "labels": [], "entities": [{"text": "PDD corpus", "start_pos": 101, "end_pos": 111, "type": "DATASET", "confidence": 0.8094286024570465}, {"text": "CTB corpus", "start_pos": 134, "end_pos": 144, "type": "DATASET", "confidence": 0.9272990524768829}]}, {"text": "Surprisedly, we find that the one-way mapping can also improve the performances of both corpora.", "labels": [], "entities": []}, {"text": "The results are shown in   \"ModelS\" is the model which is trained on both CTB-5 and PDD training datasets with just just using the unidirectional mapping function \u03c8 PDD\u2192CTB .: Performances of unidirectional PPD\u2192CTB mapping on CTB-5 and PPD.", "labels": [], "entities": [{"text": "CTB-5 and PDD training datasets", "start_pos": 74, "end_pos": 105, "type": "DATASET", "confidence": 0.6943726241588593}]}, {"text": "shows the experiment results on the heterogeneous corpora CTB-S + PPD.", "labels": [], "entities": []}, {"text": "Our method obtains an error reductions of 7.41% and 10.59% over the baseline on CTB-S and PDD respectively.", "labels": [], "entities": [{"text": "error", "start_pos": 22, "end_pos": 27, "type": "METRIC", "confidence": 0.9825050830841064}, {"text": "CTB-S", "start_pos": 80, "end_pos": 85, "type": "DATASET", "confidence": 0.8917602896690369}]}], "tableCaptions": [{"text": " Table 2: Examples of mapping between CTB and  PDD's tagset", "labels": [], "entities": []}, {"text": " Table 3: Data partitioning for CTB and PD", "labels": [], "entities": [{"text": "Data partitioning", "start_pos": 10, "end_pos": 27, "type": "TASK", "confidence": 0.6641770750284195}, {"text": "CTB", "start_pos": 32, "end_pos": 35, "type": "DATASET", "confidence": 0.8685998320579529}]}, {"text": " Table 5: Performances of different systems on CTB-5 and PPD.", "labels": [], "entities": [{"text": "CTB-5", "start_pos": 47, "end_pos": 52, "type": "DATASET", "confidence": 0.9433104395866394}]}, {"text": " Table 6: Performances of different systems on CTB-S and PPD.", "labels": [], "entities": [{"text": "CTB-S", "start_pos": 47, "end_pos": 52, "type": "DATASET", "confidence": 0.9355267286300659}]}]}