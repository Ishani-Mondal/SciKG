{"title": [{"text": "Dynamic Feature Selection for Dependency Parsing", "labels": [], "entities": [{"text": "Dependency Parsing", "start_pos": 30, "end_pos": 48, "type": "TASK", "confidence": 0.7730759382247925}]}], "abstractContent": [{"text": "Feature computation and exhaustive search have significantly restricted the speed of graph-based dependency parsing.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 97, "end_pos": 115, "type": "TASK", "confidence": 0.6710655093193054}]}, {"text": "We propose a faster framework of dynamic feature selection , where features are added sequentially as needed, edges are pruned early, and decisions are made online for each sentence.", "labels": [], "entities": [{"text": "dynamic feature selection", "start_pos": 33, "end_pos": 58, "type": "TASK", "confidence": 0.6540766954421997}]}, {"text": "We model this as a sequential decision-making problem and solve it by imitation learning techniques.", "labels": [], "entities": []}, {"text": "We test our method on 7 languages.", "labels": [], "entities": []}, {"text": "Our dynamic parser can achieve accuracies comparable or even superior to parsers using a full set of features, while computing fewer than 30% of the feature templates.", "labels": [], "entities": []}], "introductionContent": [{"text": "Graph-based dependency parsing usually consists of two stages.", "labels": [], "entities": [{"text": "Graph-based dependency parsing", "start_pos": 0, "end_pos": 30, "type": "TASK", "confidence": 0.7955270806948344}]}, {"text": "In the scoring stage, we score all possible edges (or other small substructures) using a learned function; in the decoding stage, we use combinatorial optimization to find the dependency tree with the highest total score.", "labels": [], "entities": []}, {"text": "Generally linear edge-scoring functions are used for speed.", "labels": [], "entities": [{"text": "speed", "start_pos": 53, "end_pos": 58, "type": "METRIC", "confidence": 0.9446025490760803}]}, {"text": "But they use a large set of features, derived from feature templates that consider different conjunctions of the edge's attributes.", "labels": [], "entities": []}, {"text": "As a result, parsing time is dominated by the scoring stagecomputing edge attributes, using them to instantiate feature templates, and looking up the weights of the resulting features in a hash table.", "labels": [], "entities": [{"text": "parsing", "start_pos": 13, "end_pos": 20, "type": "TASK", "confidence": 0.9825381636619568}]}, {"text": "For example, used on average about 120 first-order feature templates on each edge, built from attributes such as the edge direction and length, the two words connected by the edge, and the parts of speech of these and nearby words.", "labels": [], "entities": []}, {"text": "We therefore ask the question: can we use fewer features to score the edges, while maintaining the effect that the true dependency tree still gets a higher score?", "labels": [], "entities": []}, {"text": "Motivated by recent progress on dynamic feature selection (, we propose to add features one group at a time to the dependency graph, and to use these features together with interactions among edges (as determined by intermediate parsing results) to make hard decisions on some edges before all their features have been seen.", "labels": [], "entities": [{"text": "dynamic feature selection", "start_pos": 32, "end_pos": 57, "type": "TASK", "confidence": 0.6222859819730123}]}, {"text": "Our approach has a similar flavor to cascaded classifiers () in that we make decisions for each edge at every stage.", "labels": [], "entities": []}, {"text": "However, in place of relatively simple heuristics such as a global relative pruning threshold, we learn a featurized decisionmaking policy of a more complex form.", "labels": [], "entities": []}, {"text": "Since each decision can affect later stages, or later decisions in the same stage, we model this problem as a sequential decision-making process and solve it by Dataset Aggregation (DAgger)), a recent iterative imitation learning technique for structured prediction.", "labels": [], "entities": [{"text": "structured prediction", "start_pos": 244, "end_pos": 265, "type": "TASK", "confidence": 0.7331236898899078}]}, {"text": "Previous work has made much progress on the complementary problem: speeding up the decoding stage by pruning the search space of tree structures.", "labels": [], "entities": []}, {"text": "In and, pruning decisions are made locally as a preprocessing step.", "labels": [], "entities": []}, {"text": "In the recent vine pruning approach), significant speedup is gained by leveraging structured information via a coarse-to-fine projective parsing cas-cade ().", "labels": [], "entities": []}, {"text": "These approaches do not directly tackle the feature selection problem.", "labels": [], "entities": [{"text": "feature selection", "start_pos": 44, "end_pos": 61, "type": "TASK", "confidence": 0.8251112103462219}]}, {"text": "Although pruned edges do not require further feature computation, the pruning step must itself compute similar high-dimensional features just to decide which edges to prune.", "labels": [], "entities": []}, {"text": "For this reason, restrict the pruning models to a smaller feature set for time efficiency.", "labels": [], "entities": []}, {"text": "We aim to do feature selection and edge pruning dynamically, balancing speed and accuracy by using only as many features as needed.", "labels": [], "entities": [{"text": "feature selection", "start_pos": 13, "end_pos": 30, "type": "TASK", "confidence": 0.7666488885879517}, {"text": "accuracy", "start_pos": 81, "end_pos": 89, "type": "METRIC", "confidence": 0.9991889595985413}]}, {"text": "In this paper, we first explore standard static feature selection methods for dependency parsing, and show that even a few feature templates can give decent accuracy (Section 3.2).", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 78, "end_pos": 96, "type": "TASK", "confidence": 0.878501683473587}, {"text": "accuracy", "start_pos": 157, "end_pos": 165, "type": "METRIC", "confidence": 0.9984082579612732}]}, {"text": "We then propose a novel way to dynamically select features for each edge while keeping the overhead of decision making low (Section 4).", "labels": [], "entities": []}, {"text": "Our present experiments use the Maximum Spanning Tree (MST) parsing algorithm ().", "labels": [], "entities": [{"text": "Maximum Spanning Tree (MST) parsing", "start_pos": 32, "end_pos": 67, "type": "TASK", "confidence": 0.5542810601847512}]}, {"text": "However, our approach applies to other graph-based dependency parsers as well-including non-projective parsing, higher-order parsing, or approximations to higher-order parsing that use stacking (, belief propagation (, or structured boosting ().", "labels": [], "entities": [{"text": "belief propagation", "start_pos": 197, "end_pos": 215, "type": "TASK", "confidence": 0.708076149225235}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Comparison of speedup and accuracy with the vine pruning cascade approach for six languages. In the setup,  DYNFS means our dynamic feature selection model, VINEP means the vine pruning cascade model, UAS(D) and  UAS(F) refer to the unlabeled attachment score of the dynamic model (D) and the full-feature model (F) respectively.  For each language, the speedup is relative to its corresponding first-or second-order model using the full set of features.  Results for the vine pruning cascade model are taken from Rush and Petrov (2012). The cost is the percentage of  feature templates used per sentence on edges that are not pruned by the dictionary filter.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 36, "end_pos": 44, "type": "METRIC", "confidence": 0.9995443224906921}, {"text": "VINEP", "start_pos": 167, "end_pos": 172, "type": "METRIC", "confidence": 0.9825673699378967}, {"text": "UAS", "start_pos": 211, "end_pos": 214, "type": "METRIC", "confidence": 0.9891142845153809}, {"text": "UAS(F)", "start_pos": 223, "end_pos": 229, "type": "METRIC", "confidence": 0.9609001129865646}]}]}