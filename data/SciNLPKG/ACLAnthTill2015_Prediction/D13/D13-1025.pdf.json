{"title": [{"text": "Interactive Machine Translation using Hierarchical Translation Models", "labels": [], "entities": [{"text": "Interactive Machine Translation", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.5783460636933645}, {"text": "Hierarchical Translation", "start_pos": 38, "end_pos": 62, "type": "TASK", "confidence": 0.7022911161184311}]}], "abstractContent": [{"text": "Current automatic machine translation systems are notable to generate error-free translations and human intervention is often required to correct their output.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 18, "end_pos": 37, "type": "TASK", "confidence": 0.7028764933347702}]}, {"text": "Alternatively, an interactive framework that integrates the human knowledge into the translation process has been presented in previous works.", "labels": [], "entities": []}, {"text": "Here, we describe anew interactive machine translation approach that is able to work with phrase-based and hierarchical translation models, and integrates error-correction all in a unified statistical framework.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 35, "end_pos": 54, "type": "TASK", "confidence": 0.7269996255636215}]}, {"text": "In our experiments , our approach outperforms previous interactive translation systems, and achieves estimated effort reductions of as much as 48% relative over a traditional post-edition system.", "labels": [], "entities": []}], "introductionContent": [{"text": "Research in the field of machine translation (MT) aims to develop computer systems which are able to translate between languages automatically, without human intervention.", "labels": [], "entities": [{"text": "machine translation (MT)", "start_pos": 25, "end_pos": 49, "type": "TASK", "confidence": 0.8734922409057617}]}, {"text": "However, the quality of the translations produced by any automatic MT system still remain below than that of human translation.", "labels": [], "entities": [{"text": "MT", "start_pos": 67, "end_pos": 69, "type": "TASK", "confidence": 0.9747714400291443}]}, {"text": "Typical solutions to reach human-level quality require a subsequent manual post-editing process.", "labels": [], "entities": []}, {"text": "Such decoupled post-edition solution is rather inefficient and tedious for the human translator.", "labels": [], "entities": []}, {"text": "Moreover, it prevents the MT system from taking advantage of the knowledge of the human translator and, reciprocal, the human translator cannot take advantage of the adapting ability of MT technology.", "labels": [], "entities": [{"text": "MT", "start_pos": 26, "end_pos": 28, "type": "TASK", "confidence": 0.9691638350486755}, {"text": "MT", "start_pos": 186, "end_pos": 188, "type": "TASK", "confidence": 0.9496115446090698}]}, {"text": "An alternative way to take advantage of the existing MT technology is to use them in collaboration with human translators within a computer-assisted translation (CAT) or interactive framework ().", "labels": [], "entities": [{"text": "MT", "start_pos": 53, "end_pos": 55, "type": "TASK", "confidence": 0.9793213605880737}, {"text": "computer-assisted translation (CAT)", "start_pos": 131, "end_pos": 166, "type": "TASK", "confidence": 0.7447688579559326}]}, {"text": "The TransType and TransType2 projects () entailed an interesting focus shift in CAT technology by aiming interaction directly at the production of the target text.", "labels": [], "entities": [{"text": "CAT", "start_pos": 80, "end_pos": 83, "type": "TASK", "confidence": 0.9824299812316895}]}, {"text": "These research projects proposed to embed an MT system within an interactive translation environment.", "labels": [], "entities": [{"text": "MT", "start_pos": 45, "end_pos": 47, "type": "TASK", "confidence": 0.9852011203765869}]}, {"text": "This way, the human translator can ensure a high-quality output while the MT system ensures a significant gain of productivity.", "labels": [], "entities": [{"text": "MT", "start_pos": 74, "end_pos": 76, "type": "TASK", "confidence": 0.8974365592002869}]}, {"text": "Particularly interesting is the interactive machine translation (IMT) approach proposed in (.", "labels": [], "entities": [{"text": "interactive machine translation (IMT)", "start_pos": 32, "end_pos": 69, "type": "TASK", "confidence": 0.8117872873942057}]}, {"text": "In this scenario, a statistical MT (SMT) system uses the source sentence and a previously validated part (prefix 1 ) of its translation to propose a suitable continuation.", "labels": [], "entities": [{"text": "MT (SMT)", "start_pos": 32, "end_pos": 40, "type": "TASK", "confidence": 0.8761922419071198}]}, {"text": "Then the user finds and corrects the next system error, thereby providing a longer prefix which the system uses to suggests anew, hopefully better continuation.", "labels": [], "entities": []}, {"text": "The reported results showed that IMT can save a significant amount of human effort.", "labels": [], "entities": [{"text": "IMT", "start_pos": 33, "end_pos": 36, "type": "TASK", "confidence": 0.952350378036499}]}, {"text": "(2009) provide a thorough description of the IMT approach and describe algorithms for its practical implementation.", "labels": [], "entities": [{"text": "IMT", "start_pos": 45, "end_pos": 48, "type": "TASK", "confidence": 0.9699763655662537}]}, {"text": "Nevertheless, we identify two basic problems for which we think there is room for improvement.", "labels": [], "entities": []}, {"text": "The first problem arises when the system cannot generate the prefix validated by the user.", "labels": [], "entities": []}, {"text": "To solve this problem, the authors simply provide an ad-hoc heuristic errorcorrection technique.", "labels": [], "entities": []}, {"text": "The second problem is how the system deals with word reordering.", "labels": [], "entities": [{"text": "word reordering", "start_pos": 48, "end_pos": 63, "type": "TASK", "confidence": 0.7512848079204559}]}, {"text": "Particularly, the models used by the system were either mono-tonic by nature or non-monotonic but heuristically defined (not estimated from training data).", "labels": [], "entities": []}, {"text": "We work on the foundations of and provide formal solutions to these two challenges.", "labels": [], "entities": []}, {"text": "On the one hand, we adopt the statistical formalization of the IMT framework described in), which includes a stochastic error-correction model in its formalization to address prefix coverage problems.", "labels": [], "entities": [{"text": "IMT framework", "start_pos": 63, "end_pos": 76, "type": "DATASET", "confidence": 0.7021809071302414}]}, {"text": "Moreover, we refine this formalization proposing an alternative error-correction formalization for the IMT framework (Section 2).", "labels": [], "entities": [{"text": "IMT framework", "start_pos": 103, "end_pos": 116, "type": "DATASET", "confidence": 0.7773530185222626}]}, {"text": "Additionally, we also propose a specific error-correction model based on a statistical interpretation of the Levenshtein distance.", "labels": [], "entities": []}, {"text": "These formalizations provide a unified statistical framework for the IMT model in comparison to the ad-hoc heuristic error-correction methods previously used.", "labels": [], "entities": [{"text": "IMT", "start_pos": 69, "end_pos": 72, "type": "TASK", "confidence": 0.8907051086425781}]}, {"text": "In order to address the problem of properly deal with reordering in IMT, we introduce the use of hierarchical MT models).", "labels": [], "entities": [{"text": "IMT", "start_pos": 68, "end_pos": 71, "type": "TASK", "confidence": 0.9040055871009827}, {"text": "MT", "start_pos": 110, "end_pos": 112, "type": "TASK", "confidence": 0.9156461358070374}]}, {"text": "These methods provide a natural approach to handle long range dependencies and allow the incorporation of reordering information into a consistent statistical framework.", "labels": [], "entities": []}, {"text": "Here, we also describe how state-of-the-art hierarchical MT models can be extended to handle IMT (Sections 3 and 4).", "labels": [], "entities": [{"text": "MT", "start_pos": 57, "end_pos": 59, "type": "TASK", "confidence": 0.9524751901626587}, {"text": "IMT", "start_pos": 93, "end_pos": 96, "type": "TASK", "confidence": 0.9274047613143921}]}, {"text": "We evaluate the proposed IMT approach on two different translation task.", "labels": [], "entities": [{"text": "IMT", "start_pos": 25, "end_pos": 28, "type": "TASK", "confidence": 0.983852744102478}]}, {"text": "The comparative results against the IMT approach described by and a conventional post-edition approach show that our IMT formalization for hierarchical SMT models indeed outperform other approaches (Sections 5 and 6).", "labels": [], "entities": [{"text": "IMT", "start_pos": 36, "end_pos": 39, "type": "TASK", "confidence": 0.9131025075912476}, {"text": "IMT formalization", "start_pos": 117, "end_pos": 134, "type": "TASK", "confidence": 0.9381459653377533}, {"text": "SMT", "start_pos": 152, "end_pos": 155, "type": "TASK", "confidence": 0.9003217816352844}]}, {"text": "Moreover, it leads to large reductions in the human effort required to generate error-free translations.", "labels": [], "entities": []}], "datasetContent": [{"text": "The models and search procedure introduced in the previous sections were assessed through a series of For example, in the hypernodes that generate prefixes are those labeled with numbers 1 (\"I saw\"), 4 (\"I saw with a telescope) and 6 (\"I saw a man with a telescope\" and \"I saw with a telescope a man\").", "labels": [], "entities": []}, {"text": "IMT experiments with different corpora.", "labels": [], "entities": [{"text": "IMT", "start_pos": 0, "end_pos": 3, "type": "TASK", "confidence": 0.9305955767631531}]}, {"text": "These corpora, the experimental methodology, and the evaluation measures are presented in this section.", "labels": [], "entities": []}, {"text": "Different measures have been adopted to evaluate the proposed IMT approach.", "labels": [], "entities": [{"text": "IMT", "start_pos": 62, "end_pos": 65, "type": "TASK", "confidence": 0.9904536008834839}]}, {"text": "On the one hand, different IMT systems can be compared according to the effort needed by a human user to generate the desired translations.", "labels": [], "entities": [{"text": "IMT", "start_pos": 27, "end_pos": 30, "type": "TASK", "confidence": 0.9501281380653381}]}, {"text": "This effort is usually estimated as the number of actions performed by the user while interacting with the system.", "labels": [], "entities": []}, {"text": "In the user simulation described above these actions are: looking for the next error and moving the mouse pointer to that position (LCP computation), and correcting errors with some key strokes.", "labels": [], "entities": []}, {"text": "Hence, we implement the following IMT effort measure (: Key-stroke and mouse-action ratio (KSMR): number of key strokes plus mouse movements performed by the user, divided by the total number of characters in the reference.", "labels": [], "entities": [{"text": "mouse-action ratio (KSMR)", "start_pos": 71, "end_pos": 96, "type": "METRIC", "confidence": 0.9170052289962769}]}, {"text": "On the other hand, we also want to compare the proposed IMT approach against a conventional CAT approach without interactivity, such as a decoupled post-edition system.", "labels": [], "entities": [{"text": "IMT", "start_pos": 56, "end_pos": 59, "type": "TASK", "confidence": 0.9341763257980347}]}, {"text": "For such systems, characterlevel user effort is usually measured by the Character Error Rate (CER).", "labels": [], "entities": [{"text": "Character Error Rate (CER)", "start_pos": 72, "end_pos": 98, "type": "METRIC", "confidence": 0.8923974434534708}]}, {"text": "However, it is clear that CER is at a disadvantage due to the auto-completion approach of IMT.", "labels": [], "entities": [{"text": "CER", "start_pos": 26, "end_pos": 29, "type": "METRIC", "confidence": 0.744868814945221}, {"text": "IMT", "start_pos": 90, "end_pos": 93, "type": "TASK", "confidence": 0.9444819092750549}]}, {"text": "To perform a fairer comparison between post-edition and IMT, we implement a postediting system with autocompletion.", "labels": [], "entities": [{"text": "IMT", "start_pos": 56, "end_pos": 59, "type": "DATASET", "confidence": 0.742600679397583}]}, {"text": "Here, when the user enters a character to correct some incorrect word, the system automatically completes the word with the most probable word in the task vocabulary.", "labels": [], "entities": []}, {"text": "To evaluate the effort of a user using such a system, we implement the following measure proposed in (): Post-editing keystroke ratio (PKSR): using a post-edition system with word-autocompleting, number of user key strokes divided by the total number of reference characters.", "labels": [], "entities": [{"text": "Post-editing keystroke ratio (PKSR)", "start_pos": 105, "end_pos": 140, "type": "METRIC", "confidence": 0.7614995787541071}]}, {"text": "The counterpart of PKSR in an IMT scenario is (: Key-stroke ratio (KSR): number of key strokes, divided by the number of reference characters.", "labels": [], "entities": [{"text": "IMT", "start_pos": 30, "end_pos": 33, "type": "TASK", "confidence": 0.9196230173110962}, {"text": "Key-stroke ratio (KSR)", "start_pos": 49, "end_pos": 71, "type": "METRIC", "confidence": 0.9558687448501587}]}, {"text": "PKSR and KSR are fairly comparable and the relative difference between them gives us a good estimate of the reduction inhuman effort that can be achieved by using IMT instead of a conventional post-edition system.", "labels": [], "entities": [{"text": "PKSR", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.8122689127922058}]}, {"text": "We also evaluate the quality of the automatic translations generated by the MT models with the widespread BLEU score ().", "labels": [], "entities": [{"text": "MT", "start_pos": 76, "end_pos": 78, "type": "TASK", "confidence": 0.8893759250640869}, {"text": "BLEU score", "start_pos": 106, "end_pos": 116, "type": "METRIC", "confidence": 0.977019339799881}]}, {"text": "Finally, we provide both confidence intervals for the results and statistical significance of the observed differences in performance.", "labels": [], "entities": []}, {"text": "Confidence intervals were computed by pair-wise re-sampling as in () while statistical significance was computed using the Tukey's HSD (honest significance difference) test.) for the EU and TED tasks using the independent suffix formalization (ISF) and the conditioned suffix formalization (CSF).", "labels": [], "entities": [{"text": "statistical significance", "start_pos": 75, "end_pos": 99, "type": "METRIC", "confidence": 0.6994051337242126}, {"text": "Tukey's HSD (honest significance difference) test.", "start_pos": 123, "end_pos": 173, "type": "METRIC", "confidence": 0.773206545246972}]}, {"text": "PB stands for phrase-based model and HT stands for hierarchical translation model.", "labels": [], "entities": [{"text": "hierarchical translation", "start_pos": 51, "end_pos": 75, "type": "TASK", "confidence": 0.6909792423248291}]}, {"text": "For each task, the best result is displayed boldface, an asterisk * denotes a statistically significant better result (99% confidence) with respect to ISF with PB, and a star denotes a statistically significant difference with respect to all the other systems.", "labels": [], "entities": [{"text": "PB", "start_pos": 160, "end_pos": 162, "type": "METRIC", "confidence": 0.7510055899620056}]}], "tableCaptions": [{"text": " Table 1: Main figures of the processed EU and TED cor- pora. K and M stand for thousands and millions of ele- ments respectively.", "labels": [], "entities": [{"text": "EU and TED cor- pora", "start_pos": 40, "end_pos": 60, "type": "DATASET", "confidence": 0.6762372056643168}]}, {"text": " Table 2: BLEU score of the word-graphs (WG) and hy- pergraphs (HG) used to implement the IMT procedures.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9732618033885956}, {"text": "hy- pergraphs (HG)", "start_pos": 49, "end_pos": 67, "type": "METRIC", "confidence": 0.7424217412869135}, {"text": "IMT", "start_pos": 90, "end_pos": 93, "type": "TASK", "confidence": 0.9270466566085815}]}, {"text": " Table 3: IMT results (KSMR", "labels": [], "entities": [{"text": "IMT", "start_pos": 10, "end_pos": 13, "type": "TASK", "confidence": 0.5665332674980164}, {"text": "KSMR", "start_pos": 23, "end_pos": 27, "type": "DATASET", "confidence": 0.5779346227645874}]}]}