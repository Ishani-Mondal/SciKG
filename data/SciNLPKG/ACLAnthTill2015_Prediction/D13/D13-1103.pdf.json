{"title": [{"text": "The Answer is at your Fingertips: Improving Passage Retrieval for Web Question Answering with Search Behavior Data", "labels": [], "entities": [{"text": "Improving Passage Retrieval", "start_pos": 34, "end_pos": 61, "type": "TASK", "confidence": 0.7589556773503622}, {"text": "Web Question Answering", "start_pos": 66, "end_pos": 88, "type": "TASK", "confidence": 0.5425639649232229}]}], "abstractContent": [{"text": "Passage retrieval is a crucial first step of automatic Question Answering (QA).", "labels": [], "entities": [{"text": "Passage retrieval", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.8765947818756104}, {"text": "Question Answering (QA)", "start_pos": 55, "end_pos": 78, "type": "TASK", "confidence": 0.8358483493328095}]}, {"text": "While existing passage retrieval algorithms are effective at selecting document passages most similar to the question, or those that contain the expected answer types, they do not take into account which parts of the document the searchers actually found useful.", "labels": [], "entities": [{"text": "passage retrieval", "start_pos": 15, "end_pos": 32, "type": "TASK", "confidence": 0.8192704319953918}]}, {"text": "We propose, to the best of our knowledge, the first successful attempt to incorporate searcher examination data into passage retrieval for question answering.", "labels": [], "entities": [{"text": "passage retrieval", "start_pos": 117, "end_pos": 134, "type": "TASK", "confidence": 0.8707193732261658}, {"text": "question answering", "start_pos": 139, "end_pos": 157, "type": "TASK", "confidence": 0.9004070460796356}]}, {"text": "Specifically, we exploit detailed examination data, such as mouse cursor movements and scrolling, to infer the parts of the document the searcher found interesting, and then incorporate this signal into passage retrieval for QA.", "labels": [], "entities": []}, {"text": "Our extensive experiments and analysis demonstrate that our method significantly improves passage retrieval, compared to using textual features alone.", "labels": [], "entities": [{"text": "passage retrieval", "start_pos": 90, "end_pos": 107, "type": "TASK", "confidence": 0.9427000880241394}]}, {"text": "As an additional contribution, we make available to the research community the code and the search behavior data used in this study, with the hope of encouraging further research in this area.", "labels": [], "entities": []}], "introductionContent": [{"text": "Automated Question Answering (QA), is an attractive variation of search where the QA system automatically returns an answer to a user's question, instead of a list of document results.", "labels": [], "entities": [{"text": "Automated Question Answering (QA)", "start_pos": 0, "end_pos": 33, "type": "TASK", "confidence": 0.7569139649470648}]}, {"text": "Passage retrieval is a first critical step of QA system, where candidate passages are identified and scored as likely to contain an answer.", "labels": [], "entities": [{"text": "Passage retrieval", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.8517703711986542}, {"text": "QA", "start_pos": 46, "end_pos": 48, "type": "TASK", "confidence": 0.9211482405662537}]}, {"text": "While significant progress has been made recently on incorporating syntactic and semantic analysis for improving the QA system performance, this analysis is typically applied only on the (limited) set of candidate passages retrieved.", "labels": [], "entities": []}, {"text": "The main reason is that it is generally not practical to perform deep analysis on all documents in a large collection, and not yet feasible for the Web at large.", "labels": [], "entities": []}, {"text": "* Work done at Emory University.", "labels": [], "entities": [{"text": "Emory University", "start_pos": 15, "end_pos": 31, "type": "DATASET", "confidence": 0.9668899476528168}]}, {"text": "In the web search setting, automated question answering presents additional challenges and opportunities.", "labels": [], "entities": [{"text": "question answering", "start_pos": 37, "end_pos": 55, "type": "TASK", "confidence": 0.7459924221038818}]}, {"text": "On the downside, the questions and queries from real users are often not grammatical or well-formed, differing from the questions used in the traditional TREC Question Answering evaluations ().", "labels": [], "entities": [{"text": "TREC Question Answering evaluations", "start_pos": 154, "end_pos": 189, "type": "TASK", "confidence": 0.7604194581508636}]}, {"text": "On the upside, by interacting with a search engine, the millions of searchers implicitly provide additional clues about usefulness of documents, result ranking, and other aspects of the search process.", "labels": [], "entities": []}, {"text": "In this paper, we explore making use of the search behavior data to improve passage retrieval for automated Question Answering on the web.", "labels": [], "entities": [{"text": "passage retrieval", "start_pos": 76, "end_pos": 93, "type": "TASK", "confidence": 0.8878960907459259}, {"text": "Question Answering", "start_pos": 108, "end_pos": 126, "type": "TASK", "confidence": 0.6989598274230957}]}, {"text": "Our basic observation is that when a user is attempting to answer a question, he or she will more carefully examine the parts of the document that contain an answer.", "labels": [], "entities": []}, {"text": "This observation is intuitive, and is strongly supported by numerous eye tracking studies (e.g., and).", "labels": [], "entities": []}, {"text": "Based on this, we hypothesize that the passages containing the answers can be automatically identified from the naturalistic searcher behavior, and this prediction can be subsequently used to improve passage ranking.", "labels": [], "entities": []}, {"text": "To the best of our knowledge, our work is the first to successfully incorporate searcher examination into passage ranking for Question Answering.", "labels": [], "entities": [{"text": "Question Answering", "start_pos": 126, "end_pos": 144, "type": "TASK", "confidence": 0.7847196459770203}]}, {"text": "Our approach is primarily aimed at recurring (repeated) questions, which comprise a large fraction of the search volume (while the exact statistics vary, over 50% of search queries are submitted by multiple users).", "labels": [], "entities": []}, {"text": "For such questions, a system would track the clicked result URLs, as well as the user interactions on the landing pages.", "labels": [], "entities": []}, {"text": "Then the system would use this information to present the improved results to new users who ask the same (or similar) question.", "labels": [], "entities": []}, {"text": "Intuitively, our method uses the same general idea of result click data mining, used by the major search engines to improve result ranking, but takes it a step further to exploit user interactions on the actual landing pages.", "labels": [], "entities": [{"text": "result click data mining", "start_pos": 54, "end_pos": 78, "type": "TASK", "confidence": 0.5504330098628998}]}, {"text": "A key point to emphasize is that our approach exploits the natural browsing behavior of the users, not requiring any additional effort from the searchers.", "labels": [], "entities": []}, {"text": "Specifically, our contributions include: \u2022 A novel approach to passage retrieval for question answering, that naturally integrates textual and behavioral evidence.", "labels": [], "entities": [{"text": "passage retrieval", "start_pos": 63, "end_pos": 80, "type": "TASK", "confidence": 0.9244875609874725}, {"text": "question answering", "start_pos": 85, "end_pos": 103, "type": "TASK", "confidence": 0.7987735271453857}]}, {"text": "\u2022 A robust infrastructure for connecting finegrained searcher behavior to precise page contents.", "labels": [], "entities": []}, {"text": "\u2022 Thorough experiments over hundreds of search sessions and thousands of page views, demonstrating significant improvements to passage retrieval by harnessing the user's page examination data.", "labels": [], "entities": [{"text": "passage retrieval", "start_pos": 127, "end_pos": 144, "type": "TASK", "confidence": 0.9489048719406128}]}, {"text": "Next we describe related work, to place our contribution in context.", "labels": [], "entities": []}], "datasetContent": [{"text": "This section presents the methodology used for selecting the questions (Section 5.1), the corresponding search behavior data, and the experimental collections and metrics (Section 5.3).", "labels": [], "entities": []}, {"text": "The search behavior data for each of the questions above was acquired as described in Section 3.3.", "labels": [], "entities": []}, {"text": "A total of 270 participants finished the game.", "labels": [], "entities": []}, {"text": "After filtering out users who did not follow the game rules, we have 3047 search sessions performed by 265 users.", "labels": [], "entities": []}, {"text": "Our data for these users consists of 7800 queries, 3910 unique queries, 8574 SERP clicks on 1544 distinct URLs.", "labels": [], "entities": [{"text": "SERP", "start_pos": 77, "end_pos": 81, "type": "METRIC", "confidence": 0.9603792428970337}]}, {"text": "For 5683 page visits (66%) and 883 distinct URLs the on-page behavioral data is collected.", "labels": [], "entities": []}, {"text": "For the rest 34% of page visits the behavioral data were not collected due to conflicts between our JavaScript tracking code and other code presented on the page.", "labels": [], "entities": []}, {"text": "For each page view there are about 400 atomic browsing events (mouse movements, scrolling, key pressing) on average.", "labels": [], "entities": []}, {"text": "All the source and derived data are available at http:// ir.mathcs.emory.edu/intent.", "labels": [], "entities": []}, {"text": "The dataset is divided into training, validation, and test set in the following way.", "labels": [], "entities": []}, {"text": "The behavior dataset for the first game is divided randomly into equal-sized training and validation sets that are disjoint by URLs.", "labels": [], "entities": []}, {"text": "The training set was used to train the regression algorithm for predicting passage attractiveness, and the validation set was used to explore the influence of behavior weight \u03bb on passage retrieval performance, and to select the parameter \u03bb for using on a test set.", "labels": [], "entities": []}, {"text": "The validation set consists of 254 different URLs spread over 11 questions, and for each of them there is a collected browsing behavior.", "labels": [], "entities": []}, {"text": "The test set consists of 441 URLs spread over 24 questions, and the test set has no intersection with training and validation set by URLs, questions, and users.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Feature importance for behavioral features, as  measured by Gini coefficient", "labels": [], "entities": [{"text": "Feature importance", "start_pos": 10, "end_pos": 28, "type": "METRIC", "confidence": 0.9602844715118408}, {"text": "Gini coefficient", "start_pos": 70, "end_pos": 86, "type": "METRIC", "confidence": 0.8975937366485596}]}, {"text": " Table 3: Correlation of passage interestingness BScore  with linguistic properties of a sentence", "labels": [], "entities": []}]}