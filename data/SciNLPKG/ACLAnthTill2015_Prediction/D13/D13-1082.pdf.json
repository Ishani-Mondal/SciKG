{"title": [{"text": "Converting Continuous-Space Language Models into N-gram Language Models for Statistical Machine Translation", "labels": [], "entities": [{"text": "Statistical Machine Translation", "start_pos": 76, "end_pos": 107, "type": "TASK", "confidence": 0.7801198164621989}]}], "abstractContent": [{"text": "Neural network language models, or continuous-space language models (CSLMs), have been shown to improve the performance of statistical machine translation (SMT) when they are used for reranking n-best translations.", "labels": [], "entities": [{"text": "statistical machine translation (SMT)", "start_pos": 123, "end_pos": 160, "type": "TASK", "confidence": 0.7732620437939962}]}, {"text": "However, CSLMs have not been used in the first pass decoding of SMT, because using CSLMs in decoding takes a lot of time.", "labels": [], "entities": [{"text": "SMT", "start_pos": 64, "end_pos": 67, "type": "TASK", "confidence": 0.9689794182777405}]}, {"text": "In contrast, we propose a method for converting CSLMs into back-off n-gram language models (BNLMs) so that we can use converted CSLMs in decoding.", "labels": [], "entities": []}, {"text": "We show that they outperform the original BNLMs and are comparable with the traditional use of CSLMs in reranking.", "labels": [], "entities": [{"text": "BNLMs", "start_pos": 42, "end_pos": 47, "type": "DATASET", "confidence": 0.9202683568000793}]}], "introductionContent": [{"text": "Language models are important in natural language processing tasks such as speech recognition and statistical machine translation.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 75, "end_pos": 93, "type": "TASK", "confidence": 0.7725622951984406}, {"text": "statistical machine translation", "start_pos": 98, "end_pos": 129, "type": "TASK", "confidence": 0.7114049593607584}]}, {"text": "Traditionally, backoff n-gram language models (BNLMs)) are being widely used for these tasks.", "labels": [], "entities": []}, {"text": "Recently, neural network language models, or continuous-space language models (CSLMs) ( are being used in statistical machine translation (SMT) (.", "labels": [], "entities": [{"text": "statistical machine translation (SMT)", "start_pos": 106, "end_pos": 143, "type": "TASK", "confidence": 0.8167593330144882}]}, {"text": "These works have shown that CSLMs can improve the BLEU () scores of SMT when compared with BNLMs, on the condition that the training data for language modeling are the same size.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 50, "end_pos": 54, "type": "METRIC", "confidence": 0.9991161227226257}, {"text": "SMT", "start_pos": 68, "end_pos": 71, "type": "TASK", "confidence": 0.9914284348487854}]}, {"text": "However, in practice, CSLMs have not been widely used in SMT.", "labels": [], "entities": [{"text": "SMT", "start_pos": 57, "end_pos": 60, "type": "TASK", "confidence": 0.9964936375617981}]}, {"text": "One reason is that the computational costs of training and using CSLMs are very high.", "labels": [], "entities": []}, {"text": "Various methods have been proposed to tackle the training cost issues ().", "labels": [], "entities": []}, {"text": "However, there has been little work on reducing using costs.", "labels": [], "entities": []}, {"text": "Since the using costs of CSLMs are very high, it is difficult to use CSLMs in decoding directly.", "labels": [], "entities": []}, {"text": "A common approach in SMT using CSLMs is the two pass approach, or n-best reranking.", "labels": [], "entities": [{"text": "SMT", "start_pos": 21, "end_pos": 24, "type": "TASK", "confidence": 0.9971434473991394}]}, {"text": "In this approach, the first pass uses a BNLM in decoding to produce an n-best list.", "labels": [], "entities": [{"text": "BNLM", "start_pos": 40, "end_pos": 44, "type": "METRIC", "confidence": 0.9055189490318298}]}, {"text": "Then, a CSLM is used to rerank those n-best translations in the second pass.", "labels": [], "entities": []}, {"text": "( Another approach is using restricted Boltzmann machines (RBMs)) instead of using multi-layer neural networks ().", "labels": [], "entities": []}, {"text": "Since probability in a RBM can be calculated very efficiently, they can use the RBM language model in SMT decoding.", "labels": [], "entities": [{"text": "SMT decoding", "start_pos": 102, "end_pos": 114, "type": "TASK", "confidence": 0.9139952957630157}]}, {"text": "However, the RBM was just used in an adaptation of SMT, not in a large SMT task, because the training costs of RBMs are very high.", "labels": [], "entities": [{"text": "SMT", "start_pos": 51, "end_pos": 54, "type": "TASK", "confidence": 0.976262629032135}, {"text": "SMT task", "start_pos": 71, "end_pos": 79, "type": "TASK", "confidence": 0.9283280968666077}]}, {"text": "The last approach is using a BNLM to simulate a CSLM.", "labels": [], "entities": [{"text": "BNLM", "start_pos": 29, "end_pos": 33, "type": "DATASET", "confidence": 0.6422325372695923}]}, {"text": ") used a recurrent neural network language model (RNNLM) to generate a large amount of text, which was generated by sampling words from the probability distributions calculated by the RNNLM.", "labels": [], "entities": []}, {"text": "Then, they trained the BNLM from the text using the interpolated Kneser-Ney smoothing method.", "labels": [], "entities": [{"text": "BNLM", "start_pos": 23, "end_pos": 27, "type": "DATASET", "confidence": 0.8189908266067505}]}, {"text": "() converted neural network language models of increasing order to pruned back-off language models, using lowerorder models to constrain the n-grams allowed in higher-order models.", "labels": [], "entities": []}, {"text": "Both of these methods were used in decoding for speech recognition.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 48, "end_pos": 66, "type": "TASK", "confidence": 0.8534147143363953}]}, {"text": "These methods were applied to not-so-large scale experiments (55 million (M) words for training their BNLMs) (.", "labels": [], "entities": [{"text": "BNLMs", "start_pos": 102, "end_pos": 107, "type": "DATASET", "confidence": 0.8741027116775513}]}, {"text": "In contrast, our method is applied to SMT and can be used to improve a BNLM created from 746 M words by using a CSLM trained from 42 M words.", "labels": [], "entities": [{"text": "SMT", "start_pos": 38, "end_pos": 41, "type": "TASK", "confidence": 0.9843595623970032}]}, {"text": "Because BNLMs can be trained from much larger corpora than those that can be used for training CSLMs, improving a BNLM by using a CSLM trained from a smaller corpus is very important.", "labels": [], "entities": []}, {"text": "Actually, a CSLM trained from a smaller corpus can improve the BLEU scores of SMT if it is used in the n-best reranking.", "labels": [], "entities": [{"text": "BLEU scores", "start_pos": 63, "end_pos": 74, "type": "METRIC", "confidence": 0.9754694104194641}, {"text": "SMT", "start_pos": 78, "end_pos": 81, "type": "TASK", "confidence": 0.9610496163368225}]}, {"text": "In contrast, we will demonstrate that a BNLM simulating a CSLM can improve the BLEU scores of SMT in the first pass decoding.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 79, "end_pos": 83, "type": "METRIC", "confidence": 0.9990399479866028}, {"text": "SMT", "start_pos": 94, "end_pos": 97, "type": "TASK", "confidence": 0.9672643542289734}]}, {"text": "Our approach is as follows: First, we train a CSLM) from a corpus.", "labels": [], "entities": []}, {"text": "(2) Second, we also train a BNLM from the same corpus or larger corpus.", "labels": [], "entities": [{"text": "BNLM", "start_pos": 28, "end_pos": 32, "type": "DATASET", "confidence": 0.8266737461090088}]}, {"text": "(3) Finally, we rewrite the probability of each n-gram of the BNLM with that probability calculated from the CSLM.", "labels": [], "entities": [{"text": "BNLM", "start_pos": 62, "end_pos": 66, "type": "DATASET", "confidence": 0.9139644503593445}]}, {"text": "We also re-normalize the probabilities of the BNLM, then use the re-written BNLM in SMT decoding.", "labels": [], "entities": [{"text": "BNLM", "start_pos": 46, "end_pos": 50, "type": "DATASET", "confidence": 0.8445194959640503}, {"text": "BNLM", "start_pos": 76, "end_pos": 80, "type": "DATASET", "confidence": 0.8882407546043396}, {"text": "SMT decoding", "start_pos": 84, "end_pos": 96, "type": "TASK", "confidence": 0.8974956274032593}]}, {"text": "In Section 2, we describe the BNLM and CSLM used for re-writing BNLMs.", "labels": [], "entities": [{"text": "BNLM", "start_pos": 30, "end_pos": 34, "type": "DATASET", "confidence": 0.8701676726341248}, {"text": "BNLMs", "start_pos": 64, "end_pos": 69, "type": "DATASET", "confidence": 0.8677499294281006}]}, {"text": "In Section 3, we describe the method of converting a CSLM into a BNLM.", "labels": [], "entities": [{"text": "BNLM", "start_pos": 65, "end_pos": 69, "type": "DATASET", "confidence": 0.896040678024292}]}, {"text": "In Sections 4 and 5, we evaluate our method and conclude.", "labels": [], "entities": []}], "datasetContent": [{"text": "First, as shown in the tables, the reranking by applying CSLM42 increased the BLEU scores for all language models.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 78, "end_pos": 82, "type": "METRIC", "confidence": 0.9993865489959717}]}, {"text": "This observation is in accordance with those of previous work.", "labels": [], "entities": []}, {"text": "Second, the reranking results of BNLM42 (32.44) were not better than those of the first pass of BNLM746 (32.83).", "labels": [], "entities": [{"text": "reranking", "start_pos": 12, "end_pos": 21, "type": "METRIC", "confidence": 0.9955860376358032}, {"text": "BNLM42", "start_pos": 33, "end_pos": 39, "type": "DATASET", "confidence": 0.8951177597045898}, {"text": "BNLM746", "start_pos": 96, "end_pos": 103, "type": "DATASET", "confidence": 0.9796199202537537}]}, {"text": "This indicates that if the underlying BNLM is made from a small corpus, the reranking using CSLM cannot compensate for it.", "labels": [], "entities": []}, {"text": "Third, CONV42 was better than BNLM42 for both first-pass and reranking.", "labels": [], "entities": [{"text": "CONV42", "start_pos": 7, "end_pos": 13, "type": "METRIC", "confidence": 0.6653904914855957}, {"text": "BNLM42", "start_pos": 30, "end_pos": 36, "type": "DATASET", "confidence": 0.5818389058113098}]}, {"text": "This also holds in the case of CONV746 and BNLM746.", "labels": [], "entities": [{"text": "CONV746", "start_pos": 31, "end_pos": 38, "type": "DATASET", "confidence": 0.9184507131576538}, {"text": "BNLM746", "start_pos": 43, "end_pos": 50, "type": "DATASET", "confidence": 0.9481913447380066}]}, {"text": "This indicated that our conversion method improved the BNLMs, even if the underlying BNLM was trained on a larger corpus than that used for training the CSLM.", "labels": [], "entities": [{"text": "BNLMs", "start_pos": 55, "end_pos": 60, "type": "DATASET", "confidence": 0.7132049202919006}, {"text": "BNLM", "start_pos": 85, "end_pos": 89, "type": "DATASET", "confidence": 0.8845961689949036}]}, {"text": "As described in the introduction, this is very important because BNLMs can be trained from much larger corpora than those that can be used for training CSLMs.", "labels": [], "entities": [{"text": "BNLMs", "start_pos": 65, "end_pos": 70, "type": "DATASET", "confidence": 0.5901594161987305}]}, {"text": "This observation has not been found in the previous work.", "labels": [], "entities": []}, {"text": "In addition, the first-pass of CONV42 and CONV746 were comparable with those of the reranking results of BNLM42 and BNLM746, respectively.", "labels": [], "entities": [{"text": "CONV42", "start_pos": 31, "end_pos": 37, "type": "DATASET", "confidence": 0.7602997422218323}, {"text": "BNLM42", "start_pos": 105, "end_pos": 111, "type": "DATASET", "confidence": 0.9698740243911743}, {"text": "BNLM746", "start_pos": 116, "end_pos": 123, "type": "DATASET", "confidence": 0.9087123274803162}]}, {"text": "That is, there were no significant differences between these results.", "labels": [], "entities": []}, {"text": "This indicates that our conversion method preserves the performance of the reranking using CSLM.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Comparison of BLEU scores", "labels": [], "entities": [{"text": "BLEU", "start_pos": 24, "end_pos": 28, "type": "METRIC", "confidence": 0.9950218200683594}]}]}