{"title": [{"text": "Image Description using Visual Dependency Representations", "labels": [], "entities": [{"text": "Image Description", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.7075559794902802}]}], "abstractContent": [{"text": "Describing the main event of an image involves identifying the objects depicted and predicting the relationships between them.", "labels": [], "entities": []}, {"text": "Previous approaches have represented images as unstructured bags of regions, which makes it difficult to accurately predict meaningful relationships between regions.", "labels": [], "entities": []}, {"text": "In this paper , we introduce visual dependency representations to capture the relationships between the objects in an image, and hypothesize that this representation can improve image description.", "labels": [], "entities": [{"text": "image description", "start_pos": 178, "end_pos": 195, "type": "TASK", "confidence": 0.7830477654933929}]}, {"text": "We test this hypothesis using anew data set of region-annotated images, associated with visual dependency representations and gold-standard descriptions.", "labels": [], "entities": []}, {"text": "We describe two template-based description generation models that operate over visual dependency representations.", "labels": [], "entities": []}, {"text": "In an image description task, we find that these models outper-form approaches that rely on object proximity or corpus information to generate descriptions on both automatic measures and on human judgements.", "labels": [], "entities": [{"text": "image description task", "start_pos": 6, "end_pos": 28, "type": "TASK", "confidence": 0.7771197656790415}]}], "introductionContent": [{"text": "Humans are readily able to produce a description of an image that correctly identifies the objects and actions depicted.", "labels": [], "entities": []}, {"text": "Automating this process is useful for applications such as image retrieval, where users can go beyond keyword-search to describe their information needs, caption generation for improving the accessibility of existing image collections, story illustration, and in assistive technology for blind and partially sighted people.", "labels": [], "entities": [{"text": "image retrieval", "start_pos": 59, "end_pos": 74, "type": "TASK", "confidence": 0.7987574934959412}, {"text": "caption generation", "start_pos": 154, "end_pos": 172, "type": "TASK", "confidence": 0.9325214922428131}, {"text": "story illustration", "start_pos": 236, "end_pos": 254, "type": "TASK", "confidence": 0.8085525631904602}]}, {"text": "Automatic image description presents challenges on a number of levels: recognizing the objects in an image and their attributes are difficult computer vision problems; while determining how the objects interact, which relationships hold between them, and which events are depicted requires considerable background knowledge.", "labels": [], "entities": [{"text": "Automatic image description", "start_pos": 0, "end_pos": 27, "type": "TASK", "confidence": 0.6660865644613901}]}, {"text": "Previous approaches to automatic description generation have typically tackled the problem using an object recognition system in conjunction with a natural language generation component based on language models or templates ( ).", "labels": [], "entities": [{"text": "automatic description generation", "start_pos": 23, "end_pos": 55, "type": "TASK", "confidence": 0.6770646770795187}]}, {"text": "Some approaches have utilised the visual attributes of objects, generated descriptions by retrieving the descriptions of similar images (, relied on an external corpus to predict the relationships between objects (, or combined sentence fragments using a treesubstitution grammar (.", "labels": [], "entities": []}, {"text": "A common aspect of existing work is that an image is represented as a bag of image regions.", "labels": [], "entities": []}, {"text": "Bags of regions encode which objects co-occur in an image, but they are unable to express how the regions relate to each other, which makes it hard to describe what is happening.", "labels": [], "entities": []}, {"text": "As an example, consider Figure 1a, which depicts a man riding a bike.", "labels": [], "entities": []}, {"text": "If the man was instead repairing the bike, then the bagof-regions representation would be the same, even though the image would depict a different action and would have to be described differently.", "labels": [], "entities": []}, {"text": "This type of co-occurrence of regions indicates the need fora more structured image representation; an image description system that has access to structured repre- A man is riding a bike down the road.", "labels": [], "entities": []}, {"text": "A car and trees are in the background.", "labels": [], "entities": []}, {"text": "sentations would be able to correctly infer the action that is taking place, such as the distinction between repairing or riding a bike, which would greatly improve the descriptions it is able to generate.", "labels": [], "entities": []}, {"text": "In this paper, we introduce visual dependency representations (VDRs) to represent the structure of images.", "labels": [], "entities": []}, {"text": "This representation encodes the geometric relations between the regions of an image.", "labels": [], "entities": []}, {"text": "An example can be found in, which depicts the VDR for.", "labels": [], "entities": [{"text": "VDR", "start_pos": 46, "end_pos": 49, "type": "DATASET", "confidence": 0.9136432409286499}]}, {"text": "It encodes that the MAN is above the BIKE, and that the BIKE is on the ROAD.", "labels": [], "entities": [{"text": "MAN", "start_pos": 20, "end_pos": 23, "type": "METRIC", "confidence": 0.7499933838844299}, {"text": "BIKE", "start_pos": 37, "end_pos": 41, "type": "METRIC", "confidence": 0.9772622585296631}, {"text": "BIKE", "start_pos": 56, "end_pos": 60, "type": "METRIC", "confidence": 0.9965832829475403}, {"text": "ROAD", "start_pos": 71, "end_pos": 75, "type": "DATASET", "confidence": 0.6048303246498108}]}, {"text": "These relationships make it possible to infer that the man is riding a bike down the road, which corresponds to the first sentence of the human-generated image description in.", "labels": [], "entities": []}, {"text": "In order to test the hypothesis that structured image representations are useful for description generation, we present a series of template-based image description models.", "labels": [], "entities": [{"text": "description generation", "start_pos": 85, "end_pos": 107, "type": "TASK", "confidence": 0.8667638897895813}]}, {"text": "Two of these models are based on approaches in the literature that represent images as bags of regions.", "labels": [], "entities": []}, {"text": "The other two models use visual dependency representations, either on their own or in conjunction with gold-standard image descriptions at training time.", "labels": [], "entities": []}, {"text": "We find that descriptions generated using the VDR-based models are significantly better than those generated using bag-of-region models in automatic evaluations using smoothed BLEU scores and inhuman judgements.", "labels": [], "entities": [{"text": "BLEU scores", "start_pos": 176, "end_pos": 187, "type": "METRIC", "confidence": 0.972027599811554}]}, {"text": "The BLEU score improvements are found at bi-, tri-, and four-gram levels, and humans rate VDR-based image descriptions 1.2 points above the next-best model on a 1-5 scale.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 4, "end_pos": 14, "type": "METRIC", "confidence": 0.9735889136791229}]}, {"text": "Finally, we also show that the benefit of the visual dependency representation is maintained when image descriptions are generated from automatically parsed VDRs.", "labels": [], "entities": []}, {"text": "We use a modified version of the edge-factored parser of to predict VDRs over a set of annotated object regions.", "labels": [], "entities": []}, {"text": "This result reaffirms the potential utility of this representation as a means to describe events in images.", "labels": [], "entities": []}, {"text": "Note that throughout the paper, we work with goldstandard region annotations; this makes it possible to explore the effect of structured image representations independently of automatic object detection.", "labels": [], "entities": [{"text": "object detection", "start_pos": 186, "end_pos": 202, "type": "TASK", "confidence": 0.7295725345611572}]}], "datasetContent": [{"text": "We evaluate the image description models in an automatic setting and with human judgements.", "labels": [], "entities": []}, {"text": "In  The model-generated descriptions are compared against the human-written gold-standard descriptions using the smoothed BLEU measure ().", "labels": [], "entities": [{"text": "BLEU measure", "start_pos": 122, "end_pos": 134, "type": "METRIC", "confidence": 0.9661147594451904}]}, {"text": "BLEU is commonly used in machine translation experiments to measure the effective overlap between a reference sentence and a proposed translation sentence.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.9846262335777283}, {"text": "machine translation", "start_pos": 25, "end_pos": 44, "type": "TASK", "confidence": 0.7661813199520111}]}, {"text": "shows the results on the test data and shows sample outputs for two images.", "labels": [], "entities": []}, {"text": "PARALLEL, the model with access to both image structure and aligned image descriptions at training time outperforms all other models on higher-order BLEU measures.", "labels": [], "entities": [{"text": "PARALLEL", "start_pos": 0, "end_pos": 8, "type": "METRIC", "confidence": 0.9868085384368896}, {"text": "BLEU", "start_pos": 149, "end_pos": 153, "type": "METRIC", "confidence": 0.991063117980957}]}, {"text": "One reason for this improvement is that PARALLEL can formulate sentence fragments that relate the subject, a verb, and an object without trying to predict the best object, unlike CORPUS.", "labels": [], "entities": [{"text": "PARALLEL", "start_pos": 40, "end_pos": 48, "type": "METRIC", "confidence": 0.8053211569786072}]}, {"text": "The probability associated with each fragment generated for nodes with multiple children also tends to lead to a more accurate order of mentioning image regions.", "labels": [], "entities": []}, {"text": "It can also be seen that PARALLEL-PARSED remains significantly better than the other models when the VDRs of images are predicted by an image parser, rather than being gold-standard.", "labels": [], "entities": [{"text": "PARALLEL-PARSED", "start_pos": 25, "end_pos": 40, "type": "METRIC", "confidence": 0.971207320690155}]}, {"text": "The weakest results are obtained from a model that relies on the proximity of regions to generate descriptions.", "labels": [], "entities": []}, {"text": "PROXIMITY achieves competitive BLEU-1 scores but this is mostly due to it correctly generating region names and determiners.", "labels": [], "entities": [{"text": "BLEU-1", "start_pos": 31, "end_pos": 37, "type": "METRIC", "confidence": 0.9986496567726135}]}, {"text": "CORPUS is better than PROXIMITY at correctly producing higherorder n-grams than because it has a better model of the region-region relationships in an image.", "labels": [], "entities": [{"text": "CORPUS", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.5068256258964539}]}, {"text": "However, it has difficulties guessing the correct verb fora description, as it relies on corpus co-occurrences for this (see the second example in).", "labels": [], "entities": [{"text": "guessing the correct verb fora description", "start_pos": 29, "end_pos": 71, "type": "TASK", "confidence": 0.6528994540373484}]}, {"text": "STRUC-TURE uses the VDR of an image to generate the description, which this leads to an improvement over PROXIMITY on some of the BLEU metrics; however, it is not sufficient to outperform CORPUS.", "labels": [], "entities": [{"text": "STRUC-TURE", "start_pos": 0, "end_pos": 10, "type": "DATASET", "confidence": 0.8543500900268555}, {"text": "PROXIMITY", "start_pos": 105, "end_pos": 114, "type": "METRIC", "confidence": 0.9517603516578674}, {"text": "BLEU", "start_pos": 130, "end_pos": 134, "type": "METRIC", "confidence": 0.9967336654663086}]}], "tableCaptions": []}