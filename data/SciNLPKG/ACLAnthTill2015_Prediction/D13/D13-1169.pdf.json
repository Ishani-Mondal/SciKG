{"title": [{"text": "Deriving adjectival scales from continuous space word representations", "labels": [], "entities": []}], "abstractContent": [{"text": "Continuous space word representations extracted from neural network language models have been used effectively for natural language processing, but until recently it was not clear whether the spatial relationships of such representations were interpretable.", "labels": [], "entities": [{"text": "natural language processing", "start_pos": 115, "end_pos": 142, "type": "TASK", "confidence": 0.6701213717460632}]}, {"text": "(2013) show that these representations do capture syntactic and semantic regularities.", "labels": [], "entities": []}, {"text": "Here, we push the interpretation of continuous space word representations further by demonstrating that vector offsets can be used to derive adjectival scales (e.g., okay < good < excellent).", "labels": [], "entities": []}, {"text": "We evaluate the scales on the indirect answers to yes/no questions corpus (de Marn-effe et al., 2010).", "labels": [], "entities": []}, {"text": "We obtain 72.8% accuracy, which outperforms previous results (\u223c60%) on this corpus and highlights the quality of the scales extracted, providing further support that the continuous space word representations are meaningful.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 16, "end_pos": 24, "type": "METRIC", "confidence": 0.9996707439422607}]}], "introductionContent": [{"text": "There has recently been a surge of interest for deep learning in natural language processing.", "labels": [], "entities": []}, {"text": "In particular, neural network language models have been used to learn distributional word vectors: the models jointly learn an embedding of words into an n-dimensional feature space.", "labels": [], "entities": []}, {"text": "One of the advantages put forth for such distributed representations compared to traditional n-gram models is that similar words are likely to have similar vector representations in a continuous space model, whereas the discrete units of an n-gram model do not exhibit any inherent relation with one another.", "labels": [], "entities": []}, {"text": "It has been shown that the continuous space representations improve performance in a variety of NLP tasks, such as POS tagging, semantic role labeling, named entity resolution, parsing.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 115, "end_pos": 126, "type": "TASK", "confidence": 0.8786638975143433}, {"text": "semantic role labeling", "start_pos": 128, "end_pos": 150, "type": "TASK", "confidence": 0.6796519756317139}, {"text": "named entity resolution", "start_pos": 152, "end_pos": 175, "type": "TASK", "confidence": 0.6508694489796957}]}, {"text": "show that there are some syntactic and semantic regularities in the word representations learned, such as the singular/plural relation (the difference of singular and plural word vectors are equivalent: apple \u2212 apples \u2248 car \u2212 cars \u2248 family \u2212 families) or the gender relation (a masculine noun can be transformed into the feminine form: king \u2212 man + woman \u2248 queen).", "labels": [], "entities": []}, {"text": "We extend's approach and explore further the interpretation of the vector space.", "labels": [], "entities": []}, {"text": "We show that the word vectors learned by NNLMs are meaningful: we can extract scalar relationships between adjectives (e.g., bad < okay < good < excellent), which cannot only serve to build a sentiment lexicon but also be used for inference.", "labels": [], "entities": []}, {"text": "To evaluate the quality of the scalar relationships learned by NNLMs, we use the indirect yes/no question answer pairs (IQAP) from (de, where scales between adjectives are needed to infer a yes/no answer from a reply without explicit yes or no such as Was the movie good?", "labels": [], "entities": [{"text": "IQAP", "start_pos": 120, "end_pos": 124, "type": "METRIC", "confidence": 0.6895794868469238}]}, {"text": "Our method reaches 72.8% accuracy, which is the best result reported so far when scales are used.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 25, "end_pos": 33, "type": "METRIC", "confidence": 0.9986295700073242}]}], "datasetContent": [{"text": "To evaluate the quality of the adjective scales learned by the neural network approach, we use the corpus of indirect answers to yes/no questions created by, which consists of question-answer pairs involving gradable modifiers to test scalar implicatures.", "labels": [], "entities": []}, {"text": "We focus on the 125 pairs in the corpus where both the question and answer contain an adjective: e.g., Is Obama qualified?", "labels": [], "entities": []}, {"text": "I think he's young.", "labels": [], "entities": []}, {"text": "Each question-answer pair has been annotated via Mechanical Turk for whether the answer conveys yes, no or uncertain.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Words with corresponding vectors closest to the mean of positive:superlative word vectors.", "labels": [], "entities": []}, {"text": " Table 2: Adjectival scales extracted from the RNN: each row represent a scale, and for each intermediate point the  closest word in term of cosine similarity is given.", "labels": [], "entities": [{"text": "RNN", "start_pos": 47, "end_pos": 50, "type": "DATASET", "confidence": 0.750423789024353}]}, {"text": " Table 3: Score (%) comparison on the 125 scalar adjec- tive pairs in the IQAP corpus.", "labels": [], "entities": [{"text": "IQAP corpus", "start_pos": 74, "end_pos": 85, "type": "DATASET", "confidence": 0.950104296207428}]}]}