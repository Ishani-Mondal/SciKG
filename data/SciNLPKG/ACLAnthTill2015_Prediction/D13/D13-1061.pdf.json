{"title": [{"text": "Deep Learning for Chinese Word Segmentation and POS Tagging", "labels": [], "entities": [{"text": "Chinese Word Segmentation", "start_pos": 18, "end_pos": 43, "type": "TASK", "confidence": 0.5661697189013163}, {"text": "POS Tagging", "start_pos": 48, "end_pos": 59, "type": "TASK", "confidence": 0.7743167281150818}]}], "abstractContent": [{"text": "This study explores the feasibility of performing Chinese word segmentation (CWS) and POS tagging by deep learning.", "labels": [], "entities": [{"text": "Chinese word segmentation (CWS)", "start_pos": 50, "end_pos": 81, "type": "TASK", "confidence": 0.7388984759648641}, {"text": "POS tagging", "start_pos": 86, "end_pos": 97, "type": "TASK", "confidence": 0.7496587634086609}]}, {"text": "We try to avoid task-specific feature engineering, and use deep layers of neural networks to discover relevant features to the tasks.", "labels": [], "entities": []}, {"text": "We leverage large-scale unlabeled data to improve internal representation of Chinese characters, and use these improved representations to enhance supervised word segmentation and POS tagging models.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 158, "end_pos": 175, "type": "TASK", "confidence": 0.687593549489975}, {"text": "POS tagging", "start_pos": 180, "end_pos": 191, "type": "TASK", "confidence": 0.819008857011795}]}, {"text": "Our networks achieved close to state-of-the-art performance with minimal computational cost.", "labels": [], "entities": []}, {"text": "We also describe a perceptron-style algorithm for training the neural networks, as an alternative to maximum-likelihood method, to speedup the training process and make the learning algorithm easier to be implemented.", "labels": [], "entities": []}], "introductionContent": [{"text": "Word segmentation has been a long-standing challenge for the Chinese NLP community.", "labels": [], "entities": [{"text": "Word segmentation", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.6929998844861984}]}, {"text": "It has received steady attention over the past two decades.", "labels": [], "entities": []}, {"text": "Previous studies show that joint solutions usually lead to the improvement inaccuracy over pipelined systems by exploiting POS information to help word segmentation and avoiding error propagation.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 147, "end_pos": 164, "type": "TASK", "confidence": 0.725028783082962}]}, {"text": "However, traditional joint approaches usually involve a great number of features, which arises four limitations.", "labels": [], "entities": []}, {"text": "First, the size of the result models is too large for practical use due to the storage and computing constraints of certain real-world applications.", "labels": [], "entities": []}, {"text": "Second, the number of parameters is so large that the trained model is apt to overfit on training corpus.", "labels": [], "entities": []}, {"text": "Third, a longer training time is required.", "labels": [], "entities": []}, {"text": "Last but not the least, the decoding by dynamic programming technique might be intractable since a large search space is faced by the decoder.", "labels": [], "entities": []}, {"text": "The choice of features, therefore, is a critical success factor for these systems.", "labels": [], "entities": []}, {"text": "Most of the state-ofthe-art systems address their tasks by applying linear statistical models to the features carefully optimized for the tasks.", "labels": [], "entities": []}, {"text": "This approach is effective because researchers can incorporate a large body of linguistic knowledge into the models.", "labels": [], "entities": []}, {"text": "However, the approach does not scale well when it is used to perform more complex joint tasks, for example, the task of joint word segmentation, POS tagging, parsing, and semantic role labeling.", "labels": [], "entities": [{"text": "joint word segmentation", "start_pos": 120, "end_pos": 143, "type": "TASK", "confidence": 0.6126035451889038}, {"text": "POS tagging", "start_pos": 145, "end_pos": 156, "type": "TASK", "confidence": 0.8898962736129761}, {"text": "parsing", "start_pos": 158, "end_pos": 165, "type": "TASK", "confidence": 0.9137018322944641}, {"text": "semantic role labeling", "start_pos": 171, "end_pos": 193, "type": "TASK", "confidence": 0.659443179766337}]}, {"text": "A challenge for such a joint model is the large combined search space, which makes engineering effective task-specific features and structured learning of parameters very hard.", "labels": [], "entities": []}, {"text": "Instead, we use multilayer neural networks to discover the useful features from the input sentences.", "labels": [], "entities": []}, {"text": "There are two main contributions in this paper.", "labels": [], "entities": []}, {"text": "(1) We describe a perceptron-style algorithm for training the neural networks, which not only speeds up the training of the networks with negligible loss in performance, but also can be implemented more easily; (2) We show that the tasks of Chinese word segmentation and POS tagging can be effectively performed by the deep learning.", "labels": [], "entities": [{"text": "Chinese word segmentation", "start_pos": 241, "end_pos": 266, "type": "TASK", "confidence": 0.5745775202910105}, {"text": "POS tagging", "start_pos": 271, "end_pos": 282, "type": "TASK", "confidence": 0.7757341265678406}]}, {"text": "Our networks achieved close to state-of-the-art performance by transferring the unsupervised internal representations of Chinese characters into the supervised models.", "labels": [], "entities": []}, {"text": "Section 2 presents the general architecture of neural networks, and our perceptron-style training algorithm for tagging.", "labels": [], "entities": []}, {"text": "Section 3 describes how to leverage large unlabeled data to obtain more useful character embeddings, and reports the experimental results of our systems.", "labels": [], "entities": []}, {"text": "Section 4 presents a brief overview of related work.", "labels": [], "entities": []}, {"text": "The conclusions are given in section 5.", "labels": [], "entities": []}], "datasetContent": [{"text": "We conducted three sets of experiments.", "labels": [], "entities": []}, {"text": "The goal of the first one is to test several variants for each training algorithm on the development set, to gain some understanding of how the choice of hyperparameters impacts upon the performance.", "labels": [], "entities": []}, {"text": "We applied the network both with the sentence-level loglikelihood (SLL) and our perceptron-style training algorithm (PSA) to the two Chinese NLP problems: word segmentation, and joint CWS and POS tagging.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 155, "end_pos": 172, "type": "TASK", "confidence": 0.7645086348056793}, {"text": "POS tagging", "start_pos": 192, "end_pos": 203, "type": "TASK", "confidence": 0.6005488783121109}]}, {"text": "We ran this set of experiments on the part of Chinese Treebank 4 (CTB-4) . Ninety percent of the sentences (1529) were randomly chosen for training and the rest (168) were used as development set.", "labels": [], "entities": [{"text": "Chinese Treebank 4 (CTB-4)", "start_pos": 46, "end_pos": 72, "type": "DATASET", "confidence": 0.9612461427847544}]}, {"text": "The second set of experiments was run on the Chinese Treebank (CTB) data sets from), which contains a training and a test corpus for supervised word segmentation and POS tagging tasks.", "labels": [], "entities": [{"text": "Chinese Treebank (CTB) data sets", "start_pos": 45, "end_pos": 77, "type": "DATASET", "confidence": 0.9779719284602574}, {"text": "word segmentation", "start_pos": 144, "end_pos": 161, "type": "TASK", "confidence": 0.7086998522281647}, {"text": "POS tagging tasks", "start_pos": 166, "end_pos": 183, "type": "TASK", "confidence": 0.8372853795687357}]}, {"text": "The results were obtained without using any extra knowledge (i.e. the closed test), and are comparable with other models in the literature.", "labels": [], "entities": []}, {"text": "In the third experiment, we study to see how well large unlabeled texts can be used to enhance the supervised learning.", "labels": [], "entities": []}, {"text": "Following ), we first use large unlabeled data set to obtain character embeddings carrying more syntactic and semantic information, and then use these improved embeddings to initialize the character lookup tables of the networks instead of previous random values.", "labels": [], "entities": []}, {"text": "Our corpus is the Sina news 7 that contains about 325MB data.", "labels": [], "entities": [{"text": "Sina news 7", "start_pos": 18, "end_pos": 29, "type": "DATASET", "confidence": 0.9182946085929871}]}, {"text": "We implemented two versions of the network: one for the sentence-level log-likelihood and one for our perceptron-style training algorithm.", "labels": [], "entities": []}, {"text": "Both are written in Java language.", "labels": [], "entities": []}, {"text": "All experiments were run on a computer equipped with an Intel Core i3 processor working at 2.13GHz, with 2GB RAM, running Linux and Java Development Kit 1.6.", "labels": [], "entities": []}, {"text": "The standard F-score was used to evaluate the performance of both word segmentation and joint word segmentation and POS tagging tasks.", "labels": [], "entities": [{"text": "F-score", "start_pos": 13, "end_pos": 20, "type": "METRIC", "confidence": 0.9942507743835449}, {"text": "word segmentation", "start_pos": 66, "end_pos": 83, "type": "TASK", "confidence": 0.7775204479694366}, {"text": "word segmentation and POS tagging tasks", "start_pos": 94, "end_pos": 133, "type": "TASK", "confidence": 0.7359562168518702}]}, {"text": "F-score is the harmonic mean of precision p and recall r, which is defined as 2pr/(p + r).", "labels": [], "entities": [{"text": "F-score", "start_pos": 0, "end_pos": 7, "type": "METRIC", "confidence": 0.9765623211860657}, {"text": "precision", "start_pos": 32, "end_pos": 41, "type": "METRIC", "confidence": 0.9936500191688538}, {"text": "recall r", "start_pos": 48, "end_pos": 56, "type": "METRIC", "confidence": 0.986049622297287}]}], "tableCaptions": [{"text": " Table 1. Al- though the top performance was obtained by the net- work with window size 3, we chose the architecture  with window size 5 because a larger training corpus  will be used in the following experiments, and the  sparseness problem would be alleviated. Further- more, in order to obtain character embeddings by  using large unlabeled data, we prefer to \"observe\"  a character within a slightly larger window to better  discover its syntactic and semantic information.", "labels": [], "entities": []}, {"text": " Table 1: Hyper-parameters of the network.", "labels": [], "entities": []}, {"text": " Table 2: Word segmentation results with SLL and PSA  for the first five iterations.", "labels": [], "entities": [{"text": "Word segmentation", "start_pos": 10, "end_pos": 27, "type": "TASK", "confidence": 0.7374134063720703}]}, {"text": " Table 3: Comparison of the F-scores on the Penn Chinese  Treebank", "labels": [], "entities": [{"text": "F-scores", "start_pos": 28, "end_pos": 36, "type": "METRIC", "confidence": 0.9676535725593567}, {"text": "Penn Chinese  Treebank", "start_pos": 44, "end_pos": 66, "type": "DATASET", "confidence": 0.9568331440289816}]}, {"text": " Table 4: Comparison of computational cost.", "labels": [], "entities": []}]}