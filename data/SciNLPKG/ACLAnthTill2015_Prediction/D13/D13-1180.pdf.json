{"title": [{"text": "Automated Essay Scoring by Maximizing Human-machine Agreement", "labels": [], "entities": [{"text": "Automated Essay Scoring", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.8094216585159302}]}], "abstractContent": [{"text": "Previous approaches for automated essay scoring (AES) learn a rating model by minimizing either the classification, regression, or pairwise classification loss, depending on the learning algorithm used.", "labels": [], "entities": [{"text": "automated essay scoring (AES", "start_pos": 24, "end_pos": 52, "type": "TASK", "confidence": 0.7614407122135163}]}, {"text": "In this paper, we argue that the current AES systems can be further improved by taking into account the agreement between human and machine raters.", "labels": [], "entities": []}, {"text": "To this end, we propose a rank-based approach that utilizes listwise learning to rank algorithms for learning a rating model, where the agreement between the human and machine raters is directly incorporated into the loss function.", "labels": [], "entities": []}, {"text": "Various linguistic and statistical features are utilized to facilitate the learning algorithms.", "labels": [], "entities": []}, {"text": "Experiments on the publicly available English essay dataset, Automated Student Assessment Prize (ASAP), show that our proposed approach outperforms the state-of-the-art algorithms, and achieves performance comparable to professional human raters, which suggests the effectiveness of our proposed method for automated essay scoring.", "labels": [], "entities": [{"text": "English essay dataset, Automated Student Assessment Prize (ASAP)", "start_pos": 38, "end_pos": 102, "type": "DATASET", "confidence": 0.6008415005423806}, {"text": "essay scoring", "start_pos": 317, "end_pos": 330, "type": "TASK", "confidence": 0.6716687977313995}]}], "introductionContent": [{"text": "Automated essay scoring utilizes the NLP techniques to automatically rate essays written forgiven prompts, namely, essay topics, in an educational setting).", "labels": [], "entities": [{"text": "essay scoring", "start_pos": 10, "end_pos": 23, "type": "TASK", "confidence": 0.6559158116579056}]}, {"text": "Nowadays, AES systems have been put into practical use in large-scale English tests and play the role of one human rater.", "labels": [], "entities": []}, {"text": "For example, before AES systems enter the picture, essays in the writing assessment of Graduate Record Examination (GRE) are rated by two human raters.", "labels": [], "entities": [{"text": "Graduate Record Examination (GRE)", "start_pos": 87, "end_pos": 120, "type": "DATASET", "confidence": 0.8436353802680969}]}, {"text": "A third human rater is needed when the difference of the scores given by the two human raters is larger than one in the 6-point scale.", "labels": [], "entities": []}, {"text": "Currently, GRE essays are rated by one human rater and one AES system.", "labels": [], "entities": [{"text": "GRE essays", "start_pos": 11, "end_pos": 21, "type": "TASK", "confidence": 0.5380070060491562}, {"text": "AES", "start_pos": 59, "end_pos": 62, "type": "METRIC", "confidence": 0.9917146563529968}]}, {"text": "A second human rater is required only when there exists a non-negligible disagreement between the first human rater and the machine rater.", "labels": [], "entities": []}, {"text": "With the help of an AES system that highly agrees with human raters, the human workload can be reduced by half at most.", "labels": [], "entities": [{"text": "AES", "start_pos": 20, "end_pos": 23, "type": "METRIC", "confidence": 0.9635159969329834}]}, {"text": "Therefore, the agreement between the AES system and the human rater is an important indicator of an AES system's effectiveness.", "labels": [], "entities": []}, {"text": "There have been efforts in developing AES methods since the 1960s.", "labels": [], "entities": [{"text": "AES", "start_pos": 38, "end_pos": 41, "type": "METRIC", "confidence": 0.6976014971733093}]}, {"text": "Various kinds of algorithms and models based on NLP and machine learning techniques have been proposed to implement AES systems.", "labels": [], "entities": []}, {"text": "Existing approaches consider essay rating as a classification, regression) or preference ranking problem, where the loss function is the regression loss, classification loss and pairwise classification loss, respectively.", "labels": [], "entities": [{"text": "classification loss", "start_pos": 154, "end_pos": 173, "type": "METRIC", "confidence": 0.8063839077949524}]}, {"text": "In this paper, we argue that the purpose of AES is to predict the essay's rating that human raters would give.", "labels": [], "entities": [{"text": "AES", "start_pos": 44, "end_pos": 47, "type": "METRIC", "confidence": 0.9670690298080444}]}, {"text": "If an AES system frequently disagrees with the first human rater, a second human rater will be needed inmost cases.", "labels": [], "entities": []}, {"text": "Thus, the introduction of the AES system does not bring much benefit in reducing the human workload.", "labels": [], "entities": [{"text": "AES", "start_pos": 30, "end_pos": 33, "type": "METRIC", "confidence": 0.6829028725624084}]}, {"text": "It is therefore desirable to minimize the disagreement between the machine and human raters.", "labels": [], "entities": []}, {"text": "However, this disagreement is not explicitly, if any, addressed in the current AES methods.", "labels": [], "entities": []}, {"text": "To this end, we propose a rank-based approach in this paper that utilizes a listwise learning to rank algorithm to address automated essay scoring in the view of directly optimizing the agreement between human raters and the AES system.", "labels": [], "entities": [{"text": "AES", "start_pos": 225, "end_pos": 228, "type": "DATASET", "confidence": 0.6794033646583557}]}, {"text": "Different from the preference ranking-based approach) that maximizes the pairwise classification precision, our rank-based approach follows the listwise learning paradigm and the agreement between the machine and human raters is directly integrated into the loss function that is optimized by gradient boost regression trees.", "labels": [], "entities": [{"text": "precision", "start_pos": 97, "end_pos": 106, "type": "METRIC", "confidence": 0.8589755296707153}]}, {"text": "To the best of our knowledge, this work is the first to apply listwise learning to rank approach for AES, which aims at the optimization of the agreement between the human and machine raters.", "labels": [], "entities": [{"text": "AES", "start_pos": 101, "end_pos": 104, "type": "DATASET", "confidence": 0.48139476776123047}]}, {"text": "Experimental results on the publicly available dataset ASAP indicate that our proposed method achieves high agreement with human raters, that is about 0.80, measured by quadratic weighted.", "labels": [], "entities": [{"text": "ASAP", "start_pos": 55, "end_pos": 59, "type": "TASK", "confidence": 0.42815250158309937}]}, {"text": "Our proposed method also outperforms the previous classification, regression and preference ranking based approaches.", "labels": [], "entities": []}, {"text": "As it is widely accepted that the agreement between human raters, measured by either quadratic weighted Kappa or Pearson's correlation coefficient, ranges from 0.70 to 0.80 (), our proposed approach therefore performs as well as human raters.", "labels": [], "entities": [{"text": "Pearson's correlation coefficient", "start_pos": 113, "end_pos": 146, "type": "METRIC", "confidence": 0.7784776389598846}]}, {"text": "The rest of this paper is organized as follows.", "labels": [], "entities": []}, {"text": "In section 2, we introduce the research background of automated essay scoring and give a brief introduction to learning to rank.", "labels": [], "entities": [{"text": "automated essay scoring", "start_pos": 54, "end_pos": 77, "type": "TASK", "confidence": 0.6180127064387003}]}, {"text": "In section 3, a detailed description of our listwise learning to rank approach for automated essay scoring is presented.", "labels": [], "entities": [{"text": "automated essay scoring", "start_pos": 83, "end_pos": 106, "type": "TASK", "confidence": 0.6018058856328329}]}, {"text": "Section 4 explains the experimental setup and section 5 presents the experimental results.", "labels": [], "entities": []}, {"text": "Finally, in section 6 we conclude this research.", "labels": [], "entities": []}], "datasetContent": [{"text": "This section presents our the experimental design, including the test dataset used, configuration of testing algorithms, feature selection and the evaluation methodology.", "labels": [], "entities": [{"text": "feature selection", "start_pos": 121, "end_pos": 138, "type": "TASK", "confidence": 0.6592926234006882}]}, {"text": "The dataset used in our experiments comes from the Automated Student Assessment Prize (ASAP) 1 , which is sponsored by the William and Flora Hewlett Foundation.", "labels": [], "entities": [{"text": "Automated Student Assessment Prize (ASAP) 1", "start_pos": 51, "end_pos": 94, "type": "DATASET", "confidence": 0.6073696315288544}, {"text": "William and Flora Hewlett Foundation", "start_pos": 123, "end_pos": 159, "type": "DATASET", "confidence": 0.6512436151504517}]}, {"text": "Dataset in this competition consists of eight essay sets.", "labels": [], "entities": []}, {"text": "Each essay set was generated from a single prompt.", "labels": [], "entities": []}, {"text": "The number of essays associated with each prompt ranges from 900 to 1800 and the average length of essays in word in each essay set ranges from 150 to 650.", "labels": [], "entities": []}, {"text": "All essays were written by students in different grades and received a resolved score, namely the actual rating, from professional human raters.", "labels": [], "entities": []}, {"text": "Moreover, ASAP comes with a validation set that can be used for parameter training.", "labels": [], "entities": [{"text": "ASAP", "start_pos": 10, "end_pos": 14, "type": "TASK", "confidence": 0.7007577419281006}]}, {"text": "There is no overlap between this validation set and the test set used in our evaluation.", "labels": [], "entities": []}, {"text": "In AES, the agreement between human-machine rater is the most important measurement of success.", "labels": [], "entities": []}, {"text": "We use quadratic weighted Kappa to evaluate the agreement between the ratings given by the AES algorithm and the actual ratings.", "labels": [], "entities": [{"text": "AES", "start_pos": 91, "end_pos": 94, "type": "METRIC", "confidence": 0.6698746085166931}]}, {"text": "It is widely accepted as a reasonable evaluation measure for AES systems, and is also the official evaluation measure in the ASAP AES competition.", "labels": [], "entities": [{"text": "AES", "start_pos": 61, "end_pos": 64, "type": "TASK", "confidence": 0.47483113408088684}, {"text": "ASAP AES competition", "start_pos": 125, "end_pos": 145, "type": "DATASET", "confidence": 0.5998983879884084}]}, {"text": "It is calculated on all essay topics.", "labels": [], "entities": [{"text": "It", "start_pos": 0, "end_pos": 2, "type": "METRIC", "confidence": 0.9652283787727356}]}, {"text": "If there are essays that come from n essay topics, we calculate the agreement degree on each essay topic first and then compute the overall agreement degree in the z-space.", "labels": [], "entities": [{"text": "agreement degree", "start_pos": 68, "end_pos": 84, "type": "METRIC", "confidence": 0.971167266368866}, {"text": "agreement degree", "start_pos": 140, "end_pos": 156, "type": "METRIC", "confidence": 0.9301745891571045}]}, {"text": "In addition, analysis of variance (ANOVA)) is conducted to test whether significant difference exists between the two groups of scores given by human and machine raters.", "labels": [], "entities": [{"text": "analysis of variance (ANOVA))", "start_pos": 13, "end_pos": 42, "type": "METRIC", "confidence": 0.8074567317962646}]}, {"text": "We conduct three sets of experiments to evaluate the effectiveness of our listwise learning to rank approach for automated essay scoring.", "labels": [], "entities": [{"text": "automated essay scoring", "start_pos": 113, "end_pos": 136, "type": "TASK", "confidence": 0.5907969077428182}]}, {"text": "The first set of experiments evaluates our proposed approach under a prompt-specific setting.", "labels": [], "entities": []}, {"text": "We conduct 5-fold cross-validation, where the essays of each prompt are randomly partitioned into 5 subsets.", "labels": [], "entities": []}, {"text": "In each fold, 4 subsets are used for training, and one is used for testing.", "labels": [], "entities": []}, {"text": "To avoid bias introduced by the random partition, we repeat the 5-fold crossvalidation for 5 times on 5 different random partitions.", "labels": [], "entities": []}, {"text": "The overall quadratic weighted Kappa is averaged on all 25 test subsets.", "labels": [], "entities": [{"text": "quadratic weighted Kappa", "start_pos": 12, "end_pos": 36, "type": "METRIC", "confidence": 0.6858670512835184}]}, {"text": "It should be noticed that in random partition of the whole dataset, the overlap between any two partitions should be kept below 1.5 * 1/(#f olds) * 100%.", "labels": [], "entities": []}, {"text": "For example, in five-fold cross validation, the overlap should be kept below 30%.", "labels": [], "entities": [{"text": "cross validation", "start_pos": 26, "end_pos": 42, "type": "TASK", "confidence": 0.5086805820465088}, {"text": "overlap", "start_pos": 48, "end_pos": 55, "type": "METRIC", "confidence": 0.9971574544906616}]}, {"text": "This is because: according to the Dirichlet principle, each subset in one partition overlaps more than 20% with at least one subset in another partition in fivefold cross-validation.", "labels": [], "entities": []}, {"text": "The tolerance boundary parameter is then set to 1.5.", "labels": [], "entities": []}, {"text": "The objective of the second set of experiments is to test the performance of our listwise learning to rank approach for generic rating models.", "labels": [], "entities": []}, {"text": "We also conduct 5 times 5-fold cross-validation like the first experiment.", "labels": [], "entities": []}, {"text": "In 5-fold cross-validation, essays associated with the same prompt are randomly partitioned into 5 subsets.", "labels": [], "entities": []}, {"text": "In this way, each fold consists of essays across all prompts.", "labels": [], "entities": []}, {"text": "The overall performance is averaged on all 25 test subsets.", "labels": [], "entities": []}, {"text": "In the third set of experiments, we evaluate the quality of the features used in our rating model by feature ablation test and feature unique test.", "labels": [], "entities": [{"text": "feature ablation test", "start_pos": 101, "end_pos": 122, "type": "METRIC", "confidence": 0.8075568079948425}]}, {"text": "In ablation test, we evaluate our essay rating model's performance before and after the removal of a subset of features from the whole feature set.", "labels": [], "entities": []}, {"text": "The performance difference indicates the removed features' contribution to the rating model's overall performance.", "labels": [], "entities": []}, {"text": "In unique test, only a subset of features are used in the rating model construction and all other features are removed.", "labels": [], "entities": []}, {"text": "The learned rating model's performance indicates to which extent the features are correlated with the actual essay ratings.", "labels": [], "entities": []}, {"text": "presents the first set of experimental results obtained on the ASAP dataset, measured by quadratic weighted Kappa.", "labels": [], "entities": [{"text": "ASAP dataset", "start_pos": 63, "end_pos": 75, "type": "DATASET", "confidence": 0.9459525048732758}]}, {"text": "In, RF stands for random forests.", "labels": [], "entities": [{"text": "RF", "start_pos": 4, "end_pos": 6, "type": "METRIC", "confidence": 0.9723880290985107}]}, {"text": "SVMc, SVMr, SVMp are SVM for classification, regression and preference ranking, respectively.", "labels": [], "entities": [{"text": "SVMc", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.7694266438484192}]}, {"text": "ANOVA stands for variance analysis, which aims to test whether significant difference exists between the scores given by human and machine raters.", "labels": [], "entities": [{"text": "ANOVA", "start_pos": 0, "end_pos": 5, "type": "METRIC", "confidence": 0.6265233159065247}]}, {"text": "The improvement of our RF bagging K-LambdaMART over each baseline in percentage is also given.", "labels": [], "entities": [{"text": "RF bagging K-LambdaMART", "start_pos": 23, "end_pos": 46, "type": "TASK", "confidence": 0.6359513103961945}]}, {"text": "For prompt-specific rating model, all of these algorithms achieve good performance comparable to human raters as literatures have revealed that the agreement between two professional human raters (measured by statistics for correlation analysis, e.g. quadratic weighted Kappa) is around 0.70 to 0.80.", "labels": [], "entities": []}, {"text": "It is clear that our listwise learning to rank approach, Random Forests bagging K-LambdaMART, gives the best performance on the ASAP dataset.", "labels": [], "entities": [{"text": "ASAP dataset", "start_pos": 128, "end_pos": 140, "type": "DATASET", "confidence": 0.8696837723255157}]}, {"text": "The variance analysis result on the six groups of scores (scores given by five times of five-fold cross-validation and the scores provided by human rater), no significant difference, suggests the robustness of our proposed approach.", "labels": [], "entities": []}, {"text": "On the contrary, although pref-erence ranking based approach, SVM for ranking, and regression based approach, SVM for regression, give very good result in human-machine agreement, their variance analysis results indicate that there exists significant difference between the scores given by human and machine raters.", "labels": [], "entities": []}, {"text": "The result of the first set of experiments suggests the effectiveness and robustness of our listwise learning to rank approach in the building of prompt-specific rating model.", "labels": [], "entities": []}, {"text": "For generic rating model, one can conclude from that RF bagging LambdaMART performs better than SVM for classification, regression and preference ranking on the ASAP dataset.", "labels": [], "entities": [{"text": "RF bagging LambdaMART", "start_pos": 53, "end_pos": 74, "type": "TASK", "confidence": 0.6959859132766724}, {"text": "ASAP dataset", "start_pos": 161, "end_pos": 173, "type": "DATASET", "confidence": 0.9555126130580902}]}, {"text": "The dataset used in our experiment consists of essays generated by 8 prompts and each prompt has its own features.", "labels": [], "entities": []}, {"text": "With such a training set, both classification and regression based approaches produce not good results, as it is commonly accepted that rating model whose performance measured by interrater agreement lower than 0.70 is not applicable.", "labels": [], "entities": []}, {"text": "And the variance analysis results also reveal that there exists statistically significant difference between the scores given by human and machine raters, indicating a low robustness of these two baselines.", "labels": [], "entities": []}, {"text": "The performance comparison of the generic rating models suggest that the rank based approaches, SVMp and RF bagging KLambdaMART, are more effective than the classification based SVMc and the regression based SVMr, while our proposed RF bagging K-LambdaMART outperforms the state-of-the-art SVMp.", "labels": [], "entities": []}, {"text": "Moreover, we find that there is no obvious performance difference when our proposed method is applied to prompt-specific and generic rating models.", "labels": [], "entities": []}, {"text": "Considering the advantages generic rating models have, the result of the second set of experiments suggests the feasibility of building a rating model which is generalizable across different prompts while performs slightly inferior to the prompt-specific rating model.", "labels": [], "entities": []}, {"text": "gives the results of feature ablation and unique test.", "labels": [], "entities": []}, {"text": "In the table, \"All features\" stands for the use of all the features available, apart from the prompt-specific features that are not applicable to learning a generic model.", "labels": [], "entities": []}, {"text": "In other rows, the feature subset name stands for the feature subset to be ablated in ablation test and the feature subset to be used in unique test.", "labels": [], "entities": []}, {"text": "Note that we ablate (as in the ablation test) or use (as in the unique test) a subset of features such as the different statistics of word length as a whole since features belonging to the same subset are usually highly correlated.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Cross-validation on ASAP dataset measured by quadratic weighted Kappa.", "labels": [], "entities": [{"text": "ASAP dataset", "start_pos": 30, "end_pos": 42, "type": "DATASET", "confidence": 0.8135520815849304}]}, {"text": " Table 2: Results of feature ablation and unique test", "labels": [], "entities": []}]}