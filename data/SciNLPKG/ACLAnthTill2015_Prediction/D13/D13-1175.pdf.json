{"title": [{"text": "Boosting Cross-Language Retrieval by Learning Bilingual Phrase Associations from Relevance Rankings", "labels": [], "entities": [{"text": "Boosting Cross-Language Retrieval", "start_pos": 0, "end_pos": 33, "type": "TASK", "confidence": 0.631655772527059}]}], "abstractContent": [{"text": "We present an approach to learning bilingual n-gram correspondences from relevance rankings of English documents for Japanese queries.", "labels": [], "entities": []}, {"text": "We show that directly optimizing cross-lingual rankings rivals and complements machine translation-based cross-language information retrieval (CLIR).", "labels": [], "entities": [{"text": "machine translation-based cross-language information retrieval", "start_pos": 79, "end_pos": 141, "type": "TASK", "confidence": 0.5629095613956452}]}, {"text": "We propose an efficient boosting algorithm that deals with very large cross-product spaces of word correspondences.", "labels": [], "entities": []}, {"text": "We show in an experimental evaluation on patent prior art search that our approach , and in particular a consensus-based combination of boosting and translation-based approaches, yields substantial improvements in CLIR performance.", "labels": [], "entities": []}, {"text": "Our training and test data are made publicly available.", "labels": [], "entities": []}], "introductionContent": [{"text": "The central problem addressed in Cross-Language Information Retrieval (CLIR) is that of translating or projecting a query into the language of the document repository across which retrieval is performed.", "labels": [], "entities": [{"text": "Cross-Language Information Retrieval (CLIR)", "start_pos": 33, "end_pos": 76, "type": "TASK", "confidence": 0.7877659847338995}]}, {"text": "There are two main approaches to tackle this problem: The first approach leverages the standard Statistical Machine Translation (SMT) machinery to produce a single best translation that is used as search query in the target language.", "labels": [], "entities": [{"text": "Statistical Machine Translation (SMT)", "start_pos": 96, "end_pos": 133, "type": "TASK", "confidence": 0.7718484004338583}]}, {"text": "We will henceforth call this the direct translation approach.", "labels": [], "entities": [{"text": "direct translation", "start_pos": 33, "end_pos": 51, "type": "TASK", "confidence": 0.6890692114830017}]}, {"text": "This technique is particularly useful if large amounts of data are available in domain-specific form.", "labels": [], "entities": []}, {"text": "Alternative approaches avoid to solve the hard problem of word reordering, and instead rely on token-to-token translations that are used to project the query terms into the target language with a probabilistic weighting of the standard term tf-idf scheme.", "labels": [], "entities": [{"text": "word reordering", "start_pos": 58, "end_pos": 73, "type": "TASK", "confidence": 0.7384335696697235}]}, {"text": "termed this method the probabilistic structured query approach.", "labels": [], "entities": []}, {"text": "The advantage of this technique is an implicit query expansion effect due to the use of probability distributions over term translations ().", "labels": [], "entities": [{"text": "query expansion", "start_pos": 47, "end_pos": 62, "type": "TASK", "confidence": 0.6662722527980804}]}, {"text": "Recent research has shown that leveraging query context by extracting term translation probabilities from n-best direct translations of queries instead of using context-free translation tables outperforms both direct translation and context-free projection).", "labels": [], "entities": []}, {"text": "While direct translation as well as probabilistic structured query approaches use machine learning to optimize the SMT module, retrieval is done by standard search algorithms in both approaches.", "labels": [], "entities": [{"text": "direct translation", "start_pos": 6, "end_pos": 24, "type": "TASK", "confidence": 0.6958906054496765}, {"text": "SMT", "start_pos": 115, "end_pos": 118, "type": "TASK", "confidence": 0.9898461103439331}]}, {"text": "For example, Google's CLIR approach uses their standard proprietary search engine (.", "labels": [], "entities": []}, {"text": "use standard retrieval algorithms such as BM25 (.", "labels": [], "entities": [{"text": "BM25", "start_pos": 42, "end_pos": 46, "type": "DATASET", "confidence": 0.9282966256141663}]}, {"text": "That means, machine learning in SMT-based approaches concentrates on the cross-language aspect of CLIR and is agnostic of the ultimate ranking task.", "labels": [], "entities": [{"text": "machine learning", "start_pos": 12, "end_pos": 28, "type": "TASK", "confidence": 0.7450758218765259}, {"text": "SMT-based", "start_pos": 32, "end_pos": 41, "type": "TASK", "confidence": 0.9856815338134766}]}, {"text": "In this paper, we present a method to project search queries into the target language that is complementary to SMT-based CLIR approaches.", "labels": [], "entities": [{"text": "SMT-based CLIR", "start_pos": 111, "end_pos": 125, "type": "TASK", "confidence": 0.8578990995883942}]}, {"text": "Our method learns a table of n-gram correspondences by direct optimization of a ranking objective on relevance rankings of English documents for Japanese queries.", "labels": [], "entities": []}, {"text": "Our model is similar to the approach of  who characterize their technique as \"Learning to rank with (a Lot of) Word Features\".", "labels": [], "entities": []}, {"text": "Given a set of search queries q \u2208 IR Q and docu-ments d \u2208 IR D , where the j th dimension of a vector indicates the occurrence of the j th word for dictionaries of size Q and D, we want to learn a score f (q, d) between a query and a given document using the model 1 We take a pairwise ranking approach to optimization.", "labels": [], "entities": []}, {"text": "That is, given labeled data in the form of a set R of tuples (q, d + , d \u2212 ), where d + is a relevant (or higher ranked) document and d \u2212 an irrelevant (or lower ranked) document for query q, the goal is to find a weight matrix W \u2208 IR Q\u00d7D such that f (q, d + ) > f (q, d \u2212 ) for all data tuples from R.", "labels": [], "entities": []}, {"text": "The scoring model learns weights for all possible correspondences of query terms and document terms by directly optimizing the ranking objective at hand.", "labels": [], "entities": []}, {"text": "Such a phrase table contains domain-specific word associations that are useful to discern relevant from irrelevant documents, something that is orthogonal and complementary to standard SMT models.", "labels": [], "entities": [{"text": "SMT", "start_pos": 185, "end_pos": 188, "type": "TASK", "confidence": 0.9844861030578613}]}, {"text": "The challenge of our approach can be explained by constructing a joint feature map \u03c6 from the outer product of the vectors q and d where Using this feature map, we see that the score function f can be written in the standard form of a linear model that computes the inner product between a weight vector wand a feature vector \u03c6 where w, \u03c6 \u2208 IR Q\u00d7D and While various standard algorithms exist to optimize linear models, the difficulty lies in the memory footprint and capacity of the word-based model.", "labels": [], "entities": []}, {"text": "A fullsized model includes Q \u00d7 D parameters which is easily in the billions even for moderately sized dictionaries.", "labels": [], "entities": []}, {"text": "Clearly, an efficient implementation and remedies against overfitting are essential.", "labels": [], "entities": []}, {"text": "The main contribution of our paper is the presentation of algorithms that make learning a phrase table by direct rank optimization feasible, and an experimental verification of the benefits of this approach, especially with regard to a combination of the orthogonal information sources of rankingbased and SMT-based CLIR approaches.", "labels": [], "entities": [{"text": "SMT-based CLIR", "start_pos": 306, "end_pos": 320, "type": "TASK", "confidence": 0.8513676524162292}]}, {"text": "Our approach builds upon a boosting framework for pairwise ranking) that allows the model to grow incrementally, thus avoiding having to deal with the full matrix W . Furthermore, we present an implementation of boosting that utilizes parallel estimation on bootstrap samples from the training set for increased efficiency and reduced error.", "labels": [], "entities": [{"text": "error", "start_pos": 335, "end_pos": 340, "type": "METRIC", "confidence": 0.9653186798095703}]}, {"text": "Our \"bagged boosting\" approach allows to combine incremental feature selection, parallel training, and efficient management of large data structures.", "labels": [], "entities": []}, {"text": "We show in an experimental evaluation on largescale retrieval on patent abstracts that our boosting approach is comparable in MAP and improves significantly by 13-15 PRES points over very competitive translation-based CLIR systems that are trained on 1.8 million parallel sentence pairs from JapaneseEnglish patent documents.", "labels": [], "entities": [{"text": "largescale retrieval", "start_pos": 41, "end_pos": 61, "type": "TASK", "confidence": 0.7702447772026062}, {"text": "MAP", "start_pos": 126, "end_pos": 129, "type": "METRIC", "confidence": 0.7788555026054382}, {"text": "PRES", "start_pos": 166, "end_pos": 170, "type": "METRIC", "confidence": 0.9825757741928101}, {"text": "JapaneseEnglish patent documents", "start_pos": 292, "end_pos": 324, "type": "DATASET", "confidence": 0.8659006158510844}]}, {"text": "Moreover, a combination of the orthogonal information learned in rankingbased and translation-based approaches improves over 7 MAP points and over 15 PRES points over the respective translation-based system in a consensusbased voting approach following the Borda Count technique).", "labels": [], "entities": [{"text": "MAP", "start_pos": 127, "end_pos": 130, "type": "METRIC", "confidence": 0.9135158658027649}, {"text": "PRES", "start_pos": 150, "end_pos": 154, "type": "METRIC", "confidence": 0.960913896560669}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Statistics of ranking data.", "labels": [], "entities": []}, {"text": " Table 2: MAP and PRES scores for CLIR methods (best  configurations) on the development and test sets. Prefixed  numbers denote statistical significance of a pairwise com- parison with the baseline indicated by the superscript. For  example, the bottom right result shows that Boost-2g is  significantly better than DT (method 1), PSQ lexical ta- ble (method 2) and PSQ n-best table (method 3).", "labels": [], "entities": [{"text": "MAP", "start_pos": 10, "end_pos": 13, "type": "METRIC", "confidence": 0.782741904258728}, {"text": "PRES", "start_pos": 18, "end_pos": 22, "type": "METRIC", "confidence": 0.8838406801223755}, {"text": "Prefixed", "start_pos": 104, "end_pos": 112, "type": "METRIC", "confidence": 0.9897654056549072}]}, {"text": " Table 3: MAP and PRES scores of the aggregated mod- els on the development and test sets. Development scores  correspond to peaks in", "labels": [], "entities": [{"text": "MAP", "start_pos": 10, "end_pos": 13, "type": "METRIC", "confidence": 0.8439255952835083}, {"text": "PRES", "start_pos": 18, "end_pos": 22, "type": "METRIC", "confidence": 0.9084460139274597}]}, {"text": " Table 4: Examples of the features found by boosting.", "labels": [], "entities": []}]}