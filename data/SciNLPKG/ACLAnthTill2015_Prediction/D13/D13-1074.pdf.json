{"title": [{"text": "Joint Learning and Inference for Grammatical Error Correction", "labels": [], "entities": [{"text": "Grammatical Error Correction", "start_pos": 33, "end_pos": 61, "type": "TASK", "confidence": 0.7725600202878317}]}], "abstractContent": [{"text": "State-of-the-art systems for grammatical error correction are based on a collection of independently-trained models for specific errors.", "labels": [], "entities": [{"text": "grammatical error correction", "start_pos": 29, "end_pos": 57, "type": "TASK", "confidence": 0.7434453765551249}]}, {"text": "Such models ignore linguistic interactions at the sentence level and thus do poorly on mistakes that involve grammatical dependencies among several words.", "labels": [], "entities": []}, {"text": "In this paper, we identify linguistic structures with interacting grammatical properties and propose to address such dependencies via joint inference and joint learning.", "labels": [], "entities": []}, {"text": "We show that it is possible to identify interactions well enough to facilitate a joint approach and, consequently, that joint methods correct incoherent predictions that independently-trained classifiers tend to produce.", "labels": [], "entities": []}, {"text": "Furthermore , because the joint learning model considers interacting phenomena during training, it is able to identify mistakes that require making multiple changes simultaneously and that standard approaches miss.", "labels": [], "entities": []}, {"text": "Overall, our model significantly outperforms the Illinois system that placed first in the CoNLL-2013 shared task on grammatical error correction.", "labels": [], "entities": [{"text": "CoNLL-2013 shared task on grammatical error correction", "start_pos": 90, "end_pos": 144, "type": "TASK", "confidence": 0.562829600913184}]}], "introductionContent": [{"text": "There has recently been a lot of work addressing errors made by English as a Second Language (ESL) learners.", "labels": [], "entities": []}, {"text": "In the past two years, three competitions devoted to grammatical error correction for nonnative writers took place:), HOO-2012 (, and the CoNLL-2013 shared task ( ).", "labels": [], "entities": [{"text": "grammatical error correction", "start_pos": 53, "end_pos": 81, "type": "TASK", "confidence": 0.6824460625648499}, {"text": "HOO-2012", "start_pos": 118, "end_pos": 126, "type": "METRIC", "confidence": 0.9187451601028442}]}, {"text": "Nowadays *phone/phones *has/have many functionalities, *included/including *\u2205/a camera and *\u2205/a Wi-Fi receiver.", "labels": [], "entities": []}, {"text": "Most of the work in the area of ESL error correction has addressed the task by building statistical models that specialize in correcting a specific type of a mistake.", "labels": [], "entities": [{"text": "ESL error correction", "start_pos": 32, "end_pos": 52, "type": "TASK", "confidence": 0.9175254503885905}]}, {"text": "illustrates several types of errors common among non-native speakers of English: article, subject-verb agreement, noun number, and verb form.", "labels": [], "entities": []}, {"text": "A significant proportion of research has focused on correcting mistakes in article and preposition usage (.", "labels": [], "entities": [{"text": "correcting mistakes in article and preposition usage", "start_pos": 52, "end_pos": 104, "type": "TASK", "confidence": 0.5964799523353577}]}, {"text": "Several studies also consider verb-related and noun-related errors ().", "labels": [], "entities": []}, {"text": "The predictions made by individual models are then applied independently ) or pipelined.", "labels": [], "entities": []}, {"text": "The standard approach of training individual classifiers considers each word independently and thus assumes that there are no interactions between errors and between grammatical phenomena.", "labels": [], "entities": []}, {"text": "But an ESL writer may make multiple mistakes in a single sentence and these result in misleading local cues given to individual classifiers.", "labels": [], "entities": []}, {"text": "In the example shown in, the agreement error on the verb \"have\" interacts with the noun number error: a correction system that takes into account the context may infer, because of the word \"phone\", that the verb number is correct.", "labels": [], "entities": []}, {"text": "For this reason, a system that consid-ers noun and agreement errors separately will fail to identify and correct the interacting errors shown in.", "labels": [], "entities": []}, {"text": "Furthermore, it may also produce inconsistent predictions.", "labels": [], "entities": []}, {"text": "Even though it is quite clear that grammatical errors interact, for various conceptual and technical reasons, this issue has not been addressed in a significant way in the literature.", "labels": [], "entities": []}, {"text": "We believe that the reasons for that are three-fold: (1) Data: until very recently we did not have data that jointly annotates sufficiently many errors of interacting phenomena (see Sec. 2).", "labels": [], "entities": []}, {"text": "(2) Conceptual: Correcting errors in interacting linguistic phenomena requires that one identifies those phenomena and, more importantly, can recognize reliably the interacting components (e.g., given a verb, identify the subject to enable enforcing agreement).", "labels": [], "entities": []}, {"text": "The perception has been that this cannot be done reliably", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we describe our experimental setup and evaluate the performance of the joint approach.", "labels": [], "entities": []}, {"text": "In the joint approach, the joint components presented in Sec.", "labels": [], "entities": []}, {"text": "5 handle the interacting structures described in Sec.", "labels": [], "entities": []}, {"text": "4. The individual classifiers of the Illinois system make predictions for the remaining words.", "labels": [], "entities": [{"text": "Illinois system", "start_pos": 37, "end_pos": 52, "type": "DATASET", "confidence": 0.8922514915466309}]}, {"text": "The research question addressed by the experiments is the following: Given independentlytrained systems for different types of errors, can we improve the performance by considering the phe-nomena that interact jointly?", "labels": [], "entities": []}, {"text": "To address this, we report the results in the following settings: 1.", "labels": [], "entities": []}, {"text": "Joint Inference: we compare the Illinois system that is a collection of individually-trained models that are applied independently with a model that uses joint inference encoded as declarative constraints in the ILP formulation and show that using joint inference results in a strong performance gain.", "labels": [], "entities": []}, {"text": "2. Joint Learning: we compare the Illinois system with a model that incorporates jointly-trained components for the two linguistic structures that we described in Sec.", "labels": [], "entities": [{"text": "Illinois system", "start_pos": 34, "end_pos": 49, "type": "DATASET", "confidence": 0.8768159747123718}]}, {"text": "4. We show that joint training produces an even stronger gain in performance compared to the Illinois model.", "labels": [], "entities": []}, {"text": "2. Joint Learning and Inference: we apply joint inference to the output of the joint learning system to account for dependencies not covered by the joint learning model.", "labels": [], "entities": []}, {"text": "We report F1 performance scored using the official scorer from the shared task).", "labels": [], "entities": [{"text": "F1", "start_pos": 10, "end_pos": 12, "type": "METRIC", "confidence": 0.9949097037315369}]}, {"text": "The task reports two types of evaluation: on the original gold data and on gold data with additional corrections.", "labels": [], "entities": []}, {"text": "We refer to the results as Original and Revised.", "labels": [], "entities": []}, {"text": "shows the results of applying joint inference to the Illinois system.", "labels": [], "entities": [{"text": "Illinois system", "start_pos": 53, "end_pos": 68, "type": "DATASET", "confidence": 0.9473885297775269}]}, {"text": "Both the article-NPhead and the subject-verb constraints improve the performance.", "labels": [], "entities": []}, {"text": "The results for the joint inference are shown in two settings, adjacent and all structures, so that later we can compare joint inference with the joint learning model that handles only adjacent structures.", "labels": [], "entities": []}, {"text": "Illinois-NBArticle denotes the Illinois system, where the discriminative article model is replaced with a NB classifier.", "labels": [], "entities": [{"text": "Illinois-NBArticle", "start_pos": 0, "end_pos": 18, "type": "DATASET", "confidence": 0.9363561868667603}]}, {"text": "Adjacent denotes a setting, where the structure components are consecutive (article-NPhead or subject-verb), as described in Sec.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 5: Accuracy of subject identification on a random", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9932010769844055}, {"text": "subject identification", "start_pos": 22, "end_pos": 44, "type": "TASK", "confidence": 0.655397355556488}]}, {"text": " Table 6: Accuracy of NP head identification on a random", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9901076555252075}, {"text": "NP head identification", "start_pos": 22, "end_pos": 44, "type": "TASK", "confidence": 0.7020236253738403}]}, {"text": " Table 7: Joint Inference Results. All results are on the CoNLL-2013 test data using the original and revised gold annotations.", "labels": [], "entities": [{"text": "CoNLL-2013 test data", "start_pos": 58, "end_pos": 78, "type": "DATASET", "confidence": 0.9777108828226725}]}, {"text": " Table 8: Joint Learning Results. All results are on the CoNLL-2013 test data using the original and revised gold annotations.", "labels": [], "entities": [{"text": "CoNLL-2013 test data", "start_pos": 57, "end_pos": 77, "type": "DATASET", "confidence": 0.980472187201182}]}, {"text": " Table 10: Examples of mistakes that are corrected by the joint model but not by the Illinois model. Illinois denotes the result", "labels": [], "entities": [{"text": "Illinois", "start_pos": 101, "end_pos": 109, "type": "METRIC", "confidence": 0.8108453750610352}]}, {"text": " Table 9: Joint Learning and Inference. All results are on the", "labels": [], "entities": [{"text": "Joint Learning and Inference", "start_pos": 10, "end_pos": 38, "type": "TASK", "confidence": 0.615491695702076}]}, {"text": " Table 11: Evaluation of the joint learning performance on", "labels": [], "entities": []}]}