{"title": [{"text": "Inducing Document Plans for Concept-to-text Generation", "labels": [], "entities": []}], "abstractContent": [{"text": "Ina language generation system, a content planner selects which elements must be included in the output text and the ordering between them.", "labels": [], "entities": []}, {"text": "Recent empirical approaches perform content selection without any ordering and have thus no means to ensure that the output is coherent.", "labels": [], "entities": [{"text": "content selection", "start_pos": 36, "end_pos": 53, "type": "TASK", "confidence": 0.7654280662536621}]}, {"text": "In this paper we focus on the problem of generating text from a database and present a trainable end-to-end generation system that includes both content selection and ordering.", "labels": [], "entities": []}, {"text": "Content plans are represented intuitively by a set of grammar rules that operate on the document level and are acquired automatically from training data.", "labels": [], "entities": []}, {"text": "We develop two approaches: the first one is inspired from Rhetorical Structure Theory and represents the document as a tree of discourse relations between database records; the second one requires little linguistic sophistication and uses tree structures to represent global patterns of database record sequences within a document.", "labels": [], "entities": [{"text": "Rhetorical Structure Theory", "start_pos": 58, "end_pos": 85, "type": "TASK", "confidence": 0.7024221022923788}]}, {"text": "Experimental evaluation on two domains yields considerable improvements over the state of the art for both approaches.", "labels": [], "entities": []}], "introductionContent": [{"text": "Concept-to-text generation broadly refers to the task of automatically producing textual output from nonlinguistic input).", "labels": [], "entities": [{"text": "Concept-to-text generation", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.6932921558618546}]}, {"text": "Depending on the application and the domain at hand, the input may assume various representations including databases, expert system knowledge bases, simulations of physical systems, or formal meaning representations.", "labels": [], "entities": []}, {"text": "Generation systems typically follow a pipeline architecture consisting of three components: content planning (selecting and ordering the parts of the input to be mentioned in the output text), sentence planning (determining the structure and lexical content of individual sentences), and surface realization (verbalizing the chosen content in natural language).", "labels": [], "entities": [{"text": "sentence planning", "start_pos": 193, "end_pos": 210, "type": "TASK", "confidence": 0.7049777507781982}, {"text": "surface realization", "start_pos": 288, "end_pos": 307, "type": "TASK", "confidence": 0.7538011968135834}]}, {"text": "Traditionally, these components are hand-engineered in order to ensure output of high quality.", "labels": [], "entities": []}, {"text": "More recently there has been growing interest in the application of learning methods because of their promise to make generation more robust and adaptable.", "labels": [], "entities": []}, {"text": "Examples include learning which content should be present in a document (;, how it should be aligned to utterances, and how to select a sentence plan among many alternatives).", "labels": [], "entities": []}, {"text": "Beyond isolated components, a few approaches have emerged that tackle concept-to-text generation end-to-end.", "labels": [], "entities": []}, {"text": "Due to the complexity of the task, most models simplify the generation process, e.g., by treating sentence planning and surface realization as one component, by implementing content selection without any document planning (, or by eliminating content planning entirely.", "labels": [], "entities": [{"text": "sentence planning and surface realization", "start_pos": 98, "end_pos": 139, "type": "TASK", "confidence": 0.7232328116893768}]}, {"text": "In this paper we present a trainable end-to-end generation system that captures all components of the traditional pipeline, including document planning.", "labels": [], "entities": [{"text": "document planning", "start_pos": 134, "end_pos": 151, "type": "TASK", "confidence": 0.7388360798358917}]}, {"text": "Rather than breaking up the generation process into a sequence of local decisions, each learned separately (, our model performs content planning (i.e., document planning and content selection), sentence planning (i.e., lex-Database Records temp(time:6-21, min:9, mean:15, max:21) wind-spd(time:6-21, min:15, mean:20, max:30) sky-cover(time:6-9, percent:25-50) sky-cover(time:9-12, percent:50-75) wind-dir(time:6-21, mode:SSE) gust(time: Output Text Cloudy, with a high around 20.", "labels": [], "entities": [{"text": "sentence planning", "start_pos": 195, "end_pos": 212, "type": "TASK", "confidence": 0.681193396449089}]}, {"text": "South southeast wind between 15 and 30 mph.", "labels": [], "entities": []}, {"text": "Gusts as high as 40 mph.", "labels": [], "entities": [{"text": "Gusts", "start_pos": 0, "end_pos": 5, "type": "METRIC", "confidence": 0.9784123301506042}]}], "datasetContent": [{"text": "Data Since our aim was to evaluate the planning component of our model, we used datasets whose documents are at least a few sentences long.", "labels": [], "entities": []}, {"text": "Specifically, we generated weather forecasts and troubleshooting guides for an operating system.", "labels": [], "entities": []}, {"text": "For the first domain (henceforth WEATHERGOV) we used the dataset of, which consists of 29,528 weather scenarios for 3,753 major US cities (collected over four days).", "labels": [], "entities": []}, {"text": "The database has 12 record types, each scenario contains on average 36 records, 5.8 out of which are mentioned in the text.", "labels": [], "entities": []}, {"text": "A document has 29.3 words and is four sentences long.", "labels": [], "entities": []}, {"text": "The vocabulary is 345 words.", "labels": [], "entities": []}, {"text": "We used 25,000 scenarios from WEATHERGOV for training, 1,000 scenarios for development and 3,528 scenarios for testing.", "labels": [], "entities": [{"text": "WEATHERGOV", "start_pos": 30, "end_pos": 40, "type": "DATASET", "confidence": 0.920509934425354}]}, {"text": "For the second domain (henceforth WINHELP) we used the dataset of, which consists of 128 scenarios.", "labels": [], "entities": []}, {"text": "These are articles from Microsoft's Help and Support website 4 and contain step-by-step instructions on how to perform tasks on the Windows 2000 operating system.", "labels": [], "entities": []}, {"text": "In its original format, the database provides a semantic representation of the textual guide, i.e., it represents the user's actions on the operating system's UI.", "labels": [], "entities": []}, {"text": "We semiautomatically converted this representation into a schema of records, fields and values, following the conventions adopted in.", "labels": [], "entities": []}, {"text": "The final database has 13 record types.", "labels": [], "entities": []}, {"text": "Each scenario has 9.2 records and each document 51.92 words with 4.3 sentences.", "labels": [], "entities": []}, {"text": "The vocabulary is 629 words.", "labels": [], "entities": []}, {"text": "We performed 10-fold cross-validation on the entire dataset for training and testing.", "labels": [], "entities": []}, {"text": "Compared to WEATHER-GOV, WINHELP documents are longer with a larger vocabulary.", "labels": [], "entities": []}, {"text": "More importantly, due to the nature of the domain, i.e., giving instructions, content selection is critical not only in terms of what to say but also in what order.", "labels": [], "entities": [{"text": "content selection", "start_pos": 78, "end_pos": 95, "type": "TASK", "confidence": 0.6883000284433365}]}], "tableCaptions": [{"text": " Table 2: Automatic evaluation of system output us- ing BLEU-4.", "labels": [], "entities": [{"text": "BLEU-4", "start_pos": 56, "end_pos": 62, "type": "METRIC", "confidence": 0.9946681261062622}]}, {"text": " Table 3: Mean ratings for fluency (FL), semantic  correctness (SC) and coherence (CO) on system out- put elicited by humans.", "labels": [], "entities": [{"text": "fluency (FL)", "start_pos": 27, "end_pos": 39, "type": "METRIC", "confidence": 0.6783667728304863}, {"text": "semantic  correctness (SC)", "start_pos": 41, "end_pos": 67, "type": "METRIC", "confidence": 0.8210830569267273}, {"text": "coherence (CO)", "start_pos": 72, "end_pos": 86, "type": "METRIC", "confidence": 0.8266628235578537}]}]}