{"title": [{"text": "Towards Situated Dialogue: Revisiting Referring Expression Generation", "labels": [], "entities": [{"text": "Revisiting Referring Expression", "start_pos": 27, "end_pos": 58, "type": "TASK", "confidence": 0.9062966108322144}]}], "abstractContent": [{"text": "In situated dialogue, humans and agents have mismatched capabilities of perceiving the shared environment.", "labels": [], "entities": []}, {"text": "Their representations of the shared world are misaligned.", "labels": [], "entities": []}, {"text": "Thus referring expression generation (REG) will need to take this discrepancy into consideration.", "labels": [], "entities": [{"text": "referring expression generation (REG", "start_pos": 5, "end_pos": 41, "type": "TASK", "confidence": 0.7554281234741211}]}, {"text": "To address this issue, we developed a hypergraph-based approach to account for group-based spatial relations and uncertainties in perceiving the environment.", "labels": [], "entities": []}, {"text": "Our empirical results have shown that this approach outperforms a previous graph-based approach with an absolute gain of 9%.", "labels": [], "entities": []}, {"text": "However, while these graph-based approaches perform effectively when the agent has perfect knowledge or perception of the environment (e.g., 84%), they perform rather poorly when the agent has imperfect perception of the environment (e.g., 45%).", "labels": [], "entities": []}, {"text": "This big performance gap calls for new solutions to REG that can mediate a shared perceptual basis in situated dialogue.", "labels": [], "entities": []}], "introductionContent": [{"text": "Situated human robot dialogue has received increasing attention in recent years.", "labels": [], "entities": [{"text": "Situated human robot dialogue", "start_pos": 0, "end_pos": 29, "type": "TASK", "confidence": 0.7289236336946487}]}, {"text": "In situated dialogue, robots/artificial agents and their human partners are co-present in a shared physical world.", "labels": [], "entities": []}, {"text": "Robots need to automatically perceive and make inference of the shared environment.", "labels": [], "entities": []}, {"text": "Due to its limited perceptual and reasoning capabilities, the robot's representation of the shared world is often incomplete, error-prone, and significantly mismatched from that of its human partner's.", "labels": [], "entities": []}, {"text": "Although physically co-present, a joint perceptual basis between the human and the robot cannot be established).", "labels": [], "entities": []}, {"text": "Thus, referential communication between the human and the robot becomes difficult.", "labels": [], "entities": [{"text": "referential communication", "start_pos": 6, "end_pos": 31, "type": "TASK", "confidence": 0.9648417830467224}]}, {"text": "How this mismatched perceptual basis affects referential communication in situated dialogue was investigated in our previous work (.", "labels": [], "entities": []}, {"text": "In that work, the main focus is on reference resolution: given referential descriptions from human partners, how to identify referents in the environment even though the robot only has imperfect perception of the environment.", "labels": [], "entities": [{"text": "reference resolution", "start_pos": 35, "end_pos": 55, "type": "TASK", "confidence": 0.8054308295249939}]}, {"text": "Since robots need to collaborate with human partners to establish a joint perceptual basis, referring expression generation (REG) becomes an equally important problem in situated dialogue.", "labels": [], "entities": [{"text": "referring expression generation (REG)", "start_pos": 92, "end_pos": 129, "type": "TASK", "confidence": 0.8029985825220743}]}, {"text": "Robots have much lower perceptual capabilities of the environment than humans.", "labels": [], "entities": []}, {"text": "How can a robot effectively generate referential descriptions about the environment so that its human partner can understand which objects are being referred to?", "labels": [], "entities": []}, {"text": "There has been a tremendous amount of work on referring expression generation in the last two decades.", "labels": [], "entities": [{"text": "referring expression generation", "start_pos": 46, "end_pos": 77, "type": "TASK", "confidence": 0.8601232767105103}]}, {"text": "However, most existing REG algorithms were developed and evaluated under the assumption that agents and humans have access to the same kind of domain information.", "labels": [], "entities": []}, {"text": "For example, many experimental setups ( were developed based on a visual world for which the internal representation is assumed to be known and can be represented symbolically.", "labels": [], "entities": []}, {"text": "However, this assumption no longer holds in situated dialogue with robots.", "labels": [], "entities": []}, {"text": "There are two important distinctions in situated dialogue.", "labels": [], "entities": []}, {"text": "First, the perfect knowledge of the environment is not available to the agent ahead of time.", "labels": [], "entities": []}, {"text": "The agent needs to automatically make inferences to connect recognized lower-level visual features with symbolic labels or descriptors.", "labels": [], "entities": []}, {"text": "Both recognition and inference are error-prone and full of uncertainties.", "labels": [], "entities": [{"text": "recognition", "start_pos": 5, "end_pos": 16, "type": "TASK", "confidence": 0.9668744802474976}]}, {"text": "Second, in situated dialogue the agent and the human have mismatched representations of the environment.", "labels": [], "entities": []}, {"text": "The agent needs to take this difference into consideration to identify the most reliable features for REG.", "labels": [], "entities": [{"text": "REG", "start_pos": 102, "end_pos": 105, "type": "TASK", "confidence": 0.9000443816184998}]}, {"text": "Given these two distinctions, it is not clear whether state-of-the-art REG approaches are applicable under mismatched perceptual basis in situated dialogue.", "labels": [], "entities": []}, {"text": "To address this issue, this paper revisits the problem of REG in the context of mismatched perceptual basis.", "labels": [], "entities": [{"text": "REG", "start_pos": 58, "end_pos": 61, "type": "TASK", "confidence": 0.9563758969306946}]}, {"text": "We extended a well known graph-based approach () that has shown to be effective in previous work).", "labels": [], "entities": []}, {"text": "We incorporated uncertainties in perception into cost functions.", "labels": [], "entities": []}, {"text": "We further extended regular graph representation into hypergraph representation to account for group-based spatial relations that are important for visual descriptions;.", "labels": [], "entities": [{"text": "regular graph representation", "start_pos": 20, "end_pos": 48, "type": "TASK", "confidence": 0.7037461002667745}]}, {"text": "Our empirical results demonstrate that both enhancements lead to about a 9% absolute performance gain compared to the original approach.", "labels": [], "entities": []}, {"text": "However, while our approache performs effectively when the agent has perfect knowledge or perception of the environment (e.g., 84%), it performs poorly under the mismatched perceptual basis (e.g., 45%).", "labels": [], "entities": []}, {"text": "This performance gap calls for new solutions for REG that are capable of mediating mismatched perceptual basis.", "labels": [], "entities": [{"text": "REG", "start_pos": 49, "end_pos": 52, "type": "TASK", "confidence": 0.9253024458885193}]}, {"text": "In the following sections, we first describe our hypergraph-based representations and illustrate how uncertainties from automated perception can be incorporated.", "labels": [], "entities": []}, {"text": "We then describe an empirical study using Amazon Mechanical Turks for evaluating generated referring expressions.", "labels": [], "entities": [{"text": "Amazon Mechanical Turks", "start_pos": 42, "end_pos": 65, "type": "DATASET", "confidence": 0.9523965915044149}]}, {"text": "Finally we present evaluation results and discuss potential future directions.", "labels": [], "entities": []}], "datasetContent": [{"text": "To evaluate the performance of this hypergraphbased approach to REG, we conducted a comparative study using crowd-sourcing.", "labels": [], "entities": [{"text": "REG", "start_pos": 64, "end_pos": 67, "type": "TASK", "confidence": 0.9673385620117188}]}, {"text": "More specifically, we created 48 different scenes similar to that in.", "labels": [], "entities": []}, {"text": "Each scene has 13 objects on average and there are 621 objects in total.", "labels": [], "entities": []}, {"text": "For each of these scenes, we applied a CV algorithm () and generated scene hypergraphs as described in Section 3.1.", "labels": [], "entities": []}, {"text": "We then use different generation strategies (varied in terms of graph representations and cost functions, to be explained in Section 4.2) to automatically generate referring expressions to refer to each object.", "labels": [], "entities": []}, {"text": "To evaluate the quality of these generated referring expressions, we applied Amazon Mechanical Turk to solicit feedback from the crowd 2 . Through an interface, we displayed an original scene and generated referring expressions (from different generation strategies) in a random order.", "labels": [], "entities": [{"text": "Amazon Mechanical Turk", "start_pos": 77, "end_pos": 99, "type": "DATASET", "confidence": 0.9025060733159384}]}, {"text": "We asked each turk to select the object in the scene that he/she believed was the one referred to by the shown referring expression (i.e., reference identification task).", "labels": [], "entities": [{"text": "reference identification task", "start_pos": 139, "end_pos": 168, "type": "TASK", "confidence": 0.7543293734391531}]}, {"text": "Each referring expression received three votes from the crowd.", "labels": [], "entities": []}, {"text": "In total, 217 turks participated in our experiment.", "labels": [], "entities": []}, {"text": "As mentioned earlier, each generated referring expression received three independent votes regarding its referent from the crowd.", "labels": [], "entities": []}, {"text": "The referent with the most votes is taken as the predicted referent and is used for evaluation.", "labels": [], "entities": []}, {"text": "If all three votes are differ-: Results with different cost functions ent, then by default, it is deemed that the referent is not correctly identified for that expression.", "labels": [], "entities": []}, {"text": "We use the accuracy of the referential identification task (i.e., the percentage of generated referring expressions where the referents are correctly identified) as the metric to evaluate different generation strategies illustrated in Section 4.2.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 11, "end_pos": 19, "type": "METRIC", "confidence": 0.9985204339027405}, {"text": "referential identification task", "start_pos": 27, "end_pos": 58, "type": "TASK", "confidence": 0.85494464635849}]}, {"text": "First, when the agent does not have perfect knowledge of the environment and has to automatically infer the environment as in our setting here, cost functions based on uncertainties of perception lead to better results.", "labels": [], "entities": []}, {"text": "This occurs for both regular graphs and hypergraphs.", "labels": [], "entities": []}, {"text": "This result is not surprising and indicates that cost functions should be tied to the agent's ability to perceive and infer the environment.", "labels": [], "entities": []}, {"text": "The uncertainty based cost functions allow the agent to prefer reliable attributes or relations.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Results with different cost functions", "labels": [], "entities": []}, {"text": " Table 2: Results of comparing perfect perception and im- perfect perception of the shared world.", "labels": [], "entities": []}, {"text": " Table 3: Results of comparing minimum effort and extra  effort using hypergraphs", "labels": [], "entities": []}]}