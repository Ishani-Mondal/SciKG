{"title": [{"text": "Semantic Parsing on Freebase from Question-Answer Pairs", "labels": [], "entities": [{"text": "Semantic Parsing", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.824054628610611}]}], "abstractContent": [{"text": "In this paper, we train a semantic parser that scales up to Freebase.", "labels": [], "entities": [{"text": "Freebase", "start_pos": 60, "end_pos": 68, "type": "DATASET", "confidence": 0.9720728993415833}]}, {"text": "Instead of relying on annotated logical forms, which is especially expensive to obtain at large scale, we learn from question-answer pairs.", "labels": [], "entities": []}, {"text": "The main challenge in this setting is narrowing down the huge number of possible logical predicates fora given question.", "labels": [], "entities": []}, {"text": "We tackle this problem in two ways: First, we build a coarse mapping from phrases to predicates using a knowledge base and a large text corpus.", "labels": [], "entities": []}, {"text": "Second, we use a bridging operation to generate additional predicates based on neighboring predicates.", "labels": [], "entities": []}, {"text": "On the dataset of Cai and Yates (2013), despite not having annotated logical forms, our system outperforms their state-of-the-art parser.", "labels": [], "entities": []}, {"text": "Additionally, we collected a more realistic and challenging dataset of question-answer pairs and improves over a natural baseline.", "labels": [], "entities": []}], "introductionContent": [{"text": "We focus on the problem of semantic parsing natural language utterances into logical forms that can be executed to produce denotations.", "labels": [], "entities": [{"text": "semantic parsing natural language utterances into logical forms", "start_pos": 27, "end_pos": 90, "type": "TASK", "confidence": 0.8672466278076172}]}, {"text": "Traditional semantic parsers) have two limitations: (i) they require annotated logical forms as supervision, and (ii) they operate in limited domains with a small number of logical predicates.", "labels": [], "entities": []}, {"text": "Recent developments aim to lift these limitations, either by reducing the amount of supervision (: Our task is to map questions to answers via latent logical forms.", "labels": [], "entities": []}, {"text": "To narrow down the space of logical predicates, we use a (i) coarse alignment based on Freebase and a text corpus and (ii) a bridging operation that generates predicates compatible with neighboring predicates. predicates.", "labels": [], "entities": []}, {"text": "The goal of this paper is to do both: learn a semantic parser without annotated logical forms that scales to the large number of predicates on Freebase.", "labels": [], "entities": []}, {"text": "At the lexical level, a major challenge in semantic parsing is mapping natural language phrases (e.g., \"attend\") to logical predicates (e.g., Education).", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 43, "end_pos": 59, "type": "TASK", "confidence": 0.7354132831096649}]}, {"text": "While limited-domain semantic parsers are able to learn the lexicon from per-example supervision, at large scale they have inadequate coverage.", "labels": [], "entities": []}, {"text": "Previous work on semantic parsing on Freebase uses a combination of manual rules (, distant supervision (, and schema matching.", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 17, "end_pos": 33, "type": "TASK", "confidence": 0.8089990019798279}]}, {"text": "We use a large amount of web text and a knowledge base to build a coarse alignment between phrases and predicatesan approach similar in spirit to.", "labels": [], "entities": []}, {"text": "However, this alignment only allows us to generate a subset of the desired predicates.", "labels": [], "entities": []}, {"text": "Aligning light verbs (e.g., \"go\") and prepositions is not very informative due to polysemy, and rare predicates (e.g., \"cover price\") are difficult to cover even given a large corpus.", "labels": [], "entities": [{"text": "Aligning light verbs", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.849456787109375}]}, {"text": "To improve coverage, we propose anew bridging operation that generates predicates based on adjacent predicates rather than on words.", "labels": [], "entities": []}, {"text": "At the compositional level, a semantic parser must combine the predicates into a coherent logical form.", "labels": [], "entities": []}, {"text": "Previous work based on CCG requires manually specifying combination rules or inducing the rules from annotated logical forms (.", "labels": [], "entities": []}, {"text": "We instead define a few simple composition rules which over-generate and then use model features to simulate soft rules and categories.", "labels": [], "entities": []}, {"text": "In particular, we use POS tag features and features on the denotations of the predicted logical forms.", "labels": [], "entities": []}, {"text": "We experimented with two question answering datasets on Freebase.", "labels": [], "entities": [{"text": "question answering", "start_pos": 25, "end_pos": 43, "type": "TASK", "confidence": 0.7633779346942902}, {"text": "Freebase", "start_pos": 56, "end_pos": 64, "type": "DATASET", "confidence": 0.5324169993400574}]}, {"text": "First, on the dataset of, we showed that our system outperforms their state-of-the-art system 62% to 59%, despite using no annotated logical forms.", "labels": [], "entities": []}, {"text": "Second, we collected anew realistic dataset of questions by performing a breadth-first search using the Google Suggest API; these questions are then answered by Amazon Mechanical Turk workers.", "labels": [], "entities": []}, {"text": "Although this dataset is much more challenging and noisy, we are still able to achieve 31.4% accuracy, a 4.5% absolute improvement over a natural baseline.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 93, "end_pos": 101, "type": "METRIC", "confidence": 0.9995571970939636}]}, {"text": "Both datasets as well as the source code for SEMPRE, our semantic parser, are publicly released and can be downloaded from http://nlp.stanford.edu/ software/sempre/.", "labels": [], "entities": []}], "datasetContent": [{"text": "We now evaluate our semantic parser empirically.", "labels": [], "entities": [{"text": "semantic parser", "start_pos": 20, "end_pos": 35, "type": "TASK", "confidence": 0.7340661585330963}]}, {"text": "In Section 4.1, we compare our approach to on their recently released dataset (henceforth, FREE917) and present results on anew dataset that we collected (henceforth, WEBQUES-TIONS).", "labels": [], "entities": [{"text": "FREE917", "start_pos": 91, "end_pos": 98, "type": "METRIC", "confidence": 0.6600989103317261}, {"text": "WEBQUES-TIONS", "start_pos": 167, "end_pos": 180, "type": "DATASET", "confidence": 0.7425178289413452}]}, {"text": "In Section 4.2, we provide detailed experiments to provide additional insight on our system.", "labels": [], "entities": []}, {"text": "Setup We implemented a standard beam-based bottom-up parser which stores the k-best derivations for each span.", "labels": [], "entities": []}, {"text": "We use k = 500 for all our experiments on FREE917 and k = 200 on WEBQUES-TIONS.", "labels": [], "entities": [{"text": "FREE917", "start_pos": 42, "end_pos": 49, "type": "DATASET", "confidence": 0.9461814165115356}, {"text": "WEBQUES-TIONS", "start_pos": 65, "end_pos": 78, "type": "DATASET", "confidence": 0.9024090766906738}]}, {"text": "The root beam yields the candidate set\u02dcDset\u02dc set\u02dcD(x) and is used to approximate the sum in the objective function O(\u03b8) in.", "labels": [], "entities": []}, {"text": "In experiments on WEBQUES-TIONS, \u02dc D(x) contained 197 derivations on average.", "labels": [], "entities": [{"text": "WEBQUES-TIONS", "start_pos": 18, "end_pos": 31, "type": "DATASET", "confidence": 0.8447141647338867}, {"text": "D", "start_pos": 35, "end_pos": 36, "type": "METRIC", "confidence": 0.6971530914306641}]}, {"text": "We write the approximate objective as O(\u03b8; itly show dependence on the parameters\u02dc\u03b8parameters\u02dc parameters\u02dc\u03b8 used for beam search.", "labels": [], "entities": [{"text": "O", "start_pos": 38, "end_pos": 39, "type": "METRIC", "confidence": 0.9963257908821106}, {"text": "beam search", "start_pos": 117, "end_pos": 128, "type": "TASK", "confidence": 0.9116339981555939}]}, {"text": "We optimize the objective by initializing \u03b8 0 to 0 and applying AdaGrad (stochastic gradient ascent with per-feature adaptive step size control) (, so that \u03b8 t+1 is set based on taking a stochastic approximation of \u2202O(\u03b8;\u03b8t) \u2202\u03b8 \u03b8=\u03b8t . We make six passes over the training examples.", "labels": [], "entities": [{"text": "AdaGrad", "start_pos": 64, "end_pos": 71, "type": "DATASET", "confidence": 0.6189960837364197}]}, {"text": "We used POS tagging and named-entity recognition to restrict what phrases in the utterance could be mapped by the lexicon.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 8, "end_pos": 19, "type": "TASK", "confidence": 0.7170718312263489}, {"text": "named-entity recognition", "start_pos": 24, "end_pos": 48, "type": "TASK", "confidence": 0.7442166209220886}]}, {"text": "Entities must be named entities, proper nouns or a sequence of at least two tokens.", "labels": [], "entities": []}, {"text": "Unaries must be a sequence of nouns, and binaries must be either a content word, or a verb followed by either a noun phrase or a particle.", "labels": [], "entities": []}, {"text": "In addition, we used 17 hand-written rules to map question words such as \"where\" and \"how many\" to logical forms such as Type.Location and Count.", "labels": [], "entities": [{"text": "Count", "start_pos": 139, "end_pos": 144, "type": "METRIC", "confidence": 0.994147539138794}]}, {"text": "To compute denotations, we convert a logical form z into a SPARQL query and execute it on our copy of Freebase using the Virtuoso engine.", "labels": [], "entities": []}, {"text": "On WEBQUESTIONS, a full run over the training examples involves approximately 600,000 queries.", "labels": [], "entities": [{"text": "WEBQUESTIONS", "start_pos": 3, "end_pos": 15, "type": "DATASET", "confidence": 0.7905389666557312}]}, {"text": "For evaluation, we predict the answer from the derivation with highest probability.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: Statistics on various semantic parsing datasets. Our", "labels": [], "entities": [{"text": "semantic parsing", "start_pos": 32, "end_pos": 48, "type": "TASK", "confidence": 0.7344534993171692}]}, {"text": " Table 4: Accuracies on the development set under different", "labels": [], "entities": [{"text": "Accuracies", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9964339733123779}]}, {"text": " Table 5: Accuracies on the development set with features re-", "labels": [], "entities": [{"text": "Accuracies", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9952157735824585}]}, {"text": " Table 6: Accuracies on the development set using either", "labels": [], "entities": [{"text": "Accuracies", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9953216910362244}]}]}