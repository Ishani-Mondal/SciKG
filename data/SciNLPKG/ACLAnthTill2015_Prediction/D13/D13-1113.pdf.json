{"title": [{"text": "Identifying Multiple Userids of the Same Author", "labels": [], "entities": [{"text": "Identifying Multiple Userids", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.88645072778066}]}], "abstractContent": [{"text": "This paper studies the problem of identifying users who use multiple userids to post in social media.", "labels": [], "entities": []}, {"text": "Since multiple userids may belong to the same author, it is hard to directly apply supervised learning to solve the problem.", "labels": [], "entities": []}, {"text": "This paper proposes anew method, which still uses supervised learning but does not require training documents from the involved userids.", "labels": [], "entities": []}, {"text": "Instead , it uses documents from other userids for classifier building.", "labels": [], "entities": [{"text": "classifier building", "start_pos": 51, "end_pos": 70, "type": "TASK", "confidence": 0.8775393068790436}]}, {"text": "The classifier can be applied to documents of the involved userids.", "labels": [], "entities": []}, {"text": "This is possible because we transform the document space to a similarity space and learning is performed in this new space.", "labels": [], "entities": []}, {"text": "Our evaluation is done in the online review domain.", "labels": [], "entities": []}, {"text": "The experimental results using a large number of userids and their reviews show that the proposed method is highly effective.", "labels": [], "entities": []}], "introductionContent": [{"text": "It is common knowledge that some users in social media register multiple accounts/userids to post articles, blogs, reviews, etc.", "labels": [], "entities": []}, {"text": "There are many reasons for doing this.", "labels": [], "entities": []}, {"text": "For example, due to past postings, a user may become despised by others.", "labels": [], "entities": []}, {"text": "He/she then registers another userid in order to regain his/her status.", "labels": [], "entities": []}, {"text": "A user may also use multiple userids to instigate controversy or debates to popularize a topic to make it \"hot\" or even just to promote activities at a website.", "labels": [], "entities": []}, {"text": "Yet, a user may also use multiple userids to post fake or deceptive opinions to promote or demote some products.", "labels": [], "entities": []}, {"text": "It is thus important to develop technologies to identify such multi-id users.", "labels": [], "entities": []}, {"text": "This paper deals with this problem based on writing style and other linguistic clues.", "labels": [], "entities": []}, {"text": "Problem definition: Given a set of userids ID = {id 1 , \u2026, id n } and each id i has a set of documents Di , we want to identify userids that belong to the same physical author.", "labels": [], "entities": []}, {"text": "The main related works to ours are in the area of authorship attribution (AA), which aims to identify authors of documents.", "labels": [], "entities": [{"text": "of authorship attribution (AA)", "start_pos": 47, "end_pos": 77, "type": "TASK", "confidence": 0.65402119855086}]}, {"text": "AA is often solved using supervised learning.", "labels": [], "entities": [{"text": "AA", "start_pos": 0, "end_pos": 2, "type": "TASK", "confidence": 0.9841213226318359}]}, {"text": "Let A = {a 1 , \u2026, a k } be a set of authors (or classes) and each author a i \uf0ce A has a set of training documents Di . A classifier is then built to decide the author a of each test document d, where a \uf0ce A.", "labels": [], "entities": []}, {"text": "We will discuss this and other related works in Section 2.", "labels": [], "entities": []}, {"text": "This supervised AA formulation, however, is not suitable for our task because we only have userids but not real authors.", "labels": [], "entities": [{"text": "AA formulation", "start_pos": 16, "end_pos": 30, "type": "TASK", "confidence": 0.8905723392963409}]}, {"text": "Since some of the userids may belong to the same author, we cannot treat each userid as a class because in that case, we will be classifying based on userids, which won't help us find authors with multiple userids (see Section 7 also).", "labels": [], "entities": []}, {"text": "This paper proposes a novel algorithm.", "labels": [], "entities": []}, {"text": "To simplify the presentation, we assume that at most two userids can belong to a single author, but the algorithm can be extended to handle more than two userids from the same author.", "labels": [], "entities": []}, {"text": "Using this assumption, the algorithm works in two steps: 1.", "labels": [], "entities": []}, {"text": "Candidate identification: For each userid id i , we first find the most likely userid id j (i \u2260 j) that may have the same author as id i . We call id j the candidate of id i . We also call this function candid-iden, i.e., id j = candid-iden(id i ).", "labels": [], "entities": []}, {"text": "For easy presentation, here we only use one argument for candid-iden.", "labels": [], "entities": []}, {"text": "In the computation, it needs more arguments (see Section 4).", "labels": [], "entities": []}, {"text": "2. Candidate confirmation: In the reverse order, we apply the function candid-iden on id j , which produces id k , i.e., id k = candid-iden(id j ).", "labels": [], "entities": []}, {"text": "Decision making: If k = i, we conclude that id i and id j are from the same author.", "labels": [], "entities": [{"text": "Decision making", "start_pos": 0, "end_pos": 15, "type": "TASK", "confidence": 0.8132719993591309}]}, {"text": "Otherwise, id i and id j are not from the same author.", "labels": [], "entities": []}, {"text": "The key of the algorithm is candid-iden.", "labels": [], "entities": []}, {"text": "An obvious approach for candid-iden is to use an information retrieval method.", "labels": [], "entities": []}, {"text": "We can first split the documents Di of each id i into two subsets, a query set Q i and a sample set Si . We then compare each query document in Q i with each sample document in S j from other userids id j (\uf0ce ID -{id i }).", "labels": [], "entities": []}, {"text": "Cosine can be used here for similarity comparison.", "labels": [], "entities": []}, {"text": "All the similarity scores are then aggregated and used to rank the userids in ID -{id i }.", "labels": [], "entities": []}, {"text": "The top ranked userid is the candidate for id i . Note that partitioning the documents of a userid id i into the query set Q i and the sample set Si is crucial here.", "labels": [], "entities": []}, {"text": "We cannot use all documents in Di to compare with all documents in D j . If so and we get candid-iden(id i ) = id j , we will definitely get candid-iden(id j ) = id i since the similarity function is symmetric.", "labels": [], "entities": []}, {"text": "This cosine similarity based method, however, does notwork well (see Section 7).", "labels": [], "entities": []}, {"text": "We propose a supervised learning method to compute the scores.", "labels": [], "entities": []}, {"text": "For this, we need to reformulate the problem.", "labels": [], "entities": []}, {"text": "The idea of this reformulation is to learn in a similarity space rather than in the original document space as in traditional AA.", "labels": [], "entities": []}, {"text": "In the new formulation, each document dis still represented as a feature vector, but the vector no longer represents the document d itself.", "labels": [], "entities": []}, {"text": "Instead, it represents a set of similarities between the document d and a query q.", "labels": [], "entities": []}, {"text": "We call this method learning in the similarity space (LSS).", "labels": [], "entities": []}, {"text": "Specifically, in LSS, each document dis first represented with a document space vector (called a d-vector) based on the document itself as in the traditional classification learning of AA.", "labels": [], "entities": []}, {"text": "Each feature in the d-vector is called a d-feature (document-feature).", "labels": [], "entities": []}, {"text": "A query document q is represented in the same way.", "labels": [], "entities": []}, {"text": "We then produce a similarity vector sv (called s-vector) ford.", "labels": [], "entities": []}, {"text": "sv consists of a set of similarity values between document d (in a dvector) and query q (in a d-vector): where Sim is a similarity function consists of a set of similarity measures.", "labels": [], "entities": []}, {"text": "Thus, the d-vector for document din the document space is transformed to an s-vector sv ford in the similarity space.", "labels": [], "entities": []}, {"text": "Each feature in sv is called an s-feature.", "labels": [], "entities": []}, {"text": "For example, we have the following d-vector for query q: q: 1:1 2:1 6:2 where x:z represents a d-feature x (a word) and its frequency z in q.", "labels": [], "entities": []}, {"text": "We also have two non-query documents, one is d 1 which is written by the author of query q and the other is d 2 which is not written by query author q.", "labels": [], "entities": []}, {"text": "Their d-vectors are: If we use cosine as the first similarity measure in Sim, we can generate an s-feature 1:0.50 ford 1 (cosine(q, d 1 ) = 0.50) and an s-feature 1:0.27 ford 2 (cosine(q, d 2 ) = 0.27).", "labels": [], "entities": []}, {"text": "If we have more similarity measures more s-features can be produced.", "labels": [], "entities": []}, {"text": "The resulting two s-vectors ford 1 and d 2 with their class labels, 1 and -1, are as follows: Class 1 means \"written by author of query q\", also called q-positive, and class -1 means \"not written by author of query q\", also called q-negative.", "labels": [], "entities": []}, {"text": "LSS gives us a two-class classification problem.", "labels": [], "entities": [{"text": "LSS", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.8672990798950195}]}, {"text": "In this formulation, a test userid and his/her documents do not have to be seen in training as long as a set of known documents from this userid is available.", "labels": [], "entities": []}, {"text": "Any supervised learning method can be used to build a classifier.", "labels": [], "entities": []}, {"text": "The resulting classifier is employed to compute a score for each review to be used in the two-step algorithm above to find the candidate for each userid and then the userids with the same authors.", "labels": [], "entities": []}, {"text": "Due to the use of query documents, the LSS formulation has some resemblance to document ranking based on learning to rank.", "labels": [], "entities": []}, {"text": "However, LSS is very different because we turn the problem into a supervised classification problem.", "labels": [], "entities": [{"text": "LSS", "start_pos": 9, "end_pos": 12, "type": "TASK", "confidence": 0.5935881733894348}, {"text": "supervised classification problem", "start_pos": 66, "end_pos": 99, "type": "TASK", "confidence": 0.7406275073687235}]}, {"text": "The key difference between learning to rank and classification is that ranking will always put some documents at the top even if the desired documents do not exist.", "labels": [], "entities": []}, {"text": "However, classification will not return any document if the desired documents do not exist in the test data (unless there are classification errors).", "labels": [], "entities": []}, {"text": "Our Type II experiments in Section 7 were specifically designed for testing such non-existence situations.", "labels": [], "entities": []}, {"text": "Using online review as the application domain, we conduct experiments on a large number of reviews and their author/reviewer userids from Amazon.com.", "labels": [], "entities": []}, {"text": "The results show that the proposed algorithm is highly accurate and outperforms three strong baselines markedly.", "labels": [], "entities": []}], "datasetContent": [{"text": "We now evaluate the proposed approach and compare it with baselines.", "labels": [], "entities": []}, {"text": "All our experiments use the SVM perf classifier).", "labels": [], "entities": []}, {"text": "Experiment Data: We use a set of reviews and their authors/reviewers from Amazon.com as our experiment data.", "labels": [], "entities": []}, {"text": "We select the authors who have posted more than 30 reviews in the book category.", "labels": [], "entities": []}, {"text": "After cleaning, we have 831 authors, 731 authors for training and 100 authors for testing.", "labels": [], "entities": []}, {"text": "The numbers of reviews in the training and test author set are 59256 and 14308, respectively.", "labels": [], "entities": []}, {"text": "We use the Stanford parser ( to generate the grammar structure of review sentences for extracting syntactic d-features.", "labels": [], "entities": []}, {"text": "Note that the authors here are in fact userids.", "labels": [], "entities": []}, {"text": "However, since they are randomly selected from a large number of userids, the probability that two sampled userids belong to the same person is very small.", "labels": [], "entities": []}, {"text": "Thus, it should be safe to assume that each userid here represents a unique author.", "labels": [], "entities": []}, {"text": "Training data: We randomly choose 1 (one) review for each author as the query and all of his/her other reviews as q-positive reviews.", "labels": [], "entities": []}, {"text": "The qnegative reviews consist of reviews randomly selected from the other 730 authors, two reviews per author.", "labels": [], "entities": []}, {"text": "We also tried to use more queries from each author, but they make little difference.", "labels": [], "entities": []}, {"text": "Test data: The test authors are all unseen, i.e., their reviews have not been used in training.", "labels": [], "entities": []}, {"text": "We prepare the test case for each author as follows.", "labels": [], "entities": []}, {"text": "We first divide the reviews of each author into two equal subsets.", "labels": [], "entities": []}, {"text": "The purpose is to simulate the situation where there are two userids id ia and id ib from the same author a i . Our objective is that given one userid id ia and its query set, we want to find the other userid id ib from the same author.", "labels": [], "entities": []}, {"text": "For the review subset of id ia (or id ib ), we randomly select 9 reviews as the query set and another 10 reviews as the sample set for the userid.", "labels": [], "entities": []}, {"text": "The two sets are disjoint.", "labels": [], "entities": []}, {"text": "We don't use more queries or sample reviews from each author since in the review domain most authors do not have many reviews (.", "labels": [], "entities": []}, {"text": "In the experiments, we will vary the number of test userids, the number of queries, and the number of samples.", "labels": [], "entities": []}, {"text": "We use the following format to describe each test data: T<n>_Q<n>S<n>, where T denotes the total number of test userids, Q the query set and S the sample set, and <n> a number.", "labels": [], "entities": []}, {"text": "For example, T50_Q9S10 stands fora test data with 50 userids, and for each userid, 9 reviews are selected as queries and 10 reviews are selected as samples.", "labels": [], "entities": [{"text": "T50_Q9S10", "start_pos": 13, "end_pos": 22, "type": "DATASET", "confidence": 0.7379992802937826}]}, {"text": "* rep-resents a wildcard whose value we can vary.", "labels": [], "entities": []}, {"text": "Note that we use this \"artificial\" data rather than manually labeled data for our experiments because it is very hard to reliably label any gold-standard data manually in this case.", "labels": [], "entities": []}, {"text": "The problem is similar to labeling fake reviews.", "labels": [], "entities": [{"text": "labeling fake reviews", "start_pos": 26, "end_pos": 47, "type": "TASK", "confidence": 0.879710336526235}]}, {"text": "In the fake review detection research, researchers have manually label fake reviews and reviewers).", "labels": [], "entities": [{"text": "fake review detection", "start_pos": 7, "end_pos": 28, "type": "TASK", "confidence": 0.6276045441627502}]}, {"text": "However, based on the actual fake reviews written using Amazon Mechanical Turk, have showed that the accuracy of human labeling of fake reviews is very poor.", "labels": [], "entities": [{"text": "Amazon Mechanical Turk", "start_pos": 56, "end_pos": 78, "type": "DATASET", "confidence": 0.9224533240000407}, {"text": "accuracy", "start_pos": 101, "end_pos": 109, "type": "METRIC", "confidence": 0.9993236064910889}]}, {"text": "We also believe that our test data is realistic for evaluation as we can image that the two sets of reviews are from two accounts (userids) of the same author (reviewer).", "labels": [], "entities": []}, {"text": "Two types of experiments: For each author with two userids, we conduct two types of tests.", "labels": [], "entities": []}, {"text": "\uf0b7 Type I: Identify two userids belong to the same author.", "labels": [], "entities": []}, {"text": "The experiment runs iteratively to test every userid.", "labels": [], "entities": []}, {"text": "In each iteration, we plant one userid of an author in the test set and use the other userid of the same author as the query userid.", "labels": [], "entities": []}, {"text": "That is, in the i th run, the test data consist of the following two components: 1.", "labels": [], "entities": []}, {"text": "Query userid id ia and its query set Q ia 2.", "labels": [], "entities": []}, {"text": "Test userids {id 1a , \u2026, id (i-1)a , id ib , \u2026, id ma } and their corresponding sample review sets {S 1a , \u2026, S (i-1)a , S ib , \u2026, S ma }.", "labels": [], "entities": []}, {"text": "Note that the query userid id ia and the test userid id ib are from the same author.", "labels": [], "entities": []}, {"text": "Our objective is to use Q ia to find id ib through S ib . Evaluation measure: We use precision, recall, and F 1 score to evaluate Type I experiments as we want to identify all matching pairs.", "labels": [], "entities": [{"text": "precision", "start_pos": 85, "end_pos": 94, "type": "METRIC", "confidence": 0.9996009469032288}, {"text": "recall", "start_pos": 96, "end_pos": 102, "type": "METRIC", "confidence": 0.9981751441955566}, {"text": "F 1 score", "start_pos": 108, "end_pos": 117, "type": "METRIC", "confidence": 0.9903738697369894}]}, {"text": "The errors are \"no pair\" and \"wrong pair\" found.", "labels": [], "entities": []}, {"text": "\uf0b7 Type II: Type II experiments test the cases when no pair exists.", "labels": [], "entities": []}, {"text": "That is, we do not plant any matching userid for the query userid.", "labels": [], "entities": []}, {"text": "Then, the algorithm should not find anything.", "labels": [], "entities": []}, {"text": "For the i th run, the test data has these components: 1.", "labels": [], "entities": []}, {"text": "Query userid id ia and its query set Q ia 2.Test userids {id 1a , \u2026, id (i-1)a , id (i+1)a , \u2026, id ma } and their sample review sets {S 1a , \u2026, S (i-1)a , S (i+1)a , \u2026, S ma }.", "labels": [], "entities": []}, {"text": "Evaluation measure: Here we cannot use precision and recall because we are not trying to find any pairs.", "labels": [], "entities": [{"text": "precision", "start_pos": 39, "end_pos": 48, "type": "METRIC", "confidence": 0.999439537525177}, {"text": "recall", "start_pos": 53, "end_pos": 59, "type": "METRIC", "confidence": 0.9987432360649109}]}, {"text": "We thus use accuracy as our measure.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 12, "end_pos": 20, "type": "METRIC", "confidence": 0.9995951056480408}]}, {"text": "For each id i , if no pair is found, it is correct.", "labels": [], "entities": []}, {"text": "If a pair is found, it is wrong.", "labels": [], "entities": []}, {"text": "Baseline methods: As mentioned eariler, there are only two works that tried to identify multi-id users.", "labels": [], "entities": []}, {"text": "The first is that in).", "labels": [], "entities": []}, {"text": "However, as we discussed in related work, their approach is not applicable to reviews.", "labels": [], "entities": []}, {"text": "The other is that in), which used clustering but assumed that the number of actual authors (or clusters) is known.", "labels": [], "entities": []}, {"text": "This is unrealistic in practice.", "labels": [], "entities": []}, {"text": "For both SimUG and SimAD, their cosine similarity values are used in place of SVM scores of LSS or TSL.", "labels": [], "entities": [{"text": "SimAD", "start_pos": 19, "end_pos": 24, "type": "DATASET", "confidence": 0.8445903062820435}, {"text": "LSS", "start_pos": 92, "end_pos": 95, "type": "METRIC", "confidence": 0.9698634743690491}]}, {"text": "We then apply the same 4 strategies to decide the final author attribution except voting as cosine similarity cannot classify.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2. Sim4 for computing length s-features", "labels": [], "entities": []}, {"text": " Table 5. Positive(p)/total(t) ratio in training (Type I)", "labels": [], "entities": [{"text": "Positive(p)/total(t) ratio", "start_pos": 10, "end_pos": 36, "type": "METRIC", "confidence": 0.9099923107359145}]}, {"text": " Table 6. Positive(p)/total(t) ratio in training (Type II)", "labels": [], "entities": [{"text": "Positive(p)/total(t) ratio", "start_pos": 10, "end_pos": 36, "type": "METRIC", "confidence": 0.9357970224486457}]}, {"text": " Table 7. Using different s-features (Type I)", "labels": [], "entities": []}, {"text": " Table 8. Using different s-features (Type II)", "labels": [], "entities": []}, {"text": " Table 9: Comparison with baselines (Type I)", "labels": [], "entities": []}, {"text": " Table 10: Comparison with baselines (Type II)", "labels": [], "entities": []}]}