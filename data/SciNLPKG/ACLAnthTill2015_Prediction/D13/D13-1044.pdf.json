{"title": [{"text": "Automatic Feature Engineering for Answer Selection and Extraction", "labels": [], "entities": [{"text": "Answer Selection and Extraction", "start_pos": 34, "end_pos": 65, "type": "TASK", "confidence": 0.8582280576229095}]}], "abstractContent": [{"text": "This paper proposes a framework for automatically engineering features for two important tasks of question answering: answer sentence selection and answer extraction.", "labels": [], "entities": [{"text": "question answering", "start_pos": 98, "end_pos": 116, "type": "TASK", "confidence": 0.7646540403366089}, {"text": "answer sentence selection", "start_pos": 118, "end_pos": 143, "type": "TASK", "confidence": 0.7487602829933167}, {"text": "answer extraction", "start_pos": 148, "end_pos": 165, "type": "TASK", "confidence": 0.8221433460712433}]}, {"text": "We represent question and answer sentence pairs with linguistic structures enriched by semantic information , where the latter is produced by automatic classifiers, e.g., question classifier and Named Entity Recognizer.", "labels": [], "entities": []}, {"text": "Tree kernels applied to such structures enable a simple way to generate highly discriminative structural features that combine syntactic and semantic information encoded in the input trees.", "labels": [], "entities": []}, {"text": "We conduct experiments on a public benchmark from TREC to compare with previous systems for answer sentence selection and answer extraction.", "labels": [], "entities": [{"text": "TREC", "start_pos": 50, "end_pos": 54, "type": "DATASET", "confidence": 0.5930419564247131}, {"text": "answer sentence selection", "start_pos": 92, "end_pos": 117, "type": "TASK", "confidence": 0.7931939760843912}, {"text": "answer extraction", "start_pos": 122, "end_pos": 139, "type": "TASK", "confidence": 0.8635797202587128}]}, {"text": "The results show that our models greatly improve on the state of the art, e.g., up to 22% on F1 (relative improvement) for answer extraction , while using no additional resources and no manual feature engineering.", "labels": [], "entities": [{"text": "F1", "start_pos": 93, "end_pos": 95, "type": "METRIC", "confidence": 0.9995937943458557}, {"text": "answer extraction", "start_pos": 123, "end_pos": 140, "type": "TASK", "confidence": 0.9296373128890991}]}], "introductionContent": [{"text": "Question Answering (QA) systems are typically built from three main macro-modules: (i) search and retrieval of candidate passages; (ii) reranking or selection of the most promising passages; and (iii) answer extraction.", "labels": [], "entities": [{"text": "Question Answering (QA)", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.8544480919837951}, {"text": "answer extraction", "start_pos": 201, "end_pos": 218, "type": "TASK", "confidence": 0.8929552733898163}]}, {"text": "The last two steps are the most interesting from a Natural Language Processing viewpoint since deep linguistic analysis can be carried out as the input is just a limited set of candidates.", "labels": [], "entities": []}, {"text": "Answer sentence selection refers to the task of selecting the sentence containing the correct answer among the different sentence candidates retrieved by a search engine.", "labels": [], "entities": [{"text": "Answer sentence selection refers to the task of selecting the sentence containing the correct answer among the different sentence candidates retrieved by a search engine", "start_pos": 0, "end_pos": 169, "type": "Description", "confidence": 0.8042318546772003}]}, {"text": "Answer extraction is a final step, required for factoid questions, consisting in extracting multiwords constituting the synthetic answer, e.g., Barack Obama fora question: Who is the US president?", "labels": [], "entities": [{"text": "Answer extraction", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.9287107586860657}]}, {"text": "The definition of rules for both tasks is conceptually demanding and involves the use of syntactic and semantic properties of the questions and its related answer passages.", "labels": [], "entities": []}, {"text": "For example, given a question from TREC QA 1 : Q: What was Johnny Appleseed's real name? and a relevant passage, e.g., retrieved by a search engine: A: Appleseed, whose real name was John Chapman, planted many trees in the early 1800s.", "labels": [], "entities": [{"text": "TREC QA 1", "start_pos": 35, "end_pos": 44, "type": "DATASET", "confidence": 0.5050940016905466}]}, {"text": "a rule detecting the semantic links between Johnny Appleseed's real name and the correct answer John Chapman in the answer sentence has to be engineered.", "labels": [], "entities": [{"text": "Johnny Appleseed's real name", "start_pos": 44, "end_pos": 72, "type": "DATASET", "confidence": 0.9132603883743287}]}, {"text": "This requires the definition of other rules that associate the question pattern real name ?(X) with real name is(X) of the answer sentence.", "labels": [], "entities": []}, {"text": "Although this can be done by an expert NLP engineer, the effort for achieving the necessary coverage and a reasonable accuracy is not negligible.", "labels": [], "entities": [{"text": "coverage", "start_pos": 92, "end_pos": 100, "type": "METRIC", "confidence": 0.9815686345100403}, {"text": "accuracy", "start_pos": 118, "end_pos": 126, "type": "METRIC", "confidence": 0.999112069606781}]}, {"text": "An alternative to manual rule definition is the use of machine learning, which often shifts the problem to the easier task of feature engineering.", "labels": [], "entities": [{"text": "manual rule definition", "start_pos": 18, "end_pos": 40, "type": "TASK", "confidence": 0.7280757625897726}]}, {"text": "Unfortunately, when the learning task is semantically difficult such as in QA, e.g., features have to encode combinations of syntactic and semantic properties.", "labels": [], "entities": []}, {"text": "Thus their extraction modules basically assume the shape of high-level rules, which are, in any case, essential to achieve state-of-the-art accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 140, "end_pos": 148, "type": "METRIC", "confidence": 0.9898696541786194}]}, {"text": "For example, the great IBM Watson system) uses a learning to rank algorithm fed with hundreds of features.", "labels": [], "entities": []}, {"text": "The extraction of some of the latter requires articulated rules/algorithms, which, in terms of complexity, are very similar to those constituting typical handcrafted QA systems.", "labels": [], "entities": []}, {"text": "An immediate consequence is the reduced adaptability to new domains, which requires a substantial reengineering work.", "labels": [], "entities": []}, {"text": "In this paper, we show that tree kernels) can be applied to automatically learn complex structural patterns for both answer sentence selection and answer extraction.", "labels": [], "entities": [{"text": "answer sentence selection", "start_pos": 117, "end_pos": 142, "type": "TASK", "confidence": 0.7605907320976257}, {"text": "answer extraction", "start_pos": 147, "end_pos": 164, "type": "TASK", "confidence": 0.8467510342597961}]}, {"text": "Such patterns are syntactic/semantic structures occurring in question and answer passages.", "labels": [], "entities": []}, {"text": "To make such information available to the tree kernel functions, we rely on the shallow syntactic trees enriched with semantic information (), e.g., Named Entities (NEs) and question focus and category, automatically derived by machine learning modules, e.g., question classifier (QC) or focus classifier (FC).", "labels": [], "entities": []}, {"text": "More in detail, we (i) design a pair of shallow syntactic trees (one for the question and one for the answer sentence); (ii) connect them with relational nodes (i.e., those matching the same words in the question and in the answer passages); (iii) label the tree nodes with semantic information such as question category and focus and NEs; and (iv) use the NE type to establish additional semantic links between the candidate answer, i.e., an NE, and the focus word of the question.", "labels": [], "entities": []}, {"text": "Finally, for the task of answer extraction we also connect such semantic information to the answer sentence trees such that we can learn factoid answer patterns.", "labels": [], "entities": [{"text": "answer extraction", "start_pos": 25, "end_pos": 42, "type": "TASK", "confidence": 0.9332971274852753}]}, {"text": "We show that our models are very effective in producing features for both answer selection and extraction by experimenting with TREC QA corpora and directly comparing with the state of the art, e.g., (.", "labels": [], "entities": [{"text": "answer selection and extraction", "start_pos": 74, "end_pos": 105, "type": "TASK", "confidence": 0.8016242384910583}]}, {"text": "The results show that our methods greatly improve on both tasks yielding a large improvement in Mean Average Precision for answer selection and in F1 for answer extraction: up to 22% of relative improvement in F1, when small training data is used.", "labels": [], "entities": [{"text": "Mean Average Precision", "start_pos": 96, "end_pos": 118, "type": "METRIC", "confidence": 0.9625264604886373}, {"text": "answer selection", "start_pos": 123, "end_pos": 139, "type": "TASK", "confidence": 0.9204554557800293}, {"text": "F1", "start_pos": 147, "end_pos": 149, "type": "METRIC", "confidence": 0.9996564388275146}, {"text": "answer extraction", "start_pos": 154, "end_pos": 171, "type": "TASK", "confidence": 0.8973535597324371}, {"text": "F1", "start_pos": 210, "end_pos": 212, "type": "METRIC", "confidence": 0.9992927312850952}]}, {"text": "Moreover, in contrast to the previous work, our model does not rely on external resources, e.g., WordNet, or complex features in addition to the structural kernel model.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 97, "end_pos": 104, "type": "DATASET", "confidence": 0.9324689507484436}]}, {"text": "The reminder of this paper is organized as follows, Sec.", "labels": [], "entities": []}, {"text": "2 describes our kernel-based classifiers, Sec.", "labels": [], "entities": []}, {"text": "3 illustrates our question/answer relational structures also enriched with semantic information, Sec.", "labels": [], "entities": []}, {"text": "4 describes our model for answer selection and extraction, Sec.", "labels": [], "entities": [{"text": "answer selection and extraction", "start_pos": 26, "end_pos": 57, "type": "TASK", "confidence": 0.8464188575744629}]}, {"text": "5 illustrates our comparative experiments on TREC data, Sec.", "labels": [], "entities": [{"text": "TREC data", "start_pos": 45, "end_pos": 54, "type": "DATASET", "confidence": 0.8473005294799805}]}, {"text": "6 reports on our error analysis, Sec.", "labels": [], "entities": []}, {"text": "7 discusses the related work, and finally, Sec.", "labels": [], "entities": []}], "datasetContent": [{"text": "We provide the results on two related yet different tasks: answer sentence selection and answer extraction.", "labels": [], "entities": [{"text": "answer sentence selection", "start_pos": 59, "end_pos": 84, "type": "TASK", "confidence": 0.8765632708867391}, {"text": "answer extraction", "start_pos": 89, "end_pos": 106, "type": "TASK", "confidence": 0.904166966676712}]}, {"text": "The goal of the former is to learn a model scoring correct question and answer sentence pairs to bring in the top positions sentences containing the correct answers.", "labels": [], "entities": []}, {"text": "Answer extraction derives the cor- E \u2190 E \u222a {e} return E rect answer keywords, i.e., a text span such as multiwords or constituents, from a given sentence.", "labels": [], "entities": [{"text": "Answer extraction", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.9548342525959015}]}], "tableCaptions": [{"text": " Table 2: Accuracy (%) of focus (FC) and question classi- fiers (QC) using PTK.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9975236058235168}, {"text": "focus (FC)", "start_pos": 26, "end_pos": 36, "type": "METRIC", "confidence": 0.7844745069742203}, {"text": "question classi- fiers (QC", "start_pos": 41, "end_pos": 67, "type": "METRIC", "confidence": 0.7583231627941132}, {"text": "PTK", "start_pos": 75, "end_pos": 78, "type": "DATASET", "confidence": 0.7353053689002991}]}, {"text": " Table 3: Summary of TREC data for answer extraction  used in (Yao et al., 2013).", "labels": [], "entities": [{"text": "TREC", "start_pos": 21, "end_pos": 25, "type": "METRIC", "confidence": 0.8238721489906311}, {"text": "answer extraction", "start_pos": 35, "end_pos": 52, "type": "TASK", "confidence": 0.9051766991615295}]}, {"text": " Table 4: Answer sentence reranking on TREC 13.", "labels": [], "entities": [{"text": "Answer sentence reranking", "start_pos": 10, "end_pos": 35, "type": "TASK", "confidence": 0.6591163277626038}, {"text": "TREC 13", "start_pos": 39, "end_pos": 46, "type": "DATASET", "confidence": 0.6967252492904663}]}, {"text": " Table 5: Results on answer extraction. P/R -precision  and recall; pairs -number of QA pairs with a correctly ex- tracted answer, q -number of questions with at least one  correct answer extracted, F1 sets an upper bound on the  performance assuming the selected best answer among  extracted candidates is always correct. *-marks the set- ting", "labels": [], "entities": [{"text": "answer extraction", "start_pos": 21, "end_pos": 38, "type": "TASK", "confidence": 0.7559766173362732}, {"text": "precision", "start_pos": 45, "end_pos": 54, "type": "METRIC", "confidence": 0.9404540061950684}, {"text": "recall", "start_pos": 60, "end_pos": 66, "type": "METRIC", "confidence": 0.9993172883987427}, {"text": "F1", "start_pos": 199, "end_pos": 201, "type": "METRIC", "confidence": 0.9864870309829712}]}, {"text": " Table 6: Results on finding the best answer with voting.", "labels": [], "entities": []}]}