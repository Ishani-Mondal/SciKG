{"title": [{"text": "Automatic Extraction of Morphological Lexicons from Morphologically Annotated Corpora", "labels": [], "entities": [{"text": "Automatic Extraction of Morphological Lexicons from Morphologically Annotated Corpora", "start_pos": 0, "end_pos": 85, "type": "TASK", "confidence": 0.7689219183391995}]}], "abstractContent": [{"text": "We present a method for automatically learning inflectional classes and associated lem-mas from morphologically annotated corpora.", "labels": [], "entities": []}, {"text": "The method consists of a core language-independent algorithm, which can be optimized for specific languages.", "labels": [], "entities": []}, {"text": "The method is demonstrated on Egyptian Arabic and Ger-man, two morphologically rich languages.", "labels": [], "entities": []}, {"text": "Our best method for Egyptian Arabic provides an error reduction of 55.6% over a simple baseline; our best method for German achieves a 66.7% error reduction.", "labels": [], "entities": [{"text": "error reduction", "start_pos": 48, "end_pos": 63, "type": "METRIC", "confidence": 0.9636393785476685}]}], "introductionContent": [{"text": "Morphological lexicons specify all inflected forms for each lexeme; in a language with rich morphology, such a resource can be important for natural language processing (NLP) tasks in order to limit data sparseness.", "labels": [], "entities": []}, {"text": "For example, a morphological lexicon is an important component of a morphological tagger or of a part-of-speech (POS) tagger for languages with rich morphology.", "labels": [], "entities": []}, {"text": "Traditionally, a morphological lexicon has been created through painstaking lexicographic and morphological analysis of the language, drawing on unannotated corpora.", "labels": [], "entities": []}, {"text": "Recently, new approaches have emerged.", "labels": [], "entities": []}, {"text": "Fully or largely unsupervised approaches cannot link surface forms to morphosyntactic features and are thus not suited for building morphological lexicons.", "labels": [], "entities": []}, {"text": "This problem is overcome by approaches that use explicit linguistic knowledge.", "labels": [], "entities": []}, {"text": "In this paper, we investigate using existing morphologically annotated corpora.", "labels": [], "entities": []}, {"text": "Ina morphologically annotated corpus, the words in naturally occurring texts or transcribed speech are annotated for the correct morphological analysis (includ-ing, of course, core POS) in context.", "labels": [], "entities": []}, {"text": "While there has been much work on computational morphology, to our knowledge this is the first paper to study the question of how to extract morphological lexicons from morphologically annotated corpora, and how to determine how much annotation is needed.", "labels": [], "entities": []}, {"text": "In this paper, we assume a corpus with each word annotated with morphosyntactic features and with a lemma which tells us what lexeme the word form is part of.", "labels": [], "entities": []}, {"text": "The task is to predict the correspondence between a word form and its lemma and morphological features.", "labels": [], "entities": []}, {"text": "This paper makes two contributions.", "labels": [], "entities": []}, {"text": "First, we introduce an algorithm that learns unseen forms by analogy.", "labels": [], "entities": []}, {"text": "It incrementally merges complementary paradigm information about different lexemes into more abstract and more informative inflectional classes.", "labels": [], "entities": []}, {"text": "Second, we explore how to model stems, and we propose a generalization of the Semitic root-and-template modeling.", "labels": [], "entities": []}, {"text": "We use Egyptian Arabic (EGY), and German (GER) as our test languages.", "labels": [], "entities": []}, {"text": "We test on corpus data, in order to simulate a standard real-world application.", "labels": [], "entities": []}, {"text": "The baseline just uses the word forms seen in training and does not predict any unseen forms.", "labels": [], "entities": []}, {"text": "Our language-independent algorithm improves the performance for both EGY and GER, with error reductions over the baseline of 44.4% for EGY and of 66.7% for GER.", "labels": [], "entities": [{"text": "GER", "start_pos": 77, "end_pos": 80, "type": "DATASET", "confidence": 0.7124189138412476}, {"text": "error", "start_pos": 87, "end_pos": 92, "type": "METRIC", "confidence": 0.9721464514732361}, {"text": "GER", "start_pos": 156, "end_pos": 159, "type": "DATASET", "confidence": 0.8315030932426453}]}, {"text": "By adding language-specific modeling of the stem using templates, we obtain further error reductions for EGY (up to 55.6%) but not for GER.", "labels": [], "entities": [{"text": "error", "start_pos": 84, "end_pos": 89, "type": "METRIC", "confidence": 0.9575886726379395}, {"text": "GER", "start_pos": 135, "end_pos": 138, "type": "DATASET", "confidence": 0.6374685168266296}]}, {"text": "Next, we review related work (Section 2) and introduce the key linguistic concepts we use (Section 3).", "labels": [], "entities": []}, {"text": "We present our basic language-independent method in Section 4, and our language-specific modeling of stem variation in Section 5.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 2: Learning curve comparing system performance  for EGY on tokens (development set).", "labels": [], "entities": []}, {"text": " Table 3: Results for EGY on tokens using a blind test set.", "labels": [], "entities": []}, {"text": " Table 5: Learning curve comparing system performance  for GER on tokens on DEV corpus. BL=Baseline", "labels": [], "entities": [{"text": "GER", "start_pos": 59, "end_pos": 62, "type": "TASK", "confidence": 0.9456830620765686}, {"text": "DEV corpus", "start_pos": 76, "end_pos": 86, "type": "DATASET", "confidence": 0.9463498294353485}, {"text": "BL", "start_pos": 88, "end_pos": 90, "type": "METRIC", "confidence": 0.9976927042007446}]}, {"text": " Table 6: Results for GER on tokens using a blind test set.", "labels": [], "entities": [{"text": "GER", "start_pos": 22, "end_pos": 25, "type": "TASK", "confidence": 0.9403053522109985}]}]}