{"title": [{"text": "Bilingual Word Embeddings for Phrase-Based Machine Translation", "labels": [], "entities": [{"text": "Phrase-Based Machine Translation", "start_pos": 30, "end_pos": 62, "type": "TASK", "confidence": 0.8166481653849283}]}], "abstractContent": [{"text": "We introduce bilingual word embeddings: semantic embeddings associated across two languages in the context of neural language models.", "labels": [], "entities": []}, {"text": "We propose a method to learn bilingual embeddings from a large unlabeled corpus, while utilizing MT word alignments to constrain translational equivalence.", "labels": [], "entities": [{"text": "MT word alignments", "start_pos": 97, "end_pos": 115, "type": "TASK", "confidence": 0.8900181253751119}]}, {"text": "The new em-beddings significantly out-perform baselines in word semantic similarity.", "labels": [], "entities": [{"text": "word semantic similarity", "start_pos": 59, "end_pos": 83, "type": "TASK", "confidence": 0.6870244046052297}]}, {"text": "A single semantic similarity feature induced with bilingual em-beddings adds near half a BLEU point to the results of NIST08 Chinese-English machine translation task.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 89, "end_pos": 93, "type": "METRIC", "confidence": 0.9996562004089355}, {"text": "NIST08 Chinese-English machine translation task", "start_pos": 118, "end_pos": 165, "type": "TASK", "confidence": 0.8302860260009766}]}], "introductionContent": [{"text": "It is difficult to recognize and quantify semantic similarities across languages.", "labels": [], "entities": []}, {"text": "The Fr-En phrase-pair {'un cas de force majeure', 'case of absolute necessity'}, Zh-En phrase pair {'\u4f9d\u7136\u6545\u6211','persist in a stubborn manner'} are similar in semantics.", "labels": [], "entities": []}, {"text": "If cooccurrences of exact word combinations are rare in the training parallel text, it can be difficult for classical statistical MT methods to identify this similarity, or produce a reasonable translation given the source phrase.", "labels": [], "entities": [{"text": "MT", "start_pos": 130, "end_pos": 132, "type": "TASK", "confidence": 0.9366332292556763}]}, {"text": "We introduce an unsupervised neural model to learn bilingual semantic embedding for words across two languages.", "labels": [], "entities": []}, {"text": "As an extension to their monolingual counter-part (, bilingual embeddings capture not only semantic information of monolingual words, but also semantic relationships across different languages.", "labels": [], "entities": []}, {"text": "This property allows them to define semantic similarity metrics across phrase-pairs, making them perfect features for machine translation.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 118, "end_pos": 137, "type": "TASK", "confidence": 0.7712073922157288}]}, {"text": "To learn bilingual embeddings, we use anew objective function which embodies both monolingual semantics and bilingual translation equivalence.", "labels": [], "entities": []}, {"text": "The latter utilizes word alignments, a natural sub-task in the machine translation pipeline.", "labels": [], "entities": [{"text": "word alignments", "start_pos": 20, "end_pos": 35, "type": "TASK", "confidence": 0.7684049010276794}, {"text": "machine translation pipeline", "start_pos": 63, "end_pos": 91, "type": "TASK", "confidence": 0.8019547263781229}]}, {"text": "Through largescale curriculum training (), we obtain bilingual distributed representations which lie in the same feature space.", "labels": [], "entities": []}, {"text": "Embeddings of direct translations overlap, and semantic relationships across bilingual embeddings were further improved through unsupervised learning on a large unlabeled corpus.", "labels": [], "entities": []}, {"text": "Consequently, we produce for the research community a first set of Mandarin Chinese word embeddings with 100,000 words trained on the Chinese Gigaword corpus.", "labels": [], "entities": [{"text": "Chinese Gigaword corpus", "start_pos": 134, "end_pos": 157, "type": "DATASET", "confidence": 0.7854601939519247}]}, {"text": "We evaluate these embedding on Chinese word semantic similarity from).", "labels": [], "entities": []}, {"text": "The embeddings significantly out-perform prior work and pruned tf-idf base-lines.", "labels": [], "entities": []}, {"text": "In addition, the learned embeddings give rise to 0.11 F1 improvement in Named Entity Recognition on the OntoNotes dataset () with a neural network model.", "labels": [], "entities": [{"text": "F1", "start_pos": 54, "end_pos": 56, "type": "METRIC", "confidence": 0.999167799949646}, {"text": "Named Entity Recognition", "start_pos": 72, "end_pos": 96, "type": "TASK", "confidence": 0.5503604610761007}, {"text": "OntoNotes dataset", "start_pos": 104, "end_pos": 121, "type": "DATASET", "confidence": 0.8598407208919525}]}, {"text": "We apply the bilingual embeddings in an end-toend phrase-based MT system by computing semantic similarities between phrase pairs.", "labels": [], "entities": [{"text": "MT", "start_pos": 63, "end_pos": 65, "type": "TASK", "confidence": 0.903272807598114}]}, {"text": "On NIST08 Chinese-English translation task, we obtain an improvement of 0.48 BLEU from a competitive baseline (30.01 BLEU to 30.49 BLEU) with the Stanford Phrasal MT system.", "labels": [], "entities": [{"text": "NIST08 Chinese-English translation task", "start_pos": 3, "end_pos": 42, "type": "TASK", "confidence": 0.78420589864254}, {"text": "BLEU", "start_pos": 77, "end_pos": 81, "type": "METRIC", "confidence": 0.996543824672699}, {"text": "BLEU", "start_pos": 117, "end_pos": 121, "type": "METRIC", "confidence": 0.9945110082626343}, {"text": "BLEU", "start_pos": 131, "end_pos": 135, "type": "METRIC", "confidence": 0.984546422958374}, {"text": "Stanford Phrasal MT system", "start_pos": 146, "end_pos": 172, "type": "DATASET", "confidence": 0.8700377643108368}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Results on Chinese Semantic Similarity", "labels": [], "entities": [{"text": "Chinese Semantic Similarity", "start_pos": 21, "end_pos": 48, "type": "TASK", "confidence": 0.569814532995224}]}, {"text": " Table 1. We show two evaluation metrics: Spear- man Correlation and Kendall's Tau. For both, bilin- gual embeddings trained with the combined objec- tive defined by Equation 5 perform best. For pruned  tf-idf, we follow", "labels": [], "entities": [{"text": "Kendall's Tau", "start_pos": 69, "end_pos": 82, "type": "METRIC", "confidence": 0.6178974310557047}]}, {"text": " Table 2: Results on Named Entity Recognition", "labels": [], "entities": [{"text": "Named Entity Recognition", "start_pos": 21, "end_pos": 45, "type": "TASK", "confidence": 0.909695049126943}]}, {"text": " Table 3: Vector Matching Alignment AER (lower is bet- ter)", "labels": [], "entities": [{"text": "Vector Matching Alignment", "start_pos": 10, "end_pos": 35, "type": "TASK", "confidence": 0.8476055065790812}, {"text": "AER", "start_pos": 36, "end_pos": 39, "type": "METRIC", "confidence": 0.9544835090637207}]}, {"text": " Table 4: NIST08 Chinese-English translation BLEU", "labels": [], "entities": [{"text": "NIST08 Chinese-English translation", "start_pos": 10, "end_pos": 44, "type": "DATASET", "confidence": 0.8483017285664877}, {"text": "BLEU", "start_pos": 45, "end_pos": 49, "type": "METRIC", "confidence": 0.9595175385475159}]}]}