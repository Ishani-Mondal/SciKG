{"title": [{"text": "Factored Soft Source Syntactic Constraints for Hierarchical Machine Translation", "labels": [], "entities": [{"text": "Hierarchical Machine Translation", "start_pos": 47, "end_pos": 79, "type": "TASK", "confidence": 0.7110164761543274}]}], "abstractContent": [{"text": "This paper describes a factored approach to incorporating soft source syntactic constraints into a hierarchical phrase-based translation system.", "labels": [], "entities": [{"text": "phrase-based translation", "start_pos": 112, "end_pos": 136, "type": "TASK", "confidence": 0.6315425038337708}]}, {"text": "In contrast to traditional approaches that directly introduce syntactic constraints to translation rules by explicitly decorating them with syntactic annotations, which often exacerbate the data sparsity problem and cause other problems, our approach keeps translation rules intact and factorizes the use of syntactic constraints through two separate models: 1) a syntax mismatch model that associates each nonterminal of a translation rule with a distribution of tags that is used to measure the degree of syntactic compatibility of the translation rule on source spans; 2) a syntax-based reordering model that predicts whether a pair of sibling constituents in the constituent parse tree of the source sentence should be reordered or not when translated to the target language.", "labels": [], "entities": []}, {"text": "The features produced by both models are used as soft constraints to guide the translation process.", "labels": [], "entities": [{"text": "translation", "start_pos": 79, "end_pos": 90, "type": "TASK", "confidence": 0.9635869264602661}]}, {"text": "Experiments on Chinese-English translation show that the proposed approach significantly improves a strong string-to-dependency translation system on multiple evaluation sets.", "labels": [], "entities": [{"text": "Chinese-English translation", "start_pos": 15, "end_pos": 42, "type": "TASK", "confidence": 0.6746408492326736}]}], "introductionContent": [{"text": "Hierarchical phrase-based translation models) are widely used in machine translation systems due to their ability to achieve local fluency through phrasal translation and handle nonlocal phrase reordering using synchronous contextfree grammars.", "labels": [], "entities": [{"text": "Hierarchical phrase-based translation", "start_pos": 0, "end_pos": 37, "type": "TASK", "confidence": 0.5822347501913706}, {"text": "machine translation", "start_pos": 65, "end_pos": 84, "type": "TASK", "confidence": 0.7636498808860779}]}, {"text": "A large number of previous works have tried to introduce grammaticality to the translation process by incorporating syntactic constraints into hierarchical translation models.", "labels": [], "entities": []}, {"text": "Despite some differences in the granularity of syntax units (e.g., tree fragments (), treebank tags (, and extended tags ()), most previous work incorporates syntax into hierarchical translation models by explicitly decorating translation rules with syntactic annotations.", "labels": [], "entities": []}, {"text": "These approaches inevitably exacerbate the data sparsity problem and cause other problems such as increased grammar size, worsened derivational ambiguity, and unavoidable parsing errors.", "labels": [], "entities": []}, {"text": "In this paper, we propose a factored approach that incorporates soft source syntactic constraints into a hierarchical string-to-dependency translation model.", "labels": [], "entities": []}, {"text": "The general ideas are applicable to other hierarchical models as well.", "labels": [], "entities": []}, {"text": "Instead of enriching translation rules with explicit syntactic annotations, we keep the original translation rules intact, and factorize the use of source syntactic constraints through two separate models.", "labels": [], "entities": []}, {"text": "The first is a syntax mismatch model that introduces source syntax into the nonterminals of translation rules, and measures the degree of syntactic compatibility between a translation rule and the source spans it is applied to during decoding.", "labels": [], "entities": []}, {"text": "When a hierarchical translation rule is extracted from a parallel training sentence pair, we determine a tag for each nonterminal based on the dependency parse of the source sentence.", "labels": [], "entities": []}, {"text": "Instead of fragmenting rule statistics by directly labeling nonterminals with tags, we keep the original string-to-dependency translation rules intact and associate each nonterminal with a distribution of tags.", "labels": [], "entities": []}, {"text": "That distribution is then used to measure the syntactic compatibility between the syntactic context from which the translation rule is extracted and the syntactic analysis of a test sentence.", "labels": [], "entities": []}, {"text": "The second is a syntax-based reordering model that takes advantage of phrasal cohesion in translation).", "labels": [], "entities": []}, {"text": "The reordering model takes a pair of sibling constituents in the source parse tree as input, and uses source syntactic clues to predict the ordering distribution (straight vs. inverted) of their translations on the target side.", "labels": [], "entities": []}, {"text": "The resulting ordering distribution is used in the decoder at the word pair level to guide the translation process.", "labels": [], "entities": []}, {"text": "This separate reordering model allows us to utilize source syntax to improve reordering in hierarchical translation models without having to explicitly annotate translation rules with source syntax.", "labels": [], "entities": []}, {"text": "Our results show that both the syntax mismatch model and the syntax-based reordering model are able to achieve significant gains over a strong Chinese-English MT baseline.", "labels": [], "entities": []}, {"text": "The rest of the paper is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 discusses related work in the literature.", "labels": [], "entities": []}, {"text": "Section 3 provides an overview of our baseline string-to-dependency translation system.", "labels": [], "entities": []}, {"text": "Section 4 describes the details of the syntax mismatch and syntax-based reordering models.", "labels": [], "entities": []}, {"text": "Experimental results are presented in Section 5.", "labels": [], "entities": []}, {"text": "The last section concludes the paper.", "labels": [], "entities": []}], "datasetContent": [{"text": "Our main experiments use the Chinese-English parallel training data and development sets released by the LDC, and made available to the DARPA GALE and BOLT programs.", "labels": [], "entities": [{"text": "LDC", "start_pos": 105, "end_pos": 108, "type": "DATASET", "confidence": 0.5072808265686035}, {"text": "DARPA GALE", "start_pos": 136, "end_pos": 146, "type": "DATASET", "confidence": 0.7057276666164398}, {"text": "BOLT", "start_pos": 151, "end_pos": 155, "type": "METRIC", "confidence": 0.8089538812637329}]}, {"text": "We train the translation model on 100 million words of parallel data.", "labels": [], "entities": [{"text": "translation", "start_pos": 13, "end_pos": 24, "type": "TASK", "confidence": 0.9719967246055603}]}, {"text": "We use a 8 billion words of English monolingual data to train two language models: a trigram language model used in chart decoding, and a 5-gram language model used in n-best rescoring.", "labels": [], "entities": []}, {"text": "The systems are tuned and evaluated on a mixture of newswire and web forum text from the development sets available for the DARPA GALE and BOLT programs, with up to 4 independent references for each source sentence.", "labels": [], "entities": [{"text": "DARPA GALE", "start_pos": 124, "end_pos": 134, "type": "DATASET", "confidence": 0.6873329281806946}, {"text": "BOLT", "start_pos": 139, "end_pos": 143, "type": "METRIC", "confidence": 0.5342954993247986}]}, {"text": "We also evaluate our final systems on both newswire and web text from the NIST MT06 and MT08 evaluations using an experimental setup compatible with the NIST MT12 Chinese-English constrained track.", "labels": [], "entities": [{"text": "NIST MT06 and MT08 evaluations", "start_pos": 74, "end_pos": 104, "type": "DATASET", "confidence": 0.7999315619468689}, {"text": "NIST MT12 Chinese-English constrained track", "start_pos": 153, "end_pos": 196, "type": "DATASET", "confidence": 0.9261567711830139}]}, {"text": "In this setup, the translation and language models are trained on 35 million words of parallel data and 3.8 billion words of English monolingual data, respectively.", "labels": [], "entities": []}, {"text": "The systems are tuned on the MT02-05 development sets.", "labels": [], "entities": [{"text": "MT02-05 development sets", "start_pos": 29, "end_pos": 53, "type": "DATASET", "confidence": 0.9630886713663737}]}, {"text": "All systems are tuned and evaluated on IBM BLEU ().", "labels": [], "entities": [{"text": "IBM", "start_pos": 39, "end_pos": 42, "type": "DATASET", "confidence": 0.6725518703460693}, {"text": "BLEU", "start_pos": 43, "end_pos": 47, "type": "METRIC", "confidence": 0.9532864689826965}]}, {"text": "The baseline string-to-dependency translation system uses more than 10 core features and a large number of sparse binary features similar to the method described in.", "labels": [], "entities": [{"text": "string-to-dependency translation", "start_pos": 13, "end_pos": 45, "type": "TASK", "confidence": 0.6973616480827332}]}, {"text": "It achieves translation accuracies comparable to the top ranked systems in the NIST MT12 evaluation.", "labels": [], "entities": [{"text": "translation", "start_pos": 12, "end_pos": 23, "type": "TASK", "confidence": 0.9249394536018372}, {"text": "accuracies", "start_pos": 24, "end_pos": 34, "type": "METRIC", "confidence": 0.6120627522468567}, {"text": "NIST MT12 evaluation", "start_pos": 79, "end_pos": 99, "type": "DATASET", "confidence": 0.8354807694753011}]}, {"text": "GIZA++) is used for automatic word alignment in all of the experiments.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 30, "end_pos": 44, "type": "TASK", "confidence": 0.6955870389938354}]}, {"text": "We use Charniak's parser) on the English side to obtain string-to-dependency translation rules, and use a latent variable PCFG parser to parse the source side of the parallel training data as well as the test sentences for extracting syntax mismatch and reordering features.", "labels": [], "entities": []}, {"text": "For both languages, dependency structures are read off constituency trees using manual headword percolation rules.", "labels": [], "entities": []}, {"text": "We use a lexicon-based longest-match-first word segmenter to tokenize source Chinese sentences.", "labels": [], "entities": [{"text": "tokenize source Chinese sentences", "start_pos": 61, "end_pos": 94, "type": "TASK", "confidence": 0.9070716053247452}]}, {"text": "Since the source tokenization used in our MT system is different from the treebank tokenization used to train the Chinese parser, the source sentences are first tokenized using the treebank-trained Stanford Chinese segmenter (), then parsed with the Chinese parser, and finally projected to MT tokenization based on the character alignment between the tokens.", "labels": [], "entities": [{"text": "MT", "start_pos": 42, "end_pos": 44, "type": "TASK", "confidence": 0.9743254780769348}, {"text": "MT tokenization", "start_pos": 291, "end_pos": 306, "type": "TASK", "confidence": 0.871227353811264}]}, {"text": "The syntax-based reordering model is trained on a set of Chinese-English manual word alignment corpora released by the LDC 5 .", "labels": [], "entities": [{"text": "Chinese-English manual word alignment", "start_pos": 57, "end_pos": 94, "type": "TASK", "confidence": 0.6536106765270233}, {"text": "LDC 5", "start_pos": 119, "end_pos": 124, "type": "DATASET", "confidence": 0.9380486309528351}]}], "tableCaptions": [{"text": " Table 3: Effects of tag annotation, tag distribution, and  syntax mismatch features on MT performance on the  GALE/BOLT data set.", "labels": [], "entities": [{"text": "MT", "start_pos": 88, "end_pos": 90, "type": "TASK", "confidence": 0.9895257949829102}, {"text": "GALE/BOLT data set", "start_pos": 111, "end_pos": 129, "type": "DATASET", "confidence": 0.6528377413749695}]}, {"text": " Table 6: Number of shifting errors in TER measurement  of multiple systems on the GALE/BOLT test set", "labels": [], "entities": [{"text": "Number of shifting errors", "start_pos": 10, "end_pos": 35, "type": "METRIC", "confidence": 0.7364743947982788}, {"text": "TER", "start_pos": 39, "end_pos": 42, "type": "METRIC", "confidence": 0.8857452273368835}, {"text": "GALE/BOLT test set", "start_pos": 83, "end_pos": 101, "type": "DATASET", "confidence": 0.6729136824607849}]}]}