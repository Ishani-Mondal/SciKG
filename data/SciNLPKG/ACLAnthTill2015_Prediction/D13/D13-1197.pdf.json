{"title": [{"text": "Learning Distributions over Logical Forms for Referring Expression Generation", "labels": [], "entities": [{"text": "Referring Expression", "start_pos": 46, "end_pos": 66, "type": "TASK", "confidence": 0.8957661688327789}]}], "abstractContent": [{"text": "We present anew approach to referring expression generation, casting it as a density estimation problem where the goal is to learn distributions over logical expressions identifying sets of objects in the world.", "labels": [], "entities": [{"text": "referring expression generation", "start_pos": 28, "end_pos": 59, "type": "TASK", "confidence": 0.8415472308794657}]}, {"text": "Despite an extremely large space of possible expressions , we demonstrate effective learning of a globally normalized log-linear distribution.", "labels": [], "entities": []}, {"text": "This learning is enabled by anew, multi-stage approximate inference technique that uses a pruning model to construct only the most likely logical forms.", "labels": [], "entities": []}, {"text": "We train and evaluate the approach on anew corpus of references to sets of visual objects.", "labels": [], "entities": []}, {"text": "Experiments show the approach is able to learn accurate models, which generate over 87% of the expressions people used.", "labels": [], "entities": []}, {"text": "Additionally, on the previously studied special case of single object reference, we show a 35% relative error reduction over previous state of the art.", "labels": [], "entities": []}], "introductionContent": [{"text": "Understanding and generating natural language requires reasoning over a large space of possible meanings; while many statements might achieve the same goal in a certain situation, some are more likely to be used than others.", "labels": [], "entities": []}, {"text": "In this paper, we model these preferences by learning distributions over situated meaning use.", "labels": [], "entities": []}, {"text": "We focus on the task of referring expression generation (REG), where the goal is to produce an expression which uniquely identifies a pre-defined objector set of objects in an environment.", "labels": [], "entities": [{"text": "referring expression generation (REG)", "start_pos": 24, "end_pos": 61, "type": "TASK", "confidence": 0.8429629107316335}]}, {"text": "In practice, many such expressions can be produced.", "labels": [], "entities": []}, {"text": "Although nearly a third of the people simply listed the colors of the desired objects, many other strategies were also used and no single option dominated.", "labels": [], "entities": []}, {"text": "Learning to model such variation would enable systems to better anticipate what people are likely to say and avoid repetition during generation, by producing appropriately varied utterances themselves.", "labels": [], "entities": []}, {"text": "With these goals in mind, we cast REG as a density estimation problem, where the goal is to learn a distribution over logical forms.", "labels": [], "entities": [{"text": "REG", "start_pos": 34, "end_pos": 37, "type": "TASK", "confidence": 0.9434048533439636}]}, {"text": "Learning such distributions is challenging.", "labels": [], "entities": []}, {"text": "For a target set of objects, the number of logical forms that can be used to describe it grows combinatorially with the number of observable properties, such as color and shape.", "labels": [], "entities": []}, {"text": "However, only a tiny fraction of these possibilities are ever actually used by people.", "labels": [], "entities": []}, {"text": "We must learn to efficiently find these few, and accurately estimate their associated likelihoods.", "labels": [], "entities": []}, {"text": "We demonstrate effective learning of a globally normalized log-linear distribution with features to account for context dependence and communicative goals.", "labels": [], "entities": []}, {"text": "We use a stochastic gradient descent algorithm, where the key challenge is the need to compute feature expectations overall possible logical forms.", "labels": [], "entities": []}, {"text": "For that purpose, we present a multi-stage inference algorithm, which progressively constructs meaning representations with increasing complexity, and learns a pruning model to retain only those that are likely to lead to high probability expressions.", "labels": [], "entities": []}, {"text": "This approach allows us to consider a large The green, red, orange and yellow toys.", "labels": [], "entities": []}, {"text": "(1) The green, red, yellow, and orange objects.", "labels": [], "entities": []}, {"text": "(1) The red, green, yellow and orange toys.", "labels": [], "entities": []}, {"text": "(1) The red, yellow, orange and green objects.", "labels": [], "entities": []}, {"text": "(1) All the green, red, yellow and orange toys.", "labels": [], "entities": []}, {"text": "(1) All the yellow, orange, red and green objects.", "labels": [], "entities": []}, {"text": "(1) All the pieces that are not blue or brown.", "labels": [], "entities": []}, {"text": "(2) All items that are not brown or blue.", "labels": [], "entities": []}, {"text": "(2) All items that are not brown or blue.", "labels": [], "entities": []}, {"text": "(2) Everything that is not brown or blue.", "labels": [], "entities": []}, {"text": "(3) Everything that is not purple or blue.", "labels": [], "entities": []}, {"text": "(3) All but the black and blue ones.", "labels": [], "entities": []}, {"text": "(4) Any toy but the blue and brown toys.", "labels": [], "entities": []}, {"text": "(4) Everything that is green, red, orange or yellow.", "labels": [], "entities": []}, {"text": "(5) All objects that are not triangular or blue.", "labels": [], "entities": []}, {"text": "(6) Everything that is not blue or a wedge.", "labels": [], "entities": []}, {"text": "(7) Everything that is not a brown or blue toy.", "labels": [], "entities": []}, {"text": "(8) All but the blue piece and brown wedge.", "labels": [], "entities": []}, {"text": "(9) Everything except the brown wedge and the blue object.", "labels": [], "entities": []}, {"text": "(10) All pieces but the blue piece and brown triangle shape.", "labels": [], "entities": []}, {"text": "(b) (c): An example scene from our object selection dataset.", "labels": [], "entities": []}, {"text": "shows the image shown to subjects on Amazon Mechanical Turk.", "labels": [], "entities": [{"text": "Amazon Mechanical Turk", "start_pos": 37, "end_pos": 59, "type": "DATASET", "confidence": 0.9804645975430807}]}, {"text": "The target set G is the circled objects.", "labels": [], "entities": []}, {"text": "shows the 20 sentences provided as responses.", "labels": [], "entities": []}, {"text": "shows the empirical distribution\u02c6Pdistribution\u02c6 distribution\u02c6P (z|G, S) for this scene, estimated by labeling the sentences in.", "labels": [], "entities": []}, {"text": "The correspondence between a sentence in 1b and its labeled logical expression in 1c is indicated by the number in parentheses.", "labels": [], "entities": []}, {"text": "Section 5.1 presents a discussion of the space of possible logical forms.", "labels": [], "entities": []}, {"text": "set of possible meanings, while maintaining computational tractability.", "labels": [], "entities": []}, {"text": "To represent meaning we build on previous approaches that use lambda calculus).", "labels": [], "entities": []}, {"text": "We extend these techniques by modeling the types of plurality and coordination that are prominent in expressions which refer to sets.", "labels": [], "entities": []}, {"text": "We also present anew corpus for the task of referring expression generation.", "labels": [], "entities": [{"text": "referring expression generation", "start_pos": 44, "end_pos": 75, "type": "TASK", "confidence": 0.8331414063771566}]}, {"text": "1 While most previous REG data focused on naming single objects, The corpus was collected using Amazon Mechanical Turk and is available on the authors' websites.", "labels": [], "entities": [{"text": "Amazon Mechanical Turk", "start_pos": 96, "end_pos": 118, "type": "DATASET", "confidence": 0.9188636541366577}]}, {"text": "to the best of our knowledge, this is the first corpus with sufficient coverage for learning to name sets of objects.", "labels": [], "entities": []}, {"text": "Experiments demonstrate highly accurate learned models, able to generate over 87% of the expressions people used.", "labels": [], "entities": []}, {"text": "On the previously studied special case of single object reference, we achieve state-of-the-art performance, with over 35% relative error reduction over previous state of the art ().", "labels": [], "entities": [{"text": "relative error reduction", "start_pos": 122, "end_pos": 146, "type": "METRIC", "confidence": 0.7604486346244812}]}], "datasetContent": [{"text": "Data Collection Our dataset consists of 118 images, taken with a Microsoft Kinect camera.", "labels": [], "entities": []}, {"text": "These are the same images used by, but we create multiple prompts for each image by circling different objects, giving 269 scenes in total.", "labels": [], "entities": []}, {"text": "These scenes were shown to workers on Amazon Mechanical Turk 4 who were asked to imagine giving instructions to a robot and complete the sentence \"Please pickup \" in reference to the circled objects.", "labels": [], "entities": [{"text": "Amazon Mechanical Turk 4", "start_pos": 38, "end_pos": 62, "type": "DATASET", "confidence": 0.9575207382440567}]}, {"text": "Twenty referring expressions were collected for each scene, a total of 5380 expressions.", "labels": [], "entities": []}, {"text": "From this data, 43 scenes (860 expressions) were held-out for use in a test set.", "labels": [], "entities": []}, {"text": "Of the remaining scenes, the sentences of 30 were labeled with logical forms.", "labels": [], "entities": []}, {"text": "10 of these scenes (200 expressions) are used as a labeled initialization set, and 20 are used as a development test set (400 expressions).", "labels": [], "entities": []}, {"text": "A small number of expressions (\u223c5%) from the labeled initial set were discarded, either because they did not correctly name the target set, or because they used very rare attributes (such as texture, or location) to name the target objects.", "labels": [], "entities": []}, {"text": "Surrogate Labeling To avoid hand labeling the large majority of the scenes, we label the data with a learned semantic parser).", "labels": [], "entities": [{"text": "Surrogate Labeling", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.7865781188011169}]}, {"text": "We created a hand-made lexicon for the entire training set, which greatly simplifies the learning problem, and learned the parameters of the parsing moder on the 10-scene initialization set.", "labels": [], "entities": []}, {"text": "The weights were then further tuned using semi-supervised techniques () on the data to be labeled.", "labels": [], "entities": []}, {"text": "Testing on the development set shows that this parser achieves roughly 95% precision and 70% recall.", "labels": [], "entities": [{"text": "precision", "start_pos": 75, "end_pos": 84, "type": "METRIC", "confidence": 0.9995957016944885}, {"text": "recall", "start_pos": 93, "end_pos": 99, "type": "METRIC", "confidence": 0.9992411136627197}]}, {"text": "Using this parser, we label the sentences in our training set.", "labels": [], "entities": []}, {"text": "We only use scenes whereat least 15 sentences were successfully parsed.", "labels": [], "entities": []}, {"text": "This gives a training set of 141 scenes (2587 expressions).", "labels": [], "entities": []}, {"text": "Combining the automatically labeled training set with the hand-labelled initialization, development and heldout data, our labelled corpus totals 3938 labeled expressions.", "labels": [], "entities": []}, {"text": "By contrast, the popular TUNA furniture sub corpus ( ) contains 856 descriptions of 20 scenes, and although some of these refer to sets, these sets contain two objects at most.", "labels": [], "entities": [{"text": "TUNA furniture sub corpus", "start_pos": 25, "end_pos": 50, "type": "DATASET", "confidence": 0.8783578723669052}]}, {"text": "Framework Our experiments were implemented using the University of Washington Semantic Parsing Framework (Artzi and Zettlemoyer, 2013a).", "labels": [], "entities": []}, {"text": "Hyperparameters Our inference procedure requires two hyperparameters: M , the maximum complexity threshold, and k, the beam size.", "labels": [], "entities": []}, {"text": "In practice, we set these to the highest possible values which still allow for training to complete in a reasonable amount of time (under 12 hours).", "labels": [], "entities": []}, {"text": "M is set to 20, which is sufficient to cover 99.5% of the observed expressions.", "labels": [], "entities": [{"text": "M", "start_pos": 0, "end_pos": 1, "type": "METRIC", "confidence": 0.9775895476341248}]}, {"text": "The beam-size k is 100 for the first three complexity levels, and 50 thereafter.", "labels": [], "entities": []}, {"text": "For learning, we use the following hyperparameters, which were tuned on the development set: learning rate \u03b1 0 = .25, decay rate c = .02, number of epochs T = 10.", "labels": [], "entities": [{"text": "learning rate \u03b1 0", "start_pos": 93, "end_pos": 110, "type": "METRIC", "confidence": 0.9518283903598785}, {"text": "decay rate c", "start_pos": 118, "end_pos": 130, "type": "METRIC", "confidence": 0.9392633636792501}]}, {"text": "Evaluation Metrics Evaluation metrics used in REG research have assumed a system that produces a single output.", "labels": [], "entities": [{"text": "REG", "start_pos": 46, "end_pos": 49, "type": "TASK", "confidence": 0.972274661064148}]}, {"text": "Our goal is to achieve a distribution over logical forms that closely matches the distribution observed from human subjects.", "labels": [], "entities": []}, {"text": "Therefore, we compare our learned model to the labeled test data with mean absolute error: where Q is the empirical distribution observed in the training data.", "labels": [], "entities": [{"text": "mean absolute error", "start_pos": 70, "end_pos": 89, "type": "METRIC", "confidence": 0.6868611176808676}]}, {"text": "MAE measures the total probability mass which is assigned differently in the predicted distribution than in the empirical distribution.", "labels": [], "entities": [{"text": "MAE", "start_pos": 0, "end_pos": 3, "type": "METRIC", "confidence": 0.7450340986251831}]}, {"text": "We use MAE as opposed to KL divergence or data likelihood as both of these measures are uninformative when the support of the two distributions differ.", "labels": [], "entities": [{"text": "MAE", "start_pos": 7, "end_pos": 10, "type": "METRIC", "confidence": 0.9819726943969727}, {"text": "KL divergence", "start_pos": 25, "end_pos": 38, "type": "TASK", "confidence": 0.5986678749322891}]}, {"text": "This metric is quite strict; small differences in the estimated probabilities over a large number of logical expressions can result in a large error, even if the relative ordering is quite similar.", "labels": [], "entities": []}, {"text": "Therefore, we report the percentage of observed logical expressions which the model produces, either giving credit multiple times for duplicates (% dup ) or counting each unique logical expression in a scene once (% uniq ).", "labels": [], "entities": []}, {"text": "Put another way, % dup counts logical expression tokens, whereas % uniq counts types.", "labels": [], "entities": []}, {"text": "We also report the proportion of scenes where the most likely logical expression according to the model matched the most common one in the data (Top1).", "labels": [], "entities": []}, {"text": "Single Object Baseline In order to compare our method against the state of the art for generating referring expressions for single objects, we use the subset of our corpus where the target set is a single object.", "labels": [], "entities": []}, {"text": "This sub-corpus consists of 44 scenes for training and 11 held out for testing.", "labels": [], "entities": []}, {"text": "For comparison we re-implemented the probabilistic Visual Objects Algorithm (VOA) of.", "labels": [], "entities": []}, {"text": "We refer the readers to the original paper for details of the approach.", "labels": [], "entities": []}, {"text": "The parameters of the model were tuned on the training data: the prior likelihood estimates for each of the four attribute types (\u03b1 att ) were estimated as the relative frequency of each attribute in the data.", "labels": [], "entities": [{"text": "prior likelihood", "start_pos": 65, "end_pos": 81, "type": "METRIC", "confidence": 0.8494322896003723}]}, {"text": "We pick the ordering of attributes and the length penalty, \u03bb, from the cross-product of all possible 4!", "labels": [], "entities": [{"text": "length penalty", "start_pos": 43, "end_pos": 57, "type": "METRIC", "confidence": 0.9790085852146149}]}, {"text": "orderings and all integers on the range of, choosing the setting which results in the lowest average absolute error (AAE) on the training set.", "labels": [], "entities": [{"text": "average absolute error (AAE)", "start_pos": 93, "end_pos": 121, "type": "METRIC", "confidence": 0.8276323676109314}]}, {"text": "This process resulted in the following parameter settings: \u03b1 color = .916, \u03b1 shape = .586, \u03b1 type = .094, \u03b1 object = .506, AP ordering = [type, shape, object, color], \u03bb = 4.", "labels": [], "entities": []}, {"text": "Inference was done using 10,000 samples per scene.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Single object referring expression gener- ation results. Our approach (GenX) is compared  to the approach from Mitchell et al. (2013) (VOA).  Standard deviation over five shuffles of training set  is reported in parentheses.", "labels": [], "entities": []}, {"text": " Table 2: Results on the complete corpus for the  complete system (Full GenX), ablating the pruning  model (NoPrune) and the different features: without  coverage features (NoCOV), without structure fea- tures (NoSTRUC) and using only the logical expres- sion HeadExp features (HeadExpOnly). Standard  deviation over five runs is shown in parentheses.", "labels": [], "entities": [{"text": "Standard", "start_pos": 292, "end_pos": 300, "type": "METRIC", "confidence": 0.9959237575531006}]}]}