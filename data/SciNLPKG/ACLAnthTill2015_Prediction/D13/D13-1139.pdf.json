{"title": [{"text": "Shift-Reduce Word Reordering for Machine Translation", "labels": [], "entities": [{"text": "Shift-Reduce Word Reordering", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.8373451630274454}, {"text": "Machine Translation", "start_pos": 33, "end_pos": 52, "type": "TASK", "confidence": 0.7508981823921204}]}], "abstractContent": [{"text": "This paper presents a novel word reordering model that employs a shift-reduce parser for inversion transduction grammars.", "labels": [], "entities": [{"text": "word reordering", "start_pos": 28, "end_pos": 43, "type": "TASK", "confidence": 0.7382217347621918}]}, {"text": "Our model uses rich syntax parsing features for word reordering and runs in linear time.", "labels": [], "entities": [{"text": "word reordering", "start_pos": 48, "end_pos": 63, "type": "TASK", "confidence": 0.738554060459137}]}, {"text": "We apply it to postordering of phrase-based machine translation (PBMT) for Japanese-to-English patent tasks.", "labels": [], "entities": [{"text": "phrase-based machine translation (PBMT)", "start_pos": 31, "end_pos": 70, "type": "TASK", "confidence": 0.746018131573995}]}, {"text": "Our experimental results show that our method achieves a significant improvement of +3.1 BLEU scores against 30.15 BLEU scores of the baseline PBMT system.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 89, "end_pos": 93, "type": "METRIC", "confidence": 0.9942809343338013}, {"text": "BLEU", "start_pos": 115, "end_pos": 119, "type": "METRIC", "confidence": 0.9988358616828918}]}], "introductionContent": [{"text": "Even though phrase-based machine translation (PBMT) () and tree-based MT () systems have achieved great success, many problems remain for distinct language pairs, including long-distant word reordering.", "labels": [], "entities": [{"text": "phrase-based machine translation (PBMT)", "start_pos": 12, "end_pos": 51, "type": "TASK", "confidence": 0.7571426630020142}]}, {"text": "To improve such word reordering, one promising way is to separate it from the translation process as preordering () or postordering).", "labels": [], "entities": [{"text": "word reordering", "start_pos": 16, "end_pos": 31, "type": "TASK", "confidence": 0.7255000174045563}]}, {"text": "Many studies utilize a rulebased or a probabilistic model to perform a reordering decision at each node of a syntactic parse tree.", "labels": [], "entities": []}, {"text": "This paper presents a parser-based word reordering model that employs a shift-reduce parser for inversion transduction grammars (ITG) (.", "labels": [], "entities": [{"text": "parser-based word reordering", "start_pos": 22, "end_pos": 50, "type": "TASK", "confidence": 0.6458194951216379}]}, {"text": "To the best of our knowledge, this is the first study on a shift-reduce parser for word reordering.", "labels": [], "entities": [{"text": "word reordering", "start_pos": 83, "end_pos": 98, "type": "TASK", "confidence": 0.7277348935604095}]}, {"text": "The parser-based reordering approach uses rich syntax parsing features for reordering decisions.", "labels": [], "entities": []}, {"text": "Our propoesd method can also easily define such . .", "labels": [], "entities": []}], "datasetContent": [{"text": "We conducted experiments for NTCIR-9 and 10 patent data using a Japanese-English language pair.", "labels": [], "entities": [{"text": "NTCIR-9 and 10 patent data", "start_pos": 29, "end_pos": 55, "type": "DATASET", "confidence": 0.8260968804359436}]}, {"text": "Mecab 2 was used for the Japanese morphological analysis.", "labels": [], "entities": [{"text": "Japanese morphological analysis", "start_pos": 25, "end_pos": 56, "type": "TASK", "confidence": 0.6101934909820557}]}, {"text": "The data are summarized in.", "labels": [], "entities": []}, {"text": "We used Enju () for parsing the English training data and converted parse trees into HFE trees by a head-finalization scheme.", "labels": [], "entities": []}, {"text": "We extracted grammar rules from all the HFE trees and randomly selected 500,000 HFE trees to train the shift-reduce parser.", "labels": [], "entities": []}, {"text": "We used Moses () with lexicalized reordering and a 6-gram language model (LM) trained using SRILM) to translate the Japanese sentences into HFE sentences.", "labels": [], "entities": []}, {"text": "To recover the English sentences, our shift-reduce parser reordered only the 1-best HFE sentence.", "labels": [], "entities": []}, {"text": "Our strategy is much simpler than's because they used a linear inteporation of MT cost, parser cost and N -gram LM cost to generate the best English sentence from the n-best HFE sentences.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: NTCIR-9 and 10 data statistics.", "labels": [], "entities": [{"text": "NTCIR-9 and 10 data", "start_pos": 10, "end_pos": 29, "type": "DATASET", "confidence": 0.8420905768871307}]}, {"text": " Table 5. The re- sults clearly show that improvements of 1-gram pre-test9  test10  BLEU RIBES BLEU RIBES", "labels": [], "entities": [{"text": "BLEU RIBES BLEU RIBES", "start_pos": 84, "end_pos": 105, "type": "METRIC", "confidence": 0.7805390954017639}]}]}