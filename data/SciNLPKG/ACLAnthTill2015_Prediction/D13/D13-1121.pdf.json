{"title": [{"text": "Automatic Knowledge Acquisition for Case Alternation between the Passive and Active Voices in Japanese", "labels": [], "entities": [{"text": "Case Alternation", "start_pos": 36, "end_pos": 52, "type": "TASK", "confidence": 0.8004253506660461}]}], "abstractContent": [{"text": "We present a method for automatically acquiring knowledge for case alternation between the passive and active voices in Japanese.", "labels": [], "entities": [{"text": "case alternation", "start_pos": 62, "end_pos": 78, "type": "TASK", "confidence": 0.7482919096946716}]}, {"text": "By leveraging several linguistic constraints on alternation patterns and lexical case frames obtained from a large Web corpus, our method aligns a case frame in the passive voice to a corresponding case frame in the active voice and finds an alignment between their cases.", "labels": [], "entities": []}, {"text": "We then apply the acquired knowledge to a case alternation task and prove its usefulness.", "labels": [], "entities": [{"text": "case alternation task", "start_pos": 42, "end_pos": 63, "type": "TASK", "confidence": 0.802112857500712}]}], "introductionContent": [{"text": "Predicate-argument structure analysis is one of the fundamental techniques for many natural language applications such as recognition of textual entailment, information retrieval, and machine translation.", "labels": [], "entities": [{"text": "Predicate-argument structure analysis", "start_pos": 0, "end_pos": 37, "type": "TASK", "confidence": 0.9149978160858154}, {"text": "recognition of textual entailment", "start_pos": 122, "end_pos": 155, "type": "TASK", "confidence": 0.8983010053634644}, {"text": "information retrieval", "start_pos": 157, "end_pos": 178, "type": "TASK", "confidence": 0.8159353733062744}, {"text": "machine translation", "start_pos": 184, "end_pos": 203, "type": "TASK", "confidence": 0.8146477937698364}]}, {"text": "In Japanese, the relationship between a predicate and its argument is usually represented by using case particles 1 ().", "labels": [], "entities": []}, {"text": "However, since case particles vary depending on the voices, we have to take case alternation into account to represent predicate-argument structure.", "labels": [], "entities": []}, {"text": "There are thus two major types of representations: one uses surface cases, and the other uses normalized-cases for the base form of predicates.", "labels": [], "entities": []}, {"text": "For example, while the Kyoto University Text Corpus (), one of the major Japanese corpora that contains annotations of predicate-argument structures, adopts the former representation, the NAIST Text Corpora (, another major Japanese corpus, adopts the latter representation.", "labels": [], "entities": [{"text": "Kyoto University Text Corpus", "start_pos": 23, "end_pos": 51, "type": "DATASET", "confidence": 0.8997088074684143}, {"text": "NAIST Text Corpora", "start_pos": 188, "end_pos": 206, "type": "DATASET", "confidence": 0.9686761299769083}]}, {"text": "Examples and describe the same event in the passive and active voices, respectively.", "labels": [], "entities": []}, {"text": "When we use surface cases to represent the relationship between the predicate and its argument in Example (1), the case of \"\ud97b\udf59 (woman)\" is ga 2 and the case of \"\ud97b\udf59 (man)\" is ni.", "labels": [], "entities": [{"text": "Example", "start_pos": 98, "end_pos": 105, "type": "DATASET", "confidence": 0.8543268442153931}]}, {"text": "On the other hand, when we use the normalized-cases for the base form, the case of \"\ud97b\udf59 (woman)\" is wo 2 and the case of \"\ud97b\udf59 (man)\" is ga, which are the same as the surface cases in the active voice as in Example (2).", "labels": [], "entities": []}, {"text": "(1) woman-ga man-ni was pushed down (A woman was pushed down by a man.) man-ga woman-wo pushed down (A man pushed down a woman.)", "labels": [], "entities": []}, {"text": "Both representations have their own advantages.", "labels": [], "entities": []}, {"text": "Surface case analysis is easier than normalized-case analysis, especially when we consider omitted arguments, which are also called zero anaphors.", "labels": [], "entities": [{"text": "Surface case analysis", "start_pos": 0, "end_pos": 21, "type": "TASK", "confidence": 0.7788762052853903}]}, {"text": "In Japanese, zero anaphora frequently occurs, and the omitted unnormalizedcase of a zero anaphor is often the same as the surface case of its antecedent).", "labels": [], "entities": []}, {"text": "Therefore, surface case analysis suits zero anaphora resolution.", "labels": [], "entities": [{"text": "surface case analysis", "start_pos": 11, "end_pos": 32, "type": "TASK", "confidence": 0.7098667025566101}]}, {"text": "On the other hand, when we focus on the resulting predicate argument structures, the normalized-case structure is more useful.", "labels": [], "entities": []}, {"text": "Specifically, since a normalized-case structure represents the same meaning in the same representation, normalized-case analysis is useful for recognizing textual entailment and information retrieval.", "labels": [], "entities": [{"text": "recognizing textual entailment", "start_pos": 143, "end_pos": 173, "type": "TASK", "confidence": 0.7251017888387045}, {"text": "information retrieval", "start_pos": 178, "end_pos": 199, "type": "TASK", "confidence": 0.7958521246910095}]}, {"text": "Therefore, we need a system that first analyzes surface cases and then alternates the surface cases with normalized-cases.", "labels": [], "entities": []}, {"text": "In particular, we focus on the transformation of the passive voice into the active voice in this paper.", "labels": [], "entities": []}, {"text": "Passive-to-active voice transformation in English can be performed systematically, which does not depend on lexical information inmost cases.", "labels": [], "entities": [{"text": "Passive-to-active voice transformation", "start_pos": 0, "end_pos": 38, "type": "TASK", "confidence": 0.7259009679158529}]}, {"text": "However, in Japanese, the method of transformation depends on lexical information.", "labels": [], "entities": []}, {"text": "For example, while the case particle ni in Example (1) is alternated with gain the active voice, the case particle ni in Example (3) is not alternated in the active voice as in Example (4) even though both their predicates are \"\ud97b\udf59\ud97b\udf59\ud97b\udf59\ud97b\udf59\ud97b\udf59\ud97b\udf59\ud97b\udf59 (be pushed down).\" woman-ga sea-ni was pushed down (A woman was pushed down into the sea.) woman-wo sea-ni pushed down (\u03c6 pushed down a woman into the sea.)", "labels": [], "entities": []}, {"text": "The ni casein Example (1) indicates agent.", "labels": [], "entities": []}, {"text": "On the other hand, the ni casein Example (3) indicates direction.", "labels": [], "entities": []}, {"text": "To determine the difference is important for many NLP applications including machine translation.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 77, "end_pos": 96, "type": "TASK", "confidence": 0.8372934460639954}]}, {"text": "In fact, Google Translate (GT) 3 translates Examples (1) and (3) as \"Woman was pushed down in the man\" and \"Woman was pushed down in the sea,\" respectively, which maybe because GT cannot distinguish between the roles of ni in Examples (1) and (3).", "labels": [], "entities": []}], "datasetContent": [{"text": "We applied our algorithm to the case frames that are automatically constructed from a corpus consisting of about 6.9 billion Japanese sentences from the Web.", "labels": [], "entities": []}, {"text": "Of course, these case frames contain improper ones, that is, several frames mix several meanings or usages of the predicates.", "labels": [], "entities": []}, {"text": "Thus, it is difficult to evaluate the acquired knowledge itself.", "labels": [], "entities": []}, {"text": "Instead, we evaluate the usefulness of the acquired knowledge on a case alternation task between the passive and active voices.", "labels": [], "entities": []}, {"text": "Experiments without either development or training data In the first setting, we aligned the input passive case frame to one of the active case frames of the same predicate only by using sim SEM and sim DIST with the parameter \u03b1 = 1.", "labels": [], "entities": []}, {"text": "Therefore, this setting is fully unsupervised.", "labels": [], "entities": []}, {"text": "In this setting, the input surface cases are alternated as follows: 1.", "labels": [], "entities": []}, {"text": "If a passive sentence is input, perform syntactic and surface case structure analysis by using's model.", "labels": [], "entities": [{"text": "surface case structure analysis", "start_pos": 54, "end_pos": 85, "type": "TASK", "confidence": 0.7168664783239365}]}, {"text": "Their model identified a proper case frame for each predicate, and assigned arguments in the input sentence to case slots of the case frame.", "labels": [], "entities": []}, {"text": "2. By using the acquired knowledge for case alternation, alternate input surface cases with cases in the active voice.", "labels": [], "entities": [{"text": "case alternation", "start_pos": 39, "end_pos": 55, "type": "TASK", "confidence": 0.7443718910217285}]}, {"text": "We call this model Model 1.", "labels": [], "entities": []}, {"text": "For example, if Example (10) is input, the ga-case argument is assigned to the ga-case of the case frame \"\ud97b\udf59\ud97b\udf59\ud97b\udf59\ud97b\udf59\ud97b\udf59\ud97b\udf59\ud97b\udf59-5 (be pushed down-5).\"", "labels": [], "entities": []}, {"text": "Since this case is aligned to the wo-case of the case frame \"\ud97b\udf59\ud97b\udf59\ud97b\udf59\ud97b\udf59\ud97b\udf59-4 (push down-4)\" as shown in, this ga-case is alternated with wo-case.", "labels": [], "entities": []}, {"text": "Experiments with development data In the second setting, we aligned the input passive case frame to one of the active case frames of the same predicate by using sim SEM , sim DIST , and f PP with \u03b1 tuned on the development data.", "labels": [], "entities": []}, {"text": "In advance, we divided the tagged data into two parts just as did, both of which contained 1,788 case particles, and performed 2-fold cross-validation.", "labels": [], "entities": []}, {"text": "We used one part for development and the other for testing, and vice versa.", "labels": [], "entities": []}, {"text": "We tuned w(ga \u2192 c ga to ), w(c toga \u2192 ga) in Equation (i), and \u03b1 in Equation (ii) by a simple hill-climbing strategy.", "labels": [], "entities": []}, {"text": "Since the candidate cases for c ga to are ni, niyotte, kara, de, and NIL, and the candidate cases for c toga are wo, ni, no, and NIL, we defined parameter vector x as follows: Algorithm 2 shows the hill-climbing algorithm for tuning the parameter vector x, where f accuracy (x) is a function that returns the case alternation accuracy on the development data with parameter x.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 265, "end_pos": 273, "type": "METRIC", "confidence": 0.843716025352478}, {"text": "accuracy", "start_pos": 326, "end_pos": 334, "type": "METRIC", "confidence": 0.8407964110374451}]}, {"text": "This algorithm varies one parameter at a time with a stepsize of 0.1 until there is no accuracy improvement in the development data.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 87, "end_pos": 95, "type": "METRIC", "confidence": 0.9991331696510315}]}, {"text": "After acquiring knowledge for case alternation with the tuned parameter, we applied the same method for case alternation as the first setting.", "labels": [], "entities": [{"text": "case alternation", "start_pos": 30, "end_pos": 46, "type": "TASK", "confidence": 0.7519981265068054}, {"text": "case alternation", "start_pos": 104, "end_pos": 120, "type": "TASK", "confidence": 0.7477007806301117}]}, {"text": "We call this model Model 2.", "labels": [], "entities": []}, {"text": "Experiments with training data In the third setting, we also performed 2-fold cross validation, that is, we used one part of the divided tagged corpus  Specifically, we first tuned the parameter vector x on the training data and acquired the knowledge for case alternation with the tuned parameter.", "labels": [], "entities": [{"text": "case alternation", "start_pos": 256, "end_pos": 272, "type": "TASK", "confidence": 0.7399883568286896}]}, {"text": "By using the acquired knowledge, we alternated the input cases in both the training and test data and obtained the resulting case of Model 2.", "labels": [], "entities": []}, {"text": "Note that, we did not use any annotations for the test data in this process.", "labels": [], "entities": []}, {"text": "We then trained the SVMs on the training data and applied them to the test data using the resulting case as anew feature.", "labels": [], "entities": []}, {"text": "We call this model Model 3.", "labels": [], "entities": []}, {"text": "shows the results of the experiments without training data.", "labels": [], "entities": []}, {"text": "Baseline is a system that outputs the most frequently alternated cases in the development data, which was also used by.", "labels": [], "entities": [{"text": "Baseline", "start_pos": 0, "end_pos": 8, "type": "DATASET", "confidence": 0.8290246725082397}]}, {"text": "The baseline score was higher than that reported by Murata et al. because we modified 21 annotations.", "labels": [], "entities": []}, {"text": "We also performed experiments without using case distribution similarity or semantic similarity.", "labels": [], "entities": []}, {"text": "We call these models in the first setting Model 1 Sand Model 1 D , and these models in the second setting Model 2 Sand Model 2 D , respectively.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 5: Experimental results of case alternation without  training data.", "labels": [], "entities": [{"text": "case alternation", "start_pos": 34, "end_pos": 50, "type": "TASK", "confidence": 0.8414723873138428}]}]}