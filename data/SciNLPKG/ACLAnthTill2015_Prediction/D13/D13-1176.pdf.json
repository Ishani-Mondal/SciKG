{"title": [], "abstractContent": [{"text": "We introduce a class of probabilistic continuous translation models called Recurrent Continuous Translation Models that are purely based on continuous representations for words, phrases and sentences and do not rely on alignments or phrasal translation units.", "labels": [], "entities": [{"text": "Recurrent Continuous Translation", "start_pos": 75, "end_pos": 107, "type": "TASK", "confidence": 0.6703558365503947}]}, {"text": "The models have a generation and a conditioning aspect.", "labels": [], "entities": []}, {"text": "The generation of the translation is modelled with a target Recurrent Language Model, whereas the conditioning on the source sentence is modelled with a Convolu-tional Sentence Model.", "labels": [], "entities": []}, {"text": "Through various experiments , we show first that our models obtain a perplexity with respect to gold translations that is > 43% lower than that of state-of-the-art alignment-based translation models.", "labels": [], "entities": []}, {"text": "Secondly, we show that they are remarkably sensitive to the word order, syntax, and meaning of the source sentence despite lacking alignments.", "labels": [], "entities": []}, {"text": "Finally we show that they match a state-of-the-art system when rescoring n-best lists of translations.", "labels": [], "entities": [{"text": "rescoring n-best lists of translations", "start_pos": 63, "end_pos": 101, "type": "TASK", "confidence": 0.5795620977878571}]}], "introductionContent": [{"text": "In most statistical approaches to machine translation the basic units of translation are phrases that are composed of one or more words.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 34, "end_pos": 53, "type": "TASK", "confidence": 0.7728886902332306}]}, {"text": "A crucial component of translation systems are models that estimate translation probabilities for pairs of phrases, one phrase being from the source language and the other from the target language.", "labels": [], "entities": []}, {"text": "Such models count phrase pairs and their occurrences as distinct if the surface forms of the phrases are distinct.", "labels": [], "entities": []}, {"text": "Although distinct phrase pairs often share significant similarities, linguistic or otherwise, they do not share statistical weight in the models' estimation of their translation probabilities.", "labels": [], "entities": []}, {"text": "Besides ignoring the similarity of phrase pairs, this leads to general sparsity issues.", "labels": [], "entities": []}, {"text": "The estimation is sparse or skewed for the large number of rare or unseen phrase pairs, which grows exponentially in the length of the phrases, and the generalisation to other domains is often limited.", "labels": [], "entities": [{"text": "estimation", "start_pos": 4, "end_pos": 14, "type": "METRIC", "confidence": 0.9429709315299988}]}, {"text": "Continuous representations have shown promise at tackling these issues.", "labels": [], "entities": []}, {"text": "Continuous representations for words are able to capture their morphological, syntactic and semantic similarity).", "labels": [], "entities": []}, {"text": "They have been applied in continuous language models demonstrating the ability to overcome sparsity issues and to achieve state-of-theart performance (.", "labels": [], "entities": []}, {"text": "Word representations have also shown a marked sensitivity to conditioning information.", "labels": [], "entities": []}, {"text": "Continuous representations for characters have been deployed in character-level language models demonstrating notable language generation capabilities).", "labels": [], "entities": []}, {"text": "Continuous representations have also been constructed for phrases and sentences.", "labels": [], "entities": []}, {"text": "The representations are able to carry similarity and task dependent information, e.g. sentiment, paraphrase or dialogue labels, significantly beyond the word level and to accurately predict labels fora highly diverse range of unseen phrases and sentences).", "labels": [], "entities": []}, {"text": "Phrase-based continuous translation models were first proposed in () and re-cently further developed in).", "labels": [], "entities": [{"text": "Phrase-based continuous translation", "start_pos": 0, "end_pos": 35, "type": "TASK", "confidence": 0.6652343173821768}]}, {"text": "The models incorporate a principled way of estimating translation probabilities that robustly extends to rare and unseen phrases.", "labels": [], "entities": []}, {"text": "They achieve significant Bleu score improvements and yield semantically more suggestive translations.", "labels": [], "entities": [{"text": "Bleu score", "start_pos": 25, "end_pos": 35, "type": "METRIC", "confidence": 0.9754223227500916}]}, {"text": "Although wide-reaching in their scope, these models are limited to fixed-size source and target phrases and simplify the dependencies between the target words taking into account restricted target language modelling information.", "labels": [], "entities": []}, {"text": "We describe a class of continuous translation models called Recurrent Continuous Translation Models (RCTM) that map without loss of generality a sentence from the source language to a probability distribution over the sentences in the target language.", "labels": [], "entities": [{"text": "Recurrent Continuous Translation Models (RCTM)", "start_pos": 60, "end_pos": 106, "type": "TASK", "confidence": 0.7213025050503867}]}, {"text": "We define two specific RCTM architectures.", "labels": [], "entities": []}, {"text": "Both models adopt a recurrent language model for the generation of the target translation (.", "labels": [], "entities": []}, {"text": "In contrast to other n-gram approaches, the recurrent language model makes no Markov assumptions about the dependencies of the words in the target sentence.", "labels": [], "entities": []}, {"text": "The two RCTMs differ in the way they condition the target language model on the source sentence.", "labels": [], "entities": []}, {"text": "The first RCTM uses the convolutional sentence model to transform the source word representations into a representation for the source sentence.", "labels": [], "entities": []}, {"text": "The source sentence representation in turn constraints the generation of each target word.", "labels": [], "entities": []}, {"text": "The second RCTM introduces an intermediate representation.", "labels": [], "entities": [{"text": "RCTM", "start_pos": 11, "end_pos": 15, "type": "DATASET", "confidence": 0.9188310503959656}]}, {"text": "It uses a truncated variant of the convolutional sentence model to first transform the source word representations into representations for the target words; the latter then constrain the generation of the target sentence.", "labels": [], "entities": []}, {"text": "In both cases, the convolutional layers are used to generate combined representations for the phrases in a sentence from the representations of the words in the sentence.", "labels": [], "entities": []}, {"text": "An advantage of RCTMs is the lack of latent alignment segmentations and the sparsity associated with them.", "labels": [], "entities": [{"text": "latent alignment segmentations", "start_pos": 37, "end_pos": 67, "type": "TASK", "confidence": 0.5540643930435181}]}, {"text": "Connections between source and target words, phrases and sentences are learnt only implicitly as mappings between their continuous representations.", "labels": [], "entities": []}, {"text": "As we see in Sect.", "labels": [], "entities": []}, {"text": "5, these mappings often carry remarkably precise morphological, syntactic and semantic information.", "labels": [], "entities": []}, {"text": "Another advantage is that the probability of a translation under the models is efficiently computable requiring a small number of matrix-vector products that is linear in the length of the source and the target sentence.", "labels": [], "entities": []}, {"text": "Further, translations can be generated directly from the probability distribution of the RCTM without any external resources.", "labels": [], "entities": [{"text": "translations", "start_pos": 9, "end_pos": 21, "type": "TASK", "confidence": 0.9613542556762695}, {"text": "RCTM", "start_pos": 89, "end_pos": 93, "type": "DATASET", "confidence": 0.9116290211677551}]}, {"text": "We evaluate the performance of the models in four experiments.", "labels": [], "entities": []}, {"text": "Since the translation probabilities of the RCTMs are tractable, we can measure the perplexity of the models with respect to the reference translations.", "labels": [], "entities": []}, {"text": "The perplexity of the models is significantly lower than that of IBM Model 1 and is > 43% lower than the perplexity of a state-of-the-art variant of the IBM Model 2 (.", "labels": [], "entities": []}, {"text": "The second and third experiments aim to show the sensitivity of the output of the RCTM II to the linguistic information in the source sentence.", "labels": [], "entities": [{"text": "RCTM II", "start_pos": 82, "end_pos": 89, "type": "DATASET", "confidence": 0.8281128108501434}]}, {"text": "The second experiment shows that under a random permutation of the words in the source sentences, the perplexity of the model with respect to the reference translations becomes significantly worse, suggesting that the model is highly sensitive to word position and order.", "labels": [], "entities": []}, {"text": "The third experiment inspects the translations generated by the RCTM II.", "labels": [], "entities": [{"text": "RCTM II", "start_pos": 64, "end_pos": 71, "type": "DATASET", "confidence": 0.8961578607559204}]}, {"text": "The generated translations demonstrate remarkable morphological, syntactic and semantic agreement with the source sentence.", "labels": [], "entities": []}, {"text": "Finally, we test the RCTMs on the task of rescoring n-best lists of translations.", "labels": [], "entities": [{"text": "rescoring n-best lists of translations", "start_pos": 42, "end_pos": 80, "type": "TASK", "confidence": 0.717158830165863}]}, {"text": "The performance of the RCTM probabilities joined with a single word penalty feature matches the performance of the state-of-the-art translation system cdec that makes use of twelve features including five alignment-based translation models (.", "labels": [], "entities": [{"text": "RCTM", "start_pos": 23, "end_pos": 27, "type": "DATASET", "confidence": 0.931017279624939}]}, {"text": "2 by describing the general modelling framework underlying the RCTMs.", "labels": [], "entities": []}, {"text": "3 we describe the RCTM I and in Sect.", "labels": [], "entities": [{"text": "RCTM I", "start_pos": 18, "end_pos": 24, "type": "DATASET", "confidence": 0.894561767578125}]}, {"text": "Section 5 is dedicated to the four experiments and we conclude in Sect.", "labels": [], "entities": []}], "datasetContent": [{"text": "We report on four experiments.", "labels": [], "entities": []}, {"text": "The first experiment considers the perplexities of the models with respect to reference translations.", "labels": [], "entities": []}, {"text": "The second and third experiments test the sensitivity of the RCTM II to the linguistic aspects of the source sentences.", "labels": [], "entities": [{"text": "RCTM II", "start_pos": 61, "end_pos": 68, "type": "DATASET", "confidence": 0.8131163120269775}]}, {"text": "The final experiment tests the rescoring performance of the two models.", "labels": [], "entities": []}, {"text": "The fourth experiment tests the ability of the RCTM I and the RCTM II to choose the best translation among a large number of candidate translations produced by another system.", "labels": [], "entities": [{"text": "RCTM", "start_pos": 47, "end_pos": 51, "type": "DATASET", "confidence": 0.9508005380630493}, {"text": "RCTM", "start_pos": 62, "end_pos": 66, "type": "DATASET", "confidence": 0.9442685842514038}]}, {"text": "We use the cdec system to generate a list of 1000 best candidate translations for each English sentence in the four WMT-NT sets.", "labels": [], "entities": [{"text": "WMT-NT sets", "start_pos": 116, "end_pos": 127, "type": "DATASET", "confidence": 0.8258068859577179}]}, {"text": "We compare the rescoring performance of the RCTM I and the RCTM II with that of the cdec itself.", "labels": [], "entities": [{"text": "rescoring", "start_pos": 15, "end_pos": 24, "type": "METRIC", "confidence": 0.97185879945755}, {"text": "RCTM", "start_pos": 44, "end_pos": 48, "type": "DATASET", "confidence": 0.9258838891983032}]}, {"text": "cdec employs 12 engineered features including, among others, 5 translation models, 2 language model features and a word penalty feature (WP).", "labels": [], "entities": [{"text": "word penalty feature (WP)", "start_pos": 115, "end_pos": 140, "type": "METRIC", "confidence": 0.6964574207862219}]}, {"text": "For the RCTMs we simply interpolate the log probability assigned by the models to the candidate translations with the word penalty feature WP, tuned on the validation data.", "labels": [], "entities": [{"text": "word penalty feature WP", "start_pos": 118, "end_pos": 141, "type": "METRIC", "confidence": 0.7201173603534698}]}, {"text": "The results of the experiment are reported in Tab.", "labels": [], "entities": [{"text": "Tab.", "start_pos": 46, "end_pos": 50, "type": "DATASET", "confidence": 0.9644604027271271}]}, {"text": "4. While there is little variance in the resulting Bleu scores, the performance of the RCTMs shows that their probabilities correlate with translation quality.", "labels": [], "entities": [{"text": "Bleu", "start_pos": 51, "end_pos": 55, "type": "METRIC", "confidence": 0.9005486369132996}]}, {"text": "Combining a monolingual RLM feature with the RCTMs does not improve the scores, while reducing cdec to just one core translation probability and language model features drops its score by two to five tenths.", "labels": [], "entities": []}, {"text": "These results indicate that the RCTMs have been able to learn both translation and language modelling distributions.", "labels": [], "entities": [{"text": "translation and language modelling", "start_pos": 67, "end_pos": 101, "type": "TASK", "confidence": 0.7340768277645111}]}], "tableCaptions": [{"text": " Table 1: Perplexity results on the WMT-NT sets.", "labels": [], "entities": [{"text": "Perplexity", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9200946688652039}, {"text": "WMT-NT sets", "start_pos": 36, "end_pos": 47, "type": "DATASET", "confidence": 0.8894471824169159}]}, {"text": " Table 3: English source sentences, respective translations in French and candidate translations generated from the  RCTM II and ranked out of 2000 samples according to their decreasing probability. Note that end of sentence dots (.)  are generated as part of the translation.", "labels": [], "entities": [{"text": "RCTM II", "start_pos": 117, "end_pos": 124, "type": "DATASET", "confidence": 0.9358591437339783}]}, {"text": " Table 4: Bleu scores on the WMT-NT sets of each RCTM  linearly interpolated with a word penalty WP. The cdec  system includes WP as well as five translation models and  two language modelling features, among others.", "labels": [], "entities": [{"text": "Bleu", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9891136884689331}, {"text": "WMT-NT sets", "start_pos": 29, "end_pos": 40, "type": "DATASET", "confidence": 0.9442813694477081}]}]}