{"title": [{"text": "Breaking Out of Local Optima with Count Transforms and Model Recombination: A Study in Grammar Induction", "labels": [], "entities": [{"text": "Grammar Induction", "start_pos": 87, "end_pos": 104, "type": "TASK", "confidence": 0.6910942047834396}]}], "abstractContent": [{"text": "Many statistical learning problems in NLP call for local model search methods.", "labels": [], "entities": []}, {"text": "But accuracy tends to suffer with current techniques, which often explore either too narrowly or too broadly: hill-climbers can get stuck in local optima, whereas samplers maybe inefficient.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.9988574981689453}]}, {"text": "We propose to arrange individual local opti-mizers into organized networks.", "labels": [], "entities": []}, {"text": "Our building blocks are operators of two types: (i) transform , which suggests new places to search, via non-random restarts from already-found local optima; and (ii) join, which merges candidate solutions to find better optima.", "labels": [], "entities": []}, {"text": "Experiments on grammar induction show that pursuing different transforms (e.g., discarding parts of a learned model or ignoring portions of training data) results in improvements.", "labels": [], "entities": [{"text": "grammar induction", "start_pos": 15, "end_pos": 32, "type": "TASK", "confidence": 0.8905101418495178}]}, {"text": "Groups of locally-optimal solutions can be further perturbed jointly, by constructing mixtures.", "labels": [], "entities": []}, {"text": "Using these tools, we designed several modular dependency grammar induction networks of increasing complexity.", "labels": [], "entities": [{"text": "dependency grammar induction", "start_pos": 47, "end_pos": 75, "type": "TASK", "confidence": 0.6572726468245188}]}, {"text": "Our complete system achieves 48.6% accuracy (directed dependency macro-average overall 19 languages in the 2006/7 CoNLL data)-more than 5% higher than the previous state-of-the-art.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 35, "end_pos": 43, "type": "METRIC", "confidence": 0.9992702603340149}, {"text": "2006/7 CoNLL data", "start_pos": 107, "end_pos": 124, "type": "DATASET", "confidence": 0.6377245783805847}]}], "introductionContent": [{"text": "Statistical methods for grammar induction often boil down to solving non-convex optimization problems.", "labels": [], "entities": [{"text": "grammar induction", "start_pos": 24, "end_pos": 41, "type": "TASK", "confidence": 0.9391588568687439}]}, {"text": "Early work attempted to locally maximize the likelihood of a corpus, using EM to estimate probabilities of dependency arcs between word bigrams).", "labels": [], "entities": []}, {"text": "That parsing model has since been extended to make unsupervised learning more feasible ().", "labels": [], "entities": [{"text": "parsing", "start_pos": 5, "end_pos": 12, "type": "TASK", "confidence": 0.9629390835762024}]}, {"text": "But even the latest techniques can be quite error-prone and sensitive to initialization, because of approximate, local search.", "labels": [], "entities": [{"text": "initialization", "start_pos": 73, "end_pos": 87, "type": "TASK", "confidence": 0.9842824339866638}]}, {"text": "In theory, global optima can be found by enumerating all parse forests that derive a corpus, though this is usually prohibitively expensive in practice.", "labels": [], "entities": []}], "datasetContent": [{"text": "Evaluation is against held-out CoNLL shared task data (, spanning 19 languages.", "labels": [], "entities": [{"text": "CoNLL shared task data", "start_pos": 31, "end_pos": 53, "type": "DATASET", "confidence": 0.750266969203949}]}, {"text": "We compute performance as directed dependency accuracies (DDA), fractions of correct unlabeled arcs in parsed output (an extrinsic metric).", "labels": [], "entities": [{"text": "directed dependency accuracies (DDA)", "start_pos": 26, "end_pos": 62, "type": "METRIC", "confidence": 0.7133161723613739}]}, {"text": "For most WSJ experiments we include also sentence and parse tree cross-entropies (soft and hard EMs' intrinsic metrics), in bits per token (bpt).", "labels": [], "entities": []}, {"text": "Last, we checked how our algorithms generalize outside English WSJ, by testing in 23 more set-ups: all 2006/7 CoNLL test sets (, spanning 19 languages.", "labels": [], "entities": [{"text": "2006/7 CoNLL test sets", "start_pos": 103, "end_pos": 125, "type": "DATASET", "confidence": 0.6975415448347727}]}, {"text": "Most recent work evaluates against this multi-lingual data, with the unrealistic assumption of part-of-speech tags.", "labels": [], "entities": []}, {"text": "But since inducing high quality word clusters for many languages would be beyond the scope of our paper, here we too plugged in gold tags for word categories (instead of unsupervised tags, as in \u00a73-8).", "labels": [], "entities": []}, {"text": "We compared to the two strongest systems we knew: 10 MZ (Mare\u010dek and\u017dabokrtsk\u00b4yand\u02c7and\u017dabokrtsk\u00b4and\u017dabokrtsk\u00b4y, 2012) and SAJ (, which report average accuracies of 40.0 and 42.9% for CoNLL data (see).", "labels": [], "entities": [{"text": "Mare\u010dek and\u017dabokrtsk\u00b4yand\u02c7and\u017dabokrtsk\u00b4and\u017dabokrtsk\u00b4y, 2012)", "start_pos": 57, "end_pos": 117, "type": "DATASET", "confidence": 0.6610926568508149}, {"text": "SAJ", "start_pos": 122, "end_pos": 125, "type": "METRIC", "confidence": 0.66590815782547}, {"text": "accuracies", "start_pos": 150, "end_pos": 160, "type": "METRIC", "confidence": 0.6101457476615906}, {"text": "CoNLL data", "start_pos": 183, "end_pos": 193, "type": "DATASET", "confidence": 0.8536146581172943}]}, {"text": "Our fully-trained IFJ and GT systems score 40.0 and 47.6%.", "labels": [], "entities": [{"text": "IFJ", "start_pos": 18, "end_pos": 21, "type": "DATASET", "confidence": 0.8167028427124023}]}, {"text": "As before, combining these networks with our own implementation of the best previous state-of-the-art system (SAJ) yields a further improvement, increasing final accuracy to 48.6%.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 162, "end_pos": 170, "type": "METRIC", "confidence": 0.9292691349983215}]}, {"text": "These numbers differ from Ponvert et al.'s (2011) for the full Section 23 because we restricted their eval-ps.py script to a maximum length of 40 words, in our evaluation, to match other previous work: Golland et al.'s (2012,) for CCM and LLCCM;) for the rest.", "labels": [], "entities": [{"text": "LLCCM", "start_pos": 239, "end_pos": 244, "type": "DATASET", "confidence": 0.8407659530639648}]}, {"text": "During review, another strong system (Mare\u010dek and Straka, 2013, scoring 48.7%) of possible interest to the reader came out, exploiting prior knowledge of stopping probabilities (estimated from large POS-tagged corpora, via reducibility principles).: Harmonic mean (F 1 ) of precision (P) and recall (R) for unlabeled constituent bracketings on Section 23 of WSJ (sentences up to length 40) for our combined system (CS), recent state-of-the-art and the baselines.", "labels": [], "entities": [{"text": "Harmonic mean (F 1 ) of precision (P)", "start_pos": 250, "end_pos": 287, "type": "METRIC", "confidence": 0.9028703028505499}, {"text": "recall (R)", "start_pos": 292, "end_pos": 302, "type": "METRIC", "confidence": 0.9594183415174484}, {"text": "Section 23 of WSJ", "start_pos": 344, "end_pos": 361, "type": "DATASET", "confidence": 0.6683140248060226}]}], "tableCaptions": [{"text": " Table 1: Sentence string and parse tree cross-entropies (in bpt), and accuracies (DDA), on inter-punctuation fragments  up to length 15 (WSJ 15  split ) and its subset of simple, complete sentences (WSJ 15  simp , with exact tree accuracies -TA).", "labels": [], "entities": [{"text": "accuracies (DDA)", "start_pos": 71, "end_pos": 87, "type": "METRIC", "confidence": 0.9211219847202301}, {"text": "WSJ 15  split )", "start_pos": 138, "end_pos": 153, "type": "DATASET", "confidence": 0.8732277154922485}, {"text": "WSJ 15  simp", "start_pos": 200, "end_pos": 212, "type": "DATASET", "confidence": 0.9123383164405823}, {"text": "exact tree accuracies -TA", "start_pos": 220, "end_pos": 245, "type": "METRIC", "confidence": 0.6367146790027618}]}, {"text": " Table 2: Directed dependency accuracies (DDA) on Sec- tion 23 of WSJ (all sentences and up to length ten) for  recent systems, our full networks (IFJ and GT), and three- way combination (CS) with the previous state-of-the-art.", "labels": [], "entities": [{"text": "Directed dependency accuracies (DDA)", "start_pos": 10, "end_pos": 46, "type": "METRIC", "confidence": 0.6368620246648788}]}, {"text": " Table 3: Harmonic mean (F 1 ) of precision (P) and re- call (R) for unlabeled constituent bracketings on Section  23 of WSJ (sentences up to length 40) for our combined  system (CS), recent state-of-the-art and the baselines.", "labels": [], "entities": [{"text": "Harmonic mean (F 1 ) of precision (P)", "start_pos": 10, "end_pos": 47, "type": "METRIC", "confidence": 0.8984620300206271}, {"text": "re- call (R)", "start_pos": 52, "end_pos": 64, "type": "METRIC", "confidence": 0.9677950143814087}, {"text": "Section  23 of WSJ", "start_pos": 106, "end_pos": 124, "type": "DATASET", "confidence": 0.7765658348798752}]}, {"text": " Table 4: Blind evaluation on 2006/7 CoNLL test sets (all  sentences) for our full networks (IFJ and GT), previous  state-of-the-art systems of Spitkovsky et al. (2012b) and  Mare\u010dek and\u017dabokrtsk\u00b4yand\u02c7and\u017dabokrtsk\u00b4and\u017dabokrtsk\u00b4y (2012), and three-way combi- nation with SAJ (CS, including results up to length ten).", "labels": [], "entities": [{"text": "2006/7 CoNLL test sets", "start_pos": 30, "end_pos": 52, "type": "DATASET", "confidence": 0.697428971529007}]}]}