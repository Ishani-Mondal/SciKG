{"title": [{"text": "Translating into Morphologically Rich Languages with Synthetic Phrases", "labels": [], "entities": []}], "abstractContent": [{"text": "Translation into morphologically rich languages is an important but recalcitrant problem in MT.", "labels": [], "entities": [{"text": "Translation into morphologically rich languages", "start_pos": 0, "end_pos": 47, "type": "TASK", "confidence": 0.8322089672088623}, {"text": "MT", "start_pos": 92, "end_pos": 94, "type": "TASK", "confidence": 0.9897838234901428}]}, {"text": "We present a simple and effective approach that deals with the problem in two phases.", "labels": [], "entities": []}, {"text": "First, a discriminative model is learned to predict inflections of target words from rich source-side annotations.", "labels": [], "entities": []}, {"text": "Then, this model is used to create additional sentence-specific word-and phrase-level translations that are added to a standard translation model as \"synthetic\" phrases.", "labels": [], "entities": []}, {"text": "Our approach relies on morphological analysis of the target language, but we show that an unsupervised Bayesian model of morphology can successfully be used in place of a supervised analyzer.", "labels": [], "entities": []}, {"text": "We report significant improvements in translation quality when translating from English to Russian, Hebrew and Swahili.", "labels": [], "entities": []}], "introductionContent": [{"text": "Machine translation into morphologically rich languages is challenging, due to lexical sparsity and the large variety of grammatical features expressed with morphology.", "labels": [], "entities": [{"text": "Machine translation", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.7916207611560822}]}, {"text": "In this paper, we introduce a method that uses target language morphological grammars (either hand-crafted or learned unsupervisedly) to address this challenge and demonstrate its effectiveness at improving translation from English into several morphologically rich target languages.", "labels": [], "entities": []}, {"text": "Our approach decomposes the process of producing a translation fora word (or phrase) into two steps.", "labels": [], "entities": []}, {"text": "First, a meaning-bearing stem is chosen and then an appropriate inflection is selected using a feature-rich discriminative model that conditions on the source context of the word being translated.", "labels": [], "entities": []}, {"text": "Rather than attempting to directly produce fullsentence translations using such an elementary process, we use our model to generate translations of individual words and short phrases that augmenton a sentence-by-sentence basis-the inventory of translation rules obtained using standard translation rule extraction techniques.", "labels": [], "entities": [{"text": "fullsentence translations", "start_pos": 43, "end_pos": 68, "type": "TASK", "confidence": 0.7249721884727478}, {"text": "translation rule extraction", "start_pos": 286, "end_pos": 313, "type": "TASK", "confidence": 0.6590976019700369}]}, {"text": "We call these synthetic phrases.", "labels": [], "entities": []}, {"text": "The major advantages of our approach are: (i) synthesized forms are targeted to a specific translation context; (ii) multiple, alternative phrases maybe generated with the final choice among rules left to the global translation model; (iii) virtually no language-specific engineering is necessary; (iv) any phrase-or syntax-based decoder can be used without modification; and (v) we can generate forms that were not attested in the bilingual training data.", "labels": [], "entities": []}, {"text": "The paper is structured as follows.", "labels": [], "entities": []}, {"text": "We first present our \"translate-and-inflect\" model for predicting lexical translations into morphologically rich languages given a source word and its context ( \u00a72).", "labels": [], "entities": [{"text": "predicting lexical translations into morphologically rich languages", "start_pos": 55, "end_pos": 122, "type": "TASK", "confidence": 0.8514808927263532}]}, {"text": "Our approach requires a morphological grammar to relate surface forms to underlying stem, inflection pairs; we discuss how either a standard morphological analyzer or a simple Bayesian unsupervised analyzer can be used ( \u00a73).", "labels": [], "entities": []}, {"text": "After describing an efficient parameter estimation procedure for the inflection model ( \u00a74), we employ the translate-andinflect model in an MT system.", "labels": [], "entities": [{"text": "MT", "start_pos": 140, "end_pos": 142, "type": "TASK", "confidence": 0.9184663891792297}]}, {"text": "We describe how we use our model to synthesize translation options ( \u00a75) and then evaluate translation quality on English-Russian, English-Hebrew, and English-Swahili translation tasks, finding significant improvements in all language pairs ( \u00a76).", "labels": [], "entities": []}, {"text": "We finally review related work ( \u00a77) and conclude ( \u00a78).", "labels": [], "entities": []}], "datasetContent": [{"text": "Before considering the broader problem of integrating the inflection model in a machine translation system, we perform an artificial evaluation to verify that the model learns sensible source sentencetarget inflection patterns.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 80, "end_pos": 99, "type": "TASK", "confidence": 0.7531226873397827}]}, {"text": "To do so, we create an inflection test set as follows.", "labels": [], "entities": []}, {"text": "We preprocess the source (English) sentences exactly as during training ( \u00a72.2), and using the target language morphological analyzer, we convert each aligned target word to stem, inflection pairs.", "labels": [], "entities": []}, {"text": "We perform word alignment on the held-out MT development data for each language pair (cf. Table 1), exactly as if it were going to produce training instances, but instead we use them for testing.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 11, "end_pos": 25, "type": "TASK", "confidence": 0.7836847305297852}, {"text": "MT development", "start_pos": 42, "end_pos": 56, "type": "TASK", "confidence": 0.9179506897926331}]}, {"text": "Although the resulting dataset is noisy (e.g., due to alignment errors), this becomes our intrinsic evaluation test set.", "labels": [], "entities": []}, {"text": "Using this data, we measure inflection quality using two measurements: acc.", "labels": [], "entities": [{"text": "acc", "start_pos": 71, "end_pos": 74, "type": "METRIC", "confidence": 0.9800711274147034}]}, {"text": "We evaluate our approach in the standard discriminative MT framework.", "labels": [], "entities": [{"text": "MT", "start_pos": 56, "end_pos": 58, "type": "TASK", "confidence": 0.9105625748634338}]}, {"text": "We use cdec ( as our decoder and perform MIRA training to learn feature weights of the sentence translation model.", "labels": [], "entities": [{"text": "MIRA", "start_pos": 41, "end_pos": 45, "type": "METRIC", "confidence": 0.7725667953491211}, {"text": "sentence translation", "start_pos": 87, "end_pos": 107, "type": "TASK", "confidence": 0.7139486372470856}]}, {"text": "We compare the following configurations: \u2022 A baseline system, using a 4-gram language model trained on the entire monolingual and bilingual data available.", "labels": [], "entities": []}, {"text": "\u2022 An enriched system with a class-based n-gram language model 8 trained on the monolingual data mapped to 600 Brown clusters.", "labels": [], "entities": []}, {"text": "Classbased language modeling is a strong baseline for scenarios with high out-of-vocabulary rates but in which large amounts of monolingual target-language data are available.", "labels": [], "entities": [{"text": "Classbased language modeling", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.5755833089351654}]}, {"text": "\u2022 The enriched system further augmented with our inflected synthetic phrases.", "labels": [], "entities": []}, {"text": "We expect the class-based language model to be especially helpful here and capture some basic agreement patterns that can be learned more easily on dense clusters than from plain word sequences.", "labels": [], "entities": []}, {"text": "Detailed corpus statistics are given in: \u2022 The Russian data consist of the News Commentary parallel corpus and additional monolingual data crawled from news websites.", "labels": [], "entities": [{"text": "Russian data", "start_pos": 47, "end_pos": 59, "type": "DATASET", "confidence": 0.7235827445983887}, {"text": "News Commentary parallel corpus", "start_pos": 75, "end_pos": 106, "type": "DATASET", "confidence": 0.910433292388916}]}, {"text": "9 \u2022 The Hebrew parallel corpus is composed of transcribed TED talks (.", "labels": [], "entities": []}, {"text": "Additional monolingual news data is also used.", "labels": [], "entities": []}, {"text": "\u2022 The Swahili parallel corpus was obtained by crawling the Global Voices project website 10 for parallel articles.", "labels": [], "entities": [{"text": "Global Voices project website 10", "start_pos": 59, "end_pos": 91, "type": "DATASET", "confidence": 0.9142019510269165}]}, {"text": "Additional monolingual data was taken from the Helsinki Corpus of Swahili.", "labels": [], "entities": [{"text": "Helsinki Corpus of Swahili", "start_pos": 47, "end_pos": 73, "type": "DATASET", "confidence": 0.9814359396696091}]}, {"text": "11 We evaluate translation quality by translating and measuring the BLEU score of a 2000-3000 sentencelong evaluation corpus, averaging the results over 3 MIRA runs to control for optimizer instability).", "labels": [], "entities": [{"text": "translation", "start_pos": 15, "end_pos": 26, "type": "TASK", "confidence": 0.9644427299499512}, {"text": "BLEU score", "start_pos": 68, "end_pos": 78, "type": "METRIC", "confidence": 0.9736367464065552}, {"text": "MIRA", "start_pos": 155, "end_pos": 159, "type": "METRIC", "confidence": 0.9902779459953308}]}, {"text": "For all languages, using class language models improves over the baseline.", "labels": [], "entities": []}, {"text": "When synthetic phrases are added, significant additional improvements are obtained.", "labels": [], "entities": []}, {"text": "For the English-Russian language pair, where both supervised and unsupervised analyses can be obtained, we notice that expert-crafted morphological analyzers are more efficient at improving translation quality.", "labels": [], "entities": []}, {"text": "Globally, the amount of improvement observed varies depending on the language; this is most likely indicative of the quality of unsupervised morphological segmentations produced and the kinds of grammatical relations expressed morphologically.", "labels": [], "entities": []}, {"text": "Finally, to confirm the effectiveness of our approach as corpus size increases, we use our technique on top of a state-of-the art English-Russian system trained on data from the 8th ACL Workshop on Machine Translation (30M words of bilingual text and 410M words of monolingual text).", "labels": [], "entities": [{"text": "ACL Workshop on Machine Translation", "start_pos": 182, "end_pos": 217, "type": "TASK", "confidence": 0.5018925845623017}]}, {"text": "The setup is identical except for the addition of sparse", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Intrinsic evaluation of inflection model (N:  nouns, V: verbs, A: adjectives, M: numerals).", "labels": [], "entities": []}, {"text": " Table 3: Feature ablation experiments using supervised  Russian classification experiments.", "labels": [], "entities": []}, {"text": " Table 4: Translation quality (measured by BLEU) aver- aged over 3 MIRA runs.", "labels": [], "entities": [{"text": "Translation", "start_pos": 10, "end_pos": 21, "type": "TASK", "confidence": 0.9217962026596069}, {"text": "BLEU", "start_pos": 43, "end_pos": 47, "type": "METRIC", "confidence": 0.9988908171653748}, {"text": "MIRA", "start_pos": 67, "end_pos": 71, "type": "METRIC", "confidence": 0.8415573239326477}]}]}