{"title": [{"text": "Exploiting Domain Knowledge in Aspect Extraction", "labels": [], "entities": [{"text": "Exploiting Domain Knowledge in Aspect Extraction", "start_pos": 0, "end_pos": 48, "type": "TASK", "confidence": 0.6868011554082235}]}], "abstractContent": [{"text": "Aspect extraction is one of the key tasks in sentiment analysis.", "labels": [], "entities": [{"text": "Aspect extraction", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.9578160643577576}, {"text": "sentiment analysis", "start_pos": 45, "end_pos": 63, "type": "TASK", "confidence": 0.9649060964584351}]}, {"text": "In recent years, statistical models have been used for the task.", "labels": [], "entities": []}, {"text": "However, such models without any domain knowledge often produce aspects that are not interpreta-ble in applications.", "labels": [], "entities": []}, {"text": "To tackle the issue, some knowledge-based topic models have been proposed, which allow the user to input some prior domain knowledge to generate coherent aspects.", "labels": [], "entities": []}, {"text": "However, existing knowledge-based topic models have several major shortcomings , e.g., little work has been done to incorporate the cannot-link type of knowledge or to automatically adjust the number of topics based on domain knowledge.", "labels": [], "entities": []}, {"text": "This paper proposes a more advanced topic model, called MC-LDA (LDA with m-set and c-set), to address these problems, which is based on an Extended generalized P\u00f3lya urn (E-GPU) model (which is also proposed in this paper).", "labels": [], "entities": []}, {"text": "Experiments on real-life product reviews from a variety of domains show that MC-LDA outperforms the existing state-of-the-art models markedly.", "labels": [], "entities": []}], "introductionContent": [{"text": "In sentiment analysis and opinion mining, aspect extraction aims to extract entity aspects or features on which opinions have been expressed (.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 3, "end_pos": 21, "type": "TASK", "confidence": 0.9177259802818298}, {"text": "opinion mining", "start_pos": 26, "end_pos": 40, "type": "TASK", "confidence": 0.7349963635206223}, {"text": "aspect extraction", "start_pos": 42, "end_pos": 59, "type": "TASK", "confidence": 0.8113178312778473}]}, {"text": "For example, in a sentence \"The picture looks great,\" the aspect is \"picture.\"", "labels": [], "entities": []}, {"text": "Aspect extraction consists of two sub-tasks: (1) extracting all aspect terms (e.g., \"picture\") from the corpus, and (2) clustering aspect terms with similar meanings (e.g., cluster \"picture\" and \"photo\" into one aspect category as they mean the same in the domain \"Camera\").", "labels": [], "entities": [{"text": "Aspect extraction", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.9672180414199829}]}, {"text": "In this work, we adopt the topic modeling approach as it can perform both sub-tasks simultaneously (see \u00a7 2).", "labels": [], "entities": [{"text": "topic modeling", "start_pos": 27, "end_pos": 41, "type": "TASK", "confidence": 0.8211230933666229}]}, {"text": "Topic models, such as LDA (, provide an unsupervised framework for extracting latent topics in text documents.", "labels": [], "entities": [{"text": "extracting latent topics in text documents", "start_pos": 67, "end_pos": 109, "type": "TASK", "confidence": 0.7535423735777537}]}, {"text": "Topics are aspect categories (or simply aspects) in our context.", "labels": [], "entities": []}, {"text": "However, in recent years, researchers have found that fully unsupervised topic models may not produce topics that are very coherent fora particular application.", "labels": [], "entities": []}, {"text": "This is because the objective functions of topic models do not always correlate well with human judgments and needs.", "labels": [], "entities": []}, {"text": "To address the issue, several knowledge-based topic models have been proposed.", "labels": [], "entities": []}, {"text": "The DF-LDA model () incorporates two forms of prior knowledge, also called two types of constraints: must-links and cannot-links.", "labels": [], "entities": []}, {"text": "A must-link states that two words (or terms) should belong to the same topic whereas a cannotlink indicates that two words should not be in the same topic.", "labels": [], "entities": []}, {"text": "In), more general knowledge can be specified using firstorder logic.", "labels": [], "entities": []}, {"text": "In (, seeded models were proposed.", "labels": [], "entities": []}, {"text": "They enable the user to specify prior knowledge as seed words/terms for some topics.", "labels": [], "entities": []}, {"text": "also used word similarity as priors for guidance.", "labels": [], "entities": []}, {"text": "However, none of the existing models is capable of incorporating the cannot-link type of knowledge except DF-LDA (.", "labels": [], "entities": [{"text": "DF-LDA", "start_pos": 106, "end_pos": 112, "type": "DATASET", "confidence": 0.7593657374382019}]}, {"text": "Furthermore, none of the existing models, including DF-LDA, is able to automatically adjust the number of topics based on domain knowledge.", "labels": [], "entities": []}, {"text": "The domain knowledge, such as cannot-links, may change the number of topics.", "labels": [], "entities": []}, {"text": "There are two types of cannot-links: consistent and inconsistent with the domain corpus.", "labels": [], "entities": []}, {"text": "For example, in the reviews of domain \"Computer\", a topic model may generate two topics Battery and Screen that represent two different aspects.", "labels": [], "entities": []}, {"text": "A cannot-link {battery, screen} as the domain knowledge is thus consistent with the corpus.", "labels": [], "entities": []}, {"text": "However, words Amazon and Price may appear in the same topic due to their high cooccurrences in the Amazon.com review corpus.", "labels": [], "entities": [{"text": "Amazon", "start_pos": 15, "end_pos": 21, "type": "DATASET", "confidence": 0.8854008316993713}, {"text": "Amazon.com review corpus", "start_pos": 100, "end_pos": 124, "type": "DATASET", "confidence": 0.840813676516215}]}, {"text": "To separate them, a cannot-link {amazon, price} can be added as the domain knowledge, which is inconsistent with the corpus as these two words have high co-occurrences in the corpus.", "labels": [], "entities": []}, {"text": "In this case, the number of topics needs to be increased by 1 since the mixed topic has to be separated into two individual topics Amazon and Price.", "labels": [], "entities": [{"text": "Amazon", "start_pos": 131, "end_pos": 137, "type": "DATASET", "confidence": 0.8483288288116455}]}, {"text": "Apart from the above shortcoming, earlier knowledge-based topic models also have some major shortcomings: Incapability of handling multiple senses: A word typically has multiple meanings or senses.", "labels": [], "entities": []}, {"text": "For example, light can mean \"of little weight\" or \"something that makes things visible.\"", "labels": [], "entities": []}, {"text": "DF-LDA cannot handle multiple senses because its definition of must-link is transitive.", "labels": [], "entities": [{"text": "DF-LDA", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.8373253345489502}]}, {"text": "That is, if A and B form a must-link, and B and C form a must-link, it implies a must-link between A and C, indicating A, B, and C should be in the same topic.", "labels": [], "entities": []}, {"text": "This case also applies to the models in,, and (Mukherjee and.", "labels": [], "entities": []}, {"text": "Although the model in () allows multiple senses, it requires that each topic has at most one set of seed words (seed set), which is restrictive as the amount of knowledge should not be limited.", "labels": [], "entities": []}, {"text": "Sensitivity to the adverse effect of knowledge: When using must-links or seeds, existing models basically try to ensure that the words in a mustlink or a seed set have similar probabilities under a topic.", "labels": [], "entities": []}, {"text": "This causes a problem: if a must-link comprises of a frequent word and an infrequent word, due to the redistribution of probability mass, the probability of the frequent word will decrease while the probability of the infrequent word will increase.", "labels": [], "entities": []}, {"text": "This can harm the final topics because the attenuation of the frequent (often domain important) words can result in some irrelevant words being ranked higher (with higher probabilities).", "labels": [], "entities": []}, {"text": "To address the above shortcomings, we define m-set (for must-set) as a set of words that should belong to the same topic and c-set (cannot-set) as a set of words that should not be in the same topic.", "labels": [], "entities": []}, {"text": "They are similar to must-link and cannot-link but m-sets do not enforce transitivity.", "labels": [], "entities": []}, {"text": "Transitivity is the main cause of the inability to handle multiple senses.", "labels": [], "entities": []}, {"text": "Our m-sets and c-sets are also more concise providing knowledge in the context of a set.", "labels": [], "entities": []}, {"text": "As in (, we assume that there is no conflict between m-sets and c-sets, i.e., if \u00ed \u00b5\u00ed\u00b1\u00a4 1 is a cannot-word of \u00ed \u00b5\u00ed\u00b1\u00a4 2 (i.e., shares a c-set with \u00ed \u00b5\u00ed\u00b1\u00a4 2 ), any word that shares an m-set with \u00ed \u00b5\u00ed\u00b1\u00a4 1 is also a cannot-word of \u00ed \u00b5\u00ed\u00b1\u00a4 2 . Note that knowledge as m-sets has also been used in and.", "labels": [], "entities": []}, {"text": "We then propose anew topic model, called MC-LDA (LDA with m-set and c-set), which is not only able to deal with c-sets and automatically adjust the number of topics, but also deal with the multiple senses and adverse effect of knowledge problems at the same time.", "labels": [], "entities": []}, {"text": "For the issue of multiple senses, anew latent variable \u00ed \u00b5\u00ed\u00b1 is added to LDA to distinguish multiple senses ( \u00a7 3).", "labels": [], "entities": [{"text": "latent variable \u00ed \u00b5\u00ed\u00b1", "start_pos": 39, "end_pos": 60, "type": "METRIC", "confidence": 0.6654569983482361}, {"text": "LDA", "start_pos": 73, "end_pos": 76, "type": "DATASET", "confidence": 0.6775549650192261}]}, {"text": "Then, we employ the generalized P\u00f3lya urn (GPU) model to address the issue of adverse effect of knowledge ( \u00a7 4).", "labels": [], "entities": [{"text": "P\u00f3lya urn (GPU)", "start_pos": 32, "end_pos": 47, "type": "DATASET", "confidence": 0.6857653498649597}]}, {"text": "Deviating from the standard topic modeling approaches, we propose the Extended generalized P\u00f3lya urn (E-GPU) model ( \u00a7 5).", "labels": [], "entities": [{"text": "P\u00f3lya urn (E-GPU)", "start_pos": 91, "end_pos": 108, "type": "DATASET", "confidence": 0.7818368434906006}]}, {"text": "E-GPU extends the GPU model to enable multi-urn interactions.", "labels": [], "entities": [{"text": "E-GPU", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.9383585453033447}]}, {"text": "This is necessary for handling c-sets and for adjusting the number of topics.", "labels": [], "entities": []}, {"text": "E-GPU is the heart of MC-LDA.", "labels": [], "entities": []}, {"text": "Due to the extension, anew inference mechanism is designed for MC-LDA ( \u00a7 6).", "labels": [], "entities": []}, {"text": "Note that E-GPU is generic and can be used in any appropriate application.", "labels": [], "entities": []}, {"text": "In summary, this paper makes the following three contributions: 1.", "labels": [], "entities": []}, {"text": "It proposed anew knowledge-based topic model called MC-LDA, which is able to use both m-sets and c-sets, as well as automatically adjust the number of topics based on domain knowledge.", "labels": [], "entities": []}, {"text": "At the same time, it can deal with some other major shortcomings of early existing models.", "labels": [], "entities": []}, {"text": "To our knowledge, none of the existing knowledge-based models is as comprehensive as MC-LDA in terms of capabilities.", "labels": [], "entities": []}, {"text": "2. It proposed the E-GPU model to enable multiurn interactions, which enables c-sets to be naturally integrated into a topic model.", "labels": [], "entities": []}, {"text": "To the best of our knowledge, E-GPU has not been proposed and used before.", "labels": [], "entities": []}, {"text": "3. A comprehensive evaluation has been conducted to compare MC-LDA with several state-ofthe-art models.", "labels": [], "entities": []}, {"text": "Experimental results based on both qualitative and quantitative measures demonstrate the superiority of MC-LDA.", "labels": [], "entities": []}, {"text": "1656 studied opinion summarization outside the reviews.", "labels": [], "entities": [{"text": "opinion summarization", "start_pos": 13, "end_pos": 34, "type": "TASK", "confidence": 0.6133916527032852}]}, {"text": "Some other works related with sentiment analysis include.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 30, "end_pos": 48, "type": "TASK", "confidence": 0.9719782471656799}]}, {"text": "In this work, we focus on topic models owing to their advantage of performing both aspect extraction and clustering simultaneously.", "labels": [], "entities": [{"text": "aspect extraction", "start_pos": 83, "end_pos": 100, "type": "TASK", "confidence": 0.7178846895694733}]}, {"text": "All other approaches only perform extraction.", "labels": [], "entities": []}, {"text": "Although there are several related works on clustering aspect terms (e.g.,), they all assume that the aspect terms have been extracted beforehand.", "labels": [], "entities": [{"text": "clustering aspect terms", "start_pos": 44, "end_pos": 67, "type": "TASK", "confidence": 0.8450746536254883}]}, {"text": "We also notice that some aspect extraction models in sentiment analysis separately discover aspect words and aspect specific sentiment words (e.g.,.", "labels": [], "entities": [{"text": "aspect extraction", "start_pos": 25, "end_pos": 42, "type": "TASK", "confidence": 0.7249284386634827}, {"text": "sentiment analysis", "start_pos": 53, "end_pos": 71, "type": "TASK", "confidence": 0.9071058034896851}]}, {"text": "Our proposed model does not separate them as most sentiment words also imply aspects and most adjectives modify specific attributes of objects.", "labels": [], "entities": []}, {"text": "For example, sentiment words expensive and beautiful imply aspects price and appearance respectively.", "labels": [], "entities": []}, {"text": "Regarding the knowledge-based models, besides those discussed in \u00a7 1, the model ( ) enables the user to provide guidance interactively. and used document labels in supervised setting.", "labels": [], "entities": []}, {"text": "In (), we proposed MDK-LDA to leverage multi-domain knowledge, which serves as the basic mechanism to exploit m-sets in MC-LDA.", "labels": [], "entities": []}, {"text": "In (), we proposed a framework (called GK-LDA) to explicitly deal with the wrong knowledge when exploring the lexical semantic relations as the general (domain independent) knowledge in topic models.", "labels": [], "entities": []}, {"text": "But these models above did not consider the knowledge in the form of c-sets (or cannot-links).", "labels": [], "entities": []}, {"text": "The generalized P\u00f3lya urn (GPU) model was first introduced in LDA by show that using domain knowledge can significantly improve aspect extraction.", "labels": [], "entities": [{"text": "aspect extraction", "start_pos": 128, "end_pos": 145, "type": "TASK", "confidence": 0.8217687606811523}]}, {"text": "The GPU model was also employed in topic models in our work of.", "labels": [], "entities": []}, {"text": "In this paper, we propose the Extended GPU (E-GPU) model.", "labels": [], "entities": []}, {"text": "The E-GPU model is more powerful in handling complex situations in dealing with c-sets.", "labels": [], "entities": []}], "datasetContent": [{"text": "We now evaluate the proposed MC-LDA model and compare it with state-of-the-art existing models.", "labels": [], "entities": []}, {"text": "Two unsupervised baseline models that we compare with are: \u2022 LDA: LDA is the basic unsupervised topic model ().", "labels": [], "entities": []}, {"text": "\u2022 LDA-GPU: LDA with GPU ().", "labels": [], "entities": []}, {"text": "Specifically, LDA-GPU applies GPU in LDA using co-document frequency.", "labels": [], "entities": []}, {"text": "As for knowledge-based models, we focus on comparing with DF-LDA model, which is perhaps the best known knowledge-based model and it allows both mustlinks and cannot-links.", "labels": [], "entities": []}, {"text": "For a comprehensive evaluation, we consider the following variations of MC-LDA and DF-LDA: \u2022 MC-LDA: MC-LDA with both m-sets and csets.", "labels": [], "entities": []}, {"text": "This is the newly proposed model.", "labels": [], "entities": []}, {"text": "\u2022 M-LDA: MC-LDA with m-sets only.", "labels": [], "entities": []}, {"text": "This is the MDK-LDA model in ().", "labels": [], "entities": []}, {"text": "\u2022 DF-M: DF-LDA with must-links only.", "labels": [], "entities": [{"text": "DF-M", "start_pos": 2, "end_pos": 6, "type": "DATASET", "confidence": 0.5830986499786377}]}, {"text": "\u2022 DF-MC: DF-LDA with both must-links and cannot-links.", "labels": [], "entities": []}, {"text": "This is the full DF-LDA model in ().", "labels": [], "entities": []}, {"text": "We do not compare with seeded models in ( as seed sets are special cases of must-links and they also do not allow c-sets (or cannot-links).", "labels": [], "entities": []}, {"text": "Datasets: We use product reviews from four domains (types of products) from Amazon.com for evaluation.", "labels": [], "entities": []}, {"text": "The corpus statistics are shown in Table 2 (columns 2 and 3).", "labels": [], "entities": []}, {"text": "The domains are \"Camera,\" \"Food,\" \"Computer,\" and \"Care\" (short for \"Personal Care\").", "labels": [], "entities": []}, {"text": "We have made the datasets publically available at the website of the first author.", "labels": [], "entities": []}, {"text": "Pre-processing: We ran the Stanford Core NLP Tools 1 to perform sentence detection and lemmatization.", "labels": [], "entities": [{"text": "Stanford Core NLP Tools 1", "start_pos": 27, "end_pos": 52, "type": "DATASET", "confidence": 0.9384854674339295}, {"text": "sentence detection", "start_pos": 64, "end_pos": 82, "type": "TASK", "confidence": 0.7867798805236816}]}, {"text": "Punctuations, stopwords 2 , numbers and words appearing less than 5 times in each corpus were removed.", "labels": [], "entities": []}, {"text": "The domain name was also removed, e.g., word camera in the domain \"Camera\", since it co-occurs with most words in the corpus, leading to high similarity among topics/aspects.", "labels": [], "entities": []}, {"text": "Sentences as documents: As noted in , when standard topic models are applied to reviews as documents, they tend to produce topics that correspond to global properties of products (e.g., brand name), which make topics overlapping with each other.", "labels": [], "entities": []}, {"text": "The reason is that all reviews of the same type of products discuss about the same aspects of these products.", "labels": [], "entities": []}, {"text": "Only the brand names and product names are different.", "labels": [], "entities": []}, {"text": "Thus, using individual reviews for modeling is not very effective.", "labels": [], "entities": []}, {"text": "Although there are approaches which model sentences (, we take the approach of, dividing each review into sentences and treating each sentence as an independent document.", "labels": [], "entities": []}, {"text": "Sentences can be used by all three baselines without any change to their models.", "labels": [], "entities": []}, {"text": "Although the relationships between sentences are lost, the data is fair to all models.", "labels": [], "entities": []}, {"text": "Parameter settings: For all models, posterior inference was drawn using 1000 Gibbs iterations with an initial burn-in of 100 iterations.", "labels": [], "entities": []}, {"text": "For all models, we set \u00ed \u00b5\u00ed\u00bb\u00bc = 1 and \u00ed \u00b5\u00ed\u00bb\u00bd = 0.1.", "labels": [], "entities": []}, {"text": "We found that small changes of \u00ed \u00b5\u00ed\u00bb\u00bc and \u00ed \u00b5\u00ed\u00bb\u00bd did not affect the results much, which was also reported in (Jo and Oh, 2011) who also used online reviews.", "labels": [], "entities": []}, {"text": "For the number of topics T, we tried different values (see \u00a77.2) as it is hard to know the exact number of topics.", "labels": [], "entities": []}, {"text": "While non-parametric Bayesian approaches () aim to estimate \u00ed \u00b5\u00ed\u00b1\u0087 from the corpus, they are often sensitive to the hyper-parameters..", "labels": [], "entities": [{"text": "\u00ed \u00b5\u00ed\u00b1\u0087", "start_pos": 60, "end_pos": 66, "type": "METRIC", "confidence": 0.7766446471214294}]}, {"text": "Corpus statistics with #m-sets and #c-sets having at least two words.", "labels": [], "entities": []}, {"text": "For DF-LDA, we followed () to generate must-links and cannot-links from our domain knowledge.", "labels": [], "entities": []}, {"text": "We then ran DF-LDA 3 while keeping its parameters as proposed in) (we also experimented with different parameter settings but they did not produce better results).", "labels": [], "entities": []}, {"text": "For our proposed model, we estimated the thresholds using cross validation in our pilot experiments.", "labels": [], "entities": []}, {"text": "Estimated value \u00ed \u00b5\u00ed\u00bc\u008e = 0.2 in equation 3 yielded good results.", "labels": [], "entities": [{"text": "Estimated value \u00ed \u00b5\u00ed\u00bc\u008e", "start_pos": 0, "end_pos": 22, "type": "METRIC", "confidence": 0.7322218179702759}]}, {"text": "The second stage (steps 2 and 3) of the Gibbs sampler for MC-LDA (for dealing with c-sets) is applied after burn-in phrase.", "labels": [], "entities": []}, {"text": "Domain knowledge: User knowledge about a domain can vary a great deal.", "labels": [], "entities": []}, {"text": "Different users may have very different knowledge.", "labels": [], "entities": []}, {"text": "To reduce this variance fora more reliable evaluation, instead of asking a human user to provide m-sets, we obtain the synonym sets and the antonym sets of each word that is a noun or adjective (as words of other parts-of-speech usually do not indicate aspects) from WordNet and manually verify the words in those sets for the domain.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 267, "end_pos": 274, "type": "DATASET", "confidence": 0.9654688239097595}]}, {"text": "Note that if a word \u00ed \u00b5\u00ed\u00b1\u00a4 is not provided with any m-set, it is treated as a singleton m-set {\u00ed \u00b5\u00ed\u00b1\u00a4}.", "labels": [], "entities": []}, {"text": "For c-sets, we ran LDA in each domain and provide c-sets based on the wrong results of LDA as in (.", "labels": [], "entities": []}, {"text": "Then, the knowledge is provided to each model in the format required by each model.", "labels": [], "entities": []}, {"text": "The numbers of m-sets and c-sets are listed in columns 4 and 5 of.", "labels": [], "entities": []}, {"text": "Duplicate sets have been removed.", "labels": [], "entities": []}, {"text": "In this section, we evaluate our proposed MC-3 http://pages.cs.wisc.edu/~andrzeje/research/df_lda.html LDA model objectively.", "labels": [], "entities": []}, {"text": "Topic models are often evaluated using perplexity on held-out test data.", "labels": [], "entities": []}, {"text": "However, the perplexity metric does not reflect the semantic coherence of individual topics learned by a topic model (.", "labels": [], "entities": []}, {"text": "Recent research has shown potential issues with perplexity as a measure: suggested that the perplexity can sometimes be contrary to human judgments.", "labels": [], "entities": []}, {"text": "Also, perplexity does not really reflect our goal of finding coherent aspects with accurate semantic clustering.", "labels": [], "entities": []}, {"text": "It only provides a measure of how well the model fits the data.", "labels": [], "entities": []}, {"text": "The Topic Coherence metric (Mimno et al., 2011) (also called the \"UMass\" measure () was proposed as a better alternative for assessing topic quality.", "labels": [], "entities": []}, {"text": "This metric relies upon word co-occurrence statistics within the documents, and does not depend on external resources or human labeling.", "labels": [], "entities": []}, {"text": "It was shown that topic coherence is highly consistent with human expert labeling by.", "labels": [], "entities": []}, {"text": "Higher topic coherence score indicates higher quality of topics, i.e., better topic interpretability.", "labels": [], "entities": []}, {"text": "Since our aim is to make topics more interpretable and conformable to human judgments, we worked with two judges who are familiar with Amazon products and reviews to evaluate the models subjectively.", "labels": [], "entities": []}, {"text": "Since topics from topic models are rankings based on word probability and we do not know the number of correct topical words, a natural way to evaluate these rankings is to use Precision@n (or p@n) which was also used in , where n is the rank position.", "labels": [], "entities": []}, {"text": "We give p@n for n = 5 and 10.", "labels": [], "entities": []}, {"text": "There are two steps inhuman evaluation: topic labeling and word labeling.", "labels": [], "entities": [{"text": "topic labeling", "start_pos": 40, "end_pos": 54, "type": "TASK", "confidence": 0.6880173832178116}, {"text": "word labeling", "start_pos": 59, "end_pos": 72, "type": "TASK", "confidence": 0.7589590847492218}]}, {"text": "Topic Labeling: We followed the instructions in) and asked the judges to label each topic as good or bad.", "labels": [], "entities": [{"text": "Topic Labeling", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.8242278397083282}]}, {"text": "Each topic was presented as a list of 10 most probable words in descending order of their probabilities under that topic.", "labels": [], "entities": []}, {"text": "The models which generated the topics for labeling were obscure to the judges.", "labels": [], "entities": [{"text": "labeling", "start_pos": 42, "end_pos": 50, "type": "TASK", "confidence": 0.9616191983222961}]}, {"text": "In general, each topic was annotated as good if it had more than half of its words coherently related to each other representing a semantic concept together; otherwise bad.", "labels": [], "entities": []}, {"text": "Agreement of human judges on topic labeling using Cohen's Kappa yielded a score of 0.92 indicating almost perfect agreements according to the scale in.", "labels": [], "entities": []}, {"text": "This is reasonable as topic labeling is an easy task and semantic coherence can be judged well by humans.", "labels": [], "entities": [{"text": "topic labeling", "start_pos": 22, "end_pos": 36, "type": "TASK", "confidence": 0.7872863709926605}]}, {"text": "Word Labeling: After topic labeling, we chose the topics, which were labeled as good by both judges, as good topics.", "labels": [], "entities": [{"text": "Word Labeling", "start_pos": 0, "end_pos": 13, "type": "TASK", "confidence": 0.6951932013034821}, {"text": "topic labeling", "start_pos": 21, "end_pos": 35, "type": "TASK", "confidence": 0.7270251512527466}]}, {"text": "Then, we asked the two judges to label each word of the top 10 words in these good topics.", "labels": [], "entities": []}, {"text": "Each word was annotated as correct if it was coherently related to the concept represented by the topic; otherwise incorrect.", "labels": [], "entities": []}, {"text": "Since judges already had the conception of each topic in mind when they were labeling topics, labeling each word was not difficult which explains the high Kappa score for this labeling task (score = 0.892).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2. Corpus statistics with #m-sets and #c-sets  having at least two words.", "labels": [], "entities": []}, {"text": " Table 3. Number of good topics of each model.", "labels": [], "entities": [{"text": "Number", "start_pos": 10, "end_pos": 16, "type": "METRIC", "confidence": 0.9636781215667725}]}]}