{"title": [{"text": "A Unified Model for Topics, Events and Users on Twitter", "labels": [], "entities": []}], "abstractContent": [{"text": "With the rapid growth of social media, Twitter has become one of the most widely adopted platforms for people to post short and instant message.", "labels": [], "entities": []}, {"text": "On the one hand, people tweets about their daily lives, and on the other hand, when major events happen, people also follow and tweet about them.", "labels": [], "entities": []}, {"text": "Moreover, people's posting behaviors on events are often closely tied to their personal interests.", "labels": [], "entities": []}, {"text": "In this paper, we try to model topics, events and users on Twitter in a unified way.", "labels": [], "entities": []}, {"text": "We propose a model which combines an LDA-like topic model and the Recurrent Chinese Restaurant Process to capture topics and events.", "labels": [], "entities": []}, {"text": "We further propose a duration-based regularization component to find bursty events.", "labels": [], "entities": []}, {"text": "We also propose to use event-topic affinity vectors to model the association between events and topics.", "labels": [], "entities": []}, {"text": "Our experiments shows that our model can accurately identify meaningful events and the event-topic affinity vectors are effective for event recommendation and grouping events by topics.", "labels": [], "entities": [{"text": "event recommendation", "start_pos": 134, "end_pos": 154, "type": "TASK", "confidence": 0.6982271373271942}]}], "introductionContent": [{"text": "Twitter is arguably the most popular microblog site where people can post short, instant messages to share with families, friends and the rest of the world.", "labels": [], "entities": []}, {"text": "For content analysis on Twitter, two important concepts have been repeatedly visited: (1) Topics.", "labels": [], "entities": []}, {"text": "These are longstanding themes that many personal tweets revolve around.", "labels": [], "entities": []}, {"text": "Example topics range from music and sports to more serious ones like politics and religion.", "labels": [], "entities": []}, {"text": "Much work has been done to analyze topics on Twitter (.", "labels": [], "entities": []}, {"text": "These are things that take place at a certain time and attract many people's shortterm attention in social media.", "labels": [], "entities": []}, {"text": "Example events include concerts, sports games, scandals and elections.", "labels": [], "entities": []}, {"text": "Event detection on Twitter has been a hot research topic in recent years).", "labels": [], "entities": [{"text": "Event detection", "start_pos": 0, "end_pos": 15, "type": "TASK", "confidence": 0.8359169661998749}]}, {"text": "The concepts of topics and events are orthogonal in that many events fall under certain topics.", "labels": [], "entities": []}, {"text": "For example, concerts fall under the topic about music.", "labels": [], "entities": []}, {"text": "Furthermore, being social media, Twitter users play important roles in forming topics and events on Twitter.", "labels": [], "entities": []}, {"text": "Each user has her own topic interests, which influence the content of her tweets.", "labels": [], "entities": []}, {"text": "Whether a user publishes a tweet related to an event also largely depends on whether her topic interests match the nature of the event.", "labels": [], "entities": []}, {"text": "Modeling the interplay between topics, events and users can deepen our understanding of Twitter content and potentially aid many predication and recommendation tasks.", "labels": [], "entities": []}, {"text": "In this paper, we aim to construct a unified model of topics, events and users on Twitter.", "labels": [], "entities": []}, {"text": "Although there has been a number of recent studies on event detection on Twitter, to the best of our knowledge, ours is the first that links the topic interests of users to their tweeting behaviors on events.", "labels": [], "entities": [{"text": "event detection", "start_pos": 54, "end_pos": 69, "type": "TASK", "confidence": 0.718751534819603}]}, {"text": "Specifically, we propose a probabilistic latent variable model that identifies both topics and events on Twitter.", "labels": [], "entities": []}, {"text": "To do so, we first separate tweets into topic tweets and event tweets.", "labels": [], "entities": []}, {"text": "The former are related to a user's personal life, such as a tweet complaining about the traffic condition or wishing a friend happy birthday.", "labels": [], "entities": []}, {"text": "The latter are about some major global event interesting to a large group of people, such as a tweet advertising a concert or commenting on an election result.", "labels": [], "entities": []}, {"text": "Although considering only topic tweets and event tweets is a much simplified view of the diverse range of tweets, we find it effective in finding meaningful topics and events.", "labels": [], "entities": []}, {"text": "We further use an LDA-like model ( to discover topics and the Recurrent Chinese Restaurant Process (Ahmed and Xing, ) to discover events.", "labels": [], "entities": []}, {"text": "Details are given in Section 3.1.", "labels": [], "entities": []}, {"text": "Our major contributions lie in two novel modifications to the base model described above.", "labels": [], "entities": []}, {"text": "The first is a duration-based regularization component that punishes long-term events (Section 3.2).", "labels": [], "entities": []}, {"text": "Because events on Twitter tend to be bursty, this modification presumably can produce more meaningful events.", "labels": [], "entities": []}, {"text": "More specifically, we borrow the idea of using pseudo-observed variables to regularize graphical models, and carefully design the pseudo-observed variable in our task to capture the burstiness of events.", "labels": [], "entities": []}, {"text": "The second modification is adding event-topic affinity vectors inspired by PMF-based collaborative filtering (Salakhutdinov and Mnih, 2008) (Section 3.3).", "labels": [], "entities": [{"text": "PMF-based collaborative filtering", "start_pos": 75, "end_pos": 108, "type": "TASK", "confidence": 0.7724759181340536}]}, {"text": "It uses the latent topics to explain users' preferences of events and subsequently infers the association between topics and events.", "labels": [], "entities": []}, {"text": "We use areal Twitter data set consisting of 500 users to evaluate our model (Section 4).", "labels": [], "entities": [{"text": "Twitter data set", "start_pos": 13, "end_pos": 29, "type": "DATASET", "confidence": 0.7858348389466604}]}, {"text": "We find that the model can discover meaningful topics and events.", "labels": [], "entities": []}, {"text": "Comparison with our base model and with an existing model for event discovery on Twitter shows that the two modifications are both effective.", "labels": [], "entities": [{"text": "event discovery", "start_pos": 62, "end_pos": 77, "type": "TASK", "confidence": 0.7663893103599548}]}, {"text": "The duration-based regularization helps find more meaningful events; the event-topic affinity vectors improve an event recommendation task and helps produce a meaningful organization of events by topics.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluate our model on a Twitter dataset that contains 500 users.", "labels": [], "entities": [{"text": "Twitter dataset", "start_pos": 27, "end_pos": 42, "type": "DATASET", "confidence": 0.7725637853145599}]}, {"text": "These users are randomly selected from a much larger pool of around 150K users based in Singapore.", "labels": [], "entities": []}, {"text": "Selecting users from the same country/city ensures that we find coherent and meaningful topics and events.", "labels": [], "entities": []}, {"text": "We use tweets published between April 1 and June 30, 2012 for our experiments.", "labels": [], "entities": []}, {"text": "For preprocessing, we use the CMU Twitter POS Tagger 1 to tag these tweets and remove those non-standard words (i.e. words tagged as punctuation marks, emoticons, urls, at-mentions, pronouns, etc.) and stop words.", "labels": [], "entities": [{"text": "CMU Twitter POS Tagger 1", "start_pos": 30, "end_pos": 54, "type": "DATASET", "confidence": 0.9391541600227356}]}, {"text": "We also remove tweets 1 http://www.ark.cs.cmu.edu/TweetNLP/ with less than three words.", "labels": [], "entities": []}, {"text": "After preprocessing, the dataset contains 655,881 tweets in total.", "labels": [], "entities": []}, {"text": "Recall that our model is designed to identify topics, events and their relations with users.", "labels": [], "entities": []}, {"text": "We therefore would like to evaluate the quality of the identified topics and events as well as the usefulness of the discovered topic distributions of users and eventtopic affinity vectors.", "labels": [], "entities": []}, {"text": "Because our topic discovery mechanism is fairly standard and a quick inspection shows that the discovered topics are generally meaningful and comparable to those discovered by standard LDA, here we do not focus on evaluation of topics.", "labels": [], "entities": [{"text": "topic discovery", "start_pos": 12, "end_pos": 27, "type": "TASK", "confidence": 0.75824373960495}]}, {"text": "In Section 4.2 we evaluate the quality of the discovered events.", "labels": [], "entities": []}, {"text": "In Section 4.3 we show how the discovered event-topic affinity vectors can be useful.", "labels": [], "entities": []}, {"text": "For comparison, we consider an existing method called TimeUserLDA introduced in our previous work).", "labels": [], "entities": []}, {"text": "TimeUserLDA also models topics and events by separating topic tweets from event tweets.", "labels": [], "entities": [{"text": "TimeUserLDA", "start_pos": 0, "end_pos": 11, "type": "DATASET", "confidence": 0.9008793830871582}]}, {"text": "However, it groups event tweets into a fixed number of bursty topics and then uses a twostate machine in a postprocessing step to identify events from these bursty topics.", "labels": [], "entities": []}, {"text": "Thus, events are not directly modeled within the generative process itself.", "labels": [], "entities": []}, {"text": "In contrast, events are inherent in our generative model.", "labels": [], "entities": []}, {"text": "We do not compare with other event detection methods because our objective is not online event detection.", "labels": [], "entities": [{"text": "event detection", "start_pos": 29, "end_pos": 44, "type": "TASK", "confidence": 0.7846570312976837}, {"text": "online event detection", "start_pos": 82, "end_pos": 104, "type": "TASK", "confidence": 0.6553780337174734}]}, {"text": "We also compare our final model with two degenerate versions of it.", "labels": [], "entities": []}, {"text": "We refer to the base model described in Section 3.1 as Base and the model with the duration-based regularization as Base+Reg.", "labels": [], "entities": []}, {"text": "Comparison with these two degenerate models allows us to assess the effect of the two modifications we propose.", "labels": [], "entities": []}, {"text": "We refer to the final model with both the duration-based regularization and the event-topic affinity vectors as Base+Reg+Aff.", "labels": [], "entities": [{"text": "Aff", "start_pos": 121, "end_pos": 124, "type": "METRIC", "confidence": 0.5559874176979065}]}, {"text": "For the parameter setting, we empirically set A to 40, \u03b3 to 50 A , \u03c4 to 1, \u03b2 to 0.01, \u03b1 to 1, \u03b9 to 10, \u03f5 to 1, and the duration regularization parameter \u03bb to 0.01.", "labels": [], "entities": [{"text": "duration regularization parameter \u03bb", "start_pos": 119, "end_pos": 154, "type": "METRIC", "confidence": 0.8520895540714264}]}, {"text": "When anew event k is created, the inner popularity bias term \u03b7 0 k is set to 1, and the factors in eventtopic affinity vectors \u03b7 k are all set to 0.", "labels": [], "entities": [{"text": "inner popularity bias term \u03b7 0 k", "start_pos": 34, "end_pos": 66, "type": "METRIC", "confidence": 0.7705213129520416}]}, {"text": "We run the stochastic EM sampling scheme for 300 iterations.", "labels": [], "entities": [{"text": "EM sampling", "start_pos": 22, "end_pos": 33, "type": "TASK", "confidence": 0.7747431099414825}]}, {"text": "After Gibbs sampling assigns each variable a value at the end of each iteration, we update the values of \u03b7 0 k and \u03b7 k for the existing events using gradient descent.: The top-5 events identified by Base+Reg+Aff.", "labels": [], "entities": [{"text": "Base+Reg+Aff", "start_pos": 199, "end_pos": 211, "type": "METRIC", "confidence": 0.6202166438102722}]}, {"text": "We show the story name which is manually labeled, top ten ranking words, lasting duration and the inner popularity (\u03b7 0 k ) for each event.", "labels": [], "entities": [{"text": "duration", "start_pos": 81, "end_pos": 89, "type": "METRIC", "confidence": 0.5119732022285461}]}], "tableCaptions": [{"text": " Table 2: Precision@K for the various methods.", "labels": [], "entities": [{"text": "Precision", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9821848273277283}]}, {"text": " Table 3: Precision of the event tweets for the 4 common events.", "labels": [], "entities": [{"text": "Precision", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9825108647346497}]}, {"text": " Table 4: For the 8 test events that happened in June 2012, we compute the Average Precision for each event. We also  show the Mean Average Precision (MAP) when applicable.", "labels": [], "entities": [{"text": "Average Precision", "start_pos": 75, "end_pos": 92, "type": "METRIC", "confidence": 0.9070876836776733}, {"text": "Mean Average Precision (MAP)", "start_pos": 127, "end_pos": 155, "type": "METRIC", "confidence": 0.9702244102954865}]}]}