{"title": [{"text": "MCTest: A Challenge Dataset for the Open-Domain Machine Comprehension of Text", "labels": [], "entities": [{"text": "MCTest", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.9014816880226135}]}], "abstractContent": [{"text": "We present MCTest, a freely available set of stories and associated questions intended for research on the machine comprehension of text.", "labels": [], "entities": []}, {"text": "Previous work on machine comprehension (e.g., semantic modeling) has made great strides, but primarily focuses either on limited domain datasets, or on solving a more restricted goal (e.g., open-domain relation extraction).", "labels": [], "entities": [{"text": "semantic modeling)", "start_pos": 46, "end_pos": 64, "type": "TASK", "confidence": 0.8303911089897156}, {"text": "open-domain relation extraction)", "start_pos": 190, "end_pos": 222, "type": "TASK", "confidence": 0.7107525616884232}]}, {"text": "In contrast, MCTest requires machines to answer multiple-choice reading comprehension questions about fictional stories , directly tackling the high-level goal of open-domain machine comprehension.", "labels": [], "entities": []}, {"text": "Reading comprehension can test advanced abilities such as causal reasoning and understanding the world, yet, by being multiple-choice, still provide a clear metric.", "labels": [], "entities": []}, {"text": "By being fictional, the answer typically can be found only in the story itself.", "labels": [], "entities": []}, {"text": "The stories and questions are also carefully limited to those a young child would understand, reducing the world knowledge that is required for the task.", "labels": [], "entities": []}, {"text": "We present the scalable crowd-sourcing methods that allow us to cheaply construct a dataset of 500 stories and 2000 questions.", "labels": [], "entities": []}, {"text": "By screening workers (with grammar tests) and stories (with grading), we have ensured that the data is the same quality as another set that we manually edited, but atone tenth the editing cost.", "labels": [], "entities": []}, {"text": "By being open-domain, yet carefully restricted, we hope MCTest will serve to encourage research and provide a clear metric for advancement on the machine comprehension of text.", "labels": [], "entities": []}], "introductionContent": [], "datasetContent": [{"text": "In, we present results demonstrating the value of the grammar test and curation process.", "labels": [], "entities": []}, {"text": "As expected, manually curating MC160 resulted in increased grammar quality and percent of questions answered correctly by raters.", "labels": [], "entities": [{"text": "grammar quality", "start_pos": 59, "end_pos": 74, "type": "METRIC", "confidence": 0.8230437934398651}]}, {"text": "The goal of MC500 was to find a more scalable method to achieve the same quality as the curated MC160.", "labels": [], "entities": [{"text": "MC500", "start_pos": 12, "end_pos": 17, "type": "DATASET", "confidence": 0.8628362417221069}]}, {"text": "As shows, the grammar test improved story grammar quality from 1.70 to 1.77 (both uncurated).", "labels": [], "entities": [{"text": "grammar", "start_pos": 14, "end_pos": 21, "type": "METRIC", "confidence": 0.9045606851577759}]}, {"text": "The rating and one-day curation process in-.", "labels": [], "entities": []}, {"text": "Just 4.1% of raters had an accuracy below 80% (constituting 4.2% of the judgments)..", "labels": [], "entities": [{"text": "accuracy", "start_pos": 27, "end_pos": 35, "type": "METRIC", "confidence": 0.9993938207626343}]}, {"text": "Average age appropriateness, story clarity, grammar quality (0-2, with 2 being best), and percent of questions answered correctly by raters, for the original and curated versions of the data.", "labels": [], "entities": [{"text": "clarity", "start_pos": 35, "end_pos": 42, "type": "METRIC", "confidence": 0.8344280123710632}, {"text": "grammar quality", "start_pos": 44, "end_pos": 59, "type": "METRIC", "confidence": 0.9097425639629364}]}, {"text": "Bold indicates statistical significance vs. the original version of the same set, using the two-sample t-test with unequal variance.", "labels": [], "entities": [{"text": "statistical significance", "start_pos": 15, "end_pos": 39, "type": "METRIC", "confidence": 0.677513062953949}]}, {"text": "The \u01c2 indicates the only statistical difference between 500 curated and 160 curated.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2. Average age appropriateness, story clarity,  grammar quality (0-2, with 2 being best), and percent of  questions answered correctly by raters, for the original  and curated versions of the data. Bold indicates statisti- cal significance vs. the original version of the same set,  using the two-sample t-test with unequal variance. The  \u01c2  indicates the only statistical difference between 500  curated and 160 curated.", "labels": [], "entities": [{"text": "statisti- cal significance", "start_pos": 220, "end_pos": 246, "type": "METRIC", "confidence": 0.8260916322469711}]}, {"text": " Table 3. Corpus statistics for MC160 and MC500.", "labels": [], "entities": [{"text": "MC160", "start_pos": 32, "end_pos": 37, "type": "DATASET", "confidence": 0.9312793016433716}, {"text": "MC500", "start_pos": 42, "end_pos": 47, "type": "DATASET", "confidence": 0.6539744734764099}]}, {"text": " Table 5. The MC160 train and devel- opment sets were used for tuning. The baseline  algorithm was authored without seeing any portion  of MC500, so both the MC160 test set and all of", "labels": [], "entities": [{"text": "MC500", "start_pos": 139, "end_pos": 144, "type": "DATASET", "confidence": 0.9444175958633423}, {"text": "MC160 test set", "start_pos": 158, "end_pos": 172, "type": "DATASET", "confidence": 0.9238815307617188}]}, {"text": " Table 4. Percent correct for the multiple choice ques- tions for MC160. SW: sliding window algorithm.  SW+D: combined results with sliding window and  distance based algorithms. Single/Multi: questions  marked by worker as requiring a single/multiple sen- tence(s) to answer. All differences between SW and  SW+D are significant (p<0.01 using the two-tailed  paired t-test).", "labels": [], "entities": []}, {"text": " Table 5. Percent correct for the multiple choice ques- tions for MC500, notation as above. All differences  between SW and SW+D are significant (p<0.01, test- ed as above).", "labels": [], "entities": [{"text": "MC500", "start_pos": 66, "end_pos": 71, "type": "DATASET", "confidence": 0.9324231147766113}]}, {"text": " Table 6. Percent correct for MC160 and MC500 test  sets. The  \u01c2 indicates statistical significance vs. baseline  (p<0.01 using the two-tailed paired t-test). MC160  combined vs. baseline has p-value 0.063.", "labels": [], "entities": [{"text": "MC500 test  sets", "start_pos": 40, "end_pos": 56, "type": "DATASET", "confidence": 0.8580115636189779}]}]}