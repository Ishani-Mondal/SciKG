{"title": [{"text": "A discourse-driven content model for summarising scientific articles evaluated in a complex question answering task", "labels": [], "entities": [{"text": "summarising scientific articles evaluated in a complex question answering", "start_pos": 37, "end_pos": 110, "type": "TASK", "confidence": 0.7214616537094116}]}], "abstractContent": [{"text": "We present a method which exploits automatically generated scientific discourse annotations to create a content model for the summarisation of scientific articles.", "labels": [], "entities": [{"text": "summarisation of scientific articles", "start_pos": 126, "end_pos": 162, "type": "TASK", "confidence": 0.9110116362571716}]}, {"text": "Full papers are first automatically annotated using the CoreSC scheme, which captures 11 content-based concepts such as Hypothesis, Result, Conclusion etc at the sentence level.", "labels": [], "entities": []}, {"text": "A content model which follows the sequence of CoreSC categories observed in abstracts is used to provide the skeleton of the summary, making a distinction between dependent and independent categories.", "labels": [], "entities": []}, {"text": "Summary creation is also guided by the distribution of CoreSC categories found in the full articles, in order to adequately represent the article content.", "labels": [], "entities": [{"text": "Summary creation", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.963870495557785}]}, {"text": "Finally , we demonstrate the usefulness of the summaries by evaluating them in a complex question answering task.", "labels": [], "entities": [{"text": "question answering task", "start_pos": 89, "end_pos": 112, "type": "TASK", "confidence": 0.7920994857947031}]}, {"text": "Results are very encouraging as summaries of papers from automatically obtained CoreSCs enable experts to answer 66% of complex content-related questions designed on the basis of paper abstracts.", "labels": [], "entities": []}, {"text": "The questions were answered with a precision of 75%, where the upper bound for human summaries (abstracts) was 95%.", "labels": [], "entities": [{"text": "precision", "start_pos": 35, "end_pos": 44, "type": "METRIC", "confidence": 0.999462902545929}]}], "introductionContent": [{"text": "The publication boom of the last few years, especially in the life sciences, has highlighted the need to facilitate automatic access to the information content of articles.", "labels": [], "entities": []}, {"text": "Researchers, curators, reviewers all need to process a continuously expanding flow of articles whether the purpose is to follow the state of the art, curate large knowledge bases or have a good working knowledge of their own and related disciplines to assess progress in research.", "labels": [], "entities": []}, {"text": "While a lot of effort has concentrated on information extraction of particular types of entities and relations from the scientific literature, with a view to support scientists in obtaining relevant information from scientific articles and abstracts, less work has focussed on automatically combining such information in the form of a cohesive summary which preserves the context.", "labels": [], "entities": [{"text": "information extraction of particular types of entities and relations", "start_pos": 42, "end_pos": 110, "type": "TASK", "confidence": 0.8391453756226434}]}, {"text": "Researchers rely to a great extent on author-written abstracts, but the latter suffer from a number of problems; they are less structured, vary significantly in terms of length, are often not self-contained and have been written independently of the main document.,) identify argumentative zones within scientific articles and use them to create use-targeted extractive summaries.", "labels": [], "entities": []}, {"text": "Argumentative zones are annotations which designate the type of knowledge claim and rhetorical status fora sentence and how these relate to the communicative function of the entire paper.", "labels": [], "entities": []}, {"text": "A selection of various combinations of argumentative zones are chosen for the use-targeted extractive summaries (rhetorical extracts), each of which fulfills a different role.", "labels": [], "entities": [{"text": "use-targeted extractive summaries (rhetorical extracts)", "start_pos": 78, "end_pos": 133, "type": "TASK", "confidence": 0.7532074451446533}]}, {"text": "For instance, purpose-oriented extracts less than 10 sentences long are generated containing a predetermined number of AIM, SOLU-TION and BACKGROUND zones.", "labels": [], "entities": [{"text": "BACKGROUND", "start_pos": 138, "end_pos": 148, "type": "METRIC", "confidence": 0.797172486782074}]}, {"text": "As the emphasis of this approach was the identification of the argumentative zones, less attention was given to the sentence selection criteria for the extractive summaries.", "labels": [], "entities": []}, {"text": "The sentences chosen for the rhetorical extracts were either all sentences of a particular category (in the case of rare categories) (, selected according to a classifier trained on a relevance gold standard (), manually or randomly selected).", "labels": [], "entities": [{"text": "rhetorical extracts", "start_pos": 29, "end_pos": 48, "type": "TASK", "confidence": 0.895974338054657}]}, {"text": "More recently have used automatically annotated argumentative zones) to guide the creation of extractive summaries of scientific articles.", "labels": [], "entities": []}, {"text": "Here argumentative zones are used as features for the summariser, along with verbs, tf-idf values and sentence location.", "labels": [], "entities": []}, {"text": "They use a standard approach to summarisation, with a binary classification recognising candidate sentences which are then fed into a clustering mechanism.", "labels": [], "entities": [{"text": "summarisation", "start_pos": 32, "end_pos": 45, "type": "TASK", "confidence": 0.9863529205322266}]}, {"text": "Extracts can be created to summarise the entire paper or focus on specific user-specified aspects.", "labels": [], "entities": []}, {"text": "The number of sentences to include in the summary is prespecified (either directly or using a compression ratio).", "labels": [], "entities": []}, {"text": "Our approach also makes use of the scientific discourse for summarisation purposes.", "labels": [], "entities": [{"text": "summarisation", "start_pos": 60, "end_pos": 73, "type": "TASK", "confidence": 0.980828583240509}]}, {"text": "We use the scientific discourse to create a content model for extractive summarisation, with a focus on representing the content of the full paper, while keeping the cohesion of the narrative.", "labels": [], "entities": [{"text": "extractive summarisation", "start_pos": 62, "end_pos": 86, "type": "TASK", "confidence": 0.7528673410415649}]}, {"text": "We first automatically annotate the articles with a scheme which captures fine-grained aspects of the content and conceptual structure of the papers, namely the Core Scientific Concepts (CoreSC) scheme ().", "labels": [], "entities": []}, {"text": "The CoreSC scheme is \"uniquely suited to recovering common types of scientific arguments about hypotheses, explanations, and evidence\"), which are not readily identifiable by other annotation schemes.", "labels": [], "entities": [{"text": "recovering common types of scientific arguments about hypotheses, explanations, and evidence", "start_pos": 41, "end_pos": 133, "type": "TASK", "confidence": 0.6900706039025233}]}, {"text": "Also, when compared to argumentative zoning and more specifically its extension for chemistry papers, AZ-II (, it was shown to provide a greater level of detail in terms of categories denoting objectives, methods and outcomes whereas AZ-II focusses on the attribution of knowledge claims and the relation with previous work (.", "labels": [], "entities": [{"text": "AZ-II", "start_pos": 102, "end_pos": 107, "type": "METRIC", "confidence": 0.7326036095619202}, {"text": "detail", "start_pos": 154, "end_pos": 160, "type": "METRIC", "confidence": 0.9635407328605652}]}, {"text": "We then use the distribution of CoreSC categories observed in abstracts to create a content model which provides a skeleton for extractive summaries.", "labels": [], "entities": []}, {"text": "The reasoning behind this is to try to preserve cohesion within the summaries and we hypothesise that the sequence of CoreSC categories is a good proxy for cohesion (see section 3.1).", "labels": [], "entities": []}, {"text": "In creating the summary, instantiating the content model, we identify independent categories and dependent categories, and we argue that in order to preserve the cohesion of the text the independent categories should be determined first (see section 3.2).", "labels": [], "entities": []}, {"text": "We also preserve in the summary the distribution of CoreSC categories found in the corresponding full paper.", "labels": [], "entities": []}, {"text": "Finally, we evaluate the extractive summaries in a complex real world question-answering task, in which we assess the usefulness of the summaries as well as to what extent the generated CoreSC summaries represent the content of the original article.", "labels": [], "entities": []}, {"text": "Experts are presented with different types of summaries and are asked to answer article-specific questions on the basis of the summaries (see section 4.1).", "labels": [], "entities": []}, {"text": "Our results show that automatically generated CoreSC summaries can answer 66% of complex questions with 75% precision, outperforming a baseline of microsoft autosummarise summaries (See section 4.2).", "labels": [], "entities": [{"text": "CoreSC summaries", "start_pos": 46, "end_pos": 62, "type": "TASK", "confidence": 0.5473954379558563}, {"text": "precision", "start_pos": 108, "end_pos": 117, "type": "METRIC", "confidence": 0.9985777139663696}]}, {"text": "We have also peformed an intrinsic evaluation of the summaries using ROUGE and automatic measures for summary informativeness, such as the Jensen-Shannon divergence, yielding positive results (See section 4.2).", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 69, "end_pos": 74, "type": "METRIC", "confidence": 0.9769418835639954}]}, {"text": "However, as such measures have not yet reached maturity and are harder to interpret, we consider the user-based evaluation to be a more reliable measure of summary quality.", "labels": [], "entities": []}, {"text": "Code for generating the summaries can be obtained by contacting the first author and/or visiting http://www.sapientaproject.com/software.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluate the extractive CoreSC summaries in terms of how well they enable 12 chemistry experts/evaluators (with at least a Masters degree in chemistry) to answer complex questions about the papers.", "labels": [], "entities": [{"text": "CoreSC summaries", "start_pos": 27, "end_pos": 43, "type": "DATASET", "confidence": 0.7086511105298996}]}, {"text": "Our test corpus consists of 28 papers held out from the ART/CoreSC corpus, roughly 1/9, which were annotated automatically with the SVM and CRF classifiers described in () trained on the remaining 8/9 of the corpus.", "labels": [], "entities": [{"text": "ART/CoreSC corpus", "start_pos": 56, "end_pos": 73, "type": "DATASET", "confidence": 0.8788642138242722}]}, {"text": "For each of the 28 papers in the test corpus, we generated CoreSC summaries automatically using the method described in section 3.", "labels": [], "entities": []}, {"text": "We compare the performance of the experts on a question answering (Q-A) task when given the CoreSC summaries and two other types of summary, amounting to a total of three experimental conditions (A,B,C).", "labels": [], "entities": [{"text": "question answering (Q-A) task", "start_pos": 47, "end_pos": 76, "type": "TASK", "confidence": 0.8338799079259237}, {"text": "CoreSC summaries", "start_pos": 92, "end_pos": 108, "type": "DATASET", "confidence": 0.9214754998683929}]}, {"text": "The other two types of summary are the original paper abstracts (summaries A), in the absence of human summaries, and summaries generated by Microsoft Office Word 2007 AutoSummarize (summaries B).", "labels": [], "entities": [{"text": "Microsoft Office Word 2007 AutoSummarize", "start_pos": 141, "end_pos": 181, "type": "DATASET", "confidence": 0.5323244750499725}]}, {"text": "Microsoft Office Word 2007 AutoSummarize (MA) is a widely available commercial system with reportedly good results ( and performance equivalent to TextRank.", "labels": [], "entities": [{"text": "Microsoft Office Word 2007 AutoSummarize (MA)", "start_pos": 0, "end_pos": 45, "type": "DATASET", "confidence": 0.7540374174714088}]}, {"text": "MA works by assigning a score to each word in a sentence depending on its frequency in the document and sentences are ranked and extracted according to the combination of scores of the words they contain.", "labels": [], "entities": []}, {"text": "MA therefore follows classic lexicalised text extraction techniques, is domain independent and is completely agnostic of the discourse.", "labels": [], "entities": [{"text": "MA", "start_pos": 0, "end_pos": 2, "type": "DATASET", "confidence": 0.5924603939056396}, {"text": "text extraction", "start_pos": 41, "end_pos": 56, "type": "TASK", "confidence": 0.7245945036411285}]}, {"text": "For the latter reason, we considered MA to be a suitable baseline the comparison with which would illustrate the effect of using CoreSC categories on the summary and the merits of having a discourse based model for summarisation.", "labels": [], "entities": [{"text": "MA", "start_pos": 37, "end_pos": 39, "type": "METRIC", "confidence": 0.5292935371398926}, {"text": "summarisation", "start_pos": 215, "end_pos": 228, "type": "TASK", "confidence": 0.9757413864135742}]}, {"text": "Neither the paper title nor section headings were available to any of the summarising systems as our extractive system does not make direct use of them and we were not sure how they would influence MA.", "labels": [], "entities": [{"text": "summarising", "start_pos": 74, "end_pos": 85, "type": "TASK", "confidence": 0.9610213041305542}, {"text": "MA", "start_pos": 198, "end_pos": 200, "type": "TASK", "confidence": 0.8627596497535706}]}, {"text": "To ensure that each evaluator considered only one type of summary per paper, so as to avoid bias from previous stimuli, and to make sure all experts were exposed to all papers and all types of summary, the 12 experts were assigned to four groups (G1-G4) and were allocated 28 summaries each according to the Latin Square design in.", "labels": [], "entities": []}, {"text": "The experimental setup follows the paradigm of).", "labels": [], "entities": []}, {"text": "However, while) developed a Q-A task to evaluate summaries showing the contribution of a scientific article in relation to previous work, the purpose of the Q-A task at hand is to show the usefulness of the extracted summaries in answering questions on the paper, and how they compare to a discourse-agnostic baseline.", "labels": [], "entities": []}, {"text": "In the case of) the task consists of a fixed set of five questions, the same for all articles tuned particularly to the relation of current and previous work.", "labels": [], "entities": []}, {"text": "By contrast, the current Q-A task aims to show how well the summaries represent the content of the entire paper, which means that questions are individual to each paper and required domain knowledge to create.", "labels": [], "entities": []}, {"text": "Each of the 12 experts answered three contentbased questions per summary, where the questions were individual to each paper.", "labels": [], "entities": []}, {"text": "An example of the questions and the corresponding answers fora given paper can be found below.", "labels": [], "entities": []}, {"text": "Example 4.1.1 \u2022 Q:What do DNJ imino sugars inhibit the action of?", "labels": [], "entities": [{"text": "Q", "start_pos": 16, "end_pos": 17, "type": "METRIC", "confidence": 0.9454762935638428}]}, {"text": "A: They inhibit glycosidases and ceramide glucosyltransferases.", "labels": [], "entities": [{"text": "A", "start_pos": 0, "end_pos": 1, "type": "METRIC", "confidence": 0.911062479019165}]}, {"text": "\u2022 Q:What methods do the authors use to study the conformation of N-benzyl-DNJ?", "labels": [], "entities": [{"text": "Q", "start_pos": 2, "end_pos": 3, "type": "METRIC", "confidence": 0.9751890301704407}]}, {"text": "A: They use resonant two-photon ionization (R2PI), ultraviolet-ultraviolet (UV-UV) hole burning, and infrared (IR) ion-dip spectroscopies in conjunction with electronic structure theory calculations.", "labels": [], "entities": []}, {"text": "\u2022 Q:What is the conformation of the exocyclic hydroxymethyl group?", "labels": [], "entities": [{"text": "Q", "start_pos": 2, "end_pos": 3, "type": "METRIC", "confidence": 0.9829658269882202}]}, {"text": "A: The exocyclic hydroxymethyl group is axial to the piperidine ring (gauche-to the ring nitrogen).", "labels": [], "entities": []}, {"text": "As one can see, the questions are complex whquestions and correspond to answers with multiple components.", "labels": [], "entities": []}, {"text": "Questions were complex, to minimise the likelihood of correct random answers.", "labels": [], "entities": []}, {"text": "They were designed by a senior chemistry expert with knowledge of linguistics, so that they could be answered based on the abstracts (A).", "labels": [], "entities": []}, {"text": "For this purpose, the senior expert chose abstracts that were at least three sentences long.", "labels": [], "entities": []}, {"text": "Ideally, the questions and answers should have been set on the basis of the entire paper, but this was not possible given our timeframe for the experiment.The underlying assumption is that a good summary should cover most of the main points of the paper.", "labels": [], "entities": []}, {"text": "One of the merits of setting the questions on the basis of the abstracts was that the answers to be identified were deemed sufficiently important to be expressed in the humanly created abstract.", "labels": [], "entities": []}, {"text": "However, automatic summaries created in the way proposed here could potentially answer questions beyond the scope of the abstract and in cases of very short abstracts be much more informative.", "labels": [], "entities": []}, {"text": "Experts were told that summaries were automatically generated with no details about different types of summary; it is assumed that none of them is completely familiar with the work mentioned in the 28 papers.", "labels": [], "entities": []}, {"text": "On average, it took experts less than 10 minutes to read a summary and answer the three content-based questions.", "labels": [], "entities": []}, {"text": "Papers Evaluator groups 1-7 8-14 15-21 22-28", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Matches between summary-based answers and  model answers", "labels": [], "entities": []}, {"text": " Table 3: Precision, Recall and F-score for answering  questions using the four types of summary. A: abstracts,  B: autosummarize, C:automatic CoreSC summaries.", "labels": [], "entities": [{"text": "Precision", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9984617233276367}, {"text": "Recall", "start_pos": 21, "end_pos": 27, "type": "METRIC", "confidence": 0.9885720014572144}, {"text": "F-score", "start_pos": 32, "end_pos": 39, "type": "METRIC", "confidence": 0.9968760013580322}, {"text": "CoreSC summaries", "start_pos": 143, "end_pos": 159, "type": "TASK", "confidence": 0.5605417191982269}]}, {"text": " Table 4: Test for statistical significance betwen sum- maries B (microsoft) and C (CoreSC)", "labels": [], "entities": [{"text": "statistical significance betwen sum- maries B", "start_pos": 19, "end_pos": 64, "type": "METRIC", "confidence": 0.5724800143923078}, {"text": "CoreSC", "start_pos": 84, "end_pos": 90, "type": "DATASET", "confidence": 0.6122039556503296}]}, {"text": " Table 5: Macro-averaged divergence scores for the 28  test summaries. B: Autosummarize, C: CoreSC, random:  random summaries each 20 sentences long for each paper.  KLI-S: Average Kullback Leibler divergence between in- put and summary. KLS-I: Kullback Leibler divergence  between summary and input, since KL divergence is not  symmetric. UnJSD: Jensen Shannon divergence between  input and summary. No smoothing. SJSD:A version with  smoothing.", "labels": [], "entities": []}]}