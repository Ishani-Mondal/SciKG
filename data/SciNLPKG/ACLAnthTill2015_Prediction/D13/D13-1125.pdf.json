{"title": [{"text": "Sentiment Analysis: How to Derive Prior Polarities from SentiWordNet", "labels": [], "entities": [{"text": "Sentiment Analysis", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.9545591175556183}, {"text": "Derive Prior Polarities from SentiWordNet", "start_pos": 27, "end_pos": 68, "type": "TASK", "confidence": 0.8462117671966553}]}], "abstractContent": [{"text": "Assigning a positive or negative score to a word out of context (i.e. a word's prior polarity) is a challenging task for sentiment analysis.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 121, "end_pos": 139, "type": "TASK", "confidence": 0.9688853919506073}]}, {"text": "In the literature, various approaches based on SentiWordNet have been proposed.", "labels": [], "entities": []}, {"text": "In this paper, we compare the most often used techniques together with newly proposed ones and incorporate all of them in a learning framework to see whether blending them can further improve the estimation of prior polarity scores.", "labels": [], "entities": []}, {"text": "Using two different versions of Sen-tiWordNet and testing regression and classification models across tasks and datasets, our learning approach consistently outperforms the single metrics, providing anew state-of-the-art approach in computing words' prior polarity for sentiment analysis.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 269, "end_pos": 287, "type": "TASK", "confidence": 0.9387100338935852}]}, {"text": "We conclude our investigation showing interesting biases in calculated prior polarity scores when word Part of Speech and annotator gender are considered .", "labels": [], "entities": [{"text": "word Part of Speech", "start_pos": 98, "end_pos": 117, "type": "TASK", "confidence": 0.7628804594278336}]}], "introductionContent": [{"text": "Many approaches to sentiment analysis make use of lexical resources -i.e. lists of positive and negative words -often deployed as baselines or as features for other methods (usually machine learning based) for sentiment analysis research (.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 19, "end_pos": 37, "type": "TASK", "confidence": 0.9685904085636139}, {"text": "sentiment analysis research", "start_pos": 210, "end_pos": 237, "type": "TASK", "confidence": 0.9091275533040365}]}, {"text": "In these lexica, words are associated with their prior polarity, i.e. if that word out of context evokes something positive or something negative.", "labels": [], "entities": []}, {"text": "For example, wonderful has a positive connotation -prior polarity -while horrible has a negative one.", "labels": [], "entities": []}, {"text": "These approaches have the advantage of not needing deep semantic analysis or word sense disambiguation to assign an affective score to a word and are domain independent (they are thus less precise but more portable).", "labels": [], "entities": [{"text": "word sense disambiguation", "start_pos": 77, "end_pos": 102, "type": "TASK", "confidence": 0.635692298412323}]}, {"text": "SentiWordNet (henceforth SWN) is one of these resources and has been widely adopted since it provides a broad-coverage lexicon -built in a semiautomatic manner -for English).", "labels": [], "entities": []}, {"text": "Given that SWN provides polarities scores for each word sense (also called 'posterior polarities'), it is necessary to derive prior polarities from the posteriors.", "labels": [], "entities": []}, {"text": "For example, the word cold has a posterior polarity for the meaning \"having a low temperature\" -like in \"cold beer\" -that is different from the one in \"cold person\" which refers to \"being emotionless\".", "labels": [], "entities": []}, {"text": "This information must be considered when reconstructing the prior polarity of cold.", "labels": [], "entities": []}, {"text": "Several formulae to compute prior polarities starting from posterior polarities scores have been used in the literature.", "labels": [], "entities": []}, {"text": "However, their performance varies significantly depending on the adopted variant.", "labels": [], "entities": []}, {"text": "We show that researchers have not paid sufficient attention to this posterior-to-prior polarity issue.", "labels": [], "entities": []}, {"text": "Indeed, we show that some variants outperform others on different datasets and can represent a fairer state-ofthe-art approach using SWN.", "labels": [], "entities": []}, {"text": "On top of this, we attempt to outperform the state-of-the-art formula using a learning framework that combines the various formulae together.", "labels": [], "entities": []}, {"text": "In detail, we will address five main research questions: (i) is there any relevant difference in the posterior-to-prior polarity formulae performance (both in regression and classification tasks), (ii) is there any relevant variation in prior polarity values if we use different releases of SWN (i.e. SW N 1 or SW N 3 ), (iii) can a learning framework boost performance of such formulae, (iv) considering word Part of Speech (PoS), is there any relevant difference in formulae performance, (v) considering the gender dimension of the annotators (male/female) and the sentiment dimension (positive/negative), is there any relevant difference in SWN performance.", "labels": [], "entities": []}, {"text": "In Section 2 we briefly describe our approach and how it differentiates from similar sentiment analysis tasks.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 85, "end_pos": 103, "type": "TASK", "confidence": 0.9307191967964172}]}, {"text": "Then, in Sections 3 and 4, we present SentiWordNet and overview various posterior-to-prior polarity formulae based on this resource that appeared in the literature (included some new ones we identified as potentially relevant).", "labels": [], "entities": []}, {"text": "In Section 5 we describe the learning approach adopted on priorpolarity formulae.", "labels": [], "entities": []}, {"text": "In Section 6 we introduce the ANEW and General Inquirer resources that will be used as gold standards.", "labels": [], "entities": [{"text": "ANEW", "start_pos": 30, "end_pos": 34, "type": "DATASET", "confidence": 0.7924436330795288}, {"text": "General Inquirer resources", "start_pos": 39, "end_pos": 65, "type": "DATASET", "confidence": 0.9128715395927429}]}, {"text": "Finally, in the two last sections, we present a series of experiments, both in regression and classification tasks, that give an answer to the aforementioned research questions.", "labels": [], "entities": []}, {"text": "The results support the hypothesis that using a learning framework we can improve on state-of-the-art performance and that there are some interesting phenomena connected to PoS and annotator gender.", "labels": [], "entities": []}], "datasetContent": [{"text": "In order to use the ANEW dataset to measure prior polarities formulae performance, we had to assign a PoS to all the words to obtain the SWN lemma#PoS format.", "labels": [], "entities": [{"text": "ANEW dataset", "start_pos": 20, "end_pos": 32, "type": "DATASET", "confidence": 0.9618300497531891}, {"text": "PoS", "start_pos": 102, "end_pos": 105, "type": "METRIC", "confidence": 0.9769549369812012}]}, {"text": "To do so, we proceeded as follows: for each word, check if it is present among both SW N 1 and SW N 3 lemmas; if not, lemmatize the word with the TextPro tool suite ( and check if the lemma is present instead . If it is not found (i.e., the word cannot be aligned automatically), remove the word from the list (this was the case for 30 words of the 1,034 present in ANEW).", "labels": [], "entities": [{"text": "TextPro tool suite", "start_pos": 146, "end_pos": 164, "type": "DATASET", "confidence": 0.8954192399978638}, {"text": "ANEW", "start_pos": 366, "end_pos": 370, "type": "DATASET", "confidence": 0.929042398929596}]}, {"text": "The remaining 1,004 lemmas were then associated with all the PoS present in SWN to get the final lemma#PoS.", "labels": [], "entities": []}, {"text": "Note that a lemma can have more than one PoS, for example, writer is present only as a noun (writer#n), while yellow is present as a verb, a noun and an adjective (yellow#v, yellow#n, yellow#a).", "labels": [], "entities": [{"text": "PoS", "start_pos": 41, "end_pos": 44, "type": "METRIC", "confidence": 0.8773008584976196}]}, {"text": "This gave us a list of 1,484 words in the lemma#PoS format.", "labels": [], "entities": []}, {"text": "Ina similar way we pre-processed the GI words that uses the generic modif label to indicate either adjective or adverb (noun and verb PoS were instead consistently used).", "labels": [], "entities": []}, {"text": "Finally, all the sensedisambiguated words in the lemma#PoS#n format were discarded (1,114 words out of the 4,206 words with positive or negative valence).", "labels": [], "entities": []}, {"text": "After the two datasets were built this way, we removed the words for which the posScore and negScore contained all 0 in both SW N 1 and SW N 3 (523 lemma#PoS for ANEW and 484 for the GI dataset), since these words are not informative for our experiments.", "labels": [], "entities": [{"text": "GI dataset", "start_pos": 183, "end_pos": 193, "type": "DATASET", "confidence": 0.8383897840976715}]}, {"text": "The final dataset included 961 entries for ANEW and 2,557 for GI.", "labels": [], "entities": [{"text": "ANEW", "start_pos": 43, "end_pos": 47, "type": "DATASET", "confidence": 0.9040681719779968}, {"text": "GI", "start_pos": 62, "end_pos": 64, "type": "DATASET", "confidence": 0.9283758997917175}]}, {"text": "For each lemma#PoS in GI and ANEW, we then applied the prior polarity formulae described in Section 4, using both SW N 1 and SW N 3 and annotated the results.", "labels": [], "entities": [{"text": "PoS", "start_pos": 15, "end_pos": 18, "type": "METRIC", "confidence": 0.875146210193634}, {"text": "GI", "start_pos": 22, "end_pos": 24, "type": "DATASET", "confidence": 0.9022361040115356}, {"text": "ANEW", "start_pos": 29, "end_pos": 33, "type": "DATASET", "confidence": 0.669467568397522}]}, {"text": "According to the nature of the human labels (real numbers or -1/1), we ran several regression and classification experiments.", "labels": [], "entities": []}, {"text": "In both cases, each dataset was randomly split into 70% for training and the remaining for test.", "labels": [], "entities": []}, {"text": "This process was repeated 5 times to generate different splits.", "labels": [], "entities": []}, {"text": "For each partition, optimization of the learning algorithm parameters was performed on the training data (in 10-fold crossvalidation for SVMs).", "labels": [], "entities": []}, {"text": "Training and test sets were normalized using the z-score.", "labels": [], "entities": []}, {"text": "To evaluate the performance of our regression experiments on ANEW we used the Mean Absolute Error (M AE), that averages the error over a given test set.", "labels": [], "entities": [{"text": "ANEW", "start_pos": 61, "end_pos": 65, "type": "DATASET", "confidence": 0.9710261821746826}, {"text": "Mean Absolute Error (M AE)", "start_pos": 78, "end_pos": 104, "type": "METRIC", "confidence": 0.9395677873066494}]}, {"text": "Accuracy was used for the classification experiments on GI instead.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 0, "end_pos": 8, "type": "METRIC", "confidence": 0.9903826117515564}, {"text": "GI", "start_pos": 56, "end_pos": 58, "type": "DATASET", "confidence": 0.7334670424461365}]}, {"text": "We opted for accuracyrather than F1 -since for us True Negatives have same importance as True Positives.", "labels": [], "entities": [{"text": "accuracyrather", "start_pos": 13, "end_pos": 27, "type": "METRIC", "confidence": 0.9986609220504761}, {"text": "F1", "start_pos": 33, "end_pos": 35, "type": "METRIC", "confidence": 0.9986794590950012}]}, {"text": "For each experiments we reported the average performance and the standard deviation over the 5 random splits.", "labels": [], "entities": [{"text": "standard", "start_pos": 65, "end_pos": 73, "type": "METRIC", "confidence": 0.9645623564720154}]}, {"text": "In the following sections, to check if there was a statistically significant difference in the results, we used Student's t-test for regression experiments, while an approximate randomization test) was used for the classification experiments.", "labels": [], "entities": []}, {"text": "In, the results of regression experiments over the ANEW dataset, using SW N 1 and SW N 3 , are presented.", "labels": [], "entities": [{"text": "ANEW dataset", "start_pos": 51, "end_pos": 63, "type": "DATASET", "confidence": 0.9810998439788818}]}, {"text": "The results of the classification experiments over the GI dataset, using SW N 1 and SW N 3 are shown in Tables 4 and 5.", "labels": [], "entities": [{"text": "GI dataset", "start_pos": 55, "end_pos": 65, "type": "DATASET", "confidence": 0.9593763053417206}]}, {"text": "For the sake of interpretability, results are divided according to the main approaches: randoms, posterior-toprior formulae, learning algorithms.", "labels": [], "entities": [{"text": "interpretability", "start_pos": 16, "end_pos": 32, "type": "TASK", "confidence": 0.971682608127594}]}, {"text": "Note that for classification we report the generics f and not the f m and f d variants.", "labels": [], "entities": [{"text": "classification", "start_pos": 14, "end_pos": 28, "type": "TASK", "confidence": 0.975716769695282}]}, {"text": "In fact, both versions always return the same classification answer (we are classifying according to the sign off result and not its strength).", "labels": [], "entities": []}, {"text": "For the GPs, we report the two best configurations only.: MAE results for metrics using SW N 1  Next, we wanted to understand if the performance of our approach, using SW N 3 , was consistent across word PoS.", "labels": [], "entities": [{"text": "MAE", "start_pos": 58, "end_pos": 61, "type": "METRIC", "confidence": 0.9969450831413269}]}, {"text": "In we report the results for the best performing formulae and learning algorithm on the GI PoS classes.", "labels": [], "entities": [{"text": "GI PoS classes", "start_pos": 88, "end_pos": 102, "type": "DATASET", "confidence": 0.9048590262730917}]}, {"text": "In particular for ADJ there are 1,073 words, 922 for NOUN and 508 for VERB.", "labels": [], "entities": [{"text": "NOUN", "start_pos": 53, "end_pos": 57, "type": "DATASET", "confidence": 0.723528265953064}, {"text": "VERB", "start_pos": 70, "end_pos": 74, "type": "DATASET", "confidence": 0.5024434328079224}]}, {"text": "We discarded adverbs since the class was too small to allow reliable evaluation and efficient learning (only 54 instances).", "labels": [], "entities": []}, {"text": "The results show a greater accuracy for adjectives (p < 0.01), while performance for nouns and verbs are similar.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 27, "end_pos": 35, "type": "METRIC", "confidence": 0.9994348883628845}]}, {"text": "Finally we test against the male and female ratings provided by ANEW.", "labels": [], "entities": [{"text": "ANEW", "start_pos": 64, "end_pos": 68, "type": "DATASET", "confidence": 0.9100164175033569}]}, {"text": "As can be seen from, SWN approaches are far more precise in predicting Male judgments rather than Female ones (MAE\u00b5 goes from 0.392 to 0.323 with the best formula and from 0.369 to 0.292 with SV M f s, both differences are significant p < 0.001).", "labels": [], "entities": [{"text": "MAE\u00b5", "start_pos": 111, "end_pos": 115, "type": "METRIC", "confidence": 0.9947564601898193}]}, {"text": "Instead, in which displays the results along gender and polarity dimensions -there is no statistically significant difference in M AE on positive words between male and female, while there is a strong statistical significance for negative words (p < 0.001).", "labels": [], "entities": [{"text": "M AE", "start_pos": 129, "end_pos": 133, "type": "METRIC", "confidence": 0.9342198669910431}]}, {"text": "Interestingly, there is also a large difference between positive and negative affective words (both for male and female dimensions).", "labels": [], "entities": []}, {"text": "This difference is maximum for male scores on positive words compared to female scores on negative words (0.283 vs. 0.399, p < 0.001).", "labels": [], "entities": []}, {"text": "Recent work by inspected the differences in prior polarity assessment due to gender.", "labels": [], "entities": []}, {"text": "At this stage we can only note that prior polarities calculated with SWN are closer to ANEW male annotations than female ones.", "labels": [], "entities": [{"text": "ANEW", "start_pos": 87, "end_pos": 91, "type": "METRIC", "confidence": 0.45461201667785645}]}, {"text": "Understanding why this happens would require an accurate examination of the methods used to create WordNet and SWN (which will be the focus of our future work).", "labels": [], "entities": [{"text": "WordNet", "start_pos": 99, "end_pos": 106, "type": "DATASET", "confidence": 0.9353049397468567}]}], "tableCaptions": [{"text": " Table 1: First five SentiWordNet entries for cold#a", "labels": [], "entities": [{"text": "cold#a", "start_pos": 46, "end_pos": 52, "type": "TASK", "confidence": 0.5206161042054495}]}, {"text": " Table 2: MAE results for metrics using SW N 1", "labels": [], "entities": [{"text": "MAE", "start_pos": 10, "end_pos": 13, "type": "METRIC", "confidence": 0.8956272602081299}]}, {"text": " Table 3: MAE results for regression using SW N 3", "labels": [], "entities": [{"text": "MAE", "start_pos": 10, "end_pos": 13, "type": "METRIC", "confidence": 0.8734040856361389}]}, {"text": " Table 4: Accuracy results for classification using SW N 1", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9994869232177734}, {"text": "SW N", "start_pos": 52, "end_pos": 56, "type": "TASK", "confidence": 0.6128589957952499}]}, {"text": " Table 6: Accuracy results for PoS using SW N 3", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9992947578430176}, {"text": "PoS", "start_pos": 31, "end_pos": 34, "type": "TASK", "confidence": 0.761086106300354}]}, {"text": " Table 7: MAE results for Male vs Female using SW N 3", "labels": [], "entities": [{"text": "MAE", "start_pos": 10, "end_pos": 13, "type": "METRIC", "confidence": 0.9033926725387573}]}, {"text": " Table 8: MAE for Male/Female -Pos/Neg using SW N 3", "labels": [], "entities": [{"text": "MAE", "start_pos": 10, "end_pos": 13, "type": "METRIC", "confidence": 0.9978010058403015}]}]}