{"title": [{"text": "Lexical Chain Based Cohesion Models for Document-Level Statistical Machine Translation", "labels": [], "entities": [{"text": "Document-Level Statistical Machine Translation", "start_pos": 40, "end_pos": 86, "type": "TASK", "confidence": 0.7200494855642319}]}], "abstractContent": [{"text": "Lexical chains provide a representation of the lexical cohesion structure of a text.", "labels": [], "entities": []}, {"text": "In this paper , we propose two lexical chain based cohesion models to incorporate lexical cohesion into document-level statistical machine translation: 1) a count cohesion model that rewards a hypothesis whenever a chain word occurs in the hypothesis, 2) and a probability cohesion model that further takes chain word translation probabilities into account.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 119, "end_pos": 150, "type": "TASK", "confidence": 0.5867311060428619}]}, {"text": "We compute lexical chains for each source document to be translated and generate target lexical chains based on the computed source chains via maximum entropy classifiers.", "labels": [], "entities": []}, {"text": "We then use the generated target chains to provide constraints for word selection in document-level machine translation through the two proposed lexical chain based cohesion models.", "labels": [], "entities": [{"text": "word selection", "start_pos": 67, "end_pos": 81, "type": "TASK", "confidence": 0.7409656345844269}, {"text": "document-level machine translation", "start_pos": 85, "end_pos": 119, "type": "TASK", "confidence": 0.5953961809476217}]}, {"text": "We verify the effectiveness of the two models using a hierarchical phrase-based translation system.", "labels": [], "entities": [{"text": "phrase-based translation", "start_pos": 67, "end_pos": 91, "type": "TASK", "confidence": 0.6728167533874512}]}, {"text": "Experiments on large-scale training data show that they can substantially improve translation quality in terms of BLEU and that the probability cohesion model outperforms previous models based on lexical cohesion devices.", "labels": [], "entities": [{"text": "translation", "start_pos": 82, "end_pos": 93, "type": "TASK", "confidence": 0.957757830619812}, {"text": "BLEU", "start_pos": 114, "end_pos": 118, "type": "METRIC", "confidence": 0.9989650249481201}]}], "introductionContent": [{"text": "Given a source document, traditionally most statistical machine translation (SMT) systems translate the document sentence by sentence.", "labels": [], "entities": [{"text": "statistical machine translation (SMT)", "start_pos": 44, "end_pos": 81, "type": "TASK", "confidence": 0.7810791730880737}]}, {"text": "In such a translation scheme, sentences are translated independent of any other sentences.", "labels": [], "entities": []}, {"text": "However, a text is normally written cohesively, in which sentences are connected * Corresponding author to each other via syntactic and lexical devices.", "labels": [], "entities": []}, {"text": "This linguistic phenomenon is called as textual cohesion.", "labels": [], "entities": []}, {"text": "Cohesion is a surface-level property of wellformed texts.", "labels": [], "entities": []}, {"text": "It deals with five categories of relationships between text units, namely co-reference, ellipsis, substitution, conjunction and lexical cohesion that is realized via semantically related words.", "labels": [], "entities": []}, {"text": "The former four cohesion relations can be grouped as grammatical cohesion.", "labels": [], "entities": []}, {"text": "Generally speaking, grammatical cohesion is less common and harder to identify than lexical cohesion ().", "labels": [], "entities": []}, {"text": "As most SMT systems translate a text in a sentence-by-sentence fashion, they tend to build less lexical cohesion than human translators.", "labels": [], "entities": [{"text": "SMT", "start_pos": 8, "end_pos": 11, "type": "TASK", "confidence": 0.9897109866142273}]}, {"text": "We therefore study lexical cohesion for document-level translation.", "labels": [], "entities": [{"text": "document-level translation", "start_pos": 40, "end_pos": 66, "type": "TASK", "confidence": 0.7088803052902222}]}, {"text": "We use lexical chains to capture lexical cohesion in a text.", "labels": [], "entities": []}, {"text": "Lexical chains are connected graphs that represent the lexical cohesion structure of a text.", "labels": [], "entities": []}, {"text": "They have been successfully used for information retrieval, document summarization () and soon.", "labels": [], "entities": [{"text": "information retrieval", "start_pos": 37, "end_pos": 58, "type": "TASK", "confidence": 0.8543904423713684}, {"text": "document summarization", "start_pos": 60, "end_pos": 82, "type": "TASK", "confidence": 0.7046714723110199}]}, {"text": "In this paper, we investigate how lexical chains can be used to incorporate lexical cohesion into document-level translation.", "labels": [], "entities": [{"text": "document-level translation", "start_pos": 98, "end_pos": 124, "type": "TASK", "confidence": 0.7166480422019958}]}, {"text": "Our basic assumption is that the lexical chains of a target document are direct correspondences of the lexical chains of its counterpart source document.", "labels": [], "entities": []}, {"text": "This assumption is reasonable as the target document translation should be faithful to the source document in terms of both text meaning and structure.", "labels": [], "entities": []}, {"text": "Based on this assumption, we propose a framework to incorporate lexical cohesion into target document translation via lexical chains, which works as follows.", "labels": [], "entities": []}, {"text": "\u2022 Compute lexical chains for each source document that is to be translated; \u2022 Project the computed source lexical chains onto the corresponding target document by translating source chain words into target chain words using maximum entropy classifiers; \u2022 Incorporate lexical cohesion into the target document translation via cohesion models built on the projected target lexical chains . We build two lexical chain based cohesion models.", "labels": [], "entities": []}, {"text": "The first model is a count model that rewards a hypothesis whenever a word in the projected target lexical chains occur in the hypothesis.", "labels": [], "entities": []}, {"text": "As a source chain word maybe translated into many different target words, we further extend the count model to a second cohesion model: a probability model that takes chain word translation probabilities into account.", "labels": [], "entities": []}, {"text": "We test the two lexical chain based cohesion models on a hierarchical phrase-based SMT system that is trained with large-scale Chinese-English bilingual data.", "labels": [], "entities": [{"text": "SMT", "start_pos": 83, "end_pos": 86, "type": "TASK", "confidence": 0.8800869584083557}]}, {"text": "Experiment results show that our lexical chain based cohesion models can achieve substantial improvements over the baseline.", "labels": [], "entities": []}, {"text": "Furthermore, the probability cohesion model is better than the count model and it also outperforms previous cohesion models based on lexical cohesion devices ( . To the best of our knowledge, this is the first attempt to explore lexical chains for statistical machine translation.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 248, "end_pos": 279, "type": "TASK", "confidence": 0.7493061025937399}]}, {"text": "The remainder of this paper is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 discusses related work and highlights the differences between our method and previous work.", "labels": [], "entities": []}, {"text": "Section 3 briefly introduces lexical chains and algorithms that compute lexical chains.", "labels": [], "entities": []}, {"text": "Section 4 elaborates the proposed lexical chain based framework, including details on source lexical chain computation, target lexical chain generation and the two lexical chain based cohesion models.", "labels": [], "entities": [{"text": "target lexical chain generation", "start_pos": 120, "end_pos": 151, "type": "TASK", "confidence": 0.6702986806631088}]}, {"text": "Section 5 presents our large-scale experiments and results.", "labels": [], "entities": []}, {"text": "Finally, we conclude with future directions in Section 6.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we conducted a series of experiments to validate the effectiveness of the proposed lexical chain based cohesion models for Chinese-to-English document-level machine translation.", "labels": [], "entities": [{"text": "Chinese-to-English document-level machine translation", "start_pos": 140, "end_pos": 193, "type": "TASK", "confidence": 0.5465367808938026}]}, {"text": "We used a hierarchical phrased-based SMT system trained on large-scale data.", "labels": [], "entities": [{"text": "SMT", "start_pos": 37, "end_pos": 40, "type": "TASK", "confidence": 0.8833773732185364}]}, {"text": "In particular, we aim at: \u2022 Measuring the impact of the threshold on the probability cohesion model and selecting the best threshold on a development test set.", "labels": [], "entities": []}, {"text": "\u2022 Investigating the effect of the two lexical-chain based cohesion models.", "labels": [], "entities": []}, {"text": "\u2022 Comparing our lexical chain based cohesion models against the previous lexical cohesion device based models ( ).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Statistics of the training, development and test  sets, which show the number of documents (#Doc) and  sentences (#Sent), the number of lexical chains extracted  from the source documents (#Chain), the average number  of lexical chains per document (#AvgC) and the average  number of words per lexical chain (#AvgW).", "labels": [], "entities": []}, {"text": " Table 2: BLEU scores of the probability cohesion  model M p (T Dt , { LC k  t } N  k=1 ) with different values for  the threshold .", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9989008903503418}]}, {"text": " Table 3: Effects of the lexical chain based count and  probability cohesion models. LexChainCount: the count  model defined in Eq. (3). LexChainProb: the probability  model defined in Eq. (5).", "labels": [], "entities": [{"text": "LexChainCount", "start_pos": 85, "end_pos": 98, "type": "DATASET", "confidence": 0.9737550020217896}, {"text": "Eq.", "start_pos": 128, "end_pos": 131, "type": "DATASET", "confidence": 0.9363016486167908}, {"text": "LexChainProb", "start_pos": 137, "end_pos": 149, "type": "DATASET", "confidence": 0.9583450555801392}, {"text": "Eq.", "start_pos": 185, "end_pos": 188, "type": "DATASET", "confidence": 0.9395258128643036}]}, {"text": " Table 4: The lexical chain based probability cohesion  model (LexChainProb) vs. the lexical cohesion device  based trigger model (LexDeviceTrigger).", "labels": [], "entities": [{"text": "LexChainProb", "start_pos": 63, "end_pos": 75, "type": "DATASET", "confidence": 0.9360086917877197}, {"text": "LexDeviceTrigger", "start_pos": 131, "end_pos": 147, "type": "DATASET", "confidence": 0.9566920399665833}]}]}