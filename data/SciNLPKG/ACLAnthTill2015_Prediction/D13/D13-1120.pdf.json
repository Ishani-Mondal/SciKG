{"title": [{"text": "Ubertagging: Joint segmentation and supertagging for English", "labels": [], "entities": []}], "abstractContent": [{"text": "A precise syntacto-semantic analysis of En-glish requires a large detailed lexicon with the possibility of treating multiple tokens as a single meaning-bearing unit, a word-with-spaces.", "labels": [], "entities": []}, {"text": "However parsing with such a lexicon, as included in the English Resource Grammar, can be very slow.", "labels": [], "entities": [{"text": "parsing", "start_pos": 8, "end_pos": 15, "type": "TASK", "confidence": 0.9626104235649109}, {"text": "English Resource Grammar", "start_pos": 56, "end_pos": 80, "type": "DATASET", "confidence": 0.8720798293749491}]}, {"text": "We show that we can apply supertagging techniques over an ambiguous token lattice without resorting to previously used heuristics, a process we call ubertagging.", "labels": [], "entities": []}, {"text": "Our model achieves an ubertagging accuracy that can lead to a four to eight fold speedup while improving parser accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 34, "end_pos": 42, "type": "METRIC", "confidence": 0.994182288646698}, {"text": "parser", "start_pos": 105, "end_pos": 111, "type": "TASK", "confidence": 0.9524920582771301}, {"text": "accuracy", "start_pos": 112, "end_pos": 120, "type": "METRIC", "confidence": 0.8038955926895142}]}], "introductionContent": [], "datasetContent": [{"text": "In order to develop and tune the ubertagging model, we first looked at segmentation and tagging performance in isolation over the development set.", "labels": [], "entities": []}, {"text": "We looked at three tag granularities: lexical types (LTYPE) which have previously been shown to be the optimal granularity for supertagging with the ERG, inflected types (INFL) which encompass inflectional and derivational rules applied to the lexical type, and the full lexical item (FULL), which also includes affixation rules used for punctuation handling.", "labels": [], "entities": [{"text": "full lexical item (FULL)", "start_pos": 266, "end_pos": 290, "type": "METRIC", "confidence": 0.5982388059298197}]}, {"text": "Examples of each tag type are shown in, along with the number of tags of each type seen in the training data.: Segmentation and tagging performance of the best path found for each model, measured per segment in terms of F 1 , and also as complete sentence accuracy.", "labels": [], "entities": [{"text": "F 1", "start_pos": 220, "end_pos": 223, "type": "METRIC", "confidence": 0.9829848408699036}, {"text": "accuracy", "start_pos": 256, "end_pos": 264, "type": "METRIC", "confidence": 0.9486460089683533}]}], "tableCaptions": [{"text": " Table 1: Test, development and training data used in these experiments. The final two columns show the  total number of lexitems used for training (All), as well as how many of those were multi-token lexitems  (M-T).", "labels": [], "entities": []}, {"text": " Table 2: Segmentation and tagging performance of  the best path found for each model, measured per  segment in terms of F 1 , and also as complete sen- tence accuracy.", "labels": [], "entities": [{"text": "F 1", "start_pos": 121, "end_pos": 124, "type": "METRIC", "confidence": 0.9914528131484985}, {"text": "complete sen- tence accuracy", "start_pos": 139, "end_pos": 167, "type": "METRIC", "confidence": 0.8194748282432556}]}, {"text": " Table 3: Accuracy and ambiguity after pruning lex- items in WSJ 20 , at a selection of thresholds \u03c1 for  each model. Accuracy is measured as the percent- age of gold lexitems remaining after pruning, while  ambiguity is presented both as a percentage of lex- items kept, and the average number of lexitems per  initial token still remaining.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9988351464271545}, {"text": "ambiguity", "start_pos": 23, "end_pos": 32, "type": "METRIC", "confidence": 0.9459061622619629}, {"text": "WSJ 20", "start_pos": 61, "end_pos": 67, "type": "DATASET", "confidence": 0.8976580500602722}, {"text": "Accuracy", "start_pos": 118, "end_pos": 126, "type": "METRIC", "confidence": 0.9991143345832825}, {"text": "ambiguity", "start_pos": 208, "end_pos": 217, "type": "METRIC", "confidence": 0.9627784490585327}]}, {"text": " Table 4: Lexitem and bracket F 1 over WSJ 20 , with  average per sentence parsing time in seconds.", "labels": [], "entities": [{"text": "WSJ", "start_pos": 39, "end_pos": 42, "type": "DATASET", "confidence": 0.8243244290351868}]}, {"text": " Table 5: Parsing accuracy in terms of labelled  bracket F 1 and average time per sentence when pars- ing the test sets, without pruning, and then with lex- ical pruning using the INFL model with a threshold  of 0.001.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 18, "end_pos": 26, "type": "METRIC", "confidence": 0.9484214782714844}]}]}