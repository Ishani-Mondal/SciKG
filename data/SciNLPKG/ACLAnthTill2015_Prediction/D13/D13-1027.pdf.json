{"title": [{"text": "Error-Driven Analysis of Challenges in Coreference Resolution", "labels": [], "entities": [{"text": "Error-Driven Analysis of Challenges", "start_pos": 0, "end_pos": 35, "type": "TASK", "confidence": 0.7410760670900345}, {"text": "Coreference Resolution", "start_pos": 39, "end_pos": 61, "type": "TASK", "confidence": 0.8952996730804443}]}], "abstractContent": [{"text": "Coreference resolution metrics quantify errors but do not analyze them.", "labels": [], "entities": [{"text": "Coreference resolution", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.8583045303821564}]}, {"text": "Here, we consider an automated method of categorizing errors in the output of a coreference system into intuitive underlying error types.", "labels": [], "entities": []}, {"text": "Using this tool, we first compare the error distributions across a large set of systems, then analyze common errors across the top ten systems, empirically characterizing the major unsolved challenges of the coreference resolution task.", "labels": [], "entities": [{"text": "coreference resolution task", "start_pos": 208, "end_pos": 235, "type": "TASK", "confidence": 0.9489415884017944}]}], "introductionContent": [{"text": "Metrics produce measurements that concisely summarize performance on the full range of error types, and for coreference resolution there has been extensive work on developing effective metrics).", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 108, "end_pos": 130, "type": "TASK", "confidence": 0.9772964119911194}]}, {"text": "However, it is also valuable to tease apart the errors to understand their relative importance.", "labels": [], "entities": []}, {"text": "Previous investigations of coreference errors have focused on quantifying the importance of subtasks such as named entity recognition and anaphoricity detection, typically by measuring accuracy improvements when partial gold annotations are provided ().", "labels": [], "entities": [{"text": "coreference errors", "start_pos": 27, "end_pos": 45, "type": "TASK", "confidence": 0.8974814116954803}, {"text": "named entity recognition", "start_pos": 109, "end_pos": 133, "type": "TASK", "confidence": 0.6440366903940836}, {"text": "anaphoricity detection", "start_pos": 138, "end_pos": 160, "type": "TASK", "confidence": 0.7056116461753845}, {"text": "accuracy", "start_pos": 185, "end_pos": 193, "type": "METRIC", "confidence": 0.9968950748443604}]}, {"text": "For coreference resolution the drawback of this approach is that decisions are often interdependent, and so even partial gold information is extremely informative.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 4, "end_pos": 26, "type": "TASK", "confidence": 0.9742432236671448}]}, {"text": "Also, previous work only considered errors by counting links, which does not capture certain errors in a natural way, e.g. when a system incorrectly divides a large entity into two parts, each with multiple mentions.", "labels": [], "entities": []}, {"text": "Recent work has considered some of these issues, but only with small scale manual analysis).", "labels": [], "entities": []}, {"text": "We present anew tool that automatically classifies errors in the standard output of any coreference resolution system.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 88, "end_pos": 110, "type": "TASK", "confidence": 0.8926003277301788}]}, {"text": "Our approach is to identify changes that convert the system output into the gold annotations, and map the steps in the conversion onto linguistically intuitive error types.", "labels": [], "entities": []}, {"text": "Since our tool uses only system output, we are able to classify errors made by systems of any architecture, including both systems that use link-based inference and systems that use global inference methods.", "labels": [], "entities": []}, {"text": "Using our tool we perform two studies to understand similarities and differences between systems.", "labels": [], "entities": []}, {"text": "First, we compare the error distributions on coreference resolution of all of the systems from the CoNLL 2011 shared task plus several publicly available systems.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 45, "end_pos": 67, "type": "TASK", "confidence": 0.9575881063938141}, {"text": "CoNLL 2011 shared task", "start_pos": 99, "end_pos": 121, "type": "DATASET", "confidence": 0.902873694896698}]}, {"text": "This comparison adds to the analysis from the shared task by illustrating the substantial variation in the types of errors different systems make.", "labels": [], "entities": []}, {"text": "Second, we investigate the aggregate behavior often state-of-the-art systems, providing a detailed characterization of each error type.", "labels": [], "entities": []}, {"text": "This investigation identifies key outstanding challenges and presents the impact that solving each of them would have in terms of changes in the standard coreference resolution metrics.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 154, "end_pos": 176, "type": "TASK", "confidence": 0.9207848310470581}]}, {"text": "We find that the best systems are not best across all error types, that a large proportion of span errors are due to superficial parse differences, and that the biggest performance loss is on missed entities that contain a small number of mentions.", "labels": [], "entities": []}, {"text": "This work presents a comprehensive investigation of common errors in coreference resolution, identifying particular issues worth focusing on in future research.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 69, "end_pos": 91, "type": "TASK", "confidence": 0.9803326725959778}]}, {"text": "Our analysis tool is available at code.google.com/p/berkeley-coreference-analyser/.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Counts for each error type on the test set of the 2011 CoNLL task. Bars indicate the number of errors, with  white as zero and fully filled as the number in the Most Errors row. -OPEN indicates a system using external resources.", "labels": [], "entities": [{"text": "OPEN", "start_pos": 189, "end_pos": 193, "type": "METRIC", "confidence": 0.6481493711471558}]}, {"text": " Table 2: Counts of Span Errors grouped by the label over  the extra/missing part of the mention.", "labels": [], "entities": []}, {"text": " Table 3: Counts of Missing and Extra Mention errors by  mention type, and the most common mentions.", "labels": [], "entities": [{"text": "Counts of Missing and Extra Mention errors", "start_pos": 10, "end_pos": 52, "type": "METRIC", "confidence": 0.6567968002387455}]}, {"text": " Table 4: Counts of Extra and Missing Mentions, grouped  by properties of the mention and the entity it is in.", "labels": [], "entities": [{"text": "Counts", "start_pos": 10, "end_pos": 16, "type": "METRIC", "confidence": 0.9311404228210449}]}, {"text": " Table 5: Counts of Extra and Missing Entity errors,  grouped by the composition of the entity (Names, Nomi- nals, Pronouns).", "labels": [], "entities": []}, {"text": " Table 6: Counts of Extra and Missing Entity errors  grouped by properties of the mentions in the entity.", "labels": [], "entities": [{"text": "Counts", "start_pos": 10, "end_pos": 16, "type": "METRIC", "confidence": 0.9698460698127747}]}, {"text": " Table 7: Counts of common Missing and Extra Entity  errors where the entity has just two mentions: a pronoun  and either a nominal or a proper name.", "labels": [], "entities": [{"text": "Counts", "start_pos": 10, "end_pos": 16, "type": "METRIC", "confidence": 0.947599470615387}]}, {"text": " Table 8: Counts of Conflated and Divided entities errors  grouped by the Name / Nominal / Pronoun composition  of the parts involved.", "labels": [], "entities": []}, {"text": " Table 9: Occurrence of mistakes involving cataphora.", "labels": [], "entities": [{"text": "Occurrence", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9697504639625549}]}, {"text": " Table 10: Average accuracy improvement if all errors of a particular type are corrected. Each row in the lower section  is calculated independently, relative to the change after the span errors have been corrected. Some values are negative  because the merge operations involved in fixing the errors are applying to clusters that contain mentions from more  than one gold entity.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 19, "end_pos": 27, "type": "METRIC", "confidence": 0.9855072498321533}]}, {"text": " Table 11: Percentage of entities that contain mentions  with properties that disagree.", "labels": [], "entities": []}]}