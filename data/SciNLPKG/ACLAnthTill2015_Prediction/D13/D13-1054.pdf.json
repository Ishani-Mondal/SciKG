{"title": [], "abstractContent": [{"text": "While inversion transduction grammar (ITG) is well suited for modeling ordering shifts between languages, how to make applying the two reordering rules (i.e., straight and inverted) dependent on actual blocks being merged remains a challenge.", "labels": [], "entities": [{"text": "inversion transduction grammar (ITG)", "start_pos": 6, "end_pos": 42, "type": "TASK", "confidence": 0.8588104844093323}]}, {"text": "Unlike previous work that only uses boundary words, we propose to use recursive autoencoders to make full use of the entire merging blocks alternatively.", "labels": [], "entities": []}, {"text": "The recursive autoencoders are capable of generating vector space representations for variable-sized phrases, which enable predicting orders to exploit syntactic and semantic information from a neural language modeling's perspective.", "labels": [], "entities": [{"text": "predicting orders", "start_pos": 123, "end_pos": 140, "type": "TASK", "confidence": 0.8980244994163513}]}, {"text": "Experiments on the NIST 2008 dataset show that our system significantly improves over the MaxEnt classifier by 1.07 BLEU points.", "labels": [], "entities": [{"text": "NIST 2008 dataset", "start_pos": 19, "end_pos": 36, "type": "DATASET", "confidence": 0.9837790131568909}, {"text": "BLEU", "start_pos": 116, "end_pos": 120, "type": "METRIC", "confidence": 0.9988704323768616}]}], "introductionContent": [{"text": "Phrase-based models () have been widely used in practical machine translation (MT) systems due to their effectiveness, simplicity, and applicability.", "labels": [], "entities": [{"text": "machine translation (MT)", "start_pos": 58, "end_pos": 82, "type": "TASK", "confidence": 0.8673842072486877}]}, {"text": "First, as sequences of consecutive words, phrases are capable of memorizing local word selection and reordering, making them an effective mechanism for translating idioms or translations with word insertions or omissions.", "labels": [], "entities": [{"text": "translating idioms or translations with word insertions or omissions", "start_pos": 152, "end_pos": 220, "type": "TASK", "confidence": 0.7800945970747206}]}, {"text": "Moreover, n-gram language models can be seamlessly integrated into phrase-based decoders since partial translations grow left to right in decoding.", "labels": [], "entities": []}, {"text": "Finally, phrase-based systems can be applicable to most domains and languages, especially for resource-scarce languages without highaccuracy parsers.", "labels": [], "entities": []}, {"text": "However, as phrase-based decoding casts translation as a string concatenation problem and permits arbitrary permutations, it proves to be NP-complete.", "labels": [], "entities": []}, {"text": "Therefore, phrase reordering modeling has attracted intensive attention in the past decade (e.g.,).", "labels": [], "entities": [{"text": "phrase reordering modeling", "start_pos": 11, "end_pos": 37, "type": "TASK", "confidence": 0.8945124348004659}]}, {"text": "Among them, reordering models based on inversion transduction grammar (ITG) (Wu, 1997) are one of the important ongoing research directions.", "labels": [], "entities": [{"text": "inversion transduction grammar (ITG) (Wu, 1997)", "start_pos": 39, "end_pos": 86, "type": "TASK", "confidence": 0.7950451997193423}]}, {"text": "As a formalism for bilingual modeling of sentence pairs, ITG is particularly well suited to predicting ordering shifts between languages.", "labels": [], "entities": [{"text": "bilingual modeling of sentence pairs", "start_pos": 19, "end_pos": 55, "type": "TASK", "confidence": 0.7569213509559631}, {"text": "predicting ordering shifts between languages", "start_pos": 92, "end_pos": 136, "type": "TASK", "confidence": 0.8693433165550232}]}, {"text": "As a result, a number of authors have incorporated ITG into leftto-right decoding to constrain the reordering space and reported significant improvements (e.g.,.", "labels": [], "entities": []}, {"text": "Along another line, propose a maximum entropy (MaxEnt) reordering model based on ITG.", "labels": [], "entities": [{"text": "ITG", "start_pos": 81, "end_pos": 84, "type": "DATASET", "confidence": 0.9254403710365295}]}, {"text": "They use the CKY algorithm to recursively merge two blocks (i.e., a pair of source and target strings) into larger blocks, either in a straight or an inverted order.", "labels": [], "entities": []}, {"text": "Unlike lexicalized reordering models) that are defined on individual bilingual phrases, the MaxEnt ITG reordering model is a two-category classifier (i.e., straight or inverted) for two arbitrary bilingual phrases of which the source phrases are adjacent.", "labels": [], "entities": [{"text": "MaxEnt ITG reordering", "start_pos": 92, "end_pos": 113, "type": "DATASET", "confidence": 0.8758752346038818}]}, {"text": "This potentially alleviates the data sparseness problem since there are usually a large number of reordering training examples available ().", "labels": [], "entities": []}, {"text": "As a result, the MaxEnt ITG model and its extensions ( have achieved competing performance as compared with state-of-the-art phrase-based systems.", "labels": [], "entities": []}, {"text": "Despite these successful efforts, the ITG reordering classifiers still face a major challenge: how to extract features from training examples (i.e., a pair of bilingual strings).", "labels": [], "entities": []}, {"text": "It is hard to decide which words are representative for predicting reordering, either manually or automatically, especially for long sentences.", "labels": [], "entities": [{"text": "predicting reordering", "start_pos": 56, "end_pos": 77, "type": "TASK", "confidence": 0.9302397668361664}]}, {"text": "As a result, only use boundary words (i.e., the first and the last words in a string) to predict the ordering.", "labels": [], "entities": []}, {"text": "What if we look inside?", "labels": [], "entities": []}, {"text": "Is it possible to avoid manual feature engineering and learn semantic representations from the data?", "labels": [], "entities": []}, {"text": "Fortunately, the rapid development of intersecting deep learning with natural language processing () brings hope for alleviating this problem.", "labels": [], "entities": []}, {"text": "In these efforts, natural language words are represented as real-valued vectors, which can be naturally fed to neural networks as input.", "labels": [], "entities": []}, {"text": "More importantly, it is possible to learn vector space representations for multi-word phrases using recursive autoencoders, which opens the door to leveraging semantic representations of phrases in reordering models from a neural language modeling point of view.", "labels": [], "entities": []}, {"text": "In this work, we propose an ITG reordering classifier based on recursive autoencoders.", "labels": [], "entities": []}, {"text": "The neural network consists of four autoencoders (i.e., the first source phrase, the first target phrase, the second source phrase, and the second target phrase) and a softmax layer.", "labels": [], "entities": []}, {"text": "The recursive autoencoders, which are trained on reordering examples extracted from word-aligned bilingual corpus, are capable of producing vector space representations for arbitrary multi-word strings in decoding.", "labels": [], "entities": []}, {"text": "Therefore, our model takes the whole phrases rather than only boundary words into consideration when predicting phrase permutations.", "labels": [], "entities": []}, {"text": "Experiments on the NIST 2008 dataset show that our system significantly improves over the MaxEnt classifier by 1.07 in terms of case-insensitive BLEU score.", "labels": [], "entities": [{"text": "NIST 2008 dataset", "start_pos": 19, "end_pos": 36, "type": "DATASET", "confidence": 0.9843305349349976}, {"text": "BLEU score", "start_pos": 145, "end_pos": 155, "type": "METRIC", "confidence": 0.9673442244529724}]}], "datasetContent": [{"text": "One possible reason is that there is limited room for improvement as our system makes fewer wrong predictions for long composed blocks.", "labels": [], "entities": []}, {"text": "The above results suggest that our system does go beyond using boundary words and make a better use of the merging blocks by using vector space representations.", "labels": [], "entities": []}, {"text": "shows the effect of training dataset size on BLEU scores.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 45, "end_pos": 49, "type": "METRIC", "confidence": 0.9972267746925354}]}, {"text": "We find that BLEU scores on both the development and test sets rise with the increase of the training dataset size.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 13, "end_pos": 17, "type": "METRIC", "confidence": 0.9991375207901001}]}, {"text": "As the training process is very time-consuming, only the reordering examples extracted from 1/5 of the entire parallel training corpus are used in our experiments to train our model.", "labels": [], "entities": []}, {"text": "Obviously, with more efficient training algorithms, making full use of all the reordering examples extracted from the entire corpus will result in better results.", "labels": [], "entities": []}, {"text": "We leave this for future work.", "labels": [], "entities": []}, {"text": "shows a number of words and phrases that are close (measured by Euclidean distance) in the n-dimensional space.", "labels": [], "entities": []}, {"text": "We randomly select about 370K target side phrases used in our experiments and cluster them into 983 clusters using k-means algorithm.", "labels": [], "entities": []}, {"text": "The distance between two phrases are measured by the Euclidean distance between their vector representations.", "labels": [], "entities": []}, {"text": "As shown in, cluster 1 mainly consists of nouns, cluster 2 mainly contains verb/noun+preposition structures, cluster 3 contains compound phrases, cluster 4 consists of phrases which should be followed by a clause, and cluster 5 mainly contains the beginning parts of prepositional phrases that tend to be followed by a noun phrase or word.", "labels": [], "entities": []}, {"text": "We find that the words and phrases in the same cluster have similar behaviors from a reordering point of view rather than relatedness.", "labels": [], "entities": []}, {"text": "This indicates that the vector representations produced by the recursive autoencoders are helpful for capturing reordering regularities.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: The effect of reordering training data size on  BLEU scores. The BLEU scores rise with the increase of  training data size. Due to the computational cost, we only  used 1/5 of the entire bilingual corpus to train our neural  reordering model.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 58, "end_pos": 62, "type": "METRIC", "confidence": 0.989528477191925}, {"text": "BLEU", "start_pos": 75, "end_pos": 79, "type": "METRIC", "confidence": 0.9981221556663513}]}]}