{"title": [{"text": "Max-Margin Synchronous Grammar Induction for Machine Translation", "labels": [], "entities": [{"text": "Synchronous Grammar Induction", "start_pos": 11, "end_pos": 40, "type": "TASK", "confidence": 0.7812026143074036}, {"text": "Machine Translation", "start_pos": 45, "end_pos": 64, "type": "TASK", "confidence": 0.7413563877344131}]}], "abstractContent": [{"text": "Traditional synchronous grammar induction estimates parameters by maximizing likelihood , which only has a loose relation to translation quality.", "labels": [], "entities": [{"text": "synchronous grammar induction", "start_pos": 12, "end_pos": 41, "type": "TASK", "confidence": 0.6622784237066904}, {"text": "likelihood", "start_pos": 77, "end_pos": 87, "type": "METRIC", "confidence": 0.9438583254814148}]}, {"text": "Alternatively, we propose a max-margin estimation approach to discrim-inatively inducing synchronous grammars for machine translation, which directly optimizes translation quality measured by BLEU.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 114, "end_pos": 133, "type": "TASK", "confidence": 0.7757717669010162}, {"text": "BLEU", "start_pos": 192, "end_pos": 196, "type": "METRIC", "confidence": 0.993493378162384}]}, {"text": "In the max-margin estimation of parameters, we only need to calculate Viterbi translations.", "labels": [], "entities": []}, {"text": "This further facilitates the incorporation of various non-local features that are defined on the target side.", "labels": [], "entities": []}, {"text": "We test the effectiveness of our max-margin estimation framework on a competitive hierarchical phrase-based system.", "labels": [], "entities": [{"text": "max-margin estimation", "start_pos": 33, "end_pos": 54, "type": "TASK", "confidence": 0.5993963032960892}]}, {"text": "Experiments show that our max-margin method significantly outperforms the traditional two-step pipeline for synchronous rule extraction by 1.3 BLEU points and is also better than previous max-likelihood estimation method.", "labels": [], "entities": [{"text": "synchronous rule extraction", "start_pos": 108, "end_pos": 135, "type": "TASK", "confidence": 0.6523421704769135}, {"text": "BLEU", "start_pos": 143, "end_pos": 147, "type": "METRIC", "confidence": 0.9981818199157715}]}], "introductionContent": [{"text": "Synchronous grammar induction, which refers to the process of learning translation rules from bilingual corpus, still remains an open problem in statistical machine translation (SMT).", "labels": [], "entities": [{"text": "Synchronous grammar induction", "start_pos": 0, "end_pos": 29, "type": "TASK", "confidence": 0.9054223895072937}, {"text": "statistical machine translation (SMT)", "start_pos": 145, "end_pos": 182, "type": "TASK", "confidence": 0.7807980477809906}]}, {"text": "Although stateof-the-art SMT systems model the translation process based on synchronous grammars (including bilingual phrases), most of them still learn translation rules via a pipeline with word-based heuristics (.", "labels": [], "entities": [{"text": "SMT", "start_pos": 25, "end_pos": 28, "type": "TASK", "confidence": 0.9592390656471252}]}, {"text": "This pipeline first builds word alignments using heuristic combination strategies, then heuristically extracts rules that are consistent with word alignments.", "labels": [], "entities": [{"text": "word alignments", "start_pos": 27, "end_pos": 42, "type": "TASK", "confidence": 0.726312518119812}]}, {"text": "Such heuristic pipeline * Corresponding author is not elegant theoretically.", "labels": [], "entities": []}, {"text": "It brings an undesirable gap that separates modeling and learning in an SMT system.", "labels": [], "entities": [{"text": "SMT", "start_pos": 72, "end_pos": 75, "type": "TASK", "confidence": 0.9894524812698364}]}, {"text": "Therefore, researchers have proposed alternative approaches to learning synchronous grammars directly from sentence pairs without word alignments, via generative models () or discriminative models ().", "labels": [], "entities": []}, {"text": "Theoretically, these approaches describe how sentence pairs are generated by applying sequences of synchronous rules in an elegant way.", "labels": [], "entities": []}, {"text": "However, they learn synchronous grammars by maximizing likelihood, which only has a loose relation to translation quality.", "labels": [], "entities": [{"text": "likelihood", "start_pos": 55, "end_pos": 65, "type": "METRIC", "confidence": 0.9557569622993469}]}, {"text": "Moreover, generative models are normally hard to be extended to incorporate useful features, and the discriminative synchronous grammar induction model proposed by only incorporates local features defined on parse trees of the source language.", "labels": [], "entities": []}, {"text": "Nonlocal features, which encode information from parse trees of the target language, have never been exploited before due to the computational complexity of normalization in max-likelihood estimation.", "labels": [], "entities": []}, {"text": "Consequently, we would like to learn synchronous grammars in a discriminative way that can directly maximize the end-to-end translation quality measured by BLEU (, and is also able to incorporate non-local features from target parse trees.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 156, "end_pos": 160, "type": "METRIC", "confidence": 0.9974156618118286}]}, {"text": "We thus propose a max-margin estimation method to discriminatively induce synchronous grammar directly from sentence pairs without word alignments.", "labels": [], "entities": []}, {"text": "We try to maximize the margin between a reference translation and a candidate translation with translation errors that are measured by BLEU.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 135, "end_pos": 139, "type": "METRIC", "confidence": 0.9977109432220459}]}, {"text": "The more serious the translation errors, the larger the margin.", "labels": [], "entities": [{"text": "translation", "start_pos": 21, "end_pos": 32, "type": "TASK", "confidence": 0.9434448480606079}, {"text": "margin", "start_pos": 56, "end_pos": 62, "type": "METRIC", "confidence": 0.9972395896911621}]}, {"text": "In this way, our max-margin method is able to learn synchronous grammars according to their translation performance.", "labels": [], "entities": []}, {"text": "We further incorporate various nonlocal features defined on target parse trees.", "labels": [], "entities": []}, {"text": "We efficiently calculate the non-local feature values of a translation over its exponential derivation space using the inside-outside algorithm.", "labels": [], "entities": []}, {"text": "Because our maxmargin estimation optimizes feature weights only by the feature values of Viterbi and reference translations, we are able to efficiently perform optimization even with non-local features.", "labels": [], "entities": []}, {"text": "We apply the proposed max-margin estimation method to learn synchronous grammars fora hierarchical phrase-based translation system which typically produces state-of-the-art performance.", "labels": [], "entities": [{"text": "phrase-based translation", "start_pos": 99, "end_pos": 123, "type": "TASK", "confidence": 0.6636013686656952}]}, {"text": "With non-local features defined on target parse trees, our max-margin method significantly outperforms the baseline that uses synchronous rules learned from the traditional pipeline by 1.3 BLEU points on large-scale Chinese-English bilingual training data.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 189, "end_pos": 193, "type": "METRIC", "confidence": 0.9981369972229004}]}, {"text": "The remainder of this paper is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 presents the discriminative synchronous grammar induction model with the nonlocal features.", "labels": [], "entities": []}, {"text": "In Section 3, we elaborate our maxmargin estimation method which is able to directly optimize BLEU, and discuss how we induce grammar rules.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 94, "end_pos": 98, "type": "METRIC", "confidence": 0.9953489899635315}]}, {"text": "Local and non-local features are described in Section 4.", "labels": [], "entities": []}, {"text": "Finally, in Section 5, we verify the effectiveness of our method through experiments by comparing it against both the traditional pipeline and max-likelihood estimation method.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we present our experiments on the NIST Chinese-to-English translation tasks.", "labels": [], "entities": [{"text": "NIST Chinese-to-English translation tasks", "start_pos": 51, "end_pos": 92, "type": "TASK", "confidence": 0.7877257764339447}]}, {"text": "We first compare our max-margin based method with the traditional pipeline on a large bitext which contains 1.1 million sentences.", "labels": [], "entities": []}, {"text": "We then present a detailed comparison on a smaller dataset, in order to analyze the effectiveness of max-margin estimation comparing with the max likelihood estimation (, and also the effectiveness of the non-local features that are defined on the target side.", "labels": [], "entities": [{"text": "max likelihood estimation", "start_pos": 142, "end_pos": 167, "type": "METRIC", "confidence": 0.8246327638626099}]}, {"text": "We also incorporate the sparse features during decoding in away similar to and.", "labels": [], "entities": []}, {"text": "In order to optimize these sparse features with the dense features by MERT, we group features of the same type into one coarse \"summary feature\", and get three such features including: rule, phrase-boundary and phrase orientation features.", "labels": [], "entities": []}, {"text": "In this way, we rescale the weights of the three \"summary features\" with the 8 dense features by MERT.", "labels": [], "entities": [{"text": "MERT", "start_pos": 97, "end_pos": 101, "type": "DATASET", "confidence": 0.6623486876487732}]}, {"text": "We achieve a further improvement of +0.37 BLEU points.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 42, "end_pos": 46, "type": "METRIC", "confidence": 0.997010350227356}]}, {"text": "Therefore, our training algorithm is able to learn the useful information encoded by the sparse features for translation.", "labels": [], "entities": [{"text": "translation", "start_pos": 109, "end_pos": 120, "type": "TASK", "confidence": 0.9795247912406921}]}], "tableCaptions": [{"text": " Table 2: Experiment results. Baseline is an in-house implementation of hierarchical phrase based system. Moses  denotes the implementation of hierarchical phrased-model in Moses (", "labels": [], "entities": []}, {"text": " Table 3: Comparison of Max-margin and Max-likelihood estimation on a smaller corpus. For max-margin method, we  present two results according to the usages of non-local features. The max-margin with non-local features significantly  outperforms the Baseline (p < 0.01) and also the max-likelihood estimation (p < 0.05).", "labels": [], "entities": []}]}