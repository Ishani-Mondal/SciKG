{"title": [{"text": "Easy Victories and Uphill Battles in Coreference Resolution", "labels": [], "entities": [{"text": "Coreference Resolution", "start_pos": 37, "end_pos": 59, "type": "TASK", "confidence": 0.9305394291877747}]}], "abstractContent": [{"text": "Classical coreference systems encode various syntactic, discourse, and semantic phenomena explicitly, using heterogenous features computed from hand-crafted heuristics.", "labels": [], "entities": []}, {"text": "In contrast , we present a state-of-the-art coreference system that captures such phenomena implicitly , with a small number of homogeneous feature templates examining shallow properties of mentions.", "labels": [], "entities": []}, {"text": "Surprisingly, our features are actually more effective than the corresponding hand-engineered ones at modeling these key linguistic phenomena, allowing us to win \"easy victories\" without crafted heuris-tics.", "labels": [], "entities": []}, {"text": "These features are successful on syntax and discourse; however, they do not model semantic compatibility well, nor do we see gains from experiments with shallow semantic features from the literature, suggesting that this approach to semantics is an \"uphill battle .\" Nonetheless, our final system 1 outper-forms the Stanford system (Lee et al.", "labels": [], "entities": []}, {"text": "(2011), the winner of the CoNLL 2011 shared task) by 3.5% absolute on the CoNLL metric and outperforms the IMS system (Bj\u00f6rkelund and Farkas (2012), the best publicly available En-glish coreference system) by 1.9% absolute.", "labels": [], "entities": [{"text": "CoNLL 2011 shared task", "start_pos": 26, "end_pos": 48, "type": "TASK", "confidence": 0.7335466891527176}, {"text": "CoNLL metric", "start_pos": 74, "end_pos": 86, "type": "DATASET", "confidence": 0.8833518028259277}]}], "introductionContent": [{"text": "Coreference resolution is a multi-faceted task: humans resolve references by exploiting contextual and grammatical clues, as well as semantic information and world knowledge, so capturing each of The Berkeley Coreference Resolution System is available at http://nlp.cs.berkeley.edu.", "labels": [], "entities": [{"text": "Coreference resolution", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.935003936290741}]}, {"text": "these will be necessary for an automatic system to fully solve the problem.", "labels": [], "entities": []}, {"text": "Acknowledging this complexity, coreference systems, either learning-based ( or rule-based (), draw on diverse information sources and complex heuristics to resolve pronouns, model discourse, determine anaphoricity, and identify semantically compatible mentions.", "labels": [], "entities": []}, {"text": "However, this leads to systems with many heterogenous parts that can be difficult to interpret or modify.", "labels": [], "entities": []}, {"text": "We build a learning-based, mention-synchronous coreference system that aims to use the simplest possible set of features to tackle the various aspects of coreference resolution.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 154, "end_pos": 176, "type": "TASK", "confidence": 0.9376902282238007}]}, {"text": "Though they arise from a small number of simple templates, our features are numerous, which works to our advantage: we can both implicitly model important linguistic effects and capture other patterns in the data that are not easily teased out by hand.", "labels": [], "entities": []}, {"text": "As a result, our datadriven, homogeneous feature set is able to achieve high performance despite only using surface-level document characteristics and shallow syntactic information.", "labels": [], "entities": []}, {"text": "We win \"easy victories\" without designing features and heuristics explicitly targeting particular phenomena.", "labels": [], "entities": []}, {"text": "Though our approach is successful at modeling syntax, we find semantics to be a much more challenging aspect of coreference.", "labels": [], "entities": []}, {"text": "Our base system uses only two recall-oriented features on nominal and proper mentions: head match and exact string match.", "labels": [], "entities": [{"text": "recall-oriented", "start_pos": 30, "end_pos": 45, "type": "METRIC", "confidence": 0.9918584227561951}, {"text": "head match", "start_pos": 87, "end_pos": 97, "type": "METRIC", "confidence": 0.8443749845027924}, {"text": "exact string match", "start_pos": 102, "end_pos": 120, "type": "METRIC", "confidence": 0.9114566842714945}]}, {"text": "Building on these features, we critically evaluate several classes of semantic features which intu-itively should prove useful but have had mixed results in the literature, and we observe that they are ineffective for our system.", "labels": [], "entities": []}, {"text": "However, these features are beneficial when gold mentions are provided to our system, leading us to conclude that the large number of system mentions extracted by most coreference systems) means that weak indicators cannot overcome the bias against making coreference links.", "labels": [], "entities": []}, {"text": "Capturing semantic information in this shallow way is an \"uphill battle\" due to this structural property of coreference resolution.", "labels": [], "entities": [{"text": "Capturing semantic information", "start_pos": 0, "end_pos": 30, "type": "TASK", "confidence": 0.862427810827891}, {"text": "coreference resolution", "start_pos": 108, "end_pos": 130, "type": "TASK", "confidence": 0.9431325793266296}]}, {"text": "Nevertheless, using a simple architecture and feature set, our final system outperforms the two best publicly available English coreference systems, the Stanford system () and the IMS system, by wide margins: 3.5% absolute and 1.9% absolute, respectively, on the CoNLL metric.", "labels": [], "entities": [{"text": "Stanford system", "start_pos": 153, "end_pos": 168, "type": "DATASET", "confidence": 0.882988303899765}, {"text": "CoNLL metric", "start_pos": 263, "end_pos": 275, "type": "DATASET", "confidence": 0.7702800929546356}]}], "datasetContent": [{"text": "Throughout this work, we use the datasets from the CoNLL 2011 shared task, which is derived from the OntoNotes corpus ().", "labels": [], "entities": [{"text": "CoNLL 2011 shared task", "start_pos": 51, "end_pos": 73, "type": "DATASET", "confidence": 0.9321264177560806}, {"text": "OntoNotes corpus", "start_pos": 101, "end_pos": 117, "type": "DATASET", "confidence": 0.9226095676422119}]}, {"text": "When applicable, we use the standard automatic parses and NER tags for each document.", "labels": [], "entities": []}, {"text": "All experiments use system mentions except where otherwise indicated.", "labels": [], "entities": []}, {"text": "For each experiment, we report MUC (, B 3 (Bagga and, and CEAF e (, as well as their average, the CoNLL metric.", "labels": [], "entities": [{"text": "MUC", "start_pos": 31, "end_pos": 34, "type": "METRIC", "confidence": 0.9586381316184998}, {"text": "B 3", "start_pos": 38, "end_pos": 41, "type": "METRIC", "confidence": 0.9497109949588776}, {"text": "CEAF e", "start_pos": 58, "end_pos": 64, "type": "METRIC", "confidence": 0.9772364497184753}, {"text": "CoNLL metric", "start_pos": 98, "end_pos": 110, "type": "METRIC", "confidence": 0.7786033749580383}]}, {"text": "All metrics are computed using version 5 of the official CoNLL scorer.", "labels": [], "entities": [{"text": "CoNLL scorer", "start_pos": 57, "end_pos": 69, "type": "DATASET", "confidence": 0.8234187364578247}]}], "tableCaptions": [{"text": " Table 1: Our SURFACE feature set, which exploits a  small number of surface-level mention properties. Fea- ture counts for each template are computed over the train- ing set, and include features generated by our conjunction  scheme (not explicitly shown in the table; see", "labels": [], "entities": [{"text": "Fea- ture counts", "start_pos": 103, "end_pos": 119, "type": "METRIC", "confidence": 0.9028883427381516}]}, {"text": " Table 2: Results for our SURFACE system, the STAN- FORD system, and the IMS system on the CoNLL 2011  development set. Complete results are shown in Ta- ble 7. Despite using limited information sources, our sys- tem is able to substantially outperform the other two, the  two best publicly-available English coreference systems.  Bolded values are significant with p < 0.05 according to  a bootstrap resampling test.", "labels": [], "entities": [{"text": "STAN- FORD", "start_pos": 46, "end_pos": 56, "type": "METRIC", "confidence": 0.3856127957503001}, {"text": "CoNLL 2011  development set", "start_pos": 91, "end_pos": 118, "type": "DATASET", "confidence": 0.969277873635292}, {"text": "Ta- ble 7", "start_pos": 150, "end_pos": 159, "type": "DATASET", "confidence": 0.7696760147809982}]}, {"text": " Table 3: CoNLL metric scores on the development set,  for the three different ablations and replacement features  described in Section 4.2. Feature types are described in  the text; + indicates inclusion of that feature class, \u2212 in- dicates exclusion. Each individual shallow indicator ap- pears to do as well at capturing its target phenomenon as  the hand-engineered features, while capturing other infor- mation as well. Moreover, the hand-engineered features  give no benefit over the SURFACE system.", "labels": [], "entities": []}, {"text": " Table 4: Analysis of our SURFACE system on the de- velopment set. We characterize each predicted mention  by its status in the gold standard (singleton, starting a  new entity, or anaphoric), its type (pronominal or nom- inal/proper), and by whether its head has appeared as the  head of a previous mention. Each cell shows our sys- tem's accuracy on that mention class as well as the size  of the class. The biggest weakness of our system appears  to be its inability to resolve anaphoric mentions with new  heads (bottom-left cell).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 340, "end_pos": 348, "type": "METRIC", "confidence": 0.9982929825782776}]}, {"text": " Table 5: CoNLL metric scores on the development set  for our SEM features when added on top of our SURFACE  features. We experiment on both system mentions and  gold mentions. Surprisingly, despite the fact that absolute  performance numbers are much higher on gold mentions  and there is less room for improvement, the semantic fea- tures help much more than they do on system mentions.", "labels": [], "entities": []}, {"text": " Table 7. Again, we compare to", "labels": [], "entities": []}, {"text": " Table 7: CoNLL metric scores for our systems on the CoNLL development and blind test sets, compared to the results  of Lee et al. (2011) (STANFORD) and Bj\u00f6rkelund and Farkas (2012) (IMS). Starred systems are contributions of this  work. Bolded F 1 values represent statistically significant improvements over other systems with p < 0.05 using a  bootstrap resampling test. Metric values reflect version 5 of the CoNLL scorer.", "labels": [], "entities": [{"text": "CoNLL development and blind test sets", "start_pos": 53, "end_pos": 90, "type": "DATASET", "confidence": 0.8054821590582529}]}]}