{"title": [{"text": "Simulating Early-Termination Search for Verbose Spoken Queries", "labels": [], "entities": []}], "abstractContent": [{"text": "Building search engines that can respond to spoken queries with spoken content requires that the system not just be able to find useful responses, but also that it know when it has heard enough about what the user wants to be able to do so.", "labels": [], "entities": []}, {"text": "This paper describes a simulation study with queries spoken by non-native speakers that suggests that indicates that finding relevant content is often possible within a half minute, and that combining features based on automatically recognized words with features designed for automated prediction of query difficulty can serve as a useful basis for predicting when that useful content has been found.", "labels": [], "entities": []}], "introductionContent": [{"text": "Much of the early work on what has come to be called \"speech retrieval\" has focused on the use of text queries to rank segments that are automatically extracted from spoken content.", "labels": [], "entities": [{"text": "speech retrieval\"", "start_pos": 54, "end_pos": 71, "type": "TASK", "confidence": 0.8010603388150533}]}, {"text": "While such an approach can be useful in a desktop environment, half of the world's Internet users can access the global information network only using a voice-only mobile phone.", "labels": [], "entities": []}, {"text": "This raises two challenges: 1) in such settings, both the query and the content must bespoken, and 2) the language being spoken will often be one for which we lack accurate speech recognition.", "labels": [], "entities": []}, {"text": "The Web has taught us that the \"ten blue links\" paradigm can be a useful response to short queries.", "labels": [], "entities": []}, {"text": "That works because typed queries are often fairly precise, and tabular responses are easily skimmed.", "labels": [], "entities": []}, {"text": "However, spoken queries, and in particular opendomain spoken queries for unrestricted spoken content, pose new challenges that call for new thinking about interaction design.", "labels": [], "entities": []}, {"text": "This paper explores the potential of a recently proposed alternative, in which the spoken queries are long, and only one response can be played at a time by the system.", "labels": [], "entities": []}, {"text": "This approach, which has been called Query by Babbling, requires that the user ramble on about what they are looking for, that the system be able to estimate when it has found a good response, and that the user be able to continue the search interaction by babbling on if the first response does not fully meet their needs.", "labels": [], "entities": []}, {"text": "One might question whether users actually will \"babble\" for extended periods about their information need.", "labels": [], "entities": []}, {"text": "There are two reasons to believe that some users might.", "labels": [], "entities": []}, {"text": "First, we are particularly interested in ultimately serving users who search for information in languages for which we do not have usable speech recognition systems.", "labels": [], "entities": []}, {"text": "Speech-to-speech matching in such cases will be challenging, and we would not expect short queries to work well.", "labels": [], "entities": [{"text": "Speech-to-speech matching", "start_pos": 0, "end_pos": 25, "type": "TASK", "confidence": 0.7252348959445953}]}, {"text": "Second, we seek to principally serve users who will be new to search, and thus not yet conditioned to issue short queries.", "labels": [], "entities": []}, {"text": "As with Web searchers, we can expect them to explore initially, then to ultimately settle on query strategies that work well enough to meet their needs.", "labels": [], "entities": []}, {"text": "If longer queries work better for them, it seems reasonable to expect that they would use longer queries.", "labels": [], "entities": []}, {"text": "Likewise, if systems cannot effectively use longer queries to produce useful results, then people will not use them.", "labels": [], "entities": []}, {"text": "To get a sense for whether such an interaction modality is feasible, we performed a simulation study for this paper in which we asked people to babble on some topic for which we already have relevance judgments results.", "labels": [], "entities": []}, {"text": "We transcribe those babbles using automatic speech recognition (ASR), then note how many words must be babbled in each case before an information retrieval system is first able to place a relevant document in rank one.", "labels": [], "entities": [{"text": "automatic speech recognition (ASR)", "start_pos": 34, "end_pos": 68, "type": "TASK", "confidence": 0.7971422175566355}]}, {"text": "From this perspective, our results show that people are indeed often able to babble usefully; and, moreover, that current information retrieval technology could often place relevant results at rank one within half a minute or so of babbling even with contemporary speech recognition technology.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 264, "end_pos": 282, "type": "TASK", "confidence": 0.7248286306858063}]}, {"text": "The question then arises as to whether a system can be built that would recognize when an answer is available at rank one.", "labels": [], "entities": []}, {"text": "Barging in with an answer before that point wastes time and disrupts the user; barging in long after that point also wastes time, but also risks user abandonment.", "labels": [], "entities": [{"text": "Barging in with an answer before that point", "start_pos": 0, "end_pos": 43, "type": "TASK", "confidence": 0.8641342669725418}]}, {"text": "We therefore want a \"Goldilocks\" system that can get it just about right.", "labels": [], "entities": []}, {"text": "To this end, we introduce an evaluation measure that differentially penalizes early and late responses.", "labels": [], "entities": []}, {"text": "Our experiments using such a measure show that systems can be built that, on average, do better than could be achieved by any fixed response delay.", "labels": [], "entities": []}, {"text": "The remainder of this paper is organized as follows: We begin in Section 2 with a brief review of related work.", "labels": [], "entities": []}, {"text": "Section 3 then describes the design of the ranking component of our experiment; Section 4 follows with some exploratory analysis of the ranking results using our test collection.", "labels": [], "entities": []}, {"text": "Section 6 completes the description of our methods with an explanation of how the stopping classifier is built; Section 7 then presents end-to-end evaluation results using anew measure designed for this task.", "labels": [], "entities": []}, {"text": "Section 8 concludes the paper with some remarks on future work.", "labels": [], "entities": []}], "datasetContent": [{"text": "This section describes our evaluation measure and the baselines to which we compared.", "labels": [], "entities": []}, {"text": "To evaluate a stopping prediction model, the fundamental goal is to stop with a relevant document in rank one, and to do so as close in time as possible to the first such opportunity.", "labels": [], "entities": [{"text": "stopping prediction", "start_pos": 14, "end_pos": 33, "type": "TASK", "confidence": 0.8782088160514832}]}, {"text": "If the first guess is bad, it would be reasonable to score a second guess, with some penalty.", "labels": [], "entities": []}, {"text": "Specifically, there are several things that we would like our evaluation framework to describe.", "labels": [], "entities": []}, {"text": "Keeping in mind that ultimately the system will interrupt the speaker to notify them of results, we first want to avoid the interruption before we have found a good answer.", "labels": [], "entities": []}, {"text": "Our evaluation measure gives no credit for such a guess.", "labels": [], "entities": []}, {"text": "Second, we want to avoid interrupting long after finding the first relevant answer.", "labels": [], "entities": []}, {"text": "Credit is reduced with increasing delays after the first point where we could have barged in.", "labels": [], "entities": [{"text": "Credit", "start_pos": 0, "end_pos": 6, "type": "METRIC", "confidence": 0.9162861704826355}]}, {"text": "Third, when we do barge-in, there must indeed be a good answer in rank one.", "labels": [], "entities": []}, {"text": "This will be true if we bargein at the first opportunity, but if we barge-in later the good answer we had found might have dropped back out of the first position.", "labels": [], "entities": []}, {"text": "No credit is given if we barge-in such a case.", "labels": [], "entities": [{"text": "credit", "start_pos": 3, "end_pos": 9, "type": "METRIC", "confidence": 0.970607578754425}]}, {"text": "Finally, if a bad position for first barge-in is chosen, we would like at least to get it right the second time.", "labels": [], "entities": []}, {"text": "Thus, we limit ourselves to two tries, awarding half the credit on the second try that we could have received had we barged in at the same point on the first try.", "labels": [], "entities": []}, {"text": "The delay penalty is modeled using an exponential distribution that declines with each new word that arrives after the first opportunity.", "labels": [], "entities": [{"text": "delay penalty", "start_pos": 4, "end_pos": 17, "type": "METRIC", "confidence": 0.966342568397522}]}, {"text": "Let q 0 be the first point within a query where the reciprocal rank is one.", "labels": [], "entities": []}, {"text": "Let pi be the first \"yes\" guess of the predictor after point q 0 . The score is thus e \u03bb(q 0 \u2212p i ) , where \u03bb is the half-life, or the number of words by which the exponential decay has dropped to one-half.", "labels": [], "entities": []}, {"text": "The equation is scaled by 0.5 if i is the second element (guess) of p, and by 0.25 if it is the third., some cases the potential stopping points are consecutive, while in others they are intermittentwe penalize delays from the first good opportunity even when there is no relevant document in position one because we feel that best models the user experience.", "labels": [], "entities": []}, {"text": "Unjudged documents in position one are treated as non-relevant.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Text from an example babble (274-1). The left is transcribed through human comprehension; the right is the  output from an automatic speech recognition engine.", "labels": [], "entities": []}, {"text": " Table 2: Average ASR Word Error Rate over 3 babbles  per topic (SD=Standard Deviation).", "labels": [], "entities": [{"text": "Average ASR Word Error Rate", "start_pos": 10, "end_pos": 37, "type": "METRIC", "confidence": 0.7911438822746277}]}, {"text": " Table 3: Rank-1 relevance (\"Rel\") judgments and position of first and last scorable guesses.", "labels": [], "entities": [{"text": "Rank-1 relevance (\"Rel\")", "start_pos": 10, "end_pos": 34, "type": "METRIC", "confidence": 0.7680941104888916}]}, {"text": " Table 4: Cross validation accuracy (\"Acy.\") measures for  stop-prediction classifiers: naive Bayes, logistic regres- sion, and Decision trees.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 27, "end_pos": 35, "type": "METRIC", "confidence": 0.7407646775245667}, {"text": "Acy.", "start_pos": 38, "end_pos": 42, "type": "METRIC", "confidence": 0.9117706418037415}]}]}