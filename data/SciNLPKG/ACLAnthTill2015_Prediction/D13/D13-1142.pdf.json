{"title": [{"text": "Application of Localized Similarity for Web Documents", "labels": [], "entities": []}], "abstractContent": [{"text": "In this paper we present a novel approach to automatic creation of anchor texts for hyper-links in a document pointing to similar documents.", "labels": [], "entities": []}, {"text": "Methods used in this approach rank parts of a document based on the similarity to a presumably related document.", "labels": [], "entities": []}, {"text": "Ranks are then used to automatically construct the best anchor text fora link inside original document to the compared document.", "labels": [], "entities": []}, {"text": "A number of different methods from information retrieval and natural language processing are adapted for this task.", "labels": [], "entities": [{"text": "information retrieval", "start_pos": 35, "end_pos": 56, "type": "TASK", "confidence": 0.8423626124858856}]}, {"text": "Automatically constructed anchor texts are manually evaluated in terms of relat-edness to linked documents and compared to baseline consisting of originally inserted anchor texts.", "labels": [], "entities": []}, {"text": "Additionally we use crowdsourc-ing for evaluation of original anchors and automatically constructed anchors.", "labels": [], "entities": []}, {"text": "Results show that our best adapted methods rival the precision of the baseline method.", "labels": [], "entities": [{"text": "precision", "start_pos": 53, "end_pos": 62, "type": "METRIC", "confidence": 0.9979360103607178}]}], "introductionContent": [{"text": "One of the features of hypertext documents are hyperlinks that point to other resources -pictures, videos, tweets, or other hypertext documents.", "labels": [], "entities": []}, {"text": "A fairly familiar category of the latter is related articles; these usually appear at the end of a news article or a blog post with the title of the target document as anchor text.", "labels": [], "entities": []}, {"text": "The target document is similar in content to original document; it may tell the story from another point of view, it maybe a more detailed version of apart of the events in the original document, etc.", "labels": [], "entities": []}, {"text": "Another category are the in-text links; these appear inside the main body of text and use some of the existing text as anchor.", "labels": [], "entities": []}, {"text": "Ideally the anchor text is selected in such away that it conveys some information about the target document; in reality sometimes just an adverb (e.g. here, there) is used, or even the destination URL may serve as anchor.", "labels": [], "entities": []}, {"text": "Our goal is to develop a system that automatically constructs in-text links, i.e. fora query document finds a target document and an appropriate part of the text of the query document that serves as the anchor text for the hyperlink.", "labels": [], "entities": []}, {"text": "We want the target document to be similar in content to the query document and the anchor text to indicate that content.", "labels": [], "entities": []}, {"text": "There are many potential uses for such a system, especially for simplifying and streamlining document creation.", "labels": [], "entities": [{"text": "document creation", "start_pos": 93, "end_pos": 110, "type": "TASK", "confidence": 0.6973712593317032}]}, {"text": "This includes authors of blogs that may use the system for adding related content from other sources without exhausting manual search for such material.", "labels": [], "entities": []}, {"text": "It may also be used when writing a scientific paper, automatically adding citations to other relevant papers inside the main body.", "labels": [], "entities": []}, {"text": "This accelerates the writing, again reducing the time spent searching for possible existing research in the field.", "labels": [], "entities": []}, {"text": "A citation can be considered an in-text link without a defined starting point.", "labels": [], "entities": []}, {"text": "We have addressed the problem in two steps, separately finding a similar document, and finding the anchor text for it.", "labels": [], "entities": []}, {"text": "Since the retrieval of similar documents was a research focus for many years and is thus better researched, we have decided in this paper to focus on the placement of the anchor text fora link to a preselected document.", "labels": [], "entities": []}, {"text": "This paper is organized as follows: related work is discussed in Section 2, the methods, corpus, and evaluation are described in Section 3, followed by results and discussion in Section 4 and ending with conclusions in Section 5.", "labels": [], "entities": []}], "datasetContent": [{"text": "We have used each of the methods described in Subsection 3.3 to automatically construct anchor text for each of the 236 pairs of documents in the final corpus.", "labels": [], "entities": []}, {"text": "If a method could not find a suitable anchor, no result was returned; on average there were 147 anchors per method.", "labels": [], "entities": []}, {"text": "All the automatically created links were then manually scored by the authors with an in-house evaluation tool using scores and guidelines summarized in.", "labels": [], "entities": []}, {"text": "To calculate precision and recall, we have counted scores 2 and 3 as positive result.", "labels": [], "entities": [{"text": "precision", "start_pos": 13, "end_pos": 22, "type": "METRIC", "confidence": 0.9996248483657837}, {"text": "recall", "start_pos": 27, "end_pos": 33, "type": "METRIC", "confidence": 0.9994753003120422}]}, {"text": "Additionally we crowdsourced the evaluation of results for some of the methods.", "labels": [], "entities": []}, {"text": "For this task we prepared a special description of evaluation tasks and defined a set of questions for collecting results.", "labels": [], "entities": []}, {"text": "We provided simplified guidelines for assigning scores to automatically created anchors and set a confidence threshold of 0.55 for an assignment to be considered valid.", "labels": [], "entities": []}, {"text": "It is important to mention that the use of crowdsourcing for such tasks has to be carefully CrowdFlower: http://crowdflower.com/ Score Description 0 Anchor does not signify anything about RA or gets it wrong 1 Some connection can be established (anchor is a shared Named Entity, Noun Phrase, Verb Phrase, etc.)", "labels": [], "entities": []}, {"text": "2 Anchor is a good estimation of RA topics, but not wholly (anchor is a non-main topic in RA) 3 RA topics can be directly inferred from the anchor: Scores used for internal evaluation of automatically created anchors planned, because many issues related to monetary incentives, which are out of the scope of this paper, may arise.", "labels": [], "entities": [{"text": "Anchor", "start_pos": 2, "end_pos": 8, "type": "METRIC", "confidence": 0.9822841882705688}]}, {"text": "Results are presented as precision and recall for different methods and both evaluations in.", "labels": [], "entities": [{"text": "precision", "start_pos": 25, "end_pos": 34, "type": "METRIC", "confidence": 0.9996036887168884}, {"text": "recall", "start_pos": 39, "end_pos": 45, "type": "METRIC", "confidence": 0.9995074272155762}]}, {"text": "Empty cells in the table indicate that these methods were not evaluated using CrowdFlower.", "labels": [], "entities": [{"text": "CrowdFlower", "start_pos": 78, "end_pos": 89, "type": "DATASET", "confidence": 0.914013147354126}]}, {"text": "Recall is the fraction of relevant results out of all the possible results (236) and precision is the fraction of relevant results out of all the retrieved results.", "labels": [], "entities": [{"text": "Recall", "start_pos": 0, "end_pos": 6, "type": "METRIC", "confidence": 0.9923202395439148}, {"text": "precision", "start_pos": 85, "end_pos": 94, "type": "METRIC", "confidence": 0.9997298121452332}]}, {"text": "The first thing we notice is the general disagreement between results from the authors and CrowdFlower workers; the latter tend to give higher scores, which leads to higher precision and recall.", "labels": [], "entities": [{"text": "precision", "start_pos": 173, "end_pos": 182, "type": "METRIC", "confidence": 0.9994412064552307}, {"text": "recall", "start_pos": 187, "end_pos": 193, "type": "METRIC", "confidence": 0.998812198638916}]}, {"text": "The reason for this might be in the authors' background knowledge and thus higher expectations.", "labels": [], "entities": []}, {"text": "As a contrast almost half of CrowdFlower workers stated they don't blog and of the rest, more than a third of them don't link out, i.e. do not use related articles.", "labels": [], "entities": []}, {"text": "We also have only 74% median interannotator agreement leading us to believe that some of the annotators answered without being familiar with the question (monetary incentive issue).", "labels": [], "entities": []}, {"text": "Furthermore, CrowdFlower results for original links (our baseline) indicate that almost all of them were recognized as relevant, while our evaluators discarded 30% of them.", "labels": [], "entities": []}, {"text": "Clearly seen in the results of different sorted n-grams methods is also the precision-recall trade-off.", "labels": [], "entities": [{"text": "precision-recall", "start_pos": 76, "end_pos": 92, "type": "METRIC", "confidence": 0.9983624815940857}]}], "tableCaptions": [{"text": " Table 2: Precision and recall for manual and Crowd- Flower evaluation", "labels": [], "entities": [{"text": "Precision", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9982008934020996}, {"text": "recall", "start_pos": 24, "end_pos": 30, "type": "METRIC", "confidence": 0.9988719820976257}]}]}