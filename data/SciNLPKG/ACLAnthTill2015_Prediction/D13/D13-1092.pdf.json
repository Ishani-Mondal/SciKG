{"title": [{"text": "Implicit Feature Detection via a Constrained Topic Model and SVM", "labels": [], "entities": [{"text": "Implicit Feature Detection", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.8964169422785441}, {"text": "SVM", "start_pos": 61, "end_pos": 64, "type": "TASK", "confidence": 0.4142291843891144}]}], "abstractContent": [{"text": "Implicit feature detection, also known as implicit feature identification, is an essential aspect of feature-specific opinion mining but previous works have often ignored it.", "labels": [], "entities": [{"text": "Implicit feature detection", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.7152222196261088}, {"text": "implicit feature identification", "start_pos": 42, "end_pos": 73, "type": "TASK", "confidence": 0.5620636840661367}, {"text": "feature-specific opinion mining", "start_pos": 101, "end_pos": 132, "type": "TASK", "confidence": 0.6936584313710531}]}, {"text": "We think, based on the explicit sentences, several Support Vector Machine (SVM) classifier-s can be established to do this task.", "labels": [], "entities": []}, {"text": "Nevertheless , we believe it is possible to do better by using a constrained topic model instead of traditional attribute selection methods.", "labels": [], "entities": []}, {"text": "Experiments show that this method outperforms the traditional attribute selection methods by a large margin and the detection task can be completed better.", "labels": [], "entities": []}], "introductionContent": [{"text": "Feature-specific opinion mining has been well defined by.", "labels": [], "entities": [{"text": "Feature-specific opinion mining", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.6366774141788483}]}, {"text": "Example 1 is a cellphone review in which two features are mentioned.", "labels": [], "entities": []}, {"text": "Example 1 This cellphone is fashion in appearance, and it is also very cheap.", "labels": [], "entities": []}, {"text": "If a feature appears in a review directly, it is called an explicit feature.", "labels": [], "entities": []}, {"text": "If a feature is only implied, it is called an implicit feature.", "labels": [], "entities": []}, {"text": "In Example 1, appearance is an explicit feature while price is an implicit feature, which is implied by cheap.", "labels": [], "entities": []}, {"text": "Furthermore, an explicit sentence is defined as a sentence containing at least one explicit feature, and an implicit sentence is the sentence only containing implicit features.", "labels": [], "entities": []}, {"text": "Thus, the first sentence is an explicit sentence, while the second is an implicit one.", "labels": [], "entities": []}, {"text": "This paper proposes an approach for implicit feature detection based on SVM and Topic Model(TM).", "labels": [], "entities": [{"text": "implicit feature detection", "start_pos": 36, "end_pos": 62, "type": "TASK", "confidence": 0.6304340263207754}]}, {"text": "The Topic Model, which incorporated into constraints based on the pre-defined product feature, is established to extract the training attributes for SVM.", "labels": [], "entities": []}, {"text": "In the end, several SVM classifiers are constructed to train the selected attributes and utilized to detect the implicit features.", "labels": [], "entities": []}], "datasetContent": [{"text": "Figure 1a depicts the performance of using traditional attribute selection methods on SVM.", "labels": [], "entities": []}, {"text": "Using \u03c7 2 test on SVM can achieve the best performance, which is about 66.7%.", "labels": [], "entities": []}, {"text": "In our constrained topic model, we use different T iter and t ratio . We conducted experiments by incorporating different types prior knowledge.", "labels": [], "entities": []}, {"text": "From and 1c, we conclude that: (1)All these methods perform much better than the traditional feature selection methods, the improvements are more than 6%.The reason for the little improvement of must-links is that the topic clusters have already obtained these linked word-s.", "labels": [], "entities": []}, {"text": "(3)All the pre-existing knowledge performs best and shows 3% improvement over non prior knowledge.", "labels": [], "entities": []}, {"text": "(4)Different types of prior knowledge have different impact on the stabilities of different parameters.", "labels": [], "entities": []}, {"text": "(5)As we have expected, by combing all prior knowledge, the best performance can reach 77.78%.", "labels": [], "entities": []}, {"text": "Furthermore, as t ratio or T iter changes, our constrained topic model incorporating all prior knowledge look like very stable.", "labels": [], "entities": []}], "tableCaptions": []}