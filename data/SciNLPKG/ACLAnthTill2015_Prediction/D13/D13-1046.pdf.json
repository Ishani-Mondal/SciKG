{"title": [{"text": "Building Specialized Bilingual Lexicons Using Large-Scale Background Knowledge", "labels": [], "entities": []}], "abstractContent": [{"text": "Bilingual lexicons are central components of machine translation and cross-lingual information retrieval systems.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 45, "end_pos": 64, "type": "TASK", "confidence": 0.8011609613895416}, {"text": "cross-lingual information retrieval", "start_pos": 69, "end_pos": 104, "type": "TASK", "confidence": 0.61783433953921}]}, {"text": "Their manual construction requires strong expertise in both languages involved and is a costly process.", "labels": [], "entities": []}, {"text": "Several automatic methods were proposed as an alternative but they often rely on resources available in a limited number of languages and their performances are still far behind the quality of manual translations.", "labels": [], "entities": []}, {"text": "We introduce a novel approach to the creation of specific domain bilingual lexicon that relies on Wikipedia.", "labels": [], "entities": []}, {"text": "This massively multilingual encyclopedia makes it possible to create lexicons fora large number of language pairs.", "labels": [], "entities": []}, {"text": "Wikipedia is used to extract domains in each language, to link domains between languages and to create generic translation dictionaries.", "labels": [], "entities": []}, {"text": "The approach is tested on four specialized domains and is compared to three state of the art approaches using two language pairs: French-English and Romanian-English.", "labels": [], "entities": []}, {"text": "The newly introduced method compares favorably to existing methods in all configurations tested.", "labels": [], "entities": []}], "introductionContent": [{"text": "The plethora of textual information shared on the Web is strongly multilingual and users' information needs often go well beyond their knowledge of foreign languages.", "labels": [], "entities": []}, {"text": "In such cases, efficient machine translation and cross-lingual information retrieval systems are needed.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 25, "end_pos": 44, "type": "TASK", "confidence": 0.7321934998035431}, {"text": "cross-lingual information retrieval", "start_pos": 49, "end_pos": 84, "type": "TASK", "confidence": 0.6162320872147878}]}, {"text": "Machine translation already has a decades long history and an array of commercial systems were already deployed, including Google Translate 1 and Systran 2 . However, due to the intrinsic difficulty of the task, a number of related problems remain open, including: the gap between text semantics and statistically derived translations, the scarcity of resources in a large majority of languages and the quality of automatically obtained resources and translations.", "labels": [], "entities": [{"text": "Machine translation", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.8253107070922852}]}, {"text": "While the first challenge is general and inherent to any automatic approach, the second and the third can beat least partially addressed by an appropriate exploitation of multilingual resources that are increasingly available on the Web.", "labels": [], "entities": []}, {"text": "In this paper we focus on the automatic creation of domain-specific bilingual lexicons.", "labels": [], "entities": []}, {"text": "Such resources play a vital role in Natural Language Processing (NLP) applications that involve different languages.", "labels": [], "entities": []}, {"text": "At first, research on lexical extraction has relied on the use of parallel corpora.", "labels": [], "entities": [{"text": "lexical extraction", "start_pos": 22, "end_pos": 40, "type": "TASK", "confidence": 0.829818457365036}]}, {"text": "The scarcity of such corpora, in particular for specialized domains and for language pairs not involving English, pushed researchers to investigate the use of comparable corpora).", "labels": [], "entities": []}, {"text": "These corpora include texts which are not exact translation of each other but share common features such as domain, genre, sampling period, etc.", "labels": [], "entities": []}, {"text": "The basic intuition that underlies bilingual lexicon creation is the distributional hypothesis which puts that words with similar meanings occur in similar contexts.", "labels": [], "entities": [{"text": "bilingual lexicon creation", "start_pos": 35, "end_pos": 61, "type": "TASK", "confidence": 0.6886078218619028}]}, {"text": "Ina multilingual formulation, this hypothesis states that the translations of a word are likely to appear in similar lexical environments across languages.", "labels": [], "entities": []}, {"text": "The standard approach to bilingual lexicon extraction builds on the distributional hypothesis and compares context vectors for each word of the source and target languages.", "labels": [], "entities": [{"text": "bilingual lexicon extraction", "start_pos": 25, "end_pos": 53, "type": "TASK", "confidence": 0.6987507442633311}]}, {"text": "In this approach, the comparison of context vectors is conditioned by the existence of a seed bilingual dictionary.", "labels": [], "entities": []}, {"text": "A weakness of the method is that poor results are obtained for language pairs that are not closely related.", "labels": [], "entities": []}, {"text": "Another important problem occurs whenever the size of the seed dictionary is small due to ignoring many context words.", "labels": [], "entities": []}, {"text": "Conversely, when dictionaries are detailed, ambiguity becomes an important drawback.", "labels": [], "entities": []}, {"text": "We introduce a bilingual lexicon extraction approach that exploits Wikipedia in an innovative manner in order to tackle some of the problems mentioned above.", "labels": [], "entities": [{"text": "bilingual lexicon extraction", "start_pos": 15, "end_pos": 43, "type": "TASK", "confidence": 0.6734277804692587}]}, {"text": "Important advantages of using Wikipedia are: \u2022 The resource is available in hundreds of languages and it is structured as unambiguous concepts (i.e. articles).", "labels": [], "entities": []}, {"text": "\u2022 The languages are explicitly linked through concept translations proposed by Wikipedia contributors.", "labels": [], "entities": []}, {"text": "\u2022 It covers a large number of domains and is thus potentially useful in order to mine a wide array of specialized lexicons.", "labels": [], "entities": []}, {"text": "Mirroring the advantages, there area number of challenges associated with the use of Wikipedia: \u2022 The comparability of concept descriptions in different languages is highly variable.", "labels": [], "entities": []}, {"text": "\u2022 The translation graph is partial since, when considering any language pair, only apart of the concepts are available in both languages and explicitly connected.", "labels": [], "entities": [{"text": "translation", "start_pos": 6, "end_pos": 17, "type": "TASK", "confidence": 0.9593969583511353}]}, {"text": "\u2022 Domains are unequally covered in Wikipedia ( and efficient domain targeting is needed.", "labels": [], "entities": [{"text": "Wikipedia", "start_pos": 35, "end_pos": 44, "type": "DATASET", "confidence": 0.9336491823196411}]}, {"text": "The approach introduced in this paper aims to draw on Wikipedia's advantages while appropriately addressing associated challenges.", "labels": [], "entities": []}, {"text": "Among the techniques devised to mine Wikipedia content, we hypothesize that an adequate adaptation of Explicit Semantic Analysis (ESA) () is fitted to our application context.", "labels": [], "entities": [{"text": "Explicit Semantic Analysis (ESA)", "start_pos": 102, "end_pos": 134, "type": "TASK", "confidence": 0.5804981291294098}]}, {"text": "ESA was already successfully tested in different NLP tasks, such as word relatedness estimation or text classification, and we modify it to mine specialized domains, to characterize these domains and to link them across languages.", "labels": [], "entities": [{"text": "ESA", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.7817812561988831}, {"text": "word relatedness estimation", "start_pos": 68, "end_pos": 95, "type": "TASK", "confidence": 0.7938904563585917}, {"text": "text classification", "start_pos": 99, "end_pos": 118, "type": "TASK", "confidence": 0.7520723640918732}]}, {"text": "The evaluation of the newly introduced approach is realized on four diversified specialized domains (Breast Cancer, Corporate Finance, Wind Energy and Mobile Technology) and for two pairs of languages: French-English and Romanian-English.", "labels": [], "entities": []}, {"text": "This choice allows us to study the behavior of different approaches fora pair of languages that are richly represented and fora pair that includes Romanian, a language that has fewer associated resources than French and English.", "labels": [], "entities": []}, {"text": "Experimental results show that the newly introduced approach outperforms the three state of the art methods that were implemented for comparison.", "labels": [], "entities": []}], "datasetContent": [{"text": "The performances of our approach are evaluated against the standard approach and its developments proposed by and.", "labels": [], "entities": []}, {"text": "In this section, we first describe the data and resources we used in our experiments.", "labels": [], "entities": []}, {"text": "We then present differents parameters needed in the implementation of the different methods tested.", "labels": [], "entities": []}, {"text": "Finally, we discuss the obtained results.", "labels": [], "entities": []}, {"text": "Aside from those already mentioned, three parameters need to beset up: (1) the window size that defines contexts, (2) the association measure that measures the strength of the association between words and the (3) similarity measure that ranks candidate translations for state of the art methods.", "labels": [], "entities": []}, {"text": "Context vectors are defined using a seven-word window which approximates syntactic dependencies.", "labels": [], "entities": []}, {"text": "The association and the similarity measures (Discounted Log-Odds ratio (equation 5) and the cosine simi-6 Reference lists will be shared publicly 7 http://www.microfinance.lu/en/ 8 http://www.nlm.nih.gov/ larity) were set following, a comprehensive study of the influence of these parameters on the bilingual alignment.", "labels": [], "entities": []}, {"text": "where O ij are the cells of the 2 \u00d7 2 contingency matrix of a token s co-occurring with the term S within a given window size.", "labels": [], "entities": []}, {"text": "The F-measure of the Top 20 results (FMeasure@20), which measures the harmonic mean of precision and recall, is used as evaluation metric.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 4, "end_pos": 13, "type": "METRIC", "confidence": 0.9968817234039307}, {"text": "FMeasure@20)", "start_pos": 37, "end_pos": 49, "type": "METRIC", "confidence": 0.8856188952922821}, {"text": "precision", "start_pos": 87, "end_pos": 96, "type": "METRIC", "confidence": 0.991370677947998}, {"text": "recall", "start_pos": 101, "end_pos": 107, "type": "METRIC", "confidence": 0.9944374561309814}]}, {"text": "Precision is the total number of correct translations divided by the number of terms for which the system returned at least one answer.", "labels": [], "entities": [{"text": "Precision", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.9925845265388489}]}, {"text": "Recall is equal to the ratio between the number of correct translation and the total number of words to translate (W cand ).", "labels": [], "entities": [{"text": "Recall", "start_pos": 0, "end_pos": 6, "type": "METRIC", "confidence": 0.9939269423484802}, {"text": "W cand )", "start_pos": 115, "end_pos": 123, "type": "METRIC", "confidence": 0.9299550851186117}]}], "tableCaptions": [{"text": " Table 2: Number of content words in the  comparable corpora.", "labels": [], "entities": []}, {"text": " Table 3: Sizes of the evaluation lists.", "labels": [], "entities": []}, {"text": " Table 4: Results of the specialized dictionary creation on four specific domains, two pairs of languages.Three  state of the art methods were used for comparison: STAPP is the standard approach, MP11 is the improve- ment of the standard approach introduced in (Morin and Prochasson, 2011), BA13 is a recent method that  we developed (", "labels": [], "entities": [{"text": "BA13", "start_pos": 291, "end_pos": 295, "type": "DATASET", "confidence": 0.679686427116394}]}]}