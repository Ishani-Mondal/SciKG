{"title": [{"text": "What is Hidden among Translation Rules", "labels": [], "entities": []}], "abstractContent": [{"text": "Most of the machine translation systems rely on a large set of translation rules.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 12, "end_pos": 31, "type": "TASK", "confidence": 0.7541995942592621}]}, {"text": "These rules are treated as discrete and independent events.", "labels": [], "entities": []}, {"text": "In this short paper, we propose a novel method to model rules as observed generation output of a compact hidden model, which leads to better generalization capability.", "labels": [], "entities": []}, {"text": "We present a preliminary generative model to test this idea.", "labels": [], "entities": []}, {"text": "Experimental results show about one point improvement on TER-BLEU over a strong base-line in Chinese-to-English translation.", "labels": [], "entities": [{"text": "TER-BLEU", "start_pos": 57, "end_pos": 65, "type": "METRIC", "confidence": 0.9904928803443909}]}], "introductionContent": [{"text": "Most of the modern Statistical Machine Translation (SMT) systems, for example (, employ a large rule set that may contain tens of millions of translation rules or even more.", "labels": [], "entities": [{"text": "Statistical Machine Translation (SMT)", "start_pos": 19, "end_pos": 56, "type": "TASK", "confidence": 0.8223128318786621}]}, {"text": "In these systems, each translation rule has about 20 dense features, which represent key statistics collected from the training data, such as word translation probability, phrase translation probability etc.", "labels": [], "entities": [{"text": "word translation", "start_pos": 142, "end_pos": 158, "type": "TASK", "confidence": 0.7443512678146362}, {"text": "phrase translation", "start_pos": 172, "end_pos": 190, "type": "TASK", "confidence": 0.7833977937698364}]}, {"text": "Except for these common features, there is no connection among the translation rules.", "labels": [], "entities": []}, {"text": "The translation rules are treated as independent events.", "labels": [], "entities": [{"text": "translation", "start_pos": 4, "end_pos": 15, "type": "TASK", "confidence": 0.9686456322669983}]}, {"text": "The use of sparse features as in) to some extent mitigated this problem.", "labels": [], "entities": []}, {"text": "In their work, there are as many as 10,000 features defined on the appearance of certain frequent words and Part of Speech (POS) tags in rules.", "labels": [], "entities": []}, {"text": "They provide significant improvement in automatic evaluation metrics.", "labels": [], "entities": []}, {"text": "However, these sparse features fire quite randomly and infrequently on each rule.", "labels": [], "entities": []}, {"text": "Thus, there is still plenty of space to better model translation rules.", "labels": [], "entities": []}, {"text": "In this paper, we will explore the relationship among translation rules.", "labels": [], "entities": [{"text": "translation", "start_pos": 54, "end_pos": 65, "type": "TASK", "confidence": 0.9645837545394897}]}, {"text": "We no longer view rules as discrete or unrelated events.", "labels": [], "entities": []}, {"text": "Instead, we view rules, which are observed from training data, as random variables generated by a hidden model.", "labels": [], "entities": []}, {"text": "This generative process itself is also hidden.", "labels": [], "entities": [{"text": "generative process", "start_pos": 5, "end_pos": 23, "type": "TASK", "confidence": 0.9083453118801117}]}, {"text": "All possible generative processes can be represented with factorized structures such as weighted hypergraphs and finite state machines.", "labels": [], "entities": []}, {"text": "This approach leads to a compact model that has better generalization capability and allows translation rules not explicitly observed in training date.", "labels": [], "entities": []}, {"text": "This paper reports work-in-progress to exploit hidden relations among rules.", "labels": [], "entities": []}, {"text": "Preliminary experiments show about one point improvement on TER-BLEU over a strong baseline in Chinese-to-English translation.", "labels": [], "entities": [{"text": "TER-BLEU", "start_pos": 60, "end_pos": 68, "type": "METRIC", "confidence": 0.9955118298530579}, {"text": "Chinese-to-English translation", "start_pos": 95, "end_pos": 125, "type": "TASK", "confidence": 0.573357954621315}]}], "datasetContent": [{"text": "We carryout our experiments on web genre of Chinese-to-English translation.", "labels": [], "entities": [{"text": "Chinese-to-English translation", "start_pos": 44, "end_pos": 74, "type": "TASK", "confidence": 0.6505456864833832}]}, {"text": "The training set contains about 10 million parallel sentences available to Phase 1 of the DARPA BOLT MT task.", "labels": [], "entities": [{"text": "DARPA BOLT MT task", "start_pos": 90, "end_pos": 108, "type": "TASK", "confidence": 0.7205322086811066}]}, {"text": "The tune set contains 1275 sentences.", "labels": [], "entities": []}, {"text": "There are two test sets.", "labels": [], "entities": []}, {"text": "Test-1 is from a similar source of the tune set, and it contains 1239 sentences.", "labels": [], "entities": []}, {"text": "Test-2 is the web part of the MT08 evaluation data.", "labels": [], "entities": [{"text": "MT08 evaluation data", "start_pos": 30, "end_pos": 50, "type": "DATASET", "confidence": 0.8541742165883383}]}, {"text": "Our baseline system is a home-made Hiero (Chiang, 2005) style system.", "labels": [], "entities": [{"text": "Hiero (Chiang, 2005)", "start_pos": 35, "end_pos": 55, "type": "DATASET", "confidence": 0.8236093819141388}]}, {"text": "The baseline rule set contains about 17 million rules.", "labels": [], "entities": []}, {"text": "It contains about 40 dense features, including a 6-gram LM.", "labels": [], "entities": []}, {"text": "The sparse feature optimization algorithm is similar to the MIRA recipe described in (.", "labels": [], "entities": [{"text": "sparse feature optimization", "start_pos": 4, "end_pos": 31, "type": "TASK", "confidence": 0.6187837421894073}, {"text": "MIRA", "start_pos": 60, "end_pos": 64, "type": "METRIC", "confidence": 0.7316223382949829}]}, {"text": "We optimize on TER-BLEU ().", "labels": [], "entities": [{"text": "TER-BLEU", "start_pos": 15, "end_pos": 23, "type": "METRIC", "confidence": 0.9929836988449097}]}, {"text": "The BLEU, TER and T-B scores on the two tests are shown in  system already provides a very competitive BLEU score on MT08-WB as compared the best system in the evaluation 1 , thanks to comprehensive features in the baseline system and more data in training.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 4, "end_pos": 8, "type": "METRIC", "confidence": 0.99916672706604}, {"text": "TER", "start_pos": 10, "end_pos": 13, "type": "METRIC", "confidence": 0.9966902732849121}, {"text": "BLEU", "start_pos": 103, "end_pos": 107, "type": "METRIC", "confidence": 0.9990442395210266}, {"text": "MT08-WB", "start_pos": 117, "end_pos": 124, "type": "DATASET", "confidence": 0.8433791399002075}]}, {"text": "All the three types of systems provide consistent improvement on both test sets in terms of T-B, our optimization metric.", "labels": [], "entities": []}, {"text": "Type 1 gives marginal improvement of 0.2.", "labels": [], "entities": []}, {"text": "This shows the limitation of the generative feature.", "labels": [], "entities": [{"text": "generative", "start_pos": 33, "end_pos": 43, "type": "TASK", "confidence": 0.9700531959533691}]}, {"text": "When we use meta-rules as binary sparse features in Type 2, we obtain about one point improvement on T-B on both sets.", "labels": [], "entities": []}, {"text": "This shows the advantage of tuning individual meta-rule weights over a generative model.", "labels": [], "entities": []}, {"text": "Type 3 (0.01) and Type 2 are at the same level.", "labels": [], "entities": []}, {"text": "Proper smoothing is important to Type 3.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: scores on test-1", "labels": [], "entities": []}, {"text": " Table 2: scores on test-2 (MT08-WB)", "labels": [], "entities": [{"text": "MT08-WB", "start_pos": 28, "end_pos": 35, "type": "DATASET", "confidence": 0.7785881757736206}]}]}