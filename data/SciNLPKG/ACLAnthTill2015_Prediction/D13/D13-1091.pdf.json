{"title": [{"text": "Is Twitter A Better Corpus for Measuring Sentiment Similarity?", "labels": [], "entities": [{"text": "Measuring Sentiment Similarity", "start_pos": 31, "end_pos": 61, "type": "TASK", "confidence": 0.7096012632052103}]}], "abstractContent": [{"text": "Extensive experiments have validated the effectiveness of the corpus-based method for classifying the word's sentiment polarity.", "labels": [], "entities": [{"text": "classifying the word's sentiment polarity", "start_pos": 86, "end_pos": 127, "type": "TASK", "confidence": 0.8677635292212168}]}, {"text": "However, no work is done for comparing different corpora in the polarity classification task.", "labels": [], "entities": [{"text": "polarity classification task", "start_pos": 64, "end_pos": 92, "type": "TASK", "confidence": 0.8459024826685587}]}, {"text": "Nowadays, Twitter has aggregated huge amount of data that are full of people's sentiments.", "labels": [], "entities": []}, {"text": "In this paper, we empirically evaluate the performance of different corpora in sentiment similarity measurement, which is the fundamental task for word polarity classification.", "labels": [], "entities": [{"text": "sentiment similarity measurement", "start_pos": 79, "end_pos": 111, "type": "TASK", "confidence": 0.8425034880638123}, {"text": "word polarity classification", "start_pos": 147, "end_pos": 175, "type": "TASK", "confidence": 0.8019696672757467}]}, {"text": "Experiment results show that the Twitter data can achieve a much better performance than the Google, Web1T and Wikipedia based methods.", "labels": [], "entities": []}], "introductionContent": [{"text": "Measuring semantic similarity for words and short texts has long been a fundamental problem for many applications such as word sense disambiguation, query expansion, search advertising and soon.", "labels": [], "entities": [{"text": "word sense disambiguation", "start_pos": 122, "end_pos": 147, "type": "TASK", "confidence": 0.690432051817576}, {"text": "query expansion", "start_pos": 149, "end_pos": 164, "type": "TASK", "confidence": 0.7528603672981262}]}, {"text": "Determining the word's polarity plays a critical role in opinion mining and sentiment analysis task.", "labels": [], "entities": [{"text": "opinion mining", "start_pos": 57, "end_pos": 71, "type": "TASK", "confidence": 0.8458068370819092}, {"text": "sentiment analysis", "start_pos": 76, "end_pos": 94, "type": "TASK", "confidence": 0.9569374620914459}]}, {"text": "Usually we can detect the word's polarity by measuring it's semantic similarity with a positive seed word se p and a negative seed word se n respectively, as shown in Formula (1): SO(w) = sim(w, se p ) \u2212 sim(w, se n ) where sim(w i , w j ) is the semantic similarity measurement method for the given word w i and w j . A lot of papers have been published for designing appropriate similarity measurements.", "labels": [], "entities": []}, {"text": "One direction is to learn similarity from the knowledge base or concept taxonomy).", "labels": [], "entities": []}, {"text": "Another direction is to learn semantic similarity with the help of large corpus such as Web or Wikipedia data (.", "labels": [], "entities": []}, {"text": "The basic assumption of this kind of methods is that the word with similar semantic meanings often co-occur in the given corpus.", "labels": [], "entities": []}, {"text": "Extensive experiments have validated the effectiveness of the corpusbased method in polarity classification task.", "labels": [], "entities": [{"text": "polarity classification task", "start_pos": 84, "end_pos": 112, "type": "TASK", "confidence": 0.8778138160705566}]}, {"text": "For example, PMI is a well-known similarity measurement, which makes use of the whole Web as the corpus, and utilizes the search engine hits number to estimate the co-occurrence probability of the give word pairs.", "labels": [], "entities": []}, {"text": "The PMI based method has achieved promising results.", "labels": [], "entities": []}, {"text": "However, according to Kanayama's investigation, only 60% co-occurrences in the same window in Web pages reflect the same sentiment orientation).", "labels": [], "entities": []}, {"text": "Therefore, we may ask the question whether the choosing of corpus can change the performance of sim and is there any better corpus than the Web page data for measuring the sentiment similarity?", "labels": [], "entities": [{"text": "Web page data", "start_pos": 140, "end_pos": 153, "type": "DATASET", "confidence": 0.7296014229456583}]}, {"text": "Everyday, enormous numbers of tweets that contain people's rich sentiments are published in Twitter.", "labels": [], "entities": []}, {"text": "The Twitter maybe a good source for measuring the sentiment similarity.", "labels": [], "entities": []}, {"text": "Compared with the Web page data, the tweets have a higher rate of subjective text posts.", "labels": [], "entities": [{"text": "Web page data", "start_pos": 18, "end_pos": 31, "type": "DATASET", "confidence": 0.8330259919166565}]}, {"text": "The length limitation can guarantee the polarity consistency of each tweet.", "labels": [], "entities": []}, {"text": "Moreover, the tweets contain graphical emoticons, which can be considered as natural sentiment labels for the corresponding tweets in Twitter.", "labels": [], "entities": []}, {"text": "In this paper, we attempt to empirically evaluate the performance of different corpora in sentiment similarity measurement task.", "labels": [], "entities": [{"text": "sentiment similarity measurement", "start_pos": 90, "end_pos": 122, "type": "TASK", "confidence": 0.8658225933710734}]}, {"text": "As far as we know, no work is done on this topic.", "labels": [], "entities": []}], "datasetContent": [{"text": "The Twitter corpus corresponds to the 476 million Twitter tweets (Yang and Leskovec, 2011), which includes over 476 million Twitter posts from 20 million users, covering a 7 month period from June 1, 2009 to December 31, 2009.", "labels": [], "entities": []}, {"text": "We filter out the non-English tweets and the spam tweets that have only few words with URLs.", "labels": [], "entities": []}, {"text": "The tweets that contain three or more trending topics are also removed.", "labels": [], "entities": []}, {"text": "Finally, we construct the Twitter corpus that consists of 266.8 million English tweets.", "labels": [], "entities": []}, {"text": "For calculating page counts in Web data, the candidate words were launched to Google from February 2013 to April 2013.", "labels": [], "entities": []}, {"text": "We also conduct the experiments on the Google Web 1T data that consists of Google n-gram counts (frequency of occurrence of each n-gram) for 1 \u2264 n \u2264 5 ().", "labels": [], "entities": [{"text": "Google Web 1T data", "start_pos": 39, "end_pos": 57, "type": "DATASET", "confidence": 0.8970793634653091}]}, {"text": "The Web 1T data provides a nice approximation to the word co-occurrence statistics in Web pages in a predefined window size (1 \u2264 n \u2264 5).", "labels": [], "entities": [{"text": "Web 1T data", "start_pos": 4, "end_pos": 15, "type": "DATASET", "confidence": 0.7895199060440063}]}, {"text": "For example, the 5 gram Web1T data means the cooccurrence window size is 5.", "labels": [], "entities": [{"text": "Web1T data", "start_pos": 24, "end_pos": 34, "type": "DATASET", "confidence": 0.9536305665969849}]}, {"text": "The English Wikipedia dump 1 we used was extracted at the end of March 2013, which contained more than 13 million articles.", "labels": [], "entities": [{"text": "English Wikipedia dump 1", "start_pos": 4, "end_pos": 28, "type": "DATASET", "confidence": 0.9176143705844879}]}, {"text": "We extracted the plain texts of the Wikipedia data as the training corpus for the Formula (6).", "labels": [], "entities": [{"text": "Wikipedia data", "start_pos": 36, "end_pos": 50, "type": "DATASET", "confidence": 0.889114499092102}]}, {"text": "Two well-know sentiment lexicons are utilized as gold standard for polarity classification task.", "labels": [], "entities": [{"text": "polarity classification", "start_pos": 67, "end_pos": 90, "type": "TASK", "confidence": 0.8508245646953583}]}, {"text": "The statistics of Liu's sentiment lexicon () and MPQA subjectivity lexicon () are shown in.", "labels": [], "entities": [{"text": "MPQA subjectivity lexicon", "start_pos": 49, "end_pos": 74, "type": "DATASET", "confidence": 0.8214339017868042}]}, {"text": "For each word win the lexicons, we employ the Formula (6) to calculate the word's polarity using different corpora.", "labels": [], "entities": []}, {"text": "If SO(w) > 0, the word w is classified into the positive category.", "labels": [], "entities": []}, {"text": "Otherwise if SO(w) < 0, it is classified into the negative category.", "labels": [], "entities": []}, {"text": "The accuracy of the classification result is used to measure the quality of the corpus.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.9994487166404724}]}, {"text": "Firstly, we chose the seed words excellent and poor as Turney's (2002) settings.", "labels": [], "entities": [{"text": "Turney's (2002)", "start_pos": 55, "end_pos": 70, "type": "DATASET", "confidence": 0.8819885373115539}]}, {"text": "The polarity classification accuracies are shown in.", "labels": [], "entities": []}, {"text": "In, Google, Web1T, Wikipedia, Twitter represent the corpora that used in the experiment; CJ, CD, CP, GD represent the Formula (2) to Formula (5) respectively.", "labels": [], "entities": []}, {"text": "We can see from the that the Twitter based method can achieve the best performance.", "labels": [], "entities": []}, {"text": "The rich sentiment information and: Polarity classification accuracies using excellent and poor as seed words natural window size (140 characters) have a positive impact on determining the word's polarity.", "labels": [], "entities": []}, {"text": "The Google based method gets a lower accuracy, this maybe due to the length of Web documents which cannot usually guarantee the semantic consistency in the returned data.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 37, "end_pos": 45, "type": "METRIC", "confidence": 0.9991281628608704}]}, {"text": "Even though two words appear in one page (returned by Google), they might not be semantically related.", "labels": [], "entities": []}, {"text": "Furthermore, the Google based method is time-consuming, because we have to periodically send queries in order to avoid being blocked by Google.", "labels": [], "entities": []}, {"text": "The Web1T based method gets a much worse accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 41, "end_pos": 49, "type": "METRIC", "confidence": 0.9986469149589539}]}, {"text": "After detailed analysis, we find that although the small window size (4 or 5) can guarantee the semantic consistency, the short length also brings in lower co-occurrence probability.", "labels": [], "entities": [{"text": "consistency", "start_pos": 105, "end_pos": 116, "type": "METRIC", "confidence": 0.9079557657241821}]}, {"text": "Statistics show that about 38% SO values are zero when using Web1T corpus.", "labels": [], "entities": [{"text": "SO", "start_pos": 31, "end_pos": 33, "type": "METRIC", "confidence": 0.8563540577888489}, {"text": "Web1T corpus", "start_pos": 61, "end_pos": 73, "type": "DATASET", "confidence": 0.9781455993652344}]}, {"text": "Due to the short length, the Twitter data also suffers from the low co-occurrence problem.", "labels": [], "entities": [{"text": "Twitter data", "start_pos": 29, "end_pos": 41, "type": "DATASET", "confidence": 0.7921858429908752}]}, {"text": "To tackle the low co-occurrence problem, the seed word sets are selected as settings.", "labels": [], "entities": []}, {"text": "The positive word set PS={good, nice, excellent, positive, fortunate, correct, superior} and negative word set NS = {bad, nasty, poor, negative, unfortunate, wrong, inferior} for the Formula (6).", "labels": [], "entities": [{"text": "NS", "start_pos": 111, "end_pos": 113, "type": "METRIC", "confidence": 0.5530109405517578}]}, {"text": "These seed words have been verified to be effective in Turney's paper for polarity classification.", "labels": [], "entities": [{"text": "polarity classification", "start_pos": 74, "end_pos": 97, "type": "TASK", "confidence": 0.9448487758636475}]}, {"text": "The experiment results are shown in. shows that the performance of Twitter corpus is much improved since the multiple seed words alleviate the problem of low co-occurrence probability in tweets.", "labels": [], "entities": []}, {"text": "Generally, when using the seed word groups the Twitter can achieve a much better performance than all the other corpora.", "labels": [], "entities": []}, {"text": "The improvements are statistically significant (p-value < 0.05).", "labels": [], "entities": []}, {"text": "We further add the emoticons ':)' and ':(' into the seed word groups, denoted by Twitter + in.", "labels": [], "entities": []}, {"text": "The emoticons are natural sentiment labels.", "labels": [], "entities": []}, {"text": "We can see that the performances are further improved by considering emoticons as seed words.", "labels": [], "entities": []}, {"text": "The above experiment results have validated the effectiveness of Twitter data as a better corpus for measuring the sentiment similarity.", "labels": [], "entities": []}, {"text": "The results also reveal the potential usefulness of Twitter corpus in semantic similarity measurement.", "labels": [], "entities": [{"text": "semantic similarity measurement", "start_pos": 70, "end_pos": 101, "type": "TASK", "confidence": 0.8330902457237244}]}], "tableCaptions": [{"text": " Table 1. For  each word w in the lexicons, we employ the Formu- la (6) to calculate the word's polarity using different  corpora. If SO(w) > 0, the word w is classified in- to the positive category. Otherwise if SO(w) < 0, it  is classified into the negative category. The accura- cy of the classification result is used to measure the  quality of the corpus.", "labels": [], "entities": [{"text": "accura- cy", "start_pos": 274, "end_pos": 284, "type": "METRIC", "confidence": 0.9527442852656046}]}, {"text": " Table 2: Polarity classification accuracies using excellent  and poor as seed words", "labels": [], "entities": [{"text": "Polarity classification", "start_pos": 10, "end_pos": 33, "type": "TASK", "confidence": 0.7742217183113098}, {"text": "accuracies", "start_pos": 34, "end_pos": 44, "type": "METRIC", "confidence": 0.8388184309005737}]}, {"text": " Table 3: Polarity classification accuracies using the seed  word groups", "labels": [], "entities": [{"text": "Polarity classification", "start_pos": 10, "end_pos": 33, "type": "TASK", "confidence": 0.8026415407657623}]}]}