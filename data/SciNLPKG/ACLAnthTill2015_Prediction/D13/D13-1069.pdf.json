{"title": [{"text": "Automatically Determining a Proper Length for Multi-document Summarization: A Bayesian Nonparametric Approach", "labels": [], "entities": [{"text": "Multi-document Summarization", "start_pos": 46, "end_pos": 74, "type": "TASK", "confidence": 0.6591015458106995}]}], "abstractContent": [{"text": "Document summarization is an important task in the area of natural language processing, which aims to extract the most important information from a single document or a cluster of documents.", "labels": [], "entities": [{"text": "Document summarization", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.9139961004257202}, {"text": "natural language processing", "start_pos": 59, "end_pos": 86, "type": "TASK", "confidence": 0.6808789372444153}]}, {"text": "In various summarization tasks, the summary length is manually defined.", "labels": [], "entities": [{"text": "summarization tasks", "start_pos": 11, "end_pos": 30, "type": "TASK", "confidence": 0.918183833360672}, {"text": "summary length", "start_pos": 36, "end_pos": 50, "type": "METRIC", "confidence": 0.7455166578292847}]}, {"text": "However, how to find the proper summary length is quite a problem; and keeping all summaries restricted to the same length is not always a good choice.", "labels": [], "entities": []}, {"text": "It is obviously improper to generate summaries with the same length for two clusters of documents which contain quite different quantity of information.", "labels": [], "entities": []}, {"text": "In this paper, we propose a Bayesian nonparametric model for multi-document summarization in order to automatically determine the proper lengths of summaries.", "labels": [], "entities": []}, {"text": "Assuming that an original document can be reconstructed from its summary, we describe the \"reconstruction\" by a Bayesian framework which selects sentences to form a good summary.", "labels": [], "entities": []}, {"text": "Experimental results on DUC2004 data sets and some expanded data demonstrate the good quality of our summaries and the rationality of the length determination .", "labels": [], "entities": [{"text": "DUC2004 data sets", "start_pos": 24, "end_pos": 41, "type": "DATASET", "confidence": 0.9850346446037292}]}], "introductionContent": [{"text": "Text summarization is the process of generating a short version of a given text to indicate its main topics.", "labels": [], "entities": [{"text": "Text summarization", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.7424007058143616}]}, {"text": "As the number of documents on the web exponentially increases, text summarization has attracted increasing attention, because it can help people get the most important information within a short time.", "labels": [], "entities": [{"text": "text summarization", "start_pos": 63, "end_pos": 81, "type": "TASK", "confidence": 0.7600708305835724}]}, {"text": "In most of the existing summarization systems, people need to first define a constant length to restrict all the output summaries.", "labels": [], "entities": [{"text": "summarization", "start_pos": 24, "end_pos": 37, "type": "TASK", "confidence": 0.9848398566246033}]}, {"text": "However, in many cases it is improper to require all summaries are of the same length.", "labels": [], "entities": []}, {"text": "Take the multi-document summarization as an example, generating the summaries of the same length fora 5-document cluster and a 50-document cluster is intuitively improper.", "labels": [], "entities": []}, {"text": "More specifically, consider two different clusters of documents: one cluster contains very similar articles which all focus on the same event at the same time; the other contains different steps of the event but each step has its own topics.", "labels": [], "entities": []}, {"text": "The former cluster may need only one or two sentences to explain its information, while the latter needs to include more.", "labels": [], "entities": []}, {"text": "Research on summary length dates back in the late 90s.) studied the characteristics of a good summary (single-document summarization for news) and showed an empirical distribution of summary length over document size.", "labels": [], "entities": []}, {"text": "However, the length problem has been gradually ignored later, since researchers need to fix the length so as to estimate different summarization models conveniently.", "labels": [], "entities": [{"text": "length", "start_pos": 13, "end_pos": 19, "type": "METRIC", "confidence": 0.9769505262374878}, {"text": "summarization", "start_pos": 131, "end_pos": 144, "type": "TASK", "confidence": 0.9680531024932861}]}, {"text": "A typical instance is the Document Understanding Conferences (DUC) 1 , which provide authoritative evaluation for summarization systems.", "labels": [], "entities": [{"text": "Document Understanding Conferences (DUC)", "start_pos": 26, "end_pos": 66, "type": "TASK", "confidence": 0.7910463809967041}, {"text": "summarization", "start_pos": 114, "end_pos": 127, "type": "TASK", "confidence": 0.9792502522468567}]}, {"text": "The DUC conferences collect news aritcles as the input data and define various summarization tasks, such as generic multi-document summarization, query-focused summarization and update summarization.", "labels": [], "entities": [{"text": "DUC conferences collect news aritcles", "start_pos": 4, "end_pos": 41, "type": "DATASET", "confidence": 0.9260204315185547}, {"text": "multi-document summarization", "start_pos": 116, "end_pos": 144, "type": "TASK", "confidence": 0.5405761748552322}, {"text": "update summarization", "start_pos": 178, "end_pos": 198, "type": "TASK", "confidence": 0.6218667030334473}]}, {"text": "In all the DUC tasks, the output is restricted within a length.", "labels": [], "entities": [{"text": "DUC tasks", "start_pos": 11, "end_pos": 20, "type": "TASK", "confidence": 0.7107928097248077}]}, {"text": "Then human-generated summaries are provided to evaluate the results of different summarization systems.", "labels": [], "entities": []}, {"text": "Limiting the length of summaries contributed a lotto the development of summarization techniques, but as we discussed before, in many cases keeping the summaries of the same size is not a good choice.", "labels": [], "entities": [{"text": "summaries", "start_pos": 23, "end_pos": 32, "type": "TASK", "confidence": 0.9460721611976624}, {"text": "summarization", "start_pos": 72, "end_pos": 85, "type": "TASK", "confidence": 0.9898219108581543}]}, {"text": "Moreover, even in constant-length summarization, how to define a proper size of summaries for the summarization tasks is quite a problem.", "labels": [], "entities": [{"text": "summarization", "start_pos": 34, "end_pos": 47, "type": "TASK", "confidence": 0.9352995157241821}, {"text": "summarization tasks", "start_pos": 98, "end_pos": 117, "type": "TASK", "confidence": 0.8951413035392761}]}, {"text": "Why does DUC2007 main task require 250 words while Update task require 100 words?", "labels": [], "entities": []}, {"text": "A short summary may sacrifice the coverage, while along summary may cause redundance.", "labels": [], "entities": [{"text": "coverage", "start_pos": 34, "end_pos": 42, "type": "METRIC", "confidence": 0.9543830752372742}]}, {"text": "Automatically determining the best size of summaries according to the input documents is valuable, and it may deepen our understanding of summarization.", "labels": [], "entities": [{"text": "summarization", "start_pos": 138, "end_pos": 151, "type": "TASK", "confidence": 0.979500949382782}]}, {"text": "In this work, we aim to find the proper length for document summarization automatically and generate varying-length summaries based on the document itself.", "labels": [], "entities": [{"text": "document summarization", "start_pos": 51, "end_pos": 73, "type": "TASK", "confidence": 0.539910614490509}]}, {"text": "The varying-length summarization is more robust for unbalanced clusters.", "labels": [], "entities": []}, {"text": "It can also provide a recommended size as the predefined summary length for general constant-length summarization systems.", "labels": [], "entities": []}, {"text": "We advance a Bayesian nonparametric model of extractive multi-document summarization to achieve this goal.", "labels": [], "entities": [{"text": "extractive multi-document summarization", "start_pos": 45, "end_pos": 84, "type": "TASK", "confidence": 0.574661652247111}]}, {"text": "As far as we are concerned, it is the first model that can learn appropriate lengths of summaries.", "labels": [], "entities": []}, {"text": "Bayesian nonparametric (BNP) methods are powerful tools to determine the size of latent variables (Gershman and Blei, 2011).", "labels": [], "entities": []}, {"text": "They let the data \"speak for itself\" and allow the dimension of latent variables to grow with the data.", "labels": [], "entities": []}, {"text": "In order to integrate the BNP methods into document summarization, we follow the assumption that the original documents should be recovered from the reconstruction of summaries ().", "labels": [], "entities": [{"text": "document summarization", "start_pos": 43, "end_pos": 65, "type": "TASK", "confidence": 0.49998265504837036}]}, {"text": "We use the Beta process as a prior to generate binary vectors for selecting active sentences that reconstruct the original documents.", "labels": [], "entities": []}, {"text": "Then we construct a Bayesian framework for summarization and use the variational approximation for inference.", "labels": [], "entities": [{"text": "summarization", "start_pos": 43, "end_pos": 56, "type": "TASK", "confidence": 0.9890758991241455}]}, {"text": "Experimental results on DUC2004 dataset demonstrate the effectiveness of our model.", "labels": [], "entities": [{"text": "DUC2004 dataset", "start_pos": 24, "end_pos": 39, "type": "DATASET", "confidence": 0.9775202870368958}]}, {"text": "Besides, we reorganize the original documents to generate some new datasets, and examine how the summary length changes on the new data.", "labels": [], "entities": []}, {"text": "The results prove that our summary length determination is rational and necessary on unbalanced data.", "labels": [], "entities": [{"text": "summary length determination", "start_pos": 27, "end_pos": 55, "type": "METRIC", "confidence": 0.6056519250075022}]}], "datasetContent": [{"text": "To test the capability of our BNP summarization systems, we design a series of experiments.", "labels": [], "entities": [{"text": "BNP summarization", "start_pos": 30, "end_pos": 47, "type": "TASK", "confidence": 0.6503278613090515}]}, {"text": "The aim of the experiments mainly includes three aspects: 1.", "labels": [], "entities": []}, {"text": "To demonstrate the summaries extracted by our model have good qualities and the summary length determined by the model is reasonable.", "labels": [], "entities": [{"text": "summary length", "start_pos": 80, "end_pos": 94, "type": "METRIC", "confidence": 0.7597403228282928}]}, {"text": "2. To give examples where varying summary length is necessary.", "labels": [], "entities": []}, {"text": "3. To observe the distribution of summary length.", "labels": [], "entities": []}, {"text": "We evaluate the performance on the dataset of DUC2004 task2.", "labels": [], "entities": [{"text": "DUC2004", "start_pos": 46, "end_pos": 53, "type": "DATASET", "confidence": 0.88129061460495}]}, {"text": "The data contains 50 document clusters, with 10 news articles in each cluster.", "labels": [], "entities": []}, {"text": "Besides, we construct three new datasets from the DUC2004 dataset to further prove the advantage of variable-length summarization.", "labels": [], "entities": [{"text": "DUC2004 dataset", "start_pos": 50, "end_pos": 65, "type": "DATASET", "confidence": 0.9865494072437286}]}, {"text": "We separate each cluster in the original dataset into two parts where each has 5 documents, hence getting the Separate Dataset; Then we randomly combine two original clusters in the DUC2004 dataset, and get two datasets called Combined1 and Combined2.", "labels": [], "entities": [{"text": "Separate Dataset", "start_pos": 110, "end_pos": 126, "type": "DATASET", "confidence": 0.7250603139400482}, {"text": "DUC2004 dataset", "start_pos": 182, "end_pos": 197, "type": "DATASET", "confidence": 0.9823147356510162}, {"text": "Combined2", "start_pos": 241, "end_pos": 250, "type": "DATASET", "confidence": 0.7166555523872375}]}, {"text": "Thus each of the clusters in the combined datasets include 20 documents with two different themes.", "labels": [], "entities": []}, {"text": "First, we implement our BNP summarization model on the DUC2004 dataset, with summary length not limited.", "labels": [], "entities": [{"text": "BNP summarization", "start_pos": 24, "end_pos": 41, "type": "TASK", "confidence": 0.6643728315830231}, {"text": "DUC2004 dataset", "start_pos": 55, "end_pos": 70, "type": "DATASET", "confidence": 0.9884571433067322}, {"text": "summary length", "start_pos": 77, "end_pos": 91, "type": "METRIC", "confidence": 0.7609183490276337}]}, {"text": "At the topic analysis step, we use the HDP model and follow the inference in ().", "labels": [], "entities": [{"text": "topic analysis", "start_pos": 7, "end_pos": 21, "type": "TASK", "confidence": 0.8727818131446838}]}, {"text": "For the sentence selection step, we use the variational inference described in Section 4, where the parameters in the beta process (5) are set as \u03b3 = 1, \u03b1 = 1.", "labels": [], "entities": [{"text": "sentence selection", "start_pos": 8, "end_pos": 26, "type": "TASK", "confidence": 0.8231552541255951}]}, {"text": "The summaries that we finally generate have an average length of 164 words.", "labels": [], "entities": []}, {"text": "We design several popular unsupervised summarization systems and compare them with our model.", "labels": [], "entities": []}, {"text": "\u2022 The Random model selects sentences randomly for each document cluster.", "labels": [], "entities": []}, {"text": "\u2022 The MMR strives to reduce redundancy while maintaining relevance.", "labels": [], "entities": [{"text": "MMR", "start_pos": 6, "end_pos": 9, "type": "TASK", "confidence": 0.9661524295806885}]}, {"text": "For generic summarization, we replace the query relevance with the relevance to documents.", "labels": [], "entities": [{"text": "generic summarization", "start_pos": 4, "end_pos": 25, "type": "TASK", "confidence": 0.8797525763511658}]}, {"text": "\u2022 The Lexrank model () is a graph-based method which choose sentences based on the concept of eigenvector centrality.", "labels": [], "entities": []}, {"text": "\u2022 The Linear Representation model has the same assumption as ours and it can be seen as an approximation of the constant-length version of our model.", "labels": [], "entities": []}, {"text": "All the compared systems are implemented at different predefined lengths from 50 to 300 words.", "labels": [], "entities": []}, {"text": "Then we evaluate the summaries with ROUGE 4 tools () in terms of the f-measure scores of Rouge-1 Rouge-2, and Rouge-L. The metric of Rouge f-measure takes into consideration the summary length in evaluation, so it is proper for our experiments. and, we can see that the result of BNP summarization (the dashed line) gets the second best value among all systems.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 36, "end_pos": 41, "type": "METRIC", "confidence": 0.9210292100906372}, {"text": "BNP summarization", "start_pos": 280, "end_pos": 297, "type": "TASK", "confidence": 0.5872147083282471}]}, {"text": "It is only defeated by the Linear model but the result is comparable to the best in and; while it exceeds other systems at all lengths.", "labels": [], "entities": []}, {"text": "This proves the good qualities of our BNP summaries.", "labels": [], "entities": [{"text": "BNP summaries", "start_pos": 38, "end_pos": 51, "type": "DATASET", "confidence": 0.9567372500896454}]}, {"text": "The reason that the Linear system gets a little better result maybe its weights for linear combination of summary sentences are guaranteed nonnegative while in our model the weights are zeromean Gaussian variables.", "labels": [], "entities": []}, {"text": "This may lead to less redundance in sentence selection for the Linear Representation model.", "labels": [], "entities": [{"text": "sentence selection", "start_pos": 36, "end_pos": 54, "type": "TASK", "confidence": 0.7251990735530853}]}, {"text": "Turn to the length determination.", "labels": [], "entities": [{"text": "length", "start_pos": 12, "end_pos": 18, "type": "METRIC", "confidence": 0.9849938750267029}]}, {"text": "We take advantage of the Linear Representation model to approximate the constant-length version of our model.", "labels": [], "entities": []}, {"text": "Comparing the summaries generated at different predefined lengths, shows the the model gets the best performance (Rouge values) at the length around 164 words, the length learned by our BNP model.", "labels": [], "entities": []}, {"text": "This result partly demonstrates our length determination is rational and it can be used as the recommended length for some constant-length summarization systems, such as the Linear .  The Rouge evaluation requires golden standard summaries as the base.", "labels": [], "entities": []}, {"text": "However, in many cases we cannot get the reference summaries.", "labels": [], "entities": []}, {"text": "For example, when we implement experiments on our expanded datasets (the separate and combined clusters of documents), we do not have exact reference summaries.", "labels": [], "entities": []}, {"text": "advanced an automatic summary evaluation without human models.", "labels": [], "entities": [{"text": "summary evaluation", "start_pos": 22, "end_pos": 40, "type": "TASK", "confidence": 0.8150312304496765}]}, {"text": "They used the Jensen-Shannon divergence(JSD) between the input documents and the summaries as a feature, and got high correlation with human evaluations and the rouge metric.", "labels": [], "entities": [{"text": "divergence(JSD)", "start_pos": 29, "end_pos": 44, "type": "METRIC", "confidence": 0.5654099658131599}]}, {"text": "Unfortunately, it was designed for comparison at a constant-length, which cannot meet our needs.", "labels": [], "entities": []}, {"text": "To extend the JSD evaluation to compare varying-length summaries, we propose anew measure based on information theory, the ratedistortion). is a measure of the cost of representing the symbol x to anew symbo\u00ee x; and the rate can indicate how much compression can be achieved.", "labels": [], "entities": [{"text": "JSD", "start_pos": 14, "end_pos": 17, "type": "TASK", "confidence": 0.8829987049102783}]}, {"text": "The problem of finding the minimum rate can be solved by minimizing the functional where I(X; \u02c6 X) denotes the mutual information.", "labels": [], "entities": []}, {"text": "The rate-distortion theory is a fundamental theory for lossy data compression.", "labels": [], "entities": [{"text": "lossy data compression", "start_pos": 55, "end_pos": 77, "type": "TASK", "confidence": 0.6983221371968588}]}, {"text": "Recently, it has also been successfully employed for text clustering) and document summarization.", "labels": [], "entities": [{"text": "text clustering)", "start_pos": 53, "end_pos": 69, "type": "TASK", "confidence": 0.8429242769877116}, {"text": "document summarization", "start_pos": 74, "end_pos": 96, "type": "TASK", "confidence": 0.7205239534378052}]}, {"text": "claims that the mutual information I(X; \u02c6 X) measures the compactness of the new representation.", "labels": [], "entities": []}, {"text": "Thus the ratedistortion function is a trade-off between the compactness of new representation and the expected distortion.", "labels": [], "entities": []}, {"text": "Specifically in summarization, the summaries can be seen as the new representation\u02c6Xrepresentation\u02c6 representation\u02c6X of original documents X.", "labels": [], "entities": [{"text": "summarization", "start_pos": 16, "end_pos": 29, "type": "TASK", "confidence": 0.9821124076843262}]}, {"text": "A good summary balances the compression ratio and the information loss, thus minimizing the function.", "labels": [], "entities": [{"text": "compression ratio", "start_pos": 28, "end_pos": 45, "type": "METRIC", "confidence": 0.9627707004547119}]}, {"text": "So we use the function (20)(we set \u03b2 = 1) to compare which summary is a better compression.", "labels": [], "entities": []}, {"text": "The JS-divergence (JSD), which has been proved to have high correlation with manual evaluation) for constant-length summary evaluation, is utilized as the distortion in the function.", "labels": [], "entities": [{"text": "JS-divergence (JSD)", "start_pos": 4, "end_pos": 23, "type": "METRIC", "confidence": 0.7766414433717728}]}, {"text": "In the following sections, we simply call the values of the function rate-dist.", "labels": [], "entities": []}, {"text": "In fact, the rate-dist values can be seen as the JSD measure with length regularization.", "labels": [], "entities": [{"text": "JSD", "start_pos": 49, "end_pos": 52, "type": "METRIC", "confidence": 0.5182813405990601}]}, {"text": "To check the effectiveness of rate-dist measure, we evaluate all summaries generated in Section 5.1 with the new measure (the lower the better).", "labels": [], "entities": []}, {"text": "shows that the results accord with the ones in and.", "labels": [], "entities": []}, {"text": "Moreover, in, the curve of ratedist values has a inverse tendency of Rouge measures (Rouge-1, Rouge-2, Rouge-L and Rouge-SU4 are all listed here), and the best performance also occurs around the summary length of 164 words.", "labels": [], "entities": []}, {"text": "This even more clearly reveals that the BNP summarization achieves a perfect tradeoff between compactness and informativeness.", "labels": [], "entities": [{"text": "BNP summarization", "start_pos": 40, "end_pos": 57, "type": "DATASET", "confidence": 0.7714755237102509}]}, {"text": "Due to the accordance with rouge measures, it is promising to be regarded as an alternative to the rouge measures in case we do not have reference summaries.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Average summary length (number of words) on  different datasets", "labels": [], "entities": [{"text": "Average summary length (number of words)", "start_pos": 10, "end_pos": 50, "type": "METRIC", "confidence": 0.8477004617452621}]}, {"text": " Table 2: Comparison of summary lengths on Separate  Dataset.", "labels": [], "entities": [{"text": "Separate  Dataset", "start_pos": 43, "end_pos": 60, "type": "DATASET", "confidence": 0.7434147000312805}]}, {"text": " Table 3: Comparison of summary lengths on Combined1  Dataset.", "labels": [], "entities": [{"text": "Combined1  Dataset", "start_pos": 43, "end_pos": 61, "type": "DATASET", "confidence": 0.9926508665084839}]}]}