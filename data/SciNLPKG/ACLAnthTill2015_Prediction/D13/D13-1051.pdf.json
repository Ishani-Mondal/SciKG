{"title": [{"text": "Improving Alignment of System Combination by Using Multi-objective Optimization", "labels": [], "entities": [{"text": "Improving Alignment of System Combination", "start_pos": 0, "end_pos": 41, "type": "TASK", "confidence": 0.9014700531959534}]}], "abstractContent": [{"text": "This paper proposes a multi-objective optimization framework which supports heterogeneous information sources to improve alignment in machine translation system combination techniques.", "labels": [], "entities": [{"text": "machine translation system combination", "start_pos": 134, "end_pos": 172, "type": "TASK", "confidence": 0.7836517989635468}]}, {"text": "In this area, most of techniques usually utilize confusion networks (CN) as their central data structure to compact an exponential number of an potential hypotheses , and because better hypothesis alignment may benefit constructing better quality confusion networks, it is natural to add more useful information to improve alignment results.", "labels": [], "entities": []}, {"text": "However, these information maybe heterogeneous , so the widely-used Viterbi algorithm for searching the best alignment may not apply here.", "labels": [], "entities": []}, {"text": "In the multi-objective optimization framework, each information source is viewed as an independent objective, and anew goal of improving all objectives can be searched by mature algorithms.", "labels": [], "entities": [{"text": "multi-objective optimization", "start_pos": 7, "end_pos": 35, "type": "TASK", "confidence": 0.6866930276155472}]}, {"text": "The solutions from this framework, termed Pareto optimal solutions, are then combined to construct confusion networks.", "labels": [], "entities": []}, {"text": "Experiments on two Chinese-to-English translation datasets show significant improvements, 0.97 and 1.06 BLEU points over a strong Indirected Hidden Markov Model-based (IHMM) system, and 4.75 and 3.53 points over the best single machine translation systems.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 104, "end_pos": 108, "type": "METRIC", "confidence": 0.9989192485809326}]}], "introductionContent": [{"text": "System combination (SC) techniques have the power of boosting translation quality in BLEU by several percent over the best among all input machine translation systems (.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 85, "end_pos": 89, "type": "METRIC", "confidence": 0.9948098659515381}]}, {"text": "A central data structure in the SC is the confusion network, and its quality greatly affects the final performance.", "labels": [], "entities": []}, {"text": "proposed anew hypothesis alignment algorithm for constructing high-quality confusion networks called Indirect Hidden Markov Model (IHMM), which does better in synonym matching compared with the classic translation edit rate (TER) based algorithm ().", "labels": [], "entities": [{"text": "synonym matching", "start_pos": 159, "end_pos": 175, "type": "TASK", "confidence": 0.8010475635528564}]}, {"text": "Now, current state-of-the-art SC systems have been using IHMM or variants in their alignment algorithms more or less ().", "labels": [], "entities": []}, {"text": "Our motivation derives from an observation that in an ideal alignment of a pair of sentences, many-tomany alignments often exist.", "labels": [], "entities": []}, {"text": "For instance, \"be about to\" has the same meaning with \"be on the point of\".", "labels": [], "entities": []}, {"text": "Because Hidden Markov Model based alignment algorithms, e.g. IHMM for system combination, HMM in GIZA++ software for statistical machine translation (SMT), are designed for one-to-many alignment, and running GIZA++ from two directions to gain better performance turns into a standard operation in SMT, therefore we are seeking away to empower IHMM by introducing bi-directional information.", "labels": [], "entities": [{"text": "statistical machine translation (SMT)", "start_pos": 117, "end_pos": 154, "type": "TASK", "confidence": 0.7874170740445455}, {"text": "SMT", "start_pos": 297, "end_pos": 300, "type": "TASK", "confidence": 0.9857094287872314}]}, {"text": "However, it appears to be intractable in an IHMM model to search the optimal solution by simply defining anew goal as a product of probabilities from two directions.", "labels": [], "entities": []}, {"text": "To bypass this problem, adopts a simple and effective variational inference algorithm.", "labels": [], "entities": []}, {"text": "Further, different alignment algorithms capture different information and linguistic phenomena fora pair of sentences, hence more information would be expected to benefit the final alignment.", "labels": [], "entities": []}, {"text": "Liang's method may not be suitable for this expected outcome.", "labels": [], "entities": []}, {"text": "We propose to adopt multi-objective optimization framework to support heterogeneous information sources which may induce difficulties in a conventional search algorithm.", "labels": [], "entities": []}, {"text": "In this framework, there exist a variety of matured multi-objective optimization algorithms, e.g. evolutionary algorithm (), Tabu search, ants colony, and simulated annealing.", "labels": [], "entities": [{"text": "Tabu search", "start_pos": 125, "end_pos": 136, "type": "TASK", "confidence": 0.5192612856626511}]}, {"text": "In this work, we select the multi-objective evolutionary algorithm because of its public open source software (http://www.iitk.ac.in/kangal/codes.shtml).", "labels": [], "entities": []}, {"text": "On the other hand, this framework is also totally unsupervised.", "labels": [], "entities": []}, {"text": "It prevents weights of a linearly combined goal from training even if all information is homogeneous and applicable in a Viterbi search.", "labels": [], "entities": []}, {"text": "This framework views any useful information benefiting alignment as an independent objective, and researchers just need to write short codes for objective definitions.", "labels": [], "entities": []}, {"text": "The search algorithm seeks for potentially better solutions which are no worse than the current solution set.", "labels": [], "entities": []}, {"text": "The output from multiobjective optimization algorithms includes a set of solutions, called Pareto optimal solutions, each one being a many-to-many alignment.", "labels": [], "entities": []}, {"text": "We then combine and normalize them into a unique one-to-one alignment to perform confusion network construction (Section 3.3).", "labels": [], "entities": [{"text": "confusion network construction", "start_pos": 81, "end_pos": 111, "type": "TASK", "confidence": 0.8020768165588379}]}, {"text": "Our work is conducted on the classic pipeline which has three modules, pair-wise hypothesis alignment, confusion network construction, and training.", "labels": [], "entities": [{"text": "pair-wise hypothesis alignment", "start_pos": 71, "end_pos": 101, "type": "TASK", "confidence": 0.7100568016370138}, {"text": "confusion network construction", "start_pos": 103, "end_pos": 133, "type": "TASK", "confidence": 0.7549371719360352}]}, {"text": "Now many work integrates neighboring modules to avoid propagated errors to gain improved performance.", "labels": [], "entities": []}, {"text": "For example,, and  combine the first and the second module, and combine all modules into one directly.", "labels": [], "entities": []}, {"text": "Nevertheless, the classic structure also owns its merits.", "labels": [], "entities": []}, {"text": "Because of the independence between modules, a system is relatively simple to maintain, and improvements on each module might contribute to final performance additively.", "labels": [], "entities": []}, {"text": "Based on our work, lattice-based minimum error rate training (lattice-MERT) and minimum bayes risk training techniques () could be adopted on the third module.", "labels": [], "entities": [{"text": "minimum error rate training", "start_pos": 33, "end_pos": 60, "type": "METRIC", "confidence": 0.7456877827644348}]}, {"text": "And in the second module adopts a different data structure called lattice which could directly use our better many-to-many alignment for construction.", "labels": [], "entities": []}, {"text": "Experiments on the Chinese-to-English task on two datasets use four objectives, IHMM probability (Section 3.2.1), and alignment probability from GIZA++ (Section 3.2.2) from two directions.", "labels": [], "entities": [{"text": "IHMM probability", "start_pos": 80, "end_pos": 96, "type": "METRIC", "confidence": 0.7367016673088074}]}, {"text": "Results show multi-objective optimization framework efficiently integrates different information to gain approximately 1 BLEU point improvement over a strong baseline.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 121, "end_pos": 125, "type": "METRIC", "confidence": 0.9989619255065918}]}], "datasetContent": [{"text": "We evaluate our method in two datasets in the Chinese-to-English task.", "labels": [], "entities": []}, {"text": "In the first one, NIST MT 2002 and 2005 are used for tuning and testing respectively, and in the second, the newswire part of MT 2006 and 2008 are for tuning and testing.", "labels": [], "entities": [{"text": "NIST MT 2002", "start_pos": 18, "end_pos": 30, "type": "DATASET", "confidence": 0.8884151577949524}]}, {"text": "A 5-gram language model is trained on the Xinhua portion of the Gigaword corpus.", "labels": [], "entities": [{"text": "Xinhua portion of the Gigaword corpus", "start_pos": 42, "end_pos": 79, "type": "DATASET", "confidence": 0.7227759013573328}]}, {"text": "We report the casesensitive NIST-BLEU score.", "labels": [], "entities": [{"text": "NIST-BLEU score", "start_pos": 28, "end_pos": 43, "type": "METRIC", "confidence": 0.868634432554245}]}, {"text": "Four single machine translation systems participating in the system combination consist of a BTGbased system using a Max-Entropy based reordering model, a hierarchical phrase-based system, a Moses decoder and a syntax-based system.", "labels": [], "entities": [{"text": "BTGbased", "start_pos": 93, "end_pos": 101, "type": "DATASET", "confidence": 0.8329789042472839}]}, {"text": "10-best unique hypotheses from a single system on the development  and test sets are collected as the input of the system combination.", "labels": [], "entities": []}, {"text": "Our baseline systems are described as follows.", "labels": [], "entities": []}, {"text": "Two main baseline systems are IHMM based and incremental IHMM ( ).", "labels": [], "entities": []}, {"text": "The first system differs from our method just in hypothesis alignment algorithm, and the second combines the first and second module of the system combination pipeline.", "labels": [], "entities": [{"text": "hypothesis alignment", "start_pos": 49, "end_pos": 69, "type": "TASK", "confidence": 0.7701325118541718}]}, {"text": "Because our method utilizes bidirectional information, we also provide another two alternative systems for comparison, which are GIZA++ based alignment and the posterior probability based alignment ().", "labels": [], "entities": [{"text": "GIZA++ based alignment", "start_pos": 129, "end_pos": 151, "type": "METRIC", "confidence": 0.8787481486797333}]}, {"text": "Finally, we also provide an N-best alignment IHMM system, which combines an N-best alignment list to simulate the Pareto optimal solutions in our method.", "labels": [], "entities": []}, {"text": "The method that linearly combines all objectives is not listed as our baseline like () does, because their algorithm finds the best weighted solution in a fixed and small solution set, while in our problem, the solution space is a trellis-style structure consisting of an exponential number of solutions, and no efficient algorithms apply here.", "labels": [], "entities": []}, {"text": "The IHMM based alignment utilizes typical settings ().", "labels": [], "entities": []}, {"text": "The smoothing factor for the surface similarity model, and \u03c1 = 3 the controlling factor for the distortion model, K = 2.", "labels": [], "entities": []}, {"text": "The bilingual probabilistic dictionary is trained in the FBIS corpus which includes about 230k parallel sentence pairs.", "labels": [], "entities": [{"text": "FBIS corpus", "start_pos": 57, "end_pos": 68, "type": "DATASET", "confidence": 0.9489968121051788}]}, {"text": "GIZA++ based system is to run GIZA++ from two directions to align all the hypotheses, and make the intersection using grow-diag-final heuristics (.", "labels": [], "entities": [{"text": "GIZA", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.9131289720535278}]}, {"text": "The many-to-many alignments are normalized with the same method with ours.", "labels": [], "entities": []}, {"text": "Our system employs NSGA-II software to realize the MOEA algorithm.", "labels": [], "entities": [{"text": "MOEA", "start_pos": 51, "end_pos": 55, "type": "METRIC", "confidence": 0.4537270963191986}]}, {"text": "The main parameters, generation number, cross probability and mutation probability, and population size, are empirically set as 100, 0.9, 0.001 and 40, and we examine the influence of difference populations sizes in the full system combination.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: PPBD is a posterior probabilistic-based decod- ing (section 5.3). N-best IHMM simulates the Pareto op- timal solutions in our method (section 5.3). The last five  systems adopt different objective combinations. The im- provement percents in parentheses are compared to the  best single. dH: directed IHMM, rH: reversed IHMM,  dT: directed translation probability, rT: reversed transla- tion probability.  *  *  significance at 0.01 level, and  *  sig- nificance at 0.05 level over the IHMM model.", "labels": [], "entities": []}, {"text": " Table 3: Oracle BLEUs of CNs. GIZA++: invoking  GIZA++ software. TER: minimum translation edit rate.  IHMM: indirect hidden markov model. IncIHMM: in- cremental indirect hidden markov model. MOEA: multi- objective evolution algorithm.", "labels": [], "entities": [{"text": "TER", "start_pos": 66, "end_pos": 69, "type": "METRIC", "confidence": 0.9956380724906921}]}, {"text": " Table 4: Posterior decoding. When threshold \u03b4 are set  to suitable values, simple bi-directional alignment could  overpass the baseline.", "labels": [], "entities": []}, {"text": " Table 5: Big population size consumes more CPU time.  In our experiments, we use a multi-thread technique to  speed up the alignment, and choose 40 as the parameter  to leverage the time and BLEU.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 192, "end_pos": 196, "type": "METRIC", "confidence": 0.9991569519042969}]}]}