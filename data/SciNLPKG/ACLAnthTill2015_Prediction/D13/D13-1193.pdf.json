{"title": [], "abstractContent": [{"text": "Work on authorship attribution has traditionally focused on long texts.", "labels": [], "entities": [{"text": "authorship attribution", "start_pos": 8, "end_pos": 30, "type": "TASK", "confidence": 0.7861729562282562}]}, {"text": "In this work, we tackle the question of whether the author of a very short text can be successfully identified.", "labels": [], "entities": []}, {"text": "We use Twitter as an experimental testbed.", "labels": [], "entities": []}, {"text": "We introduce the concept of an au-thor's unique \"signature\", and show that such signatures are typical of many authors when writing very short texts.", "labels": [], "entities": []}, {"text": "We also present anew authorship attribution feature (\"flexible pat-terns\") and demonstrate a significant improvement over our baselines.", "labels": [], "entities": []}, {"text": "Our results show that the author of a single tweet can be identified with good accuracy in an array of flavors of the authorship attribution task.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 79, "end_pos": 87, "type": "METRIC", "confidence": 0.9982587695121765}]}], "introductionContent": [{"text": "Research in authorship attribution has developed substantially over the last decade.", "labels": [], "entities": [{"text": "authorship attribution", "start_pos": 12, "end_pos": 34, "type": "TASK", "confidence": 0.8852248191833496}]}, {"text": "The vast majority of such research has been dedicated towards finding the author of long texts, ranging from single passages to book chapters.", "labels": [], "entities": []}, {"text": "In recent years, the growing popularity of social media has created special interest, both theoretical and computational, in short texts.", "labels": [], "entities": []}, {"text": "This has led to many recent authorship attribution projects that experimented with web data such as emails (, web forum messages () and blogs ().", "labels": [], "entities": []}, {"text": "This paper addresses the question to what extent the authors of very short texts can be identified.", "labels": [], "entities": []}, {"text": "To answer this question, we experiment with Twitter tweets.", "labels": [], "entities": []}, {"text": "Twitter messages (tweets) are limited to 140 characters.", "labels": [], "entities": []}, {"text": "This restriction imposes major difficulties on authorship attribution systems, since authorship attribution methods that work well on long texts are often not as useful when applied to short texts;).", "labels": [], "entities": [{"text": "authorship attribution", "start_pos": 47, "end_pos": 69, "type": "TASK", "confidence": 0.8478821218013763}]}, {"text": "Nonetheless, tweets are relatively self-contained and have smaller sentence length variance compared to excerpts from longer texts (see Section 3).", "labels": [], "entities": [{"text": "sentence length variance", "start_pos": 67, "end_pos": 91, "type": "METRIC", "confidence": 0.6756018896897634}]}, {"text": "These characteristics make Twitter data appealing as a testbed when focusing on short texts.", "labels": [], "entities": []}, {"text": "Moreover, an authorship attribution system of tweets may have various applications.", "labels": [], "entities": []}, {"text": "Specifically, a range of cybercrimes can be addressed using such a system, including identity fraud and phishing.", "labels": [], "entities": [{"text": "identity fraud", "start_pos": 85, "end_pos": 99, "type": "TASK", "confidence": 0.6903729885816574}]}, {"text": "In this paper, we introduce the concept of ksignatures.", "labels": [], "entities": []}, {"text": "We denote the k-signatures of an author a as the features that appear in at least k% of a's training samples, while not appearing in the training set of any other author.", "labels": [], "entities": []}, {"text": "When k is large, such signatures capture a unique style used by a.", "labels": [], "entities": []}, {"text": "An analysis of our training set reveals that unique k-signatures are typical of many authors.", "labels": [], "entities": []}, {"text": "Moreover, a substantial portion of the tweets in our training set contain at least one such signature.", "labels": [], "entities": []}, {"text": "These findings suggest that a single tweet, although short and sparse, often contains sufficient information for identifying its author.", "labels": [], "entities": []}, {"text": "Our results show that this is indeed the case.", "labels": [], "entities": []}, {"text": "We train an SVM classifier with a set of features that include character n-grams and word n-grams.", "labels": [], "entities": []}, {"text": "We use a rigorous experimental setup, with varying number of authors (values between 50-1,000) and various sizes of the training set, ranging from 50 to 1,000 tweets per author.", "labels": [], "entities": []}, {"text": "In all our experiments, a single tweet is used as test document.", "labels": [], "entities": []}, {"text": "We also use a setting in which the system is allowed to respond don't know in cases of uncertainty.", "labels": [], "entities": []}, {"text": "Applying this option results in higher precision, at the expense of lower recall.", "labels": [], "entities": [{"text": "precision", "start_pos": 39, "end_pos": 48, "type": "METRIC", "confidence": 0.9996398687362671}, {"text": "recall", "start_pos": 74, "end_pos": 80, "type": "METRIC", "confidence": 0.9993249177932739}]}, {"text": "Our results show that the author of a tweet can be successfully identified.", "labels": [], "entities": []}, {"text": "For example, when using a dataset of as many as 1,000 authors with 200 training tweets per author, we are able to obtain 30.3% accuracy (as opposed to a random baseline of only 0.1%).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 127, "end_pos": 135, "type": "METRIC", "confidence": 0.9992570281028748}]}, {"text": "Using a dataset of 50 authors with as few as 50 training tweets per author, we obtain 50.7% accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 92, "end_pos": 100, "type": "METRIC", "confidence": 0.9989110231399536}]}, {"text": "Using a dataset of 50 authors with 1,000 training tweets per author, our results reach as high as 71.2% in the standard classification setting, and exceed 91% accuracy with 60% recall in the don't know setting.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 159, "end_pos": 167, "type": "METRIC", "confidence": 0.9991745352745056}, {"text": "recall", "start_pos": 177, "end_pos": 183, "type": "METRIC", "confidence": 0.9988547563552856}]}, {"text": "We also apply anew set of features, never previously used for this task -flexible patterns.", "labels": [], "entities": []}, {"text": "Flexible patterns essentially capture the context in which function words are used.", "labels": [], "entities": []}, {"text": "The effectiveness of function words as authorship attribution features () suggests using flexible pattern features.", "labels": [], "entities": []}, {"text": "The fact that flexible patterns are learned from plain text in a fully unsupervised manner makes them domain and language independent.", "labels": [], "entities": []}, {"text": "We demonstrate that using flexible patterns gives significant improvement over our baseline system.", "labels": [], "entities": []}, {"text": "Furthermore, using flexible patterns, our system obtains a 6.1% improvement over current state-of-the-art results in authorship attribution on Twitter.", "labels": [], "entities": []}, {"text": "To summarize, the contribution of this paper is threefold.", "labels": [], "entities": []}, {"text": "\u2022 We provide the most extensive research to date on authorship attribution of micro-messages, and show that authors of very short texts can be successfully identified.", "labels": [], "entities": []}, {"text": "\u2022 We introduce the concept of an author's unique k-signature, and demonstrate that such signatures are used by many authors in their writing of micro-messages.", "labels": [], "entities": []}, {"text": "\u2022 We present anew feature for authorship attribution -flexible patterns -and show its significant added value over other methods.", "labels": [], "entities": [{"text": "authorship attribution", "start_pos": 30, "end_pos": 52, "type": "TASK", "confidence": 0.9144739806652069}]}, {"text": "Using this feature, our system obtains a 6.1% improvement over the current state-of-the-art.", "labels": [], "entities": []}, {"text": "The rest of the paper is organized as follows.", "labels": [], "entities": []}, {"text": "Sections 2 and 3 describe our methods and our experimental testbed (Twitter).", "labels": [], "entities": []}, {"text": "Section 4 presents the concept of k-signatures.", "labels": [], "entities": []}, {"text": "Sections 5 and 6 present our experiments and results.", "labels": [], "entities": []}, {"text": "Flexible patterns are presented in Section 7 and related work is presented in Section 8.", "labels": [], "entities": []}], "datasetContent": [{"text": "Our main research question in this paper is to determine the extent to which authors of very short texts can be identified.", "labels": [], "entities": []}, {"text": "A major issue in working with short texts is selecting the right dataset.", "labels": [], "entities": []}, {"text": "One approach is breaking longer texts into shorter chunks).", "labels": [], "entities": []}, {"text": "We take a different approach and experiment with micro-messages (specifically, tweets).", "labels": [], "entities": []}, {"text": "Tweets have several properties making them an ideal testbed for authorship attribution of short texts.", "labels": [], "entities": [{"text": "authorship attribution of short texts", "start_pos": 64, "end_pos": 101, "type": "TASK", "confidence": 0.8352782487869262}]}, {"text": "First, tweets are posted as single units and do not necessarily refer to each other.", "labels": [], "entities": []}, {"text": "As a result, they tend to be self contained.", "labels": [], "entities": []}, {"text": "Second, tweets have more standardized length distribution compared to other types of web data.", "labels": [], "entities": [{"text": "standardized length distribution", "start_pos": 25, "end_pos": 57, "type": "METRIC", "confidence": 0.8196317354838053}]}, {"text": "We compared the mean and standard deviation of sentence length in our Twitter dataset and in a corpus of English web data.", "labels": [], "entities": [{"text": "Twitter dataset", "start_pos": 70, "end_pos": 85, "type": "DATASET", "confidence": 0.7273632884025574}, {"text": "English web data", "start_pos": 105, "end_pos": 121, "type": "DATASET", "confidence": 0.7253717482089996}]}, {"text": "We found that (a) tweets are shorter than standard web data2 words compared to 20.9), and (b) the standard deviation of the length of tweets is much smaller (6.4 vs. 21.4).", "labels": [], "entities": []}, {"text": "We use a Twitter corpus that includes approximately 5 \u00d7 10 8 tweets.", "labels": [], "entities": []}, {"text": "All nonEnglish tweets and tweets that contain fewer than 3 words are removed from the dataset.", "labels": [], "entities": []}, {"text": "We also remove tweets marked as retweets (using the RT sign, a standard Twitter symbol to indicate that this tweet was written by a different user).", "labels": [], "entities": [{"text": "RT sign", "start_pos": 52, "end_pos": 59, "type": "DATASET", "confidence": 0.627643495798111}]}, {"text": "As some users retweet without using the RT sign, we also remove tweets that are an exact copy of an existing tweet posted in the previous seven days.", "labels": [], "entities": [{"text": "RT sign", "start_pos": 40, "end_pos": 47, "type": "DATASET", "confidence": 0.6624958515167236}]}, {"text": "Apart from plain text, some tweets contain references to other Twitter users (in the format of @<user>).", "labels": [], "entities": []}, {"text": "Since using reference information makes this task substantially easier (, we replace each user reference with the special meta tag REF.", "labels": [], "entities": [{"text": "REF", "start_pos": 131, "end_pos": 134, "type": "METRIC", "confidence": 0.7593094706535339}]}, {"text": "For sparsity reasons, we also replace web addresses with the meta tag URL, num-2 In practice, 0.05 or 0.1 are selected in almost all cases.", "labels": [], "entities": []}, {"text": "bers with the meta tag NUM, time of day with the meta tag TIME and dates with the meta tag DATE.", "labels": [], "entities": [{"text": "TIME", "start_pos": 58, "end_pos": 62, "type": "METRIC", "confidence": 0.9261831045150757}, {"text": "DATE", "start_pos": 91, "end_pos": 95, "type": "METRIC", "confidence": 0.8296492695808411}]}, {"text": "We report of three different experimental configurations.", "labels": [], "entities": []}, {"text": "In the experiments described below, each dataset is divided into training and test sets using ten-fold cross validation.", "labels": [], "entities": []}, {"text": "On the test phase, each document contains a single tweet.", "labels": [], "entities": []}, {"text": "Experimenting with varying Training Set Sizes.", "labels": [], "entities": []}, {"text": "In order to test the affect of the training set size, we experiment with an increasingly larger number of tweets per author.", "labels": [], "entities": []}, {"text": "Experimenting with a range of training set sizes serves two purposes: (a) to check whether the author of a tweet can be identified using a very small number of (short) training samples, and (b) check how much our system can benefit from training on a larger corpus.", "labels": [], "entities": []}, {"text": "In our experiments we only consider users who posted between 1,000-2,000 tweets 6 (a total of 6 This range is selected since on one hand we want at least 1,000 tweets per author for our experiments, and on the other hand we noticed that users with a larger number of tweets in corpus tend to be spammers or bots that are very easy to identify, so we limit this number to 2,000.", "labels": [], "entities": []}, {"text": "10,183 users), and randomly select 1,000 tweets per user.", "labels": [], "entities": []}, {"text": "From these users, we select 10 groups of 50 users each.", "labels": [], "entities": []}, {"text": "We perform a set of classification experiments, selecting for each author an increasingly larger subset of her 1,000 tweets as training set.", "labels": [], "entities": []}, {"text": "Subset sizes are (50, 100, 200, 500, 1,000).", "labels": [], "entities": []}, {"text": "Threshold values for our features in each setting (see Section 2) are (2, 2, 4, 10, 20) fort cng and (2, 2, 2, 3, 5) fort wng , respectively.", "labels": [], "entities": []}, {"text": "Experimenting with varying Numbers of Authors.", "labels": [], "entities": []}, {"text": "Ina second set of experiments, we use an increasingly larger number of authors (values between 100-1,000), in order to check whether the author of a very short text can be identified in a \"needle in a haystack\" type of setting.", "labels": [], "entities": []}, {"text": "Due to complexity issues, we only experiment with 200 tweets per author as training set.", "labels": [], "entities": []}, {"text": "We select groups of size 100, 200, 500 and 1,000 users (one group per size).", "labels": [], "entities": []}, {"text": "We use the same threshold values as the 200 tweets per author setting previously described (t cng = 4, t wng = 2).", "labels": [], "entities": []}, {"text": "Another aspect of our research question is the level of certainty our system has when suggesting an author fora given tweet.", "labels": [], "entities": []}, {"text": "In cases of uncertainty, many real life applications would prefer not to get any response instead of getting a response with low certainty.", "labels": [], "entities": []}, {"text": "Moreover, in real life applications we are often not even sure that the real author is part of our training set.", "labels": [], "entities": []}, {"text": "Consequently, we allow our system to respond \"don't know\" in cases of low confidence ().", "labels": [], "entities": []}, {"text": "This allows our system to obtain higher precision, at the expense of lower recall.", "labels": [], "entities": [{"text": "precision", "start_pos": 40, "end_pos": 49, "type": "METRIC", "confidence": 0.9993166923522949}, {"text": "recall", "start_pos": 75, "end_pos": 81, "type": "METRIC", "confidence": 0.9992628693580627}]}, {"text": "To implement this feature, we use SVM's probability estimates, as implemented in libsvm.", "labels": [], "entities": []}, {"text": "These estimates give a score to each potential author.", "labels": [], "entities": []}, {"text": "These scores reflect the probability that this author is the correct author, as decided by the prediction model.", "labels": [], "entities": []}, {"text": "The selected author is always the one with the highest probability estimate.", "labels": [], "entities": []}, {"text": "As selection criterion, we use a set of increasingly larger thresholds (0.05-0.9) for the probability of the selected author.", "labels": [], "entities": []}, {"text": "This means that we do not select test samples for which the selected author has a probability estimate value lower than the threshold.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Examples of tweets published by very structured users, suspected to be bots, along with one of their 20%- signatures.", "labels": [], "entities": []}]}