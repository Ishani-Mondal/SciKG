{"title": [{"text": "Modeling Scientific Impact with Topical Influence Regression", "labels": [], "entities": [{"text": "Modeling Scientific Impact", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.847830573717753}]}], "abstractContent": [{"text": "When reviewing scientific literature, it would be useful to have automatic tools that identify the most influential scientific articles as well as how ideas propagate between articles.", "labels": [], "entities": []}, {"text": "In this context, this paper introduces topical influence, a quantitative measure of the extent to which an article tends to spread its topics to the articles that cite it.", "labels": [], "entities": []}, {"text": "Given the text of the articles and their citation graph, we show how to learn a probabilistic model to recover both the degree of topical influence of each article and the influence relationships between articles.", "labels": [], "entities": []}, {"text": "Experimental results on corpora from two well-known computer science conferences are used to illustrate and validate the proposed approach.", "labels": [], "entities": []}], "introductionContent": [{"text": "Scientific articles are not created equal.", "labels": [], "entities": []}, {"text": "Some articles generate entire disciplines or sub-disciplines of research, or revolutionize how we think about a problem, while others contribute relatively little.", "labels": [], "entities": []}, {"text": "When we are first introduced to anew area of scientific study, it would be useful to automatically find the most important articles, and the relationships of influence between articles.", "labels": [], "entities": []}, {"text": "Understanding the impact of scientific work is also crucial for hiring decisions, allocation of funding, university rankings and other tasks that involve the assessment of scientific merit.", "labels": [], "entities": []}, {"text": "If scientific works stand on the shoulders of giants, we would like to be able to find the giants.", "labels": [], "entities": []}, {"text": "The importance of a scientific work has previously been measured chiefly through metrics derived from citation counts, such as impact factors.", "labels": [], "entities": []}, {"text": "However, citation counts are not the whole story.", "labels": [], "entities": [{"text": "citation counts", "start_pos": 9, "end_pos": 24, "type": "METRIC", "confidence": 0.9578316807746887}]}, {"text": "Many citations are made in passing, are relevant to only one section of an article, or make no impact on a work but are referenced out of \"politeness, policy or piety\".", "labels": [], "entities": []}, {"text": "In reality, scientific impact has many dimensions.", "labels": [], "entities": []}, {"text": "Some articles are important because they describe scientific discoveries that alter our understanding of the world, while some develop essential tools and techniques which facilitate future research.", "labels": [], "entities": []}, {"text": "Other articles are influential because they introduce the seeds of new ideas, which in turn inspire many other articles.", "labels": [], "entities": []}, {"text": "In this work we introduce topical influence, a quantitative metric for measuring the latter type of scientific influence, defined in the context of an unsupervised generative model for scientific corpora.", "labels": [], "entities": []}, {"text": "The model posits that articles \"coerce\" the articles that cite them into having similar topical content to them.", "labels": [], "entities": []}, {"text": "Thus, articles with higher topical influence have a larger effect on the topics of the articles that cite them.", "labels": [], "entities": []}, {"text": "We model this influence mechanism via a regression on the parameters of the Dirichlet prior over topics in an LDA-style topic model.", "labels": [], "entities": []}, {"text": "We show how the models can be used to recover meaningful influence scores, both for articles and for specific citations.", "labels": [], "entities": []}, {"text": "By looking not just at the citation graph but also taking into account the content of the articles, topical influence can provide a better picture of scientific impact than simple citation counts.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section we experimentally investigate the properties of TIR and TIRE.", "labels": [], "entities": [{"text": "TIR", "start_pos": 64, "end_pos": 67, "type": "METRIC", "confidence": 0.9181131720542908}, {"text": "TIRE", "start_pos": 72, "end_pos": 76, "type": "METRIC", "confidence": 0.844586193561554}]}, {"text": "We consider two scientific corpora: a collection of 3286 of articles from the Association for Computational Linguistics (ACL) conference 2 () published between 1987 and 2011, and a corpus of articles from the Neural Information Processing Systems (NIPS) conference 3 containing 1740 articles from 1987 to 1999.", "labels": [], "entities": []}, {"text": "The corpora both contained a small number (53, and 14, respectively) of citation graph loops due to insider knowledge of simultaneous publications.", "labels": [], "entities": []}, {"text": "Some loops were removed by manual deletion of \"insider knowledge\" edges, and others were removed by deleting edges in the loop uniformly at random.", "labels": [], "entities": []}, {"text": "For computational efficiency, we performed approximate Gibbs updates where we drop the multiplicative Polya likelihood terms in Equation 4.", "labels": [], "entities": [{"text": "Polya likelihood", "start_pos": 102, "end_pos": 118, "type": "METRIC", "confidence": 0.7794160842895508}]}, {"text": "This corresponds to only transmitting influence information downward in the citation DAG, but not transmitting \"reverse influence\" information upwards.", "labels": [], "entities": [{"text": "citation DAG", "start_pos": 76, "end_pos": 88, "type": "DATASET", "confidence": 0.6571647822856903}]}, {"text": "Preliminary experiments on synthetic data indicated that this did not significantly impact the ability of the model to recover the topical influence weights.", "labels": [], "entities": []}, {"text": "As one might expect, LDA is already capable of inferring topic distributions which are good enough to perform the regression on, without fully exploiting the additional feedback from the regression.", "labels": [], "entities": []}, {"text": "This algorithm has a similar running time to the standard collapsed Gibbs sampler for LDA, as the regression step is not a bottleneck.", "labels": [], "entities": []}, {"text": "In all experiments, we set the hyper-parameters to \u03b1 = 0.1, \u03b2 = 0.1 and the \u03c3 parameter for the truncated Gaussian in TIRE to be 1.", "labels": [], "entities": [{"text": "TIRE", "start_pos": 118, "end_pos": 122, "type": "METRIC", "confidence": 0.5235258340835571}]}, {"text": "We interleaved regression steps every 10 Gibbs iterations.", "labels": [], "entities": []}, {"text": "For exploratory data analysis experiments the models were trained for 500 burn-in iterations, and the samples from the final iterations were used for the analysis.", "labels": [], "entities": []}, {"text": "We also used a document prediction task to explore whether the posited latent structure is predictively useful.", "labels": [], "entities": [{"text": "document prediction task", "start_pos": 15, "end_pos": 39, "type": "TASK", "confidence": 0.7855633993943533}]}, {"text": "We selected roughly 10% of the articles in each corpus (170 and 330 documents for NIPS and ACL, respectively) for testing, chosen among the articles that made at least one citation.", "labels": [], "entities": [{"text": "NIPS", "start_pos": 82, "end_pos": 86, "type": "DATASET", "confidence": 0.8937473297119141}]}, {"text": "We held out a randomly selected set of 50% of their words and evaluated the log probability of the held out partial documents under each model.", "labels": [], "entities": []}, {"text": "This is equivalent to evaluating on a set of new documents with the same set of references as the held outset.", "labels": [], "entities": []}, {"text": "Evaluation was performed using annealed importance sampling), as in except we used multiple samples per likelihood computation.", "labels": [], "entities": []}, {"text": "The TIR models were compared to LDA and an \"additive\" version of DMR with link function \u03b1 where the \u03bbs were constrained to be positive and given an exponential prior with mean one.", "labels": [], "entities": [{"text": "DMR", "start_pos": 65, "end_pos": 68, "type": "DATASET", "confidence": 0.8701732754707336}]}, {"text": "For DMR, binary feature vectors encoded the presence or absence of each possible citation.", "labels": [], "entities": [{"text": "DMR", "start_pos": 4, "end_pos": 7, "type": "TASK", "confidence": 0.95908522605896}]}, {"text": "For each algorithm, we burned in for 250 iterations, then executed 1000 iterations, optimizing topical influence weights/DMR parameters every 10th iteration.", "labels": [], "entities": []}, {"text": "Held-out log probability scores were computed by performing AIS with every 100th sample, and averaging the results to estimate the posterior predictive probability P r(held out article|training set, citations, model).", "labels": [], "entities": [{"text": "posterior predictive probability P r", "start_pos": 131, "end_pos": 167, "type": "METRIC", "confidence": 0.7516318559646606}]}, {"text": "It was found that all of the regression methods had superior predictive performance to LDA on these corpora, demonstrating that topical influence has predictive value.", "labels": [], "entities": []}, {"text": "Although DMR performed slightly better than TIR predictively, TIR was competitive despite the fact that it has a factor of K less regression parameters.", "labels": [], "entities": [{"text": "TIR", "start_pos": 62, "end_pos": 65, "type": "METRIC", "confidence": 0.43266940116882324}]}, {"text": "Note that DMR does not provide an interpretable notion of influence.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Wins, losses and average improvement for log probabilities of held-out articles, versus LDA. Each \"Win\"  corresponds to the model assigning a higher log probability score for the test portion of a held-out document than LDA  assigned to that document.", "labels": [], "entities": [{"text": "LDA", "start_pos": 98, "end_pos": 101, "type": "METRIC", "confidence": 0.9550088047981262}]}, {"text": " Table 2: Most influential articles in the ACL Conference corpus, according to citation counts (top), topical influence  l (d) inferred by TIR (middle), and total topical influence T (d) inferred by TIR (bottom). For total topical influence,  the breakdown of T (d) = l (d) \u00d7 citation count is shown in parentheses.", "labels": [], "entities": [{"text": "ACL Conference corpus", "start_pos": 43, "end_pos": 64, "type": "DATASET", "confidence": 0.9033340414365133}, {"text": "topical influence  l (d) inferred", "start_pos": 102, "end_pos": 135, "type": "METRIC", "confidence": 0.897226767880576}, {"text": "TIR", "start_pos": 139, "end_pos": 142, "type": "METRIC", "confidence": 0.8037818074226379}]}, {"text": " Table 3: Most influential articles in the NIPS corpus, according to citation counts (top), topical influence l (d) inferred  by TIR (middle), and total topical influence T (d) inferred by TIR (bottom).", "labels": [], "entities": [{"text": "NIPS corpus", "start_pos": 43, "end_pos": 54, "type": "DATASET", "confidence": 0.9324812889099121}, {"text": "topical influence l (d) inferred", "start_pos": 92, "end_pos": 124, "type": "METRIC", "confidence": 0.8980956843921116}, {"text": "TIR", "start_pos": 129, "end_pos": 132, "type": "METRIC", "confidence": 0.8742605447769165}, {"text": "total topical influence T (d)", "start_pos": 147, "end_pos": 176, "type": "METRIC", "confidence": 0.731080881186894}, {"text": "TIR", "start_pos": 189, "end_pos": 192, "type": "METRIC", "confidence": 0.7506236433982849}]}]}