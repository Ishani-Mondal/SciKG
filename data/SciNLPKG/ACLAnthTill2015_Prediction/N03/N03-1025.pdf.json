{"title": [{"text": "Language and Task Independent Text Categorization with Simple Language Models", "labels": [], "entities": []}], "abstractContent": [{"text": "We present a simple method for language independent and task independent text categoriza-tion learning, based on character-level n-gram language models.", "labels": [], "entities": [{"text": "text categoriza-tion learning", "start_pos": 73, "end_pos": 102, "type": "TASK", "confidence": 0.6916364828745524}]}, {"text": "Our approach uses simple information theoretic principles and achieves effective performance across a variety of languages and tasks without requiring feature selection or extensive pre-processing.", "labels": [], "entities": []}, {"text": "To demonstrate the language and task independence of the proposed technique, we present experimental results on several languages-Greek, En-glish, Chinese and Japanese-in several text categorization problems-language identification , authorship attribution, text genre classification , and topic detection.", "labels": [], "entities": [{"text": "authorship attribution", "start_pos": 234, "end_pos": 256, "type": "TASK", "confidence": 0.7624124884605408}, {"text": "text genre classification", "start_pos": 258, "end_pos": 283, "type": "TASK", "confidence": 0.8006299138069153}, {"text": "topic detection", "start_pos": 290, "end_pos": 305, "type": "TASK", "confidence": 0.8592361807823181}]}, {"text": "Our experimental results show that the simple approach achieves state of the art performance in each case.", "labels": [], "entities": []}], "introductionContent": [{"text": "Text categorization concerns the problem of automatically assigning given text passages (paragraphs or documents) into predefined categories.", "labels": [], "entities": [{"text": "Text categorization", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.8112559616565704}, {"text": "assigning given text passages (paragraphs or documents) into predefined categories", "start_pos": 58, "end_pos": 140, "type": "TASK", "confidence": 0.6615785111983618}]}, {"text": "Due to the rapid explosion of texts in digital form, text categorization has become an important area of research owing to the need to automatically organize and index large text collections in various ways.", "labels": [], "entities": [{"text": "text categorization", "start_pos": 53, "end_pos": 72, "type": "TASK", "confidence": 0.7840857803821564}]}, {"text": "Such techniques are currently being applied in many areas, including language identification, authorship attribution (), text genre classification (;), topic identification (, and subjective sentiment classification).", "labels": [], "entities": [{"text": "language identification", "start_pos": 69, "end_pos": 92, "type": "TASK", "confidence": 0.7605831623077393}, {"text": "authorship attribution", "start_pos": 94, "end_pos": 116, "type": "TASK", "confidence": 0.7285901308059692}, {"text": "text genre classification", "start_pos": 121, "end_pos": 146, "type": "TASK", "confidence": 0.81751549243927}, {"text": "topic identification", "start_pos": 152, "end_pos": 172, "type": "TASK", "confidence": 0.9301020801067352}, {"text": "subjective sentiment classification", "start_pos": 180, "end_pos": 215, "type": "TASK", "confidence": 0.6758527159690857}]}, {"text": "Many standard machine learning techniques have been applied to automated text categorization problems, such as naive-Bayes classifiers, support vector machines, linear least squares models, neural networks, and K-nearest neighbor classifiers.", "labels": [], "entities": []}, {"text": "A common aspect of these approaches is that they treat text categorization as a standard classification problem, and thereby reduce the learning process to two simple steps: feature engineering, and classification learning over the feature space.", "labels": [], "entities": [{"text": "feature engineering", "start_pos": 174, "end_pos": 193, "type": "TASK", "confidence": 0.73790243268013}, {"text": "classification learning", "start_pos": 199, "end_pos": 222, "type": "TASK", "confidence": 0.9354648292064667}]}, {"text": "Of these two steps, feature engineering is critical to achieving good performance in text categorization problems.", "labels": [], "entities": [{"text": "feature engineering", "start_pos": 20, "end_pos": 39, "type": "TASK", "confidence": 0.8304297924041748}]}, {"text": "Once good features are identified, almost any reasonable technique for learning a classifier seems to perform well).", "labels": [], "entities": []}, {"text": "Unfortunately, the standard classification learning methodology has several drawbacks for text categorization.", "labels": [], "entities": [{"text": "text categorization", "start_pos": 90, "end_pos": 109, "type": "TASK", "confidence": 0.8012523949146271}]}, {"text": "First, feature construction is usually language dependent.", "labels": [], "entities": [{"text": "feature construction", "start_pos": 7, "end_pos": 27, "type": "TASK", "confidence": 0.7629024386405945}]}, {"text": "Various techniques such as stop-word removal or stemming require language specific knowledge to design adequately.", "labels": [], "entities": [{"text": "stop-word removal", "start_pos": 27, "end_pos": 44, "type": "TASK", "confidence": 0.785963386297226}]}, {"text": "Moreover, whether one can use a purely word-level approach is itself a language dependent issue.", "labels": [], "entities": []}, {"text": "In many Asian languages such as Chinese or Japanese, identifying words from character sequences is hard, and any word-based approach must suffer added complexity in coping with segmentation errors.", "labels": [], "entities": [{"text": "identifying words from character sequences", "start_pos": 53, "end_pos": 95, "type": "TASK", "confidence": 0.8355836391448974}]}, {"text": "Second, feature selection is task dependent.", "labels": [], "entities": [{"text": "feature selection", "start_pos": 8, "end_pos": 25, "type": "TASK", "confidence": 0.7583497762680054}]}, {"text": "For example, tasks like authorship attribution or genre classification require attention to linguistic style markers (), whereas topic detection systems rely more heavily on bag of words features.", "labels": [], "entities": [{"text": "authorship attribution", "start_pos": 24, "end_pos": 46, "type": "TASK", "confidence": 0.78197380900383}, {"text": "genre classification", "start_pos": 50, "end_pos": 70, "type": "TASK", "confidence": 0.7049395889043808}, {"text": "topic detection", "start_pos": 129, "end_pos": 144, "type": "TASK", "confidence": 0.8226649761199951}]}, {"text": "Third, there are an enormous number of possible features to consider in text categorization problems, and standard feature selection approaches do not always cope well in such circumstances.", "labels": [], "entities": [{"text": "text categorization", "start_pos": 72, "end_pos": 91, "type": "TASK", "confidence": 0.7817249000072479}]}, {"text": "For example, given an enormous number of features, the cumulative effect of uncommon features can still have an important effect on classification accuracy, even though infrequent features contribute less information than common features individually.", "labels": [], "entities": [{"text": "classification", "start_pos": 132, "end_pos": 146, "type": "TASK", "confidence": 0.9732836484909058}, {"text": "accuracy", "start_pos": 147, "end_pos": 155, "type": "METRIC", "confidence": 0.9543704390525818}]}, {"text": "Consequently, throwing away uncommon features is usually not an appropriate strategy in this domain).", "labels": [], "entities": []}, {"text": "Another problem is that feature selection normally uses indirect tests, such as \u03c7 2 or mutual information, which involve setting arbi-trary thresholds and conducting a heuristic greedy search to find good feature sets.", "labels": [], "entities": [{"text": "feature selection", "start_pos": 24, "end_pos": 41, "type": "TASK", "confidence": 0.8000321686267853}]}, {"text": "Finally, by treating text categorization as a classical classification problem, standard approaches can ignore the fact that texts are written in natural language, meaning that they have many implicit regularities that can be well modeled with specific tools from natural language processing.", "labels": [], "entities": [{"text": "text categorization", "start_pos": 21, "end_pos": 40, "type": "TASK", "confidence": 0.7358390390872955}]}, {"text": "In this paper, we propose a straightforward text categorization learning method based on learning categoryspecific, character-level, n-gram language models.", "labels": [], "entities": []}, {"text": "Although this is a very simple approach, it has not yet been systematically investigated in the literature.", "labels": [], "entities": []}, {"text": "We find that, surprisingly, we obtain competitive (and often superior) results to more sophisticated learning and feature construction techniques, while requiring almost no feature engineering or pre-processing.", "labels": [], "entities": [{"text": "feature construction", "start_pos": 114, "end_pos": 134, "type": "TASK", "confidence": 0.7096138298511505}]}, {"text": "In fact, the overall approach requires almost no language specific or task specific pre-processing to achieve effective performance.", "labels": [], "entities": []}, {"text": "The success of this simple method, we think, is due to the effectiveness of well known statistical language modeling techniques, which surprisingly have had little significant impact on the learning algorithms normally applied to text categorization.", "labels": [], "entities": [{"text": "statistical language modeling", "start_pos": 87, "end_pos": 116, "type": "TASK", "confidence": 0.6698212226231893}, {"text": "text categorization", "start_pos": 230, "end_pos": 249, "type": "TASK", "confidence": 0.7283981442451477}]}, {"text": "Nevertheless, statistical language modeling is also concerned with modeling the semantic, syntactic, lexicographical and phonological regularities of natural language-and would seem to provide a natural foundation for text categorization problems.", "labels": [], "entities": [{"text": "statistical language modeling", "start_pos": 14, "end_pos": 43, "type": "TASK", "confidence": 0.7104816834131876}]}, {"text": "One interesting difference, however, is that instead of explicitly pre-computing features and selecting a subset based on arbitrary decisions, the language modeling approach simply considers all character (or word) subsequences occurring in the text as candidate features, and implicitly considers the contribution of every feature in the final model.", "labels": [], "entities": []}, {"text": "Thus, the language modeling approach completely avoids a potentially error-prone feature selection process.", "labels": [], "entities": [{"text": "language modeling", "start_pos": 10, "end_pos": 27, "type": "TASK", "confidence": 0.7350858449935913}]}, {"text": "Also, by applying character-level language models, one also avoids the word segmentation problems that arise in many Asian languages, and thereby achieves a language independent method for constructing accurate text categorizers.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 71, "end_pos": 88, "type": "TASK", "confidence": 0.7106991559267044}]}], "datasetContent": [{"text": "We now proceed to present our results on several text categorization problems on different languages.", "labels": [], "entities": []}, {"text": "Specifically, we consider language identification, Greek authorship attribution, Greek genre classification, English topic detection, Chinese topic detection and Japanese topic detection.", "labels": [], "entities": [{"text": "language identification", "start_pos": 26, "end_pos": 49, "type": "TASK", "confidence": 0.7191779166460037}, {"text": "Greek authorship attribution", "start_pos": 51, "end_pos": 79, "type": "TASK", "confidence": 0.6426187952359518}, {"text": "Greek genre classification", "start_pos": 81, "end_pos": 107, "type": "TASK", "confidence": 0.6604067285855612}, {"text": "English topic detection", "start_pos": 109, "end_pos": 132, "type": "TASK", "confidence": 0.645398736000061}, {"text": "Chinese topic detection", "start_pos": 134, "end_pos": 157, "type": "TASK", "confidence": 0.6918722987174988}, {"text": "Japanese topic detection", "start_pos": 162, "end_pos": 186, "type": "TASK", "confidence": 0.6949862241744995}]}, {"text": "For the sake of consistency with previous research), we measure categorization performance by the overall accuracy, which is the number of correctly identified texts divided by the total number of texts considered.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 106, "end_pos": 114, "type": "METRIC", "confidence": 0.9991464614868164}]}, {"text": "We also measure the performance with Macro Fmeasure, which is the average of the F-measures across all categories.", "labels": [], "entities": [{"text": "F-measures", "start_pos": 81, "end_pos": 91, "type": "METRIC", "confidence": 0.929750382900238}]}, {"text": "F-measure is a combination of precision and recall).", "labels": [], "entities": [{"text": "F-measure", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.9615644812583923}, {"text": "precision", "start_pos": 30, "end_pos": 39, "type": "METRIC", "confidence": 0.9993458390235901}, {"text": "recall", "start_pos": 44, "end_pos": 50, "type": "METRIC", "confidence": 0.997837483882904}]}], "tableCaptions": [{"text": " Table 1: Results on Greek authorship attribution", "labels": [], "entities": [{"text": "Greek authorship attribution", "start_pos": 21, "end_pos": 49, "type": "TASK", "confidence": 0.5782332817713419}]}, {"text": " Table 2: Results on Greek text genre classification", "labels": [], "entities": [{"text": "Greek text genre classification", "start_pos": 21, "end_pos": 52, "type": "TASK", "confidence": 0.7248020097613335}]}, {"text": " Table 3: Topic detection results on English 20 Newsgroup  data", "labels": [], "entities": [{"text": "Topic detection", "start_pos": 10, "end_pos": 25, "type": "TASK", "confidence": 0.9070361256599426}, {"text": "English 20 Newsgroup  data", "start_pos": 37, "end_pos": 63, "type": "DATASET", "confidence": 0.9840366840362549}]}, {"text": " Table 4: Chinese topic detection results", "labels": [], "entities": [{"text": "Chinese topic detection", "start_pos": 10, "end_pos": 33, "type": "TASK", "confidence": 0.7524409095446268}]}, {"text": " Table 5: Japanese topic detection results", "labels": [], "entities": [{"text": "Japanese topic detection", "start_pos": 10, "end_pos": 34, "type": "TASK", "confidence": 0.7238689462343851}]}]}