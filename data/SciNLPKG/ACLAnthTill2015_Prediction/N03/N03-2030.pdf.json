{"title": [{"text": "A Hybrid Approach to Content Analysis for Automatic Essay Grading", "labels": [], "entities": [{"text": "Content Analysis", "start_pos": 21, "end_pos": 37, "type": "TASK", "confidence": 0.7148532718420029}, {"text": "Automatic Essay Grading", "start_pos": 42, "end_pos": 65, "type": "TASK", "confidence": 0.77252596616745}]}], "abstractContent": [{"text": "We present CarmelTC, a novel hybrid text classification approach for automatic essay grading.", "labels": [], "entities": [{"text": "CarmelTC", "start_pos": 11, "end_pos": 19, "type": "DATASET", "confidence": 0.8895960450172424}]}, {"text": "Our evaluation demonstrates that the hybrid CarmelTC approach outperforms two \"bag of words\" approaches, namely LSA and a Naive Bayes, as well as a purely symbolic approach.", "labels": [], "entities": [{"text": "CarmelTC", "start_pos": 44, "end_pos": 52, "type": "DATASET", "confidence": 0.5705021023750305}]}], "introductionContent": [{"text": "In this paper we describe CarmelTC , a novel automatic essay grading approach using a hybrid text classification technique for analyzing essay answers to qualitative physics questions inside the Why2 tutorial dialogue system ().", "labels": [], "entities": [{"text": "Why2 tutorial dialogue system", "start_pos": 195, "end_pos": 224, "type": "DATASET", "confidence": 0.8590628355741501}]}, {"text": "In contrast to many previous approaches to automated essay grading (, our goal is not to assign a letter grade to student essays.", "labels": [], "entities": []}, {"text": "Instead, our purpose is to tally which set of \"correct answer aspects\" are present in student essays.", "labels": [], "entities": []}, {"text": "Previously, tutorial dialogue systems such as AUTO-TUTOR) and Research Methods Tutor () have used LSA ) to perform the same type of content analysis for student essays that we do in Why2.", "labels": [], "entities": [{"text": "AUTO-TUTOR", "start_pos": 46, "end_pos": 56, "type": "DATASET", "confidence": 0.7712256908416748}]}, {"text": "While Bag of Words approaches such as LSA have performed successfully on the content analysis task in domains such as Computer Literacy, they have been demonstrated to perform poorly in causal domains such as research methods () because they base their predictions only on the words included in a text and not on the functional relationships between them.", "labels": [], "entities": [{"text": "Bag of Words", "start_pos": 6, "end_pos": 18, "type": "TASK", "confidence": 0.8565585215886434}, {"text": "content analysis task", "start_pos": 77, "end_pos": 98, "type": "TASK", "confidence": 0.7762356996536255}]}, {"text": "Thus, we propose CarmelTC as an alternative.", "labels": [], "entities": [{"text": "CarmelTC", "start_pos": 17, "end_pos": 25, "type": "DATASET", "confidence": 0.8166792988777161}]}, {"text": "CarmelTC is a rule learning text classification approach that bases its predictions both on features extracted from CARMEL's deep \u00a1 This research was supported by the ONR, Cognitive Science Division under grant number N00014-0-1-0600 and by NSF grant number 9720359 to CIRCLE.", "labels": [], "entities": [{"text": "CarmelTC", "start_pos": 0, "end_pos": 8, "type": "DATASET", "confidence": 0.9206300377845764}, {"text": "rule learning text classification", "start_pos": 14, "end_pos": 47, "type": "TASK", "confidence": 0.7522528916597366}, {"text": "ONR", "start_pos": 167, "end_pos": 170, "type": "DATASET", "confidence": 0.7118990421295166}, {"text": "CIRCLE", "start_pos": 269, "end_pos": 275, "type": "DATASET", "confidence": 0.9035962224006653}]}, {"text": "syntactic functional analyses of texts) and a \"bag of words\" classification of that text obtained from Rainbow Naive Bayes.", "labels": [], "entities": [{"text": "Rainbow Naive Bayes", "start_pos": 103, "end_pos": 122, "type": "DATASET", "confidence": 0.9513538479804993}]}, {"text": "We evaluate CarmelTC in the physics domain, which is a highly causal domain like research methods.", "labels": [], "entities": [{"text": "CarmelTC", "start_pos": 12, "end_pos": 20, "type": "DATASET", "confidence": 0.697981595993042}]}, {"text": "In our evaluation we demonstrate that CarmelTC outperforms both Latent Semantic Analysis (LSA) ( ) and Rainbow Naive Bayes, as well as a purely symbolic approach similar to ().", "labels": [], "entities": [{"text": "CarmelTC", "start_pos": 38, "end_pos": 46, "type": "DATASET", "confidence": 0.5104085803031921}]}, {"text": "Thus, our evaluation demonstrates the advantage of combining predictions from symbolic and \"bag of words\" approaches for content analysis aspects of automatic essay grading.", "labels": [], "entities": []}], "datasetContent": [{"text": "We conducted an evaluation to compare the effectiveness of CarmelTC at analyzing student essays in comparison to LSA, Rainbow, and a purely symbolic approach similar to (), which we refer to here as CarmelTCsymb.", "labels": [], "entities": [{"text": "Rainbow", "start_pos": 118, "end_pos": 125, "type": "DATASET", "confidence": 0.8840581774711609}]}, {"text": "CarmelTCsymb is identical to CarmelTC except that it does not include in its feature set the prediction from Rainbow.", "labels": [], "entities": [{"text": "CarmelTCsymb", "start_pos": 0, "end_pos": 12, "type": "DATASET", "confidence": 0.9102460145950317}, {"text": "CarmelTC", "start_pos": 29, "end_pos": 37, "type": "DATASET", "confidence": 0.9557341933250427}]}, {"text": "We conducted our evaluation over a corpus of 126 previously unseen student essays in response to the Pumpkin Problem described above, with a total of 500 text segments, and just under 6000 words altogether.", "labels": [], "entities": []}, {"text": "Each text segment was hand tagged by at least two coders, and conflicts were resolved at a consensus meeting.", "labels": [], "entities": []}, {"text": "Pairwise Kappas between our three coders computed over initial codings of our data was always above .75.", "labels": [], "entities": [{"text": "Pairwise Kappas", "start_pos": 0, "end_pos": 15, "type": "METRIC", "confidence": 0.8692746758460999}]}, {"text": "The LSA space used for this evaluation was trained over three first year physics text books.", "labels": [], "entities": [{"text": "LSA space", "start_pos": 4, "end_pos": 13, "type": "DATASET", "confidence": 0.8495454490184784}]}, {"text": "The Rainbow models used to generate the Rainbow predictions that are part of the feature set provided to CarmelTC were trained over a development corpus of 248 hand tagged example sentences extracted from a corpus of human-human tutoring dialogues, just like those included in the 126 essays mentioned above.", "labels": [], "entities": [{"text": "CarmelTC", "start_pos": 105, "end_pos": 113, "type": "DATASET", "confidence": 0.8964480757713318}]}, {"text": "However, when we evaluated the performance of Rainbow for comparison with CarmelTC, LSA, and the symbolic approach, we ran a 50 fold cross validation evaluation using the complete set of examples in both sets (i.e., the 248 sentences used to train the Rainbow models used to by CarmelTC as well as the 126 essays) so that Rainbow would have access to the exact same training data as CarmelTC, to make it a fair comparison between alternative machine learning approaches.", "labels": [], "entities": [{"text": "CarmelTC", "start_pos": 74, "end_pos": 82, "type": "DATASET", "confidence": 0.94112628698349}]}, {"text": "On each iteration, we randomly selected a subset of essays such that the number of text segments included in the test set were greater than 10 but less than 15 and then training Rainbow using the remaining text segments.", "labels": [], "entities": []}, {"text": "Thus, CarmelTC uses the same set of training data, but unlike the other approaches, it uses its training data in two separate parts, namely one to train the Rainbow models it uses to produce the Rainbow prediction that is part of the vector representation it builds for each text and one to train the decision trees.", "labels": [], "entities": [{"text": "CarmelTC", "start_pos": 6, "end_pos": 14, "type": "DATASET", "confidence": 0.9221088886260986}]}, {"text": "This is because for CarmelTC, the data for training Rainbow must be separate from that used to train the decision trees so the decision trees are trained from a realistic distribution of assigned Rainbow classes based on its performance on unseen data rather than on  Rainbow's training data.", "labels": [], "entities": [{"text": "CarmelTC", "start_pos": 20, "end_pos": 28, "type": "DATASET", "confidence": 0.8085384964942932}]}, {"text": "Thus, for CarmelTC, we also performed a 50 fold cross validation, but this time only over the set of 126 example essays not used to train the Rainbow models used by CarmelTC.", "labels": [], "entities": [{"text": "CarmelTC", "start_pos": 10, "end_pos": 18, "type": "DATASET", "confidence": 0.9710878133773804}, {"text": "CarmelTC", "start_pos": 165, "end_pos": 173, "type": "DATASET", "confidence": 0.9620993733406067}]}, {"text": "Note that LSA works by using its trained LSA space to construct a vector representation for any text based on the set of words included therein.", "labels": [], "entities": []}, {"text": "It can thus be used for text classification by comparing the vector obtained fora set of exemplar texts for each class with that obtained from the text to be classified.", "labels": [], "entities": [{"text": "text classification", "start_pos": 24, "end_pos": 43, "type": "TASK", "confidence": 0.8031516671180725}]}, {"text": "We tested LSA using as exemplars the same set of examples used as Rainbow training data, but it always performed better when using a small set of hand picked exemplars.", "labels": [], "entities": [{"text": "LSA", "start_pos": 10, "end_pos": 13, "type": "TASK", "confidence": 0.6710413098335266}, {"text": "Rainbow training data", "start_pos": 66, "end_pos": 87, "type": "DATASET", "confidence": 0.7571083903312683}]}, {"text": "Thus, we present results here using only those hand picked exemplars.", "labels": [], "entities": []}, {"text": "For every approach except LSA, we first segmented the essays at sentence boundaries and classified each sentence separately.", "labels": [], "entities": []}, {"text": "However, for LSA, rather than classify each segment separately, we compared the LSA vector for the entire essay to the exemplars for each class (other than \"nothing\"), since LSA's performance is better with longer texts.", "labels": [], "entities": []}, {"text": "We verified that LSA also performed better specifically on our task under these circumstances.", "labels": [], "entities": [{"text": "LSA", "start_pos": 17, "end_pos": 20, "type": "DATASET", "confidence": 0.4473467171192169}]}, {"text": "Thus, we compared each essay to each exemplar, and we counted LSA as identifying the corresponding \"correct answer aspect\" if the cosine value obtained by comparing the two vectors was above a threshold.", "labels": [], "entities": [{"text": "LSA", "start_pos": 62, "end_pos": 65, "type": "METRIC", "confidence": 0.9883698225021362}]}, {"text": "We used a threshold value of .53, which we determined experimentally to achieve the optimal f-score result, using a beta value of 1 in order to treat precision and recall as equally important.", "labels": [], "entities": [{"text": "f-score", "start_pos": 92, "end_pos": 99, "type": "METRIC", "confidence": 0.9636157751083374}, {"text": "precision", "start_pos": 150, "end_pos": 159, "type": "METRIC", "confidence": 0.9990905523300171}, {"text": "recall", "start_pos": 164, "end_pos": 170, "type": "METRIC", "confidence": 0.9980239868164062}]}, {"text": "demonstrates that CarmelTC out performs the other approaches, achieving the highest f-score, which combines the precision and recall scores into a single measure.", "labels": [], "entities": [{"text": "CarmelTC", "start_pos": 18, "end_pos": 26, "type": "DATASET", "confidence": 0.6304550170898438}, {"text": "f-score", "start_pos": 84, "end_pos": 91, "type": "METRIC", "confidence": 0.9737694263458252}, {"text": "precision", "start_pos": 112, "end_pos": 121, "type": "METRIC", "confidence": 0.9991301894187927}, {"text": "recall", "start_pos": 126, "end_pos": 132, "type": "METRIC", "confidence": 0.9846211671829224}]}, {"text": "Thus, it performs better at this task than two commonly used purely \"bag of words\" approaches as well as to an otherwise equivalent purely symbolic approach.", "labels": [], "entities": []}], "tableCaptions": []}