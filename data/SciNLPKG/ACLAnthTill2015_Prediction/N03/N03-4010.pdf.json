{"title": [{"text": "JAVELIN: A Flexible, Planner-Based Architecture for Question Answering", "labels": [], "entities": [{"text": "JAVELIN", "start_pos": 0, "end_pos": 7, "type": "METRIC", "confidence": 0.7314555048942566}, {"text": "Question Answering", "start_pos": 52, "end_pos": 70, "type": "TASK", "confidence": 0.8023260533809662}]}], "abstractContent": [{"text": "The JAVELIN system integrates a flexible, planning-based architecture with a variety of language processing modules to provide an open-domain question answering capability on free text.", "labels": [], "entities": [{"text": "JAVELIN", "start_pos": 4, "end_pos": 11, "type": "DATASET", "confidence": 0.6209098100662231}, {"text": "question answering", "start_pos": 142, "end_pos": 160, "type": "TASK", "confidence": 0.7213642299175262}]}, {"text": "The demonstration will focus on how JAVELIN processes questions and retrieves the most likely answer candidates from the given text corpus.", "labels": [], "entities": [{"text": "JAVELIN processes questions", "start_pos": 36, "end_pos": 63, "type": "TASK", "confidence": 0.6698155403137207}]}, {"text": "The operation of the system will be explained in depth through browsing the repository of data objects created by the system during each question answering session.", "labels": [], "entities": [{"text": "question answering session", "start_pos": 137, "end_pos": 163, "type": "TASK", "confidence": 0.7657779653867086}]}], "introductionContent": [{"text": "Simple factoid questions can now be answered reasonably well using pattern matching.", "labels": [], "entities": []}, {"text": "Some systems) use surface patterns enhanced with semantic categories and question types in order to model the likelihood of answers given the question.", "labels": [], "entities": []}, {"text": "Furthermore,) have obtained good results using only surface patterns pre-extracted from the web.", "labels": [], "entities": []}, {"text": "However, pattern-based approaches don't represent the meaning of the patterns they use, and it is not clear whether they can be generalized for more difficult, non-factoid questions.", "labels": [], "entities": []}, {"text": "Open domain question answering is a complex, multifaceted task, where question type, information availability, user needs, and a combination of text processing techniques (statistical, NLP, etc.) must be combined dynamically to determine the optimal answer.", "labels": [], "entities": [{"text": "Open domain question answering", "start_pos": 0, "end_pos": 30, "type": "TASK", "confidence": 0.7218616902828217}]}, {"text": "For more complex questions, a more flexible and powerful control mechanism is required.", "labels": [], "entities": []}, {"text": "For example, LCC (D.) has implemented feedback loops which ensure that processing constraints are met by retrieving more documents or expanding question terms.", "labels": [], "entities": [{"text": "LCC (D.)", "start_pos": 13, "end_pos": 21, "type": "DATASET", "confidence": 0.8231000304222107}]}, {"text": "The LCC system includes a passage retrieval loop, a lexico-semantic loop and a logic proving loop.", "labels": [], "entities": []}, {"text": "The IBM PIQUANT system) combines knowledge-based agents using predictive annotation with a statistical approach based on a maximum entropy model ().", "labels": [], "entities": []}, {"text": "Both the LCC and IBM systems represent a departure from the standard pipelined approach to QA architecture, and both work well for straightforward factoid questions.", "labels": [], "entities": [{"text": "LCC", "start_pos": 9, "end_pos": 12, "type": "DATASET", "confidence": 0.9048996567726135}]}, {"text": "Nevertheless, both approaches incorporate a pre-determined set of processing steps or strategies, and have limited ability to reason about new types of questions not previously encountered.", "labels": [], "entities": []}, {"text": "Practically useful question answering in non-factoid domains (e.g., intelligence analysis) requires more sophisticated question decomposition, reasoning, and answer synthesis.", "labels": [], "entities": [{"text": "question answering", "start_pos": 19, "end_pos": 37, "type": "TASK", "confidence": 0.7893079817295074}, {"text": "intelligence analysis", "start_pos": 68, "end_pos": 89, "type": "TASK", "confidence": 0.7157387137413025}, {"text": "question decomposition", "start_pos": 119, "end_pos": 141, "type": "TASK", "confidence": 0.7308167666196823}, {"text": "answer synthesis", "start_pos": 158, "end_pos": 174, "type": "TASK", "confidence": 0.8086909055709839}]}, {"text": "For these hard questions, QA architectures must define relationships among entities, gather information from multiple sources, and reason over the data to produce an effective answer.", "labels": [], "entities": []}, {"text": "As QA functionality becomes more sophisticated, the set of decisions made by a system will not be captured by pipelined architectures or multi-pass constraint relaxation, but must be modeled as a step-by-step decision flow, where the set of processing steps is determined at run time for each question.", "labels": [], "entities": []}, {"text": "This demonstration illustrates the JAVELIN QA architecture ( ), which includes a general, modular infrastructure controlled by a step-by-step planning component.", "labels": [], "entities": [{"text": "JAVELIN QA architecture", "start_pos": 35, "end_pos": 58, "type": "DATASET", "confidence": 0.8022316495577494}]}, {"text": "JAVELIN combines analysis modules, information sources, user discourse and answer synthesis as required for each question-answering interaction.", "labels": [], "entities": [{"text": "JAVELIN", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.5856277346611023}, {"text": "answer synthesis", "start_pos": 75, "end_pos": 91, "type": "TASK", "confidence": 0.7089816629886627}]}, {"text": "JAVELIN also incorporates a global memory, or reposEdmonton, itory, which maintains a linked set of object dependencies for each question answering session.", "labels": [], "entities": [{"text": "JAVELIN", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.7636155486106873}, {"text": "question answering session", "start_pos": 129, "end_pos": 155, "type": "TASK", "confidence": 0.7576934893925985}]}, {"text": "The repository can be used to provide a processing summary or answer justification for the user.", "labels": [], "entities": []}, {"text": "The repository also provides a straightforward way to compare the results of different versions of individual processing modules running on the same question.", "labels": [], "entities": []}, {"text": "The modularity and flexibility of the architecture provide a good platform for component-based (glass box) evaluation ).", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": []}