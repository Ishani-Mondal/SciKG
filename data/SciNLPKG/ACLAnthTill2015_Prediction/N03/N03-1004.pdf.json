{"title": [{"text": "In Question Answering, Two Heads Are Better Than One", "labels": [], "entities": [{"text": "Question Answering", "start_pos": 3, "end_pos": 21, "type": "TASK", "confidence": 0.8267918527126312}]}], "abstractContent": [{"text": "Motivated by the success of ensemble methods in machine learning and other areas of natural language processing, we developed a multi-strategy and multi-source approach to question answering which is based on combining the results from different answering agents searching for answers in multiple corpora.", "labels": [], "entities": [{"text": "question answering", "start_pos": 172, "end_pos": 190, "type": "TASK", "confidence": 0.821513295173645}]}, {"text": "The answering agents adopt fundamentally different strategies , one utilizing primarily knowledge-based mechanisms and the other adopting statistical techniques.", "labels": [], "entities": []}, {"text": "We present our multi-level answer resolution algorithm that combines results from the answering agents at the question, passage, and/or answer levels.", "labels": [], "entities": [{"text": "answer resolution", "start_pos": 27, "end_pos": 44, "type": "TASK", "confidence": 0.7267780601978302}]}, {"text": "Experiments evaluating the effectiveness of our answer resolution algorithm show a 35.0% relative improvement over our baseline system in the number of questions correctly answered, and a 32.8% improvement according to the average precision metric.", "labels": [], "entities": [{"text": "answer resolution", "start_pos": 48, "end_pos": 65, "type": "TASK", "confidence": 0.8685627579689026}, {"text": "precision", "start_pos": 231, "end_pos": 240, "type": "METRIC", "confidence": 0.9966177344322205}]}], "introductionContent": [{"text": "Traditional question answering (QA) systems typically employ a pipeline approach, consisting roughly of question analysis, document/passage retrieval, and answer selection (see e.g., ().", "labels": [], "entities": [{"text": "question answering (QA)", "start_pos": 12, "end_pos": 35, "type": "TASK", "confidence": 0.898403549194336}, {"text": "question analysis", "start_pos": 104, "end_pos": 121, "type": "TASK", "confidence": 0.7325249314308167}, {"text": "document/passage retrieval", "start_pos": 123, "end_pos": 149, "type": "TASK", "confidence": 0.5666146203875542}, {"text": "answer selection", "start_pos": 155, "end_pos": 171, "type": "TASK", "confidence": 0.7599051594734192}]}, {"text": "Although atypical QA system classifies questions based on expected answer types, it adopts the same strategy for locating potential answers from the same corpus regardless of the question classification.", "labels": [], "entities": []}, {"text": "In our own earlier work, we developed a specialized mechanism called Virtual Annotation for handling definition questions (e.g., \"Who was Galileo?\" and \"What are antibiotics?\") that consults, in addition to the standard reference corpus, a structured knowledge source (WordNet) for answering such questions ().", "labels": [], "entities": []}, {"text": "We have shown that better performance is achieved by applying Virtual Annotation and our general purpose QA strategy in parallel.", "labels": [], "entities": []}, {"text": "In this paper, we investigate the impact of adopting such a multistrategy and multi-source approach to QA in a more general fashion.", "labels": [], "entities": []}, {"text": "Our approach to question answering is additionally motivated by the success of ensemble methods in machine learning, where multiple classifiers are employed and their results are combined to produce the final output of the ensemble (for an overview, see).", "labels": [], "entities": [{"text": "question answering", "start_pos": 16, "end_pos": 34, "type": "TASK", "confidence": 0.9096957445144653}]}, {"text": "Such ensemble methods have recently been adopted in question answering ().", "labels": [], "entities": [{"text": "question answering", "start_pos": 52, "end_pos": 70, "type": "TASK", "confidence": 0.9166795909404755}]}, {"text": "In our question answering system, PI-QUANT, we utilize in parallel multiple answering agents that adopt different processing strategies and consult different knowledge sources in identifying answers to given questions, and we employ resolution mechanisms to combine the results produced by the individual answering agents.", "labels": [], "entities": [{"text": "question answering", "start_pos": 7, "end_pos": 25, "type": "TASK", "confidence": 0.8550807535648346}]}, {"text": "We call our approach multi-strategy since we combine the results from a number of independent agents implementing different answer finding strategies.", "labels": [], "entities": [{"text": "answer finding", "start_pos": 124, "end_pos": 138, "type": "TASK", "confidence": 0.8552951812744141}]}, {"text": "We also call it multi-source since the different agents can search for answers in multiple knowledge sources.", "labels": [], "entities": []}, {"text": "In this paper, we focus on two answering agents that adopt fundamentally different strategies: one agent uses predominantly knowledge-based mechanisms, whereas the other agent is based on statistical methods.", "labels": [], "entities": []}, {"text": "Our multi-level resolution algorithm enables combination of results from each answering agent at the question, passage, and/or answer levels.", "labels": [], "entities": [{"text": "multi-level resolution", "start_pos": 4, "end_pos": 26, "type": "TASK", "confidence": 0.6616636514663696}]}, {"text": "Our experiments show that inmost cases our multi-level resolution algorithm outperforms its components, supporting a tightly-coupled design for multiagent QA systems.", "labels": [], "entities": [{"text": "multi-level resolution", "start_pos": 43, "end_pos": 65, "type": "TASK", "confidence": 0.6760923564434052}]}, {"text": "Experimental results show significant performance improvement over our single-strategy, single-source baselines, with the best performing multilevel resolution algorithm achieving a 35.0% relative improvement in the number of correct answers and a 32.8% improvement in average precision, on a previously unseen test set.", "labels": [], "entities": [{"text": "precision", "start_pos": 277, "end_pos": 286, "type": "METRIC", "confidence": 0.9464974403381348}]}], "datasetContent": [{"text": "To assess the effectiveness of our multi-level answer resolution algorithm, we devised experiments to evaluate the impact of the question, passage, and answer-level combination algorithms described in the previous section.", "labels": [], "entities": [{"text": "multi-level answer resolution", "start_pos": 35, "end_pos": 64, "type": "TASK", "confidence": 0.6122607489426931}]}, {"text": "The baseline systems are the knowledge-based and statistical agents performing individually against a single reference corpus.", "labels": [], "entities": []}, {"text": "In addition, our earlier experiments showed that when employing a single answer finding strategy, consulting multiple text corpora yielded better performance than using a single corpus.", "labels": [], "entities": [{"text": "answer finding", "start_pos": 73, "end_pos": 87, "type": "TASK", "confidence": 0.7088625878095627}]}, {"text": "We thus configured aversion of our knowledge-based agent to make use of three available text corpora, 6 the AQUAINT corpus (news articles from 1998-2000), the TREC corpus (news articles from 1988-1994), and a subset of the Encyclopedia Britannica.", "labels": [], "entities": [{"text": "AQUAINT corpus", "start_pos": 108, "end_pos": 122, "type": "DATASET", "confidence": 0.7561801075935364}, {"text": "TREC corpus", "start_pos": 159, "end_pos": 170, "type": "DATASET", "confidence": 0.7680339217185974}, {"text": "Encyclopedia Britannica", "start_pos": 223, "end_pos": 246, "type": "DATASET", "confidence": 0.8178008198738098}]}, {"text": "This multi-source version of the knowledge-based agent will be used in all answer resolution experiments in conjunction with the statistical agent.", "labels": [], "entities": [{"text": "answer resolution", "start_pos": 75, "end_pos": 92, "type": "TASK", "confidence": 0.8729749917984009}]}, {"text": "We configured multiple versions of PIQUANT to evaluate our question, passage, and answer-level combination algorithms individually and cumulatively.", "labels": [], "entities": []}, {"text": "For cumulative effects, we 1) combined the algorithms pair-wise, and 2) employed all three algorithms together.", "labels": [], "entities": []}, {"text": "The two test sets were selected from the TREC 10 and 11 QA track questions.", "labels": [], "entities": [{"text": "TREC 10 and 11 QA track questions", "start_pos": 41, "end_pos": 74, "type": "DATASET", "confidence": 0.7901097791535514}]}, {"text": "For both test sets, we eliminated those questions that did not have known answers in the reference corpus.", "labels": [], "entities": []}, {"text": "Furthermore, from the TREC 10 test set, we discarded all definition questions, 8 since the knowledge-based agent adopts a specialized strategy for handling definition questions which greatly reduces potential contributions from other answering agents.", "labels": [], "entities": [{"text": "TREC 10 test set", "start_pos": 22, "end_pos": 38, "type": "DATASET", "confidence": 0.8721901774406433}]}, {"text": "This results in a TREC 10 test set of 313 questions and a TREC 11 test set of 453 questions.", "labels": [], "entities": [{"text": "TREC 10 test set", "start_pos": 18, "end_pos": 34, "type": "DATASET", "confidence": 0.6732674464583397}, {"text": "TREC 11 test set", "start_pos": 58, "end_pos": 74, "type": "DATASET", "confidence": 0.7471508383750916}]}, {"text": "We ran each of the baseline and combined systems on the two test sets.", "labels": [], "entities": []}, {"text": "For each run, the system outputs its top answer and its confidence score for each question.", "labels": [], "entities": [{"text": "confidence score", "start_pos": 56, "end_pos": 72, "type": "METRIC", "confidence": 0.9804790616035461}]}, {"text": "All answers fora run are then sorted in descending order of the confidence scores.", "labels": [], "entities": [{"text": "confidence", "start_pos": 64, "end_pos": 74, "type": "METRIC", "confidence": 0.9689050912857056}]}, {"text": "Two established TREC QA evaluation metrics are adopted to assess the results for each run as follows: 1.", "labels": [], "entities": [{"text": "TREC QA evaluation", "start_pos": 16, "end_pos": 34, "type": "TASK", "confidence": 0.49516244729359943}]}, {"text": "% Correct: Percentage of correct answers.", "labels": [], "entities": [{"text": "Correct", "start_pos": 2, "end_pos": 9, "type": "METRIC", "confidence": 0.997529923915863}]}, {"text": "2. Average Precision: A confidence-weighted score that rewards systems with high confidence incorrect answers as follows, where N is the number of questions: # correct up to question i/i shows our experimental results.", "labels": [], "entities": [{"text": "Average", "start_pos": 3, "end_pos": 10, "type": "METRIC", "confidence": 0.9858922362327576}, {"text": "Precision", "start_pos": 11, "end_pos": 20, "type": "METRIC", "confidence": 0.6394809484481812}]}, {"text": "The top section shows the comparable baseline results from the statistical agent (SA-SS) and the single-source knowledgebased agent (KBA-SS).", "labels": [], "entities": []}, {"text": "It also includes results for the multi-source knowledge-based agent (KBA-MS), which improve upon those for its single-source counterpart (KBA-SS).", "labels": [], "entities": []}, {"text": "The middle section of the table shows the answer resolution results, including applying the question, passage, and answer-level combination algorithms individually (Q, P, and A, respectively), applying them pair-wise (Q+P, P+A, and Q+A), and employing all three algorithms (Q+P+A).", "labels": [], "entities": [{"text": "answer resolution", "start_pos": 42, "end_pos": 59, "type": "TASK", "confidence": 0.8450249135494232}]}, {"text": "Finally, the last row of the table shows the relative improvement by comparing the best performing system configuration (highlighted in boldface) with the better performing single-source, single-strategy baseline system (SA-SS or KBA-SS, in italics).", "labels": [], "entities": []}, {"text": "Overall, PIQUANT's multi-strategy and multi-source approach achieved a 35.0% relative improvement in the: Experimental Results number of correct answers and a 32.8% improvement in average precision on the TREC 11 data set.", "labels": [], "entities": [{"text": "precision", "start_pos": 188, "end_pos": 197, "type": "METRIC", "confidence": 0.9840212464332581}, {"text": "TREC 11 data set", "start_pos": 205, "end_pos": 221, "type": "DATASET", "confidence": 0.9255847334861755}]}, {"text": "Of the combined improvement, approximately half was achieved by the multi-source aspect of PIQUANT, while the other half was obtained by PIQUANT's multi-strategy feature.", "labels": [], "entities": [{"text": "PIQUANT", "start_pos": 91, "end_pos": 98, "type": "DATASET", "confidence": 0.938908576965332}]}, {"text": "Although the absolute average precision values are comparable on both test sets and the absolute percentage of correct answers is lower on the TREC 11 data, the improvement is greater on TREC 11 in both cases.", "labels": [], "entities": [{"text": "precision", "start_pos": 30, "end_pos": 39, "type": "METRIC", "confidence": 0.9262726902961731}, {"text": "TREC 11 data", "start_pos": 143, "end_pos": 155, "type": "DATASET", "confidence": 0.8652773499488831}, {"text": "TREC 11", "start_pos": 187, "end_pos": 194, "type": "DATASET", "confidence": 0.5185685753822327}]}, {"text": "This is because the TREC 10 questions were taken into account for manual rule refinement in the knowledge-based agent, resulting in higher baselines on the TREC 10 test set.", "labels": [], "entities": [{"text": "TREC 10 questions", "start_pos": 20, "end_pos": 37, "type": "DATASET", "confidence": 0.7359360257784525}, {"text": "TREC 10 test set", "start_pos": 156, "end_pos": 172, "type": "DATASET", "confidence": 0.8807405978441238}]}, {"text": "We believe that the larger improvement on the previously unseen TREC 11 data is a more reliable estimate of PIQUANT's performance on future test sets.", "labels": [], "entities": [{"text": "TREC 11 data", "start_pos": 64, "end_pos": 76, "type": "DATASET", "confidence": 0.771227240562439}, {"text": "PIQUANT", "start_pos": 108, "end_pos": 115, "type": "METRIC", "confidence": 0.48211920261383057}]}, {"text": "We applied an earlier version of our combination algorithms, which performed between our current P and P+A algorithms, in our submission to the TREC 11 QA track.", "labels": [], "entities": [{"text": "TREC 11 QA track", "start_pos": 144, "end_pos": 160, "type": "DATASET", "confidence": 0.8440379947423935}]}, {"text": "Using the average precision metric, that version of PI-QUANT was among the top 5 best performing systems out of 67 runs submitted by 34 groups.", "labels": [], "entities": [{"text": "precision", "start_pos": 18, "end_pos": 27, "type": "METRIC", "confidence": 0.9620332717895508}]}], "tableCaptions": [{"text": " Table 2: Passage Retrieval Analysis", "labels": [], "entities": [{"text": "Passage Retrieval Analysis", "start_pos": 10, "end_pos": 36, "type": "TASK", "confidence": 0.9274383187294006}]}, {"text": " Table 3: Answer Voting Analysis", "labels": [], "entities": [{"text": "Answer Voting Analysis", "start_pos": 10, "end_pos": 32, "type": "TASK", "confidence": 0.9650613069534302}]}]}