{"title": [{"text": "Feature-Rich Part-of-Speech Tagging with a Cyclic Dependency Network", "labels": [], "entities": [{"text": "Part-of-Speech Tagging", "start_pos": 13, "end_pos": 35, "type": "TASK", "confidence": 0.6245429813861847}]}], "abstractContent": [{"text": "We present anew part-of-speech tagger that demonstrates the following ideas: (i) explicit use of both preceding and following tag contexts via a dependency network representation , (ii) broad use of lexical features, including jointly conditioning on multiple consecutive words, (iii) effective use of priors in conditional loglinear models, and (iv) fine-grained modeling of unknown word features.", "labels": [], "entities": []}, {"text": "Using these ideas together, the resulting tagger gives a 97.24% accuracy on the Penn Treebank WSJ, an error reduction of 4.4% on the best previous single automatically learned tagging result.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 64, "end_pos": 72, "type": "METRIC", "confidence": 0.9995120763778687}, {"text": "Penn Treebank WSJ", "start_pos": 80, "end_pos": 97, "type": "DATASET", "confidence": 0.9890777866045634}]}], "introductionContent": [{"text": "Almost all approaches to sequence problems such as partof-speech tagging take a unidirectional approach to conditioning inference along the sequence.", "labels": [], "entities": [{"text": "partof-speech tagging", "start_pos": 51, "end_pos": 72, "type": "TASK", "confidence": 0.8549611270427704}]}, {"text": "Regardless of whether one is using HMMs, maximum entropy conditional sequence models, or other techniques like decision trees, most systems work in one direction through the sequence (normally left to right, but occasionally right to left, e.g.,).", "labels": [], "entities": []}, {"text": "There area few exceptions, such as Brill's transformation-based learning, but most of the best known and most successful approaches of recent years have been unidirectional.", "labels": [], "entities": []}, {"text": "Most sequence models can be seen as chaining together the scores or decisions from successive local models to form a global model for an entire sequence.", "labels": [], "entities": []}, {"text": "Clearly the identity of a tag is correlated with both past and future tags' identities.", "labels": [], "entities": []}, {"text": "However, in the unidirectional (causal) case, only one direction of influence is explicitly considered at each local point.", "labels": [], "entities": []}, {"text": "For example, in a left-to-right first-order HMM, the current tag t 0 is predicted based on the previous tag t \u22121 (and the current word).", "labels": [], "entities": []}, {"text": "The backward interaction between t 0 and the next tag t +1 shows up implicitly later, when t +1 is generated in turn.", "labels": [], "entities": []}, {"text": "While unidirectional models are therefore able to capture both directions of influence, there are good reasons for suspecting that it would be advantageous to make information from both directions explicitly available for conditioning at each local point in the model: (i) because of smoothing and interactions with other modeled features, terms like P(t 0 |t +1 , . . .) might give a sharp estimate oft 0 even when terms like P(t +1 |t 0 , . . .) do not, and (ii) jointly considering the left and right context together might be especially revealing.", "labels": [], "entities": []}, {"text": "In this paper we exploit this idea, using dependency networks, with a series of local conditional loglinear (aka maximum entropy or multiclass logistic regression) models as one way of providing efficient bidirectional inference.", "labels": [], "entities": []}, {"text": "Secondly, while all taggers use lexical information, and, indeed, it is well-known that lexical probabilities are much more revealing than tag sequence probabilities, most taggers make quite limited use of lexical probabilities (compared with, for example, the bilexical probabilities commonly used in current statistical parsers).", "labels": [], "entities": []}, {"text": "While modern taggers maybe more principled than the classic CLAWS tagger, they are in some respects inferior in their use of lexical information: CLAWS, through its IDIOMTAG module, categorically captured many important, correct taggings of frequent idiomatic word sequences.", "labels": [], "entities": []}, {"text": "In this work, we incorporate appropriate multiword feature templates so that such facts can be learned and used automatically by the model.", "labels": [], "entities": []}, {"text": "Having expressive templates leads to a large number of features, but we show that by suitable use of a prior (i.e., regularization) in the conditional loglinear modelsomething not used by previous maximum entropy taggers -many such features can be added with an overall positive effect on the model.", "labels": [], "entities": []}, {"text": "Indeed, as for the voted perceptron of, we can get performance gains by reducing the support threshold for features to be included in the model.", "labels": [], "entities": []}, {"text": "Combining all these ideas, together with a few additional handcrafted unknown word features, gives us a part-of-speech tagger with a per-position tag accuracy of 97.24%, and a whole-sentence correct rate of 56.34% on Penn Treebank WSJ data.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 150, "end_pos": 158, "type": "METRIC", "confidence": 0.898043692111969}, {"text": "correct rate", "start_pos": 191, "end_pos": 203, "type": "METRIC", "confidence": 0.9524171948432922}, {"text": "Penn Treebank WSJ data", "start_pos": 217, "end_pos": 239, "type": "DATASET", "confidence": 0.9758636653423309}]}, {"text": "This is the best automatically learned part-of-speech tagging result known to us, representing an error reduction of 4.4% on the model presented in, using the same data splits, and a larger error reduction of 12.1% from the more similar best previous loglinear model in.", "labels": [], "entities": [{"text": "part-of-speech tagging", "start_pos": 39, "end_pos": 61, "type": "TASK", "confidence": 0.6776489615440369}]}], "datasetContent": [{"text": "The part of speech tagged data used in our experiments is the Wall Street Journal data from Penn Treebank III.", "labels": [], "entities": [{"text": "speech tagged", "start_pos": 12, "end_pos": 25, "type": "TASK", "confidence": 0.6931859850883484}, {"text": "Wall Street Journal data from Penn Treebank III", "start_pos": 62, "end_pos": 109, "type": "DATASET", "confidence": 0.9516720771789551}]}, {"text": "We extracted tagged sentences from the parse trees.", "labels": [], "entities": []}, {"text": "We split the data into training, development, and test sets as in. lists character-5 Note that these tags (and sentences) are not identical to those obtained from the tagged/pos directories of the same disk: hundreds of tags in the RB/RP/IN set were changed to be more consistent in the parsed/mrg version.", "labels": [], "entities": [{"text": "RB/RP/IN set", "start_pos": 232, "end_pos": 244, "type": "DATASET", "confidence": 0.5352324595053991}]}, {"text": "Maybe we were the last to discover this, but we've never seen it in print.", "labels": [], "entities": []}, {"text": "istics of the three splits.", "labels": [], "entities": []}, {"text": "Except where indicated for the model BEST, all results are on the development set.", "labels": [], "entities": [{"text": "BEST", "start_pos": 37, "end_pos": 41, "type": "METRIC", "confidence": 0.9947202205657959}]}, {"text": "One innovation in our reporting of results is that we present whole-sentence accuracy numbers as well as the traditional per-tag accuracy measure (over all tokens, even unambiguous ones).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 77, "end_pos": 85, "type": "METRIC", "confidence": 0.9434046745300293}, {"text": "per-tag accuracy measure", "start_pos": 121, "end_pos": 145, "type": "METRIC", "confidence": 0.7145690222581228}]}, {"text": "This is the quantity that most sequence models attempt to maximize (and has been motivated over doing per-state optimization as being more useful for subsequent linguistic processing: one wants to find a coherent sentence interpretation).", "labels": [], "entities": []}, {"text": "Further, while some tag errors matter much more than others, to a first cut getting a single tag wrong in many of the more common ways (e.g., proper noun vs. common noun; noun vs. verb) would lead to errors in a subsequent processor such as an information extraction system or a parser that would greatly degrade results for the entire sentence.", "labels": [], "entities": []}, {"text": "Finally, the fact that the measure has much more dynamic range has some appeal when reporting tagging results.", "labels": [], "entities": []}, {"text": "The per-state models in this paper are log-linear models, building upon the models in and), though some models are in fact strictly simpler.", "labels": [], "entities": []}, {"text": "The features in the models are defined using templates; there are different templates for rare words aimed at learning the correct tags for unknown words.", "labels": [], "entities": []}, {"text": "We present the results of three classes of experiments: experiments with directionality, experiments with lexicalization, and experiments with smoothing.", "labels": [], "entities": []}, {"text": "In this section, we report experiments using log-linear CMMs to populate nets with various structures, exploring the relative value of neighboring words' tags.", "labels": [], "entities": []}, {"text": "All networks have the same vertical feature templates: t 0 , w 0 features for known words and various t 0 , \u03c3 (w 1n ) word signature features for all words, known or not, including spelling and capitalization features (see section 3.3).", "labels": [], "entities": []}, {"text": "Just this vertical conditioning gives an accuracy of 93.69% (denoted as \"Baseline\" in table 2).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 41, "end_pos": 49, "type": "METRIC", "confidence": 0.9996863603591919}]}, {"text": "8 Condition- Tagger results are only comparable when tested not only on the same data and tag set, but with the same amount of training data.", "labels": [], "entities": []}, {"text": "illustrates very clearly how tagging performance increases as training set size grows, largely because the percentage of unknown words decreases while system performance on them increases (they become increasingly restricted as to word class).", "labels": [], "entities": []}, {"text": "Except where otherwise stated, a count cutoff of 2 was used for common word features and 35 for rare word features (templates need a support set strictly greater in size than the cutoff before they are included in the model).", "labels": [], "entities": []}, {"text": "8 noted that such a simple model got 90.25%, but this was with no unknown word model beyond a prior distribution over tags.", "labels": [], "entities": []}, {"text": "raise this baseline to 92.34%, and with our sophisticated unknown word model, it gets even higher.", "labels": [], "entities": []}, {"text": "The large number of unambiguous tokens and ones with very skewed distributions make the base-    ing on the previous tag as well (model L, t 0 , t \u22121 features) gives 95.79%.", "labels": [], "entities": [{"text": "base-    ing", "start_pos": 88, "end_pos": 100, "type": "METRIC", "confidence": 0.8244797786076864}]}, {"text": "The reverse, model R, using the next tag instead, is slightly inferior at 95.14%.", "labels": [], "entities": [{"text": "R", "start_pos": 19, "end_pos": 20, "type": "METRIC", "confidence": 0.963959276676178}]}, {"text": "Model L+R, using both tags simultaneously (but with only the individual-direction features) gives a much better accuracy of 96.57%.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 112, "end_pos": 120, "type": "METRIC", "confidence": 0.9995185136795044}]}, {"text": "Since this model has roughly twice as many tag-tag features, the fact that it outperforms the unidirectional models is not by itself compelling evidence for using bidirectional networks.", "labels": [], "entities": []}, {"text": "However, it also outperforms model L+L 2 which adds the t 0 , t \u22122 secondprevious word features instead of next word features, which gives only 96.05% (and R+R 2 gives 95.25%).", "labels": [], "entities": []}, {"text": "We conclude that, if one wishes to condition on two neighboring nodes (using two sets of 2-tag features), the symmetric bidirectional model is superior.", "labels": [], "entities": []}, {"text": "High-performance taggers typically also include joint three-tag counts in someway, either as tag trigrams) or tag-triple features).", "labels": [], "entities": []}, {"text": "Models LL, RR, and CR use only the vertical features and a single set of tag-triple features: the left tags (t \u22122 , t \u22121 and t 0 ), right tags (t 0 , t +1 , t +2 ), or centered tags (t \u22121 , t 0 , t +1 ) respectively.", "labels": [], "entities": []}, {"text": "Again, with roughly equivalent feature sets, the left context is better than the right, and the centered context is better than either unidirectional context.", "labels": [], "entities": []}, {"text": "line for this task high, while substantial annotator noise creates an unknown upper bound on the task.", "labels": [], "entities": [{"text": "line", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.9499087333679199}]}], "tableCaptions": [{"text": " Table 1: Data set splits used.", "labels": [], "entities": []}, {"text": " Table 2: Tagging accuracy on the development set with different sequence feature templates.  \u2020All models include the same vertical  word-tag features (t 0 , w 0 and various t 0 , \u03c3 (w 1n )), though the baseline uses a lower cutoff for these features.", "labels": [], "entities": [{"text": "Tagging", "start_pos": 10, "end_pos": 17, "type": "TASK", "confidence": 0.9389346241950989}, {"text": "accuracy", "start_pos": 18, "end_pos": 26, "type": "METRIC", "confidence": 0.9844439029693604}]}, {"text": " Table 3: Tagging accuracy with different lexical feature templates on the development set.", "labels": [], "entities": [{"text": "Tagging", "start_pos": 10, "end_pos": 17, "type": "TASK", "confidence": 0.9834511280059814}, {"text": "accuracy", "start_pos": 18, "end_pos": 26, "type": "METRIC", "confidence": 0.9829387664794922}]}, {"text": " Table 4: Final tagging accuracy for the best model on the test set.", "labels": [], "entities": [{"text": "tagging", "start_pos": 16, "end_pos": 23, "type": "TASK", "confidence": 0.8637117147445679}, {"text": "accuracy", "start_pos": 24, "end_pos": 32, "type": "METRIC", "confidence": 0.9791472554206848}]}, {"text": " Table 5: Accuracy with and without quadratic regularization.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9973649382591248}]}, {"text": " Table 6: Effect of changing common word feature cutoffs (fea- tures with support \u2264 cutoff are excluded from the model).", "labels": [], "entities": []}]}