{"title": [{"text": "A low-complexity, broad-coverage probabilistic Dependency Parser for English", "labels": [], "entities": []}], "abstractContent": [{"text": "Large-scale parsing is still a complex and time-consuming process, often so much that it is in-feasible in real-world applications.", "labels": [], "entities": [{"text": "Large-scale parsing", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.5552268326282501}]}, {"text": "The parsing system described here addresses this problem by combining finite-state approaches, statistical parsing techniques and engineering knowledge , thus keeping parsing complexity as low as possible at the cost of a slight decrease in performance.", "labels": [], "entities": [{"text": "parsing", "start_pos": 4, "end_pos": 11, "type": "TASK", "confidence": 0.9736567735671997}, {"text": "statistical parsing", "start_pos": 95, "end_pos": 114, "type": "TASK", "confidence": 0.6639619767665863}]}, {"text": "The parser is robust and fast and at the same time based on strong linguistic foundations.", "labels": [], "entities": []}], "introductionContent": [{"text": "Many extensions to text-based, data-intensive knowledge management approaches, such as Information Retrieval or Data Mining, focus on integrating the impressive recent advances in language technology.", "labels": [], "entities": [{"text": "Information Retrieval", "start_pos": 87, "end_pos": 108, "type": "TASK", "confidence": 0.791714608669281}, {"text": "Data Mining", "start_pos": 112, "end_pos": 123, "type": "TASK", "confidence": 0.6777868866920471}]}, {"text": "For this, they need fast, robust parsers that deliver linguistic data which is meaningful for the subsequent processing stages.", "labels": [], "entities": []}, {"text": "This paper presents such a parsing system.", "labels": [], "entities": [{"text": "parsing", "start_pos": 27, "end_pos": 34, "type": "TASK", "confidence": 0.9696238040924072}]}, {"text": "Its output is a hierarchical structure of syntactic relations, functional dependency structures, which are discussed in section 2.", "labels": [], "entities": []}, {"text": "The parser differs on the one hand from successful Dependency Grammar implementations (e.g.,) by using a statistical base, and on the other hand from state-of-the-art statistical approaches (e.g.) by carefully following an established formal grammar theory, Dependency Grammar (DG).", "labels": [], "entities": []}, {"text": "It combines two probabilistic models of language, similar to, which are discussed in section 3.", "labels": [], "entities": []}, {"text": "Both are supervised and based on Maximum Likelihood Estimation (MLE).", "labels": [], "entities": []}, {"text": "The first one is based on the lexical probabilities of the heads of phrases, similar to.", "labels": [], "entities": []}, {"text": "It calculates the probability of finding specific syntactic relations (such as subject, sentential object, etc.) between given lexical heads.", "labels": [], "entities": []}, {"text": "Two simple extensions for the interaction between several dependents of the same mother node are also used.", "labels": [], "entities": []}, {"text": "The second probability model is a PCFG for the production of the VP.", "labels": [], "entities": [{"text": "PCFG", "start_pos": 34, "end_pos": 38, "type": "METRIC", "confidence": 0.7368043661117554}]}, {"text": "Although traditional CFGs are not part of DG, VP PCFG rules can model verb subcategorization frames, an important DG component.", "labels": [], "entities": []}, {"text": "The parser has been trained, developed and tested on a large collection of syntactically analyzed sentences, the Penn Treebank (.", "labels": [], "entities": [{"text": "Penn Treebank", "start_pos": 113, "end_pos": 126, "type": "DATASET", "confidence": 0.9953202605247498}]}, {"text": "It is broad-coverage and robust and returns an optimal set of partial structures when it fails to find a complete structure fora sentence.", "labels": [], "entities": []}, {"text": "It has been designed to keep complexity as low as possible during the parsing process in order to be fast enough to be useful for parsing large amounts of unrestricted text.", "labels": [], "entities": [{"text": "parsing process", "start_pos": 70, "end_pos": 85, "type": "TASK", "confidence": 0.8957974016666412}, {"text": "parsing large amounts of unrestricted text", "start_pos": 130, "end_pos": 172, "type": "TASK", "confidence": 0.8478249112764994}]}, {"text": "This has been achieved by observing the following constraints, discussed in section 4: \u2022 using a syntactic theory known for its relatively flat structures and lack of empty nodes (see also subsection 2.4) \u2022 relying on finite-state preprocessing \u2022 discarding unlikely readings with abeam search \u2022 using the fast Cocke-Younger-Kasami (CYK) parsing algorithm \u2022 using a restrictive hand-written linguistic grammar The parsing system uses a divide-and-conquer approach.", "labels": [], "entities": []}, {"text": "Low-level linguistic tasks that can be reliably solved by finite-state techniques are handed over to them.", "labels": [], "entities": []}, {"text": "These low-level tasks are the recognition of part-of-speech by means of tagging, and the recognition of base NPs and verbal groups by means of chunking.", "labels": [], "entities": []}, {"text": "The parser then relies on the disambiguation decisions of the tagging and chunking stage and can profit from a reduced search space, at the cost of a slightly decreased performance due to tagging and chunking errors.", "labels": [], "entities": []}, {"text": "The paper ends with a preliminary evaluation of this work in progress.", "labels": [], "entities": []}], "datasetContent": [{"text": "The probabilistic language models have been trained on section 2 to 24 and the parser tested on section 0.", "labels": [], "entities": []}, {"text": "The: Provisional precision and recall values held out training data and the first-ranked reading for each sentence of section 0 are compared for evaluation.", "labels": [], "entities": [{"text": "Provisional", "start_pos": 5, "end_pos": 16, "type": "METRIC", "confidence": 0.9721195101737976}, {"text": "precision", "start_pos": 17, "end_pos": 26, "type": "METRIC", "confidence": 0.883145809173584}, {"text": "recall", "start_pos": 31, "end_pos": 37, "type": "METRIC", "confidence": 0.9982434511184692}]}, {"text": "Parsing the 46527 words of section 0 takes 30 minutes on a 800 MHz Pentium 3 PC, including about 3 minutes for tagging and chunking.", "labels": [], "entities": [{"text": "Parsing the 46527 words of section 0", "start_pos": 0, "end_pos": 36, "type": "TASK", "confidence": 0.8097170506204877}]}, {"text": "Current precision and recall values for subject, object and PP-attachment relations, and for the disambiguation between prepositions and complements are in table 1.", "labels": [], "entities": [{"text": "precision", "start_pos": 8, "end_pos": 17, "type": "METRIC", "confidence": 0.9991450309753418}, {"text": "recall", "start_pos": 22, "end_pos": 28, "type": "METRIC", "confidence": 0.9981897473335266}]}, {"text": "These results, slightly lower than state-of-the-art (,), are least merit figures or a proof of concept rather than accurate figures.", "labels": [], "entities": []}, {"text": "On the one hand, the performance of the parser suffers from mistaggings and mischunkings or a limited grammar, the price for the speed increase.", "labels": [], "entities": [{"text": "grammar", "start_pos": 102, "end_pos": 109, "type": "METRIC", "confidence": 0.9670640826225281}]}, {"text": "On the other hand, different grammatical assumptions both between the Treebank and the chunker, and between the Treebank and functional dependency, seriously affect the evaluation.", "labels": [], "entities": []}, {"text": "For example, the chunker often recognizes units longer than base-NPs like [many of the people], or smaller or longer than verbal groups fora longtime, [likely to bring] -correct chunks which are currently considered as errors.", "labels": [], "entities": []}, {"text": "In addition, it is very difficult to avoid tgrep overgenerating or missing.", "labels": [], "entities": []}, {"text": "It turns out that the mapping is accurate enough fora statistical model but not fora reliable evaluation.", "labels": [], "entities": []}, {"text": "Some possible configurations are missed by the current extraction queries.", "labels": [], "entities": []}, {"text": "For example, extraposed PPs such as the one starting this sentence, have escaped unmapped until now.", "labels": [], "entities": []}, {"text": "For the future, the use of a standardized DG test suite is envisaged).", "labels": [], "entities": [{"text": "DG test suite", "start_pos": 42, "end_pos": 55, "type": "DATASET", "confidence": 0.7833694815635681}]}, {"text": "The grammar explicitly excludes a number of grammatical phenomena which cannot currently be treated reliably.", "labels": [], "entities": []}, {"text": "For example, since no PP-interaction model such as PCFG rules for NP-attached PPs exists yet, the current grammar does not allow a NP to take several PPs, which affects the analysis of relational nouns.", "labels": [], "entities": []}, {"text": "The statistical models, the dependency extraction, the grammar, the tagger and chunker approach and the evaluation method will continue to be improved.", "labels": [], "entities": [{"text": "dependency extraction", "start_pos": 28, "end_pos": 49, "type": "TASK", "confidence": 0.795398086309433}]}], "tableCaptions": [{"text": " Table 1: Provisional precision and recall values", "labels": [], "entities": [{"text": "Provisional", "start_pos": 10, "end_pos": 21, "type": "METRIC", "confidence": 0.9294852614402771}, {"text": "precision", "start_pos": 22, "end_pos": 31, "type": "METRIC", "confidence": 0.9369434118270874}, {"text": "recall", "start_pos": 36, "end_pos": 42, "type": "METRIC", "confidence": 0.9978187084197998}]}]}