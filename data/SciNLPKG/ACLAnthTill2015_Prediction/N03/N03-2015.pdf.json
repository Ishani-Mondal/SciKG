{"title": [{"text": "Unsupervised Learning of Morphology for English and Inuktitut", "labels": [], "entities": [{"text": "Unsupervised Learning of Morphology", "start_pos": 0, "end_pos": 35, "type": "TASK", "confidence": 0.5924406945705414}]}], "abstractContent": [{"text": "We describe a simple unsupervised technique for learning morphology by identifying hubs in an automaton.", "labels": [], "entities": []}, {"text": "For our purposes, a hub is anode in a graph with in-degree greater than one and out-degree greater than one.", "labels": [], "entities": []}, {"text": "We create a word-trie, transform it into a minimal DFA, then identify hubs.", "labels": [], "entities": []}, {"text": "Those hubs mark the boundary between root and suffix, achieving similar performance to more complex mixtures of techniques.", "labels": [], "entities": []}], "introductionContent": [{"text": "To recognize a morpheme boundary, for example between a root and a suffix, a learner must have seen at least two roots with that suffix and at least two suffixes with that root.", "labels": [], "entities": []}, {"text": "For instance, 'helpful', 'helpless', 'harmful', and 'harmless' would be enough evidence to guess that those words could be divided as 'help/ful', 'help/less', 'harm/ful', and 'harm/less'.", "labels": [], "entities": []}, {"text": "Without seeing varying roots and varying suffixes, there is no reason to prefer one division to another.", "labels": [], "entities": []}, {"text": "We can represent a language's morphology as a graph or automaton, with the links labeled by characters and the nodes organizing which characters can occur after specific prefixes.", "labels": [], "entities": []}, {"text": "In such an automaton, the morpheme boundaries would be hubs, that is, nodes with indegree greater than one and out-degree greater than one.", "labels": [], "entities": []}, {"text": "Furthermore, this automaton could be simplified bypath compression to remove all nodes with in-degree and out-degree of one.", "labels": [], "entities": []}, {"text": "The remaining automaton could be further modified to produce a graph with one source, one sink, and all other nodes would be hubs.", "labels": [], "entities": []}, {"text": "A hub-automaton, as described above, matches the intuitive idea that a language's morphology allows one to assemble a word by chaining morphemes together.", "labels": [], "entities": []}, {"text": "This representation highlights the morphemes while also representing morphotactic information.", "labels": [], "entities": []}, {"text": "Phonological information can be represented in the same graph but maybe more economically represented in a separate transducer that can be composed with the hubautomaton.", "labels": [], "entities": []}, {"text": "For identifying the boundary between roots and suffixes, the idea of hubs is essentially the same as signatures or the variations between Gaussier's (1999) p-similarity words.", "labels": [], "entities": [{"text": "identifying the boundary between roots and suffixes", "start_pos": 4, "end_pos": 55, "type": "TASK", "confidence": 0.6850105694362095}]}, {"text": "A signature is a set of suffixes, any of which can be added to several roots to create a word.", "labels": [], "entities": []}, {"text": "For example, in English any suffix in the set: NULL, 's', 'ed', 'ing', can be added to 'want' or 'wander' to form a word.", "labels": [], "entities": [{"text": "NULL", "start_pos": 47, "end_pos": 51, "type": "METRIC", "confidence": 0.9479490518569946}]}, {"text": "Here, NULL means the empty suffix.", "labels": [], "entities": [{"text": "NULL", "start_pos": 6, "end_pos": 10, "type": "METRIC", "confidence": 0.9606067538261414}]}, {"text": "Ina hub automaton, the idea is more general than in previous work and applies to more complex morphologies, such as those for agglutinative or polysynthetic languages.", "labels": [], "entities": []}, {"text": "In particular, we are interested in unsupervised learning of Inuktitut morphology in which a single lexical unit can often include a verb, two pronouns, adverbs, and temporal information.", "labels": [], "entities": []}, {"text": "In this paper, we describe a very simple technique for identifying hubs as a first step in building a hubautomaton.", "labels": [], "entities": []}, {"text": "We show that, for English, this technique does as well as more complex collections of techniques using signatures.", "labels": [], "entities": []}, {"text": "We then show that the technique also works, in a limited way, for Inuktitut.", "labels": [], "entities": []}, {"text": "We close with a discussion of the limitations and our plans for more complete learning of hub-automata.", "labels": [], "entities": []}], "datasetContent": [{"text": "As noted above, Linguistica uses many techniques to learn morphology, including a fairly complex system for counting bits.", "labels": [], "entities": []}, {"text": "We tested whether the two techniques presented in this paper, hub searching and simple node merging, achieve the same performance as Linguistica.", "labels": [], "entities": [{"text": "hub searching", "start_pos": 62, "end_pos": 75, "type": "TASK", "confidence": 0.7779565155506134}, {"text": "node merging", "start_pos": 87, "end_pos": 99, "type": "TASK", "confidence": 0.7258927226066589}]}, {"text": "If so, the simpler techniques might be preferred.", "labels": [], "entities": []}, {"text": "Also, we would be justified using them for more complex morphologies.", "labels": [], "entities": []}, {"text": "The input to Linguistica and HubMorph was the text of Tom Sawyer.", "labels": [], "entities": [{"text": "HubMorph", "start_pos": 29, "end_pos": 37, "type": "DATASET", "confidence": 0.9448310732841492}]}, {"text": "The performance of both was compared against a gold standard division of the distinct words in that novel.", "labels": [], "entities": []}, {"text": "The gold standard was based on dictionary entries and the judgment of two English speakers.", "labels": [], "entities": []}, {"text": "In matching the gold standard words to divisions predicted by either system, we made the following assumptions.", "labels": [], "entities": []}, {"text": "a) Words with hyphens are split at the hyphen to match Linguistica's assumption.", "labels": [], "entities": []}, {"text": "b) If the gold standard has a break before and after a single character, to capture non-concatenative modification, either break matches.", "labels": [], "entities": []}, {"text": "An example would be 'mud-d-y'.", "labels": [], "entities": []}, {"text": "c) An apostrophe at a morpheme boundary is ignored for comparison matching to allow it to stick to the root or to the suffix.", "labels": [], "entities": [{"text": "comparison matching", "start_pos": 55, "end_pos": 74, "type": "TASK", "confidence": 0.6946592926979065}]}, {"text": "d) The suffix split proposed must result in a suffix of 5 or fewer characters, again to match Linguistica's assumption.", "labels": [], "entities": []}, {"text": "show the results of this comparison for Linguistica, hub-searching alone, and HubMorph (both hub searching and node merging).", "labels": [], "entities": [{"text": "HubMorph", "start_pos": 78, "end_pos": 86, "type": "DATASET", "confidence": 0.9253365397453308}]}, {"text": "Hub-searching alone is sufficient to achieve the same precision as Linguistica and nearly the same recall.", "labels": [], "entities": [{"text": "precision", "start_pos": 54, "end_pos": 63, "type": "METRIC", "confidence": 0.9993070363998413}, {"text": "recall", "start_pos": 99, "end_pos": 105, "type": "METRIC", "confidence": 0.9984971284866333}]}, {"text": "Both of the techniques together are sufficient to achieve the same precision and recall as Linguistica.", "labels": [], "entities": [{"text": "precision", "start_pos": 67, "end_pos": 76, "type": "METRIC", "confidence": 0.999306321144104}, {"text": "recall", "start_pos": 81, "end_pos": 87, "type": "METRIC", "confidence": 0.9989056587219238}]}, {"text": "The recall for all is low because the list of words in Tom Sawyer is not long enough to include most acceptable combinations of roots and suffixes.", "labels": [], "entities": [{"text": "recall", "start_pos": 4, "end_pos": 10, "type": "METRIC", "confidence": 0.9995645880699158}]}, {"text": "A longer input word list would improve this score.", "labels": [], "entities": []}, {"text": "System Recall Precision Linguistica 0.5753 0.9059 Hub-Searching 0.4451 0.9189 HubMorph 0.5904 0.9215: The recall and precision of Linguistica, Hub-searching alone, and HubMorph.", "labels": [], "entities": [{"text": "Hub-Searching 0.4451 0.9189 HubMorph 0.5904 0.9215", "start_pos": 50, "end_pos": 100, "type": "DATASET", "confidence": 0.8707546691099802}, {"text": "recall", "start_pos": 106, "end_pos": 112, "type": "METRIC", "confidence": 0.9991983771324158}, {"text": "precision", "start_pos": 117, "end_pos": 126, "type": "METRIC", "confidence": 0.996764063835144}, {"text": "HubMorph", "start_pos": 168, "end_pos": 176, "type": "DATASET", "confidence": 0.9334115386009216}]}, {"text": "Recall is the proportion of distinct words from Tom Sawyer that are correctly divided into root and suffix.", "labels": [], "entities": [{"text": "Recall", "start_pos": 0, "end_pos": 6, "type": "METRIC", "confidence": 0.978391706943512}]}, {"text": "Precision is the proportion of predicted divisions that are correct.", "labels": [], "entities": [{"text": "Precision", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.9912183880805969}]}], "tableCaptions": []}