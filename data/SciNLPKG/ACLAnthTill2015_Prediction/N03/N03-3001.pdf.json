{"title": [{"text": "Semantic Language Models for Topic Detection and Tracking", "labels": [], "entities": [{"text": "Topic Detection and Tracking", "start_pos": 29, "end_pos": 57, "type": "TASK", "confidence": 0.8260784894227982}]}], "abstractContent": [{"text": "In this work, we present anew semantic language modeling approach to model news stories in the Topic Detection and Tracking (TDT) task.", "labels": [], "entities": [{"text": "Topic Detection and Tracking (TDT) task", "start_pos": 95, "end_pos": 134, "type": "TASK", "confidence": 0.8626395687460899}]}, {"text": "In the new approach, we build a unigram language model for each semantic class in a news story.", "labels": [], "entities": []}, {"text": "We also cast the link detection sub-task of TDT as a two-class classification problem in which the features of each sample consist of the generative log-likelihood ratios from each semantic class.", "labels": [], "entities": [{"text": "link detection", "start_pos": 17, "end_pos": 31, "type": "TASK", "confidence": 0.7835467755794525}]}, {"text": "We then compute a linear discriminant classifier using the perceptron learning algorithm on the training set.", "labels": [], "entities": []}, {"text": "Results on the test set show a marginal improvement over the unigram performance, but are not very encouraging on the whole.", "labels": [], "entities": []}], "introductionContent": [{"text": "TDT is a research program investigating methods for automatically organizing news stories by the events that they discuss.", "labels": [], "entities": [{"text": "TDT", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.7189135551452637}, {"text": "automatically organizing news stories by the events that they discuss", "start_pos": 52, "end_pos": 121, "type": "TASK", "confidence": 0.7939565777778625}]}, {"text": "The goal of TDT consists of breaking the stream of news into individual news stories, to monitor the stories for events that have not been seen before and to gather stories into groups that each discuss a single topic.", "labels": [], "entities": [{"text": "TDT", "start_pos": 12, "end_pos": 15, "type": "TASK", "confidence": 0.7386337518692017}]}, {"text": "Several approaches have been explored for comparing news stories in TDT.", "labels": [], "entities": [{"text": "comparing news stories in TDT", "start_pos": 42, "end_pos": 71, "type": "TASK", "confidence": 0.7565796971321106}]}, {"text": "The traditional vector space approach () using cosine similarity has by far been the most consistently successful approach across different tasks and several data sets.", "labels": [], "entities": []}, {"text": "In the recent past, anew probabilistic approach called Language Modeling () has proven to be very effective in several information retrieval tasks.", "labels": [], "entities": [{"text": "Language Modeling", "start_pos": 55, "end_pos": 72, "type": "TASK", "confidence": 0.7290101945400238}, {"text": "information retrieval tasks", "start_pos": 119, "end_pos": 146, "type": "TASK", "confidence": 0.8390284975369772}]}, {"text": "One of the attractive features of language models is that they are firmly rooted in the theory of probability thereby allowing a researcher to explore more sophisticated models guided by the theoretical framework.", "labels": [], "entities": []}, {"text": "Allan et al) applied language models to the first story detection task of TDT and found that its performance is on par with the traditional vector space models, if not better.", "labels": [], "entities": [{"text": "story detection task", "start_pos": 50, "end_pos": 70, "type": "TASK", "confidence": 0.782695879538854}]}, {"text": "In the language modeling approach to TDT, we measure the similarity of a news story D to a topic by the probability of its generation from the topic model M . Using the unigram assumption of independence of terms, one can compute the probability of generation of a news story as the product of probabilities of generation of the terms in the story, as shown in the following equation: where w i is the i-th term in the story.", "labels": [], "entities": [{"text": "TDT", "start_pos": 37, "end_pos": 40, "type": "TASK", "confidence": 0.9492411017417908}]}, {"text": "The topic model M is typically evaluated from the statistics of a set of stories that are known to be on the topic in consideration.", "labels": [], "entities": []}, {"text": "One potential drawback of the unigram language model is that it treats all terms on an equal footing and seems to ignore semantic information of the terms.", "labels": [], "entities": []}, {"text": "We believe that such information could be useful in determining the relative importance of a term to the topic of the story.", "labels": [], "entities": []}, {"text": "For example, terms that belong to the named-entity type such as person, location, organization may convey more information about the topic of the story than other entity types.", "labels": [], "entities": []}, {"text": "Likewise, one might expect that nouns and verbs play a more important role than adjectives, adverbs or propositions in determining the topic of the story.", "labels": [], "entities": []}, {"text": "The present work is an attempt to extend the language modeling framework to incorporate a model of the relative importance of terms according to the semantic class they belong to.", "labels": [], "entities": []}, {"text": "The remainder of the report is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 summarizes attempts made in the past in capturing semantic-class information in information retrieval related tasks.", "labels": [], "entities": []}, {"text": "We present the methodology of the new semantic language modeling approach in section 3.", "labels": [], "entities": [{"text": "semantic language modeling", "start_pos": 38, "end_pos": 64, "type": "TASK", "confidence": 0.6660123666127523}]}, {"text": "In section 4, we present details of the link detection task and Edmonton, May-June 2003 Student Research Workshop , pp.", "labels": [], "entities": [{"text": "link detection task", "start_pos": 40, "end_pos": 59, "type": "TASK", "confidence": 0.9130736986796061}, {"text": "Edmonton, May-June 2003 Student Research Workshop , pp", "start_pos": 64, "end_pos": 118, "type": "DATASET", "confidence": 0.9375254644287957}]}, {"text": "1-6 Proceedings of HLT-NAACL 2003 its evaluation.", "labels": [], "entities": [{"text": "HLT-NAACL 2003", "start_pos": 19, "end_pos": 33, "type": "DATASET", "confidence": 0.6300559490919113}]}, {"text": "Section 5 describes the experiments performed and presents the results obtained.", "labels": [], "entities": []}, {"text": "In section 6 we analyze the performance of the new model.", "labels": [], "entities": []}, {"text": "Section 7 ends the discussion with a few observations and lays down the path to future work.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we describe one of the tasks called link detection, on which we performed the experiments reported in this work.", "labels": [], "entities": [{"text": "link detection", "start_pos": 53, "end_pos": 67, "type": "TASK", "confidence": 0.8794483840465546}]}, {"text": "Link detection requires determining whether or not two randomly selected stories (D 1 , D 2 ) discuss the same topic.", "labels": [], "entities": [{"text": "Link detection", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.9708790183067322}]}, {"text": "The evaluation methodology of a link detection system requires the system to output a score for each story pair that represents the system's confidence that both stories in the pair discuss the same topic.", "labels": [], "entities": [{"text": "link detection", "start_pos": 32, "end_pos": 46, "type": "TASK", "confidence": 0.7554278373718262}]}, {"text": "The system's performance is then evaluated using a topicweighted Detection Error Trade-off (DET) curve) that plots miss rate against false alarm over a large number of story pairs, at different values of decision-threshold.", "labels": [], "entities": [{"text": "topicweighted Detection Error Trade-off (DET) curve", "start_pos": 51, "end_pos": 102, "type": "METRIC", "confidence": 0.7679915763437748}, {"text": "miss rate", "start_pos": 115, "end_pos": 124, "type": "METRIC", "confidence": 0.9833259284496307}]}, {"text": "A Link Detection cost function C link is then used to combine the miss and false alarm probabilities at each value of threshold into a single normalized evaluation score.", "labels": [], "entities": [{"text": "miss and false alarm probabilities", "start_pos": 66, "end_pos": 100, "type": "METRIC", "confidence": 0.7817564010620117}]}, {"text": "We use the minimum value of C link as the primary measure of effectiveness and show DET curves to illustrate the error trade-offs.", "labels": [], "entities": [{"text": "DET", "start_pos": 84, "end_pos": 87, "type": "METRIC", "confidence": 0.9357703924179077}]}, {"text": "It maybe useful for the reader to remember that, since the DET curve is an error-tradeoff plot, the closer the curve is to the origin, the better is the performance, unlike the standard precision-recall curve familiar to the IR community.", "labels": [], "entities": [{"text": "DET", "start_pos": 59, "end_pos": 62, "type": "METRIC", "confidence": 0.5741025805473328}]}, {"text": "We have used Identifinder () and Jtag () respectively, to tag each term by its named-entity type and its part of speech category.", "labels": [], "entities": []}, {"text": "Additionally, we have used a list of 423 most frequent words to remove stop words from stories.", "labels": [], "entities": []}, {"text": "Stemming is done using the Porter stemmer while the model is implemented using Java.", "labels": [], "entities": []}, {"text": "As a training set, we have used a subset of TDT3 corpus that consists of news stories from eight English sources collected roughly from October through December 1998.", "labels": [], "entities": [{"text": "TDT3 corpus", "start_pos": 44, "end_pos": 55, "type": "DATASET", "confidence": 0.9676668047904968}]}, {"text": "We have used manual transcriptions of stories when the source is audio/video.", "labels": [], "entities": []}, {"text": "The training set consists of 7200 story pairs.", "labels": [], "entities": []}, {"text": "For the general English model for this set, we have used the same TDT3 natural English manually transcribed set consisting of 37,526 news stories.", "labels": [], "entities": [{"text": "TDT3 natural English manually transcribed set consisting of 37,526 news stories", "start_pos": 66, "end_pos": 145, "type": "DATASET", "confidence": 0.8945989175276323}]}, {"text": "For the test set, we have used a randomly chosen subset of natural English, manually transcribed stories from TDT2 corpus.", "labels": [], "entities": [{"text": "TDT2 corpus", "start_pos": 110, "end_pos": 121, "type": "DATASET", "confidence": 0.9752828478813171}]}, {"text": "It consists of 6,363 story pairs and the general English statistics are derived from 40,851 stories.", "labels": [], "entities": []}, {"text": "In the unigram language modeling approach to link detection, which we have used as baseline in our experiments, we build a topic model M (D 1 ) from one of the stories D 1 in the pair.", "labels": [], "entities": [{"text": "link detection", "start_pos": 45, "end_pos": 59, "type": "TASK", "confidence": 0.8772093057632446}]}, {"text": "We then compute the loglikelihood ratio L(D 2 |D 1 ) of the second story D 2 with respect to M (D 1 ) similar to equation 5 but considering the entire document as a single feature list.", "labels": [], "entities": []}, {"text": "The semantic language model score, on the other hand, is computed as described in section 3.", "labels": [], "entities": []}, {"text": "Sometimes we may use a symmetrized version of the formula, as shown below: However, in this work, we have considered only the asymmetric version of the formula to maintain simplicity of the scoring function.", "labels": [], "entities": [{"text": "simplicity", "start_pos": 172, "end_pos": 182, "type": "METRIC", "confidence": 0.9875307679176331}]}, {"text": "For fair comparison, we have used an asymmetric version of the baseline unigram language model too.", "labels": [], "entities": []}, {"text": "We have considered the categories in as our semantic classes.", "labels": [], "entities": []}, {"text": "Note that only terms that are not classified as persons, organizations or locations are considered as candidates for nouns.", "labels": [], "entities": []}, {"text": "The numbers in the table indicate the weight assigned by the perceptron to each class.", "labels": [], "entities": []}, {"text": "We have trained the perceptron using the 7200 labeled story-pairs of the training set.", "labels": [], "entities": []}, {"text": "The class All corresponds to the unigram model and consists of all the terms of the story.", "labels": [], "entities": []}, {"text": "Note that some of the classes are defined as the union of two or more subclasses.", "labels": [], "entities": []}, {"text": "We have done this to nullify the labeling error of the named-entity and parts-of-speech taggers.", "labels": [], "entities": []}, {"text": "For example, we have noticed that Identifinder mislabels Persons as Organizations and vice versa quite frequently.", "labels": [], "entities": [{"text": "Identifinder mislabels Persons as Organizations", "start_pos": 34, "end_pos": 81, "type": "TASK", "confidence": 0.722345507144928}]}, {"text": "Our hope is that creating anew class that is a union of both Persons and Organizations will offset such tagging errors.", "labels": [], "entities": []}], "tableCaptions": []}