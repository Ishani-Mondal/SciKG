{"title": [{"text": "A Robust Retrieval Engine for Proximal and Structural Search", "labels": [], "entities": [{"text": "Robust Retrieval", "start_pos": 2, "end_pos": 18, "type": "TASK", "confidence": 0.7081511914730072}, {"text": "Proximal and Structural Search", "start_pos": 30, "end_pos": 60, "type": "TASK", "confidence": 0.862994447350502}]}], "abstractContent": [], "introductionContent": [{"text": "In the text retrieval area including XML and Region Algebra, many researchers pursued models for specifying what kinds of information should appear in specified structural positions and linear positions.", "labels": [], "entities": []}, {"text": "The models attracted many researchers because they are considered to be basic frameworks for retrieving or extracting complex information like events.", "labels": [], "entities": [{"text": "retrieving or extracting complex information like events", "start_pos": 93, "end_pos": 149, "type": "TASK", "confidence": 0.7568881256239754}]}, {"text": "However, unlike IR by keywordbased search, their models are not robust, that is, they support only exact matching of queries, while we would like to know to what degree the contents in specified structural positions are relevant to those in the query even when the structure does not exactly match the query.", "labels": [], "entities": []}, {"text": "This paper describes anew ranked retrieval model that enables proximal and structural search for structured texts.", "labels": [], "entities": []}, {"text": "We extend the model proposed in Region Algebra to be robust by i) incorporating the idea of rankedness in keyword-based search, and ii) expanding queries.", "labels": [], "entities": []}, {"text": "While in ordinary ranked retrieval models relevance measures are computed in terms of words, our model assumes that they are defined in more general structural fragments, i.e., extents (continuous fragments in a text) proposed in Region Algebra.", "labels": [], "entities": []}, {"text": "We decompose queries into subqueries to allow the system not only to retrieve exactly matched extents but also to retrieve partially matched ones.", "labels": [], "entities": []}, {"text": "Our model is robust like keyword-based search, and also enables us to specify the structural and linear positions in texts as done by Region Algebra.", "labels": [], "entities": []}, {"text": "The significance of this work is not in the development of anew relevance measure nor in showing superiority of structure-based search over keyword-based search, but in the proposal of a framework for integrating proximal and structural ranking models.", "labels": [], "entities": []}, {"text": "Since the model treats all types of structures in texts, not only ordinary text structures like \"title,\" \"abstract,\" \"authors,\" etc., but also semantic tags corresponding to recognized named entities or events can also be used for indexing text fragments and contribute to the relevance measure.", "labels": [], "entities": []}, {"text": "Since extents are treated similarly to keywords in traditional models, our model will be integrated with any ranking and scalability techniques used by keyword-based models.", "labels": [], "entities": []}, {"text": "We have implemented the ranking model in our retrieval engine, and had preliminary experiments to evaluate our model.", "labels": [], "entities": []}, {"text": "Unfortunately, we used a rather small corpus for the experiments.", "labels": [], "entities": []}, {"text": "This is mainly because there is no test collection of the structured query and tag-annotated text.", "labels": [], "entities": []}, {"text": "Instead, we used the GENIA corpus () as structured texts, which was an XML document annotated with semantics tags in the filed of biomedical science.", "labels": [], "entities": [{"text": "GENIA corpus", "start_pos": 21, "end_pos": 33, "type": "DATASET", "confidence": 0.9402591586112976}]}, {"text": "The experiments show that our model succeeded in retrieving the relevant answers that an exact-matching model fails to retrieve because of lack of robustness, and the relevant answers that a nonstructured model fails because of lack of structural specification.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we show the results of our preliminary experiments of text retrieval using our model.", "labels": [], "entities": [{"text": "text retrieval", "start_pos": 71, "end_pos": 85, "type": "TASK", "confidence": 0.7955671846866608}]}, {"text": "Because there is no test collection of the structured query and tagannotated text, we used the GENIA corpus () as a structured text, which was an XML document composed of paper abstracts in the field of biomedical science.", "labels": [], "entities": [{"text": "GENIA corpus", "start_pos": 95, "end_pos": 107, "type": "DATASET", "confidence": 0.9305289089679718}]}, {"text": "The corpus consisted of 1,990 articles, 873,087 words (including tags), and 16,391 sentences.", "labels": [], "entities": []}, {"text": "We compared three retrieval models, i) our model, ii) exact matching of the region algebra (exact), and iii) not-structured flat model.", "labels": [], "entities": []}, {"text": "In the flat model, the query was submitted as a query composed of the words in the queries in connected by the \"and\" operator ().", "labels": [], "entities": []}, {"text": "The queries submitted to our system are shown in Table 1, and the document was \"sentence\" represented by \"sentence\" tags.", "labels": [], "entities": []}, {"text": "Query 1, 2, and 3 are real queries made by an expert in the field of biomedicine.", "labels": [], "entities": []}, {"text": "Query 4 is a toy query made by us to seethe robustness compared with the exact model easily.", "labels": [], "entities": []}, {"text": "The system output the ten results that had the highest relevance for each model 2 . shows the number of the results that were judged relevant in the top ten results when the ranking was done using \u03c1 sum . The results show that our model was superior to the exact and flat models for Query 1, 2, and 3.", "labels": [], "entities": []}, {"text": "Compared to the exact model, our model output more relevant documents, since our model allows the partial matching of the query, which shows the robustness of our model.", "labels": [], "entities": []}, {"text": "In addition, our model outperforms the flat model, which means that the structural specification of the query was effective for finding the relevant documents.", "labels": [], "entities": []}, {"text": "For Query 4, our model succeeded in finding the relevant results although the exact model failed to find results because Query 4 includes the tag not contained in the text (\"dummy\" tag).", "labels": [], "entities": []}, {"text": "This result shows the robustness of our model.", "labels": [], "entities": []}, {"text": "Although we omit the results of using \u03c1 sc and \u03c1 ic because of the limit of the space, here we summarize the results of them.", "labels": [], "entities": []}, {"text": "The number of relevant results using \u03c1 sc was the same as that of \u03c1 sum , but the rank of irrelevant  results using \u03c1 sc was lower than that of \u03c1 sum . The results using \u03c1 ic varied between the results of the flat model and the results of \u03c1 sum depending on the value of \u03bb.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: (The number of relevant results) / (the number  of all results) in top 10 results.", "labels": [], "entities": []}]}