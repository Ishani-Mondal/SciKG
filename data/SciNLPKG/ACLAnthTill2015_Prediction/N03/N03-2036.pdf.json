{"title": [{"text": "A Phrase-Based Unigram Model for Statistical Machine Translation", "labels": [], "entities": [{"text": "Statistical Machine Translation", "start_pos": 33, "end_pos": 64, "type": "TASK", "confidence": 0.866262157758077}]}], "abstractContent": [{"text": "In this paper, we describe a phrase-based un-igram model for statistical machine translation that uses a much simpler set of model parameters than similar phrase-based models.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 61, "end_pos": 92, "type": "TASK", "confidence": 0.7247564097245535}]}, {"text": "The units of translation are blocks-pairs of phrases.", "labels": [], "entities": []}, {"text": "During decoding, we use a block un-igram model and a word-based trigram language model.", "labels": [], "entities": []}, {"text": "During training, the blocks are learned from source interval projections using an underlying word alignment.", "labels": [], "entities": []}, {"text": "We show experimental results on block selection criteria based on unigram counts and phrase length.", "labels": [], "entities": []}, {"text": "1 Phrase-based Unigram Model Various papers use phrase-based translation systems (Och et al., 1999; Marcu and Wong, 2002; Yamada and Knight, 2002) that have shown to improve translation quality over single-word based translation systems introduced in (Brown et al., 1993).", "labels": [], "entities": []}, {"text": "In this paper, we present a similar system with a much simpler set of model parameters.", "labels": [], "entities": []}, {"text": "Specifically, we compute the probability of a block sequence b n 1.", "labels": [], "entities": []}, {"text": "The block sequence probability P r(b n 1) is decomposed into conditional probabilities using the chain rule: P r(b n 1) \u2248 n \ud97b\udf59 i=1 P r(b i |b i\u22121) (1) = n \ud97b\udf59 i=1 p \u03b1 (b i |b i\u22121) \u00b7 p (1\u2212\u03b1) (b i |b i\u22121) \u2248 n \ud97b\udf59 i=1 p \u03b1 (b i) \u00b7 p (1\u2212\u03b1) (b i |b i\u22121) We try to find the block sequence that maximizes P r(b n 1): b n 1 = arg max b n 1 P r(b n 1).", "labels": [], "entities": []}, {"text": "The model proposed is a joint 1 1", "labels": [], "entities": []}], "introductionContent": [], "datasetContent": [{"text": "The translation system is tested on a Chinese-to-English translation task.", "labels": [], "entities": [{"text": "translation", "start_pos": 4, "end_pos": 15, "type": "TASK", "confidence": 0.965043842792511}, {"text": "Chinese-to-English translation task", "start_pos": 38, "end_pos": 73, "type": "TASK", "confidence": 0.750191350777944}]}, {"text": "The training data come from several news sources.", "labels": [], "entities": []}, {"text": "For testing, we use the DARPA/NIST MT 2001 dry-run testing data, which consists of 793 sentences with 20, 333 words arranged in 80 documents.", "labels": [], "entities": [{"text": "DARPA/NIST MT 2001 dry-run testing data", "start_pos": 24, "end_pos": 63, "type": "DATASET", "confidence": 0.8700240030884743}]}, {"text": "The training data is provided by the LDC and labeled by NIST as the Large Data condition for the MT 2002 evaluation.", "labels": [], "entities": [{"text": "NIST", "start_pos": 56, "end_pos": 60, "type": "DATASET", "confidence": 0.9661003947257996}, {"text": "MT 2002", "start_pos": 97, "end_pos": 104, "type": "TASK", "confidence": 0.6902918815612793}]}, {"text": "The Chinese sentences are segmented into words.", "labels": [], "entities": []}, {"text": "The training data contains 23.7 million Chinese and 25.3 million English words.", "labels": [], "entities": []}, {"text": "Experimental results are presented in and Table 2.", "labels": [], "entities": []}, {"text": "shows the effect of the unigram threshold.", "labels": [], "entities": [{"text": "unigram threshold", "start_pos": 24, "end_pos": 41, "type": "METRIC", "confidence": 0.9111650288105011}]}, {"text": "The second column shows the number of blocks selected.", "labels": [], "entities": []}, {"text": "The third column reports the BLEU score () along with 95% confidence interval.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 29, "end_pos": 39, "type": "METRIC", "confidence": 0.9823430776596069}, {"text": "confidence interval", "start_pos": 58, "end_pos": 77, "type": "METRIC", "confidence": 0.9161655306816101}]}, {"text": "We use IBM  Model 1 as a baseline model which is similar to our block model: neither model uses distortion or alignment probabilities.", "labels": [], "entities": [{"text": "IBM  Model 1", "start_pos": 7, "end_pos": 19, "type": "DATASET", "confidence": 0.8840729395548502}]}, {"text": "The best results are obtained for the N 2 and the N 3 sets.", "labels": [], "entities": []}, {"text": "The N 3 set uses only 1.22 million blocks in contrast to N 2 which has 4.23 million blocks.", "labels": [], "entities": []}, {"text": "This indicates that the number of blocks can be reduced drastically without affecting the translation performance significantly.", "labels": [], "entities": []}, {"text": "shows the effect of the maximum phrase length on the BLEU score for the N 2 block set.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 53, "end_pos": 63, "type": "METRIC", "confidence": 0.9785487055778503}]}, {"text": "Including blocks with longer phrases actually helps to improve performance, although length 4 already obtains good results.", "labels": [], "entities": []}, {"text": "We also ran the N2 on the June 2002 DARPA TIDES Large Data evaluation test set.", "labels": [], "entities": [{"text": "DARPA TIDES Large Data evaluation test set", "start_pos": 36, "end_pos": 78, "type": "DATASET", "confidence": 0.8280382667269025}]}, {"text": "Six research sites and four commercial off-the-shelf systems were evaluated in Large Data track.", "labels": [], "entities": [{"text": "Large Data track", "start_pos": 79, "end_pos": 95, "type": "DATASET", "confidence": 0.7330475250879923}]}, {"text": "A majority of the systems were phrasebased translation systems.", "labels": [], "entities": [{"text": "phrasebased translation", "start_pos": 31, "end_pos": 54, "type": "TASK", "confidence": 0.7611004412174225}]}, {"text": "For comparison with other sites, we quote the NIST score) on this test set: N2 system scores 7.44 whereas the official top two systems scored 7.65 and 7.34 respectively.", "labels": [], "entities": [{"text": "NIST score", "start_pos": 46, "end_pos": 56, "type": "DATASET", "confidence": 0.8796030580997467}]}], "tableCaptions": [{"text": " Table 1: Effect of the unigram threshold on the BLEU  score. The maximum phrase length is 8.", "labels": [], "entities": [{"text": "BLEU  score", "start_pos": 49, "end_pos": 60, "type": "METRIC", "confidence": 0.9756535887718201}]}, {"text": " Table 2: Effect of the maximum phrase length on the  BLEU score. The unigram threshold is N (b) \u2265 2.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 54, "end_pos": 64, "type": "METRIC", "confidence": 0.9746487140655518}]}]}