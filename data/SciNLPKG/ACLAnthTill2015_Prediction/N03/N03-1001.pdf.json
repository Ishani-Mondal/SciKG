{"title": [{"text": "Effective Utterance Classification with Unsupervised Phonotactic Models", "labels": [], "entities": [{"text": "Effective Utterance Classification", "start_pos": 0, "end_pos": 34, "type": "TASK", "confidence": 0.7108057339986166}]}], "abstractContent": [{"text": "This paper describes a method for utterance classification that does not require manual transcription of training data.", "labels": [], "entities": [{"text": "utterance classification", "start_pos": 34, "end_pos": 58, "type": "TASK", "confidence": 0.9518518447875977}]}, {"text": "The method combines domain independent acoustic models with off-the-shelf classifiers to give utterance classification performance that is surprisingly close to what can be achieved using conventional word-trigram recognition requiring manual transcription.", "labels": [], "entities": [{"text": "utterance classification", "start_pos": 94, "end_pos": 118, "type": "TASK", "confidence": 0.8232798278331757}, {"text": "word-trigram recognition", "start_pos": 201, "end_pos": 225, "type": "TASK", "confidence": 0.7168879359960556}]}, {"text": "In our method, unsupervised training is first used to train a phone n-gram model fora particular domain; the output of recognition with this model is then passed to a phone-string classifier.", "labels": [], "entities": []}, {"text": "The classification accuracy of the method is evaluated on three different spoken language system domains.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 19, "end_pos": 27, "type": "METRIC", "confidence": 0.9574658274650574}]}], "introductionContent": [{"text": "A major bottleneck in building data-driven speech processing applications is the need to manually transcribe training utterances into words.", "labels": [], "entities": []}, {"text": "The resulting corpus of transcribed word strings is then used to train applicationspecific language models for speech recognition, and in some cases also to train the natural language components of the application.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 111, "end_pos": 129, "type": "TASK", "confidence": 0.7550847828388214}]}, {"text": "Some of these speech processing applications make use of utterance classification, for example when assigning a call destination to naturally spoken user utterances (), or as an initial step in converting speech to actions in spoken interfaces.", "labels": [], "entities": [{"text": "utterance classification", "start_pos": 57, "end_pos": 81, "type": "TASK", "confidence": 0.8155820071697235}]}, {"text": "In this paper we present an approach to utterance classification that avoids the manual effort of transcribing training utterances into word strings.", "labels": [], "entities": [{"text": "utterance classification", "start_pos": 40, "end_pos": 64, "type": "TASK", "confidence": 0.9365033805370331}]}, {"text": "Instead, only the desired utterance class needs to be associated with each sample utterance.", "labels": [], "entities": []}, {"text": "The method combines automatic training of application-specific phonotactic models together with token sequence classifiers.", "labels": [], "entities": []}, {"text": "The accuracy of this phone-string utterance classification method turns out to be surprisingly close to what can be achieved by conventional methods involving word-trigram language models that require manual transcription.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.9989438652992249}, {"text": "phone-string utterance classification", "start_pos": 21, "end_pos": 58, "type": "TASK", "confidence": 0.6847830613454183}]}, {"text": "To quantify this, we present empirical accuracy results from three different call-routing applications comparing our method with conventional utterance classification using word-trigram recognition.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 39, "end_pos": 47, "type": "METRIC", "confidence": 0.9847401976585388}, {"text": "utterance classification", "start_pos": 142, "end_pos": 166, "type": "TASK", "confidence": 0.748489648103714}, {"text": "word-trigram recognition", "start_pos": 173, "end_pos": 197, "type": "TASK", "confidence": 0.7212885022163391}]}, {"text": "Previous work at AT&T on utterance classification without words used information theoretic metrics to discover \"acoustic morphemes\" from untranscribed utterances paired with routing destinations ().", "labels": [], "entities": [{"text": "utterance classification", "start_pos": 25, "end_pos": 49, "type": "TASK", "confidence": 0.8511062264442444}]}, {"text": "However, that approach has so far proved impractical: the major obstacle to practical utility was the low runtime detection rate of acoustic morphemes discovered during training.", "labels": [], "entities": []}, {"text": "This led to a high false rejection rate (between 40% and 50% for 1-best recognition output) when a word-based classification algorithm (the one described by ) was applied to the detected sequence of acoustic morphemes.", "labels": [], "entities": [{"text": "false rejection rate", "start_pos": 19, "end_pos": 39, "type": "METRIC", "confidence": 0.7539267937342325}, {"text": "word-based classification", "start_pos": 99, "end_pos": 124, "type": "TASK", "confidence": 0.6949302405118942}]}, {"text": "More generally, previous work using phone string (or phone-lattice) recognition has concentrated on tasks involving retrieval of audio or video ().", "labels": [], "entities": [{"text": "phone-lattice) recognition", "start_pos": 53, "end_pos": 79, "type": "TASK", "confidence": 0.6484000484148661}]}, {"text": "In those tasks, performance of phone-based systems was not comparable to the accuracy obtainable from wordbased systems, but rather the rationale was avoiding the difficulty of building wide coverage statistical language models for handling the wide range of subject matter that atypical retrieval system, such as a system for retrieving news clips, needs to cover.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 77, "end_pos": 85, "type": "METRIC", "confidence": 0.9952019453048706}]}, {"text": "In the work presented here, the task is somewhat different: the system can automatically learn to identify and act on relatively short phone subsequences that are specific to the speech in a limited domain of discourse, resulting in task accuracy that is comparable to word-based methods.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 238, "end_pos": 246, "type": "METRIC", "confidence": 0.981462836265564}]}, {"text": "In section 2 we describe the utterance classification method.", "labels": [], "entities": [{"text": "utterance classification", "start_pos": 29, "end_pos": 53, "type": "TASK", "confidence": 0.9231550693511963}]}, {"text": "Section 3 describes the experimental setup and the data sets used in the experiments.", "labels": [], "entities": []}, {"text": "Section 4 presents the main comparison of the performance of the method against a \"conventional\" approach using manual transcription and word-based models.", "labels": [], "entities": []}, {"text": "Section 5 gives some concluding remarks.", "labels": [], "entities": []}], "datasetContent": [{"text": "Three experimental conditions are considered.", "labels": [], "entities": []}, {"text": "The suffixes (M and H) in the condition names refer to whether the two training phases (i.e. training for recognition and classification respectively) use inputs produced by machine (M) or human (H) processing.", "labels": [], "entities": [{"text": "recognition and classification", "start_pos": 106, "end_pos": 136, "type": "TASK", "confidence": 0.6999294062455496}]}, {"text": "PhonesMM This experimental condition is the method described in this paper, so no human transcriptions are used.", "labels": [], "entities": []}, {"text": "Unsupervised training from the training speech files is used to build a phone recognition model.", "labels": [], "entities": [{"text": "phone recognition", "start_pos": 72, "end_pos": 89, "type": "TASK", "confidence": 0.877726286649704}]}, {"text": "The classifier is trained on the phone strings resulting from recognizing the training speech files with this model.", "labels": [], "entities": []}, {"text": "At runtime, the classifier is applied to the results of recognizing the test files with this model.", "labels": [], "entities": []}, {"text": "The initial recogition model for the unsupervised recognition training process was an unweighted phone loop.", "labels": [], "entities": [{"text": "unsupervised recognition training process", "start_pos": 37, "end_pos": 78, "type": "TASK", "confidence": 0.7459350228309631}]}, {"text": "The final n-gram order used in the recognition training procedure (N max in section 2) was 5.", "labels": [], "entities": [{"text": "recognition training", "start_pos": 35, "end_pos": 55, "type": "TASK", "confidence": 0.9494567215442657}]}, {"text": "For all three conditions, median recognition and classification time for test data was less than real time (i.e. the duration of test speech files) on current micro-processors.", "labels": [], "entities": []}, {"text": "As noted earlier, the acoustic model, the number of boosting rounds, and the use of prompts as an additional classification feature, are the same for all experimental conditions.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Phone-based and word-based utterance classifi- cation accuracy for domain A", "labels": [], "entities": [{"text": "word-based utterance classifi- cation accuracy", "start_pos": 26, "end_pos": 72, "type": "TASK", "confidence": 0.5887948274612427}]}, {"text": " Table 3: Phone-based and word-based utterance classifi- cation accuracy for domain B", "labels": [], "entities": [{"text": "word-based utterance classifi- cation accuracy", "start_pos": 26, "end_pos": 72, "type": "TASK", "confidence": 0.6118521938721339}]}, {"text": " Table 4: Phone-based and word-based utterance classifi- cation accuracy for domain C", "labels": [], "entities": [{"text": "word-based utterance classifi- cation accuracy", "start_pos": 26, "end_pos": 72, "type": "TASK", "confidence": 0.5975870837767919}]}, {"text": " Table 5: Phone recognition accuracy and phone string  classification accuracy (PhoneMM with no rejection) for  increasing values of N max for domain A.", "labels": [], "entities": [{"text": "Phone recognition", "start_pos": 10, "end_pos": 27, "type": "TASK", "confidence": 0.8662567138671875}, {"text": "accuracy", "start_pos": 28, "end_pos": 36, "type": "METRIC", "confidence": 0.9543702602386475}, {"text": "phone string  classification", "start_pos": 41, "end_pos": 69, "type": "TASK", "confidence": 0.6232283810774485}, {"text": "accuracy", "start_pos": 70, "end_pos": 78, "type": "METRIC", "confidence": 0.875342071056366}]}, {"text": " Table 6: Phone recognition accuracy and phone string  classification accuracy (PhoneMM with no rejection) for  increasing values of N max for domain B.", "labels": [], "entities": [{"text": "Phone recognition", "start_pos": 10, "end_pos": 27, "type": "TASK", "confidence": 0.8656204640865326}, {"text": "accuracy", "start_pos": 28, "end_pos": 36, "type": "METRIC", "confidence": 0.9582398533821106}, {"text": "phone string  classification", "start_pos": 41, "end_pos": 69, "type": "TASK", "confidence": 0.6256751616795858}, {"text": "accuracy", "start_pos": 70, "end_pos": 78, "type": "METRIC", "confidence": 0.8752037286758423}]}, {"text": " Table 7: Phone recognition accuracy and phone string  classification accuracy (PhoneMM with no rejection) for  increasing values of N max for domain C.", "labels": [], "entities": [{"text": "Phone recognition", "start_pos": 10, "end_pos": 27, "type": "TASK", "confidence": 0.8644539415836334}, {"text": "accuracy", "start_pos": 28, "end_pos": 36, "type": "METRIC", "confidence": 0.9574142694473267}, {"text": "phone string  classification", "start_pos": 41, "end_pos": 69, "type": "TASK", "confidence": 0.6250466008981069}, {"text": "accuracy", "start_pos": 70, "end_pos": 78, "type": "METRIC", "confidence": 0.8782060146331787}]}]}