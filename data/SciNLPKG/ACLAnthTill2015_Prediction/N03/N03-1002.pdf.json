{"title": [{"text": "Japanese Named Entity Extraction with Redundant Morphological Analysis", "labels": [], "entities": [{"text": "Japanese Named Entity Extraction", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.7165089845657349}, {"text": "Redundant Morphological Analysis", "start_pos": 38, "end_pos": 70, "type": "TASK", "confidence": 0.7049481272697449}]}], "abstractContent": [{"text": "Named Entity (NE) extraction is an important subtask of document processing such as information extraction and question answering.", "labels": [], "entities": [{"text": "Named Entity (NE) extraction", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.5927589237689972}, {"text": "information extraction", "start_pos": 84, "end_pos": 106, "type": "TASK", "confidence": 0.8535836935043335}, {"text": "question answering", "start_pos": 111, "end_pos": 129, "type": "TASK", "confidence": 0.8583272993564606}]}, {"text": "A typical method used for NE extraction of Japanese texts is a cascade of morphological analysis, POS tagging and chunking.", "labels": [], "entities": [{"text": "NE extraction of Japanese texts", "start_pos": 26, "end_pos": 57, "type": "TASK", "confidence": 0.9504363417625428}, {"text": "POS tagging", "start_pos": 98, "end_pos": 109, "type": "TASK", "confidence": 0.8323200345039368}]}, {"text": "However, there are some cases where segmentation gran-ularity contradicts the results of morphological analysis and the building units of NEs, so that extraction of some NEs are inherently impossible in this setting.", "labels": [], "entities": []}, {"text": "To cope with the unit problem, we propose a character-based chunk-ing method.", "labels": [], "entities": []}, {"text": "Firstly, the input sentence is analyzed redundantly by a statistical morphological analyzer to produce multiple (n-best) answers.", "labels": [], "entities": []}, {"text": "Then, each character is annotated with its character types and its possible POS tags of the top n-best answers.", "labels": [], "entities": [{"text": "POS", "start_pos": 76, "end_pos": 79, "type": "METRIC", "confidence": 0.9004877209663391}]}, {"text": "Finally, a support vector machine-based chunker picks up some portions of the input sentence as NEs.", "labels": [], "entities": []}, {"text": "This method introduces richer information to the chunker than previous methods that base on a single morphological analysis result.", "labels": [], "entities": []}, {"text": "We apply our method to IREX NE extraction task.", "labels": [], "entities": [{"text": "IREX NE extraction", "start_pos": 23, "end_pos": 41, "type": "TASK", "confidence": 0.6362535754839579}]}, {"text": "The cross validation result of the F-measure being 87.2 shows the superiority and effectiveness of the method.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 35, "end_pos": 44, "type": "METRIC", "confidence": 0.9858907461166382}]}], "introductionContent": [{"text": "Named Entity (NE) extraction aims at identifying proper nouns and numerical expressions in a text, such as persons, locations, organizations, dates, and soon.", "labels": [], "entities": [{"text": "Named Entity (NE) extraction", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.6564642588297526}]}, {"text": "This is an important subtask of document processing like information extraction and question answering.", "labels": [], "entities": [{"text": "document processing", "start_pos": 32, "end_pos": 51, "type": "TASK", "confidence": 0.7464064955711365}, {"text": "information extraction", "start_pos": 57, "end_pos": 79, "type": "TASK", "confidence": 0.8283584117889404}, {"text": "question answering", "start_pos": 84, "end_pos": 102, "type": "TASK", "confidence": 0.8754397332668304}]}, {"text": "A common standard data set for Japanese NE extraction is provided by IREX workshop).", "labels": [], "entities": [{"text": "Japanese NE extraction", "start_pos": 31, "end_pos": 53, "type": "TASK", "confidence": 0.7019615570704142}, {"text": "IREX workshop", "start_pos": 69, "end_pos": 82, "type": "DATASET", "confidence": 0.9506517052650452}]}, {"text": "Generally, Japanese NE extraction is done in the following steps: Firstly, a Japanese text is segmented into words and is annotated with POS tags by a morphological analyzer.", "labels": [], "entities": [{"text": "Japanese NE extraction", "start_pos": 11, "end_pos": 33, "type": "TASK", "confidence": 0.7089915672938029}]}, {"text": "Then, a chunker brings together the words into NE chunks based on contextual information.", "labels": [], "entities": []}, {"text": "However, such a straightforward method cannot extract NEs whose segmentation boundary contradicts that of morphological analysis outputs.", "labels": [], "entities": []}, {"text": "\" (\"September\") as a date will be extracted by combining word units.", "labels": [], "entities": []}, {"text": "On the other hand, \"# \" (abbreviation of North Korea) cannot be extracted as a name of location because it is contained by the word unit \"!", "labels": [], "entities": []}, {"text": "\u00a4 # \" (visiting North Korea).", "labels": [], "entities": []}, {"text": "illustrates the example with English translation.", "labels": [], "entities": []}, {"text": "Some previous works try to cope with the word unit problem: Uchimoto () introduces transformation rules to modify the word units given by a morphological analyzer.) controls the parameters of a statistical morphological analyzer so as to produce more fine-grained output.", "labels": [], "entities": []}, {"text": "These method are used as a preprocessing of chunking.", "labels": [], "entities": []}, {"text": "By contrast, we propose more straightforward method in which we perform the chunking process based on character units.", "labels": [], "entities": []}, {"text": "Each character receives annotations with character type and multiple POS information of the words found by a morphological analyzer.", "labels": [], "entities": [{"text": "POS", "start_pos": 69, "end_pos": 72, "type": "METRIC", "confidence": 0.9739155173301697}]}, {"text": "We make use of redundant outputs of the morphological analysis as the base features for the chunker to introduce more informationrich features.", "labels": [], "entities": []}, {"text": "We use a support vector machine (SVM)-based chunker yamcha () for the chunking process.", "labels": [], "entities": []}, {"text": "Our method achieves better score than all the systems reported previously for IREX NE extraction task.", "labels": [], "entities": [{"text": "IREX NE extraction task", "start_pos": 78, "end_pos": 101, "type": "TASK", "confidence": 0.7163732871413231}]}, {"text": "Section 2 presents the IREX NE extraction task.", "labels": [], "entities": [{"text": "IREX NE extraction", "start_pos": 23, "end_pos": 41, "type": "TASK", "confidence": 0.7097468177477518}]}, {"text": "Section 3 describes our method in detail.", "labels": [], "entities": []}, {"text": "In section 4, we show the results of experiments, and finally we give conclusions in section 5.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 4: The length of contextual feature and the extraction accuracy", "labels": [], "entities": []}, {"text": " Table 8: The thesaurus and the extraction accuracy", "labels": [], "entities": []}, {"text": " Table 5: The depth of redundant analysis and the extraction accuracy", "labels": [], "entities": []}, {"text": " Table 9: The best model and the extraction accuracy", "labels": [], "entities": []}, {"text": " Table 6: The feature set and the extraction accuracy", "labels": [], "entities": []}, {"text": " Table 7: The degree of polynomial kernel function and the extraction accuracy", "labels": [], "entities": []}, {"text": " Table 10: Comparison with related works", "labels": [], "entities": []}]}