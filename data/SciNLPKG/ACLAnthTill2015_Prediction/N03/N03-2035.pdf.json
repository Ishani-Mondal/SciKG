{"title": [{"text": "A Context-Sensitive Homograph Disambiguation in Thai Text-to-Speech Synthesis", "labels": [], "entities": [{"text": "Context-Sensitive Homograph Disambiguation", "start_pos": 2, "end_pos": 44, "type": "TASK", "confidence": 0.6320578555266062}, {"text": "Thai Text-to-Speech Synthesis", "start_pos": 48, "end_pos": 77, "type": "TASK", "confidence": 0.7377074758211771}]}], "abstractContent": [{"text": "Homograph ambiguity is an original issue in Text-to-Speech (TTS).", "labels": [], "entities": [{"text": "Homograph ambiguity", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.9028013944625854}, {"text": "Text-to-Speech (TTS)", "start_pos": 44, "end_pos": 64, "type": "TASK", "confidence": 0.623542346060276}]}, {"text": "To disambiguate homograph, several efficient approaches have been proposed such as part-of-speech (POS) n-gram, Bayesian classifier, decision tree, and Bayesian-hybrid approaches.", "labels": [], "entities": []}, {"text": "These methods need words or/and POS tags surrounding the question homographs in disambiguation.", "labels": [], "entities": []}, {"text": "Some languages such as Thai, Chinese, and Japanese have no word-boundary delimiter.", "labels": [], "entities": []}, {"text": "Therefore before solving homograph ambiguity , we need to identify word boundaries.", "labels": [], "entities": []}, {"text": "In this paper, we propose a unique framework that solves both word segmentation and homograph ambiguity problems altogether.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 62, "end_pos": 79, "type": "TASK", "confidence": 0.7435969114303589}]}, {"text": "Our model employs both local and long-distance contexts, which are automatically extracted by a machine learning technique called Winnow.", "labels": [], "entities": []}], "introductionContent": [{"text": "In traditional Thai TTS, it consists of four main modules: word segmentation, grapheme-to-phoneme, prosody generation, and speech signal processing.", "labels": [], "entities": [{"text": "Thai TTS", "start_pos": 15, "end_pos": 23, "type": "TASK", "confidence": 0.49331551790237427}, {"text": "word segmentation", "start_pos": 59, "end_pos": 76, "type": "TASK", "confidence": 0.7668609023094177}, {"text": "prosody generation", "start_pos": 99, "end_pos": 117, "type": "TASK", "confidence": 0.8182432651519775}, {"text": "speech signal processing", "start_pos": 123, "end_pos": 147, "type": "TASK", "confidence": 0.6860716541608175}]}, {"text": "The accuracy of pronunciation in Thai TTS mainly depends on accuracies of two modules: word segmentation, and grapheme-to-phoneme.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.9980613589286804}, {"text": "Thai TTS", "start_pos": 33, "end_pos": 41, "type": "TASK", "confidence": 0.5435413718223572}, {"text": "word segmentation", "start_pos": 87, "end_pos": 104, "type": "TASK", "confidence": 0.6839577108621597}]}, {"text": "In word segmentation process, if word boundaries cannot be identified correctly, it leads Thai TTS to the incorrect pronunciation such as a string \"\u0e15\u0e32\u0e01\u0e25\u0e21\" which can be separated into two different ways with different meanings and pronunciations.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 3, "end_pos": 20, "type": "TASK", "confidence": 0.7577339708805084}, {"text": "Thai TTS", "start_pos": 90, "end_pos": 98, "type": "TASK", "confidence": 0.6702643930912018}]}, {"text": "The first one is \"\u0e15\u0e32(eye) \u0e01\u0e25\u0e21(round)\", pronounced [ta:0 klom0] and the other one is \"\u0e15\u0e32\u0e01(expose) \u0e25\u0e21(wind)\", pronounced [ta:k1 lom0].", "labels": [], "entities": []}, {"text": "In grapheme-to-phoneme module, it may produce error pronunciations fora homograph which can be pronounced more than one way such as a word \"\u0e40\u0e1e\u0e25\u0e32\" which can be pronounced or [phe:0 la:0].", "labels": [], "entities": []}, {"text": "Therefore, to improve an accuracy of Thai TTS, we have to focus on solving the problems of word boundary ambiguity and homograph ambiguity which can be viewed as a disambiguation task.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 25, "end_pos": 33, "type": "METRIC", "confidence": 0.9993199110031128}, {"text": "Thai TTS", "start_pos": 37, "end_pos": 45, "type": "TASK", "confidence": 0.6160719692707062}]}, {"text": "A number of feature-based methods have been tried for several disambiguation tasks in NLP, including decision lists, Bayesian hybrids, and Winnow.", "labels": [], "entities": []}, {"text": "These methods are superior to the previously proposed methods in that they can combine evidence from various sources in disambiguation.", "labels": [], "entities": []}, {"text": "To apply the methods in our task, we treat problems of word boundary and homograph ambiguity as a task of word pronunciation disambiguation.", "labels": [], "entities": [{"text": "word pronunciation disambiguation", "start_pos": 106, "end_pos": 139, "type": "TASK", "confidence": 0.8006713191668192}]}, {"text": "This task is to decide using the context which was actually intended.", "labels": [], "entities": []}, {"text": "Instead of using only one type of syntactic evidence as in N-gram approaches, we employ the synergy of several types of features.", "labels": [], "entities": []}, {"text": "Following previous works, we adopted two types of features: context words, and collections.", "labels": [], "entities": []}, {"text": "Context-word feature is used to test for the presence of a particular word within +/-K words of the target word and collocation test fora pattern of up to L contiguous words and/or part-of-speech tags surrounding the target word.", "labels": [], "entities": []}, {"text": "To automatically extract the discriminative features from feature space and to combine them in disambiguation, we have to investigate an efficient technique in our task.", "labels": [], "entities": []}, {"text": "The problem becomes how to select and combine various kinds of features.", "labels": [], "entities": []}, {"text": "Yarowsky proposed decision list as away to pool several types of features, and to solve the target problem by applying a single strongest feature, whatever type it is.", "labels": [], "entities": []}, {"text": "Golding proposed a Bayesian hybrid method to take into account all available evidence, instead of only the strongest one.", "labels": [], "entities": []}, {"text": "The method was applied to the task of context-sentitive spelling correction and was reported to be superior to decision lists.", "labels": [], "entities": [{"text": "context-sentitive spelling correction", "start_pos": 38, "end_pos": 75, "type": "TASK", "confidence": 0.6565775374571482}]}, {"text": "Later, Golding and Roth applied Winnow algorithm in the same task and found that the algorithm performs comparably to the Bayesian hybrid method when using pruned feature sets, and is better when using unpruned sets or unfamiliar test set.", "labels": [], "entities": []}, {"text": "In this paper, we propose a unified framework in solving the problems of word boundary ambiguity and homograph ambiguity altogether.", "labels": [], "entities": []}, {"text": "Our approach employs both local and long-distance contexts, which can be automatically extracted by a machine learning technique.", "labels": [], "entities": []}, {"text": "In this task, we employ the machine learning technique called Winnow.", "labels": [], "entities": []}, {"text": "We then construct our system based on the algorithm and evaluate them by comparing with other existing approaches to Thai homograph problems.", "labels": [], "entities": []}], "datasetContent": [{"text": "To test the performance of the different approaches, we select sentences containing Thai homographs and boundary ambiguity strings from our 25K-words corpus to use in benchmark tests.", "labels": [], "entities": []}, {"text": "Every sentence is manually separated into words.", "labels": [], "entities": []}, {"text": "Their parts of speech and pronunciations are manually tagged by linguists.", "labels": [], "entities": []}, {"text": "The resulting corpus is divided into two parts; the first part, about 80% of corpus, is utilized for training and the rest is used for testing.", "labels": [], "entities": []}, {"text": "In the experiment, we classify the data into three group depending on types of text ambiguity according to section 2: CDSA, CISA and Homograph, and compare the results from different approaches; Winnow, Bayseian hybrid and POS trigram.", "labels": [], "entities": []}], "tableCaptions": []}