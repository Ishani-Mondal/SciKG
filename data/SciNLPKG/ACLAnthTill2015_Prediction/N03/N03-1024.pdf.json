{"title": [{"text": "Syntax-based Alignment of Multiple Translations: Extracting Paraphrases and Generating New Sentences", "labels": [], "entities": [{"text": "Syntax-based Alignment of Multiple Translations", "start_pos": 0, "end_pos": 47, "type": "TASK", "confidence": 0.8487576961517334}]}], "abstractContent": [{"text": "We describe a syntax-based algorithm that automatically builds Finite State Automata (word lattices) from semantically equivalent translation sets.", "labels": [], "entities": []}, {"text": "These FSAs are good representations of paraphrases.", "labels": [], "entities": [{"text": "FSAs", "start_pos": 6, "end_pos": 10, "type": "DATASET", "confidence": 0.6621220111846924}]}, {"text": "They can be used to extract lexical and syntactic paraphrase pairs and to generate new, unseen sentences that express the same meaning as the sentences in the input sets.", "labels": [], "entities": []}, {"text": "Our FSAs can also predict the correctness of alternative semantic renderings, which maybe used to evaluate the quality of translations.", "labels": [], "entities": [{"text": "FSAs", "start_pos": 4, "end_pos": 8, "type": "DATASET", "confidence": 0.7741554379463196}]}], "introductionContent": [{"text": "In the past, paraphrases have come under the scrutiny of many research communities.", "labels": [], "entities": [{"text": "paraphrases", "start_pos": 13, "end_pos": 24, "type": "TASK", "confidence": 0.9588962197303772}]}, {"text": "Information retrieval researchers have used paraphrasing techniques for query reformulation in order to increase the recall of information retrieval engines.", "labels": [], "entities": [{"text": "Information retrieval", "start_pos": 0, "end_pos": 21, "type": "TASK", "confidence": 0.7415046393871307}, {"text": "query reformulation", "start_pos": 72, "end_pos": 91, "type": "TASK", "confidence": 0.7291347086429596}, {"text": "recall", "start_pos": 117, "end_pos": 123, "type": "METRIC", "confidence": 0.9952253103256226}]}, {"text": "Natural language generation researchers have used paraphrasing to increase the expressive power of generation systems ().", "labels": [], "entities": [{"text": "Natural language generation", "start_pos": 0, "end_pos": 27, "type": "TASK", "confidence": 0.6416109999020895}]}, {"text": "And researchers in multi-document text summarization (), information extraction (), and question answering () have focused on identifying and exploiting paraphrases in the context of recognizing redundancies, alternative formulations of the same meaning, and improving the performance of question answering systems.", "labels": [], "entities": [{"text": "multi-document text summarization", "start_pos": 19, "end_pos": 52, "type": "TASK", "confidence": 0.65836434563001}, {"text": "information extraction", "start_pos": 57, "end_pos": 79, "type": "TASK", "confidence": 0.7993756234645844}, {"text": "question answering", "start_pos": 88, "end_pos": 106, "type": "TASK", "confidence": 0.9177293479442596}, {"text": "question answering", "start_pos": 288, "end_pos": 306, "type": "TASK", "confidence": 0.8030506372451782}]}, {"text": "In previous work (), paraphrases are represented as sets or pairs of semantically equivalent words, phrases, and patterns.", "labels": [], "entities": []}, {"text": "Although this is adequate in the context of some applications, it is clearly too weak from a generative perspective.", "labels": [], "entities": []}, {"text": "Assume, for example, that we know that text pairs (stock market rose, stock market gained) and (stock market rose, stock prices rose) have the same meaning.", "labels": [], "entities": []}, {"text": "If we memorized only these two pairs, it would be impossible to infer that, in fact, consistent with our intuition, any of the following sets of phrases are also semantically equivalent: {stock market rose, stock market gained, stock prices rose, stock prices gained } and {stock market, stock prices } in the context of rose or gained; {market rose }, {market gained }, {prices rose } and {prices gained } in the context of stock; and soon.", "labels": [], "entities": []}, {"text": "In this paper, we propose solutions for two problems: the problem of paraphrase representation and the problem of paraphrase induction.", "labels": [], "entities": [{"text": "paraphrase representation", "start_pos": 69, "end_pos": 94, "type": "TASK", "confidence": 0.8724553287029266}, {"text": "paraphrase induction", "start_pos": 114, "end_pos": 134, "type": "TASK", "confidence": 0.8497950732707977}]}, {"text": "We propose anew, finite-statebased representation of paraphrases that enables one to encode compactly large numbers of paraphrases.", "labels": [], "entities": []}, {"text": "We also propose algorithms that automatically derive such representations from inputs that are now routinely released in conjunction with large scale machine translation evaluations: multiple English translations of many foreign language texts.", "labels": [], "entities": [{"text": "machine translation evaluations", "start_pos": 150, "end_pos": 181, "type": "TASK", "confidence": 0.7992548743883768}]}, {"text": "For instance, when given as input the 11 semantically equivalent English translations in, our algorithm automatically induces the FSA in, which represents compactly 49 distinct renderings of the same semantic meaning.", "labels": [], "entities": [{"text": "FSA", "start_pos": 130, "end_pos": 133, "type": "METRIC", "confidence": 0.9928779006004333}]}, {"text": "Our FSAs capture both lexical paraphrases, such as {fighting, battle}, {died, were killed} and structural paraphrases such as {last week's fighting, the battle of last week}.", "labels": [], "entities": [{"text": "FSAs", "start_pos": 4, "end_pos": 8, "type": "DATASET", "confidence": 0.842140257358551}]}, {"text": "The contexts in which these are correct paraphrases are also conveniently captured in the representation.", "labels": [], "entities": []}, {"text": "In previous work, used word lattices for language generation, but their method involved hand-crafted rules. and both applied the technique of multi-sequence alignment (MSA) to align parallel corpora and produced similar FSAs.", "labels": [], "entities": [{"text": "language generation", "start_pos": 41, "end_pos": 60, "type": "TASK", "confidence": 0.7357160001993179}, {"text": "multi-sequence alignment (MSA)", "start_pos": 142, "end_pos": 172, "type": "TASK", "confidence": 0.8158826470375061}]}, {"text": "For their purposes, they mainly need to ensure the correctness of consensus among different translations, so that different constituent orderings in input sentences do not pose a serious prob-  lem.", "labels": [], "entities": []}, {"text": "In contrast, we want to ensure the correctness of all paths represented by the FSAs, and direct application of MSA in the presence of different constituent orderings can be problematic.", "labels": [], "entities": [{"text": "FSAs", "start_pos": 79, "end_pos": 83, "type": "DATASET", "confidence": 0.8928054571151733}]}, {"text": "For example, when given as input the same sentences in, one instantiation of the MSA algorithm produces the FSA in, which contains many \"bad\" paths such as the battle of last week's fighting took at least 12 people lost their people died in the fighting last week's fighting (See Section 4.2.2 fora more quantitative analysis.).", "labels": [], "entities": [{"text": "FSA", "start_pos": 108, "end_pos": 111, "type": "METRIC", "confidence": 0.8459796905517578}]}, {"text": "It's still possible to use MSA if, for example, the input is pre-clustered to have the same constituent ordering ().", "labels": [], "entities": []}, {"text": "But we chose to approach this problem from another direction.", "labels": [], "entities": []}, {"text": "As a result, we propose anew syntax-based algorithm to produce FSAs.", "labels": [], "entities": [{"text": "FSAs", "start_pos": 63, "end_pos": 67, "type": "DATASET", "confidence": 0.5389376282691956}]}, {"text": "In this paper, we first introduce the multiple translation corpus that we use in our experiments (see Section 2).", "labels": [], "entities": [{"text": "multiple translation", "start_pos": 38, "end_pos": 58, "type": "TASK", "confidence": 0.6607367247343063}]}, {"text": "We then present the algorithms that we developed to induce finite-state paraphrase representations from such data (see Section 3).", "labels": [], "entities": []}, {"text": "An important part of the paper is dedicated to evaluating the quality of the finite-state representations that we derive (see Section 4).", "labels": [], "entities": []}, {"text": "Since our representations encode thousands and sometimes millions of equivalent verbalizations of the same meaning, we use both manual and automatic evaluation techniques.", "labels": [], "entities": []}, {"text": "Some of the automatic evaluations we perform are novel as well.", "labels": [], "entities": []}], "datasetContent": [{"text": "The evaluation for our finite state representations and algorithm requires careful examination.", "labels": [], "entities": []}, {"text": "Obviously, what counts as a good result largely depends on the application one has in mind.", "labels": [], "entities": []}, {"text": "If we are extracting paraphrases for question-reformulation, it doesn't really matter if we output a few syntactically incorrect paraphrases, as long as we produce a large number of semantically correct ones.", "labels": [], "entities": []}, {"text": "If we want to use the FSA for MT evaluation (for example, comparing a sentence to be evaluated with the possible paths in FSA), we would want all paths to be relatively good (which we will focus on in this paper), while in some other applications, we may only care about the quality of the best path (not addressed in this paper).", "labels": [], "entities": [{"text": "FSA", "start_pos": 22, "end_pos": 25, "type": "DATASET", "confidence": 0.776422381401062}, {"text": "MT evaluation", "start_pos": 30, "end_pos": 43, "type": "TASK", "confidence": 0.9519437253475189}]}, {"text": "Section 4.1 concentrates on evaluating the paraphrase pairs that can be extracted from the FSAs built by our system, while Section 4.2 is dedicated to evaluating the FSAs directly.", "labels": [], "entities": [{"text": "FSAs", "start_pos": 166, "end_pos": 170, "type": "DATASET", "confidence": 0.7373729944229126}]}, {"text": "By construction, different paths between any two nodes in the FSA representations that we derive are paraphrases (in the context in which the nodes occur).", "labels": [], "entities": [{"text": "FSA representations", "start_pos": 62, "end_pos": 81, "type": "DATASET", "confidence": 0.8427503705024719}]}, {"text": "To evaluate our algorithm, we extract paraphrases from our FSAs and ask human judges to evaluate their correctness.", "labels": [], "entities": [{"text": "FSAs", "start_pos": 59, "end_pos": 63, "type": "DATASET", "confidence": 0.8570622205734253}]}, {"text": "We compare the paraphrases we collect with paraphrases that are derivable from the same corpus using a cotraining-based paraphrase extraction algorithm ().", "labels": [], "entities": [{"text": "paraphrase extraction", "start_pos": 120, "end_pos": 141, "type": "TASK", "confidence": 0.7106963992118835}]}, {"text": "To the best of our knowledge, this is the most relevant work to compare against since it aims at extracting paraphrase pairs from parallel corpus.", "labels": [], "entities": []}, {"text": "Unlike our syntax-based algorithm which treats a sentence as a tree structure and uses this hierarchical structural information to guide the merging process, their algorithm treats a sentence as a sequence of phrases with surrounding contexts (no hierarchical structure involved) and cotrains classifiers to detect paraphrases and contexts for paraphrases.", "labels": [], "entities": []}, {"text": "It would be interesting to compare the results from two algorithms so different from each other.", "labels": [], "entities": []}, {"text": "For the purpose of this experiment, we randomly selected 300 paraphrase pairs (S syn ) from the FSAs produced by our system.", "labels": [], "entities": [{"text": "FSAs", "start_pos": 96, "end_pos": 100, "type": "DATASET", "confidence": 0.6360791325569153}]}, {"text": "Since the co-training-based algorithm of: A comparison of the correctness of the paraphrases produced by the syntax-based alignment (S syn ) and co-training-based (S cotr ) algorithms.", "labels": [], "entities": []}, {"text": "The resulting 600 paraphrase pairs were mixed and presented in random order to four human judges.", "labels": [], "entities": []}, {"text": "Each judge was asked to assess the correctness of 150 paraphrase pairs (75 pairs from each system) based on the context, i.e., the sentence group, from which the paraphrase pair was extracted.", "labels": [], "entities": []}, {"text": "Judges were given three choices: \"Correct\", for perfect paraphrases, \"Partially correct\", for paraphrases in which there is only a partial overlap between the meaning of two paraphrases (e.g. while {saving set, aid package} is a correct paraphrase pair in the given context, {set, aide package} is considered partially correct), and \"Incorrect\".", "labels": [], "entities": [{"text": "Correct", "start_pos": 34, "end_pos": 41, "type": "METRIC", "confidence": 0.9801830053329468}, {"text": "Incorrect", "start_pos": 334, "end_pos": 343, "type": "METRIC", "confidence": 0.9507083892822266}]}, {"text": "The results of the evaluation are presented in.", "labels": [], "entities": []}, {"text": "Although the four evaluators were judging four different sets, each clearly rated a higher percentage of the outputs produced by the syntax-based alignment algorithm as \"Correct\".", "labels": [], "entities": [{"text": "Correct", "start_pos": 170, "end_pos": 177, "type": "METRIC", "confidence": 0.9754019379615784}]}, {"text": "We should note that there are parameters specific to the co-training algorithm that we did not tune to work for this particular corpus.", "labels": [], "entities": []}, {"text": "In addition, the cotraining algorithm recovered more paraphrase pairs: the syntax-based algorithm extracted 8666 pairs in total with 1051 of them extracted at least twice (i.e. more or less reliable), while the numbers for the co-training algorithm is 2934 out of a total of 16993 pairs.", "labels": [], "entities": []}, {"text": "This means we are not comparing the accuracy on the same recall level.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 36, "end_pos": 44, "type": "METRIC", "confidence": 0.9994736313819885}, {"text": "recall", "start_pos": 57, "end_pos": 63, "type": "METRIC", "confidence": 0.9981387853622437}]}, {"text": "Aside from evaluating the correctness of the paraphrases, we are also interested in the degree of overlap between the paraphrase pairs discovered by the two algorithms so different from each other.", "labels": [], "entities": []}, {"text": "We find that out of the 1051 paraphrase pairs that were extracted from more than one sentence group by the syntax-based algorithm, 62.3% were also extracted by the co-training algorithm; and out of the 2934 paraphrase pairs from the results of co-training algorithm, 33.4% were also extracted by the syntax-based algorithm.", "labels": [], "entities": []}, {"text": "This shows that in spite of the very different cues the two different algorithms rely on, range of ASL 1-10 10-20 20-30 30-45 recall 30.7% 16.3% 7.8% 3.8%: Recall of WordNet-consistent synonyms.", "labels": [], "entities": [{"text": "recall", "start_pos": 126, "end_pos": 132, "type": "METRIC", "confidence": 0.75255286693573}]}, {"text": "they do discover a lot of common pairs.", "labels": [], "entities": []}, {"text": "If we take our claims seriously, each path in our FSAs that connects the start and end nodes should correspond to a well-formed sentence.", "labels": [], "entities": [{"text": "FSAs", "start_pos": 50, "end_pos": 54, "type": "DATASET", "confidence": 0.6705703735351562}]}, {"text": "We are interested in both quantity (how many sentences our automata are able to produce) and quality (how good these sentences are).", "labels": [], "entities": []}, {"text": "To answer the first question, we simply count the number of paths produced by our FSAs.", "labels": [], "entities": [{"text": "FSAs", "start_pos": 82, "end_pos": 86, "type": "DATASET", "confidence": 0.911315381526947}]}, {"text": "1.74259 1.05749: Quality judged by LM gives the statistics on the number of paths produced by our FSAs, reported by the average length of sentences in the input sentence groups.", "labels": [], "entities": []}, {"text": "For example, the sentence groups that have between 10 and 20 words produce, on average, automata that can yield 4468 alternative, semantically equivalent formulations.", "labels": [], "entities": []}, {"text": "Note that if we always get the same degree of merging per word across all sentence groups, the number of paths would tend to increase with the sentence length.", "labels": [], "entities": []}, {"text": "This is not the case here.", "labels": [], "entities": []}, {"text": "Apparently we are getting less merging with longer sentences.", "labels": [], "entities": []}, {"text": "But still, given 11 sentences, we are capable of generating hundreds, thousands, and in some cases even millions of sentences.", "labels": [], "entities": []}, {"text": "Obviously, we should not get too happy with our ability to boost the number of equivalent meanings if they are incorrect.", "labels": [], "entities": []}, {"text": "To assess the quality of the FSAs generated by our algorithm, we use a language model-based metric.", "labels": [], "entities": [{"text": "FSAs generated", "start_pos": 29, "end_pos": 43, "type": "TASK", "confidence": 0.8524190783500671}]}, {"text": "We train a 4-gram model over one year of the Wall Street Journal using the CMU-Cambridge Statistical Language Modeling toolkit (v2).", "labels": [], "entities": [{"text": "Wall Street Journal", "start_pos": 45, "end_pos": 64, "type": "DATASET", "confidence": 0.9737942814826965}]}, {"text": "For each sentence group SG, we use this language model to estimate the average entropy of the 11 original sentences in that group (ent(SG)).", "labels": [], "entities": []}, {"text": "We also compute the average entropy of all the sentences in the corresponding FSA built by our syntax-based algorithm (ent(F SA)).", "labels": [], "entities": []}, {"text": "As the statistics in show, there is little difference between the average entropy of the original sentences and the average entropy of the paraphrase sentences we produce.", "labels": [], "entities": []}, {"text": "To better calibrate this result, we compare it with the average entropy of 6 corresponding machine translation outputs (ent(M T S)), which were also made available by LDC in conjunction with the same corpus.", "labels": [], "entities": []}, {"text": "As one can see, the difference between the average entropy of the machine produced output and the average entropy of the original 11 sentences is much higher than the difference between the average entropy of the FSA-produced outputs and the average entropy of the original 11 sentences.", "labels": [], "entities": [{"text": "FSA-produced", "start_pos": 213, "end_pos": 225, "type": "DATASET", "confidence": 0.763309895992279}]}, {"text": "Obviously, this does not mean that our FSAs only produce well-formed sentences.", "labels": [], "entities": [{"text": "FSAs", "start_pos": 39, "end_pos": 43, "type": "DATASET", "confidence": 0.7518177032470703}]}, {"text": "But it does mean that our FSAs produce sentences that look more like human produced sentences than machine produced ones according to a language model.", "labels": [], "entities": []}, {"text": "Recently, have proposed an automatic MT system evaluation technique (the BLEU score).", "labels": [], "entities": [{"text": "MT system evaluation", "start_pos": 37, "end_pos": 57, "type": "TASK", "confidence": 0.8115909298261007}, {"text": "BLEU score", "start_pos": 73, "end_pos": 83, "type": "METRIC", "confidence": 0.9772163033485413}]}, {"text": "Given an MT system output and a set of refer-range 0-1 1-2 2-3 3-4 4-5 count 546 256 80 15 2: Statistics for ed gain ence translations, one can estimate the \"goodness\" of the MT output by measuring the n-gram overlap between the output and the reference set.", "labels": [], "entities": [{"text": "MT", "start_pos": 9, "end_pos": 11, "type": "TASK", "confidence": 0.9517292380332947}, {"text": "MT", "start_pos": 175, "end_pos": 177, "type": "TASK", "confidence": 0.9675720930099487}]}, {"text": "The higher the overlap, i.e., the closer an output string is to a set of reference translations, the better a translation it is.", "labels": [], "entities": []}, {"text": "We hypothesize that our FSAs provide a better representation against which the outputs of MT systems can be evaluated because they encode not just a few but thousands of equivalent semantic formulations of the desired meaning.", "labels": [], "entities": [{"text": "MT", "start_pos": 90, "end_pos": 92, "type": "TASK", "confidence": 0.9657332301139832}]}, {"text": "Ideally, if the FSAs we build accept all and only the correct renderings of a given meaning, we can just give a test sentence to the reference FSA and see if it is accepted by it.", "labels": [], "entities": [{"text": "FSA", "start_pos": 143, "end_pos": 146, "type": "DATASET", "confidence": 0.8903906345367432}]}, {"text": "Since this is not a realistic expectation, we measure the edit distance between a string and an FSA instead: the smaller this distance is, the closer it is to the meaning represented by the FSA.", "labels": [], "entities": [{"text": "FSA", "start_pos": 190, "end_pos": 193, "type": "DATASET", "confidence": 0.9146876931190491}]}, {"text": "To assess whether our FSAs are more appropriate representations for evaluating the output of MT systems, we perform the following experiment.", "labels": [], "entities": [{"text": "FSAs", "start_pos": 22, "end_pos": 26, "type": "METRIC", "confidence": 0.8553764820098877}, {"text": "MT", "start_pos": 93, "end_pos": 95, "type": "TASK", "confidence": 0.976265013217926}]}, {"text": "For each sentence group, we holdout one sentence as test sentence, and try to evaluate how much of it can be predicted from the other 10 sentences.", "labels": [], "entities": []}, {"text": "We compare two different ways of estimating the predictive power.", "labels": [], "entities": []}, {"text": "(a) we compute the edit distance between the test sentence and the other 10 sentences in the set.", "labels": [], "entities": []}, {"text": "The minimum of this distance is ed(input).", "labels": [], "entities": []}, {"text": "(b) we use dynamic programming to efficiently compute the minimum distance (ed(F SA)) between the test sentence and all the paths in the FSA built from the other 10 sentences.", "labels": [], "entities": [{"text": "ed(F SA))", "start_pos": 76, "end_pos": 85, "type": "METRIC", "confidence": 0.8076098561286926}, {"text": "FSA", "start_pos": 137, "end_pos": 140, "type": "DATASET", "confidence": 0.8557326793670654}]}, {"text": "The smaller the edit distance is, the better we are predicting a test sentence.", "labels": [], "entities": []}, {"text": "Mathematically, the difference between these two measures ed(input) \u2212 ed(F SA) characterizes how much is gained in predictive power by building the FSA.", "labels": [], "entities": [{"text": "F SA)", "start_pos": 73, "end_pos": 78, "type": "METRIC", "confidence": 0.9614925781885783}, {"text": "FSA", "start_pos": 148, "end_pos": 151, "type": "DATASET", "confidence": 0.9419402480125427}]}, {"text": "We carryout the experiment described above in a \"leave-one-out\" fashion (i.e. each sentence serves as a test sentence once).", "labels": [], "entities": []}, {"text": "Now let ed gain be the average of ed(input) \u2212 ed(F SA) over the 11 runs fora given group.", "labels": [], "entities": [{"text": "F SA)", "start_pos": 49, "end_pos": 54, "type": "METRIC", "confidence": 0.8831971089045206}]}, {"text": "We compute this for all 899 groups and find the mean for ed gain to be 0.91 (std. dev = 0.78).", "labels": [], "entities": [{"text": "ed gain", "start_pos": 57, "end_pos": 64, "type": "METRIC", "confidence": 0.9653284251689911}]}, {"text": "gives the count for groups whose ed gain falls into the specified range.", "labels": [], "entities": [{"text": "ed gain", "start_pos": 33, "end_pos": 40, "type": "METRIC", "confidence": 0.900477796792984}]}, {"text": "We can see that the majority of ed gain falls under 2.", "labels": [], "entities": [{"text": "ed gain", "start_pos": 32, "end_pos": 39, "type": "METRIC", "confidence": 0.9589186310768127}]}, {"text": "We are also interested in the relation between the predictive power of the FSAs and the number of reference translations they are derived from.", "labels": [], "entities": [{"text": "FSAs", "start_pos": 75, "end_pos": 79, "type": "DATASET", "confidence": 0.6908464431762695}]}, {"text": "For a given group, we randomly order the sentences in it, set the last one as the test sentence, and try to predict it with the first 1, 2, 3, ...", "labels": [], "entities": []}, {"text": "We investigate whether more sentences ed(F SA n ) ed(input n ) \u2212ed(F SA 10 ) \u2212ed(F SA n ) n mean std.", "labels": [], "entities": []}, {"text": "dev 1: Effect of monotonically increasing the number of reference sentences yield an increase in the predictive power.", "labels": [], "entities": []}, {"text": "Let ed(F SA n ) be the edit distance from the test sentence to the FSA built on the first n sentences; similarly, let ed(input n ) be the minimum edit distance from the test sentence to an input set that consists of only the first n sentences.", "labels": [], "entities": [{"text": "FSA", "start_pos": 67, "end_pos": 70, "type": "DATASET", "confidence": 0.8747552633285522}]}, {"text": "reports the effect of using different number of reference translations.", "labels": [], "entities": []}, {"text": "The first column shows that each translation is contributing to the predictive power of our FSA.", "labels": [], "entities": [{"text": "FSA", "start_pos": 92, "end_pos": 95, "type": "DATASET", "confidence": 0.669796884059906}]}, {"text": "Even when we add the tenth translation to our FSA, we still improve its predictive power.", "labels": [], "entities": [{"text": "FSA", "start_pos": 46, "end_pos": 49, "type": "DATASET", "confidence": 0.9093620777130127}]}, {"text": "The second column shows that the more sentences we add to the FSA the larger the difference between its predictive power and that of a simple set.", "labels": [], "entities": [{"text": "FSA", "start_pos": 62, "end_pos": 65, "type": "DATASET", "confidence": 0.7120674252510071}]}, {"text": "The results in suggest that our FSA maybe used in order to refine the BLEU metric ().", "labels": [], "entities": [{"text": "FSA", "start_pos": 32, "end_pos": 35, "type": "METRIC", "confidence": 0.7850520610809326}, {"text": "BLEU metric", "start_pos": 70, "end_pos": 81, "type": "METRIC", "confidence": 0.9720114767551422}]}], "tableCaptions": [{"text": " Table 1: A comparison of the correctness of the para- phrases produced by the syntax-based alignment (S syn )  and co-training-based (S cotr ) algorithms.", "labels": [], "entities": []}, {"text": " Table 4: Quality judged by LM", "labels": [], "entities": [{"text": "Quality", "start_pos": 10, "end_pos": 17, "type": "METRIC", "confidence": 0.9716438055038452}, {"text": "LM", "start_pos": 28, "end_pos": 30, "type": "TASK", "confidence": 0.48487913608551025}]}, {"text": " Table 6: Effect of monotonically increasing the number  of reference sentences", "labels": [], "entities": []}]}