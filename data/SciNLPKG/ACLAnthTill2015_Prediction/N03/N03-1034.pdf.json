{"title": [{"text": "Evaluating the Evaluation: A Case Study Using the TREC 2002 Question Answering Track", "labels": [], "entities": [{"text": "TREC 2002 Question Answering Track", "start_pos": 50, "end_pos": 84, "type": "DATASET", "confidence": 0.8333077073097229}]}], "abstractContent": [{"text": "Evaluating competing technologies on a common problem set is a powerful way to improve the state of the art and hasten technology transfer.", "labels": [], "entities": [{"text": "hasten technology transfer", "start_pos": 112, "end_pos": 138, "type": "TASK", "confidence": 0.7164922455946604}]}, {"text": "Yet poorly designed evaluations can waste research effort or even mislead researchers with faulty conclusions.", "labels": [], "entities": []}, {"text": "Thus it is important to examine the quality of anew evaluation task to establish its reliability.", "labels": [], "entities": []}, {"text": "This paper provides an example of one such assessment by analyzing the task within the TREC 2002 question answering track.", "labels": [], "entities": [{"text": "TREC 2002 question answering", "start_pos": 87, "end_pos": 115, "type": "TASK", "confidence": 0.8279256224632263}]}, {"text": "The analysis demonstrates that comparative results from the new task are stable, and empirically estimates the size of the difference required between scores to confidently conclude that two runs are different.", "labels": [], "entities": []}, {"text": "Metric-based evaluations of human language technology such as MUC and TREC and DUC continue to proliferate (Sparck Jones, 2001).", "labels": [], "entities": [{"text": "TREC", "start_pos": 70, "end_pos": 74, "type": "METRIC", "confidence": 0.8753957748413086}, {"text": "Sparck Jones, 2001)", "start_pos": 108, "end_pos": 127, "type": "DATASET", "confidence": 0.8831561923027038}]}, {"text": "This proliferation is not difficult to understand: evaluations can forge communities , accelerate technology transfer, and advance the state of the art.", "labels": [], "entities": []}, {"text": "Yet evaluations are not without their costs.", "labels": [], "entities": []}, {"text": "In addition to the financial resources required to support the evaluation, there are also the costs of researcher time and focus.", "labels": [], "entities": []}, {"text": "Since a poorly defined evaluation task wastes research effort, it is important to examine the validity of an evaluation task.", "labels": [], "entities": []}, {"text": "In this paper, we assess the quality of the new question answering task that was the focus of the TREC 2002 question answering track.", "labels": [], "entities": [{"text": "question answering task", "start_pos": 48, "end_pos": 71, "type": "TASK", "confidence": 0.9027667244275411}, {"text": "TREC 2002 question answering track", "start_pos": 98, "end_pos": 132, "type": "TASK", "confidence": 0.7380969524383545}]}, {"text": "TREC is a workshop series designed to encourage research on text retrieval for realistic applications by providing large test collections, uniform scoring procedures, and a forum for organizations interested in comparing results.", "labels": [], "entities": [{"text": "TREC", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.5085633993148804}, {"text": "text retrieval", "start_pos": 60, "end_pos": 74, "type": "TASK", "confidence": 0.7483190596103668}]}, {"text": "The conference has focused primarily on the traditional information retrieval problem of retrieving a ranked list of documents in response to a statement of information need, but also includes other tasks, called tracks, that focus on new areas or particularly difficult aspects of information retrieval.", "labels": [], "entities": [{"text": "information retrieval problem of retrieving a ranked list of documents in response to a statement of information need", "start_pos": 56, "end_pos": 173, "type": "TASK", "confidence": 0.7590739727020264}, {"text": "information retrieval", "start_pos": 282, "end_pos": 303, "type": "TASK", "confidence": 0.7061694860458374}]}, {"text": "A question answering (QA) track was started in TREC in 1999 (TREC-8) to address the problem of returning answers, rather than document lists, in response to a question.", "labels": [], "entities": [{"text": "question answering (QA)", "start_pos": 2, "end_pos": 25, "type": "TASK", "confidence": 0.840966922044754}, {"text": "TREC in 1999 (TREC-8)", "start_pos": 47, "end_pos": 68, "type": "DATASET", "confidence": 0.8466047644615173}]}, {"text": "The task for each of the first three years of the QA track was essentially the same.", "labels": [], "entities": [{"text": "QA track", "start_pos": 50, "end_pos": 58, "type": "DATASET", "confidence": 0.6841083914041519}]}, {"text": "Participants received a large corpus of newswire documents and a set of factoid questions such as How many calories are in a Big Mac? and Who invented the paper clip?.", "labels": [], "entities": []}, {"text": "Systems were required to return a ranked list of up to five [document-id, answer-string] pairs per question such that each answer string was believed to contain an answer to the question.", "labels": [], "entities": []}, {"text": "Human assessors read each string and decided whether the string actually did contain an answer to the question.", "labels": [], "entities": []}, {"text": "An individual question received a score equal to the reciprocal of the rank at which the first correct response was returned, or zero if none of the five responses contained a correct answer.", "labels": [], "entities": []}, {"text": "The score fora submission was then the mean of the individual questions' reciprocal ranks.", "labels": [], "entities": []}, {"text": "Analysis of the TREC-8 track confirmed the reliability of this evaluation task (Voorhees and Tice, 2000): the assessors understood and could do their assessing job; relative scores between systems were stable despite differences of opinion by assessors; and intuitively better systems received better scores.", "labels": [], "entities": [{"text": "TREC-8 track", "start_pos": 16, "end_pos": 28, "type": "DATASET", "confidence": 0.6974721550941467}]}, {"text": "The task for the TREC 2002 QA track changed significantly from the previous years' task, and thus anew assessment of the track is needed.", "labels": [], "entities": [{"text": "TREC 2002 QA track", "start_pos": 17, "end_pos": 35, "type": "DATASET", "confidence": 0.8897179812192917}]}, {"text": "This paper provides that assessment by examining both the ability of the human assessors to make the required judgments and the effect that differences in assessor opinions have on comparative results, plus empirically establishing confidence intervals for the reliability of a comparison as a function of the difference in effectiveness scores.", "labels": [], "entities": []}, {"text": "The first section defines the 2002 QA task and provides a brief summary of the system results.", "labels": [], "entities": [{"text": "2002 QA task", "start_pos": 30, "end_pos": 42, "type": "DATASET", "confidence": 0.8336364428202311}]}, {"text": "The following three sections look at each of the evaluation issues in turn.", "labels": [], "entities": []}], "introductionContent": [], "datasetContent": [], "tableCaptions": []}