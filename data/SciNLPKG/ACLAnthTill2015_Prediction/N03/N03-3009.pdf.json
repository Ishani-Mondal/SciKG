{"title": [{"text": "Spoken and Written News Story Segmentation using Lexical Chains", "labels": [], "entities": [{"text": "Spoken and Written News Story Segmentation", "start_pos": 0, "end_pos": 42, "type": "TASK", "confidence": 0.8169464468955994}]}], "abstractContent": [{"text": "In this paper we describe a novel approach to lexical chain based segmentation of broadcast news stories.", "labels": [], "entities": [{"text": "lexical chain based segmentation of broadcast news stories", "start_pos": 46, "end_pos": 104, "type": "TASK", "confidence": 0.8128268420696259}]}, {"text": "Our segmentation system SeLeCT is evaluated with respect to two other lexical cohesion based segmenters TextTiling and C99.", "labels": [], "entities": []}, {"text": "Using the P k and WindowDiff evaluation metrics we show that SeLeCT outperforms both systems on spoken news transcripts (CNN) while the C99 algorithm performs best on the written newswire collection (Reuters).", "labels": [], "entities": [{"text": "written newswire collection (Reuters)", "start_pos": 171, "end_pos": 208, "type": "DATASET", "confidence": 0.7716932247082392}]}, {"text": "We also examine the differences between spoken and written news styles and how these differences can affect segmentation accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 121, "end_pos": 129, "type": "METRIC", "confidence": 0.8797953724861145}]}], "introductionContent": [{"text": "Text segmentation can be defined as the automatic identification of boundaries between distinct textual units (segments) in a textual document.", "labels": [], "entities": [{"text": "Text segmentation", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.7166135311126709}, {"text": "automatic identification of boundaries between distinct textual units (segments) in a textual document", "start_pos": 40, "end_pos": 142, "type": "TASK", "confidence": 0.7528298755486806}]}, {"text": "The aim of early segmentation research was to model the discourse structure of a text, thus focusing on the detection of finegrained topic shifts, at a clausal, sentence or passage/subtopic level).", "labels": [], "entities": []}, {"text": "More recently with the introduction of the TDT initiative () segmentation research has concentrated on the detection of coarse-grained topic shifts resulting in the identification of story boundaries in news feeds.", "labels": [], "entities": [{"text": "identification of story boundaries in news feeds", "start_pos": 165, "end_pos": 213, "type": "TASK", "confidence": 0.8477096813065665}]}, {"text": "In particular, unsegmented broadcast news streams represent a challenging real-world application for text segmentation approaches, since the success of other tasks such as topic tracking or first story detection depend heavily on the correct identification of distinct and non-overlapping news stories.", "labels": [], "entities": [{"text": "text segmentation", "start_pos": 101, "end_pos": 118, "type": "TASK", "confidence": 0.7195422351360321}, {"text": "topic tracking", "start_pos": 172, "end_pos": 186, "type": "TASK", "confidence": 0.8089152276515961}, {"text": "first story detection", "start_pos": 190, "end_pos": 211, "type": "TASK", "confidence": 0.7095353603363037}]}, {"text": "Most approaches to story segmentation use either Information Extraction techniques (cue phrase extraction), techniques based on lexical cohesion analysis or a combination of both).", "labels": [], "entities": [{"text": "story segmentation", "start_pos": 19, "end_pos": 37, "type": "TASK", "confidence": 0.7512214183807373}, {"text": "cue phrase extraction", "start_pos": 84, "end_pos": 105, "type": "TASK", "confidence": 0.7042494614919027}]}, {"text": "More recently promising results have also been achieved though the use of Hidden Markov modeling techniques, which are commonly used in speech recognition applications).", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 136, "end_pos": 154, "type": "TASK", "confidence": 0.797123372554779}]}, {"text": "In this paper we focus on lexical cohesion based approaches to story segmentation.", "labels": [], "entities": [{"text": "story segmentation", "start_pos": 63, "end_pos": 81, "type": "TASK", "confidence": 0.7550835311412811}]}, {"text": "Lexical cohesion is one element of a broader linguistic device called cohesion which is describe as the textual quality responsible for making the elements of a text appear unified or connected.", "labels": [], "entities": []}, {"text": "More specifically, lexical cohesion 'is the cohesion that arises from semantic relationships between words'.", "labels": [], "entities": []}, {"text": "With respect to segmentation, an analysis of lexical cohesion can be used to indicate portions of text that represent single topical units or segments i.e. they contain a high number of semantically related words.", "labels": [], "entities": []}, {"text": "Almost all approaches to lexical cohesion based segmentation examine patterns of syntactic repetition in the text e.g..", "labels": [], "entities": [{"text": "lexical cohesion based segmentation", "start_pos": 25, "end_pos": 60, "type": "TASK", "confidence": 0.6254373714327812}]}, {"text": "However, there are four additional types of lexical cohesion present in text: synonymy (car, automobile), specialization/generalization (horse, stallion), part-whole/whole-part (politicians, government) and statistical co-occurrences (Osama bin Laden, World Trade Center).", "labels": [], "entities": [{"text": "World Trade Center)", "start_pos": 252, "end_pos": 271, "type": "DATASET", "confidence": 0.7977519780397415}]}, {"text": "Lexical chaining based approaches to text segmentation, on the other hand, analyse all aspects of lexical cohesion in text.", "labels": [], "entities": [{"text": "Lexical chaining", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.8412176072597504}, {"text": "text segmentation", "start_pos": 37, "end_pos": 54, "type": "TASK", "confidence": 0.713519424200058}]}, {"text": "Lexical chains are defined as groups of semantically related words that represent the lexical cohesive structure of a text e.g. {flower, petal, rose, garden, tree}.", "labels": [], "entities": [{"text": "Lexical chains are defined as groups of semantically related words that represent the lexical cohesive structure of a text e.g. {flower, petal, rose, garden, tree", "start_pos": 0, "end_pos": 162, "type": "Description", "confidence": 0.7843147406975428}]}, {"text": "In our lexical chaining implementation, words are clustered based on the existence of statistical relationships and lexicographical associations (provided by the WordNet online thesaurus) between terms in a text.", "labels": [], "entities": [{"text": "WordNet online thesaurus", "start_pos": 162, "end_pos": 186, "type": "DATASET", "confidence": 0.9593560099601746}]}, {"text": "There have been three previous attempts to tackle text segmentation using lexical chains.", "labels": [], "entities": [{"text": "text segmentation", "start_pos": 50, "end_pos": 67, "type": "TASK", "confidence": 0.7574510872364044}]}, {"text": "The first by involved an evaluation based on five Japanese texts, the second by used twelve general interest magazine articles and the third by used fifteen Wall Street Journal and five Economist articles.", "labels": [], "entities": [{"text": "Wall Street Journal", "start_pos": 157, "end_pos": 176, "type": "DATASET", "confidence": 0.9105110963185629}]}, {"text": "All of these attempts focus on sub-topic rather than story segmentation.", "labels": [], "entities": [{"text": "story segmentation", "start_pos": 53, "end_pos": 71, "type": "TASK", "confidence": 0.7294857800006866}]}, {"text": "In contrast, this paper investigates the usefulness of lexical chains as a technique for determining story segments in spoken and written broadcast news streams.", "labels": [], "entities": [{"text": "determining story segments in spoken and written broadcast news streams", "start_pos": 89, "end_pos": 160, "type": "TASK", "confidence": 0.8048086106777191}]}, {"text": "In Section 2, we explain how this technique can be refined Edmonton, to address story segmentation.", "labels": [], "entities": [{"text": "story segmentation", "start_pos": 80, "end_pos": 98, "type": "TASK", "confidence": 0.7655020952224731}]}, {"text": "In Section 3, we compare the segmentation performance of our lexical chaining algorithm with two other well known lexical cohesion based approaches to segmentation; namely TextTiling (Hearst 1997) and C99 (Choi 2000).", "labels": [], "entities": []}, {"text": "Finally we examine the grammatical differences between written and spoken news media and show how these differences can be utilized to improve spoken transcript segmentation accuracy.", "labels": [], "entities": [{"text": "spoken transcript segmentation", "start_pos": 143, "end_pos": 173, "type": "TASK", "confidence": 0.6766552229722341}, {"text": "accuracy", "start_pos": 174, "end_pos": 182, "type": "METRIC", "confidence": 0.6978434920310974}]}], "datasetContent": [{"text": "In this section we give details of two news story segmentation test sets, some evaluation metrics used to determine segmentation accuracy, and the performance results of the SeLeCT, C99 and TextTiling algorithms.", "labels": [], "entities": [{"text": "news story segmentation", "start_pos": 39, "end_pos": 62, "type": "TASK", "confidence": 0.6361962854862213}, {"text": "accuracy", "start_pos": 129, "end_pos": 137, "type": "METRIC", "confidence": 0.7523419857025146}]}, {"text": "There has been much debate in the segmentation literature regarding appropriate evaluation metrics for estimating segmentation accuracy.", "labels": [], "entities": [{"text": "estimating segmentation", "start_pos": 103, "end_pos": 126, "type": "TASK", "confidence": 0.7970505952835083}, {"text": "accuracy", "start_pos": 127, "end_pos": 135, "type": "METRIC", "confidence": 0.6050613522529602}]}, {"text": "Earlier experiments favored an IR style evaluation that measures performance in terms of recall and precision.", "labels": [], "entities": [{"text": "recall", "start_pos": 89, "end_pos": 95, "type": "METRIC", "confidence": 0.9991432428359985}, {"text": "precision", "start_pos": 100, "end_pos": 109, "type": "METRIC", "confidence": 0.9946824908256531}]}, {"text": "However these metrics were deemed insufficiently sensitive when trying to determine system parameters that yield optimal performance.", "labels": [], "entities": []}, {"text": "The most widely used evaluation metric is Beeferman et al.'s (1999) probabilistic error metric P k , which calculates segmentation accuracy with respect to three different types of segmentation error: false positives (falsely detected segments), false negatives (missed segments) and near-misses (very close but not exact boundaries).", "labels": [], "entities": [{"text": "probabilistic error metric P k", "start_pos": 68, "end_pos": 98, "type": "METRIC", "confidence": 0.7520188212394714}, {"text": "accuracy", "start_pos": 131, "end_pos": 139, "type": "METRIC", "confidence": 0.9071817398071289}]}, {"text": "However, in a recent publication highlight several faults with the P k metric.", "labels": [], "entities": []}, {"text": "Most notable they criticize P k for its unfair penalization of false negatives over false positives and its over-penalization of near-misses.", "labels": [], "entities": []}, {"text": "In their paper, the authors proposed an alternative error metric called WindowDiff which rectifies these problems.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: P k and WD (WindowDiff) values for segmenta- tion systems on CNN and Reuters Collections.", "labels": [], "entities": [{"text": "CNN and Reuters Collections", "start_pos": 71, "end_pos": 98, "type": "DATASET", "confidence": 0.7969873398542404}]}, {"text": " Table 2: Improvements in system performance as a result  of system modifications discuss in Sections 4.1 and 4.3.", "labels": [], "entities": []}]}