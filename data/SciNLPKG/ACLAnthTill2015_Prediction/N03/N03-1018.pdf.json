{"title": [{"text": "A Generative Probabilistic OCR Model for NLP Applications", "labels": [], "entities": []}], "abstractContent": [{"text": "In this paper, we introduce a generative prob-abilistic optical character recognition (OCR) model that describes an end-to-end process in the noisy channel framework, progressing from generation of true text through its transformation into the noisy output of an OCR system.", "labels": [], "entities": [{"text": "generative prob-abilistic optical character recognition (OCR)", "start_pos": 30, "end_pos": 91, "type": "TASK", "confidence": 0.8075001314282417}]}, {"text": "The model is designed for use in error correction , with a focus on post-processing the output of black-box OCR systems in order to make it more useful for NLP tasks.", "labels": [], "entities": [{"text": "error correction", "start_pos": 33, "end_pos": 49, "type": "TASK", "confidence": 0.6959204971790314}]}, {"text": "We present an implementation of the model based on finite-state models, demonstrate the model's ability to significantly reduce character and word error rate, and provide evaluation results involving automatic extraction of translation lexicons from printed text.", "labels": [], "entities": []}], "introductionContent": [{"text": "Although a great deal of text is now available in electronic form, vast quantities of information still exist primarily (or only) in print.", "labels": [], "entities": []}, {"text": "Critical applications of NLP technology, such as rapid, rough document translation in the field) or information retrieval from scanned documents, can depend heavily on the quality of optical character recognition (OCR) output.", "labels": [], "entities": [{"text": "rough document translation", "start_pos": 56, "end_pos": 82, "type": "TASK", "confidence": 0.7286940415700277}, {"text": "information retrieval from scanned documents", "start_pos": 100, "end_pos": 144, "type": "TASK", "confidence": 0.861348819732666}, {"text": "optical character recognition (OCR) output", "start_pos": 183, "end_pos": 225, "type": "TASK", "confidence": 0.8012785911560059}]}, {"text": "comments, \"Although the concept of a raw document image database is attractive, comprehensive solutions which do not require complete and accurate conversion to a machine-readable form continue to be elusive for practical systems.\"", "labels": [], "entities": []}, {"text": "Unfortunately, the output of commercial OCR systems is far from perfect, especially when the language in question is resource-poor (.", "labels": [], "entities": []}, {"text": "And efforts to acquire new language resources from hardcopy using OCR () face something of a chicken-and-egg problem.", "labels": [], "entities": []}, {"text": "The problem is compounded by the fact that most OCR system are black boxes that do not allow user tuning or re-training -Baird) comments that the lack of ability to rapidly retarget OCR/NLP applications to new languages is \"largely due to the monolithic structure of current OCR technology, where language-specific constraints are deeply enmeshed with all the other code.\"", "labels": [], "entities": []}, {"text": "In this paper, we describe a complete probabilistic, generative model for OCR, motivated specifically by (a) the need to deal with monolithic OCR systems, (b) the focus on OCR as a component in NLP applications, and (c) the ultimate goal of using OCR to help acquire resources for new languages from printed text.", "labels": [], "entities": [{"text": "OCR", "start_pos": 74, "end_pos": 77, "type": "TASK", "confidence": 0.9249868988990784}]}, {"text": "After presenting the model itself, we discuss the model's implementation, training, and its use for post-OCR error correction.", "labels": [], "entities": [{"text": "post-OCR error correction", "start_pos": 100, "end_pos": 125, "type": "TASK", "confidence": 0.6600735187530518}]}, {"text": "We then present two evaluations: one for standalone OCR correction, and one in which OCR is used to acquire a translation lexicon from printed text.", "labels": [], "entities": [{"text": "OCR correction", "start_pos": 52, "end_pos": 66, "type": "TASK", "confidence": 0.8831327259540558}]}, {"text": "We conclude with a discussion of related research and directions for future work.", "labels": [], "entities": []}], "datasetContent": [{"text": "We report on two experiments.", "labels": [], "entities": []}, {"text": "In the first, we evaluate the correction performance of our model on real OCR data.", "labels": [], "entities": [{"text": "OCR data", "start_pos": 74, "end_pos": 82, "type": "DATASET", "confidence": 0.756099671125412}]}, {"text": "In the second, we evaluate the effect of correction in a representative NLP scenario, acquiring a translation lexicon from hardcopy text.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Post-correction WER and CER and their reduction rates under various conditions", "labels": [], "entities": [{"text": "WER", "start_pos": 26, "end_pos": 29, "type": "METRIC", "confidence": 0.4178768992424011}, {"text": "CER", "start_pos": 34, "end_pos": 37, "type": "METRIC", "confidence": 0.9536190032958984}]}, {"text": " Table 2: WER, CER, and reduction rates ignoring single characters and non-alphabetical tokens", "labels": [], "entities": [{"text": "WER", "start_pos": 10, "end_pos": 13, "type": "METRIC", "confidence": 0.995639443397522}, {"text": "CER", "start_pos": 15, "end_pos": 18, "type": "METRIC", "confidence": 0.9959932565689087}, {"text": "reduction rates", "start_pos": 24, "end_pos": 39, "type": "METRIC", "confidence": 0.9313048124313354}]}]}