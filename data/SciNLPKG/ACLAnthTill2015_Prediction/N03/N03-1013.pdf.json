{"title": [{"text": "A Categorial Variation Database for English", "labels": [], "entities": []}], "abstractContent": [{"text": "We describe our approach to the construction and evaluation of a large-scale database called \"CatVar\" which contains categorial variations of English lexemes.", "labels": [], "entities": []}, {"text": "Due to the prevalence of cross-language categorial variation in multilingual applications, our categorial-variation resource may serve as an integral part of a diverse range of natural language applications.", "labels": [], "entities": []}, {"text": "Thus, the research reported herein overlaps heavily with that of the machine-translation, lexicon-construction, and information-retrieval communities.", "labels": [], "entities": []}, {"text": "We apply the information-retrieval metrics of precision and recall to evaluate the accuracy and coverage of our database with respect to a human-produced gold standard.", "labels": [], "entities": [{"text": "precision", "start_pos": 46, "end_pos": 55, "type": "METRIC", "confidence": 0.9994840621948242}, {"text": "recall", "start_pos": 60, "end_pos": 66, "type": "METRIC", "confidence": 0.9983243346214294}, {"text": "accuracy", "start_pos": 83, "end_pos": 91, "type": "METRIC", "confidence": 0.9989896416664124}, {"text": "coverage", "start_pos": 96, "end_pos": 104, "type": "METRIC", "confidence": 0.8908523917198181}]}, {"text": "This evaluation reveals that the categorial database achieves a high degree of precision and recall.", "labels": [], "entities": [{"text": "precision", "start_pos": 79, "end_pos": 88, "type": "METRIC", "confidence": 0.9996318817138672}, {"text": "recall", "start_pos": 93, "end_pos": 99, "type": "METRIC", "confidence": 0.9985062479972839}]}, {"text": "Additionally, we demonstrate that the database improves on the linkability of Porter stemmer by over 30%.", "labels": [], "entities": [{"text": "Porter stemmer", "start_pos": 78, "end_pos": 92, "type": "DATASET", "confidence": 0.9263863265514374}]}], "introductionContent": [{"text": "Natural Language Processing (NLP) applications may only be as good as the resources upon which they rely.", "labels": [], "entities": []}, {"text": "Resources specifying the relations among lexical items such as WordNet and HowNet) (among others) have inspired the work of many researchers in NLP;.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 63, "end_pos": 70, "type": "DATASET", "confidence": 0.9566634893417358}, {"text": "HowNet", "start_pos": 75, "end_pos": 81, "type": "DATASET", "confidence": 0.873985230922699}]}, {"text": "In this paper we introduce anew resource called CatVar which specifies the lexical relation Categorial Variation on a large scale for English.", "labels": [], "entities": []}, {"text": "This resource has already been used effectively in a wide range of monolingual and multilingual NLP applications.", "labels": [], "entities": []}, {"text": "Upon its first public release, CatVar will be freely available to the research community.", "labels": [], "entities": [{"text": "CatVar", "start_pos": 31, "end_pos": 37, "type": "DATASET", "confidence": 0.9217130541801453}]}, {"text": "We expect that the contribution of this resource will become more widely recognized through its future incorporation into additional NLP applications.", "labels": [], "entities": []}, {"text": "A categorial variation of a word with a certain partof-speech is a derivationally-related word with possibly a different part-of-speech.", "labels": [], "entities": []}, {"text": "For example, hunger and stab\u00a1 . Although this relation seems basic on the surface, this relation is critical to work in Information Retrieval (IR), Natural Language Generation (NLG) and Machine Translation (MT)-yet there is no large scale resource available for English that focuses on categorial variations.", "labels": [], "entities": [{"text": "Information Retrieval (IR)", "start_pos": 120, "end_pos": 146, "type": "TASK", "confidence": 0.8350292563438415}, {"text": "Natural Language Generation (NLG)", "start_pos": 148, "end_pos": 181, "type": "TASK", "confidence": 0.8066172401110331}, {"text": "Machine Translation (MT)-", "start_pos": 186, "end_pos": 211, "type": "TASK", "confidence": 0.8611672163009644}]}, {"text": "In the rest of this paper, we discuss other available resources and how they differ from the CatVar database.", "labels": [], "entities": [{"text": "CatVar database", "start_pos": 93, "end_pos": 108, "type": "DATASET", "confidence": 0.9752513766288757}]}, {"text": "We then discuss how and what resources were used to build CatVar.", "labels": [], "entities": [{"text": "CatVar", "start_pos": 58, "end_pos": 64, "type": "DATASET", "confidence": 0.961488664150238}]}, {"text": "Afterwards, we present three applications that use CatVar in different ways: Generation-Heavy MT, headline generation, and cross-language divergence unraveling for bilingual alignment.", "labels": [], "entities": [{"text": "headline generation", "start_pos": 98, "end_pos": 117, "type": "TASK", "confidence": 0.7548814713954926}, {"text": "cross-language divergence unraveling", "start_pos": 123, "end_pos": 159, "type": "TASK", "confidence": 0.8292245467503866}, {"text": "bilingual alignment", "start_pos": 164, "end_pos": 183, "type": "TASK", "confidence": 0.619807556271553}]}, {"text": "Finally, we present a multi-component evaluation of the database.", "labels": [], "entities": []}, {"text": "Our evaluation reveals that the categorial database achieves a high degree of precision and recall and that it improves on the linkability of Porter stemmer by over 30%.", "labels": [], "entities": [{"text": "precision", "start_pos": 78, "end_pos": 87, "type": "METRIC", "confidence": 0.9996813535690308}, {"text": "recall", "start_pos": 92, "end_pos": 98, "type": "METRIC", "confidence": 0.9994801878929138}, {"text": "Porter stemmer", "start_pos": 142, "end_pos": 156, "type": "DATASET", "confidence": 0.9393713772296906}]}], "datasetContent": [{"text": "This section includes two evaluations concerned with different aspects of the CatVar database.", "labels": [], "entities": [{"text": "CatVar database", "start_pos": 78, "end_pos": 93, "type": "DATASET", "confidence": 0.9708415567874908}]}, {"text": "The first evaluation calculates the recall and precision of CatVar's clustering and the second determines the contribution of CatVar over Porter stemmer.", "labels": [], "entities": [{"text": "recall", "start_pos": 36, "end_pos": 42, "type": "METRIC", "confidence": 0.9997418522834778}, {"text": "precision", "start_pos": 47, "end_pos": 56, "type": "METRIC", "confidence": 0.9969016313552856}, {"text": "Porter stemmer", "start_pos": 138, "end_pos": 152, "type": "DATASET", "confidence": 0.8703307211399078}]}, {"text": "To determine the recall and precision of CatVar given the lack of a gold standard, we asked 8 native speakers to evaluate 400 randomly-selected clusters.", "labels": [], "entities": [{"text": "recall", "start_pos": 17, "end_pos": 23, "type": "METRIC", "confidence": 0.9992884397506714}, {"text": "precision", "start_pos": 28, "end_pos": 37, "type": "METRIC", "confidence": 0.997046172618866}]}, {"text": "Each annotator was given a set of 100 clusters (with two annotators per set).", "labels": [], "entities": []}, {"text": "shows a segment of the evaluation interface which was web-browseable.", "labels": [], "entities": []}, {"text": "The annotators were given detailed instructions and many examples to help them with the task.", "labels": [], "entities": []}, {"text": "They were asked to classify each word in every cluster as belonging to one of the following categories: Perfect: This word definitely belongs in this cluster.", "labels": [], "entities": [{"text": "Perfect", "start_pos": 104, "end_pos": 111, "type": "METRIC", "confidence": 0.9369416236877441}]}, {"text": "Perfect (except for part of speech problem).", "labels": [], "entities": [{"text": "Perfect", "start_pos": 0, "end_pos": 7, "type": "METRIC", "confidence": 0.9778904318809509}]}, {"text": "Perfect (except for spelling problem).", "labels": [], "entities": [{"text": "Perfect", "start_pos": 0, "end_pos": 7, "type": "METRIC", "confidence": 0.9770177602767944}, {"text": "spelling problem)", "start_pos": 20, "end_pos": 37, "type": "TASK", "confidence": 0.729111522436142}]}, {"text": "Not Sure: It is not clear whether a word that is derivationally correct belongs in a set or not.", "labels": [], "entities": []}, {"text": "Doesn't Belong: This word doesn't belong in this cluster.", "labels": [], "entities": []}, {"text": "May not be a Real Word: This word is not known and couldn't be found it in a dictionary.", "labels": [], "entities": []}, {"text": "The interface also provided an input text box to add missing words to a cluster.", "labels": [], "entities": []}, {"text": "In calculating the inter-annotator agreement, we did not consider mismatches in word additions as disagreement since some annotators could not think up as many possible variations as others.", "labels": [], "entities": []}, {"text": "After all, this was not an evaluation of their ability to think up variations, but rather of the coverage of the CatVar database.", "labels": [], "entities": [{"text": "CatVar database", "start_pos": 113, "end_pos": 128, "type": "DATASET", "confidence": 0.9766218662261963}]}, {"text": "The inter-annotator agreement was calculated as the percentage of words where both annotators agreed out of all words.", "labels": [], "entities": []}, {"text": "Even though there were six fine-grained classifications, the average inter-annotator agreement was high (80.75%).", "labels": [], "entities": [{"text": "agreement", "start_pos": 85, "end_pos": 94, "type": "METRIC", "confidence": 0.7268646955490112}]}, {"text": "Many of the disagreements, however, resulted from the fine-grainedness of the options available to the annotators.", "labels": [], "entities": []}, {"text": "Ina second calculation of inter-annotator agreement, we simplified the annotators' choices by placing them into three groups corresponding to Perfect (Perfect and Perfect-but), Not-sure (Not-sure and May-not-be-a-realword) and Wrong (Does-not-belong).", "labels": [], "entities": [{"text": "Perfect", "start_pos": 142, "end_pos": 149, "type": "METRIC", "confidence": 0.9615471959114075}]}, {"text": "This annotationgrouping approach is comparable to the clustering techniques used by to \"super-tag\" fine grained annotations.", "labels": [], "entities": []}, {"text": "After grouping the annotations, average inter-annotator agreement rose up to 98.35%.", "labels": [], "entities": [{"text": "agreement", "start_pos": 56, "end_pos": 65, "type": "METRIC", "confidence": 0.7391740083694458}]}, {"text": "The cluster modifications produced by each pair of annotators assigned to the same cluster were then combined automatically in an approximation to post-annotation inter-annotator discussion, which traditionally results in agreement: (1) If both annotators agreed on a category, then it stands; (2) One annotator overrides another in cases where one is more sure than the other (i.e., Perfect overrides Perfect-but-with-error/Not-sure and Wrong overrides Not-sure); (3) In cases where one annotator considers a word Perfect while the other annotator considered it Wrong, we compromise at Not-sure.", "labels": [], "entities": []}, {"text": "The union of all added words was included in the combined cluster.", "labels": [], "entities": []}, {"text": "The 400 combined clusters covered 808 words.", "labels": [], "entities": []}, {"text": "68% of the words were ranked as Perfect.", "labels": [], "entities": [{"text": "Perfect", "start_pos": 32, "end_pos": 39, "type": "METRIC", "confidence": 0.9923987984657288}]}, {"text": "None had spelling errors and only one word had a part-of-speech issue.", "labels": [], "entities": []}, {"text": "23 words (less than 3%) were marked as Not-sures.", "labels": [], "entities": []}, {"text": "And only 6 words (less than 1%) were marked as Wrong.", "labels": [], "entities": []}, {"text": "There were 209 added words (about 26%).", "labels": [], "entities": []}, {"text": "However 128 words (or 61% of missing words) were not actually missing, but rather not linked into the set of clusters evaluated by a particular annotator.", "labels": [], "entities": []}, {"text": "Some of these words were clustered separately in the database.", "labels": [], "entities": []}, {"text": "The rest of the missing words (81 words or 10% of all words) were not present in the database, but 50 of them (or 62%) were linkable to existing words in the CatVar using simple stemming (e.g., the Porter stemmer, whose relevance is described next).", "labels": [], "entities": [{"text": "CatVar", "start_pos": 158, "end_pos": 164, "type": "DATASET", "confidence": 0.9588374495506287}]}, {"text": "The precision was calculated as the ratio of perfect words to all original (i.e. not added) words: 91.82%.", "labels": [], "entities": [{"text": "precision", "start_pos": 4, "end_pos": 13, "type": "METRIC", "confidence": 0.9994520545005798}]}, {"text": "The recall was calculated as the ratio of perfect words divided by all perfect plus all added words: 72.46%.", "labels": [], "entities": [{"text": "recall", "start_pos": 4, "end_pos": 10, "type": "METRIC", "confidence": 0.9992988109588623}]}, {"text": "However, if we exclude the not-really missing words, the adjusted recall value becomes 87.16%.", "labels": [], "entities": [{"text": "recall", "start_pos": 66, "end_pos": 72, "type": "METRIC", "confidence": 0.9970852732658386}]}, {"text": "The harmonic mean or Fscore 9 of the precision and recall is 81.00% (or 89.43% for adjusted recall).", "labels": [], "entities": [{"text": "harmonic mean", "start_pos": 4, "end_pos": 17, "type": "METRIC", "confidence": 0.889563113451004}, {"text": "Fscore 9", "start_pos": 21, "end_pos": 29, "type": "METRIC", "confidence": 0.9908867180347443}, {"text": "precision", "start_pos": 37, "end_pos": 46, "type": "METRIC", "confidence": 0.9996620416641235}, {"text": "recall", "start_pos": 51, "end_pos": 57, "type": "METRIC", "confidence": 0.999276340007782}, {"text": "recall", "start_pos": 92, "end_pos": 98, "type": "METRIC", "confidence": 0.8682135343551636}]}, {"text": "To measure the contribution of CatVar with respect to the \"linking together\" of related words, it is important to define the concept of linkability as the percentage of wordto-word links in the database resulting from a specific source.", "labels": [], "entities": []}, {"text": "For example, Natural linkability refers to pairs of words whose form doesn't change across categories such as zip and zip\u00a1 or afghan\u00a1 and afghan\u00a2 \u00a4 \u00a3 . Porter linkability refers to words linkable by reduction to a common Porter stem.", "labels": [], "entities": []}, {"text": "CatVar linkability is the linkability of two words appearing in the same CatVar cluster.", "labels": [], "entities": []}, {"text": "shows an example of all three types of links in the hunger cluster.", "labels": [], "entities": []}, {"text": "Here, hunger\u00a1 and hunger are linked in three ways, Naturally (N), by the Porter stemmer (P), and in CatVar (C).", "labels": [], "entities": [{"text": "Porter stemmer", "start_pos": 73, "end_pos": 87, "type": "DATASET", "confidence": 0.6912581324577332}]}, {"text": "Porter links hungry\u00a2 \u00a4 \u00a3 and hungriness\u00a1 via the common stem hungri but Porter doesn't link either of these to hunger\u00a1 or hunger (stem hunger).", "labels": [], "entities": []}, {"text": "The total number of links in this cluster is six, two of which are Porter-determinable and only one of which is naturally-determinable.", "labels": [], "entities": []}, {"text": "The calculation of linkability applies only to the portion of the database containing multi-word clusters (about half of the database) since single-word clusters have zero links.", "labels": [], "entities": []}, {"text": "The 48,867 linked words are distributed over 14,731 clusters with 89,638 total number of links.", "labels": [], "entities": []}, {"text": "About 12% of these links are naturally-determinable and 70% are Porter-linkable.", "labels": [], "entities": []}, {"text": "The last 30% of the links is a significant contribution of the CatVar database, compared to the Porter stemmer, particularly since this stemmer is an industry standard in the IR community.", "labels": [], "entities": [{"text": "CatVar database", "start_pos": 63, "end_pos": 78, "type": "DATASET", "confidence": 0.9674164354801178}, {"text": "Porter stemmer", "start_pos": 96, "end_pos": 110, "type": "DATASET", "confidence": 0.8613674938678741}, {"text": "IR community", "start_pos": 175, "end_pos": 187, "type": "TASK", "confidence": 0.8854010999202728}]}, {"text": "It is important to point out that, for CatVar to be used in IR, it must be accompanied by an inflectional analyzer that reduces words to their lexeme form (removing plural endings from nouns or gerund ending from verbs).", "labels": [], "entities": [{"text": "IR", "start_pos": 60, "end_pos": 62, "type": "TASK", "confidence": 0.9631589651107788}]}, {"text": "The contribution of CatVar is in its linking of words related derivationally not inflectionally.", "labels": [], "entities": []}, {"text": "Work by demonstrates an improved performance with derivational stemming over the Porter stemmer most of the time.", "labels": [], "entities": [{"text": "derivational stemming", "start_pos": 50, "end_pos": 71, "type": "TASK", "confidence": 0.6046231538057327}]}], "tableCaptions": []}