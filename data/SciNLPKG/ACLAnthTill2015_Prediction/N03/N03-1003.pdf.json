{"title": [{"text": "Learning to Paraphrase: An Unsupervised Approach Using Multiple-Sequence Alignment", "labels": [], "entities": [{"text": "Learning to Paraphrase", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.49187977115313214}, {"text": "Multiple-Sequence Alignment", "start_pos": 55, "end_pos": 82, "type": "TASK", "confidence": 0.6988841742277145}]}], "abstractContent": [{"text": "We address the text-to-text generation problem of sentence-level paraphrasing-a phenomenon distinct from and more difficult than word-or phrase-level paraphrasing.", "labels": [], "entities": [{"text": "text-to-text generation", "start_pos": 15, "end_pos": 38, "type": "TASK", "confidence": 0.7200044542551041}]}, {"text": "Our approach applies multiple-sequence alignment to sentences gathered from unannotated comparable corpora: it learns a set of paraphrasing patterns represented byword lattice pairs and automatically determines how to apply these patterns to rewrite new sentences.", "labels": [], "entities": [{"text": "multiple-sequence alignment", "start_pos": 21, "end_pos": 48, "type": "TASK", "confidence": 0.7056126892566681}]}, {"text": "The results of our evaluation experiments show that the system derives accurate paraphrases, outperform-ing baseline systems.", "labels": [], "entities": []}], "introductionContent": [{"text": "This is a late parrot!", "labels": [], "entities": []}, {"text": "Bereft of life, it rests in peace!", "labels": [], "entities": []}, {"text": "If you hadn't nailed him to the perch he would be pushing up the daisies!", "labels": [], "entities": []}, {"text": "Its metabolical processes are of interest only to historians!", "labels": [], "entities": []}, {"text": "It's hopped the twig!", "labels": [], "entities": []}, {"text": "It's shuffled off this mortal coil!", "labels": [], "entities": []}, {"text": "It's rung down the curtain and joined the choir invisible!", "labels": [], "entities": []}, {"text": "-Monty Python, \"Pet Shop\" A mechanism for automatically generating multiple paraphrases of a given sentence would be of significant practical import for text-to-text generation systems.", "labels": [], "entities": [{"text": "text-to-text generation", "start_pos": 153, "end_pos": 176, "type": "TASK", "confidence": 0.736012876033783}]}, {"text": "Applications include summarization) and rewriting (Chandrasekar and Bangalore, 1997): both could employ such a mechanism to produce candidate sentence paraphrases that other system components would filter for length, sophistication level, and so forth.", "labels": [], "entities": [{"text": "summarization", "start_pos": 21, "end_pos": 34, "type": "TASK", "confidence": 0.9871805310249329}]}, {"text": "1 Not surprisingly, therefore, paraphrasing has been a focus of generation research for quite sometime.", "labels": [], "entities": [{"text": "paraphrasing", "start_pos": 31, "end_pos": 43, "type": "TASK", "confidence": 0.9683820009231567}]}, {"text": "One might initially suppose that sentence-level paraphrasing is simply the result of word-for-word or phraseby-phrase substitution applied in a domain-and contextindependent fashion.", "labels": [], "entities": []}, {"text": "However, in studies of paraphrases across several domains (, this was generally not the case.", "labels": [], "entities": []}, {"text": "For instance, consider the following two sentences (similar to examples found in): After the latest Fed rate cut, stocks rose across the board.", "labels": [], "entities": [{"text": "Fed rate cut", "start_pos": 100, "end_pos": 112, "type": "DATASET", "confidence": 0.852904220422109}]}, {"text": "Winners strongly outpaced losers after Greenspan cut interest rates again.", "labels": [], "entities": [{"text": "Greenspan", "start_pos": 39, "end_pos": 48, "type": "DATASET", "confidence": 0.948010265827179}]}, {"text": "Observe that \"Fed\" (Federal Reserve) and \"Greenspan\" are interchangeable only in the domain of US financial matters.", "labels": [], "entities": []}, {"text": "Also, note that one cannot draw one-to-one correspondences between single words or phrases.", "labels": [], "entities": []}, {"text": "For instance, nothing in the second sentence is really equivalent to \"across the board\"; we can only say that the entire clauses \"stocks rose across the board\" and \"winners strongly outpaced losers\" are paraphrases.", "labels": [], "entities": []}, {"text": "This evidence suggests two consequences: (1) we cannot rely solely on generic domain-independent lexical resources for the task of paraphrasing, and (2) sentence-level paraphrasing is an important problem extending beyond that of paraphrasing smaller lexical units.", "labels": [], "entities": []}, {"text": "Our work presents a novel knowledge-lean algorithm that uses multiple-sequence alignment (MSA) to learn to generate sentence-level paraphrases essentially from unannotated corpus data alone.", "labels": [], "entities": []}, {"text": "In contrast to previous work using MSA for generation (Barzilay and Lee, several versions of their component sentences.", "labels": [], "entities": []}, {"text": "This could, for example, aid machine-translation evaluation, where it has become common to evaluate systems by comparing their output against a bank of several reference translations for the same sentences (.", "labels": [], "entities": [{"text": "machine-translation evaluation", "start_pos": 29, "end_pos": 59, "type": "TASK", "confidence": 0.8162548840045929}]}, {"text": "See and for other uses of such data., we need neither parallel data nor explicit information about sentence semantics.", "labels": [], "entities": []}, {"text": "Rather, we use two comparable corpora, in our case, collections of articles produced by two different newswire agencies about the same events.", "labels": [], "entities": []}, {"text": "The use of related corpora is key: we can capture paraphrases that on the surface bear little resemblance but that, by the nature of the data, must be descriptions of the same information.", "labels": [], "entities": []}, {"text": "Note that we also acquire paraphrases from each of the individual corpora; but the lack of clues as to sentence equivalence in single corpora means that we must be more conservative, only selecting as paraphrases items that are structurally very similar.", "labels": [], "entities": []}, {"text": "Our approach has three main steps.", "labels": [], "entities": []}, {"text": "First, working on each of the comparable corpora separately, we compute lattices -compact graph-based representations -to find commonalities within (automatically derived) groups of structurally similar sentences.", "labels": [], "entities": []}, {"text": "Next, we identify pairs of lattices from the two different corpora that are paraphrases of each other; the identification process checks whether the lattices take similar arguments.", "labels": [], "entities": []}, {"text": "Finally, given an input sentence to be paraphrased, we match it to a lattice and use a paraphrase from the matched lattice's mate to generate an output sentence.", "labels": [], "entities": []}, {"text": "The key features of this approach are: Focus on paraphrase generation.", "labels": [], "entities": [{"text": "paraphrase generation", "start_pos": 48, "end_pos": 69, "type": "TASK", "confidence": 0.9363305270671844}]}, {"text": "In contrast to earlier work, we not only extract paraphrasing rules, but also automatically determine which of the potentially relevant rules to apply to an input sentence and produce a revised form using them.", "labels": [], "entities": []}, {"text": "Previous approaches to paraphrase acquisition focused on certain rigid types of paraphrases, for instance, limiting the number of arguments.", "labels": [], "entities": [{"text": "paraphrase acquisition", "start_pos": 23, "end_pos": 45, "type": "TASK", "confidence": 0.9749230444431305}]}, {"text": "In contrast, our method is not limited to a set of a priori-specified paraphrase types.", "labels": [], "entities": []}, {"text": "Use of comparable corpora and minimal use of knowledge resources.", "labels": [], "entities": []}, {"text": "In addition to the advantages mentioned above, comparable corpora can be easily obtained for many domains, whereas previous approaches to paraphrase acquisition (and the related problem of phrasebased machine translation)) required parallel corpora.", "labels": [], "entities": [{"text": "paraphrase acquisition", "start_pos": 138, "end_pos": 160, "type": "TASK", "confidence": 0.9140636920928955}, {"text": "phrasebased machine translation", "start_pos": 189, "end_pos": 220, "type": "TASK", "confidence": 0.6633881827195486}]}, {"text": "We point out that one such approach, recently proposed by, also represents paraphrases by lattices, similarly to our method, although their lattices are derived using parse information.", "labels": [], "entities": []}, {"text": "Moreover, our algorithm does not employ knowledge resources such as parsers or lexical databases, which may not be available or appropriate for all domains -a key issue since paraphrasing is typically domain-dependent.", "labels": [], "entities": []}, {"text": "Nonetheless, our algorithm achieves good performance.", "labels": [], "entities": []}], "datasetContent": [{"text": "All evaluations involved judgments by native speakers of English who were not familiar with the paraphrasing systems under consideration.", "labels": [], "entities": []}, {"text": "We implemented our system on a pair of comparable corpora consisting of articles produced between September 2000 and August 2002 by the Agence France-Presse (AFP) and Reuters news agencies.", "labels": [], "entities": []}, {"text": "Given our interest in domain-dependent paraphrasing, we limited attention to 9MB of articles, collected using a TDT-style document clustering system, concerning individual acts of violence in Israel and army raids on the Palestinian territories.", "labels": [], "entities": []}, {"text": "From this data (after removing 120 articles as a held-  Before evaluating the quality of the rewritings produced by our templates and lattices, we first tested the quality of a random sample of just the template pairs.", "labels": [], "entities": []}, {"text": "In our instructions to the judges, we defined two text units (such as sentences or snippets) to be paraphrases if one of them can generally be substituted for the other without great loss of information (but not necessarily vice versa).", "labels": [], "entities": []}, {"text": "Given a pair of templates produced by a system, the judges marked them as paraphrases if for many instantiations of the templates' variables, the resulting text units were paraphrases.", "labels": [], "entities": []}, {"text": "(Several labelled examples were provided to supply further guidance).", "labels": [], "entities": []}, {"text": "To put the evaluation results into context, we wanted to compare against another system, but we are not aware The extracted paraphrases are available at http://www.cs.cornell.edu/Info/Projects/ NLP/statpar.html We switched to this \"one-sided\" definition because in initial tests judges found it excruciating to decide on equivalence.", "labels": [], "entities": []}, {"text": "Also, in applications such as summarization some information loss is acceptable. of any previous work creating templates precisely for the task of generating paraphrases.", "labels": [], "entities": [{"text": "summarization", "start_pos": 30, "end_pos": 43, "type": "TASK", "confidence": 0.9940787553787231}]}, {"text": "Instead, we made a good-faith effort to adapt the DIRT system) to the problem, selecting the 6,534 highestscoring templates it produced when run on our datasets.", "labels": [], "entities": []}, {"text": "(The system of was unsuitable for evaluation purposes because their paraphrase extraction component is too tightly coupled to the underlying information extraction system.)", "labels": [], "entities": [{"text": "paraphrase extraction", "start_pos": 68, "end_pos": 89, "type": "TASK", "confidence": 0.7142269462347031}]}, {"text": "It is important to note some important caveats in making this comparison, the most prominent being that DIRT was not designed with sentence-paraphrase generation in mind -its templates are much shorter than ours, which may have affected the evaluators' judgments -and was originally implemented on much larger data sets.", "labels": [], "entities": [{"text": "sentence-paraphrase generation", "start_pos": 131, "end_pos": 161, "type": "TASK", "confidence": 0.7279334664344788}]}, {"text": "The point of this evaluation is simply to determine whether another corpusbased paraphrase-focused approach could easily achieve the same performance level.", "labels": [], "entities": []}, {"text": "In brief, the DIRT system works as follows.", "labels": [], "entities": []}, {"text": "Dependency trees are constructed from parsing a large corpus.", "labels": [], "entities": []}, {"text": "Leaf-to-leaf paths are extracted from these dependency trees, with the leaves serving as slots.", "labels": [], "entities": []}, {"text": "Then, pairs of paths in which the slots tend to be filled by similar values, where the similarity measure is based on the mutual information between the value and the slot, are deemed to be paraphrases.", "labels": [], "entities": []}, {"text": "We randomly extracted 500 pairs from the two algorithms' output sets.", "labels": [], "entities": []}, {"text": "Of these, 100 paraphrases (50 per system) made up a \"common\" set evaluated by all four judges, allowing us to compute agreement rates; in addition, each judge also evaluated another \"individual\" set, seen only by him-or herself, consisting of another 100 pairs (50 per system).", "labels": [], "entities": []}, {"text": "The \"individual\" sets allowed us to broaden our sample's coverage of the corpus.", "labels": [], "entities": []}, {"text": "The pairs were presented in random order, and the judges were not told which system produced a given pair.", "labels": [], "entities": []}, {"text": "As shows, our system outperforms the DIRT system, with a consistent performance gap for all the judges of about 38%, although the absolute scores vary (for example, Judge 4 seems lenient).", "labels": [], "entities": []}, {"text": "The judges' assessment of correctness was fairly constant between the full 100-instance set and just the 50-instance common set alone.", "labels": [], "entities": []}, {"text": "In terms of agreement, the Kappa value (measuring pairwise agreement discounting chance occurrences 9 ) on the common set was 0.54, which corresponds to moderate agreement.", "labels": [], "entities": []}, {"text": "Multiway agreement is depicted in -there, we see that in 86 of 100 cases, at least three of the judges gave the same correctness assessment, and in 60 cases all four judges concurred.", "labels": [], "entities": [{"text": "correctness assessment", "start_pos": 117, "end_pos": 139, "type": "METRIC", "confidence": 0.9461719691753387}]}, {"text": "Finally, we evaluated the quality of the paraphrase sentences generated by our system, thus (indirectly) testing all the system components: pattern selection, paraphrase acquisition, and generation.", "labels": [], "entities": [{"text": "pattern selection", "start_pos": 140, "end_pos": 157, "type": "TASK", "confidence": 0.6743890643119812}, {"text": "paraphrase acquisition", "start_pos": 159, "end_pos": 181, "type": "TASK", "confidence": 0.8901743292808533}]}, {"text": "We are not aware of another system generating sentence-level paraphrases.", "labels": [], "entities": []}, {"text": "Therefore, we used as a baseline a simple paraphrasing system that just replaces words with one of their randomly-chosen WordNet synonyms (using the most frequent sense of the word that WordNet listed synonyms for).", "labels": [], "entities": []}, {"text": "The number of substitutions was set proportional to the number of words our method replaced in the same sentence.", "labels": [], "entities": []}, {"text": "The point of this comparison is to check whether simple synonym substitution yields results comparable to those of our algo-rithm.", "labels": [], "entities": [{"text": "synonym substitution", "start_pos": 56, "end_pos": 76, "type": "TASK", "confidence": 0.7718400359153748}]}, {"text": "For this experiment, we randomly selected 20 AFP articles about violence in the Middle East published later than the articles in our training corpus.", "labels": [], "entities": [{"text": "AFP articles about violence in the Middle East", "start_pos": 45, "end_pos": 91, "type": "TASK", "confidence": 0.6105587035417557}]}, {"text": "Out of 484 sentences in this set, our system was able to paraphrase 59 (12.2%).", "labels": [], "entities": []}, {"text": "(We chose parameters that optimized precision rather than recall on our small held-out set.)", "labels": [], "entities": [{"text": "precision", "start_pos": 36, "end_pos": 45, "type": "METRIC", "confidence": 0.9987232089042664}, {"text": "recall", "start_pos": 58, "end_pos": 64, "type": "METRIC", "confidence": 0.9988334774971008}]}, {"text": "We found that after proper name substitution, only seven sentences in the test set appeared in the training set, 11 which implies that lattices boost the generalization power of our method significantly: from seven to 59 sentences.", "labels": [], "entities": [{"text": "proper name substitution", "start_pos": 20, "end_pos": 44, "type": "TASK", "confidence": 0.6850431164105734}]}, {"text": "Interestingly, the coverage of the system varied significantly with article length.", "labels": [], "entities": [{"text": "coverage", "start_pos": 19, "end_pos": 27, "type": "METRIC", "confidence": 0.9745625853538513}]}, {"text": "For the eight articles often or fewer sentences, we paraphrased 60.8% of the sentences per article on average, but for longer articles only 9.3% of the sentences per article on average were paraphrased.", "labels": [], "entities": []}, {"text": "Our analysis revealed that long articles tend to include large portions that are unique to the article, such as personal stories of the event participants, which explains why our algorithm had a lower paraphrasing rate for such articles.", "labels": [], "entities": []}, {"text": "All 118 instances (59 per system) were presented in random order to two judges, who were asked to indicate whether the meaning had been preserved.", "labels": [], "entities": []}, {"text": "Of the paraphrases generated by our system, the two evaluators deemed 81.4% and 78%, respectively, to be valid, whereas for the baseline system, the correctness results were 69.5% and 66.1%, respectively.", "labels": [], "entities": []}, {"text": "Agreement according to the Kappa statistic was 0.6.", "labels": [], "entities": [{"text": "Kappa statistic", "start_pos": 27, "end_pos": 42, "type": "DATASET", "confidence": 0.628401443362236}]}, {"text": "Note that judging full sentences is inherently easier than judging templates, because template comparison requires considering a variety of possible slot values, while sentences are self-contained units.", "labels": [], "entities": []}, {"text": "shows two example sentences, one where our MSA-based paraphrase was deemed correct by both judges, and one where both judges deemed the MSAgenerated paraphrase incorrect.", "labels": [], "entities": []}, {"text": "Examination of the results indicates that the two systems make essentially orthogonal types of errors.", "labels": [], "entities": []}, {"text": "The baseline system's relatively poor performance supports our claim that whole-sentence paraphrasing is a hard task even when accurate wordlevel paraphrases are given.", "labels": [], "entities": [{"text": "whole-sentence paraphrasing", "start_pos": 74, "end_pos": 101, "type": "TASK", "confidence": 0.7438217401504517}]}], "tableCaptions": []}