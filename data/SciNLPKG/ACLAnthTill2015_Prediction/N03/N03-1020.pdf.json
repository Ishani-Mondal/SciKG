{"title": [{"text": "Automatic Evaluation of Summaries Using N-gram Co-Occurrence Statistics", "labels": [], "entities": [{"text": "Automatic Evaluation of Summaries", "start_pos": 0, "end_pos": 33, "type": "TASK", "confidence": 0.46910687536001205}]}], "abstractContent": [{"text": "Following the recent adoption by the machine translation community of automatic evaluation using the BLEU/NIST scoring process, we conduct an in-depth study of a similar idea for evaluating summaries.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 37, "end_pos": 56, "type": "TASK", "confidence": 0.7287601232528687}, {"text": "BLEU", "start_pos": 101, "end_pos": 105, "type": "METRIC", "confidence": 0.9816067218780518}]}, {"text": "The results show that automatic evaluation using unigram co-occurrences between summary pairs correlates surprising well with human evaluations, based on various statistical metrics; while direct application of the BLEU evaluation procedure does not always give good results.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 215, "end_pos": 219, "type": "METRIC", "confidence": 0.9469953775405884}]}], "introductionContent": [{"text": "Automated text summarization has drawn a lot of interest in the natural language processing and information retrieval communities in the recent years.", "labels": [], "entities": [{"text": "Automated text summarization", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.7178348004817963}, {"text": "natural language processing and information retrieval", "start_pos": 64, "end_pos": 117, "type": "TASK", "confidence": 0.6223242084185282}]}, {"text": "A series of workshops on automatic text summarization), special topic sessions in ACL, COLING, and SIGIR, and government sponsored evaluation efforts in the United States and Japan ( have advanced the technology and produced a couple of experimental online systems (.", "labels": [], "entities": [{"text": "text summarization", "start_pos": 35, "end_pos": 53, "type": "TASK", "confidence": 0.649661511182785}]}, {"text": "Despite these efforts, however, there are no common, convenient, and repeatable evaluation methods that can be easily applied to support system development and just-in-time comparison among different summarization methods.", "labels": [], "entities": []}, {"text": "The Document Understanding Conference run by the National Institute of Standards and Technology (NIST) sets out to address this problem by providing annual large scale common evaluations in text summarization.", "labels": [], "entities": [{"text": "Document Understanding", "start_pos": 4, "end_pos": 26, "type": "TASK", "confidence": 0.8130073547363281}, {"text": "text summarization", "start_pos": 190, "end_pos": 208, "type": "TASK", "confidence": 0.5979876220226288}]}, {"text": "However, these evaluations involve human judges and hence are subject to variability).", "labels": [], "entities": []}, {"text": "For example, pointed out that 18% of the data contained multiple judgments in the DUC 2001 single document evaluation . To further progress in automatic summarization, in this paper we conduct an in-depth study of automatic evaluation methods based on n-gram co-occurrence in the context of DUC.", "labels": [], "entities": [{"text": "DUC 2001 single document evaluation", "start_pos": 82, "end_pos": 117, "type": "DATASET", "confidence": 0.9371506214141846}, {"text": "summarization", "start_pos": 153, "end_pos": 166, "type": "TASK", "confidence": 0.768818736076355}, {"text": "DUC", "start_pos": 291, "end_pos": 294, "type": "DATASET", "confidence": 0.9513285756111145}]}, {"text": "Due to the setup in DUC, the evaluations we discussed here are intrinsic evaluations.", "labels": [], "entities": [{"text": "DUC", "start_pos": 20, "end_pos": 23, "type": "DATASET", "confidence": 0.9218215942382812}]}, {"text": "Section 2 gives an overview of the evaluation procedure used in DUC.", "labels": [], "entities": [{"text": "DUC", "start_pos": 64, "end_pos": 67, "type": "TASK", "confidence": 0.6030477285385132}]}, {"text": "Section 3 discusses the IBM BLEU () and NIST (2002) n-gram co-occurrence scoring procedures and the application of a similar idea in evaluating summaries.", "labels": [], "entities": [{"text": "IBM", "start_pos": 24, "end_pos": 27, "type": "DATASET", "confidence": 0.5185441374778748}, {"text": "BLEU", "start_pos": 28, "end_pos": 32, "type": "METRIC", "confidence": 0.9405550360679626}, {"text": "NIST (2002) n-gram", "start_pos": 40, "end_pos": 58, "type": "DATASET", "confidence": 0.8673213958740235}]}, {"text": "Section 4 compares n-gram cooccurrence scoring procedures in terms of their correlation to human results and on the recall and precision of statistical significance prediction.", "labels": [], "entities": [{"text": "recall", "start_pos": 116, "end_pos": 122, "type": "METRIC", "confidence": 0.9994606375694275}, {"text": "precision", "start_pos": 127, "end_pos": 136, "type": "METRIC", "confidence": 0.9978904128074646}, {"text": "statistical significance prediction", "start_pos": 140, "end_pos": 175, "type": "TASK", "confidence": 0.6237636109193166}]}, {"text": "Section 5 concludes this paper and discusses future directions.", "labels": [], "entities": []}], "datasetContent": [{"text": "For each document or document set, one human summary was created as the 'ideal' model summary at each specified length.", "labels": [], "entities": []}, {"text": "Two other human summaries were also created at each length.", "labels": [], "entities": []}, {"text": "In addition, baseline summaries were created automatically for each length as reference points.", "labels": [], "entities": []}, {"text": "For the multi-document summarization task, one baseline, lead baseline, took the first 50, 100, 200, and 400 words in the last document in the collection.", "labels": [], "entities": []}, {"text": "A second baseline, coverage baseline, took the first sentence in the first document, the first sentence in the second document and soon until it had a summary of 50, 100, 200, or 400 words.", "labels": [], "entities": [{"text": "coverage", "start_pos": 19, "end_pos": 27, "type": "METRIC", "confidence": 0.9415697455406189}]}, {"text": "Only one baseline (baseline1) was created for the single document summarization task.", "labels": [], "entities": [{"text": "single document summarization task", "start_pos": 50, "end_pos": 84, "type": "TASK", "confidence": 0.6276254057884216}]}, {"text": "To evaluate system performance NIST assessors who created the 'ideal' written summaries did pairwise comparisons of their summaries to the system-generated summaries, other assessors' summaries, and baseline summaries.", "labels": [], "entities": []}, {"text": "They used the Summary Evaluation Environment (SEE) 2.0 developed by to support the process.", "labels": [], "entities": [{"text": "Summary Evaluation Environment (SEE)", "start_pos": 14, "end_pos": 50, "type": "TASK", "confidence": 0.7257344027360281}]}, {"text": "Using SEE, the assessors compared the system's text (the peer text) to the ideal (the model text).", "labels": [], "entities": [{"text": "SEE", "start_pos": 6, "end_pos": 9, "type": "METRIC", "confidence": 0.7416700124740601}]}, {"text": "As shown in, each text was decomposed into a list of units and displayed in separate windows.", "labels": [], "entities": []}, {"text": "SEE 2.0 provides interfaces for assessors to judge both the content and the quality of summaries.", "labels": [], "entities": [{"text": "SEE 2.0", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.6989935040473938}]}, {"text": "To measure content, assessors step through each model unit, mark all system units sharing content with the current model unit (green/dark gray highlight in the model summary window), and specify that the marked system units express all, most, some, or hardly any of the content of the.", "labels": [], "entities": []}, {"text": "SEE in an evaluation session.", "labels": [], "entities": [{"text": "SEE", "start_pos": 0, "end_pos": 3, "type": "METRIC", "confidence": 0.9741078615188599}]}, {"text": "To measure quality, assessors rate grammaticality 3 , cohesion , and coherence 5 at five different levels: all, most, some, hardly any, or none 6 . For example, as shown in, an assessor marked system units 1.1 and 10.4 (red/dark underlines in the left pane) as sharing some content with the current model unit 2.2 (highlighted green/dark gray in the right).", "labels": [], "entities": []}, {"text": "Recall at different compression ratios has been used in summarization research to measure how well an automatic system retains important content of original documents ().", "labels": [], "entities": [{"text": "Recall", "start_pos": 0, "end_pos": 6, "type": "METRIC", "confidence": 0.9596813321113586}, {"text": "summarization", "start_pos": 56, "end_pos": 69, "type": "TASK", "confidence": 0.9926260113716125}]}, {"text": "However, the simple sentence recall measure cannot differentiate system performance appropriately, as is pointed out by.", "labels": [], "entities": [{"text": "recall", "start_pos": 29, "end_pos": 35, "type": "METRIC", "confidence": 0.8603695034980774}]}, {"text": "Therefore, instead of pure sentence recall score, we use coverage score C.", "labels": [], "entities": [{"text": "recall score", "start_pos": 36, "end_pos": 48, "type": "METRIC", "confidence": 0.9590692520141602}, {"text": "coverage score C", "start_pos": 57, "end_pos": 73, "type": "METRIC", "confidence": 0.9681392113367716}]}, {"text": "We define it as follows 7 : E, the ratio of completeness, ranges from 1 to 0: 1 for all, 3/4 for most, 1/2 for some, 1/4 for hardly any, and 0 for none.", "labels": [], "entities": []}, {"text": "If we ignore E (set it to 1), we obtain simple sentence recall score.", "labels": [], "entities": [{"text": "recall score", "start_pos": 56, "end_pos": 68, "type": "METRIC", "confidence": 0.9783341288566589}]}, {"text": "We use average coverage scores derived from human judgments as the references to evaluate various automatic scoring methods in the following sections.", "labels": [], "entities": [{"text": "average coverage scores", "start_pos": 7, "end_pos": 30, "type": "METRIC", "confidence": 0.7344899574915568}]}, {"text": "In order to evaluate the effectiveness of automatic evaluation metrics, we propose two criteria: 1.", "labels": [], "entities": []}, {"text": "Automatic evaluations should correlate highly, positively, and consistently with human assessments.", "labels": [], "entities": []}, {"text": "2. The statistical significance of automatic evaluations should be a good predictor of the statistical significance of human assessments with high reliability.", "labels": [], "entities": []}, {"text": "The first criterion ensures whenever a human recognizes a good summary/translation/system, an automatic evaluation will do the same with high probability.", "labels": [], "entities": [{"text": "summary/translation/system", "start_pos": 63, "end_pos": 89, "type": "TASK", "confidence": 0.700090742111206}]}, {"text": "This enables us to use an automatic evaluation procedure in place of human assessments to compare system performance, as in the NIST MT evaluations.", "labels": [], "entities": [{"text": "NIST MT evaluations", "start_pos": 128, "end_pos": 147, "type": "DATASET", "confidence": 0.7265635331471761}]}, {"text": "The second criterion is critical in interpreting the significance of automatic evaluation results.", "labels": [], "entities": []}, {"text": "For example, if an automatic evaluation shows there is a significant difference between run A and run B at \u03b1 = 0.05 using the z-test (t-test or bootstrap resampling), how does this translate to \"real\" significance, i.e. the statistical significance in a human assessment of run A and run B?", "labels": [], "entities": []}, {"text": "Ideally, we would like thereto be a positive correlation between them.", "labels": [], "entities": []}, {"text": "If this can be asserted with strong reliability (high recall and precision), then we can use the automatic evaluation to assist system development and to be reasonably sure that we have made progress.", "labels": [], "entities": [{"text": "recall", "start_pos": 54, "end_pos": 60, "type": "METRIC", "confidence": 0.9986956715583801}, {"text": "precision", "start_pos": 65, "end_pos": 74, "type": "METRIC", "confidence": 0.9966371059417725}]}], "tableCaptions": [{"text": " Table 1. Spearman rank order correlation coeffi- cients of different DUC 2001 data between  Ngram(1, 4) n rankings and human rankings includ- ing (S) and excluding (SX) stopwords. SD-100 is  for single document summaries of 100 words and  MD-50, 100, 200, and 400 are for multi-document  summaries of 50, 100, 200, and 400 words. MD-All  averages results from summaries of all sizes.", "labels": [], "entities": [{"text": "DUC 2001 data", "start_pos": 70, "end_pos": 83, "type": "DATASET", "confidence": 0.967934787273407}]}, {"text": " Table 3. Various Ngram(i, j) rank/score correlations  for 4 different statistics (with stopwords).", "labels": [], "entities": [{"text": "Ngram(i, j) rank/score correlations", "start_pos": 18, "end_pos": 53, "type": "METRIC", "confidence": 0.49721601903438567}]}, {"text": " Table 2. Various Ngram(i,j) rank/score correlations  for 4 different statistics (without stopwords): Spear- man rank order coefficient correlation (Spearman \u03c1),  linear regression t-test (LR t ), Pearson product mo- ment coefficient of correlation (Pearson \u03c1), and co- efficient of determination (CD).", "labels": [], "entities": [{"text": "Spear- man rank order coefficient correlation (Spearman \u03c1)", "start_pos": 102, "end_pos": 160, "type": "METRIC", "confidence": 0.669683732769706}, {"text": "Pearson product mo- ment coefficient of correlation (Pearson \u03c1)", "start_pos": 197, "end_pos": 260, "type": "METRIC", "confidence": 0.8126368224620819}, {"text": "co- efficient of determination (CD)", "start_pos": 266, "end_pos": 301, "type": "METRIC", "confidence": 0.8040028810501099}]}]}