{"title": [{"text": "Example Selection for Bootstrapping Statistical Parsers", "labels": [], "entities": []}], "abstractContent": [{"text": "This paper investigates bootstrapping for statistical parsers to reduce their reliance on manually annotated training data.", "labels": [], "entities": []}, {"text": "We consider both a mostly-unsupervised approach, co-training, in which two parsers are iteratively retrained on each other's output; and a semi-supervised approach, corrected co-training, in which a human corrects each parser's output before adding it to the training data.", "labels": [], "entities": []}, {"text": "The selection of labeled training examples is an integral part of both frameworks.", "labels": [], "entities": []}, {"text": "We propose several selection methods based on the criteria of minimizing errors in the data and maximizing training utility.", "labels": [], "entities": []}, {"text": "We show that incorporating the utility criterion into the selection method results in better parsers for both frameworks.", "labels": [], "entities": []}], "introductionContent": [{"text": "Current state-of-the-art statistical parsers) are trained on large annotated corpora such as the Penn Treebank (.", "labels": [], "entities": [{"text": "Penn Treebank", "start_pos": 97, "end_pos": 110, "type": "DATASET", "confidence": 0.9941006898880005}]}, {"text": "However, the production of such corpora is expensive and labor-intensive.", "labels": [], "entities": []}, {"text": "Given this bottleneck, there is considerable interest in (partially) automating the annotation process.", "labels": [], "entities": []}, {"text": "To overcome this bottleneck, two approaches from machine learning have been applied to training parsers.", "labels": [], "entities": []}, {"text": "One is sample selection), a variant of active learning, which tries to identify a small set of unlabeled sentences with high training utility for the human to label . Sentences with high training utility are those most likely to improve the parser.", "labels": [], "entities": []}, {"text": "The other approach, and the focus of this paper, is co-training), a mostlyunsupervised algorithm that replaces the human by having two (or more) parsers label training examples for each other.", "labels": [], "entities": []}, {"text": "The goal is for both parsers to improve by bootstrapping off each other's strengths.", "labels": [], "entities": []}, {"text": "Because the parsers may label examples incorrectly, only a subset of their output, chosen by some selection mechanism, is used in order to minimize errors.", "labels": [], "entities": []}, {"text": "The choice of selection method significantly affects the quality of the resulting parsers.", "labels": [], "entities": []}, {"text": "We investigate a novel approach of selecting training examples for co-training parsers by incorporating the idea of maximizing training utility from sample selection.", "labels": [], "entities": []}, {"text": "The selection mechanism is integral to both sample selection and co-training; however, because co-training and sample selection have different goals, their selection methods focus on different criteria: co-training typically favors selecting accurately labeled examples, while sample selection typically favors selecting examples with high training utility, which often are not sentences that the parsers already label accurately.", "labels": [], "entities": []}, {"text": "In this work, we investigate selection methods for co-training that explore the trade-off between maximizing training utility and minimizing errors.", "labels": [], "entities": []}, {"text": "Empirical studies were conducted to compare selection methods under both co-training and a semi-supervised framework called corrected co-training, in which the selected examples are manually checked and corrected before being added to the training data.", "labels": [], "entities": []}, {"text": "For co-training, we show that the benefit of selecting examples with high training utility can offset the additional errors they contain.", "labels": [], "entities": []}, {"text": "For corrected co-training, we show that selecting examples with high training utility reduces the number of sentences the human annotator has to check.", "labels": [], "entities": []}, {"text": "For both frameworks, we show that selection methods that maximize training utility find labeled examples that result in better trained parsers than those that only minimize error.", "labels": [], "entities": []}, {"text": "introduced co-training to bootstrap two classifiers with different views of the data.", "labels": [], "entities": []}, {"text": "The two classifiers are initially trained on a small amount of annotated seed data; then they label unannotated data for each other in an iterative training process.", "labels": [], "entities": []}, {"text": "Blum and Mitchell prove that, when the two views are conditionally independent given the label, and each view is sufficient for learning the task, co-training can boost an initial weak learner using unlabeled data.", "labels": [], "entities": []}], "datasetContent": [{"text": "Experiments were performed to compare the effect of the selection methods on co-training and corrected cotraining.", "labels": [], "entities": []}, {"text": "We consider a selection method, S 1 , superior to another, S 2 , if, when a large unlabeled pool of sentences has been exhausted, the examples selected by S 1 (as labeled by the machine, and possibly corrected by the human) improve the parser more than those selected by S 2 . All experiments shared the same general setup, as described below.", "labels": [], "entities": []}, {"text": "For two parsers to co-train, they should generate comparable output but use independent statistical models.", "labels": [], "entities": []}, {"text": "In our experiments, we used a lexicalized context free grammar parser developed by, and a lexicalized Tree Adjoining Grammar parser developed by.", "labels": [], "entities": []}, {"text": "Both parsers were initialized with some seed data.", "labels": [], "entities": []}, {"text": "Since the goal is to minimize human annotated data, the size of the seed data should be small.", "labels": [], "entities": []}, {"text": "In this paper we used a seed set size of 1, 000 sentences, taken from section 2 of the Wall Street Journal (WSJ) Penn Treebank.", "labels": [], "entities": [{"text": "Wall Street Journal (WSJ) Penn Treebank", "start_pos": 87, "end_pos": 126, "type": "DATASET", "confidence": 0.9268932417035103}]}, {"text": "The total pool of unlabeled sentences was the remainder of sections 2-21 (stripped of their annotations), consisting of about 38,000 sentences.", "labels": [], "entities": []}, {"text": "The cache size is set at 500 sentences.", "labels": [], "entities": []}, {"text": "We have explored using different settings for the seed set size (.", "labels": [], "entities": []}, {"text": "The parsers were evaluated on unseen test sentences (section 23 of the WSJ corpus).", "labels": [], "entities": [{"text": "WSJ corpus", "start_pos": 71, "end_pos": 81, "type": "DATASET", "confidence": 0.9687391221523285}]}, {"text": "Section 0 was used as a development set for determining parameters.", "labels": [], "entities": []}, {"text": "The evaluation metric is the Parseval F-score over labeled constituents: F-score = 2\u00d7LR\u00d7LP LR+LP , where LP and LR are labeled precision and recall rate, respectively.", "labels": [], "entities": [{"text": "Parseval F-score", "start_pos": 29, "end_pos": 45, "type": "METRIC", "confidence": 0.6149576306343079}, {"text": "F-score", "start_pos": 73, "end_pos": 80, "type": "METRIC", "confidence": 0.9957370758056641}, {"text": "precision", "start_pos": 127, "end_pos": 136, "type": "METRIC", "confidence": 0.8626437187194824}, {"text": "recall rate", "start_pos": 141, "end_pos": 152, "type": "METRIC", "confidence": 0.9868482649326324}]}, {"text": "Both parsers were evaluated, but for brevity, all results reported here are for the Collins parser, which received higher Parseval scores.", "labels": [], "entities": [{"text": "Collins parser", "start_pos": 84, "end_pos": 98, "type": "DATASET", "confidence": 0.90380859375}, {"text": "Parseval", "start_pos": 122, "end_pos": 130, "type": "METRIC", "confidence": 0.9308766722679138}]}, {"text": "The average accuracy rates (except for those selected by S diff-10% ) are about 95%.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 12, "end_pos": 20, "type": "METRIC", "confidence": 0.9995449185371399}]}, {"text": "We first examine the effect of the three selection methods on co-training without correction (i.e., the chosen machine-labeled training examples may contain errors).", "labels": [], "entities": [{"text": "correction", "start_pos": 82, "end_pos": 92, "type": "METRIC", "confidence": 0.9695619344711304}]}, {"text": "Because the selection decisions are based on the scores that the parsers assign to their outputs, the reliability of the scoring function has a significant impact on the performance of the selection methods.", "labels": [], "entities": []}, {"text": "We evaluate the effectiveness of the selection methods using two scoring functions.", "labels": [], "entities": []}, {"text": "In Section 4.2.1, each parser assesses its output with an oracle scoring function that returns the Parseval F-score of the output (as compared to the human annotated gold-standard).", "labels": [], "entities": [{"text": "Parseval", "start_pos": 99, "end_pos": 107, "type": "METRIC", "confidence": 0.9873132109642029}, {"text": "F-score", "start_pos": 108, "end_pos": 115, "type": "METRIC", "confidence": 0.5352650284767151}]}, {"text": "This is an idealized condition that gives us direct control over the error rate of the labeled training data.", "labels": [], "entities": [{"text": "error rate", "start_pos": 69, "end_pos": 79, "type": "METRIC", "confidence": 0.9605360925197601}]}, {"text": "By keeping the error rates constant, our goal is to determine which selection method is more successful in finding sentences with high training utility.", "labels": [], "entities": []}, {"text": "In Section 4.2.2 we replace the oracle scoring function with f prob , which returns the conditional probability of the best parse as the score.", "labels": [], "entities": []}, {"text": "We compare how the selection methods' performances degrade under the realistic condition of basing selection decisions on unreliable parser output assessment scores.", "labels": [], "entities": []}, {"text": "To address the problem of the training data accumulating too many errors overtime, Pierce and Cardie proposed a semi-supervised variant of co-training called corrected co-training, which allows a human annotator to review and correct the output of the parsers before adding it to the training data.", "labels": [], "entities": []}, {"text": "The main selection criterion in their co-training system is accuracy (approximated by confidence).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 60, "end_pos": 68, "type": "METRIC", "confidence": 0.9996073842048645}]}, {"text": "They argue that selecting examples with nearly correct labels would require few manual interventions from the annotator.", "labels": [], "entities": []}, {"text": "We hypothesize that it maybe beneficial to consider the training utility criterion in this framework as well.", "labels": [], "entities": []}, {"text": "We perform experiments to determine whether selecting fewer (and possibly less accurately labeled) exam- ples with higher training utility would require less effort from the annotator.", "labels": [], "entities": []}, {"text": "In our experiments, we simulated the interactive sample selection process by revealing the gold standard.", "labels": [], "entities": []}, {"text": "As before, we compare the three selection methods using both f F-score and f prob as scoring functions.", "labels": [], "entities": [{"text": "F-score", "start_pos": 63, "end_pos": 70, "type": "METRIC", "confidence": 0.9591180682182312}]}, {"text": "6 shows the effect of the three selection methods (using the strict parameter setting) on corrected cotraining.", "labels": [], "entities": []}, {"text": "As a point of reference, we plot the improvement rate fora fully supervised parser (same as the one in).", "labels": [], "entities": []}, {"text": "In addition to charting the parser's performance in terms of the number of labeled training sentences (left graph), we also chart the parser's performance in terms of the the number of constituents the machine mislabeled (right graph).", "labels": [], "entities": []}, {"text": "The pair of graphs indicates the amount of human effort required: the left graph shows the number of sentences the human has to check, and the right graph shows the number of constituents the human has to correct.", "labels": [], "entities": []}], "tableCaptions": []}