{"title": [], "abstractContent": [{"text": "We describe initial experiments in combining the output of question answering systems using data from the 2002 TREC Question Answering task.", "labels": [], "entities": [{"text": "question answering", "start_pos": 59, "end_pos": 77, "type": "TASK", "confidence": 0.8282650709152222}, {"text": "TREC Question Answering task", "start_pos": 111, "end_pos": 139, "type": "TASK", "confidence": 0.7809217274188995}]}, {"text": "We explore several distance-based combining methods, as well as a number of distance metrics involving both word and character ngrams.", "labels": [], "entities": []}], "introductionContent": [{"text": "Progress in question answering technology can be measured as individual systems improve inaccuracy, but it is not the only way to witness technological progress.", "labels": [], "entities": [{"text": "question answering", "start_pos": 12, "end_pos": 30, "type": "TASK", "confidence": 0.7592005729675293}]}, {"text": "A question one can ask is how well we can perform automatic question answering as a community.", "labels": [], "entities": [{"text": "question answering", "start_pos": 60, "end_pos": 78, "type": "TASK", "confidence": 0.6867815554141998}]}, {"text": "If we were asked to enter an Earth English system in an intergalactic TREC, how well would we do?", "labels": [], "entities": []}, {"text": "One easy answer is that we would perform as well as the best QA system.", "labels": [], "entities": []}, {"text": "A second answer is that perhaps we could do even better by combining systems-this might be expected to work if different systems were independent in their errors.", "labels": [], "entities": []}, {"text": "The follow-up question is how would we build such a system?", "labels": [], "entities": []}, {"text": "Lower bounds on the highest possible performance current technology can achieve on a given dataset have practical value, as well.", "labels": [], "entities": []}, {"text": "They allow us to better estimate how well systems are doing with respect to the underlying difficulty of the dataset, and continually provide performance targets that are known to be achievable.", "labels": [], "entities": []}, {"text": "Without such lower bounds on optimal performance, one cannot determine if technological progress in a domain has simply stalled.", "labels": [], "entities": []}, {"text": "NIST's ROVER system for combining speech recognizer output gives ASR researchers an updated goal to shoot for after every evaluation, as well as an implicit measure of the extent to which systems are making the same errors).", "labels": [], "entities": [{"text": "NIST", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.9589691758155823}, {"text": "ROVER", "start_pos": 7, "end_pos": 12, "type": "METRIC", "confidence": 0.8858535885810852}, {"text": "combining speech recognizer output", "start_pos": 24, "end_pos": 58, "type": "TASK", "confidence": 0.6826600059866905}, {"text": "ASR", "start_pos": 65, "end_pos": 68, "type": "TASK", "confidence": 0.973298192024231}]}, {"text": "The work herein initiates a similar set of experiments for question answering technology.", "labels": [], "entities": [{"text": "question answering", "start_pos": 59, "end_pos": 77, "type": "TASK", "confidence": 0.9280194938182831}]}], "datasetContent": [{"text": "Several measurements were made to ascertain the quality of the various selection techniques, as seen in.", "labels": [], "entities": []}, {"text": "Precision, P, indicates the accuracy of the technique, the percentage of the answers that were judged to be correct.", "labels": [], "entities": [{"text": "Precision, P", "start_pos": 0, "end_pos": 12, "type": "METRIC", "confidence": 0.9556215206782023}, {"text": "accuracy", "start_pos": 28, "end_pos": 36, "type": "METRIC", "confidence": 0.9996178150177002}]}, {"text": "avgP is the main measure used by NIST this year-the average precision of all prefixes of the sequence of answers placed in order of high to low confidence.", "labels": [], "entities": [{"text": "avgP", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.9569898843765259}, {"text": "NIST", "start_pos": 33, "end_pos": 37, "type": "DATASET", "confidence": 0.9358930587768555}, {"text": "precision", "start_pos": 60, "end_pos": 69, "type": "METRIC", "confidence": 0.9423775672912598}]}, {"text": "Strict corresponds to the correctness criterion used by NISTthe answer must be exact and justified by the referenced document (assessor judgment v S ).", "labels": [], "entities": [{"text": "NISTthe", "start_pos": 56, "end_pos": 63, "type": "DATASET", "confidence": 0.8994169235229492}]}, {"text": "The Loose figures discard these two criteria (assessor judgment ).", "labels": [], "entities": []}, {"text": "The Loose P measure was the one that was optimized during development.", "labels": [], "entities": [{"text": "Loose P measure", "start_pos": 4, "end_pos": 19, "type": "METRIC", "confidence": 0.6856356660525004}]}, {"text": "In we see both development and test set results for answer selection experiments involving a sample of the distance measures with which we experimented, as well as the best-performing system involved in the evaluation.", "labels": [], "entities": [{"text": "answer selection", "start_pos": 52, "end_pos": 68, "type": "TASK", "confidence": 0.8879766464233398}]}, {"text": "All of the design and selection of the distance measures was done using hill-climbing on the development set, and only after this exploration was complete was the performance on the test set measured.", "labels": [], "entities": []}, {"text": "Two general observations can be made about these results (and others not shown): taking into account a prior based on the document source (including NIL) is useful, as is working with feature bags from the answers rather than sets.", "labels": [], "entities": []}, {"text": "The bestperforming selection system used all character strings of length 5 and less as features, combined with the multiset Tanimoto distance measure described above, and scaled with document source priors.", "labels": [], "entities": [{"text": "Tanimoto distance measure", "start_pos": 124, "end_pos": 149, "type": "METRIC", "confidence": 0.6910702188809713}]}, {"text": "Furthermore, a numeric string mismatch was weighted to be twice as costly as mismatching a non-numeric string.", "labels": [], "entities": []}, {"text": "Question 1674 provides an example that contrasts this best selector with a simple voting scheme (exact string match): What day did Neil Armstrong land on the moon?", "labels": [], "entities": [{"text": "exact string match", "start_pos": 97, "end_pos": 115, "type": "METRIC", "confidence": 0.8456851442654928}]}, {"text": "1969 (simple voting-incorrect) July 20, 1969 (best measure above-correct) While a plurality of systems answered with 1969, many others answered with variants of the correct answer that differed in punctuation, as well as on; even simply 20.", "labels": [], "entities": []}, {"text": "All of these, including the incorrect instances of 1969, contributed to the correct answer being selected.", "labels": [], "entities": []}], "tableCaptions": []}