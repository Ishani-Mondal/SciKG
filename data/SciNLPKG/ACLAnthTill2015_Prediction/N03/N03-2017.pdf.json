{"title": [], "abstractContent": [{"text": "We present a syntax-based constraint for word alignment, known as the cohesion constraint.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 41, "end_pos": 55, "type": "TASK", "confidence": 0.764273464679718}]}, {"text": "It requires disjoint English phrases to be mapped to non-overlapping intervals in the French sentence.", "labels": [], "entities": []}, {"text": "We evaluate the utility of this constraint in two different algorithms.", "labels": [], "entities": []}, {"text": "The results show that it can provide a significant improvement in alignment quality.", "labels": [], "entities": [{"text": "alignment", "start_pos": 66, "end_pos": 75, "type": "TASK", "confidence": 0.9746506810188293}]}], "introductionContent": [{"text": "The IBM statistical machine translation (SMT) models have been extremely influential in computational linguistics in the past decade.", "labels": [], "entities": [{"text": "statistical machine translation (SMT)", "start_pos": 8, "end_pos": 45, "type": "TASK", "confidence": 0.7401683380206426}, {"text": "computational linguistics", "start_pos": 88, "end_pos": 113, "type": "TASK", "confidence": 0.7333325147628784}]}, {"text": "The (arguably) most striking characteristic of the IBM-style SMT models is their total lack of linguistic knowledge.", "labels": [], "entities": [{"text": "SMT", "start_pos": 61, "end_pos": 64, "type": "TASK", "confidence": 0.8026146292686462}]}, {"text": "The IBM models demonstrated how much one can do with pure statistical techniques, which have inspired a whole new generation of NLP research and systems.", "labels": [], "entities": []}, {"text": "More recently, there have been many proposals to introduce syntactic knowledge into SMT models.", "labels": [], "entities": [{"text": "SMT", "start_pos": 84, "end_pos": 87, "type": "TASK", "confidence": 0.9955850839614868}]}, {"text": "A common theme among these approaches is the assumption that the syntactic structures of a pair of source-target sentences are isomorphic (or nearly isomorphic).", "labels": [], "entities": []}, {"text": "This assumption seems too strong.", "labels": [], "entities": []}, {"text": "Human translators often use non-literal translations, which result in differences in syntactic structures.", "labels": [], "entities": []}, {"text": "According to a study in (), such translational divergences are quite common, involving 11-31% of the sentences.", "labels": [], "entities": [{"text": "translational divergences", "start_pos": 33, "end_pos": 58, "type": "TASK", "confidence": 0.9482290148735046}]}, {"text": "We introduce a constraint that uses the dependency tree of the English sentence to maintain phrasal cohesion in the French sentence.", "labels": [], "entities": []}, {"text": "In other words, if two phrases are disjoint in the English sentence, the alignment must not map them to overlapping intervals in the French sentence.", "labels": [], "entities": []}, {"text": "For example, in, the cohesion constraint will rule out the possibility of aligning to with\u00e0with`with\u00e0.", "labels": [], "entities": []}, {"text": "The phrases the reboot and the host to discover all the devices are disjoint, but the partial alignment in maps them to overlapping intervals.", "labels": [], "entities": []}, {"text": "This constraint is weaker than isomorphism.", "labels": [], "entities": []}, {"text": "However, we will show that it can produce a significant increase in alignment quality.", "labels": [], "entities": [{"text": "alignment", "start_pos": 68, "end_pos": 77, "type": "TASK", "confidence": 0.9678424596786499}]}], "datasetContent": [{"text": "To determine the utility of the cohesion constraint, we incorporated it into two alignment algorithms.", "labels": [], "entities": []}, {"text": "The algorithms take as input an English-French sentence pair and the dependency tree of the English sentence.", "labels": [], "entities": []}, {"text": "Both algorithms build an alignment by adding one link at a time.", "labels": [], "entities": []}, {"text": "We implement two versions of each algorithm: one with the cohesion constraint and one without.", "labels": [], "entities": []}, {"text": "We will describe the versions without cohesion constraint below.", "labels": [], "entities": []}, {"text": "For the versions with cohesion constraint, it is understood that each new link must also pass the test described in Section 2.", "labels": [], "entities": []}, {"text": "The first algorithm is similar to Competitive Linking (.", "labels": [], "entities": [{"text": "Competitive Linking", "start_pos": 34, "end_pos": 53, "type": "TASK", "confidence": 0.7170000076293945}]}, {"text": "We use a sentence-aligned corpus to compute the \u03c6 2 correlation metric ( between all English-French word pairs.", "labels": [], "entities": [{"text": "\u03c6 2 correlation metric", "start_pos": 48, "end_pos": 70, "type": "METRIC", "confidence": 0.8906173259019852}]}, {"text": "For a given sentence pair, we begin with an empty alignment.", "labels": [], "entities": []}, {"text": "We then add links in the order of their \u03c6 2 scores so that each word participates in at most one link.", "labels": [], "entities": [{"text": "\u03c6 2 scores", "start_pos": 40, "end_pos": 50, "type": "METRIC", "confidence": 0.8934403856595358}]}, {"text": "We will refer to this as the \u03c6 2 method.", "labels": [], "entities": []}, {"text": "The second algorithm uses a best-first search (with fixed beam width and agenda size) to find an alignment that maximizes P (A|E, F ).", "labels": [], "entities": []}, {"text": "A state in this search space is a partial alignment.", "labels": [], "entities": []}, {"text": "A transition is defined as the addition of a single link to the current state.", "labels": [], "entities": []}, {"text": "The algorithm computes P (A|E, F ) based on statistics obtained from a word-aligned corpus.", "labels": [], "entities": []}, {"text": "We construct the initial corpus with a system that is similar to the \u03c6 2 method.", "labels": [], "entities": []}, {"text": "The algorithm then re-aligns the corpus and trains again for three iterations.", "labels": [], "entities": []}, {"text": "We will refer to this as the P (A|E, F ) method.", "labels": [], "entities": []}, {"text": "The details of this algorithm are described in.", "labels": [], "entities": []}, {"text": "We trained our alignment programs with the same 50K pairs of sentences as) and tested it on the same 500 manually aligned sentences.", "labels": [], "entities": []}, {"text": "Both the training and testing sentences are from the Hansard corpus.", "labels": [], "entities": [{"text": "Hansard corpus", "start_pos": 53, "end_pos": 67, "type": "DATASET", "confidence": 0.9845965206623077}]}, {"text": "We parsed the training and testing corpora with Minipar.", "labels": [], "entities": [{"text": "Minipar", "start_pos": 48, "end_pos": 55, "type": "DATASET", "confidence": 0.9738962054252625}]}, {"text": "We adopted the evaluation methodology in), which defines three metrics: precision, recall and alignment error rate (AER).", "labels": [], "entities": [{"text": "precision", "start_pos": 72, "end_pos": 81, "type": "METRIC", "confidence": 0.9995840191841125}, {"text": "recall", "start_pos": 83, "end_pos": 89, "type": "METRIC", "confidence": 0.9970596432685852}, {"text": "alignment error rate (AER)", "start_pos": 94, "end_pos": 120, "type": "METRIC", "confidence": 0.9392273922761282}]}, {"text": "shows the results of our experiments.", "labels": [], "entities": []}, {"text": "The first four rows correspond to the methods described above.", "labels": [], "entities": []}, {"text": "As a reference point, we also provide the results reported in).", "labels": [], "entities": []}, {"text": "They implemented IBM Model 4 by bootstrapping from an HMM model.", "labels": [], "entities": []}, {"text": "The rows F\u2192E and E\u2192F are the results obtained by this model when treating French as the source and English as the target or vice versa.", "labels": [], "entities": []}, {"text": "The row Refined shows results obtained by taking the intersection of E\u2192F and F\u2192E and then refining this intersection to increase recall.", "labels": [], "entities": [{"text": "recall", "start_pos": 129, "end_pos": 135, "type": "METRIC", "confidence": 0.9984092116355896}]}, {"text": "From, we can see that the addition of the cohesion constraint leads to significant improvements in performance with both algorithms.", "labels": [], "entities": []}, {"text": "The relative reduction in error rate is 16% with the \u03c6 2 method and 36% with the P (A|E, F ) method.", "labels": [], "entities": [{"text": "error rate", "start_pos": 26, "end_pos": 36, "type": "METRIC", "confidence": 0.9881507456302643}]}, {"text": "The improvement comes primarily from increased precision.", "labels": [], "entities": [{"text": "precision", "start_pos": 47, "end_pos": 56, "type": "METRIC", "confidence": 0.9985834360122681}]}, {"text": "With the P (A|E, F ) method, this increase in precision does not come at the expense of recall.", "labels": [], "entities": [{"text": "P (A|E, F )", "start_pos": 9, "end_pos": 20, "type": "METRIC", "confidence": 0.8580478951334953}, {"text": "precision", "start_pos": 46, "end_pos": 55, "type": "METRIC", "confidence": 0.9994984865188599}, {"text": "recall", "start_pos": 88, "end_pos": 94, "type": "METRIC", "confidence": 0.9988172650337219}]}], "tableCaptions": []}