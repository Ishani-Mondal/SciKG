{"title": [{"text": "Shallow Parsing with Conditional Random Fields", "labels": [], "entities": []}], "abstractContent": [{"text": "Conditional random fields for sequence labeling offer advantages over both generative models like HMMs and classifiers applied at each sequence position.", "labels": [], "entities": [{"text": "sequence labeling", "start_pos": 30, "end_pos": 47, "type": "TASK", "confidence": 0.6774303913116455}]}, {"text": "Among sequence labeling tasks in language processing, shallow parsing has received much attention, with the development of standard evaluation datasets and extensive comparison among methods.", "labels": [], "entities": [{"text": "sequence labeling tasks", "start_pos": 6, "end_pos": 29, "type": "TASK", "confidence": 0.7438237567742666}, {"text": "shallow parsing", "start_pos": 54, "end_pos": 69, "type": "TASK", "confidence": 0.6815014183521271}]}, {"text": "We show here how to train a conditional random field to achieve performance as good as any reported base noun-phrase chunking method on the CoNLL task, and better than any reported single model.", "labels": [], "entities": [{"text": "noun-phrase chunking", "start_pos": 105, "end_pos": 125, "type": "TASK", "confidence": 0.7396641373634338}]}, {"text": "Improved training methods based on modern optimization algorithms were critical in achieving these results.", "labels": [], "entities": []}, {"text": "We present extensive comparisons between models and training methods that confirm and strengthen previous results on shallow parsing and training methods for maximum-entropy models.", "labels": [], "entities": []}], "introductionContent": [{"text": "Sequence analysis tasks in language and biology are often described as mappings from input sequences to sequences of labels encoding the analysis.", "labels": [], "entities": [{"text": "Sequence analysis tasks", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.935052732626597}]}, {"text": "In language processing, examples of such tasks include part-of-speech tagging, named-entity recognition, and the task we shall focus on here, shallow parsing.", "labels": [], "entities": [{"text": "language processing", "start_pos": 3, "end_pos": 22, "type": "TASK", "confidence": 0.709121435880661}, {"text": "part-of-speech tagging", "start_pos": 55, "end_pos": 77, "type": "TASK", "confidence": 0.7166025638580322}, {"text": "named-entity recognition", "start_pos": 79, "end_pos": 103, "type": "TASK", "confidence": 0.7216381132602692}, {"text": "shallow parsing", "start_pos": 142, "end_pos": 157, "type": "TASK", "confidence": 0.6715248823165894}]}, {"text": "Shallow parsing identifies the non-recursive cores of various phrase types in text, possibly as a precursor to full parsing or information extraction.", "labels": [], "entities": [{"text": "Shallow parsing", "start_pos": 0, "end_pos": 15, "type": "TASK", "confidence": 0.641149178147316}, {"text": "full parsing", "start_pos": 111, "end_pos": 123, "type": "TASK", "confidence": 0.65674889087677}, {"text": "information extraction", "start_pos": 127, "end_pos": 149, "type": "TASK", "confidence": 0.7348127961158752}]}, {"text": "The paradigmatic shallowparsing problem is NP chunking, which finds the nonrecursive cores of noun phrases called base NPs.", "labels": [], "entities": [{"text": "NP chunking", "start_pos": 43, "end_pos": 54, "type": "TASK", "confidence": 0.8280921578407288}]}, {"text": "The pioneering work of introduced NP chunking as a machine-learning problem, with standard datasets and evaluation metrics.", "labels": [], "entities": [{"text": "NP chunking", "start_pos": 34, "end_pos": 45, "type": "TASK", "confidence": 0.8820737600326538}]}, {"text": "The task was extended to additional phrase types for the CoNLL-2000 shared task), which is now the standard evaluation task for shallow parsing.", "labels": [], "entities": [{"text": "shallow parsing", "start_pos": 128, "end_pos": 143, "type": "TASK", "confidence": 0.6318469643592834}]}, {"text": "Most previous work used two main machine-learning approaches to sequence labeling.", "labels": [], "entities": [{"text": "sequence labeling", "start_pos": 64, "end_pos": 81, "type": "TASK", "confidence": 0.6899255514144897}]}, {"text": "The first approach relies on k-order generative probabilistic models of paired input sequences and label sequences, for instance hidden Markov models (HMMs) or multilevel Markov models ().", "labels": [], "entities": []}, {"text": "The second approach views the sequence labeling problem as a sequence of classification problems, one for each of the labels in the sequence.", "labels": [], "entities": [{"text": "sequence labeling problem", "start_pos": 30, "end_pos": 55, "type": "TASK", "confidence": 0.7249974608421326}]}, {"text": "The classification result at each position may depend on the whole input and on the previous k classifications.", "labels": [], "entities": []}, {"text": "The generative approach provides well-understood training and decoding algorithms for HMMs and more general graphical models.", "labels": [], "entities": []}, {"text": "However, effective generative models require stringent conditional independence assumptions.", "labels": [], "entities": []}, {"text": "For instance, it is not practical to make the label at a given position depend on a window on the input sequence as well as the surrounding labels, since the inference problem for the corresponding graphical model would be intractable.", "labels": [], "entities": []}, {"text": "Non-independent features of the inputs, such as capitalization, suffixes, and surrounding words, are important in dealing with words unseen in training, but they are difficult to represent in generative models.", "labels": [], "entities": []}, {"text": "The sequential classification approach can handle many correlated features, as demonstrated in work on maximum-entropy ( and a variety of other linear classifiers, including winnow), AdaBoost (Abney et al., 1999), and support-vector machines ().", "labels": [], "entities": [{"text": "sequential classification", "start_pos": 4, "end_pos": 29, "type": "TASK", "confidence": 0.7836441397666931}]}, {"text": "Furthermore, they are trained to minimize some function related to labeling error, leading to smaller error in practice if enough training data are available.", "labels": [], "entities": []}, {"text": "In contrast, generative models are trained to maximize the joint probability of the training data, which is not as closely tied to the accuracy metrics of interest if the actual data was not generated by the model, as is always the casein practice.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 135, "end_pos": 143, "type": "METRIC", "confidence": 0.9942188262939453}]}, {"text": "However, since sequential classifiers are trained to make the best local decision, unlike generative models they cannot trade off decisions at different positions against each other.", "labels": [], "entities": []}, {"text": "In other words, sequential classifiers are myopic about the impact of their current decision on later decisions).", "labels": [], "entities": []}, {"text": "This forced the best sequential classifier systems to resort to heuristic combinations of forward-moving and backward-moving sequential classifiers ().", "labels": [], "entities": []}, {"text": "Conditional random fields (CRFs) bring together the best of generative and classification models.", "labels": [], "entities": [{"text": "generative and classification", "start_pos": 60, "end_pos": 89, "type": "TASK", "confidence": 0.8296660780906677}]}, {"text": "Like classification models, they can accommodate many statistically correlated features of the inputs, and they are trained discriminatively.", "labels": [], "entities": []}, {"text": "But like generative models, they can trade off decisions at different sequence positions to obtain a globally optimal labeling.", "labels": [], "entities": []}, {"text": "showed that CRFs beat related classification models as well as HMMs on synthetic data and on a part-of-speech tagging task.", "labels": [], "entities": [{"text": "part-of-speech tagging task", "start_pos": 95, "end_pos": 122, "type": "TASK", "confidence": 0.7637873987356821}]}, {"text": "In the present work, we show that CRFs beat all reported single-model NP chunking results on the standard evaluation dataset, and are statistically indistinguishable from the previous best performer, a voting arrangement of 24 forward-and backward-looking support-vector classifiers ().", "labels": [], "entities": [{"text": "NP chunking", "start_pos": 70, "end_pos": 81, "type": "TASK", "confidence": 0.7575804889202118}]}, {"text": "To obtain these results, we had to abandon the original iterative scaling CRF training algorithm for convex optimization algorithms with better convergence properties.", "labels": [], "entities": []}, {"text": "We provide detailed comparisons between training methods.", "labels": [], "entities": []}, {"text": "The generalized perceptron proposed by is closely related to CRFs, but the best CRF training methods seem to have a slight edge over the generalized perceptron.", "labels": [], "entities": []}], "datasetContent": [{"text": "The standard evaluation metrics fora chunker are precision P (fraction of output chunks that exactly match the reference chunks), recall R (fraction of reference chunks returned by the chunker), and their harmonic mean, the F1 score F 1 = 2 * P * R/(P + R) (which we call just F score in what follows).", "labels": [], "entities": [{"text": "precision P", "start_pos": 49, "end_pos": 60, "type": "METRIC", "confidence": 0.9781957566738129}, {"text": "recall R", "start_pos": 130, "end_pos": 138, "type": "METRIC", "confidence": 0.9805645048618317}, {"text": "F1 score F 1", "start_pos": 224, "end_pos": 236, "type": "METRIC", "confidence": 0.9713508933782578}, {"text": "F score", "start_pos": 277, "end_pos": 284, "type": "METRIC", "confidence": 0.952456146478653}]}, {"text": "The relationships between F score and labeling error or log-likelihood are not direct, so we report both F score and the other metrics for the models we tested.", "labels": [], "entities": [{"text": "F score", "start_pos": 26, "end_pos": 33, "type": "METRIC", "confidence": 0.9781152904033661}, {"text": "F score", "start_pos": 105, "end_pos": 112, "type": "METRIC", "confidence": 0.9876284003257751}]}, {"text": "For comparisons with other reported results we use F score.", "labels": [], "entities": [{"text": "F score", "start_pos": 51, "end_pos": 58, "type": "METRIC", "confidence": 0.9892231523990631}]}], "tableCaptions": [{"text": " Table 2: NP chunking F scores", "labels": [], "entities": [{"text": "NP chunking", "start_pos": 10, "end_pos": 21, "type": "TASK", "confidence": 0.7417209446430206}, {"text": "F", "start_pos": 22, "end_pos": 23, "type": "METRIC", "confidence": 0.6351698637008667}]}, {"text": " Table 3: Runtime for various training methods", "labels": [], "entities": []}, {"text": " Table 4: McNemar's tests on labeling disagreements", "labels": [], "entities": [{"text": "McNemar", "start_pos": 10, "end_pos": 17, "type": "DATASET", "confidence": 0.5343425869941711}]}]}