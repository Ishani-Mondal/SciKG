{"title": [{"text": "Getting More Mileage from Web Text Sources for Conversational Speech Language Modeling using Class-Dependent Mixtures", "labels": [], "entities": [{"text": "Conversational Speech Language Modeling", "start_pos": 47, "end_pos": 86, "type": "TASK", "confidence": 0.6208389103412628}]}], "abstractContent": [{"text": "Sources of training data suitable for language modeling of conversational speech are limited.", "labels": [], "entities": [{"text": "language modeling of conversational speech", "start_pos": 38, "end_pos": 80, "type": "TASK", "confidence": 0.7891518712043762}]}, {"text": "In this paper, we show how training data can be supplemented with text from the web filtered to match the style and/or topic of the target recognition task, but also that it is possible to get bigger performance gains from the data by using class-dependent interpolation of N-grams.", "labels": [], "entities": [{"text": "target recognition task", "start_pos": 132, "end_pos": 155, "type": "TASK", "confidence": 0.807551383972168}]}], "introductionContent": [{"text": "Language models constitute one of the key components in modern speech recognition systems.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 63, "end_pos": 81, "type": "TASK", "confidence": 0.8041177690029144}]}, {"text": "Training an Ngram language model, the most commonly used type of model, requires large quantities of text that is matched to the target recognition task both in terms of style and topic.", "labels": [], "entities": []}, {"text": "In tasks involving conversational speech the ideal training material, i.e. transcripts of conversational speech, is costly to produce, which limits the amount of training data currently available.", "labels": [], "entities": []}, {"text": "Methods have been developed for the purpose of language model adaptation, i.e. the adaptation of an existing model to new topics, domains, or tasks for which little or no training material maybe available.", "labels": [], "entities": [{"text": "language model adaptation", "start_pos": 47, "end_pos": 72, "type": "TASK", "confidence": 0.6556670566399893}]}, {"text": "Since out-of-domain data can contain relevant as well as irrelevant information, various methods are used to identify the most relevant portions of the out-of-domain data prior to combination.", "labels": [], "entities": []}, {"text": "Past work on pre-selection has been based on word frequency counts, probability (or perplexity) of word or part-of-speech sequences), latent semantic analysis, and information retrieval techniques).", "labels": [], "entities": [{"text": "latent semantic analysis", "start_pos": 134, "end_pos": 158, "type": "TASK", "confidence": 0.6593307554721832}, {"text": "information retrieval", "start_pos": 164, "end_pos": 185, "type": "TASK", "confidence": 0.7748988568782806}]}, {"text": "Perplexitybased clustering has also been used for defining topicspecific subsets of in-domain data), and test set perplexity has been used to prune documents from a training corpus).", "labels": [], "entities": []}, {"text": "The most common method for using the additional text sources is to train separate language models on a small amount of in-domain and large amounts of out-of-domain data and to combine them by interpolation, also referred to as mixtures of language models.", "labels": [], "entities": []}, {"text": "The technique was reported by IBM in 1995 (, and has been used by many sites since then.", "labels": [], "entities": []}, {"text": "An alternative approach involves decomposition of the language model into a class n-gram for interpolation, allowing content words to be interpolated with different weights than filled pauses, for example, which gives an improvement over standard mixture modeling for conversational speech.", "labels": [], "entities": []}, {"text": "Recently researchers have turned to the World Wide Web as an additional source of training data for language modeling.", "labels": [], "entities": [{"text": "language modeling", "start_pos": 100, "end_pos": 117, "type": "TASK", "confidence": 0.7679147720336914}]}, {"text": "For \"just-in-time\" language modeling, adaptation data is obtained by submitting words from initial hypotheses of user utterances as queries to a web search engine.", "labels": [], "entities": [{"text": "language modeling", "start_pos": 19, "end_pos": 36, "type": "TASK", "confidence": 0.7348138689994812}]}, {"text": "Their queries, however, treated words as individual tokens and ignored function words.", "labels": [], "entities": []}, {"text": "Such a search strategy typically generates text of a non-conversational style, hence not ideally suited for ASR.", "labels": [], "entities": [{"text": "ASR", "start_pos": 108, "end_pos": 111, "type": "TASK", "confidence": 0.9892712831497192}]}, {"text": "In (), instead of downloading the actual web pages, the authors retrieved Ngram counts provided by the search engine.", "labels": [], "entities": []}, {"text": "Such an approach generates valuable statistics but limits the set of N-grams to ones occurring in the baseline model.", "labels": [], "entities": []}, {"text": "In this paper, we present an approach to extracting additional training data from the web by searching for text that is better matched to a conversational speaking style.", "labels": [], "entities": []}, {"text": "We also show how we can make better use of this new data by applying class-dependent interpolation.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluated on two tasks: 1) Switchboard (, specifically the HUB5 eval 2001 set having a total of 60K words spoken by 120 speakers, and 2) an ICSI Meeting recorder) eval set having a total of 44K words spoken by 25 speakers.", "labels": [], "entities": [{"text": "HUB5 eval 2001 set", "start_pos": 62, "end_pos": 80, "type": "DATASET", "confidence": 0.9597083479166031}, {"text": "ICSI Meeting recorder) eval set", "start_pos": 143, "end_pos": 174, "type": "DATASET", "confidence": 0.9158264299233755}]}, {"text": "Both sets featured spontaneous conversational speech.", "labels": [], "entities": []}, {"text": "There were 45K words of held-out data for each task.", "labels": [], "entities": []}, {"text": "Text corpora of conversational telephone speech (CTS) available for training language models consisted of Switchboard, Callhome English, and Switchboardcellular, a total of 3 million words.", "labels": [], "entities": [{"text": "Text corpora of conversational telephone speech (CTS)", "start_pos": 0, "end_pos": 53, "type": "TASK", "confidence": 0.6005339192019569}]}, {"text": "In addition to that we used 150 million words of Broadcast News (BN) transcripts, and we collected 191 million words of \"conversational\" text from the web.", "labels": [], "entities": [{"text": "Broadcast News (BN) transcripts", "start_pos": 49, "end_pos": 80, "type": "DATASET", "confidence": 0.8925959666570028}]}, {"text": "For the Meetings task, there were 200K words of meeting transcripts available for training, and we collected 28 million words of \"topicrelated\" text from the web.", "labels": [], "entities": []}, {"text": "The experiments were conducted using the SRI large vocabulary speech recognizer) in the N-best rescoring mode.", "labels": [], "entities": [{"text": "SRI large vocabulary speech recognizer", "start_pos": 41, "end_pos": 79, "type": "TASK", "confidence": 0.6801013946533203}]}, {"text": "A baseline bigram language model was used to generate N-best lists, which were then rescored with various trigram models.", "labels": [], "entities": []}, {"text": "shows word error rates (WER) on the HUB5 test set, comparing performance of the class-based mixture against standard (i.e. class-independent) interpolation.", "labels": [], "entities": [{"text": "word error rates (WER)", "start_pos": 6, "end_pos": 28, "type": "METRIC", "confidence": 0.8798086047172546}, {"text": "HUB5 test set", "start_pos": 36, "end_pos": 49, "type": "DATASET", "confidence": 0.970086395740509}]}, {"text": "The class-based mixture gave better results in all cases except when only CTS sources were used, probably because these sources are similar to each other and the class-based mixture is mainly useful when data sources are more diverse.", "labels": [], "entities": []}, {"text": "We also obtained lower WER by using the web data instead of BN, which indicates that the web data is better matched to our task (i.e. it is more \"conversational\").", "labels": [], "entities": [{"text": "WER", "start_pos": 23, "end_pos": 26, "type": "METRIC", "confidence": 0.9987038373947144}, {"text": "BN", "start_pos": 60, "end_pos": 62, "type": "METRIC", "confidence": 0.9370864629745483}]}, {"text": "If training data is completely arbitrary, then its benefits to the recognition task are minimal, as shown by an example of using a 66M-word corpus collected from random web pages.", "labels": [], "entities": [{"text": "recognition task", "start_pos": 67, "end_pos": 83, "type": "TASK", "confidence": 0.9023054540157318}]}, {"text": "The baseline Switchboard model gave test set perplexity of 96, which is reduced to 87 with a standard mixture CTS and BN data, reduced further to 83 by adding the web data, and to a best case of 82 with class-dependent interpolation and the added web data.", "labels": [], "entities": []}, {"text": "Increasing the amount of web training data from 61M to 191M gave relatively small performance gains.", "labels": [], "entities": []}, {"text": "We \"trimmed\" the 191M-word web corpus down to 61M words by choosing documents with lowest perplexity according to the combined CTS model, yielding the \"Web2\" data source.", "labels": [], "entities": [{"text": "Web2\" data source", "start_pos": 152, "end_pos": 169, "type": "DATASET", "confidence": 0.7329685762524605}]}, {"text": "The model that used Web2 gave the same WER as the one trained with the original 61M web corpus.", "labels": [], "entities": [{"text": "WER", "start_pos": 39, "end_pos": 42, "type": "METRIC", "confidence": 0.9982359409332275}, {"text": "61M web corpus", "start_pos": 80, "end_pos": 94, "type": "DATASET", "confidence": 0.92893519004186}]}, {"text": "It could be that the web text obtained with \"Google\" filtering is fairly homogeneous, so little is gained by further perplexity filtering.", "labels": [], "entities": []}, {"text": "Or, it could be that when choosing better matched data, we also exclude new N-grams that may occur only in testing.", "labels": [], "entities": []}, {"text": "Results on the Meeting test set are shown in, where the baseline model was trained on CTS and BN sources.", "labels": [], "entities": [{"text": "Meeting test set", "start_pos": 15, "end_pos": 31, "type": "DATASET", "confidence": 0.9683143099149069}]}, {"text": "As in the HUB5 experiments, the classbased mixture outperformed standard interpolation.", "labels": [], "entities": [{"text": "HUB5", "start_pos": 10, "end_pos": 14, "type": "DATASET", "confidence": 0.9053504467010498}]}, {"text": "We achieved lower WER by using the web data instead of the meeting transcripts, but the best results are obtained by using all data sources.", "labels": [], "entities": [{"text": "WER", "start_pos": 18, "end_pos": 21, "type": "METRIC", "confidence": 0.9951677322387695}]}, {"text": "Language model perplexity is reduced from 122 for the baseline to a best case of 95.", "labels": [], "entities": []}, {"text": "We also tried different class assignments for the classbased mixture on the HUB5 set and we found that using automatically derived classes instead of part-of-speech tags does not lead to performance degradation as long as we allocate individual classes for the top 100 words.", "labels": [], "entities": [{"text": "HUB5 set", "start_pos": 76, "end_pos": 84, "type": "DATASET", "confidence": 0.9779720902442932}]}, {"text": "Automatic class mapping can make class-based mixtures feasible for other languages where part-of-speech tags are difficult to derive.", "labels": [], "entities": [{"text": "class mapping", "start_pos": 10, "end_pos": 23, "type": "TASK", "confidence": 0.7358629107475281}]}], "tableCaptions": [{"text": " Table 1: HUB5 (eval 2001) WER results using standard  and class-based mixtures.  LM Data Sources  Std. mix Class mix  Baseline CTS  38.9%  38.9%  + 150M BN  37.9%  37.8%  + 66M Web (Random)  38.6%  38.3%  + 61M Web  37.7%  37.6%  + 191M Web  37.6%  37.4%  + 150M BN + 61M Web  37.7%  37.3%  + 150M BN + 191M Web  37.5%  37.2%  + 150M BN + 61M Web2  37.7%  37.3%", "labels": [], "entities": [{"text": "HUB5", "start_pos": 10, "end_pos": 14, "type": "DATASET", "confidence": 0.9252158999443054}, {"text": "WER", "start_pos": 27, "end_pos": 30, "type": "METRIC", "confidence": 0.8426515460014343}, {"text": "LM Data Sources  Std. mix Class mix  Baseline CTS", "start_pos": 82, "end_pos": 131, "type": "DATASET", "confidence": 0.8889820277690887}]}, {"text": " Table 2: Meetings results (WER).", "labels": [], "entities": [{"text": "WER", "start_pos": 28, "end_pos": 31, "type": "METRIC", "confidence": 0.9693366289138794}]}]}