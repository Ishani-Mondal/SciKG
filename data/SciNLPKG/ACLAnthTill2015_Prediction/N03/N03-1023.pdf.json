{"title": [{"text": "Weakly Supervised Natural Language Learning Without Redundant Views", "labels": [], "entities": []}], "abstractContent": [{"text": "We investigate single-view algorithms as an alternative to multi-view algorithms for weakly supervised learning for natural language processing tasks without a natural feature split.", "labels": [], "entities": []}, {"text": "In particular, we apply co-training, self-training, and EM to one such task and find that both self-training and FS-EM, anew variation of EM that incorporates feature selection, outperform co-training and are comparatively less sensitive to parameter changes.", "labels": [], "entities": [{"text": "FS-EM", "start_pos": 113, "end_pos": 118, "type": "METRIC", "confidence": 0.9587485790252686}]}], "introductionContent": [{"text": "Multi-view weakly supervised learning paradigms such as co-training) and co-EM) learn a classification task from a small set of labeled data and a large pool of unlabeled data using separate, but redundant, views of the data (i.e. using disjoint feature subsets to represent the data).", "labels": [], "entities": []}, {"text": "Multi-view learning has been successfully applied to a number of tasks in natural language processing (NLP), including text classification), named entity classification (), base noun phrase bracketing, and statistical parsing.", "labels": [], "entities": [{"text": "natural language processing (NLP)", "start_pos": 74, "end_pos": 107, "type": "TASK", "confidence": 0.7630506157875061}, {"text": "text classification", "start_pos": 119, "end_pos": 138, "type": "TASK", "confidence": 0.782820075750351}, {"text": "named entity classification", "start_pos": 141, "end_pos": 168, "type": "TASK", "confidence": 0.6247606873512268}, {"text": "base noun phrase bracketing", "start_pos": 173, "end_pos": 200, "type": "TASK", "confidence": 0.6197903752326965}, {"text": "statistical parsing", "start_pos": 206, "end_pos": 225, "type": "TASK", "confidence": 0.8608743250370026}]}, {"text": "The theoretical performance guarantees of multi-view weakly supervised algorithms come with two fairly strong assumptions on the views.", "labels": [], "entities": []}, {"text": "First, each view must be sufficient to learn the given concept.", "labels": [], "entities": []}, {"text": "Second, the views must be conditionally independent of each other given the class label.", "labels": [], "entities": []}, {"text": "When both conditions are met, Blum and Mitchell prove that an initial weak learner can be boosted using unlabeled data.", "labels": [], "entities": []}, {"text": "Unfortunately, finding a set of views that satisfies both of these conditions is by no means an easy problem.", "labels": [], "entities": []}, {"text": "In addition, recent empirical results by and have shown that multi-view algorithms are quite sensitive to the two underlying assumptions on the views.", "labels": [], "entities": []}, {"text": "Effective view factorization in multi-view learning paradigms, therefore, remains an important issue for their successful application.", "labels": [], "entities": []}, {"text": "In practice, views are supplied by users or domain experts, who determine a natural feature split that is expected to be redundant (i.e. each view is expected to be sufficient to learn the target concept) and conditionally independent given the class label.", "labels": [], "entities": []}, {"text": "We investigate here the application of weakly supervised learning algorithms to problems for which no obvious natural feature split exists and hypothesize that, in these cases, single-view weakly supervised algorithms will perform better than their multi-view counterparts.", "labels": [], "entities": []}, {"text": "Motivated, in part, by the results in, we use the task of noun phrase coreference resolution for illustration throughout the paper.", "labels": [], "entities": [{"text": "noun phrase coreference resolution", "start_pos": 58, "end_pos": 92, "type": "TASK", "confidence": 0.7701516300439835}]}, {"text": "In our experiments, we compare the performance of the Blum and Mitchell co-training algorithm with that of two commonly used single-view algorithms, namely, self-training and Expectation-Maximization (EM).", "labels": [], "entities": []}, {"text": "In comparison to co-training, self-training achieves substantially superior performance and is less sensitive to its input parameters.", "labels": [], "entities": []}, {"text": "EM, on the other hand, fails to boost performance, and we attribute this phenomenon to the presence of redundant features in the underlying generative model.", "labels": [], "entities": [{"text": "EM", "start_pos": 0, "end_pos": 2, "type": "DATASET", "confidence": 0.5653815865516663}]}, {"text": "Consequently, we propose a wrapper-based feature selection method ( for EM that results in performance improvements comparable to that observed with self-training.", "labels": [], "entities": []}, {"text": "Overall, our results suggest that single-view weakly supervised learning algorithms area viable alternative to multi-view algorithms for data sets where a natural feature split into separate, redundant views is not available.", "labels": [], "entities": []}, {"text": "The remainder of the paper is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 presents an overview of the three weakly supervised learning algorithms mentioned previously.", "labels": [], "entities": []}, {"text": "In section 3, we introduce noun phrase coreference resolution and describe the machine learning framework for the problem.", "labels": [], "entities": [{"text": "noun phrase coreference resolution", "start_pos": 27, "end_pos": 61, "type": "TASK", "confidence": 0.5919416099786758}]}, {"text": "In section 4, we evaluate the weakly supervised learning algorithms on the task of coreference resolution.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 83, "end_pos": 105, "type": "TASK", "confidence": 0.9664726555347443}]}, {"text": "Section 5 introduces a method for improving the performance of weakly supervised EM via feature selection.", "labels": [], "entities": []}, {"text": "We conclude with future work in section 6.", "labels": [], "entities": []}], "datasetContent": [{"text": "To ensure a fair comparison of the weakly supervised algorithms, the experiments are designed to determine the best parameter setting of each algorithm (in terms of its effectiveness to improve performance) for the data sets we investigate.", "labels": [], "entities": []}, {"text": "Specifically, we keep the parameters common to all three weakly supervised algorithms (i.e. the labeled and unlabeled data) constant and vary the algorithm-specific parameters, as described below.", "labels": [], "entities": []}, {"text": "We use the MUC-6 (1995) and MUC-7 (1998) coreference data sets for evaluation.", "labels": [], "entities": [{"text": "MUC-6", "start_pos": 11, "end_pos": 16, "type": "DATASET", "confidence": 0.9381407499313354}, {"text": "MUC-7 (1998) coreference data sets", "start_pos": 28, "end_pos": 62, "type": "DATASET", "confidence": 0.8182836856160846}]}, {"text": "The training set is composed of 30 \"dry run\" texts, 1 of which is selected to be the annotated text and the remaining 29 texts are used as unannotated data.", "labels": [], "entities": []}, {"text": "For MUC-6, 3486 training instances are generated from 84 NPs in the annotated text.", "labels": [], "entities": [{"text": "MUC-6", "start_pos": 4, "end_pos": 9, "type": "DATASET", "confidence": 0.7967336177825928}]}, {"text": "For MUC-7, 3741 training instances are generated from 87 NPs.", "labels": [], "entities": [{"text": "MUC-7", "start_pos": 4, "end_pos": 9, "type": "DATASET", "confidence": 0.6680377125740051}]}, {"text": "The unlabeled data is composed of 488173 instances and 478384 instances for the MUC-6 and MUC-7 data sets, respectively.", "labels": [], "entities": [{"text": "MUC-6", "start_pos": 80, "end_pos": 85, "type": "DATASET", "confidence": 0.9503225088119507}, {"text": "MUC-7 data sets", "start_pos": 90, "end_pos": 105, "type": "DATASET", "confidence": 0.8768367568651835}]}, {"text": "Testing is performed by applying the bootstrapped coreference classifier and the clustering algorithm described in section 3 on the 20-30 \"formal evaluation\" texts for each of the MUC-6 and MUC-7 data sets.", "labels": [], "entities": [{"text": "MUC-6 and MUC-7 data sets", "start_pos": 180, "end_pos": 205, "type": "DATASET", "confidence": 0.8149924159049988}]}, {"text": "The co-training parameters are set as follows.", "labels": [], "entities": []}, {"text": "We tested three pairs of views.", "labels": [], "entities": []}, {"text": "reproduces the 25 features of the coreference system and shows the views we employ.", "labels": [], "entities": []}, {"text": "Specifically, the three view pairs are generated by the following methods.", "labels": [], "entities": []}, {"text": "Mueller et al.'s heuristic method.", "labels": [], "entities": []}, {"text": "Starting from two empty views, the iterative algorithm selects for each view the feature whose addition maximizes the performance of the respective view on the labeled data at each iteration.", "labels": [], "entities": []}, {"text": "3 This method produces the view pair V1 and V2 in for the MUC-6 data set.", "labels": [], "entities": [{"text": "MUC-6 data set", "start_pos": 58, "end_pos": 72, "type": "DATASET", "confidence": 0.9653423229853312}]}, {"text": "A different view pair is produced for MUC-7.", "labels": [], "entities": [{"text": "MUC-7", "start_pos": 38, "end_pos": 43, "type": "DATASET", "confidence": 0.8526960611343384}]}, {"text": "Random splitting of features into views.", "labels": [], "entities": []}, {"text": "Starting from two empty views, an iterative algorithm that randomly chooses a feature for each view at each step is used to split the feature set.", "labels": [], "entities": []}, {"text": "The resulting view pair V3 and V4 is used for both the MUC-6 and MUC-7 data sets.", "labels": [], "entities": [{"text": "MUC-6 and MUC-7 data sets", "start_pos": 55, "end_pos": 80, "type": "DATASET", "confidence": 0.8032827615737915}]}, {"text": "Splitting of features according to the feature type.", "labels": [], "entities": [{"text": "Splitting of features", "start_pos": 0, "end_pos": 21, "type": "TASK", "confidence": 0.8711161216100057}]}, {"text": "Specifically, one view comprises the lexicosyntactic features and the other the remaining ones.", "labels": [], "entities": []}, {"text": "This approach produces the view pair V5 and V6, which is used for both data sets.", "labels": [], "entities": []}, {"text": "We tested pool sizes of 500, 1000, 5000.", "labels": [], "entities": []}, {"text": "We monitored performance on the test data at every 10 iterations of cotraining and ran the algorithm until performance stabilized.", "labels": [], "entities": []}, {"text": "Given the labeled and unlabeled data, self-training requires only the specification of the number of bags.", "labels": [], "entities": []}, {"text": "We tested all odd number of bags between 1 and 25.", "labels": [], "entities": []}, {"text": "Given the labeled and unlabeled data, EM has only one parameter -the number of iterations.", "labels": [], "entities": []}, {"text": "We ran EM to convergence and kept track of its test set performance at every iteration.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: Comparative results of co-training, self-training, EM, and FS-EM (to be described in section 5). Recall,", "labels": [], "entities": [{"text": "FS-EM", "start_pos": 69, "end_pos": 74, "type": "METRIC", "confidence": 0.9871613383293152}]}, {"text": " Table 3. The performance of EM depends  in part on the correctness of the underlying generative  model (", "labels": [], "entities": [{"text": "EM", "start_pos": 29, "end_pos": 31, "type": "TASK", "confidence": 0.9435591697692871}]}]}