{"title": [{"text": "Greedy Decoding for Statistical Machine Translation in Almost Linear Time", "labels": [], "entities": [{"text": "Statistical Machine Translation", "start_pos": 20, "end_pos": 51, "type": "TASK", "confidence": 0.8549633026123047}]}], "abstractContent": [{"text": "We present improvements to a greedy decoding algorithm for statistical machine translation that reduce its time complexity from at least cubic (\u00a2 \u00a1 \u00a4 \u00a3 \u00a6 \u00a5 \u00a8 \u00a7 when applied na\u00a8\u0131velyna\u00a8\u0131vely) to practically linear time 1 without sacrificing translation quality.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 59, "end_pos": 90, "type": "TASK", "confidence": 0.7287178635597229}]}, {"text": "We achieve this by integrating hypothesis evaluation into hypothesis creation , tiling improvements over the translation hypothesis at the end of each search iteration, and by imposing restrictions on the amount of word reordering during decoding.", "labels": [], "entities": [{"text": "hypothesis creation", "start_pos": 58, "end_pos": 77, "type": "TASK", "confidence": 0.7397225201129913}]}], "introductionContent": [{"text": "Most of the current work in statistical machine translation builds on word replacement models developed at IBM in the early 1990s).", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 28, "end_pos": 59, "type": "TASK", "confidence": 0.7024662295977274}, {"text": "word replacement", "start_pos": 70, "end_pos": 86, "type": "TASK", "confidence": 0.7092623561620712}]}, {"text": "Based on the conventions established in, these models are commonly referred to as the (IBM) Models 1-5.", "labels": [], "entities": [{"text": "IBM) Models 1-5", "start_pos": 87, "end_pos": 102, "type": "DATASET", "confidence": 0.8241698294878006}]}, {"text": "One of the big challenges in building actual MT systems within this framework is that of decoding: finding the translation candidate a restricted stack search, to search errors.", "labels": [], "entities": [{"text": "MT", "start_pos": 45, "end_pos": 47, "type": "TASK", "confidence": 0.9849741458892822}]}, {"text": "Using the same evaluation metric (but different evaluation data), report search error rates of 7.9% and 9.3%, respectively, for their decoders. and both implemented optimal decoders and benchmarked approximative algorithms against them.", "labels": [], "entities": []}, {"text": "Och et al. report word error rates of 68.68% for optimal search (based on a variant of the A* algorithm), and 69.65% for the most restricted version of a decoder that combines dynamic programming with abeam search).", "labels": [], "entities": [{"text": "word error", "start_pos": 18, "end_pos": 28, "type": "METRIC", "confidence": 0.5651847422122955}]}, {"text": "compare translations obtained by a multi-stack decoder and a greedy hill-climbing algorithm against those produced by an optimal integer programming decoder that treats decoding as a variant of the traveling-salesman problem (cf.).", "labels": [], "entities": []}, {"text": "Their overall performance metric is the sentence error rate (SER).", "labels": [], "entities": [{"text": "sentence error rate (SER)", "start_pos": 40, "end_pos": 65, "type": "METRIC", "confidence": 0.8671621382236481}]}, {"text": "For decoding with IBM Model 3, they report SERs of about 57% (6-word sentences) and 76% (8-word sentences) for optimal decoding, 58% and 75% for stack decoding, and 60% and 75% for greedy decoding, which is the focus of this paper.", "labels": [], "entities": [{"text": "SERs", "start_pos": 43, "end_pos": 47, "type": "METRIC", "confidence": 0.9993545413017273}]}, {"text": "All these numbers suggest that approximative algorithms area feasible choice for practical applications.", "labels": [], "entities": []}, {"text": "The purpose of this paper is to describe speed improvements to the greedy decoder mentioned above.", "labels": [], "entities": []}, {"text": "While acceptably fast for the kind of evaluation used in, namely sentences of up to 20 words, its speed becomes an issue for more realistic applications.", "labels": [], "entities": []}, {"text": "Brute force translation of the 100 short news articles in Chinese from the TIDES MT evaluation in June 2002 (878 segments; ca.", "labels": [], "entities": [{"text": "TIDES MT evaluation in June 2002", "start_pos": 75, "end_pos": 107, "type": "DATASET", "confidence": 0.8966396749019623}]}, {"text": "25k tokens) requires, without any of the improvements described in this paper, over 440 CPU hours, using the simpler, \"faster\" algorithm # \u00a2 $ (described below).", "labels": [], "entities": []}, {"text": "We will show that this time can be reduced to ca.", "labels": [], "entities": []}, {"text": "40 minutes without sacrificing translation quality.", "labels": [], "entities": []}, {"text": "In the following, we first describe the underlying IBM Figure 1: How the IBM models model the translation process.", "labels": [], "entities": [{"text": "IBM Figure 1", "start_pos": 51, "end_pos": 63, "type": "DATASET", "confidence": 0.9200617869695028}, {"text": "translation", "start_pos": 94, "end_pos": 105, "type": "TASK", "confidence": 0.9630163311958313}]}, {"text": "This is a hypothetical example and not taken from any actual training or decoding logs.", "labels": [], "entities": []}, {"text": "model(s) of machine translation (Section 2) and our hillclimbing algorithm (Section 3).", "labels": [], "entities": [{"text": "machine translation", "start_pos": 12, "end_pos": 31, "type": "TASK", "confidence": 0.7856676578521729}]}, {"text": "In Section 4, we discuss improvements to the algorithm and its implementation, and the effect of restrictions on word reordering. is typically calculated using an n-gram language model.", "labels": [], "entities": []}, {"text": "For the sake of simplicity, we assume here and everywhere else in the paper that the ultimate task is to translate from a foreign language into English.", "labels": [], "entities": [{"text": "translate from a foreign language", "start_pos": 105, "end_pos": 138, "type": "TASK", "confidence": 0.8487004995346069}]}, {"text": "The model pictures the conversion from English to a foreign language roughly as follows (cf.; note that because of the noisy channel approach, the modeling is \"backwards\").", "labels": [], "entities": []}, {"text": "After that, the linear order of the foreign words is rearranged.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Decoder performance on the June 2002 TIDES MT evluation test set with multiple searches from randomized  starting points (MSD=2, MSSS=5).", "labels": [], "entities": [{"text": "June 2002 TIDES MT evluation test set", "start_pos": 37, "end_pos": 74, "type": "DATASET", "confidence": 0.5773503482341766}]}]}