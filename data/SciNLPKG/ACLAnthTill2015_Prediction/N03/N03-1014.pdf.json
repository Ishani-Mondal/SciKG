{"title": [{"text": "Inducing History Representations for Broad Coverage Statistical Parsing", "labels": [], "entities": [{"text": "Inducing History Representations", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.8437043031056722}, {"text": "Broad Coverage Statistical Parsing", "start_pos": 37, "end_pos": 71, "type": "TASK", "confidence": 0.5373172983527184}]}], "abstractContent": [{"text": "We present a neural network method for inducing representations of parse histories and using these history representations to estimate the probabilities needed by a statistical left-corner parser.", "labels": [], "entities": []}, {"text": "The resulting statistical parser achieves performance (89.1% F-measure) on the Penn Treebank which is only 0.6% below the best current parser for this task, despite using a smaller vocabulary size and less prior linguistic knowledge.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 61, "end_pos": 70, "type": "METRIC", "confidence": 0.9982086420059204}, {"text": "Penn Treebank", "start_pos": 79, "end_pos": 92, "type": "DATASET", "confidence": 0.9946494698524475}]}, {"text": "Crucial to this success is the use of structurally determined soft biases in inducing the representation of the parse history, and no use of hard independence assumptions.", "labels": [], "entities": []}], "introductionContent": [{"text": "Unlike most problems addressed with machine learning, parsing natural language sentences requires choosing between an unbounded (or even infinite) number of possible phrase structure trees.", "labels": [], "entities": [{"text": "parsing natural language sentences", "start_pos": 54, "end_pos": 88, "type": "TASK", "confidence": 0.8907387256622314}]}, {"text": "The standard approach to this problem is to decompose this choice into an unbounded sequence of choices between a finite number of possible parser actions.", "labels": [], "entities": []}, {"text": "This sequence is the parse for the phrase structure tree.", "labels": [], "entities": []}, {"text": "We can then define a probabilistic model of phrase structure trees by defining a probabilistic model of each parser action in its parse context, and apply machine learning techniques to learn this model of parser actions.", "labels": [], "entities": []}, {"text": "Many statistical parsers) are based on a history-based model of parser actions.", "labels": [], "entities": []}, {"text": "In these models, the probability of each parser action is conditioned on the history of previous actions in the parse.", "labels": [], "entities": []}, {"text": "But here again we are faced with an unusual situation for machine learning problems, conditioning on an unbounded amount of information.", "labels": [], "entities": []}, {"text": "A major challenge in designing a history-based statistical parser is choosing a finite representation of the unbounded parse history from which the probability of the next parser action can be accurately estimated.", "labels": [], "entities": []}, {"text": "Previous approaches have used a hand-crafted finite set of features to represent the parse history).", "labels": [], "entities": []}, {"text": "In the work presented here, we automatically induce a finite set of real valued features to represent the parse history.", "labels": [], "entities": []}, {"text": "We perform the induction of a history representation using an artificial neural network architecture, called Simple Synchrony Networks (SSNs)).", "labels": [], "entities": []}, {"text": "This machine learning method is specifically designed for processing unbounded structures.", "labels": [], "entities": []}, {"text": "It allows us to avoid making a priori independence assumptions, unlike with hand-crafted history features.", "labels": [], "entities": []}, {"text": "But it also allows us to make use of our a priori knowledge by imposing structurally specified and linguistically appropriate biases on the search fora good history representation.", "labels": [], "entities": []}, {"text": "The combination of automatic feature induction and linguistically appropriate biases results in a history-based parser with state-of-the-art performance.", "labels": [], "entities": []}, {"text": "When trained on just part-of-speech tags, the resulting parser achieves the best current performance of a non-lexicalized parser on the Penn Treebank.", "labels": [], "entities": [{"text": "Penn Treebank", "start_pos": 136, "end_pos": 149, "type": "DATASET", "confidence": 0.9957084953784943}]}, {"text": "When a relatively small vocabulary of words is used, performance is only marginally below the best current parser accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 114, "end_pos": 122, "type": "METRIC", "confidence": 0.9621333479881287}]}, {"text": "If either the biases are reduced or the induced history representations are replaced with hand-crafted features, performance degrades.", "labels": [], "entities": []}], "datasetContent": [{"text": "We used the Penn Treebank ( test the effects of varying vocabulary sizes on performance and tractability, we trained three different models.", "labels": [], "entities": [{"text": "Penn Treebank", "start_pos": 12, "end_pos": 25, "type": "DATASET", "confidence": 0.9943846464157104}]}, {"text": "The simplest model (\"SSN-Tags\") includes no words in the vocabulary, relying completely on the information provided by the part-of-speech tags of the words.", "labels": [], "entities": []}, {"text": "The second model (\"SSN-Freq\u00a1 200\") uses all tag-word pairs which occur at least 200 times in the training set.", "labels": [], "entities": []}, {"text": "The remaining words were all treated as instances of the unknown-word.", "labels": [], "entities": []}, {"text": "This resulted in a vocabulary size of 512 tag-word pairs.", "labels": [], "entities": []}, {"text": "The third model (\"SSN-Freq\u00a1 20\") thresholds the vocabulary at 20 instances in the training set, resulting in 4242 tag-word pairs.", "labels": [], "entities": []}, {"text": "We determined appropriate training parameters and network size based on intermediate validation results and our previous experience with networks similar to the models SSN-Tags and SSN-Freq\u00a1 200.", "labels": [], "entities": []}, {"text": "We trained two or three networks for each of the three vocabulary sizes and chose the best ones based on their validation performance.", "labels": [], "entities": []}, {"text": "Training times vary but are long, being around 4 days fora SSN-Tags model, 6 days fora SSN-Freq\u00a1 200 model, and 10 days fora SSN-Freq\u00a1 20 model (on a 502 MHz Sun Blade computer).", "labels": [], "entities": []}, {"text": "We then tested the best models for each vocabulary size on the testing set.", "labels": [], "entities": []}, {"text": "Standard measures of performance are shown in table 1.", "labels": [], "entities": []}, {"text": "The top panel of table 1 lists the results for the nonlexicalized model (SSN-Tags) and the available results for three other models which only use part-of-speech tags as inputs, another neural network parser (), an earlier statistical left-corner parser, and a PCFG).", "labels": [], "entities": [{"text": "PCFG", "start_pos": 261, "end_pos": 265, "type": "DATASET", "confidence": 0.892036497592926}]}, {"text": "The SSN-Tags model achieves performance which is much better than the only other broad coverage neural network parser ().", "labels": [], "entities": [{"text": "broad coverage neural network parser", "start_pos": 81, "end_pos": 117, "type": "TASK", "confidence": 0.6190668344497681}]}, {"text": "The SSN-Tags model also does better than any other published results on parsing with just part-of-speech tags, as exemplified by the results for) and).", "labels": [], "entities": []}, {"text": "The bottom panel of table 1 lists the results for the two lexicalized models (SSN-Freq\u00a1 200 and SSN-Freq\u00a1 20) and five recent statistical parsers).", "labels": [], "entities": []}, {"text": "On the complete testing set, the performance of our lexicalized models is very close to the three best current parsers, which all achieve equivalent performance.", "labels": [], "entities": []}, {"text": "The performance of the best current parser) represents only a 4% reduction in precision error and only a 7% reduction in recall error over the SSNFreq\u00a1 20 model.", "labels": [], "entities": [{"text": "precision error", "start_pos": 78, "end_pos": 93, "type": "METRIC", "confidence": 0.9802426993846893}, {"text": "recall error", "start_pos": 121, "end_pos": 133, "type": "METRIC", "confidence": 0.9803341031074524}]}, {"text": "The SSN parser achieves this result using much less lexical knowledge than other approaches, which all minimally use the words which occur at least 5 times, plus morphological features of the remaining words.", "labels": [], "entities": [{"text": "SSN parser", "start_pos": 4, "end_pos": 14, "type": "TASK", "confidence": 0.9130308926105499}]}, {"text": "Another diffference between the three best parsers and ours is that we parse incrementally using abeam search.", "labels": [], "entities": []}, {"text": "This allows use to trade off parsing accuracy for parsing speed, which is a much more important issue than training time.", "labels": [], "entities": [{"text": "parsing", "start_pos": 29, "end_pos": 36, "type": "TASK", "confidence": 0.979068398475647}, {"text": "accuracy", "start_pos": 37, "end_pos": 45, "type": "METRIC", "confidence": 0.9399669170379639}, {"text": "parsing", "start_pos": 50, "end_pos": 57, "type": "TASK", "confidence": 0.9835619330406189}, {"text": "speed", "start_pos": 58, "end_pos": 63, "type": "METRIC", "confidence": 0.7810717821121216}]}, {"text": "Running times to achieve the above levels of performance on the testing set averaged around 30 seconds per sentence for SSN-Tags, 1 minute per sentence for SSN-Freq\u00a1 200, and 2 minutes per sentence for SSN-Freq\u00a1 20 (on a 502 MHz Sun Blade computer, average 22.5 words per sentence).", "labels": [], "entities": []}, {"text": "But by reducing the number of alternatives considered in the search for the most probable parse, we can greatly increase parsing speed without much loss inaccuracy.", "labels": [], "entities": [{"text": "parsing", "start_pos": 121, "end_pos": 128, "type": "TASK", "confidence": 0.9689444899559021}, {"text": "speed", "start_pos": 129, "end_pos": 134, "type": "METRIC", "confidence": 0.6136255264282227}]}, {"text": "With the SSN-Freq\u00a1 200 model, accuracy slightly better than can be achieved at 2.7 seconds per sentence, and accuracy slightly better than can be achieved at 0.5 seconds per sentence) (on validation sentences at most 100 words long, average 23.3 words per sentence).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 30, "end_pos": 38, "type": "METRIC", "confidence": 0.9992871880531311}, {"text": "accuracy", "start_pos": 109, "end_pos": 117, "type": "METRIC", "confidence": 0.9989904761314392}]}], "tableCaptions": [{"text": " Table 1: Percentage labeled constituent recall and preci- sion on the testing set.", "labels": [], "entities": [{"text": "recall", "start_pos": 41, "end_pos": 47, "type": "METRIC", "confidence": 0.9194628596305847}]}, {"text": " Table 2: Percentage labeled constituent recall, precision,  and F-measure on the validation set for different versions  of the SSN-Freq\u00a1 200 model.", "labels": [], "entities": [{"text": "recall", "start_pos": 41, "end_pos": 47, "type": "METRIC", "confidence": 0.97145015001297}, {"text": "precision", "start_pos": 49, "end_pos": 58, "type": "METRIC", "confidence": 0.9994520545005798}, {"text": "F-measure", "start_pos": 65, "end_pos": 74, "type": "METRIC", "confidence": 0.9991558790206909}, {"text": "SSN-Freq\u00a1 200 model", "start_pos": 128, "end_pos": 147, "type": "DATASET", "confidence": 0.6667984127998352}]}]}