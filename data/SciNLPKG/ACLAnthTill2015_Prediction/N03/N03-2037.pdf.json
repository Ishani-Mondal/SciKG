{"title": [], "abstractContent": [{"text": "This paper describes an initial evaluation of systems that answer questions seeking definitions.", "labels": [], "entities": []}, {"text": "The results suggest that humans agree sufficiently as to what the basic concepts that should be included in the definition of a particular subject are to permit the computation of concept recall.", "labels": [], "entities": []}, {"text": "Computing concept precision is more problematic, however.", "labels": [], "entities": [{"text": "Computing concept precision", "start_pos": 0, "end_pos": 27, "type": "TASK", "confidence": 0.6320080657800039}]}, {"text": "Using the length in characters of a definition is a crude approximation to concept precision that is nonetheless sufficient to correlate with humans' subjective assessment of definition quality.", "labels": [], "entities": [{"text": "length", "start_pos": 10, "end_pos": 16, "type": "METRIC", "confidence": 0.9577528834342957}, {"text": "precision", "start_pos": 83, "end_pos": 92, "type": "METRIC", "confidence": 0.9190659523010254}]}, {"text": "The TREC question answering track has sponsored a series of evaluations of systems' abilities to answer closed class questions in many domains (Voorhees, 2001).", "labels": [], "entities": [{"text": "TREC question answering", "start_pos": 4, "end_pos": 27, "type": "TASK", "confidence": 0.6477137406667074}]}, {"text": "Closed class questions are fact-based, short answer questions.", "labels": [], "entities": []}, {"text": "The evaluation of QA systems for closed class questions is relatively simple because a response to such a question can be meaningfully judged on a binary scale of right/wrong.", "labels": [], "entities": []}, {"text": "Increasing the complexity of the question type even slightly significantly increases the difficulty of the evaluation because partial credit for responses must then be accommodated.", "labels": [], "entities": []}, {"text": "The ARDA AQUAINT 1 program is a research initiative sponsored by the U.S. Department of Defense aimed at increasing the kinds and difficulty of the questions automatic systems can answer.", "labels": [], "entities": [{"text": "ARDA AQUAINT 1", "start_pos": 4, "end_pos": 18, "type": "TASK", "confidence": 0.41452762484550476}]}, {"text": "A series of pilot evaluations has been planned as part of the research agenda of the AQUAINT program.", "labels": [], "entities": [{"text": "AQUAINT", "start_pos": 85, "end_pos": 92, "type": "DATASET", "confidence": 0.667975902557373}]}, {"text": "The purpose of each pilot is to develop an effective evaluation methodology for systems that answer a certain kind of question.", "labels": [], "entities": []}, {"text": "One of the first pilots to be implemented was the Definitions Pilot, a pilot to develop an evaluation methodology for questions such as What is mold? and Who is Colin Powell?.", "labels": [], "entities": [{"text": "What is mold? and Who is Colin Powell?", "start_pos": 136, "end_pos": 174, "type": "TASK", "confidence": 0.514213652908802}]}], "introductionContent": [], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Average F scores per system per assessor type.", "labels": [], "entities": [{"text": "F scores", "start_pos": 18, "end_pos": 26, "type": "METRIC", "confidence": 0.9243598282337189}]}]}