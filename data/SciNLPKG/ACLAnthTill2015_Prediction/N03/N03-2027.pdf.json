{"title": [{"text": "Bayesian Nets in Syntactic Categorization of Novel Words", "labels": [], "entities": [{"text": "Syntactic Categorization of Novel Words", "start_pos": 17, "end_pos": 56, "type": "TASK", "confidence": 0.8686683297157287}]}], "abstractContent": [{"text": "This paper presents an application of a Dynamic Bayesian Network (DBN) to the task of assigning Part-of-Speech (PoS) tags to novel text.", "labels": [], "entities": []}, {"text": "This task is particularly challenging for non-standard corpora, such as Internet lingo, where a large proportion of words are unknown.", "labels": [], "entities": []}, {"text": "Previous work reveals that PoS tags depend on a variety of morphological and contextual features.", "labels": [], "entities": [{"text": "PoS tags", "start_pos": 27, "end_pos": 35, "type": "TASK", "confidence": 0.848839670419693}]}, {"text": "Representing these dependencies in a DBN results into an elegant and effective PoS tagger.", "labels": [], "entities": [{"text": "PoS tagger", "start_pos": 79, "end_pos": 89, "type": "TASK", "confidence": 0.7177474200725555}]}], "introductionContent": [{"text": "Uncovering the syntactic structure of texts is a necessary step towards extracting their meaning.", "labels": [], "entities": []}, {"text": "In order to obtain an accurate parse for an unseen text, we need to assign Part-of-Speech (PoS) tags to a string of words.", "labels": [], "entities": []}, {"text": "This paper covers one aspect of our work of PoS tagging with Dynamic Bayesian Networks (DBNs), which demonstrates their success at tagging unknown (OoV) words.", "labels": [], "entities": [{"text": "PoS tagging", "start_pos": 44, "end_pos": 55, "type": "TASK", "confidence": 0.9522932171821594}, {"text": "tagging unknown (OoV) words", "start_pos": 131, "end_pos": 158, "type": "TASK", "confidence": 0.7964078982671102}]}, {"text": "Please refer to the companion paper for substantial discussion of our method and other details.", "labels": [], "entities": []}, {"text": "Although currently existing algorithms exhibit high word-level accuracy, PoS tagging is not a solved problem.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 63, "end_pos": 71, "type": "METRIC", "confidence": 0.9531248211860657}, {"text": "PoS tagging", "start_pos": 73, "end_pos": 84, "type": "TASK", "confidence": 0.9217686951160431}]}, {"text": "First, even a small percentage of errors may derail subsequent processing steps.", "labels": [], "entities": []}, {"text": "Second, the results of tagging are not robust if a large proportion of words are unknown, or if the testing corpus differs in style from the training corpus.", "labels": [], "entities": [{"text": "tagging", "start_pos": 23, "end_pos": 30, "type": "TASK", "confidence": 0.9680200219154358}]}, {"text": "At the same time, diverse training corpora are lacking and most taggers are trained on a large annotated corpus extracted from the Wall Street Journal (WSJ).", "labels": [], "entities": [{"text": "Wall Street Journal (WSJ)", "start_pos": 131, "end_pos": 156, "type": "DATASET", "confidence": 0.9190738499164581}]}, {"text": "These factors significantly hamper the use PoS tagging to extract information from non-standard corpora, such as email messages and websites.", "labels": [], "entities": [{"text": "PoS tagging", "start_pos": 43, "end_pos": 54, "type": "TASK", "confidence": 0.9155833423137665}]}, {"text": "Our work on Information Extraction from an email corpus left us searching fora PoS tagger that would perform well on Internet texts and integrate easily into a large probabilistic reasoning system by producing a distribution over tags rather than deterministic answer.", "labels": [], "entities": [{"text": "Information Extraction", "start_pos": 12, "end_pos": 34, "type": "TASK", "confidence": 0.7306239306926727}]}, {"text": "Internet sources exhibit a set of idiosyncratic characteristics not present in the training corpora available to taggers to date.", "labels": [], "entities": []}, {"text": "They are often written in telegraphic style, omitting closed-class words, which leads to a higher percentage of ambiguous items.", "labels": [], "entities": []}, {"text": "Most importantly, as a consequence of the rapidly evolving Netlingo, Internet texts are full of new words, misspelled words and one-time expressions.", "labels": [], "entities": []}, {"text": "These characteristics are bound to lower the accuracy of existing taggers.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 45, "end_pos": 53, "type": "METRIC", "confidence": 0.9993795156478882}]}, {"text": "A look at the literature confirms that error rates for unknown words are quite high.", "labels": [], "entities": [{"text": "error rates", "start_pos": 39, "end_pos": 50, "type": "METRIC", "confidence": 0.9559859335422516}]}, {"text": "According to several recent publications OoV tagging presents a serious challenge to the field.", "labels": [], "entities": [{"text": "OoV tagging", "start_pos": 41, "end_pos": 52, "type": "TASK", "confidence": 0.943247377872467}]}, {"text": "The transformation-based Brill tagger, achieves 96.5% accuracy for the WSJ, but a mere 85% on unknown words.", "labels": [], "entities": [{"text": "transformation-based Brill tagger", "start_pos": 4, "end_pos": 37, "type": "TASK", "confidence": 0.5972561240196228}, {"text": "accuracy", "start_pos": 54, "end_pos": 62, "type": "METRIC", "confidence": 0.9948369264602661}, {"text": "WSJ", "start_pos": 71, "end_pos": 74, "type": "DATASET", "confidence": 0.8664242029190063}]}, {"text": "Existing probabilistic taggers also don't farewell on unknown words.", "labels": [], "entities": []}, {"text": "Reported results on OoV rarely exceed Brill's performance by a tiny fraction.", "labels": [], "entities": [{"text": "Brill", "start_pos": 38, "end_pos": 43, "type": "METRIC", "confidence": 0.8321346640586853}]}, {"text": "They are mostly based on (Hidden) Markov Models.", "labels": [], "entities": []}, {"text": "A model based on Conditional Random Fields [ outperforms the HMM tagger on unknown words yielding 24% error rate.", "labels": [], "entities": [{"text": "error rate", "start_pos": 102, "end_pos": 112, "type": "METRIC", "confidence": 0.9574964642524719}]}, {"text": "The best result known to us is achieved by Toutanova by enriching the feature representation of the MaxEnt approach.", "labels": [], "entities": []}, {"text": "Unlike Toutanova, we deliberately base our model on the original feature set of Ratnaparkhi's MaxEnt.", "labels": [], "entities": [{"text": "Ratnaparkhi's MaxEnt", "start_pos": 80, "end_pos": 100, "type": "DATASET", "confidence": 0.8245488405227661}]}, {"text": "Our Bayesian network includes a set of binary features (1-3, below) and a set of vocabulary features (4-6, below).", "labels": [], "entities": []}, {"text": "The binary features indicate the presence or absence of a particular character in the token: 1.", "labels": [], "entities": []}, {"text": "does the token contain a capital letter; 2.", "labels": [], "entities": []}, {"text": "does the token contain a hyphen; 3.", "labels": [], "entities": []}, {"text": "does the token contain a number.", "labels": [], "entities": []}, {"text": "We used Ratnaparkhi's vocabulary lists to encode the values of 6458 frequent Words, 3602 Prefixes and 2925 Suffixes up to 4 letters long.", "labels": [], "entities": [{"text": "Ratnaparkhi's vocabulary lists", "start_pos": 8, "end_pos": 38, "type": "DATASET", "confidence": 0.9193834215402603}]}, {"text": "A Dynamic Bayesian network (DBN) is a Bayesian network unwrapped in time, such that it can represent dependencies between variables at adjacent positions (see.", "labels": [], "entities": []}, {"text": "For a good overview of DBNs, see Murphy.", "labels": [], "entities": [{"text": "DBNs", "start_pos": 23, "end_pos": 27, "type": "TASK", "confidence": 0.6246234774589539}, {"text": "Murphy", "start_pos": 33, "end_pos": 39, "type": "DATASET", "confidence": 0.9702984094619751}]}, {"text": "The set of observable variables in our network consists of the binary and vocabulary features mentioned above.", "labels": [], "entities": []}, {"text": "In addition, there are two hidden variables: PoS and Memory which reflects contextual information about past PoS tags.", "labels": [], "entities": [{"text": "Memory", "start_pos": 53, "end_pos": 59, "type": "METRIC", "confidence": 0.9789806604385376}]}, {"text": "Unlike Ratnaparkhi we do not directly consider any information about preceding words even the previous one.", "labels": [], "entities": []}, {"text": "However, a special value of Memory indicates whether we are at the beginning of the sentence.", "labels": [], "entities": [{"text": "Memory", "start_pos": 28, "end_pos": 34, "type": "METRIC", "confidence": 0.9840483665466309}]}, {"text": "Learning in our model is equivalent to collecting statistics over co-occurrences of feature values and tags.", "labels": [], "entities": []}, {"text": "This is implemented in GAWK scripts and takes minutes on the WSJ training corpus.", "labels": [], "entities": [{"text": "GAWK scripts", "start_pos": 23, "end_pos": 35, "type": "DATASET", "confidence": 0.8512242436408997}, {"text": "WSJ training corpus", "start_pos": 61, "end_pos": 80, "type": "DATASET", "confidence": 0.9486990372339884}]}, {"text": "Compare this to laborious Improved Iterative Scaling for MaxEnt.", "labels": [], "entities": [{"text": "MaxEnt", "start_pos": 57, "end_pos": 63, "type": "DATASET", "confidence": 0.8818608522415161}]}, {"text": "Tagging is carried out by the standard Forward-Backward algorithm (see e.g. Murphy).", "labels": [], "entities": [{"text": "Tagging", "start_pos": 0, "end_pos": 7, "type": "TASK", "confidence": 0.9675154089927673}, {"text": "Forward-Backward", "start_pos": 39, "end_pos": 55, "type": "METRIC", "confidence": 0.9602986574172974}, {"text": "Murphy", "start_pos": 76, "end_pos": 82, "type": "DATASET", "confidence": 0.9660233855247498}]}, {"text": "We do not need to use specialized search algorithms such as Ratnaparkhi's \"beam search\".", "labels": [], "entities": []}, {"text": "In addition, our method does not require a \"Development\" stage.", "labels": [], "entities": []}, {"text": "Following established data split we use sections (0-22) of WSJ for training and the rest (23-24) as a test set.", "labels": [], "entities": [{"text": "WSJ", "start_pos": 59, "end_pos": 62, "type": "DATASET", "confidence": 0.9269027709960938}]}, {"text": "The test sections contain 4792 sentences out of about 55600 total sentences in WSJ corpus.", "labels": [], "entities": [{"text": "WSJ corpus", "start_pos": 79, "end_pos": 89, "type": "DATASET", "confidence": 0.9418981969356537}]}, {"text": "The average length of a sentence is 23 tokens.", "labels": [], "entities": []}, {"text": "In addition, we created two specialized testing corpora (available upon request for comparison purposes).", "labels": [], "entities": []}, {"text": "A small Email corpus was prepared from excerpts from the MUC seminar announcement corpus.", "labels": [], "entities": [{"text": "Email corpus", "start_pos": 8, "end_pos": 20, "type": "DATASET", "confidence": 0.776250034570694}, {"text": "MUC seminar announcement corpus", "start_pos": 57, "end_pos": 88, "type": "DATASET", "confidence": 0.9126401841640472}]}, {"text": "\"The Jabberwocky\" is a poem by Louis Carol where the majority of words are made-up, but their PoS tags are apparent to speakers of English.", "labels": [], "entities": []}, {"text": "We use \"The Jabberwocky\" to illustrate performance on unknown words.", "labels": [], "entities": []}, {"text": "Both the Email corpus and the Jabberwocky were pre-tagged by the Brill tagger then manually corrected.", "labels": [], "entities": [{"text": "Email corpus", "start_pos": 9, "end_pos": 21, "type": "DATASET", "confidence": 0.970225989818573}, {"text": "Brill tagger", "start_pos": 65, "end_pos": 77, "type": "DATASET", "confidence": 0.8666484653949738}]}, {"text": "We began our experiments by using the original set of features and vocabulary lists of Ratnaparkhi for the variables Word, Prefix and Suffix.", "labels": [], "entities": []}, {"text": "This produced a reasonable performance.", "labels": [], "entities": []}, {"text": "While investigating the relative contribution of each feature in this setting, we discovered that the removal of the three binary features from the feature set does not significantly alter performance.", "labels": [], "entities": []}, {"text": "Upon close examination, the vocabularies we used turned out to contain a lot of redundant information that is otherwise handled by these features.", "labels": [], "entities": []}, {"text": "For example, Prefix list contained 84 hyphens (e.g. both \"co-\" and \"co\"), 530 numbers and 150 capitalised words, including capital letters.", "labels": [], "entities": [{"text": "Prefix list", "start_pos": 13, "end_pos": 24, "type": "DATASET", "confidence": 0.9253654181957245}]}, {"text": "We proceed, using reduced vocabularies obtained by removing redundant information from the original lists.", "labels": [], "entities": []}, {"text": "The results are presented in for various testing conditions.", "labels": [], "entities": []}, {"text": "Since Toutanova report that Prefix information worsens performance, we conducted the second set of experiments with a network that contained no information about prefix.", "labels": [], "entities": []}, {"text": "We found no significant change in performance.", "labels": [], "entities": []}, {"text": "Our overall performance is comparable to the best result known on this benchmark (e.g. Toutanova.", "labels": [], "entities": []}, {"text": "At the same time, our performance on OoV words is significantly better (9.4% versus 13.3%).", "labels": [], "entities": []}, {"text": "We attribute this difference to the purer representation of morphologically relevant suffixes in our factored vocabulary, which excludes redundant and therefore potentially confusing information.", "labels": [], "entities": []}, {"text": "Another reason maybe that our method puts a greater emphasis on the syntactically relevant facts, such as morphology and tag sequence information by refraining to use word-specific cues.", "labels": [], "entities": []}, {"text": "Despite our good performance on the WSJ corpus, we failed to improve Brill's tagging on our two specialized corpora.", "labels": [], "entities": [{"text": "WSJ corpus", "start_pos": 36, "end_pos": 46, "type": "DATASET", "confidence": 0.959429919719696}, {"text": "Brill", "start_pos": 69, "end_pos": 74, "type": "METRIC", "confidence": 0.9103657603263855}]}, {"text": "Both Brill and our method achieved 89% on the Jabberwocky poem.", "labels": [], "entities": [{"text": "Jabberwocky poem", "start_pos": 46, "end_pos": 62, "type": "DATASET", "confidence": 0.9597660005092621}]}, {"text": "Note, however, that Brill uses much more sophisticated mechanisms to obtain this result.", "labels": [], "entities": []}, {"text": "It was particularly disappointing for us to find out that we did not succeed in labeling the Email corpus accurately (16.3% versus 14.9% of Brill).", "labels": [], "entities": [{"text": "Email corpus", "start_pos": 93, "end_pos": 105, "type": "DATASET", "confidence": 0.8774757981300354}]}, {"text": "However, the reason for this poor performance appears to be partly related to a labeling convention of the Penn Treebank, which essentially causes most capitalized words to be categorized as NNPs.", "labels": [], "entities": [{"text": "Penn Treebank", "start_pos": 107, "end_pos": 120, "type": "DATASET", "confidence": 0.9943166971206665}]}, {"text": "In our view, there is a significant difference between the grammatical status of a proper name \"Virginia Savova\", where words can't be said to modify one another, and a name of an institution such as \"Department of Chemical Engineering\", where \"chemical\" clearly modifies \"engineering\".", "labels": [], "entities": [{"text": "Virginia Savova\"", "start_pos": 96, "end_pos": 112, "type": "DATASET", "confidence": 0.9534448385238647}]}, {"text": "While a rule-based system profits from this simplistic convention, our method is harmed by it.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": []}