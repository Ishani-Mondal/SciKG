{"title": [{"text": "Semantic Coherence Scoring Using an Ontology", "labels": [], "entities": [{"text": "Semantic Coherence Scoring", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.7518671552340189}]}], "abstractContent": [{"text": "In this paper we present ONTOSCORE, a system for scoring sets of concepts on the basis of an ontology.", "labels": [], "entities": [{"text": "ONTOSCORE", "start_pos": 25, "end_pos": 34, "type": "METRIC", "confidence": 0.9750596880912781}]}, {"text": "We apply our system to the task of scoring alternative speech recognition hypotheses (SRH) in terms of their semantic coherence.", "labels": [], "entities": [{"text": "scoring alternative speech recognition hypotheses (SRH)", "start_pos": 35, "end_pos": 90, "type": "TASK", "confidence": 0.7657030522823334}]}, {"text": "We conducted an annotation experiment and showed that human annotators can reliably differentiate between semantically coherent and incoherent speech recognition hypotheses.", "labels": [], "entities": [{"text": "semantically coherent and incoherent speech recognition hypotheses", "start_pos": 106, "end_pos": 172, "type": "TASK", "confidence": 0.7396116639886584}]}, {"text": "An evaluation of our system against the annotated data shows that, it successfully classifies 73.2% in a German corpus of 2.284 SRHs as either coherent or incoherent (given a baseline of 54.55%).", "labels": [], "entities": [{"text": "German corpus of 2.284 SRHs", "start_pos": 105, "end_pos": 132, "type": "DATASET", "confidence": 0.8996889591217041}]}], "introductionContent": [{"text": "Following, we can distinguish between controlled and conversational dialogue systems.", "labels": [], "entities": []}, {"text": "Since controlled and restricted interactions between the user and the system increase recognition and understanding accuracy, such systems are reliable enough to be deployed in various real world applications, e.g. public transportation or cinema information systems.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 116, "end_pos": 124, "type": "METRIC", "confidence": 0.979496419429779}]}, {"text": "The more conversational a dialogue system becomes, the less predictable are the users' utterances.", "labels": [], "entities": []}, {"text": "Recognition and processing become increasingly difficult and unreliable.", "labels": [], "entities": [{"text": "Recognition and processing", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.6286544700463613}]}, {"text": "Today's dialogue systems employ domain-and discourse-specific knowledge bases, so-called ontologies, to represent the individual discourse entities as concepts, and their relations to each other.", "labels": [], "entities": []}, {"text": "In this paper we present an algorithm for measuring the semantic coherence of sets of concepts against such an ontology.", "labels": [], "entities": []}, {"text": "In the following, we will show how the semantic coherence measurement can be applied to estimate how well a given speech recognition hypothesis (SRH) fits with respect to the existing knowledge representation, thereby providing a mechanism that increases the robustness and reliability of dialogue systems.", "labels": [], "entities": [{"text": "speech recognition hypothesis (SRH)", "start_pos": 114, "end_pos": 149, "type": "TASK", "confidence": 0.803020844856898}]}, {"text": "In Section 2 we discuss the problem of scoring and classifying SRHs in terms of their semantic coherence followed by a description of our annotation experiment.", "labels": [], "entities": [{"text": "classifying SRHs", "start_pos": 51, "end_pos": 67, "type": "TASK", "confidence": 0.624403327703476}]}, {"text": "Section 3 contains a description of the kind of knowledge representations employed by ONTOSCORE.", "labels": [], "entities": [{"text": "ONTOSCORE", "start_pos": 86, "end_pos": 95, "type": "DATASET", "confidence": 0.7766481637954712}]}, {"text": "We present the algorithm in Section 4, and an evaluation of the corresponding system for scoring SRHs is given in Section 5.", "labels": [], "entities": []}, {"text": "A conclusion and additional applications are given in Section 6.", "labels": [], "entities": [{"text": "Section 6", "start_pos": 54, "end_pos": 63, "type": "DATASET", "confidence": 0.8396904468536377}]}], "datasetContent": [{"text": "Ina previous study), we tested if human annotators could reliably classify SRHs in terms of their semantic coherence.", "labels": [], "entities": []}, {"text": "The task of the annotators was to determine whether a given hypothesis representsa n internally coherent utterance or not.", "labels": [], "entities": []}, {"text": "In order to test the reliability of such annotations, we collected a corpus of SRHs.", "labels": [], "entities": [{"text": "SRHs", "start_pos": 79, "end_pos": 83, "type": "DATASET", "confidence": 0.7602797746658325}]}, {"text": "The data collection was conducted by means of a hidden operator test.", "labels": [], "entities": []}, {"text": "We had 29 subjects prompted to say certain inputs in 8 dialogues.", "labels": [], "entities": []}, {"text": "Each user-turn in the dialogue corresponded to a single intention, e.g. a route request or a sight information request.", "labels": [], "entities": []}, {"text": "The audio files were then sent to the speech recognizer and the input to the semantic coherence scoring module, i.e. n-best lists of SRHs were recorded in log-files.", "labels": [], "entities": [{"text": "speech recognizer", "start_pos": 38, "end_pos": 55, "type": "TASK", "confidence": 0.7010608017444611}]}, {"text": "The final corpus consisted of 2.284 SRHs.", "labels": [], "entities": []}, {"text": "All hypotheses were then randomly mixed to avoid contextual influences and given to separate annotators.", "labels": [], "entities": []}, {"text": "The resulting Kappa statistics over the annotated data yields \u00a2 \u00a1 \u00a3 \u00a5 \u00a4 \u00a7 \u00a6 , which seems to indicate that human annotators can reliably distinguish between coherent samples (as in Example) and incoherent ones (as in Example).", "labels": [], "entities": []}, {"text": "The aim of the work presented here, then, was to provide a knowledge-based score, that can be employed by any NLU system to select the best hypothesis from a given n-best list.", "labels": [], "entities": []}, {"text": "ONTOSCORE, the resulting system will be described below, followed by its evaluation against the human gold standard.", "labels": [], "entities": [{"text": "ONTOSCORE", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.9822722673416138}]}], "tableCaptions": []}