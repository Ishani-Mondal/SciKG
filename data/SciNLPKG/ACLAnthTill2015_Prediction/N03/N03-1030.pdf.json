{"title": [{"text": "Sentence Level Discourse Parsing using Syntactic and Lexical Information", "labels": [], "entities": [{"text": "Sentence Level Discourse Parsing", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.8900718539953232}]}], "abstractContent": [{"text": "We introduce two probabilistic models that can be used to identify elementary discourse units and build sentence-level discourse parse trees.", "labels": [], "entities": [{"text": "sentence-level discourse parse trees", "start_pos": 104, "end_pos": 140, "type": "TASK", "confidence": 0.7262964695692062}]}, {"text": "The models use syntactic and lexical features.", "labels": [], "entities": []}, {"text": "A discourse parsing algorithm that implements these models derives discourse parse trees with an error reduction of 18.8% over a state-of-the-art decision-based discourse parser.", "labels": [], "entities": [{"text": "discourse parsing", "start_pos": 2, "end_pos": 19, "type": "TASK", "confidence": 0.7067622095346451}, {"text": "error reduction", "start_pos": 97, "end_pos": 112, "type": "METRIC", "confidence": 0.9302063584327698}]}, {"text": "A set of empirical evaluations shows that our discourse parsing model is sophisticated enough to yield discourse trees at an accuracy level that matches near-human levels of performance.", "labels": [], "entities": [{"text": "discourse parsing", "start_pos": 46, "end_pos": 63, "type": "TASK", "confidence": 0.7277078330516815}, {"text": "accuracy", "start_pos": 125, "end_pos": 133, "type": "METRIC", "confidence": 0.998111367225647}]}], "introductionContent": [{"text": "By exploiting information encoded in human-produced syntactic trees, research on probabilistic models of syntax has driven the performance of syntactic parsers to about 90% accuracy).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 173, "end_pos": 181, "type": "METRIC", "confidence": 0.9980167150497437}]}, {"text": "The absence of semantic and discourse annotated corpora prevented similar developments in semantic/discourse parsing.", "labels": [], "entities": [{"text": "semantic/discourse parsing", "start_pos": 90, "end_pos": 116, "type": "TASK", "confidence": 0.65965985506773}]}, {"text": "Fortunately, recent annotation projects have taken significant steps towards developing semantic () and discourse) annotated corpora.", "labels": [], "entities": []}, {"text": "Some of these annotation efforts have already had a computational impact.", "labels": [], "entities": []}, {"text": "For example, Gildea and Jurafsky (2002) developed statistical models for automatically inducing semantic roles.", "labels": [], "entities": []}, {"text": "In this paper, we describe probabilistic models and algorithms that exploit the discourseannotated corpus produced by.", "labels": [], "entities": []}, {"text": "A discourse structure is a tree whose leaves correspond to elementary discourse units (edu)s, and whose internal nodes correspond to contiguous text spans (called discourse spans).", "labels": [], "entities": []}, {"text": "An example of a discourse structure is the tree given in.", "labels": [], "entities": []}, {"text": "Each internal node in a discourse tree is characterized by a rhetorical relation, such as ATTRIBUTION and ENABLEMENT.", "labels": [], "entities": [{"text": "ATTRIBUTION", "start_pos": 90, "end_pos": 101, "type": "METRIC", "confidence": 0.9975085258483887}, {"text": "ENABLEMENT", "start_pos": 106, "end_pos": 116, "type": "METRIC", "confidence": 0.7328814268112183}]}, {"text": "Within a rhetorical relation a discourse span is also labeled as either NUCLEUS or SATELLITE.", "labels": [], "entities": [{"text": "SATELLITE", "start_pos": 83, "end_pos": 92, "type": "METRIC", "confidence": 0.973271906375885}]}, {"text": "The distinction between nuclei and satellites comes from the empirical observation that a nucleus expresses what is more essential to the writer's purpose than a satellite.", "labels": [], "entities": []}, {"text": "Discourse trees can be represented graphically in the style shown in.", "labels": [], "entities": []}, {"text": "The arrows link the satellite to the nucleus of a rhetorical relation.", "labels": [], "entities": []}, {"text": "Arrows are labeled with the name of the rhetorical relation that holds between the linked units.", "labels": [], "entities": []}, {"text": "Horizontal lines correspond to text spans, and vertical lines identify text spans which are nuclei.", "labels": [], "entities": []}, {"text": "In this paper, we introduce two probabilistic models that can be used to identify elementary discourse units and build sentence-level discourse parse trees.", "labels": [], "entities": [{"text": "sentence-level discourse parse trees", "start_pos": 119, "end_pos": 155, "type": "TASK", "confidence": 0.7230195701122284}]}, {"text": "We show how syntactic and lexical information can be exploited in the process of identifying elementary units of discourse and building sentence-level discourse trees.", "labels": [], "entities": []}, {"text": "Our evaluation indicates that the discourse parsing model we propose is sophisticated enough to achieve near-human levels of performance on the task of deriving sentence-level discourse trees, when working with human-produced syntactic trees and discourse segments.", "labels": [], "entities": [{"text": "discourse parsing", "start_pos": 34, "end_pos": 51, "type": "TASK", "confidence": 0.7315786182880402}]}, {"text": "Wall Street Journal articles from the Penn Treebank.", "labels": [], "entities": [{"text": "Wall Street Journal articles from the Penn Treebank", "start_pos": 0, "end_pos": 51, "type": "DATASET", "confidence": 0.9333360344171524}]}, {"text": "The corpus comes conveniently partitioned into a Training set of 347 articles (6132 sentences) and a Test set of 38 articles (991 sentences).", "labels": [], "entities": []}, {"text": "Each document in the corpus is paired with a discourse structure (tree) that was manually builtin the style of Rhetorical Structure Theory () for details concerning the corpus and the annotation process.)", "labels": [], "entities": []}, {"text": "Out of the 385 articles in the corpus, 53 have been independently annotated by two human annotators.", "labels": [], "entities": []}, {"text": "We used this doubly-annotated subset to compute human agreement on the task of discourse structure derivation.", "labels": [], "entities": [{"text": "discourse structure derivation", "start_pos": 79, "end_pos": 109, "type": "TASK", "confidence": 0.6926144460837046}]}, {"text": "In our experiments we used as discourse structures only the discourse sub-trees spanning over individual sentences.", "labels": [], "entities": []}, {"text": "Because the discourse structures had been built on top of sentences already associated with syntactic trees from the Penn Treebank, we were able to create a composite corpus which allowed us to perform an empirically driven syntax-discourse relationship study.", "labels": [], "entities": [{"text": "Penn Treebank", "start_pos": 117, "end_pos": 130, "type": "DATASET", "confidence": 0.9945980310440063}]}, {"text": "This composite corpus was created by associating each sentence \u00a2 in the discourse corpus with its corresponding Penn Treebank syntactic parse tree which are used to train the parameters of the statistical models.", "labels": [], "entities": [{"text": "Penn Treebank syntactic parse tree", "start_pos": 112, "end_pos": 146, "type": "DATASET", "confidence": 0.9610679745674133}]}, {"text": "Our Test section consists of a set of 946 triples of a similar form, which are used to evaluate the performance of our discourse parser.", "labels": [], "entities": []}, {"text": "The) corpus uses 110 different rhetorical relations.", "labels": [], "entities": []}, {"text": "We found it useful to also compact these relations into classes, as described by, and operate with the resulting 18 labels as well (seen as coarser granularity rhetorical relations).", "labels": [], "entities": []}, {"text": "Operating with different levels of granularity allows one to get deeper insight into the difficulties of assigning the appropriate rhetorical relation, if any, to two adjacent text spans.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section we present the evaluations carried out for both the discourse segmentation task and the discourse parsing task.", "labels": [], "entities": [{"text": "discourse segmentation task", "start_pos": 68, "end_pos": 95, "type": "TASK", "confidence": 0.7746994098027548}, {"text": "discourse parsing task", "start_pos": 104, "end_pos": 126, "type": "TASK", "confidence": 0.7901342511177063}]}, {"text": "For this evaluation, we re-trained Charniak's parser (2000) such that the test sentences from the discourse corpus were not seen by the syntactic parser during training.", "labels": [], "entities": []}, {"text": "We train our discourse segmenter on the Training section of the corpus described in Section 2, and test it on the Test section.", "labels": [], "entities": []}, {"text": "The training regime uses syntactic trees from the Penn Treebank.", "labels": [], "entities": [{"text": "Penn Treebank", "start_pos": 50, "end_pos": 63, "type": "DATASET", "confidence": 0.9961136281490326}]}, {"text": "The metric we use to evaluate the discourse segmenter records the accuracy of the discourse segmenter with respect to its ability to insert inside-sentence discourse boundaries.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 66, "end_pos": 74, "type": "METRIC", "confidence": 0.9993544220924377}]}, {"text": "That is, if a sentence has 3 edus, which correspond to 2 inside-sentence discourse boundaries, we measure the ability of our algorithm to correctly identify these 2 boundaries.", "labels": [], "entities": []}, {"text": "We report our evaluation results using recall, precision, and Fscore figures.", "labels": [], "entities": [{"text": "recall", "start_pos": 39, "end_pos": 45, "type": "METRIC", "confidence": 0.9997219443321228}, {"text": "precision", "start_pos": 47, "end_pos": 56, "type": "METRIC", "confidence": 0.9995118379592896}, {"text": "Fscore", "start_pos": 62, "end_pos": 68, "type": "METRIC", "confidence": 0.9993225336074829}]}, {"text": "This metric is harsher than the metric previously used by, who assesses the performance of a discourse segmentation algorithm by counting how often the algorithm makes boundary and noboundary decisions for every word in a sentence.", "labels": [], "entities": [{"text": "discourse segmentation algorithm", "start_pos": 93, "end_pos": 125, "type": "TASK", "confidence": 0.8023968935012817}]}, {"text": "We compare the performance of our probabilistic discourse segmenter with the performance of the decisionbased segmenter proposed by) and the performance of two baseline algorithms.", "labels": [], "entities": []}, {"text": "The first base- ) uses punctuation to determine when to insert a boundary; because commas are often used to indicate breaks inside long sentences, inserts discourse boundaries after each text span whose corresponding syntactic subtree is labeled S, SBAR, or SINV.", "labels": [], "entities": []}, {"text": "We also compute the agreement between human annotators on the discourse segmentation task (\u0095 \u0080 p \u00aa \u0093 ), using the doubly-annotated discourse corpus mentioned in Section 2.", "labels": [], "entities": []}, {"text": ", on the same test set.", "labels": [], "entities": []}, {"text": "Crucial to the performance of the discourse segmenter is the recall because we want to find as many discourse boundaries as possible.", "labels": [], "entities": [{"text": "recall", "start_pos": 61, "end_pos": 67, "type": "METRIC", "confidence": 0.998630702495575}]}, {"text": "The baseline algorithms are too simplistic to yield good results (recall figures of 28.2% and 25.4%).", "labels": [], "entities": [{"text": "recall", "start_pos": 66, "end_pos": 72, "type": "METRIC", "confidence": 0.9996960163116455}]}, {"text": "The algorithm presented in this paper gives an error reduction in missed discourse boundaries of 24.5% (recall accuracy improvement from 77.1% to 82.7%) over).", "labels": [], "entities": [{"text": "error reduction", "start_pos": 47, "end_pos": 62, "type": "METRIC", "confidence": 0.9710974097251892}, {"text": "recall", "start_pos": 104, "end_pos": 110, "type": "METRIC", "confidence": 0.9972917437553406}, {"text": "accuracy", "start_pos": 111, "end_pos": 119, "type": "METRIC", "confidence": 0.6479282379150391}]}, {"text": "The overall error reduction is of 15.1% (improvement in F-score from 80.1% to 83.1%).", "labels": [], "entities": [{"text": "error reduction", "start_pos": 12, "end_pos": 27, "type": "METRIC", "confidence": 0.9425147473812103}, {"text": "F-score", "start_pos": 56, "end_pos": 63, "type": "METRIC", "confidence": 0.9993434548377991}]}, {"text": "We train our discourse parsing model on the Training section of the corpus described in Section 2, and test it on the Test section.", "labels": [], "entities": [{"text": "discourse parsing", "start_pos": 13, "end_pos": 30, "type": "TASK", "confidence": 0.7271900177001953}]}, {"text": "The training regime uses syntactic trees from the Penn Treebank.", "labels": [], "entities": [{"text": "Penn Treebank", "start_pos": 50, "end_pos": 63, "type": "DATASET", "confidence": 0.9961136281490326}]}, {"text": "The performance is assessed using labeled recall and labeled precision as defined by the standard Parseval metric.", "labels": [], "entities": [{"text": "recall", "start_pos": 42, "end_pos": 48, "type": "METRIC", "confidence": 0.9324922561645508}, {"text": "precision", "start_pos": 61, "end_pos": 70, "type": "METRIC", "confidence": 0.8056353330612183}]}, {"text": "As mentioned in Section 2, we use both 18 labels and 110 labels for the discourse relations.", "labels": [], "entities": []}, {"text": "The recall and precision figures are combined into an F-score figure in the usual manner.", "labels": [], "entities": [{"text": "recall", "start_pos": 4, "end_pos": 10, "type": "METRIC", "confidence": 0.9995481371879578}, {"text": "precision", "start_pos": 15, "end_pos": 24, "type": "METRIC", "confidence": 0.998795747756958}, {"text": "F-score", "start_pos": 54, "end_pos": 61, "type": "METRIC", "confidence": 0.9875103235244751}]}, {"text": "The discourse parsing model uses syntactic trees produced by and discourse segments produced by the algorithm described in Section 3.", "labels": [], "entities": [{"text": "discourse parsing", "start_pos": 4, "end_pos": 21, "type": "TASK", "confidence": 0.7256438732147217}]}, {"text": "We compare the performance of our model (\u0093   ).", "labels": [], "entities": []}, {"text": "The baseline algorithm builds right-branching discourse trees labeled with the most frequent relation encountered in the training set (i.e., ELABORATION-NS).", "labels": [], "entities": [{"text": "ELABORATION-NS", "start_pos": 141, "end_pos": 155, "type": "METRIC", "confidence": 0.9638363718986511}]}, {"text": "We also compute the agreement between human annotators on the discourse parsing task (\u0095 \u0088 p q W ), using the doubly-annotated discourse corpus mentioned in Section 2.", "labels": [], "entities": [{"text": "discourse parsing task", "start_pos": 62, "end_pos": 84, "type": "TASK", "confidence": 0.7627955675125122}]}, {"text": "The results are shown in.", "labels": [], "entities": []}, {"text": "The baseline algorithm has a performance of 23.4% and 20.7% F-score, when using 18 labels and 110 labels, respectively.", "labels": [], "entities": [{"text": "F-score", "start_pos": 60, "end_pos": 67, "type": "METRIC", "confidence": 0.999431312084198}]}, {"text": "Our algorithm has a performance of 49.0% and 45.6% F-score, when using 18 labels and 110 labels, respectively.", "labels": [], "entities": [{"text": "F-score", "start_pos": 51, "end_pos": 58, "type": "METRIC", "confidence": 0.999381422996521}]}, {"text": "These results represent an error reduction of 18.8% (F-score improvement from 37.2% to 49.0%) over a state-of-the-art discourse parser) when using 18 labels, and an error reduction of 15.7% (F-score improvement from 35.5% to 45.6%) when using 110 labels.", "labels": [], "entities": [{"text": "error", "start_pos": 27, "end_pos": 32, "type": "METRIC", "confidence": 0.9602686166763306}, {"text": "F-score", "start_pos": 53, "end_pos": 60, "type": "METRIC", "confidence": 0.9940183162689209}, {"text": "F-score", "start_pos": 191, "end_pos": 198, "type": "METRIC", "confidence": 0.9921483993530273}]}, {"text": "The performance ceiling for sentence-level discourse structure derivation is given by the human annotation agreement F-score of 77.0% and 71.9%, when using 18 labels and 110 labels, respectively.", "labels": [], "entities": [{"text": "sentence-level discourse structure derivation", "start_pos": 28, "end_pos": 73, "type": "TASK", "confidence": 0.6410318836569786}, {"text": "F-score", "start_pos": 117, "end_pos": 124, "type": "METRIC", "confidence": 0.8011811971664429}]}, {"text": "The performance gap between the results of \u0093 b \u00a3 \u00a6 \u00a5 up q W and human agreement is still large, and it can be attributed to three possible causes: errors made by the syntactic parser, errors made by the discourse segmenter, and the weakness of our discourse model.", "labels": [], "entities": []}, {"text": "In order to quantitatively asses the impact in performance of each possible cause of error, we perform further experiments.", "labels": [], "entities": []}, {"text": "We replace the syntactic parse trees produced by Charniak's parser at 90% accuracy ( \u00b7 ) with the corresponding Penn Treebank syntactic parse trees produced by human annotators ( \u00ba \u00b8 ).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 74, "end_pos": 82, "type": "METRIC", "confidence": 0.9988806843757629}, {"text": "Penn Treebank syntactic parse trees", "start_pos": 112, "end_pos": 147, "type": "DATASET", "confidence": 0.966517174243927}]}, {"text": "We also replace the discourse boundaries produced by our discourse segmenter at 83% accuracy (\u0093 \u00a7 \u00b7 ) with the discourse boundaries taken from), which are produced by the human annotators (\u0093 Q \u00b8 show that using perfect syntactic trees leads to an error reduction of 14.5% (F-score improvement from 49.0% to 56.4%) when using 18 labels, and an error reduction of 12.9% (F-score improvement from 45.6% to 52.6%) when using 110 labels.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 84, "end_pos": 92, "type": "METRIC", "confidence": 0.9984208345413208}, {"text": "F-score", "start_pos": 273, "end_pos": 280, "type": "METRIC", "confidence": 0.9894586801528931}, {"text": "error", "start_pos": 343, "end_pos": 348, "type": "METRIC", "confidence": 0.9633942246437073}, {"text": "F-score", "start_pos": 369, "end_pos": 376, "type": "METRIC", "confidence": 0.9910033941268921}]}, {"text": "The results in column \u00b7 \u0093 \u00b8 show that the impact of perfect discourse segmentation is double the impact of perfect syntactic trees.", "labels": [], "entities": [{"text": "perfect discourse segmentation", "start_pos": 52, "end_pos": 82, "type": "TASK", "confidence": 0.7028722763061523}]}, {"text": "Human-level performance on discourse segmentation leads to an error reduction of 29.0% (F-score improvement from 49.0% to 63.8%) when using 18 labels, and an error reduction of 25.6% (F-score improvement from 45.6% to 59.5%) when using 110 labels.", "labels": [], "entities": [{"text": "discourse segmentation", "start_pos": 27, "end_pos": 49, "type": "TASK", "confidence": 0.7180217802524567}, {"text": "error reduction", "start_pos": 62, "end_pos": 77, "type": "METRIC", "confidence": 0.9804097414016724}, {"text": "F-score", "start_pos": 88, "end_pos": 95, "type": "METRIC", "confidence": 0.9963797926902771}, {"text": "error", "start_pos": 158, "end_pos": 163, "type": "METRIC", "confidence": 0.9537696242332458}, {"text": "F-score", "start_pos": 184, "end_pos": 191, "type": "METRIC", "confidence": 0.9858668446540833}]}, {"text": "Together, perfect syntactic trees and perfect discourse segmentation lead to an error reduction of 52.0% (F-score improvement from 49.0% to 75.5%) when using 18 labels, and an error reduction of 45.5% (F-score improvement from 45.6% to 70.3%) when using 110 labels.", "labels": [], "entities": [{"text": "error reduction", "start_pos": 80, "end_pos": 95, "type": "METRIC", "confidence": 0.9785507619380951}, {"text": "F-score improvement", "start_pos": 106, "end_pos": 125, "type": "METRIC", "confidence": 0.9765536487102509}, {"text": "F-score", "start_pos": 202, "end_pos": 209, "type": "METRIC", "confidence": 0.9831772446632385}]}, {"text": "The results in column 9 \u00b8 \u0089 \u0093 \u0089 \u00b8 in compare extremely favorable with the results in column.", "labels": [], "entities": []}, {"text": "The discourse parsing model produces unlabeled discourse structure at a performance level similar to human annotators (F-score of 96.2%).", "labels": [], "entities": [{"text": "discourse parsing", "start_pos": 4, "end_pos": 21, "type": "TASK", "confidence": 0.7016118466854095}, {"text": "F-score", "start_pos": 119, "end_pos": 126, "type": "METRIC", "confidence": 0.9991771578788757}]}, {"text": "When using 18 labels, the distance between our discourse parsing model performance level and human annotators performance level is of absolute 1.5% (75.5% versus 77%).", "labels": [], "entities": [{"text": "discourse parsing", "start_pos": 47, "end_pos": 64, "type": "TASK", "confidence": 0.7508608996868134}]}, {"text": "When using 110 labels, the distance is of absolute 1.6% (70.3% versus 71.9%).", "labels": [], "entities": [{"text": "distance", "start_pos": 27, "end_pos": 35, "type": "METRIC", "confidence": 0.9870027899742126}]}, {"text": "Our evaluation shows that our discourse model is sophisticated enough to match near-human levels of performance.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Discourse segmenter evaluation", "labels": [], "entities": [{"text": "Discourse segmenter", "start_pos": 10, "end_pos": 29, "type": "TASK", "confidence": 0.8084381520748138}]}]}