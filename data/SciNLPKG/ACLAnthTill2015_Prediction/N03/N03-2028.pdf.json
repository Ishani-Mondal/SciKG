{"title": [{"text": "LM Studies on Filled Pauses in Spontaneous Medical Dictation", "labels": [], "entities": [{"text": "Spontaneous Medical Dictation", "start_pos": 31, "end_pos": 60, "type": "TASK", "confidence": 0.8102215131123861}]}], "abstractContent": [{"text": "We investigate the optimal LM treatment of abundant filled pauses (FP) in spontaneous monologues of a professional dictation task.", "labels": [], "entities": [{"text": "LM treatment of abundant filled pauses (FP)", "start_pos": 27, "end_pos": 70, "type": "TASK", "confidence": 0.5878003040949503}]}, {"text": "Questions addressed here are (1) how to deal with FP in the LM history and (2) to which extent can the LM distinguish between positions with high and low FP likelihood.", "labels": [], "entities": [{"text": "FP likelihood", "start_pos": 154, "end_pos": 167, "type": "METRIC", "confidence": 0.9400399923324585}]}, {"text": "Our results differ partly from observations reported on dialogues.", "labels": [], "entities": []}, {"text": "Discarding FP from all LM histories clearly improves the performance.", "labels": [], "entities": [{"text": "FP", "start_pos": 11, "end_pos": 13, "type": "METRIC", "confidence": 0.9130030274391174}]}, {"text": "Local per-plexities, entropies and word rankings at positions following FP suggest that most FP indicate hesitations rather than restarts.", "labels": [], "entities": []}, {"text": "Proper prediction of FP allows to distinguish FP from word positions by a doubled FP probability.", "labels": [], "entities": [{"text": "FP probability", "start_pos": 82, "end_pos": 96, "type": "METRIC", "confidence": 0.977632999420166}]}, {"text": "Recognition experiments confirm the improvements found in our perplexity studies.", "labels": [], "entities": []}], "introductionContent": [{"text": "Speech disfluencies are characteristic for spontaneous speech.", "labels": [], "entities": []}, {"text": "Different disfluency types can be distinguished: Filled pauses (FP) such as 'UH' or 'UM', restarts or repairs, and repetitions.", "labels": [], "entities": [{"text": "Filled pauses (FP", "start_pos": 49, "end_pos": 66, "type": "METRIC", "confidence": 0.8523354232311249}, {"text": "repetitions", "start_pos": 115, "end_pos": 126, "type": "METRIC", "confidence": 0.926680326461792}]}, {"text": "It is widely accepted that disfluencies considerably degrade the performance of speech recognition due to unexpected word sequences and due to the acoustic confusability of FP with short function words.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 80, "end_pos": 98, "type": "TASK", "confidence": 0.7271379083395004}]}, {"text": "Most publications investigate different types of disfluencies in spontaneous dialogues.", "labels": [], "entities": []}, {"text": "This paper, instead, reports analyses on spontaneous dictation of medical reports, i.e. on spontaneous monologues.", "labels": [], "entities": []}, {"text": "Our studies focus on FP which are clearly dominant in our data (8% frequency) and which appear to be mainly associated with hesitations.", "labels": [], "entities": [{"text": "FP", "start_pos": 21, "end_pos": 23, "type": "METRIC", "confidence": 0.9916232228279114}]}, {"text": "As opposed to dialogues, FP are never used hereto prevent interruptions by the dialogue partner as the speaker is searching for some formulation.", "labels": [], "entities": [{"text": "FP", "start_pos": 25, "end_pos": 27, "type": "METRIC", "confidence": 0.9145803451538086}]}, {"text": "Central questions for language modeling are the optimal prediction of FP and its treatment in the LM history.", "labels": [], "entities": [{"text": "language modeling", "start_pos": 22, "end_pos": 39, "type": "TASK", "confidence": 0.7643714249134064}, {"text": "prediction of FP", "start_pos": 56, "end_pos": 72, "type": "TASK", "confidence": 0.6911991834640503}]}, {"text": "Discarding FP from the history should be helpful if the sentence is continued after the interruption.", "labels": [], "entities": [{"text": "FP", "start_pos": 11, "end_pos": 13, "type": "METRIC", "confidence": 0.7718675136566162}]}, {"text": "For complete restarts, however, preceding words maybe misleading and a conditioning on FP maybe better.", "labels": [], "entities": [{"text": "FP", "start_pos": 87, "end_pos": 89, "type": "METRIC", "confidence": 0.9505484104156494}]}, {"text": "On Switchboard,  found that words following FP are better predicted if FP is not discarded from the history.", "labels": [], "entities": [{"text": "FP", "start_pos": 44, "end_pos": 46, "type": "METRIC", "confidence": 0.8967030048370361}, {"text": "FP", "start_pos": 71, "end_pos": 73, "type": "METRIC", "confidence": 0.9504297375679016}]}, {"text": "This was attributed to the tendency of FP to appear at sentence boundaries where the word context from the preceding sentence appears to be harmful.", "labels": [], "entities": [{"text": "FP", "start_pos": 39, "end_pos": 41, "type": "METRIC", "confidence": 0.7203661203384399}]}, {"text": "Measurements after sentence-internal FP only, however, showed a local perplexity reduction for FP-cleaned histories by 20-30%.", "labels": [], "entities": []}, {"text": "This was expected since most sentences are continued after the FP.", "labels": [], "entities": [{"text": "FP", "start_pos": 63, "end_pos": 65, "type": "DATASET", "confidence": 0.4612096846103668}]}, {"text": "These observations were confirmed by) for sentence-internal FP but the local perplexity reduction due to skipping FP was much smaller.", "labels": [], "entities": [{"text": "local perplexity reduction", "start_pos": 71, "end_pos": 97, "type": "METRIC", "confidence": 0.7086378137270609}, {"text": "FP", "start_pos": 114, "end_pos": 116, "type": "METRIC", "confidence": 0.615858793258667}]}, {"text": "Interestingly, there, local trigram perplexities after FP are about 40% worse than bigram perplexities, no matter whether FP was discarded from the history or not.", "labels": [], "entities": [{"text": "FP", "start_pos": 55, "end_pos": 57, "type": "METRIC", "confidence": 0.41936808824539185}]}, {"text": "For a How May I Help You task,) report an improved LM prediction if FP is explicitly used for the conditioning of following words.", "labels": [], "entities": [{"text": "LM prediction", "start_pos": 51, "end_pos": 64, "type": "METRIC", "confidence": 0.8235556781291962}, {"text": "FP", "start_pos": 68, "end_pos": 70, "type": "METRIC", "confidence": 0.9928709864616394}]}, {"text": "This paper is organized as follows: Section 2 describes the dictation task and our corpora.", "labels": [], "entities": []}, {"text": "Section 3 lists three basic approaches to treat FP in trigram LMs.", "labels": [], "entities": [{"text": "FP in trigram LMs", "start_pos": 48, "end_pos": 65, "type": "TASK", "confidence": 0.651759684085846}]}, {"text": "Section 4 discusses various perplexity comparisons, especially focussing on the question how to treat FP in the LM history.", "labels": [], "entities": [{"text": "FP", "start_pos": 102, "end_pos": 104, "type": "TASK", "confidence": 0.7569072842597961}]}, {"text": "An extra study is concerned with LM uncertainties after FP.", "labels": [], "entities": [{"text": "LM uncertainties", "start_pos": 33, "end_pos": 49, "type": "TASK", "confidence": 0.7232907712459564}, {"text": "FP", "start_pos": 56, "end_pos": 58, "type": "TASK", "confidence": 0.4400988519191742}]}, {"text": "Finally, we analyze how well our LMs can discriminate FP from word positions.", "labels": [], "entities": []}, {"text": "Section 5 summarizes our results and cites related speech recognition experiments.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 51, "end_pos": 69, "type": "TASK", "confidence": 0.7074692994356155}]}], "datasetContent": [{"text": "The three approaches are evaluated in terms of the overall perplexity (PP) and local values: PP FP and PP word are measured at FP and word positions only, and PP after * are measured immediately thereafter.", "labels": [], "entities": [{"text": "FP", "start_pos": 96, "end_pos": 98, "type": "METRIC", "confidence": 0.9309715032577515}, {"text": "FP", "start_pos": 127, "end_pos": 129, "type": "METRIC", "confidence": 0.8623039126396179}]}, {"text": "The results in show that discarding FP from the history clearly improves the performance (2. versus 1.).", "labels": [], "entities": [{"text": "FP", "start_pos": 36, "end_pos": 38, "type": "METRIC", "confidence": 0.9871358275413513}]}, {"text": "The overall PP is reduced by 4-5%.", "labels": [], "entities": [{"text": "PP", "start_pos": 12, "end_pos": 14, "type": "METRIC", "confidence": 0.9986536502838135}]}, {"text": "Big reductions by 30-40% are found at positions immediately following FP.", "labels": [], "entities": [{"text": "FP", "start_pos": 70, "end_pos": 72, "type": "DATASET", "confidence": 0.679701566696167}]}, {"text": "This, and the improvements as we go from bi-to trigrams (which are contrary to (), indicates that sentences are -on average -continued after FP.", "labels": [], "entities": [{"text": "FP", "start_pos": 141, "end_pos": 143, "type": "METRIC", "confidence": 0.8475006222724915}]}, {"text": "Using merged counts further improves our LMs.", "labels": [], "entities": []}, {"text": "Gains are (almost) additive to those from FP-skipping.", "labels": [], "entities": [{"text": "FP-skipping", "start_pos": 42, "end_pos": 53, "type": "DATASET", "confidence": 0.6752310991287231}]}, {"text": "Especially, PP after FP decreases by another 10% for approach 2.", "labels": [], "entities": [{"text": "PP", "start_pos": 12, "end_pos": 14, "type": "METRIC", "confidence": 0.9954147338867188}, {"text": "FP", "start_pos": 21, "end_pos": 23, "type": "METRIC", "confidence": 0.8466243743896484}]}, {"text": "which shows that the \"recovered\" FP-free M-Grams are indeed valuable if we use FP-free histories.", "labels": [], "entities": []}, {"text": "A comparison of PP after FP and PP afterword confirms the common knowledge that word prediction after FP is pretty hard.", "labels": [], "entities": [{"text": "word prediction after FP", "start_pos": 80, "end_pos": 104, "type": "TASK", "confidence": 0.6829290390014648}]}, {"text": "Even the unigram perplexity is almost 50% higher for words following FP than for words following fluent contexts.", "labels": [], "entities": []}, {"text": "This supports  where the reduced predictability after FP is partly attributed to the chosen words in those positions.", "labels": [], "entities": [{"text": "FP", "start_pos": 54, "end_pos": 56, "type": "METRIC", "confidence": 0.787603497505188}]}, {"text": "For trigrams, the discrepancy between PP after FP and PP afterword is much larger.", "labels": [], "entities": [{"text": "FP", "start_pos": 47, "end_pos": 49, "type": "METRIC", "confidence": 0.6336719393730164}]}, {"text": "Asking \"how unexpected is a word in a given context ?\" we evaluated the entropy H(h i ) = \u2212 w p LM (w | hi ) \u00b7 log p LM (w | hi ) and the rank R i of w i following hi in the distribution p LM ( * | hi ).", "labels": [], "entities": []}, {"text": "Both quantities were averaged over histories hi ending on FP or on words.", "labels": [], "entities": [{"text": "FP", "start_pos": 58, "end_pos": 60, "type": "METRIC", "confidence": 0.899344801902771}]}, {"text": "1 Note that e Hmean represents a perplexity for the case that words following each history are distributed according top LM ( * | h).", "labels": [], "entities": []}, {"text": "An actually measured PP above e Hmean indicates a bias in the corpus towards words with low p LM (w | h).", "labels": [], "entities": []}, {"text": "The results from show almost no such bias after words.", "labels": [], "entities": []}, {"text": "After FP, however, following words are clearly biased to low probabilities within the trigram distributions.", "labels": [], "entities": [{"text": "FP", "start_pos": 6, "end_pos": 8, "type": "METRIC", "confidence": 0.7814711928367615}]}, {"text": "Also, the mean ranks are considerably higher after FP than after words.", "labels": [], "entities": [{"text": "mean ranks", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9191495180130005}, {"text": "FP", "start_pos": 51, "end_pos": 53, "type": "METRIC", "confidence": 0.960936963558197}]}, {"text": "Together, these findings support our impression that FP often represents a hesitation where the speaker is searching fora less common word or formulation.", "labels": [], "entities": [{"text": "FP", "start_pos": 53, "end_pos": 55, "type": "METRIC", "confidence": 0.7650952339172363}]}, {"text": "cannot discriminate between positions with an increased or reduced FP probability.", "labels": [], "entities": [{"text": "FP probability", "start_pos": 67, "end_pos": 81, "type": "METRIC", "confidence": 0.9866042733192444}]}, {"text": "To evaluate the discrimination for approaches 1. and 2.", "labels": [], "entities": []}, {"text": "we calculated p(FP| h) instead of p(w | h) at each position in the corpus.", "labels": [], "entities": [{"text": "FP", "start_pos": 16, "end_pos": 18, "type": "METRIC", "confidence": 0.9675581455230713}]}, {"text": "The crucial result is that the mean FP probability is reduced by 48% and 45% (approach 1. and 2.) at word as compared to FP positions.", "labels": [], "entities": [{"text": "FP probability", "start_pos": 36, "end_pos": 50, "type": "METRIC", "confidence": 0.922082245349884}]}, {"text": "This is an important feature of these LMs since small FP probabilities reduce confusions of proper words with FP.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Perplexities and error bars (95% confidence) on the Dev set for linearly interpolated LMs.", "labels": [], "entities": [{"text": "Dev set", "start_pos": 62, "end_pos": 69, "type": "DATASET", "confidence": 0.885778397321701}]}, {"text": " Table 3: Measured PP versus e Hmean and mean rank after  histories ending on FP or on word (using pruned LMs).", "labels": [], "entities": [{"text": "mean rank", "start_pos": 41, "end_pos": 50, "type": "METRIC", "confidence": 0.9441992342472076}, {"text": "FP", "start_pos": 78, "end_pos": 80, "type": "METRIC", "confidence": 0.8276628255844116}]}]}