{"title": [{"text": "Toward a Task-based Gold Standard for Evaluation of NP Chunks and Technical Terms", "labels": [], "entities": [{"text": "NP Chunks", "start_pos": 52, "end_pos": 61, "type": "DATASET", "confidence": 0.7687948346138}]}], "abstractContent": [{"text": "We propose a gold standard for evaluating two types of information extraction output-noun phrase (NP) chunks (Abney 1991; Ramshaw and Marcus 1995) and technical terms (Justeson and Katz 1995; Daille 2000; Jacquemin 2002).", "labels": [], "entities": [{"text": "information extraction output-noun phrase (NP) chunks", "start_pos": 55, "end_pos": 108, "type": "TASK", "confidence": 0.8414889536798}]}, {"text": "The gold standard is built around the notion that since different semantic and syntactic variants of terms are arguably correct, a fully satisfactory assessment of the quality of the output must include task-based evaluation.", "labels": [], "entities": []}, {"text": "We conducted an experiment that assessed subjects' choice of index terms in an information access task.", "labels": [], "entities": []}, {"text": "Subjects showed significant preference for index terms that are longer, as measured by number of words, and more complex, as measured by number of prepositions.", "labels": [], "entities": []}, {"text": "These terms, which were identified by a human indexer, serve as the gold standard.", "labels": [], "entities": []}, {"text": "The experimental protocol is a reliable and rigorous method for evaluating the quality of a set of terms.", "labels": [], "entities": []}, {"text": "An important advantage of this task-based evaluation is that a set of index terms which is different than the gold standard can 'win' by providing better information access than the gold standard itself does.", "labels": [], "entities": []}, {"text": "And although the individual human subject experiments are time consuming, the experimental interface, test materials and data analysis programs are completely re-usable.", "labels": [], "entities": []}], "introductionContent": [{"text": "The standard metrics for evaluation of the output of NLP systems are precision and recall.", "labels": [], "entities": [{"text": "precision", "start_pos": 69, "end_pos": 78, "type": "METRIC", "confidence": 0.9997313618659973}, {"text": "recall", "start_pos": 83, "end_pos": 89, "type": "METRIC", "confidence": 0.9990105628967285}]}, {"text": "Given an arguably correct list of the units that a system would identify if it performed perfectly, there should in principle be no discrepancy between the units identified by a system and the units that are either useful in a particular application or are preferred by human beings for use in a particular task.", "labels": [], "entities": []}, {"text": "But when the satisfactory output can take many different forms, as in summarization and generation, evaluation by precision and recall is not sufficient.", "labels": [], "entities": [{"text": "summarization and generation", "start_pos": 70, "end_pos": 98, "type": "TASK", "confidence": 0.7706084549427032}, {"text": "precision", "start_pos": 114, "end_pos": 123, "type": "METRIC", "confidence": 0.9992260932922363}, {"text": "recall", "start_pos": 128, "end_pos": 134, "type": "METRIC", "confidence": 0.9991550445556641}]}, {"text": "In these cases, the challenge for system designers and users is to effectively distinguish between systems that provide generally satisfactory output and systems that do not.", "labels": [], "entities": []}, {"text": "NP chunks and technical terms fall into this difficult-toassess category.", "labels": [], "entities": []}, {"text": "For the maximal NP large number of recent newspaper articles on biomedical science and clinical practice, a fullfledged parser would legitimately identify (at least) seven NPs in addition to the maximal one: large number; recent newspaper articles; large number of recent newspaper articles; biomedical science; clinical practice; biomedical science and clinical practice; and recent newspaper articles on biomedical science and clinical practice.", "labels": [], "entities": []}, {"text": "To evaluate the performance of a parser, NP chunks can usefully be evaluated by a gold standard; many systems (e.g., use the Penn Treebank for this type of evaluation.", "labels": [], "entities": [{"text": "Penn Treebank", "start_pos": 125, "end_pos": 138, "type": "DATASET", "confidence": 0.9906557202339172}]}, {"text": "But for most applications, output that lists a maximal NP and each of its component NPs is bulky and redundant.", "labels": [], "entities": []}, {"text": "Even a system that achieves 100% precision and recall in identifying all of the NPs in a document needs criteria for determining which units to use in different contexts or applications.", "labels": [], "entities": [{"text": "precision", "start_pos": 33, "end_pos": 42, "type": "METRIC", "confidence": 0.999127209186554}, {"text": "recall", "start_pos": 47, "end_pos": 53, "type": "METRIC", "confidence": 0.9993711113929749}]}, {"text": "Technical terms area subset of NP chunks.", "labels": [], "entities": []}, {"text": "Jacquemin3) defines terms as multi-word \"vehicles of scientific and technical information\".", "labels": [], "entities": [{"text": "Jacquemin3", "start_pos": 0, "end_pos": 10, "type": "DATASET", "confidence": 0.9730178713798523}]}, {"text": "The operational difficulty, of course, is to decide whether a specific term is a vehicle of scientific and technical information (e.g., birth date or light truck).", "labels": [], "entities": []}, {"text": "Evaluation of mechanisms that filter out some terms while retaining others is subject to this difficulty.", "labels": [], "entities": []}, {"text": "This is exactly the kind of case where context plays a significant role in deciding whether a term conforms to a definition and where experts disagree.", "labels": [], "entities": []}, {"text": "In this paper, we turn to an information access task in order to assess terms identified by different techniques.", "labels": [], "entities": []}, {"text": "There are two basic types of information access mechanisms, searching and browsing.", "labels": [], "entities": []}, {"text": "In searching, the user generates the search terms; in browsing, the user recognizes potentially useful terms from a list of terms presented by the system.", "labels": [], "entities": []}, {"text": "When an information seeker can readily think up a suitable term or linguistic expression to represent the information need, direct searching of text by user-generated terms is faster and more effective than browsing.", "labels": [], "entities": []}, {"text": "However, when users do not know (or can't remember) the exact expression used in relevant documents, they necessarily struggle to find relevant information in full-text search systems.", "labels": [], "entities": []}, {"text": "Experimental studies have repeatedly shown that information seekers use many different terms to describe the same concept and few of these terms are used frequently ().", "labels": [], "entities": []}, {"text": "When information seekers are unable to figure out the term used to describe a concept in a relevant document, electronic indexes are required for successful information access.", "labels": [], "entities": []}, {"text": "NP chunks and technical terms have been proposed for use in this task.", "labels": [], "entities": []}, {"text": "NP chunks and technical terms have also been used in phrase browsing and phrase hierarchies and summarization (e.g.,.", "labels": [], "entities": [{"text": "summarization", "start_pos": 96, "end_pos": 109, "type": "TASK", "confidence": 0.8637557029724121}]}, {"text": "In fact, the distinction between task-based evaluation of a system and precision/recall evaluation of the quality of system output is similar to the extrinsic/intrinsic evaluation of summarization (.", "labels": [], "entities": [{"text": "precision", "start_pos": 71, "end_pos": 80, "type": "METRIC", "confidence": 0.9981784820556641}, {"text": "recall", "start_pos": 81, "end_pos": 87, "type": "METRIC", "confidence": 0.8432137966156006}]}, {"text": "In order to focus on the subjects' choice of index terms rather than on other aspects of the information access process, we asked subject to find answers to questions in a college level textbook.", "labels": [], "entities": []}, {"text": "Subjects used the Experimental Searching and Browsing Interface (ESBI) to browse a list of terms that were identified by different techniques and then merged.", "labels": [], "entities": [{"text": "Experimental Searching and Browsing Interface (ESBI)", "start_pos": 18, "end_pos": 70, "type": "TASK", "confidence": 0.620596669614315}]}, {"text": "Subjects select an index term by clicking on it in order to hyperlink to the text itself.", "labels": [], "entities": []}, {"text": "By design, ESBI forces the subjects to access the text indirectly, by searching and browsing the list of index terms, rather than by direct searching of the text.", "labels": [], "entities": []}, {"text": "Three sets of terms were used in the experiment: one set (HS) was identified using the head-sorting method of; the second set (TT) was identified by an implementation of the technical term algorithm of; a third set (HUM) was created by a human indexer.", "labels": [], "entities": []}, {"text": "The methods for identifying these terms will be discussed in greater detail below.", "labels": [], "entities": []}, {"text": "Somewhat to our surprise, subjects displayed a very strong preference for the index terms that were identified by the human indexer.", "labels": [], "entities": []}, {"text": "shows that when measured by percentage terms selected, subjects chose over 13% of the available human terms, but only 1.73% and 1.43% of the automatically selected terms; by this measure the subjects' preference for the human terms was more than 7 times greater than the preference for either of the automatic techniques.", "labels": [], "entities": []}, {"text": "(In  This initial experiment strongly indicates that 1) people have a demonstrable preference for different types of index terms; 2) these human terms area very good gold standard.", "labels": [], "entities": []}, {"text": "If subjects use a greater proportion of the terms identified by a particular technique, the terms can be judged better than the terms identified by another technique, even if the terms are different.", "labels": [], "entities": []}, {"text": "Any automatic technique capable of identifying terms that are preferred over these human terms would be a very strong system indeed.", "labels": [], "entities": []}, {"text": "Furthermore, the properties of the terms preferred by the experimental subjects can be used to guide design of systems for identifying and selecting NP chunks and technical terms.", "labels": [], "entities": []}, {"text": "In the next section, we describe the design of the experiment and in Section 3, we report on what the experimental data shows about human preferences for different kinds of index terms.", "labels": [], "entities": []}], "datasetContent": [{"text": "Our experiment assesses the index terms visa vis their usefulness in a strictly controlled information access task.", "labels": [], "entities": []}, {"text": "Subjects responded to a set of questions whose answers were contained in a 350 page collegelevel text (Rice, Ronald E., McCreadie, Maureen and Chang, Shan-ju L.", "labels": [], "entities": []}, {"text": "(2001) Accessing and Browsing Information and Communication.", "labels": [], "entities": [{"text": "Accessing and Browsing Information and Communication", "start_pos": 7, "end_pos": 59, "type": "TASK", "confidence": 0.7618260979652405}]}, {"text": "Cambridge, MA: MIT Press.)", "labels": [], "entities": []}, {"text": "Subjects used the Experimental Searching and Browsing Interface (ESBI) which forces them to access text via the index terms; direct text searching was prohibited.", "labels": [], "entities": []}, {"text": "25 subjects participated in the experiment; they were undergraduate and graduate students at Rutgers University.", "labels": [], "entities": []}, {"text": "The experiments were conducted by graduate students at the Rutgers University School of Communication, Information and Library Studies (SCILS).", "labels": [], "entities": [{"text": "Rutgers University School of Communication, Information and Library Studies (SCILS)", "start_pos": 59, "end_pos": 142, "type": "TASK", "confidence": 0.6564495861530304}]}, {"text": "Subjects used the Experimental Searching and Browsing Interface (ESBI) to find the answers to the questions.", "labels": [], "entities": []}, {"text": "After an initial training session, ESBI presents the user with a Search/Browse screen (not shown); the question appears at the top of the screen.", "labels": [], "entities": [{"text": "Browse", "start_pos": 72, "end_pos": 78, "type": "METRIC", "confidence": 0.8677027225494385}]}, {"text": "The subject may enter a string to search for in the index, or click on the \"Browse\" button for access to the whole index.", "labels": [], "entities": [{"text": "Browse", "start_pos": 76, "end_pos": 82, "type": "METRIC", "confidence": 0.9628388285636902}]}, {"text": "At this point, \"search\" and \"browse\" apply only to the list of index terms, not to the text.", "labels": [], "entities": []}, {"text": "The user may either browse the entire list of index terms or may enter a search term and specify criteria to select the subset of terms that will be returned.", "labels": [], "entities": []}, {"text": "Most people begin with the latter option because the complete list of index terms is too long to be easily browsed.", "labels": [], "entities": []}, {"text": "The user may select (click on) an index term to view a list of the contexts in which the term appears.", "labels": [], "entities": []}, {"text": "If the context appears useful, the user may choose to view the term in its full context; if not, the user may either do additional browsing or start the process over again.", "labels": [], "entities": []}, {"text": "shows a screenshot of ESBI after the searcher has entered the string democracy in the search box.", "labels": [], "entities": [{"text": "ESBI", "start_pos": 22, "end_pos": 26, "type": "DATASET", "confidence": 0.81998211145401}]}, {"text": "This view shows the demo question and the workspace for entering answers.", "labels": [], "entities": []}, {"text": "The string was (previously) entered in the search box and all index terms that include the word democracy are displayed.", "labels": [], "entities": []}, {"text": "Although it is not illustrated here, ESBI also permits substring searching and the option to specify case sensitivity.", "labels": [], "entities": [{"text": "ESBI", "start_pos": 37, "end_pos": 41, "type": "DATASET", "confidence": 0.8064869046211243}, {"text": "substring searching", "start_pos": 55, "end_pos": 74, "type": "TASK", "confidence": 0.6750210076570511}]}, {"text": "Regardless of the technique by which the term was identified, terms are organized by grammatical head of the phrase.", "labels": [], "entities": []}, {"text": "Preliminary analysis of our results has shown that most subjects like this analysis, which resembles standard organization of back-of-the-book indexes.", "labels": [], "entities": []}, {"text": "Readers may notice that the word participation appears at the left-most margin, where it represents the set of terms whose head is participation.", "labels": [], "entities": []}, {"text": "The indented occurrence represents the individual term.", "labels": [], "entities": []}, {"text": "Selecting the left-most occurrence brings up contexts for all phrases for which participation is ahead.", "labels": [], "entities": []}, {"text": "Selecting on the indented occurrence brings up contexts for the noun participation only when it is not part of a larger phrase.", "labels": [], "entities": []}, {"text": "This is explained to subjects during the pre-experimental training and an experimenter is present to remind subjects of this distinction if a question arises during the experiment.", "labels": [], "entities": []}, {"text": "Readers may also notice that in, one of the terms, participation require, is ungrammatical.", "labels": [], "entities": []}, {"text": "This particular error was caused by a faulty part-ofspeech tag.", "labels": [], "entities": []}, {"text": "But since automatically identified index terms typically include some nonsensical terms, we have left these terms in -these terms are one of the problems that information seekers have to cope within a realistic task-based evaluation.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Percentage of terms selected by human  subjects relative to number of terms in the entire  index.", "labels": [], "entities": [{"text": "Percentage", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9603985548019409}]}, {"text": " Table 2: Number of terms in index by method of  identification", "labels": [], "entities": [{"text": "identification", "start_pos": 49, "end_pos": 63, "type": "TASK", "confidence": 0.5767384171485901}]}, {"text": " Table 3: Subject selection of index terms, by  method.", "labels": [], "entities": [{"text": "Subject selection of index terms", "start_pos": 10, "end_pos": 42, "type": "TASK", "confidence": 0.8640968799591064}]}, {"text": " Table 4: Measures of index term complexity", "labels": [], "entities": []}, {"text": " Table 5: Result of two-independent-sample two- tailed t-test on index term complexity. The num- bers in the cells are p-value of the test.", "labels": [], "entities": []}]}