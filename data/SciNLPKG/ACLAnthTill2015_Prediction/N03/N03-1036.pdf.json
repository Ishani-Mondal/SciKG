{"title": [{"text": "Unsupervised methods for developing taxonomies by combining syntactic and statistical information", "labels": [], "entities": []}], "abstractContent": [{"text": "This paper describes an unsupervised algorithm for placing unknown words into a taxon-omy and evaluates its accuracy on a large and varied sample of words.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 108, "end_pos": 116, "type": "METRIC", "confidence": 0.9991330504417419}]}, {"text": "The algorithm works by first using a large corpus to find semantic neighbors of the unknown word, which we accomplish by combining latent semantic analysis with part-of-speech information.", "labels": [], "entities": []}, {"text": "We then place the unknown word in the part of the tax-onomy where these neighbors are most concentrated , using a class-labelling algorithm developed especially for this task.", "labels": [], "entities": []}, {"text": "This method is used to reconstruct parts of the existing Word-Net database, obtaining results for common nouns, proper nouns and verbs.", "labels": [], "entities": [{"text": "Word-Net database", "start_pos": 57, "end_pos": 74, "type": "DATASET", "confidence": 0.8979640007019043}]}, {"text": "We evaluate the contribution made by part-of-speech tagging and show that automatic filtering using the class-labelling algorithm gives a fourfold improvement inaccuracy.", "labels": [], "entities": [{"text": "part-of-speech tagging", "start_pos": 37, "end_pos": 59, "type": "TASK", "confidence": 0.7135603427886963}]}], "introductionContent": [{"text": "The importance of automatic methods for enriching lexicons, taxonomies and knowledge bases from free text is well-recognized.", "labels": [], "entities": []}, {"text": "For rapidly changing domains such as current affairs, static knowledge bases are inadequate for responding to new developments, and the cost of building and maintaining resources by hand is prohibitive.", "labels": [], "entities": []}, {"text": "This paper describes experiments which develop automatic methods for taking an original taxonomy as a skeleton and fleshing it outwith new terms which are discovered in free text.", "labels": [], "entities": []}, {"text": "The method is completely automatic and it is completely unsupervised apart from using the original taxonomic skeleton to suggest possible classifications for new terms.", "labels": [], "entities": []}, {"text": "We evaluate how accurately our methods can reconstruct the WordNet taxonomy.", "labels": [], "entities": [{"text": "WordNet taxonomy", "start_pos": 59, "end_pos": 75, "type": "DATASET", "confidence": 0.9539503455162048}]}, {"text": "The problem of enriching the lexical information in a taxonomy can be posed in two complementary ways.", "labels": [], "entities": []}, {"text": "Firstly, given a particular taxonomic class (such as fruit) one could seek members of this class (such as apple, banana).", "labels": [], "entities": []}, {"text": "This problem is addressed by, and more recently by . Secondly, given a particular word (such as apple), one could seek suitable taxonomic classes for describing this object (such as fruit, foodstuff).", "labels": [], "entities": []}, {"text": "The work in this paper addresses the second of these questions.", "labels": [], "entities": []}, {"text": "The goal of automatically placing new words into a taxonomy has been attempted in various ways for at least ten years.", "labels": [], "entities": []}, {"text": "The process for placing a word win a taxonomy T using a corpus C often contains some version of the following stages: \u2022 For a word w, find words from the corpus C whose occurrences are similar to those of w.", "labels": [], "entities": []}, {"text": "Consider these the 'corpus-derived neighbors' N (w) of w.", "labels": [], "entities": []}, {"text": "\u2022 Assuming that at least some of these neighbors are already in the taxonomy T , map w to the place in the taxonomy where these neighbors are most concentrated.", "labels": [], "entities": []}, {"text": "added 27 words to WordNet using aversion of this process, with a 63% accuracy at assigning new words to one of a number of disjoint WordNet 'classes' produced by a previous algorithm.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 18, "end_pos": 25, "type": "DATASET", "confidence": 0.9776305556297302}, {"text": "accuracy", "start_pos": 69, "end_pos": 77, "type": "METRIC", "confidence": 0.9994057416915894}]}, {"text": "(Direct comparison with this result is problematic since the number of classes used is not stated.)", "labels": [], "entities": []}, {"text": "A more recent example is the top-down algorithm of, which seeks the node in T which shares the most collocational properties with the word w, adding 42 concepts taken from The Lord of the Rings with an accuracy of 28%.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 202, "end_pos": 210, "type": "METRIC", "confidence": 0.9988971948623657}]}, {"text": "The algorithm as presented above leaves many degrees of freedom and open questions.", "labels": [], "entities": []}, {"text": "What methods should be used to obtain the corpus-derived neighbors N (w)?", "labels": [], "entities": []}, {"text": "This question is addressed in Section 2.", "labels": [], "entities": []}, {"text": "Given a collection of neighbors, how should we define a \"place in the taxonomy where these neighbors are most concentrated?\"", "labels": [], "entities": []}, {"text": "This question is addressed in Section 3, which defines a robust class-labelling algorithm for mapping a list of words into a taxonomy.", "labels": [], "entities": []}, {"text": "In Section 4 we describe experiments, determining the accuracy with which these methods can be used to reconstruct the WordNet taxonomy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 54, "end_pos": 62, "type": "METRIC", "confidence": 0.9993896484375}, {"text": "WordNet taxonomy", "start_pos": 119, "end_pos": 135, "type": "DATASET", "confidence": 0.9481559693813324}]}, {"text": "To our knowledge, this is the first such evaluation fora large sample of words.", "labels": [], "entities": []}, {"text": "Section 5 discusses related work and other problems to which these techniques can be adapted.", "labels": [], "entities": []}], "datasetContent": [{"text": "To test the success of our approach to placing unknown words into the WordNet taxonomy on a large and significant sample, we designed the following experiment.", "labels": [], "entities": [{"text": "WordNet taxonomy", "start_pos": 70, "end_pos": 86, "type": "DATASET", "confidence": 0.9333720505237579}]}, {"text": "If the algorithm is successful at placing unknown words in the correct new place in a taxonomy, we would expect it to place already known words in their current position.", "labels": [], "entities": []}, {"text": "The experiment to test this worked as follows.", "labels": [], "entities": []}, {"text": "\u2022 For a word w, find the neighbors N (w) of win WordSpace.", "labels": [], "entities": [{"text": "WordSpace", "start_pos": 48, "end_pos": 57, "type": "DATASET", "confidence": 0.8875448703765869}]}, {"text": "Remove w itself from this set.", "labels": [], "entities": []}, {"text": "\u2022 Find the best class-label h max (N (w)) for this set (using Definition 1).", "labels": [], "entities": []}, {"text": "\u2022 Test to see if, according to WordNet, h max is a hypernym of the original word w, and if so check how closely h max subsumes win the taxonomy.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 31, "end_pos": 38, "type": "DATASET", "confidence": 0.9700324535369873}]}, {"text": "Since our class-labelling algorithm gives a ranked list of possible hypernyms, credit was given for correct classifications in the top 4 places.", "labels": [], "entities": []}, {"text": "This algorithm was tested on singular common nouns (PoS-tag nn1), proper nouns (PoS-tag np0) and finite present-tense verbs (PoS-tag vvb).", "labels": [], "entities": []}, {"text": "For each of these classes, a random sample of words was selected with corpus frequencies ranging from 1000 to 250.", "labels": [], "entities": []}, {"text": "For the noun categories, 600 words were sampled, and for the finite verbs, 420.", "labels": [], "entities": []}, {"text": "For each word w, we found semantic neighbors with and without using part-ofspeech information.", "labels": [], "entities": []}, {"text": "The same experiments were carried out using 3, 6 and 12 neighbors: we will focus on the results for 3 and 12 neighbors since those for 6 neighbors turned out to be reliably 'somewhere in between' these two.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Semantic neighbors of fire with different parts-of-speech. The scores are cosine similarities", "labels": [], "entities": []}, {"text": " Table 2: Percentage of words which were automatically assigned class-labels which subsume them in the WordNet  taxonomy, showing the number of taxonomic levels between the target word and the class-label", "labels": [], "entities": [{"text": "WordNet  taxonomy", "start_pos": 103, "end_pos": 120, "type": "DATASET", "confidence": 0.9279626309871674}]}, {"text": " Table 3: Average affinity score of class-labels for successful and unsuccessful classifications", "labels": [], "entities": [{"text": "affinity score", "start_pos": 18, "end_pos": 32, "type": "METRIC", "confidence": 0.9124996066093445}, {"text": "classifications", "start_pos": 81, "end_pos": 96, "type": "TASK", "confidence": 0.5713247656822205}]}]}