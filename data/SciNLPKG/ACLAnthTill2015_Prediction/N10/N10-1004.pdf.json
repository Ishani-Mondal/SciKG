{"title": [], "abstractContent": [{"text": "Current statistical parsers tend to perform well only on their training domain and nearby gen-res.", "labels": [], "entities": []}, {"text": "While strong performance on a few related domains is sufficient for many situations, it is advantageous for parsers to be able to generalize to a wide variety of domains.", "labels": [], "entities": []}, {"text": "When parsing document collections involving heterogeneous domains (e.g. the web), the optimal parsing model for each document is typically not obvious.", "labels": [], "entities": [{"text": "parsing document collections", "start_pos": 5, "end_pos": 33, "type": "TASK", "confidence": 0.9058362444241842}]}, {"text": "We study this problem as anew task-multiple source parser adaptation.", "labels": [], "entities": [{"text": "task-multiple source parser adaptation", "start_pos": 30, "end_pos": 68, "type": "TASK", "confidence": 0.7062943652272224}]}, {"text": "Our system trains on corpora from many different domains.", "labels": [], "entities": []}, {"text": "It learns not only statistics of those domains but quantitative measures of domain differences and how those differences affect parsing accuracy.", "labels": [], "entities": [{"text": "parsing", "start_pos": 128, "end_pos": 135, "type": "TASK", "confidence": 0.9690013527870178}, {"text": "accuracy", "start_pos": 136, "end_pos": 144, "type": "METRIC", "confidence": 0.8310168385505676}]}, {"text": "Given a specific target text, the resulting system proposes linear combinations of parsing models trained on the source corpora.", "labels": [], "entities": []}, {"text": "Tested across six domains, our system outperforms all non-oracle base-lines including the best domain-independent parsing model.", "labels": [], "entities": []}, {"text": "Thus, we are able to demonstrate the value of customizing parsing models to specific domains.", "labels": [], "entities": []}], "introductionContent": [{"text": "In statistical parsing literature, it is common to see parsers trained and tested on the same textual domain, among others).", "labels": [], "entities": [{"text": "statistical parsing", "start_pos": 3, "end_pos": 22, "type": "TASK", "confidence": 0.7182783782482147}]}, {"text": "Unfortunately, the performance of these systems degrades on sentences drawn from a different domain.", "labels": [], "entities": []}, {"text": "This issue can be seen across different parsing models).", "labels": [], "entities": []}, {"text": "Given that some aspects of syntax are domain dependent (typically at the lexical level), single parsing models tend to not perform well across all domains (see).", "labels": [], "entities": []}, {"text": "Thus, statistical parsers inevitably learn some domain-specific properties in addition to the more general properties of a language's syntax. and showed techniques for training models that attempt to separate domainspecific and general properties.", "labels": [], "entities": []}, {"text": "However, even when given models for multiple training domains, it is not straightforward to determine which model performs best on an arbitrary piece of novel text.", "labels": [], "entities": []}, {"text": "This problem comes to the fore when one wants to parse document collections where each document is potentially its own domain.", "labels": [], "entities": [{"text": "parse document collections", "start_pos": 49, "end_pos": 75, "type": "TASK", "confidence": 0.869186262289683}]}, {"text": "This shows up particularly when parsing the web.", "labels": [], "entities": [{"text": "parsing the web", "start_pos": 32, "end_pos": 47, "type": "TASK", "confidence": 0.9036878148714701}]}, {"text": "Recently, there has been much interest in applying parsers to the web for the purposes of information extraction and other forms of analysis (c.f. the CLSP 2009 summer workshop \"Parsing the Web: Large-Scale Syntactic Processing\").", "labels": [], "entities": [{"text": "information extraction", "start_pos": 90, "end_pos": 112, "type": "TASK", "confidence": 0.7394585013389587}, {"text": "CLSP 2009 summer workshop \"Parsing the Web: Large-Scale Syntactic Processing", "start_pos": 151, "end_pos": 227, "type": "TASK", "confidence": 0.6447228044271469}]}, {"text": "The scale of the web demands an automatic solution to the domain detection and adaptation problems.", "labels": [], "entities": [{"text": "domain detection and adaptation", "start_pos": 58, "end_pos": 89, "type": "TASK", "confidence": 0.7295374125242233}]}, {"text": "Furthermore, it is not obvious that human annotators can determine the optimal parsing models for each web page.", "labels": [], "entities": []}, {"text": "Our goal is to study this exact problem.", "labels": [], "entities": []}, {"text": "We create anew parsing task, multiple source parser adaptation, designed to capture cross-domain performance along with evaluation metrics and baselines.", "labels": [], "entities": [{"text": "parsing task", "start_pos": 15, "end_pos": 27, "type": "TASK", "confidence": 0.8952018618583679}, {"text": "multiple source parser adaptation", "start_pos": 29, "end_pos": 62, "type": "TASK", "confidence": 0.6601420491933823}]}, {"text": "Our new task involves training parsing models on labeled and unlabeled corpora from a variety of domains (source domains).", "labels": [], "entities": []}, {"text": "This is in contrast to standard domain adaptation tasks where there is a single source domain.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 32, "end_pos": 49, "type": "TASK", "confidence": 0.7915782928466797}]}, {"text": "For evaluation, one is given a text (target text) but not the identity of its domain.", "labels": [], "entities": []}, {"text": "The challenge is determining how to best use the available resources from training to maximize accuracy across multiple target texts.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 95, "end_pos": 103, "type": "METRIC", "confidence": 0.9952297210693359}]}, {"text": "Broadly put, we model how domain differences influence parsing accuracy.", "labels": [], "entities": [{"text": "parsing", "start_pos": 55, "end_pos": 62, "type": "TASK", "confidence": 0.9771208763122559}, {"text": "accuracy", "start_pos": 63, "end_pos": 71, "type": "METRIC", "confidence": 0.9004054069519043}]}, {"text": "This is done by taking several computational measures of domain differences between the target text and each source domain.", "labels": [], "entities": []}, {"text": "We use these features in a simple linear regression model which is trained to predict the accuracy of a parsing model (or, more generally, a mixture of parsing models) on a target text.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 90, "end_pos": 98, "type": "METRIC", "confidence": 0.9990789890289307}]}, {"text": "To parse the target text, one simply uses the mixture of parsing models with the highest predicted accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 99, "end_pos": 107, "type": "METRIC", "confidence": 0.8890528082847595}]}, {"text": "We show that our method is able to predict these accuracies quite well and thus effectively rank parsing models formed from mixtures of labeled and automatically labeled corpora.", "labels": [], "entities": []}, {"text": "In Section 2, we detail recent work on similar tasks.", "labels": [], "entities": []}, {"text": "Our regression-based approach is covered in Section 3.", "labels": [], "entities": []}, {"text": "We describe an evaluation strategy in Section 4.", "labels": [], "entities": []}, {"text": "Section 5 presents new baselines which are intended to give a sense of current approaches and their limitations.", "labels": [], "entities": []}, {"text": "The results of our experiments are detailed in Section 6 where we show that our system outperforms all non-oracle baselines.", "labels": [], "entities": []}, {"text": "We conclude with a discussion and future work (Section 7).", "labels": [], "entities": []}], "datasetContent": [{"text": "Multiple-source domain adaptation is anew task for parsing and thus some thought must be given to evaluation methodology.", "labels": [], "entities": [{"text": "Multiple-source domain adaptation", "start_pos": 0, "end_pos": 33, "type": "TASK", "confidence": 0.6539453268051147}, {"text": "parsing", "start_pos": 51, "end_pos": 58, "type": "TASK", "confidence": 0.9780780673027039}]}, {"text": "We describe two evaluation scenarios which differ in how foreign the target text is from our source domains.", "labels": [], "entities": []}, {"text": "Schemas for these evaluation scenarios are shown in.", "labels": [], "entities": []}, {"text": "Note that training and testing here refer to training and testing of our regression model, not the parsing models.", "labels": [], "entities": []}, {"text": "In the first scenario, out-of-domain evaluation, one target domain is completely removed from consideration and only used to evaluate proposed models attest time.", "labels": [], "entities": []}, {"text": "The regressor is trained on training points that use any of the remaining corpora, C\\{t}, as sources or targets.", "labels": [], "entities": []}, {"text": "For example, if t = WSJ, we can train the regressor on all data points which don't use WSJ (or any self-trained corpora derived from WSJ) as a source or target domain.", "labels": [], "entities": [{"text": "WSJ", "start_pos": 20, "end_pos": 23, "type": "DATASET", "confidence": 0.8603487610816956}]}, {"text": "At test time, we are given the text of WSJ's test set.", "labels": [], "entities": [{"text": "WSJ's test set", "start_pos": 39, "end_pos": 53, "type": "DATASET", "confidence": 0.9295614659786224}]}, {"text": "From this, our system creates a parsing model using the remaining available corpora for parsing the raw WSJ text.", "labels": [], "entities": [{"text": "parsing the raw WSJ text", "start_pos": 88, "end_pos": 112, "type": "TASK", "confidence": 0.6534874498844147}]}, {"text": "This evaluation scenario is intended to evaluate how well our system can adapt to an entirely new domain with only raw text from the new domain (for example, parsing biomedical text when none is available in our list of source domains annotation guidelines.", "labels": [], "entities": []}, {"text": "Instead, we holdout each hand-annotated domain, t, (including any automatically parsed corpora derived from that source domain) as a test set in a round-robin fashion.", "labels": [], "entities": []}, {"text": "For each round of the round robin we obtain an f-score and we report the mean and variance of the f-scores for each model.", "labels": [], "entities": []}, {"text": "The second scenario, in-domain evaluation, allows the target domain, t, to be used as a source domain in training but not as a target domain.", "labels": [], "entities": []}, {"text": "This is intended to evaluate the situation where the target domain is not actually that different from our source domains.", "labels": [], "entities": []}, {"text": "The in-domain evaluation can approximate how our system would perform when, for example, we have WSJ as a source domain and the target text is news from a source other than WSJ.", "labels": [], "entities": [{"text": "WSJ", "start_pos": 173, "end_pos": 176, "type": "DATASET", "confidence": 0.9314224720001221}]}, {"text": "Thus, our model still has to learn that WSJ and the North American News Text corpus (NANC) are good for parsing news text like WSJ without seeing any direct evaluations of the sort (WSJ and NANC can be used in models which are evaluated on all other corpora, though).", "labels": [], "entities": [{"text": "WSJ", "start_pos": 40, "end_pos": 43, "type": "DATASET", "confidence": 0.8235957026481628}, {"text": "North American News Text corpus (NANC)", "start_pos": 52, "end_pos": 90, "type": "DATASET", "confidence": 0.7500190697610378}, {"text": "parsing news text", "start_pos": 104, "end_pos": 121, "type": "TASK", "confidence": 0.9118359883626302}]}, {"text": "Our experiments use the Charniak (2000) generative parser.", "labels": [], "entities": []}, {"text": "We describe the corpora used in our nine source and six target domains in Section 6.1.", "labels": [], "entities": []}, {"text": "In Section 6.2, we provide a greedy strategy for picking features to include in our regression model.", "labels": [], "entities": []}, {"text": "The results of our experiments are in Section 6.3.", "labels": [], "entities": [{"text": "Section 6.3", "start_pos": 38, "end_pos": 49, "type": "DATASET", "confidence": 0.8904206454753876}]}], "tableCaptions": [{"text": " Table 1: Cross-domain f-score performance of the Charniak (2000) parser. Averages are macro-averages.  Performance drops as training and test domains diverge. On average, the WSJ model is the most accurate.", "labels": [], "entities": [{"text": "WSJ", "start_pos": 176, "end_pos": 179, "type": "DATASET", "confidence": 0.7256295084953308}]}, {"text": " Table 3: List of source and target domains, sizes of each division in trees, and average sentence length.", "labels": [], "entities": []}]}