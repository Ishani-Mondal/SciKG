{"title": [{"text": "From Baby Steps to Leapfrog: How \"Less is More\" in Unsupervised Dependency Parsing *", "labels": [], "entities": [{"text": "Unsupervised Dependency Parsing", "start_pos": 51, "end_pos": 82, "type": "TASK", "confidence": 0.6157041688760122}]}], "abstractContent": [{"text": "We present three approaches for unsupervised grammar induction that are sensitive to data complexity and apply them to Klein and Man-ning's Dependency Model with Valence.", "labels": [], "entities": [{"text": "unsupervised grammar induction", "start_pos": 32, "end_pos": 62, "type": "TASK", "confidence": 0.6847195625305176}]}, {"text": "The first, Baby Steps, bootstraps itself via iterated learning of increasingly longer sentences and requires no initialization.", "labels": [], "entities": []}, {"text": "This method substantially exceeds Klein and Manning's published scores and achieves 39.4% accuracy on Section 23 (all sentences) of the Wall Street Journal corpus.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 90, "end_pos": 98, "type": "METRIC", "confidence": 0.9986587762832642}, {"text": "Wall Street Journal corpus", "start_pos": 136, "end_pos": 162, "type": "DATASET", "confidence": 0.8914641737937927}]}, {"text": "The second, Less is More, uses a low-complexity subset of the available data: sentences up to length 15.", "labels": [], "entities": []}, {"text": "Focus-ing on fewer but simpler examples trades off quantity against ambiguity; it attains 44.1% accuracy, using the standard linguistically-informed prior and batch training, beating state-of-the-art.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 96, "end_pos": 104, "type": "METRIC", "confidence": 0.9992464780807495}]}, {"text": "Leapfrog, our third heuristic, combines Less is More with Baby Steps by mixing their models of shorter sentences, then rapidly ramping up exposure to the full training set, driving up accuracy to 45.0%.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 184, "end_pos": 192, "type": "METRIC", "confidence": 0.9996022582054138}]}, {"text": "These trends generalize to the Brown corpus; awareness of data complexity may improve other parsing models and unsupervised algorithms.", "labels": [], "entities": [{"text": "Brown corpus", "start_pos": 31, "end_pos": 43, "type": "DATASET", "confidence": 0.9620537161827087}]}], "introductionContent": [{"text": "Unsupervised learning of hierarchical syntactic structure from free-form natural language text is a hard problem whose eventual solution promises to benefit applications ranging from question answering to speech recognition and machine translation.", "labels": [], "entities": [{"text": "question answering", "start_pos": 183, "end_pos": 201, "type": "TASK", "confidence": 0.8238976299762726}, {"text": "speech recognition", "start_pos": 205, "end_pos": 223, "type": "TASK", "confidence": 0.7097984105348587}, {"text": "machine translation", "start_pos": 228, "end_pos": 247, "type": "TASK", "confidence": 0.7881582379341125}]}, {"text": "A restricted version that targets dependencies and * Partially funded by NSF award IIS-0811974; first author supported by the assumes partial annotation, e.g., sentence boundaries, tokenization and typically even part-of-speech (POS) tagging, has received much attention, eliciting a diverse array of techniques.", "labels": [], "entities": [{"text": "NSF award IIS-0811974", "start_pos": 73, "end_pos": 94, "type": "DATASET", "confidence": 0.8100203076998392}, {"text": "part-of-speech (POS) tagging", "start_pos": 213, "end_pos": 241, "type": "TASK", "confidence": 0.5910232841968537}]}, {"text": "Dependency Model with Valence (DMV) was the first to beat a simple parsing heuristic -the right-branching baseline.", "labels": [], "entities": []}, {"text": "Today's state-of-the-art systems) are still rooted in the DMV.", "labels": [], "entities": [{"text": "DMV", "start_pos": 58, "end_pos": 61, "type": "DATASET", "confidence": 0.8514160513877869}]}, {"text": "Despite recent advances, unsupervised parsers lag far behind their supervised counterparts.", "labels": [], "entities": []}, {"text": "Although large amounts of unlabeled data are known to improve semi-supervised parsing (, the best unsupervised systems useless data than is available for supervised training, relying on complex models instead: Extended Valence Grammar (EVG) combats data sparsity with smoothing alone, training on the same small subset of the tree-bank as the classic implementation of the DMV; use more complicated algorithms (variational EM and MBR decoding) and stronger linguistic hints (tying related parts of speech and syntactically similar bilingual data).", "labels": [], "entities": []}, {"text": "We explore what can be achieved through judicious use of data and simple, scalable techniques.", "labels": [], "entities": []}, {"text": "Our first approach iterates over a series of training sets that gradually increase in size and complexity, forming an initialization-independent scaffolding for learning a grammar.", "labels": [], "entities": []}, {"text": "It works with Manning's simple model (the original DMV) and training algorithm (classic EM) but eliminates their crucial dependence on manually-tuned priors.", "labels": [], "entities": []}, {"text": "The second technique is consistent with the intuition that learning is most successful within a band of the sizecomplexity spectrum.", "labels": [], "entities": []}, {"text": "Both could be applied to more intricate models and advanced learning algorithms.", "labels": [], "entities": []}, {"text": "We combine them in a third, efficient hybrid method.", "labels": [], "entities": []}], "datasetContent": [{"text": "We packed thousands of empirical outcomes into the space of several graphs).", "labels": [], "entities": []}, {"text": "The colors (also in correspond to different initialization strategies -to a first approximation, the learning algorithm was held constant (see \u00a75).", "labels": [], "entities": []}, {"text": "Figures 4 and 5 tell one part of our story.", "labels": [], "entities": []}, {"text": "As data sets increase in size, training algorithms gain access to more information; however, since in this unsupervised setting training and test sets are the same, additional longer sentences make for substantially more challenging evaluation.", "labels": [], "entities": []}, {"text": "To control for these dynamics, we applied Laplace smoothing to all (otherwise unsmoothed) models and re-plotted their performance, holding several test sets fixed, in.", "labels": [], "entities": []}, {"text": "We report undirected accuracies parenthetically.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Directed and undirected accuracies on Section 23 of WSJ \u221e , WSJ100 and Brown100 for Ad-Hoc  *  , Baby  Steps and Leapfrog, trained at WSJ15 and WSJ45.", "labels": [], "entities": [{"text": "Section 23 of WSJ \u221e", "start_pos": 48, "end_pos": 67, "type": "DATASET", "confidence": 0.8320615768432618}, {"text": "WSJ100", "start_pos": 70, "end_pos": 76, "type": "DATASET", "confidence": 0.8612848520278931}, {"text": "WSJ15", "start_pos": 144, "end_pos": 149, "type": "DATASET", "confidence": 0.9835573434829712}, {"text": "WSJ45", "start_pos": 154, "end_pos": 159, "type": "DATASET", "confidence": 0.9223559498786926}]}, {"text": " Table 2: Directed accuracies on Section 23 of WSJ{10, 20, \u221e } for several baselines and recent state-of-the-art systems.", "labels": [], "entities": [{"text": "Section 23 of WSJ{10, 20, \u221e }", "start_pos": 33, "end_pos": 62, "type": "DATASET", "confidence": 0.7725419347936456}]}]}