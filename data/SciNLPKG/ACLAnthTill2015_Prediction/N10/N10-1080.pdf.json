{"title": [{"text": "The Best Lexical Metric for Phrase-Based Statistical MT System Optimization", "labels": [], "entities": [{"text": "Phrase-Based Statistical MT System Optimization", "start_pos": 28, "end_pos": 75, "type": "TASK", "confidence": 0.7188583612442017}]}], "abstractContent": [{"text": "Translation systems are generally trained to optimize BLEU, but many alternative metrics are available.", "labels": [], "entities": [{"text": "Translation", "start_pos": 0, "end_pos": 11, "type": "TASK", "confidence": 0.9507054090499878}, {"text": "BLEU", "start_pos": 54, "end_pos": 58, "type": "METRIC", "confidence": 0.9954177141189575}]}, {"text": "We explore how optimizing toward various automatic evaluation metrics (BLEU, METEOR, NIST, TER) affects the resulting model.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 71, "end_pos": 75, "type": "METRIC", "confidence": 0.9986883997917175}, {"text": "METEOR", "start_pos": 77, "end_pos": 83, "type": "METRIC", "confidence": 0.9772639274597168}, {"text": "TER", "start_pos": 91, "end_pos": 94, "type": "METRIC", "confidence": 0.9906685948371887}]}, {"text": "We train a state-of-the-art MT system using MERT on many parameteriza-tions of each metric and evaluate the resulting models on the other metrics and also using human judges.", "labels": [], "entities": [{"text": "MT", "start_pos": 28, "end_pos": 30, "type": "TASK", "confidence": 0.9936908483505249}, {"text": "MERT", "start_pos": 44, "end_pos": 48, "type": "METRIC", "confidence": 0.7806762456893921}]}, {"text": "In accordance with popular wisdom, we find that it's important to train on the same metric used in testing.", "labels": [], "entities": []}, {"text": "However, we also find that training to a newer metric is only useful to the extent that the MT model's structure and features allow it to take advantage of the metric.", "labels": [], "entities": [{"text": "MT", "start_pos": 92, "end_pos": 94, "type": "TASK", "confidence": 0.9338383078575134}]}, {"text": "Contrasting with TER's good correlation with human judgments, we show that people tend to prefer BLEU and NIST trained models to those trained on edit distance based metrics like TER or WER.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 97, "end_pos": 101, "type": "METRIC", "confidence": 0.9973205924034119}, {"text": "WER", "start_pos": 186, "end_pos": 189, "type": "METRIC", "confidence": 0.587159276008606}]}, {"text": "Human preferences for METEOR trained models varies depending on the source language.", "labels": [], "entities": []}, {"text": "Since using BLEU or NIST produces models that are more robust to evaluation by other metrics and perform well inhuman judgments, we conclude they are still the best choice for training.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 12, "end_pos": 16, "type": "METRIC", "confidence": 0.993975818157196}, {"text": "NIST", "start_pos": 20, "end_pos": 24, "type": "DATASET", "confidence": 0.9357474446296692}]}], "introductionContent": [{"text": "Since their introduction, automated measures of machine translation quality have played a critical role in the development and evolution of SMT systems.", "labels": [], "entities": [{"text": "machine translation quality", "start_pos": 48, "end_pos": 75, "type": "TASK", "confidence": 0.7499210238456726}, {"text": "SMT", "start_pos": 140, "end_pos": 143, "type": "TASK", "confidence": 0.9973487854003906}]}, {"text": "While such metrics were initially intended for evaluation, popular training methods such as minimum error rate training (MERT) and margin infused relaxed algorithm (MIRA)) train translation models toward a specific evaluation metric.", "labels": [], "entities": [{"text": "minimum error rate training (MERT)", "start_pos": 92, "end_pos": 126, "type": "METRIC", "confidence": 0.8445251286029816}, {"text": "margin infused relaxed algorithm (MIRA))", "start_pos": 131, "end_pos": 171, "type": "METRIC", "confidence": 0.8668524708066668}]}, {"text": "This makes the quality of the resulting model dependent on how accurately the automatic metric actually reflects human preferences.", "labels": [], "entities": []}, {"text": "The most popular metric for both comparing systems and tuning MT models has been BLEU.", "labels": [], "entities": [{"text": "MT", "start_pos": 62, "end_pos": 64, "type": "TASK", "confidence": 0.9062361121177673}, {"text": "BLEU", "start_pos": 81, "end_pos": 85, "type": "METRIC", "confidence": 0.9977057576179504}]}, {"text": "While BLEU () is relatively simple, scoring translations according to their n-gram overlap with reference translations, it still achieves a reasonable correlation with human judgments of translation quality.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 6, "end_pos": 10, "type": "METRIC", "confidence": 0.9974008798599243}]}, {"text": "It is also robust enough to use for automatic optimization.", "labels": [], "entities": [{"text": "automatic optimization", "start_pos": 36, "end_pos": 58, "type": "TASK", "confidence": 0.6901855319738388}]}, {"text": "However, BLEU does have a number of shortcomings.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 9, "end_pos": 13, "type": "METRIC", "confidence": 0.9499022960662842}]}, {"text": "It doesn't penalize n-gram scrambling), and since it isn't aware of synonymous words or phrases, it can inappropriately penalize translations that use them.", "labels": [], "entities": []}, {"text": "Recently, there have been efforts to develop better evaluation metrics.", "labels": [], "entities": []}, {"text": "Metrics such as Translation Edit Rate (TER) and METEOR 1 perform a more sophisticated analysis of the translations being evaluated and the scores they produce tend to achieve a better correlation with human judgments than those produced by BLEU ().", "labels": [], "entities": [{"text": "Translation Edit Rate (TER)", "start_pos": 16, "end_pos": 43, "type": "METRIC", "confidence": 0.8403597474098206}, {"text": "METEOR 1", "start_pos": 48, "end_pos": 56, "type": "METRIC", "confidence": 0.9836841821670532}, {"text": "BLEU", "start_pos": 240, "end_pos": 244, "type": "METRIC", "confidence": 0.9888486862182617}]}, {"text": "Their better correlations suggest that we might obtain higher quality translations by making use of these new metrics when training our models.", "labels": [], "entities": []}, {"text": "We expect that training on a specific metric will produce the best performing model according to that met-ric.", "labels": [], "entities": []}, {"text": "Doing better on metrics that better reflect human judgments seems to imply the translations produced by the model would be preferred by human judges.", "labels": [], "entities": []}, {"text": "However, there are four potential problems.", "labels": [], "entities": []}, {"text": "First, some metrics could be susceptible to systematic exploitation by the training algorithm and result in model translations that have a high score according to the evaluation metric but that are of low quality.", "labels": [], "entities": []}, {"text": "Second, other metrics may result in objective functions that are harder to optimize.", "labels": [], "entities": []}, {"text": "Third, some may result in better generalization performance attest time by not encouraging overfitting of the training data.", "labels": [], "entities": [{"text": "generalization", "start_pos": 33, "end_pos": 47, "type": "TASK", "confidence": 0.9743192195892334}]}, {"text": "Finally, as a practical concern, metrics used for training cannot be too slow.", "labels": [], "entities": []}, {"text": "In this paper, we systematically explore these four issues for the most popular metrics available to the MT community.", "labels": [], "entities": [{"text": "MT", "start_pos": 105, "end_pos": 107, "type": "TASK", "confidence": 0.9761801362037659}]}, {"text": "We examine how well models perform both on the metrics on which they were trained and on the other alternative metrics.", "labels": [], "entities": []}, {"text": "Multiple models are trained using each metric in order to determine the stability of the resulting models.", "labels": [], "entities": []}, {"text": "Select models are scored by human judges in order to determine how performance differences obtained by tuning to different automated metrics relates to actual human preferences.", "labels": [], "entities": []}, {"text": "The next sections introduce the metrics and our training procedure.", "labels": [], "entities": []}, {"text": "We follow with two sets of core results, machine evaluation in section 5, and human evaluation in section 6.", "labels": [], "entities": []}], "datasetContent": [{"text": "Designing good automated metrics for evaluating machine translations is challenging due to the variety of acceptable translations for each foreign sentence.", "labels": [], "entities": [{"text": "evaluating machine translations", "start_pos": 37, "end_pos": 68, "type": "TASK", "confidence": 0.651591956615448}]}, {"text": "Popular metrics produce scores primarily based on matching sequences of words in the system translation to those in one or more reference translations.", "labels": [], "entities": []}, {"text": "The metrics primarily differ in how they account for reorderings and synonyms.", "labels": [], "entities": []}, {"text": "Experiments were run using Phrasal (, a left-to-right beam search decoder that achieves a matching BLEU score to) on a variety of data sets.", "labels": [], "entities": [{"text": "Phrasal", "start_pos": 27, "end_pos": 34, "type": "DATASET", "confidence": 0.7104867100715637}, {"text": "BLEU score", "start_pos": 99, "end_pos": 109, "type": "METRIC", "confidence": 0.9643832743167877}]}, {"text": "During decoding we made use of a stack size of 100, set the distortion limit to 6, and retrieved 20 translation options for each unique source phrase.", "labels": [], "entities": []}, {"text": "Using the selected metrics, we train both Chinese to English and Arabic to English models.", "labels": [], "entities": []}, {"text": "The Chinese to English models are trained using NIST MT02 and evaluated on NIST MT03.", "labels": [], "entities": [{"text": "NIST", "start_pos": 48, "end_pos": 52, "type": "DATASET", "confidence": 0.978894829750061}, {"text": "MT02", "start_pos": 53, "end_pos": 57, "type": "DATASET", "confidence": 0.48197245597839355}, {"text": "NIST", "start_pos": 75, "end_pos": 79, "type": "DATASET", "confidence": 0.9895775318145752}, {"text": "MT03", "start_pos": 80, "end_pos": 84, "type": "DATASET", "confidence": 0.5110400319099426}]}, {"text": "The Arabic to English experiments use NIST MT06 for training and GALE dev07 for evaluation.", "labels": [], "entities": [{"text": "NIST MT06", "start_pos": 38, "end_pos": 47, "type": "DATASET", "confidence": 0.8090241253376007}, {"text": "GALE", "start_pos": 65, "end_pos": 69, "type": "METRIC", "confidence": 0.8801155090332031}]}, {"text": "The resulting models are scored using all of the standalone metrics used during training.", "labels": [], "entities": []}, {"text": "The best evaluation metric to use during training is the one that ultimately leads to the best translations according to human judges.", "labels": [], "entities": [{"text": "translations", "start_pos": 95, "end_pos": 107, "type": "TASK", "confidence": 0.9415366053581238}]}, {"text": "We perform a human evaluation of select models using Amazon Mechanical Turk, an online service for cheaply performing simple tasks that require human intelligence.", "labels": [], "entities": [{"text": "Amazon Mechanical Turk", "start_pos": 53, "end_pos": 75, "type": "DATASET", "confidence": 0.9022880593935648}]}, {"text": "To use the service, tasks are broken down into individual units of work known as human intelligence tasks (HITs).", "labels": [], "entities": []}, {"text": "HITs are assigned a small amount of money that is paid out to the workers that complete them.", "labels": [], "entities": []}, {"text": "For many natural language annotation tasks, including machine translation evaluation, it is possible to obtain annotations that are as good as those pro-   duced by experts by having multiple workers complete each HIT and then combining their answers ().", "labels": [], "entities": [{"text": "machine translation evaluation", "start_pos": 54, "end_pos": 84, "type": "TASK", "confidence": 0.8787226478258768}]}, {"text": "We perform a pairwise comparison of the translations produced for the first 200 sentences of our Chinese to English test data (MT03) and our Arabic to English test data (dev07).", "labels": [], "entities": [{"text": "Chinese to English test data (MT03", "start_pos": 97, "end_pos": 131, "type": "DATASET", "confidence": 0.6887084415980748}]}, {"text": "The HITs consist of a pair of machine translated sentences and a single human generated reference translation.", "labels": [], "entities": []}, {"text": "The reference is chosen at random from those available for each sentence.", "labels": [], "entities": []}, {"text": "Capitalization of the translated sentences is restored using an HMM based truecaser (.", "labels": [], "entities": []}, {"text": "Turkers are instructed to \".", "labels": [], "entities": []}, {"text": "select the machine translation generated sentence that is easiest to read and best conveys what is stated in the reference\".", "labels": [], "entities": []}, {"text": "Differences between the two machine translations are emphasized by being underlined and bold faced.", "labels": [], "entities": []}, {"text": "The resulting HITs are made available only to workers in the United States, as pilot experiments indicated this results in more consistent preference judgments.", "labels": [], "entities": []}, {"text": "Three preference judgments are obtained for each pair of translations and are combined using weighted majority vote.", "labels": [], "entities": []}, {"text": "As shown in table 5, in many cases the quality of the translations produced by models trained to different metrics is remarkably similar.", "labels": [], "entities": []}, {"text": "Training to the simpler edit distance metric WER produces translations that are as good as those from models tuned to the similar but more advanced TERp metric that allows for swaps.", "labels": [], "entities": [{"text": "TERp", "start_pos": 148, "end_pos": 152, "type": "METRIC", "confidence": 0.8885573744773865}]}, {"text": "Similarly, training to TERpA, which makes use of both a paraphrase table and edit costs tuned to human judgments, is no better than TERp.", "labels": [], "entities": [{"text": "TERpA", "start_pos": 23, "end_pos": 28, "type": "METRIC", "confidence": 0.8020462989807129}, {"text": "TERp", "start_pos": 132, "end_pos": 136, "type": "METRIC", "confidence": 0.7907609939575195}]}, {"text": "For the Chinese to English results, there is a statistically significant human preference for translations that are produced by training to BLEU:4 and a marginally significant preferences for training to NIST over the default configuration of TERp.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 140, "end_pos": 144, "type": "METRIC", "confidence": 0.9946377873420715}, {"text": "NIST", "start_pos": 204, "end_pos": 208, "type": "DATASET", "confidence": 0.950115442276001}]}, {"text": "This contrasts sharply with earlier work showing that TER and TERp correlate better with human judgements than BLEU ().", "labels": [], "entities": [{"text": "TER", "start_pos": 54, "end_pos": 57, "type": "METRIC", "confidence": 0.9965983033180237}, {"text": "TERp", "start_pos": 62, "end_pos": 66, "type": "METRIC", "confidence": 0.9849623441696167}, {"text": "BLEU", "start_pos": 111, "end_pos": 115, "type": "METRIC", "confidence": 0.9984408020973206}]}, {"text": "While it is assumed that, by using MERT, \"improved evaluation measures lead directly to improved machine translation quality\", these results show improved correlations with human judgments are not always sufficient to establish that tuning to a metric will result in higher quality translations.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 97, "end_pos": 116, "type": "TASK", "confidence": 0.6853959411382675}]}, {"text": "In the Arabic results, we see a similar pattern where NIST is preferred to TERp, again with marginal signficance.", "labels": [], "entities": [{"text": "TERp", "start_pos": 75, "end_pos": 79, "type": "METRIC", "confidence": 0.9962977766990662}]}, {"text": "Strangely, however, there is no real difference between TERp vs. BLEU:4.", "labels": [], "entities": [{"text": "TERp", "start_pos": 56, "end_pos": 60, "type": "METRIC", "confidence": 0.975408673286438}, {"text": "BLEU", "start_pos": 65, "end_pos": 69, "type": "METRIC", "confidence": 0.9977213740348816}]}, {"text": "For Arabic, training to ranking METEOR is worse than BLEU:4, with the differences being very significant.", "labels": [], "entities": [{"text": "METEOR", "start_pos": 32, "end_pos": 38, "type": "METRIC", "confidence": 0.93864506483078}, {"text": "BLEU:4", "start_pos": 53, "end_pos": 59, "type": "METRIC", "confidence": 0.9127498269081116}]}, {"text": "The Arabic results also trend toward suggesting that BLEU:4 is better than either standard METEOR and METEOR \u03b1 0.5.", "labels": [], "entities": [{"text": "BLEU:4", "start_pos": 53, "end_pos": 59, "type": "METRIC", "confidence": 0.9673015475273132}, {"text": "METEOR", "start_pos": 91, "end_pos": 97, "type": "METRIC", "confidence": 0.5581681132316589}, {"text": "METEOR \u03b1 0.5", "start_pos": 102, "end_pos": 114, "type": "METRIC", "confidence": 0.7869903047879537}]}, {"text": "However, for the Chinese models, training to standard METEOR and METEOR \u03b1 0.5 is about as good as training to BLEU:4.", "labels": [], "entities": [{"text": "METEOR", "start_pos": 54, "end_pos": 60, "type": "METRIC", "confidence": 0.6496366858482361}, {"text": "METEOR \u03b1 0.5", "start_pos": 65, "end_pos": 77, "type": "METRIC", "confidence": 0.8624764482180277}, {"text": "BLEU", "start_pos": 110, "end_pos": 114, "type": "METRIC", "confidence": 0.9985964894294739}]}, {"text": "In both the Chinese and Arabic results, the METEOR \u03b1 0.5 models are at least as good as those trained to standard METEOR and METEOR ranking.", "labels": [], "entities": [{"text": "METEOR \u03b1 0.5", "start_pos": 44, "end_pos": 56, "type": "METRIC", "confidence": 0.7907527486483256}, {"text": "METEOR", "start_pos": 114, "end_pos": 120, "type": "DATASET", "confidence": 0.8250537514686584}, {"text": "METEOR", "start_pos": 125, "end_pos": 131, "type": "DATASET", "confidence": 0.6168054938316345}]}, {"text": "In contrast to the cross evaluation metric results, where the differences between the \u03b1 0.5 models and the standard METEOR models were always fairly dramatic, the human preferences suggest there is often not much of a difference in the true quality of the translations produced by these models.", "labels": [], "entities": [{"text": "METEOR", "start_pos": 116, "end_pos": 122, "type": "METRIC", "confidence": 0.5829340815544128}]}], "tableCaptions": [{"text": " Table 1: Chinese to English test set performance on MT03 using models trained using MERT on MT02. In each column,  cells shaded blue are better than average and those shaded red are below average. The intensity of the shading indicates  the degree of deviation from average. For BLEU, NIST, and METEOR, higher is better. For edit distance metrics like  TER and WER, lower is better.", "labels": [], "entities": [{"text": "MT03", "start_pos": 53, "end_pos": 57, "type": "DATASET", "confidence": 0.7479748725891113}, {"text": "MT02", "start_pos": 93, "end_pos": 97, "type": "DATASET", "confidence": 0.9232077598571777}, {"text": "BLEU", "start_pos": 280, "end_pos": 284, "type": "METRIC", "confidence": 0.9948516488075256}, {"text": "NIST", "start_pos": 286, "end_pos": 290, "type": "DATASET", "confidence": 0.8690757155418396}, {"text": "METEOR", "start_pos": 296, "end_pos": 302, "type": "DATASET", "confidence": 0.5165508389472961}, {"text": "TER", "start_pos": 354, "end_pos": 357, "type": "METRIC", "confidence": 0.9804940223693848}, {"text": "WER", "start_pos": 362, "end_pos": 365, "type": "METRIC", "confidence": 0.9315195083618164}]}, {"text": " Table 2: Arabic to English test set performance on dev07 using models trained using MERT on MT06. As above, in each  column, cells shaded blue are better than average and those shaded red are below average. The intensity of the shading  indicates the degree of deviation from average.", "labels": [], "entities": [{"text": "MT06", "start_pos": 93, "end_pos": 97, "type": "DATASET", "confidence": 0.9379104375839233}]}, {"text": " Table 3: Chinese to English MERT iterations and training  times, given in hours:mins and excluding decoder time.", "labels": [], "entities": [{"text": "MERT", "start_pos": 29, "end_pos": 33, "type": "TASK", "confidence": 0.5816377997398376}]}, {"text": " Table 4: MERT model variation for Chinese to English. We train five models to each metric listed above. The  collection of models trained to a given metric is then evaluated using the other metrics. We report the resulting  standard devation for the collection on each of the metrics. The collection with the lowest varience is bolded.", "labels": [], "entities": [{"text": "MERT", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.5619869232177734}]}, {"text": " Table 5: Select pairwise preference for models trained to  different evaluation metrics. For A vs. B, preferred indi- cates how often A was preferred to B. We bold the better  training metric for statistically significant differences.", "labels": [], "entities": []}]}