{"title": [{"text": "Qme! : A Speech-based Question-Answering system on Mobile Devices", "labels": [], "entities": []}], "abstractContent": [{"text": "Mobile devices are becoming the dominant mode of information access despite being cumbersome to input text using small keyboards and browsing web pages on small screens.", "labels": [], "entities": []}, {"text": "We present Qme!, a speech-based question-answering system that allows for spoken queries and retrieves answers to the questions instead of web pages.", "labels": [], "entities": []}, {"text": "We present bootstrap methods to distinguish dynamic questions from static questions and we show the benefits of tight coupling of speech recognition and retrieval components of the system.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 130, "end_pos": 148, "type": "TASK", "confidence": 0.6941171288490295}]}], "introductionContent": [{"text": "Access to information has moved from desktop and laptop computers in office and home environments to bean anyplace, anytime activity due to mobile devices.", "labels": [], "entities": []}, {"text": "Although mobile devices have small keyboards that make typing text input cumbersome compared to conventional desktop and laptops, the ability to access unlimited amount of information, almost everywhere, through the Internet, using these devices have made them pervasive.", "labels": [], "entities": []}, {"text": "Even so, information access using text input on mobile devices with small screens and soft/small keyboards is tedious and unnatural.", "labels": [], "entities": [{"text": "information access", "start_pos": 9, "end_pos": 27, "type": "TASK", "confidence": 0.8990269005298615}]}, {"text": "In addition, by the mobile nature of these devices, users often like to use them in hands-busy environments, ruling out the possibility of typing text.", "labels": [], "entities": []}, {"text": "We address this issue by allowing the user to query an information repository using speech.", "labels": [], "entities": []}, {"text": "We expect that spoken language queries to be a more natural and less cumbersome way of information access using mobile devices.", "labels": [], "entities": []}, {"text": "A second issue we address is related to directly and precisely answering the user's query beyond serving web pages.", "labels": [], "entities": []}, {"text": "This is in contrast to the current approach where a user types in a query using keywords to a search engine, browses the returned results on the small screen to select a potentially relevant document, suitably magnifies the screen to view the document and searches for the answer to her question in the document.", "labels": [], "entities": []}, {"text": "By providing a method for the user to pose her query in natural language and presenting the relevant answer(s) to her question, we expect the user's information need to be fulfilled in a shorter period of time.", "labels": [], "entities": []}, {"text": "We present a speech-driven question answering system, Qme!, as a solution toward addressing these two issues.", "labels": [], "entities": [{"text": "question answering", "start_pos": 27, "end_pos": 45, "type": "TASK", "confidence": 0.714119166135788}]}, {"text": "The system provides a natural input modality -spoken language input -for the users to pose their information need and presents a collection of answers that potentially address the information need directly.", "labels": [], "entities": []}, {"text": "For a subclass of questions that we term static questions, the system retrieves the answers from an archive of human generated answers to questions.", "labels": [], "entities": []}, {"text": "This ensures higher accuracy for the answers retrieved (if found in the archive) and also allows us to retrieve related questions on the user's topic of interest.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 20, "end_pos": 28, "type": "METRIC", "confidence": 0.9991794228553772}]}, {"text": "For a second subclass of questions that we term dynamic questions, the system retrieves the answer from information databases accessible over the Internet using web forms.", "labels": [], "entities": []}, {"text": "The layout of the paper is as follows.", "labels": [], "entities": []}, {"text": "In Section 2, we review the related literature.", "labels": [], "entities": []}, {"text": "In Section 3, we illustrate the system for speech-driven question answering.", "labels": [], "entities": [{"text": "question answering", "start_pos": 57, "end_pos": 75, "type": "TASK", "confidence": 0.7531071603298187}]}, {"text": "We present the retrieval methods we used to implement the system in Section 4.", "labels": [], "entities": []}, {"text": "In Section 5, we discuss and evaluate our approach to tight coupling of speech recognition and search components.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 72, "end_pos": 90, "type": "TASK", "confidence": 0.74226513504982}]}, {"text": "In Section 6, we present bootstrap techniques to distinguish dynamic questions from static questions, and evaluate the efficacy of these techniques on a test corpus.", "labels": [], "entities": []}, {"text": "We conclude in Section 7.", "labels": [], "entities": []}], "datasetContent": [{"text": "For the set of Seen queries, we evaluate the relevance of the retrieved top-20 question-answer pairs in two ways: 1.", "labels": [], "entities": []}, {"text": "Retrieval Accuracy of Top-N results: We evaluate whether the question that matches the user query exactly is located in the top-1, top-5, top-10, top-20 or not in top-20 of the retrieved questions.", "labels": [], "entities": [{"text": "Retrieval Accuracy", "start_pos": 0, "end_pos": 18, "type": "METRIC", "confidence": 0.768083393573761}]}, {"text": "2. Coherence metric: We compute the coherence of the retrieved set as the mean of the BLEUscore between the input query and the set of top-5 retrieved questions.", "labels": [], "entities": [{"text": "BLEUscore", "start_pos": 86, "end_pos": 95, "type": "METRIC", "confidence": 0.9990031123161316}]}, {"text": "The intuition is that we do not want the top-5 retrieved QA pairs to distract the user by not being relevant to the user's query.", "labels": [], "entities": []}, {"text": "For the set of Unseen queries, since there are no questions in the database that exactly match the input query, we evaluate the relevance of the top-20 retrieved question-answer pairs in the following way.", "labels": [], "entities": []}, {"text": "For each of the 645 Unseen queries, we know the human-generated answer.", "labels": [], "entities": []}, {"text": "We manually annotated each unseen query with the Best-Matched QA pair whose answer was the closest semantic match to the human-generated answer for that unseen query.", "labels": [], "entities": []}, {"text": "We evaluate the position of the Best-Matched QA in the list of top twenty retrieved QA pairs for each retrieval method.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Coherence metric results for top-5 queries re- trieved using different retrieval techniques for the seen  set.", "labels": [], "entities": []}, {"text": " Table 2: Retrieval results for the Unseen queries", "labels": [], "entities": [{"text": "Retrieval", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.7742137312889099}]}, {"text": " Table 3: ASR accuracies of the best path before and after  (in parenthesis) the composition of SearchFST", "labels": [], "entities": [{"text": "ASR", "start_pos": 10, "end_pos": 13, "type": "TASK", "confidence": 0.6381374597549438}, {"text": "accuracies", "start_pos": 14, "end_pos": 24, "type": "METRIC", "confidence": 0.5918660163879395}, {"text": "SearchFST", "start_pos": 96, "end_pos": 105, "type": "DATASET", "confidence": 0.7948480844497681}]}]}