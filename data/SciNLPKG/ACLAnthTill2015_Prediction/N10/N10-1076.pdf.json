{"title": [{"text": "Automatic Diacritization for Low-Resource Languages Using a Hybrid Word and Consonant CMM", "labels": [], "entities": []}], "abstractContent": [{"text": "We are interested in diacritizing Semitic languages , especially Syriac, using only dia-critized texts.", "labels": [], "entities": []}, {"text": "Previous methods have required the use of tools such as part-of-speech taggers, segmenters, morphological analyzers, and linguistic rules to produce state-of-the-art results.", "labels": [], "entities": [{"text": "part-of-speech taggers", "start_pos": 56, "end_pos": 78, "type": "TASK", "confidence": 0.7227550446987152}]}, {"text": "We present a low-resource, data-driven, and language-independent approach that uses a hybrid word-and consonant-level conditional Markov model.", "labels": [], "entities": []}, {"text": "Our approach rivals the best previously published results in Arabic (15% WER with case endings), without the use of a morphological analyzer.", "labels": [], "entities": [{"text": "WER", "start_pos": 73, "end_pos": 76, "type": "METRIC", "confidence": 0.9949325919151306}]}, {"text": "In Syriac, we reduce the WER over a strong baseline by 30% to achieve a WER of 10.5%.", "labels": [], "entities": [{"text": "WER", "start_pos": 25, "end_pos": 28, "type": "METRIC", "confidence": 0.6953113675117493}, {"text": "WER", "start_pos": 72, "end_pos": 75, "type": "METRIC", "confidence": 0.9750030040740967}]}, {"text": "We also report results for Hebrew and English.", "labels": [], "entities": []}], "introductionContent": [{"text": "Abjad writing systems omit vowels and other diacritics.", "labels": [], "entities": []}, {"text": "The ability to restore these diacritics is useful for personal, industrial, and governmental purposes-especially for Semitic languages.", "labels": [], "entities": []}, {"text": "In its own right, the ability to diacritize can aid language learning and is necessary for speech-based assistive technologies, including speech recognition and text-to-speech.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 138, "end_pos": 156, "type": "TASK", "confidence": 0.722927138209343}]}, {"text": "Diacritics are also useful for tasks such as segmentation, morphological disambiguation, and machine translation, making diacritization important to Natural Language Processing (NLP) systems and intelligence gathering.", "labels": [], "entities": [{"text": "morphological disambiguation", "start_pos": 59, "end_pos": 87, "type": "TASK", "confidence": 0.7114528119564056}, {"text": "machine translation", "start_pos": 93, "end_pos": 112, "type": "TASK", "confidence": 0.8034399151802063}, {"text": "intelligence gathering", "start_pos": 195, "end_pos": 217, "type": "TASK", "confidence": 0.7397104799747467}]}, {"text": "In alphabetic writing systems, similar techniques have been used to restore accents from plain text and could be used to recover missing letters in the compressed writing styles found in email, text, and instant messages.", "labels": [], "entities": []}, {"text": "We are particularly interested in diacritizing Syriac, a low-resource dialect of Aramaic, which possesses properties similar to Arabic and Hebrew.", "labels": [], "entities": []}, {"text": "This work employs conditional Markov models (CMMs) () to diacritize Semitic (and other) languages and requires only diacritized texts for training.", "labels": [], "entities": []}, {"text": "Such an approach is useful for languages (like Syriac) in which annotated data and linguistic tools such as part-of-speech (POS) taggers, segmenters, and morphological analyzers are not available.", "labels": [], "entities": []}, {"text": "Our main contributions are as follows: (1) we introduce a hybrid word and consonant CMM that allows access to the diacritized form of the previous words; (2) we introduce new features available in the proposed model; and (3) we describe an efficient, approximate decoder.", "labels": [], "entities": []}, {"text": "Our models significantly outperform existing low-resource approaches across multiple related and unrelated languages and even achieve near state-of-the-art results when compared to resource-rich systems.", "labels": [], "entities": []}, {"text": "In the next section, we review previous work relevant to our approach.", "labels": [], "entities": []}, {"text": "Section 3 then motivates and describes the models and features used in our framework, including a description of the decoder.", "labels": [], "entities": []}, {"text": "We describe our data in Section 4 and detail our experimental setup in Section 5.", "labels": [], "entities": []}, {"text": "Section 6 presents our results.", "labels": [], "entities": []}, {"text": "Finally, Section 7 briefly discusses our conclusions and offers ideas for future work.", "labels": [], "entities": []}], "datasetContent": [{"text": "For all feature engineering and tuning, we trained and tested on training and development test sets, respectively (as specified above).", "labels": [], "entities": []}, {"text": "Final results are reported by folding the development test set into the training data and evaluating on the blind test set.", "labels": [], "entities": []}, {"text": "We retain only those features that occur more than once.", "labels": [], "entities": []}, {"text": "For each approach, we report the Word Error Rate (WER) (i.e., the percentage of words that were incorrectly diacritized), along with the Diacritic Error Rate (DER) (i.e., the percentage of diacritics, including the null diacritic, that were incorrectly predicted).", "labels": [], "entities": [{"text": "Word Error Rate (WER)", "start_pos": 33, "end_pos": 54, "type": "METRIC", "confidence": 0.9185384511947632}, {"text": "Diacritic Error Rate (DER)", "start_pos": 137, "end_pos": 163, "type": "METRIC", "confidence": 0.8610454797744751}]}, {"text": "We also report both WER and DER for only those words that were not seen during training (UWER and UDER, respectively).", "labels": [], "entities": [{"text": "WER", "start_pos": 20, "end_pos": 23, "type": "METRIC", "confidence": 0.9980815649032593}, {"text": "DER", "start_pos": 28, "end_pos": 31, "type": "METRIC", "confidence": 0.9813677668571472}, {"text": "UWER", "start_pos": 89, "end_pos": 93, "type": "METRIC", "confidence": 0.8361960649490356}, {"text": "UDER", "start_pos": 98, "end_pos": 102, "type": "METRIC", "confidence": 0.9446331858634949}]}, {"text": "We found that precision, recall, and f-score were nearly perfectly correlated with DER; hence, we omit this information for brevity.", "labels": [], "entities": [{"text": "precision", "start_pos": 14, "end_pos": 23, "type": "METRIC", "confidence": 0.9996705055236816}, {"text": "recall", "start_pos": 25, "end_pos": 31, "type": "METRIC", "confidence": 0.9995232820510864}, {"text": "f-score", "start_pos": 37, "end_pos": 44, "type": "METRIC", "confidence": 0.9902592897415161}, {"text": "DER", "start_pos": 83, "end_pos": 86, "type": "METRIC", "confidence": 0.8332406878471375}]}, {"text": "In previous work, report the lowest error rates of the low-resource models.", "labels": [], "entities": [{"text": "error rates", "start_pos": 36, "end_pos": 47, "type": "METRIC", "confidence": 0.9682238101959229}]}, {"text": "Although their results are not directly comparable to, we have independently confirmed that the former slightly outperforms the latter using the same diacritics and on the same dataset (see, thereby providing the strongest published baseline for Arabic on a common dataset.", "labels": [], "entities": []}, {"text": "We denote this model as k\u00fcbler and use it as a strong baseline for all datasets.", "labels": [], "entities": []}, {"text": "For the Arabic results, we additionally include lexical model (zitouni-lex) and their model that uses a segmenter and POS tagger (zitouni-all), which are not immediately available to us for Syriac.", "labels": [], "entities": []}, {"text": "For yet another point of reference for Arabic, we provide the results from the state-of-the-art (resource-rich) approach of.", "labels": [], "entities": []}, {"text": "This model is at an extreme advantage, having access to a full morphological analyzer.", "labels": [], "entities": []}, {"text": "Note that for these three models we simply report their published results and do not attempt to reproduce them.", "labels": [], "entities": []}, {"text": "Since k\u00fcbler is of a different model class than ours, we consider an additional baseline that is a consonant-level CMM with access to the same information, namely, only those consonants within a window of 5 to either side (ccmm).", "labels": [], "entities": []}, {"text": "This is equivalent to a special case of our hybrid model wherein both the word-level and the consonant-level Markov order are 0.", "labels": [], "entities": []}, {"text": "The features that we extract from this window are the windowed n-gram features.", "labels": [], "entities": []}, {"text": "In order to assess the utility of previous diacritics and how effectively our features leverage them, we build a model based on the methodology from Section 3 but specify that all words are rare, effectively creating a consonant-only model that has access to the diacritics of previous words.", "labels": [], "entities": []}, {"text": "We call this model cons-only.", "labels": [], "entities": []}, {"text": "We note that the main difference between this model and zitouni-lex are features that depend on previous diacritized words.", "labels": [], "entities": []}, {"text": "Finally, we present results using our full hybrid model (hybrid).", "labels": [], "entities": []}, {"text": "We use a Markov order of 2 at the word and consonant level for both hybrid and cons-only.", "labels": [], "entities": []}], "tableCaptions": []}