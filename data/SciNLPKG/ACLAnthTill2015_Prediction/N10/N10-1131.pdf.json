{"title": [{"text": "An extractive supervised two-stage method for sentence compression", "labels": [], "entities": [{"text": "sentence compression", "start_pos": 46, "end_pos": 66, "type": "TASK", "confidence": 0.8218854367733002}]}], "abstractContent": [{"text": "We present anew method that compresses sentences by removing words.", "labels": [], "entities": []}, {"text": "Ina first stage, it generates candidate compressions by removing branches from the source sentence's dependency tree using a Maximum Entropy classifier.", "labels": [], "entities": []}, {"text": "Ina second stage, it chooses the best among the candidate compressions using a Support Vector Machine Regression model.", "labels": [], "entities": []}, {"text": "Experimental results show that our method achieves state-of-the-art performance without requiring any manually written rules.", "labels": [], "entities": []}], "introductionContent": [{"text": "Sentence compression is the task of producing a shorter form of a single given sentence, so that the new form is grammatical and retains the most important information of the original one).", "labels": [], "entities": [{"text": "Sentence compression", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.9507867991924286}]}, {"text": "Sentence compression is valuable in many applications, for example when displaying texts on small screens), in subtitle generation (, and in text summarization (.", "labels": [], "entities": [{"text": "Sentence compression", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.9226835072040558}, {"text": "subtitle generation", "start_pos": 111, "end_pos": 130, "type": "TASK", "confidence": 0.7426793873310089}, {"text": "text summarization", "start_pos": 141, "end_pos": 159, "type": "TASK", "confidence": 0.727801501750946}]}, {"text": "People use various methods to shorten sentences, including word or phrase removal, using shorter paraphrases, and commonsense knowledge.", "labels": [], "entities": [{"text": "word or phrase removal", "start_pos": 59, "end_pos": 81, "type": "TASK", "confidence": 0.5994212403893471}]}, {"text": "However, reasonable machine-generated sentence compressions can often be obtained by only removing words.", "labels": [], "entities": [{"text": "sentence compressions", "start_pos": 38, "end_pos": 59, "type": "TASK", "confidence": 0.759669303894043}]}, {"text": "We use the term extractive to refer to methods that compress sentences by only removing words, as opposed to abstractive methods, where more elaborate transformations are also allowed.", "labels": [], "entities": []}, {"text": "Most of the existing compression methods are extractive.", "labels": [], "entities": []}, {"text": "Although abstractive methods have also been proposed (, and they may shed more light on how people compress sentences, they do not always manage to outperform extractive methods.", "labels": [], "entities": []}, {"text": "Hence, from an engineering perspective, it is still important to investigate how extractive methods can be improved.", "labels": [], "entities": []}, {"text": "In this paper, we present anew extractive sentence compression method that relies on supervised machine learning.", "labels": [], "entities": [{"text": "extractive sentence compression", "start_pos": 31, "end_pos": 62, "type": "TASK", "confidence": 0.615875373284022}]}, {"text": "Ina first stage, the method generates candidate compressions by removing branches from the source sentence's dependency tree using a Maximum Entropy classifier).", "labels": [], "entities": []}, {"text": "Ina second stage, it chooses the best among the candidate compressions using a Support Vector Machine Regression (SVR) model).", "labels": [], "entities": []}, {"text": "We show experimentally that our method compares favorably to a state-of-the-art extractive compression method, without requiring any manually written rules, unlike other recent work.", "labels": [], "entities": []}, {"text": "In essence, our method is a twotier over-generate and select (or rerank) approach to sentence compression; similar approaches have been adopted in natural language generation and parsing).", "labels": [], "entities": [{"text": "sentence compression", "start_pos": 85, "end_pos": 105, "type": "TASK", "confidence": 0.7627428472042084}, {"text": "natural language generation", "start_pos": 147, "end_pos": 174, "type": "TASK", "confidence": 0.7134791215260824}]}], "datasetContent": [{"text": "We used Stanford's parser) and ME classifier (.", "labels": [], "entities": []}, {"text": "For the (trigram) language model, we used SRILM with modified Kneser-Ney smoothing.", "labels": [], "entities": [{"text": "SRILM", "start_pos": 42, "end_pos": 47, "type": "DATASET", "confidence": 0.5686302781105042}]}, {"text": "The language model was trained on approximately 4.5 million sentences of the TIPSTER corpus.", "labels": [], "entities": [{"text": "TIPSTER corpus", "start_pos": 77, "end_pos": 91, "type": "DATASET", "confidence": 0.9390072226524353}]}, {"text": "To obtain idf (w i ) values, we used approximately 19.5 million verbs and nouns from the TIPSTER corpus.", "labels": [], "entities": [{"text": "TIPSTER corpus", "start_pos": 89, "end_pos": 103, "type": "DATASET", "confidence": 0.9593024849891663}]}, {"text": "T3 requires the syntax trees of the source-gold pairs in Penn Treebank format, as well as a trigram language model.", "labels": [], "entities": [{"text": "T3", "start_pos": 0, "end_pos": 2, "type": "DATASET", "confidence": 0.8105579614639282}, {"text": "Penn Treebank format", "start_pos": 57, "end_pos": 77, "type": "DATASET", "confidence": 0.9837445020675659}]}, {"text": "We obtained T3's trees using Stanford's parser, as in our system, unlike Cohn and Lapata (2009) that use parser.", "labels": [], "entities": []}, {"text": "The language models in T3 and our system are trained on the same data and with the same options used by.", "labels": [], "entities": []}, {"text": "T3 also needs a word-toword alignment of the source-gold pairs, which was obtained by computing the edit distance, as in and SVR-TOKACC-LM.", "labels": [], "entities": [{"text": "SVR-TOKACC-LM", "start_pos": 125, "end_pos": 138, "type": "DATASET", "confidence": 0.9294903874397278}]}, {"text": "We used Edinburgh's \"written\" sentence compression corpus (section 2), which consists of source-gold pairs (one gold compression per source sentence).", "labels": [], "entities": [{"text": "Edinburgh's \"written\" sentence compression corpus", "start_pos": 8, "end_pos": 57, "type": "DATASET", "confidence": 0.8680446818470955}]}, {"text": "The gold compressions were created by deleting words.", "labels": [], "entities": []}, {"text": "We split the corpus in 3 parts: 1024 training, 324 development, and 291 testing pairs.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Results on 80 test sentences. G: grammaticality,  M: meaning preservation, Ov: overall score, CR: com- pression rate, SVR: SVR-TOKACC-LM.", "labels": [], "entities": [{"text": "meaning preservation", "start_pos": 63, "end_pos": 83, "type": "TASK", "confidence": 0.6936687976121902}, {"text": "Ov: overall score", "start_pos": 85, "end_pos": 102, "type": "METRIC", "confidence": 0.91881063580513}, {"text": "CR: com- pression rate", "start_pos": 104, "end_pos": 126, "type": "METRIC", "confidence": 0.7985044568777084}, {"text": "SVR", "start_pos": 128, "end_pos": 131, "type": "METRIC", "confidence": 0.9094880223274231}]}]}