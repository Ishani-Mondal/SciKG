{"title": [{"text": "Bitext-Based Resolution of German Subject-Object Ambiguities", "labels": [], "entities": [{"text": "Bitext-Based Resolution of German Subject-Object Ambiguities", "start_pos": 0, "end_pos": 60, "type": "TASK", "confidence": 0.8229006826877594}]}], "abstractContent": [{"text": "We present a method for disambiguating syntactic subjects from syntactic objects (a frequent ambiguity) in German sentences taken from an English-German bitext.", "labels": [], "entities": [{"text": "disambiguating syntactic subjects from syntactic objects", "start_pos": 24, "end_pos": 80, "type": "TASK", "confidence": 0.7603301505247752}]}, {"text": "We exploit the fact that subject and object are usually easily determined in English.", "labels": [], "entities": []}, {"text": "We show that a simple method disambiguates some subject-object ambiguities in German, while making few errors.", "labels": [], "entities": []}, {"text": "We view this procedure as the first step in automatically acquiring (mostly) correct labeled data.", "labels": [], "entities": []}, {"text": "We also evaluate using it to improve a state of the art statistical parser.", "labels": [], "entities": []}], "introductionContent": [{"text": "Ambiguity of grammatical role is a problem when parsing a number of natural languages.", "labels": [], "entities": [{"text": "Ambiguity of grammatical role", "start_pos": 0, "end_pos": 29, "type": "TASK", "confidence": 0.6533329635858536}]}, {"text": "In German, subject-object ambiguities are frequent.", "labels": [], "entities": []}, {"text": "The sentence \"Die Maus jagt die Katze\" \"the -mousechases -the -cat\" exhibits such an ambiguity.", "labels": [], "entities": []}, {"text": "Because word order is freer in German than in English, the sentence has two possible meanings: (i) The cat is chasing the mouse and (ii) the mouse is chasing the cat.", "labels": [], "entities": []}, {"text": "We exploit the fact that such ambiguities are much less frequent in languages that possess a less flexible syntax than German.", "labels": [], "entities": []}, {"text": "In English, the translation of the sentence \"Die Maus jagt die Katze\" is not ambiguous.", "labels": [], "entities": []}, {"text": "If we have access to this translation, we can use this information to disambiguate the German sentence.", "labels": [], "entities": []}, {"text": "The English translation is viewed as a surrogate for both contextual knowledge from the text and for world knowledge.", "labels": [], "entities": []}, {"text": "We present a method for disambiguating the subject and object roles in German sentences.", "labels": [], "entities": []}, {"text": "We use an English-German bitext and exploit the fact that subject and object roles are rarely ambiguous in English.", "labels": [], "entities": []}, {"text": "Using anew gold standard we created we show that our method disambiguates a significant proportion of subject-object ambiguities in German with high precision.", "labels": [], "entities": [{"text": "precision", "start_pos": 149, "end_pos": 158, "type": "METRIC", "confidence": 0.9782496094703674}]}, {"text": "We view this procedure as the first step in automatically acquiring (mostly) correct labeled data for training a statistical disambiguator that can be used on German text (even when no translation is available).", "labels": [], "entities": []}, {"text": "In addition to measuring algorithm performance directly, we present experiments on improving the disambiguation of BitPar, a state of the art statistical parser.", "labels": [], "entities": []}], "datasetContent": [{"text": "We had access to a small set of gold standard parses), but decided to create a larger corpus.", "labels": [], "entities": []}, {"text": "We found that FSPar had acceptable performance for finding subjectobject ambiguities . The syntactic roles of words marked as ambiguous by FSPar were annotated.", "labels": [], "entities": [{"text": "FSPar", "start_pos": 14, "end_pos": 19, "type": "DATASET", "confidence": 0.730394184589386}, {"text": "FSPar", "start_pos": 139, "end_pos": 144, "type": "DATASET", "confidence": 0.9035353064537048}]}, {"text": "Four annotators annotated the syntactic roles in 4000 sentences using a graphical user interface (GUI).", "labels": [], "entities": []}, {"text": "The GUI showed the ambiguous words in context and gave the annotator four different subject-object labels to choose from for each ambiguous word: subject, object, expletive es and none.", "labels": [], "entities": []}, {"text": "Because the syntactic expletive \"es\" (English gloss: 'it') is frequent in German, as in \"es scheint zu regnen\" 'it appears to be raining,' we created a separate label for expletive \"es\", which is not treated as a subject.", "labels": [], "entities": []}, {"text": "The statistics are shown in table 1. 1000 sentences were annotated by all four annotators.", "labels": [], "entities": []}, {"text": "Inter-annotator agreement was sufficient (\u03ba = 0.77 on average).", "labels": [], "entities": []}, {"text": "The output of our algorithm labels each word that FSPar classified as ambiguous with one of the three possible labels subject, FSPar has a very high precision in detecting subject-object ambiguities, as can be seen in (approximately 0.955, the sum of two left columns divided by sum of all cells).", "labels": [], "entities": [{"text": "FSPar", "start_pos": 127, "end_pos": 132, "type": "DATASET", "confidence": 0.8002197742462158}, {"text": "precision", "start_pos": 149, "end_pos": 158, "type": "METRIC", "confidence": 0.9882035851478577}]}, {"text": "We tried to get an idea of recall using the smaller gold standard.", "labels": [], "entities": [{"text": "recall", "start_pos": 27, "end_pos": 33, "type": "METRIC", "confidence": 0.9240833520889282}]}, {"text": "We made conservative assumptions about recall errors which we manually checked on a small sample, details are omitted.", "labels": [], "entities": [{"text": "recall errors", "start_pos": 39, "end_pos": 52, "type": "METRIC", "confidence": 0.9738286733627319}]}, {"text": "Using these assumptions led to an estimate for recall of 0.733, but true recall is likely higher.", "labels": [], "entities": [{"text": "recall", "start_pos": 47, "end_pos": 53, "type": "METRIC", "confidence": 0.9994803071022034}, {"text": "recall", "start_pos": 73, "end_pos": 79, "type": "METRIC", "confidence": 0.9699018001556396}]}, {"text": "2 German \"es\" is also frequently used as a non-expletive, where it can take a syntactic role.", "labels": [], "entities": []}, {"text": "object and no decision 3 . We use the standard evaluation metrics Precision (P , the percentage of subject and object labelings in our hypothesis that are correct), Recall (R, the percentage of subject and object labelings in the gold standard that are correctly labeled in the hypothesis), and balanced F (F 1 ).", "labels": [], "entities": [{"text": "Precision", "start_pos": 66, "end_pos": 75, "type": "METRIC", "confidence": 0.9958439469337463}, {"text": "Recall", "start_pos": 165, "end_pos": 171, "type": "METRIC", "confidence": 0.9984371066093445}, {"text": "balanced F (F 1", "start_pos": 295, "end_pos": 310, "type": "METRIC", "confidence": 0.6820169806480407}]}, {"text": "shows the performance of our algorithm when evaluated against the manual annotation 4 . The lines nosg, sg, filter-nosg and filter-sg denote different configurations of the algorithm: Second guessing (section 2) was (\"sg\") or was not (\"nosg\") applied and filtering was (\"filter\") or was not applied.", "labels": [], "entities": []}, {"text": "The filter increases precision by only keeping labels of subjects and objects that occur in the default order (e.g., the subject is to the left of the object in the main clause).", "labels": [], "entities": [{"text": "precision", "start_pos": 21, "end_pos": 30, "type": "METRIC", "confidence": 0.9989413619041443}]}, {"text": "As an aid to the user, FSPar presents such a determination of default order depending on its classification of clause type . The columns indicate the heuristic postpro- If expletive es or none was annotated, the system is correct if it does not make a decision.", "labels": [], "entities": [{"text": "FSPar", "start_pos": 23, "end_pos": 28, "type": "DATASET", "confidence": 0.8330152034759521}]}, {"text": "cessing we applied to GIZA++'s alignment.", "labels": [], "entities": [{"text": "GIZA++'", "start_pos": 22, "end_pos": 29, "type": "DATASET", "confidence": 0.8580135107040405}]}, {"text": "DE2EN is the 1-to-N alignment calculated using German as the source language and English as the target language (i.e., each English word is linked to exactly zero or one German words).", "labels": [], "entities": [{"text": "DE2EN", "start_pos": 0, "end_pos": 5, "type": "DATASET", "confidence": 0.5586579442024231}]}, {"text": "As we see in table 2, with the most strict setup, filter-nosg, the algorithm resolves subject-object ambiguities with a precision of more than 92% but the best recall is only 39.4%, obtained using DE2EN.", "labels": [], "entities": [{"text": "precision", "start_pos": 120, "end_pos": 129, "type": "METRIC", "confidence": 0.996550440788269}, {"text": "recall", "start_pos": 160, "end_pos": 166, "type": "METRIC", "confidence": 0.9990602135658264}]}, {"text": "Second guessing increases recall but leads to losses in precision.", "labels": [], "entities": [{"text": "recall", "start_pos": 26, "end_pos": 32, "type": "METRIC", "confidence": 0.9995307922363281}, {"text": "precision", "start_pos": 56, "end_pos": 65, "type": "METRIC", "confidence": 0.9990689158439636}]}, {"text": "The best precision result without the filter is 85.5%.", "labels": [], "entities": [{"text": "precision", "start_pos": 9, "end_pos": 18, "type": "METRIC", "confidence": 0.9996079802513123}]}, {"text": "Improving BitPar's Subject-Object Decisions.", "labels": [], "entities": []}, {"text": "For improving BitPar (which always tries to disambiguate subject-object), our baseline is the accuracy of the most probable parse shown in table 3, row 1.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 94, "end_pos": 102, "type": "METRIC", "confidence": 0.999441921710968}]}, {"text": "Using the most probable parse from BitPar, we relabel a word \"subject\" or \"object\" if our system indicates to do so.", "labels": [], "entities": []}, {"text": "With the algorithm alone we are able to improve recall (table 3, row 2).", "labels": [], "entities": [{"text": "recall", "start_pos": 48, "end_pos": 54, "type": "METRIC", "confidence": 0.9993359446525574}]}, {"text": "When we add the filter both precision and recall are improved (row 3).", "labels": [], "entities": [{"text": "precision", "start_pos": 28, "end_pos": 37, "type": "METRIC", "confidence": 0.999789297580719}, {"text": "recall", "start_pos": 42, "end_pos": 48, "type": "METRIC", "confidence": 0.9997742772102356}]}, {"text": "This experiment measures the improvement possible if our syntactic role information were directly integrated as a hard constraint into a parser (see section 5).", "labels": [], "entities": []}, {"text": "We now perform a simple reranking experiment, using BitPar's 100-best parses.", "labels": [], "entities": []}, {"text": "For each sentence we choose the parse which agrees with as many of the subject/object decisions of the algorithm as possible (once again ignoring words where the algorithm chooses no decision).", "labels": [], "entities": []}, {"text": "In case of ties in the number of agreements, we take the most probable parse.", "labels": [], "entities": []}, {"text": "The results are in rows 4-5.", "labels": [], "entities": []}, {"text": "Reranking increases F 1 by about 0.8%.", "labels": [], "entities": [{"text": "Reranking", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.9777905344963074}, {"text": "F 1", "start_pos": 20, "end_pos": 23, "type": "METRIC", "confidence": 0.9906220138072968}]}], "tableCaptions": [{"text": " Table 1: Annotator decisions on the full gold standard", "labels": [], "entities": []}, {"text": " Table 2: Precision, Recall and F 1 of the algorithm.", "labels": [], "entities": [{"text": "Precision", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9986110925674438}, {"text": "Recall", "start_pos": 21, "end_pos": 27, "type": "METRIC", "confidence": 0.998330295085907}, {"text": "F 1", "start_pos": 32, "end_pos": 35, "type": "METRIC", "confidence": 0.9926921427249908}]}]}