{"title": [{"text": "Relaxed Marginal Inference and its Application to Dependency Parsing", "labels": [], "entities": [{"text": "Relaxed Marginal Inference", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.833435575167338}, {"text": "Dependency Parsing", "start_pos": 50, "end_pos": 68, "type": "TASK", "confidence": 0.6730871349573135}]}], "abstractContent": [{"text": "Recently, relaxation approaches have been successfully used for MAP inference on NLP problems.", "labels": [], "entities": [{"text": "MAP inference", "start_pos": 64, "end_pos": 77, "type": "TASK", "confidence": 0.9887968003749847}]}, {"text": "In this work we show how to extend the relaxation approach to marginal inference used in conditional likelihood training, posterior decoding, confidence estimation, and other tasks.", "labels": [], "entities": [{"text": "confidence estimation", "start_pos": 142, "end_pos": 163, "type": "TASK", "confidence": 0.6559369564056396}]}, {"text": "We evaluate our approach for the case of second-order dependency parsing and observe a tenfold increase in parsing speed, with no loss inaccuracy, by performing inference over a small subset of the full factor graph.", "labels": [], "entities": [{"text": "second-order dependency parsing", "start_pos": 41, "end_pos": 72, "type": "TASK", "confidence": 0.6028911769390106}]}, {"text": "We also contribute abound on the error of the marginal probabilities by a sub-graph with respect to the full graph.", "labels": [], "entities": [{"text": "error", "start_pos": 33, "end_pos": 38, "type": "METRIC", "confidence": 0.967775821685791}]}, {"text": "Finally, while only evaluated with BP in this paper, our approach is general enough to be applied with any marginal inference method in the inner loop.", "labels": [], "entities": [{"text": "BP", "start_pos": 35, "end_pos": 37, "type": "METRIC", "confidence": 0.9972094893455505}]}], "introductionContent": [{"text": "In statistical natural language processing (NLP) we are often concerned with finding the marginal probabilities of events in our models or the expectations of features.", "labels": [], "entities": [{"text": "statistical natural language processing (NLP)", "start_pos": 3, "end_pos": 48, "type": "TASK", "confidence": 0.7517836093902588}]}, {"text": "When training to optimize conditional likelihood, feature expectations are needed to calculate the gradient.", "labels": [], "entities": []}, {"text": "Marginalization also allows a statistical NLP component to give confidence values for its predictions or to marginalize out latent variables.", "labels": [], "entities": [{"text": "Marginalization", "start_pos": 0, "end_pos": 15, "type": "TASK", "confidence": 0.8975346088409424}]}, {"text": "Finally, given the marginal probabilities of variables, we can pick the values that maximize these marginal probabilities (perhaps subject to hard constraints) in order to predict a good variable assignment.", "labels": [], "entities": []}, {"text": "Traditionally, marginal inference in NLP has been performed via dynamic programming (DP); however, because this requires the model to factor in away that lends itself to DP algorithms, we have to restrict the class of probabilistic models we consider.", "labels": [], "entities": []}, {"text": "For example, since we cannot derive a dynamic program for marginal inference in second order non-projective dependency parsing, we have non-projective languages such as Dutch using second order projective models if we want to apply DP.", "labels": [], "entities": [{"text": "second order non-projective dependency parsing", "start_pos": 80, "end_pos": 126, "type": "TASK", "confidence": 0.8210507273674011}]}, {"text": "Some previous work has circumvented this problem for MAP inference by starting with a second-order projective solution and then greedily flipping edges to find a better nonprojective solution).", "labels": [], "entities": [{"text": "MAP inference", "start_pos": 53, "end_pos": 66, "type": "TASK", "confidence": 0.9851929545402527}]}, {"text": "In order to explore richer model structures, the NLP community has recently started to investigate the use of other, well-known machine learning techniques for marginal inference.", "labels": [], "entities": []}, {"text": "One such technique is Markov chain Monte Carlo, and in particular Gibbs sampling (), another is (loopy) sum-product belief propagation).", "labels": [], "entities": [{"text": "sum-product belief propagation", "start_pos": 104, "end_pos": 134, "type": "TASK", "confidence": 0.6644004782040914}]}, {"text": "In both cases we usually work in the framework of graphical models-in our case, with factor graphs that describe our distributions through variables, factors, and factor potentials.", "labels": [], "entities": []}, {"text": "In theory, methods such as belief propagation can take any graph and perform marginal inference.", "labels": [], "entities": [{"text": "belief propagation", "start_pos": 27, "end_pos": 45, "type": "TASK", "confidence": 0.8000712990760803}]}, {"text": "This means that we gain a great amount of flexibility to represent more global and joint distributions for NLP tasks.", "labels": [], "entities": []}, {"text": "The graphical models of interest, however, are often too large and densely connected for efficient inference in them.", "labels": [], "entities": []}, {"text": "For example, in second order often very effective.", "labels": [], "entities": []}, {"text": "dependency parsing models, we have O(n 2 ) variables and O(n 3 ) factors, each of which may have to be inspected several times.", "labels": [], "entities": []}, {"text": "While belief propagation is still tractable here (assuming we follow the approach of to enforce tree constraints), it is still much slower than simpler greedy parsing methods, and the advantage second order models give inaccuracy is often not significant enough to offset the lack of speed in practice.", "labels": [], "entities": [{"text": "belief propagation", "start_pos": 6, "end_pos": 24, "type": "TASK", "confidence": 0.8773820102214813}]}, {"text": "Moreover, if we extend such parsing models to, say, penalizing all pairs of crossing edges or scoring syntax-based alignments, we will need to inspect at least O n 4 factors, increasing our efficiency concerns.", "labels": [], "entities": []}, {"text": "When looking at the related task of finding the most likely assignment in large graphical models (i.e., MAP inference), we notice that several recent approaches have significantly sped up computation through relaxation methods (;).", "labels": [], "entities": []}, {"text": "Here we start with a small subset of the full graph, and run inference for this simpler problem.", "labels": [], "entities": []}, {"text": "Then we search for factors that are \"violated\" in the solution, and add them to the graph.", "labels": [], "entities": []}, {"text": "This is repeated until no more new factors can be added.", "labels": [], "entities": []}, {"text": "Empirically this approach has shown impressive success.", "labels": [], "entities": []}, {"text": "It often dramatically reduces the effective network size, with no loss inaccuracy.", "labels": [], "entities": []}, {"text": "How can we extend or generalize MAP relaxation algorithms to the case of marginal inference?", "labels": [], "entities": [{"text": "MAP relaxation", "start_pos": 32, "end_pos": 46, "type": "TASK", "confidence": 0.9715366661548615}]}, {"text": "Roughly speaking, we answer it by introducing a notion of factor gain that is defined as the KL divergence between the current distribution with and without the given factor.", "labels": [], "entities": []}, {"text": "This quantity is then used in an algorithm that starts with a sub-model, runs marginal inference in it and then determines the gains of the not-yet-added factors.", "labels": [], "entities": []}, {"text": "In turn, all factors for which the gain exceeds some threshold are added to the current model.", "labels": [], "entities": []}, {"text": "This process is repeated until no more new factors can be found or a maximum number of iterations is reached.", "labels": [], "entities": []}, {"text": "We evaluate this form of relaxed marginal inference for the case of second-order dependency parsing.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 81, "end_pos": 99, "type": "TASK", "confidence": 0.6548149436712265}]}, {"text": "We follow Smith and Eisner's tree-aware belief propagation procedure for inference in the inner loop of our algorithm.", "labels": [], "entities": [{"text": "belief propagation", "start_pos": 40, "end_pos": 58, "type": "TASK", "confidence": 0.7251293808221817}]}, {"text": "This leads to a tenfold increase in parsing speed with no loss inaccuracy.", "labels": [], "entities": [{"text": "parsing", "start_pos": 36, "end_pos": 43, "type": "TASK", "confidence": 0.9775220155715942}, {"text": "speed", "start_pos": 44, "end_pos": 49, "type": "METRIC", "confidence": 0.6769599318504333}]}, {"text": "We also contribute abound on the error on marginal probabilities the sub-graph defines with respect to the full graph.", "labels": [], "entities": []}, {"text": "This bound can be used both for terminating (although not done here) and understanding the dynamics of inference.", "labels": [], "entities": []}, {"text": "Finally, while only evaluated with BP so far, it is general enough to be applied with any marginal inference method in the inner loop.", "labels": [], "entities": [{"text": "BP", "start_pos": 35, "end_pos": 37, "type": "METRIC", "confidence": 0.9940933585166931}]}, {"text": "In the following, we first give a sketch of the graphical model we apply.", "labels": [], "entities": []}, {"text": "Then we briefly discuss marginal inference.", "labels": [], "entities": []}, {"text": "In turn we describe our relaxation algorithm for marginal inference and some of its theoretic guarantees.", "labels": [], "entities": []}, {"text": "Then we present empirical support for the effectiveness of our approach, and conclude.", "labels": [], "entities": []}], "datasetContent": [{"text": "In our experiments we seek to answer the following questions.", "labels": [], "entities": []}, {"text": "First, how fast is our relaxation approach compared to full marginal inference at comparable dependency accuracy?", "labels": [], "entities": []}, {"text": "This requires us to find the best tree in terms of marginal probabilities on the link variables.", "labels": [], "entities": []}, {"text": "Second, how good is the final relaxed graph as an approximation of the full graph?", "labels": [], "entities": []}, {"text": "Finally, how does incremental relaxation scale with sentence length?", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Dependency accuracy (%) and average parsing time (sec.) using second order models.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 21, "end_pos": 29, "type": "METRIC", "confidence": 0.9288930296897888}, {"text": "average parsing time (sec.)", "start_pos": 38, "end_pos": 65, "type": "METRIC", "confidence": 0.8660978178183237}]}, {"text": " Table 2: Ratio of partial and full graph size (Size),  runtime in seconds (Time), avg. error on marginals  (Err.) and tree accuracy (Acc.) for Danish.", "labels": [], "entities": [{"text": "Time)", "start_pos": 76, "end_pos": 81, "type": "METRIC", "confidence": 0.8772442936897278}, {"text": "avg. error on marginals  (Err.)", "start_pos": 83, "end_pos": 114, "type": "METRIC", "confidence": 0.8234517127275467}, {"text": "accuracy", "start_pos": 124, "end_pos": 132, "type": "METRIC", "confidence": 0.9366520643234253}, {"text": "Acc.)", "start_pos": 134, "end_pos": 139, "type": "METRIC", "confidence": 0.8955530822277069}]}]}