{"title": [{"text": "A Direct Syntax-Driven Reordering Model for Phrase-Based Machine Translation", "labels": [], "entities": [{"text": "Phrase-Based Machine Translation", "start_pos": 44, "end_pos": 76, "type": "TASK", "confidence": 0.8061124483744303}]}], "abstractContent": [{"text": "This paper presents a direct word reordering model with novel syntax-based features for statistical machine translation.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 88, "end_pos": 119, "type": "TASK", "confidence": 0.7072377304236094}]}, {"text": "Reordering models address the problem of reordering source language into the word order of the target language.", "labels": [], "entities": []}, {"text": "IBM Models 3 through 5 have reordering components that use surface word information but very little context information to determine the traversal order of the source sentence.", "labels": [], "entities": []}, {"text": "Since the late 1990s, phrase-based machine translation solves much of the local reorderings by using phrasal translations.", "labels": [], "entities": [{"text": "phrase-based machine translation", "start_pos": 22, "end_pos": 54, "type": "TASK", "confidence": 0.6960318585236868}]}, {"text": "The problem of long-distance reordering has become a central research topic in modeling distortions.", "labels": [], "entities": []}, {"text": "We present a syntax driven maximum entropy reordering model that directly predicts the source traversal order and is able to model arbitrarily long distance word movement.", "labels": [], "entities": []}, {"text": "We show that this model significantly improves machine translation quality .", "labels": [], "entities": [{"text": "machine translation", "start_pos": 47, "end_pos": 66, "type": "TASK", "confidence": 0.8109526038169861}]}], "introductionContent": [{"text": "Machine translation reordering models model the problem of the word order when translating a source language into a target language.", "labels": [], "entities": [{"text": "Machine translation reordering", "start_pos": 0, "end_pos": 30, "type": "TASK", "confidence": 0.8391189972559611}]}, {"text": "For example in Spanish and Arabic, adjectives often come after the nouns they modify whereas in English modifying adjectives usually precede the nouns.", "labels": [], "entities": []}, {"text": "When translating Spanish or Arabic into English, the position of the adjectives need to be properly reordered to be placed before the nouns to make fluent English.", "labels": [], "entities": []}, {"text": "In this paper, we present a word reordering model that models the word reordering process in translation.", "labels": [], "entities": []}, {"text": "The paper is organized as follows.", "labels": [], "entities": []}, {"text": "\u00a72 outlines previous approaches to reordering.", "labels": [], "entities": [{"text": "reordering", "start_pos": 35, "end_pos": 45, "type": "TASK", "confidence": 0.9690999984741211}]}, {"text": "\u00a73 details our model and its training and decoding process.", "labels": [], "entities": []}, {"text": "\u00a74 discusses experiments to evaluate the model and \u00a75 presents machine translation results.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 63, "end_pos": 82, "type": "TASK", "confidence": 0.7429603040218353}]}, {"text": "\u00a76 is discussion and conclusion.", "labels": [], "entities": []}], "datasetContent": [{"text": "To evaluate how accurate the reordering model is, we first compute its prediction accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 82, "end_pos": 90, "type": "METRIC", "confidence": 0.8171796202659607}]}, {"text": "We choose the first 100 sentences from NIST MT03 as our test set for this evaluation.", "labels": [], "entities": [{"text": "NIST MT03", "start_pos": 39, "end_pos": 48, "type": "DATASET", "confidence": 0.8915102183818817}]}, {"text": "We manually word align them to the first set of reference using LDC annotation guidelines version 1.0 of April 2006.", "labels": [], "entities": [{"text": "LDC annotation guidelines version 1.0 of April 2006", "start_pos": 64, "end_pos": 115, "type": "DATASET", "confidence": 0.8911675438284874}]}, {"text": "An average of 73% of the training sentences contain unaligned source words and over 87% of the test sentences contain unaligned source words.", "labels": [], "entities": []}, {"text": "The unaligned source words are mostly function words.", "labels": [], "entities": []}, {"text": "Because the visit sequence of unaligned source words are determined not by truth but by heuristics, they pose a problem in evaluation.", "labels": [], "entities": []}, {"text": "We thus evaluate the model by measuring the accuracy of its decision conditioned on true history.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 44, "end_pos": 52, "type": "METRIC", "confidence": 0.9993351101875305}]}, {"text": "We measure performance on the model's top-N choices for N = 1,2, and 3.", "labels": [], "entities": []}, {"text": "The table also shows the accuracy of no reordering in the Monotone column.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 25, "end_pos": 33, "type": "METRIC", "confidence": 0.9997193217277527}]}, {"text": "Top-N Accuracy Monotone 1 80.56% 65.39% 2 90.66% -3 93.05% -.", "labels": [], "entities": [{"text": "Accuracy Monotone 1", "start_pos": 6, "end_pos": 25, "type": "METRIC", "confidence": 0.9043532808621725}]}, {"text": "Reordering model performance  We analyze 50 errors from the top-1 run.", "labels": [], "entities": []}, {"text": "The errors are categorized and shown in  'Lexical' errors are those that rise from lexical choice of source words.", "labels": [], "entities": []}, {"text": "For example, an \"ADVP VP\" structure would normally be visited monotonically.", "labels": [], "entities": []}, {"text": "However, in case of Chinese phrase 'so do', they should be swapped.", "labels": [], "entities": []}, {"text": "More than a third of the errors are of this nature.", "labels": [], "entities": []}, {"text": "Errors in the Refer-ence category are those that are marked wrong because of the particular English reference.", "labels": [], "entities": [{"text": "Errors", "start_pos": 0, "end_pos": 6, "type": "METRIC", "confidence": 0.960012674331665}]}, {"text": "The proposed reorderings are correct but they don't match the reference reorderings.", "labels": [], "entities": []}, {"text": "Another 30% of the errors are due to parsing errors.", "labels": [], "entities": [{"text": "errors", "start_pos": 19, "end_pos": 25, "type": "METRIC", "confidence": 0.9830921292304993}, {"text": "parsing", "start_pos": 37, "end_pos": 44, "type": "TASK", "confidence": 0.9774854183197021}]}, {"text": "The Model errors are due to two sources.", "labels": [], "entities": []}, {"text": "One is the depth problem mentioned above.", "labels": [], "entities": []}, {"text": "Local statistics for some very deep treelets overwhelm the global statistics and local jumps win over the long jumps in these cases.", "labels": [], "entities": []}, {"text": "Another problem is the data sparseness.", "labels": [], "entities": []}, {"text": "For example, the model has learned to reorder the 'PP VP' structure but there is not much data for 'PP ADVP VP'.", "labels": [], "entities": []}, {"text": "The model fails to jump over PP into ADVP.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1. We use the (Levy  and Manning 2003) parser on Chinese.  Data  #Sentences #Words  LDC2006E93 10,408  230,764  LDC2008E57 11,463  194,024  Table 1. Training Data", "labels": [], "entities": []}, {"text": " Table 3.  The table also shows the accuracy of no reorder- ing in the Monotone column.  Top-N  Accuracy Monotone  1  80.56%  65.39%  2  90.66%  - 3", "labels": [], "entities": [{"text": "accuracy", "start_pos": 36, "end_pos": 44, "type": "METRIC", "confidence": 0.9996771812438965}, {"text": "reorder- ing", "start_pos": 51, "end_pos": 63, "type": "METRIC", "confidence": 0.835050106048584}, {"text": "Accuracy", "start_pos": 96, "end_pos": 104, "type": "METRIC", "confidence": 0.8230372667312622}]}, {"text": " Table 3. Reordering model performance", "labels": [], "entities": [{"text": "Reordering", "start_pos": 10, "end_pos": 20, "type": "TASK", "confidence": 0.8373952507972717}]}, {"text": " Table 4.  Error Category  Percentage  Lexical  34%  Parse  30%  Model  20%  Reference  16%", "labels": [], "entities": [{"text": "Parse", "start_pos": 53, "end_pos": 58, "type": "METRIC", "confidence": 0.9815473556518555}]}, {"text": " Table  5. The first row keeps all the features. The Sub- tract column shows performance after subtracting  each feature while keeping all the other features.  The Add column shows performance of adding the  feature.  Using just first-child features gets  75.97%. Adding node-jump features moves the  accuracy to 78.40% and so on.  Features  #Features Sub- tract", "labels": [], "entities": [{"text": "accuracy", "start_pos": 301, "end_pos": 309, "type": "METRIC", "confidence": 0.9995284080505371}]}, {"text": " Table 6. Decoding with MaxEnt reorder  lattices shows significant improvement for all con- ditions.", "labels": [], "entities": []}]}