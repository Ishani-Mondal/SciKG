{"title": [], "abstractContent": [{"text": "We describe a class of translation model in which a set of input variants encoded as a context-free forest is translated using a finite-state translation model.", "labels": [], "entities": []}, {"text": "The forest structure of the input is well-suited to representing word order alternatives, making it straightforward to model translation as a two step process: (1) tree-based source reordering and (2) phrase transduction.", "labels": [], "entities": [{"text": "phrase transduction", "start_pos": 201, "end_pos": 220, "type": "TASK", "confidence": 0.7397553324699402}]}, {"text": "By treating the reordering process as a latent variable in a probabilistic translation model, we can learn a long-range source reordering model without example reordered sentences, which are problematic to construct.", "labels": [], "entities": []}, {"text": "The resulting model has state-of-the-art translation performance, uses linguistically motivated features to effectively model long range reordering, and is significantly smaller than a comparable hierarchical phrase-based translation model.", "labels": [], "entities": []}], "introductionContent": [{"text": "Translation models based on synchronous contextfree grammars (SCFGs) have become widespread in recent years ().", "labels": [], "entities": []}, {"text": "Compared to phrase-based models, which can be represented as finite-state transducers (FSTs,), one important benefit that SCFG models have is the ability to process long range reordering patterns in space and time that is polynomial in the length of the displacement, whereas an FST must generally explore a number of states that is exponential in this length.", "labels": [], "entities": []}, {"text": "As one would expect, for language pairs with substantial structural differences (and thus requiring long-range reordering during translation), SCFG models have come to outperform the best FST models (.", "labels": [], "entities": []}, {"text": "In this paper, we explore anew way to take advantage of the computational benefits of CFGs during translation.", "labels": [], "entities": []}, {"text": "Rather than using a single SCFG to both reorder and translate a source sentence into the target language, we break the translation process into a two step pipeline where (1) the source language is reordered into a target-like order, with alternatives encoded in a context-free forest, and (2) the reordered source is transduced into the target language using an FST that represents phrasal correspondences.", "labels": [], "entities": []}, {"text": "While multi-step decompositions of the translation problem have been proposed before (), they are less practical with the rise of SCFG models, since the context-free languages are not closed under intersection.", "labels": [], "entities": [{"text": "translation problem", "start_pos": 39, "end_pos": 58, "type": "TASK", "confidence": 0.9286012947559357}]}, {"text": "However, the CFLs are closed under intersection with regular languages.", "labels": [], "entities": []}, {"text": "By restricting ourselves to a finite-state phrase transducer and representing reorderings of the source in a context-free forest, exact inference over the composition of the two models is possible.", "labels": [], "entities": []}, {"text": "The paper proceeds as follows.", "labels": [], "entities": []}, {"text": "We first explore reordering forests and describe how to translate them with an FST ( \u00a72).", "labels": [], "entities": [{"text": "FST", "start_pos": 79, "end_pos": 82, "type": "METRIC", "confidence": 0.6567951440811157}]}, {"text": "Since we would like our reordering model to discriminate between good reorderings of the source and bad ones, we show how to train our reordering component as a latent variable in an end-to-end translation model ( \u00a73).", "labels": [], "entities": []}, {"text": "We then presents experimental results on language pairs requiring small amounts and large amounts of reordering ( \u00a74).", "labels": [], "entities": []}, {"text": "We conclude with a discussion of related work ( \u00a76) and possible extensions ( \u00a77).", "labels": [], "entities": []}], "datasetContent": [{"text": "We now turn to an experimental validation of the models we have introduced.", "labels": [], "entities": []}, {"text": "We define three conditions: a small data scenario consisting of a translation task based on the BTEC Chinese-English corpus (), a large data ChineseEnglish condition designed to be more comparable to conditions in a NIST MT evaluation, and a large data Arabic-English task.", "labels": [], "entities": [{"text": "BTEC Chinese-English corpus", "start_pos": 96, "end_pos": 123, "type": "DATASET", "confidence": 0.9420604507128397}, {"text": "NIST MT evaluation", "start_pos": 216, "end_pos": 234, "type": "TASK", "confidence": 0.5089295506477356}]}, {"text": "For each condition, phrase tables were extracted as described in with a maximum phrase size of 5.", "labels": [], "entities": []}, {"text": "The parallel training data was aligned using the Giza++ implementation of IBM Model 4 (.", "labels": [], "entities": []}, {"text": "The Chinese text was segmented using a CRF-based word segmenter ().", "labels": [], "entities": [{"text": "CRF-based word segmenter", "start_pos": 39, "end_pos": 63, "type": "TASK", "confidence": 0.6278952459494272}]}, {"text": "The Arabic text was segmented using the technique described in.", "labels": [], "entities": []}, {"text": "The Stanford parser was used to generate source parses for all conditions, and these were then used to generate the reordering forests as described in \u00a72.1.", "labels": [], "entities": []}, {"text": "summarizes statistics about the corpora used.", "labels": [], "entities": []}, {"text": "The reachability statistic indicates what percentage of sentence pairs in the training data could be regenerated using our reordering/translation model.", "labels": [], "entities": []}, {"text": "To train the reordering model, we used all of the reachable sentence pairs from BTEC, 20% of the reachable set in the Chinese-English condition, and all reachable sentence pairs under 40 words (source) in length in the Arabic-English condition.", "labels": [], "entities": [{"text": "BTEC", "start_pos": 80, "end_pos": 84, "type": "DATASET", "confidence": 0.9789422750473022}]}, {"text": "Error analysis indicates that a substantial portion of unreachable sentence pairs are due to alignment (word or sentence) or parse errors; however, in some cases the reordering forests did not contain an adequate source reordering to produce the necessary target.", "labels": [], "entities": []}, {"text": "For example, in Arabic, which is a VSO language, the treebank annotation is to place the subject NP as the 'middle child' between the V and the object constituent.", "labels": [], "entities": []}, {"text": "This can be reordered into an English SVO order using our child-permutation rules; however, if the source VP is modified by a modal particle, the parser makes the particle the parent of the VP, and it is no longer possible to move the subject to the first position in the sentence.", "labels": [], "entities": []}, {"text": "Richer reordering rules are needed to address this problem.", "labels": [], "entities": []}, {"text": "Other solutions to the reachability problem include targeting reachable oracles instead of the reference translation ( or making use of alternative training criteria, such as minimum risk training ().", "labels": [], "entities": []}, {"text": "We now consider how to apply this model to a translation task.", "labels": [], "entities": [{"text": "translation task", "start_pos": 45, "end_pos": 61, "type": "TASK", "confidence": 0.9312193989753723}]}, {"text": "The training we described in \u00a73.2 is suboptimal for state-of-the-art translation systems, since (1) it optimizes likelihood rather than an MT metric and (2) it does not include a language model.", "labels": [], "entities": []}, {"text": "We describe how we addressed these problems here, and then present our results in the three conditions defined above.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Translation results (BLEU)", "labels": [], "entities": [{"text": "Translation", "start_pos": 10, "end_pos": 21, "type": "TASK", "confidence": 0.9109115600585938}, {"text": "BLEU", "start_pos": 31, "end_pos": 35, "type": "METRIC", "confidence": 0.9985760450363159}]}]}