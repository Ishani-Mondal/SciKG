{"title": [{"text": "Evaluation Metrics for the Lexical Substitution Task", "labels": [], "entities": [{"text": "Lexical Substitution", "start_pos": 27, "end_pos": 47, "type": "TASK", "confidence": 0.8263008892536163}]}], "abstractContent": [{"text": "We identify some problems of the evaluation metrics used for the English Lexical Substitution Task of SemEval-2007, and propose alternative metrics that avoid these problems, which we hope will better guide the future development of lexical substitution systems.", "labels": [], "entities": [{"text": "English Lexical Substitution Task of SemEval-2007", "start_pos": 65, "end_pos": 114, "type": "TASK", "confidence": 0.6732809990644455}]}], "introductionContent": [{"text": "The English Lexical Substitution task at SemEval-2007 (here called ELS07) requires systems to find substitutes for target words in a given sentence.", "labels": [], "entities": [{"text": "English Lexical Substitution task at SemEval-2007", "start_pos": 4, "end_pos": 53, "type": "TASK", "confidence": 0.6946960687637329}]}, {"text": "For example, we might replace the target word match with game in the sentence they lost the match.", "labels": [], "entities": []}, {"text": "System outputs are evaluated against a set of candidate substitutes proposed by human subjects for test items.", "labels": [], "entities": []}, {"text": "Targets are typically sense ambiguous (e.g. match in the above example), and so task performance requires a combination of word sense disambiguation (by exploiting the given sentential context) and (near) synonym generation.", "labels": [], "entities": [{"text": "word sense disambiguation", "start_pos": 123, "end_pos": 148, "type": "TASK", "confidence": 0.7113110820452372}, {"text": "synonym generation", "start_pos": 205, "end_pos": 223, "type": "TASK", "confidence": 0.7357222735881805}]}, {"text": "In this paper, we discuss some problems of the evaluation metrics used in ELS07, and then propose some alternative measures that avoid these problems, and which we believe will better serve to guide the development of lexical substitution systems in future work.", "labels": [], "entities": [{"text": "ELS07", "start_pos": 74, "end_pos": 79, "type": "DATASET", "confidence": 0.8654510974884033}]}, {"text": "The subtasks within ELS07 divide into two groups, in terms of whether they focus on a system's 'best' answer fora test item, or address the broader set of answer candidates a system can produce.", "labels": [], "entities": []}, {"text": "In what follows, we address these two cases in separate sections, and then present some results for applying our new metrics for the second case.", "labels": [], "entities": []}, {"text": "We begin by briefly introducing the test materials that were created for the ELS07 evaluation.", "labels": [], "entities": [{"text": "ELS07 evaluation", "start_pos": 77, "end_pos": 93, "type": "DATASET", "confidence": 0.7235329747200012}]}], "datasetContent": [{"text": "Briefly stated, the ELS07 dataset comprises around 2000 sentences, providing 10 test sentences each for some 201 preselected target words, which were required to be sense ambiguous and have at least one synonym, and which include nouns, verbs, adjectives and adverbs.", "labels": [], "entities": [{"text": "ELS07 dataset", "start_pos": 20, "end_pos": 33, "type": "DATASET", "confidence": 0.9585983157157898}]}, {"text": "Five human annotators were asked to suggest up to three substitutes for the target word of each test sentence, and their collected suggestions serve as the gold standard against which system outputs are compared.", "labels": [], "entities": []}, {"text": "Around 300 sentences were distributed as development data, and the remainder retained for the final evaluation.", "labels": [], "entities": []}, {"text": "To assist defining our metrics, we formally describe this data as follows.", "labels": [], "entities": []}, {"text": "For each sentence ti in the test data (1 \u2264 i \u2264 N , N the number of test items), let H i denote the set of human proposed substitutes.", "labels": [], "entities": []}, {"text": "A key aspect of the data is the count of human annotators that proposed each candidate (since a term appears a stronger candidate if proposed by annotators).", "labels": [], "entities": []}, {"text": "For each ti , there is a function freq i which returns this count for each term within H i (and 0 for any other term), and a value maxfreq i corresponding to the maximal count for any term in H i . The pairing of H i and freq i in effect provides a multiset representation of the human answer set.", "labels": [], "entities": []}, {"text": "We use |S| i in what follows to denote the multiset cardinality of S according to freq i , i.e. \u03a3 a\u2208S freq i (a).", "labels": [], "entities": []}, {"text": "Some of the ELS07 metrics use a notion of mode answer mi , which exists only for test items that have a single most-frequent human response, i.e. a unique a \u2208 H i such that freq i (a) = maxfreq i . To adapt an example from M&N, an item with target word happy (adj) might have human answers {glad, merry, sunny, jovial, cheerful } with counts (3,3,2,1,1) respectively.", "labels": [], "entities": []}, {"text": "We will abbreviate this answer set as H i = {G:3,M:3,S:2,J:1,Ch:1} where it is used later in the paper.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: Coverage F-scores (macro-avgd), for simple boundary strategies (with penalty factor k = 1).", "labels": [], "entities": [{"text": "F-scores", "start_pos": 19, "end_pos": 27, "type": "METRIC", "confidence": 0.7794528007507324}]}, {"text": " Table 1: Out-of-ten recall scores for all the systems (with  a subdivision by pos of target item).", "labels": [], "entities": [{"text": "Out-of-ten", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9498562812805176}, {"text": "recall", "start_pos": 21, "end_pos": 27, "type": "METRIC", "confidence": 0.9283437728881836}]}, {"text": " Table 2: Optimal F-scores (macro-avgd) for coverage,  computed over the (oot) ranked outputs of the systems  (with penalty factor k = 1).", "labels": [], "entities": [{"text": "Optimal", "start_pos": 10, "end_pos": 17, "type": "METRIC", "confidence": 0.9073311686515808}, {"text": "F-scores", "start_pos": 18, "end_pos": 26, "type": "METRIC", "confidence": 0.4936651587486267}]}]}