{"title": [{"text": "Utility Evaluation of Cross-document Information Extraction \ud97b\udf59", "labels": [], "entities": [{"text": "Utility Evaluation", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.9145516157150269}, {"text": "Cross-document Information Extraction", "start_pos": 22, "end_pos": 59, "type": "TASK", "confidence": 0.6978212197621664}]}], "abstractContent": [{"text": "We describe a utility evaluation to determine whether cross-document information extraction (IE) techniques measurably improve user performance in news summary writing.", "labels": [], "entities": [{"text": "cross-document information extraction (IE)", "start_pos": 54, "end_pos": 96, "type": "TASK", "confidence": 0.7791427324215571}, {"text": "news summary writing", "start_pos": 147, "end_pos": 167, "type": "TASK", "confidence": 0.6319428781668345}]}, {"text": "Two groups of subjects were asked to perform the same time-restricted summary writing tasks, reading news under different conditions: with no IE results at all, with traditional single-document IE results, and with cross-document IE results.", "labels": [], "entities": []}, {"text": "Our results show that, in comparison to using source documents only, the quality of summary reports assembled using IE results, especially from cross-document IE, was significantly better and user satisfaction was higher.", "labels": [], "entities": []}, {"text": "We also compare the impact of different user groups on the results.", "labels": [], "entities": []}], "introductionContent": [{"text": "Information Extraction (IE) is a task of identifying \u0091'facts\u0092' (entities, relations and events) within unstructured documents, and converting them into structured representations (e.g., databases).", "labels": [], "entities": [{"text": "Information Extraction (IE) is a task of identifying \u0091'facts\u0092' (entities, relations and events) within unstructured documents, and converting them into structured representations (e.g., databases)", "start_pos": 0, "end_pos": 196, "type": "Description", "confidence": 0.8157964161464146}]}, {"text": "IE techniques have been effectively applied to different domains (e.g. daily news, Wikipedia, biomedical reports, financial analysis and legal documentations) and different languages.", "labels": [], "entities": [{"text": "IE", "start_pos": 0, "end_pos": 2, "type": "TASK", "confidence": 0.9760568141937256}]}, {"text": "Recently we described anew cross-document IE task ( ) to extract events across-documents and track them on a timeline.", "labels": [], "entities": [{"text": "cross-document IE", "start_pos": 27, "end_pos": 44, "type": "TASK", "confidence": 0.5888740420341492}]}, {"text": "Compared to traditional single-document IE, this new task can extract more salient, accurate and concise event information.", "labels": [], "entities": [{"text": "IE", "start_pos": 40, "end_pos": 42, "type": "TASK", "confidence": 0.8307048678398132}]}, {"text": "However, a significant question remains: will the events extracted by IE, especially this new cross-document IE task, actually help end-users to make better use of the large volumes of news?", "labels": [], "entities": []}, {"text": "In order to investigate whether we have reached this goal, we performed an extrinsic utility (i.e., usefulness) and usability evaluation on IE results.", "labels": [], "entities": [{"text": "IE", "start_pos": 140, "end_pos": 142, "type": "TASK", "confidence": 0.905170202255249}]}, {"text": "Two groups of subjects were asked to perform the same time-restricted summary writing tasks, reading news under different conditions: with no IE results at all, with traditional single-document IE results, and with cross-document IE results.", "labels": [], "entities": []}, {"text": "Our results show that, in comparison to using source documents only, the quality of summary reports assembled using IE techniques, especially from cross-document IE, was significantly better.", "labels": [], "entities": []}, {"text": "Also, as extraction quality increases from no IE at all to single-document IE and then to cross-document IE, user satisfaction increases.", "labels": [], "entities": []}, {"text": "We also compare the impact of different user groups on the results.", "labels": [], "entities": []}, {"text": "To the best of our knowledge, this is the first systematic evaluation of cross-document IE from a usability perspective.", "labels": [], "entities": [{"text": "IE", "start_pos": 88, "end_pos": 90, "type": "TASK", "confidence": 0.7332792282104492}]}], "datasetContent": [{"text": "We chose the first group of users with a \u0093\"Hallway Testing\u0094\" user-study method described in.", "labels": [], "entities": []}, {"text": "We randomly asked 11 PhD students in the field of natural language processing to conduct the evaluation.", "labels": [], "entities": [{"text": "natural language processing", "start_pos": 50, "end_pos": 77, "type": "TASK", "confidence": 0.6809977889060974}]}, {"text": "In order to evaluate these three levels independently, each student was asked to write at most one summary, using one of the three levels, for any single centroid entity.", "labels": [], "entities": []}, {"text": "To avoid the impact of diverse text comprehension abilities, each student was involved in all of these three levels for different centroid entities.", "labels": [], "entities": []}, {"text": "An effective utility evaluation will require users with a diversity of prior knowledge and computer experience.", "labels": [], "entities": []}, {"text": "Therefore we asked the second group of 11 users in a remote usability testing mode.", "labels": [], "entities": []}, {"text": "We sent out the request to university-wide undergraduate student mailing lists and found 11 users to work on the evaluation.", "labels": [], "entities": []}, {"text": "The evaluation procedure follows the Hallway Testing method, except that the tests are carried out in the user\u0092's own environment (rather than labs) helping further simulate real-life scenario testing.", "labels": [], "entities": []}, {"text": "Also the users didn\u0092't meet with the observers and thus they were not aware of any expectations for results.", "labels": [], "entities": []}, {"text": "In this section we will focus on reporting the results from Hallway Evaluation, while providing comparisons with Remote Evaluation.", "labels": [], "entities": [{"text": "Hallway Evaluation", "start_pos": 60, "end_pos": 78, "type": "DATASET", "confidence": 0.8928041756153107}]}], "tableCaptions": [{"text": " Table 1. # (uniquely correct sentences)/ #(redundant correct sentences)/  #(spurious sentences) in a summary in Hallway Evaluation", "labels": [], "entities": [{"text": "Hallway Evaluation", "start_pos": 113, "end_pos": 131, "type": "DATASET", "confidence": 0.972183495759964}]}, {"text": " Table 2  shows the evaluation results for the three different  methods.", "labels": [], "entities": []}]}