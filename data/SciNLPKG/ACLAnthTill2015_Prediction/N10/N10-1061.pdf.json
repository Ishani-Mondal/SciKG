{"title": [{"text": "Coreference Resolution in a Modular, Entity-Centered Model", "labels": [], "entities": [{"text": "Coreference Resolution", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.9293156862258911}]}], "abstractContent": [{"text": "Coreference resolution is governed by syntactic , semantic, and discourse constraints.", "labels": [], "entities": [{"text": "Coreference resolution", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.9241571128368378}]}, {"text": "We present a generative, model-based approach in which each of these factors is modularly encapsulated and learned in a primarily unsu-pervised manner.", "labels": [], "entities": []}, {"text": "Our semantic representation first hypothesizes an underlying set of latent entity types, which generate specific entities that in turn render individual mentions.", "labels": [], "entities": []}, {"text": "By sharing lexical statistics at the level of abstract entity types, our model is able to substantially reduce semantic compatibility errors, resulting in the best results to date on the complete end-to-end coreference task.", "labels": [], "entities": []}], "introductionContent": [{"text": "Coreference systems exploit a variety of information sources, ranging from syntactic and discourse constraints, which are highly configurational, to semantic constraints, which are highly contingent on lexical meaning and world knowledge.", "labels": [], "entities": []}, {"text": "Perhaps because configurational features are inherently easier to learn from small data sets, past work has often emphasized them over semantic knowledge.", "labels": [], "entities": []}, {"text": "Of course, all state-of-the-art coreference systems have needed to capture semantic compatibility to some degree.", "labels": [], "entities": []}, {"text": "As an example of nominal headword compatibility, a \"president\" can be a \"leader\" but cannot be not an \"increase.\"", "labels": [], "entities": []}, {"text": "Past systems have often computed the compatibility of specific headword pairs, extracted either from lexical resources, web statistics (), or surface syntactic patterns.", "labels": [], "entities": []}, {"text": "While the pairwise approach has high precision, it is neither realistic nor scalable to explicitly enumerate all pairs of compatible word pairs.", "labels": [], "entities": [{"text": "precision", "start_pos": 37, "end_pos": 46, "type": "METRIC", "confidence": 0.9970945119857788}]}, {"text": "A more compact approach has been to rely on named-entity recognition (NER) systems to give coarse-grained entity types for each mention).", "labels": [], "entities": [{"text": "named-entity recognition (NER)", "start_pos": 44, "end_pos": 74, "type": "TASK", "confidence": 0.8391764640808106}]}, {"text": "Unfortunately, current systems use small inventories of types and so provide little constraint.", "labels": [], "entities": []}, {"text": "In general, coreference errors in state-of-theart systems are frequently due to poor models of semantic compatibility.", "labels": [], "entities": []}, {"text": "In this work, we take a primarily unsupervised approach to coreference resolution, broadly similar to, which addresses this issue.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 59, "end_pos": 81, "type": "TASK", "confidence": 0.9677508473396301}]}, {"text": "Our generative model exploits a large inventory of distributional entity types, including standard NER types like PERSON and ORG, as well as more refined types like WEAPON and VEHICLE.", "labels": [], "entities": [{"text": "VEHICLE", "start_pos": 176, "end_pos": 183, "type": "DATASET", "confidence": 0.797741174697876}]}, {"text": "For each type, distributions over typical heads, modifiers, and governors are learned from large amounts of unlabeled data, capturing type-level semantic information (e.g. \"spokesman\" is a likely head fora PER-SON).", "labels": [], "entities": []}, {"text": "Each entity inherits from a type but captures entity-level semantic information (e.g. \"giant\" maybe a likely head for the Microsoft entity but not all ORGs).", "labels": [], "entities": []}, {"text": "Separately from the type-entity semantic module, a log-linear discourse model captures configurational effects.", "labels": [], "entities": []}, {"text": "Finally, a mention model assembles each textual mention by selecting semantically appropriate words from the entities and types.", "labels": [], "entities": []}, {"text": "Despite being almost entirely unsupervised, our model yields the best reported end-to-end results on a range of standard coreference data sets.", "labels": [], "entities": []}], "datasetContent": [{"text": "We considered the challenging end-to-end system mention setting, wherein addition to predicting mention partitions, a system must identify the mentions themselves and their boundaries automatically.", "labels": [], "entities": []}, {"text": "Our system deterministically extracts mention boundaries from parse trees (Section 5.2).", "labels": [], "entities": []}, {"text": "We utilized no coreference annotation during training, but did use minimal prototype information to prime the learning of entity types (Section 5.3).", "labels": [], "entities": []}, {"text": "For evaluation, we used standard coreference data sets derived from the ACE corpora: \u2022 A04CU: Train/dev/test split of the newswire portion of the ACE 2004 training set 7 utilized in, and For all experiments, we evaluated on the dev and test sets above.", "labels": [], "entities": [{"text": "ACE corpora", "start_pos": 72, "end_pos": 83, "type": "DATASET", "confidence": 0.9611870646476746}, {"text": "A04CU", "start_pos": 87, "end_pos": 92, "type": "METRIC", "confidence": 0.9401092529296875}, {"text": "ACE 2004 training set 7", "start_pos": 146, "end_pos": 169, "type": "DATASET", "confidence": 0.9103292942047119}]}, {"text": "To train, we included the text of all documents above, though of course not looking at either their mention boundaries or reference annotations in anyway.", "labels": [], "entities": []}, {"text": "We also trained on the following much larger unlabeled datasets utilized in Haghighi and Klein (2009): \u2022 BLLIP: 5k articles of newswire parsed with the Charniak (2000) parser.", "labels": [], "entities": [{"text": "BLLIP", "start_pos": 105, "end_pos": 110, "type": "METRIC", "confidence": 0.9965734481811523}]}, {"text": "\u2022 WIKI: 8k abstracts of English Wikipedia articles parsed by the Berkeley parser ().", "labels": [], "entities": [{"text": "WIKI", "start_pos": 2, "end_pos": 6, "type": "DATASET", "confidence": 0.8659178018569946}]}, {"text": "Articles were selected to have subjects amongst the frequent proper nouns in the evaluation datasets.", "labels": [], "entities": []}, {"text": "We evaluated on multiple coreference resolution metrics, as no single one is clearly superior, partic-ularly in dealing with the system mention setting.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 25, "end_pos": 47, "type": "TASK", "confidence": 0.8451636731624603}]}, {"text": "We utilized MUC (, B 3 All (, B 3 N one (, and Pairwise F1.", "labels": [], "entities": [{"text": "MUC", "start_pos": 12, "end_pos": 15, "type": "DATASET", "confidence": 0.6346703171730042}, {"text": "Pairwise F1", "start_pos": 47, "end_pos": 58, "type": "METRIC", "confidence": 0.8074844777584076}]}, {"text": "The B 3 All and B 3 N one are B 3 variants () that differ in their treatment of spurious mentions.", "labels": [], "entities": []}, {"text": "For Pairwise F1, precision measures how often pairs of predicted coreferent mentions are in the same annotated entity.", "labels": [], "entities": [{"text": "Pairwise F1", "start_pos": 4, "end_pos": 15, "type": "TASK", "confidence": 0.5377083122730255}, {"text": "precision", "start_pos": 17, "end_pos": 26, "type": "METRIC", "confidence": 0.9996111989021301}]}, {"text": "We eliminated any mention pair from this calculation where both mentions were spurious.", "labels": [], "entities": []}, {"text": "numbers represent their highestperforming cluster ranking model.", "labels": [], "entities": []}, {"text": "We also compared to the strong deterministic system of.", "labels": [], "entities": []}, {"text": "Across all data sets, our model, despite being largely unsupervised, consistently outperforms these systems, which are the best previously reported results on end-to-end coreference resolution (i.e. including mention detection).", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 170, "end_pos": 192, "type": "TASK", "confidence": 0.866388589143753}, {"text": "mention detection", "start_pos": 209, "end_pos": 226, "type": "TASK", "confidence": 0.6835653930902481}]}, {"text": "Performance on the A05RA dataset is generally lower because it includes articles from blogs and web forums where parser quality is significantly degraded.", "labels": [], "entities": [{"text": "A05RA dataset", "start_pos": 19, "end_pos": 32, "type": "DATASET", "confidence": 0.9620549380779266}]}], "tableCaptions": [{"text": " Table 1: Experimental results with system mentions. All systems except Haghighi and Klein (2009) and current work  are fully supervised. The current work outperforms all other systems, supervised or unsupervised. For comparison pur- poses, the B 3 N one variant used on A05RA is calculated slightly differently than other B 3 N one results; see Rahman  and Ng (2009).", "labels": [], "entities": [{"text": "A05RA", "start_pos": 271, "end_pos": 276, "type": "DATASET", "confidence": 0.9297739863395691}]}]}