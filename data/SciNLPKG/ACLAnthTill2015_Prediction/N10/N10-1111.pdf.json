{"title": [{"text": "Constraint-Driven Rank-Based Learning for Information Extraction", "labels": [], "entities": [{"text": "Information Extraction", "start_pos": 42, "end_pos": 64, "type": "TASK", "confidence": 0.730804517865181}]}], "abstractContent": [{"text": "Most learning algorithms for undirected graphical models require complete inference over at least one instance before parameter updates can be made.", "labels": [], "entities": []}, {"text": "SampleRank is a rank-based learning framework that alleviates this problem by updating the parameters during inference.", "labels": [], "entities": []}, {"text": "Most semi-supervised learning algorithms also perform full inference on at least one instance before each parameter update.", "labels": [], "entities": []}, {"text": "We extend SampleRank to semi-supervised learning in order to circumvent this computational bottleneck.", "labels": [], "entities": []}, {"text": "Different approaches to incorporate unlabeled data and prior knowledge into this framework are explored.", "labels": [], "entities": []}, {"text": "When evaluated on a standard information extraction dataset, our method significantly outperforms the supervised method, and matches results of a competing state-of-the-art semi-supervised learning approach.", "labels": [], "entities": []}], "introductionContent": [{"text": "Most supervised learning algorithms for undirected graphical models require full inference over the dataset (e.g., gradient descent), small subsets of the dataset (e.g., stochastic gradient descent), or at least a single instance (e.g., perceptron,) before parameter updates are made.", "labels": [], "entities": []}, {"text": "Often this is the main computational bottleneck during training.", "labels": [], "entities": []}, {"text": "SampleRank) is a rank-based learning framework that alleviates this problem by performing parameter updates within inference.", "labels": [], "entities": []}, {"text": "Every pair of samples generated during inference is ranked according to the model and the ground truth, and the parameters are updated when the rankings disagree.", "labels": [], "entities": []}, {"text": "SampleRank has enabled efficient learning for massive information extraction tasks ().", "labels": [], "entities": [{"text": "information extraction tasks", "start_pos": 54, "end_pos": 82, "type": "TASK", "confidence": 0.7896302342414856}]}, {"text": "The problem of requiring a complete inference iteration before parameters are updated also exists in the semi-supervised learning scenario.", "labels": [], "entities": []}, {"text": "Here the situation is often considerably worse since inference has to be applied to potentially very large unlabeled datasets.", "labels": [], "entities": []}, {"text": "Most semi-supervised learning algorithms rely on marginals or MAP assignments (CODL,.", "labels": [], "entities": []}, {"text": "Calculating these is computationally inexpensive for many simple tasks (such as classification and regression).", "labels": [], "entities": [{"text": "classification", "start_pos": 80, "end_pos": 94, "type": "TASK", "confidence": 0.966590404510498}]}, {"text": "However, marginal and MAP inference tends to be expensive for complex structured prediction models (such as the joint information extraction models of ), making semisupervised learning intractable.", "labels": [], "entities": [{"text": "information extraction", "start_pos": 118, "end_pos": 140, "type": "TASK", "confidence": 0.7220755815505981}]}, {"text": "In this work we employ a fast rank-based learning algorithm for semi-supervised learning to circumvent the inference bottleneck.", "labels": [], "entities": []}, {"text": "The ranking function is extended to capture both the preference expressed by the labeled data, and the preference of the domain expert when the labels are not available.", "labels": [], "entities": []}, {"text": "This allows us to perform SampleRank as is, without sacrificing its scalability, which is crucial for future large scale applications of semi-supervised learning.", "labels": [], "entities": []}, {"text": "We applied our method to a standard information extraction dataset used for semi-supervised learning.", "labels": [], "entities": [{"text": "information extraction", "start_pos": 36, "end_pos": 58, "type": "TASK", "confidence": 0.7217013835906982}]}, {"text": "Empirically we demonstrate improvements over the supervised model, and closely match the results of a competing state-of-the-art semi-supervised learner.", "labels": [], "entities": []}], "datasetContent": [{"text": "We carried out experiments on the Cora citation dataset.", "labels": [], "entities": [{"text": "Cora citation dataset", "start_pos": 34, "end_pos": 55, "type": "DATASET", "confidence": 0.7221479217211405}]}, {"text": "The task is to segment each citation into different fields, such as \"author\" and \"title\".", "labels": [], "entities": []}, {"text": "We use 300 instances as training data, 100 instances as development data, and 100 instances as test data.", "labels": [], "entities": []}, {"text": "Some instances from the training data are selected as labeled instances, and the remaining data (including development) as unlabeled.", "labels": [], "entities": []}, {"text": "We use the same tokenlabel constraints as.", "labels": [], "entities": []}, {"text": "We use the objective functions defined in Section 3, specifically self-training (Self:F s ), direct constraints (Cons:F c ), the combination of the two (Self+Cons:F sc ), and combination of the model score and the constraints (Model+Cons:F mc ).", "labels": [], "entities": []}, {"text": "We set pi = 1.0, \u03b1 = 1.0, \u03bb s = 10, and \u03bb m = 0.0001.", "labels": [], "entities": []}, {"text": "Average token accuracy for 5 runs is reported and compared with CODL 1 in.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 14, "end_pos": 22, "type": "METRIC", "confidence": 0.9634146094322205}, {"text": "CODL", "start_pos": 64, "end_pos": 68, "type": "METRIC", "confidence": 0.9436004757881165}]}, {"text": "We also report supervised results from ( and SampleRank.", "labels": [], "entities": []}, {"text": "All of our methods show vast improvement over the supervised method for smaller training sizes, but this difference decreases as the training size increases.", "labels": [], "entities": []}, {"text": "When the complete training data is used, additional unlabeled data hurts our performance.", "labels": [], "entities": []}, {"text": "This is not observed in CODL since they use more unlabeled data, which may also explain their slightly higher accuracy.", "labels": [], "entities": [{"text": "CODL", "start_pos": 24, "end_pos": 28, "type": "DATASET", "confidence": 0.8027986288070679}, {"text": "accuracy", "start_pos": 110, "end_pos": 118, "type": "METRIC", "confidence": 0.9975812435150146}]}, {"text": "Note that Self+Cons performs better than Self or Cons individually.", "labels": [], "entities": []}, {"text": "Model+Cons also performs competitively, and may potentially outperform other methods if a better \u03bb m is chosen.", "labels": [], "entities": []}, {"text": "Note, however, that \u03bb m is much harder to tune than \u03bb s since \u03bb m weighs the contribution of the unnormalized model score, the range of which depends on many different factors such as properties of the data, the learning rate, number of samples, proposal function, etc.", "labels": [], "entities": []}, {"text": "For self+cons (\u03bb s ), the ranges of the predictions and constraint penalties are fixed and known, making the task simpler.", "labels": [], "entities": []}, {"text": "Self training takes 90 minutes to run on average, while Self+Cons and Model+Cons need 100 minutes.", "labels": [], "entities": []}, {"text": "Since the Cons method skips the inference step over unlabeled data, it takes only 30 minutes to run.", "labels": [], "entities": []}, {"text": "As the size of the model and unlabeled data set grows, this saving will become more significant.", "labels": [], "entities": []}, {"text": "Running time of CODL was not reported.", "labels": [], "entities": [{"text": "Running time", "start_pos": 0, "end_pos": 12, "type": "METRIC", "confidence": 0.9219200313091278}, {"text": "CODL", "start_pos": 16, "end_pos": 20, "type": "METRIC", "confidence": 0.4429120421409607}]}], "tableCaptions": [{"text": " Table 1: Tokenwise Accuracy: for different methods as we vary the size of the labeled data", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 20, "end_pos": 28, "type": "METRIC", "confidence": 0.9014796614646912}]}]}