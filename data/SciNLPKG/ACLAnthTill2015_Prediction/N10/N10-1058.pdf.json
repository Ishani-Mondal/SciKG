{"title": [{"text": "Improving Semantic Role Classification with Selectional Preferences", "labels": [], "entities": [{"text": "Improving Semantic Role Classification", "start_pos": 0, "end_pos": 38, "type": "TASK", "confidence": 0.8877274245023727}]}], "abstractContent": [{"text": "This work incorporates Selectional Preferences (SP) into a Semantic Role (SR) Classification system.", "labels": [], "entities": [{"text": "Selectional Preferences (SP)", "start_pos": 23, "end_pos": 51, "type": "TASK", "confidence": 0.5653715431690216}, {"text": "Semantic Role (SR) Classification", "start_pos": 59, "end_pos": 92, "type": "TASK", "confidence": 0.6727118939161301}]}, {"text": "We learn separate selec-tional preferences for noun phrases and prepo-sitional phrases and we integrate them in a state-of-the-art SR classification system both in the form of features and individual class predictors.", "labels": [], "entities": [{"text": "SR classification", "start_pos": 131, "end_pos": 148, "type": "TASK", "confidence": 0.9288881421089172}]}, {"text": "We show that the inclusion of the refined SPs yields statistically significant improvements on both in domain and out of domain data (14.07% and 11.67% error reduction , respectively).", "labels": [], "entities": [{"text": "error reduction", "start_pos": 152, "end_pos": 167, "type": "METRIC", "confidence": 0.9610409140586853}]}, {"text": "The key factor for success is the combination of several SP methods with the original classification model using meta-classification.", "labels": [], "entities": []}], "introductionContent": [{"text": "Semantic Role Labeling (SRL) is the process of extracting simple event structures, i.e., \"who\" did \"what\" to \"whom\", \"when\" and \"where\".", "labels": [], "entities": [{"text": "Semantic Role Labeling (SRL)", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.8380144337813059}]}, {"text": "Current systems usually perform SRL in two pipelined steps: argument identification and argument classification.", "labels": [], "entities": [{"text": "SRL", "start_pos": 32, "end_pos": 35, "type": "TASK", "confidence": 0.9922462105751038}, {"text": "argument identification", "start_pos": 60, "end_pos": 83, "type": "TASK", "confidence": 0.7256688177585602}, {"text": "argument classification", "start_pos": 88, "end_pos": 111, "type": "TASK", "confidence": 0.7247752100229263}]}, {"text": "While identification is mostly syntactic, classification requires semantic knowledge to betaken into account.", "labels": [], "entities": [{"text": "identification", "start_pos": 6, "end_pos": 20, "type": "TASK", "confidence": 0.9669013619422913}]}, {"text": "Semantic information is usually captured through lexicalized features on the predicate and the head-word of the argument to be classified.", "labels": [], "entities": []}, {"text": "Since lexical features tend to be sparse, SRL systems are prone to overfit the training data and generalize poorly to new corpora.", "labels": [], "entities": [{"text": "SRL", "start_pos": 42, "end_pos": 45, "type": "TASK", "confidence": 0.9802112579345703}]}, {"text": "Indeed, the SRL evaluation exercises at) observed that all systems showed a significant performance degradation (\u223c10 F 1 points) when applied to test data from a different genre of that of the training set.", "labels": [], "entities": [{"text": "SRL evaluation", "start_pos": 12, "end_pos": 26, "type": "TASK", "confidence": 0.7766455709934235}]}, {"text": "showed that this performance degradation is essentially caused by the argument classification subtask, and suggested the lexical data sparseness as one of the main reasons.", "labels": [], "entities": []}, {"text": "The same authors studied the contribution of the different feature types in SRL and concluded that the lexical features were the most salient features in argument classification (.", "labels": [], "entities": [{"text": "SRL", "start_pos": 76, "end_pos": 79, "type": "TASK", "confidence": 0.9564942717552185}, {"text": "argument classification", "start_pos": 154, "end_pos": 177, "type": "TASK", "confidence": 0.7785775065422058}]}, {"text": "In recent work, we showed () how automatically generated selectional preferences (SP) for verbs were able to perform better than pure lexical features in a role classification experiment, disconnected from a full-fledged SRL system.", "labels": [], "entities": [{"text": "role classification", "start_pos": 156, "end_pos": 175, "type": "TASK", "confidence": 0.7467046678066254}]}, {"text": "SPs introduce semantic generalizations on the type of arguments preferred by the predicates and, thus, they are expected to improve results on infrequent and unknown words.", "labels": [], "entities": []}, {"text": "The positive effect was especially relevant for out-of-domain data.", "labels": [], "entities": []}, {"text": "In this paper we advance () in two directions: (1) We learn separate SPs for prepositions and verbs, showing improvement over using SPs for verbs alone.", "labels": [], "entities": []}, {"text": "(2) We integrate the information of several SP models in a state-of-the-art SRL system (SwiRL 1 ) and show significant improvements in SR classification.", "labels": [], "entities": [{"text": "SR classification", "start_pos": 135, "end_pos": 152, "type": "TASK", "confidence": 0.9775071740150452}]}, {"text": "The key for the improvement lies in a metaclassifier, trained to select among the predictions provided by several role classification models.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section we evaluate the use of SPs for classification in isolation, i.e., we use formula 1, and no other information.", "labels": [], "entities": []}, {"text": "In addition we contrast the use of both verb-role and preposition-role SPs, as compared to the use of verb-role SPs alone.", "labels": [], "entities": []}, {"text": "The dataset used in these experiments (and in Section 4) is the same as provided by the CoNLL-2005 shared task on SRL ().", "labels": [], "entities": [{"text": "CoNLL-2005 shared task on SRL", "start_pos": 88, "end_pos": 117, "type": "DATASET", "confidence": 0.8156047344207764}]}, {"text": "This dataset comprises several sections of the PropBank corpus (news from the WSJ) as well as an extract of the Brown Corpus.", "labels": [], "entities": [{"text": "PropBank corpus (news from the WSJ)", "start_pos": 47, "end_pos": 82, "type": "DATASET", "confidence": 0.9212761968374252}, {"text": "Brown Corpus", "start_pos": 112, "end_pos": 124, "type": "DATASET", "confidence": 0.9910690486431122}]}, {"text": "Sections 02-21 are used for generating the SPs and training, Section 00 for development, and Section 23 for testing, as customary.", "labels": [], "entities": [{"text": "SPs", "start_pos": 43, "end_pos": 46, "type": "TASK", "confidence": 0.9217190146446228}]}, {"text": "The Brown Corpus is used for out-of-domain testing, but due to the limited size of the provided section, we extended it with instances from SemLink . Since the focus of this work is on argument are comparable to those we reported in ().", "labels": [], "entities": [{"text": "Brown Corpus", "start_pos": 4, "end_pos": 16, "type": "DATASET", "confidence": 0.9590083956718445}]}, {"text": "The differences are due to the fact that we do not discard roles like MOD, DIS, NEG and that our previous work used only the subset of the data that could be mapped to VerbNet (around 50%).", "labels": [], "entities": [{"text": "VerbNet", "start_pos": 168, "end_pos": 175, "type": "DATASET", "confidence": 0.9141185283660889}]}, {"text": "All in all, the table shows that splitting SPs into verb and preposition SPs yields better results, both in precision and recall, improving F 1 up to 10 points in some cases.", "labels": [], "entities": [{"text": "SPs into verb and preposition SPs", "start_pos": 43, "end_pos": 76, "type": "TASK", "confidence": 0.6975709696610769}, {"text": "precision", "start_pos": 108, "end_pos": 117, "type": "METRIC", "confidence": 0.9996023774147034}, {"text": "recall", "start_pos": 122, "end_pos": 128, "type": "METRIC", "confidence": 0.9980378746986389}, {"text": "F 1", "start_pos": 140, "end_pos": 143, "type": "METRIC", "confidence": 0.9936551451683044}]}], "tableCaptions": [{"text": " Table 1: Results for SPs in isolation, left for verb SPs, and right both preposition and verb SPs.", "labels": [], "entities": []}, {"text": " Table 3: Classification accuracy for the combination ap- proaches. +SP x stands for SwiRL plus each SP model.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 25, "end_pos": 33, "type": "METRIC", "confidence": 0.9852336049079895}]}]}