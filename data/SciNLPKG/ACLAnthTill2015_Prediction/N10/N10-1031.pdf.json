{"title": [{"text": "Extending the METEOR Machine Translation Evaluation Metric to the Phrase Level", "labels": [], "entities": [{"text": "METEOR Machine Translation Evaluation", "start_pos": 14, "end_pos": 51, "type": "TASK", "confidence": 0.6962094679474831}]}], "abstractContent": [{"text": "This paper presents METEOR-NEXT, an extended version of the METEOR metric designed to have high correlation with post-editing measures of machine translation quality.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 138, "end_pos": 157, "type": "TASK", "confidence": 0.7201761603355408}]}, {"text": "We describe changes made to the met-ric's sentence aligner and scoring scheme as well as a method for tuning the metric's parameters to optimize correlation with human-targeted Translation Edit Rate (HTER).", "labels": [], "entities": [{"text": "human-targeted Translation Edit Rate (HTER)", "start_pos": 162, "end_pos": 205, "type": "METRIC", "confidence": 0.757646769285202}]}, {"text": "We then show that METEOR-NEXT improves correlation with HTER over baseline metrics, including earlier versions of METEOR, and approaches the correlation level of a state-of-the-art metric, TER-plus (TERp).", "labels": [], "entities": [{"text": "correlation", "start_pos": 39, "end_pos": 50, "type": "METRIC", "confidence": 0.9601914286613464}, {"text": "TER-plus (TERp)", "start_pos": 189, "end_pos": 204, "type": "METRIC", "confidence": 0.8800046592950821}]}], "introductionContent": [{"text": "Recent focus on the need for accurate automatic metrics for evaluating the quality of machine translation output has spurred much development in the field of MT.", "labels": [], "entities": [{"text": "machine translation output", "start_pos": 86, "end_pos": 112, "type": "TASK", "confidence": 0.794588029384613}, {"text": "MT", "start_pos": 158, "end_pos": 160, "type": "TASK", "confidence": 0.9954675436019897}]}, {"text": "Workshops such as WMT09) and the MetricsMATR08 challenge () encourage the development of new MT metrics and reliable human judgment tasks.", "labels": [], "entities": [{"text": "WMT09", "start_pos": 18, "end_pos": 23, "type": "DATASET", "confidence": 0.9153671860694885}, {"text": "MT", "start_pos": 93, "end_pos": 95, "type": "TASK", "confidence": 0.9886839985847473}]}, {"text": "This paper describes our work extending the ME-TEOR metric to improve correlation with humantargeted Translation Edit Rate (HTER)), a semi-automatic post-editing based metric which measures the distance between MT output and a targeted reference.", "labels": [], "entities": [{"text": "ME-TEOR metric", "start_pos": 44, "end_pos": 58, "type": "METRIC", "confidence": 0.9053019285202026}, {"text": "humantargeted Translation Edit Rate (HTER))", "start_pos": 87, "end_pos": 130, "type": "METRIC", "confidence": 0.800460866519383}]}, {"text": "We identify several limitations of the original METEOR metric and describe our modifications to improve performance on this task.", "labels": [], "entities": [{"text": "METEOR", "start_pos": 48, "end_pos": 54, "type": "METRIC", "confidence": 0.8767305612564087}]}, {"text": "Our extended metric, METEOR-NEXT, is then tuned to maximize segment-level correlation with HTER scores and tested against several baseline metrics.", "labels": [], "entities": [{"text": "METEOR-NEXT", "start_pos": 21, "end_pos": 32, "type": "METRIC", "confidence": 0.9767928123474121}]}, {"text": "We show that METEOR-NEXT outperforms earlier versions of METEOR when tuned to the same HTER data and approaches the performance of a state-of-the-art TER-based metric, TER-plus.", "labels": [], "entities": [{"text": "METEOR-NEXT", "start_pos": 13, "end_pos": 24, "type": "METRIC", "confidence": 0.6834650039672852}, {"text": "HTER data", "start_pos": 87, "end_pos": 96, "type": "DATASET", "confidence": 0.7792414724826813}]}], "datasetContent": [{"text": "The GALE () Phase 3 unsequestered data includes HTER scores for Arabic-to-English MT output.", "labels": [], "entities": [{"text": "GALE () Phase 3 unsequestered data", "start_pos": 4, "end_pos": 38, "type": "DATASET", "confidence": 0.7054432779550552}, {"text": "HTER", "start_pos": 48, "end_pos": 52, "type": "METRIC", "confidence": 0.9270055890083313}, {"text": "MT output", "start_pos": 82, "end_pos": 91, "type": "TASK", "confidence": 0.8780703842639923}]}, {"text": "We created a test set from HTER scores of 2245 segments from 195 documents in this data set.", "labels": [], "entities": []}, {"text": "Our evaluation metric (METEOR-NEXT-hter) was tested against the following established metrics: BLEU () with a maximum Ngram length of 4, TER), versions of METEOR based on release 0.7 tuned for adequacy and fluency (METEOR-0.7-af), ranking (METEOR-0.7-rank), and HTER (METEOR-0.7-hter).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 95, "end_pos": 99, "type": "METRIC", "confidence": 0.998772919178009}, {"text": "TER", "start_pos": 137, "end_pos": 140, "type": "METRIC", "confidence": 0.9883537292480469}, {"text": "HTER", "start_pos": 262, "end_pos": 266, "type": "METRIC", "confidence": 0.9971231818199158}]}, {"text": "Also included is the HTER-tuned version of TER-plus (TERp-hter), a metric with state-of-the-art performance in recent evaluations (.", "labels": [], "entities": [{"text": "HTER-tuned", "start_pos": 21, "end_pos": 31, "type": "METRIC", "confidence": 0.860032320022583}, {"text": "TER-plus (TERp-hter)", "start_pos": 43, "end_pos": 63, "type": "METRIC", "confidence": 0.8278014063835144}]}, {"text": "Length-weighted Pearson's and Spearman's correlation are shown for all metrics at both the segment and document level.", "labels": [], "entities": [{"text": "Length-weighted Pearson's", "start_pos": 0, "end_pos": 25, "type": "METRIC", "confidence": 0.8904756903648376}, {"text": "Spearman's correlation", "start_pos": 30, "end_pos": 52, "type": "METRIC", "confidence": 0.5392347077528635}]}, {"text": "System level correlations are not shown as the Phase 3 data only contained the output of 2 systems.", "labels": [], "entities": []}, {"text": "METEOR-NEXT-hter outperforms all baseline metrics at both the segment and document level.", "labels": [], "entities": [{"text": "METEOR-NEXT-hter", "start_pos": 0, "end_pos": 16, "type": "METRIC", "confidence": 0.8164827823638916}]}, {"text": "Bootstrap sampling indicates that the segment-level correlation improvements of 0.026 in Pearson's rand 0.019 in Spearman's \u03c1 over METEOR-0.7-hter are statistically significant at the 95% level.", "labels": [], "entities": [{"text": "Pearson's rand 0.019", "start_pos": 89, "end_pos": 109, "type": "METRIC", "confidence": 0.763311117887497}, {"text": "METEOR-0.7-hter", "start_pos": 131, "end_pos": 146, "type": "METRIC", "confidence": 0.9528161883354187}]}, {"text": "TERp's correlation with HTER is still significantly higher across all categories.", "labels": [], "entities": [{"text": "TERp", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.9622920751571655}, {"text": "HTER", "start_pos": 24, "end_pos": 28, "type": "METRIC", "confidence": 0.4503091871738434}]}, {"text": "Our metric does run significantly faster than TERp, scoring approximately 120 segments per second to TERp's 3.8.", "labels": [], "entities": [{"text": "TERp", "start_pos": 46, "end_pos": 50, "type": "METRIC", "confidence": 0.6786183714866638}, {"text": "TERp", "start_pos": 101, "end_pos": 105, "type": "METRIC", "confidence": 0.4802573025226593}]}], "tableCaptions": [{"text": " Table 1: Parameter values for various METEOR tasks for  translations into English.", "labels": [], "entities": [{"text": "Parameter", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9436642527580261}, {"text": "METEOR", "start_pos": 39, "end_pos": 45, "type": "METRIC", "confidence": 0.5183945298194885}, {"text": "translations into English", "start_pos": 57, "end_pos": 82, "type": "TASK", "confidence": 0.8657750686009725}]}, {"text": " Table 2: Segment level correlation with HTER.", "labels": [], "entities": [{"text": "HTER", "start_pos": 41, "end_pos": 45, "type": "DATASET", "confidence": 0.5133607387542725}]}, {"text": " Table 3: Document level correlation with HTER.", "labels": [], "entities": [{"text": "HTER", "start_pos": 42, "end_pos": 46, "type": "DATASET", "confidence": 0.6250379681587219}]}]}