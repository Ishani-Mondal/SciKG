{"title": [{"text": "A Hybrid Morphologically Decomposed Factored Language Models for Arabic LVCSR", "labels": [], "entities": []}], "abstractContent": [{"text": "In this work, we try a hybrid methodology for language modeling where both morphological decomposition and factored language model-ing (FLM) are exploited to deal with the complex morphology of Arabic language.", "labels": [], "entities": [{"text": "language modeling", "start_pos": 46, "end_pos": 63, "type": "TASK", "confidence": 0.7270776480436325}]}, {"text": "At the end, we are able to obtain from 3.5% to 7.0% relative reduction in word error rate (WER) with respect to a traditional full-words system , and from 1.0% to 2.0% relative WER reduction with respect to a non-factored decomposed system.", "labels": [], "entities": [{"text": "relative reduction in word error rate (WER)", "start_pos": 52, "end_pos": 95, "type": "METRIC", "confidence": 0.7672612071037292}, {"text": "WER reduction", "start_pos": 177, "end_pos": 190, "type": "METRIC", "confidence": 0.9764233529567719}]}], "introductionContent": [{"text": "Arabic language is characterized by a complex morphological structure where different kinds of prefixes and suffixes are appended to the word stems producing a very large number of inflectional forms.", "labels": [], "entities": []}, {"text": "This leads to poor language model (LM) probability estimates, and thus high LM perplexities (PPLs) causing problems in large vocabulary continuous speech recognition (LVCSR).", "labels": [], "entities": [{"text": "large vocabulary continuous speech recognition (LVCSR)", "start_pos": 119, "end_pos": 173, "type": "TASK", "confidence": 0.7137117385864258}]}, {"text": "One successful approach to deal with this problem is to consider LMs including morphologically decomposed words.", "labels": [], "entities": []}, {"text": "Another approach is to use the factored language models (FLMs) which are powerful models that combine multiple sources of information and efficiently integrate them via a complex backoff mechanism.", "labels": [], "entities": []}, {"text": "Morphological decomposition is successfully used for Arabic LMs in several previous works.", "labels": [], "entities": [{"text": "Morphological decomposition", "start_pos": 0, "end_pos": 27, "type": "TASK", "confidence": 0.8455942869186401}]}, {"text": "Some are based on linguistic knowledge, and others are based on unsupervised methods.", "labels": [], "entities": []}, {"text": "Some of the linguistic methods are based on the Buckwalter Arabic Morphological Analyzer (BAMA) like in (.", "labels": [], "entities": [{"text": "Buckwalter Arabic Morphological Analyzer (BAMA)", "start_pos": 48, "end_pos": 95, "type": "DATASET", "confidence": 0.8840840288570949}]}, {"text": "Alternatively, in our previous work, we use the Morphological Analyzer and Disambiguator for Arabic (MADA).", "labels": [], "entities": []}, {"text": "On the other side, most of the unsupervised methods are based on the minimum description length principle (MDL) like in (.", "labels": [], "entities": [{"text": "minimum description length principle (MDL)", "start_pos": 69, "end_pos": 111, "type": "METRIC", "confidence": 0.7347932713372367}]}, {"text": "Another type of models is the FLM, in which words are viewed as vectors of K factors, so that wt := {f 1:K t }.", "labels": [], "entities": [{"text": "FLM", "start_pos": 30, "end_pos": 33, "type": "TASK", "confidence": 0.5103408098220825}]}, {"text": "A factor could be any feature of the word such as morphological class, stem, root or even a semantic feature.", "labels": [], "entities": []}, {"text": "An FLM is a model over factors, i.e., p(f 1:K t |f 1:K t\u22121 , f 1:K t\u22122 , ..., f 1:K t\u2212n+1 ), which could be reformed as a product of probabilities of the form p(f |f 1 , f 2 , ..., f N ).", "labels": [], "entities": []}, {"text": "The main idea of the model is to backoff to other factors when some word n-gram is not observed in the training data, thus improving the probability estimates.", "labels": [], "entities": []}, {"text": "In this work we try to combine the strengths of morphological decomposition and factored language modeling.", "labels": [], "entities": [{"text": "morphological decomposition", "start_pos": 48, "end_pos": 75, "type": "TASK", "confidence": 0.7135832756757736}, {"text": "factored language modeling", "start_pos": 80, "end_pos": 106, "type": "TASK", "confidence": 0.704339842001597}]}, {"text": "Therefore, language models with factored morphemes are used.", "labels": [], "entities": []}, {"text": "For this purpose, the LM training data are processed such that full-words are decomposed into prefix-stem-suffix format with different added features.", "labels": [], "entities": []}, {"text": "We compare our approach with the standard full-word, decomposed word, and factored full-word n-gram approaches.", "labels": [], "entities": []}], "datasetContent": [{"text": "Our recognition system is close to the one described in section 3.", "labels": [], "entities": []}, {"text": "However, we use within and acrossword models at different recognition passes.", "labels": [], "entities": []}, {"text": "In addition, we use 70k or 256k lexicon of full-words or partially decomposed words.", "labels": [], "entities": []}, {"text": "Alternatively, we evaluate the results on the GALE 2007 development and evaluation sets (dev07: 2.5h; eval07: 4h).", "labels": [], "entities": [{"text": "GALE 2007 development and evaluation sets", "start_pos": 46, "end_pos": 87, "type": "DATASET", "confidence": 0.9131278097629547}]}, {"text": "Our recognizer works in 3 passes.", "labels": [], "entities": []}, {"text": "In the first pass, withinword acoustic models are used with no adaptation, along with a standard bi-gram LM to generate lattices, followed by a standard tri-gram or 4-gram LM rescoring of lattices.", "labels": [], "entities": []}, {"text": "The second pass does the same, but it uses across-word models with Constrained Maximum Likelihood Linear Regression (CMLLR) adaptation.", "labels": [], "entities": [{"text": "Constrained Maximum Likelihood Linear Regression (CMLLR) adaptation", "start_pos": 67, "end_pos": 134, "type": "METRIC", "confidence": 0.7710950407716963}]}, {"text": "Then, a third pass with additional Maximum Likelihood Linear Regression (MLLR) adaptation is performed, using a standard bi-gram LM to generate lattices or N-best lists.", "labels": [], "entities": [{"text": "Maximum Likelihood Linear Regression (MLLR) adaptation", "start_pos": 35, "end_pos": 89, "type": "METRIC", "confidence": 0.8493656441569328}]}, {"text": "Then, one of the following is performed: 1) lattice rescoring using standard tri-gram or 4-gram LM, 2) N-best list rescoring using FLMs based on full-words, partially or fully decomposed words.", "labels": [], "entities": []}, {"text": "In this section, we record our recognition results for: 1) systems based on full-words, and 2) systems based on decomposed words.", "labels": [], "entities": []}, {"text": "Also, we introduce additional results for larger lexicon sizes.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: perplexities of the FLMs using vocabularies:  (FW: 70k full-words; PD: partially decomposed with 20k  ful-words + 50k morphemes; FD: 70k fully decomposed).", "labels": [], "entities": [{"text": "FLMs", "start_pos": 30, "end_pos": 34, "type": "TASK", "confidence": 0.9299238920211792}, {"text": "FW", "start_pos": 57, "end_pos": 59, "type": "METRIC", "confidence": 0.5836918950080872}, {"text": "FD", "start_pos": 139, "end_pos": 141, "type": "METRIC", "confidence": 0.6228921413421631}]}, {"text": " Table 2: WERs using FLMs based on 70k full-words.", "labels": [], "entities": [{"text": "WERs", "start_pos": 10, "end_pos": 14, "type": "TASK", "confidence": 0.44729575514793396}, {"text": "FLMs", "start_pos": 21, "end_pos": 25, "type": "TASK", "confidence": 0.858416736125946}]}, {"text": " Table 3: WERs for 70k full-words systems.", "labels": [], "entities": [{"text": "WERs", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9728517532348633}]}, {"text": " Table 4: WERs for 70k partially decomposed systems  (20k full-words + 50k morphemes).", "labels": [], "entities": [{"text": "WERs", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9482280015945435}]}, {"text": " Table 5: WERs for 256k full-words, and partially decom- posed systems (20k full-words + 236k morphemes).", "labels": [], "entities": [{"text": "WERs", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9807615876197815}]}, {"text": " Table 6: OOVs [%] of the used vocabularies.", "labels": [], "entities": [{"text": "OOVs", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.940498411655426}]}]}