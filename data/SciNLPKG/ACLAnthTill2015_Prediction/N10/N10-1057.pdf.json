{"title": [{"text": "Predicting Human-Targeted Translation Edit Rate via Untrained Human Annotators", "labels": [], "entities": [{"text": "Predicting Human-Targeted Translation Edit Rate", "start_pos": 0, "end_pos": 47, "type": "TASK", "confidence": 0.7919423460960389}]}], "abstractContent": [{"text": "In the field of machine translation, automatic metrics have proven quite valuable in system development for tracking progress and measuring the impact of incremental changes.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 16, "end_pos": 35, "type": "TASK", "confidence": 0.8036282062530518}]}, {"text": "However, human judgment still plays a large role in the context of evaluating MT systems.", "labels": [], "entities": [{"text": "MT", "start_pos": 78, "end_pos": 80, "type": "TASK", "confidence": 0.9643441438674927}]}, {"text": "For example, the GALE project uses human-targeted translation edit rate (HTER), wherein the MT output is scored against a post-edited version of itself (as opposed to being scored against an existing human reference).", "labels": [], "entities": [{"text": "translation edit rate (HTER)", "start_pos": 50, "end_pos": 78, "type": "METRIC", "confidence": 0.7905362794796625}, {"text": "MT", "start_pos": 92, "end_pos": 94, "type": "TASK", "confidence": 0.9508962631225586}]}, {"text": "This poses a problem for MT researchers, since HTER is not an easy metric to calculate, and would require hiring and training human an-notators to perform the editing task.", "labels": [], "entities": [{"text": "MT", "start_pos": 25, "end_pos": 27, "type": "TASK", "confidence": 0.9876169562339783}]}, {"text": "In this work, we explore soliciting those edits from untrained human annotators, via the online service Amazon Mechanical Turk.", "labels": [], "entities": []}, {"text": "We show that the collected data allows us to predict HTER-ranking of documents at a significantly higher level than the ranking obtained using automatic metrics.", "labels": [], "entities": [{"text": "HTER-ranking of documents", "start_pos": 53, "end_pos": 78, "type": "TASK", "confidence": 0.7221567233403524}]}], "introductionContent": [{"text": "In the early days of machine translation (MT), it was typical to evaluate MT output by soliciting judgments from human subjects, such as evaluating the fluency and adequacy of MT output.", "labels": [], "entities": [{"text": "machine translation (MT)", "start_pos": 21, "end_pos": 45, "type": "TASK", "confidence": 0.8578565657138825}, {"text": "MT output", "start_pos": 74, "end_pos": 83, "type": "TASK", "confidence": 0.9057476222515106}]}, {"text": "While this approach was appropriate (indeed desired) for evaluating a system, it was not a practical means of tracking the progress of a system during its development, since collecting human judgments is both costly and time-consuming.", "labels": [], "entities": []}, {"text": "The introduction of automatic metrics like BLEU contributed greatly to MT research, for instance allowing researchers to measure and evaluate the impact of small modifications to an MT system.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 43, "end_pos": 47, "type": "METRIC", "confidence": 0.9965686798095703}, {"text": "MT", "start_pos": 71, "end_pos": 73, "type": "TASK", "confidence": 0.996402382850647}, {"text": "MT", "start_pos": 182, "end_pos": 184, "type": "TASK", "confidence": 0.9666679501533508}]}, {"text": "However, manual evaluation remains a core component of system evaluation.", "labels": [], "entities": [{"text": "system evaluation", "start_pos": 55, "end_pos": 72, "type": "TASK", "confidence": 0.7727549970149994}]}, {"text": "Teams on the GALE project, a DARPA-sponsored MT research program, are evaluated using the HTER metric, which is aversion of TER whereby the output is scored against a post-edited version of itself, instead of a preexisting reference.", "labels": [], "entities": [{"text": "GALE", "start_pos": 13, "end_pos": 17, "type": "TASK", "confidence": 0.771126389503479}, {"text": "MT research", "start_pos": 45, "end_pos": 56, "type": "TASK", "confidence": 0.8661133050918579}, {"text": "HTER metric", "start_pos": 90, "end_pos": 101, "type": "METRIC", "confidence": 0.9424208104610443}, {"text": "TER", "start_pos": 124, "end_pos": 127, "type": "METRIC", "confidence": 0.9979852437973022}]}, {"text": "Moreover, emphasis is placed on performing well across all documents and across all genres.", "labels": [], "entities": []}, {"text": "Therefore, it is important fora research team to be able to evaluate their system using HTER, or at least determine the ranking of the documents according to HTER, for purposes of error analysis.", "labels": [], "entities": [{"text": "HTER", "start_pos": 88, "end_pos": 92, "type": "DATASET", "confidence": 0.7266694903373718}, {"text": "HTER", "start_pos": 158, "end_pos": 162, "type": "DATASET", "confidence": 0.8143025636672974}]}, {"text": "Instead of hiring a human translator and training them, we propose moving the task to the virtual world of Amazon's Mechanical Turk (AMT), hiring workers to edit the MT output and predict HTER from those edits.", "labels": [], "entities": [{"text": "Amazon's Mechanical Turk (AMT)", "start_pos": 107, "end_pos": 137, "type": "DATASET", "confidence": 0.8839789118085589}]}, {"text": "We show that edits collected this way are better at predicting document ranking than automatic metrics, and furthermore that it can be done at a low cost, both in terms of time and money.", "labels": [], "entities": [{"text": "predicting document ranking", "start_pos": 52, "end_pos": 79, "type": "TASK", "confidence": 0.8975072900454203}]}, {"text": "The paper is organized as follows.", "labels": [], "entities": []}, {"text": "We first discuss options available to predict HTER, such as automatic metrics.", "labels": [], "entities": [{"text": "HTER", "start_pos": 46, "end_pos": 50, "type": "TASK", "confidence": 0.7644079327583313}]}, {"text": "We then discuss the possibility of relying on human annotators, and the inherent difficulty in training them, before discussing the concept of soliciting edits over AMT.", "labels": [], "entities": [{"text": "AMT", "start_pos": 165, "end_pos": 168, "type": "DATASET", "confidence": 0.8630920052528381}]}, {"text": "We detail the task given to the workers and summarize the data that we collected, then show how we can combine their data to obtain significanly better rank predictions of documents.", "labels": [], "entities": []}], "datasetContent": [{"text": "We solicited edits of the output from one of GALE's teams on the Arabic-to-English task.", "labels": [], "entities": [{"text": "GALE", "start_pos": 45, "end_pos": 49, "type": "DATASET", "confidence": 0.800025224685669}]}, {"text": "This MT output was submitted by this team and HTERscored by LDC-hired human translators.", "labels": [], "entities": [{"text": "MT", "start_pos": 5, "end_pos": 7, "type": "TASK", "confidence": 0.9842568039894104}, {"text": "HTERscored", "start_pos": 46, "end_pos": 56, "type": "METRIC", "confidence": 0.5804327726364136}, {"text": "LDC-hired human translators", "start_pos": 60, "end_pos": 87, "type": "TASK", "confidence": 0.6332197984059652}]}, {"text": "Therefore, we already had the edits produced by a professional translator.", "labels": [], "entities": []}, {"text": "These edits were used as the \"gold-standard\" to evaluate the edits solicited from AMT and to evaluate our methods of combining Turkers' submissions.", "labels": [], "entities": [{"text": "AMT", "start_pos": 82, "end_pos": 85, "type": "DATASET", "confidence": 0.8626963496208191}]}, {"text": "The MT output is a translation of more than 2,153 Arabic segments spread across 195 documents in 4 different genres: broadcast conversations (BC), broadcast news (BN), newswire (NW), and blogs (WB).", "labels": [], "entities": [{"text": "MT", "start_pos": 4, "end_pos": 6, "type": "TASK", "confidence": 0.9587799310684204}]}, {"text": "For each of the 2,153 MT output segments, we collected edits from 5 distinct workers on AMT, fora total of 10,765 post-edited segments by a total of about 500 distinct workers.", "labels": [], "entities": [{"text": "MT output", "start_pos": 22, "end_pos": 31, "type": "TASK", "confidence": 0.8253394961357117}, {"text": "AMT", "start_pos": 88, "end_pos": 91, "type": "DATASET", "confidence": 0.8359193801879883}]}, {"text": "The segments were presented in 1,210 groups of up to 15 segments each, with a reward of $0.25 per group.", "labels": [], "entities": []}, {"text": "Hence the total rewards to workers was around $300, at a rate of 36 post-edited segments per dollar (or 2.8 pennies per segment).", "labels": [], "entities": []}, {"text": "We examine the effectiveness of any of the above methods by comparing the resulting document ranking versus the desired ranking by HTER.", "labels": [], "entities": [{"text": "HTER", "start_pos": 131, "end_pos": 135, "type": "DATASET", "confidence": 0.9139404892921448}]}, {"text": "In addition to the above methods, we use a baseline a ranking predicted by TER to a human reference.", "labels": [], "entities": [{"text": "TER", "start_pos": 75, "end_pos": 78, "type": "METRIC", "confidence": 0.981940746307373}]}, {"text": "(For clarity, we omit discussion with other metrics such as BLEU and (TER-BLEU)/2, since those baselines are not as strong as the TER baseline.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 60, "end_pos": 64, "type": "METRIC", "confidence": 0.9985082745552063}, {"text": "TER-BLEU)/2", "start_pos": 70, "end_pos": 81, "type": "METRIC", "confidence": 0.9595605532328287}, {"text": "TER", "start_pos": 130, "end_pos": 133, "type": "METRIC", "confidence": 0.8169500231742859}]}, {"text": "We examine each genre individually, since genres vary quite a bit in difficulty, and, more importantly, we care about the internal ranking within each genre, to mirror the GALE evaluation procedure.", "labels": [], "entities": [{"text": "GALE evaluation", "start_pos": 172, "end_pos": 187, "type": "TASK", "confidence": 0.4080640524625778}]}, {"text": "We examine the effect of varying the amount of data by which we judge a Turker's data quality.", "labels": [], "entities": []}, {"text": "The amount of this \"verification\" data is varied as a percentage of the total available segments.", "labels": [], "entities": []}, {"text": "Those segments are chosen at random, and we perform 100 trials for each point.", "labels": [], "entities": []}, {"text": "shows the rank correlations for various methods across different sizes of verification subsets.", "labels": [], "entities": []}, {"text": "Notice that some methods, such as the TER baseline, have horizontal lines, since these do not rate a Turker based on a verification subset.", "labels": [], "entities": [{"text": "TER baseline", "start_pos": 38, "end_pos": 50, "type": "DATASET", "confidence": 0.7058111727237701}]}, {"text": "It is worth noting that the oracle performs very well.", "labels": [], "entities": []}, {"text": "This is an indication that predicting HTER accurately is mostly a matter of identifying the best worker.", "labels": [], "entities": [{"text": "predicting HTER", "start_pos": 27, "end_pos": 42, "type": "TASK", "confidence": 0.7666325867176056}]}, {"text": "While oracle scenarios usually represent unachievable upper bounds, keep in mind that there are only a very small number of editors per segment (five, as opposed to oracle scenarios dealing with 100-best lists, etc).", "labels": [], "entities": []}, {"text": "Other than that, in general, it is possible to achieve very high rank correlation using Turkers' data, significantly outperforming the TER ranking, even with a small verification subset.", "labels": [], "entities": [{"text": "Turkers' data", "start_pos": 88, "end_pos": 101, "type": "DATASET", "confidence": 0.8741153180599213}, {"text": "TER", "start_pos": 135, "end_pos": 138, "type": "METRIC", "confidence": 0.9798935651779175}]}, {"text": "The genres do vary quite a bit in difficulty for Turkers, with BC and especially NW being quite difficult, though in the case of NW for instance, this is due to the human reference doing quite well to begin with, rather than Turkers performing poorly.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: The 4 genres of the dataset.", "labels": [], "entities": []}]}