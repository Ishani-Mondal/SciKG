{"title": [{"text": "Crowdsourcing the evaluation of a domain-adapted named entity recognition system", "labels": [], "entities": [{"text": "domain-adapted named entity recognition", "start_pos": 34, "end_pos": 73, "type": "TASK", "confidence": 0.667198158800602}]}], "abstractContent": [{"text": "Named entity recognition systems sometimes have difficulty when applied to data from domains that do not closely match the training data.", "labels": [], "entities": [{"text": "Named entity recognition", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.6197166740894318}]}, {"text": "We first use a simple rule-based technique for domain adaptation.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 47, "end_pos": 64, "type": "TASK", "confidence": 0.7794550359249115}]}, {"text": "Data for robust validation of the technique is then generated, and we use crowdsourcing techniques to show that this strategy produces reliable results even on data not seen by the rule designers.", "labels": [], "entities": []}, {"text": "We show that it is possible to extract large improvements on the target data rapidly at low cost using these techniques.", "labels": [], "entities": []}], "introductionContent": [], "datasetContent": [{"text": "Performance is evaluated in terms of standard precision and recall of entities.", "labels": [], "entities": [{"text": "precision", "start_pos": 46, "end_pos": 55, "type": "METRIC", "confidence": 0.9618868827819824}, {"text": "recall", "start_pos": 60, "end_pos": 66, "type": "METRIC", "confidence": 0.9986656904220581}]}, {"text": "If the system output contains a person or organization labelled correctly as such, it considers this to be a hit.", "labels": [], "entities": []}, {"text": "If it contains a person or organization that is mislabelled or otherwise incorrect in the gold standard annotation, it is amiss.", "labels": [], "entities": []}, {"text": "We compute the F-measure as the harmonic mean of precision and recall.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 15, "end_pos": 24, "type": "METRIC", "confidence": 0.9880473613739014}, {"text": "precision", "start_pos": 49, "end_pos": 58, "type": "METRIC", "confidence": 0.9996278285980225}, {"text": "recall", "start_pos": 63, "end_pos": 69, "type": "METRIC", "confidence": 0.9985176920890808}]}, {"text": "As the IdentiFinder output is the baseline, and we ignore missed entities, by definition the baseline recall is 100%.", "labels": [], "entities": [{"text": "recall", "start_pos": 102, "end_pos": 108, "type": "METRIC", "confidence": 0.9634217023849487}]}, {"text": "Here we delve into further detail about the techniques we used and the results that they yielded.", "labels": [], "entities": []}, {"text": "The results are summarized in table 1.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Results of evaluation of different document sets against ground truth source by annotation technique.", "labels": [], "entities": []}]}