{"title": [], "abstractContent": [{"text": "This paper presents efficient algorithms for expected similarity maximization, which coincides with minimum Bayes decoding fora similarity-based loss function.", "labels": [], "entities": [{"text": "expected similarity maximization", "start_pos": 45, "end_pos": 77, "type": "TASK", "confidence": 0.5585950314998627}]}, {"text": "Our algorithms are designed for similarity functions that are sequence kernels in a general class of positive definite symmetric kernels.", "labels": [], "entities": []}, {"text": "We discuss both a general algorithm and a more efficient algorithm applicable in a common unambigu-ous scenario.", "labels": [], "entities": []}, {"text": "We also describe the application of our algorithms to machine translation and report the results of experiments with several translation data sets which demonstrate a substantial speed-up.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 54, "end_pos": 73, "type": "TASK", "confidence": 0.7920079231262207}]}, {"text": "In particular, our results show a speed-up by two orders of magnitude with respect to the original method of Tromble et al.", "labels": [], "entities": [{"text": "speed-up", "start_pos": 34, "end_pos": 42, "type": "METRIC", "confidence": 0.9871026873588562}]}, {"text": "(2008) and by a factor of 3 or more even with respect to an approximate algorithm specifically designed for that task.", "labels": [], "entities": []}, {"text": "These results open the path for the exploration of more appropriate or optimal kernels for the specific tasks considered.", "labels": [], "entities": []}], "introductionContent": [{"text": "The output of many complex natural language processing systems such as information extraction, speech recognition, or machine translation systems is a probabilistic automaton.", "labels": [], "entities": [{"text": "information extraction", "start_pos": 71, "end_pos": 93, "type": "TASK", "confidence": 0.7839581966400146}, {"text": "speech recognition", "start_pos": 95, "end_pos": 113, "type": "TASK", "confidence": 0.7197913229465485}, {"text": "machine translation", "start_pos": 118, "end_pos": 137, "type": "TASK", "confidence": 0.7179830819368362}]}, {"text": "Exploiting the full information provided by this probabilistic automaton can lead to more accurate results than just using the one-best sequence.", "labels": [], "entities": []}, {"text": "Different techniques have been explored in the past to take advantage of the full lattice, some based on the use of a more complex model applied to the automaton as in rescoring, others using additional data or information for reranking the hypotheses represented by the automaton.", "labels": [], "entities": []}, {"text": "One method for using these probabilistic automata that has been successful in large-vocabulary speech recognition) and machine translation () applications and that requires no additional data or other complex models is the minimum Bayes risk (MBR) decoding technique.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 95, "end_pos": 113, "type": "TASK", "confidence": 0.7438836395740509}, {"text": "machine translation", "start_pos": 119, "end_pos": 138, "type": "TASK", "confidence": 0.6836562007665634}, {"text": "minimum Bayes risk (MBR", "start_pos": 223, "end_pos": 246, "type": "METRIC", "confidence": 0.688540780544281}]}, {"text": "This returns that sequence of the automaton having the minimum expected loss with respect to all sequences accepted by the automaton).", "labels": [], "entities": []}, {"text": "Often, minimizing the loss function L can be equivalently viewed as maximizing a similarity function K between sequences, which corresponds to a kernel function when it is positive definite symmetric (.", "labels": [], "entities": []}, {"text": "The technique can then bethought of as an expected sequence similarity maximization.", "labels": [], "entities": []}, {"text": "This paper considers this expected similarity maximization view.", "labels": [], "entities": [{"text": "similarity maximization", "start_pos": 35, "end_pos": 58, "type": "TASK", "confidence": 0.6473893076181412}]}, {"text": "Since different similarity functions can be used within this framework, one may wish to select the one that is the most appropriate or relevant to the task considered.", "labels": [], "entities": []}, {"text": "However, a crucial requirement for this choice to be realistic is to ensure that for the family of similarity functions considered the expected similarity maximization is efficiently computable.", "labels": [], "entities": []}, {"text": "Thus, we primarily focus on this algorithmic problem in this paper, leaving it to future work to study the question of determining how to select the similarity function and report on the benefits of this choice.", "labels": [], "entities": []}, {"text": "A general family of sequence kernels including the sequence kernels used in computational biology, text categorization, spoken-dialog classification, and many other tasks is that of rational kernels).", "labels": [], "entities": [{"text": "text categorization", "start_pos": 99, "end_pos": 118, "type": "TASK", "confidence": 0.7681866884231567}, {"text": "spoken-dialog classification", "start_pos": 120, "end_pos": 148, "type": "TASK", "confidence": 0.7228336185216904}]}, {"text": "We show how the expected similarity maximization can be efficiently computed for these kernels.", "labels": [], "entities": []}, {"text": "In section 3, we describe more specifically the framework of expected similarity maximization in the case of rational kernels and the corresponding algorithmic problem.", "labels": [], "entities": [{"text": "expected similarity maximization", "start_pos": 61, "end_pos": 93, "type": "TASK", "confidence": 0.5720317363739014}]}, {"text": "In Section 4, we describe both a general method for the computation of the expected similarity maximization, and a more efficient method that can be used with abroad sub-family of rational kernels that verify a condition of nonambiguity.", "labels": [], "entities": []}, {"text": "This latter family includes the class of n-gram kernels which have been previously used to apply MBR to machine translation (.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 104, "end_pos": 123, "type": "TASK", "confidence": 0.7349607050418854}]}, {"text": "We examine in more detail the use and application of our algorithms to machine translation in Section 5.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 71, "end_pos": 90, "type": "TASK", "confidence": 0.7695646584033966}]}, {"text": "Section 6 reports the results of experiments applying our algorithms in several large data sets in machine translation.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 99, "end_pos": 118, "type": "TASK", "confidence": 0.7680911421775818}]}, {"text": "These experiments demonstrate the efficiency of our algorithm which is shown empirically to be two orders of magnitude faster than and more than 3 times faster than even an approximation algorithm specifically designed for this problem ().", "labels": [], "entities": []}, {"text": "We start with some preliminary definitions and algorithms related to weighted automata and transducers, following the definitions and terminology of.", "labels": [], "entities": []}], "datasetContent": [{"text": "Lattices were generated using a phrase-based MT system similar to the alignment template system described in ().", "labels": [], "entities": [{"text": "MT", "start_pos": 45, "end_pos": 47, "type": "TASK", "confidence": 0.8957625031471252}]}, {"text": "Given a source sentence, the system produces a word lattice A that is a compact representation of a very large N -best list of translation hypotheses for that source sentence and their likelihoods.", "labels": [], "entities": []}, {"text": "The lattice A is converted into a lattice X that represents a probability distribution (i.e. the posterior probability distribution given the source sentence) following: where the scaling factor \u03b1 \u2208 [0, \u221e) flattens the distribution when \u03b1 < 1 and sharpens it when \u03b1 > 1.", "labels": [], "entities": []}, {"text": "We then applied the methods described in Section 5 to the lattice X using as hypothesis set H the unweighted lattice obtained from X.", "labels": [], "entities": []}, {"text": "The following parameters for the n-gram factors were used: Experiments were conducted on two language pairs Arabic-English (aren) and Chinese-English (zhen) and fora variety of datasets from the NIST Open Machine Translation (OpenMT) Evaluation.", "labels": [], "entities": [{"text": "NIST Open Machine Translation (OpenMT) Evaluation", "start_pos": 195, "end_pos": 244, "type": "DATASET", "confidence": 0.7772907614707947}]}, {"text": "The values of \u03b1, p and r used for each pair are given  in.", "labels": [], "entities": []}, {"text": "We used the IBM implementation of the BLEU score ().", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 38, "end_pos": 48, "type": "METRIC", "confidence": 0.9785340130329132}]}, {"text": "We implemented the following methods using the OpenFst library): \u2022 exact: uses the similarity measure K LB based on the linearized log-BLEU, implemented as described in); \u2022 approx: uses the approximation to K LB from () and described in the appendix; \u2022 ngram: uses the similarity measure K N G implemented using the algorithm of Section 4.2; \u2022 ngram1: uses the similarity measure K 1 N G also implemented using the algorithm of Section 4.2.", "labels": [], "entities": [{"text": "similarity measure K LB", "start_pos": 83, "end_pos": 106, "type": "METRIC", "confidence": 0.7843055576086044}]}, {"text": "The results from show that ngram1 performs as well as exact on all datasets 5 while being two orders of magnitude faster than exact and overall more than 3 times faster than approx.", "labels": [], "entities": [{"text": "ngram1", "start_pos": 27, "end_pos": 33, "type": "METRIC", "confidence": 0.7493183016777039}, {"text": "exact", "start_pos": 54, "end_pos": 59, "type": "METRIC", "confidence": 0.9859208464622498}]}], "tableCaptions": [{"text": " Table 1: BLEU score (%)", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9400628507137299}]}, {"text": " Table 2: MBR Time (in seconds)", "labels": [], "entities": [{"text": "MBR Time", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.5822189748287201}]}]}