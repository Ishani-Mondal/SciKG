{"title": [{"text": "Products of Random Latent Variable Grammars", "labels": [], "entities": []}], "abstractContent": [{"text": "We show that the automatically induced latent variable grammars of Petrov et al.", "labels": [], "entities": []}, {"text": "(2006) vary widely in their underlying representations, depending on their EM initialization point.", "labels": [], "entities": []}, {"text": "We use this to our advantage, combining multiple automatically learned grammars into an un-weighted product model, which gives significantly improved performance over state-of-the-art individual grammars.", "labels": [], "entities": []}, {"text": "In our model, the probability of a constituent is estimated as a product of posteriors obtained from multiple grammars that differ only in the random seed used for initialization, without any learning or tuning of combination weights.", "labels": [], "entities": []}, {"text": "Despite its simplicity, a product of eight automatically learned grammars improves parsing accuracy from 90.2% to 91.8% on English, and from 80.3% to 84.5% on German.", "labels": [], "entities": [{"text": "parsing", "start_pos": 83, "end_pos": 90, "type": "TASK", "confidence": 0.9585910439491272}, {"text": "accuracy", "start_pos": 91, "end_pos": 99, "type": "METRIC", "confidence": 0.978988766670227}]}], "introductionContent": [{"text": "Learning a context-free grammar for parsing requires the estimation of a more highly articulated model than the one embodied by the observed treebank.", "labels": [], "entities": [{"text": "parsing", "start_pos": 36, "end_pos": 43, "type": "TASK", "confidence": 0.9745160937309265}]}, {"text": "This is because the naive treebank grammar) is too permissive, making unrealistic context-freedom assumptions.", "labels": [], "entities": []}, {"text": "For example, it postulates that there is only one type of noun phrase (NP), which can appear in all positions (subject, object, etc.), regardless of case, number or gender.", "labels": [], "entities": []}, {"text": "As a result, the grammar can generate millions of (incorrect) parse trees fora given sentence, and has a flat posterior distribution.", "labels": [], "entities": []}, {"text": "High accuracy grammars therefore add soft constraints on the way categories can be combined, and enrich the label set with additional information.", "labels": [], "entities": []}, {"text": "These constraints can be lexicalized), unlexicalized) or automatically learned ().", "labels": [], "entities": []}, {"text": "The constraints serve the purpose of weakening the independence assumptions, and reduce the number of possible (but incorrect) parses.", "labels": [], "entities": []}, {"text": "Here, we focus on the latent variable approach of, where an Expectation Maximization (EM) algorithm is used to induce a hierarchy of increasingly more refined grammars.", "labels": [], "entities": [{"text": "Expectation Maximization (EM)", "start_pos": 60, "end_pos": 89, "type": "TASK", "confidence": 0.766430926322937}]}, {"text": "Each round of refinement introduces new constraints on how constituents can be combined, which in turn leads to a higher parsing accuracy.", "labels": [], "entities": [{"text": "parsing", "start_pos": 121, "end_pos": 128, "type": "TASK", "confidence": 0.9601077437400818}, {"text": "accuracy", "start_pos": 129, "end_pos": 137, "type": "METRIC", "confidence": 0.9271337389945984}]}, {"text": "However, EM is a local method, and there are no guarantees that it will find the same grammars when initialized from different starting points.", "labels": [], "entities": []}, {"text": "In fact, it turns out that even though the final performance of these grammars is consistently high, there are significant variations in the learned refinements.", "labels": [], "entities": []}, {"text": "We use these variations to our advantage, and treat grammars learned from different random seeds as independent and equipotent experts.", "labels": [], "entities": []}, {"text": "We use a product distribution for joint prediction, which gives more peaked posteriors than a sum, and enforces all constraints of the individual grammars, without the need to tune mixing weights.", "labels": [], "entities": [{"text": "joint prediction", "start_pos": 34, "end_pos": 50, "type": "TASK", "confidence": 0.7709493935108185}]}, {"text": "It should be noted here that our focus is on improving parsing performance using a single underlying grammar class, which is somewhat orthogonal to the issue of parser combination, that has been studied elsewhere in the literature (.", "labels": [], "entities": [{"text": "parsing", "start_pos": 55, "end_pos": 62, "type": "TASK", "confidence": 0.9688247442245483}, {"text": "parser combination", "start_pos": 161, "end_pos": 179, "type": "TASK", "confidence": 0.7794657647609711}]}, {"text": "In contrast to that line of work, we also do not restrict ourselves to working with kbest output, but work directly with a packed forest representation of the posteriors, much in the spirit of, except that we work with several forests rather than rescoring a single one.", "labels": [], "entities": []}, {"text": "In our experimental section we give empirical answers to some of the remaining theoretical questions.", "labels": [], "entities": []}, {"text": "We address the question of averaging versus multiplying classifier predictions, we investigate different ways of introducing more diversity into the underlying grammars, and also compare combining partial (constituent-level) and complete (tree-level) predictions.", "labels": [], "entities": []}, {"text": "Quite serendipitously, the simplest approaches work best in our experiments.", "labels": [], "entities": []}, {"text": "A product of eight latent variable grammars, learned on the same data, and only differing in the seed used in the random number generator that initialized EM, improves parsing accuracy from 90.2% to 91.8% on English, and from 80.3% to 84.5% on German.", "labels": [], "entities": [{"text": "parsing", "start_pos": 168, "end_pos": 175, "type": "TASK", "confidence": 0.9458127617835999}, {"text": "accuracy", "start_pos": 176, "end_pos": 184, "type": "METRIC", "confidence": 0.9718573093414307}]}, {"text": "These parsing results are even better than those obtained by discriminative systems which have access to additional non-local features).", "labels": [], "entities": [{"text": "parsing", "start_pos": 6, "end_pos": 13, "type": "TASK", "confidence": 0.9729409217834473}]}], "datasetContent": [{"text": "In our experiments, we follow the standard setups described in, and use the EVALB tool for computing parsing figures.", "labels": [], "entities": [{"text": "parsing", "start_pos": 101, "end_pos": 108, "type": "TASK", "confidence": 0.6797275543212891}]}, {"text": "Unless noted otherwise, we use CONSTITUENT-LEVEL inference.", "labels": [], "entities": []}, {"text": "All our experiments are based on the publicly available BerkeleyParser.", "labels": [], "entities": [{"text": "BerkeleyParser", "start_pos": 56, "end_pos": 70, "type": "DATASET", "confidence": 0.9610133767127991}]}], "tableCaptions": [{"text": " Table 2: Final test set accuracies for English and German.", "labels": [], "entities": [{"text": "accuracies", "start_pos": 25, "end_pos": 35, "type": "METRIC", "confidence": 0.7188130021095276}]}]}