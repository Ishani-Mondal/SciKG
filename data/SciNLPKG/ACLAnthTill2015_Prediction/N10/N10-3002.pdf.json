{"title": [{"text": "On Automated Evaluation of Readability of Summaries: Capturing Grammaticality, Focus, Structure and Coherence", "labels": [], "entities": [{"text": "Automated Evaluation of Readability of Summaries", "start_pos": 3, "end_pos": 51, "type": "TASK", "confidence": 0.6037902136643728}, {"text": "Capturing Grammaticality", "start_pos": 53, "end_pos": 77, "type": "TASK", "confidence": 0.746754378080368}]}], "abstractContent": [{"text": "Readability of a summary is usually graded manually on five aspects of readability: gram-maticality, coherence and structure, focus, referential clarity and non-redundancy.", "labels": [], "entities": [{"text": "clarity", "start_pos": 145, "end_pos": 152, "type": "METRIC", "confidence": 0.8364490866661072}]}, {"text": "In the context of automated metrics for evaluation of summary quality, content evaluations have been presented through the last decade and continue to evolve, however a careful examination of readability aspects of summary quality has not been as exhaustive.", "labels": [], "entities": []}, {"text": "In this paper we explore alternative evaluation metrics for 'grammaticality' and 'coherence and struc-ture' that are able to strongly correlate with manual ratings.", "labels": [], "entities": []}, {"text": "Our results establish that our methods are able to perform pair-wise ranking of summaries based on grammaticality, as strongly as ROUGE is able to distinguish for content evaluations.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 130, "end_pos": 135, "type": "METRIC", "confidence": 0.7099050879478455}]}, {"text": "We observed that none of the five aspects of readability are independent of each other, and hence by addressing the individual criterion of evaluation we aim to achieve automated appreciation of readability of summaries.", "labels": [], "entities": []}], "introductionContent": [{"text": "Automated text summarization deals with both the problem of identifying relevant snippets of information and presenting it in a pertinent format.", "labels": [], "entities": [{"text": "Automated text summarization", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.565907617410024}]}, {"text": "Automated evaluation is crucial to automatic text summarization to be used both to rank multiple participant systems in shared tasks 1 , and to developers whose goal is to improve the summarization systems.", "labels": [], "entities": [{"text": "text summarization", "start_pos": 45, "end_pos": 63, "type": "TASK", "confidence": 0.5873831361532211}]}, {"text": "Summarization evaluations help in the creation of reusable resources and infrastructure; it sets up the stage for comparison and replication of results by introducing an element of competition to produce better results).", "labels": [], "entities": [{"text": "Summarization evaluations", "start_pos": 0, "end_pos": 25, "type": "TASK", "confidence": 0.8911676406860352}]}, {"text": "The summarization tracks at Text Analysis Conference (TAC) and its predecessors at Document Understanding Conferences (DUC).", "labels": [], "entities": [{"text": "Text Analysis Conference (TAC)", "start_pos": 28, "end_pos": 58, "type": "TASK", "confidence": 0.85881374279658}]}, {"text": "Readability or Fluency of a summary is categorically measured based on a set of linguistic quality questions that manual assessors answer for each summary.", "labels": [], "entities": [{"text": "Fluency", "start_pos": 15, "end_pos": 22, "type": "METRIC", "confidence": 0.9786091446876526}]}, {"text": "The linguistic quality markers are: grammaticality, Non-Redundancy, Referential Clarity, Focus and Structure and Coherence.", "labels": [], "entities": []}, {"text": "Hence readability assessment is a manual method where expert assessors give a rating for each summary on the Likert Scale for each of the linguistic quality markers.", "labels": [], "entities": [{"text": "readability assessment", "start_pos": 6, "end_pos": 28, "type": "TASK", "confidence": 0.85287806391716}]}, {"text": "Manual evaluation being time-consuming and expensive doesn't help system developerswho appreciate fast, reliable and most importantly automated evaluation metric.", "labels": [], "entities": []}, {"text": "So despite having a sound manual evaluation methodology for readability, there is an need for reliable automatic metrics.", "labels": [], "entities": []}, {"text": "All the early approaches like Flesch Reading Ease were developed for general texts and none of these techniques have tried to characterize themselves as approximations to grammaticality or structure or coherence.", "labels": [], "entities": []}, {"text": "In assessing readability of summaries, there hasn't been much of dedicated analysis with text summaries, except in () where local coherence was modeled for text summaries and in) where grammaticality of text summaries were explored.", "labels": [], "entities": [{"text": "summaries", "start_pos": 28, "end_pos": 37, "type": "TASK", "confidence": 0.8584702610969543}]}, {"text": "Ina marginally related work in Natural Language Generation, () addresses sentence level fluency regardless of content, while recent work in gives a systematic study on how syntactic features were able to distinguish machine generated translations from human translations.", "labels": [], "entities": [{"text": "Natural Language Generation", "start_pos": 31, "end_pos": 58, "type": "TASK", "confidence": 0.6633164683977762}, {"text": "distinguish machine generated translations from human translations", "start_pos": 204, "end_pos": 270, "type": "TASK", "confidence": 0.6832265257835388}]}, {"text": "In another related work, investigated the impact of certain surface linguistic features, syntactic, entity coherence and discourse features on the readability of Wall Street Journal (WSJ) Corpus.", "labels": [], "entities": [{"text": "Wall Street Journal (WSJ) Corpus", "start_pos": 162, "end_pos": 194, "type": "DATASET", "confidence": 0.9449340530804226}]}, {"text": "We use the syntactic features used in as baselines for our experiments on grammaticality in this paper.", "labels": [], "entities": []}, {"text": "7 While studying the coherence patterns in student essays,) identified that grammatical errors affect the overall expressive quality of the essays.", "labels": [], "entities": []}, {"text": "In this paper, due to the lack of an appropriate baseline and due to the interesting-ness of the above observation we use metrics for grammaticality as a baseline measure for structure and coherence.", "labels": [], "entities": []}, {"text": "Focus of a summary, is the only aspect of readability that relies to a larger extent on the content of the summary.", "labels": [], "entities": []}, {"text": "In this paper, we use Recall Oriented Understudy of Gisting Evaluation (ROUGE)) based metrics as one of the baselines to capture focus in a summary.", "labels": [], "entities": [{"text": "Recall Oriented Understudy of Gisting Evaluation (ROUGE", "start_pos": 22, "end_pos": 77, "type": "METRIC", "confidence": 0.5963455066084862}]}], "datasetContent": [{"text": "This paper deals with methods that imitate manual evaluation metric for grammaticality and structure and coherence by producing a score for each summary.", "labels": [], "entities": []}, {"text": "An evaluation of these new summarization evaluation metrics is based on how well the system rankings produced by them correlate with manual evaluations.", "labels": [], "entities": [{"text": "summarization evaluation", "start_pos": 27, "end_pos": 51, "type": "TASK", "confidence": 0.9125936925411224}]}, {"text": "We use 3 types of correlation evaluations -Spearman's Rank Correlation, Pearson's Correlation and Kendall's Tau -each describing some aspect of ordering problems.", "labels": [], "entities": [{"text": "Pearson's Correlation", "start_pos": 72, "end_pos": 93, "type": "METRIC", "confidence": 0.932765543460846}]}, {"text": "We used reference summaries from for the reference corpus and the experiments described were tested on DUC 2007 query-focused multi-document summarization datasets which have 45 topics and 32 system summaries for each topic apart from 4 human reference summaries.", "labels": [], "entities": [{"text": "DUC 2007 query-focused multi-document summarization datasets", "start_pos": 103, "end_pos": 163, "type": "DATASET", "confidence": 0.8627671500047048}]}, {"text": "shows the system level correlations of our approaches to grammaticality assessment with that of human ratings.", "labels": [], "entities": [{"text": "grammaticality assessment", "start_pos": 57, "end_pos": 82, "type": "TASK", "confidence": 0.859239786863327}]}, {"text": "We have used four baseline approaches: AverageNPs, AverageVPs, AverageSBARs and AverageParseTreeHeight.", "labels": [], "entities": [{"text": "AverageNPs", "start_pos": 39, "end_pos": 49, "type": "METRIC", "confidence": 0.9253367781639099}, {"text": "AverageVPs", "start_pos": 51, "end_pos": 61, "type": "METRIC", "confidence": 0.8287993669509888}, {"text": "AverageSBARs", "start_pos": 63, "end_pos": 75, "type": "METRIC", "confidence": 0.9343789219856262}]}, {"text": "Our approaches constitute of the following runs: Ngram (POS), Ngram (POS-Chunk), Class (POS 2 level), Class (POS-Chunk 2 level), Hybrid (POS), Hybrid (POS-Chunk).: System level correlations of automated and manual metrics for focus best performing system for grammaticality are used as baselines for structure and coherence assessment.", "labels": [], "entities": []}, {"text": "Again, like we previously mentioned, focus can be easily characterized using structure and coherence, and to an extent the grammatical well-formedness.", "labels": [], "entities": []}, {"text": "Also the focus of a summary is also dependent on content of the summary.", "labels": [], "entities": []}, {"text": "Hence, we use ROUGE-2, manual rating for grammaticality, manual rating for coherence, and our approaches to both grammaticality and structure and coherence as baselines as shown in.", "labels": [], "entities": [{"text": "ROUGE-2", "start_pos": 14, "end_pos": 21, "type": "METRIC", "confidence": 0.9945979118347168}]}], "tableCaptions": [{"text": " Table 1: Percentage distribution of grammaticality scores  in system summaries", "labels": [], "entities": []}, {"text": " Table 2: System level correlations of automated and man- ual metrics for grammaticality.", "labels": [], "entities": []}, {"text": " Table 3: Summary level correlations of automated and manual met- rics for grammaticality .", "labels": [], "entities": []}, {"text": " Table 4: System level correlations of automated and manual metrics  for coherence .", "labels": [], "entities": []}, {"text": " Table 5: System level correlations of automated and manual metrics", "labels": [], "entities": []}]}