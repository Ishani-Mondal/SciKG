{"title": [], "abstractContent": [{"text": "Adaptor grammars extend probabilistic context-free grammars to define prior distributions over trees with \"rich get richer\" dynamics.", "labels": [], "entities": []}, {"text": "Inference for adaptor grammars seeks to find parse trees for raw text.", "labels": [], "entities": []}, {"text": "This paper describes a variational inference algorithm for adaptor grammars, providing an alternative to Markov chain Monte Carlo methods.", "labels": [], "entities": []}, {"text": "To derive this method, we develop a stick-breaking representation of adaptor grammars, a representation that enables us to define adaptor grammars with recursion.", "labels": [], "entities": []}, {"text": "We report experimental results on a word segmentation task, showing that variational inference performs comparably to MCMC.", "labels": [], "entities": [{"text": "word segmentation task", "start_pos": 36, "end_pos": 58, "type": "TASK", "confidence": 0.8173899451891581}]}, {"text": "Further, we show a significant speed-up when parallelizing the algorithm.", "labels": [], "entities": []}, {"text": "Finally, we report promising results fora new application for adaptor grammars, dependency grammar induction.", "labels": [], "entities": [{"text": "dependency grammar induction", "start_pos": 80, "end_pos": 108, "type": "TASK", "confidence": 0.8047765096028646}]}], "introductionContent": [{"text": "Recent research in unsupervised learning for NLP focuses on Bayesian methods for probabilistic grammars.", "labels": [], "entities": []}, {"text": "Such methods have been made more flexible with nonparametric Bayesian (NP Bayes) methods, such as Dirichlet process mixture models.", "labels": [], "entities": []}, {"text": "One line of research uses NP Bayes methods on whole tree structures, in the form of adaptor grammars, in order to identify recurrent subtree patterns.", "labels": [], "entities": []}, {"text": "Adaptor grammars provide a flexible distribution over parse trees that has more structure than a traditional context-free grammar.", "labels": [], "entities": []}, {"text": "Adaptor grammars are used via posterior inference, the computational problem of determining the posterior distribution of parse trees given a set of observed sentences.", "labels": [], "entities": []}, {"text": "Current posterior inference algorithms for adaptor grammars are based on MCMC sampling methods).", "labels": [], "entities": []}, {"text": "MCMC methods are theoretically guaranteed to converge to the true posterior, but come at great expense: they are notoriously slow to converge, especially with complex hidden structures such as syntactic trees.", "labels": [], "entities": []}, {"text": "Johnson (2008b) comments on this, and suggests the use of variational inference as a possible remedy.", "labels": [], "entities": []}, {"text": "Variational inference provides a deterministic alternative to sampling.", "labels": [], "entities": [{"text": "Variational inference", "start_pos": 0, "end_pos": 21, "type": "TASK", "confidence": 0.805151104927063}]}, {"text": "It was introduced for Dirichlet process mixtures by and applied to infinite grammars by.", "labels": [], "entities": []}, {"text": "With NP Bayes models, variational methods are based on the stick-breaking representation.", "labels": [], "entities": []}, {"text": "Devising a stick-breaking representation is a central challenge to using variational inference in this setting.", "labels": [], "entities": []}, {"text": "The rest of this paper is organized as follows.", "labels": [], "entities": []}, {"text": "In \u00a72 we describe a stick-breaking representation of adaptor grammars, which enables variational inference ( \u00a73) and a well-defined incorporation of recursion into adaptor grammars.", "labels": [], "entities": []}, {"text": "In \u00a74 we give an empirical comparison of the algorithm to MCMC inference and describe a novel application of adaptor grammars to unsupervised dependency parsing.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 142, "end_pos": 160, "type": "TASK", "confidence": 0.6987358778715134}]}], "datasetContent": [{"text": "We describe experiments with variational inference for adaptor grammars for word segmentation and dependency grammar induction.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 76, "end_pos": 93, "type": "TASK", "confidence": 0.7444819509983063}, {"text": "dependency grammar induction", "start_pos": 98, "end_pos": 126, "type": "TASK", "confidence": 0.7371824383735657}]}], "tableCaptions": [{"text": " Table 1: F 1 performance for word segmentation on the  Brent corpus. Dir. stands for Dirichlet Process adaptor  (b = 0), PY stands for Pitman-Yor adaptor (b optimized),  and PY+inc. stands for Pitman-Yor with iteratively in- creasing N A for A \u2208 M (see footnote 5). J&G 2009 are  the results adapted from Johnson and Goldwater (2009);  SA is sample average decoding, and MM is maximum  marginal decoding.", "labels": [], "entities": [{"text": "F", "start_pos": 10, "end_pos": 11, "type": "METRIC", "confidence": 0.9643312096595764}, {"text": "word segmentation", "start_pos": 30, "end_pos": 47, "type": "TASK", "confidence": 0.7584632635116577}, {"text": "Brent corpus", "start_pos": 56, "end_pos": 68, "type": "DATASET", "confidence": 0.8552340269088745}]}, {"text": " Table 2: Attachment accuracy for different models for  dependency grammar induction. Bold marks best overall  accuracy per evaluation set, and  \u2020 marks figures that are  not significantly worse (binomial sign test, p < 0.05).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 21, "end_pos": 29, "type": "METRIC", "confidence": 0.6730321049690247}, {"text": "dependency grammar induction", "start_pos": 56, "end_pos": 84, "type": "TASK", "confidence": 0.8237558205922445}, {"text": "accuracy", "start_pos": 111, "end_pos": 119, "type": "METRIC", "confidence": 0.9821849465370178}]}]}