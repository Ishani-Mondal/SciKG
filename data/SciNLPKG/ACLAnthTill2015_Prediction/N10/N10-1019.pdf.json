{"title": [{"text": "Using Mostly Native Data to Correct Errors in Learners' Writing: A Meta-Classifier Approach", "labels": [], "entities": []}], "abstractContent": [{"text": "We present results from a range of experiments on article and preposition error correction for non-native speakers of English.", "labels": [], "entities": [{"text": "article and preposition error correction", "start_pos": 50, "end_pos": 90, "type": "TASK", "confidence": 0.602040272951126}]}, {"text": "We first compare a language model and error-specific classifiers (all trained on large Eng-lish corpora) with respect to their performance in error detection and correction.", "labels": [], "entities": [{"text": "error detection and correction", "start_pos": 142, "end_pos": 172, "type": "TASK", "confidence": 0.758153036236763}]}, {"text": "We then combine the language model and the classifi-ers in a meta-classification approach by combining evidence from the classifiers and the language model as input features to the meta-classifier.", "labels": [], "entities": []}, {"text": "The meta-classifier in turn is trained on error-annotated learner data, optimizing the error detection and correction performance on this domain.", "labels": [], "entities": [{"text": "error detection and correction", "start_pos": 87, "end_pos": 117, "type": "TASK", "confidence": 0.6598586663603783}]}, {"text": "The meta-classification approach results in substantial gains over the classifier-only and language-model-only scenario.", "labels": [], "entities": []}, {"text": "Since the meta-classifier requires error-annotated data for training, we investigate how much training data is needed to improve results over the baseline of not using a meta-classifier.", "labels": [], "entities": []}, {"text": "All evaluations are conducted on a large error-annotated corpus of learner English.", "labels": [], "entities": []}], "introductionContent": [{"text": "Research on the automatic correction of grammatical errors has undergone a renaissance in the past decade.", "labels": [], "entities": [{"text": "automatic correction of grammatical errors", "start_pos": 16, "end_pos": 58, "type": "TASK", "confidence": 0.8397791922092438}]}, {"text": "This is, at least in part, based on the recognition that non-native speakers of English now outnumber native speakers by 2:1 in some estimates, so any tool in this domain could be of tremendous value.", "labels": [], "entities": []}, {"text": "While earlier work in both native and non-native error correction was focused on the construction of grammars and analysis systems to detect and correct specific errors (see fora detailed overview), more recent approaches have been based on data-driven methods.", "labels": [], "entities": [{"text": "error correction", "start_pos": 49, "end_pos": 65, "type": "TASK", "confidence": 0.7557668089866638}]}, {"text": "The majority of the data-driven methods use a classification technique to determine whether a word is used appropriately in its context, continuing the tradition established for contextual spelling correction by and.", "labels": [], "entities": [{"text": "contextual spelling correction", "start_pos": 178, "end_pos": 208, "type": "TASK", "confidence": 0.6256872415542603}]}, {"text": "The words investigated are typically articles and prepositions.", "labels": [], "entities": []}, {"text": "They have two distinct advantages as the subject matter for investigation: They area closed class and they comprise a substantial proportion of learners' errors.", "labels": [], "entities": []}, {"text": "The investigation of preposition corrections can even be narrowed further: amongst the more than 150 English prepositions, the usage of the ten most frequent prepositions accounts for 82% of preposition errors in the 20 million word Cambridge University Press Learners' Corpus.", "labels": [], "entities": [{"text": "preposition corrections", "start_pos": 21, "end_pos": 44, "type": "TASK", "confidence": 0.8767044246196747}, {"text": "Cambridge University Press Learners' Corpus", "start_pos": 233, "end_pos": 276, "type": "DATASET", "confidence": 0.9568309187889099}]}, {"text": "Learning correct article use is most difficult for native speakers of an L1 that does not overtly mark definiteness and indefiniteness as English does.", "labels": [], "entities": []}, {"text": "Prepositions, on the other hand, pose difficulties for language learners from all L1 backgrounds).", "labels": [], "entities": []}, {"text": "Contextual classification methods represent the context of a preposition or article as a feature vector gleaned from a window of a few words around the preposition/article.", "labels": [], "entities": [{"text": "Contextual classification", "start_pos": 0, "end_pos": 25, "type": "TASK", "confidence": 0.7336421459913254}]}, {"text": "Different systems typically vary along three dimensions: choice of features, choice of classifier, and choice of training data.", "labels": [], "entities": []}, {"text": "Features range from words and morphological information ( to the inclusion of part-of-speech tags ( to features based on linguistic analysis and on WordNet. and used decision tree classifiers but, in general, maximum entropy classifiers have become the classification algorithm of choice.", "labels": [], "entities": [{"text": "WordNet.", "start_pos": 148, "end_pos": 156, "type": "DATASET", "confidence": 0.9511115550994873}]}, {"text": "Training data are normally drawn from sizeable corpora of native English text, Wall Street Journal in, a mix of Reuters and Encarta in.", "labels": [], "entities": [{"text": "Wall Street Journal", "start_pos": 79, "end_pos": 98, "type": "DATASET", "confidence": 0.9569153189659119}, {"text": "Reuters", "start_pos": 112, "end_pos": 119, "type": "DATASET", "confidence": 0.9498275518417358}]}, {"text": "In order to partially address the problem of domain mismatch between learners' writing and the news-heavy data sets often used in data-driven NLP applications,) use 31.5 million words from the MetaMetrics corpus, a diverse corpus of fiction, non-fiction and textbooks categorized by reading level.", "labels": [], "entities": []}, {"text": "In addition to the classification approach to error detection, there is a line of research -going back to at least -that uses language models.", "labels": [], "entities": [{"text": "error detection", "start_pos": 46, "end_pos": 61, "type": "TASK", "confidence": 0.699804037809372}]}, {"text": "The idea here is to detect errors in areas where the language model score is suspiciously low.", "labels": [], "entities": []}, {"text": "uses a part-of-speech tag language model to detect errors, use mutual information and chi square statistics to identify unlikely function word and part-of-speech tag sequences, employ a language model based on a generative statistical parser, and investigate a diverse set of language models with different backoff strategies to determine which choice, from a set of confusable words, is most likely in a given context.", "labels": [], "entities": []}, {"text": "use a combination of error-specific classifiers and a large generic language model with handtuned heuristics for combining their scores to maximize precision.", "labels": [], "entities": [{"text": "precision", "start_pos": 148, "end_pos": 157, "type": "METRIC", "confidence": 0.9956249594688416}]}, {"text": "Finally, use n-gram counts from the web as a language model approximation to identify likely errors and correction candidates.", "labels": [], "entities": []}], "datasetContent": [{"text": "To evaluate, we run our meta-classifier system on the preposition and article test sets described in above and calculate precision and recall.", "labels": [], "entities": [{"text": "precision", "start_pos": 121, "end_pos": 130, "type": "METRIC", "confidence": 0.9996669292449951}, {"text": "recall", "start_pos": 135, "end_pos": 141, "type": "METRIC", "confidence": 0.9984026551246643}]}, {"text": "Precision and recall for the overall system are controlled by thresholding the meta-classifier class probability.", "labels": [], "entities": [{"text": "Precision", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.9891719222068787}, {"text": "recall", "start_pos": 14, "end_pos": 20, "type": "METRIC", "confidence": 0.9992596507072449}]}, {"text": "As a point of comparison, we also evaluate the performance of the primary models (the error-specific classifier and the language model) in isolation.", "labels": [], "entities": []}, {"text": "Precision and recall for the error-specific classifier is controlled by thresholding class probability.", "labels": [], "entities": [{"text": "Precision", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.9873358607292175}, {"text": "recall", "start_pos": 14, "end_pos": 20, "type": "METRIC", "confidence": 0.9985846281051636}]}, {"text": "To control the precision-recall tradeoff for the language model, we calculate the difference between the log probabilities of the original user input and the suggested correction.", "labels": [], "entities": [{"text": "precision-recall", "start_pos": 15, "end_pos": 31, "type": "METRIC", "confidence": 0.9972431659698486}]}, {"text": "We then vary that difference across all observed values in small increments, which affects precision and recall: the higher the difference, the fewer instances we find, but the higher the reliability of these instances is.", "labels": [], "entities": [{"text": "precision", "start_pos": 91, "end_pos": 100, "type": "METRIC", "confidence": 0.9994587302207947}, {"text": "recall", "start_pos": 105, "end_pos": 111, "type": "METRIC", "confidence": 0.9992685914039612}, {"text": "reliability", "start_pos": 188, "end_pos": 199, "type": "METRIC", "confidence": 0.9930351972579956}]}, {"text": "This evaluation differs from many of the evaluations reported in the error detection/correction literature in several respects.", "labels": [], "entities": [{"text": "error detection/correction", "start_pos": 69, "end_pos": 95, "type": "TASK", "confidence": 0.8002431467175484}]}, {"text": "First, the test set is abroad random sample across all proficiency levels in the CLC data.", "labels": [], "entities": [{"text": "CLC data", "start_pos": 81, "end_pos": 89, "type": "DATASET", "confidence": 0.920067548751831}]}, {"text": "Second, it is far larger than any sets that have been so far to report results of preposition/article correction on learner data.", "labels": [], "entities": [{"text": "preposition/article correction", "start_pos": 82, "end_pos": 112, "type": "TASK", "confidence": 0.5674367398023605}]}, {"text": "Finally, we are only considering cases in which the system suggests a correction.", "labels": [], "entities": []}, {"text": "In other words, we do not count as correct instances where the system's prediction matches a correct preposition/article.", "labels": [], "entities": []}, {"text": "This evaluation scheme, however, ignores one aspect of areal user scenario.", "labels": [], "entities": []}, {"text": "Of all the suggested changes that are counted as wrong in our evaluation because they do not match an annotated error, some may in fact be innocuous or even helpful fora real user.", "labels": [], "entities": []}, {"text": "Such a situation can arise fora variety of reasons: In some cases, there are legitimate alternative ways to correct an error.", "labels": [], "entities": []}, {"text": "In other cases, the classifier has identified the location of an error although that error is of a different kind (which can be beneficial because it causes the user to make a correction -see ).", "labels": [], "entities": []}, {"text": ", for example manually evaluate preposition suggestions as belonging to one of three categories: (a) properly correcting an existing error, (b) offering a suggestion that neither improves nor degrades the user sentence, (c) offering a suggestion that would degrade the user input.", "labels": [], "entities": []}, {"text": "Obviously, (c) is a more serious error than (b).", "labels": [], "entities": []}, {"text": "Similarly, Tetrault and annotate their test set with preposition choices that are valid alternatives.", "labels": [], "entities": [{"text": "Tetrault", "start_pos": 11, "end_pos": 19, "type": "DATASET", "confidence": 0.7693918943405151}]}, {"text": "We do not have similar information in the CLC data, but we can perform a manual analysis of a random subset of test data to estimate an \"upper bound\" for our precision/recall curve.", "labels": [], "entities": [{"text": "CLC data", "start_pos": 42, "end_pos": 50, "type": "DATASET", "confidence": 0.848892480134964}, {"text": "precision", "start_pos": 158, "end_pos": 167, "type": "METRIC", "confidence": 0.9990625977516174}, {"text": "recall", "start_pos": 168, "end_pos": 174, "type": "METRIC", "confidence": 0.9060231447219849}]}, {"text": "Our annotator manually categorized each suggested correction into one of seven categories.", "labels": [], "entities": []}, {"text": "Details of the distribution of suggested corrections into the seven categories are shown in This analysis involves costly manual evaluation, so we only performed it atone point of the precision/recall curve (our current runtime system setting).", "labels": [], "entities": [{"text": "precision", "start_pos": 184, "end_pos": 193, "type": "METRIC", "confidence": 0.9991632699966431}, {"text": "recall", "start_pos": 194, "end_pos": 200, "type": "METRIC", "confidence": 0.8064330816268921}]}, {"text": "The sample size was 6,000 sentences for prepositions and 5981 sentences for articles (half of the sentences were flagged as containing at least one article/preposition error while the other half were not).", "labels": [], "entities": []}, {"text": "On this manual evaluation, we achieve 32.87% precision if we count all flags that do not perfectly match a CLC annotation as a false positive.", "labels": [], "entities": [{"text": "precision", "start_pos": 45, "end_pos": 54, "type": "METRIC", "confidence": 0.9995166063308716}]}, {"text": "Only counting the last category (introduction of an error) as a false positive, precision is at 85.34%.", "labels": [], "entities": [{"text": "precision", "start_pos": 80, "end_pos": 89, "type": "METRIC", "confidence": 0.9998111128807068}]}, {"text": "Similarly, for articles, the manual estimation arrives at 76.74% precision, where pure CLC annotation matching gives us 33.34%. and show the evaluation results of the meta-classifier for prepositions and articles, compared to the performance of the error-specific classifier and language model alone.", "labels": [], "entities": [{"text": "precision", "start_pos": 65, "end_pos": 74, "type": "METRIC", "confidence": 0.9988225102424622}]}, {"text": "For both prepositions and articles, the first notable observation is that the language model outperforms the classifier by a large margin.", "labels": [], "entities": []}, {"text": "This came as a surprise to us, given the recent prevalence of classification approaches in this area of research and the fact that our classifiers produce state-of-the art performance when compared to other systems, on well-formed data.", "labels": [], "entities": []}, {"text": "Second, the combination of scores from the classifier and language model through a metaclassifier clearly outperforms either one of them in isolation.", "labels": [], "entities": []}, {"text": "This result, again, is consistent across prepositions and articles.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Manual analysis of suggested corrections on  CLC data.", "labels": [], "entities": [{"text": "CLC data", "start_pos": 55, "end_pos": 63, "type": "DATASET", "confidence": 0.860279381275177}]}]}