{"title": [{"text": "Distributed Training Strategies for the Structured Perceptron", "labels": [], "entities": []}], "abstractContent": [{"text": "Perceptron training is widely applied in the natural language processing community for learning complex structured models.", "labels": [], "entities": []}, {"text": "Like all structured prediction learning frameworks, the structured perceptron can be costly to train as training complexity is proportional to inference , which is frequently non-linear in example sequence length.", "labels": [], "entities": []}, {"text": "In this paper we investigate distributed training strategies for the structured perceptron as a means to reduce training times when computing clusters are available.", "labels": [], "entities": []}, {"text": "We look at two strategies and provide convergence bounds fora particular mode of distributed structured perceptron training based on iterative parameter mixing (or averaging).", "labels": [], "entities": []}, {"text": "We present experiments on two structured prediction problems-named-entity recognition and dependency parsing-to highlight the efficiency of this method.", "labels": [], "entities": [{"text": "structured prediction problems-named-entity recognition", "start_pos": 30, "end_pos": 85, "type": "TASK", "confidence": 0.7528222650289536}, {"text": "dependency parsing-to", "start_pos": 90, "end_pos": 111, "type": "TASK", "confidence": 0.7548159956932068}]}], "introductionContent": [{"text": "One of the most popular training algorithms for structured prediction problems in natural language processing is the perceptron).", "labels": [], "entities": []}, {"text": "The structured perceptron has many desirable properties, most notably that there is no need to calculate a partition function, which is necessary for other structured prediction paradigms such as CRFs ().", "labels": [], "entities": []}, {"text": "Furthermore, it is robust to approximate inference, which is often required for problems where the search space is too large and where strong structural independence assumptions are insufficient, such as parsing (; and machine translation ().", "labels": [], "entities": [{"text": "parsing", "start_pos": 204, "end_pos": 211, "type": "TASK", "confidence": 0.9720048308372498}, {"text": "machine translation", "start_pos": 219, "end_pos": 238, "type": "TASK", "confidence": 0.7752024233341217}]}, {"text": "However, like all structured prediction learning frameworks, the structure perceptron can still be cumbersome to train.", "labels": [], "entities": []}, {"text": "This is both due to the increasing size of available training sets as well as the fact that training complexity is proportional to inference, which is frequently nonlinear in sequence length, even with strong structural independence assumptions.", "labels": [], "entities": []}, {"text": "In this paper we investigate distributed training strategies for the structured perceptron as a means of reducing training times when large computing clusters are available.", "labels": [], "entities": []}, {"text": "Traditional machine learning algorithms are typically designed fora single machine, and designing an efficient training mechanism for analogous algorithms on a computing clusteroften via a map-reduce framework) -is an active area of research (.", "labels": [], "entities": []}, {"text": "However, unlike many batch learning algorithms that can easily be distributed through the gradient calculation, a distributed training analog for the perceptron is less clear cut.", "labels": [], "entities": []}, {"text": "It employs online updates and its loss function is technically non-convex.", "labels": [], "entities": []}, {"text": "A recent study by has shown that distributed training through parameter mixing (or averaging) for maximum entropy models can be empirically powerful and has strong theoretical guarantees.", "labels": [], "entities": []}, {"text": "A parameter mixing strategy, which can be applied to any parameterized learning algorithm, trains separate models in parallel, each on a disjoint subset of the training data, and then takes an average of all the parameters as the final model.", "labels": [], "entities": []}, {"text": "In this paper, we provide results which suggest that the perceptron is ill-suited for straight-forward parameter mixing, even though it is commonly used for large-scale structured learning, e.g., for named-entity recognition.", "labels": [], "entities": [{"text": "named-entity recognition", "start_pos": 200, "end_pos": 224, "type": "TASK", "confidence": 0.7468625009059906}]}, {"text": "However, a slight mod-ification we call iterative parameter mixing can be shown to: 1) have similar convergence properties to the standard perceptron algorithm, 2) find a separating hyperplane if the training set is separable, 3) reduce training times significantly, and 4) produce models with comparable (or superior) accuracies to those trained serially on all the data.", "labels": [], "entities": []}], "datasetContent": [{"text": "To investigate the distributed perceptron strategies discussed in Section 4 we look at two structured prediction tasks -named entity recognition and dependency parsing.", "labels": [], "entities": [{"text": "entity recognition", "start_pos": 126, "end_pos": 144, "type": "TASK", "confidence": 0.7089171856641769}, {"text": "dependency parsing", "start_pos": 149, "end_pos": 167, "type": "TASK", "confidence": 0.775349885225296}]}, {"text": "We compare up to four systems: 1.", "labels": [], "entities": []}, {"text": "Serial (All Data): This is the classifier returned if trained serially on all the available data.", "labels": [], "entities": []}, {"text": "2. Serial (Sub Sampling): Shard the data, select one shard randomly and train serially.", "labels": [], "entities": []}, {"text": "For all four systems we compare results for both the standard perceptron algorithm as well as the averaged perceptron algorithm).", "labels": [], "entities": []}, {"text": "We report the final test set metrics of the converged classifiers to determine whether any loss inaccuracy is observed as a consequence of distributed training strategies.", "labels": [], "entities": []}, {"text": "We define convergence as either: 1) the training set is separated, or 2) the training set performance measure (accuracy, f-measure, etc.) does not change by more than some pre-defined threshold on three consecutive epochs.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 111, "end_pos": 119, "type": "METRIC", "confidence": 0.9482925534248352}]}, {"text": "As with most real world data sets, convergence by training set separation was rarely observed, though in both cases training set accuracies approached 100%.", "labels": [], "entities": [{"text": "training set accuracies", "start_pos": 116, "end_pos": 139, "type": "METRIC", "confidence": 0.4811481336752574}]}, {"text": "For both tasks we also plot test set metrics relative to the user wall-clock taken to obtain the classifier.", "labels": [], "entities": []}, {"text": "The results were computed by collecting the metrics at the end of each epoch for every classifier.", "labels": [], "entities": []}, {"text": "All experiments used 10 shards (Section 5.1 looks at convergence relative to different shard size).", "labels": [], "entities": []}, {"text": "Our first experiment is a named-entity recognition task using the English data from the CoNLL 2003 shared-task).", "labels": [], "entities": [{"text": "named-entity recognition task", "start_pos": 26, "end_pos": 55, "type": "TASK", "confidence": 0.7768190999825796}, {"text": "CoNLL 2003 shared-task", "start_pos": 88, "end_pos": 110, "type": "DATASET", "confidence": 0.8446124394734701}]}, {"text": "The task is to detect entities in sentences and label them as one of four types: people, organizations, locations or miscellaneous.", "labels": [], "entities": []}, {"text": "For our experiments we used the entire training set (14041 sentences) and evaluated on the official development set (3250 sentences).", "labels": [], "entities": []}, {"text": "We used a straight-forward IOB label encoding with a 1st order Markov factorization.", "labels": [], "entities": []}, {"text": "Our feature set consisted of predicates extracted over word identities, word affixes, orthography, part-of-speech tags and corresponding concatenations.", "labels": [], "entities": []}, {"text": "The evaluation metric used was micro f-measure over the four entity class types.", "labels": [], "entities": []}, {"text": "There area number of things to observe here: 1) training on a single shard clearly provides inferior performance to training on all data, 2) the simple parameter mixing strategy improves upon a single shard, but does not meet the performance of training on all data, 3) iterative parameter mixing achieves performance as good as or better than training serially on all the data, and 4) the distributed algorithms return better classifiers much quicker than training serially on all the data.", "labels": [], "entities": []}, {"text": "This is true regardless of whether the underlying algorithm is the regular or the averaged perceptron.", "labels": [], "entities": []}, {"text": "Point 3 deserves more discussion.", "labels": [], "entities": []}, {"text": "In particular, the iterative parameter mixing strategy has a higher final f-measure than training on all the data serially than the standard perceptron (f-measure of 87.9 vs. 85.8).", "labels": [], "entities": []}, {"text": "We suspect this happens for two reasons.", "labels": [], "entities": []}, {"text": "First, the parameter mixing has a bagging like effect which helps to reduce the variance of the per-shard classifiers.", "labels": [], "entities": []}, {"text": "Second, the fact that parameter mixing is just a form of parameter averaging perhaps has the same effect as the averaged perceptron.", "labels": [], "entities": []}, {"text": "Our second set of experiments looked at the much more computationally intensive task of dependency parsing.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 88, "end_pos": 106, "type": "TASK", "confidence": 0.8966732025146484}]}, {"text": "We used the Prague Dependency Treebank (PDT)), which is a Czech language treebank and currently one of the largest dependency treebanks in existence.", "labels": [], "entities": [{"text": "Prague Dependency Treebank (PDT))", "start_pos": 12, "end_pos": 45, "type": "DATASET", "confidence": 0.9265855550765991}]}, {"text": "We used the CoNLL-X training (72703 sentences) and testing splits (365 sentences) of this data) and dependency parsing models based on which factors features over pairs of dependency arcs in a tree.", "labels": [], "entities": [{"text": "CoNLL-X training", "start_pos": 12, "end_pos": 28, "type": "DATASET", "confidence": 0.8025558292865753}, {"text": "dependency parsing", "start_pos": 100, "end_pos": 118, "type": "TASK", "confidence": 0.8346831500530243}]}, {"text": "To parse all the sentences in the PDT, one must use a non-projective parsing algorithm, which is a known NP-complete inference problem when not assuming strong independence assumptions.", "labels": [], "entities": []}, {"text": "Thus, the use of approximate inference techniques is common in order to find the highest weighted tree fora sentence.", "labels": [], "entities": []}, {"text": "We use the approximate parsing algorithm given in, which runs in time roughly cubic in sentence length.", "labels": [], "entities": []}, {"text": "To train such a model is computationally expensive and can take on the order of days to train on a single machine.", "labels": [], "entities": []}, {"text": "Unlabeled attachment scores) are given in.", "labels": [], "entities": []}, {"text": "The same trends are seen for dependency parsing that are seen for named-entity recognition.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 29, "end_pos": 47, "type": "TASK", "confidence": 0.8307997286319733}, {"text": "named-entity recognition", "start_pos": 66, "end_pos": 90, "type": "TASK", "confidence": 0.7462251782417297}]}, {"text": "That is, iterative parameter mixing learns classifiers faster and has a final accuracy as good as or better than training serially on all data.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 78, "end_pos": 86, "type": "METRIC", "confidence": 0.9525575041770935}]}, {"text": "Again we see that the iterative parameter mixing model returns a more accurate classifier than the regular perceptron, but at about the same level as the averaged perceptron.", "labels": [], "entities": []}], "tableCaptions": []}