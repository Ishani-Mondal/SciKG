{"title": [{"text": "An Efficient Algorithm for Easy-First Non-Directional Dependency Parsing", "labels": [], "entities": [{"text": "Easy-First Non-Directional Dependency Parsing", "start_pos": 27, "end_pos": 72, "type": "TASK", "confidence": 0.5345080122351646}]}], "abstractContent": [{"text": "We present a novel deterministic dependency parsing algorithm that attempts to create the easiest arcs in the dependency structure first in a non-directional manner.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 33, "end_pos": 51, "type": "TASK", "confidence": 0.7764952480792999}]}, {"text": "Traditional deterministic parsing algorithms are based on a shift-reduce framework: they traverse the sentence from left-to-right and, at each step, perform one of a possible set of actions, until a complete tree is built.", "labels": [], "entities": [{"text": "deterministic parsing", "start_pos": 12, "end_pos": 33, "type": "TASK", "confidence": 0.5802278816699982}]}, {"text": "A drawback of this approach is that it is extremely local: while decisions can be based on complex structures on the left, they can look only at a few words to the right.", "labels": [], "entities": []}, {"text": "In contrast, our algorithm builds a dependency tree by iteratively selecting the best pair of neighbours to connect at each parsing step.", "labels": [], "entities": []}, {"text": "This allows incorporation of features from already built structures both to the left and to the right of the attachment point.", "labels": [], "entities": []}, {"text": "The parser learns both the attachment preferences and the order in which they should be performed.", "labels": [], "entities": []}, {"text": "The result is a determin-istic, best-first, O(nlogn) parser, which is significantly more accurate than best-first transition based parsers, and nears the performance of globally optimized parsing models.", "labels": [], "entities": [{"text": "O", "start_pos": 44, "end_pos": 45, "type": "METRIC", "confidence": 0.9493594765663147}]}], "introductionContent": [{"text": "Dependency parsing has been a topic of active research in natural language processing in the last several years.", "labels": [], "entities": [{"text": "Dependency parsing", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.9035273790359497}, {"text": "natural language processing", "start_pos": 58, "end_pos": 85, "type": "TASK", "confidence": 0.649076501528422}]}, {"text": "An important part of this research effort are the CoNLL 2006 and 2007 shared tasks, which allowed fora comparison of many algorithms and approaches for this task on many languages.", "labels": [], "entities": [{"text": "CoNLL 2006 and 2007 shared tasks", "start_pos": 50, "end_pos": 82, "type": "DATASET", "confidence": 0.8755213618278503}]}, {"text": "* Supported by the Lynn and William Frankel Center for Computer Sciences, Ben Gurion University Current dependency parsers can be categorized into three families: local-and-greedy transitionbased parsers (e.g., MALTPARSER)), globally optimized graph-based parsers (e.g., MSTPARSER), and hybrid systems (e.g.,), which combine the output of various parsers into anew and improved parse, and which are orthogonal to our approach.", "labels": [], "entities": []}, {"text": "Transition-based parsers scan the input from left to right, are fast (O(n)), and can make use of rich feature sets, which are based on all the previously derived structures.", "labels": [], "entities": []}, {"text": "However, all of their decisions are very local, and the strict left-to-right order implies that, while the feature set can use rich structural information from the left of the current attachment point, it is also very restricted in information to the right of the attachment point: traditionally, only the next two or three input tokens are available to the parser.", "labels": [], "entities": []}, {"text": "This limited look-ahead window leads to error propagation and worse performance on root and long distant dependencies relative to graphbased parsers . Graph-based parsers, on the other hand, are globally optimized.", "labels": [], "entities": []}, {"text": "They perform an exhaustive search overall possible parse trees fora sentence, and find the highest scoring tree.", "labels": [], "entities": []}, {"text": "In order to make the search tractable, the feature set needs to be restricted to features over single edges (first-order models) or edges pairs (higher-order models, e.g. ().", "labels": [], "entities": []}, {"text": "There are several attempts at incorporating arbitrary tree-based features but these involve either solving an ILP problem () or using computa- tionally intensive sampling-based methods.", "labels": [], "entities": []}, {"text": "As a result, these models, while accurate, are slow (O(n 3 ) for projective, first-order models, higher polynomials for higher-order models, and worse for richer tree-feature models).", "labels": [], "entities": [{"text": "O", "start_pos": 53, "end_pos": 54, "type": "METRIC", "confidence": 0.9938156604766846}]}, {"text": "We propose anew category of dependency parsing algorithms, inspired by: nondirectional easy-first parsing.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 28, "end_pos": 46, "type": "TASK", "confidence": 0.7727446854114532}, {"text": "nondirectional easy-first parsing", "start_pos": 72, "end_pos": 105, "type": "TASK", "confidence": 0.5872623721758524}]}, {"text": "This is a greedy, deterministic parsing approach, which relaxes the leftto-right processing order of transition-based parsing algorithms.", "labels": [], "entities": []}, {"text": "By doing so, we allow the explicit incorporation of rich structural features derived from both sides of the attachment point, and implicitly take into account the entire previously derived structure of the whole sentence.", "labels": [], "entities": []}, {"text": "This extension allows the incorporation of much richer features than those available to transition-and especially to graph-based parsers, and greatly reduces the locality of transition-based algorithm decisions.", "labels": [], "entities": []}, {"text": "On the other hand, it is still a greedy, best-first algorithm leading to an efficient implementation.", "labels": [], "entities": []}, {"text": "We present a concrete O(nlogn) parsing algorithm, which significantly outperforms state-of-theart transition-based parsers, while closing the gap to graph-based parsers.", "labels": [], "entities": [{"text": "O(nlogn) parsing", "start_pos": 22, "end_pos": 38, "type": "TASK", "confidence": 0.42661307454109193}]}], "datasetContent": [{"text": "We evaluate the parser using the WSJ Treebank.", "labels": [], "entities": [{"text": "WSJ Treebank", "start_pos": 33, "end_pos": 45, "type": "DATASET", "confidence": 0.9903341233730316}]}, {"text": "The trees were converted to dependency structures with the Penn2Malt conversion program, 6 using the headfinding rules from.", "labels": [], "entities": [{"text": "Penn2Malt", "start_pos": 59, "end_pos": 68, "type": "DATASET", "confidence": 0.9623695611953735}]}, {"text": "We use Sections 2-21 for training, Section 22 for development, and Section 23 as the final test set.", "labels": [], "entities": []}, {"text": "The text is automatically POS tagged using a trigram HMM based POS tagger prior to training and parsing.", "labels": [], "entities": [{"text": "POS tagged", "start_pos": 26, "end_pos": 36, "type": "TASK", "confidence": 0.6493375599384308}]}, {"text": "Each section is tagged after training the tagger on all other sections.", "labels": [], "entities": []}, {"text": "The tagging accuracy of the tagger is 96.5 for the training set and 96.8 for the test set.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 12, "end_pos": 20, "type": "METRIC", "confidence": 0.9477531909942627}]}, {"text": "While better taggers exist, we believe that the simpler HMM tagger overfits less, and is more representative of the tagging performance on non-WSJ corpus texts.", "labels": [], "entities": [{"text": "HMM tagger", "start_pos": 56, "end_pos": 66, "type": "TASK", "confidence": 0.8301877975463867}]}, {"text": "Parsers We evaluate our parser against the transition-based MALT parser and the graph-based MST parser.", "labels": [], "entities": []}, {"text": "We use version 1.2 of MALT parser , with the settings used for parsing English in the CoNLL 2007 shared task.", "labels": [], "entities": [{"text": "MALT parser", "start_pos": 22, "end_pos": 33, "type": "TASK", "confidence": 0.5932997167110443}, {"text": "CoNLL 2007 shared task", "start_pos": 86, "end_pos": 108, "type": "DATASET", "confidence": 0.901224210858345}]}, {"text": "For the MST parser 9 , we use the default first-order, projective parser settings, which provide state-of-the-art results for English.", "labels": [], "entities": [{"text": "MST parser 9", "start_pos": 8, "end_pos": 20, "type": "TASK", "confidence": 0.5693548917770386}]}, {"text": "All parsers are trained and tested on the same data.", "labels": [], "entities": []}, {"text": "Our parser is trained for 20 iterations.", "labels": [], "entities": []}, {"text": "Evaluation Measures We evaluate the parsers using three common measures: (unlabeled) Accuracy: percentage of tokens which got assigned their correct parent.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 85, "end_pos": 93, "type": "METRIC", "confidence": 0.9977715611457825}]}, {"text": "Root: The percentage of sentences in which the ROOT attachment is correct.", "labels": [], "entities": [{"text": "Root", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.9658427834510803}, {"text": "ROOT", "start_pos": 47, "end_pos": 51, "type": "METRIC", "confidence": 0.843593955039978}]}, {"text": "Complete: the percentage of sentences in which all tokens were assigned their correct parent.", "labels": [], "entities": [{"text": "Complete", "start_pos": 0, "end_pos": 8, "type": "METRIC", "confidence": 0.9895465970039368}]}, {"text": "Unlike most previous work on English dependency parsing, we do not exclude punctuation marks from the evaluation.", "labels": [], "entities": [{"text": "English dependency parsing", "start_pos": 29, "end_pos": 55, "type": "TASK", "confidence": 0.6122705539067587}]}, {"text": "Our nondirectional easy-first parser significantly outperforms the left-to-right greedy MALT parser in terms of accuracy and root prediction, and significantly outperforms both parsers in terms of exact match.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 112, "end_pos": 120, "type": "METRIC", "confidence": 0.9992330074310303}, {"text": "exact match", "start_pos": 197, "end_pos": 208, "type": "METRIC", "confidence": 0.9330523014068604}]}, {"text": "The globally optimized MST parser is better in rootprediction, and slightly better in terms of accuracy.", "labels": [], "entities": [{"text": "MST parser", "start_pos": 23, "end_pos": 33, "type": "TASK", "confidence": 0.8972186744213104}, {"text": "accuracy", "start_pos": 95, "end_pos": 103, "type": "METRIC", "confidence": 0.998096764087677}]}, {"text": "We evaluated the parsers also on the English dataset from the CoNLL 2007 shared task.", "labels": [], "entities": [{"text": "English dataset from the CoNLL 2007 shared task", "start_pos": 37, "end_pos": 84, "type": "DATASET", "confidence": 0.8688583225011826}]}, {"text": "While this dataset is also derived from the WSJ Treebank, it differs from the previous dataset in two important aspects: it is much smaller in size, and it is created using a different conversion procedure, which is more linguistically adequate.", "labels": [], "entities": [{"text": "WSJ Treebank", "start_pos": 44, "end_pos": 56, "type": "DATASET", "confidence": 0.98705193400383}]}, {"text": "For these experiments, we use the dataset POS tags, and the same parameters as in the previous set of experiments: we train the nondirectional parser for 20 iterations, with the same feature set.", "labels": [], "entities": []}, {"text": "The CoNLL dataset contains some nonprojective constructions.", "labels": [], "entities": [{"text": "CoNLL dataset", "start_pos": 4, "end_pos": 17, "type": "DATASET", "confidence": 0.9550021588802338}]}, {"text": "MALT and MST deal with non-projectivity.", "labels": [], "entities": [{"text": "MALT", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.6127519607543945}]}, {"text": "For the non-directional parser, we projectivize the training set prior to training using the procedure described in.", "labels": [], "entities": []}, {"text": "Results are presented in   While all models suffer from the move to the smaller dataset and the more challenging annotation scheme, the overall story remains the same: the nondirectional parser is better than MALT but not as good as MST in terms of parent-accuracy and root prediction, and is better than both MALT and MST in terms of producing complete correct parses.", "labels": [], "entities": []}, {"text": "That the non-directional parser has lower accuracy but more exact matches than the MST parser can be explained by it being a deterministic parser, and hence still vulnerable to error propagation: once it erred once, it is likely to do so again, resulting in low accuracies for some sentences.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 42, "end_pos": 50, "type": "METRIC", "confidence": 0.9984208345413208}, {"text": "accuracies", "start_pos": 262, "end_pos": 272, "type": "METRIC", "confidence": 0.9682347774505615}]}, {"text": "However, due to the easy-first policy, it manages to parse many sentences without a single error, which lead to higher exact-match scores.", "labels": [], "entities": [{"text": "exact-match scores", "start_pos": 119, "end_pos": 137, "type": "METRIC", "confidence": 0.951526015996933}]}, {"text": "The non-directional parser avoids error propagation by not making the initial error.", "labels": [], "entities": [{"text": "error propagation", "start_pos": 34, "end_pos": 51, "type": "TASK", "confidence": 0.7252059280872345}]}, {"text": "On average, the non-directional parser manages to assign correct heads to over 60% of the tokens before making its first error.", "labels": [], "entities": []}, {"text": "The MST parser would have ranked 5 thin the shared task, and NONDIR would have ranked 7 th . The better ranking systems in the shared task are either higher-order global models, beam-search based systems, or ensemble-based systems, all of which are more complex and less efficient than the NONDIR parser.", "labels": [], "entities": [{"text": "NONDIR", "start_pos": 61, "end_pos": 67, "type": "DATASET", "confidence": 0.8723371624946594}]}, {"text": "Parse Diversity The parses produced by the nondirectional parser are different than the parses produced by the graph-based and left-to-right parsers.", "labels": [], "entities": [{"text": "Parse Diversity", "start_pos": 0, "end_pos": 15, "type": "TASK", "confidence": 0.713016539812088}]}, {"text": "To demonstrate this difference, we performed an Oracle experiment, in which we combine the output of several parsers by choosing, for each sentence, the parse with the highest score.", "labels": [], "entities": []}, {"text": "A non-oracle blending of MALT+MST+NONDIR using simplest combination method assigning each component the same weight, yield an accuracy of 90.8 on the CoNLL 2007 English dataset, making it the highest scoring system among the participants.", "labels": [], "entities": [{"text": "MALT", "start_pos": 25, "end_pos": 29, "type": "METRIC", "confidence": 0.950688898563385}, {"text": "NONDIR", "start_pos": 34, "end_pos": 40, "type": "METRIC", "confidence": 0.9536189436912537}, {"text": "accuracy", "start_pos": 126, "end_pos": 134, "type": "METRIC", "confidence": 0.9993718266487122}, {"text": "CoNLL 2007 English dataset", "start_pos": 150, "end_pos": 176, "type": "DATASET", "confidence": 0.9678758978843689}]}], "tableCaptions": [{"text": " Table 2: Unlabeled dependency accuracy on PTB Section  23, automatic POS-tags, including punctuation.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 31, "end_pos": 39, "type": "METRIC", "confidence": 0.9360906481742859}, {"text": "PTB Section  23", "start_pos": 43, "end_pos": 58, "type": "DATASET", "confidence": 0.9116715391476949}]}, {"text": " Table 3: Unlabeled dependency accuracy on CoNLL  2007 English test set, including punctuation.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 31, "end_pos": 39, "type": "METRIC", "confidence": 0.9108187556266785}, {"text": "CoNLL  2007 English test set", "start_pos": 43, "end_pos": 71, "type": "DATASET", "confidence": 0.9634692311286926}]}, {"text": " Table 4: Parser combination with Oracle, choosing the  highest scoring parse for each sentence of the test-set.", "labels": [], "entities": []}]}