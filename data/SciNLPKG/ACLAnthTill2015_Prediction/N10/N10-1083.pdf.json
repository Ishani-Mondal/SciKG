{"title": [], "abstractContent": [{"text": "We show how features can easily be added to standard generative models for unsuper-vised learning, without requiring complex new training methods.", "labels": [], "entities": []}, {"text": "In particular, each component multinomial of a generative model can be turned into a miniature logistic regression model if feature locality permits.", "labels": [], "entities": []}, {"text": "The intuitive EM algorithm still applies, but with a gradient-based M-step familiar from discrim-inative training of logistic regression models.", "labels": [], "entities": []}, {"text": "We apply this technique to part-of-speech induction, grammar induction, word alignment , and word segmentation, incorporating a few linguistically-motivated features into the standard generative model for each task.", "labels": [], "entities": [{"text": "part-of-speech induction", "start_pos": 27, "end_pos": 51, "type": "TASK", "confidence": 0.7348619103431702}, {"text": "grammar induction", "start_pos": 53, "end_pos": 70, "type": "TASK", "confidence": 0.732381671667099}, {"text": "word alignment", "start_pos": 72, "end_pos": 86, "type": "TASK", "confidence": 0.8065698742866516}, {"text": "word segmentation", "start_pos": 93, "end_pos": 110, "type": "TASK", "confidence": 0.7479712665081024}]}, {"text": "These feature-enhanced models each outper-form their basic counterparts by a substantial margin, and even compete with and surpass more complex state-of-the-art models.", "labels": [], "entities": []}], "introductionContent": [{"text": "Unsupervised learning methods have been increasingly successful in recent NLP research.", "labels": [], "entities": []}, {"text": "The reasons are varied: increased supplies of unlabeled data, improved understanding of modeling methods, additional choices of optimization algorithms, and, perhaps most importantly for the present work, incorporation of richer domain knowledge into structured models.", "labels": [], "entities": []}, {"text": "Unfortunately, that knowledge has generally been encoded in the form of conditional independence structure, which means that injecting it is both tricky (because the connection between independence and knowledge is subtle) and timeconsuming (because new structure often necessitates new inference algorithms).", "labels": [], "entities": []}, {"text": "In this paper, we present a range of experiments wherein we improve existing unsupervised models by declaratively adding richer features.", "labels": [], "entities": []}, {"text": "In particular, we parameterize the local multinomials of existing generative models using features, in away which does not require complex new machinery but which still provides substantial flexibility.", "labels": [], "entities": []}, {"text": "In the featureengineering paradigm, one can worry less about the backbone structure and instead use hand-designed features to declaratively inject domain knowledge into a model.", "labels": [], "entities": []}, {"text": "While feature engineering has historically been associated with discriminative, supervised learning settings, we argue that it can and should be applied more broadly to the unsupervised setting.", "labels": [], "entities": [{"text": "feature engineering", "start_pos": 6, "end_pos": 25, "type": "TASK", "confidence": 0.8251210451126099}]}, {"text": "The idea of using features in unsupervised learning is neither new nor even controversial.", "labels": [], "entities": []}, {"text": "Many top unsupervised results use feature-based models ().", "labels": [], "entities": []}, {"text": "However, such approaches have presented their own barriers, from challenging normalization problems, to neighborhood design, to the need for complex optimization procedures.", "labels": [], "entities": [{"text": "neighborhood design", "start_pos": 104, "end_pos": 123, "type": "TASK", "confidence": 0.7711905837059021}]}, {"text": "As a result, most work still focuses on the stable and intuitive approach of using the EM algorithm to optimize data likelihood in locally normalized, generative models.", "labels": [], "entities": []}, {"text": "The primary contribution of this paper is to demonstrate the clear empirical success of a simple and accessible approach to unsupervised learning with features, which can be optimized by using standard NLP building blocks.", "labels": [], "entities": []}, {"text": "We consider the same generative, locally-normalized models that dominate past work on a range of tasks.", "labels": [], "entities": []}, {"text": "However, we follow,, and, and allow each component multinomial of the model to be a miniature multi-class logistic regression model.", "labels": [], "entities": []}, {"text": "In this case, the EM algorithm still applies with the E-step unchanged.", "labels": [], "entities": []}, {"text": "The M-step involves gradient-based training familiar from standard supervised logistic regression (i.e., maximum entropy models).", "labels": [], "entities": []}, {"text": "By integrating these two familiar learning techniques, we add features to unsupervised models without any specialized learning or inference.", "labels": [], "entities": []}, {"text": "A second contribution of this work is to show that further gains can be achieved by directly optimizing data likelihood with LBFGS (.", "labels": [], "entities": []}, {"text": "This alternative optimization procedure requires no additional machinery beyond what EM uses.", "labels": [], "entities": []}, {"text": "This approach is still very simple to implement, and we found that it empirically outperforms EM.", "labels": [], "entities": []}, {"text": "This paper is largely empirical; the underlying optimization techniques are known, even if the overall approach will be novel to many readers.", "labels": [], "entities": []}, {"text": "As an empirical demonstration, our results span an array of unsupervised learning tasks: part-of-speech induction, grammar induction, word alignment, and word segmentation.", "labels": [], "entities": [{"text": "part-of-speech induction", "start_pos": 89, "end_pos": 113, "type": "TASK", "confidence": 0.7109787166118622}, {"text": "grammar induction", "start_pos": 115, "end_pos": 132, "type": "TASK", "confidence": 0.7229856103658676}, {"text": "word alignment", "start_pos": 134, "end_pos": 148, "type": "TASK", "confidence": 0.8038643896579742}, {"text": "word segmentation", "start_pos": 154, "end_pos": 171, "type": "TASK", "confidence": 0.7528843581676483}]}, {"text": "In each task, we show that declaring a few linguistically motivated feature templates yields state-of-the-art results.", "labels": [], "entities": []}], "datasetContent": [{"text": "We train and test on the entire WSJ tag corpus).", "labels": [], "entities": [{"text": "WSJ tag corpus", "start_pos": 32, "end_pos": 46, "type": "DATASET", "confidence": 0.9092100858688354}]}, {"text": "We attempt the most difficult version of this task where the only information our system can make use of is the unlabeled text itself.", "labels": [], "entities": []}, {"text": "In particular, we do not make use of a tagging dictionary.", "labels": [], "entities": []}, {"text": "We use 45 tag clusters, the number of POS tags that appear in the WSJ corpus.", "labels": [], "entities": [{"text": "WSJ corpus", "start_pos": 66, "end_pos": 76, "type": "DATASET", "confidence": 0.9721771776676178}]}, {"text": "There is an identifiability issue when evaluating inferred tags.", "labels": [], "entities": []}, {"text": "In order to measure accuracy on the hand-labeled corpus, we map each cluster to the tag that gives the highest accuracy, the many-1 evaluation approach.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 20, "end_pos": 28, "type": "METRIC", "confidence": 0.9984198808670044}, {"text": "accuracy", "start_pos": 111, "end_pos": 119, "type": "METRIC", "confidence": 0.9967150688171387}]}, {"text": "We run all POS induction models for 1000 iterations, with 10 random initializations.", "labels": [], "entities": []}, {"text": "The mean and standard deviation of many-1 accuracy appears in.", "labels": [], "entities": [{"text": "standard deviation", "start_pos": 13, "end_pos": 31, "type": "METRIC", "confidence": 0.9076967835426331}, {"text": "accuracy", "start_pos": 42, "end_pos": 50, "type": "METRIC", "confidence": 0.9462807774543762}]}, {"text": "For our English experiments we train and report directed attachment accuracy on portions of the WSJ corpus.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 68, "end_pos": 76, "type": "METRIC", "confidence": 0.9342166781425476}, {"text": "WSJ corpus", "start_pos": 96, "end_pos": 106, "type": "DATASET", "confidence": 0.9602565467357635}]}, {"text": "We work with a standard, reduced version of WSJ, WSJ10, that contains only sentences of length 10 or less after punctuation has been removed.", "labels": [], "entities": [{"text": "WSJ", "start_pos": 44, "end_pos": 47, "type": "DATASET", "confidence": 0.9292052984237671}, {"text": "WSJ10", "start_pos": 49, "end_pos": 54, "type": "DATASET", "confidence": 0.8801592588424683}]}, {"text": "We train on sections 2-21, and use section 22 as a development set.", "labels": [], "entities": []}, {"text": "We report accuracy on section 23.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9997597336769104}]}, {"text": "These are the same training, development, and test sets used by.", "labels": [], "entities": []}, {"text": "The regularization parameter (\u03ba) is tuned on the development set to maximize accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 77, "end_pos": 85, "type": "METRIC", "confidence": 0.9979952573776245}]}, {"text": "For our Chinese experiments, we use the same corpus and training/test split as.", "labels": [], "entities": []}, {"text": "We train on sections 1-270 of the Penn Chinese Treebank (), similarly reduced (CTB10).", "labels": [], "entities": [{"text": "Penn Chinese Treebank", "start_pos": 34, "end_pos": 55, "type": "DATASET", "confidence": 0.9750941395759583}, {"text": "CTB10", "start_pos": 79, "end_pos": 84, "type": "DATASET", "confidence": 0.939153790473938}]}, {"text": "We test on sections 271-300 of CTB10, and use sections 400-454 as a development set.", "labels": [], "entities": [{"text": "CTB10", "start_pos": 31, "end_pos": 36, "type": "DATASET", "confidence": 0.9808809757232666}]}, {"text": "The DMV is known to be sensitive to initialization.", "labels": [], "entities": []}, {"text": "We use the deterministic harmonic initializer from.", "labels": [], "entities": []}, {"text": "We ran each optimization procedure for 100 iterations.", "labels": [], "entities": []}, {"text": "The results are reported in.", "labels": [], "entities": []}, {"text": "We evaluate on the standard hand-aligned portion of the NIST 2002 Chinese-English development set ().", "labels": [], "entities": [{"text": "NIST 2002 Chinese-English development set", "start_pos": 56, "end_pos": 97, "type": "DATASET", "confidence": 0.9748329043388366}]}, {"text": "The set is annotated with sure Sand possible P alignments.", "labels": [], "entities": []}, {"text": "We measure alignment quality using alignment error rate (AER)).", "labels": [], "entities": [{"text": "alignment error rate (AER))", "start_pos": 35, "end_pos": 62, "type": "METRIC", "confidence": 0.9382748206456503}]}, {"text": "We train the models on 10,000 sentences of FBIS Chinese-English newswire.", "labels": [], "entities": [{"text": "FBIS Chinese-English newswire", "start_pos": 43, "end_pos": 72, "type": "DATASET", "confidence": 0.9381177425384521}]}, {"text": "This is not a large-scale experiment, but large enough to be relevant for lowresource languages.", "labels": [], "entities": []}, {"text": "LBFGS experiments are not provided because computing expectations in these models is too computationally intensive to run for many iterations.", "labels": [], "entities": []}, {"text": "Hence, EM training is a more appropriate optimization approach: computing the Mstep gradient requires only summing over word type pairs, while the marginal likelihood gradient needed for LBFGS requires summing over training sentence alignments.", "labels": [], "entities": [{"text": "EM training", "start_pos": 7, "end_pos": 18, "type": "TASK", "confidence": 0.8914718627929688}]}, {"text": "The final alignments, in both the baseline and the feature-enhanced models, are computed by training the generative models in both directions, combining the result with hard union competitive thresholding, and using agreement training for the HMM ().", "labels": [], "entities": []}, {"text": "The combination of these techniques yields a state-of-the-art unsupervised baseline for ChineseEnglish.", "labels": [], "entities": [{"text": "ChineseEnglish", "start_pos": 88, "end_pos": 102, "type": "DATASET", "confidence": 0.9455152750015259}]}, {"text": "We train and test on the phonetic version of the Bernstein-Ratner corpus.", "labels": [], "entities": []}, {"text": "This is the same set-up used by,.", "labels": [], "entities": []}, {"text": "This corpus consists of 9790 child-directed utterances transcribed using a phonetic representation.", "labels": [], "entities": []}, {"text": "We measure segment F1 score on the entire corpus.", "labels": [], "entities": [{"text": "F1 score", "start_pos": 19, "end_pos": 27, "type": "METRIC", "confidence": 0.8667504787445068}]}, {"text": "We run all word segmentation models for 300 iterations with 10 random initializations and report the mean and standard deviation of F1 in.", "labels": [], "entities": [{"text": "word segmentation", "start_pos": 11, "end_pos": 28, "type": "TASK", "confidence": 0.7232728451490402}, {"text": "F1", "start_pos": 132, "end_pos": 134, "type": "METRIC", "confidence": 0.9638909697532654}]}], "tableCaptions": [{"text": " Table 1: Locally normalized feature-based models outperform  all proposed baselines for all four tasks. LBFGS outperformed  EM in all cases where the algorithm was sufficiently fast to run.  Details of each experiment appear in the main text.", "labels": [], "entities": [{"text": "LBFGS", "start_pos": 105, "end_pos": 110, "type": "METRIC", "confidence": 0.8083198666572571}]}]}