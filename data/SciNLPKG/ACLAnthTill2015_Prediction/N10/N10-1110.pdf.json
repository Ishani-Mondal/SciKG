{"title": [{"text": "Investigations into the Crandem Approach to Word Recognition", "labels": [], "entities": [{"text": "Word Recognition", "start_pos": 44, "end_pos": 60, "type": "TASK", "confidence": 0.6891817003488541}]}], "abstractContent": [{"text": "We suggest improvements to a previously proposed framework for integrating Conditional Random Fields and Hidden Markov Models, dubbed a Crandem system (2009).", "labels": [], "entities": []}, {"text": "The previous authors' work suggested that local label posteriors derived from the CRF were too low-entropy for use in word-level automatic speech recognition.", "labels": [], "entities": [{"text": "word-level automatic speech recognition", "start_pos": 118, "end_pos": 157, "type": "TASK", "confidence": 0.6407227963209152}]}, {"text": "As an alternative to the log posterior representation used in their system , we explore frame-level representations derived from the CRF feature functions.", "labels": [], "entities": []}, {"text": "We also describe a weight normalization transformation that leads to increased entropy of the CRF posteriors.", "labels": [], "entities": []}, {"text": "We report significant gains over the previous Crandem system on the Wall Street Journal word recognition task.", "labels": [], "entities": [{"text": "Wall Street Journal", "start_pos": 68, "end_pos": 87, "type": "DATASET", "confidence": 0.9364816745122274}, {"text": "word recognition task", "start_pos": 88, "end_pos": 109, "type": "TASK", "confidence": 0.8677932024002075}]}], "introductionContent": [{"text": "Conditional Random Fields (CRFs) ( have recently emerged as a promising new paradigm in the domain of Automatic Speech Recognition (ASR).", "labels": [], "entities": [{"text": "Automatic Speech Recognition (ASR)", "start_pos": 102, "end_pos": 136, "type": "TASK", "confidence": 0.8015727351109186}]}, {"text": "Unlike Hidden Markov Models (HMMs), CRFs are direct discriminative models: they predict the probability of a label sequence conditioned on the input.", "labels": [], "entities": []}, {"text": "As a result, CRFs can capture long-range dependencies in the data and avoid the need for restrictive independence assumptions.", "labels": [], "entities": []}, {"text": "Variants of CRFs have been successfully used in phone recognition tasks.", "labels": [], "entities": [{"text": "phone recognition tasks", "start_pos": 48, "end_pos": 71, "type": "TASK", "confidence": 0.9052934249242147}]}, {"text": "While the improvements in the phone recognition task are encouraging, recent efforts have been directed towards extending the CRF paradigm to the word recognition level ().", "labels": [], "entities": [{"text": "phone recognition task", "start_pos": 30, "end_pos": 52, "type": "TASK", "confidence": 0.8322592576344808}, {"text": "word recognition", "start_pos": 146, "end_pos": 162, "type": "TASK", "confidence": 0.8339429795742035}]}, {"text": "The Crandem system) is one of the promising approaches in this regard.", "labels": [], "entities": [{"text": "Crandem system", "start_pos": 4, "end_pos": 18, "type": "DATASET", "confidence": 0.8488684892654419}]}, {"text": "The Crandem system is directly inspired by the techniques of the Tandem system (), where phone-label posterior estimates produced by a Multi-Layer Perceptron (MLP) are transformed into a suitable acoustic representation fora standard HMM.", "labels": [], "entities": []}, {"text": "In both systems, the frame-based log posterior vector of P (phone|acoustics) overall phones is decorrelated using the Karhunen-Loeve (KL) transform; unlike MLPs, CRFs take into account the entire label sequence when computing local posteriors.", "labels": [], "entities": [{"text": "frame-based log posterior vector", "start_pos": 21, "end_pos": 53, "type": "METRIC", "confidence": 0.7024200335144997}]}, {"text": "However, posterior estimates from the CRF tend to be overconfident compared to MLP posteriors.", "labels": [], "entities": []}, {"text": "In this paper, we analyze the interplay between the various steps involved in the Crandem process.", "labels": [], "entities": []}, {"text": "Is the local posterior representation from the CRF the best representation?", "labels": [], "entities": []}, {"text": "Given that the CRF posterior estimates can be overconfident, what transformations to the posteriors are appropriate?", "labels": [], "entities": []}, {"text": "In Section 2 we briefly describe CRFs and the Crandem framework.", "labels": [], "entities": [{"text": "Crandem framework", "start_pos": 46, "end_pos": 63, "type": "DATASET", "confidence": 0.9107547998428345}]}, {"text": "We suggest techniques for improving Crandem word recognition performance in Section 3.", "labels": [], "entities": [{"text": "Crandem word recognition", "start_pos": 36, "end_pos": 60, "type": "TASK", "confidence": 0.712154746055603}]}, {"text": "Details of experiments and our results are discussed in Sections 4 and 5 respectively.", "labels": [], "entities": []}, {"text": "We conclude with a discussion of future work in Section 6.", "labels": [], "entities": []}], "datasetContent": [{"text": "To evaluate our proposed techniques, we carried out word recognition experiments on the speakerindependent portion of the Wall Street Journal 5K closed vocabulary task (WSJ0).", "labels": [], "entities": [{"text": "word recognition", "start_pos": 52, "end_pos": 68, "type": "TASK", "confidence": 0.7950874865055084}, {"text": "Wall Street Journal 5K closed vocabulary task (WSJ0)", "start_pos": 122, "end_pos": 174, "type": "DATASET", "confidence": 0.9234212934970856}]}, {"text": "Since the corpus is not phonetically transcribed, we first trained a standard HMM recognition system using PLP features and produced phonetic transcriptions by force aligning the training data.", "labels": [], "entities": [{"text": "HMM recognition", "start_pos": 78, "end_pos": 93, "type": "TASK", "confidence": 0.8651014566421509}]}, {"text": "These were used to train an MLP phone classifier with a softmax output layer, using a 9-frame window of PLPs with 4000 hidden layer units to predict one of the 41 phone labels (including silence and short pause).", "labels": [], "entities": [{"text": "MLP phone classifier", "start_pos": 28, "end_pos": 48, "type": "TASK", "confidence": 0.5535616974035898}]}, {"text": "The linear outputs of the MLP were used to train a baseline Tandem system.", "labels": [], "entities": [{"text": "MLP", "start_pos": 26, "end_pos": 29, "type": "DATASET", "confidence": 0.7118264436721802}]}, {"text": "We then trained a CRF using the MLP linear outputs as its state feature functions.", "labels": [], "entities": []}, {"text": "We extract System Accuracy (%) Crandem-baseline 89.4% 91.8% Crandem-NormMax 91.4% Crandem-Norm5 92.1% Crandem-state 91.7% Crandem-trans 91.0%: Word recognition results on the WSJ0 task local posteriors as well as the two 'local' representations described in Section 3.", "labels": [], "entities": [{"text": "Word recognition", "start_pos": 143, "end_pos": 159, "type": "TASK", "confidence": 0.7412304878234863}, {"text": "WSJ0", "start_pos": 175, "end_pos": 179, "type": "DATASET", "confidence": 0.7450448274612427}]}, {"text": "These input representations were then normalized at the utterance level, before applying a KL-transformation to decorrelate them and reduce dimensionality to 39 dimensions.", "labels": [], "entities": []}, {"text": "Finally, each of these representations was used to train a HMM system with intra-word triphones and 16 Gaussians per mixture using the Hidden Markov Model Toolkit ().", "labels": [], "entities": []}], "tableCaptions": []}