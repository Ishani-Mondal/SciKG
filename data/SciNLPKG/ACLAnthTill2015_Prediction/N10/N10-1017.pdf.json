{"title": [{"text": "Hitting the Right Paraphrases in Good Time", "labels": [], "entities": [{"text": "Hitting the Right Paraphrases", "start_pos": 0, "end_pos": 29, "type": "TASK", "confidence": 0.8807750344276428}]}], "abstractContent": [{"text": "We present a random-walk-based approach to learning paraphrases from bilingual parallel corpora.", "labels": [], "entities": []}, {"text": "The corpora are represented as a graph in which anode corresponds to a phrase, and an edge exists between two nodes if their corresponding phrases are aligned in a phrase table.", "labels": [], "entities": []}, {"text": "We sample random walks to compute the average number of steps it takes to reach a ranking of paraphrases with better ones being \"closer\" to a phrase of interest.", "labels": [], "entities": []}, {"text": "This approach allows \"feature\" nodes that represent domain knowledge to be built into the graph, and incorporates truncation techniques to prevent the graph from growing too large for efficiency.", "labels": [], "entities": []}, {"text": "Current approaches, by contrast, implicitly presuppose the graph to be bipartite, are limited to finding paraphrases that are of length two away from a phrase, and do not generally permit easy incorporation of domain knowledge.", "labels": [], "entities": []}, {"text": "Manual evaluation of generated output shows that our approach outperforms the state-of-the-art system of Callison-Burch (2008).", "labels": [], "entities": []}], "introductionContent": [{"text": "Automatically learning paraphrases, or alternative ways of expressing the same meaning, is an active area of NLP research because of its usefulness in a variety of applications, e.g., question answering (, document summarization (), natural language generation (), machine translation (.", "labels": [], "entities": [{"text": "question answering", "start_pos": 184, "end_pos": 202, "type": "TASK", "confidence": 0.9157392680644989}, {"text": "document summarization", "start_pos": 206, "end_pos": 228, "type": "TASK", "confidence": 0.7232924997806549}, {"text": "natural language generation", "start_pos": 233, "end_pos": 260, "type": "TASK", "confidence": 0.643398771683375}, {"text": "machine translation", "start_pos": 265, "end_pos": 284, "type": "TASK", "confidence": 0.8322888612747192}]}, {"text": "Early work on paraphrase acquisition has focused on using monolingual parallel corpora (;).", "labels": [], "entities": [{"text": "paraphrase acquisition", "start_pos": 14, "end_pos": 36, "type": "TASK", "confidence": 0.9668087065219879}]}, {"text": "While effective, such methods are hampered by the scarcity of monolingual parallel corpora, an obstacle that limits both the quantity and quality of the paraphrases learned.", "labels": [], "entities": []}, {"text": "To address this limitation, focused their attention on the abundance of bilingual parallel corpora.", "labels": [], "entities": []}, {"text": "The crux of this system (referred to below as \"BCB\") is to align phrases in a bilingual parallel corpus and hypothesize English phrases as potential paraphrases if they are aligned to the same phrase in another language (the \"pivot\").", "labels": [], "entities": []}, {"text": "further refines BCB with a system that constrains paraphrases to have the same syntactic structure (Syntactic Bilingual Phrases: SBP).", "labels": [], "entities": []}, {"text": "We take a graphical view of the state-of-the-art BCB and SBP approaches by representing the bilingual parallel corpora as a graph.", "labels": [], "entities": []}, {"text": "A node corresponds to a phrase, and an edge exists between two nodes if their corresponding phrases are aligned.", "labels": [], "entities": []}, {"text": "This graphical form makes the limitations of the BCB/SBP approaches more evident.", "labels": [], "entities": []}, {"text": "The BCB/SBP graph is limited to be bipartite with English nodes on one side and foreign language nodes on the other, and an edge can only exist between nodes on different sides.", "labels": [], "entities": [{"text": "BCB/SBP graph", "start_pos": 4, "end_pos": 17, "type": "DATASET", "confidence": 0.8318894952535629}]}, {"text": "This neglects information between foreign language nodes that may aid in learning paraphrases.", "labels": [], "entities": []}, {"text": "Further, by only considering English nodes that are linked via a foreign language node as potential paraphrases, these approaches will fail to find paraphrases separated by distances greater than length two.", "labels": [], "entities": []}, {"text": "In this paper, we present HTP (Hitting Time Paraphraser), a paraphrase learning approach that is based on random walks and hitting times).", "labels": [], "entities": [{"text": "HTP", "start_pos": 26, "end_pos": 29, "type": "METRIC", "confidence": 0.6745206713676453}]}, {"text": "Hitting time measures the average number of steps one needs to take in a random traversal of a graph before reaching a destination node from a source node.", "labels": [], "entities": []}, {"text": "Intuitively, the smaller the hitting time from a phrase E to E (i.e., the closer E is to E), the more likely it is that E is a good paraphrase of E.", "labels": [], "entities": []}, {"text": "The advantages of HTP are as follows: \u2022 By traversing paths of lengths greater than two, our approach is able to find more paraphrases of a given phrase.", "labels": [], "entities": []}, {"text": "\u2022 We do not require the graph to be bipartite.", "labels": [], "entities": []}, {"text": "Edges can exist between nodes of different foreign languages if their corresponding phrases are aligned.", "labels": [], "entities": []}, {"text": "This allows information from foreign phrase alignments to be used in finding English paraphrases.", "labels": [], "entities": [{"text": "finding English paraphrases", "start_pos": 69, "end_pos": 96, "type": "TASK", "confidence": 0.7714385390281677}]}, {"text": "\u2022 We permit domain knowledge to be easily incorporated as nodes in the graph.", "labels": [], "entities": []}, {"text": "This allows domain knowledge to favor good paraphrases over bad ones, thereby improving performance.", "labels": [], "entities": []}, {"text": "In this paper, we focus on learning English paraphrases.", "labels": [], "entities": [{"text": "learning English paraphrases", "start_pos": 27, "end_pos": 55, "type": "TASK", "confidence": 0.5422414640585581}]}, {"text": "However, our system can be applied to learning paraphrases in any language.", "labels": [], "entities": []}, {"text": "We begin by reviewing random walks and hitting times in the next section.", "labels": [], "entities": []}, {"text": "Then we describe our paraphrase learning algorithm (Section 3), and report our experiments (Section 4).", "labels": [], "entities": []}, {"text": "We discuss related work in Section 5.", "labels": [], "entities": []}, {"text": "Finally, we conclude with future work (Section 6).", "labels": [], "entities": []}], "datasetContent": [{"text": "We conducted experiments to investigate how HTP compares with the state of the art, and to evaluate the contributions of its components.", "labels": [], "entities": [{"text": "HTP", "start_pos": 44, "end_pos": 47, "type": "TASK", "confidence": 0.9417110681533813}]}, {"text": "We used the Europarl dataset ( for our experiments.", "labels": [], "entities": [{"text": "Europarl dataset", "start_pos": 12, "end_pos": 28, "type": "DATASET", "confidence": 0.9955599009990692}]}, {"text": "This dataset contains English transcripts of the proceedings of the European Parliament, and their translations into 10 other European languages.", "labels": [], "entities": []}, {"text": "In the dataset, there are about a million sentences per language, and English sentences are aligned with sentences in the other languages.", "labels": [], "entities": []}, {"text": "aligned English phrases with phrases in each of the other languages using Giza++ (.", "labels": [], "entities": []}, {"text": "We used his English-foreign phrasal alignments which are publicly available on the web at http://ironman.jhu.edu/emnlp08.tar.", "labels": [], "entities": []}, {"text": "In addition, we paired sentences of different non-English languages that correspond to the same English sentence, and aligned the phrases using 5 iterations of IBM model 1 in each direction, followed by 5 iterations of HMM alignment with paired training using the algorithm described in.", "labels": [], "entities": [{"text": "HMM alignment", "start_pos": 219, "end_pos": 232, "type": "TASK", "confidence": 0.7795077264308929}]}, {"text": "We further used the technique of to remove a phrase alignment F -G (where F and G are phrases in different foreign languages) if it was always aligned to different phrases in a third \"bridge\" foreign language.", "labels": [], "entities": []}, {"text": "As observed by Chen et al., this helped to remove spurious alignments.", "labels": [], "entities": []}, {"text": "We used Finnish as the bridge language; when either F or G is Finnish, we used Spanish as the bridge language; when F and G were Finnish and Spanish, we used English as the bridge language.", "labels": [], "entities": []}, {"text": "In our experiments, we used phrases of length 1 to 4 of the following six languages: English, Danish, German, Spanish, Finnish, and Dutch.", "labels": [], "entities": []}, {"text": "All the phrasal alignments between each pair of languages (15 in total) were used as input to HTP and its comparison systems.", "labels": [], "entities": []}, {"text": "A small subset of the remaining phrase alignments were used for tuning parameters.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: HTP vs. SBP.  HTP SBP  Correct top-1 paraphrases  71% 53%  Correct top-k paraphrases  54% 39%  Count of correct paraphrases 420  145  Correct paraphrases  43% 39%", "labels": [], "entities": [{"text": "Count", "start_pos": 105, "end_pos": 110, "type": "METRIC", "confidence": 0.9509468078613281}]}, {"text": " Table 4: HTP vs. HTP without feature nodes.  HTP  HTP- NoFeatNodes  Correct top-1 paraphrases  61%  41%  Correct top-k paraphrases  43%  29%  Count of correct paraphrases 420  283  Correct paraphrases  43%  29%", "labels": [], "entities": []}, {"text": " Table 5: HTP vs. HTP with bipartite graph.", "labels": [], "entities": []}]}