{"title": [], "abstractContent": [{"text": "This paper introduces the novel task of topic coherence evaluation, whereby a set of words, as generated by a topic model, is rated for coherence or interpretability.", "labels": [], "entities": [{"text": "topic coherence evaluation", "start_pos": 40, "end_pos": 66, "type": "TASK", "confidence": 0.6746751666069031}]}, {"text": "We apply a range of topic scoring models to the evaluation task, drawing on WordNet, Wikipedia and the Google search engine, and existing research on lexical similarity/relatedness.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 76, "end_pos": 83, "type": "DATASET", "confidence": 0.9659826755523682}]}, {"text": "In comparison with human scores fora set of learned topics over two distinct datasets, we show a simple co-occurrence measure based on point-wise mutual information over Wikipedia data is able to achieve results for the task at or nearing the level of inter-annotator correlation , and that other Wikipedia-based lexical relatedness methods also achieve strong results.", "labels": [], "entities": []}, {"text": "Google produces strong, if less consistent , results, while our results over WordNet are patchy at best.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 77, "end_pos": 84, "type": "DATASET", "confidence": 0.9571847319602966}]}], "introductionContent": [{"text": "There has traditionally been strong interest within computational linguistics in techniques for learning sets of words (aka topics) which capture the latent semantics of a document or document collection, in the form of methods such as latent semantic analysis), probabilistic latent semantic analysis, random projection (, and more recently, latent Dirichlet allocation ().", "labels": [], "entities": [{"text": "latent semantic analysis", "start_pos": 236, "end_pos": 260, "type": "TASK", "confidence": 0.6305970946947733}, {"text": "probabilistic latent semantic analysis", "start_pos": 263, "end_pos": 301, "type": "TASK", "confidence": 0.6477442607283592}, {"text": "random projection", "start_pos": 303, "end_pos": 320, "type": "TASK", "confidence": 0.6839087754487991}, {"text": "latent Dirichlet allocation", "start_pos": 343, "end_pos": 370, "type": "TASK", "confidence": 0.5433437327543894}]}, {"text": "Such methods have been successfully applied to a myriad of tasks including word sense discrimination, document summarisation, areal linguistic analysis and text segmentation (.", "labels": [], "entities": [{"text": "word sense discrimination", "start_pos": 75, "end_pos": 100, "type": "TASK", "confidence": 0.7056206464767456}, {"text": "document summarisation", "start_pos": 102, "end_pos": 124, "type": "TASK", "confidence": 0.6861655712127686}, {"text": "areal linguistic analysis", "start_pos": 126, "end_pos": 151, "type": "TASK", "confidence": 0.6301063299179077}, {"text": "text segmentation", "start_pos": 156, "end_pos": 173, "type": "TASK", "confidence": 0.7562810778617859}]}, {"text": "In each case, extrinsic evaluation has been used to demonstrate the effectiveness of the learned topics in the application domain, but standardly, no attempt has been made to perform intrinsic evaluation of the topics themselves, either qualitatively or quantitatively.", "labels": [], "entities": []}, {"text": "In machine learning, on the other hand, researchers have modified and extended topic models in a variety of ways, and evaluated intrinsically in terms of model perplexity (), but there has been less effort on qualitative understanding of the semantic nature of the learned topics.", "labels": [], "entities": []}, {"text": "This research seeks to fill the gap between topic evaluation in computational linguistics and machine learning, in developing techniques to perform intrinsic qualitative evaluation of learned topics.", "labels": [], "entities": [{"text": "topic evaluation", "start_pos": 44, "end_pos": 60, "type": "TASK", "confidence": 0.6989727318286896}]}, {"text": "That is, we develop methods for evaluating the quality of a given topic, in terms of its coherence to a human.", "labels": [], "entities": []}, {"text": "After learning topics from a collection of news articles and a collection of books, we ask humans to decide whether individual learned topics are coherent, in terms of their interpretability and association with a single over-arching semantic concept.", "labels": [], "entities": []}, {"text": "We then propose models to predict topic coherence, based on resources such as WordNet, Wikipedia and the Google search engine, and methods ranging from ontological similarity to link overlap and term co-occurrence.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 78, "end_pos": 85, "type": "DATASET", "confidence": 0.9554207921028137}]}, {"text": "Over topics learned from two distinct datasets, we demonstrate that there is remarkable inter-annotator agreement on what is a coherent topic, and additionally that our methods based on Wikipedia are able to achieve nearly perfect agreement with humans over the evaluation of topic coherence.", "labels": [], "entities": []}, {"text": "This research forms part of a larger research agenda on the utility of topic modelling in gisting and visualising document collections, and ultimately enhancing search/discovery interfaces over document collections.", "labels": [], "entities": []}, {"text": "Evaluating topic coherence is a component of the larger question of what are good topics, what characteristics of a document collection make it more amenable to topic modelling, and how can the potential of topic modelling be harnessed for human consumption (Newman et al., to appearb).", "labels": [], "entities": [{"text": "topic modelling", "start_pos": 161, "end_pos": 176, "type": "TASK", "confidence": 0.7484162747859955}]}], "datasetContent": [{"text": "We experiment with scoring methods based on WordNet (Section 4.1), Wikipedia (Section 4.2) and the Google search engine (Section 4.3).", "labels": [], "entities": [{"text": "WordNet", "start_pos": 44, "end_pos": 51, "type": "DATASET", "confidence": 0.9682738780975342}, {"text": "Wikipedia", "start_pos": 67, "end_pos": 76, "type": "DATASET", "confidence": 0.9454430341720581}]}, {"text": "In the case of Google, we query for the entire topic, but with WordNet and Wikipedia, this takes the form of scoring each word-pair in a given topic w based on the component words (w 1 , . .", "labels": [], "entities": [{"text": "WordNet", "start_pos": 63, "end_pos": 70, "type": "DATASET", "confidence": 0.931789755821228}]}, {"text": "Given some (symmetric) word-similarity measure D(w i , w j ), two straightforward ways of producing a combined score from the 45 (i.e. ( ) word-pair scores are: (1) the arithmetic mean, and (2) the median, as follows: Intuitively, the median seems the more natural representation, as it is less affected by outlier scores, but we experiment with both, and fallback to empirical verification of which is the better combination method.", "labels": [], "entities": []}, {"text": "We learned topics for two document collections: a collection of news articles, and a collection of books.", "labels": [], "entities": []}, {"text": "These collections were chosen to produce sets of topics that have more variable quality than one typically observes when topic modeling highly uniform content.", "labels": [], "entities": []}, {"text": "The collection of D = 55, 000 news articles was selected from English Gigaword, and the collection of D = 12, 000 books was downloaded from the Internet Archive.", "labels": [], "entities": [{"text": "English Gigaword", "start_pos": 62, "end_pos": 78, "type": "DATASET", "confidence": 0.8645375370979309}, {"text": "Internet Archive", "start_pos": 144, "end_pos": 160, "type": "DATASET", "confidence": 0.9196139276027679}]}, {"text": "We refer to these collections as NEWS and BOOKS, respectively.", "labels": [], "entities": [{"text": "NEWS", "start_pos": 33, "end_pos": 37, "type": "DATASET", "confidence": 0.8434138298034668}, {"text": "BOOKS", "start_pos": 42, "end_pos": 47, "type": "METRIC", "confidence": 0.9758642315864563}]}, {"text": "Standard procedures were used to tokenize each collection and create the bags-of-words.", "labels": [], "entities": [{"text": "tokenize each collection", "start_pos": 33, "end_pos": 57, "type": "TASK", "confidence": 0.8370569149653116}]}, {"text": "We learned  topic models of NEWS and BOOKS using T = 200 and T = 400 topics respectively.", "labels": [], "entities": [{"text": "BOOKS", "start_pos": 37, "end_pos": 42, "type": "METRIC", "confidence": 0.5592516660690308}]}, {"text": "We randomly selected a total of 237 topics from the two collections for user scoring.", "labels": [], "entities": []}, {"text": "We asked N = 9 users to score each of the 237 topics on a 3-point scale where 3=\"useful\" (coherent) and 1=\"useless\" (less coherent).", "labels": [], "entities": []}, {"text": "We provided annotators with a rubric and guidelines on how to judge whether a topic was useful or useless.", "labels": [], "entities": []}, {"text": "In addition to showing several examples of useful and useless topics, we instructed users to decide whether the topic was to some extent coherent, meaningful, interpretable, subject-heading-like, and something-you-could-easily-label.", "labels": [], "entities": []}, {"text": "For our purposes, the usefulness of a topic can bethought of as whether one could imagine using the topic in a search interface to retrieve documents about a particular subject.", "labels": [], "entities": []}, {"text": "One indicator of usefulness is the ease by which one could think of a short label to describe a topic.", "labels": [], "entities": []}, {"text": "shows a selection of high-and lowscoring topics, as scored by the N = 9 users.", "labels": [], "entities": []}, {"text": "The first topic illustrates the notion of labelling coherence, as space exploration, e.g., would bean obvious label for the topic.", "labels": [], "entities": []}, {"text": "The low-scoring topics display little coherence, and one would not expect them  to be useful as categories or facets in a search interface.", "labels": [], "entities": []}, {"text": "Note that the useless topics from both collections are not chance artifacts produced by the models, but are in fact stable and robust statistical features in the data sets.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Spearman rank correlation \u03c1 values for the  different scoring methods over the NEWS dataset (best- performing method for each resource underlined; best- performing method overall in boldface)", "labels": [], "entities": [{"text": "Spearman rank correlation \u03c1", "start_pos": 10, "end_pos": 37, "type": "METRIC", "confidence": 0.6742511615157127}, {"text": "NEWS dataset", "start_pos": 89, "end_pos": 101, "type": "DATASET", "confidence": 0.9805422425270081}]}, {"text": " Table 3: Spearman rank correlation \u03c1 values for the dif- ferent scoring methods over the BOOKS dataset (best- performing method for each resource underlined; best- performing method overall in boldface)", "labels": [], "entities": [{"text": "Spearman rank correlation \u03c1", "start_pos": 10, "end_pos": 37, "type": "METRIC", "confidence": 0.6582349315285683}, {"text": "BOOKS dataset", "start_pos": 90, "end_pos": 103, "type": "DATASET", "confidence": 0.9674692451953888}]}]}