{"title": [{"text": "The viability of web-derived polarity lexicons", "labels": [], "entities": []}], "abstractContent": [{"text": "We examine the viability of building large polarity lexicons semi-automatically from the web.", "labels": [], "entities": []}, {"text": "We begin by describing a graph propagation framework inspired by previous work on constructing polarity lexicons from lexical graphs (Kim and Hovy, 2004; Hu and Liu, 2004; Esuli and Sabastiani, 2009; Blair-Goldensohn et al., 2008; Rao and Ravichan-dran, 2009).", "labels": [], "entities": [{"text": "graph propagation", "start_pos": 25, "end_pos": 42, "type": "TASK", "confidence": 0.7866527736186981}]}, {"text": "We then apply this technique to build an English lexicon that is significantly larger than those previously studied.", "labels": [], "entities": []}, {"text": "Crucially, this web-derived lexicon does not require WordNet, part-of-speech taggers, or other language-dependent resources typical of sentiment analysis systems.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 53, "end_pos": 60, "type": "DATASET", "confidence": 0.9502147436141968}, {"text": "sentiment analysis", "start_pos": 135, "end_pos": 153, "type": "TASK", "confidence": 0.890952855348587}]}, {"text": "As a result, the lexicon is not limited to specific word classes-e.g., adjectives that occur in WordNet-and in fact contains slang, misspellings, multi-word expressions, etc.", "labels": [], "entities": [{"text": "WordNet-and", "start_pos": 96, "end_pos": 107, "type": "DATASET", "confidence": 0.9418073296546936}]}, {"text": "We evaluate a lexicon derived from English documents, both qualitatively and quantitatively, and show that it provides superior performance to previously studied lexicons, including one derived from WordNet.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 199, "end_pos": 206, "type": "DATASET", "confidence": 0.9399649500846863}]}], "introductionContent": [{"text": "Polarity lexicons are large lists of phrases that encode the polarity of each phrase within it -either positive or negative -often with some score representing the magnitude of the polarity (Hatzivassiloglou and).", "labels": [], "entities": []}, {"text": "Though classifiers built with machine learning algorithms have become commonplace in the sentiment analysis literature, e.g.,, the core of many academic and commercial sentiment analysis systems remains the polarity lexicon, which can be constructed manually, through heuristics () or using machine learning.", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 89, "end_pos": 107, "type": "TASK", "confidence": 0.9088855385780334}, {"text": "sentiment analysis", "start_pos": 168, "end_pos": 186, "type": "TASK", "confidence": 0.7285616993904114}]}, {"text": "Often lexicons are combined with machine learning for improved results).", "labels": [], "entities": []}, {"text": "The pervasiveness and sustained use of lexicons can be ascribed to a number of reasons, including their interpretability in large-scale systems as well as the granularity of their analysis.", "labels": [], "entities": []}, {"text": "In this work we investigate the viability of polarity lexicons that are derived solely from unlabeled web documents.", "labels": [], "entities": []}, {"text": "We propose a method based on graph propagation algorithms inspired by previous work on constructing polarity lexicons from lexical graphs (.", "labels": [], "entities": [{"text": "graph propagation", "start_pos": 29, "end_pos": 46, "type": "TASK", "confidence": 0.8147702217102051}]}, {"text": "Whereas past efforts have used linguistic resources -e.g., WordNet -to construct the lexical graph over which propagation runs, our lexicons are constructed using a graph built from co-occurrence statistics from the entire web.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 59, "end_pos": 66, "type": "DATASET", "confidence": 0.9284838438034058}]}, {"text": "Thus, the method we investigate can be seen as a combination of methods for propagating sentiment across lexical graphs and methods for building sentiment lexicons based on distributional characteristics of phrases in raw data).", "labels": [], "entities": []}, {"text": "The advantage of breaking the dependence on WordNet (or related resources like thesauri () is that it allows the lexicons to include non-standard entries, most notably spelling mistakes and variations, slang, and multiword expressions.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 44, "end_pos": 51, "type": "DATASET", "confidence": 0.9700230360031128}]}, {"text": "The primary goal of our study is to understand the characteristics and practical usefulness of such a lexicon.", "labels": [], "entities": []}, {"text": "Towards this end, we provide both a qualitative and quantitative analysis fora web-derived English lexicon relative to two previously published lexicons -the lexicon used in and the lexicon used in Blair-.", "labels": [], "entities": []}, {"text": "Our experiments show that a web-derived lexicon is not only significantly larger, but has improved accuracy on a sentence polarity classification task, which is an important problem in many sentiment analysis applications, including sentiment aggregation and summarization (.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 99, "end_pos": 107, "type": "METRIC", "confidence": 0.9989025592803955}, {"text": "sentence polarity classification task", "start_pos": 113, "end_pos": 150, "type": "TASK", "confidence": 0.8228979408740997}, {"text": "sentiment analysis", "start_pos": 190, "end_pos": 208, "type": "TASK", "confidence": 0.8712528049945831}, {"text": "sentiment aggregation", "start_pos": 233, "end_pos": 254, "type": "TASK", "confidence": 0.8279668092727661}, {"text": "summarization", "start_pos": 259, "end_pos": 272, "type": "TASK", "confidence": 0.9853926301002502}]}, {"text": "These results hold true both when the lexicons are used in conjunction with string matching to classify sentences, and when they are included within a contextual classifier framework ().", "labels": [], "entities": []}, {"text": "Extracting polarity lexicons from the web has been investigated previously by, who study the problem exclusively for Japanese.", "labels": [], "entities": [{"text": "Extracting polarity lexicons from the web", "start_pos": 0, "end_pos": 41, "type": "TASK", "confidence": 0.8805375099182129}]}, {"text": "In that work a set of positive/negative sentences are first extracted from the web using cues from a syntactic parser as well as the document structure.", "labels": [], "entities": []}, {"text": "Adjectives phrases are then extracted from these sentences based on different statistics of their occurrence in the positive or negative set.", "labels": [], "entities": []}, {"text": "Our work, on the other hand, does not rely on syntactic parsers or restrict the set of candidate lexicon entries to specific syntactic classes, i.e., adjective phrases.", "labels": [], "entities": []}, {"text": "As a result, the lexicon builtin our study is on a different scale than that examined in.", "labels": [], "entities": []}, {"text": "Though this hypothesis is not tested here, it also makes our techniques more amenable to adaptation for other languages.", "labels": [], "entities": []}], "datasetContent": [{"text": "We ran the best path graph propagation algorithm over a graph constructed from the web using manually constructed positive and negative seed sets of 187 and 192 words in size, respectively.", "labels": [], "entities": [{"text": "path graph propagation", "start_pos": 16, "end_pos": 38, "type": "TASK", "confidence": 0.726673444112142}]}, {"text": "These words were generated by a set of five humans and many are morphological variants of the same root, e.g., excel/excels/excelled.", "labels": [], "entities": []}, {"text": "The algorithm produced a lexicon that contained 178,104 entries.", "labels": [], "entities": []}, {"text": "Depending on the threshold \u03b3 (see), this lexicon could be larger or smaller.", "labels": [], "entities": []}, {"text": "As stated earlier, our selection of \u03b3 and all hyperparameters was based on manual inspection of the resulting lexicons and performance on held-out data.", "labels": [], "entities": []}, {"text": "In the rest of this section we investigate the properties of this lexicon to understand both its general characteristics as well as its possible utility in sentiment applications.", "labels": [], "entities": []}, {"text": "To this end we compare three different lexicons: 1.", "labels": [], "entities": []}, {"text": "Wilson et al.: Described in.", "labels": [], "entities": []}, {"text": "Lexicon constructed by combining the lexicon builtin with other sources 1 . Entries are are coarsely rated -strong/weak positive/negative -which we weighted as 1.0, 0.5, -0.5, and -1.0 for our experiments.", "labels": [], "entities": []}, {"text": "2. WordNet LP: Described in Blair-.", "labels": [], "entities": [{"text": "WordNet LP: Described in Blair-", "start_pos": 3, "end_pos": 34, "type": "DATASET", "confidence": 0.9442352993147713}]}, {"text": "Constructed using label propagation over a graph derived from WordNet synonym and antonym links.", "labels": [], "entities": [{"text": "label propagation", "start_pos": 18, "end_pos": 35, "type": "TASK", "confidence": 0.7101821303367615}]}, {"text": "Note that label propagation is not prone to the kinds of errors discussed in Section 2.3 since the lexical graph is derived from a high quality source.", "labels": [], "entities": [{"text": "label propagation", "start_pos": 10, "end_pos": 27, "type": "TASK", "confidence": 0.8316785991191864}]}, {"text": "3. Web GP: The web-derived lexicon described in Section 2.1 and Section 2.2.", "labels": [], "entities": []}, {"text": "Table 1 breaks down the lexicon by the number of positive and negative entries of each lexicon, which clearly shows that the lexicon derived from the web is more than an order of magnitude larger than previously constructed lexicons.", "labels": [], "entities": []}, {"text": "This in and of itself is not much of an achievement if the additional phrases are of poor quality.", "labels": [], "entities": []}, {"text": "However, in Section 3.2 we present an empirical evaluation that suggests that these terms provide both additional and useful information.", "labels": [], "entities": []}, {"text": "also shows the recall of the each lexicon relative to the other.", "labels": [], "entities": [{"text": "recall", "start_pos": 15, "end_pos": 21, "type": "METRIC", "confidence": 0.9988318085670471}]}, {"text": "Whereas the and WordNet lexicon have a recall of only 3% relative to the web lexicon, the web lexicon has a recall of 48% and 70% relative to the two other lexicons, indicating that it contains a significant amount of information from the other lexicons.", "labels": [], "entities": [{"text": "WordNet lexicon", "start_pos": 16, "end_pos": 31, "type": "DATASET", "confidence": 0.9658180177211761}, {"text": "recall", "start_pos": 39, "end_pos": 45, "type": "METRIC", "confidence": 0.9986414313316345}, {"text": "recall", "start_pos": 108, "end_pos": 114, "type": "METRIC", "confidence": 0.9972289204597473}]}, {"text": "However, this overlap is still small, suggesting that a combination of all the lexicons could provide the best performance.", "labels": [], "entities": []}, {"text": "In Section 3.2 we investigate this empirically through a meta classification system.", "labels": [], "entities": []}, {"text": "shows the distribution of phrases in the web-derived lexicon relative to the number of tokens in each phrase.", "labels": [], "entities": []}, {"text": "Here a token is simply defined by whitespace and punctuation, with punctuation counting as a token, e.g., \"half-baked\" is counted as 3 tokens.", "labels": [], "entities": []}, {"text": "For the most part, we see what one might expect, as the number of tokens increases, the number of corresponding phrases in the lexicon also decreases.", "labels": [], "entities": []}, {"text": "Longer phrases are less frequent and thus will have both fewer and lower weighted edges to adjacent nodes in the graph.", "labels": [], "entities": []}, {"text": "There is a single phrase of length 9, which is \"motion to dismiss for failure to state a claim\".", "labels": [], "entities": []}, {"text": "In fact, the lexicon contains quite a number of legal and medical phrases.", "labels": [], "entities": []}, {"text": "This should not be surprising, since in a graph induced from the web, a phrase like \"cancer\" (or any disease) should be distributionally similar to phrases like \"illness\", \"sick\", and \"death\", which themselves will be similar to standard sentiment phrases like \"bad\" and \"terrible\".", "labels": [], "entities": []}, {"text": "These terms are predominantly negative in the lexicon representing the broad notion that legal and medical events are undesirable.", "labels": [], "entities": []}, {"text": "Perhaps the most interesting characteristic of the lexicon is that the most frequent phrase length is 2 and not 1.", "labels": [], "entities": []}, {"text": "The primary reason for this is an abundance of adjective phrases consisting of an adverb and an adjective, such as \"more brittle\" and \"less brittle\".", "labels": [], "entities": []}, {"text": "Almost every adjective of length 1 is frequently combined in such away on the web, so it not surprising that we see many of these phrases in the lexicon.", "labels": [], "entities": []}, {"text": "Ideally we would see an order on such phrases, e.g., \"more brittle\" has a larger negative polarity than \"brittle\", which in turn has a larger negative polarity than \"less brittle\".", "labels": [], "entities": []}, {"text": "However, this is rarely the case and usually the adjective has the highest polarity magnitude.", "labels": [], "entities": []}, {"text": "Again, this is easily explained.", "labels": [], "entities": []}, {"text": "These phrases are necessarily more common and will thus have more edges with larger weights in the graph and thus a greater chance of accumulating a high sentiment score.", "labels": [], "entities": []}, {"text": "The prominence of such phrases suggests that a more principled treatment of them should be investigated in the future.", "labels": [], "entities": []}, {"text": "Finally, presents a selection of phrases from both the positive and negative lexicons categorized into revealing verticals.", "labels": [], "entities": []}, {"text": "For both positive and negative phrases we present typical examples of phrases -usually adjectives -that one would expect to be in a sentiment lexicon.", "labels": [], "entities": []}, {"text": "These are phrases not included in the seed sets.", "labels": [], "entities": []}, {"text": "We also present multiword phrases for both positive and negative cases, which displays concretely the advantage of building lexicons from the web as opposed to using restricted linguistic resources such as WordNet.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 206, "end_pos": 213, "type": "DATASET", "confidence": 0.9414108395576477}]}, {"text": "Finally, we show two special cases.", "labels": [], "entities": []}, {"text": "The first is spelling variations (and mistakes) for positive phrases, which were far more prominent than for negative phrases.", "labels": [], "entities": []}, {"text": "Many of these correspond to social media text where one expresses an increased level of sentiment by repeating characters.", "labels": [], "entities": []}, {"text": "The second is vulgarity in negative phrases, which was far more prominent than for positive phrases.", "labels": [], "entities": []}, {"text": "Some of these are clearly appropri-  ate, e.g., \"shitty\", but some are clearly insults and outbursts that are most likely included due to their co-occurrence with angry texts.", "labels": [], "entities": []}, {"text": "There were also a number of derogatory terms and racial slurs in the lexicon, again most of which received negative sentiment due to their typical disparaging usage.", "labels": [], "entities": []}, {"text": "To determine the practical usefulness of a polarity lexicon derived from the web, we measured the performance of the lexicon on a sentence classification/ranking task.", "labels": [], "entities": [{"text": "sentence classification/ranking task", "start_pos": 130, "end_pos": 166, "type": "TASK", "confidence": 0.8348381280899048}]}, {"text": "The input is a set of sentences and the output is a classification of the sentences as being either positive, negative or neutral in sentiment.", "labels": [], "entities": []}, {"text": "Additionally, the system outputs two rankings, the first a ranking of the sentence by positive polarity and the second a ranking of the sentence by negative polarity.", "labels": [], "entities": []}, {"text": "Classifying sentences by their sentiment is a subtask of sentiment aggregation systems ().", "labels": [], "entities": []}, {"text": "Ranking sentences by their polarity is a critical sub-task in extractive sentiment summarization).", "labels": [], "entities": [{"text": "extractive sentiment summarization", "start_pos": 62, "end_pos": 96, "type": "TASK", "confidence": 0.7630443175633749}]}, {"text": "To classify sentences as being positive, negative or neutral, we used an augmented vote-flip algorithm (, which is given in.", "labels": [], "entities": []}, {"text": "This intuition behind this algorithm is simple.", "labels": [], "entities": []}, {"text": "The number of matched positive and negative phrases from the lexicon are counted and whichever has the most votes wins.", "labels": [], "entities": []}, {"text": "The algorithm flips the decision if the number of negations is odd.", "labels": [], "entities": []}, {"text": "Though this algorithm appears crude, it benefits from not relying on threshold values for neutral classification, which is difficult due to the fact that the polarity scores in the three lexicons are not on the same scale.", "labels": [], "entities": [{"text": "neutral classification", "start_pos": 90, "end_pos": 112, "type": "TASK", "confidence": 0.7366925776004791}]}, {"text": "To rank sentences we defined the purity of a sentence X as the normalized sum of the sentiment scores for each phrase x in the sentence: This is a normalized score in the range.", "labels": [], "entities": []}, {"text": "Intuitively, sentences with many terms of the same polarity will have purity scores at the extreme points of the range.", "labels": [], "entities": [{"text": "purity", "start_pos": 70, "end_pos": 76, "type": "METRIC", "confidence": 0.9907786846160889}]}, {"text": "Before calculating purity, a simple negation heuristic was implemented that reversed the sentiment scores of terms that were within the scope of negations.", "labels": [], "entities": [{"text": "purity", "start_pos": 19, "end_pos": 25, "type": "METRIC", "confidence": 0.9499713778495789}]}, {"text": "The term \u03b4 helps to favor sentences with multiple phrase matches.", "labels": [], "entities": []}, {"text": "Purity is a common metric used for ranking sentences for inclusion in sentiment summaries ().", "labels": [], "entities": [{"text": "Purity", "start_pos": 0, "end_pos": 6, "type": "METRIC", "confidence": 0.9829133152961731}, {"text": "inclusion in sentiment summaries", "start_pos": 57, "end_pos": 89, "type": "TASK", "confidence": 0.751502513885498}]}, {"text": "Purity and negative purity were used to rank sentences as being positive and negative sentiment, respectively.", "labels": [], "entities": [{"text": "Purity", "start_pos": 0, "end_pos": 6, "type": "METRIC", "confidence": 0.961310863494873}]}, {"text": "The data used in our initial English-only experi-: Positive and negative precision (P), recall (R), and average precision (AP) for three lexicons using either lexical matching or contextual classification strategies.", "labels": [], "entities": [{"text": "precision (P)", "start_pos": 73, "end_pos": 86, "type": "METRIC", "confidence": 0.9192236810922623}, {"text": "recall (R)", "start_pos": 88, "end_pos": 98, "type": "METRIC", "confidence": 0.9641458839178085}, {"text": "average precision (AP)", "start_pos": 104, "end_pos": 126, "type": "METRIC", "confidence": 0.941310453414917}]}, {"text": "\u2020 Web GP is statistically significantly better than Wilson et al. and WordNet LP (p < 0.05).", "labels": [], "entities": [{"text": "WordNet LP", "start_pos": 70, "end_pos": 80, "type": "DATASET", "confidence": 0.9657630622386932}]}, {"text": "\u2021 Meta Classifier is statistically significantly better than all other systems (p < 0.05).", "labels": [], "entities": []}, {"text": "Input: Scored lexicon pol, negation list NG, input sentence X Output: sentiment \u2208 {POS, NEG, NEU}", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Number of phrases by phrase length in lexicon  built from the web.", "labels": [], "entities": []}, {"text": " Table 1: Lexicon statistics. Wilson et al. is the lexicon used in Wilson et al. (2005), WordNet LP is the lexicon  constructed by Blair-Goldensohn et al. (2008) that uses label propagation algorithms over a graph constructed through  WordNet, and Web GP is the web-derived lexicon from this study.", "labels": [], "entities": [{"text": "WordNet LP", "start_pos": 89, "end_pos": 99, "type": "DATASET", "confidence": 0.8786106705665588}, {"text": "label propagation", "start_pos": 172, "end_pos": 189, "type": "TASK", "confidence": 0.7375566959381104}]}, {"text": " Table 4: Positive and negative precision (P), recall (R), and average precision (AP) for three lexicons using either  lexical matching or contextual classification strategies.  \u2020 Web GP is statistically significantly better than Wilson et al.  and WordNet LP (p < 0.05).  \u2021 Meta Classifier is statistically significantly better than all other systems (p < 0.05).", "labels": [], "entities": [{"text": "precision (P)", "start_pos": 32, "end_pos": 45, "type": "METRIC", "confidence": 0.9260547161102295}, {"text": "recall (R)", "start_pos": 47, "end_pos": 57, "type": "METRIC", "confidence": 0.9546945095062256}, {"text": "average precision (AP)", "start_pos": 63, "end_pos": 85, "type": "METRIC", "confidence": 0.9440510630607605}, {"text": "WordNet LP", "start_pos": 249, "end_pos": 259, "type": "DATASET", "confidence": 0.935566633939743}]}]}