{"title": [{"text": "Taxonomy Learning Using Word Sense Induction", "labels": [], "entities": []}], "abstractContent": [{"text": "Taxonomies are an important resource fora variety of Natural Language Processing (NLP) applications.", "labels": [], "entities": []}, {"text": "Despite this, the current state-of-the-art methods in taxonomy learning have disregarded word polysemy, in effect, developing taxonomies that conflate word senses.", "labels": [], "entities": []}, {"text": "In this paper, we present an unsupervised method that builds a taxonomy of senses learned automatically from an unlabelled corpus.", "labels": [], "entities": []}, {"text": "Our evaluation on two WordNet-derived taxonomies shows that the learned taxonomies capture a higher number of correct taxonomic relations compared to those produced by traditional distributional similarity approaches that merge senses by grouping the features of each word into a single vector.", "labels": [], "entities": []}], "introductionContent": [{"text": "A concept or a sense, s, can be defined as the meaning of a word or a multiword expression.", "labels": [], "entities": []}, {"text": "A concept scan be linguistically realised by more than one word while at the same time a word w can be the linguistic realisation of more than one concept.", "labels": [], "entities": []}, {"text": "Given a set of concepts S, taxonomy learning is the task of hierarchically classifying the elements in S in an automatic manner.", "labels": [], "entities": [{"text": "taxonomy learning", "start_pos": 27, "end_pos": 44, "type": "TASK", "confidence": 0.8716092109680176}]}, {"text": "For example, consider a set of concepts linguistically realised by the words/multiword expressions LAN, computer network, internet, meshwork, gauze, snood.", "labels": [], "entities": []}, {"text": "Taxonomy learning methods produce taxonomies, such as the ones shown in By observing (a), we can express IS-A statements, such as Internet IS-A Computer Network etc.", "labels": [], "entities": []}, {"text": "However, the same does not apply to the , since this taxonomy is not fully labelled.", "labels": [], "entities": []}, {"text": "Despite this, its hierarchical organisation clearly shows that the concepts are divided into groups, which are further subdivided into subgroups and so forth, until we reach a level where each concept belongs to its own group.", "labels": [], "entities": []}, {"text": "Unlabelled taxonomies are typically produced by agglomerative hierarchical clustering algorithms.", "labels": [], "entities": []}, {"text": "The knowledge encoded in taxonomies can be utilised in a range of NLP applications.", "labels": [], "entities": []}, {"text": "For instance, taxonomies can be used in information retrieval to expand a user query with semantically related words or to enhance document representation by abstracting from plain words and adding conceptual information).", "labels": [], "entities": [{"text": "information retrieval", "start_pos": 40, "end_pos": 61, "type": "TASK", "confidence": 0.7239721119403839}]}, {"text": "WordNet's) taxonomic relations have also been used in Word Sense Disambiguation (WSD)).", "labels": [], "entities": [{"text": "WordNet", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.9553962349891663}, {"text": "Word Sense Disambiguation (WSD))", "start_pos": 54, "end_pos": 86, "type": "TASK", "confidence": 0.7926430900891622}]}, {"text": "In named entity recognition, methods relying on gazetteers could make use of automatically acquired taxonomies), while question answering systems have also benefited.", "labels": [], "entities": [{"text": "named entity recognition", "start_pos": 3, "end_pos": 27, "type": "TASK", "confidence": 0.6387615899244944}, {"text": "question answering", "start_pos": 119, "end_pos": 137, "type": "TASK", "confidence": 0.8897674083709717}]}, {"text": "Despite the wide uses of taxonomies, the majority of methods disregard or do not deal effectively with word polysemy, in effect, developing taxonomies that conflate the senses of words (see Section 2).", "labels": [], "entities": []}, {"text": "In this work, we show that Word Sense Induction (WSI) can be effectively employed to address this limitation of existing methods.", "labels": [], "entities": [{"text": "Word Sense Induction (WSI", "start_pos": 27, "end_pos": 52, "type": "TASK", "confidence": 0.7254662573337555}]}, {"text": "We present a novel method that employs WSI to generate the different senses of a set of target words from an unlabelled corpus and then produces a taxonomy of senses using Hierarchical Agglomerative Clustering (HAC).", "labels": [], "entities": []}, {"text": "We evaluate our method on two WordNetderived sub-taxonomies and show that our method leads to the development of concept hierarchies that capture a higher number of correct taxonomic relations in comparison to those generated by current distributional similarity approaches.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluate our method with respect to two WordNet-derived sub-taxonomies (Section 4.3).", "labels": [], "entities": []}, {"text": "For that reason, it is necessary to map the induced senses to WordNet before applying HAC.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 62, "end_pos": 69, "type": "DATASET", "confidence": 0.9740352630615234}]}, {"text": "Note that the mapping process might map more than one induced senses to the same WordNet sense.", "labels": [], "entities": [{"text": "WordNet sense", "start_pos": 81, "end_pos": 94, "type": "DATASET", "confidence": 0.9240088164806366}]}, {"text": "In that case, these induced senses are merged to a single one along with their corresponding collocations.", "labels": [], "entities": []}, {"text": "For the purposes of this section we present one gold standard taxonomy) and a second derived from our method).", "labels": [], "entities": []}, {"text": "The comparison of these taxonomies is based on the semantic cotopy of anode, which has also been used in).", "labels": [], "entities": []}, {"text": "In particular, the semantic cotopy of anode is defined as the set of all its super-and subnodes excluding the root and including that node.", "labels": [], "entities": []}, {"text": "For example, the semantic cotopy of computer network in is {computer network, internet, LAN}.", "labels": [], "entities": []}, {"text": "There are two issues, which make the evaluation difficult.", "labels": [], "entities": []}, {"text": "The first one is that HAC produces a taxonomy in which all internal nodes are unlabelled, as opposed to the gold standard taxonomy.", "labels": [], "entities": []}, {"text": "In, we have manually labelled internal nodes with their IDs for clarity.", "labels": [], "entities": []}, {"text": "For example, the semantic cotopy of the node New Cluster 1 in is {computer network, internet, LAN, New Cluster 1, New Cluster 0}.", "labels": [], "entities": []}, {"text": "By comparing the cotopies of nodes computer network in (a) and New Cluster 1 in, we observe that the automatic method has successfully grouped all of the hypernyms and hyponyms of computer network under New Cluster 1.", "labels": [], "entities": []}, {"text": "However, the corresponding cotopies are not identical, because the cotopy of New Cluster 1 also includes the labels produced by HAC.", "labels": [], "entities": [{"text": "New Cluster 1", "start_pos": 77, "end_pos": 90, "type": "DATASET", "confidence": 0.9326532085736593}, {"text": "HAC", "start_pos": 128, "end_pos": 131, "type": "DATASET", "confidence": 0.9687966704368591}]}, {"text": "To deal with this problem, we use aversion of semantic cotopy for nodes in the automatically learned taxonomy which excludes nodes that do not exist in WordNet.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 152, "end_pos": 159, "type": "DATASET", "confidence": 0.9587640166282654}]}, {"text": "That way the semantic cotopies of New Cluster 1 in (b) and computer network in (a) will yield maximum similarity.", "labels": [], "entities": [{"text": "similarity", "start_pos": 102, "end_pos": 112, "type": "METRIC", "confidence": 0.9637731313705444}]}, {"text": "The second issue is that the nodes that exist in the gold standard taxonomy are leaf nodes in the automatically learned taxonomy.", "labels": [], "entities": []}, {"text": "As a result, the semantic cotopy of LAN in is {LAN} since all of its supernodes do not exist in WordNet.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 96, "end_pos": 103, "type": "DATASET", "confidence": 0.9554662704467773}]}, {"text": "In contrast, the semantic cotopy of LAN in (a) is {LAN, computer network}.", "labels": [], "entities": []}, {"text": "We observe that there is an overlap between the two cotopies derived by the existence of the same concept in both taxonomies, i.e. LAN.", "labels": [], "entities": []}, {"text": "In fact, all of the leaf nodes of a learned taxonomy will have a small overlap with the corresponding concept in the gold standard.", "labels": [], "entities": []}, {"text": "For this problem, we observe that in our automatically learned taxonomies it does not make sense to calculate the semantic cotopy of leaf nodes.", "labels": [], "entities": []}, {"text": "On the contrary, we need to evaluate the internal nodes that group the leaf nodes.", "labels": [], "entities": []}, {"text": "Let us assume the following notation: TA = automatically learned taxonomy \u03b7 i = node in a taxonomy C(T A ) = internal nodes + leaf nodes of TA I(T A ) = internal nodes of TA T G = gold standard taxonomy C(T G ) = internal nodes + leaf nodes of T G I(T G ) = internal nodes of T G hyper(\u03b7 i ) = supernodes of \u03b7 i excluding the root hypo(\u03b7 i ) = subnodes of \u03b7 i including \u03b7 i For \u03b7 i \u2208 I(T A ), the semantic cotopy is defined as: Precision, recall and harmonic mean of node \u03b7 i \u2208 I(T A ) with respect to node \u03b7 j \u2208 C(T G ) are defined in Equations 1, 2 and 3.", "labels": [], "entities": [{"text": "Precision", "start_pos": 428, "end_pos": 437, "type": "METRIC", "confidence": 0.9948432445526123}, {"text": "recall", "start_pos": 439, "end_pos": 445, "type": "METRIC", "confidence": 0.9980236291885376}]}, {"text": "The F-score, F S, of node \u03b7 i \u2208 I(T A ) is the maximum F attained at any . Finally, the similarity T S of the entire taxonomy to the gold standard taxonomy is the average of the F-scores of each \u03b7 i \u2208 I(T A ) (Equation 4).", "labels": [], "entities": [{"text": "F-score", "start_pos": 4, "end_pos": 11, "type": "METRIC", "confidence": 0.9911853671073914}, {"text": "F", "start_pos": 55, "end_pos": 56, "type": "METRIC", "confidence": 0.9950178265571594}]}, {"text": "The T S(T A , T G ) in is 0.9.", "labels": [], "entities": []}, {"text": "All nodes of TA have a perfect match, apart from New Cluster 0 and New Cluster 2, which are matched against computer network and meshwork respectively, having a perfect precision but a lower recall since the cotopies of computer network and meshwork consist of three concepts.", "labels": [], "entities": [{"text": "precision", "start_pos": 169, "end_pos": 178, "type": "METRIC", "confidence": 0.9966918230056763}, {"text": "recall", "start_pos": 191, "end_pos": 197, "type": "METRIC", "confidence": 0.9990226030349731}]}, {"text": "The automatically learned taxonomy has two redundant clusters that decrease its similarity.", "labels": [], "entities": [{"text": "similarity", "start_pos": 80, "end_pos": 90, "type": "METRIC", "confidence": 0.9698715209960938}]}, {"text": "The similarity measure T S(T A , T G ) provides the similarity of the automatically learned taxonomy to the gold standard one, but it is not symmetric.", "labels": [], "entities": [{"text": "similarity measure T S", "start_pos": 4, "end_pos": 26, "type": "METRIC", "confidence": 0.8429828137159348}, {"text": "similarity", "start_pos": 52, "end_pos": 62, "type": "METRIC", "confidence": 0.9819896221160889}]}, {"text": "Calculating the taxonomic similarity one way might not provide accurate results, in cases where TA misses senses of the gold standard.", "labels": [], "entities": [{"text": "TA", "start_pos": 96, "end_pos": 98, "type": "TASK", "confidence": 0.6895801424980164}]}, {"text": "This is due to the fact that we would only evaluate the internal nodes of TA , partially ignoring the fact that TA might have missed some parts of the gold standard taxonomy.", "labels": [], "entities": []}, {"text": "For that reason, we also calculate T S(T G , TA ) which provides the similarity of the gold standard taxonomy to the automatically learned one.", "labels": [], "entities": [{"text": "T S", "start_pos": 35, "end_pos": 38, "type": "METRIC", "confidence": 0.9319785237312317}, {"text": "similarity", "start_pos": 69, "end_pos": 79, "type": "METRIC", "confidence": 0.9601995944976807}]}, {"text": "Finally, taxonomic similarities are combined to produce their harmonic mean (Equation 5).", "labels": [], "entities": []}, {"text": "The first gold standard taxonomy is derived by extracting from WordNet all the hyponyms of the senses of the word network.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 63, "end_pos": 70, "type": "DATASET", "confidence": 0.978921115398407}]}, {"text": "sue a query to Yahoo! that contains wand we download a maximum of 1000 pages.", "labels": [], "entities": []}, {"text": "In cases where a particular sense is expressed by more than one word, the query was formulated by including all the words and putting the keyword OR between them.", "labels": [], "entities": [{"text": "OR", "start_pos": 146, "end_pos": 148, "type": "METRIC", "confidence": 0.9340226650238037}]}, {"text": "For each page we extracted fragments of text (paragraphs) that occur in <p> </p> html tags.", "labels": [], "entities": []}, {"text": "We extracted 58956 and 78691 paragraphs for the network and speaker dataset respectively.", "labels": [], "entities": [{"text": "speaker dataset", "start_pos": 60, "end_pos": 75, "type": "DATASET", "confidence": 0.6738645136356354}]}, {"text": "The reason we extracted on average less content for the second dataset was that Yahoo!", "labels": [], "entities": []}, {"text": "provided a small number of results for rare words such as alliterator, anecdotist, etc.", "labels": [], "entities": []}, {"text": "shows the parameter ranges for the WSI method.", "labels": [], "entities": [{"text": "WSI", "start_pos": 35, "end_pos": 38, "type": "TASK", "confidence": 0.6360764503479004}]}, {"text": "Our method is evaluated according to these parameters.", "labels": [], "entities": []}, {"text": "Our first baseline is RAND, which performs a random hierarchical clustering of senses to produce a binary tree.", "labels": [], "entities": [{"text": "RAND", "start_pos": 22, "end_pos": 26, "type": "TASK", "confidence": 0.5252500176429749}]}, {"text": "In each iteration two clusters are randomly chosen and form anew cluster, until we end up with one cluster taken to be the root.", "labels": [], "entities": []}, {"text": "The performance of RAND is calculated by executing the random algorithm 10 times and then averaging the results.", "labels": [], "entities": [{"text": "RAND", "start_pos": 19, "end_pos": 23, "type": "TASK", "confidence": 0.9342089891433716}]}, {"text": "The second baseline is the taxonomy most frequent sense baseline (TL MFS), in which we do not perform WSI.", "labels": [], "entities": [{"text": "frequent sense baseline (TL MFS)", "start_pos": 41, "end_pos": 73, "type": "METRIC", "confidence": 0.5864374424730029}, {"text": "WSI", "start_pos": 102, "end_pos": 105, "type": "TASK", "confidence": 0.8662635087966919}]}, {"text": "Instead, given a parameter setting and a word w, all the collocations of ware grouped into one vector, which will possibly be dominated by collocations related to the MFS of w.", "labels": [], "entities": []}, {"text": "WordNet mapping takes place and finally HAC with averagelinkage is applied to create the taxonomy.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.8752515912055969}, {"text": "HAC", "start_pos": 40, "end_pos": 43, "type": "METRIC", "confidence": 0.8200792670249939}]}, {"text": "In the network dataset, both of them achieve their highest distance from RAND (27.84%) at p 2 = 8 and p 3 = 0.2.", "labels": [], "entities": [{"text": "RAND", "start_pos": 73, "end_pos": 77, "type": "DATASET", "confidence": 0.4161297380924225}]}, {"text": "In the speaker dataset, their highest distance from RAND (20.97% and 19.63% respectively) is achieved at p 2 = 4 and p 3 = 0.1.", "labels": [], "entities": [{"text": "speaker dataset", "start_pos": 7, "end_pos": 22, "type": "DATASET", "confidence": 0.7422149032354355}, {"text": "distance from RAND", "start_pos": 38, "end_pos": 56, "type": "METRIC", "confidence": 0.7832125325997671}]}, {"text": "HAC CMP performs worse than the other HAC versions, yet it clearly outperforms RAND in all but one parameter combinations (p 1 = 5, p 2 = 6, p 3 = 0.4) in the speaker dataset.", "labels": [], "entities": [{"text": "HAC CMP", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.8727456331253052}]}], "tableCaptions": [{"text": " Table 1: Similarity matrix for HAC.", "labels": [], "entities": [{"text": "HAC", "start_pos": 32, "end_pos": 35, "type": "DATASET", "confidence": 0.5540562868118286}]}, {"text": " Table 2: Semantically related words/phrases to network", "labels": [], "entities": []}]}