{"title": [{"text": "Bayesian Inference for Finite-State Transducers *", "labels": [], "entities": []}], "abstractContent": [{"text": "We describe a Bayesian inference algorithm that can be used to train any cascade of weighted finite-state transducers on end-to-end data.", "labels": [], "entities": []}, {"text": "We also investigate the problem of automatically selecting from among multiple training runs.", "labels": [], "entities": []}, {"text": "Our experiments on four different tasks demonstrate the genericity of this framework, and, where applicable, large improvements in performance over EM.", "labels": [], "entities": []}, {"text": "We also show, for unsupervised part-of-speech tagging, that automatic run selection gives a large improvement over previous Bayesian approaches .", "labels": [], "entities": [{"text": "part-of-speech tagging", "start_pos": 31, "end_pos": 53, "type": "TASK", "confidence": 0.7002822458744049}]}], "introductionContent": [{"text": "In this paper, we investigate Bayesian inference for weighted finite-state transducers (WFSTs).", "labels": [], "entities": []}, {"text": "Many natural language models can be captured by weighted finite-state transducers), which offer several benefits: \u2022 WFSTs provide a uniform knowledge representation.", "labels": [], "entities": []}, {"text": "\u2022 Complex problems can be broken down into a cascade of simple WFSTs.", "labels": [], "entities": [{"text": "WFSTs", "start_pos": 63, "end_pos": 68, "type": "DATASET", "confidence": 0.6099377274513245}]}, {"text": "\u2022 Input-and output-epsilon transitions allow compact designs.", "labels": [], "entities": []}, {"text": "\u2022 Generic algorithms exist for doing inferences with WFSTs.", "labels": [], "entities": [{"text": "WFSTs", "start_pos": 53, "end_pos": 58, "type": "DATASET", "confidence": 0.885982871055603}]}, {"text": "These include best-path decoding, k-best path extraction, composition, * The authors are listed in alphabetical order.", "labels": [], "entities": [{"text": "k-best path extraction", "start_pos": 34, "end_pos": 56, "type": "TASK", "confidence": 0.6424118479092916}]}, {"text": "Please direct correspondence to Sujith Ravi (sravi@isi.edu).", "labels": [], "entities": []}, {"text": "This work was supported by NSF grant IIS-0904684 and DARPA contract HR0011-06-C0022.", "labels": [], "entities": [{"text": "NSF grant IIS-0904684", "start_pos": 27, "end_pos": 48, "type": "DATASET", "confidence": 0.6167543530464172}, {"text": "HR0011-06-C0022", "start_pos": 68, "end_pos": 83, "type": "DATASET", "confidence": 0.42975565791130066}]}, {"text": "intersection, minimization, determinization, forward-backward training, forward-backward pruning, stochastic generation, and projection.", "labels": [], "entities": [{"text": "stochastic generation", "start_pos": 98, "end_pos": 119, "type": "TASK", "confidence": 0.7349919080734253}]}, {"text": "\u2022 Software toolkits implement these generic algorithms, allowing designers to concentrate on novel models rather than problem-specific inference code.", "labels": [], "entities": []}, {"text": "This leads to faster scientific experimentation with fewer bugs.", "labels": [], "entities": []}, {"text": "Weighted tree transducers play the same role for problems that involve the creation and transformation of tree structures).", "labels": [], "entities": []}, {"text": "Of course, many problems do not fit either the finitestate string or tree transducer framework, but in this paper, we concentrate on those that do.", "labels": [], "entities": []}, {"text": "Bayesian inference schemes have become popular recently in natural language processing for their ability to manage uncertainty about model parameters and to allow designers to incorporate prior knowledge flexibly.", "labels": [], "entities": []}, {"text": "Task-accuracy results have generally been favorable.", "labels": [], "entities": []}, {"text": "However, it can be timeconsuming to apply Bayesian inference methods to each new problem.", "labels": [], "entities": []}, {"text": "Designers typically build custom, problem-specific sampling operators for exploring the derivation space.", "labels": [], "entities": []}, {"text": "They may factor their programs to get some code re-use from one problem to the next, but highly generic tools for string and tree processing are not available.", "labels": [], "entities": []}, {"text": "In this paper, we marry the world of finite-state machines with the world of Bayesian inference, and we test our methods across a range of natural language problems.", "labels": [], "entities": []}, {"text": "Our contributions are: \u2022 We describe a Bayesian inference algorithm that can be used to train any cascade of WFSTs on end-to-end data.", "labels": [], "entities": []}, {"text": "\u2022 We propose a method for automatic run selec-tion, i.e., how to automatically select among multiple training runs in order to achieve the best possible task accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 158, "end_pos": 166, "type": "METRIC", "confidence": 0.9705500602722168}]}, {"text": "The natural language applications we consider in this paper are: (1) unsupervised part-of-speech (POS) tagging, (2) letter substitution decipherment (, (3) segmentation of space-free English (, and (4) Japanese/English phoneme alignment).", "labels": [], "entities": [{"text": "part-of-speech (POS) tagging", "start_pos": 82, "end_pos": 110, "type": "TASK", "confidence": 0.6774121224880219}, {"text": "letter substitution decipherment", "start_pos": 116, "end_pos": 148, "type": "TASK", "confidence": 0.8446517984072367}, {"text": "segmentation of space-free English", "start_pos": 156, "end_pos": 190, "type": "TASK", "confidence": 0.832057997584343}, {"text": "Japanese/English phoneme alignment", "start_pos": 202, "end_pos": 236, "type": "TASK", "confidence": 0.566110473871231}]}, {"text": "shows how each of these problems can be represented as a cascade of finite-state acceptors (FSAs) and finite-state transducers (FSTs).", "labels": [], "entities": []}], "datasetContent": [{"text": "We run experiments for various natural language applications and compare the task accuracies achieved by the EM and Bayesian learning methods.", "labels": [], "entities": []}, {"text": "The tasks we consider are: Unsupervised POS tagging.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 40, "end_pos": 51, "type": "TASK", "confidence": 0.8173792660236359}]}, {"text": "We adopt the common problem formulation for this task described by, in which we are given a raw 24,115-word sequence and a dictionary of legal tags for each word type.", "labels": [], "entities": []}, {"text": "The tagset consists of 45 distinct grammatical tags.", "labels": [], "entities": []}, {"text": "We use the same modeling approach as as, using a probabilistic tag bigram model in conjunction with a tag-to-word model.", "labels": [], "entities": []}, {"text": "Here, the task is to decipher a 414-letter substitution cipher and uncover the original English letter sequence.", "labels": [], "entities": []}, {"text": "The task accuracy is defined as the percent of ciphertext tokens that are deciphered correctly.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 9, "end_pos": 17, "type": "METRIC", "confidence": 0.9487587809562683}]}, {"text": "We work on the same standard cipher described in previous literature.", "labels": [], "entities": []}, {"text": "The model consists of an English letter bigram model, whose probabilities are fixed and an English-to-ciphertext channel model, which is learnt during training.", "labels": [], "entities": []}, {"text": "Given a space-free English text corpus (e.g., iwalkedtothe...), the task is to segment the text into words (e.g., i walked to the ...).", "labels": [], "entities": []}, {"text": "Our input text corpus consists of 11,378 words, with spaces removed.", "labels": [], "entities": []}, {"text": "As illustrated in, our method uses a unigram FSA that models every letter sequence seen in the data, which includes both words and non-words (at most 10 letters long) composed with a deterministic spell-out model.", "labels": [], "entities": [{"text": "FSA", "start_pos": 45, "end_pos": 48, "type": "METRIC", "confidence": 0.6329140067100525}]}, {"text": "In order to evaluate the quality of our segmented output, we compare it against the gold segmentation and compute the word token f-measure.", "labels": [], "entities": []}, {"text": "Japanese/English phoneme alignment.", "labels": [], "entities": [{"text": "Japanese/English phoneme alignment", "start_pos": 0, "end_pos": 34, "type": "TASK", "confidence": 0.5350560307502746}]}, {"text": "We use the problem formulation of.", "labels": [], "entities": []}, {"text": "Given an input English/Japanese katakana phoneme sequence pair, the task is to produce an alignment that connects each English phoneme to its corresponding Japanese sounds (a sequence of one or more Japanese phonemes).", "labels": [], "entities": []}, {"text": "For example, given a phoneme sequence pair ((AH B AW T) \u2192 (a b a u t o)), we have to produce the alignments ((AH \u2192 a), (B \u2192 b), (AW \u2192 a u), (T \u2192 t o)).", "labels": [], "entities": []}, {"text": "The input data consists of 2,684 English/Japanese phoneme sequence pairs.", "labels": [], "entities": []}, {"text": "We use a model that consists of mappings from each English phoneme to Japanese phoneme sequences (of length up to 3), and the mapping probabilities are learnt during training.", "labels": [], "entities": []}, {"text": "We manually analyzed the alignments produced by the EM method for this task and found them to be nearly perfect.", "labels": [], "entities": []}, {"text": "Hence, for the purpose of this task we treat the EM alignments as our gold standard, since there are no gold alignments available for this data.", "labels": [], "entities": []}, {"text": "In all the experiments reported here, we run EM for 200 iterations and Bayesian for 5000 iterations (the first 2000 for burn-in).", "labels": [], "entities": [{"text": "EM", "start_pos": 45, "end_pos": 47, "type": "METRIC", "confidence": 0.7218573689460754}]}, {"text": "We apply automatic run selection using the objective function value for EM and the averaging method for Bayesian.", "labels": [], "entities": []}, {"text": "shows accuracy results for our four tasks, using run selection for both EM and Bayesian learning.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 6, "end_pos": 14, "type": "METRIC", "confidence": 0.9995335340499878}]}, {"text": "For the Bayesian runs, we compared two inference methods: Gibbs sampling, as described above, and Variational Bayesian EM (), both of which are implemented in Carmel.", "labels": [], "entities": []}, {"text": "We used the hyperparameters (\u03b1, \u03b2) as shown in the table.", "labels": [], "entities": []}, {"text": "Setting a high value yields a final distribution that is close to the original one (P 0 ).", "labels": [], "entities": []}, {"text": "For example, in letter decipherment we want to keep the language model probabilities fixed during training, and hence we set the prior on that model to be very strong (\u03b1 = 10 6 ).", "labels": [], "entities": [{"text": "letter decipherment", "start_pos": 16, "end_pos": 35, "type": "TASK", "confidence": 0.7536362409591675}]}, {"text": "shows that the Bayesian methods consistently outperform EM for all the tasks (except phoneme alignment, where EM was taken as the gold standard).", "labels": [], "entities": [{"text": "phoneme alignment", "start_pos": 85, "end_pos": 102, "type": "TASK", "confidence": 0.7127484530210495}]}, {"text": "Each iteration of Gibbs sampling was 2.3 times slower than EM for POS tagging, and in general about twice as slow.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 66, "end_pos": 77, "type": "TASK", "confidence": 0.7968299090862274}]}], "tableCaptions": [{"text": " Table 1: Gibbs sampling for Bayesian inference outperforms both EM and Variational Bayesian EM.  *  The output of  EM alignment was used as the gold standard.", "labels": [], "entities": [{"text": "EM alignment", "start_pos": 116, "end_pos": 128, "type": "TASK", "confidence": 0.6185299605131149}]}]}