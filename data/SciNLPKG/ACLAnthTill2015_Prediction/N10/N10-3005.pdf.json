{"title": [{"text": "Towards a Matrix-based Distributional Model of Meaning", "labels": [], "entities": [{"text": "Meaning", "start_pos": 47, "end_pos": 54, "type": "TASK", "confidence": 0.7773050665855408}]}], "abstractContent": [{"text": "Vector-based distributional models of semantics have proven useful and adequate in a variety of natural language processing tasks.", "labels": [], "entities": []}, {"text": "However , most of them lack at least one key requirement in order to serve as an adequate representation of natural language, namely sensitivity to structural information such as word order.", "labels": [], "entities": []}, {"text": "We propose a novel approach that offers a potential of integrating order-dependent word contexts in a completely un-supervised manner by assigning to words characteristic distributional matrices.", "labels": [], "entities": []}, {"text": "The proposed model is applied to the task of free associations.", "labels": [], "entities": []}, {"text": "In the end, the first results as well as directions for future work are discussed.", "labels": [], "entities": []}], "introductionContent": [{"text": "In natural language processing as well as in information retrieval, Vector Space Model (VSM) ( and Word Space Model (WSM) have become the mainstream for text representation.", "labels": [], "entities": [{"text": "information retrieval", "start_pos": 45, "end_pos": 66, "type": "TASK", "confidence": 0.7386193722486496}, {"text": "text representation", "start_pos": 153, "end_pos": 172, "type": "TASK", "confidence": 0.7690120041370392}]}, {"text": "VSMs embody the distributional hypothesis of meaning, the main assumption of which is that a word is known \"by the company it keeps\".", "labels": [], "entities": []}, {"text": "VSMs proved to perform well in a number of cognitive tasks such as synonymy identification, automatic thesaurus construction) and many others.", "labels": [], "entities": [{"text": "synonymy identification", "start_pos": 67, "end_pos": 90, "type": "TASK", "confidence": 0.9051069021224976}, {"text": "automatic thesaurus construction", "start_pos": 92, "end_pos": 124, "type": "TASK", "confidence": 0.6045818825562795}]}, {"text": "However, it has been long recognized that these models are too weak to represent natural language to a satisfactory extent.", "labels": [], "entities": []}, {"text": "With VSMs, the assumption is made that word cooccurrence is essentially independent of word order.", "labels": [], "entities": []}, {"text": "All the co-occurrence information is thus fed into one vector per word.", "labels": [], "entities": []}, {"text": "Suppose our \"background knowledge\" corpus consists of one sentence: Peter kicked the ball.", "labels": [], "entities": []}, {"text": "It follows that the distributional meanings of both PE-TER and BALL would be in a similar way defined by the co-occurring KICK which is insufficient, as BALL can be only kicked by somebody but not kick itself; in case of PETER, both ways of interpretation should be possible.", "labels": [], "entities": [{"text": "BALL", "start_pos": 63, "end_pos": 67, "type": "METRIC", "confidence": 0.883975088596344}, {"text": "KICK", "start_pos": 122, "end_pos": 126, "type": "METRIC", "confidence": 0.9097511768341064}]}, {"text": "To overcome the aforementioned problems with vector-based models, we suggest a novel distributional paradigm for representing text in that we introduce a further dimension into a \"standard\" two-dimensional word space model.", "labels": [], "entities": []}, {"text": "That allows us to count correlations for three words at a time.", "labels": [], "entities": []}, {"text": "In short, given a vocabulary V , context width w = m and tokens t 1 , t 2 , t 3 , ..., ti \u2208 V , for token ti a matrix of size V \u00d7 V is generated that has nonzero values in cells where ti appears between t i\u2212m and t i+m . Note that this 3-dimensional representation allows us to integrate word order information into the model in a completely unsupervised manner as well as to achieve a richer word representation as a matrix instead of a vector.", "labels": [], "entities": []}, {"text": "The remainder of the paper is organized as follows.", "labels": [], "entities": []}, {"text": "After a recap of basic mathematical notions and operations used in the model in Section 2, we introduce the proposed three-dimensional tensorbased model of text representation in Section 3.", "labels": [], "entities": [{"text": "text representation", "start_pos": 156, "end_pos": 175, "type": "TASK", "confidence": 0.7170472890138626}]}, {"text": "First evaluation experiments are reported in Section 4.", "labels": [], "entities": []}, {"text": "After a brief overview of related work in Section 5, we provide some concluding remarks and suggestions for future work in Section 6.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Slices of T for the terms KICK (i = 3) and BALL  (i = 4).", "labels": [], "entities": [{"text": "KICK", "start_pos": 36, "end_pos": 40, "type": "METRIC", "confidence": 0.6900620460510254}, {"text": "BALL", "start_pos": 53, "end_pos": 57, "type": "METRIC", "confidence": 0.9972659349441528}]}, {"text": " Table 2: Accuracies for the \"equally distributed\" threshold for training and test sets", "labels": [], "entities": [{"text": "Accuracies", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9993403553962708}]}, {"text": " Table 3: Accuracies for a \"linearly growing\" threshold for training and test sets", "labels": [], "entities": [{"text": "Accuracies", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9990633130073547}]}]}