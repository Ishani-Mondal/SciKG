{"title": [{"text": "Identifying Opinion Holders and Targets with Dependency Parser in Chinese News Texts", "labels": [], "entities": [{"text": "Identifying Opinion Holders", "start_pos": 0, "end_pos": 27, "type": "TASK", "confidence": 0.9115773439407349}, {"text": "Chinese News Texts", "start_pos": 66, "end_pos": 84, "type": "DATASET", "confidence": 0.7726621627807617}]}], "abstractContent": [{"text": "In this paper, we propose to identify opinion holders and targets with dependency parser in Chinese news texts, i.e. to identify opinion holders by means of reporting verbs and to identify opinion targets by considering both opinion holders and opinion-bearing words.", "labels": [], "entities": []}, {"text": "The experiments with NTCIR-7 MOAT's Chinese test data show that our approach provides better performance than the baselines and most systems reported at NTCIR-7.", "labels": [], "entities": [{"text": "NTCIR-7 MOAT's Chinese test data", "start_pos": 21, "end_pos": 53, "type": "DATASET", "confidence": 0.8979986707369486}, {"text": "NTCIR-7", "start_pos": 153, "end_pos": 160, "type": "DATASET", "confidence": 0.9707455039024353}]}], "introductionContent": [{"text": "In recent years, sentiment analysis, which mines opinions from information sources such as news, blogs and product reviews, has drawn much attention in the NLP field).", "labels": [], "entities": [{"text": "sentiment analysis", "start_pos": 17, "end_pos": 35, "type": "TASK", "confidence": 0.9716507792472839}]}, {"text": "An opinion expressed in a text involves different components, including opinion expression, opinion holder and target.", "labels": [], "entities": []}, {"text": "Opinion holder is usually an entity that holds an opinion, and opinion target is what the opinion is about).", "labels": [], "entities": []}, {"text": "Although there have been research on identifying opinion holders and targets in English product reviews and news texts, little work has been reported on similar tasks involving Chinese news texts.", "labels": [], "entities": []}, {"text": "In this study, we investigate how dependency parsing can be used to help the task on opinion holder/target identification in Chinese news texts.", "labels": [], "entities": [{"text": "dependency parsing", "start_pos": 34, "end_pos": 52, "type": "TASK", "confidence": 0.7518779635429382}, {"text": "opinion holder/target identification in Chinese news texts", "start_pos": 85, "end_pos": 143, "type": "TASK", "confidence": 0.7843938337432014}]}, {"text": "Three possible contributions from this study are: 1) we propose that the existence of reporting verbs is a very important feature for identifying opinion holders in news texts, which has not been clearly indicated; 2) we argue that the identification of opinion targets should not be done alone without considering opinion holders, because opinion holders are much easier to be identified in news texts and the identified holders are quite useful for the identification of the associated targets.", "labels": [], "entities": [{"text": "identifying opinion holders in news texts", "start_pos": 134, "end_pos": 175, "type": "TASK", "confidence": 0.79209436972936}]}, {"text": "Our approach shows encouraging performance on opinion holder/target identification, and the results are much better than the baseline results and most results reported in NTCIR-7 (.", "labels": [], "entities": [{"text": "opinion holder/target identification", "start_pos": 46, "end_pos": 82, "type": "TASK", "confidence": 0.6265514552593231}, {"text": "NTCIR-7", "start_pos": 171, "end_pos": 178, "type": "DATASET", "confidence": 0.9628705382347107}]}, {"text": "The paper is organized as follows.", "labels": [], "entities": []}, {"text": "3 gives the linguistic analysis of opinion holder/target.", "labels": [], "entities": []}, {"text": "The proposed approach is described in Sec.", "labels": [], "entities": []}, {"text": "4, followed by the experiments in Sec.", "labels": [], "entities": []}, {"text": "5. Lastly we conclude in Sec.", "labels": [], "entities": []}], "datasetContent": [{"text": "We use the traditional Chinese test data in NTCIR-7 MOAT ( for our experiments.", "labels": [], "entities": [{"text": "Chinese test data", "start_pos": 23, "end_pos": 40, "type": "DATASET", "confidence": 0.7561363577842712}, {"text": "NTCIR-7", "start_pos": 44, "end_pos": 51, "type": "DATASET", "confidence": 0.8606970906257629}, {"text": "MOAT", "start_pos": 52, "end_pos": 56, "type": "METRIC", "confidence": 0.6167691349983215}]}, {"text": "Out of 4465 sentences, 2174 are annotated as opinionated by the lenient standard, and the opinion holders of some opinionated sentences are marked as POST_AUTHOR denoting the author of the news article.", "labels": [], "entities": [{"text": "POST_AUTHOR", "start_pos": 150, "end_pos": 161, "type": "METRIC", "confidence": 0.9165517091751099}]}, {"text": "We use the final list given by the organizers as the gold standard.", "labels": [], "entities": []}, {"text": "Baselines for opinion holder identification: Baseline 1: We just use the subject of reporting verbs as the opinion holder, without sentence preprocessing described in Sec.", "labels": [], "entities": [{"text": "opinion holder identification", "start_pos": 14, "end_pos": 43, "type": "TASK", "confidence": 0.6304367482662201}]}, {"text": "4.2 and any heuristic rules introduced in Sec.", "labels": [], "entities": []}, {"text": "Baseline 2: We also implement the CRF model for detecting opinion holders () by using CRF++.", "labels": [], "entities": [{"text": "detecting opinion holders", "start_pos": 48, "end_pos": 73, "type": "TASK", "confidence": 0.8295592665672302}]}, {"text": "The training data is the NTCIR-6 Chinese test data.", "labels": [], "entities": [{"text": "NTCIR-6 Chinese test data", "start_pos": 25, "end_pos": 50, "type": "DATASET", "confidence": 0.9532419294118881}]}, {"text": "The labels used by CRF comprise Holder, Parent of Holder, None (not holder or parent) and the features for each word in our implementation include: basic features (i.e. word, POS-tag, whether the word itself is a reporting verb or not), dependency features (i.e. parent word, POS-tag of its parent, dependency relation with its parent, whether its parent is a reporting verb) and semantic features (i.e. WSD entry in Tongyici Cilin, WSD entry of its parent).", "labels": [], "entities": []}, {"text": "Baseline for opinion target identification: Baseline 1: we try to find the subject or object of opinion-bearing words as the targets.", "labels": [], "entities": [{"text": "opinion target identification", "start_pos": 13, "end_pos": 42, "type": "TASK", "confidence": 0.6609418392181396}]}, {"text": "If both a subject and an object are found, we just simply choose the subject as the target.", "labels": [], "entities": []}, {"text": "We evaluate performance using 3 measures: exact match (EM), head match (HM), and partial match (PM), similar to.", "labels": [], "entities": [{"text": "exact match (EM)", "start_pos": 42, "end_pos": 58, "type": "METRIC", "confidence": 0.9572709441184998}, {"text": "head match (HM)", "start_pos": 60, "end_pos": 75, "type": "METRIC", "confidence": 0.9561645150184631}, {"text": "partial match (PM)", "start_pos": 81, "end_pos": 99, "type": "METRIC", "confidence": 0.9070380926132202}]}, {"text": "We use three evaluation metrics: recall (Rec), precision (Pre), and F1.", "labels": [], "entities": [{"text": "recall (Rec)", "start_pos": 33, "end_pos": 45, "type": "METRIC", "confidence": 0.9356018155813217}, {"text": "precision (Pre)", "start_pos": 47, "end_pos": 62, "type": "METRIC", "confidence": 0.9135055541992188}, {"text": "F1", "start_pos": 68, "end_pos": 70, "type": "METRIC", "confidence": 0.9996891021728516}]}, {"text": "For opinion holder identification, we consider two cases: 1) all opinionated sentences; 2) only the opinionated sentences whose opinion holders do not contain POST_AUTHOR.", "labels": [], "entities": [{"text": "opinion holder identification", "start_pos": 4, "end_pos": 33, "type": "TASK", "confidence": 0.7088121573130289}, {"text": "POST_AUTHOR", "start_pos": 159, "end_pos": 170, "type": "METRIC", "confidence": 0.8663029074668884}]}, {"text": "The metric ALL_Pre reported below is the precision in case 1 which is the same with recall and F1.", "labels": [], "entities": [{"text": "ALL_Pre", "start_pos": 11, "end_pos": 18, "type": "METRIC", "confidence": 0.9193494121233622}, {"text": "precision", "start_pos": 41, "end_pos": 50, "type": "METRIC", "confidence": 0.9992063641548157}, {"text": "recall", "start_pos": 84, "end_pos": 90, "type": "METRIC", "confidence": 0.9992401599884033}, {"text": "F1", "start_pos": 95, "end_pos": 97, "type": "METRIC", "confidence": 0.9959291815757751}]}], "tableCaptions": [{"text": " Table 1. Results for Opinion Holders  Unexpectedly, even the unsupervised baseline 1  achieves better performance than baseline 2 (the  CRF-based method). The possible reasons are: 1)  the training data is not large enough to cover the  cases in the test data, resulting in low recall of the  CRF model; 2) the features used by the CRF model  could be refined to improve the performance.", "labels": [], "entities": [{"text": "Opinion Holders", "start_pos": 22, "end_pos": 37, "type": "TASK", "confidence": 0.6754635274410248}, {"text": "recall", "start_pos": 279, "end_pos": 285, "type": "METRIC", "confidence": 0.9985767602920532}]}, {"text": " Table 2. Results for Opinion Targets  We also investigate the influences of the  following four factors on the performance:  sentence preprocessing (SP) in Sec. 4.2, target", "labels": [], "entities": [{"text": "sentence preprocessing (SP)", "start_pos": 126, "end_pos": 153, "type": "METRIC", "confidence": 0.6025928795337677}]}]}