{"title": [], "abstractContent": [{"text": "Datasets annotated with semantic roles are an important prerequisite to developing high-performance role labeling systems.", "labels": [], "entities": []}, {"text": "Unfortunately , the reliance on manual annotations, which are both difficult and highly expensive to produce, presents a major obstacle to the widespread application of these systems across different languages and text genres.", "labels": [], "entities": []}, {"text": "In this paper we describe a method for inducing the semantic roles of verbal arguments directly from unannotated text.", "labels": [], "entities": []}, {"text": "We formulate the role induction problem as one of detecting alternations and finding a canonical syntactic form for them.", "labels": [], "entities": [{"text": "role induction", "start_pos": 17, "end_pos": 31, "type": "TASK", "confidence": 0.8236033618450165}]}, {"text": "Both steps are implemented in a novel probabilistic model, a latent-variable variant of the logistic classifier.", "labels": [], "entities": []}, {"text": "Our method increases the purity of the induced role clusters by a wide margin over a strong baseline.", "labels": [], "entities": [{"text": "purity", "start_pos": 25, "end_pos": 31, "type": "METRIC", "confidence": 0.9764389991760254}]}], "introductionContent": [{"text": "Semantic role labeling (SRL, is the task of automatically classifying the arguments of a predicate with roles such as Agent, Patient or Location.", "labels": [], "entities": [{"text": "Semantic role labeling (SRL", "start_pos": 0, "end_pos": 27, "type": "TASK", "confidence": 0.7963178873062133}]}, {"text": "These labels capture aspects of the semantics of the relationship between the predicate and the argument while abstracting over surface syntactic configurations.", "labels": [], "entities": []}, {"text": "SRL has received much attention in recent years (, partly because of its potential to improve applications that require broad coverage semantic processing.", "labels": [], "entities": [{"text": "SRL", "start_pos": 0, "end_pos": 3, "type": "TASK", "confidence": 0.7906137704849243}]}, {"text": "Examples include information extraction (), question answering, summarization), and machine translation (.", "labels": [], "entities": [{"text": "information extraction", "start_pos": 17, "end_pos": 39, "type": "TASK", "confidence": 0.8399308919906616}, {"text": "question answering", "start_pos": 44, "end_pos": 62, "type": "TASK", "confidence": 0.9007869958877563}, {"text": "summarization", "start_pos": 64, "end_pos": 77, "type": "TASK", "confidence": 0.9204123616218567}, {"text": "machine translation", "start_pos": 84, "end_pos": 103, "type": "TASK", "confidence": 0.8153090178966522}]}, {"text": "Given sentences (1-a) and (1-b) as input, an SRL system would have to identify the verb predicate (shown in boldface), its arguments (Michael and sandwich) and label them with semantic roles (Agent and Patient).", "labels": [], "entities": [{"text": "SRL", "start_pos": 45, "end_pos": 48, "type": "TASK", "confidence": 0.9707505702972412}]}, {"text": "Here, sentence (1-b) is an alternation of (1-a).", "labels": [], "entities": []}, {"text": "The verbal arguments bear the same semantic role, even though they appear in different syntactic positions: sandwich is the object of eat in sentence (1-a) and its subject in (1-b) but it is in both instances assigned the role Patient.", "labels": [], "entities": []}, {"text": "The example illustrates the passive alternation.", "labels": [], "entities": []}, {"text": "The latter is merely one type of alternation, many others exist, and their computational treatment is one of the main challenges faced by semantic role labelers.", "labels": [], "entities": [{"text": "semantic role labelers", "start_pos": 138, "end_pos": 160, "type": "TASK", "confidence": 0.6805680791536967}]}, {"text": "Most SRL systems to date conceptualize semantic role labeling as a supervised learning problem and rely on role-annotated data for model training.", "labels": [], "entities": [{"text": "SRL", "start_pos": 5, "end_pos": 8, "type": "TASK", "confidence": 0.9842891097068787}, {"text": "semantic role labeling", "start_pos": 39, "end_pos": 61, "type": "TASK", "confidence": 0.6141756276289622}]}, {"text": "PropBank () has been widely used for the development of semantic role labelers as well as FrameNet (.", "labels": [], "entities": [{"text": "PropBank", "start_pos": 0, "end_pos": 8, "type": "DATASET", "confidence": 0.9171851873397827}]}, {"text": "Under the PropBank annotation framework (which we will assume throughout this paper) each predicate is associated with a set of core roles (named A0, A1, A2, and so on) whose interpretations are specific to that predicate and a set of adjunct roles (e.g., Location or Time) whose interpretation is common across predicates.", "labels": [], "entities": [{"text": "A2", "start_pos": 154, "end_pos": 156, "type": "METRIC", "confidence": 0.6218770742416382}]}, {"text": "In addition to large amounts of role-annotated data, SRL systems often make use of a parser to obtain syntactic analyses which subsequently serve as input to a pipeline of components concerned with identifying predicates and their arguments (argument identification) and labeling them with semantic roles (argument classification).", "labels": [], "entities": [{"text": "SRL", "start_pos": 53, "end_pos": 56, "type": "TASK", "confidence": 0.9793257713317871}, {"text": "argument identification)", "start_pos": 242, "end_pos": 266, "type": "TASK", "confidence": 0.8067821661631266}, {"text": "argument classification", "start_pos": 306, "end_pos": 329, "type": "TASK", "confidence": 0.7024607211351395}]}, {"text": "Supervised SRL methods deliver reasonably good performance (a system will recall around 81% of the arguments correctly and 95% of those will be assigned a correct semantic role; see M` arquez et al. 2008 for details).", "labels": [], "entities": [{"text": "SRL", "start_pos": 11, "end_pos": 14, "type": "TASK", "confidence": 0.9811087250709534}]}, {"text": "Unfortunately, the reliance on labeled training data, which is both difficult and highly expensive to produce, presents a major obstacle to the widespread application of semantic role labeling across different languages and text genres.", "labels": [], "entities": [{"text": "semantic role labeling", "start_pos": 170, "end_pos": 192, "type": "TASK", "confidence": 0.6423804859320322}]}, {"text": "And although corpora with semantic role annotations exist nowadays in other languages (e.g., German, Spanish, Catalan, Chinese, Korean), they tend to be smaller than their English equivalents and of limited value for modeling purposes.", "labels": [], "entities": []}, {"text": "Moreover, the performance of supervised systems degrades considerably (by 10%) on out-of-domain data even within English, a language for which two major annotated corpora are available.", "labels": [], "entities": []}, {"text": "Interestingly, find that the main reason for this are errors in the assignment of semantic roles, rather than the identification of argument boundaries.", "labels": [], "entities": []}, {"text": "Therefore, a mechanism for inducing the semantic roles observed in the data without additional manual effort would enhance the robustness of existing SRL systems and enable their portability to languages for which annotations are unavailable or sparse.", "labels": [], "entities": [{"text": "SRL", "start_pos": 150, "end_pos": 153, "type": "TASK", "confidence": 0.9573394060134888}]}, {"text": "In this paper we describe an unsupervised approach to argument classification or role induction 2 that does not make use of role-annotated data.", "labels": [], "entities": [{"text": "argument classification", "start_pos": 54, "end_pos": 77, "type": "TASK", "confidence": 0.8706202805042267}, {"text": "role induction 2", "start_pos": 81, "end_pos": 97, "type": "TASK", "confidence": 0.8471642136573792}]}, {"text": "Role induction can be naturally formalized as a clustering problem where argument instances are assigned to clusters.", "labels": [], "entities": [{"text": "Role induction", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.9014110863208771}]}, {"text": "Ideally, each cluster should contain arguments corresponding to a specific semantic role and each role should correspond to exactly one cluster.", "labels": [], "entities": []}, {"text": "A key insight in our approach is that many predicates are associated with a standard linking.", "labels": [], "entities": []}, {"text": "A linking is a deterministic mapping from semantic roles onto syntactic functions such as subject, or object.", "labels": [], "entities": []}, {"text": "Most predicates will exhibit a standard linking, i.e., they will be predominantly used with a specific mapping.", "labels": [], "entities": []}, {"text": "Alternations occur when a different linking is used.", "labels": [], "entities": []}, {"text": "In sentence (1-a) the predicate eat is used with its standard linking (the Agent role is mapped onto the subject function and the Patient onto the object), whereas in sentence (1-b) eat is used with its passive-linking (the Patient is mapped onto subject and the Agent appears as a prepositional phrase).", "labels": [], "entities": []}, {"text": "When faced with such alternations, we will attempt to determine for each argument the syntactic function it would have had, had the standard linking been used.", "labels": [], "entities": []}, {"text": "We will refer to this function as the arguments' canonical function, and use the term canonicalization to describe the process of inferring these canonical functions in the case of alternations.", "labels": [], "entities": []}, {"text": "So, in sentence (1-b) the canonical functions of the arguments by Michael and sandwich are subject and object, respectively.", "labels": [], "entities": []}, {"text": "Since linkings are injective, i.e., no two semantic roles are mapped onto the same syntactic function, the canonical function of an argument uniquely references a specific semantic role.", "labels": [], "entities": []}, {"text": "We define a probabilistic model for detecting non-standard linkings and for canonicalization.", "labels": [], "entities": []}, {"text": "The model specifies a distribution p(F) over the possible canonical functions F of an argument.", "labels": [], "entities": []}, {"text": "We present an extension of the logistic classifier with the addition of latent variables which crucially allow to learn generalizations over varying syntactic configurations.", "labels": [], "entities": []}, {"text": "Rather than using manually labeled data, we train our model on observed syntactic functions which can be obtained automatically from a parser.", "labels": [], "entities": []}, {"text": "These training instances are admittedly noisy but readily available and as we show experimentally a useful data source for inducing semantic roles.", "labels": [], "entities": []}, {"text": "Application of the model to a benchmark dataset yields improvements over a strong baseline.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section we discuss the experimental design for assessing the performance of the model described above.", "labels": [], "entities": []}, {"text": "We give details on the dataset, features and evaluation measures employed and present the baseline methods used for comparison with our model.", "labels": [], "entities": []}, {"text": "Data Our experiments were carried out on the CoNLL 2008 () training dataset which contains both verbal and nominal predicates.", "labels": [], "entities": [{"text": "CoNLL 2008 () training dataset", "start_pos": 45, "end_pos": 75, "type": "DATASET", "confidence": 0.9546990752220154}]}, {"text": "However, we focused solely on verbal predicates, following most previous work on semantic role labeling).", "labels": [], "entities": [{"text": "semantic role labeling", "start_pos": 81, "end_pos": 103, "type": "TASK", "confidence": 0.6370391945044199}]}, {"text": "The CoNLL dataset is taken form the Wall Street Journal portion of the Penn Treebank corpus.", "labels": [], "entities": [{"text": "CoNLL dataset", "start_pos": 4, "end_pos": 17, "type": "DATASET", "confidence": 0.920466274023056}, {"text": "Wall Street Journal portion", "start_pos": 36, "end_pos": 63, "type": "DATASET", "confidence": 0.966509073972702}, {"text": "Penn Treebank corpus", "start_pos": 71, "end_pos": 91, "type": "DATASET", "confidence": 0.9573647777239481}]}, {"text": "Role semantic annotations are based on PropBank and have been converted from a constituent-based to a dependency-based representation (see).", "labels": [], "entities": [{"text": "PropBank", "start_pos": 39, "end_pos": 47, "type": "DATASET", "confidence": 0.9498100280761719}]}, {"text": "For each argument of a predicate only the headword is annotated with the corresponding semantic role, rather than the whole constituent.", "labels": [], "entities": []}, {"text": "In this paper we are only concerned with role induction, not argument identification.", "labels": [], "entities": [{"text": "role induction", "start_pos": 41, "end_pos": 55, "type": "TASK", "confidence": 0.8720172047615051}, {"text": "argument identification", "start_pos": 61, "end_pos": 84, "type": "TASK", "confidence": 0.8724178075790405}]}, {"text": "Therefore, we identify the arguments of each predicate by consulting the gold standard.", "labels": [], "entities": []}, {"text": "The CoNLL dataset also supplies an automatic dependency parse of each input sentence obtained from the MaltParser (.", "labels": [], "entities": [{"text": "CoNLL dataset", "start_pos": 4, "end_pos": 17, "type": "DATASET", "confidence": 0.945881575345993}]}, {"text": "The target and features used in our model are extracted from these parses.", "labels": [], "entities": []}, {"text": "Syntactic functions occurring more than 1, 000 times in the gold standard are shown in (for more details we refer the interested reader to).", "labels": [], "entities": [{"text": "gold standard", "start_pos": 60, "end_pos": 73, "type": "DATASET", "confidence": 0.8446222543716431}]}, {"text": "Syntactic functions were further modified to include prepositions if specified, resulting in a set of functions with which arguments can be distinguished more precisely.", "labels": [], "entities": []}, {"text": "This was often the case with functions such as ADV, TMP, LOC, etc.", "labels": [], "entities": [{"text": "LOC", "start_pos": 57, "end_pos": 60, "type": "METRIC", "confidence": 0.8262014985084534}]}, {"text": "Also, instead of using the preposition itself as the argument head, we used the actual content word modifying the preposition.", "labels": [], "entities": []}, {"text": "We made no attempt to treat split arguments, namely instances where the semantic argument of a predicate has several syntactic heads.", "labels": [], "entities": []}, {"text": "These are infrequent in the dataset, they makeup for less than 1% of all arguments.", "labels": [], "entities": []}, {"text": "Model Setup The specific instantiation of the model used in our experiments has 10 latent variables.", "labels": [], "entities": []}, {"text": "With 10 binary latent variables we can encode 1024 different target values, which seems reasonable for our set of syntactic functions which comprises around 350 elements.", "labels": [], "entities": []}, {"text": "Features representing argument instances were extracted from dependency parses like the one shown in.", "labels": [], "entities": []}, {"text": "We used a relatively small feature set consisting of: the predicate lemma, the argument lemma, the argument part-of-speech, the preposition involved in dependency between predicate and argument (if there is one), the lemma of left-most/rightmost child of the argument, the part-of-speech of left-most/right-most child of argument, and a key formed by concatenating all syntactic functions of the argument's children.", "labels": [], "entities": []}, {"text": "The features for the argument maker in are [sell, maker, NN, -, the, auto, DT, NN, NMOD+NMOD].", "labels": [], "entities": []}, {"text": "The target for this instance (and observed syntactic function) is SBJ.", "labels": [], "entities": []}, {"text": "Evaluation Evaluating the output of our model is no different from other clustering problems.", "labels": [], "entities": []}, {"text": "We can therefore use well-known measures from the clustering literature to assess the quality of our role induction method.", "labels": [], "entities": [{"text": "role induction", "start_pos": 101, "end_pos": 115, "type": "TASK", "confidence": 0.8089165985584259}]}, {"text": "We first created a set of gold-standard role labeled argument instances which were obtained from the training partition of the CoNLL 2008 dataset (corresponding to sections 02-21 of PropBank).", "labels": [], "entities": [{"text": "CoNLL 2008 dataset", "start_pos": 127, "end_pos": 145, "type": "DATASET", "confidence": 0.9448931018511454}]}, {"text": "We used 10 clusters for each predicate and restricted the set of predicates to those attested with more than 20 instances.", "labels": [], "entities": []}, {"text": "This rules out simple cases with only few instances relative to the number of clusters, which trivially yield high scores.", "labels": [], "entities": []}, {"text": "We compared the output of our method against the gold-standard using the following common measures.", "labels": [], "entities": []}, {"text": "Let K denote the number of clusters, c i the set of instances in the i-th cluster and g j the set of instances having the j-th gold standard semantic role label.", "labels": [], "entities": []}, {"text": "Cluster purity (PU) is defined as: We also used cluster accuracy (CA, Equation 8),: Clustering results using our model (LogLV) against the baseline (SyntFunc) and upper bounds (UpperBndS and UpperBndG).", "labels": [], "entities": [{"text": "Cluster purity (PU)", "start_pos": 0, "end_pos": 19, "type": "METRIC", "confidence": 0.7583403706550598}, {"text": "accuracy", "start_pos": 56, "end_pos": 64, "type": "METRIC", "confidence": 0.6990422606468201}]}, {"text": "cluster precision (CP, Equation 9), and cluster recall (CR, Equation 9).", "labels": [], "entities": [{"text": "precision", "start_pos": 8, "end_pos": 17, "type": "METRIC", "confidence": 0.8595805764198303}, {"text": "recall", "start_pos": 48, "end_pos": 54, "type": "METRIC", "confidence": 0.9718197584152222}, {"text": "CR", "start_pos": 56, "end_pos": 58, "type": "METRIC", "confidence": 0.7612413167953491}]}, {"text": "Cluster F1 (CF1) is the harmonic mean of precision and recall.", "labels": [], "entities": [{"text": "Cluster F1 (CF1)", "start_pos": 0, "end_pos": 16, "type": "METRIC", "confidence": 0.8150947213172912}, {"text": "precision", "start_pos": 41, "end_pos": 50, "type": "METRIC", "confidence": 0.9993706345558167}, {"text": "recall", "start_pos": 55, "end_pos": 61, "type": "METRIC", "confidence": 0.9954766631126404}]}, {"text": "Here T P is the number of pairs of instances which have the same role and are in the same cluster, TN is the number of pairs of instances which have different roles and are in different clusters, FP is the number of pairs of instances with different roles in the same cluster and FN the number of pairs of instances with the same role in different clusters.", "labels": [], "entities": [{"text": "FP", "start_pos": 196, "end_pos": 198, "type": "METRIC", "confidence": 0.9918871521949768}, {"text": "FN", "start_pos": 280, "end_pos": 282, "type": "METRIC", "confidence": 0.9933944344520569}]}], "tableCaptions": [{"text": " Table 1: Contingency table between syntactic func- tion and semantic role for two core roles Agent (A0)  and Patient (A1) and two adjunct roles, Time (TMP)  and Manner (MNR). Only syntactic functions occur- ring more than 1000 times are listed. Counts were  obtained from the CoNLL 2008 training dataset us- ing gold standard parses (the marginals in the bottom  row also include counts of unlisted co-occurrences).", "labels": [], "entities": [{"text": "Manner (MNR)", "start_pos": 162, "end_pos": 174, "type": "METRIC", "confidence": 0.9060579538345337}, {"text": "CoNLL 2008 training dataset us- ing gold standard parses", "start_pos": 277, "end_pos": 333, "type": "DATASET", "confidence": 0.9354182422161103}]}, {"text": " Table 2: Clustering results using our model (LogLV) against the baseline (SyntFunc) and upper bounds  (UpperBndS and UpperBndG).", "labels": [], "entities": []}, {"text": " Table 3: Clustering results using our model to detect alternate linkings (LogLV) against the baseline (Synt- Func).", "labels": [], "entities": []}]}