{"title": [{"text": "Multi-Prototype Vector-Space Models of Word Meaning", "labels": [], "entities": [{"text": "Word Meaning", "start_pos": 39, "end_pos": 51, "type": "TASK", "confidence": 0.6605444550514221}]}], "abstractContent": [{"text": "Current vector-space models of lexical semantics create a single \"prototype\" vector to represent the meaning of a word.", "labels": [], "entities": []}, {"text": "However, due to lexical ambiguity, encoding word meaning with a single vector is problematic.", "labels": [], "entities": []}, {"text": "This paper presents a method that uses clustering to produce multiple \"sense-specific\" vectors for each word.", "labels": [], "entities": []}, {"text": "This approach provides a context-dependent vector representation of word meaning that naturally accommodates homonymy and polysemy.", "labels": [], "entities": []}, {"text": "Experimental comparisons to human judgements of semantic similarity for both isolated words as well as words in sentential contexts demonstrate the superiority of this approach over both prototype and exemplar based vector-space models.", "labels": [], "entities": []}], "introductionContent": [{"text": "Automatically judging the degree of semantic similarity between words is an important task useful in text classification (, information retrieval, textual entailment, and other language processing tasks.", "labels": [], "entities": [{"text": "text classification", "start_pos": 101, "end_pos": 120, "type": "TASK", "confidence": 0.7707492709159851}, {"text": "information retrieval", "start_pos": 124, "end_pos": 145, "type": "TASK", "confidence": 0.7901788055896759}, {"text": "textual entailment", "start_pos": 147, "end_pos": 165, "type": "TASK", "confidence": 0.750046044588089}]}, {"text": "The standard empirical approach to this task exploits the distributional hypothesis, i.e. that similar words appear in similar contexts (;.", "labels": [], "entities": []}, {"text": "Traditionally, word types are represented by a single vector of contextual features derived from cooccurrence information, and semantic similarity is computed using some measure of vector distance).", "labels": [], "entities": []}, {"text": "However, due to homonymy and polysemy, capturing the semantics of a word with a single vector is problematic.", "labels": [], "entities": []}, {"text": "For example, the word club is similar to both bat and association, which are not at all similar to each other.", "labels": [], "entities": []}, {"text": "Word meaning violates the triangle inequality when viewed at the level of word types, posing a problem for vector-space models.", "labels": [], "entities": [{"text": "Word meaning", "start_pos": 0, "end_pos": 12, "type": "TASK", "confidence": 0.6806714683771133}]}, {"text": "A single \"prototype\" vector is simply incapable of capturing phenomena such as homonymy and polysemy.", "labels": [], "entities": []}, {"text": "Also, most vector-space models are context independent, while the meaning of a word clearly depends on context.", "labels": [], "entities": []}, {"text": "The word club in \"The caveman picked up the club\" is similar to bat in \"John hit the robber with a bat,\" but not in \"The bat flew out of the cave.\"", "labels": [], "entities": []}, {"text": "We present anew resource-lean vector-space model that represents a word's meaning by a set of distinct \"sense specific\" vectors.", "labels": [], "entities": []}, {"text": "The similarity of two isolated words A and B is defined as the minimum distance between one of A's vectors and one of B's vectors.", "labels": [], "entities": []}, {"text": "In addition, a context-dependent meaning fora word is determined by choosing one of the vectors in its set based on minimizing the distance to the vector representing the current context.", "labels": [], "entities": []}, {"text": "Consequently, the model supports judging the similarity of both words in isolation and words in context.", "labels": [], "entities": []}, {"text": "The set of vectors fora word is determined by unsupervised word sense discovery (WSD), which clusters the contexts in which a word appears.", "labels": [], "entities": [{"text": "word sense discovery (WSD)", "start_pos": 59, "end_pos": 85, "type": "TASK", "confidence": 0.7411000927289327}]}, {"text": "In previous work, vector-space lexical similarity and word sense discovery have been treated as two separate tasks.", "labels": [], "entities": [{"text": "vector-space lexical similarity", "start_pos": 18, "end_pos": 49, "type": "TASK", "confidence": 0.8760042985280355}, {"text": "word sense discovery", "start_pos": 54, "end_pos": 74, "type": "TASK", "confidence": 0.8273275097211202}]}, {"text": "This paper shows how they can be combined to create an improved vector-space model of lexical semantics.", "labels": [], "entities": []}, {"text": "First, a word's contexts are clustered to produce groups of similar context vectors.", "labels": [], "entities": []}, {"text": "An average \"prototype\" vector is then computed separately for each cluster, producing a set of vectors for each word.", "labels": [], "entities": []}, {"text": "Finally, as described above, these cluster vectors can be used to determine the se-mantic similarity of both isolated words and words in context.", "labels": [], "entities": []}, {"text": "The approach is completely modular, and can integrate any clustering method with any traditional vector-space model.", "labels": [], "entities": []}, {"text": "We present experimental comparisons to human judgements of semantic similarity for both isolated words and words in sentential context.", "labels": [], "entities": []}, {"text": "The results demonstrate the superiority of a clustered approach over both traditional prototype and exemplar-based vector-space models.", "labels": [], "entities": []}, {"text": "For example, given the isolated target word singer our method produces the most similar word vocalist, while using a single prototype gives musician.", "labels": [], "entities": []}, {"text": "Given the word cell in the context: \"The book was published while Piasecki was still in prison, and a copy was delivered to his cell.\" the standard approach produces protein while our method yields incarcerated.", "labels": [], "entities": []}, {"text": "The remainder of the paper is organized as follows: Section 2 gives relevant background on prototype and exemplar methods for lexical semantics, Section 3 presents our multi-prototype method, Section 4 presents our experimental evaluations, Section 5 discusses future work, and Section 6 concludes.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Spearman correlation on the WordSim-353 dataset broken down by corpus and feature type.", "labels": [], "entities": [{"text": "Spearman correlation", "start_pos": 10, "end_pos": 30, "type": "METRIC", "confidence": 0.5616820305585861}, {"text": "WordSim-353 dataset", "start_pos": 38, "end_pos": 57, "type": "DATASET", "confidence": 0.97918701171875}]}]}