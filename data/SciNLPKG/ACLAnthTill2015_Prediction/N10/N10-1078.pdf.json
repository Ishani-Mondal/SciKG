{"title": [{"text": "Enabling Monolingual Translators: Post-Editing vs. Options", "labels": [], "entities": [{"text": "Enabling Monolingual Translators", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.6118945876757304}]}], "abstractContent": [{"text": "We carried out a study on monolingual translators with no knowledge of the source language , but aided by post-editing and the display of translation options.", "labels": [], "entities": []}, {"text": "On Arabic-English and Chinese-English, using standard test data and current statistical machine translation systems , 10 monolingual translators were able to translate 35% of Arabic and 28% of Chinese sentences correctly on average, with some of the participants coming close to professional bilingual performance on some of the documents.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 76, "end_pos": 107, "type": "TASK", "confidence": 0.7094988822937012}]}, {"text": "While machine translation systems have advanced greatly over the last decade, nobody seriously expects human-level performance anytime soon, except for very constraint settings.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 6, "end_pos": 25, "type": "TASK", "confidence": 0.8189790844917297}]}, {"text": "But are todays systems good enough to enable monolingual speakers of the target language without knowledge of the source language to generate correct translations?", "labels": [], "entities": []}, {"text": "And what type of assistance from machine translation is most helpful for such translators?", "labels": [], "entities": [{"text": "machine translation", "start_pos": 33, "end_pos": 52, "type": "TASK", "confidence": 0.7548405826091766}]}, {"text": "We carried out a study that involved monolin-gual translators who had no knowledge of Chinese and Arabic to translate documents from the NIST 2008 1 test sets, being assisted by statistical machine translation systems trained on data created under the GALE 2 research program.", "labels": [], "entities": [{"text": "NIST 2008 1 test sets", "start_pos": 137, "end_pos": 158, "type": "DATASET", "confidence": 0.976883590221405}, {"text": "statistical machine translation", "start_pos": 178, "end_pos": 209, "type": "TASK", "confidence": 0.6499160925547282}, {"text": "GALE 2 research program", "start_pos": 252, "end_pos": 275, "type": "DATASET", "confidence": 0.874049186706543}]}, {"text": "Our study shows that monolingual translators were able to translate 35% of Arabic and 28% of Chinese sentences, under a strict standard of correct-ness that scored professional bilingual translations as 61% and 66% correct for Arabic and Chinese, respectively.", "labels": [], "entities": []}, {"text": "We found also large variability among the participants and between the documents in the 1 http://www.itl.nist.gov/iad/mig/tests/mt/ 2 http://www.darpa.mil/ipto/programs/gale/gale.asp study, indicating the importance of general language skills and domain knowledge.", "labels": [], "entities": []}, {"text": "The results suggest that a skilled monolingual translator can compete with a bilingual translator, when using todays machine translation systems.", "labels": [], "entities": []}], "introductionContent": [], "datasetContent": [{"text": "Chinese-English is considered significantly harder than Arabic-English, as measured by automatic metrics (which measure similarity to a human reference translation), human evaluation metrics such as HTER (which measures the number of editing steps necessary to correct the output into an acceptable translation), or human judgment on the correctness of the translation, its fluency or adequacy (which is typically measured on a scale from 1 to 5).", "labels": [], "entities": [{"text": "HTER", "start_pos": 199, "end_pos": 203, "type": "METRIC", "confidence": 0.9207962155342102}]}, {"text": "All these metrics have been criticized in the past as too simple, biased towards statistical systems, non-repeatable, having low intra and inter-annotator agreement, or plainly too expensive to use.", "labels": [], "entities": []}, {"text": "How to properly evaluate machine translation quality is still an open problem.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 25, "end_pos": 44, "type": "TASK", "confidence": 0.7606388926506042}]}, {"text": "From the view of machine translation evaluation, this paper explores the question if current machine translation systems have reached the goal to bring across the meaning of a foreign text.", "labels": [], "entities": [{"text": "machine translation evaluation", "start_pos": 17, "end_pos": 47, "type": "TASK", "confidence": 0.8643894592920939}, {"text": "machine translation", "start_pos": 93, "end_pos": 112, "type": "TASK", "confidence": 0.7585448026657104}]}, {"text": "The ability of a monolingual target language speaker to produce correct translations (based on her understanding of the machine translation output) is a test for this goal.", "labels": [], "entities": []}, {"text": "It sets aside the problems of clumsy wordings and grammatical errors.", "labels": [], "entities": []}, {"text": "To relate this to traditional error metrics in machine translation: we focus on the adequacy opposed to the fluency of translation.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 47, "end_pos": 66, "type": "TASK", "confidence": 0.716804563999176}]}, {"text": "We trained translation models using Moses () on the bilingual data provided by the LDC, with additional monolingual data from the English Gigaword corpus for an interpolated 5-gram language model.", "labels": [], "entities": [{"text": "LDC", "start_pos": 83, "end_pos": 86, "type": "DATASET", "confidence": 0.869584858417511}, {"text": "English Gigaword corpus", "start_pos": 130, "end_pos": 153, "type": "DATASET", "confidence": 0.7683769265810648}]}, {"text": "Basic statistics about the corpus are given in.", "labels": [], "entities": []}, {"text": "The systems are close to the state of the art.", "labels": [], "entities": []}, {"text": "We used four news stories for each of the two languages for the monolingual translators.", "labels": [], "entities": []}, {"text": "The news stories were selected from the evaluation sets of the 2008 machine translation evaluation campaign organized by NIST.", "labels": [], "entities": [{"text": "machine translation evaluation", "start_pos": 68, "end_pos": 98, "type": "TASK", "confidence": 0.8029981454213461}, {"text": "NIST", "start_pos": 121, "end_pos": 125, "type": "DATASET", "confidence": 0.8065791726112366}]}, {"text": "The news stories are rather short (around 10 sentences each), since we opted fora variety of stories rather than long stories.", "labels": [], "entities": []}, {"text": "The evaluation set comes with four reference translations.", "labels": [], "entities": []}, {"text": "This allowed us to use one of the reference translation as gold standard for the evaluation, and the other three reference translations as competitors for the monolingual translations.", "labels": [], "entities": []}, {"text": "We recruited 10 monolingual translators, students at the University of Edinburgh for the study.", "labels": [], "entities": []}, {"text": "None of the students had knowledge of either Chinese or Arabic.", "labels": [], "entities": []}, {"text": "Each translator was given all eight stories to translate, half of the stories with only the machine translation output (Post-editing task) and half of the stories with interactive assistance as described in Section 3.2: prediction of sentence completion and translation options (Options).", "labels": [], "entities": [{"text": "prediction of sentence completion", "start_pos": 220, "end_pos": 253, "type": "TASK", "confidence": 0.8120580762624741}]}, {"text": "In both cases, we also displayed the Arabic or Chinese source sentence to the translator, which may show some clues regarding punctuation, numbers, or the length of source words.", "labels": [], "entities": []}, {"text": "The translators had no knowledge of the source script.", "labels": [], "entities": []}, {"text": "After all the translations were completed, we assessed the translation quality.", "labels": [], "entities": []}, {"text": "Since we did not have access to bilingual speakers for this, we resorted to the standard manual setup, where human judges are asked to assess the quality of each sentence transla-: Correctness of translations (with 95% confidence interval) under the two types of assistance, compared against professional reference translations tion compared to a reference translation in context -the first reference translation in the NIST evaluation set which was produced by a professional translation agency.", "labels": [], "entities": [{"text": "NIST evaluation set", "start_pos": 420, "end_pos": 439, "type": "DATASET", "confidence": 0.9563449621200562}]}, {"text": "We used a strict evaluation metric: a binary judgment, if the translation is correct.", "labels": [], "entities": []}, {"text": "Correct was defined as a fluent translation that contains the same meaning in the document context.", "labels": [], "entities": [{"text": "Correct", "start_pos": 0, "end_pos": 7, "type": "METRIC", "confidence": 0.8803390264511108}]}, {"text": "The reference translation was shown with its document context (two sentences before and after).", "labels": [], "entities": []}, {"text": "We used a variant of the web-based evaluation tool of the 2009 Workshop on Statistical Machine Translation.", "labels": [], "entities": [{"text": "Statistical Machine Translation", "start_pos": 75, "end_pos": 106, "type": "TASK", "confidence": 0.7385410269101461}]}], "tableCaptions": [{"text": " Table 1: Training data: number of sentences and English  words in the parallel training data", "labels": [], "entities": []}, {"text": " Table 2: News stories used in the experiment with headlines from the reference translation", "labels": [], "entities": []}, {"text": " Table 4: Correctness by translator (note: different bilin- gual translators for Arabic and Chinese)", "labels": [], "entities": [{"text": "Correctness", "start_pos": 10, "end_pos": 21, "type": "METRIC", "confidence": 0.9274780750274658}]}, {"text": " Table 5: Correctness by story and BLEU score of MT", "labels": [], "entities": [{"text": "Correctness", "start_pos": 10, "end_pos": 21, "type": "METRIC", "confidence": 0.9903601408004761}, {"text": "BLEU score", "start_pos": 35, "end_pos": 45, "type": "METRIC", "confidence": 0.982397735118866}, {"text": "MT", "start_pos": 49, "end_pos": 51, "type": "TASK", "confidence": 0.6598174571990967}]}, {"text": " Table 6: Correctness by sentence length", "labels": [], "entities": [{"text": "Correctness", "start_pos": 10, "end_pos": 21, "type": "METRIC", "confidence": 0.9441351890563965}]}]}