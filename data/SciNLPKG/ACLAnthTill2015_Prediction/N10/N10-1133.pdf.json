{"title": [{"text": "Quantifying the Limits and Success of Extractive Summarization Systems Across Domains", "labels": [], "entities": [{"text": "Extractive Summarization", "start_pos": 38, "end_pos": 62, "type": "TASK", "confidence": 0.6076489388942719}]}], "abstractContent": [{"text": "This paper analyzes the topic identification stage of single-document automatic text sum-marization across four different domains, consisting of newswire, literary, scientific and legal documents.", "labels": [], "entities": [{"text": "topic identification", "start_pos": 24, "end_pos": 44, "type": "TASK", "confidence": 0.7492778599262238}, {"text": "single-document automatic text sum-marization", "start_pos": 54, "end_pos": 99, "type": "TASK", "confidence": 0.5470065549015999}]}, {"text": "We present a study that explores the summary space of each domain via an exhaustive search strategy, and finds the probability density function (pdf) of the ROUGE score distributions for each domain.", "labels": [], "entities": [{"text": "probability density function (pdf)", "start_pos": 115, "end_pos": 149, "type": "METRIC", "confidence": 0.8314304252465566}]}, {"text": "We then use this pdf to calculate the per-centile rank of extractive summarization systems.", "labels": [], "entities": []}, {"text": "Our results introduce anew way to judge the success of automatic summarization systems and bring quantified explanations to questions such as why it was so hard for the systems to date to have a statistically significant improvement over the lead baseline in the news domain.", "labels": [], "entities": [{"text": "summarization", "start_pos": 65, "end_pos": 78, "type": "TASK", "confidence": 0.8054543137550354}]}], "introductionContent": [{"text": "Topic identification is the first stage of the generally accepted three-phase model in automatic text summarization, in which the goal is to identify the most important units in a document, i.e., phrases, sentences, or paragraphs ().", "labels": [], "entities": [{"text": "Topic identification", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.7877987325191498}, {"text": "automatic text summarization", "start_pos": 87, "end_pos": 115, "type": "TASK", "confidence": 0.6017315487066904}]}, {"text": "This stage is followed by the topic interpretation and summary generation steps where the identified units are further processed to bring the summary into a coherent, human readable abstract form.", "labels": [], "entities": [{"text": "topic interpretation", "start_pos": 30, "end_pos": 50, "type": "TASK", "confidence": 0.772113710641861}, {"text": "summary generation", "start_pos": 55, "end_pos": 73, "type": "TASK", "confidence": 0.7421792596578598}]}, {"text": "The extractive summarization systems, however, only employ the topic identification stage, and simply output a ranked list of the units according to a compression ratio criterion.", "labels": [], "entities": [{"text": "topic identification", "start_pos": 63, "end_pos": 83, "type": "TASK", "confidence": 0.7281002104282379}]}, {"text": "In general, for most systems sentences are the preferred units in this stage, as they are the smallest grammatical units that can express a statement.", "labels": [], "entities": []}, {"text": "Since the sentences in a document are reproduced verbatim in extractive summaries, it is theoretically possible to explore the search space of this problem through an enumeration of all possible extracts fora document.", "labels": [], "entities": []}, {"text": "Such an exploration would not only allow us to see how far we can go with extractive summarization, but we would also be able to judge the difficulty of the problem by looking at the distribution of the evaluation scores for the generated extracts.", "labels": [], "entities": [{"text": "extractive summarization", "start_pos": 74, "end_pos": 98, "type": "TASK", "confidence": 0.6422512531280518}]}, {"text": "Moreover, the high scoring extracts could also be used to train a machine learning algorithm.", "labels": [], "entities": []}, {"text": "However, such an enumeration strategy has an exponential complexity as it requires all possible sentence combinations of a document to be generated, constrained by a given word or sentence length.", "labels": [], "entities": []}, {"text": "Thus the problem quickly becomes impractical as the number of sentences in a document increases and the compression ratio decreases.", "labels": [], "entities": []}, {"text": "In this work, we try to overcome this bottleneck by using a large cluster of computers, and decomposing the task into smaller problems by using the given section boundaries or a linear text segmentation method.", "labels": [], "entities": []}, {"text": "As a result of this exploration, we generate a probability density function (pdf) of the ROUGE score) distributions for four different domains, which shows the distribution of the evaluation scores for the generated extracts, and allows us to assess the difficulty of each domain for extractive summarization.", "labels": [], "entities": [{"text": "ROUGE score", "start_pos": 89, "end_pos": 100, "type": "METRIC", "confidence": 0.9551040828227997}, {"text": "summarization", "start_pos": 295, "end_pos": 308, "type": "TASK", "confidence": 0.648531973361969}]}, {"text": "Furthermore, using these pdfs, we introduce anew success measure for extractive summarization systems.", "labels": [], "entities": [{"text": "extractive summarization", "start_pos": 69, "end_pos": 93, "type": "TASK", "confidence": 0.6004396080970764}]}, {"text": "Namely, given a system's average score over a data set, we show how to calculate the per-centile rank of this system from the corresponding pdf of the data set.", "labels": [], "entities": []}, {"text": "This allows us to seethe true improvement a system achieves over another, such as a baseline, and provides a standardized scoring scheme for systems performing on the same data set.", "labels": [], "entities": []}], "datasetContent": [{"text": "As mentioned in Section 1, an exhaustive search algorithm requires generating all possible sentence combinations from a document, and evaluating each one individually.", "labels": [], "entities": []}, {"text": "For example, using the values from, and assuming 20 words per sentence, we find that the search space for the news domain contains approximately 32 5 \u00d7 50 = 10, 068, 800 summaries.", "labels": [], "entities": []}, {"text": "The same calculation method for the scientific domain gives us 99 8 \u00d7 50 = 8.56 \u00d7 10 12 summaries.", "labels": [], "entities": []}, {"text": "Obviously the search space gets much bigger for the legal and literary domains due to their larger text size.", "labels": [], "entities": []}, {"text": "In order to be able to cope with such a huge search space, the first thing we did was to modify the ROUGE 1.5.5 4 Perl script by fixing the parameters to those used in the DUC experiments, and also by modifying the way it handles the input and output to make it suitable for streaming on the cluster.", "labels": [], "entities": []}, {"text": "The resulting script evaluates around 25-30 summaries per second on an Intel 2.33 GHz processor.", "labels": [], "entities": []}, {"text": "Next, we streamed the resulting ROUGE script for each (document, summary) pair on a large cluster of computers running on an Hadoop Map-Reduce framework.", "labels": [], "entities": []}, {"text": "Based on the size of the search space fora (document, summary) pair, the number of computers allocated in the cluster ranged from just a few to more than one thousand.", "labels": [], "entities": []}, {"text": "Although the combination of a large cluster and a faster ROUGE is enough to handle most of the documents in the news domain in just a few hours, a simple calculation shows that the problem is still impractical for the other domains.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 57, "end_pos": 62, "type": "METRIC", "confidence": 0.9911409020423889}]}, {"text": "Hence for the scientific, legal, and literary domains, rather than considering each document as a whole, we divide them into sections, and create extracts for each section such that the length of the extract is proportional to the length of the section in the original document.", "labels": [], "entities": []}, {"text": "For the legal and scientific domains, we use the given section boundaries (without considering the subsections for scientific documents).", "labels": [], "entities": []}, {"text": "For the novels, we treat each chapter as a single document (since each chapter has its own summary), which is further divided into sections using a publicly available linear http://berouge.com 5 -n 2 -x -m -2 4 -u -c 95 -r 1000 -f A -p 0.5 -t 0 6 http://hadoop.apache.org/ text segmentation algorithm by).", "labels": [], "entities": [{"text": "text segmentation", "start_pos": 273, "end_pos": 290, "type": "TASK", "confidence": 0.6918231844902039}]}, {"text": "In all cases, we let the algorithm pick the number of segments automatically.", "labels": [], "entities": []}, {"text": "To evaluate the sections, we modified ROUGE further so that it applies the length constraint to the extracts only, not to the model summaries.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 38, "end_pos": 43, "type": "METRIC", "confidence": 0.9737382531166077}]}, {"text": "This is due to the fact that we evaluate the extracts of each section individually against the whole model summary, which is larger than the extract.", "labels": [], "entities": []}, {"text": "This way, we can get an overall ROUGE recall score fora document extract, simply by summing up the recall scores of each section extracts.", "labels": [], "entities": [{"text": "ROUGE recall score", "start_pos": 32, "end_pos": 50, "type": "METRIC", "confidence": 0.8889647920926412}, {"text": "recall", "start_pos": 99, "end_pos": 105, "type": "METRIC", "confidence": 0.9965797066688538}]}, {"text": "The precision score for the entire document can also be found by adding the weighted precision scores for each section, where the weight is proportional to the length of the section in the original document.", "labels": [], "entities": [{"text": "precision score", "start_pos": 4, "end_pos": 19, "type": "METRIC", "confidence": 0.9872771501541138}, {"text": "precision", "start_pos": 85, "end_pos": 94, "type": "METRIC", "confidence": 0.9580066204071045}]}, {"text": "In our study, however, we only use recall scores.", "labels": [], "entities": [{"text": "recall", "start_pos": 35, "end_pos": 41, "type": "METRIC", "confidence": 0.9992190599441528}]}, {"text": "Note that, since for the legal, scientific, and literary domains we consider each section of a document independently, we are not performing a true exhaustive search for these domains, but rather solving a suboptimal problem, as we divide the number of words in the model summary to each section proportional to the section's length.", "labels": [], "entities": []}, {"text": "However, we believe that this is a fair assumption, as it has been shown repeatedly in the past that text segmentation helps improving the performance of text summarization systems).", "labels": [], "entities": [{"text": "text segmentation", "start_pos": 101, "end_pos": 118, "type": "TASK", "confidence": 0.7515445053577423}, {"text": "text summarization", "start_pos": 154, "end_pos": 172, "type": "TASK", "confidence": 0.6640229523181915}]}], "tableCaptions": [{"text": " Table 1: Statistical properties of the data set. \u00b5 Dw , and  \u00b5 Sw represent the average number of words for each doc- ument and summary respectively; \u00b5 R indicates the av- erage compression ratio; and \u00b5 C and \u00b5 Cw represent the  average number of sections for each document, and the  average number of words for each section respectively.", "labels": [], "entities": [{"text": "av- erage compression ratio", "start_pos": 169, "end_pos": 196, "type": "METRIC", "confidence": 0.6964902222156525}]}, {"text": " Table 2: Statistical properties of the pdfs", "labels": [], "entities": []}, {"text": " Table 3: ROUGE recall scores of the Lead baseline, Tex- tRank, and Random sentence selector across domains", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 10, "end_pos": 15, "type": "METRIC", "confidence": 0.9908546209335327}, {"text": "recall", "start_pos": 16, "end_pos": 22, "type": "METRIC", "confidence": 0.8805208206176758}, {"text": "Lead baseline", "start_pos": 37, "end_pos": 50, "type": "DATASET", "confidence": 0.8714617788791656}, {"text": "Tex- tRank", "start_pos": 52, "end_pos": 62, "type": "DATASET", "confidence": 0.8451106349627177}]}]}