{"title": [{"text": "Appropriately Handled Prosodic Breaks Help PCFG Parsing", "labels": [], "entities": [{"text": "Appropriately Handled Prosodic Breaks", "start_pos": 0, "end_pos": 37, "type": "TASK", "confidence": 0.697960801422596}, {"text": "Parsing", "start_pos": 48, "end_pos": 55, "type": "TASK", "confidence": 0.4008718729019165}]}], "abstractContent": [{"text": "This paper investigates using prosodic information in the form of ToBI break indexes for parsing spontaneous speech.", "labels": [], "entities": [{"text": "parsing spontaneous speech", "start_pos": 89, "end_pos": 115, "type": "TASK", "confidence": 0.918501635392507}]}, {"text": "We revisit two previously studied approaches, one that hurt parsing performance and one that achieved minor improvements, and propose anew method that aims to better integrate prosodic breaks into parsing.", "labels": [], "entities": [{"text": "parsing", "start_pos": 60, "end_pos": 67, "type": "TASK", "confidence": 0.969614565372467}]}, {"text": "Although these approaches can improve the performance of basic probabilistic context free grammar (PCFG) parsers, they all fail to produce fine-grained PCFG models with latent annotations (PCFG-LA) (Matsuzaki et al., 2005; Petrov and Klein, 2007) that perform significantly better than the baseline PCFG-LA model that does not use break indexes, partially due to mis-alignments between automatic prosodic breaks and true phrase boundaries.", "labels": [], "entities": [{"text": "probabilistic context free grammar (PCFG) parsers", "start_pos": 63, "end_pos": 112, "type": "TASK", "confidence": 0.675547681748867}]}, {"text": "We propose two alternative ways to restrict the search space of the prosodically enriched parser models to the n-best parses from the baseline PCFG-LA parser to avoid egregious parses caused by incorrect breaks.", "labels": [], "entities": []}, {"text": "Our experiments show that all of the prosodically enriched parser models can then achieve significant improvement over the baseline PCFG-LA parser.", "labels": [], "entities": []}], "introductionContent": [{"text": "Speech conveys more than a sequence of words to a listener.", "labels": [], "entities": []}, {"text": "An important additional type of information that phoneticians investigate is called prosody, which includes phenomena such as pauses, pitch, energy, duration, grouping, and emphasis.", "labels": [], "entities": []}, {"text": "For a review of the role of prosody in processing spoken language, see ().", "labels": [], "entities": []}, {"text": "Prosody can help with the disambiguation of lexical meaning (via accents and tones) and sentence type (e.g., yesno question versus statement), provide discourselevel information like focus, prominence, and discourse segment, and help a listener to discern a speaker's emotion or hesitancy, etc.", "labels": [], "entities": []}, {"text": "Prosody often draws a listener's attention to important information through contrastive pitch or duration patterns associated words or phrases.", "labels": [], "entities": []}, {"text": "In addition, prosodic cues can help one to segment speech into chunks that are hypothesized to have a hierarchical structure, although not necessarily identical to that of syntax.", "labels": [], "entities": []}, {"text": "This suggests that prosodic cues may help in the parsing of speech inputs, the topic of this paper.", "labels": [], "entities": [{"text": "parsing of speech inputs", "start_pos": 49, "end_pos": 73, "type": "TASK", "confidence": 0.8838142454624176}]}, {"text": "Prosodic information such as pause length, duration of words and phones, pitch contours, energy contours, and their normalized values have been used in speech processing tasks like sentence boundary detection ( ).", "labels": [], "entities": [{"text": "sentence boundary detection", "start_pos": 181, "end_pos": 208, "type": "TASK", "confidence": 0.6639552613099416}]}, {"text": "In contrast, other researchers use linguistic encoding schemes like, which encodes tones, the degree of juncture between words, and prominence symbolically.", "labels": [], "entities": []}, {"text": "For example, a simplified ToBI encoding scheme uses the symbol 4 for major intonational breaks, p for hesitation, and 1 for all other breaks.", "labels": [], "entities": [{"text": "ToBI encoding", "start_pos": 26, "end_pos": 39, "type": "TASK", "confidence": 0.5813679844141006}]}, {"text": "In the literature, there have been several attempts to integrate prosodic information to improve parse accuracy of speech transcripts.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 103, "end_pos": 111, "type": "METRIC", "confidence": 0.916199266910553}]}, {"text": "These studies have used either quantized acoustic measurements of prosody or automatically detected break indexes.", "labels": [], "entities": []}, {"text": "attempted to integrate quantized prosodic features as additional tokens in the same manner that punctuation marks are added into text.", "labels": [], "entities": []}, {"text": "Although punctuation marks can significantly improve parse accuracy of newswire text, the quantized prosodic tokens were found harm-ful to parse accuracy when inserted into humangenerated speech transcripts of the Switchboard corpus.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 59, "end_pos": 67, "type": "METRIC", "confidence": 0.8593598008155823}, {"text": "accuracy", "start_pos": 145, "end_pos": 153, "type": "METRIC", "confidence": 0.959227442741394}, {"text": "Switchboard corpus", "start_pos": 214, "end_pos": 232, "type": "DATASET", "confidence": 0.9241454601287842}]}, {"text": "The authors hypothesized that the inserted pseudo-punctuation break n-gram dependencies in the parser model, leading to lower accuracies.", "labels": [], "entities": [{"text": "accuracies", "start_pos": 126, "end_pos": 136, "type": "METRIC", "confidence": 0.9789844155311584}]}, {"text": "However, another possible cause is that the prosody has not been effectively utilized due to the fact that it is overloaded; it not only provides information about phrases, but also about the state of the speaker and his/her sentence planning process.", "labels": [], "entities": []}, {"text": "Hence, the prosodic information may at times be more harmful than helpful to parsing performance.", "labels": [], "entities": [{"text": "parsing", "start_pos": 77, "end_pos": 84, "type": "TASK", "confidence": 0.9861150979995728}]}, {"text": "Ina follow-on experiment,, instead of using raw quantized prosodic features, used three classes of automatically detected ToBI break indexes (1, 4, or p) and their posteriors.", "labels": [], "entities": []}, {"text": "Rather than directly incorporating the breaks into the parse trees, they used the breaks to generate additional features for re-ranking the n-best parse trees from a generative parsing model trained without prosody.", "labels": [], "entities": [{"text": "generative parsing", "start_pos": 166, "end_pos": 184, "type": "TASK", "confidence": 0.7983991503715515}]}, {"text": "They were able to obtain a significant 0.6% improvement on Switchboard over the generative parser, and a more modest 0.1% to 0.2% improvement over the reranking model that also utilizes syntactic features.", "labels": [], "entities": [{"text": "generative parser", "start_pos": 80, "end_pos": 97, "type": "TASK", "confidence": 0.9593676328659058}]}, {"text": "added prosodic breaks into a generative parsing model with latent variables.", "labels": [], "entities": [{"text": "generative parsing", "start_pos": 29, "end_pos": 47, "type": "TASK", "confidence": 0.9664922952651978}]}, {"text": "They utilized three classes of ToBI break indexes (1, 4, and p), automatically predicted by the approach described in).", "labels": [], "entities": []}, {"text": "Breaks were modeled as a sequence of observations parallel to the sentence and each break was generated by the preterminal of the preceding word, assuming that the observation of a break, b, was conditionally independent of its preceding word, w, given preterminal X: Their approach has advantages over () in that it does not break n-gram dependencies in parse modeling.", "labels": [], "entities": []}, {"text": "It also has disadvantages in that the breaks are modeled by preterminals rather than higher level nonterminals, and thus cannot directly affect phrasing in a basic PCFG grammar.", "labels": [], "entities": []}, {"text": "However, they addressed this independence drawback by splitting each nonterminal into latent tags so that the impact of prosodic breaks could be percolated into the phrasing process through the interaction of latent tags.", "labels": [], "entities": []}, {"text": "They achieved a minor 0.2% improvement over their baseline model without prosodic cues and also found that prosodic breaks can be used to build more compact grammars.", "labels": [], "entities": []}, {"text": "In this paper, we re-investigate the models of () and, and propose anew way of modeling that can potentially address the shortcomings of the two previous approaches.", "labels": [], "entities": []}, {"text": "We also attribute part of the failure or ineffectiveness of the previously investigated approaches to errors in the quantized prosodic tokens or automatic break indexes, which are predicted based only on acoustic cues and could misalign with phrase boundaries.", "labels": [], "entities": []}, {"text": "We illustrate that these prosodically enriched models are in fact highly effective if we systematically eliminate bad phrase and hesitation breaks given their projection onto the reference parse trees.", "labels": [], "entities": []}, {"text": "Inspired by this, we propose two alternative rescoring methods to restrict the search space of the prosodically enriched parser models to the n-best parses from the baseline PCFG-LA parser to avoid egregious parse trees.", "labels": [], "entities": []}, {"text": "The effectiveness of our rescoring method suggests that the reranking approach of () was successful not only because of their prosodic feature design, but also because they restrict the search space for reranking to n-best lists generated by a syntactic model alone.", "labels": [], "entities": []}], "datasetContent": [{"text": "Due to our goal of investigating the effect of prosodic information on the accuracy of state of the art parsing of conversational speech, we utilize both Penn Switchboard () and Fisher treebanks), for which we also had automatically generated break indexes from 1 . The Fisher treebank is a higher quality parsing resource than Switchboard due to its greater use of audio and refined specifications for sentence segmentation and disfluency markups, and so we utilize its eval set for our parser evaluation; the first 1,020 trees (7,184 words) were used for development and the remaining 3,917 trees (29,173 words) for evaluation.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 75, "end_pos": 83, "type": "METRIC", "confidence": 0.9925946593284607}, {"text": "Penn", "start_pos": 154, "end_pos": 158, "type": "DATASET", "confidence": 0.9752269983291626}, {"text": "Fisher treebanks", "start_pos": 178, "end_pos": 194, "type": "DATASET", "confidence": 0.8882162570953369}, {"text": "sentence segmentation", "start_pos": 403, "end_pos": 424, "type": "TASK", "confidence": 0.7349385619163513}]}, {"text": "We utilized the Fisher dev1 and dev2 sets containing 16,519 trees (112,717 words) as the main training data source and used the Penn Switchboard treebank containing 110,504 trees (837,863 words) as an additional training source to evaluate the effect of training data size on parsing performance.", "labels": [], "entities": [{"text": "Penn Switchboard treebank", "start_pos": 128, "end_pos": 153, "type": "DATASET", "confidence": 0.9525588154792786}]}, {"text": "The treebank trees are normalized by downcasing all terminal strings and deleting punctuation, empty nodes, and nonterminal-yield unary rules that are not related to edits.", "labels": [], "entities": []}, {"text": "We will compare 2 three prosodically enriched PCFG models described in the next section, with a baseline PCFG parser.", "labels": [], "entities": []}, {"text": "We will also utilize a state of the art PCFG-LA parser to examine the effect of prosodic enrichment 3 . Unlike (), we do not remove EDITED regions prior to parsing because parsing of EDITED regions is likely to benefit from prosodic information.", "labels": [], "entities": []}, {"text": "Also, parses from all models are compared with the gold standard parses in the Fisher evaluation set using SParseval bracket scoring) without flattening the EDITED constituents.", "labels": [], "entities": [{"text": "Fisher evaluation set", "start_pos": 79, "end_pos": 100, "type": "DATASET", "confidence": 0.6151330769062042}]}], "tableCaptions": [{"text": " Table 1: Fisher evaluation parsing results for the basic  PCFGs without latent annotations trained on the Fisher  training set.", "labels": [], "entities": [{"text": "Fisher evaluation parsing", "start_pos": 10, "end_pos": 35, "type": "TASK", "confidence": 0.8021944165229797}, {"text": "Fisher  training set", "start_pos": 107, "end_pos": 127, "type": "DATASET", "confidence": 0.7846052646636963}]}, {"text": " Table 2: F scores of the seven most frequent non- terminals of the REGULAR, BRKPHRASE, and BRK- PHRASE+DIRECTRESCORE models.", "labels": [], "entities": [{"text": "F scores", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9723619520664215}, {"text": "REGULAR", "start_pos": 68, "end_pos": 75, "type": "METRIC", "confidence": 0.8262113928794861}, {"text": "BRKPHRASE", "start_pos": 77, "end_pos": 86, "type": "DATASET", "confidence": 0.7193320393562317}]}]}