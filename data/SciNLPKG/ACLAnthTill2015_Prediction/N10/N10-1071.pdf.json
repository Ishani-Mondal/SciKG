{"title": [{"text": "Learning Dense Models of Query Similarity from User Click Logs", "labels": [], "entities": []}], "abstractContent": [{"text": "The goal of this work is to integrate query similarity metrics as features into a dense model that can be trained on large amounts of query log data, in order to rank query rewrites.", "labels": [], "entities": []}, {"text": "We propose features that incorporate various notions of syntactic and semantic similarity in a generalized edit distance framework.", "labels": [], "entities": []}, {"text": "We use the implicit feedback of user clicks on search results as weak labels in training linear ranking models on large data sets.", "labels": [], "entities": []}, {"text": "We optimize different ranking objectives in a stochastic gradient descent framework.", "labels": [], "entities": []}, {"text": "Our experiments show that a pairwise SVM ranker trained on multipartite rank levels outperforms other pairwise and listwise ranking methods under a variety of evaluation metrics.", "labels": [], "entities": []}], "introductionContent": [{"text": "Measures of query similarity are used fora wide range of web search applications, including query expansion, query suggestions, or listings of related queries.", "labels": [], "entities": [{"text": "query expansion", "start_pos": 92, "end_pos": 107, "type": "TASK", "confidence": 0.7426982522010803}]}, {"text": "Several recent approaches deploy user query logs to learn query similarities.", "labels": [], "entities": []}, {"text": "One set of approaches focuses on user reformulations of queries that differ only in one phrase, e.g.,.", "labels": [], "entities": [{"text": "user reformulations of queries", "start_pos": 33, "end_pos": 63, "type": "TASK", "confidence": 0.7794123962521553}]}, {"text": "Such phrases are then identified as candidate expansion terms, and filtered by various signals such as co-occurrence in similar sessions, or loglikelihood ratio of original and expansion phrase.", "labels": [], "entities": []}, {"text": "Other approaches focus on the relation of queries and search results, either by clustering queries based * The work presented in this paper was done while the authors were visiting Google Research, Z\u00fcrich.", "labels": [], "entities": []}, {"text": "on their search results, e.g.,, or by deploying the graph of queries and results to find related queries, e.g.,.", "labels": [], "entities": []}, {"text": "The approach closest to ours is that of.", "labels": [], "entities": []}, {"text": "Similar to their approach, we create a training set of candidate query rewrites from user query logs, and use it to train learners.", "labels": [], "entities": []}, {"text": "While the dataset used in is in the order of a few thousand query-rewrite pairs, our dataset comprises around 1 billion query-rewrite pairs.", "labels": [], "entities": []}, {"text": "Clearly, manual labeling of rewrite quality is not feasible for our dataset, and perhaps not even desirable.", "labels": [], "entities": []}, {"text": "Instead, our intent is to learn from large amounts of user query log data.", "labels": [], "entities": []}, {"text": "Such data permit to learn smooth models because of the effectiveness of large data sets to capture even rare aspects of language, and they also are available as in the wild, i.e., they reflect the actual input-output behaviour that we seek to automate).", "labels": [], "entities": []}, {"text": "We propose a technique to automatically create weak labels from co-click information in user query logs of search engines.", "labels": [], "entities": []}, {"text": "The central idea is that two queries are related if they lead to user clicks on the same documents fora large amount of documents.", "labels": [], "entities": []}, {"text": "A manual evaluation of a small subset showed that a determination of positive versus negative rewrites by thresholding the number of co-clicks correlates well with human judgements of similarity, thus justifying our method of eliciting labels from co-clicks.", "labels": [], "entities": []}, {"text": "Similar to, the features of our models are not based on word identities, but instead on general string similarity metrics.", "labels": [], "entities": []}, {"text": "This leads to dense rather than sparse feature spaces.", "labels": [], "entities": []}, {"text": "The dif-ference of our approach to lies in our particular choice of string similarity metrics.", "labels": [], "entities": []}, {"text": "While deploy \"syntactic\" features such as Levenshtein distance, and \"semantic\" features such as log-likelihood ratio or mutual information, we combine syntactic and semantic aspects into generalized edit-distance features where the cost of each edit operation is weighted by various term probability models.", "labels": [], "entities": [{"text": "Levenshtein distance", "start_pos": 42, "end_pos": 62, "type": "METRIC", "confidence": 0.6969181895256042}]}, {"text": "Lastly, the learners used in our approach are applicable to very large datasets by an integration of linear ranking models into a stochastic gradient descent framework for optimization.", "labels": [], "entities": []}, {"text": "We compare several linear ranking models, including a log-linear probability model for bipartite ranking, and pairwise and listwise SVM rankers.", "labels": [], "entities": []}, {"text": "We show in an experimental evaluation that a pairwise SVM ranker trained on multipartite rank levels outperforms state-of-the-art pairwise and listwise ranking methods under a variety of evaluation metrics.", "labels": [], "entities": []}], "datasetContent": [{"text": "In the experiments reported in this paper, we trained linear ranking models on 1 billion query-rewrite pairs using 60 dense features, combined of the building blocks of syntactic and semantic similarity metrics under different estimations of cost matrices.", "labels": [], "entities": []}, {"text": "Development testing was done on a data set that was held-out from the training set.", "labels": [], "entities": []}, {"text": "Final testing was carried out on the manually labeled dataset.", "labels": [], "entities": []}, {"text": "Data statistics for all sets are given in.", "labels": [], "entities": []}, {"text": "Model selection was performed by adjusting meta-parameters on the development set.", "labels": [], "entities": [{"text": "Model selection", "start_pos": 0, "end_pos": 15, "type": "TASK", "confidence": 0.6929529756307602}]}, {"text": "We trained each model at constant learning rates \u03b7 \u2208 {1, 0.5, 0.1, 0.01, 0.001}, and evaluated each variant after every fifth out of 100 passes over the training set.", "labels": [], "entities": []}, {"text": "The variant with the highest MAP score on the development set was chosen and evaluated on the test set.", "labels": [], "entities": [{"text": "MAP score", "start_pos": 29, "end_pos": 38, "type": "METRIC", "confidence": 0.964661717414856}]}, {"text": "This early stopping routine also served for regularization.", "labels": [], "entities": [{"text": "regularization", "start_pos": 44, "end_pos": 58, "type": "TASK", "confidence": 0.939350426197052}]}, {"text": "Evaluation results for the systems are reported in.", "labels": [], "entities": []}, {"text": "We evaluate all models according to the following evaluation metrics: Mean Average Precision (MAP), Normalized Discounted Cumulative Gain with a cutoff at rank 10 (NDCG@10), Area-underthe-ROC-curve (AUC), Precision@n 3 . As baselines we report a random permutation of rewrites (random), and the single dense feature that performed best on the development set (best-feature).", "labels": [], "entities": [{"text": "Mean Average Precision (MAP)", "start_pos": 70, "end_pos": 98, "type": "METRIC", "confidence": 0.9684337476889292}, {"text": "Area-underthe-ROC-curve (AUC)", "start_pos": 174, "end_pos": 203, "type": "METRIC", "confidence": 0.9576881378889084}]}, {"text": "The latter is the log-probability assigned to the query-rewrite pair by the probabilistic clustering model used for cost matrix estimation (see Section 2.5).", "labels": [], "entities": [{"text": "cost matrix estimation", "start_pos": 116, "end_pos": 138, "type": "TASK", "confidence": 0.6670936942100525}]}, {"text": "P-values are reported in for all pairwise comparisons of systems (except the random baseline) using an Approximate Randomization test where stratified shuffling is applied to results on the query level (see).", "labels": [], "entities": [{"text": "Approximate", "start_pos": 103, "end_pos": 114, "type": "METRIC", "confidence": 0.9906644821166992}]}, {"text": "The rows in are ranked according to MAP values of the systems.", "labels": [], "entities": [{"text": "MAP", "start_pos": 36, "end_pos": 39, "type": "METRIC", "confidence": 0.9606212973594666}]}, {"text": "SVM-multipartite outperforms all other ranking systems under all evaluation metrics at a significance level \u2265 0.995.", "labels": [], "entities": []}, {"text": "For all other pairwise comparisons of result differences, we find result differences of systems ranked next to each other to be not statistically significant.", "labels": [], "entities": []}, {"text": "All systems outperform the random and best-feature baselines with statistically significant result differences.", "labels": [], "entities": []}, {"text": "The distinctive advantage of the SVM-multipartite models lies in the possibil-", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Statistics of co-click data sets.  train  dev  test  number of queries  250,000 2,500 100  average number of  rewrites per query  4,500  4,500 30  percentage of rewrites  with \u2265 10 coclicks  0.2  0.2  43", "labels": [], "entities": []}, {"text": " Table 2: Experimental evaluation of random and best feature baselines, and log-linear, SVM-MAP, SVM-bipartite,  SVM-multipartite, and SVM-multipartite-margin-rescaled learning-to-rank models on manually labeled test set.", "labels": [], "entities": []}]}