{"title": [{"text": "A Learning-based Sampling Approach to Extractive Summarization", "labels": [], "entities": [{"text": "Extractive Summarization", "start_pos": 38, "end_pos": 62, "type": "TASK", "confidence": 0.7383714616298676}]}], "abstractContent": [{"text": "In this paper we present a novel resampling model for extractive meeting summarization.", "labels": [], "entities": [{"text": "extractive meeting summarization", "start_pos": 54, "end_pos": 86, "type": "TASK", "confidence": 0.8284893234570821}]}, {"text": "With resampling based on the output of a base-line classifier, our method outperforms previous research in the field.", "labels": [], "entities": []}, {"text": "Further, we compare an existing resampling technique with our model.", "labels": [], "entities": []}, {"text": "We report on an extensive series of experiments on a large meeting corpus which leads to classification improvement in weighted precision and f-score.", "labels": [], "entities": [{"text": "classification", "start_pos": 89, "end_pos": 103, "type": "TASK", "confidence": 0.8637247681617737}, {"text": "precision", "start_pos": 128, "end_pos": 137, "type": "METRIC", "confidence": 0.9054197669029236}]}], "introductionContent": [{"text": "Feature-based machine learning approaches have become a standard technique in the field of extractive summarization wherein the most important sections within a meeting transcripts need to be identified.", "labels": [], "entities": [{"text": "extractive summarization", "start_pos": 91, "end_pos": 115, "type": "TASK", "confidence": 0.8519845902919769}]}, {"text": "We perceive the problem as recognizing the most extract-worthy meeting dialog acts (DAs) in a binary classification framework.", "labels": [], "entities": []}, {"text": "In this paper, firstly, in section 4 we create a gold standard to train the classifier, by improvising upon the existing annotations in our meeting corpus.", "labels": [], "entities": []}, {"text": "Then in section 5 we present actual numbers which display a very skewed class distribution to learn for the binary classifier.", "labels": [], "entities": []}, {"text": "This skewness is attributed to the less number of actual extract-worthy and important DAs (positive examples) compared to ordinary chit-chat, backchannel noises etc (negative examples) spoken during the course of the meeting.", "labels": [], "entities": [{"text": "DAs", "start_pos": 86, "end_pos": 89, "type": "METRIC", "confidence": 0.9046549797058105}]}, {"text": "We tackle this data skewness with a novel resampling approach which reselects the data set to create a more comparable class distribution between these postive and negative instances.", "labels": [], "entities": []}, {"text": "Resampling methods have been found effective in catering to the data imbalance problem mentioned above.) used a resampling module for chemical named entity recognition.", "labels": [], "entities": [{"text": "chemical named entity recognition", "start_pos": 134, "end_pos": 167, "type": "TASK", "confidence": 0.647054560482502}]}, {"text": "The pre-classifier, based on n-gram character features, assigned a probability of being a chemical word, to each token.", "labels": [], "entities": []}, {"text": "Only tokens having probability greater than a predefined threshold were preserved and the output of the first stage classification along with word suffix were used as features in further classification steps.", "labels": [], "entities": []}, {"text": "() used a hybrid approach for Computational Anaphora Resolution (CAR) combining rule based filtering with Memory based learning to reduce the huge population of anaphora/candidate-antecedent pairs.", "labels": [], "entities": [{"text": "Computational Anaphora Resolution (CAR", "start_pos": 30, "end_pos": 68, "type": "TASK", "confidence": 0.775244164466858}]}, {"text": "(, in their experimentation on the ICSI meeting corpus, employ the salience scores generated by a TFIDF classifier in the resampling task.", "labels": [], "entities": [{"text": "ICSI meeting corpus", "start_pos": 35, "end_pos": 54, "type": "DATASET", "confidence": 0.9113017121950785}]}, {"text": "We discuss the actual technique and our resampling module further in section 6.", "labels": [], "entities": [{"text": "resampling", "start_pos": 40, "end_pos": 50, "type": "TASK", "confidence": 0.9610787034034729}]}, {"text": "We compare its performance with the TFIDF model of () in section 8.2 and observe a general improvement in summary scores through resampling.", "labels": [], "entities": [{"text": "TFIDF", "start_pos": 36, "end_pos": 41, "type": "METRIC", "confidence": 0.9018283486366272}]}], "datasetContent": [{"text": "The main metric we use for evaluating the summaries is the extension of the weighted precision evaluation scheme introduced by.", "labels": [], "entities": []}, {"text": "The measure relies on having multiple annotations fora meeting and a many-to-many mapping discussed in section 2.", "labels": [], "entities": []}, {"text": "To calculate weighted precision, the number of times that each extractive summary DA was linked by each annotator is counted and averaged to get a single DA score.", "labels": [], "entities": [{"text": "precision", "start_pos": 22, "end_pos": 31, "type": "METRIC", "confidence": 0.9925920963287354}, {"text": "DA score", "start_pos": 154, "end_pos": 162, "type": "METRIC", "confidence": 0.9327836334705353}]}, {"text": "The DA scores are then averaged overall DAs in the summary to get the weighted precision score for the entire summary.", "labels": [], "entities": [{"text": "DA", "start_pos": 4, "end_pos": 6, "type": "METRIC", "confidence": 0.7342261075973511}, {"text": "DAs", "start_pos": 40, "end_pos": 43, "type": "METRIC", "confidence": 0.9854261875152588}, {"text": "precision score", "start_pos": 79, "end_pos": 94, "type": "METRIC", "confidence": 0.9704477488994598}]}, {"text": "The total number of links in an extractive summary divided by the total number of links to the abstract as a whole gives the weighted recall score.", "labels": [], "entities": [{"text": "recall", "start_pos": 134, "end_pos": 140, "type": "METRIC", "confidence": 0.970902144908905}]}, {"text": "By this definition, weighted recall can have a maximum score of 1 since it is a fraction of the total links for the entire summary.", "labels": [], "entities": [{"text": "recall", "start_pos": 29, "end_pos": 35, "type": "METRIC", "confidence": 0.9848909974098206}]}, {"text": "Also, there is no theoretical maximum for weighted precision as annotators were allowed to create any number of links fora single DA.", "labels": [], "entities": [{"text": "precision", "start_pos": 51, "end_pos": 60, "type": "METRIC", "confidence": 0.9610480666160583}]}, {"text": "Both weighted precision and recall share the same numerator: num = \u03a3 d L d /N where L dis the number of links fora DA din the extractive summary, and N is the number of annotators.", "labels": [], "entities": [{"text": "precision", "start_pos": 14, "end_pos": 23, "type": "METRIC", "confidence": 0.9902713894844055}, {"text": "recall", "start_pos": 28, "end_pos": 34, "type": "METRIC", "confidence": 0.9977396726608276}]}, {"text": "Weighted precision is equal to wp = num/D s where D sis the number of DAs in the extractive summary.", "labels": [], "entities": [{"text": "precision", "start_pos": 9, "end_pos": 18, "type": "METRIC", "confidence": 0.9920394420623779}]}, {"text": "Weighted recall is given by recall = num/(L t /N ) where Lt is the total number of links made between DAs and abstract sentences by all annotators, and N is the number of annotators.", "labels": [], "entities": [{"text": "recall", "start_pos": 9, "end_pos": 15, "type": "METRIC", "confidence": 0.9945091605186462}, {"text": "recall", "start_pos": 28, "end_pos": 34, "type": "METRIC", "confidence": 0.9991037249565125}, {"text": "Lt", "start_pos": 57, "end_pos": 59, "type": "METRIC", "confidence": 0.990581214427948}]}, {"text": "The f-score is calculated as: In simple terms a DA which might be discussing an important meeting topic e.g. selling price of the remote control etc is more likely to be linked by more than one annotator and possibly more than once by an annotator.", "labels": [], "entities": []}, {"text": "Therefore the high scoring DAs are in away indicative of quintessential topics and agenda points of the meeting.", "labels": [], "entities": []}, {"text": "Hence, weighted precision which is number of links per annotator averaged overall the meeting DAs is a figure that aligns itself with average information content per DA in the summary.", "labels": [], "entities": [{"text": "precision", "start_pos": 16, "end_pos": 25, "type": "METRIC", "confidence": 0.8320943713188171}]}, {"text": "Low scoring meeting chit-chats will tend to bring the precision score down.", "labels": [], "entities": [{"text": "precision score", "start_pos": 54, "end_pos": 69, "type": "METRIC", "confidence": 0.9866904318332672}]}, {"text": "We report a weighted precision of 1.33 for 700 word summary extracted using the procedure described in 2 for obtaining gold standard.", "labels": [], "entities": [{"text": "precision", "start_pos": 21, "end_pos": 30, "type": "METRIC", "confidence": 0.9549136757850647}]}, {"text": "This is hence a ceiling to the weighted precision score that can be obtained by any summary corresponding to this compression rate.", "labels": [], "entities": [{"text": "precision score", "start_pos": 40, "end_pos": 55, "type": "METRIC", "confidence": 0.9528655409812927}]}, {"text": "Weighted Recall on the other hand signifies total information content of the meeting.", "labels": [], "entities": [{"text": "Weighted Recall", "start_pos": 0, "end_pos": 15, "type": "METRIC", "confidence": 0.749549150466919}]}, {"text": "For intelligent systems in general the recall rate increases with increasing summary compression rates while weighted precision decreases 2 . Since we experiment with short summaries that have at most 700 words, we do most of the comparisons in terms of weighted precision values.", "labels": [], "entities": [{"text": "recall rate", "start_pos": 39, "end_pos": 50, "type": "METRIC", "confidence": 0.9867286086082458}, {"text": "precision", "start_pos": 118, "end_pos": 127, "type": "METRIC", "confidence": 0.7433022260665894}]}, {"text": "In the final system evaluation in section 8.3, we include weighted recall and f-score values.", "labels": [], "entities": [{"text": "recall", "start_pos": 67, "end_pos": 73, "type": "METRIC", "confidence": 0.9782596826553345}, {"text": "f-score", "start_pos": 78, "end_pos": 85, "type": "METRIC", "confidence": 0.9348459243774414}]}, {"text": "shows the weighted precision results on training an SVM classifier with different gold standard thresholds.", "labels": [], "entities": [{"text": "precision", "start_pos": 19, "end_pos": 28, "type": "METRIC", "confidence": 0.9457300901412964}]}, {"text": "For example, at a threshold of 60%, the top 60% of the linked DA segments are defined as the gold standard positive examples, all other DA segments of the meeting are defined as negative, non-extraction worthy.", "labels": [], "entities": []}, {"text": "The tests are performed on a single stage classifier similar to.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3: TFIDF weighted Precision, f-score for 700 and  1000 word summaries", "labels": [], "entities": [{"text": "TFIDF weighted", "start_pos": 10, "end_pos": 24, "type": "METRIC", "confidence": 0.9157182276248932}, {"text": "Precision", "start_pos": 25, "end_pos": 34, "type": "METRIC", "confidence": 0.6208680868148804}]}, {"text": " Table 4: weighted precision, f-scores on LBS model", "labels": [], "entities": [{"text": "precision", "start_pos": 19, "end_pos": 28, "type": "METRIC", "confidence": 0.9246712923049927}]}, {"text": " Table 5: Results on the AMI corpus.", "labels": [], "entities": [{"text": "AMI corpus", "start_pos": 25, "end_pos": 35, "type": "DATASET", "confidence": 0.8211226463317871}]}]}