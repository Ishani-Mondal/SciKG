{"title": [{"text": "Improved Models of Distortion Cost for Statistical Machine Translation", "labels": [], "entities": [{"text": "Statistical Machine Translation", "start_pos": 39, "end_pos": 70, "type": "TASK", "confidence": 0.8684629599253336}]}], "abstractContent": [{"text": "The distortion cost function used in Moses-style machine translation systems has two flaws.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 49, "end_pos": 68, "type": "TASK", "confidence": 0.6905055493116379}]}, {"text": "First, it does not estimate the future cost of known required moves, thus increasing search errors.", "labels": [], "entities": []}, {"text": "Second, all distortion is penalized linearly, even when appropriate re-orderings are performed.", "labels": [], "entities": []}, {"text": "Because the cost function does not effectively constrain search, translation quality decreases at higher distortion limits, which are often needed when translating between languages of different ty-pologies such as Arabic and English.", "labels": [], "entities": []}, {"text": "To address these problems, we introduce a method for estimating future linear distortion cost, and anew discriminative distortion model that predicts word movement during translation.", "labels": [], "entities": []}, {"text": "In combination, these extensions give a statistically significant improvement over a base-line distortion parameterization.", "labels": [], "entities": []}, {"text": "When we triple the distortion limit, our model achieves a +2.32 BLEU average gain over Moses.", "labels": [], "entities": [{"text": "distortion", "start_pos": 19, "end_pos": 29, "type": "METRIC", "confidence": 0.9857938885688782}, {"text": "BLEU average gain", "start_pos": 64, "end_pos": 81, "type": "METRIC", "confidence": 0.9719217816988627}]}], "introductionContent": [{"text": "It is well-known that translation performance in Moses-style () machine translation (MT) systems deteriorates when high distortion is allowed.", "labels": [], "entities": [{"text": "translation", "start_pos": 22, "end_pos": 33, "type": "TASK", "confidence": 0.9719758033752441}, {"text": "machine translation (MT)", "start_pos": 64, "end_pos": 88, "type": "TASK", "confidence": 0.8545894265174866}]}, {"text": "The linear distortion cost model used in these systems is partly at fault.", "labels": [], "entities": []}, {"text": "It includes no estimate of future distortion cost, thereby increasing the risk of search errors.", "labels": [], "entities": []}, {"text": "Linear distortion also penalizes all re-orderings equally, even when appropriate re-orderings are performed.", "labels": [], "entities": [{"text": "Linear distortion", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.77752485871315}]}, {"text": "Because linear distortion, which is a soft constraint, does not effectively constrain search, a distortion limit is imposed on the translation model.", "labels": [], "entities": []}, {"text": "But hard constraints are ultimately undesirable since they prune the search space.", "labels": [], "entities": []}, {"text": "For languages with very different word or- Figure 1: The oracle translation for this Arabic VOS sentence would be pruned during search using typical distortion parameters.", "labels": [], "entities": []}, {"text": "The Arabic phrases read right-to-left, but we have ordered the sentence from left-to-right in order to clearly illustrate the re-ordering problem.", "labels": [], "entities": []}, {"text": "ders in which significant re-ordering is required, the distortion limit can eliminate the oracle, or \"best,\" translation prior to search, placing an artificial limit on translation performance (.", "labels": [], "entities": []}, {"text": "To illustrate this problem, consider the ArabicEnglish example in.", "labels": [], "entities": []}, {"text": "Assuming that the English translation is constructed left-to-right, the verb shaaraka must be translated after the noun phrase (NP) subject.", "labels": [], "entities": []}, {"text": "If P phrases are used to translate the Arabic source s to the English target t, then the (unsigned) linear distortion is given by where pf irst and p last are the first and last source word indices, respectively, in phrase i.", "labels": [], "entities": [{"text": "linear distortion", "start_pos": 100, "end_pos": 117, "type": "METRIC", "confidence": 0.8467718660831451}]}, {"text": "By this formula, the cost of the step to translate the NP subject before the verb is 9, which is high relative to the monotone translation path.", "labels": [], "entities": []}, {"text": "Moreover, a conventional distortion limit (e.g., 5) would likely force translation of the verb prior to the full subject unless the exact subject phrase existed in the phrase table.", "labels": [], "entities": []}, {"text": "1 Therefore, the correct re-ordering is either improbable or impossible, depending on the choice of distortion parameters.", "labels": [], "entities": []}, {"text": "The objective of this work is to develop a distortion cost model that allows the distortion limit to be raised significantly without a catastrophic decrease in performance.", "labels": [], "entities": []}, {"text": "We first describe an admissible future cost heuristic for linear distortion that restores baseline performance at high distortion limits.", "labels": [], "entities": []}, {"text": "Then we add a feature-rich discriminative distortion model that captures e.g. the tendency of Arabic verbs to move right during translation to English.", "labels": [], "entities": []}, {"text": "Model parameters are learned from automatic bitext alignments.", "labels": [], "entities": []}, {"text": "Together these two extensions allow us to triple the distortion limit in our NIST MT09 Arabic-English system while maintaining a statistically significant improvement over the low distortion baseline.", "labels": [], "entities": [{"text": "distortion", "start_pos": 53, "end_pos": 63, "type": "METRIC", "confidence": 0.957537829875946}, {"text": "NIST MT09 Arabic-English system", "start_pos": 77, "end_pos": 108, "type": "DATASET", "confidence": 0.9116923362016678}]}, {"text": "At the high distortion limit, we also show a +2.32 BLEU average gain over Moses with an equivalent distortion parameterization.", "labels": [], "entities": [{"text": "BLEU average gain", "start_pos": 51, "end_pos": 68, "type": "METRIC", "confidence": 0.9769908587137858}]}], "datasetContent": [{"text": "Our MT system is Phrasal (, which is a Java re-implementation of the Moses: BLEU-4 [%] scores (uncased) at the distortion limit (5) used in our baseline NIST MT09 Arabic-English system (  decoder with the same standard features: four translation features (phrase-based translation probabilities and lexically-weighted probabilities), word penalty, phrase penalty, linear distortion, and language model score.", "labels": [], "entities": [{"text": "BLEU-4", "start_pos": 76, "end_pos": 82, "type": "METRIC", "confidence": 0.9986125230789185}, {"text": "NIST MT09 Arabic-English system", "start_pos": 153, "end_pos": 184, "type": "DATASET", "confidence": 0.8918934166431427}]}, {"text": "We disable baseline linear distortion when evaluating the other distortion cost models.", "labels": [], "entities": []}, {"text": "To tune parameters, we run MERT with the Downhill Simplex algorithm on the MT04 dataset.", "labels": [], "entities": [{"text": "MERT", "start_pos": 27, "end_pos": 31, "type": "METRIC", "confidence": 0.6834048628807068}, {"text": "Downhill Simplex", "start_pos": 41, "end_pos": 57, "type": "DATASET", "confidence": 0.8761129081249237}, {"text": "MT04 dataset", "start_pos": 75, "end_pos": 87, "type": "DATASET", "confidence": 0.9870606958866119}]}, {"text": "For all models, we use 20 random starting points and generate 300-best lists.", "labels": [], "entities": []}, {"text": "We use the NIST MT09 constrained track training data, but remove the UN and comparable data.", "labels": [], "entities": [{"text": "NIST MT09 constrained track training data", "start_pos": 11, "end_pos": 52, "type": "DATASET", "confidence": 0.8586175739765167}, {"text": "UN", "start_pos": 69, "end_pos": 71, "type": "DATASET", "confidence": 0.8173468708992004}]}, {"text": "The reduced training bitext has 181k aligned sentences with 6.20M English and 5.73M Arabic tokens.", "labels": [], "entities": []}, {"text": "We create word alignments using the Berkeley Aligner () and take the intersection of the alignments in both directions.", "labels": [], "entities": []}, {"text": "Phrase pairs with a maximum target or source length of 7 tokens are extracted using the method of.", "labels": [], "entities": []}, {"text": "We build a 5-gram language model from the Xinhua and AFP sections of the Gigaword corpus (LDC2007T40), in addition to all of the target side training data permissible in the NIST MT09 constrained competition.", "labels": [], "entities": [{"text": "AFP", "start_pos": 53, "end_pos": 56, "type": "DATASET", "confidence": 0.853789210319519}, {"text": "Gigaword corpus", "start_pos": 73, "end_pos": 88, "type": "DATASET", "confidence": 0.9417915642261505}, {"text": "NIST MT09 constrained competition", "start_pos": 174, "end_pos": 207, "type": "DATASET", "confidence": 0.8703559339046478}]}, {"text": "We manually remove Giga- word documents that were released during periods that overlapped with the development and test sets.", "labels": [], "entities": []}, {"text": "The language model is smoothed with the modified Kneser-Ney algorithm, retaining only trigrams, 4-grams, and 5-grams that occurred two, three, and three times, respectively, in the training data.", "labels": [], "entities": []}, {"text": "We remove from the test sets source tokens not present in the phrase tables.", "labels": [], "entities": []}, {"text": "For the discriminative distortion models, we tag the pre-processed input using the log-linear POS tagger of.", "labels": [], "entities": []}, {"text": "After decoding, we strip any punctuation that appears at the beginning of a translation.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: BLEU-4 [%] dev set scores (uncased) for the  linear distortion with future cost estimation.", "labels": [], "entities": [{"text": "BLEU-4", "start_pos": 10, "end_pos": 16, "type": "METRIC", "confidence": 0.9981663823127747}]}, {"text": " Table 3: BLEU-4 [%] scores (uncased) at the distortion limit (5) used in our baseline NIST MT09 Arabic-English  system (", "labels": [], "entities": [{"text": "BLEU-4", "start_pos": 10, "end_pos": 16, "type": "METRIC", "confidence": 0.9988692402839661}, {"text": "NIST MT09 Arabic-English  system", "start_pos": 87, "end_pos": 119, "type": "DATASET", "confidence": 0.8633002936840057}]}, {"text": " Table 4: BLEU-4 [%] scores (uncased) at a very high distortion limit (15). DISCRIM+FUTURE also achieves a  statistically significant gain over the MOSESLINEAR dlimit=5 baseline for MT05 (p \u2264 0.06), MT06 (p \u2264 0.01), and  MT08 (p \u2264 0.01).", "labels": [], "entities": [{"text": "BLEU-4", "start_pos": 10, "end_pos": 16, "type": "METRIC", "confidence": 0.9983664155006409}, {"text": "FUTURE", "start_pos": 84, "end_pos": 90, "type": "METRIC", "confidence": 0.8709397912025452}, {"text": "MOSESLINEAR dlimit=5 baseline", "start_pos": 148, "end_pos": 177, "type": "METRIC", "confidence": 0.8571026563644409}, {"text": "MT05", "start_pos": 182, "end_pos": 186, "type": "DATASET", "confidence": 0.8309313058853149}, {"text": "MT06", "start_pos": 199, "end_pos": 203, "type": "DATASET", "confidence": 0.915152907371521}, {"text": "MT08", "start_pos": 221, "end_pos": 225, "type": "DATASET", "confidence": 0.9387919902801514}]}]}