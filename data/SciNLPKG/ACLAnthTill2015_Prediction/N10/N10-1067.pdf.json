{"title": [{"text": "Some Empirical Evidence for Annotation Noise in a Benchmarked Dataset", "labels": [], "entities": [{"text": "Annotation Noise", "start_pos": 28, "end_pos": 44, "type": "TASK", "confidence": 0.9494275748729706}, {"text": "Benchmarked Dataset", "start_pos": 50, "end_pos": 69, "type": "DATASET", "confidence": 0.7007746398448944}]}], "abstractContent": [{"text": "A number of recent articles in computational linguistics venues called fora closer examination of the type of noise present in annotated datasets used for benchmarking (Rei-dsma and Carletta, 2008; Beigman Klebanov and Beigman, 2009).", "labels": [], "entities": []}, {"text": "In particular, Beigman Klebanov and Beigman articulated a type of noise they call annotation noise and showed that in worst case such noise can severely degrade the generalization ability of a linear classifier (Beigman and Beigman Klebanov, 2009).", "labels": [], "entities": []}, {"text": "In this paper, we provide quantitative empirical evidence for the existence of this type of noise in a recently benchmarked dataset.", "labels": [], "entities": []}, {"text": "The proposed methodology can be used to zero in on unreliable instances, facilitating generation of cleaner gold standards for benchmarking.", "labels": [], "entities": []}], "introductionContent": [{"text": "Traditionally, studies in computational linguistics use few trained annotators.", "labels": [], "entities": [{"text": "computational linguistics", "start_pos": 26, "end_pos": 51, "type": "TASK", "confidence": 0.7548100650310516}]}, {"text": "Lately this might be changing, as inexpensive annotators are available in large numbers through projects like Amazon Mechanical Turk or through online games where annotations are produced as a by-product (), and, at least for certain tasks, the quality of multiple non-expert annotations is close to that of a small number of experts (.", "labels": [], "entities": [{"text": "Amazon Mechanical Turk", "start_pos": 110, "end_pos": 132, "type": "DATASET", "confidence": 0.8656619389851888}]}, {"text": "Apart from the reduced costs, mass annotation is a promising way to get detailed information about the dataset, such as the level of difficulty of the difference instances.", "labels": [], "entities": []}, {"text": "Such information is important both from the linguistic and from the machine learning perspective, as the existence of a group of instances difficult enough to look like they have been labeled by random guesses can in the worst case induce the machine learner training on the dataset to misclassify a constant proportion of easy, noncontroversial instances, as well as produce incorrect comparative results in a benchmarking setting  . In this article, we employ annotation generation models to estimate the types of instances in a multiply annotated dataset fora binary classification task.", "labels": [], "entities": []}, {"text": "We provide the first quantitative empirical demonstration, to our knowledge, of the existence of what Beigman  call \"annotation noise\" in a benchmarked dataset, that is, fora case where instances cannot be plausibly assigned to just two classes, and where instances in the third class can be plausibly described as having been annotated by flips of a nearly fair coin.", "labels": [], "entities": []}, {"text": "The ability to identify such instances helps improve the gold standard by eliminating them, and allows further empirical investigation of their impact on machine learning for the task in question.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": []}