{"title": [{"text": "Putting the User in the Loop: Interactive Maximal Marginal Relevance for Query-Focused Summarization", "labels": [], "entities": [{"text": "Maximal Marginal Relevance", "start_pos": 42, "end_pos": 68, "type": "TASK", "confidence": 0.665087471405665}, {"text": "Summarization", "start_pos": 87, "end_pos": 100, "type": "TASK", "confidence": 0.7279496192932129}]}], "abstractContent": [{"text": "This work represents an initial attempt to move beyond \"single-shot\" summarization to interactive summarization.", "labels": [], "entities": []}, {"text": "We present an extension to the classic Maximal Marginal Relevance (MMR) algorithm that places a user \"in the loop\" to assist in candidate selection.", "labels": [], "entities": [{"text": "Maximal Marginal Relevance (MMR)", "start_pos": 39, "end_pos": 71, "type": "TASK", "confidence": 0.696314866344134}]}, {"text": "Experiments in the complex interactive Question Answering (ciQA) task at TREC 2007 show that interactively-constructed responses are significantly higher in quality than automatically-generated ones.", "labels": [], "entities": [{"text": "Question Answering (ciQA) task at TREC 2007", "start_pos": 39, "end_pos": 82, "type": "TASK", "confidence": 0.761411276128557}]}, {"text": "This novel algorithm provides a starting point for future work on interactive summarization.", "labels": [], "entities": [{"text": "summarization", "start_pos": 78, "end_pos": 91, "type": "TASK", "confidence": 0.7272130846977234}]}], "introductionContent": [{"text": "Document summarization, as captured in modern comparative evaluations such as TAC and DUC, is mostly conceived as a \"one-shot\" task.", "labels": [], "entities": [{"text": "Document summarization", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.9512394666671753}, {"text": "TAC", "start_pos": 78, "end_pos": 81, "type": "DATASET", "confidence": 0.7893704175949097}, {"text": "DUC", "start_pos": 86, "end_pos": 89, "type": "DATASET", "confidence": 0.7778165936470032}]}, {"text": "However, researchers have long known that information seeking is an iterative activity, which suggests that an interactive approach might be worth exploring.", "labels": [], "entities": [{"text": "information seeking", "start_pos": 42, "end_pos": 61, "type": "TASK", "confidence": 0.7489198744297028}]}, {"text": "This paper present a simple extension of a wellknown algorithm, Maximal Marginal Relevance (MMR) (, that places the user in the loop.", "labels": [], "entities": [{"text": "Maximal Marginal Relevance (MMR)", "start_pos": 64, "end_pos": 96, "type": "METRIC", "confidence": 0.8201737801233927}]}, {"text": "MMR is an iterative algorithm, whereat each step a candidate extract c (e.g., a sentence) is assigned the following score: The score consists of two components: the relevance of the candidate c with respect to the query q (Rel) and the similarity of the candidate c to each extract sin the current summary S (Sim).", "labels": [], "entities": []}, {"text": "The maximum score from these similarity comparisons is subtracted from the relevance score, subjected to a tuning parameter that controls the emphasis on relevance and anti-redundancy.", "labels": [], "entities": [{"text": "relevance score", "start_pos": 75, "end_pos": 90, "type": "METRIC", "confidence": 0.9343944191932678}]}, {"text": "Scores are recomputed after each step and the algorithm iterates until a stopping criterion has been met (e.g., length quota).", "labels": [], "entities": [{"text": "length quota", "start_pos": 112, "end_pos": 124, "type": "METRIC", "confidence": 0.9207051992416382}]}, {"text": "We propose a simple extension to MMR: at each step, we interactively ask the user to select the best sentence for inclusion in the summary.", "labels": [], "entities": [{"text": "MMR", "start_pos": 33, "end_pos": 36, "type": "TASK", "confidence": 0.9324332475662231}]}, {"text": "That is, instead of the system automatically selecting the candidate with the highest score, it presents the user with a ranked list of candidates for selection.", "labels": [], "entities": []}], "datasetContent": [{"text": "This section describes our experiments for the TREC 2007 ciQA task.", "labels": [], "entities": [{"text": "TREC 2007 ciQA task", "start_pos": 47, "end_pos": 66, "type": "TASK", "confidence": 0.6575184762477875}]}, {"text": "In summary: the initial run was generated automatically using standard MMR.", "labels": [], "entities": [{"text": "MMR", "start_pos": 71, "end_pos": 74, "type": "DATASET", "confidence": 0.6772463321685791}]}, {"text": "The web-based interactions consisted of iterations of interactive MMR, where the user selected the best candidate extract at each step.", "labels": [], "entities": [{"text": "MMR", "start_pos": 66, "end_pos": 69, "type": "TASK", "confidence": 0.8331815600395203}]}, {"text": "The final run consisted of the output of interactive MMR padded with automatically-generated output.", "labels": [], "entities": []}, {"text": "Sentence extracts were used as the basic response unit.", "labels": [], "entities": [{"text": "Sentence extracts", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.7522715628147125}]}, {"text": "For each topic, the top 100 documents were retrieved from the AQUAINT-2 collection with Lucene, using the topic template verbatim as the query.", "labels": [], "entities": [{"text": "AQUAINT-2 collection", "start_pos": 62, "end_pos": 82, "type": "DATASET", "confidence": 0.961370199918747}, {"text": "Lucene", "start_pos": 88, "end_pos": 94, "type": "DATASET", "confidence": 0.8083130121231079}]}, {"text": "Neither the template structure nor the narrative text were exploited.", "labels": [], "entities": []}, {"text": "All documents were then broken into individual sentences, which served as the pool of candidates.", "labels": [], "entities": []}, {"text": "The relevance of each sentence was computed as the sum of the inverse document frequencies of matching terms from the topic template.", "labels": [], "entities": []}, {"text": "Redundancy was computed as the cosine similarity between the current answer (consisting of all previously-selected sentences) and the current candidate.", "labels": [], "entities": []}, {"text": "The relevance and redundancy scores were then normalized and combined (\u03bb = 0.8).", "labels": [], "entities": [{"text": "relevance", "start_pos": 4, "end_pos": 13, "type": "METRIC", "confidence": 0.9283816814422607}]}, {"text": "For the initial run, the MMR algorithm iterated until 25 candidates had been selected.", "labels": [], "entities": [{"text": "MMR", "start_pos": 25, "end_pos": 28, "type": "TASK", "confidence": 0.9705873727798462}]}, {"text": "For interactive MMR, a screenshot of the webbased system is shown in.", "labels": [], "entities": [{"text": "MMR", "start_pos": 16, "end_pos": 19, "type": "TASK", "confidence": 0.9249594211578369}]}, {"text": "The interface consists of three elements: at the top (label A) is the current topic; in the middle (label B) is the current answer, containing user selections from previous iterations; the bottom area (label C) shows a ranked list of candidate sentences ordered by MMR score.", "labels": [], "entities": []}, {"text": "At each iteration, the user is asked to select one candidate by clicking the \"Add to answer\" button next to that candidate.", "labels": [], "entities": []}, {"text": "The selected candidate is then added to the current answer.", "labels": [], "entities": []}, {"text": "Ten answer candidates are shown per page.", "labels": [], "entities": []}, {"text": "Clicking on a button labeled \"Show more candidates\" at the bottom of the page (not shown in the screenshot) displays the next ten candidates.", "labels": [], "entities": []}, {"text": "In the ciQA 2007 evaluation, NIST assessors engaged with this interface for the entire allotted five minute interaction period.", "labels": [], "entities": [{"text": "ciQA 2007 evaluation", "start_pos": 7, "end_pos": 27, "type": "DATASET", "confidence": 0.9039829969406128}]}, {"text": "Note that this simple interface was designed only to assess the effectiveness of interactive MMR, and not intended to represent an actual interactive system.", "labels": [], "entities": [{"text": "MMR", "start_pos": 93, "end_pos": 96, "type": "TASK", "confidence": 0.8950855731964111}]}, {"text": "To prevent users from seeing the same sentences repeatedly once a candidate selection has been recorded, we divide the scores of all candidates ranked higher than the selected candidate by two (an Number arbitrary constant).", "labels": [], "entities": []}, {"text": "For example, if the user clicked on candidate five, scores for candidates one through four are cut in half.", "labels": [], "entities": []}, {"text": "Previous studies have shown that users generally examine ranked lists in order, so the lack of a selection can be interpreted as negative feedback (.", "labels": [], "entities": []}, {"text": "The answers constructed interactively were submitted to NIST as the final (post-interaction) run.", "labels": [], "entities": [{"text": "NIST", "start_pos": 56, "end_pos": 60, "type": "DATASET", "confidence": 0.9656931161880493}]}, {"text": "However, since these answers were significantly shorter than the initial run (given the short interaction period), the responses were \"padded\" by running additional iterations of automatic MMR until a length quota of 4000 characters had been achieved.", "labels": [], "entities": []}], "tableCaptions": []}