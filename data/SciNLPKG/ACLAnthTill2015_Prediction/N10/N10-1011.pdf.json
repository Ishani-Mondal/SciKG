{"title": [], "abstractContent": [{"text": "The question of how meaning might be acquired by young children and represented by adult speakers of a language is one of the most debated topics in cognitive science.", "labels": [], "entities": []}, {"text": "Existing semantic representation models are primarily amodal based on information provided by the linguistic input despite ample evidence indicating that the cognitive system is also sensitive to perceptual information.", "labels": [], "entities": []}, {"text": "In this work we exploit the vast resource of images and associated documents available on the web and develop a model of multimodal meaning representation which is based on the linguistic and visual context.", "labels": [], "entities": [{"text": "multimodal meaning representation", "start_pos": 121, "end_pos": 154, "type": "TASK", "confidence": 0.6687570412953695}]}, {"text": "Experimental results show that a closer correspondence to human data can be obtained by taking the visual modality into account .", "labels": [], "entities": []}], "introductionContent": [{"text": "The representation and modeling of word meaning has been a central problem in cognitive science and natural language processing.", "labels": [], "entities": [{"text": "representation and modeling of word meaning", "start_pos": 4, "end_pos": 47, "type": "TASK", "confidence": 0.8534214496612549}, {"text": "natural language processing", "start_pos": 100, "end_pos": 127, "type": "TASK", "confidence": 0.6584079066912333}]}, {"text": "Both disciplines are concerned with how semantic knowledge is acquired, organized, and ultimately used in language processing and understanding.", "labels": [], "entities": [{"text": "language processing and understanding", "start_pos": 106, "end_pos": 143, "type": "TASK", "confidence": 0.7162119150161743}]}, {"text": "A popular tradition of studying semantic representation has been driven by the assumption that word meaning can be learned from the linguistic environment.", "labels": [], "entities": [{"text": "semantic representation", "start_pos": 32, "end_pos": 55, "type": "TASK", "confidence": 0.7164465934038162}]}, {"text": "Words that are similar in meaning tend to behave similarly in terms of their distributions across different contexts.", "labels": [], "entities": []}, {"text": "Semantic space models, among which Latent Semantic Analysis (LSA, Landauer and Dumais 1997) is perhaps known best, operationalize this idea by capturing word meaning quantitatively in terms of simple co-occurrence statistics.", "labels": [], "entities": [{"text": "Latent Semantic Analysis", "start_pos": 35, "end_pos": 59, "type": "TASK", "confidence": 0.5927222768465678}]}, {"text": "Each word w is represented by a k element vector reflecting the local distributional context of w relative to k context words.", "labels": [], "entities": []}, {"text": "More recently, topic models have been gaining ground as a more structured representation of word meaning.", "labels": [], "entities": []}, {"text": "In contrast to more standard semantic space models where word senses are conflated into a single representation, topic models assume that words observed in a corpus manifest some latent structureword meaning is a probability distribution over a set of topics (corresponding to coarse-grained senses).", "labels": [], "entities": []}, {"text": "Each topic is a probability distribution over words, and the content of the topic is reflected in the words to which it assigns high probability.", "labels": [], "entities": []}, {"text": "Semantic space (and topic) models are extracted from real language corpora, and thus provide a direct means of investigating the influence of the statistics of language on semantic representation.", "labels": [], "entities": []}, {"text": "They have been successful in explaining a wide range of behavioral data -examples include lexical priming, deep dyslexia, text comprehension, synonym selection, and human similarity judgments (see Landauer and Dumais 1997 and the references therein).", "labels": [], "entities": [{"text": "synonym selection", "start_pos": 142, "end_pos": 159, "type": "TASK", "confidence": 0.9217333197593689}, {"text": "human similarity judgments", "start_pos": 165, "end_pos": 191, "type": "TASK", "confidence": 0.6374590198198954}]}, {"text": "They also underlie a large number of natural language processing (NLP) tasks including lexicon acquisition, word sense discrimination, text segmentation and notably information retrieval.", "labels": [], "entities": [{"text": "natural language processing (NLP)", "start_pos": 37, "end_pos": 70, "type": "TASK", "confidence": 0.7820245623588562}, {"text": "lexicon acquisition", "start_pos": 87, "end_pos": 106, "type": "TASK", "confidence": 0.7679360806941986}, {"text": "word sense discrimination", "start_pos": 108, "end_pos": 133, "type": "TASK", "confidence": 0.710147758324941}, {"text": "text segmentation", "start_pos": 135, "end_pos": 152, "type": "TASK", "confidence": 0.7628316879272461}, {"text": "information retrieval", "start_pos": 165, "end_pos": 186, "type": "TASK", "confidence": 0.8276495933532715}]}, {"text": "Despite their popularity, these models offer a somewhat impoverished representation of word meaning based solely on information provided by the linguistic input.", "labels": [], "entities": []}, {"text": "Many experimental studies in language acquisition suggest that word meaning arises not only from exposure to the linguistic environment but also from our interaction with the physical world.", "labels": [], "entities": [{"text": "language acquisition", "start_pos": 29, "end_pos": 49, "type": "TASK", "confidence": 0.7296832501888275}, {"text": "word meaning", "start_pos": 63, "end_pos": 75, "type": "TASK", "confidence": 0.7188369184732437}]}, {"text": "For example, infants are from an early age able to form perceptually-based category representations (.", "labels": [], "entities": []}, {"text": "Perhaps unsurprisingly, words that refer to concrete entities and actions are among the first words being learned as these are directly observable in the environment ().", "labels": [], "entities": []}, {"text": "Experimental evidence also shows that children respond to categories on the basis of visual features, e.g., they generalize object names to new objects often on the basis of similarity in shape () and texture (.", "labels": [], "entities": []}, {"text": "In this paper we aim to develop a unified mod-eling framework of word meaning that captures the mutual dependence between the linguistic and visual context.", "labels": [], "entities": []}, {"text": "This is a challenging task for at least two reasons.", "labels": [], "entities": []}, {"text": "First, in order to emulate the environment within which word meanings are acquired, we must have recourse to a corpus of verbal descriptions and their associated images.", "labels": [], "entities": []}, {"text": "Such corpora are in short supply compared to the large volumes of solely textual data.", "labels": [], "entities": []}, {"text": "Secondly, our model should integrate linguistic and visual information in a single representation.", "labels": [], "entities": []}, {"text": "It is unlikely that we have separate representations for different aspects of word meaning).", "labels": [], "entities": []}, {"text": "We meet the first challenge by exploiting multimodal corpora, namely collections of documents that contain pictures.", "labels": [], "entities": []}, {"text": "Although large scale corpora with a one-to-one correspondence between words and images are difficult to come by, datasets that contain images and text are ubiquitous.", "labels": [], "entities": []}, {"text": "For example, online news documents are often accompanied by pictures.", "labels": [], "entities": []}, {"text": "Using this data, we develop a model that combines textual and visual information to learn semantic representations.", "labels": [], "entities": []}, {"text": "We assume that images and their surrounding text have been generated by a shared set of latent variables or topics.", "labels": [], "entities": []}, {"text": "Our model follows the general rationale of topic models -it is based upon the idea that documents are mixtures of topics.", "labels": [], "entities": []}, {"text": "Importantly, our topics are inferred from the joint distribution of textual and visual words.", "labels": [], "entities": []}, {"text": "Our experimental results show that a closer correspondence to human word similarity and association can be obtained by taking the visual modality into account.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section we discuss our experimental design for assessing the performance of the model presented above.", "labels": [], "entities": []}, {"text": "We give details on our training procedure and parameter estimation and present the baseline method used for comparison with our model.", "labels": [], "entities": []}, {"text": "Model Selection The multimodal topic model has several parameters that must be instantiated.", "labels": [], "entities": []}, {"text": "These include the quantization of the image features, the number of topics, the choice of similarity function, and the values for \u03b1 and \u03b2.", "labels": [], "entities": []}, {"text": "We explored the parameter space on held-out data.", "labels": [], "entities": []}, {"text": "Specifically, we fit the parameters for the word association and similarity models separately using a third of the association norms and WordSim353 similarity judgments, respectively.", "labels": [], "entities": [{"text": "WordSim353 similarity judgments", "start_pos": 137, "end_pos": 168, "type": "METRIC", "confidence": 0.6313928663730621}]}, {"text": "As mentioned in Section 3.1 we used K-means to quantize the image features into a discrete set of visual terms.", "labels": [], "entities": []}, {"text": "We varied K from 250 to 2000.", "labels": [], "entities": [{"text": "K", "start_pos": 10, "end_pos": 11, "type": "METRIC", "confidence": 0.9890636801719666}]}, {"text": "We also varied the number of topics from 25 to 750 for both the multimodal and text-based topic models.", "labels": [], "entities": []}, {"text": "The parameter \u03b1 was set to 0.1 and \u03b2 was initialized randomly.", "labels": [], "entities": []}, {"text": "The model was trained using variational Bayes until convergence of its bound on the likelihood objective.", "labels": [], "entities": []}, {"text": "shows how word association performance varies on the development set with different numbers of topics (t) and visual terms (r) according to three similarity measures: KL divergence, JS divergence, and P(w 2 |w 1 ), the probability of word w 2 given w 1 (see Section 3.3).", "labels": [], "entities": [{"text": "KL divergence", "start_pos": 167, "end_pos": 180, "type": "METRIC", "confidence": 0.8667232096195221}, {"text": "JS divergence", "start_pos": 182, "end_pos": 195, "type": "METRIC", "confidence": 0.8824661374092102}, {"text": "P", "start_pos": 201, "end_pos": 202, "type": "METRIC", "confidence": 0.9896697402000427}]}, {"text": "shows results on the development set for the word similarity task.", "labels": [], "entities": [{"text": "word similarity task", "start_pos": 45, "end_pos": 65, "type": "TASK", "confidence": 0.8349523345629374}]}, {"text": "As far as word association is concerned, we obtain best results with P(w 2 |w 1 ), 750 visual terms and 750 topics (r = 0.188).", "labels": [], "entities": [{"text": "word association", "start_pos": 10, "end_pos": 26, "type": "TASK", "confidence": 0.776382327079773}]}, {"text": "On word similarity, JS performs best with 500 visual terms and 25 topics (r = 0.374).", "labels": [], "entities": [{"text": "word similarity", "start_pos": 3, "end_pos": 18, "type": "TASK", "confidence": 0.7685442566871643}, {"text": "JS", "start_pos": 20, "end_pos": 22, "type": "TASK", "confidence": 0.744732141494751}]}, {"text": "It is not surprising that P(w 2 |w 1 ) works best for word association.", "labels": [], "entities": [{"text": "word association", "start_pos": 54, "end_pos": 70, "type": "TASK", "confidence": 0.7956070899963379}]}, {"text": "The measure expresses the associative relations between words as a conditional distribution over potential response words w 2 for cue word w 1 . A symmetric function is more appropriate for word similarity as the task involves measuring the degree to which to words share some meaning (expressed as topics in our model) rather than whether a word is likely to be generated as a response to another word.", "labels": [], "entities": []}, {"text": "These differences also lead to different parametrizations of the semantic space.", "labels": [], "entities": []}, {"text": "A rich visual term vocabulary (750 terms) is needed for modeling association as broader aspects of word meaning are taken into account, whereas a sparser more focused representation (with 500 visual terms and 25 overall topics) is better at isolating the common semantic content between two words.", "labels": [], "entities": []}, {"text": "We explored the parameter space for the text-based topic model in a similar fashion.", "labels": [], "entities": []}, {"text": "On the word association task the best correlation coefficient was achieved with 750 topics and P(w 2 |w 1 ) (r = 0.139).", "labels": [], "entities": [{"text": "word association task", "start_pos": 7, "end_pos": 28, "type": "TASK", "confidence": 0.8441241979598999}, {"text": "correlation coefficient", "start_pos": 38, "end_pos": 61, "type": "METRIC", "confidence": 0.9815709292888641}, {"text": "P", "start_pos": 95, "end_pos": 96, "type": "METRIC", "confidence": 0.9909181594848633}]}, {"text": "On word similarity, the best results were obtained with 75 topics and the JS divergence (r = 0.309).", "labels": [], "entities": [{"text": "word similarity", "start_pos": 3, "end_pos": 18, "type": "TASK", "confidence": 0.7044537365436554}, {"text": "JS divergence", "start_pos": 74, "end_pos": 87, "type": "METRIC", "confidence": 0.7852195203304291}]}], "tableCaptions": [{"text": " Table 2: Model performance on word association and  similarity (test set).", "labels": [], "entities": []}]}