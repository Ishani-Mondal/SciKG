{"title": [{"text": "Learning Words and Their Meanings from Unsegmented Child-directed Speech", "labels": [], "entities": [{"text": "Learning Words and Their Meanings from Unsegmented Child-directed Speech", "start_pos": 0, "end_pos": 72, "type": "TASK", "confidence": 0.7138181659910414}]}], "abstractContent": [{"text": "Most work on language acquisition treats word segmentation-the identification of linguistic segments from continuous speech-and word learning-the mapping of those segments to meanings-as separate problems.", "labels": [], "entities": [{"text": "language acquisition", "start_pos": 13, "end_pos": 33, "type": "TASK", "confidence": 0.7286413609981537}, {"text": "word segmentation-the identification of linguistic segments from continuous speech-and word learning-the mapping of those segments to meanings-as", "start_pos": 41, "end_pos": 186, "type": "TASK", "confidence": 0.8874386917142307}]}, {"text": "These two abilities develop in parallel, however , raising the question of whether they might interact.", "labels": [], "entities": []}, {"text": "To explore the question, we present anew Bayesian segmentation model that incorporates aspects of word learning and compare it to a model that ignores word meanings.", "labels": [], "entities": []}, {"text": "The model that learns word meanings proposes more adult-like segmentations for the meaning-bearing words.", "labels": [], "entities": []}, {"text": "This result suggests that the non-linguistic context may supply important information for learning word segmentations as well as word meanings.", "labels": [], "entities": [{"text": "learning word segmentations", "start_pos": 90, "end_pos": 117, "type": "TASK", "confidence": 0.6276823182900747}]}], "introductionContent": [{"text": "Acquiring a language entails mastering many learning tasks simultaneously, including identifying where words begin and end in continuous speech and learning meanings for those words.", "labels": [], "entities": []}, {"text": "It is common to treat these tasks as separate, sequential processes, where segmentation is a prerequisite to word learning but otherwise there are few if any dependencies.", "labels": [], "entities": []}, {"text": "The earliest evidence of segmentation, however, is for words bordering a child's own name ().", "labels": [], "entities": []}, {"text": "In addition, infants begin learning their first words before they achieve adultlevel competence in segmentation.", "labels": [], "entities": []}, {"text": "These two pieces of evidence raise the question of whether the tasks of meaning learning and segmentation might mutually inform one another.", "labels": [], "entities": [{"text": "meaning learning", "start_pos": 72, "end_pos": 88, "type": "TASK", "confidence": 0.7704529464244843}]}, {"text": "To explore this question we present a joint model that simultaneously identifies word boundaries and attempts to associate meanings with words.", "labels": [], "entities": []}, {"text": "In doing so we make two contributions.", "labels": [], "entities": []}, {"text": "First, by modeling the two levels of structure in parallel we simulate a more realistic situation.", "labels": [], "entities": []}, {"text": "Second, a joint model allows us to explore possible synergies and interactions.", "labels": [], "entities": []}, {"text": "We find evidence that our joint model performs better on a segmentation task than an alternative model that does not learn word meanings.", "labels": [], "entities": []}, {"text": "The picture in depicts a language learning situation from our corpus (originally from) where a mother talks while playing with various toys.", "labels": [], "entities": []}, {"text": "Setting down the dog and picking up the hand puppet of a pig, she asks, \"Is that the pig?\"", "labels": [], "entities": []}, {"text": "Starting out, a young learner not only does not know that the word \"pig\" refers to the puppet but does not even know that \"pig\" is a word at all.", "labels": [], "entities": []}, {"text": "Our model simulates the learning task, taking as input the unsegmented phonemic representation of the speech along with the set of objects in the non-linguistic context as shown in  One can formulate the word learning task as that of finding a reasonably small set of reusable word-meaning pairs consistent with the underlying communicative intent.", "labels": [], "entities": []}, {"text": "Infant directed speech often refers to objects in the immediate environment, and early word learning seems to involve associating frequently co-occurring word-object pairs.", "labels": [], "entities": []}, {"text": "Several computational models are based on this idea that a word: (a) The input to our system for the utterance \"Is that the pig?\" consists of an unsegmented sequence of phonemes and the set of objects representing the nonlinguistic context.", "labels": [], "entities": []}, {"text": "These objects were manually identified by inspecting the associated video, a frame from which is shown above.", "labels": [], "entities": []}, {"text": "(b) The gold-standard segmentation and word-object assignments of the same utterance, against which the output of our system is evaluated (all words except \"pIg\" are mapped to a special \"null\" object, as explained in the text). that frequently occurs in the presence of an object and not so frequently in its absence is likely to refer to that object (.", "labels": [], "entities": []}, {"text": "Importantly, all these models assume words are pre-segmented in the input.", "labels": [], "entities": []}, {"text": "While the word segmentation task relates less clearly to the communicative content, it can be formulated according to a similar objective, that of attempting to explain the sound sequences in the input in terms of some reasonably small set of reusable units, or words.", "labels": [], "entities": [{"text": "word segmentation task", "start_pos": 10, "end_pos": 32, "type": "TASK", "confidence": 0.7894618014494578}]}, {"text": "Computational models have successfully addressed the problem in much this way, and the general approach is consistent with experimental observations that humans are sensitive to statistics of sound sequences).", "labels": [], "entities": []}, {"text": "The two tasks can be integrated in a relatively seamless way, since, as we have just formulated them, they have a common objective, that of finding a minimal, consistent set of reusable units.", "labels": [], "entities": []}, {"text": "However, the two deal with different types of information with different dependencies.", "labels": [], "entities": []}, {"text": "The basic idea is that learning a vocabulary that both meets the constraints of the word-learning task and is consistent with the objective of the segmentation task can yield a better segmentation.", "labels": [], "entities": []}, {"text": "That is, we hope to find a synergy in the joint inference of meaning and segmentation.", "labels": [], "entities": []}, {"text": "Note that to the best of our knowledge there is very little computational work that combines word form and word meaning learning ( takes a first step but their model is applicable only to small artificial languages). and review pure word learning models and, in addition to the papers we have already cited, Brent (1999) presents a fairly comprehensive review of previous pure segmentation models.", "labels": [], "entities": [{"text": "word meaning learning", "start_pos": 107, "end_pos": 128, "type": "TASK", "confidence": 0.6675670544306437}]}, {"text": "However, none of the models reviewed make any attempt to jointly address the two problems.", "labels": [], "entities": []}, {"text": "Similarly, in the behavioral literature on development, we are aware of only one segmentation study) that involves non-linguistic context, though this study treats the two tasks sequentially rather than jointly.", "labels": [], "entities": []}, {"text": "We now describe our model and inference procedure and follow with evaluation and discussion.", "labels": [], "entities": []}], "datasetContent": [{"text": "We ran the sampler ten times for 100,000 iterations with parameter settings of \u03b1 1 = 0.01, \u03b1 0 = 20, and p # = 0.5, keeping only the final sample for evaluation.", "labels": [], "entities": []}, {"text": "We defined the word-object pairs fora sample as the words in the referring category that were paired at least once with a particular topic.", "labels": [], "entities": []}, {"text": "These pairs were then compared against a gold standard set of word-object pairs, while segmentation performance was evaluated by comparing the final boundary assignments against the gold standard segmentation.", "labels": [], "entities": [{"text": "segmentation", "start_pos": 87, "end_pos": 99, "type": "TASK", "confidence": 0.9660966992378235}]}], "tableCaptions": [{"text": " Table 1: Subset of an inferred word-object mapping. For  clarity, the proposed words have been converted to stan- dard English orthography.", "labels": [], "entities": []}, {"text": " Table 2: Word Learning Performance. Comparing  precision, recall, and F-score of word-object pairs,  D KL (P (w, z)||Q(w, z)), and accuracy of utterance top- ics for the full joint model and a variant that only infers  meanings given a gold standard segmentation.", "labels": [], "entities": [{"text": "precision", "start_pos": 48, "end_pos": 57, "type": "METRIC", "confidence": 0.9995926022529602}, {"text": "recall", "start_pos": 59, "end_pos": 65, "type": "METRIC", "confidence": 0.9984949827194214}, {"text": "F-score", "start_pos": 71, "end_pos": 78, "type": "METRIC", "confidence": 0.9984256029129028}, {"text": "D KL", "start_pos": 102, "end_pos": 106, "type": "METRIC", "confidence": 0.924045205116272}, {"text": "accuracy", "start_pos": 132, "end_pos": 140, "type": "METRIC", "confidence": 0.9992578625679016}]}, {"text": " Table 3: Segmentation performance. F-score for three  subsets and the full corpus for three variants: the model  without non-linguistic context, the model with random  topics, and the full joint model.", "labels": [], "entities": [{"text": "F-score", "start_pos": 36, "end_pos": 43, "type": "METRIC", "confidence": 0.9988780617713928}]}]}