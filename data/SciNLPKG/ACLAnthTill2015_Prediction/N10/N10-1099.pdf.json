{"title": [{"text": "Using Entity-Based Features to Model Coherence in Student Essays", "labels": [], "entities": []}], "abstractContent": [{"text": "We show how the Barzilay and Lapata entity-based coherence algorithm (2008) can be applied to anew, noisy data domain-student essays.", "labels": [], "entities": []}, {"text": "We demonstrate that by combining Barzilay and Lapata's entity-based features with novel features related to grammar errors and word usage, one can greatly improve the performance of automated coherence prediction for student essays for different populations.", "labels": [], "entities": []}], "introductionContent": [{"text": "There is a small body of work that has investigated using NLP for the problem of identifying coherence in student essays.", "labels": [], "entities": [{"text": "identifying coherence in student essays", "start_pos": 81, "end_pos": 120, "type": "TASK", "confidence": 0.7766799569129944}]}, {"text": "For example, Foltz,, and Higgins, Burstein, have developed systems that examine coherence in student writing.", "labels": [], "entities": []}, {"text": "systems measure lexical relatedness between text segments by using vector-based similarity between adjacent sentences; Higgins et al's (2004) system computes similarity across text segments.", "labels": [], "entities": []}, {"text": "approach is inline with the earlier TextTiling method that identifies subtopic structure in text.", "labels": [], "entities": []}, {"text": "addressed essay coherence using Centering Theory.", "labels": [], "entities": []}, {"text": "More recently, approach (henceforth, BL08) used an entity-based representation to evaluate coherence.", "labels": [], "entities": [{"text": "BL08", "start_pos": 37, "end_pos": 41, "type": "DATASET", "confidence": 0.7041547298431396}]}, {"text": "In BL08, entities (nouns and pronouns) are represented by their sentence roles in a text.", "labels": [], "entities": [{"text": "BL08", "start_pos": 3, "end_pos": 7, "type": "DATASET", "confidence": 0.8519216775894165}]}, {"text": "The algorithm keeps track of the distribution of entity transitions between adjacent sentences, and computes a value for all transition types based on their proportion of occurrence in a text.", "labels": [], "entities": []}, {"text": "BL08 apply their algorithm to three tasks, using wellformed newspaper corpora: text ordering, summary coherence evaluation, and readability assessment.", "labels": [], "entities": [{"text": "BL08", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.9543819427490234}, {"text": "text ordering", "start_pos": 79, "end_pos": 92, "type": "TASK", "confidence": 0.7576896250247955}]}, {"text": "For each task, their system outperforms a Latent Semantic Analysis baseline.", "labels": [], "entities": []}, {"text": "In addition, best performance on each task is achieved using different system and feature configurations.", "labels": [], "entities": []}, {"text": "applied BL08 to detect text coherence in well-formed texts.", "labels": [], "entities": [{"text": "BL08", "start_pos": 8, "end_pos": 12, "type": "METRIC", "confidence": 0.9255702495574951}]}, {"text": "Coherence quality is typically present in scoring criteria for evaluating a student's essay.", "labels": [], "entities": []}, {"text": "This paper focuses on the development of models to predict low-and high-coherence ratings for essays.", "labels": [], "entities": []}, {"text": "Student essay data, unlike newspaper text, is typically noisy, especially when students are nonnative English speakers (NNES).", "labels": [], "entities": []}, {"text": "Here, we evaluate how BL08 algorithm features can be used to model coherence in anew, noisy data domain --student essays.", "labels": [], "entities": []}, {"text": "We found that coherence can be best modeled by combining BL08 entity-based features with novel writing quality features.", "labels": [], "entities": []}, {"text": "Further, our use of data sets from three different test-taker populations also shows that coherence models will differ across populations.", "labels": [], "entities": []}, {"text": "Different populations might use language differently which could affect how coherence is presented.", "labels": [], "entities": []}, {"text": "We expect to incorporate coherence ratings into erater \u00ae , ETS's automated essay scoring system).", "labels": [], "entities": [{"text": "erater \u00ae", "start_pos": 48, "end_pos": 56, "type": "METRIC", "confidence": 0.9395152032375336}]}], "datasetContent": [{"text": "For all experiments, we used a series of n-fold cross-validation runs with C5.0 to evaluate performance for numerous feature configurations.", "labels": [], "entities": []}, {"text": "In and 4, we report: baselines, results on our data with BL08's best system configuration from the summary coherence evaluation task (closest to our task), and our best systems.", "labels": [], "entities": [{"text": "BL08", "start_pos": 57, "end_pos": 61, "type": "DATASET", "confidence": 0.7414127588272095}]}, {"text": "In the Tables, \"best systems\" combined feature sets and outperformed baselines.", "labels": [], "entities": []}, {"text": "Rows in bold indicate final independent best systems that contribute to best performance in the majority vote method.", "labels": [], "entities": []}, {"text": "Agreement is reported as Weighted Kappa (WK), Precision (P), Recall (R) and F-measure (F).", "labels": [], "entities": [{"text": "Agreement", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.9810104966163635}, {"text": "Weighted Kappa (WK)", "start_pos": 25, "end_pos": 44, "type": "METRIC", "confidence": 0.9586044192314148}, {"text": "Precision (P)", "start_pos": 46, "end_pos": 59, "type": "METRIC", "confidence": 0.9610101282596588}, {"text": "Recall (R)", "start_pos": 61, "end_pos": 71, "type": "METRIC", "confidence": 0.9656467586755753}, {"text": "F-measure (F)", "start_pos": 76, "end_pos": 89, "type": "METRIC", "confidence": 0.9623171389102936}]}, {"text": "We implemented three non-trivial baseline systems.", "labels": [], "entities": []}, {"text": "E-rater indicates use of the full feature set from e-rater.", "labels": [], "entities": []}, {"text": "The GUMS (GUMS+) feature baseline, uses the Grammar (G+), Usage", "labels": [], "entities": [{"text": "GUMS (GUMS+) feature baseline", "start_pos": 4, "end_pos": 33, "type": "DATASET", "confidence": 0.8221970101197561}]}], "tableCaptions": [{"text": " Table 2: Non-native English Speaker Test-taker Data (TOEFL): Annotator/System Agreement", "labels": [], "entities": []}]}