{"title": [{"text": "Language identification of names with SVMs", "labels": [], "entities": [{"text": "Language identification of names", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.7890192866325378}]}], "abstractContent": [{"text": "The task of identifying the language of text or utterances has a number of applications in natural language processing.", "labels": [], "entities": [{"text": "identifying the language of text or utterances", "start_pos": 12, "end_pos": 58, "type": "TASK", "confidence": 0.8561646086829049}, {"text": "natural language processing", "start_pos": 91, "end_pos": 118, "type": "TASK", "confidence": 0.6450216372807821}]}, {"text": "Language identification has traditionally been approached with character-level language models.", "labels": [], "entities": [{"text": "Language identification", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.7767952382564545}]}, {"text": "However , the language model approach crucially depends on the length of the text in question.", "labels": [], "entities": []}, {"text": "In this paper, we consider the problem of language identification of names.", "labels": [], "entities": [{"text": "language identification of names", "start_pos": 42, "end_pos": 74, "type": "TASK", "confidence": 0.8292273506522179}]}, {"text": "We show that an approach based on SVMs with n-gram counts as features performs much better than language models.", "labels": [], "entities": []}, {"text": "We also experiment with applying the method to pre-process transliter-ation data for the training of separate models.", "labels": [], "entities": []}], "introductionContent": [{"text": "The task of identifying the language of text or utterances has a number of applications in natural language processing.", "labels": [], "entities": [{"text": "identifying the language of text or utterances", "start_pos": 12, "end_pos": 58, "type": "TASK", "confidence": 0.8561646086829049}, {"text": "natural language processing", "start_pos": 91, "end_pos": 118, "type": "TASK", "confidence": 0.6450216372807821}]}, {"text": "Font show that language identification can improve the accuracy of letter-to-phoneme conversion.", "labels": [], "entities": [{"text": "language identification", "start_pos": 15, "end_pos": 38, "type": "TASK", "confidence": 0.7210174202919006}, {"text": "accuracy", "start_pos": 55, "end_pos": 63, "type": "METRIC", "confidence": 0.998988926410675}, {"text": "letter-to-phoneme conversion", "start_pos": 67, "end_pos": 95, "type": "TASK", "confidence": 0.7163282334804535}]}, {"text": "use language identification in a transliteration system to account for different semantic transliteration rules between languages when the target language is Chinese.", "labels": [], "entities": [{"text": "language identification", "start_pos": 4, "end_pos": 27, "type": "TASK", "confidence": 0.7302964925765991}]}, {"text": "improves the accuracy of machine transliteration by clustering his training data according to the source language.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 13, "end_pos": 21, "type": "METRIC", "confidence": 0.9988983869552612}, {"text": "machine transliteration", "start_pos": 25, "end_pos": 48, "type": "TASK", "confidence": 0.7450421154499054}]}, {"text": "Language identification has traditionally been approached using character-level n-gram language models.", "labels": [], "entities": [{"text": "Language identification", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.757943332195282}]}, {"text": "In this paper, we propose the use of support vector machines (SVMs) for the language identification of very short texts such as proper nouns.", "labels": [], "entities": [{"text": "language identification of very short texts such as proper nouns", "start_pos": 76, "end_pos": 140, "type": "TASK", "confidence": 0.8415735125541687}]}, {"text": "We show that SVMs outperform language models on two different data sets consisting of personal names.", "labels": [], "entities": []}, {"text": "Furthermore, we test the hypothesis that language identification can improve transliteration by pre-processing the source data and training separate models using a state-of-the-art transliteration system.", "labels": [], "entities": [{"text": "language identification", "start_pos": 41, "end_pos": 64, "type": "TASK", "confidence": 0.7625087201595306}]}], "datasetContent": [{"text": "We used two corpora to test our SVM-based approach: the Transfermarkt corpus of soccer player names, and the Chinese-English-Japanese (CEJ) corpus of first names and surnames.", "labels": [], "entities": [{"text": "Transfermarkt corpus of soccer player names", "start_pos": 56, "end_pos": 99, "type": "DATASET", "confidence": 0.9133035441239675}]}, {"text": "These corpora are described in further detail below.", "labels": [], "entities": []}, {"text": "We tested a simple method of combining language identification with transliteration.", "labels": [], "entities": [{"text": "language identification", "start_pos": 39, "end_pos": 62, "type": "TASK", "confidence": 0.7416232973337173}]}, {"text": "We use a language identification model to split the training, development, and test sets into disjoint classes.", "labels": [], "entities": [{"text": "language identification", "start_pos": 9, "end_pos": 32, "type": "TASK", "confidence": 0.7256298661231995}]}, {"text": "We train a transliteration model on each separate class, and then combine the results.", "labels": [], "entities": []}, {"text": "Our transliteration system was DIRECTL).", "labels": [], "entities": [{"text": "DIRECTL", "start_pos": 31, "end_pos": 38, "type": "METRIC", "confidence": 0.888171911239624}]}, {"text": "We trained the language identification model over the entire set of 1000 tagged names using the parameters from above.", "labels": [], "entities": [{"text": "language identification", "start_pos": 15, "end_pos": 38, "type": "TASK", "confidence": 0.7239629775285721}]}, {"text": "Because these names comprised most of the test set and were now being used as the training set for the language identification model, we swapped various names between sets such that none of the words used for training the language identification model were in the final transliteration test set.", "labels": [], "entities": [{"text": "language identification", "start_pos": 103, "end_pos": 126, "type": "TASK", "confidence": 0.7686896324157715}, {"text": "language identification", "start_pos": 222, "end_pos": 245, "type": "TASK", "confidence": 0.7110258042812347}]}, {"text": "Using this language identification model, we split the data.", "labels": [], "entities": [{"text": "language identification", "start_pos": 11, "end_pos": 34, "type": "TASK", "confidence": 0.7340285778045654}]}, {"text": "After splitting, the \"Indian\" training, development, and testing sets had 5032, 575, and 483 words respectively while the \"non-Indian\" sets had 11081, 993, and 517 words respectively.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Language identification accuracy on the Trans- fermarkt corpus. Language models have n = 5.", "labels": [], "entities": [{"text": "Language identification", "start_pos": 10, "end_pos": 33, "type": "TASK", "confidence": 0.7148887366056442}, {"text": "accuracy", "start_pos": 34, "end_pos": 42, "type": "METRIC", "confidence": 0.9663242697715759}, {"text": "Trans- fermarkt corpus", "start_pos": 50, "end_pos": 72, "type": "DATASET", "confidence": 0.8188111186027527}]}]}