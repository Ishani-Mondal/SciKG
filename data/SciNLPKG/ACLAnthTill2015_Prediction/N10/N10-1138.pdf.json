{"title": [], "abstractContent": [{"text": "This paper contributes a formalization of frame-semantic parsing as a structure prediction problem and describes an implemented parser that transforms an English sentence into a frame-semantic representation.", "labels": [], "entities": [{"text": "frame-semantic parsing", "start_pos": 42, "end_pos": 64, "type": "TASK", "confidence": 0.7173130363225937}, {"text": "structure prediction", "start_pos": 70, "end_pos": 90, "type": "TASK", "confidence": 0.746334433555603}]}, {"text": "It finds words that evoke FrameNet frames, selects frames for them, and locates the arguments for each frame.", "labels": [], "entities": []}, {"text": "The system uses two feature-based, discriminative probabilistic (log-linear) models, one with latent variables to permit disambiguation of new predicate words.", "labels": [], "entities": []}, {"text": "The parser is demonstrated to significantly outper-form previously published results.", "labels": [], "entities": []}], "introductionContent": [{"text": "FrameNet () is a rich linguistic resource containing considerable information about lexical and predicate-argument semantics in English.", "labels": [], "entities": [{"text": "FrameNet", "start_pos": 0, "end_pos": 8, "type": "DATASET", "confidence": 0.8288244605064392}]}, {"text": "Grounded in the theory of frame semantics, it suggests-but does not formally define-a semantic representation that blends wordsense disambiguation and semantic role labeling.", "labels": [], "entities": [{"text": "semantic role labeling", "start_pos": 151, "end_pos": 173, "type": "TASK", "confidence": 0.6547816097736359}]}, {"text": "In this paper, we present a computational and statistical model for frame-semantic parsing, the problem of extracting from text semantic predicateargument structures such as those shown in.", "labels": [], "entities": [{"text": "frame-semantic parsing", "start_pos": 68, "end_pos": 90, "type": "TASK", "confidence": 0.7017696499824524}]}, {"text": "We aim to predict a frame-semantic representation as a structure, not as a pipeline of classifiers.", "labels": [], "entities": []}, {"text": "We use a probabilistic framework that cleanly integrates the FrameNet lexicon and (currently very limited) available training data.", "labels": [], "entities": [{"text": "FrameNet lexicon", "start_pos": 61, "end_pos": 77, "type": "DATASET", "confidence": 0.922569215297699}]}, {"text": "Although our models often involve strong independence assumptions, the probabilistic framework we adopt is highly amenable to future extension through new features, relaxed independence assumptions, and semisupervised learning.", "labels": [], "entities": []}, {"text": "Some novel aspects of our current approach include a latent-variable model that permits disambiguation of words not in the FrameNet lexicon, a unified model for finding and labeling arguments, and a precision-boosting constraint that forbids arguments of the same predicate to overlap.", "labels": [], "entities": [{"text": "FrameNet lexicon", "start_pos": 123, "end_pos": 139, "type": "DATASET", "confidence": 0.9046467244625092}, {"text": "precision-boosting", "start_pos": 199, "end_pos": 217, "type": "METRIC", "confidence": 0.9678916931152344}]}, {"text": "Our parser achieves the best published results to date on the SemEval'07 FrameNet task ().", "labels": [], "entities": [{"text": "SemEval'07 FrameNet task", "start_pos": 62, "end_pos": 86, "type": "TASK", "confidence": 0.7309755682945251}]}], "datasetContent": [{"text": "Automatic annotations of frame-semantic structure can be broken into three parts: (1) targets, the words or phrases that evoke frames; (2) the frame type, defined in the lexicon, evoked by each target; and (3) the arguments, or spans of words that serve to fill roles defined by each evoked frame.", "labels": [], "entities": []}, {"text": "These correspond to the three subtasks in our parser, each described and evaluated in turn: target identification ( \u00a73), frame identification ( \u00a74, not unlike wordsense disambiguation), and argument identification ( \u00a75, not unlike semantic role labeling).", "labels": [], "entities": [{"text": "frame identification", "start_pos": 121, "end_pos": 141, "type": "TASK", "confidence": 0.6843606978654861}, {"text": "argument identification", "start_pos": 190, "end_pos": 213, "type": "TASK", "confidence": 0.7381380796432495}]}, {"text": "The standard evaluation script from the SemEval'07 shared task calculates precision, recall, and F 1 -measure for frames and arguments; it also provides a score that gives partial credit for hypothesizing a frame related to the correct one.", "labels": [], "entities": [{"text": "SemEval'07 shared task", "start_pos": 40, "end_pos": 62, "type": "TASK", "confidence": 0.7835126519203186}, {"text": "precision", "start_pos": 74, "end_pos": 83, "type": "METRIC", "confidence": 0.9993509650230408}, {"text": "recall", "start_pos": 85, "end_pos": 91, "type": "METRIC", "confidence": 0.9989443421363831}, {"text": "F 1 -measure", "start_pos": 97, "end_pos": 109, "type": "METRIC", "confidence": 0.9906050562858582}]}, {"text": "We present precision, recall, and F 1 -measure microaveraged across the test documents, report labels-only matching scores (spans must match exactly), and do not use named entity labels.", "labels": [], "entities": [{"text": "precision", "start_pos": 11, "end_pos": 20, "type": "METRIC", "confidence": 0.999419093132019}, {"text": "recall", "start_pos": 22, "end_pos": 28, "type": "METRIC", "confidence": 0.9995051622390747}, {"text": "F 1 -measure microaveraged", "start_pos": 34, "end_pos": 60, "type": "METRIC", "confidence": 0.979502511024475}]}, {"text": "More details can be found in.", "labels": [], "entities": []}, {"text": "For our experiments, statistical significance is measured using a reimplementation of Dan Bikel's randomized parsing evaluation comparator.", "labels": [], "entities": [{"text": "significance", "start_pos": 33, "end_pos": 45, "type": "METRIC", "confidence": 0.6579961776733398}]}], "tableCaptions": [{"text": " Table 2. Snapshot of the SemEval'07 annotated data.", "labels": [], "entities": [{"text": "SemEval'07 annotated data", "start_pos": 26, "end_pos": 51, "type": "DATASET", "confidence": 0.7200632890065511}]}, {"text": " Table 3. Target identification results for our system and  the baseline. Scores in bold denote significant improve- ments over the baseline (p < 0.05).", "labels": [], "entities": [{"text": "improve- ments", "start_pos": 108, "end_pos": 122, "type": "METRIC", "confidence": 0.8166165351867676}]}, {"text": " Table 4. Frame identification results. Precision, recall, and F 1 were evaluated under exact and partial frame matching;  see  \u00a72.3. Bold indicates statistically significant results with respect to the baseline (p < 0.05).", "labels": [], "entities": [{"text": "Frame identification", "start_pos": 10, "end_pos": 30, "type": "TASK", "confidence": 0.8614803552627563}, {"text": "Precision", "start_pos": 40, "end_pos": 49, "type": "METRIC", "confidence": 0.9967619180679321}, {"text": "recall", "start_pos": 51, "end_pos": 57, "type": "METRIC", "confidence": 0.9986531734466553}, {"text": "F 1", "start_pos": 63, "end_pos": 66, "type": "METRIC", "confidence": 0.9956353008747101}]}, {"text": " Table 5. Argument identification results.  *  indicates that gold-standard labels were used for a given pipeline stage.  For full parsing, bolded scores indicate significant improvements relative to the baseline (p < 0.05).", "labels": [], "entities": [{"text": "Argument identification", "start_pos": 10, "end_pos": 33, "type": "TASK", "confidence": 0.8576909601688385}]}]}