{"title": [], "abstractContent": [{"text": "This paper is about interpreting human communication in meetings using audio, video and other signals.", "labels": [], "entities": [{"text": "interpreting human communication in meetings", "start_pos": 20, "end_pos": 64, "type": "TASK", "confidence": 0.9095808625221252}]}, {"text": "Automatic meeting recognition and understanding is extremely challenging, since communication in a meeting is spontaneous and conversational, and involves multiple speakers and multiple modalities.", "labels": [], "entities": [{"text": "Automatic meeting recognition and understanding", "start_pos": 0, "end_pos": 47, "type": "TASK", "confidence": 0.6081136047840119}]}, {"text": "This leads to a number of significant research problems in signal processing, in speech recognition , and in discourse interpretation, taking account of both individual and group behaviours.", "labels": [], "entities": [{"text": "signal processing", "start_pos": 59, "end_pos": 76, "type": "TASK", "confidence": 0.7232006639242172}, {"text": "speech recognition", "start_pos": 81, "end_pos": 99, "type": "TASK", "confidence": 0.8464680016040802}, {"text": "discourse interpretation", "start_pos": 109, "end_pos": 133, "type": "TASK", "confidence": 0.7571346461772919}]}, {"text": "Addressing these problems requires an interdisciplinary effort.", "labels": [], "entities": []}, {"text": "In this paper, I discuss the capture and annotation of multi-modal meeting recordings-resulting in the AMI meeting corpus-and how we have built on this to develop techniques and applications for the recognition and interpretation of meetings .", "labels": [], "entities": [{"text": "AMI meeting corpus-and", "start_pos": 103, "end_pos": 125, "type": "DATASET", "confidence": 0.7514847119649252}, {"text": "recognition and interpretation of meetings", "start_pos": 199, "end_pos": 241, "type": "TASK", "confidence": 0.8844651222229004}]}], "introductionContent": [{"text": "On the face of it, meetings do not seem to form a compelling research area.", "labels": [], "entities": []}, {"text": "Although many people spend a substantial fraction of their time in meetings (e.g. the 1998 3M online survey at http://www.", "labels": [], "entities": [{"text": "3M online survey", "start_pos": 91, "end_pos": 107, "type": "DATASET", "confidence": 0.7919713258743286}]}, {"text": "3m.com/meetingnetwork/), for most people they are not the most enjoyable aspect of their work.", "labels": [], "entities": []}, {"text": "However, for all the time that is spent in meetings, technological support for the meeting process is scant.", "labels": [], "entities": []}, {"text": "Meeting records usually take the form of brief minutes, personal notes, and more recent use of collaborative web 2.0 software.", "labels": [], "entities": []}, {"text": "Such records are labour intensive to produce-because they are manually created-and usually fail to capture much of the content of a meeting, for example the factors that led to a particular decision and the different subjective attitudes displayed by the meeting attendees.", "labels": [], "entities": []}, {"text": "For all the time invested in meetings, very little of the wealth of information that is exchanged is explicitly preserved.", "labels": [], "entities": []}, {"text": "To preserve the information recorded in meetings, it is necessary to capture it.", "labels": [], "entities": []}, {"text": "Obviously this involves recording the speech of the meeting participants.", "labels": [], "entities": []}, {"text": "However, human communication is a multimodal activity with information being exchanged via gestures, handwritten diagrams, and numerous social signals.", "labels": [], "entities": []}, {"text": "The creation of a rich meeting record involves the capture of data across several modalities.", "labels": [], "entities": []}, {"text": "It is a key engineering challenge to capture such multimodal signals in a reliable, unobtrusive and flexible way, but the greater challenges arise from unlocking the multimodal recordings.", "labels": [], "entities": []}, {"text": "If such recordings are not transcribed and indexed (at the least), then access merely corresponds to replay.", "labels": [], "entities": [{"text": "replay", "start_pos": 101, "end_pos": 107, "type": "METRIC", "confidence": 0.9649150967597961}]}, {"text": "And it is rare that people will have the time, or the inclination, to replay a meeting.", "labels": [], "entities": []}, {"text": "There is along and interesting thread of research which is concerned to better understand the dynamics of meetings and the way that groups function.", "labels": [], "entities": []}, {"text": "The types of analyses and studies carried out by these authors is still someway beyond what we can do automatically.", "labels": [], "entities": []}, {"text": "The first significant work on automatic processing of meetings, coupled with an exploration of how people might interact with an archive of recorded meetings, was performed in the mid 1990s.", "labels": [], "entities": []}, {"text": "This work was limited by the fact that it was not possible at that time to transcribe meeting speech automatically.", "labels": [], "entities": [{"text": "transcribe meeting speech automatically", "start_pos": 75, "end_pos": 114, "type": "TASK", "confidence": 0.8247842043638229}]}, {"text": "Other early work in the area concentrated on the multimodal capture and broadcast of meetings (.", "labels": [], "entities": [{"text": "multimodal capture and broadcast of meetings", "start_pos": 49, "end_pos": 93, "type": "TASK", "confidence": 0.8124549686908722}]}, {"text": "Three groups further developed approaches to automatically index the content of meetings.", "labels": [], "entities": []}, {"text": "A team at Fuji Xerox PARC used video retrieval techniques such as keyframing to automatically generate manga-style summaries of meetings (), Waibel and colleagues at CMU used speech recognition and video tracking for meetings (, and Morgan and colleagues at ICSI focused on audio-only capture and speech recognition.", "labels": [], "entities": [{"text": "Fuji Xerox PARC", "start_pos": 10, "end_pos": 25, "type": "DATASET", "confidence": 0.8600300550460815}, {"text": "ICSI", "start_pos": 258, "end_pos": 262, "type": "DATASET", "confidence": 0.9107482433319092}, {"text": "speech recognition", "start_pos": 297, "end_pos": 315, "type": "TASK", "confidence": 0.7354886829853058}]}, {"text": "Since 2003 research in the recognition and understanding of meetings has developed substantially, stimulated by evaluation campaigns such as the NIST Rich Transcription (RT) and CLEAR 2 evaluations, as well as some large multidisciplinary projects such as AMI/AMIDA 3 , CHIL and CALO . This paper is about the work we have carried out in meeting capture, recognition and interpretation within the AMI and AMIDA projects since 2004.", "labels": [], "entities": [{"text": "recognition and understanding of meetings", "start_pos": 27, "end_pos": 68, "type": "TASK", "confidence": 0.8927431106567383}, {"text": "NIST Rich Transcription (RT)", "start_pos": 145, "end_pos": 173, "type": "TASK", "confidence": 0.7255694071451823}, {"text": "meeting capture, recognition and interpretation", "start_pos": 338, "end_pos": 385, "type": "TASK", "confidence": 0.748675525188446}]}, {"text": "One of the principal outputs of these projects was a multimodal corpus of meeting recordings, annotated at a number of different levels.", "labels": [], "entities": []}, {"text": "In section 2 we discuss collection of meeting data, and the construction of the AMI corpus.", "labels": [], "entities": [{"text": "AMI corpus", "start_pos": 80, "end_pos": 90, "type": "DATASET", "confidence": 0.8392772972583771}]}, {"text": "The remainder of the paper discusses the automatic recognition (section 3) and interpretation (section 4) of multimodal meeting recordings, application prototypes (section 5) and issues relating to evaluation (section 6).", "labels": [], "entities": [{"text": "automatic recognition", "start_pos": 41, "end_pos": 62, "type": "TASK", "confidence": 0.6947588622570038}]}], "datasetContent": [{"text": "The multiple streams of data and multiple layers of annotations that makeup the AMI corpus enable it to be used for evaluations of specific recognition components.", "labels": [], "entities": [{"text": "AMI corpus", "start_pos": 80, "end_pos": 90, "type": "DATASET", "confidence": 0.7578717172145844}]}, {"text": "The corpus has been used to evaluate many different things including voice activity detection, speaker diarisation and speech recognition (in the NIST RT evaluations), and head pose recognition (in the CLEAR evaluation).", "labels": [], "entities": [{"text": "voice activity detection", "start_pos": 69, "end_pos": 93, "type": "TASK", "confidence": 0.6333297590414683}, {"text": "speaker diarisation", "start_pos": 95, "end_pos": 114, "type": "TASK", "confidence": 0.7779747843742371}, {"text": "speech recognition", "start_pos": 119, "end_pos": 137, "type": "TASK", "confidence": 0.7336349338293076}, {"text": "NIST RT evaluations", "start_pos": 146, "end_pos": 165, "type": "DATASET", "confidence": 0.8161580363909403}, {"text": "head pose recognition", "start_pos": 172, "end_pos": 193, "type": "TASK", "confidence": 0.7701601386070251}]}, {"text": "In the spoken language processing domain, the AMI corpus has been used to evaluate meeting summarisation, topic segmentation, dialogue act recognition and cross-language retrieval.", "labels": [], "entities": [{"text": "AMI corpus", "start_pos": 46, "end_pos": 56, "type": "DATASET", "confidence": 0.7435938715934753}, {"text": "meeting summarisation", "start_pos": 83, "end_pos": 104, "type": "TASK", "confidence": 0.5531507432460785}, {"text": "topic segmentation", "start_pos": 106, "end_pos": 124, "type": "TASK", "confidence": 0.7632999420166016}, {"text": "dialogue act recognition", "start_pos": 126, "end_pos": 150, "type": "TASK", "confidence": 0.7425849835077921}, {"text": "cross-language retrieval", "start_pos": 155, "end_pos": 179, "type": "TASK", "confidence": 0.8575008511543274}]}, {"text": "In addition to intrinsic component-level evaluations, it is valuable to evaluate complete systems, and components in a system context.", "labels": [], "entities": []}, {"text": "In the AMI/AMIDA projects, we investigated a number of extrinsic evaluation frameworks for browsing and accessing meeting archives.", "labels": [], "entities": [{"text": "AMI/AMIDA", "start_pos": 7, "end_pos": 16, "type": "DATASET", "confidence": 0.7140954732894897}]}, {"text": "The Browser Evaluation Test (BET)) provides a framework for the comparison of arbitrary meeting browser setups, which may differ in terms of which content extraction or abstraction components are employed.", "labels": [], "entities": [{"text": "Browser Evaluation Test (BET))", "start_pos": 4, "end_pos": 34, "type": "METRIC", "confidence": 0.5252376447121302}]}, {"text": "In the BET test subjects have to answer true/false questions about a number of \"observations of interest\" relating to a recorded meeting, using the browser under test with a specified time limit (typically half the meeting length).", "labels": [], "entities": [{"text": "BET test", "start_pos": 7, "end_pos": 15, "type": "DATASET", "confidence": 0.6300583034753799}]}, {"text": "We developed of a variant of the BET to specifi- cally evaluate different summarisation approaches.", "labels": [], "entities": [{"text": "BET", "start_pos": 33, "end_pos": 36, "type": "METRIC", "confidence": 0.7168270349502563}]}, {"text": "In the Decision Audit evaluation () the user's task is to ascertain the factors across a number of meetings that lead to a particular decision being made.", "labels": [], "entities": [{"text": "Decision Audit evaluation", "start_pos": 7, "end_pos": 32, "type": "TASK", "confidence": 0.8522451519966125}]}, {"text": "A set of browsers were constructed differing in the summarisation approach employed (manual vs. ASR transcripts; extractive vs. abstractive vs. human vs. keyword-based summarisation), and the test subjects used them to perform the decision audit.", "labels": [], "entities": []}, {"text": "Like the BET this evaluation is labourintensive, but the results can be analysed using a battery of objective and subjective measures.", "labels": [], "entities": [{"text": "BET", "start_pos": 9, "end_pos": 12, "type": "DATASET", "confidence": 0.5729847550392151}]}, {"text": "Conclusions from carrying out this evaluation indicated that the task itself was quite challenging for users (even with human transcripts and summaries, most users could not find many factors involved in the decision), that automatic extractive summaries outperformed reasonably competitive baseline approaches, and that although subjects reported ASR transcripts to be unsatisfactory (due to the error rate) browsing using the ASR transcript still resulted in users' being generally able to find the relevant parts of the meeting archive.", "labels": [], "entities": []}], "tableCaptions": []}