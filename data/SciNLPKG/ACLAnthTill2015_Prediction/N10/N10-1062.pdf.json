{"title": [{"text": "Stream-based Translation Models for Statistical Machine Translation", "labels": [], "entities": [{"text": "Stream-based Translation", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.5544300377368927}, {"text": "Statistical Machine Translation", "start_pos": 36, "end_pos": 67, "type": "TASK", "confidence": 0.7984963258107504}]}], "abstractContent": [{"text": "Typical statistical machine translation systems are trained with static parallel corpora.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 8, "end_pos": 39, "type": "TASK", "confidence": 0.6373607218265533}]}, {"text": "Here we account for scenarios with a continuous incoming stream of parallel training data.", "labels": [], "entities": []}, {"text": "Such scenarios include daily governmental proceedings, sustained output from translation agencies, or crowd-sourced translations.", "labels": [], "entities": []}, {"text": "We show incorporating recent sentence pairs from the stream improves performance compared with a static baseline.", "labels": [], "entities": []}, {"text": "Since frequent batch retraining is computationally demanding we introduce a fast incremental alternative using an online version of the EM algorithm.", "labels": [], "entities": []}, {"text": "To bound our memory requirements we use a novel data-structure and associated training regime.", "labels": [], "entities": []}, {"text": "When compared to frequent batch retraining , our online time and space-bounded model achieves the same performance with significantly less computational overhead.", "labels": [], "entities": []}], "introductionContent": [{"text": "There is more parallel training data available today than there has ever been and it keeps increasing.", "labels": [], "entities": []}, {"text": "For example, the European Parliament 1 releases new parallel data in 22 languages on a regular basis.", "labels": [], "entities": [{"text": "European Parliament 1", "start_pos": 17, "end_pos": 38, "type": "DATASET", "confidence": 0.9025658766428629}]}, {"text": "Project Syndicate 2 translates editorials into seven languages (including Arabic, Chinese and Russian) everyday.", "labels": [], "entities": []}, {"text": "Existing translation systems often get 'crowd-sourced' improvements such as the option to contribute a better translation to GoogleTranslate . In these and many other instances, the data can be viewed as an incoming unbounded stream since 1 http://www.europarl.europa.eu 2 http://www.project-syndicate.org 3 http://www.translate.google.com the corpus grows continually with time.", "labels": [], "entities": []}, {"text": "Dealing with such unbounded streams of parallel sentences presents two challenges: making retraining efficient and operating within a bounded amount of space.", "labels": [], "entities": []}, {"text": "Statistical Machine Translation (SMT) systems are typically batch trained, often taking many CPUdays of computation when using large volumes of training material.", "labels": [], "entities": [{"text": "Statistical Machine Translation (SMT)", "start_pos": 0, "end_pos": 37, "type": "TASK", "confidence": 0.8642984529336294}]}, {"text": "Incorporating new data into these models forces us to retrain from scratch.", "labels": [], "entities": []}, {"text": "Clearly, this makes rapidly adding newly translated sentences into our models a daunting engineering challenge.", "labels": [], "entities": []}, {"text": "We introduce an adaptive training regime using an online variant of EM that is capable of incrementally adding new parallel sentences without incurring the burdens of full retraining.", "labels": [], "entities": []}, {"text": "For situations with large volumes of incoming parallel sentences we are also forced to consider placing space-bounds on our SMT system.", "labels": [], "entities": [{"text": "SMT", "start_pos": 124, "end_pos": 127, "type": "TASK", "confidence": 0.9834769368171692}]}, {"text": "We introduce a dynamic suffix array which allows us to add and delete parallel sentences, thereby maintaining bounded space despite processing a potentially high-rate input stream of unbounded length.", "labels": [], "entities": []}, {"text": "Taken as a whole we show that online translation models operating within bounded space can perform as well as systems which are batch-based and have no space constraints thereby making our approach suitable for stream-based translation.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section we describe the experiments conducted comparing various batch trained translation models (TMs) versus online incrementally retrained TMs in a full SMT setting with different conditions set on model coverage.", "labels": [], "entities": [{"text": "SMT", "start_pos": 163, "end_pos": 166, "type": "TASK", "confidence": 0.9700869917869568}]}, {"text": "We used publicly available resources for all our tests.", "labels": [], "entities": []}, {"text": "We start by showing that recency motivates incremental retraining.", "labels": [], "entities": [{"text": "recency motivates incremental retraining", "start_pos": 25, "end_pos": 65, "type": "TASK", "confidence": 0.7537923753261566}]}], "tableCaptions": [{"text": " Table 1. We held  out 4.5k sentence pairs as development data to opti- mize the feature function weights using minimum  error rate training", "labels": [], "entities": []}, {"text": " Table 1: Date ranges, total sentence pairs, and source and  target word counts encountered in the input stream for  example epochs. Epoch 00 is baseline data that is also  used as a seed corpus for the online models.", "labels": [], "entities": []}, {"text": " Table 2: Translation model statistics for example epochs and the next test dates grouped by experimental condition.  Test and Train Sent. is the number of sentence pairs in test and training data respectively. Rules is the count of unique  Hiero grammar rules extracted for the corresponding test set.", "labels": [], "entities": [{"text": "Translation model", "start_pos": 10, "end_pos": 27, "type": "TASK", "confidence": 0.885673999786377}]}, {"text": " Table 3: Sample BLEU results for all baseline and online EM model conditions. The static baseline is a traditional  model that is never retrained. The batch unbounded and batch bounded models incorporate new data from the stream  but retraining is slow and computationally expensive (best results are bolded). In contrast both unbounded and bounded  online models incrementally retrain only the mini-batch of new sentences collected from the incoming stream so  quickly adopt the new data (best results are italicized).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 17, "end_pos": 21, "type": "METRIC", "confidence": 0.9947850108146667}]}, {"text": " Table 4: Unbounded LM coverage improvements. Shown  are the BLEU scores for each experimental conditional  when we allow the LM coverage to increase.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 61, "end_pos": 65, "type": "METRIC", "confidence": 0.9995107650756836}]}]}