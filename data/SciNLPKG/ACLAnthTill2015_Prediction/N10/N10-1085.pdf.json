{"title": [{"text": "Prenominal Modifier Ordering via Multiple Sequence Alignment", "labels": [], "entities": [{"text": "Multiple Sequence Alignment", "start_pos": 33, "end_pos": 60, "type": "TASK", "confidence": 0.6128085255622864}]}], "abstractContent": [{"text": "Producing a fluent ordering fora set of prenominal modifiers in a noun phrase (NP) is a problematic task for natural language generation and machine translation systems.", "labels": [], "entities": [{"text": "natural language generation", "start_pos": 109, "end_pos": 136, "type": "TASK", "confidence": 0.6955624024073283}, {"text": "machine translation", "start_pos": 141, "end_pos": 160, "type": "TASK", "confidence": 0.7408806085586548}]}, {"text": "We present a novel approach to this issue, adapting multiple sequence alignment techniques used in computational biology to the alignment of modi-fiers.", "labels": [], "entities": []}, {"text": "We describe two training techniques to create such alignments based on raw text, and demonstrate ordering accuracies superior to earlier reported approaches.", "labels": [], "entities": []}], "introductionContent": [{"text": "Natural language generation and machine translation systems must produce text which not only conforms to a reasonable grammatical model, but which also sounds smooth and natural to a human consumer.", "labels": [], "entities": [{"text": "Natural language generation", "start_pos": 0, "end_pos": 27, "type": "TASK", "confidence": 0.6369185745716095}, {"text": "machine translation", "start_pos": 32, "end_pos": 51, "type": "TASK", "confidence": 0.7180669158697128}]}, {"text": "Ordering prenominal modifiers in noun phrases is particularly difficult in these applications, as the rules underlying these orderings are subtle and not well understood.", "labels": [], "entities": [{"text": "Ordering prenominal modifiers in noun phrases", "start_pos": 0, "end_pos": 45, "type": "TASK", "confidence": 0.8889575799306234}]}, {"text": "For example, the phrase \"big red ball\" seems natural, while \"red big ball\" seems more marked, suitable only in specific contexts.", "labels": [], "entities": []}, {"text": "There is some consensus that the order of prenominal modifiers in noun phrases is governed in part by semantic constraints, but there is no agreement on the exact constraints necessary to specify consistent orderings for any given set of modifiers.", "labels": [], "entities": []}, {"text": "General principles of modifier ordering based on semantic constraints also fall short on larger domains, where it is not always clear how to map prenominal modifiers to proposed semantic groups.", "labels": [], "entities": [{"text": "modifier ordering", "start_pos": 22, "end_pos": 39, "type": "TASK", "confidence": 0.8484123051166534}]}, {"text": "With the recent advantages of large corpora and powerful computational resources, work on automatically ordering prenominal modifiers has moved away from approaches based on general principles, and towards learning ordering preferences empirically from existing corpora.", "labels": [], "entities": []}, {"text": "Such approaches have several advantages: (1) The predicted orderings are based on prior evidence from 'real-world' texts, ensuring that they are therefore reasonably natural.", "labels": [], "entities": []}, {"text": "(2) Many (if not all) prenominal modifiers can be ordered.", "labels": [], "entities": []}, {"text": "(3) Expanding the training data with more and larger corpora often improves the system without requiring significant manual labor.", "labels": [], "entities": []}, {"text": "In this paper, we introduce a novel approach to prenominal modifier ordering adapted from multiple sequence alignment (MSA) techniques used in computational biology.", "labels": [], "entities": [{"text": "prenominal modifier ordering", "start_pos": 48, "end_pos": 76, "type": "TASK", "confidence": 0.67678102850914}, {"text": "multiple sequence alignment (MSA)", "start_pos": 90, "end_pos": 123, "type": "TASK", "confidence": 0.8119155466556549}]}, {"text": "MSA is generally applied to DNA, RNA, and protein sequences, aligning three or more biological sequences in order to determine, for example, common ancestry (.", "labels": [], "entities": []}, {"text": "MSA techniques have not been widely applied in NLP, but have produced some promising results for building a generation mapping dictionary (), paraphrasing (, and phone recognition ().", "labels": [], "entities": [{"text": "phone recognition", "start_pos": 162, "end_pos": 179, "type": "TASK", "confidence": 0.848108321428299}]}, {"text": "We believe that multiple sequence alignment is well-suited for aligning linguistic sequences, and that these alignments can be used to predict prenominal modifier ordering for any given set of modifiers.", "labels": [], "entities": []}, {"text": "Our technique utilizes simple features within the raw text, and does not require any semantic information.", "labels": [], "entities": []}, {"text": "We achieve good performance using this approach, with results competitive with earlier work and higher recall and F-measure than that reported in when tested on the same corpus.", "labels": [], "entities": [{"text": "recall", "start_pos": 103, "end_pos": 109, "type": "METRIC", "confidence": 0.9996500015258789}, {"text": "F-measure", "start_pos": 114, "end_pos": 123, "type": "METRIC", "confidence": 0.9915276169776917}]}], "datasetContent": [{"text": "We trained and tested on the same corpus used by, including identical 10-fold cross-validation splits.", "labels": [], "entities": []}, {"text": "The corpus consists of all NPs extracted from the Penn Treebank, the Brown corpus, and the Switchboard corpus ().", "labels": [], "entities": [{"text": "Penn Treebank", "start_pos": 50, "end_pos": 63, "type": "DATASET", "confidence": 0.9924305081367493}, {"text": "Brown corpus", "start_pos": 69, "end_pos": 81, "type": "DATASET", "confidence": 0.9675984382629395}, {"text": "Switchboard corpus", "start_pos": 91, "end_pos": 109, "type": "DATASET", "confidence": 0.8506978452205658}]}, {"text": "The corpus is heavily biased toward WSJ text (74%), with approximately 13% of the NPs from each of the other corpora.", "labels": [], "entities": [{"text": "WSJ text", "start_pos": 36, "end_pos": 44, "type": "DATASET", "confidence": 0.8542782962322235}]}, {"text": "We evaluated our system using several related but distinct metrics, and on both modifier pairs and full NPs.", "labels": [], "entities": []}, {"text": "We define: T = The set of unique orderings found in the test corpus P = The set of unique orderings predicted by the system Type Precision (|P \u2229 T|/|P|) measures the probability that a predicted ordering is 'reasonable' (where 'reasonable' is defined as orderings which are found in the test corpus).", "labels": [], "entities": []}, {"text": "Type Recall (|P \u2229 T|/|T|) measures the percentage of 'reasonable' orderings which the system recreates.", "labels": [], "entities": [{"text": "Recall", "start_pos": 5, "end_pos": 11, "type": "METRIC", "confidence": 0.5057874917984009}]}, {"text": "Note that these two metrics differ only in notation from those used by.", "labels": [], "entities": []}, {"text": "We also define a third metric, Token Accuracy, which measures accuracy on each individual ordering in the test corpus, rather than on unique orderings.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 37, "end_pos": 45, "type": "METRIC", "confidence": 0.77812260389328}, {"text": "accuracy", "start_pos": 62, "end_pos": 70, "type": "METRIC", "confidence": 0.998620867729187}]}, {"text": "This penalizes producing orderings which are legal, but uncommon.", "labels": [], "entities": [{"text": "penalizes producing orderings", "start_pos": 5, "end_pos": 34, "type": "TASK", "confidence": 0.8527243336041769}]}, {"text": "For example, if {a,b} occurs eight times in the test corpus as <a,b> and two times as <b,a>, we will be limited to a maximum accuracy of 80% (presuming our system correctly predicts the more common ordering).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 125, "end_pos": 133, "type": "METRIC", "confidence": 0.9985741376876831}]}, {"text": "However, even though suggesting <b,a> is not strictly incorrect, we generally prefer to reward a system that produces more common orderings, an attribute not emphasized by type-based metrics.", "labels": [], "entities": []}, {"text": "Our test corpus does not contain many ambiguous pairings, so our theoretical maximum token accuracy is 99.8%.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 91, "end_pos": 99, "type": "METRIC", "confidence": 0.9704728126525879}]}], "tableCaptions": [{"text": " Table 6: Full NP ordering accuracies; averages and standard deviations over a 10-fold cross validation. To  compare directly with Mitchell", "labels": [], "entities": []}, {"text": " Table 7: Descriminative model performance on NPs  of various lengths, including pairwise measures.", "labels": [], "entities": []}, {"text": " Table 8: Ablation test results on the discriminative  model.", "labels": [], "entities": [{"text": "Ablation", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9940179586410522}]}]}