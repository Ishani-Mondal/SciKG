{"title": [{"text": "The Effect of Ambiguity on the Automated Acquisition of WSD Examples", "labels": [], "entities": []}], "abstractContent": [{"text": "Several methods for automatically generating labeled examples that can be used as training data for WSD systems have been proposed, including a semi-supervised approach based on relevance feedback (Stevenson et al., 2008a).", "labels": [], "entities": [{"text": "WSD", "start_pos": 100, "end_pos": 103, "type": "TASK", "confidence": 0.9635404348373413}]}, {"text": "This approach was shown to generate examples that improved the performance of a WSD system fora set of ambiguous terms from the biomedical domain.", "labels": [], "entities": [{"text": "WSD", "start_pos": 80, "end_pos": 83, "type": "TASK", "confidence": 0.9619856476783752}]}, {"text": "However, we find that this approach does not perform as well on other data sets.", "labels": [], "entities": []}, {"text": "The levels of ambiguity in these data sets are analysed and we suggest this is the reason for this negative result.", "labels": [], "entities": []}], "introductionContent": [{"text": "Several studies, for example (, have shown that supervised approaches to Word Sense Disambiguation (WSD) outperform unsupervised ones.", "labels": [], "entities": [{"text": "Word Sense Disambiguation (WSD)", "start_pos": 73, "end_pos": 104, "type": "TASK", "confidence": 0.7699012110630671}]}, {"text": "But these rely on labeled training data which is difficult to create and not always available (e.g.).", "labels": [], "entities": []}, {"text": "Various techniques for creating labeled training data automatically have been suggested in the literature.", "labels": [], "entities": []}, {"text": "describe a semi-supervised approach that used relevance feedback to analyse existing labeled examples and use the information produced to generate further ones.", "labels": [], "entities": []}, {"text": "The approach was tested on the biomedical domain and the additional examples found to improve performance of a WSD system.", "labels": [], "entities": [{"text": "WSD", "start_pos": 111, "end_pos": 114, "type": "TASK", "confidence": 0.9448097944259644}]}, {"text": "However, biomedical documents represent a restricted domain.", "labels": [], "entities": []}, {"text": "In this paper the same approach is tested against two data sets that are not limited to a single domain.", "labels": [], "entities": []}], "datasetContent": [{"text": "Experiments were carried out comparing performance when the WSD system was trained using either the examples in the original data set (original), the examples generated from these using the relevance feedback approach (additional) or a combination of these (combined).", "labels": [], "entities": []}, {"text": "The Senseval-3 and SemEval corpora are split into training and test portions so the training portion is used as the original data set and the WSD system evaluated against the held-back data.", "labels": [], "entities": []}, {"text": "As there is no such recognised standard split for the NLM-WSD corpus, 10-fold cross-validation was used.", "labels": [], "entities": [{"text": "NLM-WSD corpus", "start_pos": 54, "end_pos": 68, "type": "DATASET", "confidence": 0.8470889031887054}]}, {"text": "For each fold the training portion is used as the original data set and automatically generated examples created by examining just that part of the data.", "labels": [], "entities": []}, {"text": "Evaluation is carried out against the fold's test data and the average result across the 10 folds reported.", "labels": [], "entities": [{"text": "fold's test data", "start_pos": 38, "end_pos": 54, "type": "DATASET", "confidence": 0.6564061269164085}]}, {"text": "shows the results of this experiment.", "labels": [], "entities": []}, {"text": "2 Examples generated using the relevance feedback approach only improve results for one data set, the NLM-WSD corpus.", "labels": [], "entities": [{"text": "NLM-WSD corpus", "start_pos": 102, "end_pos": 116, "type": "DATASET", "confidence": 0.9300219714641571}]}, {"text": "In this case there is a significant improvement (Mann-Whitney, p < 0.01) when the original and automatically generated examples are combined.", "labels": [], "entities": []}, {"text": "There is no such improvement for the other two data sets: WSD results using the additional data are noticeably worse than when the original data is used alone and, although performance improves when these examples are combined with the original data, results are still lower than using the original data.", "labels": [], "entities": [{"text": "WSD", "start_pos": 58, "end_pos": 61, "type": "TASK", "confidence": 0.6304407715797424}]}, {"text": "When examples are combined there is a drop in performance of 1.2% and 2.9% for SemEval and Senseval-3 re-spectively.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Results of relevance feedback approach  applied to three data sets", "labels": [], "entities": []}, {"text": " Table 2: Properties of Data Sets using sense distri- bution measures", "labels": [], "entities": []}]}