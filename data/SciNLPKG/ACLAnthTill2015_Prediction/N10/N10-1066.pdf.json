{"title": [{"text": "Discriminative Learning over Constrained Latent Representations", "labels": [], "entities": []}], "abstractContent": [{"text": "This paper proposes a general learning framework fora class of problems that require learning over latent intermediate representations.", "labels": [], "entities": []}, {"text": "Many natural language processing (NLP) decision problems are defined over an expressive intermediate representation that is not explicit in the input, leaving the algorithm with both the task of recovering a good intermediate representation and learning to classify correctly.", "labels": [], "entities": [{"text": "natural language processing (NLP) decision", "start_pos": 5, "end_pos": 47, "type": "TASK", "confidence": 0.7390378543308803}]}, {"text": "Most current systems separate the learning problem into two stages by solving the first step of recovering the intermediate representation heuristically and using it to learn the final classifier.", "labels": [], "entities": []}, {"text": "This paper develops a novel joint learning algorithm for both tasks, that uses the final prediction to guide the selection of the best intermediate representation.", "labels": [], "entities": []}, {"text": "We evaluate our algorithm on three different NLP tasks-transliteration, paraphrase identification and textual entailment-and show that our joint method significantly improves performance.", "labels": [], "entities": [{"text": "paraphrase identification", "start_pos": 72, "end_pos": 97, "type": "TASK", "confidence": 0.7821415364742279}, {"text": "textual entailment-and", "start_pos": 102, "end_pos": 124, "type": "TASK", "confidence": 0.6842954754829407}]}], "introductionContent": [{"text": "Many NLP tasks can be phrased as decision problems over complex linguistic structures.", "labels": [], "entities": []}, {"text": "Successful learning depends on correctly encoding these (often latent) structures as features for the learning system.", "labels": [], "entities": []}, {"text": "Tasks such as transliteration discovery, recognizing textual entailment (RTE) () and paraphrase identification ( ) area few prototypical examples.", "labels": [], "entities": [{"text": "transliteration discovery", "start_pos": 14, "end_pos": 39, "type": "TASK", "confidence": 0.9350315034389496}, {"text": "recognizing textual entailment (RTE)", "start_pos": 41, "end_pos": 77, "type": "TASK", "confidence": 0.7735273987054825}, {"text": "paraphrase identification", "start_pos": 85, "end_pos": 110, "type": "TASK", "confidence": 0.8895397186279297}]}, {"text": "However, the input to such problems does not specify the latent structures and the problem is defined in terms of surface forms only.", "labels": [], "entities": []}, {"text": "Most current solutions transform the raw input into a meaningful intermediate representation , and then encode its structural properties as features for the learning algorithm.", "labels": [], "entities": []}, {"text": "Consider the RTE task of identifying whether the meaning of a short text snippet (called the hypothesis) can be inferred from that of another snippet (called the text).", "labels": [], "entities": [{"text": "RTE task", "start_pos": 13, "end_pos": 21, "type": "TASK", "confidence": 0.888657808303833}]}, {"text": "A common solution) is to begin by defining an alignment over the corresponding entities, predicates and their arguments as an intermediate representation.", "labels": [], "entities": []}, {"text": "A classifier is then trained using features extracted from the intermediate representation.", "labels": [], "entities": []}, {"text": "The idea of using a intermediate representation also occurs frequently in other NLP tasks).", "labels": [], "entities": []}, {"text": "While the importance of finding a good intermediate representation is clear, emphasis is typically placed on the later stage of extracting features over this intermediate representation, thus separating learning into two stages -specifying the latent representation, and then extracting features for learning.", "labels": [], "entities": []}, {"text": "The latent representation is obtained by an inference process using predefined models or welldesigned heuristics.", "labels": [], "entities": []}, {"text": "While these approaches often perform well, they ignore a useful resource when generating the latent structure -the labeled data for the final learning task.", "labels": [], "entities": []}, {"text": "As we will show in this paper, this results in degraded performance for the actual classification task at hand.", "labels": [], "entities": []}, {"text": "Several works have considered this issue (); however, they provide solutions that do not easily generalize to new tasks.", "labels": [], "entities": []}, {"text": "In this paper, we propose a unified solution to the problem of learning to make the classification decision jointly with determining the intermediate representation.", "labels": [], "entities": []}, {"text": "Our Learning Constrained Latent Representations (LCLR) framework is guided by the intuition that there is no intrinsically good intermediate representation, but rather that a representation is good only to the extent to which it improves performance on the final classification task.", "labels": [], "entities": [{"text": "Learning Constrained Latent Representations (LCLR)", "start_pos": 4, "end_pos": 54, "type": "TASK", "confidence": 0.7505213618278503}]}, {"text": "In the rest of this section we discuss the properties of our framework and highlight its contributions.", "labels": [], "entities": []}, {"text": "Learning over Latent Representations This paper formulates the problem of learning over latent representations and presents a novel and general solution applicable to a wide range of NLP applications.", "labels": [], "entities": []}, {"text": "We analyze the properties of our learning solution, thus allowing new research to take advantage of a well understood learning and optimization framework rather than an ad-hoc solution.", "labels": [], "entities": []}, {"text": "We show the generality of our framework by successfully applying it to three domains: transliteration, RTE and paraphrase identification.", "labels": [], "entities": [{"text": "RTE", "start_pos": 103, "end_pos": 106, "type": "TASK", "confidence": 0.929866373538971}, {"text": "paraphrase identification", "start_pos": 111, "end_pos": 136, "type": "TASK", "confidence": 0.8662438988685608}]}, {"text": "Joint Learning Algorithm In contrast to most existing approaches that employ domain specific heuristics to construct intermediate representations to learn the final classifier, our algorithm learns to construct the optimal intermediate representation to support the learning problem.", "labels": [], "entities": []}, {"text": "Learning to represent is a difficult structured learning problem however, unlike other works that use labeled data at the intermediate level, our algorithm only uses the binary supervision supplied for the final learning problem.", "labels": [], "entities": []}, {"text": "Flexible Inference Successful learning depends on constraining the intermediate representation with task-specific knowledge.", "labels": [], "entities": [{"text": "Flexible Inference Successful learning", "start_pos": 0, "end_pos": 38, "type": "TASK", "confidence": 0.8068533912301064}]}, {"text": "Our framework uses the declarative Integer Linear Programming (ILP) inference formulation, which makes it easy to define the intermediate representation and to inject knowledge in the form of constraints.", "labels": [], "entities": []}, {"text": "While ILP has been applied to structured output learning, to the best of our knowledge, this is the first work that makes use of ILP in formalizing the general problem of learning intermediate representations.", "labels": [], "entities": []}], "datasetContent": [{"text": "We applied our framework to three different NLP tasks: transliteration discovery (), RTE (), and paraphrase identification ( . Our experiments are designed to answer the following research question: \"Given a binary classification problem defined over latent representations, will the joint LCLR algorithm perform better than a two-stage approach?\"", "labels": [], "entities": [{"text": "transliteration discovery", "start_pos": 55, "end_pos": 80, "type": "TASK", "confidence": 0.8512335419654846}, {"text": "RTE", "start_pos": 85, "end_pos": 88, "type": "TASK", "confidence": 0.52977454662323}, {"text": "paraphrase identification", "start_pos": 97, "end_pos": 122, "type": "TASK", "confidence": 0.9195181727409363}]}, {"text": "To ensure a fair comparison, both systems use the same feature functions and definition of intermediate representation.", "labels": [], "entities": []}, {"text": "We use the same ILP formulation in both configurations, with a single exception -the objective function parameters: the two stage approach uses a task-specific heuristic, while LCLR learns it iteratively.", "labels": [], "entities": []}, {"text": "The ILP formulation results in very strong two stage systems.", "labels": [], "entities": []}, {"text": "For example, in the paraphrase identification task, even our two stage system is the current state-of-the-art performance.", "labels": [], "entities": [{"text": "paraphrase identification task", "start_pos": 20, "end_pos": 50, "type": "TASK", "confidence": 0.9587226311365763}]}, {"text": "In these settings, the improvement obtained by our joint approach is non-trivial and can be clearly attributed to the superiority of the joint learning algorithm.", "labels": [], "entities": []}, {"text": "Interestingly, we find that our more general approach is better than specially designed joint approaches.", "labels": [], "entities": []}, {"text": "Since the objective function (3) of the joint approach is not convex, a good initialization is required.", "labels": [], "entities": []}, {"text": "We use the weight vector learned by the two stage approach as the starting point for the joint approach.", "labels": [], "entities": []}, {"text": "The algorithm terminates when the relative improvement of the objective is smaller than 10 \u22125 .", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Experimental results for transliteration. We compare", "labels": [], "entities": []}, {"text": " Table 2: Summary of latent variables and feature resources for", "labels": [], "entities": []}, {"text": " Table 3: Experimental results for recognizing textual entail-", "labels": [], "entities": [{"text": "recognizing textual entail", "start_pos": 35, "end_pos": 61, "type": "TASK", "confidence": 0.8165594736735026}]}, {"text": " Table 4: Experimental Result For Paraphrasing Identification.", "labels": [], "entities": [{"text": "Paraphrasing Identification", "start_pos": 34, "end_pos": 61, "type": "TASK", "confidence": 0.9697529375553131}]}]}