{"title": [{"text": "Information Content Measures of Semantic Similarity Perform Better Without Sense-Tagged Text", "labels": [], "entities": [{"text": "Information Content Measures", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.6126871903737386}]}], "abstractContent": [{"text": "This paper presents an empirical comparison of similarity measures for pairs of concepts based on Information Content.", "labels": [], "entities": []}, {"text": "It shows that using modest amounts of untagged text to derive Information Content results in higher correlation with human similarity judgments than using the largest available corpus of manually annotated sense-tagged text.", "labels": [], "entities": []}], "introductionContent": [{"text": "Measures of semantic similarity based on WordNet have been widely used in Natural Language Processing.", "labels": [], "entities": []}, {"text": "These measures rely on the structure of WordNet to produce a numeric score that quantifies the degree to which two concepts (represented by a sense or synset) are similar (or not).", "labels": [], "entities": [{"text": "WordNet", "start_pos": 40, "end_pos": 47, "type": "DATASET", "confidence": 0.9427654147148132}]}, {"text": "In their simplest form these measures use path length to identify concepts that are physically close to each other and therefore considered to be more similar than concepts that are further apart.", "labels": [], "entities": []}, {"text": "While this is a reasonable first approximation to semantic similarity, there are some well known limitations.", "labels": [], "entities": [{"text": "semantic similarity", "start_pos": 50, "end_pos": 69, "type": "TASK", "confidence": 0.8669904470443726}]}, {"text": "Most significant is that path lengths between very specific concepts imply much smaller distinctions in semantic similarity than do comparable path lengths between very general concepts.", "labels": [], "entities": []}, {"text": "One proposed improvement is to augment concepts in WordNet with Information Content values derived from sense-tagged corpora or from raw unannotated corpora.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 51, "end_pos": 58, "type": "DATASET", "confidence": 0.9278166890144348}]}, {"text": "This paper shows that Information Content measures based on modest amounts of unannotated corpora have greater correlation with human similarity judgements than do those based on the largest corpus of sense-tagged text currently available.", "labels": [], "entities": []}, {"text": "The key to this success is not in the specific type of corpora used, but rather in increasing the number of concepts in WordNet that have counts associated with them.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 120, "end_pos": 127, "type": "DATASET", "confidence": 0.9225614070892334}]}, {"text": "These results show that Information Content measures of semantic similarity can be significantly improved without requiring the creation of sensetagged corpora (which is very expensive).", "labels": [], "entities": []}], "datasetContent": [{"text": "Information Content in WordNet::Similarity is (by default) derived from SemCor (), a manually sense-tagged subset of the Brown Corpus.", "labels": [], "entities": [{"text": "Brown Corpus", "start_pos": 121, "end_pos": 133, "type": "DATASET", "confidence": 0.964738667011261}]}, {"text": "It is made up of approximately 676,000 words, of which 226,000 are sense-tagged.", "labels": [], "entities": []}, {"text": "SemCor was originally created using sense-tags from version 1.6 of WordNet, and has been mapped to subsequent versions to stay current.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 67, "end_pos": 74, "type": "DATASET", "confidence": 0.9547268152236938}]}, {"text": "3 This paper uses version 3.0 of WordNet and SemCor.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 33, "end_pos": 40, "type": "DATASET", "confidence": 0.9591650366783142}]}, {"text": "WordNet::Similarity also includes a utility (rawtextFreq.pl) that allows a user to derive Information Content values from any corpus of plain text.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 0, "end_pos": 7, "type": "DATASET", "confidence": 0.9060186147689819}, {"text": "Similarity", "start_pos": 9, "end_pos": 19, "type": "TASK", "confidence": 0.8518853783607483}]}, {"text": "This utility is used with the untagged version of SemCor and with various portions of the English GigaWord corpus (1st edition) to derive alternative Information Content values.", "labels": [], "entities": [{"text": "English GigaWord corpus", "start_pos": 90, "end_pos": 113, "type": "DATASET", "confidence": 0.8258859912554423}]}, {"text": "English GigaWord contains more than 1.7 billion words of newspaper text from the 1990's and early 21st century, divided among four different sources: Agence France Press English Service (afe), Associated Press Worldstream English Service (apw), The New York Times Newswire Service (nyt), and The Xinhua News Agency English Service (xie).", "labels": [], "entities": [{"text": "Agence France Press English Service", "start_pos": 150, "end_pos": 185, "type": "DATASET", "confidence": 0.8493971467018128}, {"text": "Associated Press Worldstream English Service (apw)", "start_pos": 193, "end_pos": 243, "type": "DATASET", "confidence": 0.8091985434293747}, {"text": "The Xinhua News Agency English Service", "start_pos": 292, "end_pos": 330, "type": "DATASET", "confidence": 0.7490524699290594}]}, {"text": "This paper compares the ranking of pairs of concepts according to Information Content measures in WordNet::Similarity with a number of manually created gold standards.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 98, "end_pos": 105, "type": "DATASET", "confidence": 0.9308068156242371}]}, {"text": "These include the (RG)) collection of 65 noun).", "labels": [], "entities": []}, {"text": "RG and MC have been scored for similarity, while WS is scored for relatedness, which is a more general and less well-defined notion than similarity.", "labels": [], "entities": [{"text": "RG", "start_pos": 0, "end_pos": 2, "type": "METRIC", "confidence": 0.8221750855445862}, {"text": "similarity", "start_pos": 31, "end_pos": 41, "type": "METRIC", "confidence": 0.9847603440284729}]}, {"text": "For example aspirin and headache are clearly related, but they aren't really similar.", "labels": [], "entities": []}, {"text": "shows the Spearman's rank correlation of several other measures of similarity and relatedness in WordNet::Similarity with the gold standards discussed above.", "labels": [], "entities": [{"text": "Spearman's rank correlation", "start_pos": 10, "end_pos": 37, "type": "METRIC", "confidence": 0.686953142285347}, {"text": "WordNet", "start_pos": 97, "end_pos": 104, "type": "DATASET", "confidence": 0.9685510396957397}]}, {"text": "The WordNet::Similarity vector relatedness measure achieves the highest correlation, followed closely by the adapted lesk measure.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 4, "end_pos": 11, "type": "DATASET", "confidence": 0.9160704016685486}, {"text": "correlation", "start_pos": 72, "end_pos": 83, "type": "METRIC", "confidence": 0.9845713376998901}]}, {"text": "These results are consistent with previous findings).", "labels": [], "entities": []}, {"text": "This table also shows results for several path-based measures.", "labels": [], "entities": []}, {"text": "4 shows the correlation of jcn, res, and lin when Information Content is derived from 1) the sense-tagged version of SemCor (semcor), 2) SemCor without sense tags (semcor-raw), and 3) steadily increasing subsets of the 133 million word xie portion of the English GigaWord corpus.", "labels": [], "entities": [{"text": "English GigaWord corpus", "start_pos": 255, "end_pos": 278, "type": "DATASET", "confidence": 0.8121403455734253}]}, {"text": "These subsets start with the entire first month of xie (199501, from January 1995) and then two months, three months (199501-03), up through all of 1995 . Thereafter the increments are annual, with two years of data, then three, and soon until the entire xie corpus is used.", "labels": [], "entities": []}, {"text": "The afe, apw, and nyt portions of GigaWord are also used individually and then combined all together along with xie (all).", "labels": [], "entities": [{"text": "GigaWord", "start_pos": 34, "end_pos": 42, "type": "DATASET", "confidence": 0.9457958936691284}]}, {"text": "The size (in tokens) of each corpus is shown in the second column of Table 2 (size), which is expressed in thousands (k), millions (m), and billions (b).", "labels": [], "entities": []}, {"text": "The third column (cover) shows what percentage of the 96,000 noun and verb synsets in WordNet receive a non-zero frequency count when Information Content is derived from the specified corpus.", "labels": [], "entities": []}, {"text": "These values show that the 226,000 sense-tagged instances in SemCor cover about 24%, and the untagged version of SemCor covers 37%.", "labels": [], "entities": []}, {"text": "As it happens the correlation results for semcor-raw are somewhat better than semcor, suggesting that coverage is at least as important (if not more so) to the performance of Information Content measures than accurate mapping of words to concepts.", "labels": [], "entities": []}, {"text": "A similar pattern can be seen with the xie results in.", "labels": [], "entities": []}, {"text": "This again shows that an increase in WordNet coverage is associated with increased performance of the Information Content measures.", "labels": [], "entities": []}, {"text": "As coverage increases the correlation improves, and in fact the results are better than the path-based measures and approach those of lesk and vector (see Table 1).", "labels": [], "entities": [{"text": "coverage", "start_pos": 3, "end_pos": 11, "type": "METRIC", "confidence": 0.9074431657791138}, {"text": "correlation", "start_pos": 26, "end_pos": 37, "type": "METRIC", "confidence": 0.9724999070167542}]}, {"text": "The one exception is with respect to the WS gold standard, where vector and lesk perform much better than the Information Content measures.", "labels": [], "entities": [{"text": "WS gold standard", "start_pos": 41, "end_pos": 57, "type": "DATASET", "confidence": 0.8670329848925272}]}, {"text": "However, this seems reasonable since they are relatedness measures, and the WS corpus is annotated for relatedness rather than similarity.", "labels": [], "entities": [{"text": "WS corpus", "start_pos": 76, "end_pos": 85, "type": "DATASET", "confidence": 0.9011967480182648}]}, {"text": "As a final test of the hypothesis that coverage matters as much or more than accurate mapping of words to concepts, a simple baseline method was created that assigns each synset a count of 1, and then propagates that count up to the ancestor concepts.", "labels": [], "entities": []}, {"text": "This is equivalent to doing add-1 smoothing without any text (add1only).", "labels": [], "entities": []}, {"text": "This results in correlation nearly as high as the best results with xie and semcor-raw, and is significantly better than semcor.", "labels": [], "entities": [{"text": "correlation", "start_pos": 16, "end_pos": 27, "type": "METRIC", "confidence": 0.9638761281967163}]}], "tableCaptions": [{"text": " Table 1: Rank Correlation of Existing Measures", "labels": [], "entities": [{"text": "Rank Correlation of Existing Measures", "start_pos": 10, "end_pos": 47, "type": "TASK", "confidence": 0.6400146067142487}]}]}