{"title": [{"text": "Integrating Joint n-gram Features into a Discriminative Training Framework", "labels": [], "entities": []}], "abstractContent": [{"text": "Phonetic string transduction problems, such as letter-to-phoneme conversion and name transliteration, have recently received much attention in the NLP community.", "labels": [], "entities": [{"text": "Phonetic string transduction", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.8065361380577087}, {"text": "letter-to-phoneme conversion", "start_pos": 47, "end_pos": 75, "type": "TASK", "confidence": 0.7599823474884033}, {"text": "name transliteration", "start_pos": 80, "end_pos": 100, "type": "TASK", "confidence": 0.7352438420057297}]}, {"text": "In the past few years, two methods have come to dominate as solutions to supervised string trans-duction: generative joint n-gram models, and discriminative sequence models.", "labels": [], "entities": [{"text": "generative joint n-gram", "start_pos": 106, "end_pos": 129, "type": "TASK", "confidence": 0.8544964591662089}]}, {"text": "Both approaches benefit from their ability to consider large, flexible spans of source context when making transduction decisions.", "labels": [], "entities": []}, {"text": "However, they encode this context in different ways, providing their respective models with different information.", "labels": [], "entities": []}, {"text": "To combine the strengths of these two systems, we include joint n-gram features inside a state-of-the-art discriminative sequence model.", "labels": [], "entities": []}, {"text": "We evaluate our approach on several letter-to-phoneme and translitera-tion data sets.", "labels": [], "entities": []}, {"text": "Our results indicate an improvement in overall performance with respect to both the joint n-gram approach and traditional feature sets for discriminative models.", "labels": [], "entities": []}], "introductionContent": [{"text": "Phonetic string transduction transforms a source string into a target representation according to its pronunciation.", "labels": [], "entities": [{"text": "Phonetic string transduction", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.6992336511611938}]}, {"text": "Two important examples of this task are letter-to-phoneme conversion and name transliteration.", "labels": [], "entities": [{"text": "letter-to-phoneme conversion", "start_pos": 40, "end_pos": 68, "type": "TASK", "confidence": 0.7207598686218262}, {"text": "name transliteration", "start_pos": 73, "end_pos": 93, "type": "TASK", "confidence": 0.8116266131401062}]}, {"text": "In general, the problem is challenging because source orthography does not unambiguously specify the target representation.", "labels": [], "entities": []}, {"text": "When considering letter-to-phoneme, ambiguities and exceptions in the pronunciation of orthography complicate conversion.", "labels": [], "entities": [{"text": "conversion", "start_pos": 110, "end_pos": 120, "type": "TASK", "confidence": 0.9701136946678162}]}, {"text": "Transliteration suffers from the same ambiguities, but the transformation is further complicated by restrictions in the target orthography that may not exist in the source.", "labels": [], "entities": []}, {"text": "Joint n-gram models ( have been widely applied to string transduction problems (.", "labels": [], "entities": [{"text": "string transduction", "start_pos": 50, "end_pos": 69, "type": "TASK", "confidence": 0.7682251632213593}]}, {"text": "The power of the approach lies in building a language model over the operations used in the conversion from source to target.", "labels": [], "entities": []}, {"text": "Crucially, this allows the inclusion of source context in the generative story.", "labels": [], "entities": [{"text": "generative story", "start_pos": 62, "end_pos": 78, "type": "TASK", "confidence": 0.9148485362529755}]}, {"text": "Smoothing techniques play an important role in joint n-gram models, greatly affecting their performance.", "labels": [], "entities": []}, {"text": "Although joint n-gram models are capable of capturing context information in both source and target, they cannot selectively use only source or target information, nor can they consider arbitrary sequences within their context window, as they are limited by their back-off schedule.", "labels": [], "entities": []}, {"text": "Discriminative sequence models have also been shown to perform extremely well on string transduction problems.", "labels": [], "entities": [{"text": "string transduction", "start_pos": 81, "end_pos": 100, "type": "TASK", "confidence": 0.7521899342536926}]}, {"text": "These begin with a Hidden Markov Model architecture, augmented with substring operations and discriminative training.", "labels": [], "entities": []}, {"text": "The primary strength of these systems is their ability to include rich indicator features representing long sequences of source context.", "labels": [], "entities": []}, {"text": "We will assume a specific instance of discriminative sequence modeling, DI-RECTL (, which achieved the best results on several language pairs in the NEWS Machine Transliteration Shared Task ().", "labels": [], "entities": [{"text": "NEWS Machine Transliteration Shared Task", "start_pos": 149, "end_pos": 189, "type": "TASK", "confidence": 0.6180128455162048}]}, {"text": "The same system matches or exceeds the performance of the joint n-gram approach on letterto-phoneme conversion (.", "labels": [], "entities": [{"text": "letterto-phoneme conversion", "start_pos": 83, "end_pos": 110, "type": "TASK", "confidence": 0.7316485047340393}]}, {"text": "Its features are optimized by an online, margin-based learning algorithm, specifically, the Margin Infused Relaxed Algorithm, MIRA.", "labels": [], "entities": [{"text": "MIRA", "start_pos": 126, "end_pos": 130, "type": "METRIC", "confidence": 0.7251870632171631}]}, {"text": "In this paper, we propose an approach that combines these two different paradigms by formulating the joint n-gram model as anew set of features in the discriminative model.", "labels": [], "entities": []}, {"text": "This leverages an advantage of discriminative training, in that it can easily and effectively incorporate arbitrary features.", "labels": [], "entities": []}, {"text": "We evaluate our approach on several letter-to-phoneme and transliteration data sets.", "labels": [], "entities": []}, {"text": "Our results demonstrate an improvement in overall performance with respect to both the generative joint n-gram approach and the original DIRECTL system.", "labels": [], "entities": [{"text": "generative joint n-gram", "start_pos": 87, "end_pos": 110, "type": "TASK", "confidence": 0.8786449035008749}]}], "datasetContent": [{"text": "We evaluate our new approach on two string transduction applications: (1) letter-to-phoneme conversion and (2) name transliteration.", "labels": [], "entities": [{"text": "letter-to-phoneme conversion", "start_pos": 74, "end_pos": 102, "type": "TASK", "confidence": 0.7208998948335648}, {"text": "name transliteration", "start_pos": 111, "end_pos": 131, "type": "TASK", "confidence": 0.751226931810379}]}, {"text": "For the letter-tophoneme conversion, we employ the English Celex, NETtalk, OALD, CMUdict, and the French Brulex data sets.", "labels": [], "entities": [{"text": "English Celex", "start_pos": 51, "end_pos": 64, "type": "DATASET", "confidence": 0.8890440464019775}, {"text": "OALD", "start_pos": 75, "end_pos": 79, "type": "METRIC", "confidence": 0.8984265923500061}, {"text": "French Brulex data sets", "start_pos": 98, "end_pos": 121, "type": "DATASET", "confidence": 0.8271578326821327}]}, {"text": "In order to perform direct comparison with the joint n-gram approach, we follow exactly the same data splits as.", "labels": [], "entities": []}, {"text": "The training sizes range from 19K to 106K words.", "labels": [], "entities": []}, {"text": "For the transliteration task, we use three data sets provided by the NEWS 2009 Machine Transliteration Shared Task (): English-Russian (EnRu), English-Chinese (EnCh), and English-Hindi (EnHi).", "labels": [], "entities": [{"text": "NEWS 2009 Machine Transliteration Shared Task", "start_pos": 69, "end_pos": 114, "type": "DATASET", "confidence": 0.8261451423168182}]}, {"text": "The training sizes range from 10K to 30K words.", "labels": [], "entities": []}, {"text": "We set n = 6 for the joint n-gram features; other parameters are set on the respective development sets.", "labels": [], "entities": []}, {"text": "show the performance of our new system in comparison with the joint n-gram approach and DIRECTL.", "labels": [], "entities": [{"text": "DIRECTL", "start_pos": 88, "end_pos": 95, "type": "METRIC", "confidence": 0.7391464710235596}]}, {"text": "The results in the rightmost column of are taken directly from, where they were evaluated on the same data splits.", "labels": [], "entities": []}, {"text": "The results in the rightmost column of are from, which was the best performing system based on joint   n-grams at NEWS 2009.", "labels": [], "entities": [{"text": "NEWS 2009", "start_pos": 114, "end_pos": 123, "type": "DATASET", "confidence": 0.9636008143424988}]}, {"text": "We report all results in terms of the word accuracy, which awards the system only for complete matches between system outputs and the references.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 43, "end_pos": 51, "type": "METRIC", "confidence": 0.9175899028778076}]}, {"text": "Our full system outperforms both DIRECTL and the joint n-gram approach in all data sets.", "labels": [], "entities": [{"text": "DIRECTL", "start_pos": 33, "end_pos": 40, "type": "METRIC", "confidence": 0.7344595789909363}]}, {"text": "This shows the utility of adding joint n-gram features to the DIRECTL system, and confirms an advantage of discriminative approaches: strong competitors can simply be folded into the model.", "labels": [], "entities": []}, {"text": "Comparing across tables, one can see that the gap between the generative joint n-gram and the DI-RECTL methods is much larger for the transliteration tasks.", "labels": [], "entities": [{"text": "generative joint n-gram", "start_pos": 62, "end_pos": 85, "type": "TASK", "confidence": 0.8579147060712179}]}, {"text": "This could be because joint n-grams area poor fit for transliteration, or the gap could stem from differences between the joint n-gram implementations used for the two tasks.", "labels": [], "entities": []}, {"text": "Looking at the improvements to DIRECTL from joint n-gram features, we see further evidence that joint n-grams are better suited to letter-to-phoneme than they are to transliteration: letter-to-phoneme improvements range from relative error reductions of 3.6 to 17.3, while in transliteration, the largest reduction is 3.1.", "labels": [], "entities": [{"text": "relative error reductions", "start_pos": 225, "end_pos": 250, "type": "METRIC", "confidence": 0.7446458538373312}]}], "tableCaptions": [{"text": " Table 2: Letter-to-phoneme conversion accuracy", "labels": [], "entities": [{"text": "Letter-to-phoneme conversion", "start_pos": 10, "end_pos": 38, "type": "TASK", "confidence": 0.7703500688076019}]}, {"text": " Table 3: Name transliteration accuracy", "labels": [], "entities": [{"text": "Name transliteration", "start_pos": 10, "end_pos": 30, "type": "TASK", "confidence": 0.8865045011043549}]}]}