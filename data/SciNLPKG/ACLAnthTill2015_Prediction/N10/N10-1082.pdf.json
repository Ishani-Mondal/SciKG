{"title": [], "abstractContent": [{"text": "Most existing algorithms for learning latent-variable models-such as EM and existing Gibbs samplers-are token-based, meaning that they update the variables associated with one sentence at a time.", "labels": [], "entities": []}, {"text": "The incremental nature of these methods makes them susceptible to local optima/slow mixing.", "labels": [], "entities": []}, {"text": "In this paper, we introduce a type-based sampler, which updates a block of variables, identified by a type, which spans multiple sentences.", "labels": [], "entities": []}, {"text": "We show improvements on part-of-speech induction, word segmentation, and learning tree-substitution grammars.", "labels": [], "entities": [{"text": "part-of-speech induction", "start_pos": 24, "end_pos": 48, "type": "TASK", "confidence": 0.6965742409229279}, {"text": "word segmentation", "start_pos": 50, "end_pos": 67, "type": "TASK", "confidence": 0.7811379730701447}]}], "introductionContent": [{"text": "A long-standing challenge in NLP is the unsupervised induction of linguistic structures, for example, grammars from raw sentences or lexicons from phoneme sequences.", "labels": [], "entities": []}, {"text": "A fundamental property of these unsupervised learning problems is multimodality.", "labels": [], "entities": []}, {"text": "In grammar induction, for example, we could analyze subject-verb-object sequences as either ((subject verb) object) (mode 1) or (subject (verb object)) (mode 2).", "labels": [], "entities": [{"text": "grammar induction", "start_pos": 3, "end_pos": 20, "type": "TASK", "confidence": 0.7064831703901291}]}, {"text": "Multimodality causes problems for token-based procedures that update variables for one example at a time.", "labels": [], "entities": []}, {"text": "In EM, for example, if the parameters already assign high probability to the ((subject verb) object) analysis, re-analyzing the sentences in E-step only reinforces the analysis, resulting in EM getting stuck in a local optimum.", "labels": [], "entities": []}, {"text": "In (collapsed) Gibbs sampling, if all sentences are already analyzed as ((subject verb) object), sampling a sentence conditioned   on all others will most likely not change its analysis, resulting in slow mixing.", "labels": [], "entities": []}, {"text": "To combat the problems associated with tokenbased algorithms, we propose anew sampling algorithm that operates on types.", "labels": [], "entities": []}, {"text": "Our sampler would, for example, be able to change all occurrences of ((subject verb) object) to (subject (verb object)) in one step.", "labels": [], "entities": []}, {"text": "These type-based operations are reminiscent of the type-based grammar operations of early chunkmerge systems), but we work within a sampling framework for increased robustness.", "labels": [], "entities": []}, {"text": "In NLP, perhaps the the most simple and popular sampler is the token-based Gibbs sampler, 1 used in,, and many others.", "labels": [], "entities": []}, {"text": "By sampling only one variable at a time, this sampler is prone to slow mixing due to the strong coupling between variables.", "labels": [], "entities": []}, {"text": "A general remedy is to sample blocks of coupled variables.", "labels": [], "entities": []}, {"text": "For example, the sentence-based sampler samples all the variables associated with a sentence at once (e.g., the entire tag sequence).", "labels": [], "entities": []}, {"text": "However, this blocking does not deal with the strong type-based coupling (e.g., all instances of a word should be tagged similarly).", "labels": [], "entities": []}, {"text": "The type-based sampler we will present is designed exactly to tackle this coupling, which we argue is stronger and more important to deal within unsupervised learning.", "labels": [], "entities": []}, {"text": "depicts the updates made by each of the three samplers.", "labels": [], "entities": []}, {"text": "We tested our sampler on three models: a Bayesian HMM for part-of-speech induction, a nonparametric Bayesian model for word segmentation), and a nonparametric Bayesian model of tree substitution grammars (.", "labels": [], "entities": [{"text": "part-of-speech induction", "start_pos": 58, "end_pos": 82, "type": "TASK", "confidence": 0.7460084557533264}, {"text": "word segmentation", "start_pos": 119, "end_pos": 136, "type": "TASK", "confidence": 0.7642814218997955}, {"text": "tree substitution grammars", "start_pos": 177, "end_pos": 203, "type": "TASK", "confidence": 0.7164614001909891}]}, {"text": "Empirically, we find that typebased sampling improves performance and is less sensitive to initialization (Section 5).", "labels": [], "entities": []}], "datasetContent": [{"text": "We now compare our proposed type-based sampler to various alternatives, evaluating on marginal like-lihood (3) and accuracy for our three models: \u2022 HMM: We learned a K = 45 state HMM on the Wall Street Journal (WSJ) portion of the Penn Treebank (49208 sentences, 45 tags) for part-ofspeech induction.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 115, "end_pos": 123, "type": "METRIC", "confidence": 0.999677300453186}, {"text": "Wall Street Journal (WSJ) portion of the Penn Treebank", "start_pos": 190, "end_pos": 244, "type": "DATASET", "confidence": 0.9121768799695101}, {"text": "part-ofspeech induction", "start_pos": 276, "end_pos": 299, "type": "TASK", "confidence": 0.7538493871688843}]}, {"text": "We fixed \u03b1 r to 0.1 and \u00b5 r to uniform for all r.", "labels": [], "entities": []}, {"text": "For accuracy, we used the standard metric based on greedy mapping, where each state is mapped to the POS tag that maximizes the number of correct matches).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.9957956075668335}]}, {"text": "We did not use a tagging dictionary.", "labels": [], "entities": []}, {"text": "\u2022 For accuracy, we used word token F 1 . \u2022 PTSG: We learned a PTSG model on sections 2-21 of the WSJ treebank.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 6, "end_pos": 14, "type": "METRIC", "confidence": 0.9988598823547363}, {"text": "PTSG", "start_pos": 43, "end_pos": 47, "type": "METRIC", "confidence": 0.9655759334564209}, {"text": "PTSG", "start_pos": 62, "end_pos": 66, "type": "METRIC", "confidence": 0.9130575060844421}, {"text": "WSJ treebank", "start_pos": 97, "end_pos": 109, "type": "DATASET", "confidence": 0.9841489195823669}]}, {"text": "5 For accuracy, we used EVALB parsing F 1 on section 22. 6 Note this is a supervised task with latent-variables, whereas the other two are purely unsupervised.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 6, "end_pos": 14, "type": "METRIC", "confidence": 0.9990416169166565}, {"text": "EVALB parsing", "start_pos": 24, "end_pos": 37, "type": "TASK", "confidence": 0.6430907845497131}]}], "tableCaptions": []}