{"title": [{"text": "Joint Parsing and Alignment with Weakly Synchronized Grammars", "labels": [], "entities": [{"text": "Joint Parsing and Alignment", "start_pos": 0, "end_pos": 27, "type": "TASK", "confidence": 0.630180336534977}]}], "abstractContent": [{"text": "Syntactic machine translation systems extract rules from bilingual, word-aligned, syntactically parsed text, but current systems for parsing and word alignment are at best cascaded and at worst totally independent of one another.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 145, "end_pos": 159, "type": "TASK", "confidence": 0.7060003280639648}]}, {"text": "This work presents a unified joint model for simultaneous parsing and word alignment.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 70, "end_pos": 84, "type": "TASK", "confidence": 0.7628897130489349}]}, {"text": "To flexibly model syntactic divergence, we develop a discriminative log-linear model over two parse trees and an ITG derivation which is encouraged but not forced to synchronize with the parses.", "labels": [], "entities": [{"text": "syntactic divergence", "start_pos": 18, "end_pos": 38, "type": "TASK", "confidence": 0.6646466255187988}]}, {"text": "Our model gives absolute improvements of 3.3 F 1 for English parsing , 2.1 F 1 for Chinese parsing, and 5.5 F 1 for word alignment over each task's independent baseline, giving the best reported results for both Chinese-English word alignment and joint parsing on the parallel portion of the Chi-nese treebank.", "labels": [], "entities": [{"text": "F", "start_pos": 45, "end_pos": 46, "type": "METRIC", "confidence": 0.9850096106529236}, {"text": "English parsing", "start_pos": 53, "end_pos": 68, "type": "TASK", "confidence": 0.67534039914608}, {"text": "F 1", "start_pos": 75, "end_pos": 78, "type": "METRIC", "confidence": 0.9851386845111847}, {"text": "Chinese parsing", "start_pos": 83, "end_pos": 98, "type": "TASK", "confidence": 0.6607181280851364}, {"text": "F 1", "start_pos": 108, "end_pos": 111, "type": "METRIC", "confidence": 0.9766415059566498}, {"text": "word alignment", "start_pos": 116, "end_pos": 130, "type": "TASK", "confidence": 0.7478727400302887}, {"text": "Chinese-English word alignment", "start_pos": 212, "end_pos": 242, "type": "TASK", "confidence": 0.6340601642926534}]}, {"text": "We also show an improvement of 1.2 BLEU in downstream MT evaluation over basic HMM alignments.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 35, "end_pos": 39, "type": "METRIC", "confidence": 0.996673583984375}, {"text": "MT", "start_pos": 54, "end_pos": 56, "type": "TASK", "confidence": 0.9844045042991638}]}], "introductionContent": [{"text": "Current syntactic machine translation (MT) systems build synchronous context free grammars from aligned syntactic fragments ().", "labels": [], "entities": [{"text": "syntactic machine translation (MT)", "start_pos": 8, "end_pos": 42, "type": "TASK", "confidence": 0.8040585319201151}]}, {"text": "Extracting such grammars requires that bilingual word alignments and monolingual syntactic parses be compatible.", "labels": [], "entities": [{"text": "bilingual word alignments", "start_pos": 39, "end_pos": 64, "type": "TASK", "confidence": 0.6731359163920084}]}, {"text": "Because of this, much recent work in both word alignment and parsing has focused on changing aligners to make use of syntactic information or changing parsers to make use of word alignments ().", "labels": [], "entities": [{"text": "word alignment", "start_pos": 42, "end_pos": 56, "type": "TASK", "confidence": 0.8049451112747192}, {"text": "parsing", "start_pos": 61, "end_pos": 68, "type": "TASK", "confidence": 0.8452227711677551}]}, {"text": "In the first case, however, parsers do not exploit bilingual information.", "labels": [], "entities": []}, {"text": "In the second, word alignment is performed with a model that does not exploit syntactic information.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 15, "end_pos": 29, "type": "TASK", "confidence": 0.8118725121021271}]}, {"text": "This work presents a single, joint model for parsing and word alignment that allows both pieces to influence one another simultaneously.", "labels": [], "entities": [{"text": "parsing", "start_pos": 45, "end_pos": 52, "type": "TASK", "confidence": 0.964413583278656}, {"text": "word alignment", "start_pos": 57, "end_pos": 71, "type": "TASK", "confidence": 0.7014215141534805}]}, {"text": "While building a joint model seems intuitive, there is no easy way to characterize how word alignments and syntactic parses should relate to each other in general.", "labels": [], "entities": [{"text": "word alignments and syntactic parses", "start_pos": 87, "end_pos": 123, "type": "TASK", "confidence": 0.6816157579421998}]}, {"text": "In the ideal situation, each pair of sentences in a bilingual corpus could be syntactically parsed using asynchronous context-free grammar.", "labels": [], "entities": []}, {"text": "Of course, real translations are almost always at least partially syntactically divergent.", "labels": [], "entities": []}, {"text": "Therefore, it is unreasonable to expect perfect matches of any kind between the two sides' syntactic trees, much less expect that those matches be well explained at a word level.", "labels": [], "entities": []}, {"text": "Indeed, it is sometimes the case that large pieces of a sentence pair are completely asynchronous and can only be explained monolingually.", "labels": [], "entities": []}, {"text": "Our model exploits synchronization where possible to perform more accurately on both word alignment and parsing, but also allows independent models to dictate pieces of parse trees and word alignments when synchronization is impossible.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 85, "end_pos": 99, "type": "TASK", "confidence": 0.7604170441627502}, {"text": "parsing", "start_pos": 104, "end_pos": 111, "type": "TASK", "confidence": 0.895843505859375}, {"text": "word alignments", "start_pos": 185, "end_pos": 200, "type": "TASK", "confidence": 0.6758425235748291}]}, {"text": "This notion of \"weak synchronization\" is parameterized and estimated from data to maximize the likelihood of the correct parses and word alignments.", "labels": [], "entities": [{"text": "word alignments", "start_pos": 132, "end_pos": 147, "type": "TASK", "confidence": 0.6739410161972046}]}, {"text": "Weak synchronization is closely related to the quasi-synchronous models of and the bilingual parse reranking model of, but those models assume that the word alignment of a sentence pair is known and fixed.", "labels": [], "entities": [{"text": "Weak synchronization", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.7046655714511871}]}, {"text": "To simultaneously model both parses and align-ments, our model loosely couples three separate combinatorial structures: monolingual trees in the source and target languages, and asynchronous ITG alignment that links the two languages (but is not constrained to match linguistic syntax).", "labels": [], "entities": []}, {"text": "The model has no hard constraints on how these three structures must align, but instead contains a set of \"synchronization\" features that are used to propagate influence between the three component grammars.", "labels": [], "entities": []}, {"text": "The presence of synchronization features couples the parses and alignments, but makes exact inference in the model intractable; we show how to use a variational mean field approximation, both for computing approximate feature expectations during training, and for performing approximate joint inference attest time.", "labels": [], "entities": []}, {"text": "We train our joint model on the parallel, gold word-aligned portion of the Chinese treebank.", "labels": [], "entities": [{"text": "Chinese treebank", "start_pos": 75, "end_pos": 91, "type": "DATASET", "confidence": 0.8242471218109131}]}, {"text": "When evaluated on parsing and word alignment, this model significantly improves over independentlytrained baselines: the monolingual parser of and the discriminative word aligner of.", "labels": [], "entities": [{"text": "parsing", "start_pos": 18, "end_pos": 25, "type": "TASK", "confidence": 0.9673603177070618}, {"text": "word alignment", "start_pos": 30, "end_pos": 44, "type": "TASK", "confidence": 0.7551815807819366}]}, {"text": "It also improves over the discriminative, bilingual parsing model of, yielding the highest joint parsing F 1 numbers on this data set.", "labels": [], "entities": [{"text": "bilingual parsing", "start_pos": 42, "end_pos": 59, "type": "TASK", "confidence": 0.49988631904125214}]}, {"text": "Finally, our model improves word alignment in the context of translation, leading to a 1.2 BLEU increase over using HMM word alignments.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 28, "end_pos": 42, "type": "TASK", "confidence": 0.7503892183303833}, {"text": "translation", "start_pos": 61, "end_pos": 72, "type": "TASK", "confidence": 0.9570552110671997}, {"text": "BLEU", "start_pos": 91, "end_pos": 95, "type": "METRIC", "confidence": 0.9989198446273804}, {"text": "HMM word alignments", "start_pos": 116, "end_pos": 135, "type": "TASK", "confidence": 0.6399935980637869}]}], "datasetContent": [{"text": "We trained and tested our model on the translated portion of the Chinese treebank (, which includes hand annotated Chinese and English parses and word alignments.", "labels": [], "entities": [{"text": "Chinese treebank", "start_pos": 65, "end_pos": 81, "type": "DATASET", "confidence": 0.7922773361206055}, {"text": "word alignments", "start_pos": 146, "end_pos": 161, "type": "TASK", "confidence": 0.6719414293766022}]}, {"text": "We separated the data into three sets: train, dev, and test, according to the standard Chinese treebank split.", "labels": [], "entities": [{"text": "Chinese treebank split", "start_pos": 87, "end_pos": 109, "type": "DATASET", "confidence": 0.8806442618370056}]}, {"text": "To speedup training, we only used training sentences of length \u2264 50 words, which left us with 1974 of 2261 sentences.", "labels": [], "entities": [{"text": "1974", "start_pos": 94, "end_pos": 98, "type": "METRIC", "confidence": 0.9355915188789368}]}, {"text": "We measured the results in two ways.", "labels": [], "entities": []}, {"text": "First, we directly measured F 1 for English parsing, Chinese parsing, and word alignment on a held out section of the hand annotated corpus used to train the model.", "labels": [], "entities": [{"text": "F 1", "start_pos": 28, "end_pos": 31, "type": "METRIC", "confidence": 0.989391952753067}, {"text": "English parsing", "start_pos": 36, "end_pos": 51, "type": "TASK", "confidence": 0.698141485452652}, {"text": "Chinese parsing", "start_pos": 53, "end_pos": 68, "type": "TASK", "confidence": 0.7748361825942993}, {"text": "word alignment", "start_pos": 74, "end_pos": 88, "type": "TASK", "confidence": 0.7827709913253784}]}, {"text": "Next, we further evaluated the quality of the word alignments produced by our model by using them as input fora machine translation system.", "labels": [], "entities": [{"text": "word alignments", "start_pos": 46, "end_pos": 61, "type": "TASK", "confidence": 0.7106457352638245}]}, {"text": "The Chinese treebank gold word alignments include significantly more many-to-many word alignments than those used by.", "labels": [], "entities": [{"text": "Chinese treebank gold word alignments", "start_pos": 4, "end_pos": 41, "type": "DATASET", "confidence": 0.8513992309570313}]}, {"text": "We are able to produce some of these many-to-many alignments by including new many-to-many terminals in our ITG word aligner, as shown in.", "labels": [], "entities": []}, {"text": "Our terminal productions sometimes capture non-literal translation like both sides or in recent years.", "labels": [], "entities": []}, {"text": "They also can allow us to capture particular, systematic changes in the annotation standard.", "labels": [], "entities": []}, {"text": "For example, the gapped pattern from captures the standard that English word the is always aligned to the Chinese head noun in a noun phrase.", "labels": [], "entities": []}, {"text": "We featurize these non-terminals with features similar to those of, and all of the alignment results we report in Section 8.2 (both joint and ITG) employ these features.", "labels": [], "entities": [{"text": "ITG", "start_pos": 142, "end_pos": 145, "type": "DATASET", "confidence": 0.901006281375885}]}], "tableCaptions": [{"text": " Table 1: Parsing results. Our joint model has the highest  reported F 1 for English-Chinese bilingual parsing.", "labels": [], "entities": [{"text": "F 1", "start_pos": 69, "end_pos": 72, "type": "METRIC", "confidence": 0.9954331517219543}, {"text": "English-Chinese bilingual parsing", "start_pos": 77, "end_pos": 110, "type": "TASK", "confidence": 0.528851161400477}]}, {"text": " Table 2: Word alignment results. Our joint model has the  highest reported F 1 for English-Chinese word alignment.", "labels": [], "entities": [{"text": "Word alignment", "start_pos": 10, "end_pos": 24, "type": "TASK", "confidence": 0.7809635400772095}, {"text": "F 1", "start_pos": 76, "end_pos": 79, "type": "METRIC", "confidence": 0.9937219023704529}, {"text": "English-Chinese word alignment", "start_pos": 84, "end_pos": 114, "type": "TASK", "confidence": 0.5807600319385529}]}, {"text": " Table 3: Tune and test BLEU results for machine transla- tion systems built with different alignment tools.  \u2020 indi- cates a statistically significant difference between a sys- tem's test performance and the one above it.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 24, "end_pos": 28, "type": "METRIC", "confidence": 0.9983470439910889}]}]}