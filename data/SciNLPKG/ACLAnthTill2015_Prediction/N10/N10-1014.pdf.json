{"title": [{"text": "Unsupervised Syntactic Alignment with Inversion Transduction Grammars", "labels": [], "entities": [{"text": "Syntactic Alignment", "start_pos": 13, "end_pos": 32, "type": "TASK", "confidence": 0.7686426341533661}]}], "abstractContent": [{"text": "Syntactic machine translation systems currently use word alignments to infer syntactic correspondences between the source and target languages.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 10, "end_pos": 29, "type": "TASK", "confidence": 0.8058179616928101}, {"text": "word alignments", "start_pos": 52, "end_pos": 67, "type": "TASK", "confidence": 0.723373681306839}]}, {"text": "Instead, we propose an un-supervised ITG alignment model that directly aligns syntactic structures.", "labels": [], "entities": [{"text": "ITG alignment", "start_pos": 37, "end_pos": 50, "type": "TASK", "confidence": 0.8087985813617706}]}, {"text": "Our model aligns spans in a source sentence to nodes in a target parse tree.", "labels": [], "entities": []}, {"text": "We show that our model produces syntactically consistent analyses where possible , while being robust in the face of syntactic divergence.", "labels": [], "entities": []}, {"text": "Alignment quality and end-to-end translation experiments demonstrate that this consistency yields higher quality alignments than our baseline.", "labels": [], "entities": []}], "introductionContent": [{"text": "Syntactic machine translation has advanced significantly in recent years, and multiple variants currently achieve state-of-the-art translation quality.", "labels": [], "entities": [{"text": "Syntactic machine translation", "start_pos": 0, "end_pos": 29, "type": "TASK", "confidence": 0.6924892067909241}]}, {"text": "Many of these systems exploit linguistically-derived syntactic information either on the target side (), the source side ( ), or both ( . Still others induce their syntax from the data (.", "labels": [], "entities": []}, {"text": "Despite differences in detail, the vast majority of syntactic methods share a critical dependence on word alignments.", "labels": [], "entities": [{"text": "word alignments", "start_pos": 101, "end_pos": 116, "type": "TASK", "confidence": 0.6976285874843597}]}, {"text": "In particular, they infer syntactic correspondences between the source and target languages through word alignment patterns, sometimes in combination with constraints from parser outputs.", "labels": [], "entities": []}, {"text": "However, word alignments are not perfect indicators of syntactic alignment, and syntactic systems are very sensitive to word alignment behavior.", "labels": [], "entities": [{"text": "word alignments", "start_pos": 9, "end_pos": 24, "type": "TASK", "confidence": 0.7733391523361206}, {"text": "syntactic alignment", "start_pos": 55, "end_pos": 74, "type": "TASK", "confidence": 0.778802752494812}, {"text": "word alignment", "start_pos": 120, "end_pos": 134, "type": "TASK", "confidence": 0.7410929501056671}]}, {"text": "Even a single spurious word alignment can invalidate a large number of otherwise extractable rules, while unaligned words can result in an exponentially large set of extractable rules to choose from.", "labels": [], "entities": []}, {"text": "Researchers have worked to incorporate syntactic information into word alignments, resulting in improvements to both alignment quality, and translation quality.", "labels": [], "entities": [{"text": "word alignments", "start_pos": 66, "end_pos": 81, "type": "TASK", "confidence": 0.7019013315439224}]}, {"text": "In this paper, we remove the dependence on word alignments and instead directly model the syntactic correspondences in the data, in a manner broadly similar to.", "labels": [], "entities": [{"text": "word alignments", "start_pos": 43, "end_pos": 58, "type": "TASK", "confidence": 0.7048917561769485}]}, {"text": "In particular, we propose an unsupervised model that aligns nodes of a parse tree (or forest) in one language to spans of a sentence in another.", "labels": [], "entities": []}, {"text": "Our model is an instance of the inversion transduction grammar (ITG) formalism, constrained in such away that one side of the synchronous derivation respects a syntactic parse.", "labels": [], "entities": [{"text": "inversion transduction grammar (ITG) formalism", "start_pos": 32, "end_pos": 78, "type": "TASK", "confidence": 0.8106718744550433}]}, {"text": "Our model is best suited to systems which use source-or target-side trees only.", "labels": [], "entities": []}, {"text": "The design of our model is such that, for divergent structures, a structurally integrated backoff to flatter word-level (or null) analyses is available.", "labels": [], "entities": []}, {"text": "Therefore, our model is empirically robust to the case where syntactic divergence between languages prevents syntactically accurate ITG derivations.", "labels": [], "entities": [{"text": "syntactic divergence between languages", "start_pos": 61, "end_pos": 99, "type": "TASK", "confidence": 0.7982056140899658}]}, {"text": "We show that, with appropriate pruning, our model can be efficiently trained on large parallel corpora.", "labels": [], "entities": []}, {"text": "When compared to standard word-alignmentbacked baselines, our model produces more consistent analyses of parallel sentences, leading to high-count, high-quality transfer rules.", "labels": [], "entities": []}, {"text": "End-toend translation experiments demonstrate that these higher quality rules improve translation quality by 1.0 BLEU over a word-alignment-backed baseline.", "labels": [], "entities": [{"text": "End-toend translation", "start_pos": 0, "end_pos": 21, "type": "TASK", "confidence": 0.589638963341713}, {"text": "BLEU", "start_pos": 113, "end_pos": 117, "type": "METRIC", "confidence": 0.9968532919883728}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 3: Alignment quality results for our syntactic  aligner and our GIZA++ baseline.", "labels": [], "entities": []}, {"text": " Table 4: Final tune and test set results for our grammars  extracted using the baseline GIZA++ alignments and our  syntactic aligner. When we filter the GIZA++ grammars  with the same count thresholds used for our aligner (\"high  count\"), BLEU score drops substantially.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 240, "end_pos": 250, "type": "METRIC", "confidence": 0.9701923727989197}]}]}