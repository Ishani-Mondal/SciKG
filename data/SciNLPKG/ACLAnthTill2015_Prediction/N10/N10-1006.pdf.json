{"title": [{"text": "Using Confusion Networks for Speech Summarization", "labels": [], "entities": [{"text": "Speech Summarization", "start_pos": 29, "end_pos": 49, "type": "TASK", "confidence": 0.7353556454181671}]}], "abstractContent": [{"text": "For extractive meeting summarization, previous studies have shown performance degradation when using speech recognition transcripts because of the relatively high speech recognition errors on meeting recordings.", "labels": [], "entities": [{"text": "extractive meeting summarization", "start_pos": 4, "end_pos": 36, "type": "TASK", "confidence": 0.7506381372610728}]}, {"text": "In this paper we investigated using confusion networks to improve the summarization performance on the ASR condition under an unsupervised framework by considering more word candidates and their confidence scores.", "labels": [], "entities": [{"text": "summarization", "start_pos": 70, "end_pos": 83, "type": "TASK", "confidence": 0.9743314385414124}, {"text": "ASR", "start_pos": 103, "end_pos": 106, "type": "TASK", "confidence": 0.9455963969230652}]}, {"text": "Our experimental results showed improved summa-rization performance using our proposed approach , with more contribution from leverag-ing the confidence scores.", "labels": [], "entities": []}, {"text": "We also observed that using these rich speech recognition results can extract similar or even better summary segments than using human transcripts.", "labels": [], "entities": []}], "introductionContent": [{"text": "Speech summarization has received increasing interest recently.", "labels": [], "entities": [{"text": "Speech summarization", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.7628284692764282}]}, {"text": "It is a very useful technique that can help users to browse a large amount of speech recordings.", "labels": [], "entities": []}, {"text": "The problem we study in this paper is extractive meeting summarization, which selects the most representative segments from the meeting transcripts to form a summary.", "labels": [], "entities": [{"text": "extractive meeting summarization", "start_pos": 38, "end_pos": 70, "type": "TASK", "confidence": 0.7395826379458109}]}, {"text": "Compared to text summarization, speech summarization is more challenging because of not only its more spontaneous style, but also word errors in automatic speech recognition (ASR) output.", "labels": [], "entities": [{"text": "text summarization", "start_pos": 12, "end_pos": 30, "type": "TASK", "confidence": 0.7382566332817078}, {"text": "speech summarization", "start_pos": 32, "end_pos": 52, "type": "TASK", "confidence": 0.7491470277309418}, {"text": "automatic speech recognition (ASR) output", "start_pos": 145, "end_pos": 186, "type": "TASK", "confidence": 0.822906630379813}]}, {"text": "Intuitively the incorrect words have a negative impact on downstream summarization performance.", "labels": [], "entities": [{"text": "summarization", "start_pos": 69, "end_pos": 82, "type": "TASK", "confidence": 0.8984975218772888}]}, {"text": "Previous research has evaluated summarization using either the human transcripts or ASR output with word errors.", "labels": [], "entities": [{"text": "summarization", "start_pos": 32, "end_pos": 45, "type": "TASK", "confidence": 0.9867286086082458}]}, {"text": "Most of the prior work showed that performance using ASR output is consistently lower (to different extent) comparing to that using human transcripts no matter whether supervised or unsupervised approaches were used.", "labels": [], "entities": [{"text": "ASR", "start_pos": 53, "end_pos": 56, "type": "TASK", "confidence": 0.9842048287391663}]}, {"text": "To address the problem caused by imperfect recognition transcripts, in this paper we investigate using rich speech recognition results for summarization.", "labels": [], "entities": []}, {"text": "N-best hypotheses, word lattices, and confusion networks have been widely used as an interface between ASR and subsequent spoken language processing tasks, such as machine translation, spoken document retrieval (, and shown outperforming using 1-best hypotheses.", "labels": [], "entities": [{"text": "ASR", "start_pos": 103, "end_pos": 106, "type": "TASK", "confidence": 0.9930543303489685}, {"text": "machine translation", "start_pos": 164, "end_pos": 183, "type": "TASK", "confidence": 0.7941305935382843}, {"text": "spoken document retrieval", "start_pos": 185, "end_pos": 210, "type": "TASK", "confidence": 0.6488401889801025}]}, {"text": "However, studies using these rich speech recognition results for speech summarization are very limited.", "labels": [], "entities": [{"text": "speech summarization", "start_pos": 65, "end_pos": 85, "type": "TASK", "confidence": 0.6748843193054199}]}, {"text": "In this paper, we demonstrate the feasibility of using confusion networks under an unsupervised MMR (maximum marginal relevance) framework to improve summarization performance.", "labels": [], "entities": [{"text": "summarization", "start_pos": 150, "end_pos": 163, "type": "TASK", "confidence": 0.9805317521095276}]}, {"text": "Our experimental results show better performance over using 1-best hypotheses with more improvement observed from using confidence measure of the words.", "labels": [], "entities": []}, {"text": "Moreover, we find that the selected summary segments are similar to or even better than those generated using human transcripts.", "labels": [], "entities": []}], "datasetContent": [{"text": "We use the ICSI meeting corpus, which contains 75 recordings from natural meetings (most are research discussions) (.", "labels": [], "entities": [{"text": "ICSI meeting corpus", "start_pos": 11, "end_pos": 30, "type": "DATASET", "confidence": 0.9507594307263693}]}, {"text": "Each meeting is about an hour long and has multiple speakers.", "labels": [], "entities": []}, {"text": "These meetings have been transcribed, and annotated with extractive summaries ().", "labels": [], "entities": []}, {"text": "The ASR output is obtained from a state-of-the-art SRI speech recognition system, including the confusion network for each sentence segment ().", "labels": [], "entities": [{"text": "ASR", "start_pos": 4, "end_pos": 7, "type": "TASK", "confidence": 0.9769482612609863}, {"text": "SRI speech recognition", "start_pos": 51, "end_pos": 73, "type": "TASK", "confidence": 0.6118219991525015}]}, {"text": "The word error rate (WER) is about 38.2% on the entire corpus.", "labels": [], "entities": [{"text": "word error rate (WER)", "start_pos": 4, "end_pos": 25, "type": "METRIC", "confidence": 0.8413268427054087}]}, {"text": "The same 6 meetings as in () are used as the test set in this study.", "labels": [], "entities": []}, {"text": "Furthermore, 6 other meetings were randomly selected from the remaining 69 meetings in the corpus to form a development set.", "labels": [], "entities": []}, {"text": "Each meeting in the development set has only one human-annotated summary; whereas for the test meetings, we use three summaries from different annotators as references for performance evaluation.", "labels": [], "entities": []}, {"text": "The lengths of the reference summaries are not fixed and vary across annotators and meetings.", "labels": [], "entities": []}, {"text": "The average word compression ratio for the test set is 14.3%, and the mean deviation is 2.9%.", "labels": [], "entities": []}, {"text": "We generated summaries with the word compression ratio ranging from 13% to 18%, and only provide the best results in this paper.", "labels": [], "entities": []}, {"text": "To evaluate summarization performance, we use ROUGE), which has been widely used in previous studies of speech summarization (;).", "labels": [], "entities": [{"text": "summarization", "start_pos": 12, "end_pos": 25, "type": "TASK", "confidence": 0.9892977476119995}, {"text": "ROUGE", "start_pos": 46, "end_pos": 51, "type": "METRIC", "confidence": 0.9979739785194397}, {"text": "speech summarization", "start_pos": 104, "end_pos": 124, "type": "TASK", "confidence": 0.6772500276565552}]}, {"text": "ROUGE compares the system generated summary with reference summaries (there can be more than one reference summary), and measures different matches, such as N-gram, longest common sequence, and skip bigrams.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 0, "end_pos": 5, "type": "METRIC", "confidence": 0.7172474265098572}]}, {"text": "In this paper, we present our results using both ROUGE-1 and ROUGE-2 F-scores.", "labels": [], "entities": [{"text": "ROUGE-1", "start_pos": 49, "end_pos": 56, "type": "METRIC", "confidence": 0.9939504861831665}, {"text": "ROUGE-2", "start_pos": 61, "end_pos": 68, "type": "METRIC", "confidence": 0.9485822916030884}, {"text": "F-scores", "start_pos": 69, "end_pos": 77, "type": "METRIC", "confidence": 0.5946508646011353}]}, {"text": "The summarization results on the test set are presented in.", "labels": [], "entities": []}, {"text": "We show four different evaluation conditions: baseline using the top hypotheses, best hypotheses with posterior probabilities, pruned CNs, and using human transcripts.", "labels": [], "entities": []}, {"text": "For each condition, the final summary is evaluated using the best hypotheses or the corresponding human transcripts of the selected segments.", "labels": [], "entities": []}, {"text": "The summarization system setups (the pruning method and threshold, \u03bb value in MMR function, and word compression ratio) used for the test set are decided based on the results on the development set.", "labels": [], "entities": [{"text": "word compression ratio", "start_pos": 96, "end_pos": 118, "type": "METRIC", "confidence": 0.611452321211497}]}, {"text": "For the results on the test set, we observe similar trends as on the development set.", "labels": [], "entities": []}, {"text": "Using the confidence scores and confusion networks can improve the summarization performance comparing with the baseline.", "labels": [], "entities": [{"text": "summarization", "start_pos": 67, "end_pos": 80, "type": "TASK", "confidence": 0.9618174433708191}]}, {"text": "The performance improvements from \"Best hyp\" to \"Best hyp (wp)\" and from \"Best hyp (wp)\" to \"Pruned CNs\" using both ROUGE-1 and ROUGE-2 measures are statistically significant according to the paired t-test (p < 0.05).", "labels": [], "entities": []}, {"text": "When the final summary is presented using the human transcripts of the selected segments, we observe slightly better results using pruned CNs than using human transcripts as input for summarization, although the difference is not statistically significant.", "labels": [], "entities": [{"text": "summarization", "start_pos": 184, "end_pos": 197, "type": "TASK", "confidence": 0.9780336022377014}]}, {"text": "This shows that using confusion networks can compensate for the impact from recognition errors and still allow us to select correct summary segments.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1. Comparing the results for  the two testing conditions, ASR output and human  transcripts, we can see the performance degradation  due to recognition errors. The difference between  them seems to be large enough to warrant investiga- tion of using rich ASR output for improved summa- rization performance.", "labels": [], "entities": [{"text": "ASR", "start_pos": 65, "end_pos": 68, "type": "TASK", "confidence": 0.9387977123260498}]}, {"text": " Table 1: ROUGE results (%) using 1-best hypotheses and  human transcripts on the development set.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 10, "end_pos": 15, "type": "METRIC", "confidence": 0.9946485161781311}]}, {"text": " Table 2: ROUGE results (%) on the development set us- ing different vector representations based on confusion  networks: non-pruned and pruned, using posterior prob- abilities (\"wp\") and without using them.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 10, "end_pos": 15, "type": "METRIC", "confidence": 0.9939060807228088}]}, {"text": " Table 3: ROUGE results (%) on the development set  using different segment representations, with the sum- maries constructed using the corresponding human tran- scripts for the selected segments.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 10, "end_pos": 15, "type": "METRIC", "confidence": 0.9949823021888733}]}, {"text": " Table 2. These two setups use  the same utterance segments. The only difference  lies in the construction of the final summary for  performance measurement, using the top hypothe- ses or the corresponding human transcripts for the  selected segments. We also notice that the differ- ence between using 1-best hypothesis and human  transcripts is greatly reduced using this new sum- mary formulation. This suggests that the incorrect  word hypotheses do not have a very negative im- pact in terms of selecting summary segments; how-", "labels": [], "entities": [{"text": "differ- ence", "start_pos": 276, "end_pos": 288, "type": "METRIC", "confidence": 0.9590964516003927}]}, {"text": " Table 4: ROUGE results (%) on the test set.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 10, "end_pos": 15, "type": "METRIC", "confidence": 0.9990280866622925}]}]}