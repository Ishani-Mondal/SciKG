{"title": [{"text": "Topic Models for Image Annotation and Text Illustration", "labels": [], "entities": [{"text": "Image Annotation", "start_pos": 17, "end_pos": 33, "type": "TASK", "confidence": 0.6932634860277176}, {"text": "Text Illustration", "start_pos": 38, "end_pos": 55, "type": "TASK", "confidence": 0.6991986930370331}]}], "abstractContent": [{"text": "Image annotation, the task of automatically generating description words fora picture, is a key component in various image search and retrieval applications.", "labels": [], "entities": [{"text": "Image annotation", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.7885906994342804}, {"text": "image search and retrieval", "start_pos": 117, "end_pos": 143, "type": "TASK", "confidence": 0.7451390475034714}]}, {"text": "Creating image databases for model development is, however, costly and time consuming, since the keywords must be hand-coded and the process repeated for new collections.", "labels": [], "entities": [{"text": "model development", "start_pos": 29, "end_pos": 46, "type": "TASK", "confidence": 0.747949093580246}]}, {"text": "In this work we exploit the vast resource of images and documents available on the web for developing image annotation models without any human involvement.", "labels": [], "entities": []}, {"text": "We describe a probabilistic model based on the assumption that images and their co-occurring textual data are generated by mixtures of latent topics.", "labels": [], "entities": []}, {"text": "We show that this model outperforms previously proposed approaches when applied to image annotation and the related task of text illustration despite the noisy nature of our dataset.", "labels": [], "entities": [{"text": "image annotation", "start_pos": 83, "end_pos": 99, "type": "TASK", "confidence": 0.7098266035318375}, {"text": "text illustration", "start_pos": 124, "end_pos": 141, "type": "TASK", "confidence": 0.7905102968215942}]}], "introductionContent": [{"text": "Recent years have witnessed the rapid growth of image collections available for searching and browsing over the Internet.", "labels": [], "entities": []}, {"text": "Although image search engines are still in their infancy, initial research suggests that the deployed algorithms are not very accurate).", "labels": [], "entities": [{"text": "image search engines", "start_pos": 9, "end_pos": 29, "type": "TASK", "confidence": 0.8476780454317728}]}, {"text": "Given a query, search engines retrieve relevant pictures by analyzing the image caption (if it exists), textual descriptions found adjacent to the image, and other text-related factors such as the filename of the image.", "labels": [], "entities": []}, {"text": "However, since they do not analyze the actual content of the images, search engines cannot be used to retrieve pictures from unannotated collections.", "labels": [], "entities": []}, {"text": "The ability to perform the annotation task automatically would be of significant practical import for many image-based applications.", "labels": [], "entities": []}, {"text": "Besides search and retrieval, other examples include browsing support (e.g., by clustering images into groups that are visually and semantically coherent) and story picturing (i.e., automatically suggesting images to illustrate text).", "labels": [], "entities": []}, {"text": "Automatic image annotation is a popular task in computer vision; a large number of approaches have been proposed in the literature under many distinct learning paradigms.", "labels": [], "entities": [{"text": "Automatic image annotation", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.6983277201652527}]}, {"text": "These range from supervised classification () to instantiations of the noisy-channel model (), to clustering (), and methods inspired by information retrieval).", "labels": [], "entities": [{"text": "supervised classification", "start_pos": 17, "end_pos": 42, "type": "TASK", "confidence": 0.7279157340526581}]}, {"text": "Despite differences in application and formulation, all these methods essentially attempt to learn the correlation between image features and words from examples of annotated images.", "labels": [], "entities": []}, {"text": "The Corel database has been extensively used as a testbed for the development and evaluation of image annotation models.", "labels": [], "entities": [{"text": "Corel database", "start_pos": 4, "end_pos": 18, "type": "DATASET", "confidence": 0.8942263722419739}]}, {"text": "It is a collection of stock photographs, divided into themes (e.g., tigers, sunsets) each of which are associated with keywords (e.g., sun, sea) that are considered appropriate descriptors for all images belonging to the same theme.", "labels": [], "entities": []}, {"text": "Unfortunately, the Corel database is not representative of the size or content of real-world image collections.", "labels": [], "entities": [{"text": "Corel database", "start_pos": 19, "end_pos": 33, "type": "DATASET", "confidence": 0.9182372093200684}]}, {"text": "It has a small number of themes with many closely related images which in turn share keyword descriptions.", "labels": [], "entities": []}, {"text": "It is therefore relatively easy to learn the associations between images and keywords and do well on annotation and retrieval tasks).", "labels": [], "entities": []}, {"text": "An appealing alternative is the use of resources where images and their annotations co-occur naturally.", "labels": [], "entities": []}, {"text": "Examples include images found in news documents, consumer photo collections, Wikipedia articles, illustrated stories and soon.", "labels": [], "entities": []}, {"text": "The key idea here is to treat the words in the surrounding text as annotations for the image.", "labels": [], "entities": []}, {"text": "These annotations are undoubt-edly noisy, but plenty and cost-free.", "labels": [], "entities": []}, {"text": "Moreover, the collateral text is often longer and more informative in comparison to the few keywords reserved for each image in Corel.", "labels": [], "entities": [{"text": "Corel", "start_pos": 128, "end_pos": 133, "type": "DATASET", "confidence": 0.9435045123100281}]}, {"text": "In this paper we propose a probabilistic image annotation model that learns to automatically label images under such noisy conditions.", "labels": [], "entities": []}, {"text": "We use the database created in which consists of news articles, images, and their captions.", "labels": [], "entities": []}, {"text": "Our model exploits the redundancy inherent in this multimodal dataset by assuming that images and their surrounding text are generated by a shared set of latent variables or topics.", "labels": [], "entities": []}, {"text": "Specifically, we describe documents and images by a common multimodal vocabulary consisting of textual words and visual terms (visiterms).", "labels": [], "entities": []}, {"text": "Due to polysemy and synonymy many words in this vocabulary will refer to the same underlying concept.", "labels": [], "entities": []}, {"text": "Using Latent Dirichlet Allocation (LDA, Blei and Jordan 2003), a probabilistic model of text generation, we represent visual and textual meaning jointly as a probability distribution over a set of topics.", "labels": [], "entities": [{"text": "text generation", "start_pos": 88, "end_pos": 103, "type": "TASK", "confidence": 0.7158374935388565}]}, {"text": "Our annotation model takes these topic distributions into account while finding the most likely keywords for an image and its associated document.", "labels": [], "entities": []}, {"text": "We also show how the model can be straightforwardly modified to perform automatic text illustration.", "labels": [], "entities": [{"text": "text illustration", "start_pos": 82, "end_pos": 99, "type": "TASK", "confidence": 0.7038298845291138}]}, {"text": "The task is routinely performed by news writers who often have to search large image collections in order to find suitable pictures for their text.", "labels": [], "entities": []}, {"text": "Here, the model takes a document as input and suggests images that match its content.", "labels": [], "entities": []}, {"text": "Experimental results on both tasks bring improvements over competitive models.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section we discuss our experimental design for assessing the performance of the models presented above.", "labels": [], "entities": []}, {"text": "We give details on our training procedure and parameter estimation, describe our features, and present the baseline methods used for comparison with our models.", "labels": [], "entities": [{"text": "parameter estimation", "start_pos": 46, "end_pos": 66, "type": "TASK", "confidence": 0.6798931360244751}]}, {"text": "Data We evaluated the image annotation and text illustration tasks on the dataset described in Section 3.", "labels": [], "entities": [{"text": "text illustration", "start_pos": 43, "end_pos": 60, "type": "TASK", "confidence": 0.7779001295566559}]}, {"text": "Documents and captions were part-of-speech tagged and lemmatized with Tree Tagger.", "labels": [], "entities": []}, {"text": "We excluded from the vocabulary low frequency words (appearing fewer than five times) and words other than nouns, verbs, and adjectives.", "labels": [], "entities": []}, {"text": "For the image annotation task we follow the data split used in.", "labels": [], "entities": []}, {"text": "The training set contains 2,881 image-caption-document tuples; 240 tuples are reserved for development and 240 for testing.", "labels": [], "entities": []}, {"text": "Our text illustration experiments, used 2,881 image-caption-document tuples for training.", "labels": [], "entities": [{"text": "text illustration", "start_pos": 4, "end_pos": 21, "type": "TASK", "confidence": 0.828705906867981}]}, {"text": "For the purposes of simulating areal story picturing engine environment, we created a large image pool of 450 image-caption pairs and tested on 300 of them.", "labels": [], "entities": []}, {"text": "Model Parameters For each image we extracted 150 (on average) SIFT features.", "labels": [], "entities": []}, {"text": "These were quantized into a discrete set of visual terms using K-means.", "labels": [], "entities": []}, {"text": "We varied K from 100 to 2,000.", "labels": [], "entities": [{"text": "K", "start_pos": 10, "end_pos": 11, "type": "METRIC", "confidence": 0.9898616075515747}]}, {"text": "We trained the LDA topic model on the multimodal document collection {d Mix } and varied the number of topics from 15 to 1,000.", "labels": [], "entities": []}, {"text": "The hyperparameter \u03b1 was initialized to 0.1; the \u03b2 probabilities were initialized randomly.", "labels": [], "entities": []}, {"text": "The maximum number of iterations for variational inference was set to 1,000.", "labels": [], "entities": [{"text": "variational inference", "start_pos": 37, "end_pos": 58, "type": "TASK", "confidence": 0.9226970970630646}]}, {"text": "We tuned the smoothing parameters q 1 , q 2 , and q 3 (see Equation) on the development set.", "labels": [], "entities": [{"text": "Equation", "start_pos": 59, "end_pos": 67, "type": "METRIC", "confidence": 0.9232214689254761}]}, {"text": "The best values were q 1 = 0.84, q 2 = 0.12, and q 3 = 0.04 (for both tasks).", "labels": [], "entities": []}, {"text": "As the number of visual terms and topics are interrelated we exhaustively examined all possible combinations on the development set.", "labels": [], "entities": []}, {"text": "We obtained best results on image annotation with 1,000 topics and 750 visual terms.", "labels": [], "entities": [{"text": "image annotation", "start_pos": 28, "end_pos": 44, "type": "TASK", "confidence": 0.7560377418994904}]}, {"text": "On text illustration the best parameters were 1,000 topics and 2,000 visual terms.", "labels": [], "entities": [{"text": "text illustration", "start_pos": 3, "end_pos": 20, "type": "TASK", "confidence": 0.7762517929077148}]}, {"text": "Baselines For the image annotation experiments, we compared our model against the following baselines.", "labels": [], "entities": []}, {"text": "Firstly, we trained a vanilla LDA model on the document collection without taking the images into account.", "labels": [], "entities": []}, {"text": "This model estimates P(w t |D) = \u2211 K k=1 P(w t |z k )P(z k |D), the probability of textual word wt given text document D.", "labels": [], "entities": []}, {"text": "We assume that the most probable words are the captions for the accompanying image.", "labels": [], "entities": []}, {"text": "Our second baseline is the extended relevance model) that also takes the document into account but crucially assumes that the process of generating the images is independent from the process of generating its keywords.", "labels": [], "entities": []}, {"text": "We also compared our approach with two closely related latent variable models (developed for image-caption pairs), a PLSA-based model) and CorrLDA ( ).", "labels": [], "entities": []}, {"text": "The former model is an asymmetric version of PLSA; it estimates the topic structure solely from the textual modality and keeps it fixed for the visual modality.", "labels": [], "entities": []}, {"text": "The technique is similar to folding-in, the standard PLSA procedure for inference in unseen documents and allows modeling an image as a mixture of latent topics that is defined only by one modality (in this case the caption words).", "labels": [], "entities": []}, {"text": "CorrLDA first generates image regions from a Gaussian multinomial distribution parametrized with Dirichlet priors.", "labels": [], "entities": []}, {"text": "Then, for each annotation word, it uniformly selects a region from the image and generates a word according to the topic used to generate that region.", "labels": [], "entities": []}, {"text": "We optimized the parameters for both models on the development set.", "labels": [], "entities": []}, {"text": "For CorrLDA, we followed the mean-field variational inference strategy proposed in.", "labels": [], "entities": [{"text": "CorrLDA", "start_pos": 4, "end_pos": 11, "type": "DATASET", "confidence": 0.7599419951438904}]}, {"text": "The optimal number of topics for PLSA, was 200 (with 2000 visual terms) and for CorrLDA 50.", "labels": [], "entities": [{"text": "PLSA", "start_pos": 33, "end_pos": 37, "type": "TASK", "confidence": 0.7861595153808594}, {"text": "CorrLDA", "start_pos": 80, "end_pos": 87, "type": "DATASET", "confidence": 0.5862573385238647}]}, {"text": "For the text illustration experiments, the proposed model was compared with three baselines.", "labels": [], "entities": [{"text": "text illustration", "start_pos": 8, "end_pos": 25, "type": "TASK", "confidence": 0.8786504566669464}]}, {"text": "The first one is essentially word overlap.", "labels": [], "entities": [{"text": "word overlap", "start_pos": 29, "end_pos": 41, "type": "TASK", "confidence": 0.6972636133432388}]}, {"text": "We select the image whose caption has the largest number of words in common with the test document.", "labels": [], "entities": []}, {"text": "The second one is a straightforward implementation of the vector space model) where documents and captions are represented by vectors whose components correspond to term-document co-occurrences.", "labels": [], "entities": []}, {"text": "We followed common practice in weighting terms by their tf-idf values, and used the cosine similarity measure to find the image whose caption is most similar to the test document.", "labels": [], "entities": [{"text": "cosine similarity measure", "start_pos": 84, "end_pos": 109, "type": "METRIC", "confidence": 0.7508239348729452}]}, {"text": "Our third baseline uses a text-based LDA model to estimate document-caption similarity probabilistically, through topic sharing.", "labels": [], "entities": []}, {"text": "The images most relevant to a document are found by maximizing the conditional probability of the candidate captions C given the document d D : where w care the caption words, P(w c |z k ) the conditional distribution of each w c given a topic z k , and P(z k |d D ) the conditional distribution of z k given d D , the document we wish to illustrate.", "labels": [], "entities": []}, {"text": "Evaluation In the image annotation task we follow the evaluation methodology proposed in.", "labels": [], "entities": []}, {"text": "We are given an un-annotated image I and asked to automatically produce the n-best keywords.", "labels": [], "entities": []}, {"text": "For all models discussed here, we report results with the top 10 annotation words using precision, recall and F1.", "labels": [], "entities": [{"text": "precision", "start_pos": 88, "end_pos": 97, "type": "METRIC", "confidence": 0.9997484087944031}, {"text": "recall", "start_pos": 99, "end_pos": 105, "type": "METRIC", "confidence": 0.9995458722114563}, {"text": "F1", "start_pos": 110, "end_pos": 112, "type": "METRIC", "confidence": 0.9993953704833984}]}, {"text": "In the text illustration task, we are given a test document d and a pool of candidate images I 1...N with captions C 1...N . The model is expected to find an image from the candidate pool that matches the test document.", "labels": [], "entities": [{"text": "text illustration", "start_pos": 7, "end_pos": 24, "type": "TASK", "confidence": 0.7850888073444366}]}, {"text": "We use equation to output a ranked list of MI visual terms.", "labels": [], "entities": []}, {"text": "The image having the highest overlap with the top 30 visual terms is selected as the illustration for the test document.", "labels": [], "entities": [{"text": "overlap", "start_pos": 29, "end_pos": 36, "type": "METRIC", "confidence": 0.9718269109725952}]}, {"text": "All illustration models were evaluated using top 1 accuracy, which is the percentage of successfully matched image-document pairs in the test set.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 51, "end_pos": 59, "type": "METRIC", "confidence": 0.9028311967849731}]}], "tableCaptions": [{"text": " Table 2: Automatic image annotation results.", "labels": [], "entities": []}, {"text": " Table 3: Text Illustration results.", "labels": [], "entities": [{"text": "Text Illustration", "start_pos": 10, "end_pos": 27, "type": "TASK", "confidence": 0.5759714245796204}]}]}