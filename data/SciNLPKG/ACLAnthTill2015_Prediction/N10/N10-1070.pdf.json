{"title": [{"text": "Term Weighting Schemes for Latent Dirichlet Allocation", "labels": [], "entities": [{"text": "Term Weighting", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.8618773818016052}]}], "abstractContent": [{"text": "Many implementations of Latent Dirichlet Allocation (LDA), including those described in Blei et al.", "labels": [], "entities": [{"text": "Latent Dirichlet Allocation (LDA)", "start_pos": 24, "end_pos": 57, "type": "TASK", "confidence": 0.6363957424958547}]}, {"text": "(2003), rely at some point on the removal of stopwords, words which are assumed to contribute little to the meaning of the text.", "labels": [], "entities": []}, {"text": "This step is considered necessary because otherwise high-frequency words tend to end up scattered across many of the latent topics without much rhyme or reason.", "labels": [], "entities": []}, {"text": "We show, however, that the 'problem' of high-frequency words can be dealt with more elegantly, and in away that to our knowledge has not been considered in LDA, through the use of appropriate weighting schemes comparable to those sometimes used in Latent Semantic Indexing (LSI).", "labels": [], "entities": [{"text": "Latent Semantic Indexing (LSI)", "start_pos": 248, "end_pos": 278, "type": "TASK", "confidence": 0.686019092798233}]}, {"text": "Our proposed weighting methods not only make theoretical sense, but can also be shown to improve precision significantly on a non-trivial cross-language retrieval task.", "labels": [], "entities": [{"text": "precision", "start_pos": 97, "end_pos": 106, "type": "METRIC", "confidence": 0.9986914992332458}]}], "introductionContent": [{"text": "Latent Dirichlet Allocation (LDA) (, like its more established competitors Latent Semantic Indexing (LSI)) and Probabilistic Latent Semantic Indexing (PLSI)), is a model which is applicable to the analysis of text corpora.", "labels": [], "entities": [{"text": "Latent Dirichlet Allocation (LDA)", "start_pos": 0, "end_pos": 33, "type": "TASK", "confidence": 0.5755785753329595}]}, {"text": "It is claimed to differ from LSI in that LDA is a generative Bayesian model (, although this may depend upon the manner in which one approaches LSI (see for example).", "labels": [], "entities": []}, {"text": "In LDA as applied to text analysis, each document in the corpus is modeled as a mixture over an underlying set of topics, and each topic is modeled as a probability distribution over the terms in the vocabulary.", "labels": [], "entities": [{"text": "text analysis", "start_pos": 21, "end_pos": 34, "type": "TASK", "confidence": 0.7049018889665604}]}, {"text": "As the newest among the above-mentioned techniques, LDA is still in a relatively early stage of development.", "labels": [], "entities": [{"text": "LDA", "start_pos": 52, "end_pos": 55, "type": "TASK", "confidence": 0.9229442477226257}]}, {"text": "It is also sufficiently different from LSI, probably the most popular and well-known compression technique for information retrieval (IR), that many practitioners of LSI may perceive a 'barrier to entry' to LDA.", "labels": [], "entities": [{"text": "information retrieval (IR)", "start_pos": 111, "end_pos": 137, "type": "TASK", "confidence": 0.8688427329063415}]}, {"text": "This in turn perhaps explains why notions such as term weighting, which have been commonplace in LSI for sometime, have not yet found a place in LDA.", "labels": [], "entities": [{"text": "term weighting", "start_pos": 50, "end_pos": 64, "type": "TASK", "confidence": 0.6666707396507263}, {"text": "LDA", "start_pos": 145, "end_pos": 148, "type": "DATASET", "confidence": 0.8830543160438538}]}, {"text": "In fact, it is often assumed that weighting is unnecessary in LDA.", "labels": [], "entities": [{"text": "LDA", "start_pos": 62, "end_pos": 65, "type": "TASK", "confidence": 0.800510585308075}]}, {"text": "For example, contrast the use of tfidf weighting in both non-reduced space and LSI on the one hand with PLSI and LDA on the other, where no mention is made of weighting.", "labels": [], "entities": []}, {"text": "propose a simple term-frequency weighting scheme for tagged documents within the framework of LDA, although term weighting is not their focus and their scheme is intended to incorporate document tags into the same model that represents the documents themselves.", "labels": [], "entities": [{"text": "term weighting", "start_pos": 108, "end_pos": 122, "type": "TASK", "confidence": 0.6795483827590942}]}, {"text": "In this paper, we produce evidence that term weighting should be given consideration within LDA.", "labels": [], "entities": [{"text": "term weighting", "start_pos": 40, "end_pos": 54, "type": "TASK", "confidence": 0.6057599484920502}]}, {"text": "First and foremost, this is shown empirically through a non-trivial multilingual retrieval task which has previously been used as the basis for tests of variants of LSI.", "labels": [], "entities": []}, {"text": "We also show that term weighting allows one to avoid maintenance of stoplists, which can be awkward especially for multilingual data.", "labels": [], "entities": []}, {"text": "With appropriate term weighting, highfrequency words (which might otherwise be eliminated as stopwords) are assigned naturally to topics by LDA, rather than dominating and being scattered across many topics as happens with the standard uniform weighting.", "labels": [], "entities": []}, {"text": "Our approach belies the usually unstated, but widespread, assumption in papers on LDA that the removal of stopwords is a necessary pre-processing step (see e.g.;).", "labels": [], "entities": []}, {"text": "It might seem that to demonstrate this it would be necessary to perform a test that directly compares the results when stoplists are used to those when weighting are used.", "labels": [], "entities": []}, {"text": "However, we believe that stopwords are highly ad-hoc to begin with.", "labels": [], "entities": []}, {"text": "Assuming a vocabulary of n words and a stoplist of x items, there are (at least in theory) possible stoplists.", "labels": [], "entities": []}, {"text": "To be sure that no stoplist improves on a particular term weighting scheme we would have to test everyone of these.", "labels": [], "entities": []}, {"text": "In addition, our tests are with a multilingual dataset, which raises the issue that a domain-appropriate stoplist fora particular corpus and language may not be available.", "labels": [], "entities": []}, {"text": "This is even more true if we pre-process the dataset morphologically (for example, with stemming).", "labels": [], "entities": []}, {"text": "Therefore, rather than attempting a direct comparison of this type, we take the position that it is possible to sidestep the need for stoplists and to do so in a non-ad-hoc way.", "labels": [], "entities": []}, {"text": "The paper is organized as follows.", "labels": [], "entities": []}, {"text": "Section 2 describes the general framework of LDA, which has only very recently been applied to cross-language IR.", "labels": [], "entities": [{"text": "IR", "start_pos": 110, "end_pos": 112, "type": "TASK", "confidence": 0.6551370024681091}]}, {"text": "In Section 3, we look at alternatives to the 'standard' uniform weighting scheme (i.e., lack of weighting scheme) commonly used in LDA.", "labels": [], "entities": []}, {"text": "Section 4 discusses the framework we use for empirical testing of our hypothesis that a weighting scheme would be beneficial.", "labels": [], "entities": []}, {"text": "We present the results of this comparison in Section 5 along with an impressionistic comparison of the output of the different alternatives.", "labels": [], "entities": []}, {"text": "We conclude in Section 6.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Summary of comparison results. This table  shows the average precision at one document (P1) for  each of the tokenization and weighting schemes we eval- uated. Detailed results are presented in Table 2.", "labels": [], "entities": [{"text": "precision", "start_pos": 71, "end_pos": 80, "type": "METRIC", "confidence": 0.9872788190841675}]}, {"text": " Table 2: Full results for precision at one document for all combinations of LDA, Log-WLDA, PMI-WLDA, word  tokenization and morphological tokenization.", "labels": [], "entities": [{"text": "precision", "start_pos": 27, "end_pos": 36, "type": "METRIC", "confidence": 0.9966714382171631}]}]}