{"title": [{"text": "Unsupervised Model Adaptation using Information-Theoretic Criterion", "labels": [], "entities": [{"text": "Unsupervised Model Adaptation", "start_pos": 0, "end_pos": 29, "type": "TASK", "confidence": 0.6077840129534403}]}], "abstractContent": [{"text": "In this paper we propose a novel general framework for unsupervised model adaptation.", "labels": [], "entities": [{"text": "unsupervised model adaptation", "start_pos": 55, "end_pos": 84, "type": "TASK", "confidence": 0.7225004235903422}]}, {"text": "Our method is based on entropy which has been used previously as a regularizer in semi-supervised learning.", "labels": [], "entities": []}, {"text": "This technique includes another term which measures the stability of posteriors w.r.t model parameters, in addition to conditional entropy.", "labels": [], "entities": []}, {"text": "The idea is to use parameters which result in both low conditional entropy and also stable decision rules.", "labels": [], "entities": []}, {"text": "As an application, we demonstrate how this framework can be used for adjusting language model interpolation weight for speech recognition task to adapt from Broadcast news data to MIT lecture data.", "labels": [], "entities": [{"text": "speech recognition task", "start_pos": 119, "end_pos": 142, "type": "TASK", "confidence": 0.809677521387736}, {"text": "Broadcast news data", "start_pos": 157, "end_pos": 176, "type": "DATASET", "confidence": 0.8808904886245728}, {"text": "MIT lecture data", "start_pos": 180, "end_pos": 196, "type": "DATASET", "confidence": 0.6580941677093506}]}, {"text": "We show how the new technique can obtain comparable performance to completely supervised estimation of interpolation parameters.", "labels": [], "entities": []}], "introductionContent": [{"text": "All statistical and machine learning techniques for classification, in principle, work under the assumption that 1.", "labels": [], "entities": [{"text": "classification", "start_pos": 52, "end_pos": 66, "type": "TASK", "confidence": 0.9628185033798218}]}, {"text": "A reasonable amount of training data is available.", "labels": [], "entities": []}, {"text": "2. Training data and test data are drawn from the same underlying distribution.", "labels": [], "entities": []}, {"text": "In fact, the success of statistical models is crucially dependent on training data.", "labels": [], "entities": []}, {"text": "Unfortunately, the latter assumption is not fulfilled in many applications.", "labels": [], "entities": []}, {"text": "Therefore, model adaptation is necessary when training data is not matched (not drawn from same distribution) with test data.", "labels": [], "entities": [{"text": "model adaptation", "start_pos": 11, "end_pos": 27, "type": "TASK", "confidence": 0.7296956479549408}]}, {"text": "It is often the case where we have plenty of labeled data for one specific domain/genre (source domain) and little amount of labeled data (or no labeled data at all) for the desired domain/genre (target domain).", "labels": [], "entities": []}, {"text": "Model adaptation techniques are commonly used to address this problem.", "labels": [], "entities": [{"text": "Model adaptation", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.7391299605369568}]}, {"text": "Model adaptation starts with trained models (trained on source domain with rich amount of labeled data) and then modify them using the available labeled data from target domain (or instead unlabeled data).", "labels": [], "entities": [{"text": "Model adaptation", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.7393707633018494}]}, {"text": "A survey on different methods of model adaptation can be found in.", "labels": [], "entities": [{"text": "model adaptation", "start_pos": 33, "end_pos": 49, "type": "TASK", "confidence": 0.7348430752754211}]}, {"text": "Information regularization framework has been previously proposed in literature to control the label conditional probabilities via input distribution).", "labels": [], "entities": [{"text": "Information regularization", "start_pos": 0, "end_pos": 26, "type": "TASK", "confidence": 0.7705004811286926}]}, {"text": "The idea is that labels should not change too much in dense regions of the input distribution.", "labels": [], "entities": []}, {"text": "The authors use the mutual information between input features and labels as a measure of label complexity.", "labels": [], "entities": []}, {"text": "Another framework previously suggested is to use label entropy (conditional entropy) on unlabeled data as a regularizer to Maximum Likelihood (ML) training on labeled data ().", "labels": [], "entities": []}, {"text": "Availability of resources for the target domain categorizes these techniques into either supervised or unsupervised.", "labels": [], "entities": []}, {"text": "In this paper we propose a general framework for unsupervised adaptation using Shannon entropy and stability of entropy.", "labels": [], "entities": []}, {"text": "The assumption is that in-domain and out-of-domain distributions are not too different such that one can improve the performance of initial models on in-domain data by little adjustment of initial decision boundaries (learned on out-of-domain data).", "labels": [], "entities": []}], "datasetContent": [{"text": "The large vocabulary continuous speech recognition (LVCSR) system used throughout this paper is based on the 2007 IBM Speech transcription system for GALE Distillation Go/No-go Evaluation).", "labels": [], "entities": [{"text": "large vocabulary continuous speech recognition (LVCSR)", "start_pos": 4, "end_pos": 58, "type": "TASK", "confidence": 0.6927934288978577}, {"text": "GALE Distillation Go/No-go Evaluation", "start_pos": 150, "end_pos": 187, "type": "TASK", "confidence": 0.6024913837512335}]}, {"text": "The acoustic models used in this system are state-of-the-art discriminatively trained models and are the same ones used for all experiments presented in this paper.", "labels": [], "entities": []}, {"text": "For LM adaptation experiments, the out-ofdomain LM (p B , Broadcast News LM) training text consists of 335M words from the following broadcast news (BN) data sources): 1996 CSR Hub4 Language Model data, EARS BN03 closed captions, GALE Phase 2 Distillation GNG Evaluation Supplemental Multilingual data, Hub4 acoustic model training transcripts, TDT4 closed captions, TDT4 newswire, and GALE Broadcast Conversations and GALE Broadcast News.", "labels": [], "entities": [{"text": "LM adaptation", "start_pos": 4, "end_pos": 17, "type": "TASK", "confidence": 0.9368694424629211}, {"text": "CSR Hub4 Language Model data", "start_pos": 173, "end_pos": 201, "type": "DATASET", "confidence": 0.8161935091018677}, {"text": "GALE Phase 2 Distillation GNG Evaluation Supplemental Multilingual data", "start_pos": 230, "end_pos": 301, "type": "DATASET", "confidence": 0.6326733860704634}, {"text": "Hub4 acoustic model training transcripts", "start_pos": 303, "end_pos": 343, "type": "DATASET", "confidence": 0.7498634934425354}, {"text": "TDT4 newswire", "start_pos": 367, "end_pos": 380, "type": "DATASET", "confidence": 0.9607402682304382}, {"text": "GALE Broadcast News", "start_pos": 419, "end_pos": 438, "type": "DATASET", "confidence": 0.9145472248395284}]}, {"text": "This language model is of order 4-gram with Kneser-Ney smoothing and contains 4.6M ngrams based on a lexicon size of 84K.", "labels": [], "entities": []}, {"text": "The second source of data is the MIT lectures data set (J. Glass, T. Hazen, S. Cyphers, I. Malioutov, D. Huynh, and R. .", "labels": [], "entities": [{"text": "MIT lectures data set", "start_pos": 33, "end_pos": 54, "type": "DATASET", "confidence": 0.9603594094514847}]}, {"text": "This serves as the target domain (in-domain) set for language model adaptation experiments.", "labels": [], "entities": [{"text": "language model adaptation", "start_pos": 53, "end_pos": 78, "type": "TASK", "confidence": 0.6998287041982015}]}, {"text": "This set is split into 8 hours for in-domain LM building, another 8 hours served as unlabeled data for interpolation weight estimation using criterion in Eqn.", "labels": [], "entities": [{"text": "LM building", "start_pos": 45, "end_pos": 56, "type": "TASK", "confidence": 0.9142294526100159}, {"text": "interpolation weight estimation", "start_pos": 103, "end_pos": 134, "type": "TASK", "confidence": 0.7028553783893585}]}, {"text": "8 (we refer to this as unsupervised training data) and finally 2.5 hours Dev set for estimating the interpolation weight w.r.t W ER (supervised tuning) . The lattice entropy and gradient of entropy w.r.t \u03bb are calculated on the unsupervised training data set.", "labels": [], "entities": [{"text": "interpolation weight w.r.t W ER", "start_pos": 100, "end_pos": 131, "type": "METRIC", "confidence": 0.6717909812927246}, {"text": "unsupervised training data set", "start_pos": 228, "end_pos": 258, "type": "DATASET", "confidence": 0.7889586687088013}]}, {"text": "The results are discussed in the next section.", "labels": [], "entities": []}], "tableCaptions": []}