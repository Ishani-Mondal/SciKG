{"title": [{"text": "Chart Mining-based Lexical Acquisition with Precision Grammars", "labels": [], "entities": [{"text": "Chart Mining-based Lexical Acquisition", "start_pos": 0, "end_pos": 38, "type": "TASK", "confidence": 0.6311868652701378}]}], "abstractContent": [{"text": "In this paper, we present an innovative chart mining technique for improving parse coverage based on partial parse outputs from precision grammars.", "labels": [], "entities": [{"text": "chart mining", "start_pos": 40, "end_pos": 52, "type": "TASK", "confidence": 0.7649296522140503}, {"text": "parse coverage", "start_pos": 77, "end_pos": 91, "type": "TASK", "confidence": 0.9570196866989136}]}, {"text": "The general approach of mining features from partial analyses is applicable to a range of lexical acquisition tasks, and is particularly suited to domain-specific lexical tuning and lexical acquisition using low-coverage grammars.", "labels": [], "entities": [{"text": "domain-specific lexical tuning", "start_pos": 147, "end_pos": 177, "type": "TASK", "confidence": 0.646959662437439}, {"text": "lexical acquisition", "start_pos": 182, "end_pos": 201, "type": "TASK", "confidence": 0.7224751263856888}]}, {"text": "As an illustration of the functionality of our proposed technique, we develop a lexical acquisition model for En-glish verb particle constructions which operates over unlexicalised features mined from a partial parsing chart.", "labels": [], "entities": []}, {"text": "The proposed technique is shown to outperform a state-of-the-art parser over the target task, despite being based on relatively simplistic features.", "labels": [], "entities": []}], "introductionContent": [{"text": "Parsing with precision grammars is increasingly achieving broad coverage over open-domain texts fora range of constraint-based frameworks (e.g., TAG, LFG, HPSG and CCG), and is being used in real-world applications including information extraction, question answering, grammar checking and machine translation;.", "labels": [], "entities": [{"text": "Parsing with precision grammars", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.760393813252449}, {"text": "information extraction", "start_pos": 225, "end_pos": 247, "type": "TASK", "confidence": 0.8488095104694366}, {"text": "question answering", "start_pos": 249, "end_pos": 267, "type": "TASK", "confidence": 0.9140111207962036}, {"text": "grammar checking", "start_pos": 269, "end_pos": 285, "type": "TASK", "confidence": 0.7855452001094818}, {"text": "machine translation", "start_pos": 290, "end_pos": 309, "type": "TASK", "confidence": 0.7945733666419983}]}, {"text": "In this context, a \"precision grammar\" is a grammar which has been engineered to model grammaticality, and contrasts with a treebank-induced grammar, for example.", "labels": [], "entities": [{"text": "precision", "start_pos": 20, "end_pos": 29, "type": "METRIC", "confidence": 0.9679539203643799}]}, {"text": "Inevitably, however, such applications demand complete parsing outputs, based on the assumption that the text under investigation will be completely analysable by the grammar.", "labels": [], "entities": []}, {"text": "As precision grammars generally make strong assumptions about complete lexical coverage and grammaticality of the input, their utility is limited over noisy or domain-specific data.", "labels": [], "entities": []}, {"text": "This lack of complete coverage can make parsing with precision grammars less attractive than parsing with shallower methods.", "labels": [], "entities": [{"text": "parsing", "start_pos": 40, "end_pos": 47, "type": "TASK", "confidence": 0.9745796918869019}]}, {"text": "One technique that has been successfully applied to improve parser and grammar coverage over a given corpus is error mining), whereby n-grams with low \"parsability\" are gathered from the large-scale output of a parser as an indication of parser or (precision) grammar errors.", "labels": [], "entities": [{"text": "error mining", "start_pos": 111, "end_pos": 123, "type": "TASK", "confidence": 0.729526624083519}, {"text": "precision", "start_pos": 249, "end_pos": 258, "type": "METRIC", "confidence": 0.9826494455337524}]}, {"text": "However, error mining is very much oriented towards grammar engineering: its results area mixture of different (mistreated) linguistic phenomena together with engineering errors for the grammar engineer to work through and act upon.", "labels": [], "entities": [{"text": "error mining", "start_pos": 9, "end_pos": 21, "type": "TASK", "confidence": 0.7909722030162811}]}, {"text": "Additionally, it generally does not provide any insight into the cause of the parser failure, and it is difficult to identify specific language phenomena from the output.", "labels": [], "entities": []}, {"text": "In this paper, we instead propose a chart mining technique that works on intermediate parsing results from a parsing chart.", "labels": [], "entities": [{"text": "chart mining", "start_pos": 36, "end_pos": 48, "type": "TASK", "confidence": 0.7370492666959763}]}, {"text": "In essence, the method analyses the validity of different analyses for words or constructions based on the \"lifetime\" and probability of each within the chart, combining the constraints of the grammar with probabilities to evaluate the plausibility of each.", "labels": [], "entities": []}, {"text": "For purposes of exemplification of the proposed technique, we apply chart mining to a deep lexical acquisition (DLA) task, using a maximum entropybased prediction model trained over a seed lexicon and treebank.", "labels": [], "entities": [{"text": "chart mining", "start_pos": 68, "end_pos": 80, "type": "TASK", "confidence": 0.780030757188797}, {"text": "deep lexical acquisition (DLA)", "start_pos": 86, "end_pos": 116, "type": "TASK", "confidence": 0.7421556711196899}]}, {"text": "The experimental setup is the following: given a set of sentences containing putative instances of English verb particle constructions, extract a list of non-compositional VPCs optionally with valence information.", "labels": [], "entities": []}, {"text": "For comparison, we parse the same sentence set using a state-of-the-art statistical parser, and extract the VPCs from the parser output.", "labels": [], "entities": []}, {"text": "Our results show that our chart mining method produces a model which is superior to the treebank parser.", "labels": [], "entities": [{"text": "chart mining", "start_pos": 26, "end_pos": 38, "type": "TASK", "confidence": 0.733196884393692}]}, {"text": "To our knowledge, the only other work that has looked at partial parsing results of precision grammars as a means of linguistic error analysis is that of and, where partial parsing models were proposed to select a set of passive edges that together cover the input sequence.", "labels": [], "entities": [{"text": "linguistic error analysis", "start_pos": 117, "end_pos": 142, "type": "TASK", "confidence": 0.607939620812734}]}, {"text": "Compared to these approaches, our proposed chart mining technique is more general and can be adapted to specific tasks and domains.", "labels": [], "entities": [{"text": "chart mining", "start_pos": 43, "end_pos": 55, "type": "TASK", "confidence": 0.8544229865074158}]}, {"text": "While we experiment exclusively with an HPSG grammar in this paper, it is important to note that the proposed method can be applied to any grammar formalism which is compatible with chart parsing, and where it is possible to describe an unlexicalised lexical entry for the different categories of lexical item that are to be extracted (see Section 3.2 for details).", "labels": [], "entities": [{"text": "chart parsing", "start_pos": 182, "end_pos": 195, "type": "TASK", "confidence": 0.77964648604393}]}, {"text": "The remainder of the paper is organised as follows.", "labels": [], "entities": []}, {"text": "Section 2 defines the task of VPC extraction.", "labels": [], "entities": [{"text": "VPC extraction", "start_pos": 30, "end_pos": 44, "type": "TASK", "confidence": 0.9853455722332001}]}, {"text": "Section 3 presents the chart mining technique and the feature extraction process for the VPC extraction task.", "labels": [], "entities": [{"text": "chart mining", "start_pos": 23, "end_pos": 35, "type": "TASK", "confidence": 0.7903113067150116}, {"text": "feature extraction", "start_pos": 54, "end_pos": 72, "type": "TASK", "confidence": 0.7094543129205704}, {"text": "VPC extraction task", "start_pos": 89, "end_pos": 108, "type": "TASK", "confidence": 0.9603330294291178}]}, {"text": "Section 4 evaluates the model performance with comparison to two competitor models over several different measures.", "labels": [], "entities": []}, {"text": "Section 5 further discusses the general applicability of chart mining.", "labels": [], "entities": [{"text": "chart mining", "start_pos": 57, "end_pos": 69, "type": "TASK", "confidence": 0.7895931601524353}]}, {"text": "Finally, Section 6 concludes the paper.", "labels": [], "entities": []}], "datasetContent": [{"text": "To evaluate the proposed chart mining-based VPC extraction model, we use the dataset from the LREC 2008 Multiword Expression Workshop shared task (see Section 2).", "labels": [], "entities": [{"text": "chart mining-based VPC extraction", "start_pos": 25, "end_pos": 58, "type": "TASK", "confidence": 0.7899360209703445}, {"text": "LREC 2008 Multiword Expression Workshop shared task", "start_pos": 94, "end_pos": 145, "type": "TASK", "confidence": 0.7028537052018302}]}, {"text": "We use this dataset to perform three distinct DLA tasks, as detailed in.", "labels": [], "entities": []}, {"text": "The chart mining feature extraction is implemented as an extension to the PET parser (Callmeier,  Since the parser has no access to any of the verbs under investigation (due to the DUMMY-V substitution), those VPC types attested in the LOGON Treebank do not directly impact on the model's performance.", "labels": [], "entities": [{"text": "chart mining feature extraction", "start_pos": 4, "end_pos": 35, "type": "TASK", "confidence": 0.8227545917034149}, {"text": "LOGON Treebank", "start_pos": 236, "end_pos": 250, "type": "DATASET", "confidence": 0.7893209755420685}]}, {"text": "The chart mining feature extraction process took over 10 CPU days, and collected a total of 44K events for 4,090 candidate VPC triples.", "labels": [], "entities": [{"text": "chart mining feature extraction", "start_pos": 4, "end_pos": 35, "type": "TASK", "confidence": 0.7653459012508392}]}, {"text": "5-fold cross validation is used to train/test the model.", "labels": [], "entities": []}, {"text": "As stated above (Section 2), the VPC triples attested in the WSJ training sections of the Penn Treebank are excluded in each testing fold for comparison with the Charniak parser-based model (see Section 4.2).", "labels": [], "entities": [{"text": "WSJ training sections of the Penn Treebank", "start_pos": 61, "end_pos": 103, "type": "DATASET", "confidence": 0.8743343864168439}]}], "tableCaptions": [{"text": " Table 1: Chart mining features used for VPC extraction", "labels": [], "entities": [{"text": "VPC", "start_pos": 41, "end_pos": 44, "type": "TASK", "confidence": 0.9762886762619019}]}, {"text": " Table 3: Results for the different methods over the three VPC extraction tasks detailed in", "labels": [], "entities": [{"text": "VPC extraction", "start_pos": 59, "end_pos": 73, "type": "TASK", "confidence": 0.9828220903873444}]}]}