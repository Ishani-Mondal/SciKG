{"title": [{"text": "Extracting Parallel Sentences from Comparable Corpora using Document Level Alignment", "labels": [], "entities": [{"text": "Extracting Parallel Sentences from Comparable Corpora using Document Level Alignment", "start_pos": 0, "end_pos": 84, "type": "TASK", "confidence": 0.8472988307476044}]}], "abstractContent": [{"text": "The quality of a statistical machine translation (SMT) system is heavily dependent upon the amount of parallel sentences used in training.", "labels": [], "entities": [{"text": "statistical machine translation (SMT)", "start_pos": 17, "end_pos": 54, "type": "TASK", "confidence": 0.776964490612348}]}, {"text": "In recent years, there have been several approaches developed for obtaining parallel sentences from non-parallel, or comparable data, such as news articles published within the same time period (Munteanu and Marcu, 2005), or web pages with a similar structure (Resnik and Smith, 2003).", "labels": [], "entities": []}, {"text": "One resource not yet thoroughly explored is Wikipedia, an on-line encyclopedia containing linked articles in many languages.", "labels": [], "entities": [{"text": "Wikipedia", "start_pos": 44, "end_pos": 53, "type": "DATASET", "confidence": 0.9386184811592102}]}, {"text": "We advance the state of the art in parallel sentence extraction by modeling the document level alignment, motivated by the observation that parallel sentence pairs are often found in close proximity.", "labels": [], "entities": [{"text": "parallel sentence extraction", "start_pos": 35, "end_pos": 63, "type": "TASK", "confidence": 0.6855111420154572}]}, {"text": "We also include features which make use of the additional annotation given by Wikipedia, and features using an automatically induced lexicon model.", "labels": [], "entities": []}, {"text": "Results for both accuracy in sentence extraction and downstream improvement in an SMT system are presented.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 17, "end_pos": 25, "type": "METRIC", "confidence": 0.9991255402565002}, {"text": "sentence extraction", "start_pos": 29, "end_pos": 48, "type": "TASK", "confidence": 0.7811205685138702}, {"text": "SMT", "start_pos": 82, "end_pos": 85, "type": "TASK", "confidence": 0.9920673370361328}]}], "introductionContent": [{"text": "For any statistical machine translation system, the size of the parallel corpus used for training is a major factor in its performance.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 8, "end_pos": 39, "type": "TASK", "confidence": 0.6364729106426239}]}, {"text": "For some language pairs, such as Chinese-English and Arabic-English, large amounts of parallel data are readily available, but for most language pairs this is not the case.", "labels": [], "entities": []}, {"text": "The domain of the parallel corpus also strongly influences the quality of translations produced.", "labels": [], "entities": []}, {"text": "Many parallel corpora are taken from the news domain, or from parliamentary proceedings.", "labels": [], "entities": []}, {"text": "Translation quality suffers when a system is not trained on any data from the domain it is tested on.", "labels": [], "entities": [{"text": "Translation", "start_pos": 0, "end_pos": 11, "type": "TASK", "confidence": 0.9625683426856995}]}, {"text": "While parallel corpora maybe scarce, comparable, or semi-parallel corpora are readily available in several domains and language pairs.", "labels": [], "entities": []}, {"text": "These corpora consist of a set of documents in two languages containing similar information.", "labels": [], "entities": []}, {"text": "(See Section 2.1 fora more detailed description of the types of nonparallel corpora.)", "labels": [], "entities": []}, {"text": "In most previous work on extraction of parallel sentences from comparable corpora, some coarse document-level similarity is used to determine which document pairs contain parallel sentences.", "labels": [], "entities": [{"text": "extraction of parallel sentences from comparable corpora", "start_pos": 25, "end_pos": 81, "type": "TASK", "confidence": 0.8206170882497515}]}, {"text": "For identifying similar web pages, compare the HTML structure.", "labels": [], "entities": []}, {"text": "use publication date and vector-based similarity (after projecting words through a bilingual dictionary) to identify similar news articles.", "labels": [], "entities": []}, {"text": "Once promising document pairs are identified, the next step is to extract parallel sentences.", "labels": [], "entities": []}, {"text": "Usually, some seed parallel data is assumed to be available.", "labels": [], "entities": []}, {"text": "This data is used to train a word alignment model, such as IBM Model 1 ( or HMM-based word alignment (.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 29, "end_pos": 43, "type": "TASK", "confidence": 0.7664689421653748}, {"text": "HMM-based word alignment", "start_pos": 76, "end_pos": 100, "type": "TASK", "confidence": 0.6095788280169169}]}, {"text": "Statistics from this word alignment model are used to train a classifier which identifies bilingual sentence pairs as parallel or not parallel.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 21, "end_pos": 35, "type": "TASK", "confidence": 0.712722897529602}]}, {"text": "This classifier is applied to all sentence pairs in documents which were found to be similar.", "labels": [], "entities": []}, {"text": "Typically, some pruning is done to reduce the number of sen-tence pairs that need to be classified.", "labels": [], "entities": []}, {"text": "While these methods have been applied to news corpora and web pages, very little attention has been given to Wikipedia as a source of parallel sentences.", "labels": [], "entities": []}, {"text": "This is surprising, given that Wikipedia contains annotated article alignments, and much work has been done on extracting bilingual lexicons on this dataset.", "labels": [], "entities": []}, {"text": "Adafre and de extracted similar sentences from Wikipedia article pairs, but only evaluated precision on a small number of extracted sentences.", "labels": [], "entities": [{"text": "precision", "start_pos": 91, "end_pos": 100, "type": "METRIC", "confidence": 0.9995425939559937}]}, {"text": "In this paper, we more thoroughly investigate Wikipedia's viability as a comparable corpus, and describe novel methods for parallel sentence extraction.", "labels": [], "entities": [{"text": "parallel sentence extraction", "start_pos": 123, "end_pos": 151, "type": "TASK", "confidence": 0.6053060293197632}]}, {"text": "Section 2 describes the multilingual resources available in Wikipedia.", "labels": [], "entities": []}, {"text": "Section 3 gives further background on previous methods for parallel sentence extraction on comparable corpora, and describes our approach, which finds a global sentence alignment between two documents.", "labels": [], "entities": [{"text": "parallel sentence extraction", "start_pos": 59, "end_pos": 87, "type": "TASK", "confidence": 0.6457303365071615}]}, {"text": "In Section 4, we compare our approach with previous methods on datasets derived from Wikipedia for three language pairs (Spanish-English, German-English, and Bulgarian-English), and show improvements in downstream SMT performance by adding the parallel data we extracted.", "labels": [], "entities": [{"text": "SMT", "start_pos": 214, "end_pos": 217, "type": "TASK", "confidence": 0.9610244035720825}]}], "datasetContent": [{"text": "Using 5-fold cross-validation on the 20 document pairs for each language condition, we compared the binary classifier, ranker, and CRF models for parallel sentence extraction.", "labels": [], "entities": [{"text": "parallel sentence extraction", "start_pos": 146, "end_pos": 174, "type": "TASK", "confidence": 0.605913499991099}]}, {"text": "To tune for precision/recall, we used minimum Bayes risk decoding.", "labels": [], "entities": [{"text": "precision", "start_pos": 12, "end_pos": 21, "type": "METRIC", "confidence": 0.9996259212493896}, {"text": "recall", "start_pos": 22, "end_pos": 28, "type": "METRIC", "confidence": 0.9972047209739685}]}, {"text": "We define the loss L(\u03c4, \u00b5) of picking target sentence \u03c4 when the correct target sentence is \u00b5 as 0 if \u03c4 = \u00b5, \u03bb if \u03c4 = NULL and \u00b5 = NULL, and 1 otherwise.", "labels": [], "entities": []}, {"text": "By modifying the null loss \u03bb, the precision/recall trade-off can be adjusted.", "labels": [], "entities": [{"text": "precision", "start_pos": 34, "end_pos": 43, "type": "METRIC", "confidence": 0.9991770386695862}, {"text": "recall", "start_pos": 44, "end_pos": 50, "type": "METRIC", "confidence": 0.9407462477684021}]}, {"text": "For the CRF model, we used posterior decoding to make the minimum risk decision rule tractable.", "labels": [], "entities": []}, {"text": "As a summary measure of the performance of the models at different levels of recall we use average precision as defined in (Ido: Average precision, recall at 90% precision, and recall at 80% precision for the Ranker and CRF in all three language pairs.", "labels": [], "entities": [{"text": "recall", "start_pos": 77, "end_pos": 83, "type": "METRIC", "confidence": 0.990331768989563}, {"text": "precision", "start_pos": 99, "end_pos": 108, "type": "METRIC", "confidence": 0.6572740077972412}, {"text": "Ido: Average precision", "start_pos": 124, "end_pos": 146, "type": "METRIC", "confidence": 0.5207736492156982}, {"text": "recall", "start_pos": 148, "end_pos": 154, "type": "METRIC", "confidence": 0.9945012331008911}, {"text": "precision", "start_pos": 162, "end_pos": 171, "type": "METRIC", "confidence": 0.9105176329612732}, {"text": "recall", "start_pos": 177, "end_pos": 183, "type": "METRIC", "confidence": 0.9981158971786499}, {"text": "precision", "start_pos": 191, "end_pos": 200, "type": "METRIC", "confidence": 0.8631722927093506}]}, {"text": "\"+Wiki\" indicates that Wikipedia features were used, and \"+Lex\" means the lexicon features were used.", "labels": [], "entities": []}, {"text": "We also report recall at precision of 90 and 80 percent.", "labels": [], "entities": [{"text": "recall", "start_pos": 15, "end_pos": 21, "type": "METRIC", "confidence": 0.9997734427452087}, {"text": "precision", "start_pos": 25, "end_pos": 34, "type": "METRIC", "confidence": 0.9992051720619202}]}, {"text": "compares the different models in all three language pairs.", "labels": [], "entities": []}, {"text": "In our next set of experiments, we looked at the effects of the Wikipedia specific features.", "labels": [], "entities": []}, {"text": "Since the ranker and CRF are asymmetric models, we also experimented with running the models in both directions and combining their outputs by intersection.", "labels": [], "entities": [{"text": "CRF", "start_pos": 21, "end_pos": 24, "type": "METRIC", "confidence": 0.8550111651420593}]}, {"text": "These results are shown in.", "labels": [], "entities": []}, {"text": "Identifying the agreement between two asymmetric models is a commonly exploited trick elsewhere in machine translation.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 99, "end_pos": 118, "type": "TASK", "confidence": 0.7914084494113922}]}, {"text": "It is mostly effective here as well, improving all cases except for the Bulgarian-English CRF where the regression is slight.", "labels": [], "entities": [{"text": "Bulgarian-English CRF", "start_pos": 72, "end_pos": 93, "type": "DATASET", "confidence": 0.6144913136959076}, {"text": "regression", "start_pos": 104, "end_pos": 114, "type": "METRIC", "confidence": 0.9578332901000977}]}, {"text": "More successful are the Wikipedia features, which provide an auxiliary signal of potential parallelism.", "labels": [], "entities": []}, {"text": "The gains from adding the lexicon-based features can be dramatic as in the case of Bulgarian (the CRF model average precision increased by nearly 9 points).", "labels": [], "entities": [{"text": "precision", "start_pos": 116, "end_pos": 125, "type": "METRIC", "confidence": 0.5699408054351807}]}, {"text": "The lower gains on Spanish and German maybe due in part to the lack of language-specific training data.", "labels": [], "entities": []}, {"text": "These results are very promising and motivate further exploration.", "labels": [], "entities": []}, {"text": "We also note that this is perhaps the first successful practical application of an automatically induced word translation lexicon.", "labels": [], "entities": [{"text": "word translation lexicon", "start_pos": 105, "end_pos": 129, "type": "TASK", "confidence": 0.7906525830427805}]}, {"text": "We also present results in the context of a full machine translation system to evaluate the potential utility of this data.", "labels": [], "entities": []}, {"text": "A standard phrasal SMT system () serves as our testbed, using a conventional set of models: phrasal mod-els of source given target and target given source; lexical weighting models in both directions, language model, word count, phrase count, distortion penalty, and a lexicalized reordering model.", "labels": [], "entities": [{"text": "SMT", "start_pos": 19, "end_pos": 22, "type": "TASK", "confidence": 0.7794819474220276}]}, {"text": "Given that the extracted Wikipedia data takes the standard form of parallel sentences, it would be easy to exploit this same data in a number of systems.", "labels": [], "entities": [{"text": "Wikipedia data", "start_pos": 25, "end_pos": 39, "type": "DATASET", "confidence": 0.9355809688568115}]}, {"text": "For each language pair we explored two training conditions.", "labels": [], "entities": []}, {"text": "The \"Medium\" data condition used easily downloadable corpora: Europarl for GermanEnglish and Spanish-English, and JRC/Acquis for Bulgarian-English.", "labels": [], "entities": [{"text": "Europarl", "start_pos": 62, "end_pos": 70, "type": "DATASET", "confidence": 0.9305226802825928}]}, {"text": "Additionally we included titles of all linked Wikipedia articles as parallel sentences in the medium data condition.", "labels": [], "entities": []}, {"text": "The \"Large\" data condition includes all the medium data, and also includes using abroad range of available sources such as data scraped from the web, data from the United Nations, phrase books, software documentation, and more.", "labels": [], "entities": []}, {"text": "In each condition, we explored the impact of including additional parallel sentences automatically extracted from Wikipedia in the system training data.", "labels": [], "entities": []}, {"text": "For German-English and Spanish-English, we extracted data with the null loss adjusted to achieve an estimated precision of 95 percent, and for English-Bulgarian a precision of 90 percent.", "labels": [], "entities": [{"text": "precision", "start_pos": 110, "end_pos": 119, "type": "METRIC", "confidence": 0.9887304902076721}, {"text": "precision", "start_pos": 163, "end_pos": 172, "type": "METRIC", "confidence": 0.9968674778938293}]}, {"text": "Table 4 summarizes the characteristics of these data sets.", "labels": [], "entities": []}, {"text": "We were pleasantly surprised at the amount of parallel sentences extracted from such a varied comparable corpus.", "labels": [], "entities": []}, {"text": "Apparently the average Wikipedia article contains at least a handful of parallel sentences, suggesting this is a very fertile ground for training MT systems.", "labels": [], "entities": [{"text": "MT", "start_pos": 146, "end_pos": 148, "type": "TASK", "confidence": 0.9622156023979187}]}, {"text": "The extracted Wikipedia data is likely to make the greatest impact on broad domain test sets -indeed, initial experimentation showed little BLEU gain on in-domain test sets such as Europarl, where out-of-domain training data is unlikely to provide appropriate phrasal translations.", "labels": [], "entities": [{"text": "Wikipedia data", "start_pos": 14, "end_pos": 28, "type": "DATASET", "confidence": 0.9199679791927338}, {"text": "BLEU", "start_pos": 140, "end_pos": 144, "type": "METRIC", "confidence": 0.9992125034332275}, {"text": "Europarl", "start_pos": 181, "end_pos": 189, "type": "DATASET", "confidence": 0.9919723272323608}]}, {"text": "Therefore, we experimented with two broad domain test sets.", "labels": [], "entities": []}, {"text": "First, Bing Translator provided a sample of translation requests along with translations in GermanEnglish and Spanish-English, which acted our standard development and test set.", "labels": [], "entities": [{"text": "Bing Translator", "start_pos": 7, "end_pos": 22, "type": "DATASET", "confidence": 0.9465276598930359}]}, {"text": "Unfortunately no such tagged set was available in Bulgarian-English, so we held out a portion of the large system's training data to use for development and test.", "labels": [], "entities": []}, {"text": "In each language pair, the test set was split into a development portion (\"Dev A\") used for minimum error rate training) and a test set (\"Test A\") used for final evaluation.", "labels": [], "entities": []}, {"text": "Second, we created new test sets in each of the three language pairs by sampling parallel sentences from held out Wikipedia articles.", "labels": [], "entities": []}, {"text": "To ensure that this test data was clean, we manually filtered the sentence pairs that were not truly parallel and edited them as necessary to improve adequacy.", "labels": [], "entities": []}, {"text": "We called this \"Wikitest\".", "labels": [], "entities": []}, {"text": "This test set is available at http://research.microsoft.com/enus/people/chrisq/wikidownload.aspx.", "labels": [], "entities": []}, {"text": "Characteristics of these test sets are summarized in.", "labels": [], "entities": []}, {"text": "We evaluated the resulting systems using BLEU-4 (); the results are presented in.", "labels": [], "entities": [{"text": "BLEU-4", "start_pos": 41, "end_pos": 47, "type": "METRIC", "confidence": 0.9961559176445007}]}, {"text": "First we note that the extracted Wikipedia data are very helpful in medium data conditions, significantly improving translation performance in all conditions.", "labels": [], "entities": [{"text": "Wikipedia data", "start_pos": 33, "end_pos": 47, "type": "DATASET", "confidence": 0.9317710399627686}, {"text": "translation", "start_pos": 116, "end_pos": 127, "type": "TASK", "confidence": 0.9574757814407349}]}, {"text": "Furthermore we found that the extracted Wikipedia sentences substantially improved translation quality on held-out Wikipedia articles.", "labels": [], "entities": []}, {"text": "In every case, training on medium data plus Wikipedia extracts led to equal or better translation quality than the large system alone.", "labels": [], "entities": [{"text": "translation", "start_pos": 86, "end_pos": 97, "type": "TASK", "confidence": 0.9560005068778992}]}, {"text": "Furthermore, adding the Wikipedia data to the large data condition still made substantial improvements.", "labels": [], "entities": [{"text": "Wikipedia data", "start_pos": 24, "end_pos": 38, "type": "DATASET", "confidence": 0.973028153181076}]}], "tableCaptions": [{"text": " Table 2: Average precision, recall at 90% precision, and recall at 80% precision for each model in all three language  pairs. In these experiments, the Wikipedia features and lexicon features are omitted.", "labels": [], "entities": [{"text": "Average", "start_pos": 10, "end_pos": 17, "type": "METRIC", "confidence": 0.9565757513046265}, {"text": "precision", "start_pos": 18, "end_pos": 27, "type": "METRIC", "confidence": 0.9117935299873352}, {"text": "recall", "start_pos": 29, "end_pos": 35, "type": "METRIC", "confidence": 0.9990440011024475}, {"text": "precision", "start_pos": 43, "end_pos": 52, "type": "METRIC", "confidence": 0.9949491024017334}, {"text": "recall", "start_pos": 58, "end_pos": 64, "type": "METRIC", "confidence": 0.9992770552635193}, {"text": "precision", "start_pos": 72, "end_pos": 81, "type": "METRIC", "confidence": 0.9932634234428406}]}, {"text": " Table 3: Average precision, recall at 90% precision, and recall at 80% precision for the Ranker and CRF in all three  language pairs. \"+Wiki\" indicates that Wikipedia features were used, and \"+Lex\" means the lexicon features were  used.", "labels": [], "entities": [{"text": "precision", "start_pos": 18, "end_pos": 27, "type": "METRIC", "confidence": 0.9215101003646851}, {"text": "recall", "start_pos": 29, "end_pos": 35, "type": "METRIC", "confidence": 0.9993448853492737}, {"text": "precision", "start_pos": 43, "end_pos": 52, "type": "METRIC", "confidence": 0.9948790073394775}, {"text": "recall", "start_pos": 58, "end_pos": 64, "type": "METRIC", "confidence": 0.9994277358055115}, {"text": "precision", "start_pos": 72, "end_pos": 81, "type": "METRIC", "confidence": 0.9960893392562866}]}, {"text": " Table 4: Statistics of the training data size in all three language pairs.", "labels": [], "entities": []}, {"text": " Table 5: Statistics of the test data sets.", "labels": [], "entities": []}, {"text": " Table 6: BLEU scores under various training and test conditions. The first column is from minimum error rate training;  the next two columns are on held-out test sets. For training data conditions including extracted Wikipedia sentences,  parenthesized values indicate absolute BLEU difference against the corresponding system without Wikipedia extracts.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9975572824478149}, {"text": "BLEU difference", "start_pos": 279, "end_pos": 294, "type": "METRIC", "confidence": 0.9562330543994904}]}]}