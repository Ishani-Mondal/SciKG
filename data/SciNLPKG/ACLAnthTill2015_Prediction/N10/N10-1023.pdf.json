{"title": [{"text": "Formatting Time-Aligned ASR Transcripts for Readability", "labels": [], "entities": [{"text": "Formatting Time-Aligned ASR Transcripts", "start_pos": 0, "end_pos": 39, "type": "TASK", "confidence": 0.5445673242211342}]}], "abstractContent": [{"text": "We address the problem of formatting the output of an automatic speech recognition (ASR) system for readability, while preserving word-level timing information of the transcript.", "labels": [], "entities": [{"text": "formatting the output of an automatic speech recognition (ASR)", "start_pos": 26, "end_pos": 88, "type": "TASK", "confidence": 0.7453291361982172}]}, {"text": "Our system enriches the ASR transcript with punctuation , capitalization and properly written dates, times and other numeric entities, and our approach can be applied to other formatting tasks.", "labels": [], "entities": [{"text": "ASR transcript", "start_pos": 24, "end_pos": 38, "type": "TASK", "confidence": 0.8316145241260529}]}, {"text": "The method we describe combines hand-crafted grammars with a class-based language model trained on written text and relies on Weighted Finite State Transducers (WF-STs) for the preservation of start and end time of each word.", "labels": [], "entities": []}], "introductionContent": [], "datasetContent": [{"text": "Section 4.1 presents our datasets and an evaluation metric specific to number formatting, and section 4.2 describes our experimental system.", "labels": [], "entities": [{"text": "number formatting", "start_pos": 71, "end_pos": 88, "type": "TASK", "confidence": 0.7034934610128403}]}, {"text": "We present quantitative evaluation of capitalization/punctuation performance and number formatting performance separately in sections 4.3 and 4.4.", "labels": [], "entities": [{"text": "number formatting", "start_pos": 81, "end_pos": 98, "type": "TASK", "confidence": 0.6852099895477295}]}, {"text": "Because the ultimate goal of our work is to improve the readability of ASR transcripts, we also present the result of a user study of transcript readability in section 4.5.", "labels": [], "entities": [{"text": "ASR transcripts", "start_pos": 71, "end_pos": 86, "type": "TASK", "confidence": 0.8795760869979858}]}, {"text": "The experimental system includes a 5-gram LM trained on TRS with spaces treated as tokens.", "labels": [], "entities": []}, {"text": "Number evaluation is performed with two sets of number classes, listed in.", "labels": [], "entities": [{"text": "Number evaluation", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.9101282358169556}]}, {"text": "System A contains LM with classes from set A, and System B contains LM with classes from set B.", "labels": [], "entities": []}, {"text": "The experimental setup also includes the following grammars: \u2022 G phone -deterministically formats as a phone number any string spoken like a US 7 or 10 digit phone number: A example of a raw transcript, reference transcript with number formatting and the hypothesis produced by the system.", "labels": [], "entities": []}, {"text": "The entities (bold) in reference and hypothesis are aligned and scored.", "labels": [], "entities": []}, {"text": "To evaluate the performance of capitalization and punctuation we run System A on NPTS with only the G cap punct (in order not to introduce errors due to numeric formatting).", "labels": [], "entities": [{"text": "NPTS", "start_pos": 81, "end_pos": 85, "type": "DATASET", "confidence": 0.9795531630516052}]}, {"text": "The precision, recall and Fmeasure rates for periods, commas and capitals are computed using PTS as reference (See).", "labels": [], "entities": [{"text": "precision", "start_pos": 4, "end_pos": 13, "type": "METRIC", "confidence": 0.9995923638343811}, {"text": "recall", "start_pos": 15, "end_pos": 21, "type": "METRIC", "confidence": 0.9988961219787598}, {"text": "Fmeasure", "start_pos": 26, "end_pos": 34, "type": "METRIC", "confidence": 0.9991008043289185}, {"text": "capitals", "start_pos": 65, "end_pos": 73, "type": "METRIC", "confidence": 0.9200035333633423}, {"text": "PTS", "start_pos": 93, "end_pos": 96, "type": "METRIC", "confidence": 0.9917194843292236}]}, {"text": "Precision It should be noted that a 5-gram language model that treats spaces as words models the same history as a 3-gram model that omits the spaces from training data.", "labels": [], "entities": [{"text": "Precision", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.9533601403236389}]}, {"text": "When this is taken into account, our results with a much smaller training set are comparable to.", "labels": [], "entities": []}, {"text": "The F-measure scores for commas and periods are also comparable to the prosody-based work of (, with the precision of the period slightly lower, but compensated by recall.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 4, "end_pos": 13, "type": "METRIC", "confidence": 0.9956644177436829}, {"text": "precision", "start_pos": 105, "end_pos": 114, "type": "METRIC", "confidence": 0.9986036419868469}, {"text": "recall", "start_pos": 164, "end_pos": 170, "type": "METRIC", "confidence": 0.9966592788696289}]}, {"text": "Thus, our system can perform additional formatting, while retaining a reasonable capitalization and punctuation performance.", "labels": [], "entities": [{"text": "formatting", "start_pos": 40, "end_pos": 50, "type": "TASK", "confidence": 0.9705265164375305}]}, {"text": "We evaluate number formatting performance of Systems A and B, which use different sets of classes for the language modeling (See).", "labels": [], "entities": [{"text": "number formatting", "start_pos": 12, "end_pos": 29, "type": "TASK", "confidence": 0.6834145784378052}]}, {"text": "We process NNTS with both systems and score against the reference formatted set NTS to obtain Numeric Entity Error Rate (NEER).", "labels": [], "entities": [{"text": "Numeric Entity Error Rate (NEER)", "start_pos": 94, "end_pos": 126, "type": "METRIC", "confidence": 0.7746606554303851}]}, {"text": "Class Set B naively breaks numbers into classes by digit count.", "labels": [], "entities": []}, {"text": "System B using this class set performs worse than System A by 1.7% absolute (See).", "labels": [], "entities": []}, {"text": "In particular, the overformatting rate (OFR) is higher by 1.2% absolute in System B than in System A. An example of overformatting is the mis-formatting of the English impersonal pronoun \"one\" as the digit \"1\".", "labels": [], "entities": [{"text": "overformatting rate (OFR)", "start_pos": 19, "end_pos": 44, "type": "METRIC", "confidence": 0.8635571479797364}]}, {"text": "Such overformatting errors are much more noticeable than the underfor-NEER IFR OFR UFR System A exact 16.1% 9.7% 5.4% 1.0% ignore space 11.2% 4.9% 5.4% 1.0% System B exact 17.8% 10.6% 6.6% 0.6% ignore space 13.2% 6.0% 6.6% 0.6%: The total NEER score, NEER due to incorrect formatting (IFR), NEER due to overformatting (OFR) and NEER due to underformatting (UFR); NEER rates with whitespace errors ignored are also listed.", "labels": [], "entities": [{"text": "NEER due to overformatting (OFR)", "start_pos": 291, "end_pos": 323, "type": "METRIC", "confidence": 0.8008361629077366}, {"text": "NEER due to underformatting (UFR)", "start_pos": 328, "end_pos": 361, "type": "METRIC", "confidence": 0.7738830617495945}]}, {"text": "matting errors, which are higher by 0.4% absolute in System A. This result shows that the choice of classes for the class-based LM significantly impacts number formatting performance.", "labels": [], "entities": [{"text": "number formatting", "start_pos": 153, "end_pos": 170, "type": "TASK", "confidence": 0.6637403964996338}]}, {"text": "Superior overall performance of System A suggests that prior knowledge in the choice of classes favorably impacts performance.", "labels": [], "entities": []}, {"text": "In order to estimate the error rate not caused by whitespace errors, we also compute the NEER with whitespace errors ignored.", "labels": [], "entities": [{"text": "error rate", "start_pos": 25, "end_pos": 35, "type": "METRIC", "confidence": 0.9486141800880432}, {"text": "NEER", "start_pos": 89, "end_pos": 93, "type": "METRIC", "confidence": 0.8292062878608704}]}, {"text": "It turns out that between 4 and 5% absolute of the errors are whitespace errors.", "labels": [], "entities": []}, {"text": "Even if all whitespace errors are significant, the 83.9% of perfectly formatted entities suggests that the proposed formatting approach can achieve good performance on the number formatting task.", "labels": [], "entities": [{"text": "number formatting task", "start_pos": 172, "end_pos": 194, "type": "TASK", "confidence": 0.7992207606633505}]}, {"text": "To estimate how well the systems perform on specific number formatting tasks we count the number of reference entities containing certain formatting characters and compute the number of these entities correctly formatted by Systems A and B (See).", "labels": [], "entities": [{"text": "number formatting", "start_pos": 53, "end_pos": 70, "type": "TASK", "confidence": 0.692012757062912}]}, {"text": "The count of different formatting characters in NTS is small, but still provides an estimate of the number formatting performance fora real appli-cation like voicemail transcription.", "labels": [], "entities": []}, {"text": "System A performs significantly better on the formatting of time expressions containing a colon, getting 74.8% correct.", "labels": [], "entities": []}, {"text": "The NEER of System A for entities containing special formatting characters is under 28% for all formatting characters except comma, which is used inconsistently in training text.", "labels": [], "entities": [{"text": "NEER", "start_pos": 4, "end_pos": 8, "type": "METRIC", "confidence": 0.9940713047981262}]}, {"text": "In addition to quantitative evaluation we have conducted a small-scale study of transcript readability.", "labels": [], "entities": []}, {"text": "The study aims to compare raw ASR transcripts, ASR transcripts formatted by our system and raw manual transcripts.", "labels": [], "entities": [{"text": "ASR transcripts formatted", "start_pos": 47, "end_pos": 72, "type": "TASK", "confidence": 0.7490437229474386}]}, {"text": "We have processed LDC Voicemail Part 1 with our ASR engine achieving an error rate of 30%, and have selected 50 voicemails with error rate under 30% and high informational content.", "labels": [], "entities": [{"text": "LDC Voicemail Part 1", "start_pos": 18, "end_pos": 38, "type": "DATASET", "confidence": 0.7784156799316406}, {"text": "error rate", "start_pos": 72, "end_pos": 82, "type": "METRIC", "confidence": 0.9841342270374298}, {"text": "error rate", "start_pos": 128, "end_pos": 138, "type": "METRIC", "confidence": 0.9482138454914093}]}, {"text": "Messages containing names, addresses and numbers were preferred.", "labels": [], "entities": []}, {"text": "The word error rate on the selected voicemails is 20%.", "labels": [], "entities": [{"text": "word error rate", "start_pos": 4, "end_pos": 19, "type": "METRIC", "confidence": 0.7969758113225301}]}, {"text": "For each voicemail we have constructed three semantic multiple-choice questions, aimed at information extraction.", "labels": [], "entities": [{"text": "information extraction", "start_pos": 90, "end_pos": 112, "type": "TASK", "confidence": 0.8348715305328369}]}, {"text": "We have asked each of 15 volunteers to answer all 3 questions about half of the voicemails.", "labels": [], "entities": []}, {"text": "The questions were shown in sequence, while the transcript remained on the screen.", "labels": [], "entities": []}, {"text": "The transcript for each voicemail was randomly selected to be ASR raw, ASR formatted or manual raw.", "labels": [], "entities": []}, {"text": "The response time was measured individually for each question.", "labels": [], "entities": []}, {"text": "The analysis of the responses reveals a statistically significant difference in response time between formatted and raw ASR transcripts (p = 0.02, even allowing for per-item and per-subject effects; see also) and comparable accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 224, "end_pos": 232, "type": "METRIC", "confidence": 0.9989392161369324}]}, {"text": "The response times for formatted ASR were comparable to the response times for manual unformatted transcripts.", "labels": [], "entities": [{"text": "formatted ASR", "start_pos": 23, "end_pos": 36, "type": "TASK", "confidence": 0.5738236904144287}]}, {"text": "This suggests that for transcripts with low error rates the formatting of the ASR output significantly impacts readability.", "labels": [], "entities": [{"text": "ASR", "start_pos": 78, "end_pos": 81, "type": "TASK", "confidence": 0.9026786684989929}]}, {"text": "This disagrees with a similar study (, which found no significant difference in the comprehension rates between raw ASR transcripts and capitalized, punctuated ASR output with disfluencies removed.", "labels": [], "entities": [{"text": "ASR transcripts", "start_pos": 116, "end_pos": 131, "type": "TASK", "confidence": 0.8604887127876282}]}, {"text": "This could be due to a number of factors, including a different type of transformation performed on the ASR transcript, a different corpus, and a lower word error rate of transcripts in our user study.", "labels": [], "entities": [{"text": "word error rate", "start_pos": 152, "end_pos": 167, "type": "METRIC", "confidence": 0.7306106388568878}]}], "tableCaptions": [{"text": " Table 1: Two sets of number classes used in our system.  Each sequence of consecutive digit characters is mapped  to the appropriate class. For example, \"$1,235.12\" would  become \"dollar 1 comma num 100 999 period  num 10 12\" in Class Set A and \"dollar num 1D  comma num 3D period num 2D in Class Set B.", "labels": [], "entities": []}, {"text": " Table 2: A example of a raw transcript, reference transcript with number formatting and the hypothesis produced by  the system. The entities (bold) in reference and hypothesis are aligned and scored.", "labels": [], "entities": []}, {"text": " Table 5: The count of formatted entities in NTS contain- ing various formatting characters; the counts of these en- tities correctly formatted by the systems A and B.", "labels": [], "entities": []}]}