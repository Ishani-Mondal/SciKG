{"title": [{"text": "Multi-document Summarization via Budgeted Maximization of Submodular Functions", "labels": [], "entities": [{"text": "Summarization", "start_pos": 15, "end_pos": 28, "type": "TASK", "confidence": 0.7892901301383972}, {"text": "Budgeted Maximization of Submodular Functions", "start_pos": 33, "end_pos": 78, "type": "TASK", "confidence": 0.6824627101421357}]}], "abstractContent": [{"text": "We treat the text summarization problem as maximizing a submodular function under a budget constraint.", "labels": [], "entities": [{"text": "text summarization", "start_pos": 13, "end_pos": 31, "type": "TASK", "confidence": 0.7330106496810913}]}, {"text": "We show, both theoretically and empirically, a modified greedy algorithm can efficiently solve the budgeted submodu-lar maximization problem near-optimally, and we derive new approximation bounds in doing so.", "labels": [], "entities": []}, {"text": "Experiments on DUC'04 task show that our approach is superior to the best-performing method from the DUC'04 evaluation on ROUGE-1 scores.", "labels": [], "entities": [{"text": "DUC'04 task", "start_pos": 15, "end_pos": 26, "type": "DATASET", "confidence": 0.9356875419616699}, {"text": "DUC'04", "start_pos": 101, "end_pos": 107, "type": "DATASET", "confidence": 0.875826895236969}, {"text": "ROUGE-1", "start_pos": 122, "end_pos": 129, "type": "METRIC", "confidence": 0.9466375708580017}]}], "introductionContent": [{"text": "Automatically generating summaries from large text corpora has long been studied in both information retrieval and natural language processing.", "labels": [], "entities": [{"text": "Automatically generating summaries from large text corpora", "start_pos": 0, "end_pos": 58, "type": "TASK", "confidence": 0.8013131150177547}, {"text": "information retrieval", "start_pos": 89, "end_pos": 110, "type": "TASK", "confidence": 0.794715017080307}, {"text": "natural language processing", "start_pos": 115, "end_pos": 142, "type": "TASK", "confidence": 0.6590316295623779}]}, {"text": "There are several types of text summarization tasks.", "labels": [], "entities": [{"text": "text summarization tasks", "start_pos": 27, "end_pos": 51, "type": "TASK", "confidence": 0.8208531538645426}]}, {"text": "For example, if an input query is given, the generated summary can be query-specific, and otherwise it is generic.", "labels": [], "entities": []}, {"text": "Also, the number of documents to be summarized can vary from one to many.", "labels": [], "entities": []}, {"text": "The constituent sentences of a summary, moreover, might be formed in a variety of different ways -summarization can be conducted using either extraction or abstraction, the former selects only sentences from the original document set, whereas the latter involves natural language generation.", "labels": [], "entities": [{"text": "summarization", "start_pos": 98, "end_pos": 111, "type": "TASK", "confidence": 0.9629904627799988}]}, {"text": "In this paper, we address the problem of generic extractive summaries from clusters of related documents, commonly known as multi-document summarization.", "labels": [], "entities": [{"text": "generic extractive summaries from clusters of related documents", "start_pos": 41, "end_pos": 104, "type": "TASK", "confidence": 0.8330589085817337}, {"text": "multi-document summarization", "start_pos": 124, "end_pos": 152, "type": "TASK", "confidence": 0.6788809597492218}]}, {"text": "In extractive text summarization, textual units (e.g., sentences) from a document set are extracted to form a summary, where grammaticality is assured at the local level.", "labels": [], "entities": [{"text": "extractive text summarization", "start_pos": 3, "end_pos": 32, "type": "TASK", "confidence": 0.621208131313324}]}, {"text": "Finding the optimal summary can be viewed as a combinatorial optimization problem which is NP-hard to solve.", "labels": [], "entities": []}, {"text": "One of the standard methods for this problem is called Maximum Marginal Relevance (MMR) (), where a greedy algorithm selects the most relevant sentences, and at the same time avoids redundancy by removing sentences that are too similar to already selected ones.", "labels": [], "entities": [{"text": "Maximum Marginal Relevance (MMR)", "start_pos": 55, "end_pos": 87, "type": "METRIC", "confidence": 0.8024709423383077}]}, {"text": "One major problem of MMR is that it is non-optimal because the decision is made based on the scores at the current iteration.", "labels": [], "entities": [{"text": "MMR", "start_pos": 21, "end_pos": 24, "type": "TASK", "confidence": 0.9891525506973267}]}, {"text": "proposed to replace the greedy search of MMR with a globally optimal formulation, where the basic MMR framework can be expressed as a knapsack packing problem, and an integer linear program (ILP) solver can be used to maximize the resulting objective function.", "labels": [], "entities": [{"text": "MMR", "start_pos": 41, "end_pos": 44, "type": "TASK", "confidence": 0.864791750907898}]}, {"text": "ILP Algorithms, however, can sometimes either be expensive for large scale problems or themselves might only be heuristic without associated theoretical approximation guarantees.", "labels": [], "entities": []}, {"text": "In this paper, we study graph-based approaches for multi-document summarization.", "labels": [], "entities": [{"text": "multi-document summarization", "start_pos": 51, "end_pos": 79, "type": "TASK", "confidence": 0.6401461064815521}]}, {"text": "Indeed, several graph-based methods have been proposed for extractive summarization in the past.", "labels": [], "entities": [{"text": "extractive summarization", "start_pos": 59, "end_pos": 83, "type": "TASK", "confidence": 0.9008126258850098}]}, {"text": "introduced a stochastic graph-based method, LexRank, for computing the relative importance of textual units for multi-document summarization.", "labels": [], "entities": [{"text": "LexRank", "start_pos": 44, "end_pos": 51, "type": "DATASET", "confidence": 0.9263372421264648}, {"text": "multi-document summarization", "start_pos": 112, "end_pos": 140, "type": "TASK", "confidence": 0.6177437603473663}]}, {"text": "In LexRank the importance of sentences is computed based on the concept of eigenvector centrality in the graph representation of sentences.", "labels": [], "entities": [{"text": "LexRank", "start_pos": 3, "end_pos": 10, "type": "DATASET", "confidence": 0.9364544153213501}]}, {"text": "Mihalcea and Tarau also proposed an eigenvector centrality algorithm on weighted graphs for document summarization ( ).) to natural language processing tasks ranging from automatic keyphrase extraction and word sense disambiguation, to extractive summarization).", "labels": [], "entities": [{"text": "document summarization", "start_pos": 92, "end_pos": 114, "type": "TASK", "confidence": 0.7158654928207397}, {"text": "keyphrase extraction", "start_pos": 181, "end_pos": 201, "type": "TASK", "confidence": 0.7322666049003601}, {"text": "word sense disambiguation", "start_pos": 206, "end_pos": 231, "type": "TASK", "confidence": 0.6594694952170054}, {"text": "extractive summarization", "start_pos": 236, "end_pos": 260, "type": "TASK", "confidence": 0.5701466500759125}]}, {"text": "Recent work in () presents a graph-based approach where an undirected weighted graph is built for the document to be summarized, and vertices represent the candidate sentences and edge weights represent the similarity between sentences.", "labels": [], "entities": []}, {"text": "The summary extraction procedure is done by maximizing a submodular set function under a cardinality constraint.", "labels": [], "entities": [{"text": "summary extraction", "start_pos": 4, "end_pos": 22, "type": "TASK", "confidence": 0.8483540117740631}]}, {"text": "Inspired by), we perform summarization by maximizing submodular functions under a budget constraint.", "labels": [], "entities": [{"text": "summarization", "start_pos": 25, "end_pos": 38, "type": "TASK", "confidence": 0.9926562905311584}]}, {"text": "A budget constraint is natural in summarization task as the length of the summary is often restricted.", "labels": [], "entities": [{"text": "summarization", "start_pos": 34, "end_pos": 47, "type": "TASK", "confidence": 0.989107608795166}]}, {"text": "The length (byte budget) limitation represents the real world scenario where summaries are displayed using only limited computer screen real estate.", "labels": [], "entities": [{"text": "length (byte budget)", "start_pos": 4, "end_pos": 24, "type": "METRIC", "confidence": 0.8758114695549011}]}, {"text": "In practice, the candidate textual/linguistic units might not have identical costs (e.g., sentence lengths vary).", "labels": [], "entities": []}, {"text": "Since a cardinality constraint is a special case (a budget constraint with unity costs), our approach is more general than ().", "labels": [], "entities": []}, {"text": "Moreover, we propose a modified greedy algorithm (Section 4) and both theoretically (Section 4.1) and empirically (Section 5.1) show that the algorithm solves the problem near-optimally, thanks to submodularity.", "labels": [], "entities": []}, {"text": "Regarding summarization performance, experiments on DUC'04 task show that our approach is superior to the best-performing method in DUC'04 evaluation on ROUGE-1 scores (Section 5).", "labels": [], "entities": [{"text": "DUC'04 task", "start_pos": 52, "end_pos": 63, "type": "DATASET", "confidence": 0.8952582776546478}, {"text": "ROUGE-1", "start_pos": 153, "end_pos": 160, "type": "METRIC", "confidence": 0.9608446359634399}]}], "datasetContent": [{"text": "We evaluated our approach on the data set of DUC'04 (2004) with the setting of task 2, which is a multi-document summarization task on English news articles.", "labels": [], "entities": [{"text": "data set of DUC'04 (2004)", "start_pos": 33, "end_pos": 58, "type": "DATASET", "confidence": 0.8213999101093837}, {"text": "multi-document summarization task on English news articles", "start_pos": 98, "end_pos": 156, "type": "TASK", "confidence": 0.6261516255991799}]}, {"text": "In this task, 50 document clusters are given, each of which consists of 10 documents.", "labels": [], "entities": []}, {"text": "For each document cluster, a short multi-document summary is to be generated.", "labels": [], "entities": []}, {"text": "The summary should not be longer than 665 bytes including spaces and punctuation, as required in the DUC'04 evaluation.", "labels": [], "entities": [{"text": "DUC'04 evaluation", "start_pos": 101, "end_pos": 118, "type": "DATASET", "confidence": 0.9284580647945404}]}, {"text": "We used DUC'03 as our development set.", "labels": [], "entities": [{"text": "DUC'03", "start_pos": 8, "end_pos": 14, "type": "DATASET", "confidence": 0.9596163034439087}]}, {"text": "All documents were segmented into sentences using a script distributed by DUC.", "labels": [], "entities": [{"text": "DUC", "start_pos": 74, "end_pos": 77, "type": "DATASET", "confidence": 0.9775201678276062}]}, {"text": "ROUGE version 1.5.5, which is widely used in the study of summarization, was used to evaluate summarization performance in our experiments 1 . We focus on ROUGE-1 (unigram) F-measure scores since it has demonstrated strong correlation with human annotation).", "labels": [], "entities": [{"text": "ROUGE version 1.5.5", "start_pos": 0, "end_pos": 19, "type": "DATASET", "confidence": 0.8108806610107422}, {"text": "summarization", "start_pos": 58, "end_pos": 71, "type": "TASK", "confidence": 0.9851336479187012}, {"text": "summarization", "start_pos": 94, "end_pos": 107, "type": "TASK", "confidence": 0.9601995944976807}]}, {"text": "The basic textual/linguistic units we consider in our experiments are sentences.", "labels": [], "entities": []}, {"text": "For each document cluster, sentences in all the documents of this cluster forms the ground set V . We built semantic graphs for each document cluster based on cosine similarity, where cosine similarity is computed based on the TF-IDF (term frequency, inverse document frequency) vectors for the words in the sentences.", "labels": [], "entities": [{"text": "TF-IDF", "start_pos": 227, "end_pos": 233, "type": "METRIC", "confidence": 0.9347699880599976}, {"text": "inverse document frequency) vectors", "start_pos": 251, "end_pos": 286, "type": "METRIC", "confidence": 0.8138124108314514}]}, {"text": "The cosine similarity measures the similarity between sentences, i.e., w i,j . Here the IDF values were calculated using all the document clusters.", "labels": [], "entities": [{"text": "cosine similarity", "start_pos": 4, "end_pos": 21, "type": "METRIC", "confidence": 0.8373835384845734}, {"text": "IDF", "start_pos": 88, "end_pos": 91, "type": "METRIC", "confidence": 0.9959192872047424}]}, {"text": "The weighted graph was built by connecting vertices (corresponding to sentences) with weight w i,j > 0.", "labels": [], "entities": []}, {"text": "Any unconnected vertex was removed from the graph, which is equivalent to preexcluding certain sentences from the summary.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Comparison of Algorithm 1 to exact algorithms  on DUC'03 dataset. All the numbers shown in the ta- ble are the average statistics (mean/std). The \"true\" ap- proximation factor is the ratio of objective function value  found by Algorithm 1 over the ILP-derived true-optimal  objective value, and the approximation bounds were esti- mated using Theorem 2.", "labels": [], "entities": [{"text": "DUC'03 dataset", "start_pos": 60, "end_pos": 74, "type": "DATASET", "confidence": 0.9885365068912506}, {"text": "true\" ap- proximation factor", "start_pos": 157, "end_pos": 185, "type": "METRIC", "confidence": 0.7591033478577932}]}, {"text": " Table 2: ROUGE-1 F-measure results (%)", "labels": [], "entities": [{"text": "ROUGE-1", "start_pos": 10, "end_pos": 17, "type": "METRIC", "confidence": 0.9919708967208862}, {"text": "F-measure", "start_pos": 18, "end_pos": 27, "type": "METRIC", "confidence": 0.8694987893104553}]}]}