{"title": [{"text": "Training Paradigms for Correcting Errors in Grammar and Usage", "labels": [], "entities": [{"text": "Grammar and Usage", "start_pos": 44, "end_pos": 61, "type": "TASK", "confidence": 0.6669187943140665}]}], "abstractContent": [{"text": "This paper proposes a novel approach to the problem of training classifiers to detect and correct grammar and usage errors in text by selectively introducing mistakes into the training data.", "labels": [], "entities": []}, {"text": "When training a classifier, we would like the distribution of examples seen in training to be as similar as possible to the one seen in testing.", "labels": [], "entities": []}, {"text": "In error correction problems, such as correcting mistakes made by second language learners, a system is generally trained on correct data, since annotating data for training is expensive.", "labels": [], "entities": [{"text": "error correction", "start_pos": 3, "end_pos": 19, "type": "TASK", "confidence": 0.7331430613994598}]}, {"text": "Error generation methods avoid expensive data annotation and create training data that resemble non-native data with errors.", "labels": [], "entities": [{"text": "Error generation", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.7387458086013794}]}, {"text": "We apply error generation methods and train classifiers for detecting and correcting article errors in essays written by non-native En-glish speakers; we show that training on data that contain errors produces higher accuracy when compared to a system that is trained on clean native data.", "labels": [], "entities": [{"text": "error generation", "start_pos": 9, "end_pos": 25, "type": "TASK", "confidence": 0.7196383327245712}, {"text": "detecting and correcting article errors", "start_pos": 60, "end_pos": 99, "type": "TASK", "confidence": 0.721055394411087}, {"text": "accuracy", "start_pos": 217, "end_pos": 225, "type": "METRIC", "confidence": 0.9967437982559204}]}, {"text": "We propose several training paradigms with error generation and show that each such paradigm is superior to training a classifier on native data.", "labels": [], "entities": [{"text": "error generation", "start_pos": 43, "end_pos": 59, "type": "TASK", "confidence": 0.6915225684642792}]}, {"text": "We also show that the most successful error generation methods are those that use knowledge about the article distribution and error patterns observed in non-native text.", "labels": [], "entities": [{"text": "error generation", "start_pos": 38, "end_pos": 54, "type": "TASK", "confidence": 0.7058385759592056}]}], "introductionContent": [{"text": "This paper considers the problem of training classifiers to detect and correct errors in grammar and word usage in text.", "labels": [], "entities": []}, {"text": "Both native and non-native speakers make a variety of errors that are not always easy to detect.", "labels": [], "entities": []}, {"text": "Consider, for example, the problem of context-sensitive spelling correction (e.g.,).", "labels": [], "entities": [{"text": "context-sensitive spelling correction", "start_pos": 38, "end_pos": 75, "type": "TASK", "confidence": 0.6344667176405588}]}, {"text": "Unlike spelling errors that result in non-words and are easy to detect, context-sensitive spelling correction task involves correcting spelling errors that result in legitimate words, such as confusing peace and piece or your and you're.", "labels": [], "entities": [{"text": "context-sensitive spelling correction", "start_pos": 72, "end_pos": 109, "type": "TASK", "confidence": 0.5647846162319183}]}, {"text": "The typical training paradigm for these context-sensitive ambiguities is to use text assumed to be error free, replacing each target word occurrence (e.g. peace) with a confusion set consisting of, say {peace, piece}, thus generating both positive and negative examples, respectively, from the same context.", "labels": [], "entities": []}, {"text": "This paper proposes a novel error generation approach to the problem of training classifiers for the purpose of detecting and correcting grammar and usage errors in text.", "labels": [], "entities": [{"text": "error generation", "start_pos": 28, "end_pos": 44, "type": "TASK", "confidence": 0.7388368248939514}, {"text": "detecting and correcting grammar and usage errors in text", "start_pos": 112, "end_pos": 169, "type": "TASK", "confidence": 0.737341993384891}]}, {"text": "Unlike previous work (e.g.,), we selectively introduce mistakes in an appropriate proportion.", "labels": [], "entities": []}, {"text": "In particular, to create training data that closely resemble text with naturally occurring errors, we use error frequency information and error distribution statistics obtained from corrected non-native text.", "labels": [], "entities": []}, {"text": "We apply the method to the problem of detecting and correcting article mistakes made by learners of English as a Second Language (ESL).", "labels": [], "entities": [{"text": "detecting and correcting article mistakes made by learners of English as a Second Language (ESL)", "start_pos": 38, "end_pos": 134, "type": "TASK", "confidence": 0.8192245995297152}]}, {"text": "The problem of correcting article errors is generally viewed as that of article selection, cast as a classification problem and is trained as described above: a machine learning algorithm is used to train a classifier on native English data, where the possible selections are used to generate positive and negative examples (e.g.,).", "labels": [], "entities": [{"text": "article selection", "start_pos": 72, "end_pos": 89, "type": "TASK", "confidence": 0.8104151487350464}]}, {"text": "The classifier is then applied to non-native text to predict the correct article in context.", "labels": [], "entities": []}, {"text": "But the article correction problem differs from the problem of article selection in that we know the original (source) article that the writer used.", "labels": [], "entities": [{"text": "article correction", "start_pos": 8, "end_pos": 26, "type": "TASK", "confidence": 0.8359761238098145}, {"text": "article selection", "start_pos": 63, "end_pos": 80, "type": "TASK", "confidence": 0.7611246407032013}]}, {"text": "When proposing a correction, we would like to use information about the original article.", "labels": [], "entities": []}, {"text": "One reason for this is that about 90% of articles are used correctly by ESL learners; this is higher than the performance of state-of-the-art classifiers for article selection.", "labels": [], "entities": [{"text": "article selection", "start_pos": 158, "end_pos": 175, "type": "TASK", "confidence": 0.6927826404571533}]}, {"text": "Consequently, not using the writer's article, when making a prediction, may result in making more mistakes than there are in the data.", "labels": [], "entities": []}, {"text": "Another reason is that statistics on article errors (e.g.,) and in the annotation performed for the present study reveal that non-native English speakers make article mistakes in a consistent manner.", "labels": [], "entities": []}, {"text": "The system can consider the article used by the writer at evaluation time, by proposing a correction only when the confidence of the classifier is high enough, but the article cannot be used in training if the classifier is trained on clean native data that do not have errors.", "labels": [], "entities": []}, {"text": "Learning Theory says that the distribution of examples seen in testing should be as similar as possible to the one seen in training, so one would like to train on errors similar to those observed in testing.", "labels": [], "entities": []}, {"text": "Ideally, we would like to train using corrected non-native text.", "labels": [], "entities": []}, {"text": "In that case, the original article of the writer can be used as a feature for the classifier and the correct article, as judged by a native English speaker, will be viewed as the label.", "labels": [], "entities": []}, {"text": "However, obtaining annotated data for training is expensive and, since the native training data do not contain errors, we cannot use the writer's article as a feature for the classifier.", "labels": [], "entities": []}, {"text": "This paper compares the traditional training paradigm that uses native data to training paradigms that use data with artificial mistakes.", "labels": [], "entities": []}, {"text": "We propose several methods of generating mistakes in native training data and demonstrate that they outperform the traditional training paradigm.", "labels": [], "entities": []}, {"text": "We also show that the most successful error generation methods use knowledge about the article distribution and error patterns observed in the ESL data.", "labels": [], "entities": [{"text": "error generation", "start_pos": 38, "end_pos": 54, "type": "TASK", "confidence": 0.6879250854253769}, {"text": "ESL data", "start_pos": 143, "end_pos": 151, "type": "DATASET", "confidence": 0.8716909885406494}]}, {"text": "The rest of the paper is organized as follows.", "labels": [], "entities": []}, {"text": "First, we discuss the baseline on the error correction task and show why the baselines used in selection tasks are not relevant for the error correction task.", "labels": [], "entities": [{"text": "error correction task", "start_pos": 38, "end_pos": 59, "type": "TASK", "confidence": 0.7870535949865977}, {"text": "error correction task", "start_pos": 136, "end_pos": 157, "type": "TASK", "confidence": 0.7738293608029684}]}, {"text": "Next, we describe prior work in error generation and show the key difference of our approach.", "labels": [], "entities": [{"text": "error generation", "start_pos": 32, "end_pos": 48, "type": "TASK", "confidence": 0.6773087531328201}]}, {"text": "Section 4 presents the ESL data that we use and statistics on article errors.", "labels": [], "entities": [{"text": "ESL data", "start_pos": 23, "end_pos": 31, "type": "DATASET", "confidence": 0.7860358655452728}]}, {"text": "Section 5 describes training paradigms that employ error generation.", "labels": [], "entities": [{"text": "error generation", "start_pos": 51, "end_pos": 67, "type": "TASK", "confidence": 0.692345917224884}]}, {"text": "In Sections 6 and 7 we present the results and discuss the results.", "labels": [], "entities": []}, {"text": "The key findings are summarized in in Section 6.", "labels": [], "entities": []}, {"text": "We conclude with a brief discussion of directions for future work.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we compare the quality of the system trained on clean native English data to the quality of the systems trained on data with errors.", "labels": [], "entities": []}, {"text": "The errors were introduced into the training data using error generation methods presented in Section 5.", "labels": [], "entities": []}, {"text": "In each training paradigm, we follow a discriminative approach, using an online learning paradigm and making use of the Averaged Perceptron Algorithm) implemented within the Sparse Network of Winnow framework) -we use the regularized version in Learning Based Java 6 (LBJ, ().", "labels": [], "entities": [{"text": "Averaged Perceptron Algorithm", "start_pos": 120, "end_pos": 149, "type": "METRIC", "confidence": 0.8736909230550131}]}, {"text": "While classical Perceptron comes with generalization bound related to the margin of the data, Averaged Perceptron also comes with a PAC-like generalization bound).", "labels": [], "entities": []}, {"text": "This linear learning algorithm is known, both theoretically and experimentally, to be among the best linear learning approaches and is competitive with SVM and Logistic Regression, while being more efficient in training.", "labels": [], "entities": []}, {"text": "It also has been shown to produce state-of-the-art results on many natural language applications.", "labels": [], "entities": []}, {"text": "Since the methods of error generation described in Section 5 rely on the distribution of articles and article mistakes and these statistics are specific to the first language of the writer, we conduct evaluation separately for each source language.", "labels": [], "entities": [{"text": "error generation", "start_pos": 21, "end_pos": 37, "type": "TASK", "confidence": 0.7126621454954147}]}, {"text": "Thus, for each language group, we train five system types: one system is trained on clean English data without errors (the same classifier for the three language groups) and four systems are trained on data with errors, where errors are produced using the four methods described in Section 5.", "labels": [], "entities": []}, {"text": "Training data are extracted from English Wikipedia.", "labels": [], "entities": [{"text": "English Wikipedia", "start_pos": 33, "end_pos": 50, "type": "DATASET", "confidence": 0.9638600945472717}]}, {"text": "All of the five systems employ the same set of features based on three tokens to the right and to the left of the target article.", "labels": [], "entities": []}, {"text": "For each context word, we use its relative position, its part-of-speech tag and the word token itself.", "labels": [], "entities": []}, {"text": "We also use the head of the noun phrase and the conjunctions of the pairs and triples of the six tokens and their part-of-speech tags . In addition to these features, the classifiers trained on data with errors also use the source article as a feature.", "labels": [], "entities": []}, {"text": "The classifier that is trained on clean English data cannot use the source feature, since in training the source always corresponds to the label.", "labels": [], "entities": []}, {"text": "By contrast, when the training data contain mistakes, the source is not always the same as the label, the situation that we also have with the test (ESL) data.", "labels": [], "entities": []}, {"text": "We refer to the classifier trained on clean data as T rainClean.", "labels": [], "entities": [{"text": "T rainClean", "start_pos": 52, "end_pos": 63, "type": "DATASET", "confidence": 0.6497513055801392}]}, {"text": "We refer to the classifiers trained on data with mistakes as T W E (TrainWithErrors).", "labels": [], "entities": [{"text": "T W E", "start_pos": 61, "end_pos": 66, "type": "METRIC", "confidence": 0.8720381657282511}]}, {"text": "There are four types of T W E systems for each language group, one for each of the methods of error generation described in Section 5.", "labels": [], "entities": [{"text": "error generation", "start_pos": 94, "end_pos": 110, "type": "TASK", "confidence": 0.7027161419391632}]}, {"text": "All results are the averaged results of training on three random samples from Wikipedia with two million training examples on each round.", "labels": [], "entities": []}, {"text": "All five classifiers are trained on exactly the same set of Wikipedia examples, except that we add article mistakes to the data used by the T W E systems.", "labels": [], "entities": []}, {"text": "The T rainClean system achieves an accuracy of 87.10% on data from English Wikipedia.", "labels": [], "entities": [{"text": "T rainClean system", "start_pos": 4, "end_pos": 22, "type": "DATASET", "confidence": 0.7778294285138448}, {"text": "accuracy", "start_pos": 35, "end_pos": 43, "type": "METRIC", "confidence": 0.9994396567344666}]}, {"text": "This performance is state-of-the-art compared to other systems reported in the literature (.", "labels": [], "entities": []}, {"text": "The best results of 92.15% are reported by De Felice and Pulman (2008).", "labels": [], "entities": []}, {"text": "But their system uses sophisticated syntactic features and they observe that the parser does not perform well on non-native data.", "labels": [], "entities": []}, {"text": "As mentioned in Section 4, the annotation of the ESL data consisted of correcting all errors in the sentence.", "labels": [], "entities": [{"text": "ESL data", "start_pos": 49, "end_pos": 57, "type": "DATASET", "confidence": 0.8870302736759186}]}, {"text": "We exclude from evaluation examples that have spelling errors in the 3-word window around the target article and errors on words that immediately precede or immediately follow the article, as such examples would obscure the evaluation of the training paradigms.", "labels": [], "entities": []}, {"text": "show performance by language group.", "labels": [], "entities": []}, {"text": "The tables show the accuracy and the error reduction on the test set.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 20, "end_pos": 28, "type": "METRIC", "confidence": 0.9997337460517883}, {"text": "error reduction", "start_pos": 37, "end_pos": 52, "type": "METRIC", "confidence": 0.9752987325191498}]}, {"text": "The results of systems T W E (methods 2 and 3) that use the distribution of articles before and after annotation are merged and appear as ArtDistr in the tables, since, as shown in, these distributions are very similar and thus produce similar results.", "labels": [], "entities": []}, {"text": "Each table compares the performance of the T rainClean system to the performance of the four systems trained on data with errors.", "labels": [], "entities": [{"text": "T rainClean system", "start_pos": 43, "end_pos": 61, "type": "DATASET", "confidence": 0.8173904617627462}]}, {"text": "For all language groups, all classifiers of type T W E outperform the T rainClean system.", "labels": [], "entities": []}, {"text": "The reduction in error rate is consistent when the T W E classifiers are compared to the T rainClean system.", "labels": [], "entities": [{"text": "error rate", "start_pos": 17, "end_pos": 27, "type": "METRIC", "confidence": 0.9851849377155304}]}, {"text": "shows results for all three languages, comparing for each language group the T rainClean classifier to the best performing system of type T W E.: Russian speakers: Performance of the T rainClean system (without errors in training) and of the best classifiers of type T W E.", "labels": [], "entities": []}, {"text": "Rows 2-4 show the performance of the systems trained with error generation methods described in 5.", "labels": [], "entities": [{"text": "error generation", "start_pos": 58, "end_pos": 74, "type": "TASK", "confidence": 0.6708237379789352}]}, {"text": "Error reduction denotes the percentage reduction in the number of errors when compared to the number of errors in the ESL data.", "labels": [], "entities": [{"text": "Error reduction", "start_pos": 0, "end_pos": 15, "type": "METRIC", "confidence": 0.9689286351203918}, {"text": "ESL data", "start_pos": 118, "end_pos": 126, "type": "DATASET", "confidence": 0.8925453722476959}]}], "tableCaptions": [{"text": " Table 1: Statistics on articles in the annotated data before and after annotation.", "labels": [], "entities": []}, {"text": " Table 2: Distribution of article errors in the annotated data by error type. Extraneous refers to using a or the where  None (no article) is correct. Confusion is using a instead of the or vice versa.", "labels": [], "entities": [{"text": "Extraneous", "start_pos": 78, "end_pos": 88, "type": "METRIC", "confidence": 0.9633206725120544}]}, {"text": " Table 3: Statistics on article corrections by the original  article (source) and the annotator's choice (label). Each  entry in the table indicates P rob(source = s|label = l)  for each article pair.", "labels": [], "entities": [{"text": "article corrections", "start_pos": 24, "end_pos": 43, "type": "TASK", "confidence": 0.7409888803958893}, {"text": "P rob", "start_pos": 149, "end_pos": 154, "type": "METRIC", "confidence": 0.8340918719768524}]}, {"text": " Table 4: Chinese speakers: Performance of the  T rainClean system (without errors in training) and of  the best classifiers of type T W E. Rows 2-4 show the  performance of the systems trained with error generation  methods described in 5. Error reduction denotes the per- centage reduction in the number of errors when compared  to the number of errors in the ESL data.", "labels": [], "entities": [{"text": "T rainClean system", "start_pos": 48, "end_pos": 66, "type": "DATASET", "confidence": 0.7587133844693502}, {"text": "Error reduction", "start_pos": 241, "end_pos": 256, "type": "METRIC", "confidence": 0.9687419831752777}, {"text": "ESL data", "start_pos": 362, "end_pos": 370, "type": "DATASET", "confidence": 0.927213728427887}]}, {"text": " Table 5: Czech speakers: Performance of the  T rainClean system (without errors in training) and of  the best classifiers of type T W E. Rows 2-4 show the  performance of the systems trained with error generation  methods described in 5. Error reduction denotes the per- centage reduction in the number of errors when compared  to the number of errors in the ESL data.", "labels": [], "entities": [{"text": "T rainClean system", "start_pos": 46, "end_pos": 64, "type": "DATASET", "confidence": 0.7659852703412374}, {"text": "Error reduction", "start_pos": 239, "end_pos": 254, "type": "METRIC", "confidence": 0.9675055742263794}, {"text": "ESL data", "start_pos": 360, "end_pos": 368, "type": "DATASET", "confidence": 0.9164429903030396}]}, {"text": " Table 6: Russian speakers: Performance of the  T rainClean system (without errors in training) and of  the best classifiers of type T W E. Rows 2-4 show the  performance of the systems trained with error generation  methods described in 5. Error reduction denotes the per- centage reduction in the number of errors when compared  to the number of errors in the ESL data.", "labels": [], "entities": [{"text": "T rainClean system", "start_pos": 48, "end_pos": 66, "type": "DATASET", "confidence": 0.7380515535672506}, {"text": "Error reduction", "start_pos": 241, "end_pos": 256, "type": "METRIC", "confidence": 0.9672398567199707}, {"text": "ESL data", "start_pos": 362, "end_pos": 370, "type": "DATASET", "confidence": 0.8977881073951721}]}, {"text": " Table 7: Improvement due to training with errors. For  each source language, the last column of the table shows  the reduction in error rate achieved by the best perform- ing T W E system when compared to the error rate of the  T rainClean system. The error rate for each system is  computed by subtracting the accuracy achieved by the  system, as shown in columns 2 and 3.", "labels": [], "entities": [{"text": "error rate", "start_pos": 131, "end_pos": 141, "type": "METRIC", "confidence": 0.9572633504867554}, {"text": "accuracy", "start_pos": 312, "end_pos": 320, "type": "METRIC", "confidence": 0.99883633852005}]}]}