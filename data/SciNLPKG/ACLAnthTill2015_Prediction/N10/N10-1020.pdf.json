{"title": [], "abstractContent": [{"text": "We propose the first unsupervised approach to the problem of modeling dialogue acts in an open domain.", "labels": [], "entities": []}, {"text": "Trained on a corpus of noisy Twitter conversations, our method discovers dialogue acts by clustering raw utterances.", "labels": [], "entities": []}, {"text": "Because it accounts for the sequential behaviour of these acts, the learned model can provide insight into the shape of communication in anew medium.", "labels": [], "entities": []}, {"text": "We address the challenge of evaluating the emergent model with a qualitative visualization and an intrinsic conversation ordering task.", "labels": [], "entities": []}, {"text": "This work is inspired by a corpus of 1.3 million Twitter conversations , which will be made publicly available.", "labels": [], "entities": []}], "introductionContent": [{"text": "Automatic detection of dialogue structure is an important first step toward deep understanding of human conversations.", "labels": [], "entities": [{"text": "Automatic detection of dialogue structure", "start_pos": 0, "end_pos": 41, "type": "TASK", "confidence": 0.7849650382995605}]}, {"text": "Dialogue acts 1 provide an initial level of structure by annotating utterances with shallow discourse roles such as \"statement\", \"question\" and \"answer\".", "labels": [], "entities": []}, {"text": "These acts are useful in many applications, including conversational agents), dialogue systems (, dialogue summarization (), and flirtation detection (.", "labels": [], "entities": [{"text": "dialogue summarization", "start_pos": 98, "end_pos": 120, "type": "TASK", "confidence": 0.8153688907623291}, {"text": "flirtation detection", "start_pos": 129, "end_pos": 149, "type": "TASK", "confidence": 0.706233873963356}]}, {"text": "Dialogue act tagging has traditionally followed an annotate-train-test paradigm, which begins with the design of annotation guidelines, followed by the collection and labeling of corpora ().", "labels": [], "entities": [{"text": "Dialogue act tagging", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.6769004662831625}]}, {"text": "Only then can one train a tagger to automatically recognize dialogue acts).", "labels": [], "entities": []}, {"text": "This paradigm has been quite successful, but the labeling process is both slow and expensive, limiting the amount of data available for training.", "labels": [], "entities": [{"text": "labeling", "start_pos": 49, "end_pos": 57, "type": "TASK", "confidence": 0.9751082062721252}]}, {"text": "The expense is compounded as we consider new methods of communication, which may require not only new annotations, but new annotation guidelines and new dialogue acts.", "labels": [], "entities": []}, {"text": "This issue becomes more pressing as the Internet continues to expand the number of ways in which we communicate, bringing us e-mail, newsgroups, IRC, forums, blogs, Facebook, Twitter, and whatever is on the horizon.", "labels": [], "entities": []}, {"text": "Previous work has taken a variety of approaches to dialogue act tagging in new media.", "labels": [], "entities": [{"text": "dialogue act tagging", "start_pos": 51, "end_pos": 71, "type": "TASK", "confidence": 0.8245087265968323}]}, {"text": "develop an inventory of dialogue acts specific to e-mail in an office domain.", "labels": [], "entities": []}, {"text": "They design their inventory by inspecting a large corpus of e-mail, and refine it during the manual tagging process.", "labels": [], "entities": []}, {"text": "use semi-supervised learning to transfer dialogue acts from labeled speech corpora to the Internet media of forums and e-mail.", "labels": [], "entities": []}, {"text": "They manually restructure the source act inventories in an attempt to create coarse, domain-independent acts.", "labels": [], "entities": []}, {"text": "Each approach relies on a human designer to inject knowledge into the system through the inventory of available acts.", "labels": [], "entities": []}, {"text": "As an alternative solution for new media, we propose a series of unsupervised conversation models, where the discovery of acts amounts to clustering utterances with similar conversational roles.", "labels": [], "entities": []}, {"text": "This avoids manual construction of an act inventory, and allows the learning algorithm to tell us something about how people converse in anew medium.", "labels": [], "entities": []}, {"text": "There is surprisingly little work in unsupervised dialogue act tagging.", "labels": [], "entities": [{"text": "dialogue act tagging", "start_pos": 50, "end_pos": 70, "type": "TASK", "confidence": 0.6607986489931742}]}, {"text": "propose an unsupervised Hidden Markov Model (HMM) for dialogue structure in a meeting scheduling domain, but model dialogue state at the word level.", "labels": [], "entities": []}, {"text": "use Dirichlet process mixture models to cluster utterances into a flexible number of acts in a travel-planning domain, but do not examine the sequential structure of dialogue.", "labels": [], "entities": []}, {"text": "In contrast to previous work, we address the problem of discovering dialogue acts in an informal, open-topic domain, where an unsupervised learner maybe distracted by strong topic clusters.", "labels": [], "entities": []}, {"text": "We also train and test our models in anew medium: Twitter.", "labels": [], "entities": []}, {"text": "Rather than test against existing dialogue inventories, we evaluate using qualitative visualizations and a novel conversation ordering task, to ensure our models have the opportunity to discover dialogue phenomena unique to this medium.", "labels": [], "entities": [{"text": "conversation ordering", "start_pos": 113, "end_pos": 134, "type": "TASK", "confidence": 0.7422996759414673}]}], "datasetContent": [{"text": "Evaluating automatically discovered dialogue acts is a difficult problem.", "labels": [], "entities": [{"text": "Evaluating automatically discovered dialogue acts", "start_pos": 0, "end_pos": 49, "type": "TASK", "confidence": 0.7743495821952819}]}, {"text": "Unlike previous work, our model automatically discovers an appropriate set of dialogue acts fora new medium; these acts will not necessarily have a close correspondence to dialogue act inventories manually designed for other corpora.", "labels": [], "entities": []}, {"text": "Instead of comparing against human annotations, we present a visualization of the automatically discovered dialogue acts, in addition to measuring the ability of our models to predict post order in unseen conversations.", "labels": [], "entities": []}, {"text": "Ideally we would evaluate performance using an end-use application such as a conversational agent; however as this is outside the scope of this paper, we leave such an evaluation to future work.", "labels": [], "entities": []}, {"text": "For all experiments we train our models on a set of 10,000 randomly sampled conversations with conversation length in posts ranging from 3 to 6.", "labels": [], "entities": []}, {"text": "Note that our implementations can likely scale to larger data by using techniques such as SparseLDA (.", "labels": [], "entities": [{"text": "SparseLDA", "start_pos": 90, "end_pos": 99, "type": "DATASET", "confidence": 0.7349867224693298}]}, {"text": "We limit our vocabulary to the 5,000 most frequent words in the corpus.", "labels": [], "entities": []}, {"text": "When using EM, we train for 100 iterations, evaluating performance on the test set at each iteration, and reporting the maximum.", "labels": [], "entities": []}, {"text": "Smoothing parameters are set using grid search on a development set.", "labels": [], "entities": []}, {"text": "When performing inference with Gibbs Sampling, we use 1,000 samples for burn-in and take 10 samples at a lag of 100.", "labels": [], "entities": []}, {"text": "Although using multiple samples introduces the possibility of poor results due to \"act drift\", we found this not to be a problem in practice; in fact, taking multiple samples substantially improved performance during development.", "labels": [], "entities": []}, {"text": "Recall that we infer hyperparameters using slice sampling.", "labels": [], "entities": []}, {"text": "The concentration parameters chosen in this manner were always sparse (< 1), which produced a moderate improvement over an uninformed prior.", "labels": [], "entities": []}, {"text": "We are quite interested in what our models can tell us about how people converse on Twitter.", "labels": [], "entities": []}, {"text": "To visualize and interpret our competing models, we examined act-emission distributions, posts with highconfidence acts, and act-transition diagrams.", "labels": [], "entities": []}, {"text": "Of the three competing systems, we found the Conversation+Topic model by far the easiest to interpret: the 10-act model has 8 acts that we found intuitive, while the other 2 are used only with low probability.", "labels": [], "entities": []}, {"text": "Conversely, the Conversation model, whether trained by EM or Gibbs sampling, suffered from the inclusion of general terms and from the conflation of topic and dialogue.", "labels": [], "entities": []}, {"text": "For example, the EMtrained conversation model discovered an \"act\" that was clearly a collection of posts about food, with no underlying dialogue theme (see).", "labels": [], "entities": []}, {"text": "In the remainder of this section, we reproduce our visualization for the 10-act Conversation+Topic model.", "labels": [], "entities": []}, {"text": "Word lists summarizing the discovered dialogue acts are shown in.", "labels": [], "entities": []}, {"text": "For each act, the top 40 words are listed in order of decreasing emission probability.", "labels": [], "entities": [{"text": "emission probability", "start_pos": 65, "end_pos": 85, "type": "METRIC", "confidence": 0.8605237305164337}]}, {"text": "An example post, drawn from the set of highest-confidence posts for that act, is also included.", "labels": [], "entities": []}, {"text": "provides a visualization of the matrix of transition probabilities between dialogue acts.", "labels": [], "entities": []}, {"text": "An arrow is drawn from one act to the next if the probability of transition is above 0.15.", "labels": [], "entities": []}, {"text": "Note that a uniform model would transition to each act with probability 0.10.", "labels": [], "entities": []}, {"text": "In both and, we use intuitive names in place of cluster numbers.", "labels": [], "entities": []}, {"text": "These are based on our interpretations of the clusters, and are provided only to benefit the reader when interpreting the transition diagram.", "labels": [], "entities": []}, {"text": "From inspecting the transition diagram), one can see that the model employs three distinct acts to initiate Twitter conversations.", "labels": [], "entities": []}, {"text": "These initial acts are quite different from one another, and lead to After setting this threshold, two Acts were cutoff from the rest of the graph (had no incoming edges), and were therefore removed In some cases, the choice in name is somewhat arbitrary, ie: answer versus response, reaction versus comment.", "labels": [], "entities": []}, {"text": "for word lists and example posts for each act different sets of possible responses.", "labels": [], "entities": []}, {"text": "We discuss each of these in turn.", "labels": [], "entities": []}, {"text": "The Status act appears to represent a post in which the user is broadcasting information about what they are currently doing.", "labels": [], "entities": []}, {"text": "This can be seen by the high amount of probability mass given to words like I and my, in addition to verbs such as go and get, as well as temporal nouns such as today, tomorrow and tonight.", "labels": [], "entities": []}, {"text": "The Reference Broadcast act consists mostly of usernames and urls.", "labels": [], "entities": [{"text": "Reference Broadcast act", "start_pos": 4, "end_pos": 27, "type": "DATASET", "confidence": 0.7498384316762289}]}, {"text": "9 Also prominent is the word rt, which has special significance on Twitter, indicating that the user is re-posting another user's post.", "labels": [], "entities": []}, {"text": "This act represents a user broadcasting an interesting link or quote to their followers.", "labels": [], "entities": []}, {"text": "Also note that this node transitions to the Reaction act with high probability.", "labels": [], "entities": []}, {"text": "Reaction appears to cover excited or appreciative responses to new information, assigning high probability to !, !!, !!!, lol, thanks, and haha.", "labels": [], "entities": []}, {"text": "Finally Question to Followers represents a user asking a question to their followers.", "labels": [], "entities": []}, {"text": "The presence of the question mark and WH question words indicate a question, while words like anyone and know indicate that the user is asking for information or an opinion.", "labels": [], "entities": []}, {"text": "Note that this is distinct from the Question act, which is in response to an initial post.", "labels": [], "entities": []}, {"text": "Another interesting point is the alternation be-Status I . to ! my , is for up in ... and going was today so at go get back day got this am but Im now tomorrow night work tonight off morning home had gon need !!", "labels": [], "entities": []}, {"text": "be just getting I just changed my twitter page bkgornd and now I can't stop looking at it, lol!!", "labels": [], "entities": []}, {"text": "Question to Followers ? you is do Ito -url-what -usr-me , know if anyone why who can \" this or of that how does -: on your are need any rt u should people want get did have would tell anyone using google voice?", "labels": [], "entities": []}, {"text": "just got my invite, should i??", "labels": [], "entities": []}, {"text": "don't know what it is?", "labels": [], "entities": []}, {"text": "-url-for the video and breakdown Reference Broadcast -usr-!", "labels": [], "entities": []}, {"text": "-url-rt : -usr-: -\" my the , is ( you new -?", "labels": [], "entities": []}, {"text": ") this for at in follow of on \u00a1 lol u are twitter your thanks via !!! by :) here 2 please check rt -usr-: -usr-word that mac lip gloss give u lock jaw!", "labels": [], "entities": []}, {"text": "lol Question ? you what ! are is how u do the did your that , lol where why or ??", "labels": [], "entities": []}, {"text": "hey about was have who it in so haha on doing going know good up get like were for there :) can DWL!!", "labels": [], "entities": [{"text": "DWL", "start_pos": 96, "end_pos": 99, "type": "METRIC", "confidence": 0.9707247018814087}]}, {"text": "what song is that??", "labels": [], "entities": []}, {"text": "Reaction ! you I :) !!", "labels": [], "entities": [{"text": "Reaction", "start_pos": 0, "end_pos": 8, "type": "TASK", "confidence": 0.9727082848548889}]}, {"text": ", thanks lol it haha that love so good too your thank is are u !!! was for :d me -usr-\u00a1 hope ? my 3 omg ...", "labels": [], "entities": []}, {"text": "oh great hey awesome -happy now aww sweet!", "labels": [], "entities": [{"text": "aww", "start_pos": 32, "end_pos": 35, "type": "METRIC", "confidence": 0.9570175409317017}]}, {"text": "Comment you I . to , ! do ? it be if me your know have we can get will :) but u that see lol would are so want go letup well need -come ca make or think them why are you in tx and why am I just now finding out about it?!", "labels": [], "entities": []}, {"text": "i'm in dfw, till I get a job.", "labels": [], "entities": []}, {"text": "i'll have to come to Htown soon!", "labels": [], "entities": [{"text": "Htown", "start_pos": 21, "end_pos": 26, "type": "DATASET", "confidence": 0.9478747248649597}]}, {"text": "Answer . I , you it \" that ? is but do was he the of a they if not would know be did or does think ) like ( as have what in are -no them said who say ' my fave was \"keeping on top of other week\" Response . I , it was that lol but is yeah ! haha he my know yes you :) like too did well she so its ...", "labels": [], "entities": []}, {"text": "though do had no -one as im thanks they think would not good oh nah im out in maryland, leaving for tour in a few days.", "labels": [], "entities": []}, {"text": "tween the personal pronouns you and I in the acts due to the focus of conversation and speaker.", "labels": [], "entities": []}, {"text": "The Status act generates the word I with high probability, whereas the likely response state Question generates you, followed by Response which again generates I.", "labels": [], "entities": []}, {"text": "Qualitative evaluations are both time-consuming and subjective.", "labels": [], "entities": []}, {"text": "The above visualization is useful for understanding the Twitter domain, but it is of little use when comparing model variants or selecting parameters.", "labels": [], "entities": []}, {"text": "Therefore, we also propose a novel quantitative evaluation that measures the intrinsic quality of a conversation model by its ability to predict the ordering of posts in a conversation.", "labels": [], "entities": []}, {"text": "This measures the model's predictive power, while requiring no tagged data, and no commitment to an existing tag inventory.", "labels": [], "entities": []}, {"text": "Our test set consists of 1,000 randomly selected conversations not found in the training data.", "labels": [], "entities": []}, {"text": "For each conversation in the test set, we generate all n!", "labels": [], "entities": []}, {"text": "The probability of each permutation is then evaluated as if it were an unseen conversation, using either the forward algorithm (EM) or the Chibb-style estimator (Gibbs).", "labels": [], "entities": []}, {"text": "Following work from the summarization community (), we employ Kendall's \u03c4 to measure the similarity of the max-probability permutation to the original order.", "labels": [], "entities": [{"text": "summarization", "start_pos": 24, "end_pos": 37, "type": "TASK", "confidence": 0.9788396954536438}, {"text": "similarity", "start_pos": 89, "end_pos": 99, "type": "METRIC", "confidence": 0.9556065797805786}]}, {"text": "The Kendall \u03c4 rank correlation coefficient measures the similarity between two permutations based on their agreement in pairwise orderings: where n + is the number of pairs that share the same order in both permutations, and n \u2212 is the number that do not.", "labels": [], "entities": [{"text": "Kendall \u03c4 rank correlation coefficient", "start_pos": 4, "end_pos": 42, "type": "METRIC", "confidence": 0.7043921053409576}]}, {"text": "This statistic ranges between -1 and +1, where -1 indicates inverse order, and +1 indicates identical order.", "labels": [], "entities": []}, {"text": "A value greater than 0 indicates a positive correlation.", "labels": [], "entities": []}, {"text": "Predicting post order on open-domain Twitter conversations is a much more difficult task than on topic-focused news data ().", "labels": [], "entities": [{"text": "Predicting post order on open-domain Twitter conversations", "start_pos": 0, "end_pos": 58, "type": "TASK", "confidence": 0.8709560547556195}]}, {"text": "We found that a simple bigram model baseline does very poorly at predicting order on Twitter, achieving only a weak positive correlation of \u03c4 = 0.0358 on our test data as compared with 0.19-0.74 reported by Barzilay and Lee on news data.", "labels": [], "entities": [{"text": "predicting order", "start_pos": 65, "end_pos": 81, "type": "TASK", "confidence": 0.8015446364879608}]}, {"text": "Note that \u03c4 is not a perfect measure of model quality for conversations; in some cases, multiple order- Figure 5: Performance at conversation ordering task.", "labels": [], "entities": [{"text": "conversation ordering task", "start_pos": 129, "end_pos": 155, "type": "TASK", "confidence": 0.7812840541203817}]}, {"text": "ings of the same set of posts may form a perfectly acceptable conversation.", "labels": [], "entities": []}, {"text": "On the other hand, there are often strong constraints on the type of response we might expect to follow a particular dialogue act; for example, answers follow questions.", "labels": [], "entities": []}, {"text": "We would expect an effective model to use these constraints to predict order.", "labels": [], "entities": []}, {"text": "Performance at the conversation ordering task while varying the number of acts for each model is displayed in.", "labels": [], "entities": [{"text": "conversation ordering task", "start_pos": 19, "end_pos": 45, "type": "TASK", "confidence": 0.8386315306027731}]}, {"text": "In general, we found that using Bayesian inference outperforms EM.", "labels": [], "entities": []}, {"text": "Also note that the Bayesian Conversation model outperforms the Conversation+Topic model at predicting conversation order.", "labels": [], "entities": [{"text": "predicting conversation order", "start_pos": 91, "end_pos": 120, "type": "TASK", "confidence": 0.7536913355191549}]}, {"text": "This is likely because modeling conversation content as a sequence can in some cases help to predict post ordering; for example, adjacent posts are more likely to contain similar content words.", "labels": [], "entities": []}, {"text": "Recall though that we found the Conversation+Topic model to be far more interpretable.", "labels": [], "entities": []}, {"text": "Additionally we compare the likelihood of these models on held out test data in.", "labels": [], "entities": []}, {"text": "Note that the Bayesian methods produce models with much higher likelihood.", "labels": [], "entities": []}, {"text": "For the EM models, likelihood tends to decrease on held out test data as we increase the number of hidden states, due to overfitting.", "labels": [], "entities": [{"text": "likelihood", "start_pos": 19, "end_pos": 29, "type": "METRIC", "confidence": 0.9858866930007935}]}], "tableCaptions": []}