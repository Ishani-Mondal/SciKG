{"title": [{"text": "Learning about Voice Search for Spoken Dialogue Systems", "labels": [], "entities": []}], "abstractContent": [{"text": "Ina Wizard-of-Oz experiment with multiple wizard subjects, each wizard viewed automated speech recognition (ASR) results for utterances whose interpretation is critical to task success: requests for books by title from a library database.", "labels": [], "entities": [{"text": "automated speech recognition (ASR)", "start_pos": 78, "end_pos": 112, "type": "TASK", "confidence": 0.7698342104752859}]}, {"text": "To avoid non-understandings, the wizard directly queried the application database with the ASR hypothesis (voice search).", "labels": [], "entities": []}, {"text": "To learn how to avoid misunderstandings, we investigated how wizards dealt with uncertainty in voice search results.", "labels": [], "entities": []}, {"text": "Wizards were quite successful at selecting the correct title from query results that included a match.", "labels": [], "entities": []}, {"text": "The most successful wizard could also tell when the query results did not contain the requested title.", "labels": [], "entities": []}, {"text": "Our learned models of the best wizard's behavior combine features available to wizards with some that are not, such as recognition confidence and acoustic model scores.", "labels": [], "entities": []}], "introductionContent": [{"text": "Wizard-of-Oz (WOz) studies have long been used for spoken dialogue system design.", "labels": [], "entities": [{"text": "spoken dialogue system design", "start_pos": 51, "end_pos": 80, "type": "TASK", "confidence": 0.7891151905059814}]}, {"text": "Ina relatively new variant, a subject (the wizard) is presented with real or simulated automated speech recognition (ASR) to observe how people deal with incorrect speech recognition output).", "labels": [], "entities": [{"text": "speech recognition (ASR)", "start_pos": 97, "end_pos": 121, "type": "TASK", "confidence": 0.849183452129364}]}, {"text": "In these experiments, when a wizard could not interpret the ASR output (non-understanding), she rarely asked users to repeat themselves.", "labels": [], "entities": [{"text": "ASR", "start_pos": 60, "end_pos": 63, "type": "TASK", "confidence": 0.9034583568572998}]}, {"text": "Instead, the wizard found other ways to continue the task.", "labels": [], "entities": []}, {"text": "This paper describes an experiment that presented wizards with ASR results for utterances whose interpretation is critical to task success: requests for books from a library database, identified by title.", "labels": [], "entities": [{"text": "ASR", "start_pos": 63, "end_pos": 66, "type": "TASK", "confidence": 0.949995219707489}]}, {"text": "To avoid non-understandings, wizards used voice search (: they directly queried the application database with ASR output.", "labels": [], "entities": []}, {"text": "To investigate how to avoid errors in understanding (misunderstandings), we examined how wizards dealt with uncertainty in voice search results.", "labels": [], "entities": []}, {"text": "When the voice search results included the requested title, all seven of our wizards were likely to identify it.", "labels": [], "entities": []}, {"text": "One wizard, however, recognized far better than the others when the voice search results did not contain the requested title.", "labels": [], "entities": []}, {"text": "The experiment employed a novel design that made it possible to include system features in models of wizard behavior.", "labels": [], "entities": []}, {"text": "The principal result is that our learned models of the best wizard's behavior combine features that are available to wizards with some that are not, such as recognition confidence and acoustic model scores.", "labels": [], "entities": []}, {"text": "The next section of the paper motivates our experiment.", "labels": [], "entities": []}, {"text": "Subsequent sections describe related work, the dialogue system and embedded wizard infrastructure, experimental design, learning methods, and results.", "labels": [], "entities": []}, {"text": "We then discuss how to generalize from the results of our study for spoken dialogue system design.", "labels": [], "entities": [{"text": "spoken dialogue system design", "start_pos": 68, "end_pos": 97, "type": "TASK", "confidence": 0.767546072602272}]}, {"text": "We conclude with a summary of results and their implications.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this experiment, a user and a wizard satin separate rooms where they could not overhear one another.", "labels": [], "entities": []}, {"text": "Each had a headset with microphone and a GUI.", "labels": [], "entities": []}, {"text": "Audio input on the wizard's headset was disabled.", "labels": [], "entities": []}, {"text": "When the user requested a title, the ASR hypothesis for the title appeared on the wizard's GUI.", "labels": [], "entities": []}, {"text": "The wizard then selected the ASR hypothesis to execute a voice search against the database.", "labels": [], "entities": [{"text": "ASR", "start_pos": 29, "end_pos": 32, "type": "TASK", "confidence": 0.9130871295928955}]}, {"text": "Given the ASR and the query return, the wizard's task was to guess which candidate in the query return, if any, matched the ASR hypothesis.", "labels": [], "entities": [{"text": "ASR", "start_pos": 10, "end_pos": 13, "type": "TASK", "confidence": 0.5975573062896729}]}, {"text": "In pilot tests, 5%-10% of returns were empty versus none in the experiment.", "labels": [], "entities": []}, {"text": "The distribution of other returns was: 46.7% Singleton, 50.5% AmbiguousList, and 2.8% NoisyList.", "labels": [], "entities": [{"text": "Singleton", "start_pos": 45, "end_pos": 54, "type": "DATASET", "confidence": 0.9649333953857422}, {"text": "AmbiguousList", "start_pos": 62, "end_pos": 75, "type": "METRIC", "confidence": 0.9318997859954834}]}, {"text": "Seven undergraduate computer science majors at Hunter College participated.", "labels": [], "entities": []}, {"text": "Two were nonnative speakers of English (one Spanish, one Romanian).", "labels": [], "entities": []}, {"text": "Each of the possible 21 pairs of students met for five trials.", "labels": [], "entities": []}, {"text": "During each trial, one student served as wizard and the other as user fora session of 20 title cycles.", "labels": [], "entities": []}, {"text": "They immediately reversed roles fora second session, as discussed further below.", "labels": [], "entities": []}, {"text": "The experiment yielded 4172 title cycles rather than the full 4200, because users were permitted to end sessions early.", "labels": [], "entities": []}, {"text": "All titles were selected from the 7500 used to construct the language model.", "labels": [], "entities": []}, {"text": "Each user received a printed list of 20 titles and a brief synopsis of each book.", "labels": [], "entities": []}, {"text": "The acoustic quality of titles read individually from a list is unlikely to approximate that of a patron asking fora specific title.", "labels": [], "entities": []}, {"text": "Therefore, immediately before each session, the user was asked to read a synopsis of each book, and to reorder the titles to reflect some logical grouping, such as genre or topic.", "labels": [], "entities": []}, {"text": "Users requested titles in this new order that they had created.", "labels": [], "entities": []}, {"text": "Participants were encouraged to maximize a session score, with a reward for the experiment winner.", "labels": [], "entities": []}, {"text": "Scoring was designed to foster cooperative strategies.", "labels": [], "entities": []}, {"text": "The wizard scored +1 fora correctly identified title, +0.5 fora thoughtful question, and -1 for an incorrect title.", "labels": [], "entities": []}, {"text": "The user scored +0.5 fora successfully recognized title.", "labels": [], "entities": []}, {"text": "User and wizard traded roles for the second session, to discourage participants from sabotaging the others' scores.", "labels": [], "entities": []}, {"text": "The wizard's GUI presented a real-time live feed of ASR hypotheses, weighted by grayscale to reflect acoustic confidence.", "labels": [], "entities": [{"text": "ASR hypotheses", "start_pos": 52, "end_pos": 66, "type": "TASK", "confidence": 0.8819966614246368}]}, {"text": "Words in each candidate title that matched a word in the ASR appeared darker: dark black for Singleton or AmbiguousList, and medium black for NoisyList.", "labels": [], "entities": [{"text": "ASR", "start_pos": 57, "end_pos": 60, "type": "TASK", "confidence": 0.6278893351554871}]}, {"text": "All other words were in grayscale in proportion to the degree of character overlap.", "labels": [], "entities": []}, {"text": "The wizard queried the database with a recognition hypothesis for one utterance at a time, but could concatenate successive utterances, possibly with some limited editing.", "labels": [], "entities": []}, {"text": "After a query, the wizard's GUI displayed candidate matches in descending order of R/O score.", "labels": [], "entities": [{"text": "R/O score", "start_pos": 83, "end_pos": 92, "type": "METRIC", "confidence": 0.9402543306350708}]}, {"text": "The wizard had four options: make a firm choice of a candidate, make a tentative choice, ask a question, or give up to end the title cycle.", "labels": [], "entities": []}, {"text": "The wizard's GUI showed the successor failure of each title cycle before the next one began.", "labels": [], "entities": []}, {"text": "The user's GUI posted the 20 titles to be read during the session.", "labels": [], "entities": []}, {"text": "On the GUI, the user rated the wizard's title choices as corrector incorrect.", "labels": [], "entities": [{"text": "GUI", "start_pos": 7, "end_pos": 10, "type": "DATASET", "confidence": 0.909544825553894}]}, {"text": "Titles were highlighted green if the user judged a wizard's offered title correct, red if incorrect, yellow if in progress, and not highlighted if still pending.", "labels": [], "entities": []}, {"text": "The user also rated the wizard's questions.", "labels": [], "entities": []}, {"text": "Average elapsed time for each 20-title session was 15.5 minutes.", "labels": [], "entities": []}, {"text": "A questionnaire similar to the type used in PARADISE evaluations () was administered to wizards and users for each pair of sessions.", "labels": [], "entities": []}, {"text": "On a 5-point Likert scale, the average response to the question \"I found the system easy to use this time\" was 4 (sd=0; 4=Agree), indicating that participants were comfortable with the task.", "labels": [], "entities": [{"text": "Agree", "start_pos": 122, "end_pos": 127, "type": "METRIC", "confidence": 0.9904422163963318}]}, {"text": "All other questions received an average score of Neutral (3) or Disagree (2).", "labels": [], "entities": [{"text": "Neutral (3)", "start_pos": 49, "end_pos": 60, "type": "METRIC", "confidence": 0.9510891735553741}, {"text": "Disagree (2)", "start_pos": 64, "end_pos": 76, "type": "METRIC", "confidence": 0.9564773142337799}]}, {"text": "For example, participants were neutral (3) regarding confidence in guessing the correct title, and disagreed (2) that they became more confident as time went on.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1. Raw session score, accuracy, proportion of offered titles that were listed first in the query return, and  frequency of correct non-offers for seven participants.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 29, "end_pos": 37, "type": "METRIC", "confidence": 0.9994800686836243}, {"text": "frequency", "start_pos": 117, "end_pos": 126, "type": "METRIC", "confidence": 0.9883235692977905}]}, {"text": " Table 2. Distribution of correct actions", "labels": [], "entities": [{"text": "Distribution", "start_pos": 10, "end_pos": 22, "type": "TASK", "confidence": 0.9646244049072266}]}, {"text": " Table 3. Learning results for wizards", "labels": [], "entities": []}]}