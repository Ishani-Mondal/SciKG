{"title": [{"text": "Tree Edit Models for Recognizing Textual Entailments, Paraphrases, and Answers to Questions", "labels": [], "entities": [{"text": "Recognizing Textual Entailments, Paraphrases, and Answers to Questions", "start_pos": 21, "end_pos": 91, "type": "TASK", "confidence": 0.7462828695774079}]}], "abstractContent": [{"text": "We describe tree edit models for representing sequences of tree transformations involving complex reordering phenomena and demonstrate that they offer a simple, intuitive, and effective method for modeling pairs of semantically related sentences.", "labels": [], "entities": []}, {"text": "To efficiently extract sequences of edits, we employ a tree kernel as a heuristic in a greedy search routine.", "labels": [], "entities": []}, {"text": "We describe a logistic regression model that uses 33 syntactic features of edit sequences to classify the sentence pairs.", "labels": [], "entities": []}, {"text": "The approach leads to competitive performance in recognizing tex-tual entailment, paraphrase identification, and answer selection for question answering.", "labels": [], "entities": [{"text": "recognizing tex-tual entailment", "start_pos": 49, "end_pos": 80, "type": "TASK", "confidence": 0.7707054615020752}, {"text": "paraphrase identification", "start_pos": 82, "end_pos": 107, "type": "TASK", "confidence": 0.8634931147098541}, {"text": "answer selection", "start_pos": 113, "end_pos": 129, "type": "TASK", "confidence": 0.670636922121048}, {"text": "question answering", "start_pos": 134, "end_pos": 152, "type": "TASK", "confidence": 0.7436122000217438}]}], "introductionContent": [{"text": "Many NLP tasks involve modeling relations between pairs of sentences or short texts in the same language.", "labels": [], "entities": []}, {"text": "Examples include recognizing textual entailment, paraphrase identification, and question answering.", "labels": [], "entities": [{"text": "recognizing textual entailment", "start_pos": 17, "end_pos": 47, "type": "TASK", "confidence": 0.881376306215922}, {"text": "paraphrase identification", "start_pos": 49, "end_pos": 74, "type": "TASK", "confidence": 0.9033571481704712}, {"text": "question answering", "start_pos": 80, "end_pos": 98, "type": "TASK", "confidence": 0.8852691352367401}]}, {"text": "Generic approaches are, of course, desirable; we believe such approaches are also feasible because these tasks exhibit some similar semantic relationships between sentences.", "labels": [], "entities": []}, {"text": "A popular method for such tasks is Tree Edit Distance (TED), which models sentence pairs by finding a low or minimal cost sequence of editing operations to transform a tree representation of one sentence (e.g., a dependency or phrase structure parse tree) into a tree for the other.", "labels": [], "entities": [{"text": "phrase structure parse tree)", "start_pos": 227, "end_pos": 255, "type": "TASK", "confidence": 0.7304673373699189}]}, {"text": "Unlike grammarbased models and shallow-feature discriminative approaches, TED provides an intuitive story for tree pairs where one tree is derived from the other by a sequence of simple transformations.", "labels": [], "entities": []}, {"text": "The available operations in standard TED are the following: insertion of anode, relabeling (i.e., renaming) of anode, and deletion (i.e., removal) of anode.", "labels": [], "entities": []}, {"text": "While the restriction to these three operations permits efficient dynamic programming solutions for finding a minimum-cost edit sequence, certain interesting and prevalent phenomena involving reordering and movement cannot be elegantly captured.", "labels": [], "entities": []}, {"text": "For example, consider the following sentence pair, which is a simplified version of a true entailment (i.e., the premise entails the hypothesis) in the development data for the RTE-3 task.", "labels": [], "entities": [{"text": "RTE-3 task", "start_pos": 177, "end_pos": 187, "type": "TASK", "confidence": 0.6855717301368713}]}, {"text": "Premise: Pierce built the home for his daughter off Rossville Blvd, as he lives nearby.", "labels": [], "entities": [{"text": "Premise", "start_pos": 0, "end_pos": 7, "type": "METRIC", "confidence": 0.9566926956176758}, {"text": "Rossville Blvd", "start_pos": 52, "end_pos": 66, "type": "DATASET", "confidence": 0.9870833456516266}]}, {"text": "Hypothesis: Pierce lives near Rossville Blvd.", "labels": [], "entities": [{"text": "Rossville Blvd", "start_pos": 30, "end_pos": 44, "type": "DATASET", "confidence": 0.9625927805900574}]}, {"text": "Ina plausible dependency tree representation of the premise, live and Rossville Blvd would be in separate subtrees under built.", "labels": [], "entities": [{"text": "Rossville Blvd", "start_pos": 70, "end_pos": 84, "type": "DATASET", "confidence": 0.9687004685401917}]}, {"text": "In the hypothesis tree, however, the corresponding nodes would be in a grandparent-child relationship as part of the same phrase, lives near Rossville Blvd.", "labels": [], "entities": [{"text": "Rossville Blvd", "start_pos": 141, "end_pos": 155, "type": "DATASET", "confidence": 0.9828830361366272}]}, {"text": "In general, one would expect that short transformation sequences to provide good evidence of true entailments.", "labels": [], "entities": []}, {"text": "However, to account for the grandparent-child relationship in the hypothesis, TED would produce a fairly long sequence, relabeling nearby to be near, deleting the two nodes for Rossville Blvd, and then reinserting those nodes under near.", "labels": [], "entities": [{"text": "Rossville Blvd", "start_pos": 177, "end_pos": 191, "type": "DATASET", "confidence": 0.9613019824028015}]}, {"text": "We describe a tree edit approach that allows for more effective modeling of such complex reordering phenomena.", "labels": [], "entities": []}, {"text": "Our approach can find a shorter and more intuitive edit sequence, relabeling nearby to be near, and then moving the whole subtree Rossville Blvd to be a child of near, as shown in.", "labels": [], "entities": [{"text": "Rossville Blvd", "start_pos": 130, "end_pos": 144, "type": "DATASET", "confidence": 0.9874566793441772}]}, {"text": "A model should also be able to consider characteristics of the tree edit sequence other than its overall length (e.g., how many proper nouns were deleted).", "labels": [], "entities": []}, {"text": "Using a classifier with a small number of syntactic  features, our approach allows us to learn-from labeled examples-how different types of edits should affect the model's decisions (e.g., about whether two sentences are paraphrases).", "labels": [], "entities": []}, {"text": "The structure of this paper is as follows.", "labels": [], "entities": []}, {"text": "\u00a72 introduces our model and describes the edit operations that were implemented for our experiments.", "labels": [], "entities": []}, {"text": "\u00a73 details the search-based procedure for extracting edit sequences for pairs of sentences.", "labels": [], "entities": []}, {"text": "\u00a74 describes the classifier for sentence pairs based on features of their corresponding edit sequences.", "labels": [], "entities": []}, {"text": "\u00a75 describes and presents the results of experiments involving recognizing textual entailment (, paraphrase identification (, and an answer selection task for question answering ().", "labels": [], "entities": [{"text": "recognizing textual entailment", "start_pos": 63, "end_pos": 93, "type": "TASK", "confidence": 0.748167077700297}, {"text": "paraphrase identification", "start_pos": 97, "end_pos": 122, "type": "TASK", "confidence": 0.8098068535327911}, {"text": "question answering", "start_pos": 159, "end_pos": 177, "type": "TASK", "confidence": 0.7621416747570038}]}, {"text": "\u00a76 addresses related work, and \u00a77 provides concluding remarks.", "labels": [], "entities": []}], "datasetContent": [{"text": "Experiments were conducted to evaluate tree edit models for three tasks: recognizing textual entailment (, paraphrase identification (, and an answer selection task () for question answering).", "labels": [], "entities": [{"text": "recognizing textual entailment", "start_pos": 73, "end_pos": 103, "type": "TASK", "confidence": 0.8013833363850912}, {"text": "paraphrase identification", "start_pos": 107, "end_pos": 132, "type": "TASK", "confidence": 0.8032219707965851}, {"text": "question answering", "start_pos": 172, "end_pos": 190, "type": "TASK", "confidence": 0.7804601788520813}]}, {"text": "The feature set and first tree edit model were developed for paraphrase, and then applied to the other tasks with very few modifications (all explained below) and no further tuning.", "labels": [], "entities": [{"text": "paraphrase", "start_pos": 61, "end_pos": 71, "type": "TASK", "confidence": 0.9836046099662781}]}], "tableCaptions": [{"text": " Table 4: Paraphrase identification results, with precision  and recall measures for true (positive) paraphrases. Wan  et al. (2006) report precision and recall values with only  two significant digits.", "labels": [], "entities": [{"text": "Paraphrase identification", "start_pos": 10, "end_pos": 35, "type": "TASK", "confidence": 0.9375940263271332}, {"text": "precision", "start_pos": 50, "end_pos": 59, "type": "METRIC", "confidence": 0.9992989301681519}, {"text": "recall", "start_pos": 65, "end_pos": 71, "type": "METRIC", "confidence": 0.9979819059371948}, {"text": "precision", "start_pos": 140, "end_pos": 149, "type": "METRIC", "confidence": 0.9982120990753174}, {"text": "recall", "start_pos": 154, "end_pos": 160, "type": "METRIC", "confidence": 0.9948370456695557}]}, {"text": " Table 5: Results for the task of answer selection for ques- tion answering. +WN denotes use of WordNet features.", "labels": [], "entities": [{"text": "answer selection", "start_pos": 34, "end_pos": 50, "type": "TASK", "confidence": 0.789342999458313}, {"text": "ques- tion answering", "start_pos": 55, "end_pos": 75, "type": "TASK", "confidence": 0.5558070838451385}, {"text": "WordNet", "start_pos": 96, "end_pos": 103, "type": "DATASET", "confidence": 0.9479637742042542}]}]}