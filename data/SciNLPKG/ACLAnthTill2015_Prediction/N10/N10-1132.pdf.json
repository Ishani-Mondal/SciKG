{"title": [{"text": "Interpretation and Transformation for Abstracting Conversations", "labels": [], "entities": []}], "abstractContent": [{"text": "We address the challenge of automatically abstracting conversations such as face-to-face meetings and emails.", "labels": [], "entities": []}, {"text": "We focus hereon the stages of interpretation, where sentences are mapped to a conversation ontology, and transformation, where the summary content is selected.", "labels": [], "entities": []}, {"text": "Our approach is fully developed and tested on meeting speech, and we subsequently explore its application to email conversations .", "labels": [], "entities": []}], "introductionContent": [{"text": "The dominant approach to the challenge of automatic summarization has been extraction, where informative sentences in a document are identified and concatenated to form a condensed version of the original document.", "labels": [], "entities": [{"text": "summarization", "start_pos": 52, "end_pos": 65, "type": "TASK", "confidence": 0.8491003513336182}]}, {"text": "Extractive summarization has been popular at least in part because it is a binary classification task that lends itself well to machine learning techniques, and does not require a natural language generation (NLG) component.", "labels": [], "entities": [{"text": "Extractive summarization", "start_pos": 0, "end_pos": 24, "type": "TASK", "confidence": 0.9038385152816772}, {"text": "natural language generation (NLG)", "start_pos": 180, "end_pos": 213, "type": "TASK", "confidence": 0.8163038988908132}]}, {"text": "There is evidence that human abstractors at times use sentences from the source documents nearly verbatim in their own summaries, justifying this approach to some extent (.", "labels": [], "entities": []}, {"text": "Extrinsic evaluations have also shown that, while extractive summaries maybe less coherent than human abstracts, users still find them to be valuable tools for browsing documents.", "labels": [], "entities": []}, {"text": "However, these same evaluations also indicate that concise abstracts are generally preferred by users and lead to higher objective task scores.", "labels": [], "entities": []}, {"text": "The limitation of a cut-and-paste summary is that the end-user does not know why the selected sentences are important; this can often only be discerned by exploring the context in which each sentence originally appeared.", "labels": [], "entities": []}, {"text": "One possible improvement is to create structured extracts that represent an increased level of abstraction, where selected sentences are grouped according to phenomena such as decisions, action items and problems, thereby giving the user more information on why the sentences are being highlighted.", "labels": [], "entities": []}, {"text": "For example, the sentence Let's go with a simple chip represents a decision.", "labels": [], "entities": []}, {"text": "An even higher level of abstraction can be provided by generating new text that synthesizes or extrapolates on the information contained in the structured summary.", "labels": [], "entities": []}, {"text": "For example, the sentence Sandra and Sue expressed negative opinions about the remote control design can be coupled with extracted sentences containing these negative opinions, forming a hybrid summary.", "labels": [], "entities": []}, {"text": "Our summarization system ultimately performs both types of abstraction, grouping sentences according to various sentence-level phenomena, and generating novel text that describes this content at a higher level.", "labels": [], "entities": []}, {"text": "In this work we describe the first two components of our abstractive summarization system.", "labels": [], "entities": []}, {"text": "In the interpretation stage, sentences are mapped to nodes in a conversation ontology by utilizing classifiers relating to a variety of sentence-level phenomena such as decisions, action items and subjective sentences.", "labels": [], "entities": []}, {"text": "These classifiers achieve high accuracy by using a very large feature set integrating conversation structure, lexical patterns, part-of-speech (POS) tags and character n-grams.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 31, "end_pos": 39, "type": "METRIC", "confidence": 0.996877908706665}]}, {"text": "In the transformation stage, we select the most informative sentences by maximizing a function based on the derived ontology mapping and the coverage of weighted entities mentioned in the conversation.", "labels": [], "entities": []}, {"text": "This transformation component utilizes integer linear programming (ILP) and we compare its performance with several greedy selection algorithms.", "labels": [], "entities": []}, {"text": "We do not discuss the generation component of our summarization system in this paper.", "labels": [], "entities": [{"text": "summarization", "start_pos": 50, "end_pos": 63, "type": "TASK", "confidence": 0.9642470479011536}]}, {"text": "The transformation component is still ex-tractive in nature, but the sentences that are selected in the transformation stage correspond to objects in the ontology and the properties linking them.", "labels": [], "entities": []}, {"text": "Specifically, these are triples of the form < participant, relation, entity > where a participant is a person in the conversation, an entity is an item under discussion, and a relation such as positive opinion or action item links the two.", "labels": [], "entities": []}, {"text": "This intermediate output enables us to create structured extracts as described above, with the triples also acting as input to the downstream NLG component.", "labels": [], "entities": []}, {"text": "We have tested our approach in summarization experiments on both meeting and email conversations, where the quality of a sentence is measured by how effectively it conveys information in a model abstract summary according to human annotators.", "labels": [], "entities": []}, {"text": "On meetings the ILP approach consistently outperforms several greedy summarization methods.", "labels": [], "entities": []}, {"text": "A key finding is that emails exhibit markedly varying conversation structures, and the email threads yielding the best summarization results are those that are structured similarly to meetings.", "labels": [], "entities": []}, {"text": "Other email conversation structures are less amenable to the current treatment and require further investigation and possibly domain adaptation.", "labels": [], "entities": [{"text": "domain adaptation", "start_pos": 126, "end_pos": 143, "type": "TASK", "confidence": 0.6919756829738617}]}], "datasetContent": [{"text": "In this section we describe our conversation corpora, the statistical classifiers used, and the evaluation metrics employed.", "labels": [], "entities": []}, {"text": "We evaluate the various classifiers described in Section 3 using the ROC curve and the area under the curve (AUROC), where a baseline AUROC is 0.5 and an ideal classifier approaches 1.", "labels": [], "entities": [{"text": "ROC", "start_pos": 69, "end_pos": 72, "type": "METRIC", "confidence": 0.9046041965484619}, {"text": "AUROC)", "start_pos": 109, "end_pos": 115, "type": "METRIC", "confidence": 0.9635808169841766}, {"text": "AUROC", "start_pos": 134, "end_pos": 139, "type": "METRIC", "confidence": 0.9711857438087463}]}, {"text": "To evaluate the content selection in the transformation stage, we use weighted recall.This evaluation metric is based on the links between extracted sentences and the human gold-standard abstracts, with the underlying motivation being that sentences with more links to the human abstract are generally more informative, as they provide the content on which an effective abstract summary should be built.", "labels": [], "entities": [{"text": "recall.This", "start_pos": 79, "end_pos": 90, "type": "METRIC", "confidence": 0.9283643364906311}]}, {"text": "If M is the number of sentences selected in the transformation step, O is the total number of sentences in the document, and N is the number of annotators, then Weighted Recall is given by where L(s i , a j ) is the number of links fora sentence s i according to annotator a j . We can compare machine performance with human performance in the following way.", "labels": [], "entities": [{"text": "Weighted Recall", "start_pos": 161, "end_pos": 176, "type": "METRIC", "confidence": 0.6908031404018402}]}, {"text": "For each annotator, we rank their sentences from most-linked to least-linked and select the best sentences until we reach the same word count as our selections.", "labels": [], "entities": []}, {"text": "We then calculate their weighted recall score by using the other N-1 annotations, and then average overall N annotators to get an average human performance.", "labels": [], "entities": [{"text": "recall score", "start_pos": 33, "end_pos": 45, "type": "METRIC", "confidence": 0.9720087945461273}]}, {"text": "We report all transformation scores normalized by human performance for that dataset.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Selected Email Features, Averaged", "labels": [], "entities": []}]}