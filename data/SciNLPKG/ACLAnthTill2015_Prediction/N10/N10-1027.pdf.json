{"title": [], "abstractContent": [{"text": "Language identification is the task of identifying the language a given document is written in.", "labels": [], "entities": [{"text": "Language identification", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.6972183287143707}]}, {"text": "This paper describes a detailed examination of what models perform best under different conditions, based on experiments across three separate datasets and a range of tokeni-sation strategies.", "labels": [], "entities": []}, {"text": "We demonstrate that the task becomes increasingly difficult as we increase the number of languages, reduce the amount of training data and reduce the length of documents.", "labels": [], "entities": []}, {"text": "We also show that it is possible to perform language identification without having to perform explicit character encoding detection.", "labels": [], "entities": [{"text": "language identification", "start_pos": 44, "end_pos": 67, "type": "TASK", "confidence": 0.7633180320262909}, {"text": "character encoding detection", "start_pos": 103, "end_pos": 131, "type": "TASK", "confidence": 0.7883483568827311}]}], "introductionContent": [{"text": "With the growth of the worldwide web, everincreasing numbers of documents have become available, in more and more languages.", "labels": [], "entities": []}, {"text": "This growth has been a double-edged sword, however, in that content in a given language has become more prevalent but increasingly hard to find, due to the web's sheer size and diversity of content.", "labels": [], "entities": []}, {"text": "While the majority of (X)HTML documents declare their character encoding, only a tiny minority specify what language they are written in, despite support for language declaration existing in the various (X)HTML standards.", "labels": [], "entities": []}, {"text": "Additionally, a single encoding can generally be used to render a large number of languages such that the document encoding at best filters out a subset of languages which are incompatible with the given encoding, rather than disambiguates the source language.", "labels": [], "entities": []}, {"text": "Given this, the need for automatic means to determine the source language of web doc-uments is crucial for web aggregators of various types.", "labels": [], "entities": []}, {"text": "There is widespread misconception of language identification being a \"solved task\", generally as a result of isolated experiments over homogeneous datasets with small numbers of languages ( ).", "labels": [], "entities": [{"text": "language identification", "start_pos": 37, "end_pos": 60, "type": "TASK", "confidence": 0.7314810454845428}]}, {"text": "Part of the motivation for this paper is to draw attention to the fact that, as afield, we are still along way off perfect language identification of web documents, as evaluated under realistic conditions.", "labels": [], "entities": [{"text": "language identification of web documents", "start_pos": 123, "end_pos": 163, "type": "TASK", "confidence": 0.8317800402641297}]}, {"text": "In this paper we describe experiments on language identification of web documents, focusing on the broad question of what combination of tokenisation strategy and classification model achieves the best overall performance.", "labels": [], "entities": [{"text": "language identification of web documents", "start_pos": 41, "end_pos": 81, "type": "TASK", "confidence": 0.8379306733608246}]}, {"text": "We additionally evaluate the impact of the volume of training data and the test document length on the accuracy of language identification, and investigate the interaction between character encoding detection and language identification.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 103, "end_pos": 111, "type": "METRIC", "confidence": 0.9983332753181458}, {"text": "language identification", "start_pos": 115, "end_pos": 138, "type": "TASK", "confidence": 0.7705742418766022}, {"text": "character encoding detection", "start_pos": 180, "end_pos": 208, "type": "TASK", "confidence": 0.7961199482282003}, {"text": "language identification", "start_pos": 213, "end_pos": 236, "type": "TASK", "confidence": 0.7724065780639648}]}, {"text": "One assumption we make in this research, following standard assumptions made in the field, is that all documents are monolingual.", "labels": [], "entities": []}, {"text": "This is clearly an unrealistic assumption when dealing with general web documents ( ), and we plan to return to investigate language identification over multilingual documents in future work.", "labels": [], "entities": [{"text": "language identification", "start_pos": 124, "end_pos": 147, "type": "TASK", "confidence": 0.7236686646938324}]}, {"text": "Our contributions in this paper are: the demonstration that language identification is: (a) trivial over datasets with smaller numbers of languages and approximately even amounts of training data per language, but (b) considerably harder over datasets with larger numbers of languages with more skew in the amount of training data per language; bytebased tokenisation without character encoding detection is superior to codepoint-based tokenisation with character encoding detection; and simple cosine similarity-based nearest neighbour classification is equal to or better than models including support vector machines and naive Bayes over the language identification task.", "labels": [], "entities": [{"text": "language identification", "start_pos": 60, "end_pos": 83, "type": "TASK", "confidence": 0.7250914722681046}, {"text": "character encoding detection", "start_pos": 454, "end_pos": 482, "type": "TASK", "confidence": 0.7283759514490763}, {"text": "language identification task", "start_pos": 645, "end_pos": 673, "type": "TASK", "confidence": 0.7864827911059061}]}, {"text": "We also develop datasets to facilitate standardised evaluation of language identification.", "labels": [], "entities": [{"text": "language identification", "start_pos": 66, "end_pos": 89, "type": "TASK", "confidence": 0.6839187294244766}]}], "datasetContent": [{"text": "In the experiments reported in this paper, we employ three novel datasets, with differing properties relevant to language identification research:  EUROGOV: longer documents, all in a single encoding, spread evenly across a relatively small number (10) of Western European languages; this dataset is comparable to the datasets conventionally used in language identification research.", "labels": [], "entities": [{"text": "language identification research", "start_pos": 113, "end_pos": 145, "type": "TASK", "confidence": 0.7821418543656667}, {"text": "EUROGOV", "start_pos": 148, "end_pos": 155, "type": "DATASET", "confidence": 0.696996808052063}, {"text": "language identification research", "start_pos": 350, "end_pos": 382, "type": "TASK", "confidence": 0.7859326004981995}]}, {"text": "As the name would suggest, the documents were sourced from the Euro-GOV document collection, as used in the 2005 Web-CLEF task.", "labels": [], "entities": [{"text": "Euro-GOV document collection", "start_pos": 63, "end_pos": 91, "type": "DATASET", "confidence": 0.9777033726374308}]}, {"text": "TCL: a larger number of languages (60) across a wider range of language families, with shorter documents and a range of character encodings (12).", "labels": [], "entities": [{"text": "TCL", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.5186392664909363}]}, {"text": "The collection was manually sourced by the Thai Computational Linguistics Laboratory (TCL) in 2005 from online news sources.", "labels": [], "entities": [{"text": "Thai Computational Linguistics Laboratory (TCL)", "start_pos": 43, "end_pos": 90, "type": "DATASET", "confidence": 0.8130630339894976}]}, {"text": "WIKIPEDIA: a slightly larger number of languages again (67), a single encoding, and shorter documents; the distribution of languages is intended to approximate that of the actual web.", "labels": [], "entities": [{"text": "WIKIPEDIA", "start_pos": 0, "end_pos": 9, "type": "DATASET", "confidence": 0.8954099416732788}]}, {"text": "This collection was automatically constructed by taking the dumps of all versions of Wikipedia with 1000 or more documents in non-constructed languages, and randomly selecting documents from them in a biaspreserving manner (i.e. preserving the document distribution in the full collection); this is intended to represent the document language bias observed on the web.", "labels": [], "entities": []}, {"text": "All three corpora are available on request.", "labels": [], "entities": []}, {"text": "We outline the characteristics of the three datasets in.", "labels": [], "entities": []}, {"text": "We further detail the language distribution in, using a constant vector of languages for all three datasets, based on the order of languages in the WIKIPEDIA dataset (in descending order of documents per language).", "labels": [], "entities": [{"text": "WIKIPEDIA dataset", "start_pos": 148, "end_pos": 165, "type": "DATASET", "confidence": 0.9760803580284119}]}, {"text": "Of note are the contrasting language distributions between the three datasets, in terms of both the languages represented and the relative skew of documents per language.", "labels": [], "entities": []}, {"text": "In the following sections, we provide details of the corpus compilation and document sampling method for each dataset.", "labels": [], "entities": [{"text": "corpus compilation", "start_pos": 53, "end_pos": 71, "type": "TASK", "confidence": 0.7023982405662537}]}, {"text": "code as the common encoding for all documents.", "labels": [], "entities": []}, {"text": "In practice, character encoding detection is an issue only for TCL, as the other two datasets are in a single encoding.", "labels": [], "entities": [{"text": "character encoding detection", "start_pos": 13, "end_pos": 41, "type": "TASK", "confidence": 0.9262593388557434}]}, {"text": "Where a character encoding was provided fora document in TCL and it was possible to transcode the document to unicode based on that encoding, we used the encoding information.", "labels": [], "entities": []}, {"text": "In cases where a unique encoding was not provided, we used an encoding detection library based on the Mozilla browser.", "labels": [], "entities": [{"text": "encoding detection", "start_pos": 62, "end_pos": 80, "type": "TASK", "confidence": 0.722813606262207}]}, {"text": "Having disambiguated the encoding for each document, we transcoded it into unicode.", "labels": [], "entities": []}, {"text": "We carryout experiments over the cross-product of the following options, as described above: n-gram (\u00d73): 1-gram, 2-gram, 3-gram fora total of 42 distinct classifiers.", "labels": [], "entities": []}, {"text": "Each classifier is run across the 3 datasets (EUROGOV, TCL and WIKIPEDIA) based on 10-fold stratified crossvalidation.", "labels": [], "entities": [{"text": "EUROGOV", "start_pos": 46, "end_pos": 53, "type": "DATASET", "confidence": 0.9430847764015198}, {"text": "TCL", "start_pos": 55, "end_pos": 58, "type": "METRIC", "confidence": 0.7260971665382385}, {"text": "WIKIPEDIA", "start_pos": 63, "end_pos": 72, "type": "DATASET", "confidence": 0.6104409098625183}]}, {"text": "We evaluate the models using micro-averaged precision (P \u00b5 ), recall (R \u00b5 ) and F-score (F \u00b5 ), as well as macro-averaged precision (P M ), recall (R M ) and F-score (F M ).", "labels": [], "entities": [{"text": "micro-averaged precision (P \u00b5 )", "start_pos": 29, "end_pos": 60, "type": "METRIC", "confidence": 0.8742193082968394}, {"text": "recall (R \u00b5 )", "start_pos": 62, "end_pos": 75, "type": "METRIC", "confidence": 0.9526154398918152}, {"text": "F-score (F \u00b5 )", "start_pos": 80, "end_pos": 94, "type": "METRIC", "confidence": 0.9463830351829529}, {"text": "macro-averaged precision (P M )", "start_pos": 107, "end_pos": 138, "type": "METRIC", "confidence": 0.8169057915608088}, {"text": "recall (R M )", "start_pos": 140, "end_pos": 153, "type": "METRIC", "confidence": 0.9244915604591369}, {"text": "F-score (F M )", "start_pos": 158, "end_pos": 172, "type": "METRIC", "confidence": 0.9082628130912781}]}, {"text": "The micro-averaged scores indicate the average performance per document; as we always make a unique prediction per document, the micro-averaged precision, recall and F-score are always identical (as is the classification accuracy).", "labels": [], "entities": [{"text": "precision", "start_pos": 144, "end_pos": 153, "type": "METRIC", "confidence": 0.8103355169296265}, {"text": "recall", "start_pos": 155, "end_pos": 161, "type": "METRIC", "confidence": 0.9969509840011597}, {"text": "F-score", "start_pos": 166, "end_pos": 173, "type": "METRIC", "confidence": 0.9938509464263916}, {"text": "accuracy", "start_pos": 221, "end_pos": 229, "type": "METRIC", "confidence": 0.6731356978416443}]}, {"text": "The macro-averaged scores, on the other hand, indicate the average performance per language.", "labels": [], "entities": []}, {"text": "In each case, we average the precision, recall and F-score across the 10 folds of cross validation.", "labels": [], "entities": [{"text": "precision", "start_pos": 29, "end_pos": 38, "type": "METRIC", "confidence": 0.9997712969779968}, {"text": "recall", "start_pos": 40, "end_pos": 46, "type": "METRIC", "confidence": 0.9993284940719604}, {"text": "F-score", "start_pos": 51, "end_pos": 58, "type": "METRIC", "confidence": 0.9991449117660522}]}, {"text": "As a baseline, we use a majority class, or ZeroR, classifier (ZEROR), which assigns the language with highest prior in the training data to each of the test documents.", "labels": [], "entities": [{"text": "ZeroR, classifier (ZEROR)", "start_pos": 43, "end_pos": 68, "type": "METRIC", "confidence": 0.7530304392178854}]}, {"text": "http://www.csie.ntu.edu.tw/\u02dccjlin/bsvm/ We do not include the results for nearest-prototype classifiers with the OOP distance metric as the results were considerably lower than the other methods.", "labels": [], "entities": []}, {"text": "Note that this means that the averaged FM is not necessarily the harmonic mean of the averaged PM and RM .", "labels": [], "entities": [{"text": "FM", "start_pos": 39, "end_pos": 41, "type": "METRIC", "confidence": 0.9218595027923584}]}], "tableCaptions": [{"text": " Table 1: Summary of the three language identification datasets", "labels": [], "entities": [{"text": "language identification", "start_pos": 31, "end_pos": 54, "type": "TASK", "confidence": 0.7625967562198639}]}, {"text": " Table 2: Results for byte vs. codepoint (bigram) tokeni- sation over EUROGOV", "labels": [], "entities": [{"text": "EUROGOV", "start_pos": 70, "end_pos": 77, "type": "DATASET", "confidence": 0.91853266954422}]}, {"text": " Table 3: Results for byte vs. codepoint (bigram) tokeni- sation over TCL", "labels": [], "entities": [{"text": "TCL", "start_pos": 70, "end_pos": 73, "type": "TASK", "confidence": 0.4507475793361664}]}, {"text": " Table 4: Results for byte vs. codepoint (bigram) tokeni- sation over WIKIPEDIA", "labels": [], "entities": [{"text": "WIKIPEDIA", "start_pos": 70, "end_pos": 79, "type": "DATASET", "confidence": 0.7482203841209412}]}, {"text": " Table 5: Results for different n-gram orders over  WIKIPEDIA", "labels": [], "entities": [{"text": "WIKIPEDIA", "start_pos": 52, "end_pos": 61, "type": "DATASET", "confidence": 0.7453027963638306}]}, {"text": " Table 6: Results for mixed n-grams (1-5) and feature se- lection over WIKIPEDIA", "labels": [], "entities": [{"text": "WIKIPEDIA", "start_pos": 71, "end_pos": 80, "type": "DATASET", "confidence": 0.8438796997070312}]}]}