{"title": [{"text": "Inducing Synchronous Grammars with Slice Sampling", "labels": [], "entities": [{"text": "Inducing Synchronous Grammars", "start_pos": 0, "end_pos": 29, "type": "TASK", "confidence": 0.7927340070406595}]}], "abstractContent": [{"text": "This paper describes an efficient sampler for synchronous grammar induction under a non-parametric Bayesian prior.", "labels": [], "entities": [{"text": "synchronous grammar induction", "start_pos": 46, "end_pos": 75, "type": "TASK", "confidence": 0.7147405942281088}]}, {"text": "Inspired by ideas from slice sampling, our sampler is able to draw samples from the posterior distributions of models for which the standard dynamic pro-graming based sampler proves intractable on non-trivial corpora.", "labels": [], "entities": [{"text": "slice sampling", "start_pos": 23, "end_pos": 37, "type": "TASK", "confidence": 0.7581786811351776}]}, {"text": "We compare our sampler to a previously proposed Gibbs sampler and demonstrate strong improvements in terms of both training log-likelihood and performance on an end-to-end translation evaluation.", "labels": [], "entities": []}], "introductionContent": [{"text": "Intractable optimisation algorithms abound in much of the recent work in Natural Language Processing.", "labels": [], "entities": []}, {"text": "In fact, there is an increasing acceptance that solutions to many of the great challenges of NLP (e.g. machine translation, summarisation, question answering) will rest on the quality of approximate inference.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 103, "end_pos": 122, "type": "TASK", "confidence": 0.7700991034507751}, {"text": "summarisation", "start_pos": 124, "end_pos": 137, "type": "TASK", "confidence": 0.9679692387580872}, {"text": "question answering)", "start_pos": 139, "end_pos": 158, "type": "TASK", "confidence": 0.8736111919085184}]}, {"text": "In this work we tackle this problem in the context of inducing synchronous grammars fora machine translation system.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 89, "end_pos": 108, "type": "TASK", "confidence": 0.6991977691650391}]}, {"text": "We concern ourselves with the lack of a principled, and scalable, algorithm for learning asynchronous context free grammar (SCFG) from sentence-aligned parallel corpora.", "labels": [], "entities": [{"text": "learning asynchronous context free grammar (SCFG) from sentence-aligned parallel corpora", "start_pos": 80, "end_pos": 168, "type": "TASK", "confidence": 0.7544549355904261}]}, {"text": "The predominant approach for learning phrasebased translation models (both finite state or synchronous grammar based) uses a cascade of heuristics beginning with predicted word alignments and producing a weighted set of translation rules (.", "labels": [], "entities": [{"text": "learning phrasebased translation", "start_pos": 29, "end_pos": 61, "type": "TASK", "confidence": 0.5244440833727518}]}, {"text": "Alternative approaches avoid such heuristics, instead learning structured alignment models directly from sentence aligned data (e.g.,).", "labels": [], "entities": []}, {"text": "Although these models are theoretically attractive, inference is intractable (at least O(|f | 3 |e| 3 )).", "labels": [], "entities": []}, {"text": "The efficacy of direct estimation of structured alignment models therefore rests on the approximations used to make inference practicable -typically heuristic constraints or Gibbs sampling.", "labels": [], "entities": []}, {"text": "In this work we show that naive Gibbs sampling (specifically,) is ineffectual for inference and reliant on a high quality initialisation, mixing very slowly and being easily caught in modes.", "labels": [], "entities": []}, {"text": "Instead, blocked sampling over sentence pairs allows much faster mixing, but done in the obvious way (following) would incur a O(|f | 3 |e| 3 ) time complexity.", "labels": [], "entities": [{"text": "O", "start_pos": 127, "end_pos": 128, "type": "METRIC", "confidence": 0.992377519607544}]}, {"text": "Here we draw inspiration from the work of Van on inference in infinite hidden Markov models to develop a novel algorithm for efficient sampling from a SCFG.", "labels": [], "entities": []}, {"text": "We develop an auxiliary variable 'slice' sampler which can dramatically reduce inference complexity, and thereby make blocked sampling practicable on real translation corpora.", "labels": [], "entities": []}, {"text": "Our evaluation demonstrates that our algorithm mixes more quickly than the local Gibbs sampler, and produces translation models which achieve state-ofthe-art BLEU scores without using GIZA++ or symmetrisation heuristics for initialisation.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 158, "end_pos": 162, "type": "METRIC", "confidence": 0.9987585544586182}]}, {"text": "We adopt the generative model of which creates a parallel sentence pair by a sequence (derivation) of SCFG productions d = (r 1 , r 2 , ..., r n ).", "labels": [], "entities": []}, {"text": "The tokens in each language can be read off the leaves of the derivation tree while their order is defined hierarchically by the productions in use.", "labels": [], "entities": []}, {"text": "The probability of a derivation is defined as p(d|\u03b8) = r\u2208d \u03b8 r where \u03b8 are the model parameters which are drawn from a Bayesian prior.", "labels": [], "entities": []}, {"text": "We deviate from that models definition of the prior over phrasal translations, instead adopting the hierarchical Dirichlet process prior from, which incorporates IBM Model 1.", "labels": [], "entities": [{"text": "IBM Model 1", "start_pos": 162, "end_pos": 173, "type": "DATASET", "confidence": 0.9173996448516846}]}, {"text": "describe a blocked sampler following which uses the Metropolis-Hastings algorithm to correct proposal samples drawn from an approximating SCFG, however this is discounted as impractical due to the O(|f | 3 |e| 3 ) complexity.", "labels": [], "entities": [{"text": "O", "start_pos": 197, "end_pos": 198, "type": "METRIC", "confidence": 0.9876536726951599}]}, {"text": "Instead a Gibbs sampler is used which samples local updates to the derivation structure of each training instance.", "labels": [], "entities": []}, {"text": "This avoids the dynamic program of the blocked sampler but at the expense of considerably slower mixing.", "labels": [], "entities": []}, {"text": "Recently proposed an auxialliary variable sampler, possibly complementary to ours, which was also evaluated on synchronous parsing.", "labels": [], "entities": []}, {"text": "Rather than slice sampling derivations in a collapsed Bayesian model, this model employed a secondary proposal model (IBM Models) and sampled expectations overrule parameters.", "labels": [], "entities": []}], "datasetContent": [{"text": "In the following experiments we compare the slice sampler and the Gibbs sampler (, in terms of mixing and translation quality.", "labels": [], "entities": []}, {"text": "We measure mixing in terms of training log-likelihood (LLH) after a fixed number of sampling iterations.", "labels": [], "entities": [{"text": "training log-likelihood (LLH)", "start_pos": 30, "end_pos": 59, "type": "METRIC", "confidence": 0.8878350973129272}]}, {"text": "Translations are produced using Moses (, initialised with the word alignments from the final sample, and are evaluated using BLEU().", "labels": [], "entities": [{"text": "BLEU", "start_pos": 125, "end_pos": 129, "type": "METRIC", "confidence": 0.9983839988708496}]}, {"text": "The slice sampled models are restricted to learning binary branching one-to-one (or null) alignments, 8 while no restriction is placed on the Gibbs sampler (both use the same model, so have comparable LLH).", "labels": [], "entities": []}, {"text": "Of particular interest is how the different samplers perform given initialisations of varying quality.", "labels": [], "entities": []}, {"text": "We evaluate three initialisers: M4: the symmetrised output of GIZA++ factorised into ITG form (as used in); M1: the output of a heavily pruned ITG parser using the IBM Model 1 prior for the rule probabilities; and LB: left-branching monotone derivations.", "labels": [], "entities": []}, {"text": "We experiment with the Chinese\u2192English translation task from IWSLT, as used in.", "labels": [], "entities": [{"text": "Chinese\u2192English translation", "start_pos": 23, "end_pos": 50, "type": "TASK", "confidence": 0.5751944333314896}, {"text": "IWSLT", "start_pos": 61, "end_pos": 66, "type": "DATASET", "confidence": 0.8159891366958618}]}, {"text": "11 shows LLH curves for the samplers initialised with the M1 and LB derivations, plus the curve for Gibbs sampler with the M4 initialiser.", "labels": [], "entities": []}, {"text": "12 gives BLEU scores on Test-05 for phrasebased translation models built from the 1500 th sample for the various models along with the average time per sample and their final log-likelihood.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 9, "end_pos": 13, "type": "METRIC", "confidence": 0.9992207288742065}, {"text": "Test-05", "start_pos": 24, "end_pos": 31, "type": "DATASET", "confidence": 0.9740964770317078}, {"text": "phrasebased translation", "start_pos": 36, "end_pos": 59, "type": "TASK", "confidence": 0.700277715921402}]}], "tableCaptions": [{"text": " Table 1: IWSLT Chinese to English translation.", "labels": [], "entities": [{"text": "IWSLT Chinese to English translation", "start_pos": 10, "end_pos": 46, "type": "TASK", "confidence": 0.6967989087104798}]}]}