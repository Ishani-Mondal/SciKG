{"title": [{"text": "Improving Phrase-Based Translation with Prototypes of Short Phrases", "labels": [], "entities": [{"text": "Improving Phrase-Based Translation", "start_pos": 0, "end_pos": 34, "type": "TASK", "confidence": 0.9053096969922384}]}], "abstractContent": [{"text": "We investigate methods of generating additional bilingual phrase pairs fora phrase-based decoder by translating short sequences of source text.", "labels": [], "entities": []}, {"text": "Because our translation task is more constrained, we can use a model that employs more linguistically rich features than a traditional decoder.", "labels": [], "entities": [{"text": "translation task", "start_pos": 12, "end_pos": 28, "type": "TASK", "confidence": 0.8853685557842255}]}, {"text": "We have implemented an example of this approach.", "labels": [], "entities": []}, {"text": "Experimental results suggest that the phrase pairs produced by our method are useful to the decoder, and lead to improved sentence translations.", "labels": [], "entities": [{"text": "sentence translations", "start_pos": 122, "end_pos": 143, "type": "TASK", "confidence": 0.6984998881816864}]}], "introductionContent": [{"text": "Recently, there have been a number of successful attempts at improving phrase-based statistical machine translation by exploiting linguistic knowledge such as morphology, part-of-speech tags, and syntax.", "labels": [], "entities": [{"text": "phrase-based statistical machine translation", "start_pos": 71, "end_pos": 115, "type": "TASK", "confidence": 0.6467570960521698}]}, {"text": "Many translation models use such knowledge before decoding () and during decoding), but they are limited to simpler features for practical reasons, often restricted to conditioning left-toright on the target sentence.", "labels": [], "entities": []}, {"text": "Traditionally, n-best rerankers) have applied expensive analysis after the translation process, on both the source and target side, though they suffer from being limited to whatever is on the n-best list.", "labels": [], "entities": [{"text": "translation process", "start_pos": 75, "end_pos": 94, "type": "TASK", "confidence": 0.8902190029621124}]}, {"text": "We argue that it can be desirable to pre-translate parts of the source text before sentence-level decoding begins, using a richer model that would typically be out of reach during sentence-level decoding.", "labels": [], "entities": []}, {"text": "In this paper, we describe a particular method of generating additional bilingual phrase pairs fora new source text, using what we call phrase prototypes, which are are learned from bilingual training data.", "labels": [], "entities": []}, {"text": "Our goal is to generate improved translations of relatively short phrase pairs to provide the SMT decoder with better phrasal choices.", "labels": [], "entities": [{"text": "SMT decoder", "start_pos": 94, "end_pos": 105, "type": "TASK", "confidence": 0.9188894331455231}]}, {"text": "We validate the idea through experiments on Arabic-English translation.", "labels": [], "entities": [{"text": "Arabic-English translation", "start_pos": 44, "end_pos": 70, "type": "TASK", "confidence": 0.7102259695529938}]}, {"text": "Our method produces a 1.3 BLEU score increase (3.3% relative) on a test set.", "labels": [], "entities": [{"text": "BLEU score increase", "start_pos": 26, "end_pos": 45, "type": "METRIC", "confidence": 0.9843859275182089}]}], "datasetContent": [{"text": "The Linguistic Data Consortium Arabic-English corpus2 3 is used to train the baseline MT system (34K sentences, about one million words), and to learn phrase prototypes.", "labels": [], "entities": [{"text": "Linguistic Data Consortium Arabic-English corpus2 3", "start_pos": 4, "end_pos": 55, "type": "DATASET", "confidence": 0.8413137793540955}, {"text": "MT", "start_pos": 86, "end_pos": 88, "type": "TASK", "confidence": 0.9404922723770142}]}, {"text": "The LDC multi-translation Arabic-English corpus (NIST2003) 4 is used for tuning and testing; the tuning set consists of the first 500 sentences, and the test set consists of the next 500 sentences.", "labels": [], "entities": [{"text": "LDC multi-translation Arabic-English corpus (NIST2003) 4", "start_pos": 4, "end_pos": 60, "type": "DATASET", "confidence": 0.8066812679171562}]}, {"text": "The language model is a 4-gram model built from the English side of the parallel corpus, plus the English side of the wmt07 GermanEnglish and French-English news commentary data.", "labels": [], "entities": [{"text": "wmt07 GermanEnglish and French-English news commentary data", "start_pos": 118, "end_pos": 177, "type": "DATASET", "confidence": 0.8266451273645673}]}, {"text": "The baseline translation system is Moses ( , with the msd-bidirectional-fe reordering model.", "labels": [], "entities": []}, {"text": "Evaluation is done using the BLEU () metric with four references.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 29, "end_pos": 33, "type": "METRIC", "confidence": 0.9984574317932129}]}, {"text": "All text is lowercased before evaluation; recasing is not used.", "labels": [], "entities": []}, {"text": "We use the Stanford Arabic POS Tagging system, based on ( . The word-to-word dictionary that is used in the phrase generation step of our method is extracted from the highest-scoring translations for each source word in the baseline phrase table.", "labels": [], "entities": [{"text": "POS Tagging", "start_pos": 27, "end_pos": 38, "type": "TASK", "confidence": 0.7477080523967743}, {"text": "phrase generation", "start_pos": 108, "end_pos": 125, "type": "TASK", "confidence": 0.7639729082584381}]}, {"text": "For some closedclass words, we use a small, manually constructed dictionary to reduce the noise in the phrase table that exists for very common words.", "labels": [], "entities": []}, {"text": "We use this in place of a stand-alone dictionary to reduce the need for additional resources.", "labels": [], "entities": []}, {"text": "To seethe effect on the BLEU score of the resulting sentence-level translation, we vary the amount of bilingual data used to build the phrase prototypes.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 24, "end_pos": 34, "type": "METRIC", "confidence": 0.9768553376197815}]}, {"text": "(approximately) no difference between building the generated phrase using the baseline phrase table, or using our generated phrase pair directly.", "labels": [], "entities": []}, {"text": "As we increase the amount of training data, we expect that the phrase prototype extraction algorithm will observe more phrase prototypes.", "labels": [], "entities": [{"text": "phrase prototype extraction", "start_pos": 63, "end_pos": 90, "type": "TASK", "confidence": 0.7363812327384949}]}, {"text": "This will cause it to generate more phrase pairs, introducing both more noise and more good phrases into the phrase table.", "labels": [], "entities": []}, {"text": "Because quite a few phrase prototypes are builtin any case, we require that each is seen at least three times before we use it to generate phrases.", "labels": [], "entities": []}, {"text": "Phrase prototypes seen fewer times than this are discarded before phrase generation begins.", "labels": [], "entities": [{"text": "phrase generation", "start_pos": 66, "end_pos": 83, "type": "TASK", "confidence": 0.7596913874149323}]}, {"text": "Varying this minimum support parameter does not affect the results noticeably.", "labels": [], "entities": []}, {"text": "The results on the tuning set are seen in.", "labels": [], "entities": []}, {"text": "The BLEU score on the tuning set generally improves as the amount of bilingual training data is increased, even as the percentage of generated phrases approaches 100%.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 4, "end_pos": 8, "type": "METRIC", "confidence": 0.9989284873008728}]}, {"text": "Manual inspection of the phrase pairs reveals that many are badly formed; this suggests that the language model is doing its job in filtering out disfluent phrases.", "labels": [], "entities": []}, {"text": "Using the first 5,000 bilingual training sentences to train our model, we compare our method to the baseline moses system.", "labels": [], "entities": []}, {"text": "Each system was tuned via MERT before running it on the test set.", "labels": [], "entities": [{"text": "MERT", "start_pos": 26, "end_pos": 30, "type": "DATASET", "confidence": 0.5992407202720642}]}, {"text": "The tuned baseline system scores 38.45.", "labels": [], "entities": []}, {"text": "Including our generated phrases improves this by 1.3 points to 39.75.", "labels": [], "entities": []}, {"text": "This is a slightly smaller gain than exists in the tuning set experiment, due in part that we did not run MERT for experiment shown in.", "labels": [], "entities": [{"text": "MERT", "start_pos": 106, "end_pos": 110, "type": "METRIC", "confidence": 0.8888098001480103}]}], "tableCaptions": []}