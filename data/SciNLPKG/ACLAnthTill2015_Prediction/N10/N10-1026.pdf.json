{"title": [{"text": "Improved Extraction Assessment through Better Language Models", "labels": [], "entities": [{"text": "Improved Extraction Assessment", "start_pos": 0, "end_pos": 30, "type": "TASK", "confidence": 0.9195292393366495}]}], "abstractContent": [{"text": "A variety of information extraction techniques rely on the fact that instances of the same relation are \"distributionally similar,\" in that they tend to appear in similar textual contexts.", "labels": [], "entities": [{"text": "information extraction", "start_pos": 13, "end_pos": 35, "type": "TASK", "confidence": 0.8238882720470428}]}, {"text": "We demonstrate that extraction accuracy depends heavily on the accuracy of the language model utilized to estimate distribu-tional similarity.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 31, "end_pos": 39, "type": "METRIC", "confidence": 0.9487944841384888}, {"text": "accuracy", "start_pos": 63, "end_pos": 71, "type": "METRIC", "confidence": 0.9978814721107483}]}, {"text": "An unsupervised model selection technique based on this observation is shown to reduce extraction and type-checking error by 26% over previous results, in experiments with Hidden Markov Models.", "labels": [], "entities": [{"text": "error", "start_pos": 116, "end_pos": 121, "type": "METRIC", "confidence": 0.7909528017044067}]}, {"text": "The results suggest that optimizing statistical language models over unlabeled data is a promising direction for improving weakly supervised and unsupervised information extraction.", "labels": [], "entities": [{"text": "information extraction", "start_pos": 158, "end_pos": 180, "type": "TASK", "confidence": 0.7420397251844406}]}], "introductionContent": [{"text": "Many weakly supervised and unsupervised information extraction techniques assess the correctness of extractions using the distributional hypothesis-the notion that words with similar meanings tend to occur in similar contexts.", "labels": [], "entities": [{"text": "information extraction", "start_pos": 40, "end_pos": 62, "type": "TASK", "confidence": 0.7439883947372437}]}, {"text": "A candidate extraction of a relation is deemed more likely to be correct when it appears in contexts similar to those of \"seed\" instances of the relation, where the seeds maybe specified by hand), taken from an existing, incomplete knowledge base (, or obtained in an unsupervised manner using a generic extractor (.", "labels": [], "entities": []}, {"text": "We refer to this technique as Assessment by Distributional Similarity (ADS).", "labels": [], "entities": [{"text": "Assessment by Distributional Similarity (ADS)", "start_pos": 30, "end_pos": 75, "type": "TASK", "confidence": 0.6553785375186375}]}, {"text": "Typically, distributional similarity is computed by comparing co-occurrence counts of extractions and seeds with various contexts found in the corpus.", "labels": [], "entities": []}, {"text": "Statistical Language Models (SLMs) include methods for more accurately estimating co-occurrence probabilities via back-off, smoothing, and clustering techniques (e.g. ().", "labels": [], "entities": [{"text": "Statistical Language Models (SLMs)", "start_pos": 0, "end_pos": 34, "type": "TASK", "confidence": 0.6684078673521677}]}, {"text": "Because SLMs can be trained from only unlabeled text, they can be applied for ADS even when the relations of interest are not specified in advance (.", "labels": [], "entities": [{"text": "ADS", "start_pos": 78, "end_pos": 81, "type": "TASK", "confidence": 0.8235349655151367}]}, {"text": "Unlabeled text is abundant in large corpora like the Web, making nearly-ceaseless automated optimization of SLMs possible.", "labels": [], "entities": []}, {"text": "But how fruitful is such an effort likely to be-to what extent does optimizing a language model over a fixed corpus lead to improvements in assessment accuracy?", "labels": [], "entities": [{"text": "accuracy", "start_pos": 151, "end_pos": 159, "type": "METRIC", "confidence": 0.9835557341575623}]}, {"text": "In this paper, we show that an ADS technique based on SLMs is improved substantially when the language model it employs becomes more accurate.", "labels": [], "entities": []}, {"text": "Ina large-scale set of experiments, we quantify how language model perplexity correlates with ADS performance over multiple data sets and SLM techniques.", "labels": [], "entities": []}, {"text": "The experiments show that accuracy over unlabeled data can be used for selecting among SLMs-for an ADS approach utilizing Hidden Markov Models, this results in an average error reduction of 26% over previous results in extraction and type-checking tasks.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 26, "end_pos": 34, "type": "METRIC", "confidence": 0.9967909455299377}, {"text": "error reduction", "start_pos": 171, "end_pos": 186, "type": "METRIC", "confidence": 0.9474721848964691}]}], "datasetContent": [{"text": "In this section, we present experiments showing that SLM accuracy correlates strongly with ADS performance.", "labels": [], "entities": [{"text": "SLM", "start_pos": 53, "end_pos": 56, "type": "TASK", "confidence": 0.9777456521987915}, {"text": "accuracy", "start_pos": 57, "end_pos": 65, "type": "METRIC", "confidence": 0.8609200716018677}]}, {"text": "We also show that SLM performance can be used for model selection, leading to an ADS technique that outperforms previous results.", "labels": [], "entities": [{"text": "SLM", "start_pos": 18, "end_pos": 21, "type": "TASK", "confidence": 0.9738160371780396}, {"text": "model selection", "start_pos": 50, "end_pos": 65, "type": "TASK", "confidence": 0.7363069355487823}]}, {"text": "We experiment with a wide range of n-gram and HMM models.", "labels": [], "entities": []}, {"text": "The n-gram models are trained using the SRILM toolkit variety of HMM configurations over a large corpus requires a scalable training architecture.", "labels": [], "entities": []}, {"text": "We constructed a parallel HMM codebase using the Message Passing Interface (MPI), and trained the models on a supercomputing cluster.", "labels": [], "entities": [{"text": "Message Passing Interface (MPI)", "start_pos": 49, "end_pos": 80, "type": "TASK", "confidence": 0.8116952528556188}]}, {"text": "All language models were trained on a corpus of 2.8M sentences of Web text (about 60 million tokens).", "labels": [], "entities": []}, {"text": "SLM performance is measured using the standard perplexity metric, and assessment accuracy is measured using area under the precision-recall curve (AUC), a standard metric for ranked lists of extractions.", "labels": [], "entities": [{"text": "SLM", "start_pos": 0, "end_pos": 3, "type": "TASK", "confidence": 0.9698485732078552}, {"text": "accuracy", "start_pos": 81, "end_pos": 89, "type": "METRIC", "confidence": 0.986171543598175}, {"text": "precision-recall curve (AUC)", "start_pos": 123, "end_pos": 151, "type": "METRIC", "confidence": 0.9766026258468627}]}, {"text": "We evaluated performance on three distinct data sets.", "labels": [], "entities": []}, {"text": "The first two data sets evaluate ADS for unsupervised information extraction, and were taken from (.", "labels": [], "entities": [{"text": "information extraction", "start_pos": 54, "end_pos": 76, "type": "TASK", "confidence": 0.7337358146905899}]}, {"text": "The first, Unary, was an extraction task for unary relations (Company, Country, Language, Film) and the second, Binary, was a type-checking task for binary relations (Conquered, Founded, Headquartered, Merged).", "labels": [], "entities": []}, {"text": "The 10 most frequent extractions served as bootstrapped seeds.", "labels": [], "entities": []}, {"text": "The two test sets contained 361 and 265 extractions, respectively.", "labels": [], "entities": []}, {"text": "The third data set, Wikipedia, evaluates ADS on weaklysupervised extraction, using seeds and extractions taken from Wikipedia 'List of' pages (.", "labels": [], "entities": [{"text": "Wikipedia", "start_pos": 20, "end_pos": 29, "type": "DATASET", "confidence": 0.9836050868034363}, {"text": "ADS", "start_pos": 41, "end_pos": 44, "type": "TASK", "confidence": 0.7253755927085876}]}, {"text": "Seed sets of various sizes were randomly selected from each list, and we present results averaged over 10 random samplings.", "labels": [], "entities": []}, {"text": "Other members of the seed list were added to a test set as correct extractions, and elements from other lists were added as errors.", "labels": [], "entities": []}, {"text": "The data set included 2264 extractions across 36 unary relations, including Composers and US Internet Companies.", "labels": [], "entities": [{"text": "US Internet Companies", "start_pos": 90, "end_pos": 111, "type": "DATASET", "confidence": 0.8303511142730713}]}], "tableCaptions": [{"text": " Table 1: Pearson Correlation value for extraction perfor- mance (in AUC) and SLM performance (in perplexity).  Extraction accuracy increases as perplexity decreases,  with an average correlation coefficient of -0.742. \"HMM  k-T \" denotes an HMM model of order k, with T states.", "labels": [], "entities": [{"text": "Pearson Correlation", "start_pos": 10, "end_pos": 29, "type": "METRIC", "confidence": 0.7622957825660706}, {"text": "Extraction", "start_pos": 112, "end_pos": 122, "type": "METRIC", "confidence": 0.9562719464302063}, {"text": "accuracy", "start_pos": 123, "end_pos": 131, "type": "METRIC", "confidence": 0.6559948921203613}]}]}