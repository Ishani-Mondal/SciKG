{"title": [{"text": "Cross-lingual Induction of Selectional Preferences with Bilingual Vector Spaces", "labels": [], "entities": [{"text": "Cross-lingual Induction of Selectional Preferences", "start_pos": 0, "end_pos": 50, "type": "TASK", "confidence": 0.8013542056083679}]}], "abstractContent": [{"text": "We describe a cross-lingual method for the induction of selectional preferences for resource-poor languages, where no accurate monolin-gual models are available.", "labels": [], "entities": []}, {"text": "The method uses bilingual vector spaces to \"translate\" foreign language predicate-argument structures into a resource-rich language like English.", "labels": [], "entities": []}, {"text": "The only prerequisite for constructing the bilingual vector space is a large unparsed corpus in the resource-poor language, although the model can profit from (even noisy) syntactic knowledge.", "labels": [], "entities": []}, {"text": "Our experiments show that the cross-lingual predictions correlate well with human ratings, clearly outperforming monolin-gual baseline models.", "labels": [], "entities": []}], "introductionContent": [{"text": "Selectional preferences capture the empirical observation that not all words are equally good arguments to a given verb in a particular argument position.", "labels": [], "entities": []}, {"text": "For instance, the subjects of the English verb to shoot are generally people, while the direct objects can be people or animals.", "labels": [], "entities": []}, {"text": "This is reflected in speakers' intuitions.", "labels": [], "entities": []}, {"text": "shows that the combination the hunter shot the deer is judged more plausible than the deer shot the hunter.", "labels": [], "entities": []}, {"text": "Selectional preferences do not only play an important role inhuman sentence processing), but are also helpful for NLP tasks like word sense disambiguation and semantic role labeling ().", "labels": [], "entities": [{"text": "sentence processing", "start_pos": 67, "end_pos": 86, "type": "TASK", "confidence": 0.7391566336154938}, {"text": "word sense disambiguation", "start_pos": 129, "end_pos": 154, "type": "TASK", "confidence": 0.6940517822901408}, {"text": "semantic role labeling", "start_pos": 159, "end_pos": 181, "type": "TASK", "confidence": 0.6386862993240356}]}, {"text": "Computational models of selectional preferences predict such plausibilities for triples of a predicate p, an argument position a, and ahead word h, such as Predicate Relation Noun Plausibility shoot subject hunter 6.9 shoot object hunter 2.8 shoot subject deer 1.0 shoot object deer 6.4: Predicate-relation-noun triples with human plausibility judgments on a 7-point scale (shoot,object,hunter).", "labels": [], "entities": [{"text": "Predicate Relation Noun Plausibility shoot subject hunter 6.9 shoot object hunter 2.8 shoot subject deer 1.0 shoot object deer 6.4", "start_pos": 156, "end_pos": 286, "type": "TASK", "confidence": 0.8114160433411598}]}, {"text": "All recent models take a twostep approach: (1), they extract all triples (p, a, h) from a large corpus; (2), they apply some type of generalization to make predictions for unseen items.", "labels": [], "entities": []}, {"text": "Clearly, the accuracy of these models relies crucially on the quality and coverage of the extracted triples, and thus on the syntactic analysis of the corpus.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 13, "end_pos": 21, "type": "METRIC", "confidence": 0.9982954859733582}]}, {"text": "Unfortunately, corpora that are both large enough and have a very good syntactic analysis are only available fora handful of Western and Asian languages, which leaves all other languages without reliable selectional preference models.", "labels": [], "entities": []}, {"text": "In this paper, we propose a cross-lingual knowledge transfer approach to this problem: We automatically translate triples (p, a, h) from resource-poor languages into English, where large and high-quality parsed corpora are available and we can compute a reliable plausibility estimate.", "labels": [], "entities": [{"text": "knowledge transfer", "start_pos": 42, "end_pos": 60, "type": "TASK", "confidence": 0.7196101099252701}]}, {"text": "The translations are extracted from a bilingual semantic space, which can be constructed via bootstrapping from large unparsed corpora in the two languages, without the need for parallel corpora or bilingual lexical resources.", "labels": [], "entities": []}, {"text": "Section 2 reviews models for selectional preferences.", "labels": [], "entities": []}, {"text": "In Section 3, we describe our approach.", "labels": [], "entities": []}, {"text": "Section 4 introduces our experimental setup, and Sections 5 and 6 present and discuss our experiments.", "labels": [], "entities": []}], "datasetContent": [{"text": "Our evaluation uses English as the target language and two source languages: German (as a very close neighbor of English) and Spanish (as a more distant one).", "labels": [], "entities": []}, {"text": "Neither of these languages are really resourcepoor, but they allow us to compare our cross-lingual model against monolingual models, to emulate different levels of \"resource poorness\" and to examine the model's learning curve.", "labels": [], "entities": []}, {"text": "For German, we used the plausibility judgments collected by.", "labels": [], "entities": []}, {"text": "The dataset contains human judgments for ninety triples sampled from the manually annotated 1 million word TiGer corpus (): ten verbs with three argument positions (subject, direct object, and oblique (prepositional) object) combined with three head words.", "labels": [], "entities": [{"text": "TiGer corpus", "start_pos": 107, "end_pos": 119, "type": "DATASET", "confidence": 0.8828293681144714}]}, {"text": "Models are evaluated against such datasets by correlating predicted plausibilities with the (not normally distributed) human judgments using Spearman's \u03c1, a non-parametric rank-order correlation coefficient.", "labels": [], "entities": []}, {"text": "We constructed a similar 90-triple data set for Spanish by sampling triples from two Spanish corpora (see below) using criteria.", "labels": [], "entities": []}, {"text": "Human judgments for the triples were collected through the Amazon Mechanical Turk (AMT) crowdsourcing platform (.", "labels": [], "entities": [{"text": "Amazon Mechanical Turk (AMT) crowdsourcing platform", "start_pos": 59, "end_pos": 110, "type": "DATASET", "confidence": 0.8435699343681335}]}, {"text": "We asked native speakers of Spanish to rate the plausibility of a simple sentence with the relevant verb-argument combination on a five-point Likert scale, obtaining between 12 and 17 judgments for each triple.", "labels": [], "entities": []}, {"text": "For each datapoint, we removed the single lowest and highest judgments and computed the mean.", "labels": [], "entities": []}, {"text": "We assessed the reliability of our data by replicating Brockmann's experiment for German with our AMT setup.", "labels": [], "entities": [{"text": "reliability", "start_pos": 16, "end_pos": 27, "type": "METRIC", "confidence": 0.9578613638877869}, {"text": "AMT", "start_pos": 98, "end_pos": 101, "type": "TASK", "confidence": 0.8298266530036926}]}, {"text": "With a Spearman \u03c1 of almost .90, our own judgments correlate very well with Brockmann's original data.", "labels": [], "entities": [{"text": "Spearman \u03c1", "start_pos": 7, "end_pos": 17, "type": "METRIC", "confidence": 0.9561537206172943}]}, {"text": "Monolingual Prior Work and Baselines.", "labels": [], "entities": []}, {"text": "For German, evaluated ontology-based models trained on TiGer triples and the GermaNet ontology.", "labels": [], "entities": []}, {"text": "The results in show that while both models are able to predict the data significantly, neither of the models can predict all of the data.", "labels": [], "entities": []}, {"text": "We attribute this to the small size of TiGer.", "labels": [], "entities": [{"text": "TiGer", "start_pos": 39, "end_pos": 44, "type": "DATASET", "confidence": 0.8488389253616333}]}, {"text": "To gauge the limits of monolingual knowledgelean approaches, we constructed two monolingual distributional models for German and Spanish according to the    two languages, using the 2,000 most frequent lemmadependency relation pairs as dimensions and adopting the popular pointwise mutual information metric as co-occurrence statistic.", "labels": [], "entities": []}, {"text": "For German, we used Schulte im Walde's verb frame resource (Schulte im), which contains the frequency of triples calculated from probabilistic parses of 30M words from the Huge German Corpus (HGC) of newswire.", "labels": [], "entities": [{"text": "Huge German Corpus (HGC) of newswire", "start_pos": 172, "end_pos": 208, "type": "DATASET", "confidence": 0.8417688682675362}]}, {"text": "For Spanish, we consulted two syntactically analyzed corpora: the AnCora () and the Encarta corpus ().", "labels": [], "entities": [{"text": "AnCora", "start_pos": 66, "end_pos": 72, "type": "METRIC", "confidence": 0.8871350288391113}]}, {"text": "At 0.5M words, the AnCora corpus is small, but manually annotated, whereas the larger, automatically parsed Encarta corpus amounts to over 18M tokens.", "labels": [], "entities": [{"text": "AnCora corpus", "start_pos": 19, "end_pos": 32, "type": "DATASET", "confidence": 0.9441902339458466}, {"text": "Encarta corpus", "start_pos": 108, "end_pos": 122, "type": "DATASET", "confidence": 0.908391535282135}]}, {"text": "shows the results for the distributional monolingual models.", "labels": [], "entities": []}, {"text": "For German, we get significant correlations for DOBJ and POBJ, an almost significant correlation for SUBJs, and high significance for the complete dataset (p < 0.01).", "labels": [], "entities": []}, {"text": "These figures rival the performance of the ontological models (cf.), without using ontological information.", "labels": [], "entities": []}, {"text": "For Spanish, the only significant correlation with human judgments is obtained for subjects, the most frequent argument position, with the clean AnCora data.", "labels": [], "entities": [{"text": "AnCora data", "start_pos": 145, "end_pos": 156, "type": "DATASET", "confidence": 0.8725016415119171}]}, {"text": "AnCora is presumably too sparse for the other argument positions.", "labels": [], "entities": [{"text": "AnCora", "start_pos": 0, "end_pos": 6, "type": "METRIC", "confidence": 0.8645032644271851}]}, {"text": "The large Encarta corpus, in turn, is very noisy, supporting our concerns from Section 2.", "labels": [], "entities": [{"text": "Encarta corpus", "start_pos": 10, "end_pos": 24, "type": "DATASET", "confidence": 0.9628313779830933}]}, {"text": "Since the Encarta data consists of individual dependency n noun adj verb all German .61 .57 .43 .56 Spanish .62 .67 .41 .58: First-translation accuracy for German-English and Spanish-English translation (n: size of gold standard).", "labels": [], "entities": [{"text": "Encarta data", "start_pos": 10, "end_pos": 22, "type": "DATASET", "confidence": 0.9359632134437561}, {"text": "accuracy", "start_pos": 143, "end_pos": 151, "type": "METRIC", "confidence": 0.920036256313324}]}, {"text": "Our architecture for the cross-lingual prediction of selectional preferences shown in consists of two components, namely the bilingual vector space and a selectional preference model in the target language.", "labels": [], "entities": [{"text": "cross-lingual prediction of selectional preferences", "start_pos": 25, "end_pos": 76, "type": "TASK", "confidence": 0.7862760126590729}]}, {"text": "As our English selectional preference model, we again use the model, trained on aversion of the BNC parsed with MINIPAR.", "labels": [], "entities": [{"text": "BNC", "start_pos": 96, "end_pos": 99, "type": "DATASET", "confidence": 0.8042246103286743}, {"text": "MINIPAR", "start_pos": 112, "end_pos": 119, "type": "DATASET", "confidence": 0.8393529057502747}]}, {"text": "The parameters of the syntactic vector space were the same as for the monolingual baseline models.", "labels": [], "entities": []}, {"text": "The bilingual vector spaces were constructed from three large, unparsed, comparable monolingual corpora.", "labels": [], "entities": []}, {"text": "For German, we used the HGC described above.", "labels": [], "entities": [{"text": "HGC", "start_pos": 24, "end_pos": 27, "type": "DATASET", "confidence": 0.9466208219528198}]}, {"text": "For Spanish, we obtained a corpus with around 100M words, consisting of 2.5 years of crawled text from two major Spanish newspapers.", "labels": [], "entities": []}, {"text": "For English, we used the BNC.", "labels": [], "entities": [{"text": "BNC", "start_pos": 25, "end_pos": 28, "type": "DATASET", "confidence": 0.9626448154449463}]}, {"text": "We first constructed initial sets of bilingual labels.", "labels": [], "entities": []}, {"text": "For German-English, we identified 1064 graphemically identical word pairs that occurred more than 4 times per million words.", "labels": [], "entities": []}, {"text": "Due to the larger lexical distance between Spanish and English, there are fewer graphemically identical tokens for this language pair.", "labels": [], "entities": []}, {"text": "We therefore applied a Porter stemmer and found 2104 identical stems, at a higher risk of \"false friends\".", "labels": [], "entities": [{"text": "Porter stemmer", "start_pos": 23, "end_pos": 37, "type": "DATASET", "confidence": 0.8616403341293335}]}, {"text": "We then applied the bootstrapping cycle from Section 3.1.", "labels": [], "entities": []}, {"text": "The set of dimensions converged after around five iterations.", "labels": [], "entities": []}, {"text": "We evaluated the (asymmetric) nearest neighbor pairs from the final spaces, (s, tr(s)), against two online dictionaries.", "labels": [], "entities": []}, {"text": "4 shows that 55% to 60% of the pairs are listed in the dictionaries, with parallel tendencies for both language pairs.", "labels": [], "entities": []}, {"text": "The bilingual space performs fairly well for nouns and adjectives, but badly for verbs, which is a well-known weakness of distributional models.", "labels": [], "entities": []}, {"text": "Even taking into account the incompleteness of dictionaries, this looks like a negative result: more relations rather than trees, we could not model the POBJ data.than half of all verb translations are incorrect.", "labels": [], "entities": [{"text": "POBJ data.than", "start_pos": 153, "end_pos": 167, "type": "DATASET", "confidence": 0.7775063514709473}]}, {"text": "However, following upon our intuitions from Section 3.2, we performed an analysis of the \"incorrect\" translations.", "labels": [], "entities": []}, {"text": "It revealed that many of the errors in are informative, semantically related words.", "labels": [], "entities": []}, {"text": "Nearest neighbor target language verbs in particular tend to represent the same event type and take the same kinds of arguments as the source verb.", "labels": [], "entities": []}, {"text": "Examples are German gef\u00e4hrden 'threaten' -English affect, and German Neugier 'curiosity' -English enthusiasm.", "labels": [], "entities": []}, {"text": "We concluded that literal translation quality is a misleading figure of merit for our task.", "labels": [], "entities": [{"text": "literal translation", "start_pos": 18, "end_pos": 37, "type": "TASK", "confidence": 0.8613687753677368}]}, {"text": "Section 3 introduced one major design decision of our model: the question of how to treat the argument position, which cannot be translated by the bilingual vector space, in the cross-lingual transfer.", "labels": [], "entities": []}, {"text": "We present two experiments that investigate the model's behavior in the absence and presence of knowledge about argument positions.", "labels": [], "entities": []}, {"text": "Experiment 1 uses no syntactic knowledge about the source language whatsoever.", "labels": [], "entities": []}, {"text": "In this situation, the best we can do is to assume that source language argument positions like SUBJ will correspond to the same argument position in the target language.", "labels": [], "entities": []}, {"text": "Experiment 2 attempts to identify, for each source language argument position, the \"best fit\" position in the target language.", "labels": [], "entities": []}, {"text": "This results in better plausibility estimates, but also means that we need at least some syntactic information about the source language.", "labels": [], "entities": []}, {"text": "In both experiments, we vary the number of translations we consider for each verb.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Monolingual baselines 1. Spearman correla- tions for ontology-based models in German as reported by  Brockmann and Lapata (2003). *: p < .05; ***: p < .001", "labels": [], "entities": []}, {"text": " Table 3: Monolingual baselines 2. Spearman correlation  and coverage for distributional models.  \u2020 : p < .1; *: p <  .05; **: p < .01.", "labels": [], "entities": [{"text": "Spearman correlation", "start_pos": 35, "end_pos": 55, "type": "METRIC", "confidence": 0.6083033382892609}, {"text": "coverage", "start_pos": 61, "end_pos": 69, "type": "METRIC", "confidence": 0.9267053008079529}]}, {"text": " Table 5: Exp.1: Spearman correlation between syntaxless  cross-lingual model and human judgments for k best verb  translations. Best k for each argument position marked in  boldface. Coverage of all models: 100%.", "labels": [], "entities": [{"text": "Spearman correlation", "start_pos": 17, "end_pos": 37, "type": "METRIC", "confidence": 0.9296668171882629}]}, {"text": " Table 6: Exp.2: Spearman correlation between syntax- aware cross-lingual model and human judgments for k  best verb translations. ES-A: AnCora corpus, ES-E: En- carta corpus. Best k for each argument position in bold- face. Coverage of all models: 100%, except c : 60%.", "labels": [], "entities": []}]}