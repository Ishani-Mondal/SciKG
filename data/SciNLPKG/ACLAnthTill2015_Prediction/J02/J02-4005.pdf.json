{"title": [], "abstractContent": [{"text": "We present and evaluate SumUM, a text summarization system that takes a raw technical text as input and produces an indicative informative summary.", "labels": [], "entities": [{"text": "SumUM", "start_pos": 24, "end_pos": 29, "type": "TASK", "confidence": 0.7996926307678223}, {"text": "text summarization", "start_pos": 33, "end_pos": 51, "type": "TASK", "confidence": 0.6815664172172546}]}, {"text": "The indicative part of the summary identifies the topics of the document, and the informative part elaborates on some of these topics according to the reader's interest.", "labels": [], "entities": []}, {"text": "SumUM motivates the topics, describes entities, and defines concepts.", "labels": [], "entities": [{"text": "SumUM", "start_pos": 0, "end_pos": 5, "type": "TASK", "confidence": 0.9106218814849854}]}, {"text": "It is a first step for exploring the issue of dynamic summarization.", "labels": [], "entities": [{"text": "summarization", "start_pos": 54, "end_pos": 67, "type": "TASK", "confidence": 0.6656213402748108}]}, {"text": "This is accomplished through a process of shallow syntactic and semantic analysis, concept identification, and text regeneration.", "labels": [], "entities": [{"text": "shallow syntactic and semantic analysis", "start_pos": 42, "end_pos": 81, "type": "TASK", "confidence": 0.6370931446552277}, {"text": "concept identification", "start_pos": 83, "end_pos": 105, "type": "TASK", "confidence": 0.7335368245840073}, {"text": "text regeneration", "start_pos": 111, "end_pos": 128, "type": "TASK", "confidence": 0.7684552073478699}]}, {"text": "Our method was developed through the study of a corpus of abstracts written by professional abstractors.", "labels": [], "entities": []}, {"text": "Relying on human judgment, we have evaluated indicativeness, informativeness, and text acceptability of the automatic summaries.", "labels": [], "entities": []}, {"text": "The results thus far indicate good performance when compared with other summarization technologies.", "labels": [], "entities": [{"text": "summarization", "start_pos": 72, "end_pos": 85, "type": "TASK", "confidence": 0.9767264127731323}]}], "introductionContent": [{"text": "A summary is a condensed version of a source document having a recognizable genre and a very specific purpose: to give the reader an exact and concise idea of the contents of the source.", "labels": [], "entities": []}, {"text": "In most cases, summaries are written by humans, but nowadays, the overwhelming quantity of information, and the need to access the essential content of documents accurately in order to satisfy users' demands calls for the development of computer programs able to produce text summaries.", "labels": [], "entities": [{"text": "summaries", "start_pos": 15, "end_pos": 24, "type": "TASK", "confidence": 0.9837994575500488}]}, {"text": "The process of automatically producing a summary from a source text consists of the following steps: 1.", "labels": [], "entities": []}, {"text": "extracting the relevant information, which ideally includes the \"topics\" of the source 3.", "labels": [], "entities": []}, {"text": "condensing the extracted information and constructing a summary representation 4.", "labels": [], "entities": []}, {"text": "presenting the summary representation to the reader in natural language.", "labels": [], "entities": []}, {"text": "Even though some approaches to text summarization produce acceptable summaries for specific tasks, it is generally agreed that the problem of coherent selection and expression of information in text summarization is far from being resolved.", "labels": [], "entities": [{"text": "text summarization", "start_pos": 31, "end_pos": 49, "type": "TASK", "confidence": 0.7465139925479889}, {"text": "text summarization", "start_pos": 194, "end_pos": 212, "type": "TASK", "confidence": 0.6342596709728241}]}, {"text": "Sparck Jones and Endres-Niggemeyer (1995) stated the need fora research program in text summarization that would study the relation between source document and summary, the different types of summaries and their functions, the development of new methods and/or combination of already existing techniques for text summarization, and the development of evaluation procedures for summaries and systems.", "labels": [], "entities": [{"text": "text summarization", "start_pos": 83, "end_pos": 101, "type": "TASK", "confidence": 0.6953173279762268}, {"text": "text summarization", "start_pos": 308, "end_pos": 326, "type": "TASK", "confidence": 0.7043065130710602}]}, {"text": "proposes the following typology of different types of document condensations: \u2022 the extract, which is a set of passages selected from a source document to represent the whole document \u2022 the summary, which occurs at the end of the document and is a restatement of the salient findings of a work \u2022 the abridgment, which is a reduction of the original document that necessarily omits secondary points \u2022 the precis, which stands for the main points of an argument \u2022 the digest, which is a condensation of a book or news article \u2022 the highlight, which is a comment included in specific parts of a document to alert a reader \u2022 the synopsis, which in cinematography represents a script of a film.", "labels": [], "entities": []}, {"text": "In our research, we are concerned only with summaries of technical articles, which are called abstracts.", "labels": [], "entities": []}, {"text": "In this context, two main types of abstracts are considered: indicative abstracts, which point to information alerting the reader about the content of an article in a given domain (these abstracts will contain sentences like \"The work of Consumer Advice Centres is examined.\"), and informative abstracts, which provide as much quantitative or qualitative information contained in the source document as possible (these abstracts will contain sentences like \"Consumer Advice Centres have dealt with preshopping advice, education on consumers' rights and complaints about goods and services, advising the client and often obtaining expert assessments.\").", "labels": [], "entities": []}, {"text": "In the course of our research, we have studied the relation between abstracts and source documents, and as a result, we have developed SumUM (Summarization at Universit\u00e9 de Montr\u00e9al), a text summarization system that produces an indicative-informative abstract for technical documents.", "labels": [], "entities": [{"text": "text summarization", "start_pos": 186, "end_pos": 204, "type": "TASK", "confidence": 0.6882281005382538}]}, {"text": "The abstracts are produced in two steps: First, the reader is presented with an indicative abstract that identifies the topics of the document (what the authors present, discuss, etc.).", "labels": [], "entities": []}, {"text": "Then, if the reader is interested in some of the topics, specific information about them from the source document is presented in an informative abstract.", "labels": [], "entities": []}, {"text": "shows an automatic abstract produced by our system.", "labels": [], "entities": []}, {"text": "The abstract was produced by a process of conceptual identification and text re-generation we call selective analysis.", "labels": [], "entities": [{"text": "conceptual identification", "start_pos": 42, "end_pos": 67, "type": "TASK", "confidence": 0.7624928653240204}]}, {"text": "The indicative abstract contains information about the topic of the document.", "labels": [], "entities": []}, {"text": "It describes the topics of sections and introduces relevant entities.", "labels": [], "entities": []}, {"text": "The identified topics are terms either appearing in the indicative abstract or obtained from the terms and words of the indicative abstract through a process of term expansion.", "labels": [], "entities": []}, {"text": "The one particular feature of these terms is that they can be used to obtain more conceptual information from the source document, such as definitions or statements of relevance, usefulness, and development, as can be seen in.", "labels": [], "entities": []}, {"text": "This article is organized as follows.", "labels": [], "entities": []}, {"text": "In the next section, we describe the analysis of a corpus of professional abstracts used to specify selective analysis; conceptual and linguistic information for the task of summarization of technical texts deduced from this corpus is also presented.", "labels": [], "entities": [{"text": "summarization of technical texts deduced", "start_pos": 174, "end_pos": 214, "type": "TASK", "confidence": 0.8603055596351623}]}, {"text": "An overview of selective analysis and the implementation Identified Topics: HuDL -IMA -aid systems -architecture -holonic manufacturing system -human -human-robot interaction -intelligent interactive service robots -intelligent machine architecture -intelligent machine software -interaction -key issue -widely used interaction -novel software architecture -overall interaction -robot -second issue -service -service robots -software -system -Technologies", "labels": [], "entities": []}], "datasetContent": [{"text": "Our objective in the evaluation of indicative content is to see whether the abstracts produced by our method convey the essential content of the source documents in order to help readers complete a categorization task.", "labels": [], "entities": []}, {"text": "In the evaluation of text quality, we want to determine whether the abstracts produced by our method are acceptable according to a number of acceptability criteria.", "labels": [], "entities": []}, {"text": "Our objective in the evaluation of content in a coselection experiment is to measure coselection between sentences selected by our system and a set of \"correct\" extracted sentences.", "labels": [], "entities": []}, {"text": "This method of evaluation has already been used in other summarization evaluations such as Edmundson and.", "labels": [], "entities": [{"text": "summarization evaluations", "start_pos": 57, "end_pos": 82, "type": "TASK", "confidence": 0.9194755852222443}]}, {"text": "The idea is that if we find a high degree of overlap between the sentences selected by an automatic method and the sentences selected by a human, the method can be regarded as effective.", "labels": [], "entities": []}, {"text": "Nevertheless, this method of evaluation has been criticized not only because of the low rate of agreement between human subjects in this task (), but also because there is no unique ideal or target abstract fora given document.", "labels": [], "entities": []}, {"text": "Instead, there is a set of main ideas that a good abstract should contain.", "labels": [], "entities": []}, {"text": "In our coselection experiment, we were also interested in comparing our system with other summarization technologies.", "labels": [], "entities": [{"text": "summarization", "start_pos": 90, "end_pos": 103, "type": "TASK", "confidence": 0.9746461510658264}]}], "tableCaptions": [{"text": " Table 2  Distribution of information.", "labels": [], "entities": [{"text": "Distribution", "start_pos": 10, "end_pos": 22, "type": "TASK", "confidence": 0.9493274688720703}]}, {"text": " Table 13  Results of human judgment in a categorization task and assessment about text quality.", "labels": [], "entities": []}, {"text": " Table 14  Coselection between sentences selected by human assessors and sentences selected by three  automatic summarization methods in recall (R), precision (P) and F-score (F).", "labels": [], "entities": [{"text": "recall (R)", "start_pos": 137, "end_pos": 147, "type": "METRIC", "confidence": 0.9502048194408417}, {"text": "precision (P)", "start_pos": 149, "end_pos": 162, "type": "METRIC", "confidence": 0.9478610306978226}, {"text": "F-score (F)", "start_pos": 167, "end_pos": 178, "type": "METRIC", "confidence": 0.9588318914175034}]}]}