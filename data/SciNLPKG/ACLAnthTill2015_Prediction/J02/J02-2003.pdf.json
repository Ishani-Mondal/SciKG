{"title": [{"text": "Class-Based Probability Estimation Using a Semantic Hierarchy", "labels": [], "entities": [{"text": "Probability Estimation", "start_pos": 12, "end_pos": 34, "type": "TASK", "confidence": 0.6949721723794937}]}], "abstractContent": [{"text": "This article concerns the estimation of a particular kind of probability, namely, the probability of a noun sense appearing as a particular argument of a predicate.", "labels": [], "entities": []}, {"text": "In order to overcome the accompanying sparse-data problem, the proposal here is to define the probabilities in terms of senses from a semantic hierarchy and exploit the fact that the senses can be grouped into classes consisting of semantically similar senses.", "labels": [], "entities": []}, {"text": "There is a particular focus on the problem of how to determine a suitable class fora given sense, or, alternatively, how to determine a suitable level of generalization in the hierarchy.", "labels": [], "entities": []}, {"text": "A procedure is developed that uses a chi-square test to determine a suitable level of generalization.", "labels": [], "entities": []}, {"text": "In order to test the performance of the estimation method, a pseudo-disambiguation task is used, together with two alternative estimation methods.", "labels": [], "entities": [{"text": "estimation", "start_pos": 40, "end_pos": 50, "type": "TASK", "confidence": 0.9695253968238831}]}, {"text": "Each method uses a different generalization procedure; the first alternative uses the minimum description length principle, and the second uses Resnik's measure of selectional preference.", "labels": [], "entities": []}, {"text": "In addition, the performance of our method is investigated using both the standard Pearson chi-square statistic and the log-likelihood chi-square statistic.", "labels": [], "entities": []}], "introductionContent": [{"text": "This article concerns the problem of how to estimate the probabilities of noun senses appearing as particular arguments of predicates.", "labels": [], "entities": []}, {"text": "Such probabilities can be useful fora variety of natural language processing (NLP) tasks, such as structural disambiguation and statistical parsing, word sense disambiguation, anaphora resolution, and language modeling.", "labels": [], "entities": [{"text": "statistical parsing", "start_pos": 128, "end_pos": 147, "type": "TASK", "confidence": 0.6692075729370117}, {"text": "word sense disambiguation", "start_pos": 149, "end_pos": 174, "type": "TASK", "confidence": 0.6875395774841309}, {"text": "anaphora resolution", "start_pos": 176, "end_pos": 195, "type": "TASK", "confidence": 0.7219906747341156}, {"text": "language modeling", "start_pos": 201, "end_pos": 218, "type": "TASK", "confidence": 0.7753076553344727}]}, {"text": "To see how such knowledge can be used to resolve structural ambiguities, consider the following prepositional phrase attachment ambiguity:", "labels": [], "entities": []}], "datasetContent": [{"text": "The task we used to compare the class-based estimation techniques is a decision task previously used by and.", "labels": [], "entities": []}, {"text": "The task is to decide which of two verbs, v and v \ud97b\udf59 , is more likely to take a given noun, n, as an object.", "labels": [], "entities": []}, {"text": "The test and training data were obtained as follows.", "labels": [], "entities": []}, {"text": "A number of verb-direct object pairs were extracted from a subset of the BNC, using the system of Briscoe and Carroll.", "labels": [], "entities": [{"text": "BNC", "start_pos": 73, "end_pos": 76, "type": "DATASET", "confidence": 0.8882295489311218}]}, {"text": "All those pairs containing a noun not in WordNet were removed, and each verb and argument was lemmatized.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 41, "end_pos": 48, "type": "DATASET", "confidence": 0.9732005000114441}]}, {"text": "This resulted in a data set of around 1.3 million (v, n) pairs.", "labels": [], "entities": []}, {"text": "To form a test set, 3,000 of these pairs were randomly selected such that each selected pair contained a fairly frequent verb.", "labels": [], "entities": []}, {"text": "(Following Pereira, Tishby, and Lee, only those verbs that occurred between 500 and 5,000 times in the data were considered.)", "labels": [], "entities": []}, {"text": "Each instance of a selected pair was then deleted from the data to ensure that the test data were unseen.", "labels": [], "entities": []}, {"text": "The remaining pairs formed the training data.", "labels": [], "entities": []}, {"text": "To complete the test set, a further fairly frequent verb, v \ud97b\udf59 , was randomly chosen for each (v, n) pair.", "labels": [], "entities": []}, {"text": "The random choice was made according to the verb's frequency in the original data set, subject to the condition that the pair (v \ud97b\udf59 , n) did not occur in the training data.", "labels": [], "entities": []}, {"text": "Given the set of (v, n, v \ud97b\udf59 ) triples, the task is to decide whether (v, n) or (v \ud97b\udf59 , n) is the correct pair.", "labels": [], "entities": []}, {"text": "We acknowledge that the task is somewhat artificial, but pseudo-disambiguation tasks of this kind are becoming popular in statistical NLP because of the ease with which training and test data can be created.", "labels": [], "entities": []}, {"text": "We also feel that the pseudo-disambiguation task is useful for evaluating the different estimation methods, since it directly addresses the question of how likely a particular predicate is to take a given noun as an argument.", "labels": [], "entities": []}, {"text": "An evaluation using a PP attachment task was attempted in, but the evaluation was limited by the relatively small size of the Penn Treebank.", "labels": [], "entities": [{"text": "PP attachment task", "start_pos": 22, "end_pos": 40, "type": "TASK", "confidence": 0.7402345836162567}, {"text": "Penn Treebank", "start_pos": 126, "end_pos": 139, "type": "DATASET", "confidence": 0.9947395324707031}]}, {"text": "Using our approach, the disambiguation decision for each (v, n, v \ud97b\udf59 ) triple was made according to the following procedure: If n has more than one sense, the sense is chosen that maximizes the relevant probability estimate; this explains the maximization over cn(n).", "labels": [], "entities": []}, {"text": "The probability estimates were obtained using our class-based method, and the G 2 statistic was used for the chi-square test.", "labels": [], "entities": []}, {"text": "This procedure was also used for the MDL alternative, but using the MDL method to estimate the probabilities.", "labels": [], "entities": []}, {"text": "Using the association score for each test triple, the decision was made according to the following procedure: We use h(c) to denote the set consisting of the hypernyms of c.", "labels": [], "entities": []}, {"text": "The inner maximization is over h(c), assuming c is the chosen sense of n, which corresponds to Resnik's method of choosing a set to represent c.", "labels": [], "entities": []}, {"text": "The outer maximization is over the senses of n, cn(n), which determines the sense of n by choosing the sense that maximizes the association score.", "labels": [], "entities": []}, {"text": "The first set of results is given in.", "labels": [], "entities": []}, {"text": "Our technique is referred to as the \"similarity class\" technique, and the approach using the association score is referred to as \"Assoc.\"", "labels": [], "entities": [{"text": "Assoc", "start_pos": 130, "end_pos": 135, "type": "METRIC", "confidence": 0.9508933424949646}]}, {"text": "The results are given fora range of \u03b1 values and demonstrate clearly that the performance of similarity class varies little with changes in \u03b1 and that similarity class outperforms both MDL and Assoc.", "labels": [], "entities": []}, {"text": "We also give a score for our approach using a simple generalization procedure, which we call \"low class.\"", "labels": [], "entities": []}, {"text": "The procedure is to select the first class that has a count greater than zero (relative to the verb and argument position), which is likely to return a low level of generalization, on the whole.", "labels": [], "entities": []}, {"text": "The results show that our generalization technique only narrowly outperforms the simple alternative.", "labels": [], "entities": [{"text": "generalization", "start_pos": 26, "end_pos": 40, "type": "TASK", "confidence": 0.9707223773002625}]}, {"text": "Note that, although low class is based on a very simple generalization method, the estimation method is still using our class-based technique, by applying Bayes' theorem and conditioning on a class, as described in Section 3; the difference is in how the class is chosen.", "labels": [], "entities": []}, {"text": "To investigate the results, we calculated the average number of generalized levels for each approach.", "labels": [], "entities": []}, {"text": "The number of generalized levels fora concept c (relative to a verb v and argument position r) is the difference in depth between c and top(c, v, r), as explained in Section 5.", "labels": [], "entities": []}, {"text": "For each test case, the number of generalized levels for both verbs, v and v \ud97b\udf59 , was calculated, but only for the chosen sense of n.", "labels": [], "entities": []}, {"text": "The results are given in the third column of and demonstrate clearly that both MDL and Assoc are generalizing to a greater extent than similarity class.", "labels": [], "entities": [{"text": "Assoc", "start_pos": 87, "end_pos": 92, "type": "METRIC", "confidence": 0.7298171520233154}, {"text": "similarity", "start_pos": 135, "end_pos": 145, "type": "METRIC", "confidence": 0.9647333025932312}]}, {"text": "(The fourth column gives a standard deviation) These results suggest that MDL and Assoc are overgeneralizing, at least for the purposes of this task.", "labels": [], "entities": [{"text": "MDL", "start_pos": 74, "end_pos": 77, "type": "DATASET", "confidence": 0.7004455327987671}, {"text": "Assoc", "start_pos": 82, "end_pos": 87, "type": "METRIC", "confidence": 0.5492011904716492}]}, {"text": "To investigate why the value for \u03b1 had no impact on the results, we repeated the experiment, but with one fifth of the data.", "labels": [], "entities": []}, {"text": "A new data set was created by taking every fifth pair of the original 1.3 million pairs.", "labels": [], "entities": []}, {"text": "A test set of 3,000 triples was created from this new data set, as before, but this time only verbs that occurred between 100 and 1,000 times were considered.", "labels": [], "entities": []}, {"text": "The results using these test and training data are given in.", "labels": [], "entities": []}, {"text": "These results show a variation in performance across values for \u03b1, with an optimal performance when \u03b1 is around 0.75.", "labels": [], "entities": []}, {"text": "(Of course, in practice, the value for \u03b1 would need to be optimized on a held-out set.)", "labels": [], "entities": []}, {"text": "But even with this variation, similarity class is still outperforming MDL and Assoc across the whole range of \u03b1 values.", "labels": [], "entities": [{"text": "similarity", "start_pos": 30, "end_pos": 40, "type": "METRIC", "confidence": 0.9708693027496338}]}, {"text": "Note that the \u03b1 values corresponding to the lowest scores lead to a significant amount of generalization, which provides additional evidence that MDL and Assoc are overgeneralizing for this task.", "labels": [], "entities": []}, {"text": "The low-class method scores highly for this data set also, but given that the task is one that apparently favors a low level of generalization, the high score is not too surprising.", "labels": [], "entities": []}, {"text": "As a final experiment, we compared the task performance using the X 2 , rather than G 2 , statistic in the chi-square test.", "labels": [], "entities": []}, {"text": "The results are given in for the complete data set.", "labels": [], "entities": [{"text": "complete data set", "start_pos": 33, "end_pos": 50, "type": "DATASET", "confidence": 0.8423726161321005}]}, {"text": "The figures in brackets give the average number of generalized levels.", "labels": [], "entities": []}, {"text": "The X 2 statistic is performing at least as well as G 2 , and the results show that the average level of generalization is slightly higher for G 2 than X 2 . This suggests a possible explanation for the results presented here and those in: that the X 2 statistic provides a less conservative test when counts in the contingency table are low.", "labels": [], "entities": []}, {"text": "(By a conservative test we mean one in which the null hypothesis is not easily rejected.)", "labels": [], "entities": []}, {"text": "A less conservative testis better suited to the pseudo-disambiguation task, since it results in a lower level of generalization, on the whole, which is good for this task.", "labels": [], "entities": []}, {"text": "In contrast, the task that Dunning considers, the discovery of bigrams, is better served by a more conservative test.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1  Contingency table for the children of \ud97b\udf59canine\ud97b\udf59 in the subject position of run.", "labels": [], "entities": []}, {"text": " Table 2  Contingency table for the children of \ud97b\udf59liquid\ud97b\udf59 in the object position of drink.", "labels": [], "entities": []}, {"text": " Table 3  Example levels of generalization for different values of \u03b1.", "labels": [], "entities": []}, {"text": " Table 4  Extent of generalization for different values of \u03b1 and sample sizes.", "labels": [], "entities": []}, {"text": " Table 5  Results for the pseudo-disambiguation task.", "labels": [], "entities": []}, {"text": " Table 6  Results for the pseudo-disambiguation task with one-fifth training data.", "labels": [], "entities": []}, {"text": " Table 7  Disambiguation results for G 2 and X 2 .", "labels": [], "entities": [{"text": "Disambiguation", "start_pos": 10, "end_pos": 24, "type": "METRIC", "confidence": 0.8388210535049438}]}]}