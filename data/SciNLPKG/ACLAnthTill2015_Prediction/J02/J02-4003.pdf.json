{"title": [{"text": "Automatic Summarization of Open-Domain Multiparty Dialogues in Diverse Genres", "labels": [], "entities": [{"text": "Automatic Summarization of Open-Domain Multiparty Dialogues", "start_pos": 0, "end_pos": 59, "type": "TASK", "confidence": 0.7216089963912964}]}], "abstractContent": [{"text": "Automatic summarization of open-domain spoken dialogues is a relatively new research area.", "labels": [], "entities": [{"text": "Automatic summarization of open-domain spoken dialogues", "start_pos": 0, "end_pos": 55, "type": "TASK", "confidence": 0.8148484925429026}]}, {"text": "This article introduces the task and the challenges involved and motivates and presents an approach for obtaining automatic-extract summaries for human transcripts of multiparty dialogues of four different genres, without any restriction on domain.", "labels": [], "entities": []}, {"text": "We address the following issues, which are intrinsic to spoken-dialogue summarization and typically can be ignored when summarizing written text such as news wire data: (1) detection and removal of speech disfluencies; (2) detection and insertion of sentence boundaries; and (3) detection and linking of cross-speaker information units (question-answer pairs).", "labels": [], "entities": [{"text": "spoken-dialogue summarization", "start_pos": 56, "end_pos": 85, "type": "TASK", "confidence": 0.5997799932956696}, {"text": "summarizing written text", "start_pos": 120, "end_pos": 144, "type": "TASK", "confidence": 0.8836281696955363}, {"text": "detection and removal of speech disfluencies", "start_pos": 173, "end_pos": 217, "type": "TASK", "confidence": 0.6426273187001547}, {"text": "detection and insertion of sentence boundaries", "start_pos": 223, "end_pos": 269, "type": "TASK", "confidence": 0.7658525953690211}]}, {"text": "A system evaluation is performed using a corpus of 23 dialogue excerpts with an average duration of about 10 minutes, comprising 80 topical segments and about 47,000 words total.", "labels": [], "entities": []}, {"text": "The corpus was manually annotated for relevant text spans by six human annotators.", "labels": [], "entities": []}, {"text": "The global evaluation shows that for the two more informal genres, our summarization system using dialogue-specific components significantly outperforms two baselines: (1) a maximum-marginal-relevance ranking algorithm using TF*IDF term weighting, and (2) a LEAD baseline that extracts the first n words from a text.", "labels": [], "entities": [{"text": "TF*IDF term weighting", "start_pos": 225, "end_pos": 246, "type": "TASK", "confidence": 0.4311941176652908}]}], "introductionContent": [{"text": "Traditionally, summarization systems have been evaluated in two major ways: (1) intrinsically, measuring the amount of the core information preserved from the original text, and (2) extrinsically, measuring how much the summary can benefit in accomplishing another task (e.g., finding a document relevant to a query or classifying a document into a topical category) ().", "labels": [], "entities": [{"text": "summarization", "start_pos": 15, "end_pos": 28, "type": "TASK", "confidence": 0.9763785600662231}]}, {"text": "In this work, we focus on intrinsic evaluation exclusively.", "labels": [], "entities": []}, {"text": "That is, we want to assess how well the summaries preserve the essential information contained in the original texts.", "labels": [], "entities": []}, {"text": "As other studies have shown), the level of agreement between human annotators about which passages to choose to form a good summary is usually quite low.", "labels": [], "entities": []}, {"text": "Our own findings, reported in section 4.2.4, support this in that the intercoder agreement, here measured on a word level, is rather low.", "labels": [], "entities": [{"text": "agreement", "start_pos": 81, "end_pos": 90, "type": "METRIC", "confidence": 0.8567910194396973}]}, {"text": "We decided to minimize the bias that would result from selecting either a particular human annotator, or even the manually created gold standard, as a reference for automatic evaluation; instead, we weigh all annotations from all human coders equally.", "labels": [], "entities": []}, {"text": "Intuitively, we want to reward summaries that contain a high number of words considered to be relevant by most annotators.", "labels": [], "entities": []}, {"text": "We formalize this notion in the following subsection.", "labels": [], "entities": []}], "datasetContent": [{"text": "All evaluations are based on topically coherent segments from the dialogue excerpts of our corpus.", "labels": [], "entities": []}, {"text": "As mentioned before, the segment boundaries were chosen from the human gold standard for the purpose of the global system evaluation.", "labels": [], "entities": []}, {"text": "For each segment s, for each annotator a, and for each word position w i , we define a boolean word vector of annotations \ud97b\udf59 w s,a , each component w s,a,i being 1 if the word w i is part of a nucleus IU or a satellite IU for that annotator and segment, and 0 otherwise.", "labels": [], "entities": []}, {"text": "We then sum overall annotators' annotation vectors and normalize them by the number of annotators per segment (A) to obtain the average relevance vector for segment s, \ud97b\udf59 r s : To obtain the summary accuracy score sa s,n for any segment summary with length n, we multiply the boolean summary vector \ud97b\udf59 summ s 26 by the average relevance vector \ud97b\udf59 r s , and then divide this product by the sum of then highest scores within \ud97b\udf59 r s (maximum achievable score), \ud97b\udf59 rsort s being the vector \ud97b\udf59 r s sorted by relevance weight in descending order: It is easy to see that the summary accuracy score always is in the interval [0.0, 1.0].", "labels": [], "entities": [{"text": "accuracy", "start_pos": 198, "end_pos": 206, "type": "METRIC", "confidence": 0.7579482197761536}, {"text": "summary accuracy score", "start_pos": 562, "end_pos": 584, "type": "METRIC", "confidence": 0.7390025655428568}]}, {"text": "Whereas section 5 was concerned with the design and evaluation of the individual system components, the goal here is to describe and analyze the quality of the global system, with all its components combined.", "labels": [], "entities": []}, {"text": "In this section, we compare our DIASUMM system with the MMR baseline system, which operates without any dialogue-specific components, and with the LEAD baseline.", "labels": [], "entities": [{"text": "MMR baseline", "start_pos": 56, "end_pos": 68, "type": "DATASET", "confidence": 0.7135573923587799}, {"text": "LEAD baseline", "start_pos": 147, "end_pos": 160, "type": "DATASET", "confidence": 0.711669847369194}]}, {"text": "We described the optimization and finetuning of the MMR system in subsection 5.6.6.", "labels": [], "entities": [{"text": "MMR", "start_pos": 52, "end_pos": 55, "type": "TASK", "confidence": 0.9428327083587646}]}, {"text": "The second column of presents the average relevance scores for this MMR baseline, averaged over the five summary sizes of 5%, 10%, 15%, 20%, and 25% length, for the four devtest set and the four eval set subcorpora; the first column of this table shows the results for the LEAD baseline.", "labels": [], "entities": [{"text": "LEAD baseline", "start_pos": 273, "end_pos": 286, "type": "DATASET", "confidence": 0.7699649930000305}]}, {"text": "We used the optimized baseline MMR parameters and varied only the emphasis parameters for (1) false starts, (2) lead factor, and (3) Q-A sentences, to optimize the CLEAN summaries further.", "labels": [], "entities": [{"text": "MMR", "start_pos": 31, "end_pos": 34, "type": "TASK", "confidence": 0.9311516880989075}, {"text": "lead factor", "start_pos": 112, "end_pos": 123, "type": "METRIC", "confidence": 0.9161006808280945}]}, {"text": "(Again, for this step, we used only the devtest subcorpora.)", "labels": [], "entities": []}, {"text": "For each corpus in the devtest set, we determined the optimal parameter settings and report the corresponding results also for the eval set subcorpora.", "labels": [], "entities": []}, {"text": "Column 3 in provides the results for this optimized DIASUMM system.", "labels": [], "entities": []}, {"text": "Further, in column 4, we provide the summary accuracy averages for the human gold standard (nucleus IUs only, fixed-length summaries).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 45, "end_pos": 53, "type": "METRIC", "confidence": 0.8367379307746887}, {"text": "human gold standard", "start_pos": 71, "end_pos": 90, "type": "DATASET", "confidence": 0.7313617467880249}]}, {"text": "shows the best emphasis parameter combinations for the DIASUMM summaries used in these evaluations.", "labels": [], "entities": [{"text": "DIASUMM summaries", "start_pos": 55, "end_pos": 72, "type": "DATASET", "confidence": 0.8289541304111481}]}, {"text": "We determined the statistical differences between the DIASUMM system and the two baselines for the eval set, using the Wilcoxon rank sum test for each of the four", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1  Data characteristics for the corpus (average over dialogues). 8E-CH, 4E-CH: English  CallHome; NHOUR: NewsHour; XFIRE: CrossFire; G-MTG: Group Meetings.", "labels": [], "entities": [{"text": "English  CallHome", "start_pos": 86, "end_pos": 103, "type": "DATASET", "confidence": 0.8876687586307526}, {"text": "NHOUR: NewsHour", "start_pos": 105, "end_pos": 120, "type": "DATASET", "confidence": 0.8056860963503519}]}, {"text": " Table 2  Nuclei and satellites: Length in tokens and relative frequency (in % of all tokens).", "labels": [], "entities": [{"text": "Length", "start_pos": 33, "end_pos": 39, "type": "METRIC", "confidence": 0.9986525774002075}, {"text": "relative frequency", "start_pos": 54, "end_pos": 72, "type": "METRIC", "confidence": 0.7333745360374451}]}, {"text": " Table 3  Intercoder annotation \u03ba agreement for topical boundaries and relevance markings.", "labels": [], "entities": []}, {"text": " Table 6  POS tagging accuracy on five subcorpora (evaluated on 500-word samples).", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 10, "end_pos": 21, "type": "TASK", "confidence": 0.7582235634326935}, {"text": "accuracy", "start_pos": 22, "end_pos": 30, "type": "METRIC", "confidence": 0.9837223887443542}]}, {"text": " Table 8  False start classification results for different corpora (F 1 ).", "labels": [], "entities": [{"text": "False start", "start_pos": 10, "end_pos": 21, "type": "METRIC", "confidence": 0.9255910813808441}]}, {"text": " Table 9  Detection accuracy for repairs on the basis of individual word tokens using the repetition filter.", "labels": [], "entities": [{"text": "Detection", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.8722201585769653}, {"text": "accuracy", "start_pos": 20, "end_pos": 28, "type": "METRIC", "confidence": 0.8832969069480896}]}, {"text": " Table 10  Sentence boundary detection accuracy (F 1 -score).", "labels": [], "entities": [{"text": "Sentence boundary detection", "start_pos": 11, "end_pos": 38, "type": "TASK", "confidence": 0.8266970912615458}, {"text": "accuracy", "start_pos": 39, "end_pos": 47, "type": "METRIC", "confidence": 0.9425874352455139}, {"text": "F 1 -score)", "start_pos": 49, "end_pos": 60, "type": "METRIC", "confidence": 0.9757615685462951}]}, {"text": " Table 14  Question detection on the 8E-CH corpus using two different classifiers.", "labels": [], "entities": [{"text": "Question detection", "start_pos": 11, "end_pos": 29, "type": "TASK", "confidence": 0.8218661844730377}, {"text": "8E-CH corpus", "start_pos": 37, "end_pos": 49, "type": "DATASET", "confidence": 0.9529600739479065}]}, {"text": " Table 15  Q-A detection results using two different classifiers for question detection (68 Q-A pairs to be  detected).", "labels": [], "entities": [{"text": "Q-A detection", "start_pos": 11, "end_pos": 24, "type": "TASK", "confidence": 0.6150622814893723}, {"text": "question detection", "start_pos": 69, "end_pos": 87, "type": "TASK", "confidence": 0.7666717767715454}]}, {"text": " Table 16  Performance comparison for Q-and Q-A detection (Q-detection with the decision tree  question classifier).", "labels": [], "entities": [{"text": "Q-and Q-A detection", "start_pos": 38, "end_pos": 57, "type": "TASK", "confidence": 0.566691925128301}]}, {"text": " Table 18  Average summary accuracy scores: devtest set and eval set subcorpora on optimized  parameters, comparing LEAD, MMR baseline, DIASUMM, and the human gold standard.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 27, "end_pos": 35, "type": "METRIC", "confidence": 0.9205263257026672}, {"text": "LEAD", "start_pos": 116, "end_pos": 120, "type": "METRIC", "confidence": 0.9709950685501099}]}, {"text": " Table 20  Average summary accuracy scores for different system configurations for the four different  subcorpora.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 27, "end_pos": 35, "type": "METRIC", "confidence": 0.86665278673172}]}]}