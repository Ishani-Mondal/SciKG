{"title": [], "abstractContent": [{"text": "We develop anew computational model for representing the fine-grained meanings of near-synonyms and the differences between them.", "labels": [], "entities": []}, {"text": "We also develop a lexical-choice process that can decide which of several near-synonyms is most appropriate in a particular situation.", "labels": [], "entities": []}, {"text": "This research has direct applications in machine translation and text generation.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 41, "end_pos": 60, "type": "TASK", "confidence": 0.829651802778244}, {"text": "text generation", "start_pos": 65, "end_pos": 80, "type": "TASK", "confidence": 0.8266091346740723}]}, {"text": "We first identify the problems of representing near-synonyms in a computational lexicon and show that no previous model adequately accounts for near-synonymy.", "labels": [], "entities": []}, {"text": "We then propose a preliminary theory to account for near-synonymy, relying crucially on the notion of granularity of representation, in which the meaning of a word arises out of a context-dependent combination of a context-independent core meaning and a set of explicit differences to its near-synonyms.", "labels": [], "entities": []}, {"text": "That is, near-synonyms cluster together.", "labels": [], "entities": []}, {"text": "We then develop a clustered model of lexical knowledge, derived from the conventional ontological model.", "labels": [], "entities": []}, {"text": "The model cuts off the ontology at a coarse grain, thus avoiding an awkward proliferation of language-dependent concepts in the ontology, yet maintaining the advantages of efficient computation and reasoning.", "labels": [], "entities": []}, {"text": "The model groups near-synonyms into subconceptual clusters that are linked to the ontology.", "labels": [], "entities": []}, {"text": "A cluster differentiates near-synonyms in terms of fine-grained aspects of denotation, implication, expressed attitude, and style.", "labels": [], "entities": []}, {"text": "The model is general enough to account for other types of variation, for instance, in collocational behavior.", "labels": [], "entities": []}, {"text": "An efficient, robust, and flexible fine-grained lexical-choice process is a consequence of a clustered model of lexical knowledge.", "labels": [], "entities": []}, {"text": "To make it work, we formalize criteria for lexical choice as preferences to express certain concepts with varying indirectness, to express attitudes, and to establish certain styles.", "labels": [], "entities": []}, {"text": "The lexical-choice process itself works on two tiers: between clusters and between near-synonyns of clusters.", "labels": [], "entities": []}, {"text": "We describe our prototype implementation of the system, called I-Saurus.", "labels": [], "entities": []}], "introductionContent": [{"text": "A word can express a myriad of implications, connotations, and attitudes in addition to its basic \"dictionary\" meaning.", "labels": [], "entities": []}, {"text": "And a word often has near-synonyms that differ from it solely in these nuances of meaning.", "labels": [], "entities": []}, {"text": "So, in order to find the right word to use in any particular situation-the one that precisely conveys the desired meaning and yet avoids unwanted implications-one must carefully consider the differences between all of the options.", "labels": [], "entities": []}, {"text": "Choosing the right word can be difficult for people, let alone present-day computer systems.", "labels": [], "entities": [{"text": "Choosing the right word", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.890989750623703}]}, {"text": "For example, how can a machine translation (MT) system determine the best English word for the French b \u00b4 evue when there are so many possible similar but slightly different translations?", "labels": [], "entities": [{"text": "machine translation (MT)", "start_pos": 23, "end_pos": 47, "type": "TASK", "confidence": 0.8300966262817383}]}, {"text": "The system could choose error, mistake, blunder, slip, lapse, boner, faux pas, boo-boo, and soon, but the most appropriate choice is a function of how b \u00b4 evue is used (in context) and of the difference in meaning between b \u00b4 evue and each of the English possibilities.", "labels": [], "entities": [{"text": "error", "start_pos": 24, "end_pos": 29, "type": "METRIC", "confidence": 0.9665342569351196}, {"text": "mistake", "start_pos": 31, "end_pos": 38, "type": "METRIC", "confidence": 0.8585841059684753}, {"text": "boner", "start_pos": 62, "end_pos": 67, "type": "METRIC", "confidence": 0.9081358313560486}]}, {"text": "Not only must the system determine the nuances that b \u00b4 evue conveys in the particular context in which it has been used, but it must also find the English word (or words) that most closely convey the same nuances in the context of the other words that it is choosing concurrently.", "labels": [], "entities": [{"text": "b \u00b4 evue", "start_pos": 52, "end_pos": 60, "type": "DATASET", "confidence": 0.8188297152519226}]}, {"text": "An exact translation is probably impossible, for b \u00b4 evue is in all likelihood as different from each of its possible translations as they are from each other.", "labels": [], "entities": []}, {"text": "That is, in general, every translation possibility will omit some nuance or express some other possibly unwanted nuance.", "labels": [], "entities": []}, {"text": "Thus, faithful translation requires a sophisticated lexical-choice process that can determine which of the near-synonyms provided by one language fora word in another language is the closest or most appropriate in any particular situation.", "labels": [], "entities": [{"text": "faithful translation", "start_pos": 6, "end_pos": 26, "type": "TASK", "confidence": 0.6623403131961823}]}, {"text": "More generally, a truly articulate natural language generation (NLG) system also requires a sophisticated lexical-choice process.", "labels": [], "entities": [{"text": "articulate natural language generation (NLG)", "start_pos": 24, "end_pos": 68, "type": "TASK", "confidence": 0.825457581451961}]}, {"text": "The system must to be able to reason about the potential effects of every available option.", "labels": [], "entities": []}, {"text": "Consider, too, the possibility of anew type of thesaurus fora word processor that, instead of merely presenting the writer with a list of similar words, actually assists the writer by ranking the options according to their appropriateness in context and in meeting general preferences set by the writer.", "labels": [], "entities": []}, {"text": "Such an intelligent thesaurus would greatly benefit many writers and would be a definite improvement over the simplistic thesauri in current word processors.", "labels": [], "entities": []}, {"text": "What is needed is a comprehensive computational model of fine-grained lexical knowledge.", "labels": [], "entities": []}, {"text": "Yet although synonymy is one of the fundamental linguistic phenomena that influence the structure of the lexicon, it has been given far less attention in linguistics, psychology, lexicography, semantics, and computational linguistics than the equally fundamental and much-studied polysemy.", "labels": [], "entities": []}, {"text": "Whatever the reasons-philosophy, practicality, or expedience-synonymy has often been thought of as a \"non-problem\": either there are synonyms, but they are completely identical in meaning and hence easy to deal with, or there are no synonyms, in which case each word can be handled like any other.", "labels": [], "entities": []}, {"text": "But our investigation of near-synonymy shows that it is just as complex a phenomenon as polysemy and that it inherently affects the structure of lexical knowledge.", "labels": [], "entities": []}, {"text": "The goal of our research has been to develop a computational model of lexical knowledge that can adequately account for near-synonymy and to deploy such a model in a computational process that could \"choose the right word\" in any situation of language production.", "labels": [], "entities": []}, {"text": "Upon surveying current machine translation and natural language generation systems, we found none that performed this kind of genuine lexical choice.", "labels": [], "entities": [{"text": "machine translation and natural language generation", "start_pos": 23, "end_pos": 74, "type": "TASK", "confidence": 0.6462589353322983}]}, {"text": "Although major advances have been made in knowledge-based models of the lexicon, present systems are concerned more with structural paraphrasing and a level of semantics allied to syntactic structure.", "labels": [], "entities": []}, {"text": "None captures the fine-grained meanings of, and differences between, near-synonyms, nor the myriad of criteria involved in lexical choice.", "labels": [], "entities": []}, {"text": "Indeed, the theories of lexical semantics upon which presentday systems are based don't even account for indirect, fuzzy, or context-dependent meanings, let alone near-synonymy.", "labels": [], "entities": []}, {"text": "And frustratingly, no one yet knows how to implement the theories that do more accurately predict the nature of word meaning (for instance, those in cognitive linguistics) in a computational system (see Hirst).", "labels": [], "entities": []}, {"text": "In this article, we present anew model of lexical knowledge that explicitly accounts for near-synonymy in a computationally implementable manner.", "labels": [], "entities": []}, {"text": "The clustered model of lexical knowledge clusters each set of near-synonyms under a common, coarse-Error implies a straying from a proper course and suggests guilt as may lie in failure to take proper advantage of a guide.", "labels": [], "entities": [{"text": "coarse-Error", "start_pos": 92, "end_pos": 104, "type": "METRIC", "confidence": 0.9630820155143738}]}, {"text": "Mistake implies misconception, misunderstanding, a wrong but not always blameworthy judgment, or inadvertence; it expresses less severe criticism than error.", "labels": [], "entities": [{"text": "Mistake", "start_pos": 0, "end_pos": 7, "type": "TASK", "confidence": 0.9586097598075867}]}, {"text": "Blunder is harsher than mistake or error; it commonly implies ignorance or stupidity, sometimes blameworthiness.", "labels": [], "entities": [{"text": "Blunder", "start_pos": 0, "end_pos": 7, "type": "TASK", "confidence": 0.9096792340278625}]}, {"text": "Slip carries a stronger implication of inadvertence or accident than mistake, and often, in addition, connotes triviality.", "labels": [], "entities": [{"text": "Slip", "start_pos": 0, "end_pos": 4, "type": "TASK", "confidence": 0.6890222430229187}, {"text": "mistake", "start_pos": 69, "end_pos": 76, "type": "METRIC", "confidence": 0.9701992869377136}]}, {"text": "Lapse, though sometimes used interchangeably with slip, stresses forgetfulness, weakness, or inattention more than accident; thus, one says a lapse of memory or a slip of the pen, but not vice versa.", "labels": [], "entities": [{"text": "Lapse", "start_pos": 0, "end_pos": 5, "type": "METRIC", "confidence": 0.9384468197822571}, {"text": "slip", "start_pos": 50, "end_pos": 54, "type": "METRIC", "confidence": 0.9880988001823425}]}, {"text": "Faux pas is most frequently applied to a mistake in etiquette.", "labels": [], "entities": [{"text": "Faux", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.9329143166542053}]}, {"text": "Bull, howler, and boner are rather informal terms applicable to blunders that typically have an amusing aspect.", "labels": [], "entities": [{"text": "boner", "start_pos": 18, "end_pos": 23, "type": "METRIC", "confidence": 0.9863089323043823}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 3  Four simultaneous preferences and the six candidates of the untruth C cluster.", "labels": [], "entities": []}]}