{"title": [{"text": "Summarizing Scientific Articles: Experiments with Relevance and Rhetorical Status", "labels": [], "entities": [{"text": "Summarizing Scientific Articles", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.9139021635055542}]}], "abstractContent": [{"text": "In this article we propose a strategy for the summarization of scientific articles that concentrates on the rhetorical status of statements in an article: Material for summaries is selected in such away that summaries can highlight the new contribution of the source article and situate it with respect to earlier work.", "labels": [], "entities": [{"text": "summarization of scientific articles", "start_pos": 46, "end_pos": 82, "type": "TASK", "confidence": 0.8860341906547546}]}, {"text": "We provide a gold standard for summaries of this kind consisting of a substantial corpus of conference articles in computational linguistics annotated with human judgments of the rhetorical status and relevance of each sentence in the articles.", "labels": [], "entities": [{"text": "summaries", "start_pos": 31, "end_pos": 40, "type": "TASK", "confidence": 0.9686296582221985}]}, {"text": "We present several experiments measuring our judges' agreement on these annotations.", "labels": [], "entities": [{"text": "agreement", "start_pos": 53, "end_pos": 62, "type": "METRIC", "confidence": 0.9521270990371704}]}, {"text": "We also present an algorithm that, on the basis of the annotated training material, selects content from unseen articles and classifies it into a fixed set of seven rhetorical categories.", "labels": [], "entities": []}, {"text": "The output of this extraction and classification system can be viewed as a single-document summary in its own right; alternatively, it provides starting material for the generation of task-oriented and user-tailored summaries designed to give users an overview of a scientific field.", "labels": [], "entities": [{"text": "extraction and classification", "start_pos": 19, "end_pos": 48, "type": "TASK", "confidence": 0.6686626970767975}]}], "introductionContent": [{"text": "Summarization systems are often two-phased, consisting of a content selection step followed by a regeneration step.", "labels": [], "entities": [{"text": "Summarization", "start_pos": 0, "end_pos": 13, "type": "TASK", "confidence": 0.968421459197998}]}, {"text": "In the first step, text fragments (sentences or clauses) are assigned a score that reflects how important or contentful they are.", "labels": [], "entities": []}, {"text": "The highestranking material can then be extracted and displayed verbatim as \"extracts\".", "labels": [], "entities": []}, {"text": "Extracts are often useful in an information retrieval environment since they give users an idea as to what the source document is about), but they are texts of relatively low quality.", "labels": [], "entities": []}, {"text": "Because of this, it is generally accepted that some kind of postprocessing should be performed to improve the final result, by shortening, fusing, or otherwise revising the material.", "labels": [], "entities": []}, {"text": "The extent to which it is possible to do postprocessing is limited, however, by the fact that contentful material is extracted without information about the general discourse context in which the material occurred in the source text.", "labels": [], "entities": []}, {"text": "For instance, a sentence describing the solution to a scientific problem might give the main contri-bution of the paper, but it might also refer to a previous approach that the authors criticize.", "labels": [], "entities": []}, {"text": "Depending on its rhetorical context, the same sentence should be treated very differently in a summary.", "labels": [], "entities": []}, {"text": "We propose in this article a method for sentence and content selection from source texts that adds context in the form of information about the rhetorical role the extracted material plays in the source text.", "labels": [], "entities": [{"text": "sentence and content selection from source texts", "start_pos": 40, "end_pos": 88, "type": "TASK", "confidence": 0.7401639478547233}]}, {"text": "This added contextual information can then be used to make the end product more informative and more valuable than sentence extracts.", "labels": [], "entities": []}, {"text": "Our application domain is the summarization of scientific articles.", "labels": [], "entities": [{"text": "summarization of scientific articles", "start_pos": 30, "end_pos": 66, "type": "TASK", "confidence": 0.8310951888561249}]}, {"text": "Summarization of such texts requires a different approach from, for example, that used in the summarization of news articles.", "labels": [], "entities": [{"text": "summarization of news articles", "start_pos": 94, "end_pos": 124, "type": "TASK", "confidence": 0.9325098991394043}]}, {"text": "For example, introduce the concept of information fusion, which is based on the identification of recurrent descriptions of the same events in news articles.", "labels": [], "entities": [{"text": "information fusion", "start_pos": 38, "end_pos": 56, "type": "TASK", "confidence": 0.7774131000041962}, {"text": "identification of recurrent descriptions of the same events in news articles", "start_pos": 80, "end_pos": 156, "type": "TASK", "confidence": 0.7942545359784906}]}, {"text": "This approach works well because in the news domain, newsworthy events are frequently repeated over a short period of time.", "labels": [], "entities": []}, {"text": "In scientific writing, however, similar \"events\" are rare: The main focus is on new scientific ideas, whose main characteristic is their uniqueness and difference from previous ideas.", "labels": [], "entities": []}, {"text": "Other approaches to the summarization of news articles make use of the typical journalistic writing style, for example, the fact that the most newsworthy information comes first; as a result, the first few sentences of a news article are good candidates fora summary.", "labels": [], "entities": [{"text": "summarization of news articles", "start_pos": 24, "end_pos": 54, "type": "TASK", "confidence": 0.9125319868326187}]}, {"text": "The structure of scientific articles does not reflect relevance this explicitly.", "labels": [], "entities": []}, {"text": "Instead, the introduction often starts with general statements about the importance of the topic and its history in the field; the actual contribution of the paper itself is often given much later.", "labels": [], "entities": []}, {"text": "The length of scientific articles presents another problem.", "labels": [], "entities": []}, {"text": "Let us assume that our overall summarization strategy is first to select relevant sentences or concepts, and then to synthesize summaries using this material.", "labels": [], "entities": [{"text": "summarization", "start_pos": 31, "end_pos": 44, "type": "TASK", "confidence": 0.9880067110061646}]}, {"text": "For atypical 10-to 20-sentence news wire story, a compression to 20% or 30% of the source provides a reasonable input set for the second step.", "labels": [], "entities": []}, {"text": "The extracted sentences are still thematically connected, and concepts in the sentences are not taken completely out of context.", "labels": [], "entities": []}, {"text": "In scientific articles, however, the compression rates have to be much higher: Shortening a 20-page journal article to a half-page summary requires a compression to 2.5% of the original.", "labels": [], "entities": [{"text": "compression", "start_pos": 37, "end_pos": 48, "type": "METRIC", "confidence": 0.9917787909507751}, {"text": "compression", "start_pos": 150, "end_pos": 161, "type": "METRIC", "confidence": 0.9591838121414185}]}, {"text": "Here, the problematic fact that sentence selection is context insensitive does make a qualitative difference.", "labels": [], "entities": [{"text": "sentence selection", "start_pos": 32, "end_pos": 50, "type": "TASK", "confidence": 0.7099553197622299}]}, {"text": "If only one sentence per two pages is selected, all information about how the extracted sentences and their concepts relate to each other is lost; without additional information, it is difficult to use the selected sentences as input to the second stage.", "labels": [], "entities": []}, {"text": "We present an approach to summarizing scientific articles that is based on the idea of restoring the discourse context of extracted material by adding the rhetorical status to each sentence in a document.", "labels": [], "entities": [{"text": "summarizing scientific articles", "start_pos": 26, "end_pos": 57, "type": "TASK", "confidence": 0.9205924272537231}]}, {"text": "The innovation of our approach is that it defines principles for content selection specifically for scientific articles and that it combines sentence extraction with robust discourse analysis.", "labels": [], "entities": [{"text": "sentence extraction", "start_pos": 141, "end_pos": 160, "type": "TASK", "confidence": 0.7658097743988037}]}, {"text": "The output of our system is a list of extracted sentences along with their rhetorical status (e.g. sentence 11 describes the scientific goal of the paper, and sentence 9 criticizes previous work), as illustrated in.", "labels": [], "entities": []}, {"text": "(The example paper we use throughout the article is F.", "labels": [], "entities": []}, {"text": "Pereira, N. Tishby, and L. Lee's \"Distributional Clustering of English Words\" [ACL-1993, cmp lg/9408011]; it was chosen because it is the paper most often cited within our collection.)", "labels": [], "entities": []}, {"text": "Such lists serve two purposes: in themselves, they already provide a better characterization of scientific articles than sentence extracts do, and in the longer run, they will serve as better input material for further processing.", "labels": [], "entities": []}, {"text": "An extrinsic evaluation shows that the output of our system is already a useful document surrogate in its own right.", "labels": [], "entities": []}], "datasetContent": [{"text": "[sic] models with substantial predictive power.", "labels": [], "entities": []}, {"text": "Contrast with Other Approaches/Weaknesses of Other Approaches: 9 His notion of similarity seems to agree with our intuitions in many cases, but it is not clear how it can be used directly to construct word classes and corresponding models of association.", "labels": [], "entities": []}, {"text": "14 Class construction is then combinatorially very demanding and depends on frequency counts for joint events involving particular words, a potentially unreliable source of information as we noted above.", "labels": [], "entities": [{"text": "Class construction", "start_pos": 3, "end_pos": 21, "type": "TASK", "confidence": 0.7302175462245941}]}, {"text": "41 However, this is not very satisfactory because one of the goals of our work is precisely to avoid the problems of data sparseness by grouping words into classes.", "labels": [], "entities": []}, {"text": "Three task-trained annotators were used: Annotators A and B have degrees in cognitive science and speech therapy.", "labels": [], "entities": []}, {"text": "They were paid for the experiment.", "labels": [], "entities": []}, {"text": "Both are well-used to reading scientific articles for their studies and roughly understand the contents of the articles they annotated because of the closeness of their fields to computational linguistics.", "labels": [], "entities": []}, {"text": "Annotator C is the first author.", "labels": [], "entities": [{"text": "Annotator", "start_pos": 0, "end_pos": 9, "type": "DATASET", "confidence": 0.9251771569252014}]}, {"text": "We did not want to declare annotator C the expert annotator; we believe that in subjective tasks like the one described here, there are no real experts.", "labels": [], "entities": []}, {"text": "Written guidelines (17 pages) describe the semantics of the categories, ambiguous cases, and decision strategies.", "labels": [], "entities": []}, {"text": "The guidelines also include the decision tree reproduced in.", "labels": [], "entities": []}, {"text": "Annotators received a total of 20 hours of training.", "labels": [], "entities": []}, {"text": "Training consisted of the presentation of annotation of six example papers and the annotation of eight training articles under real conditions (i.e., independently).", "labels": [], "entities": []}, {"text": "In subsequent training sessions, decision criteria for difficult cases encountered in the training articles were discussed.", "labels": [], "entities": []}, {"text": "Obviously, the training articles were excluded from measurements of human agreement.", "labels": [], "entities": []}, {"text": "Twenty-five articles were used for annotation.", "labels": [], "entities": []}, {"text": "As no annotation tool was available at the time, annotation was performed on paper; the categories were later transferred to the electronic versions of the articles by hand.", "labels": [], "entities": []}, {"text": "Skim-reading and annotation typically took between 20 and 30 minutes per article, but there were no time restrictions.", "labels": [], "entities": []}, {"text": "No communication between the annotators was allowed during annotation.", "labels": [], "entities": []}, {"text": "Six weeks after the initial annotation, annotators were asked to reannotate 6 random articles out of the 25.", "labels": [], "entities": []}, {"text": "We measured two formal properties of the annotation: stability and reproducibility.", "labels": [], "entities": []}, {"text": "Stability, the extent to which one annotator will produce the same classifications at different times, is important because an unstable annotation scheme can never be reproducible.", "labels": [], "entities": []}, {"text": "Reproducibility, the extent to which different annotators will produce the same classifications, is important because it measures the consistency of shared understandings (or meaning) held between annotators.", "labels": [], "entities": [{"text": "Reproducibility", "start_pos": 0, "end_pos": 15, "type": "METRIC", "confidence": 0.9172411561012268}]}, {"text": "We use the kappa coefficient K ( to measure stability and reproducibility, following.", "labels": [], "entities": [{"text": "kappa coefficient K", "start_pos": 11, "end_pos": 30, "type": "METRIC", "confidence": 0.807658871014913}]}, {"text": "The kappa coefficient is defined as follows: where P(A) is pairwise agreement and P(E) random agreement.", "labels": [], "entities": []}, {"text": "K varies between 1 when agreement is perfect and \u22121 when there is a perfect negative correlation.", "labels": [], "entities": [{"text": "K", "start_pos": 0, "end_pos": 1, "type": "METRIC", "confidence": 0.8706536889076233}]}, {"text": "K = 0 is defined as the level of agreement that would be reached by random annotation using the same distribution of categories as the actual annotators did.", "labels": [], "entities": []}, {"text": "The main advantage of kappa as an annotation measure is that it factors out random agreement by numbers of categories and by their distribution.", "labels": [], "entities": []}, {"text": "As kappa also abstracts over the number of annotators considered, it allows us to compare the agreement numerically among a group of human annotators with the agreement between the system and one or more annotators (section 5), which we use as one of the performance measures of the system.", "labels": [], "entities": []}, {"text": "Our task is to perform content selection from scientific articles, which we do by classifying sentences into seven rhetorical categories.", "labels": [], "entities": [{"text": "content selection from scientific articles", "start_pos": 23, "end_pos": 65, "type": "TASK", "confidence": 0.8491428017616272}]}, {"text": "The summaries based on this classification use some of these sentences directly, namely, sentences that express the contribution of a particular article (AIM), sentences expressing contrasts with other work (CONTRAST), and sentences stating imported solutions from other work (BASIS).", "labels": [], "entities": [{"text": "CONTRAST", "start_pos": 208, "end_pos": 216, "type": "METRIC", "confidence": 0.8034551739692688}, {"text": "BASIS", "start_pos": 277, "end_pos": 282, "type": "METRIC", "confidence": 0.9863938689231873}]}, {"text": "Other, more frequent rhetorical categories, namely OTHER, OWN, and BACKGROUND, might also be extracted into the summary.", "labels": [], "entities": [{"text": "OTHER", "start_pos": 51, "end_pos": 56, "type": "METRIC", "confidence": 0.8100665211677551}, {"text": "OWN", "start_pos": 58, "end_pos": 61, "type": "METRIC", "confidence": 0.6595288515090942}, {"text": "BACKGROUND", "start_pos": 67, "end_pos": 77, "type": "METRIC", "confidence": 0.9285917282104492}]}, {"text": "Because the task is a mixture of extraction and classification, we report system performance as follows: \u2022 We first report precision and recall values for all categories, in comparison to human performance and the text categorization baseline, as we are primarily interested in good performance on the categories AIM, CONTRAST, BASIS, and BACKGROUND.", "labels": [], "entities": [{"text": "precision", "start_pos": 123, "end_pos": 132, "type": "METRIC", "confidence": 0.9992468357086182}, {"text": "recall", "start_pos": 137, "end_pos": 143, "type": "METRIC", "confidence": 0.9974249601364136}, {"text": "BASIS", "start_pos": 328, "end_pos": 333, "type": "METRIC", "confidence": 0.7046522498130798}]}, {"text": "\u2022 We are also interested in good overall classification performance, which we report using kappa and macro-F as our metric.", "labels": [], "entities": []}, {"text": "We also discuss how well each single features does in the classification.", "labels": [], "entities": []}, {"text": "\u2022 We then compare the extracted sentences to our human gold standard for relevance and report the agreement in precision and agreement per category.", "labels": [], "entities": [{"text": "precision", "start_pos": 111, "end_pos": 120, "type": "METRIC", "confidence": 0.9983339905738831}]}], "tableCaptions": [{"text": " Table 2  Confusion matrix between annotators B and C.", "labels": [], "entities": []}, {"text": " Table 5  Formulaic expression lexicon.", "labels": [], "entities": [{"text": "Formulaic expression lexicon", "start_pos": 10, "end_pos": 38, "type": "TASK", "confidence": 0.9125957687695821}]}, {"text": " Table 8  Performance per category: F-measure (F), precision (P) and recall (R).", "labels": [], "entities": [{"text": "F-measure (F)", "start_pos": 36, "end_pos": 49, "type": "METRIC", "confidence": 0.9265976250171661}, {"text": "precision (P)", "start_pos": 51, "end_pos": 64, "type": "METRIC", "confidence": 0.9580761790275574}, {"text": "recall (R)", "start_pos": 69, "end_pos": 79, "type": "METRIC", "confidence": 0.9664388746023178}]}, {"text": " Table 9  Confusion matrix: Human versus automatic annotation.", "labels": [], "entities": []}, {"text": " Table 10  Overall classification results.", "labels": [], "entities": []}, {"text": " Table 11  Precision and recall of rhetorical classification, individual features.", "labels": [], "entities": [{"text": "recall", "start_pos": 25, "end_pos": 31, "type": "METRIC", "confidence": 0.9960590600967407}, {"text": "rhetorical classification", "start_pos": 35, "end_pos": 60, "type": "TASK", "confidence": 0.859767735004425}]}, {"text": " Table 12  Relevance by human selection: Precision (P) and recall (R).", "labels": [], "entities": [{"text": "Precision (P)", "start_pos": 41, "end_pos": 54, "type": "METRIC", "confidence": 0.935998946428299}, {"text": "recall (R)", "start_pos": 59, "end_pos": 69, "type": "METRIC", "confidence": 0.9512084424495697}]}]}