{"title": [{"text": "Introduction to the Special Issue on Summarization", "labels": [], "entities": [{"text": "Summarization", "start_pos": 37, "end_pos": 50, "type": "TASK", "confidence": 0.9602575302124023}]}], "abstractContent": [], "introductionContent": [], "datasetContent": [{"text": "Evaluating the quality of a summary has proven to be a difficult problem, principally because there is no obvious \"ideal\" summary.", "labels": [], "entities": []}, {"text": "Even for relatively straightforward news articles, human summarizers tend to agree only approximately 60% of the time, measuring sentence content overlap.", "labels": [], "entities": []}, {"text": "The use of multiple models for system evaluation could help alleviate this problem, but researchers also need to look at other methods that can yield more acceptable models, perhaps using a task as motivation.", "labels": [], "entities": []}, {"text": "Two broad classes of metrics have been developed: form metrics and content metrics.", "labels": [], "entities": []}, {"text": "Form metrics focus on grammaticality, overall text coherence, and organization and are usually measured on a point scale.", "labels": [], "entities": []}, {"text": "Content is more difficult to measure.", "labels": [], "entities": []}, {"text": "Typically, system output is compared sentence by sentence or fragment by fragment to one or more human-made ideal abstracts, and as in information retrieval, the percentage of extraneous information present in the system's summary (precision) and the percentage of important information omitted from the summary (recall) are recorded.", "labels": [], "entities": [{"text": "information retrieval", "start_pos": 135, "end_pos": 156, "type": "TASK", "confidence": 0.6911564618349075}, {"text": "precision", "start_pos": 232, "end_pos": 241, "type": "METRIC", "confidence": 0.9887223243713379}, {"text": "recall", "start_pos": 313, "end_pos": 319, "type": "METRIC", "confidence": 0.9948886036872864}]}, {"text": "Other commonly used measures include kappa and relative utility (Radev, Jing, and Budzikowska 2000), both of which take into account the performance of a summarizer that randomly picks passages from the original document to produce an extract.", "labels": [], "entities": [{"text": "kappa", "start_pos": 37, "end_pos": 42, "type": "METRIC", "confidence": 0.9503494501113892}, {"text": "relative utility", "start_pos": 47, "end_pos": 63, "type": "METRIC", "confidence": 0.8077890872955322}]}, {"text": "In the Document Understanding Conference (DUC)-01 and DUC-02 summarization competitions, NIST used the Summary Evaluation Environment (SEE) interface to record values for precision and recall.", "labels": [], "entities": [{"text": "Document Understanding Conference (DUC)-01", "start_pos": 7, "end_pos": 49, "type": "TASK", "confidence": 0.759364651782172}, {"text": "NIST", "start_pos": 89, "end_pos": 93, "type": "DATASET", "confidence": 0.8767131567001343}, {"text": "precision", "start_pos": 171, "end_pos": 180, "type": "METRIC", "confidence": 0.997595489025116}, {"text": "recall", "start_pos": 185, "end_pos": 191, "type": "METRIC", "confidence": 0.9904248118400574}]}, {"text": "These two competitions, run along the lines of TREC, have served to establish overall baselines for single-document and multidocument summarization and have provided several hundred human abstracts as training material.", "labels": [], "entities": []}, {"text": "(Another popular source of training material is the Ziff-Davis corpus of computer product announcements.)", "labels": [], "entities": []}, {"text": "Despite low interjudge agreement, DUC has shown that humans are better summary producers than machines and that, for the news article genre, certain algorithms do in fact do better than the simple baseline of picking the lead material.", "labels": [], "entities": [{"text": "DUC", "start_pos": 34, "end_pos": 37, "type": "DATASET", "confidence": 0.8139863610267639}]}, {"text": "The largest task-oriented evaluation to date, the Summarization Evaluation Conference (SUMMAC) () included three tests: the categorization task (how well can humans categorize a summary compared to its full text?), the ad hoc task (how well can humans determine whether a full text is relevant to a query just from reading the summary?) and the question task (how well can humans answer questions about the main thrust of the source text from reading just the summary?).", "labels": [], "entities": [{"text": "Summarization Evaluation Conference (SUMMAC)", "start_pos": 50, "end_pos": 94, "type": "TASK", "confidence": 0.861567348241806}]}, {"text": "But the interpretation of the results is not simple; studies ( show how the same summaries receive different scores under different measures or when compared to different (but presumably equivalent) ideal summaries created by humans.", "labels": [], "entities": []}, {"text": "With regard to interhuman agreement, Jing et al. find fairly high consistency in the news genre only when the summary (extract) length is fixed relatively short.", "labels": [], "entities": [{"text": "consistency", "start_pos": 66, "end_pos": 77, "type": "METRIC", "confidence": 0.9923868179321289}]}, {"text": "Marcu (1997a) provides some evidence that other genres will deliver less consistency.", "labels": [], "entities": [{"text": "consistency", "start_pos": 73, "end_pos": 84, "type": "METRIC", "confidence": 0.959473192691803}]}, {"text": "With regard to the lengths of the summaries produced by humans when not constrained by a particular compression rate, both Jing and Marcu find great variation.", "labels": [], "entities": []}, {"text": "Nonetheless, it is now generally accepted that for single news articles, systems produce generic summaries indistinguishable from those of humans.", "labels": [], "entities": []}, {"text": "Automated summary evaluation is agleam in everyone's eye.", "labels": [], "entities": [{"text": "Automated summary evaluation", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.5849362313747406}]}, {"text": "Clearly, when an ideal extract has been created by human(s), extractive summaries are easy to evaluate.", "labels": [], "entities": []}, {"text": "Marcu (1999) and independently developed an automated method to create extracts corresponding to abstracts.", "labels": [], "entities": []}, {"text": "But when the number of available extracts is not sufficient, it is not clear how to overcome the problems of low interhuman agreement.", "labels": [], "entities": []}, {"text": "Simply using a variant of the Bilingual Evaluation Understudy (BLEU) scoring method (based on a linear combination of matching n-grams between the system output and the ideal summary) developed for machine translation) is promising but not sufficient (Lin and Hovy 2002b).", "labels": [], "entities": [{"text": "Bilingual Evaluation Understudy (BLEU) scoring", "start_pos": 30, "end_pos": 76, "type": "METRIC", "confidence": 0.6535632227148328}, {"text": "machine translation", "start_pos": 198, "end_pos": 217, "type": "TASK", "confidence": 0.7980168163776398}]}], "tableCaptions": []}