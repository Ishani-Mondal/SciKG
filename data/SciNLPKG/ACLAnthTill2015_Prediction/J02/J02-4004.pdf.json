{"title": [{"text": "Efficiently Computed Lexical Chains as an Intermediate Representation for Automatic Text Summarization", "labels": [], "entities": [{"text": "Automatic Text Summarization", "start_pos": 74, "end_pos": 102, "type": "TASK", "confidence": 0.6196819146474203}]}], "abstractContent": [{"text": "While automatic text summarization is an area that has received a great deal of attention in recent research, the problem of efficiency in this task has not been frequently addressed.", "labels": [], "entities": [{"text": "automatic text summarization", "start_pos": 6, "end_pos": 34, "type": "TASK", "confidence": 0.6725972592830658}]}, {"text": "When the size and quantity of documents available on the Internet and from other sources are considered, the need fora highly efficient tool that produces usable summaries is clear.", "labels": [], "entities": []}, {"text": "We present a linear-time algorithm for lexical chain computation.", "labels": [], "entities": [{"text": "lexical chain computation", "start_pos": 39, "end_pos": 64, "type": "TASK", "confidence": 0.6226162711779276}]}, {"text": "The algorithm makes lexical chains a computationally feasible candidate as an intermediate representation for automatic text summarization.", "labels": [], "entities": [{"text": "text summarization", "start_pos": 120, "end_pos": 138, "type": "TASK", "confidence": 0.6412513107061386}]}, {"text": "A method for evaluating lexical chains as an intermediate step in summarization is also presented and carried out.", "labels": [], "entities": [{"text": "summarization", "start_pos": 66, "end_pos": 79, "type": "TASK", "confidence": 0.9824923276901245}]}, {"text": "Such an evaluation was heretofore not possible because of the computational complexity of previous lexical chains algorithms.", "labels": [], "entities": []}], "introductionContent": [{"text": "The overall motivation for the research presented in this article is the development of a computationally efficient system to create summaries automatically.", "labels": [], "entities": []}, {"text": "Summarization has been viewed as a two-step process.", "labels": [], "entities": [{"text": "Summarization", "start_pos": 0, "end_pos": 13, "type": "TASK", "confidence": 0.985325813293457}]}, {"text": "The first step is the extraction of important concepts from the source text by building an intermediate representation of some sort.", "labels": [], "entities": []}, {"text": "The second step uses this intermediate representation to generate a summary.", "labels": [], "entities": []}, {"text": "In the research presented here, we concentrate on the first step of the summarization process and follow in employing lexical chains to extract important concepts from a document.", "labels": [], "entities": [{"text": "summarization process", "start_pos": 72, "end_pos": 93, "type": "TASK", "confidence": 0.9027901589870453}]}, {"text": "We present a linear-time algorithm for lexical chain computation and offer an evaluation that indicates that such chains area promising avenue of study as an intermediate representation in the summarization process.", "labels": [], "entities": [{"text": "summarization process", "start_pos": 193, "end_pos": 214, "type": "TASK", "confidence": 0.9216096997261047}]}, {"text": "proposed lexical chains as an intermediate step in the text summarization process.", "labels": [], "entities": [{"text": "text summarization", "start_pos": 55, "end_pos": 73, "type": "TASK", "confidence": 0.7318691909313202}]}, {"text": "Attempts to determine the benefit of this proposal have been faced with a number of difficulties.", "labels": [], "entities": []}, {"text": "First, previous methods for computing lexical chains have either been manual ( or automated, but with exponential efficiency (Hirst and St.-Onge 1997;.", "labels": [], "entities": []}, {"text": "Because of this, computing lexical chains for documents of any reasonable size has been impossible.", "labels": [], "entities": []}, {"text": "We present here an algorithm for computing lexical chains that is linear in space and time.", "labels": [], "entities": []}, {"text": "This algorithm makes the computation of lexical chains computationally feasible even for large documents.", "labels": [], "entities": []}, {"text": "A second difficulty faced in evaluating Barzilay and Elhadad's proposal is that it is a proposal for the first stage of the summarization process, and it is not clear how to evaluate this stage independent of the second stage of summarization.", "labels": [], "entities": [{"text": "summarization process", "start_pos": 124, "end_pos": 145, "type": "TASK", "confidence": 0.9308086931705475}, {"text": "summarization", "start_pos": 229, "end_pos": 242, "type": "TASK", "confidence": 0.9846388697624207}]}, {"text": "A second contribution of this article is a method for evaluating lexical chains as an intermediate representation.", "labels": [], "entities": []}, {"text": "The intuition behind the method is as follows.", "labels": [], "entities": []}, {"text": "The (strong) lexical chains in a document are intended to identify important (noun) concepts in the document.", "labels": [], "entities": []}, {"text": "Our evaluation requires access to documents that have corresponding humangenerated summaries.", "labels": [], "entities": []}, {"text": "We run our lexical chain algorithm both on the document and on the summary and examine (1) how many of the concepts from strong lexical chains in the document also occur in the summary and (2) how many of the (noun) concepts appearing in the summary are represented in strong lexical chains in the document.", "labels": [], "entities": []}, {"text": "Essentially, if lexical chains area good intermediate representation for text summarization, we expect that concepts identified as important according to the lexical chains will be the concepts found in the summary.", "labels": [], "entities": [{"text": "text summarization", "start_pos": 73, "end_pos": 91, "type": "TASK", "confidence": 0.6555045545101166}]}, {"text": "Our evaluation of 24 documents with summaries indicates that indeed lexical chains do appear to be a promising avenue of future research in text summarization.", "labels": [], "entities": [{"text": "text summarization", "start_pos": 140, "end_pos": 158, "type": "TASK", "confidence": 0.741373598575592}]}], "datasetContent": [{"text": "Our algorithm now makes it feasible to use lexical chains as the method for identifying important concepts in a document, and thus they may now form the basis of an intermediate representation for summary generation, as proposed by Barzilay and Elhadad.", "labels": [], "entities": [{"text": "summary generation", "start_pos": 197, "end_pos": 215, "type": "TASK", "confidence": 0.8537037372589111}]}, {"text": "An important consequence of this is that Barzilay and Elhadad's proposal can now be evaluated on documents of substantial size.", "labels": [], "entities": []}, {"text": "We propose an evaluation of this intermediate stage that is independent of the generation phase of summarization.", "labels": [], "entities": []}, {"text": "This said, we make no attempt to claim that a summary can actually be generated from this representation; we do attempt, however, to show that the concepts found in a human-generated summary are indeed the concepts identified by our lexical chains algorithm.", "labels": [], "entities": []}, {"text": "The basis of our evaluation is the premise that if lexical chains area good intermediate representation for summary generation, then we would expect that each noun in a given summary should be used in the same sense as some word instance grouped into a strong chain in the original document on which the summary is based.", "labels": [], "entities": [{"text": "summary generation", "start_pos": 108, "end_pos": 126, "type": "TASK", "confidence": 0.797905445098877}]}, {"text": "Moreover, we would expect that all (most) strong chains in the document should be represented in the summary.", "labels": [], "entities": []}, {"text": "For this analysis, a corpus of documents with their human-generated summaries are required.", "labels": [], "entities": []}, {"text": "Although there are many examples of document and summary types, for the purposes of this experiment, we focus on two general categories of summaries that are readily available.", "labels": [], "entities": []}, {"text": "The first, scientific documents with abstracts, represents a readily available class of summaries often discussed in the literature (Marcu 1999).", "labels": [], "entities": []}, {"text": "The second class of document selected was chapters from university level textbooks that contain chapter summaries.", "labels": [], "entities": []}, {"text": "To prevent bias, textbooks from several fields were chosen.", "labels": [], "entities": []}, {"text": "In this analysis, we use the term concept to denote a noun in a particular sense (a given sense number in the WordNet database).", "labels": [], "entities": [{"text": "WordNet database", "start_pos": 110, "end_pos": 126, "type": "DATASET", "confidence": 0.9788595139980316}]}, {"text": "It is important to note that different nouns with the same sense number 3 are considered to be the same concept.", "labels": [], "entities": []}, {"text": "It is also important to note that for the purposes of this analysis, when we refer to the \"sense\" of a word, we mean the sense as determined by our lexical chain analysis.", "labels": [], "entities": []}, {"text": "The basic idea of our experiment is to try to determine whether the concepts represented by (strong) lexical chains in an original document appear in the summary of that document and whether the concepts appearing in the summary (as determined by the lexical chain analysis of the summary) come from strong chains in the document.", "labels": [], "entities": []}, {"text": "If both of these give 100% coverage, this would mean that all and only the concepts identified by strong lexical chains in the document occur in the summary.", "labels": [], "entities": []}, {"text": "Thus the higher these numbers turnout to be, the more likely it is that lexical chains area good intermediate representation of the text summarization task.", "labels": [], "entities": [{"text": "text summarization task", "start_pos": 132, "end_pos": 155, "type": "TASK", "confidence": 0.8104774951934814}]}, {"text": "A corpus was compiled containing the two specific types of documents, ranging in length from 2,247 to 26,320 words each.", "labels": [], "entities": []}, {"text": "These documents were selected at random, with no screening by the authors.", "labels": [], "entities": []}, {"text": "The scientific corpus consisted of 10 scientific articles (5 computer science, 3 anthropology, and 2 biology) along with their abstracts.", "labels": [], "entities": []}, {"text": "The textbook corpus consisted of 14 chapters from 10 university level textbooks in various subjects (4 computer science, 6 anthropology, 2 history, and 2 economics), including chapter summaries.", "labels": [], "entities": []}, {"text": "For each document in the corpus, the document and its summary were analyzed separately to produce lexical chains.", "labels": [], "entities": []}, {"text": "In both cases we output the sense numbers specified for each word instance as well as the overriding sense number for each chain.", "labels": [], "entities": []}, {"text": "By comparing the sense numbers of (words in) each chain in the document with the computed sense of each noun instance in the summary, we can determine whether the summary indeed contains the same \"concepts\" as indicated by the lexical chains.", "labels": [], "entities": []}, {"text": "For the analysis, the specific metrics we are interested in are \u2022 The number and percentage of strong chains from the original text that are represented in the summary.", "labels": [], "entities": []}, {"text": "Here we say a chain is represented if a word occurs in the summary in the same sense as in the document strong chain.", "labels": [], "entities": []}, {"text": "(Analogous to recall) \u2022 The number and percentage of noun instances in the summary that represent strong chains in the document.", "labels": [], "entities": [{"text": "recall", "start_pos": 14, "end_pos": 20, "type": "METRIC", "confidence": 0.9755580425262451}]}, {"text": "(Analogous to precision) By analyzing these two metrics, we can determine how well lexical chains represent the information that appears in these types of human-generated summaries.", "labels": [], "entities": [{"text": "precision", "start_pos": 14, "end_pos": 23, "type": "METRIC", "confidence": 0.9947139620780945}]}, {"text": "We will loosely use the terms recall and precision to describe these two metrics.", "labels": [], "entities": [{"text": "recall", "start_pos": 30, "end_pos": 36, "type": "METRIC", "confidence": 0.9989829659461975}, {"text": "precision", "start_pos": 41, "end_pos": 50, "type": "METRIC", "confidence": 0.9977902173995972}]}, {"text": "Each document in the corpus was analyzed by running our lexical chain algorithm and collecting the overriding sense number of each strong lexical chain computed.", "labels": [], "entities": []}, {"text": "Each summary in the corpus was analyzed by our algorithm, and the disambiguated sense (i.e., the sense of the noun instance that was selected in order to insert it into a chain) of each noun was collected.", "labels": [], "entities": []}, {"text": "shows the results of this analysis.", "labels": [], "entities": []}, {"text": "The number of strong chains computed for the document is shown in column 2.", "labels": [], "entities": []}, {"text": "Column 3 shows the total number of noun instances found in the summary.", "labels": [], "entities": []}, {"text": "Column 4 shows the number, and percentage overall, of strong chains from the document that are represented by noun instances in the summary (recall).", "labels": [], "entities": [{"text": "recall", "start_pos": 141, "end_pos": 147, "type": "METRIC", "confidence": 0.9804893732070923}]}, {"text": "The number, and the percentage overall, of nouns of a given sense from the summary that have a corresponding strong chain with the same overriding sense number (representing the chain) in the original text are presented in column 5 (precision).", "labels": [], "entities": [{"text": "precision", "start_pos": 233, "end_pos": 242, "type": "METRIC", "confidence": 0.9991151690483093}]}, {"text": "Summary statistics are also presented.", "labels": [], "entities": []}, {"text": "In 79.12% of the cases, lexical chains appropriately represent the nouns in the summary.", "labels": [], "entities": []}, {"text": "In 80.83% of the cases, nouns in the summary would have been predicted by the lexical chains.", "labels": [], "entities": []}, {"text": "The algorithm performs badly on two documents, anthropology paper 3 and computer science chapter 4, under this analysis.", "labels": [], "entities": []}, {"text": "Possible reasons for this will be discussed below, but our preliminary analysis of these documents leads us to believe that they contain a greater number of pronouns and other anaphoric expressions (which need to be resolved to compute lexical chains properly).", "labels": [], "entities": []}, {"text": "These potential reasons need to be examined further to determine why our algorithm performs so poorly on these documents.", "labels": [], "entities": []}, {"text": "Excluding these two documents, our algorithm has a recall of 83.39% and a precision of 84.63% on average.", "labels": [], "entities": [{"text": "recall", "start_pos": 51, "end_pos": 57, "type": "METRIC", "confidence": 0.9997366070747375}, {"text": "precision", "start_pos": 74, "end_pos": 83, "type": "METRIC", "confidence": 0.9995666146278381}]}, {"text": "It is important to note that strong chains represent only between 5% and 15% of the total chains computed for any document.", "labels": [], "entities": []}, {"text": "The evaluation presented here would be enhanced by having a baseline for comparison.", "labels": [], "entities": []}, {"text": "It is not clear, however, what this baseline should be.", "labels": [], "entities": []}, {"text": "One possibility would be to use straight frequency counts as an indicator and use these frequency counts for comparison.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 3  Scoring system tuned by empirical methods.", "labels": [], "entities": []}, {"text": " Table 4  Constants from WordNet 1.6.", "labels": [], "entities": [{"text": "WordNet 1.6", "start_pos": 25, "end_pos": 36, "type": "DATASET", "confidence": 0.9619822800159454}]}]}