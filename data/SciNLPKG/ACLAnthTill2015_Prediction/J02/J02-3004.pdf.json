{"title": [], "abstractContent": [{"text": "This article addresses the interpretation of nominalizations, a particular class of compound nouns whose head noun is derived from a verb and whose modifier is interpreted as an argument of this verb.", "labels": [], "entities": [{"text": "interpretation of nominalizations", "start_pos": 27, "end_pos": 60, "type": "TASK", "confidence": 0.8279392520586649}]}, {"text": "Any attempt to automatically interpret nominalizations needs to take into account: (a) the selectional constraints imposed by the nominalized compound head, (b) the fact that the relation of the modifier and the head noun can be ambiguous, and (c) the fact that these constraints can be easily overridden by contextual or pragmatic factors.", "labels": [], "entities": []}, {"text": "The interpretation of nominalizations poses a further challenge for probabilistic approaches since the argument relations between ahead and its modifier are not readily available in the corpus.", "labels": [], "entities": [{"text": "interpretation of nominalizations", "start_pos": 4, "end_pos": 37, "type": "TASK", "confidence": 0.8481136759122213}]}, {"text": "Even an approximation that maps the compound head to its underlying verb provides insufficient evidence.", "labels": [], "entities": []}, {"text": "We present an approach that treats the interpretation task as a disambiguation problem and show how we can \"re-create\" the missing distributional evidence by exploiting partial parsing, smoothing techniques, and contextual information.", "labels": [], "entities": []}, {"text": "We combine these distinct information sources using Ripper, a system that learns sets of rules from data, and achieve an accuracy of 86.1% (over a baseline of 61.5%) on the British National Corpus.", "labels": [], "entities": [{"text": "Ripper", "start_pos": 52, "end_pos": 58, "type": "METRIC", "confidence": 0.5585368275642395}, {"text": "accuracy", "start_pos": 121, "end_pos": 129, "type": "METRIC", "confidence": 0.9994456171989441}, {"text": "British National Corpus", "start_pos": 173, "end_pos": 196, "type": "DATASET", "confidence": 0.9023874203364054}]}], "introductionContent": [{"text": "The automatic interpretation of compound nouns has been a long-standing problem for natural language processing (NLP).", "labels": [], "entities": [{"text": "automatic interpretation of compound nouns", "start_pos": 4, "end_pos": 46, "type": "TASK", "confidence": 0.8132621884346009}, {"text": "natural language processing (NLP)", "start_pos": 84, "end_pos": 117, "type": "TASK", "confidence": 0.83104141553243}]}, {"text": "Compound nouns in English have three basic properties that present difficulties for their interpretation: (a) the compounding process is extremely productive (this means that a hypothetical system would have to interpret previously unseen instances), (b) the semantic relationship between the compound head and its modifier is implicit (this means that it cannot be easily recovered from syntactic or morphological analysis), and (c) the interpretation can be influenced by a variety of contextual and pragmatic factors.", "labels": [], "entities": []}, {"text": "A considerable amount of effort has gone into specifying the set of semantic relations that hold between a compound head and its modifier., for example, distinguishes two types of compound nouns: (a) compounds consisting of two nouns that are related by one of nine recoverably deletable predicates (e.g., cause relates onion tears, for relates pet spray; seethe examples in (1)) and (b) nominalizations, that is, compounds whose heads are nouns derived from a verb and whose modifiers are interpreted as arguments of the related verb (e.g., a car lover loves cars; seethe examples in (2)-(4)).", "labels": [], "entities": []}, {"text": "The prenominal modifier can be either a noun or an adjective (see the examples in (2)).", "labels": [], "entities": []}, {"text": "The nominalized verb can take a subject (see (3a)), a direct object (see (3b)) or a prepositional object (see (3c)).", "labels": [], "entities": []}, {"text": "( a. government promotion subj|obj b. satellite observation subj|obj Besides, a fair number of researchers agree that there is a limited number of regularly recurring relations between a compound head and its modifier.", "labels": [], "entities": []}, {"text": "There is far less agreement when it comes to the type and number of these relations.", "labels": [], "entities": []}, {"text": "The relations vary from recoverably deletable predicates to paraphrases and role nominals.", "labels": [], "entities": []}, {"text": "proposes eight relations, and proposes six basic relations, whereas the number of relations proposed by is potentially infinite.", "labels": [], "entities": []}, {"text": "The attempt to restrict the semantic relations between the compound head and its modifier to a prespecified number and type has been criticized by, who has shown (through a series of psycholinguistic experiments) that the underlying relations can be influenced by a variety of pragmatic factors and cannot therefore be presumed to be easily enumerable.", "labels": [], "entities": []}, {"text": "Sparck Jones (1983, page 4) further notes \"that observations about the semantic relation holding between the compound head and its modifier can only be remarks about tendencies and not about absolutes.\"", "labels": [], "entities": []}, {"text": "Consider, for instance, the compound onion tears (see (1a)).", "labels": [], "entities": []}, {"text": "The relationship cause is one of the possible interpretations the compound may receive.", "labels": [], "entities": []}, {"text": "One could easily imagine a context in which the tears are for or about the onion.", "labels": [], "entities": []}, {"text": "Consider example 1 (5a), taken from Downing.", "labels": [], "entities": []}, {"text": "Here apple-juice seat refers to the situation in which someone is instructed to sit in a seat in front of which a glass of apple juice has been placed.", "labels": [], "entities": []}, {"text": "Given this particular state of affairs, none of the relations in (1) can be used to successfully interpret apple-juice seat.", "labels": [], "entities": [{"text": "interpret apple-juice seat", "start_pos": 97, "end_pos": 123, "type": "TASK", "confidence": 0.6374362111091614}]}, {"text": "Such considerations have led to The Disambiguation of Nominalizations claim that only nominalizations are amenable to linguistic characterization, leaving all other compounds to be explained by pragmatics or discourse.", "labels": [], "entities": [{"text": "linguistic characterization", "start_pos": 118, "end_pos": 145, "type": "TASK", "confidence": 0.7816168069839478}]}, {"text": "A similar approach is put forward by for all types of compounds, including nominalizations: any two nouns can be combined, and the relation between these nouns is entirely underspecified, to be resolved pragmatically.", "labels": [], "entities": []}, {"text": "A friend of mine was once instructed to sit in the apple-juice seat. b. By the end of the 1920s, government promotion of agricultural development in Niger was limited, consisting mainly of crop trials and model sheep and ostrich farms.", "labels": [], "entities": []}, {"text": "Less controversy arises with regard to nominalizations, perhaps because of the small number of allowable relations.", "labels": [], "entities": []}, {"text": "Most approaches follow in distinguishing nominalizations as a separate class of compounds, the exception being, who claims that most compounds are nominalizations, even in cases in which the head noun is not morphologically derived from a verb (see the examples in).", "labels": [], "entities": []}, {"text": "Under Finin's analysis the head book in the compound recipe book is a role nominal, that is, a noun that refers to a particular thematic role of another concept.", "labels": [], "entities": []}, {"text": "This means that book refers to the object role of write, which is filled by recipe.", "labels": [], "entities": []}, {"text": "It is not clear, however, how the implicit verb is to be recovered or why write is more appropriate than read in this example.", "labels": [], "entities": []}, {"text": "Despite the small number of relations between the nominalized head and its modifier, the interpretation of nominalizations can readily change in different contexts.", "labels": [], "entities": []}, {"text": "In some cases, the relation of the modifier and the nominalized verb (e.g., subject or object) can be predicted either from the subcategorization properties of the verb or from the semantics of the nominalization suffix of the head noun., for example.", "labels": [], "entities": []}, {"text": "Here child can be only the subject of behavior, since the verb behave is intransitive.", "labels": [], "entities": []}, {"text": "In (3b) the agentive suffix -er of the head noun lover indicates that the modifier car is the object of the verb love.", "labels": [], "entities": []}, {"text": "In other cases, the relation of the modifier and the head noun is genuinely ambiguous.", "labels": [], "entities": []}, {"text": "Out of context the compounds government promotion and satellite observation (see example (4)) can receive either a subject or an object interpretation.", "labels": [], "entities": []}, {"text": "One might argue that the preferred analysis for government promotion is \"government that is promoted by someone.\"", "labels": [], "entities": []}, {"text": "This interpretation can be easily overridden in context, however, as shown in Example (5b): here it is the government that is doing the promotion.", "labels": [], "entities": [{"text": "Example (5b", "start_pos": 78, "end_pos": 89, "type": "DATASET", "confidence": 0.8498709996541342}]}, {"text": "The automatic interpretation of compound nouns poses a challenge for empirical approaches, since the relations between ahead and its modifier are not readily available in a corpus, and therefore they have to be somehow retrieved and approximated.", "labels": [], "entities": [{"text": "automatic interpretation of compound nouns", "start_pos": 4, "end_pos": 46, "type": "TASK", "confidence": 0.7821706414222718}]}, {"text": "Given the data sparseness and the parameter estimation difficulties, it is not surprising that afar greater number of symbolic than probabilistic solutions have been proposed for the automatic interpretation of compound nouns.", "labels": [], "entities": [{"text": "automatic interpretation of compound nouns", "start_pos": 183, "end_pos": 225, "type": "TASK", "confidence": 0.7604244947433472}]}, {"text": "With the exception of and, who use probabilistic models for compound noun interpretation (see Section 7 for details), most algorithms rely on hand-crafted knowledge bases or dictionaries that contain detailed semantic information for each noun; a sequence of rules exploit a knowledge base to choose the correct interpretation fora given compound.", "labels": [], "entities": [{"text": "compound noun interpretation", "start_pos": 60, "end_pos": 88, "type": "TASK", "confidence": 0.6969317197799683}]}, {"text": "In what follows we develop a probabilistic model for the interpretation of nominalizations.", "labels": [], "entities": [{"text": "interpretation of nominalizations", "start_pos": 57, "end_pos": 90, "type": "TASK", "confidence": 0.8864884575208029}]}, {"text": "We focus on nominalizations whose prenominal modifier is either the underlying subject or direct object of the verb corresponding to the nominalized compound head.", "labels": [], "entities": []}, {"text": "In other words, we focus on examples like (3a, 3b) and ignore for the moment nominalizations whose heads correspond to verbs taking prepositional complements (see example (3c)).", "labels": [], "entities": []}, {"text": "Nominalizations are attractive from an empirical perspective: the amount of relations is small (i.e., subject or object, at least if one focuses on direct objects only) and fairly uncontroversial (see the discussion above).", "labels": [], "entities": []}, {"text": "Although the relations are not attested in the corpus, they can be retrieved and approximated through parsing.", "labels": [], "entities": []}, {"text": "The probabilistic interpretation of nominalizations can provide a lower bound for the difficulty of the compound interpretation task: if we cannot interpret nominalizations successfully, there is little hope for modeling more complex semantic relations stochastically (see the examples in).", "labels": [], "entities": [{"text": "compound interpretation", "start_pos": 104, "end_pos": 127, "type": "TASK", "confidence": 0.7301663905382156}]}, {"text": "We present a probabilistic algorithm that treats the interpretation task as a disambiguation problem.", "labels": [], "entities": []}, {"text": "Our approach relies on the simplifying assumption that the relation of the nominalized head and its modifier noun can be approximated by the relation of the latter and the verb from which the head is derived.", "labels": [], "entities": []}, {"text": "This approach works insofar as the verb-argument relations from which the nominalizations are derived are attested in the corpus.", "labels": [], "entities": []}, {"text": "We show that a large number of verb-argument configurations do not occur in the corpus, something that is perhaps not surprising considering the ease with which novel compounds are created.", "labels": [], "entities": []}, {"text": "We estimate the frequencies of unseen verb-argument pairs by experimenting with three types of smoothing techniques proposed in the literature (back-off smoothing, class-based smoothing, and distance-weighted averaging) and show that their combination achieves good performance.", "labels": [], "entities": []}, {"text": "Furthermore, we explore the contribution of context to the disambiguation task and show that performance is increased by taking contextual features into account.", "labels": [], "entities": []}, {"text": "Our best results are achieved by combining the predictions of our probabilistic model with contextual information.", "labels": [], "entities": []}, {"text": "The remainder of this article is organized as follows: in Section 2 we present a simple statistical model for the interpretation of nominalizations and describe the procedure used to collect the data for our experiments.", "labels": [], "entities": [{"text": "interpretation of nominalizations", "start_pos": 114, "end_pos": 147, "type": "TASK", "confidence": 0.8911728858947754}]}, {"text": "Section 3 presents details on how the parameters of the model were estimated and gives a brief overview on the smoothing methods with which we experimented.", "labels": [], "entities": []}, {"text": "Section 4 describes the algorithm used for the interpretation of nominalizations, and Section 5 reports the results of several experiments that achieve a combined accuracy of 86.1% on the British National Corpus (BNC).", "labels": [], "entities": [{"text": "interpretation of nominalizations", "start_pos": 47, "end_pos": 80, "type": "TASK", "confidence": 0.9036311308542887}, {"text": "accuracy", "start_pos": 163, "end_pos": 171, "type": "METRIC", "confidence": 0.9990993738174438}, {"text": "British National Corpus (BNC)", "start_pos": 188, "end_pos": 217, "type": "DATASET", "confidence": 0.9763189454873403}]}, {"text": "Section 6 discusses the findings.", "labels": [], "entities": []}, {"text": "In Section 7 we review related work, and we conclude in Section 8.", "labels": [], "entities": []}], "datasetContent": [{"text": "Two graduate students in linguistics decided whether modifiers were the subject or object of a given nominalized head.", "labels": [], "entities": []}, {"text": "The judges were given a page of guidelines but no prior training.", "labels": [], "entities": []}, {"text": "The nominalizations were disambiguated in context: the judges were given the corpus sentence in which the nominalization occurred together with the previous and following sentence.", "labels": [], "entities": []}, {"text": "We measured the judges' agreement using the kappa coefficient (, which is the ratio of the proportion of times P(A) that k raters agree (corrected by chance agreement P(E)) to the maximum proportion of times the raters would agree (corrected for chance agreement): If there is a complete agreement among the raters, then K = 1, whereas if there is no agreement among the raters (other than the agreement that would be expected to occur by chance), then K = 0.", "labels": [], "entities": []}, {"text": "The judges' agreement on the disambiguation task was K = .78 (N = 200, k = 2).", "labels": [], "entities": [{"text": "disambiguation task", "start_pos": 29, "end_pos": 48, "type": "TASK", "confidence": 0.8382861614227295}, {"text": "K", "start_pos": 53, "end_pos": 54, "type": "METRIC", "confidence": 0.9840863347053528}]}, {"text": "This translates into a percentage agreement of 89.7%.", "labels": [], "entities": [{"text": "agreement", "start_pos": 34, "end_pos": 43, "type": "METRIC", "confidence": 0.7130150198936462}]}, {"text": "Although the Kappa coefficient has a number of advantages over percentage agreement (e.g., it takes into account the expected chance interrater agreement; see Carletta (1996) for details), we also report percentage agreement as it allows us to compare straightforwardly the human performance and the automatic methods described below, whose performance will also be reported in terms of percentage agreement.", "labels": [], "entities": []}, {"text": "Furthermore, percentage agreement establishes an intuitive upper bound for the task (i.e., 89.7%), allowing us to interpret how well our empirical models are doing in relation to humans.", "labels": [], "entities": []}, {"text": "Finally, note that the level of agreement was good, given that the judges were provided with minimal instructions and no prior training.", "labels": [], "entities": [{"text": "agreement", "start_pos": 32, "end_pos": 41, "type": "METRIC", "confidence": 0.980466902256012}]}, {"text": "Even though context was provided to aid the disambiguation task, however, the judges were not incomplete The Disambiguation of Nominalizations agreement.", "labels": [], "entities": [{"text": "disambiguation task", "start_pos": 44, "end_pos": 63, "type": "TASK", "confidence": 0.8995991051197052}, {"text": "Disambiguation of Nominalizations", "start_pos": 109, "end_pos": 142, "type": "TASK", "confidence": 0.8445886174837748}]}, {"text": "This points to the intrinsic difficulty of the task at hand.", "labels": [], "entities": []}, {"text": "Argument relations and consequently selectional restrictions are influenced by several pragmatic factors that may not be readily inferred from the immediate context (see Section 6 for discussion).", "labels": [], "entities": []}, {"text": "Before reporting the results of the disambiguation task, we describe our initial experiments on finding the optimal parameter settings for the two distance-weighted averaging smoothing methods.", "labels": [], "entities": []}, {"text": "shows how performance on the disambiguation task varies with respect to the number and frequency of verbs over which the similarity function is calculated.", "labels": [], "entities": []}, {"text": "The y-axis in shows how performance on the training set varies (for both PC and J divergence) when verb-argument pairs are selected for the 1,000 most frequent verbs in the corpus, the 2,000 most frequent verbs in the corpus, etc.", "labels": [], "entities": []}, {"text": "The best performance for both similarity functions is achieved with the 2,000 most frequent verbs.", "labels": [], "entities": []}, {"text": "Furthermore, J and PC yield comparable performances (68.0% and 68.3%, respectively under that condition).", "labels": [], "entities": [{"text": "J", "start_pos": 13, "end_pos": 14, "type": "METRIC", "confidence": 0.9489002823829651}, {"text": "PC", "start_pos": 19, "end_pos": 21, "type": "METRIC", "confidence": 0.979286253452301}]}, {"text": "Another important observation is that performance deteriorates less severely for PC than for J as the number of verbs increases: when all verbs for which verb-argument tuples are extracted from the BNC are used, the accuracy for PC is 66.9%, whereas the accuracy for J is 62.8%.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 216, "end_pos": 224, "type": "METRIC", "confidence": 0.9993963241577148}, {"text": "accuracy", "start_pos": 254, "end_pos": 262, "type": "METRIC", "confidence": 0.9993916749954224}]}, {"text": "These results are perhaps unsurprising: verb-argument pairs with low-frequency verbs introduce noise due to the errors inherent in the partial parser.", "labels": [], "entities": []}, {"text": "shows the 10 closest words to the verb accept according to PC as the number of verbs is varied: the quality of the closest neighbors deteriorates with the inclusion of less frequent verbs.", "labels": [], "entities": [{"text": "PC", "start_pos": 59, "end_pos": 61, "type": "METRIC", "confidence": 0.7867351174354553}]}, {"text": "Finally, we analyzed the role of the parameter \u03b2.", "labels": [], "entities": []}, {"text": "Recall that \u03b2 appears in the weight function for the Jensen-Shannon divergence and controls the influence of the most similar words: the contribution of the closest neighbors increases with a high value for \u03b2. shows how the value of \u03b2 affects performance on the disambiguation task when the similarity function is computed for the 1,000 and 2,000 most frequent verbs in the corpus.", "labels": [], "entities": []}, {"text": "It is clear that performance is low with high or very low \u03b2 values  An obvious question is whether a better performance can be achieved by combining the five smoothing variants, given that they seem to provide complementary information for predicting argument relations.", "labels": [], "entities": [{"text": "predicting argument relations", "start_pos": 240, "end_pos": 269, "type": "TASK", "confidence": 0.8540023962656657}]}, {"text": "For example, Wn, Ro, and PC are relatively good for the prediction of subject relations , whereas J is best for the prediction of object relations (see).", "labels": [], "entities": [{"text": "prediction of subject relations", "start_pos": 56, "end_pos": 87, "type": "TASK", "confidence": 0.8729887306690216}, {"text": "prediction of object relations", "start_pos": 116, "end_pos": 146, "type": "TASK", "confidence": 0.8389677703380585}]}, {"text": "Furthermore, note that the probabilistic model introduced in Section 2 and the algorithm based on it (see Section 4) ignore contextual information that can provide important cues for disambiguating nominalizations.", "labels": [], "entities": []}, {"text": "Consider the nominalization government promotion in (23a), which was assigned an object (instead of a subject) interpretation by all smoothing variants except Wn.", "labels": [], "entities": [{"text": "nominalization government promotion", "start_pos": 13, "end_pos": 48, "type": "TASK", "confidence": 0.9185086091359457}]}, {"text": "Contextual information could help assign the correct interpretation in cases in which the head of the compound is followed by prepositions such as of (see (23a)) or into (see (23b)).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1  Tuples extracted from the BNC.", "labels": [], "entities": [{"text": "BNC", "start_pos": 36, "end_pos": 39, "type": "DATASET", "confidence": 0.9726091623306274}]}, {"text": " Table 4  Frequency estimation for group registration using WordNet.", "labels": [], "entities": [{"text": "Frequency estimation", "start_pos": 10, "end_pos": 30, "type": "TASK", "confidence": 0.6530994772911072}, {"text": "group registration", "start_pos": 35, "end_pos": 53, "type": "TASK", "confidence": 0.7732239663600922}, {"text": "WordNet", "start_pos": 60, "end_pos": 67, "type": "DATASET", "confidence": 0.9873325228691101}]}, {"text": " Table 5  RA score for verb-argument tuples extracted from the BNC.", "labels": [], "entities": [{"text": "RA score", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.9843795001506805}, {"text": "BNC", "start_pos": 63, "end_pos": 66, "type": "DATASET", "confidence": 0.9249454736709595}]}, {"text": " Table 6  Ten closest words to verb accept for P C .", "labels": [], "entities": []}, {"text": " Table 7  Disambiguation performance without  nominalization suffixes.", "labels": [], "entities": [{"text": "nominalization suffixes", "start_pos": 46, "end_pos": 69, "type": "TASK", "confidence": 0.8874934017658234}]}, {"text": " Table 8  Disambiguation performance with  nominalization suffixes.", "labels": [], "entities": [{"text": "nominalization suffixes", "start_pos": 43, "end_pos": 66, "type": "TASK", "confidence": 0.912554532289505}]}, {"text": " Table 9  Agreement between smoothing methods.", "labels": [], "entities": [{"text": "smoothing", "start_pos": 28, "end_pos": 37, "type": "TASK", "confidence": 0.9691497087478638}]}, {"text": " Table 10  Performance at predicting argument  relations.", "labels": [], "entities": [{"text": "predicting argument  relations", "start_pos": 26, "end_pos": 56, "type": "TASK", "confidence": 0.9087703625361124}]}, {"text": " Table 11  Disambiguation performance using the  smoothing variants as features.", "labels": [], "entities": []}, {"text": " Table 12  Ripper's performance at predicting  argument relations.", "labels": [], "entities": [{"text": "Ripper", "start_pos": 11, "end_pos": 17, "type": "TASK", "confidence": 0.8620872497558594}, {"text": "predicting  argument relations", "start_pos": 35, "end_pos": 65, "type": "TASK", "confidence": 0.9193877379099528}]}, {"text": " Table 17  Performance at predicting argument relations using context.", "labels": [], "entities": [{"text": "predicting argument relations", "start_pos": 26, "end_pos": 55, "type": "TASK", "confidence": 0.864735742410024}]}]}