{"title": [{"text": "A Critique and Improvement of an Evaluation Metric for Text Segmentation", "labels": [], "entities": [{"text": "Text Segmentation", "start_pos": 55, "end_pos": 72, "type": "TASK", "confidence": 0.7752532660961151}]}], "abstractContent": [{"text": "The P k evaluation metric, initially proposed by Beeferman, Berger, and Lafferty (1997), is becoming the standard measure for assessing text segmentation algorithms.", "labels": [], "entities": [{"text": "text segmentation algorithms", "start_pos": 136, "end_pos": 164, "type": "TASK", "confidence": 0.7930437127749125}]}, {"text": "However, a theoretical analysis of the metric finds several problems: the metric penalizes false negatives more heavily than false positives, overpenalizes near misses, and is affected by variation in segment size distribution.", "labels": [], "entities": []}, {"text": "We propose a simple modification to the P k metric that remedies these problems.", "labels": [], "entities": []}, {"text": "This new metric-called WindowDiff-moves a fixed-sized window across the text and penalizes the algorithm whenever the number of boundaries within the window does not match the true number of boundaries for that window of text.", "labels": [], "entities": []}], "introductionContent": [{"text": "Text segmentation is the task of determining the positions at which topics change in a stream of text.", "labels": [], "entities": [{"text": "Text segmentation", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.7180390059947968}]}, {"text": "Interest in automatic text segmentation has blossomed over the last few years, with applications ranging from information retrieval to text summarization to story segmentation of video feeds.", "labels": [], "entities": [{"text": "automatic text segmentation", "start_pos": 12, "end_pos": 39, "type": "TASK", "confidence": 0.6606972018877665}, {"text": "information retrieval", "start_pos": 110, "end_pos": 131, "type": "TASK", "confidence": 0.7809765338897705}, {"text": "text summarization", "start_pos": 135, "end_pos": 153, "type": "TASK", "confidence": 0.7160883843898773}, {"text": "story segmentation of video feeds", "start_pos": 157, "end_pos": 190, "type": "TASK", "confidence": 0.8339632689952851}]}, {"text": "Early work in multiparagraph discourse segmentation examined the problem of subdividing texts into multiparagraph units that represent passages or subtopics.", "labels": [], "entities": [{"text": "multiparagraph discourse segmentation", "start_pos": 14, "end_pos": 51, "type": "TASK", "confidence": 0.6460884511470795}]}, {"text": "An example, drawn from, is a 21-paragraph science news article, called \"Stargazers,\" whose main topic is the existence of life on earth and other planets.", "labels": [], "entities": []}, {"text": "Its contents can be described as consisting of the following subtopic discussions (numbers indicate paragraphs): 1-3 Introduction: The search for life in space 4-5 The moon's chemical composition 6-8 How early earth-moon proximity shaped the moon 9-12 How the moon helped life evolve on earth 13 Improbability of the earth-moon system 14-16 Binary/trinary star systems make life unlikely 17-18 The low probability of nonbinary/trinary systems 19-20 Properties of earth's sun that facilitate life 21 Summary The TextTiling algorithm () attempts to recognize these subtopic changes by making use of patterns of lexical co-occurrence and distribution; subtopic boundaries are assumed to occur at the point in the documents at which large shifts in vocabulary occur.", "labels": [], "entities": []}, {"text": "Many others have used this technique, or slight variations The boxes indicate sentences or other units of subdivision, and spaces between boxes indicate potential boundary locations.", "labels": [], "entities": []}, {"text": "Algorithm A-0 makes two near misses, while Algorithm A-1 misses both boundaries by a wide margin and introduces three false positives.", "labels": [], "entities": []}, {"text": "Both algorithms would receive scores of 0 for both precision and recall.", "labels": [], "entities": [{"text": "precision", "start_pos": 51, "end_pos": 60, "type": "METRIC", "confidence": 0.9995419979095459}, {"text": "recall", "start_pos": 65, "end_pos": 71, "type": "METRIC", "confidence": 0.997112512588501}]}, {"text": "Another problem with precision and recall is that they are not sensitive to near misses.", "labels": [], "entities": [{"text": "precision", "start_pos": 21, "end_pos": 30, "type": "METRIC", "confidence": 0.9994418025016785}, {"text": "recall", "start_pos": 35, "end_pos": 41, "type": "METRIC", "confidence": 0.9993587136268616}]}, {"text": "Consider, for example, a reference segmentation and the results obtained by two different text segmentation algorithms, as depicted in.", "labels": [], "entities": [{"text": "reference segmentation", "start_pos": 25, "end_pos": 47, "type": "TASK", "confidence": 0.7618317604064941}, {"text": "text segmentation", "start_pos": 90, "end_pos": 107, "type": "TASK", "confidence": 0.7114114612340927}]}, {"text": "In both cases, the algorithms fail to match any boundary precisely; both receive scores of 0 for precision and recall.", "labels": [], "entities": [{"text": "precision", "start_pos": 97, "end_pos": 106, "type": "METRIC", "confidence": 0.9995243549346924}, {"text": "recall", "start_pos": 111, "end_pos": 117, "type": "METRIC", "confidence": 0.9975399971008301}]}, {"text": "However, Algorithm A-0 is close to correct in almost all cases, whereas Algorithm A-1 is entirely off, adding extraneous boundaries and missing important boundaries entirely.", "labels": [], "entities": [{"text": "Algorithm A-0", "start_pos": 9, "end_pos": 22, "type": "METRIC", "confidence": 0.43548689782619476}, {"text": "correct", "start_pos": 35, "end_pos": 42, "type": "METRIC", "confidence": 0.9555603265762329}, {"text": "Algorithm A-1", "start_pos": 72, "end_pos": 85, "type": "DATASET", "confidence": 0.7033796906471252}]}, {"text": "In some circumstances, it would be useful to have an evaluation metric that penalizes A-0 less harshly than A-1.", "labels": [], "entities": [{"text": "A-0", "start_pos": 86, "end_pos": 89, "type": "METRIC", "confidence": 0.9400944113731384}]}], "datasetContent": [{"text": "Beeferman, introduce anew evaluation metric that attempts to resolve the problems with precision and recall, including assigning partial credit to near misses.", "labels": [], "entities": [{"text": "precision", "start_pos": 87, "end_pos": 96, "type": "METRIC", "confidence": 0.9991212487220764}, {"text": "recall", "start_pos": 101, "end_pos": 107, "type": "METRIC", "confidence": 0.9981168508529663}]}, {"text": "They justify their metric as follows (page 43): Segmentation . .", "labels": [], "entities": [{"text": "metric", "start_pos": 19, "end_pos": 25, "type": "METRIC", "confidence": 0.9805399179458618}]}, {"text": ". is about identifying boundaries between successive units of information in a text corpus.", "labels": [], "entities": []}, {"text": "Two such units are either related or unrelated by the intent of the document author.", "labels": [], "entities": []}, {"text": "A natural way to reason about developing a segmentation algorithm is therefore to optimize the likelihood that two such units are correctly labeled as being related or being unrelated.", "labels": [], "entities": []}, {"text": "Our error metric P \u00b5 is simply the probability that two sentences drawn randomly from the corpus are correctly identified as belonging to the same document or not belonging to the same document.", "labels": [], "entities": [{"text": "error metric P \u00b5", "start_pos": 4, "end_pos": 20, "type": "METRIC", "confidence": 0.9576544314622879}]}, {"text": "The derivation of P \u00b5 is rather involved, and a much simpler version is adopted in the later work (Beeferman, Berger, and Lafferty 1999) and by others.", "labels": [], "entities": [{"text": "P \u00b5", "start_pos": 18, "end_pos": 21, "type": "METRIC", "confidence": 0.7284075021743774}]}, {"text": "This version, referred to as P k , is calculated by setting k to half of the average true segment size and then computing penalties via a moving window of length k.", "labels": [], "entities": []}, {"text": "At each location, the algorithm determines whether the two ends of the probe are in the same or different segments in the reference segmentation and increases a counter if the algorithm's segmentation disagrees.", "labels": [], "entities": []}, {"text": "The resulting count is scaled between 0 and 1 by dividing by the number of measurements taken.", "labels": [], "entities": []}, {"text": "An algorithm that assigns all boundaries correctly receives a score of 0.", "labels": [], "entities": []}, {"text": "Beeferman, Berger, and Lafferty (1999) state as part of An illustration of how the P k metric handles false negatives.", "labels": [], "entities": []}, {"text": "The arrowed lines indicate the two poles of the probe as it moves from left to right; the boxes indicate sentences or other units of subdivision; and the width of the window (k) is four, meaning four potential boundaries fall between the two ends of the probe.", "labels": [], "entities": []}, {"text": "Solid lines indicate no penalty is assigned; dashed lines indicate a penalty is assigned.", "labels": [], "entities": []}, {"text": "Total penalty is always k for false negatives.", "labels": [], "entities": [{"text": "Total penalty", "start_pos": 0, "end_pos": 13, "type": "METRIC", "confidence": 0.9499365091323853}]}, {"text": "the justification for this metric that, to discourage \"cheating\" of the metric, degenerate algorithms-those that place boundaries at every position, or place no boundaries at all-are assigned (approximately) the same score.", "labels": [], "entities": []}, {"text": "Additionally, the authors define a false negative (also referred to as a miss) as a case when a boundary is present in the reference segmentation but missing in the algorithm's hypothesized segmentation, and a false positive as an assignment of a boundary that does not exist in the reference segmentation.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2  Average error score for P k , P \ud97b\udf59  k , and WD over 10 trials  of 100 measurements each, shown by segment size  distribution range. (a) False negatives were placed  with probability 0.05 at each boundary; (b) false  positives were placed with probability 0.05,  uniformly distributed within each segment; and (c)  both false negatives and false positives were placed  with probability 0.05. (d) False negatives were placed  with probability 0.25 at each boundary; (e) false  positives were placed with probability 0.25,  uniformly distributed within each segment; and (f)  both false negatives and false positives were placed  with probability 0.25.", "labels": [], "entities": [{"text": "Average error score", "start_pos": 10, "end_pos": 29, "type": "METRIC", "confidence": 0.7727954387664795}]}, {"text": " Table 3  Average error score for P k , P \ud97b\udf59  k , and WD over 10 trials of  100 measurements each over the segment distribution  range (15, 35) and with error probabilities of 0.5. The  average penalties computed by the three metrics are  shown for seven different error distributions.", "labels": [], "entities": [{"text": "Average error score", "start_pos": 10, "end_pos": 29, "type": "METRIC", "confidence": 0.8823626637458801}, {"text": "error probabilities", "start_pos": 152, "end_pos": 171, "type": "METRIC", "confidence": 0.934260755777359}]}]}