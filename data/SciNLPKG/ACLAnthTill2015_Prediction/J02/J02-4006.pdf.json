{"title": [{"text": "Using Hidden Markov Modeling to Decompose Human-Written Summaries", "labels": [], "entities": [{"text": "Decompose Human-Written Summaries", "start_pos": 32, "end_pos": 65, "type": "TASK", "confidence": 0.791346033414205}]}], "abstractContent": [{"text": "Professional summarizers often reuse original documents to generate summaries.", "labels": [], "entities": [{"text": "summarizers", "start_pos": 13, "end_pos": 24, "type": "TASK", "confidence": 0.9657163023948669}]}, {"text": "The task of summary sentence decomposition is to deduce whether a summary sentence is constructed by reusing the original text and to identify reused phrases.", "labels": [], "entities": [{"text": "summary sentence decomposition", "start_pos": 12, "end_pos": 42, "type": "TASK", "confidence": 0.7846948107083639}]}, {"text": "Specifically, the decomposition program needs to answer three questions fora given summary sentence: (1) Is this summary sentence constructed by reusing the text in the original document?", "labels": [], "entities": []}, {"text": "(2) If so, what phrases in the sentence come from the original document? and (3) From wherein the document do the phrases come?", "labels": [], "entities": []}, {"text": "Solving the decomposition problem can lead to better text generation techniques for summarization.", "labels": [], "entities": [{"text": "text generation", "start_pos": 53, "end_pos": 68, "type": "TASK", "confidence": 0.802479475736618}, {"text": "summarization", "start_pos": 84, "end_pos": 97, "type": "TASK", "confidence": 0.9859963059425354}]}, {"text": "Decomposition can also provide large training and testing corpora for extraction-based summarizers.", "labels": [], "entities": []}, {"text": "We propose a hidden Markov model solution to the decomposition problem.", "labels": [], "entities": []}, {"text": "Evaluations show that the proposed algorithm performs well.", "labels": [], "entities": []}], "introductionContent": [{"text": "We define a problem referred to as summary sentence decomposition.", "labels": [], "entities": [{"text": "summary sentence decomposition", "start_pos": 35, "end_pos": 65, "type": "TASK", "confidence": 0.618943432966868}]}, {"text": "The goal of a decomposition program is to determine the relations between phrases in a summary and phrases in the corresponding original document.", "labels": [], "entities": []}, {"text": "Our analysis of a set of humanwritten summaries has indicated that professional summarizers often rely on cutting and pasting text from the original document to produce summaries.", "labels": [], "entities": []}, {"text": "Unlike most current automatic summarizers, however, which extract sentences or paragraphs without any modification, professional summarizers edit the extracted text using a number of revision operations.", "labels": [], "entities": [{"text": "summarizers edit the extracted text", "start_pos": 129, "end_pos": 164, "type": "TASK", "confidence": 0.845541262626648}]}, {"text": "Decomposition of human-written summaries involves analyzing a summary sentence to determine how it is constructed by humans.", "labels": [], "entities": [{"text": "Decomposition of human-written summaries", "start_pos": 0, "end_pos": 40, "type": "TASK", "confidence": 0.8710282146930695}]}, {"text": "Specifically, we define the summary sentence decomposition problem as follows: Given a human-written summary sentence, a decomposition program needs to answer three questions: (1) Is this summary sentence constructed by reusing the text in the original document?", "labels": [], "entities": [{"text": "summary sentence decomposition", "start_pos": 28, "end_pos": 58, "type": "TASK", "confidence": 0.6869218746821085}]}, {"text": "(2) If so, what phrases in the sentence come from the original document? and (3) From wherein the document do the phrases come?", "labels": [], "entities": []}, {"text": "Here, the term phrase refers to any sentence component that is cut from the original document and reused in the summary.", "labels": [], "entities": []}, {"text": "A phrase can beat any granularity, from a single word to a complicated verb phrase to a complete sentence.", "labels": [], "entities": []}, {"text": "There are two primary benefits of solving the summary sentence decomposition problem.", "labels": [], "entities": [{"text": "summary sentence decomposition problem", "start_pos": 46, "end_pos": 84, "type": "TASK", "confidence": 0.7440667971968651}]}, {"text": "First, decomposition can lead to better text generation techniques in summarization.", "labels": [], "entities": [{"text": "text generation", "start_pos": 40, "end_pos": 55, "type": "TASK", "confidence": 0.793342798948288}, {"text": "summarization", "start_pos": 70, "end_pos": 83, "type": "TASK", "confidence": 0.9783276915550232}]}, {"text": "Most domain-independent summarizers rely on simple extraction to produce summaries, even though extracted sentences can be incoherent, redundant, or misleading.", "labels": [], "entities": []}, {"text": "By decomposing human-written sentences, we can deduce how summary sen-tences are constructed by humans.", "labels": [], "entities": []}, {"text": "By learning how humans use revision operations to edit extracted sentences, we can develop automatic programs to simulate these revision operations and build a better text generation system for summarization.", "labels": [], "entities": [{"text": "summarization", "start_pos": 194, "end_pos": 207, "type": "TASK", "confidence": 0.98479163646698}]}, {"text": "Second, the decomposition result also provides large corpora for extraction-based summarizers.", "labels": [], "entities": []}, {"text": "By aligning summary sentences with original-document sentences, we can automatically annotate the most important sentences in an input document.", "labels": [], "entities": []}, {"text": "By doing this automatically, we can afford to mark content importance fora large set of documents, thereby providing valuable training and testing data sets for extraction-based summarizers.", "labels": [], "entities": []}, {"text": "We propose a hidden Markov model solution to the summary sentence decomposition problem.", "labels": [], "entities": [{"text": "summary sentence decomposition", "start_pos": 49, "end_pos": 79, "type": "TASK", "confidence": 0.6747028529644012}]}, {"text": "In the next section, we show by example the revision operations used by professional summarizers.", "labels": [], "entities": []}, {"text": "In Section 3, we present our solution to the decomposition problem by first mathematically formulating the decomposition problem and then presenting the Hidden Markov Model.", "labels": [], "entities": []}, {"text": "In Section 4, we present three evaluation experiments and their results.", "labels": [], "entities": []}, {"text": "Section 5 describes applications, and Section 6 discusses related work.", "labels": [], "entities": []}], "datasetContent": [{"text": "Three experiments were performed to evaluate the decomposition module.", "labels": [], "entities": []}, {"text": "In the first experiment, we evaluated decomposition in a task called summary alignment.", "labels": [], "entities": [{"text": "summary alignment", "start_pos": 69, "end_pos": 86, "type": "TASK", "confidence": 0.7474716603755951}]}, {"text": "This measured how successfully the decomposition program can align sentences in the summary with document sentences that are semantically equivalent.", "labels": [], "entities": []}, {"text": "In the second experiment, we asked humans to judge whether the decomposition results were correct.", "labels": [], "entities": []}, {"text": "Compared to the first experiment, this was a more direct evaluation, using a larger collection of documents.", "labels": [], "entities": []}, {"text": "The third experiment evaluated the portability of the program.", "labels": [], "entities": []}, {"text": "The corpus used in the first experiment consisted of 10 documents from the ZiffDavis corpus, which contains articles related to computer products and is available on TIPSTER discs from Linguistic Data Consortium (LDC).", "labels": [], "entities": [{"text": "TIPSTER discs from Linguistic Data Consortium (LDC)", "start_pos": 166, "end_pos": 217, "type": "DATASET", "confidence": 0.7571877572271559}]}, {"text": "The corpus used in the second experiment consisted of 50 documents related to telecommunications issues.", "labels": [], "entities": []}, {"text": "The corpus used in the third experiment consisted of legal documents on court cases, provided by the Westlaw Group.", "labels": [], "entities": [{"text": "Westlaw Group", "start_pos": 101, "end_pos": 114, "type": "DATASET", "confidence": 0.9817080795764923}]}], "tableCaptions": [{"text": " Table 1  Evaluation of decomposition program using the Ziff-Davis corpus.", "labels": [], "entities": []}]}