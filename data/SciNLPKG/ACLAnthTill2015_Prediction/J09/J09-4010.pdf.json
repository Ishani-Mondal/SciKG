{"title": [{"text": "An Empirical Study of Corpus-Based Response Automation Methods for an E-mail-Based Help-Desk Domain", "labels": [], "entities": []}], "abstractContent": [{"text": "This article presents an investigation of corpus-based methods for the automation of help-desk e-mail responses.", "labels": [], "entities": [{"text": "automation of help-desk e-mail responses", "start_pos": 71, "end_pos": 111, "type": "TASK", "confidence": 0.6183111608028412}]}, {"text": "Specifically, we investigate this problem along two operational dimensions: (1) information-gathering technique, and (2) granularity of the information.", "labels": [], "entities": []}, {"text": "We consider two information-gathering techniques (retrieval and prediction) applied to information represented at two levels of granularity (document-level and sentence-level).", "labels": [], "entities": []}, {"text": "Document-level methods correspond to the reuse of an existing response e-mail to address new requests.", "labels": [], "entities": []}, {"text": "Sentence-level methods correspond to applying extractive multi-document summarization techniques to collate units of information from more than one e-mail.", "labels": [], "entities": []}, {"text": "Evaluation of the performance of the different methods shows that in combination they are able to successfully automate the generation of responses fora substantial portion of e-mail requests in our corpus.", "labels": [], "entities": []}, {"text": "We also investigate a meta-selection process that learns to choose one method to address anew inquiry e-mail, thus providing a unified response automation solution.", "labels": [], "entities": []}], "introductionContent": [{"text": "E-mail inquiries sent to help desks often \"revolve around a small set of common questions and issues.\"", "labels": [], "entities": [{"text": "E-mail inquiries sent to help desks", "start_pos": 0, "end_pos": 35, "type": "TASK", "confidence": 0.7673416684071223}]}, {"text": "1 This means that help-desk operators spend most of their time dealing with problems that have been previously addressed.", "labels": [], "entities": []}, {"text": "Further, a significant proportion of help-desk responses contain a low level of technical content, addressing, for example, inquiries sent to the wrong group, or requests containing insufficient detail about the customer's problem.", "labels": [], "entities": []}, {"text": "Organizations and clients would benefit if an automated process was employed to deal with the easier problems, and the efforts of human operators were focused on difficult, atypical problems.", "labels": [], "entities": []}, {"text": "However, even the automation of responses to the \"easy\" problems is a difficult task.", "labels": [], "entities": []}, {"text": "Although such inquiries revolve around a relatively small set of issues, specific For example, the first sentence of the response in(a) is tailored to the user's request, whereas the rest of the response is generic, and maybe used when replying to other queries.", "labels": [], "entities": []}, {"text": "In addition to responses that contain such a mixture of specific and generic information, there are inquiries that warrant very specific or completely generic responses, as seen in Figures 1(b) and 1(c), respectively.", "labels": [], "entities": []}, {"text": "A distinctive feature of the help-desk domain is that help-desk e-mail responses contain a high level of repetition and redundancy.", "labels": [], "entities": [{"text": "repetition", "start_pos": 105, "end_pos": 115, "type": "METRIC", "confidence": 0.9935286641120911}]}, {"text": "This maybe attributed to commonalities in customer issues combined with the provision of in-house manuals to help-desk operators.", "labels": [], "entities": []}, {"text": "These manuals connect particular topics with standard response templates, prescribe a particular presentation style, and even suggest specific responses to certain queries.", "labels": [], "entities": []}, {"text": "For example, shows two rather different response e-mails which share a sentence (italicized).", "labels": [], "entities": []}, {"text": "Thus, having access to these manuals would enable us to easily identify prescribed sentences.", "labels": [], "entities": []}, {"text": "More importantly, it would enable us to determine the context in which these sentences are used, which in turn would allow us to postulate additional response sentences.", "labels": [], "entities": []}, {"text": "An interesting avenue of investigation would involve adapting our approach to help-desk situations where such manuals are accessible.", "labels": [], "entities": []}, {"text": "If you are able to seethe Internet then it sounds like it is working, you may want to get in touch with your IT department to see if you need to make any changes to your settings to get it to work.", "labels": [], "entities": []}, {"text": "Try performing a soft reset by pressing the stylus pen in the small hole on the bottom left hand side of the Ipaq and then release.", "labels": [], "entities": [{"text": "Ipaq", "start_pos": 109, "end_pos": 113, "type": "DATASET", "confidence": 0.9601852893829346}]}, {"text": "I would recommend doing a soft reset by pressing the stylus pen in the small hole on the left hand side of the Ipaq and then release.", "labels": [], "entities": [{"text": "Ipaq", "start_pos": 111, "end_pos": 115, "type": "DATASET", "confidence": 0.9558774828910828}]}, {"text": "Then charge the unit overnight to make sure it has been long enough and then see what happens.", "labels": [], "entities": []}, {"text": "If the battery is not charging then the unit will need to be sent in for repair.", "labels": [], "entities": []}], "datasetContent": [{"text": "In this section, we offer a comparative evaluation of the response automation methods presented in Section 3, where we measure the ability of the different methods to address the requests in the corpus.", "labels": [], "entities": []}, {"text": "We first describe the data used in our experiments, followed by the experimental set-up and results.", "labels": [], "entities": []}, {"text": "In our evaluation, we compare the alternative approaches for estimating performance (Equations and), and consider the effect of favoring precision when selecting a method via the weighted F-score calculation (Equation).", "labels": [], "entities": [{"text": "precision", "start_pos": 137, "end_pos": 146, "type": "METRIC", "confidence": 0.9976792931556702}, {"text": "F-score calculation (Equation)", "start_pos": 188, "end_pos": 218, "type": "METRIC", "confidence": 0.8014158129692077}]}, {"text": "To perform these comparisons we employ the following configurations.", "labels": [], "entities": []}, {"text": "r Max50: Use the argmax alternative for estimating performance (Equation), and w = 0.5 in Equation 12.", "labels": [], "entities": [{"text": "argmax", "start_pos": 17, "end_pos": 23, "type": "METRIC", "confidence": 0.9805628061294556}]}, {"text": "r Max75: As Max50, but with w = 0.75.", "labels": [], "entities": []}, {"text": "r Weighted50: Use the weighted alternative for estimating performance (Equation), and w = 0.5 in Equation (12).", "labels": [], "entities": [{"text": "Equation", "start_pos": 97, "end_pos": 105, "type": "METRIC", "confidence": 0.7668444514274597}]}, {"text": "r Weighted75: As Weighted50, but with w = 0.75.", "labels": [], "entities": [{"text": "r Weighted75", "start_pos": 0, "end_pos": 12, "type": "METRIC", "confidence": 0.760532021522522}]}, {"text": "We also devised the following baselines to help ground our results.", "labels": [], "entities": []}, {"text": "r Random: Select between the methods randomly.", "labels": [], "entities": []}, {"text": "r Gold50: Select between the methods based on their actual performance (as opposed to their estimated performance), using w = 0.5 in Equation (12).", "labels": [], "entities": [{"text": "r Gold50", "start_pos": 0, "end_pos": 8, "type": "DATASET", "confidence": 0.5759697258472443}, {"text": "Equation", "start_pos": 133, "end_pos": 141, "type": "METRIC", "confidence": 0.940001904964447}]}, {"text": "r Gold75: As Gold50, but with w = 0.75.", "labels": [], "entities": [{"text": "r Gold75", "start_pos": 0, "end_pos": 8, "type": "DATASET", "confidence": 0.5464460551738739}, {"text": "Gold50", "start_pos": 13, "end_pos": 19, "type": "DATASET", "confidence": 0.9551027417182922}]}, {"text": "As we saw from Cluster 11 in, the estimated performance can below for all the response-generation methods.", "labels": [], "entities": []}, {"text": "Therefore, we also test these configurations in Precision and F-score for the meta-learning methods averaged over the corpus.", "labels": [], "entities": [{"text": "Precision", "start_pos": 48, "end_pos": 57, "type": "METRIC", "confidence": 0.9984575510025024}, {"text": "F-score", "start_pos": 62, "end_pos": 69, "type": "METRIC", "confidence": 0.9962908029556274}]}, {"text": "a practical setting where the system has the choice of not selecting any method if the estimated performance of all the methods is poor.", "labels": [], "entities": []}, {"text": "We envisage that a practical system would behave in this manner, in the sense that a request for which none of the existing methods can produce an appropriate response would be passed to an operator.", "labels": [], "entities": []}, {"text": "As mentioned in Section 4.2, we consider precision to bean important practical criterion because it does not penalize partial but correct responses.", "labels": [], "entities": [{"text": "precision", "start_pos": 41, "end_pos": 50, "type": "METRIC", "confidence": 0.9986016154289246}]}, {"text": "Therefore, we \"implement\" our practical system by selecting only responses whose estimated precision is above 0.8.", "labels": [], "entities": [{"text": "precision", "start_pos": 91, "end_pos": 100, "type": "METRIC", "confidence": 0.9801186323165894}]}, {"text": "For these tests we also report on coverage, that is, the percentage of cases where this condition is met.", "labels": [], "entities": [{"text": "coverage", "start_pos": 34, "end_pos": 42, "type": "METRIC", "confidence": 0.999309778213501}]}, {"text": "Note that the baselines do not have an estimated precision because they do not use meta-learning.", "labels": [], "entities": [{"text": "precision", "start_pos": 49, "end_pos": 58, "type": "METRIC", "confidence": 0.9813552498817444}]}, {"text": "However, for completeness, we implement the practical system for them as well, with a threshold of 0.8 on actual precision.", "labels": [], "entities": [{"text": "precision", "start_pos": 113, "end_pos": 122, "type": "METRIC", "confidence": 0.9980459213256836}]}, {"text": "shows the results of our tests averaged overall the cases in the corpus (with standard deviations in parentheses).", "labels": [], "entities": []}, {"text": "The left-hand side corresponds to the setting where the system always selects a response-generation method, and the right-hand side corresponds to the setting where a method is selected only if its precision equals or exceeds 0.8 (this is an estimated precision for the Max and Weighted configurations, and an actual precision for the Gold and Random baselines).", "labels": [], "entities": [{"text": "precision", "start_pos": 198, "end_pos": 207, "type": "METRIC", "confidence": 0.9953617453575134}, {"text": "precision", "start_pos": 252, "end_pos": 261, "type": "METRIC", "confidence": 0.9510465264320374}, {"text": "Gold and Random baselines", "start_pos": 335, "end_pos": 360, "type": "DATASET", "confidence": 0.7824430465698242}]}, {"text": "Let us first consider the left-hand side of.", "labels": [], "entities": []}, {"text": "As expected, the Random baseline has the worst performance.", "labels": [], "entities": []}, {"text": "The Gold baselines outperform their corresponding meta-learning counterparts (except for the precision of Weighted50), but the differences in precision are not statistically significant between the Gold and the Weighted configurations (using a t-test with a 1% significance level).", "labels": [], "entities": [{"text": "precision", "start_pos": 93, "end_pos": 102, "type": "METRIC", "confidence": 0.9961243271827698}, {"text": "precision", "start_pos": 142, "end_pos": 151, "type": "METRIC", "confidence": 0.9989570379257202}]}, {"text": "Comparing the corresponding Weighted and Max configurations, the former is superior, but this is statistically significant only for the difference in precision values between Weighted50 and Max50.", "labels": [], "entities": [{"text": "precision", "start_pos": 150, "end_pos": 159, "type": "METRIC", "confidence": 0.9989880919456482}]}, {"text": "Comparing a standard F-score calculation with a precision-favoring calculation (w = 0.5 versus w = 0.75 in Equation), as expected, precision is significantly higher for the latter in all testing configurations (p < 0.01).", "labels": [], "entities": [{"text": "F-score", "start_pos": 21, "end_pos": 28, "type": "METRIC", "confidence": 0.9816193580627441}, {"text": "precision-favoring", "start_pos": 48, "end_pos": 66, "type": "METRIC", "confidence": 0.998815655708313}, {"text": "Equation", "start_pos": 107, "end_pos": 115, "type": "METRIC", "confidence": 0.9936723709106445}, {"text": "precision", "start_pos": 131, "end_pos": 140, "type": "METRIC", "confidence": 0.9997017979621887}]}, {"text": "This increase in precision is at the expense of a reduced F-score, but the increase is larger than the reduction.", "labels": [], "entities": [{"text": "precision", "start_pos": 17, "end_pos": 26, "type": "METRIC", "confidence": 0.9997169375419617}, {"text": "F-score", "start_pos": 58, "end_pos": 65, "type": "METRIC", "confidence": 0.9986829161643982}]}, {"text": "Now, in the right-hand side of, we see that the Random configuration has the best precision and the second-best F-score, 17 but its coverage is quite low (only 37.6%).", "labels": [], "entities": [{"text": "precision", "start_pos": 82, "end_pos": 91, "type": "METRIC", "confidence": 0.9995033740997314}, {"text": "F-score", "start_pos": 112, "end_pos": 119, "type": "METRIC", "confidence": 0.99930739402771}, {"text": "coverage", "start_pos": 132, "end_pos": 140, "type": "METRIC", "confidence": 0.9977748990058899}]}, {"text": "In contrast, the meta-learning configurations cover a proportion of the requests that is comparable to the coverage of the Gold baselines (approximately 57%), and all the results are substantially improved; as expected, all the precision values are high, and also more consistent than before (they have a lower standard deviation).", "labels": [], "entities": [{"text": "Gold baselines", "start_pos": 123, "end_pos": 137, "type": "DATASET", "confidence": 0.8827959299087524}, {"text": "precision", "start_pos": 228, "end_pos": 237, "type": "METRIC", "confidence": 0.9966985583305359}]}, {"text": "These results are quite impressive for the meta-learning configurations, as their selection between methods is based on estimated precision, as opposed to the baselines, whose selections are based on actual precision, which is not available in practice.", "labels": [], "entities": [{"text": "precision", "start_pos": 130, "end_pos": 139, "type": "METRIC", "confidence": 0.9172992706298828}, {"text": "precision", "start_pos": 207, "end_pos": 216, "type": "METRIC", "confidence": 0.9800472855567932}]}, {"text": "Comparing the corresponding Weighted and Max configurations, there are no significant differences in Fscore, but Weighted outperforms Max on precision (the difference between Weighted75 and Max75 produces a p-value of 0.035).", "labels": [], "entities": [{"text": "Fscore", "start_pos": 101, "end_pos": 107, "type": "METRIC", "confidence": 0.9821064472198486}, {"text": "precision", "start_pos": 141, "end_pos": 150, "type": "METRIC", "confidence": 0.9993581175804138}]}, {"text": "Finally, comparing w = 0.5 with w = 0.75 for both Weighted and Max, as for the All-cases results, the increase in precision is larger than the reduction in F-score (p < 0.01).", "labels": [], "entities": [{"text": "precision", "start_pos": 114, "end_pos": 123, "type": "METRIC", "confidence": 0.9996343851089478}, {"text": "F-score", "start_pos": 156, "end_pos": 163, "type": "METRIC", "confidence": 0.9989080429077148}]}, {"text": "The size of our corpus necessitates an automatic evaluation in order to produce meaningful results, especially because we are comparing several methods under a number of experimental settings.", "labels": [], "entities": []}, {"text": "Although our automatic evaluation has yielded useful insights, it has two main limitations.", "labels": [], "entities": []}, {"text": "r As we saw in Section 4.3 (), appropriate responses are sometimes penalized when they do not match precisely the model response.", "labels": [], "entities": []}, {"text": "However, it is often the case that there is not one single appropriate response to a query, and even a help-desk operator may respond to the same question in different ways on different occasions.", "labels": [], "entities": []}, {"text": "r The relationship between the results obtained by the automatic evaluation of the responses generated by our system and people's assessments of these responses is unclear, in particular for partial responses.", "labels": [], "entities": []}, {"text": "These limitations reinforce the notion that automated responses should be assessed on their own merit, rather than with respect to some model response.", "labels": [], "entities": []}, {"text": "In Marom and Zukerman (2007a) we identified several systems that resemble ours in that they provide answers to queries.", "labels": [], "entities": []}, {"text": "These systems addressed the evaluation issue as follows.", "labels": [], "entities": []}, {"text": "r Only qualitative observations of the responses were reported (no formal evaluation was performed)).", "labels": [], "entities": []}, {"text": "r Only an automatic evaluation was performed, which relied on having model responses ).", "labels": [], "entities": []}, {"text": "r A user study was performed, but it was either very small compared to the corpus (Carmel, Shtalhaim, and Soffer 2000; Jijkoun and de Rijke 2005), or the corpus itself was significantly smaller than ours (;).", "labels": [], "entities": []}, {"text": "The representativeness of the sample size was not discussed in any of these studies.", "labels": [], "entities": []}, {"text": "There are significant practical difficulties associated with conducting the user studies needed to produce meaningful results for our system.", "labels": [], "entities": []}, {"text": "Firstly, the size of our corpus and the number of parameters and settings that we need to test mean that in order fora user study to be representative, a fairly large sample involving several hundreds of request-response pairs would have to be used.", "labels": [], "entities": []}, {"text": "Further, user-based evaluations of the output produced by our system require the subjects to read relatively long requestresponse e-mails, which quickly becomes tedious.", "labels": [], "entities": []}, {"text": "In order to address these limitations in a practical way, we conducted a small user study where we asked four judges (graduate students from the Faculty of Information Technology at Monash University) to assess the responses generated by our system (Marom and Zukerman 2007a).", "labels": [], "entities": []}, {"text": "Our judges were instructed to position themselves as help-desk customers who know that they are receiving an automated response, and that such a response is likely to arrive quicker than a response composed by an operator.", "labels": [], "entities": []}, {"text": "Our user study assessed the response-generation methods from the following perspectives, which yield information that is beyond the F-score and precision measures obtained in the automatic evaluation.", "labels": [], "entities": [{"text": "F-score", "start_pos": 132, "end_pos": 139, "type": "METRIC", "confidence": 0.9971849322319031}, {"text": "precision", "start_pos": 144, "end_pos": 153, "type": "METRIC", "confidence": 0.9990831613540649}]}, {"text": "r Informativeness: Is there anything useful in the response that would make it a good automatic response, given that otherwise the customer has to wait fora human-generated response?", "labels": [], "entities": []}, {"text": "We used a scale from 0 to 3, where 0 corresponds to \"not at all informative\" and 3 corresponds to \"very informative.\" r Missing information: Is any crucial information item missing?", "labels": [], "entities": []}, {"text": "Y/N. r Misleading information: Is there any misleading information?", "labels": [], "entities": [{"text": "Y/N. r", "start_pos": 0, "end_pos": 6, "type": "METRIC", "confidence": 0.6874038577079773}, {"text": "Misleading information", "start_pos": 7, "end_pos": 29, "type": "TASK", "confidence": 0.48076486587524414}]}, {"text": "We asked the judges to consider only information that might misguide the customer, and ignore information that is so irrelevant that it would be ignored by a customer who knows that the response is automated (for example, receiving an answer fora printer, when the request was fora laptop).", "labels": [], "entities": []}, {"text": "r Compare to model response: How does the generated response compare with the model response?", "labels": [], "entities": []}, {"text": "Worse/Same/Better.", "labels": [], "entities": []}, {"text": "This is a summary question that rates a \"customer's\" overall impression of a response.", "labels": [], "entities": []}, {"text": "We evaluate the meta-learning system by looking at the quality of the response produced by the method selected by this system, where, as done in Section 4, quality is measured using F-score and precision.", "labels": [], "entities": [{"text": "F-score", "start_pos": 182, "end_pos": 189, "type": "METRIC", "confidence": 0.9979479908943176}, {"text": "precision", "start_pos": 194, "end_pos": 203, "type": "METRIC", "confidence": 0.9974504113197327}]}, {"text": "However, here we employ 5-fold crossvalidation (instead of 10-fold) to ensure that we get a good spread of selected methods in each testing split.", "labels": [], "entities": []}, {"text": "This is particularly important when only a few methods dominate fora data set.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1  Comparison between the three document retrieval variants.", "labels": [], "entities": []}, {"text": " Table 4  Details of the data sets included in our experiments, and overview of results per data set.", "labels": [], "entities": []}, {"text": " Table 5  Coverage, uniqueness, precision, and F-score for the response-generation methods.", "labels": [], "entities": [{"text": "Coverage", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.979446291923523}, {"text": "precision", "start_pos": 32, "end_pos": 41, "type": "METRIC", "confidence": 0.9997619986534119}, {"text": "F-score", "start_pos": 47, "end_pos": 54, "type": "METRIC", "confidence": 0.999472439289093}]}, {"text": " Table 6  Precision and F-score for the meta-learning methods averaged over the corpus.", "labels": [], "entities": [{"text": "Precision", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9988839030265808}, {"text": "F-score", "start_pos": 24, "end_pos": 31, "type": "METRIC", "confidence": 0.9990054965019226}]}]}