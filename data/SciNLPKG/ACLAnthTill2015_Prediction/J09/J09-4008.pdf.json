{"title": [{"text": "An Investigation into the Validity of Some Metrics for Automatically Evaluating Natural Language Generation Systems", "labels": [], "entities": [{"text": "Automatically Evaluating Natural Language Generation", "start_pos": 55, "end_pos": 107, "type": "TASK", "confidence": 0.8445332050323486}]}], "abstractContent": [{"text": "There is growing interest in using automatically computed corpus-based evaluation metrics to evaluate Natural Language Generation (NLG) systems, because these are often considerably cheaper than the human-based evaluations which have traditionally been used in NLG.", "labels": [], "entities": [{"text": "Natural Language Generation (NLG)", "start_pos": 102, "end_pos": 135, "type": "TASK", "confidence": 0.8183963696161906}]}, {"text": "We review previous work on NLG evaluation and on validation of automatic metrics in NLP, and then present the results of two studies of how well some metrics which are popular in other areas of NLP (notably BLEU and ROUGE) correlate with human judgments in the domain of computer-generated weather forecasts.", "labels": [], "entities": [{"text": "NLG evaluation", "start_pos": 27, "end_pos": 41, "type": "TASK", "confidence": 0.8803734481334686}, {"text": "BLEU", "start_pos": 207, "end_pos": 211, "type": "METRIC", "confidence": 0.9912477135658264}, {"text": "ROUGE", "start_pos": 216, "end_pos": 221, "type": "METRIC", "confidence": 0.8201130032539368}]}, {"text": "Our results suggest that, at least in this domain, metrics may provide a useful measure of language quality, although the evidence for this is not as strong as we would ideally like to see; however, they do not provide a useful measure of content quality.", "labels": [], "entities": []}, {"text": "We also discuss a number of caveats which must be kept in mind when interpreting this and other validation studies.", "labels": [], "entities": []}], "introductionContent": [{"text": "Evaluation is becoming an increasingly important topic in Natural Language Generation (NLG), as in other fields of computational linguistics.", "labels": [], "entities": [{"text": "Natural Language Generation (NLG)", "start_pos": 58, "end_pos": 91, "type": "TASK", "confidence": 0.8172594308853149}]}, {"text": "Many NLG researchers are impressed by the BLEU evaluation metric ( in Machine Translation (MT), which has allowed MT researchers to quickly and cheaply evaluate the impact of new ideas, algorithms, and data sets.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 42, "end_pos": 46, "type": "METRIC", "confidence": 0.996220052242279}, {"text": "Machine Translation (MT)", "start_pos": 70, "end_pos": 94, "type": "TASK", "confidence": 0.8588540434837342}]}, {"text": "BLEU and related metrics work by comparing the output of an MT system to a set of reference translations (human translations of the source text), and in principle this kind of evaluation could be done with NLG systems as well.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.9729382991790771}, {"text": "MT", "start_pos": 60, "end_pos": 62, "type": "TASK", "confidence": 0.9425792098045349}]}, {"text": "As in other areas of NLP, the advantages of automatic corpus-based evaluation are that it is potentially much cheaper and quicker than human-based evaluation, and that it is repeatable.", "labels": [], "entities": []}, {"text": "Indeed, NLG researchers have used BLEU in their evaluations for some time).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 34, "end_pos": 38, "type": "METRIC", "confidence": 0.9974060654640198}]}, {"text": "The use of such automatic evaluation metrics is, however, only sensible if they are known to be correlated with the results of reliable human-based evaluations.", "labels": [], "entities": []}, {"text": "Although a number of previous studies have analyzed correlations between human judgments and automatic evaluation metrics in machine translation and document summarization, much less is known about how well automatic metrics correlate with human judgments in In this article we present two empirical studies of how well BLEU and various other corpus-based metrics agree with human judgments, when evaluating the outputs of several NLG systems that generate texts which describe changes in the wind (for weather forecasts).", "labels": [], "entities": [{"text": "machine translation", "start_pos": 125, "end_pos": 144, "type": "TASK", "confidence": 0.7580141127109528}, {"text": "document summarization", "start_pos": 149, "end_pos": 171, "type": "TASK", "confidence": 0.6760877817869186}, {"text": "BLEU", "start_pos": 320, "end_pos": 324, "type": "METRIC", "confidence": 0.9938393235206604}]}, {"text": "We also discuss several caveats that need to be kept in mind when interpreting our study and perhaps other validation studies of automatic metrics as well.", "labels": [], "entities": []}], "datasetContent": [{"text": "As Hirschman (1998),, and others have pointed out, evaluations can be used for many purposes, and different evaluations are often needed for different stakeholders.", "labels": [], "entities": []}, {"text": "For example, the BabyTalk project at Aberdeen (), which is attempting to create a set of NLG systems which can generate textual summaries of clinical data about babies in a neonatal intensive care unit (NICU), is a collaboration between medical researchers, psychologists, computer scientists, and a commercial software house.", "labels": [], "entities": [{"text": "Aberdeen", "start_pos": 37, "end_pos": 45, "type": "DATASET", "confidence": 0.9126237034797668}]}, {"text": "Each of these groups has its own evaluation agenda: r The medical researchers want to know if BabyTalk is medically effective.", "labels": [], "entities": [{"text": "BabyTalk", "start_pos": 94, "end_pos": 102, "type": "DATASET", "confidence": 0.932621955871582}]}, {"text": "To evaluate this, they ideally would like to do a study similar to evaluation of the effectiveness of a visualization system in an intensive care unit; that is, deploy BabyTalk in a hospital, use it for half of the children in award, and determine if there is any difference in outcome (e.g., mortality) between the children in the BabyTalk group and the control group.", "labels": [], "entities": []}, {"text": "r The psychologists want to understand the effectiveness of textual presentation of information for decision support.", "labels": [], "entities": [{"text": "decision support", "start_pos": 100, "end_pos": 116, "type": "TASK", "confidence": 0.8969060480594635}]}, {"text": "To evaluate this, they would like to do a study similar to; that is, show medical subjects textual summaries (as well as standard graphical visualizations as a control) in a controlled \"off-ward\" context, ask them to make a treatment decision, and compare this decision against a gold standard.", "labels": [], "entities": []}, {"text": "r The computer scientists want to know if BabyTalk is effective (under either of these measures).", "labels": [], "entities": [{"text": "BabyTalk", "start_pos": 42, "end_pos": 50, "type": "DATASET", "confidence": 0.9462376832962036}]}, {"text": "They also would like to conduct evaluations throughout the project, so as to assess whether their development efforts are making the system better or worse; the stakeholders, in contrast, would be satisfied with a single evaluation at the end of the project.", "labels": [], "entities": []}, {"text": "r The software house would like to know if BabyTalk would be commercially profitable.", "labels": [], "entities": [{"text": "BabyTalk", "start_pos": 43, "end_pos": 51, "type": "DATASET", "confidence": 0.929719090461731}]}, {"text": "This partially depends on medical effectiveness (see previous point), which determines the demand for the system.", "labels": [], "entities": []}, {"text": "But it also depends on how expensive it is to develop and support BabyTalk; from this perspective the company is especially interested in evaluations of the cost of adapting/porting BabyTalk to different hospitals in the NICU domain in the short term, and to different medical domains in the longer term.", "labels": [], "entities": [{"text": "BabyTalk", "start_pos": 66, "end_pos": 74, "type": "DATASET", "confidence": 0.9324598908424377}, {"text": "NICU domain", "start_pos": 221, "end_pos": 232, "type": "DATASET", "confidence": 0.9331131875514984}]}, {"text": "See fora commercial perspective on medical NLG systems.", "labels": [], "entities": []}, {"text": "All of these stakeholders are interested in evaluations which assess the quality and effectiveness of generated texts; such evaluations are the focus of our article.", "labels": [], "entities": []}, {"text": "The software house and the computer scientists are also interested in engineering-cost evaluations; although this is a very important topic, we will not discuss it here: a separate article would be needed to do justice to this topic.", "labels": [], "entities": []}, {"text": "The quality of texts generated by NLG systems has been evaluated in many different ways in the past, most of which can be classified as evaluations based on task performance, human judgments and ratings, or comparison to corpus texts using automatic metrics.", "labels": [], "entities": []}, {"text": "Task-based evaluations involve directly measuring the impact of generated texts on end users; these are extrinsic evaluations, and typically involve techniques from psychology or from an application domain such as medicine.", "labels": [], "entities": []}, {"text": "One of the first task-based evaluations of an NLG system was done by, who generated instructional texts using four different algorithms, asked subjects to carryout the instructions, and then measured how many mistakes they made.", "labels": [], "entities": []}, {"text": "Although task performance is the most common measure used in task-based evaluations in NLG, other measures can also be used.", "labels": [], "entities": []}, {"text": "For example, evaluated the impact of persuasive texts (in a house-selling context) by seeing how users ranked houses in a hot list evaluated the impact of adding an NLG component to an intelligent tutoring system by measuring learning gain.", "labels": [], "entities": []}, {"text": "We have been involved in a number of task-based evaluations of NLG systems and components.", "labels": [], "entities": []}, {"text": "STOP, which generates personalized smoking-cessation letters, was evaluated on the basis of medical effectiveness; we sent a group of 2,000 smokers either STOP-generated letters or one of two kinds of control letters, and measured how many smokers in each group managed to quit smoking.)", "labels": [], "entities": []}, {"text": "(which is one of the BabyTalk systems) was evaluated for its decision-support effectiveness, using the \"psychologist\" methodology described earlier).", "labels": [], "entities": [{"text": "BabyTalk", "start_pos": 21, "end_pos": 29, "type": "DATASET", "confidence": 0.8951043486595154}]}, {"text": "SKILLSUM, which generates feedback reports from literacy assessments, was evaluated on the basis of educational effectiveness; we gave 200 assessment takers either SKILLSUM texts or control texts, and measured whether they increased the accuracy of self-assessments of their literacy skills.", "labels": [], "entities": [{"text": "SKILLSUM", "start_pos": 0, "end_pos": 8, "type": "DATASET", "confidence": 0.8266440629959106}, {"text": "accuracy", "start_pos": 237, "end_pos": 245, "type": "METRIC", "confidence": 0.9976376295089722}]}, {"text": "We also evaluated several referring-expression generation algorithms by conducting experiments in which participants were presented with generated referring expressions and asked to identify the target referent; these were carried out in conjunction with shared-task events organized under the Generation Challenges initiative (Generation Challenges is further discussed in Section 2.1.4).", "labels": [], "entities": [{"text": "referring-expression generation", "start_pos": 26, "end_pos": 57, "type": "TASK", "confidence": 0.797727108001709}]}, {"text": "Task-based evaluations have traditionally been regarded as the most meaningful kind of evaluation in NLG, especially in contexts where the evaluation needs to convince people in other communities (such as psychologists and doctors).", "labels": [], "entities": []}, {"text": "However, they can be expensive and time-consuming.", "labels": [], "entities": []}, {"text": "The STOP evaluation cost UK\u00a375,000, and required 20 months to design, carryout, and analyze; the SKILLSUM and BT45 evaluations (which are perhaps more typical) cost about UK\u00a320,000 over six months.", "labels": [], "entities": [{"text": "STOP", "start_pos": 4, "end_pos": 8, "type": "TASK", "confidence": 0.452919065952301}, {"text": "SKILLSUM", "start_pos": 97, "end_pos": 105, "type": "METRIC", "confidence": 0.47544553875923157}, {"text": "BT45", "start_pos": 110, "end_pos": 114, "type": "DATASET", "confidence": 0.6325905919075012}]}, {"text": "The referringexpression identification experiments were cheaper (less than UK\u00a31,000 each, not counting data and system creation), because they involved smaller numbers of subjects, and evaluated system components in laboratory-based settings, rather than by means of systems deployed in the real world.", "labels": [], "entities": [{"text": "referringexpression identification", "start_pos": 4, "end_pos": 38, "type": "TASK", "confidence": 0.9429010450839996}, {"text": "system creation", "start_pos": 112, "end_pos": 127, "type": "TASK", "confidence": 0.7241259813308716}]}, {"text": "In addition to monetary and time costs, all of these evaluations also depended on goodwill from participants, inmost cases busy domain experts who used their own standing in their community to arrange access to subjects and otherwise facilitate the evaluation.", "labels": [], "entities": []}, {"text": "Such goodwill in itself is a scarce resource which must be used with care.", "labels": [], "entities": []}, {"text": "Another way of evaluating an NLG system is to ask human subjects to rate generated texts on an n-point rating scale; this is an intrinsic form of evaluation.", "labels": [], "entities": []}, {"text": "This methodology was first used in NLG by, who asked eight domain experts to each rate 15 texts on a number of different dimensions: overall quality and coherence, content, organization, writing style, and correctness.", "labels": [], "entities": []}, {"text": "Some of the texts were humanwritten and some were computer-generated, but the judges did not know the origin of specific texts they read.", "labels": [], "entities": []}, {"text": "Many more such evaluations have been performed since, often with fewer dimensions.", "labels": [], "entities": []}, {"text": "For example, Binsted, Pain, and Ritchie (1997) evaluated a jokegeneration system by asking children to rate the funniness of texts on a 5-point scale evaluated the SPOT sentence-planning system by asking human subjects to rate the overall quality of generated texts on a 5-point scale.", "labels": [], "entities": []}, {"text": "A variation of this technique is to show subjects different versions of a text, and ask them which one they prefer.", "labels": [], "entities": []}, {"text": "For example, the SUMTIME weather-forecast generator was evaluated by showing subjects both human corpus texts and computer-generated texts, and asking which they preferred (.", "labels": [], "entities": [{"text": "SUMTIME weather-forecast generator", "start_pos": 17, "end_pos": 51, "type": "TASK", "confidence": 0.7036307255427042}]}, {"text": "Evaluations based on human ratings and judgments are currently probably the most popular way of evaluating NLG systems, perhaps in part because such evaluations tend to be significantly quicker and cheaper to carryout than task-based evaluations, and do not require as much support from domain experts.", "labels": [], "entities": []}, {"text": "For example, the previously mentioned evaluation of SUMTIME was carried out in two months without any external research grant funding.", "labels": [], "entities": [{"text": "SUMTIME", "start_pos": 52, "end_pos": 59, "type": "TASK", "confidence": 0.8995640277862549}]}, {"text": "In addition to resource issues, another reason why some researchers prefer evaluations based on human ratings over task-based evaluations is that task-based evaluations need to focus on a very specific task, and performance on this task may not correlate with performance on other tasks.", "labels": [], "entities": []}, {"text": "For example, as mentioned previously, the medical researchers in BabyTalk would like to conduct a medical effectiveness evaluation, which involves operationally deploying systems in areal ward and measuring impact on patient outcome.", "labels": [], "entities": [{"text": "BabyTalk", "start_pos": 65, "end_pos": 73, "type": "DATASET", "confidence": 0.9373576641082764}]}, {"text": "However, for ethical reasons such an experiment cannot be carried out until we have good evidence that the BabyTalk systems are effective, which is not yet the case.", "labels": [], "entities": [{"text": "BabyTalk", "start_pos": 107, "end_pos": 115, "type": "DATASET", "confidence": 0.9444251656532288}]}, {"text": "We carried out an off-ward task-based evaluation of BT45 using the \"psychologist\" methodology (van der), and we would like to think that the results of this evaluation would correlate with the results of a medical effectiveness evaluation.", "labels": [], "entities": [{"text": "BT45", "start_pos": 52, "end_pos": 56, "type": "DATASET", "confidence": 0.899589478969574}]}, {"text": "However, we do not have any empirical evidence that this is the case, and certainly there are major differences between the off-ward and on-ward contexts (for example, doctors in the off-ward experiment could not visually observe the babies, which is a very important information source when doctors are actually caring fora baby in a hospital ward).", "labels": [], "entities": []}, {"text": "From this perspective, an argument can be made that asking doctors to explicitly rate the medical usefulness of the texts might tell us as much about their genuine medical effectiveness as our off-ward task-based evaluation.", "labels": [], "entities": []}, {"text": "Last but not least, it is not always possible to conduct meaningful task-based evaluations of some NLG systems.", "labels": [], "entities": []}, {"text": "For example, it is unclear how to evaluate the overall quality of jokes produced by a humor generation system other than by asking for human ratings, although one can perform a task-based evaluation of the educational impact of humor generation software (, or (more speculatively) perhaps evaluate the psychological impact of a joke by monitoring facial expressions and laughter (which is a non-task-based extrinsic evaluation).", "labels": [], "entities": []}, {"text": "Little is known about how well human ratings of texts produced by NLG systems correlate with task-effectiveness measures., who worked in the same domain (NICU) as BabyTalk, conducted an off-ward decision-support evaluation which compared human-written text summaries and graphical visualizations of clinical data.", "labels": [], "entities": [{"text": "BabyTalk", "start_pos": 163, "end_pos": 171, "type": "DATASET", "confidence": 0.8736443519592285}]}, {"text": "They found that subjects preferred the visualizations, but were more likely to make correct decisions from the text summaries.", "labels": [], "entities": []}, {"text": "It is unclear whether this is because subjects had inappropriate preferences, or because there was a big difference between genuine medical effectiveness and off-ward decision-support effectiveness (as mentioned earlier).", "labels": [], "entities": []}, {"text": "The only studies we are aware of which examined how well human judgments predict task-effectiveness of computer-generated texts occurred in the recent Generation Challenges evaluations of referring expression generation, which measured the correlations between human assessments of language quality and adequacy of content with task-performance measures (referent identification time and accuracy).", "labels": [], "entities": [{"text": "referring expression generation", "start_pos": 188, "end_pos": 219, "type": "TASK", "confidence": 0.7819869716962179}, {"text": "accuracy", "start_pos": 388, "end_pos": 396, "type": "METRIC", "confidence": 0.9933919310569763}]}, {"text": "The results revealed a strong and highly significant correlation between human judgments of content adequacy and identification accuracy; there was also a significant inverse correlation between human judgments of language quality and identification speed (i.e., those systems that tended to be judged more fluent by the human assessors also tended to have shorter identification times).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 128, "end_pos": 136, "type": "METRIC", "confidence": 0.8725512027740479}]}, {"text": "In recent years there has been growing interest in evaluating NLG texts by comparing them to a corpus of human-written reference texts, using automatic metrics such as string-edit distance, tree similarity, or BLEU (; this is another type of intrinsic evaluation.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 210, "end_pos": 214, "type": "METRIC", "confidence": 0.998633086681366}]}, {"text": "Such evaluations have been used by Bangalore,, for example.", "labels": [], "entities": []}, {"text": "Langkilde (2002) evaluated an NLG system by parsing texts from a corpus, feeding the parser output to her NLG system, and then comparing the generated texts to the original corpus texts.", "labels": [], "entities": []}, {"text": "Similar \"corpus regeneration\" evaluations have since been used by a number of other researchers).", "labels": [], "entities": [{"text": "corpus regeneration\"", "start_pos": 9, "end_pos": 29, "type": "TASK", "confidence": 0.8278207182884216}]}, {"text": "Corpus-based evaluation has been especially popular in the evaluation of surface realizers.", "labels": [], "entities": [{"text": "evaluation of surface realizers", "start_pos": 59, "end_pos": 90, "type": "TASK", "confidence": 0.6879170462489128}]}, {"text": "This maybe because the most important attribute of many realizers is grammatical coverage and robust handling of special and unusual cases, and corpus-based techniques are well suited to evaluating this.", "labels": [], "entities": []}, {"text": "Also, the range of acceptable outputs can be smaller in realizer evaluations because content, microplanning, and (in some cases) lexical choices do not vary; this means there is less concern about reference texts not adequately covering the solution space.", "labels": [], "entities": []}, {"text": "Automatic corpus-based evaluations are appealing in NLG, as in other areas of NLP, because they are relatively cheap and quick to do if a corpus is available, do not require support from domain experts, and are repeatable.", "labels": [], "entities": []}, {"text": "However, their use in NLG is controversial, at least when evaluating systems as a whole instead of just surface realizers, because many people are concerned that the results of such evaluations may not be meaningful.", "labels": [], "entities": []}, {"text": "For example point out that corpus texts are often not of high enough quality to form good reference texts; and Scott and Moore express concern that metrics will not be able to evaluate many important linguistic properties such as information structure.", "labels": [], "entities": []}, {"text": "A more general concern is that automatic metrics based on comparison to reference texts measure how well a text matches what writers do, whereas most human evaluations (task or judgment-based) measure the impact of a text on readers.", "labels": [], "entities": []}, {"text": "Because writers do not always produce optimal texts from a reader's perspective), a metric which is a good evaluator of how likely it is that a text has been written by a human writer is not necessarily a good predictor of how effective and useful the text is from the perspective of a human reader.", "labels": [], "entities": []}, {"text": "Of course automatic metrics do not need to be writer-based.", "labels": [], "entities": []}, {"text": "Indeed, some reader-based automatic metrics, such as the Flesch score) (based on average sentence and word length), are widely used as practical tools to help writers, but such metrics have not been widely used to evaluate NLP systems.", "labels": [], "entities": [{"text": "Flesch score", "start_pos": 57, "end_pos": 69, "type": "METRIC", "confidence": 0.9695960879325867}]}, {"text": "An important practical consideration is that corpus-based evaluations require a corpus of human-written reference texts; BLEU-like metrics in fact work best when the reference text corpus contains several reference texts for the same input, written by different authors.", "labels": [], "entities": [{"text": "BLEU-like", "start_pos": 121, "end_pos": 130, "type": "METRIC", "confidence": 0.9914321899414062}]}, {"text": "If reference texts have to be created specifically for an evaluation, this can bean expensive endeavor.", "labels": [], "entities": []}, {"text": "In the BabyTalk domain, for example, it can take an experienced clinician several hours to write a corpus text from the raw data; hence creating a corpus of 100 reference texts in this domain could require 2-3 months effort by a clinician (as they do not create such reports in the course of their normal work).", "labels": [], "entities": [{"text": "BabyTalk domain", "start_pos": 7, "end_pos": 22, "type": "DATASET", "confidence": 0.9399076700210571}]}, {"text": "Getting this much time from an expert doctor or nurse would be difficult unless a very strong case could be made for the utility of the evaluation.", "labels": [], "entities": []}, {"text": "Of course, evaluation and experimentation are crucial to all fields of NLP; here we look at insights from two other NLP subfields which need to evaluate the quality of texts: machine translation and document summarization.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 175, "end_pos": 194, "type": "TASK", "confidence": 0.7991324365139008}, {"text": "document summarization", "start_pos": 199, "end_pos": 221, "type": "TASK", "confidence": 0.6806674599647522}]}, {"text": "There is a rich literature in MT evaluation, including a number of specialist workshops on this topic; as in NLG, there is also considerable interest in using shared-task events to provide data about how well different evaluation techniques correlate with each other.", "labels": [], "entities": [{"text": "MT evaluation", "start_pos": 30, "end_pos": 43, "type": "TASK", "confidence": 0.9882921278476715}]}, {"text": "From an NLG perspective, the most surprising aspect of current MT evaluation is the dominance of BLEU and other automatic corpus-based metrics).", "labels": [], "entities": [{"text": "MT evaluation", "start_pos": 63, "end_pos": 76, "type": "TASK", "confidence": 0.9638583362102509}, {"text": "BLEU", "start_pos": 97, "end_pos": 101, "type": "METRIC", "confidence": 0.9951565861701965}]}, {"text": "BLEU was first proposed as a supplement (the U in BLEU stands for \"understudy\") for human evaluation (, but it is now routinely used as the main technique for evaluating research contributions.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.9780891537666321}, {"text": "BLEU", "start_pos": 50, "end_pos": 54, "type": "METRIC", "confidence": 0.9821327328681946}]}, {"text": "It is accepted and indeed the norm for an article on MT in Computational Linguistics to report evaluations that are solely based on automatic corpus-based metrics; this is not the casein NLG, where human evaluations are expected at least in high-prestige venues.", "labels": [], "entities": [{"text": "MT in Computational Linguistics", "start_pos": 53, "end_pos": 84, "type": "TASK", "confidence": 0.8818749934434891}]}, {"text": "We are not aware of any studies in MT that have tried to correlate BLEU-like metrics with the results of task-effectiveness studies.", "labels": [], "entities": [{"text": "MT", "start_pos": 35, "end_pos": 37, "type": "TASK", "confidence": 0.9840055704116821}, {"text": "BLEU-like", "start_pos": 67, "end_pos": 76, "type": "METRIC", "confidence": 0.9968024492263794}]}, {"text": "Although a number of studies have analyzed the correlation between BLEU-type metrics and human judgments, most of these have used human judgments from NIST MT evaluations.", "labels": [], "entities": [{"text": "BLEU-type", "start_pos": 67, "end_pos": 76, "type": "METRIC", "confidence": 0.9909387826919556}, {"text": "NIST MT evaluations", "start_pos": 151, "end_pos": 170, "type": "DATASET", "confidence": 0.7096562186876932}]}, {"text": "Human judgments inmost of these evaluations were solicited from monolingual subjects who were asked to compare the output of MT systems to a single reference translation, without any context; also in many of these studies the subjects were asked to assess individual sentences or even phrases, not complete texts).", "labels": [], "entities": [{"text": "MT", "start_pos": 125, "end_pos": 127, "type": "TASK", "confidence": 0.9638816714286804}]}, {"text": "As and others have pointed out, it is not clear that human judgments solicited in this way would match the judgments of bilingual subjects who were shown complete source and MT texts, and asked to evaluate the quality of the translation in a specific real-world context.", "labels": [], "entities": [{"text": "MT texts", "start_pos": 174, "end_pos": 182, "type": "TASK", "confidence": 0.8077227175235748}]}, {"text": "in fact found that BLEU scores were more highly correlated with human judgments from monolingual subjects than human judgments from bilingual subjects.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 19, "end_pos": 23, "type": "METRIC", "confidence": 0.9982884526252747}]}, {"text": "In any case, regardless of the effectiveness of BLEU as an MT evaluation metric, another issue is whether an MT evaluation technique can in general be expected to work as an NLG evaluation technique.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 48, "end_pos": 52, "type": "METRIC", "confidence": 0.9961646795272827}, {"text": "MT evaluation", "start_pos": 59, "end_pos": 72, "type": "TASK", "confidence": 0.8818925023078918}, {"text": "MT evaluation", "start_pos": 109, "end_pos": 122, "type": "TASK", "confidence": 0.9195146858692169}]}, {"text": "There are some obvious differences between MT systems and NLG systems; for example: r Content determination: NLG systems need to decide on what information should be communicated in a text, as well as how this information is linguistically expressed; MT systems generally do not have to perform content determination.", "labels": [], "entities": [{"text": "MT", "start_pos": 43, "end_pos": 45, "type": "TASK", "confidence": 0.9543952941894531}, {"text": "Content determination", "start_pos": 86, "end_pos": 107, "type": "TASK", "confidence": 0.7461604177951813}, {"text": "content determination", "start_pos": 295, "end_pos": 316, "type": "TASK", "confidence": 0.7254339307546616}]}, {"text": "r Linguistic variety: Many NLG systems produce text that is fairly simple from a linguistic perspective (partially because many NLG users prefer such texts); MT systems, in contrast, usually need to produce linguistically complex texts.", "labels": [], "entities": []}, {"text": "r Genre/domain: Most applied NLG systems (with some exceptions) try to generate high-quality texts in a limited domain and genre such as marine weather forecasts; MT systems, in contrast, typically generate lower-quality texts in abroad text category such as newspaper articles.", "labels": [], "entities": []}, {"text": "These differences presumably need to be considered when deciding whether it makes sense to use an MT evaluation technique in NLG.", "labels": [], "entities": [{"text": "MT evaluation", "start_pos": 98, "end_pos": 111, "type": "TASK", "confidence": 0.901620477437973}]}, {"text": "For example, there is no reason to expect MT evaluation techniques to be useful for evaluating NLG content determination, since MT systems do not perform this task.", "labels": [], "entities": [{"text": "MT evaluation", "start_pos": 42, "end_pos": 55, "type": "TASK", "confidence": 0.9592050611972809}, {"text": "NLG content determination", "start_pos": 95, "end_pos": 120, "type": "TASK", "confidence": 0.7521207531293234}]}, {"text": "Also, MT evaluation techniques which work well when evaluating less-than-human-quality texts from an MT system may not necessarily work well when evaluating human-quality texts produced by an NLG system.", "labels": [], "entities": [{"text": "MT evaluation", "start_pos": 6, "end_pos": 19, "type": "TASK", "confidence": 0.9497159123420715}]}, {"text": "Another branch of NLP which requires the evaluation of textual documents is document summarization.", "labels": [], "entities": [{"text": "document summarization", "start_pos": 76, "end_pos": 98, "type": "TASK", "confidence": 0.6029204875230789}]}, {"text": "From an evaluation perspective, an important difference between MT and summarization is that summarization evaluations have placed much more emphasis on content determination.", "labels": [], "entities": [{"text": "MT", "start_pos": 64, "end_pos": 66, "type": "TASK", "confidence": 0.9927109479904175}, {"text": "summarization", "start_pos": 71, "end_pos": 84, "type": "TASK", "confidence": 0.9751423001289368}, {"text": "summarization evaluations", "start_pos": 93, "end_pos": 118, "type": "TASK", "confidence": 0.9025895297527313}, {"text": "content determination", "start_pos": 153, "end_pos": 174, "type": "TASK", "confidence": 0.7502281665802002}]}, {"text": "Perhaps in part because of this, the summarization community places more emphasis on human evaluations.", "labels": [], "entities": [{"text": "summarization", "start_pos": 37, "end_pos": 50, "type": "TASK", "confidence": 0.9777927398681641}]}, {"text": "Although there are automatic corpus-based metrics for summarization such as ROUGE (Lin and Hovy 2003), they do not seem to dominate summarization evaluation in the same way that BLEU-type metrics dominate MT evaluation.", "labels": [], "entities": [{"text": "summarization", "start_pos": 54, "end_pos": 67, "type": "TASK", "confidence": 0.982996940612793}, {"text": "ROUGE", "start_pos": 76, "end_pos": 81, "type": "METRIC", "confidence": 0.9822061061859131}, {"text": "summarization evaluation", "start_pos": 132, "end_pos": 156, "type": "TASK", "confidence": 0.9026302397251129}, {"text": "BLEU-type", "start_pos": 178, "end_pos": 187, "type": "METRIC", "confidence": 0.9347244501113892}, {"text": "MT evaluation", "start_pos": 205, "end_pos": 218, "type": "TASK", "confidence": 0.9362618029117584}]}, {"text": "The main summarization evaluation technique in the NIST TAC 2008 summarization track is the pyramid technique (Nenkova and Passonneau 2004), which is a structured human-based evaluation, based on asking human judges to identify 'summarization content units' (SCU) in model and system-generated summaries, and measuring how many SCUs from the model summaries occur in the system summary.", "labels": [], "entities": [{"text": "summarization evaluation", "start_pos": 9, "end_pos": 33, "type": "TASK", "confidence": 0.9302341639995575}, {"text": "NIST TAC 2008 summarization track", "start_pos": 51, "end_pos": 84, "type": "DATASET", "confidence": 0.8334023475646972}]}, {"text": "This is an interesting technique for evaluating content, and might be worth investigating for evaluating content determination in NLG systems.", "labels": [], "entities": [{"text": "content determination", "start_pos": 105, "end_pos": 126, "type": "TASK", "confidence": 0.7395790815353394}]}, {"text": "In terms of validation, a number of studies have claimed that ROUGE correlates with human ratings, for example and.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 62, "end_pos": 67, "type": "METRIC", "confidence": 0.9553384184837341}]}, {"text": "checked if ROUGE scores correlated with task effectiveness; they did not find a strong correlation.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 11, "end_pos": 16, "type": "METRIC", "confidence": 0.9863234758377075}]}, {"text": "Given the growing interest in using automatic evaluation metrics such as BLEU in NLG, we decided to carryout some experiments to determine how well such metrics predicted the results of human judgments.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 73, "end_pos": 77, "type": "METRIC", "confidence": 0.9979494214057922}, {"text": "NLG", "start_pos": 81, "end_pos": 84, "type": "DATASET", "confidence": 0.855491042137146}]}, {"text": "As in other such studies, we did this by evaluating a number of systems with the same input/output functionality, using different evaluation techniques, and then analyzing the correlation between the techniques.", "labels": [], "entities": []}, {"text": "One potential weakness of our experiments was that we did not look at correlations with task-effectiveness evaluations.", "labels": [], "entities": []}, {"text": "This was because we did not have the resources (money and domain-expert goodwill) to conduct a task-based evaluation.", "labels": [], "entities": []}, {"text": "This issue is further discussed in Section 4.2.", "labels": [], "entities": []}, {"text": "We conducted two experiments where we asked human subjects to rate texts produced by our different marine weather-forecast generators.", "labels": [], "entities": []}, {"text": "The main difference was that the first experiment focused on evaluating linguistic quality, and only looked at texts with the same information content.", "labels": [], "entities": []}, {"text": "The second experiment also evaluated content quality, and used texts that varied in content as well as in linguistic expression.", "labels": [], "entities": []}, {"text": "We also changed the experimental design in the second experiment, based on our experiences in the first experiment.", "labels": [], "entities": []}, {"text": "In Experiment 1 (the main results of which we reported previously in), we focused on the content-to-realization mapping, so we restricted ourselves to systems which generated texts from content tuples) (SUMTIME-Hybrid and the pCRU systems).", "labels": [], "entities": []}, {"text": "We also included the corresponding texts from the SUMTIME-METEO corpus.", "labels": [], "entities": [{"text": "SUMTIME-METEO corpus", "start_pos": 50, "end_pos": 70, "type": "DATASET", "confidence": 0.7926002442836761}]}, {"text": "We used a randomly selected subset of 21 forecast dates from the SUMTIME-METEO corpus.", "labels": [], "entities": [{"text": "SUMTIME-METEO corpus", "start_pos": 65, "end_pos": 85, "type": "DATASET", "confidence": 0.8462472856044769}]}, {"text": "We restricted ourselves to morning forecasts (half the corpus), as these are based on a single data file (evening forecasts are based on two data files), and to the first wind description in a forecast, as subsequent wind descriptions have the added constraint of being consistent inform and content with earlier wind descriptions.", "labels": [], "entities": []}, {"text": "For each of these dates, we obtained seven texts: the corpus text, the texts produced by the previously mentioned systems, and one of the reference texts used by the automatic metrics (Section 3.3.1).", "labels": [], "entities": []}, {"text": "1 This gave us a total of 147 texts.", "labels": [], "entities": []}, {"text": "Texts produced for 5 Oct 2000, from content tuples in.", "labels": [], "entities": []}, {"text": "For our human evaluators, we recruited nine people with experience reading forecasts for offshore oil rigs ('experts').", "labels": [], "entities": []}, {"text": "Note that these were experienced forecast readers, not forecast writers.", "labels": [], "entities": []}, {"text": "We also recruited 21 people with no experience in reading forecasts for offshore oil rigs ('non-experts').", "labels": [], "entities": []}, {"text": "The reason for including non-experts was that we wanted to see if ratings by non-experts were similar to ratings by experts (as non-experts are often much easier to recruit for experiments).", "labels": [], "entities": []}, {"text": "None of the subjects had a background in NLP, and all were native speakers of English.", "labels": [], "entities": []}, {"text": "Our first experiment focused on linguistic expression, but of course content determination is very important in NLG, so we decided to run another experiment which also included texts generated from meteorological data, by systems which performed content determination (that is, SUMTIME and Template).", "labels": [], "entities": [{"text": "content determination", "start_pos": 69, "end_pos": 90, "type": "TASK", "confidence": 0.728967010974884}, {"text": "content determination", "start_pos": 246, "end_pos": 267, "type": "TASK", "confidence": 0.7068125605583191}]}, {"text": "In this experiment we showed subjects the raw forecast data and asked them for separate ratings on \"clarity and readability\" (which was intended to elicit an assessment of linguistic quality) and \"accuracy and appropriateness\" (which was intended to elicit an assessment of content quality).", "labels": [], "entities": [{"text": "clarity", "start_pos": 100, "end_pos": 107, "type": "METRIC", "confidence": 0.9956536293029785}, {"text": "accuracy", "start_pos": 197, "end_pos": 205, "type": "METRIC", "confidence": 0.9978525638580322}]}, {"text": "For brevity, we refer to these scores as Clarity and Accuracy, respectively, herein.", "labels": [], "entities": [{"text": "Clarity", "start_pos": 41, "end_pos": 48, "type": "METRIC", "confidence": 0.9975072741508484}, {"text": "Accuracy", "start_pos": 53, "end_pos": 61, "type": "METRIC", "confidence": 0.998195469379425}]}, {"text": "We used 14 new randomly selected forecast dates, and 14 new expert subjects (we did not ask non-experts to rate these texts, because we were not confident that they could assess the accuracy and appropriateness of texts).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 182, "end_pos": 190, "type": "METRIC", "confidence": 0.9857757091522217}]}, {"text": "The subjects were asked to rate seven types of texts: corpus texts, SUMTIME texts, Template texts, and texts produced by the Experiment 1 systems (except that we dropped pCRU-2gram); we did not in this experiment ask subjects to rate reference texts.", "labels": [], "entities": []}, {"text": "We would have liked to recruit more than 14 subjects, but this proved difficult (see also Section 4.2); however, 14 subjects is an improvement over the 9 expert subjects used in Experiment 1 from the perspective of limiting the impact of individual differences between subjects.", "labels": [], "entities": []}, {"text": "We also made a number of changes to our experimental design, based on issues identified in the first experiment with expert subjects.", "labels": [], "entities": []}, {"text": "The most important ones were that we used a Latin Square design (with two subjects rating each system/date combination), we asked for ratings on a seven-point scale instead of a six-point one (so the scale had a middle position which subjects could select), we explicitly gave instructions as to what the ratings meant (to reduce variation due to differing interpretations of the scale), and we carried out a non-parametric as well as parametric statistical analysis.", "labels": [], "entities": []}, {"text": "A screenshot from the experiment is shown in.", "labels": [], "entities": []}, {"text": "There was a significant (p < 0.001) correlation between the accuracy and clarity scores that subjects gave to texts (Pearson r = 0.58), when computed on the 196 individual ratings made by subjects (when correlation is computed on the mean values, significance cannot be shown, because there are far fewer data points: Pearson's r = 0.572, p = 0.09).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 60, "end_pos": 68, "type": "METRIC", "confidence": 0.9991593360900879}, {"text": "clarity", "start_pos": 73, "end_pos": 80, "type": "METRIC", "confidence": 0.9726373553276062}, {"text": "Pearson r", "start_pos": 117, "end_pos": 126, "type": "METRIC", "confidence": 0.9676870405673981}]}, {"text": "It is not clear whether this is because subjects did not properly distinguish accuracy from clarity, or because generators that generated high-accuracy texts (such as SUMTIME) also generated high-clarity texts.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 78, "end_pos": 86, "type": "METRIC", "confidence": 0.9898478388786316}, {"text": "clarity", "start_pos": 92, "end_pos": 99, "type": "METRIC", "confidence": 0.8716956377029419}]}, {"text": "The averaged results of the human evaluations in Experiment 2 are shown in.", "labels": [], "entities": []}, {"text": "Because the Experiment 1 texts were communicating the same content, and only differed in linguistic expression, it seems likely that Experiment 2's clarity scores should correlate with Experiment 1's scores.", "labels": [], "entities": [{"text": "clarity", "start_pos": 148, "end_pos": 155, "type": "METRIC", "confidence": 0.9977746605873108}]}, {"text": "This is indeed the case: The correlation between average scores for the five systems that were included in both experiments is high with Pearson's r = 0.9 (p < 0.05).", "labels": [], "entities": [{"text": "Pearson's r", "start_pos": 137, "end_pos": 148, "type": "METRIC", "confidence": 0.9369710286458334}]}, {"text": "The SPSS GLM found a very significant effect of generator on both accuracy and clarity scores (p < 0.001).", "labels": [], "entities": [{"text": "SPSS GLM", "start_pos": 4, "end_pos": 12, "type": "DATASET", "confidence": 0.8254266381263733}, {"text": "generator", "start_pos": 48, "end_pos": 57, "type": "METRIC", "confidence": 0.9767509698867798}, {"text": "accuracy", "start_pos": 66, "end_pos": 74, "type": "METRIC", "confidence": 0.9994402527809143}, {"text": "clarity scores", "start_pos": 79, "end_pos": 93, "type": "METRIC", "confidence": 0.9781427681446075}]}, {"text": "shows the homogeneous subsets identified by the Tukey HSD post hoc test.", "labels": [], "entities": [{"text": "Tukey HSD post hoc test", "start_pos": 48, "end_pos": 71, "type": "DATASET", "confidence": 0.7146172046661377}]}, {"text": "The corresponding pairwise results are as follows.", "labels": [], "entities": []}, {"text": "For the clarity scores, SUMTIME is significantly better than Template and pCRU-random; and all systems except Template are better than pCRU-random.", "labels": [], "entities": [{"text": "clarity", "start_pos": 8, "end_pos": 15, "type": "METRIC", "confidence": 0.9984084963798523}]}, {"text": "For the accuracy scores, SUMTIME is significantly better than all systems except pCRU-greedy and Template.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 8, "end_pos": 16, "type": "METRIC", "confidence": 0.9994882345199585}, {"text": "SUMTIME", "start_pos": 25, "end_pos": 32, "type": "METRIC", "confidence": 0.6111054420471191}]}, {"text": "The GLM analysis also showed that subject and forecast date (as well as generator) had a significant impact on accuracy and clarity ratings (p < 0.002).", "labels": [], "entities": [{"text": "GLM analysis", "start_pos": 4, "end_pos": 16, "type": "DATASET", "confidence": 0.7694689035415649}, {"text": "accuracy", "start_pos": 111, "end_pos": 119, "type": "METRIC", "confidence": 0.9994139671325684}, {"text": "clarity", "start_pos": 124, "end_pos": 131, "type": "METRIC", "confidence": 0.9940347075462341}]}, {"text": "This statistical analysis assumes that it is appropriate to use ANOVA-like tests to analyze quality ratings.", "labels": [], "entities": []}, {"text": "Although this is common practice in many NLP papers, including most previous validation studies of automatic metrics which we are aware of, a good argument can be made that quality ratings should be analyzed using nonparametric tests.", "labels": [], "entities": []}, {"text": "This is because ratings are ordinal, so it is not clear that it makes sense to compute their mean, which is what the ANOVA and GLM tests do.", "labels": [], "entities": [{"text": "ANOVA", "start_pos": 117, "end_pos": 122, "type": "DATASET", "confidence": 0.5162708163261414}]}, {"text": "Hence we also carried out a non-parametric analysis to identify significant differences in the human ratings.", "labels": [], "entities": []}, {"text": "More specifically, we used the Wilcoxon Signed-Rank test, with a Bonferroni multiple-hypothesis correction, to identify pairs of systems that had significantly different ratings.", "labels": [], "entities": []}, {"text": "When comparing two systems, the Wilcoxon test requires each rating of the first system to be paired with a related rating of the second system.", "labels": [], "entities": []}, {"text": "Because we had two ratings from every subject for each system, we paired the lowest rating that a subject gave to a text produced by the first system with the lowest rating that that subject gave to a text produced by the second system; we similarly paired the highest ratings given by each subject to the two systems.", "labels": [], "entities": []}, {"text": "For example, subject AG evaluated two SUMTIME texts and gave them clarity ratings of 5 and 6; he also evaluated two Template texts, and gave them clarity ratings of 4 and 6.", "labels": [], "entities": [{"text": "clarity", "start_pos": 66, "end_pos": 73, "type": "METRIC", "confidence": 0.9989014863967896}, {"text": "clarity", "start_pos": 146, "end_pos": 153, "type": "METRIC", "confidence": 0.9974799752235413}]}, {"text": "In our non-parametric analysis, we paired the lowest rating given by AG to a SUMTIME text (that is, 5) with the lowest rating given by AG to a Template text (that is, 4); we also paired the highest rating given by AG to a SUMTIME text (that is, 6) with the highest rating given by AG to a Template text (that is, 6).", "labels": [], "entities": []}, {"text": "This pairing was possible because we used a Latin Square design in this experiment, which meant that every subject rated the same number of texts (two) from each generator.", "labels": [], "entities": []}, {"text": "This procedure identified the same four significant differences in Accuracy as in-namely, SUMTIME is significantly better than all systems except pCRU-greedy and Template.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 67, "end_pos": 75, "type": "METRIC", "confidence": 0.9990761280059814}]}, {"text": "However, it identified only three significant differences in Claritynamely, SUMTIME is significantly better than Template and pCRU-random, and pCRUgreedy is better than pCRU-random (this is a subset of the significant differences in Clarity shown in).", "labels": [], "entities": []}, {"text": "In the wider NLP community, automatic metrics are especially popular in shared-task evaluations.", "labels": [], "entities": []}, {"text": "This is partially because such metrics have a very low marginal cost compared to human evaluations.", "labels": [], "entities": []}, {"text": "Automatic metrics need reference texts, and obtaining good reference texts can be costly; but once a collection of reference texts has been created, it can be used to evaluate any number of systems.", "labels": [], "entities": []}, {"text": "Also automatic metrics are very easy to use once the software and reference corpus has been created; developers do not need to be trained in using BLEU and ROUGE.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 147, "end_pos": 151, "type": "METRIC", "confidence": 0.9963507652282715}, {"text": "ROUGE", "start_pos": 156, "end_pos": 161, "type": "METRIC", "confidence": 0.9158151149749756}]}, {"text": "In contrast human-based evaluations generally need a certain number of subjects per system, so their cost goes up with the number of systems evaluated.", "labels": [], "entities": []}, {"text": "Also, expertise and/or training is needed to conduct experiments with people, which not all NLP researchers possess.", "labels": [], "entities": []}, {"text": "Finally, evaluation with metrics is entirely reproducible.", "labels": [], "entities": []}, {"text": "However, despite the cost-effectiveness and other appealing aspects of automatic metrics in shared tasks, we do not believe that shared tasks in NLG should use automatic metrics as the sole evaluation criterion.", "labels": [], "entities": []}, {"text": "Until there is better evidence that automatic metrics correlate with human evaluations, shared tasks in NLG should also include human evaluations, preferably task-effectiveness ones.", "labels": [], "entities": []}, {"text": "This strategy is being followed in the Generation Challenges shared-task NLG events (Section 2.1.4).", "labels": [], "entities": []}, {"text": "We have focused in this article on evaluations that measure the quality of generated texts, but many NLG developers are also interested in diagnostic evaluations whose purpose is to identify problems in a system and suggest improvements.", "labels": [], "entities": []}, {"text": "From this perspective, an advantage of human evaluations is that human subjects can be asked to make free-text comments on the texts that they see, and these comments are often extremely useful from a diagnostic perspective.", "labels": [], "entities": []}, {"text": "On the other hand, an advantage of automatic metrics is that they allow developers to rapidly evaluate changes to systems and algorithms; indeed, some machine translation researchers use automatic metrics to automatically tune parameters without human intervention.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 151, "end_pos": 170, "type": "TASK", "confidence": 0.7511549592018127}]}, {"text": "However, as Och points out, this is only sensible if automatic metrics are known to be very accurate predictors of text quality.", "labels": [], "entities": []}, {"text": "Because our results suggest that current automatic metrics are not highly accurate predictors of the quality of texts produced by NLG systems, we recommend developers be cautious in using metrics for diagnostic evaluation, and do not use metrics for automatic parameter tuning.", "labels": [], "entities": [{"text": "parameter tuning", "start_pos": 260, "end_pos": 276, "type": "TASK", "confidence": 0.7220643162727356}]}, {"text": "On the other hand, automatic metrics do have a potential advantage in small diagnostic evaluations, which is that they are not influenced by the individual preferences of a small number of human subjects.", "labels": [], "entities": []}, {"text": "There are large differences in how different human subjects rate texts, as we pointed out at the end of Section 3.2.1.", "labels": [], "entities": []}, {"text": "Such differences are not unusual: we have seen them inmost human evaluations of NLG systems which we have carried out.", "labels": [], "entities": []}, {"text": "These differences can be controlled for in a large experiment which uses many subjects.", "labels": [], "entities": []}, {"text": "But if a diagnostic evaluation is conducted with a small number of subjects, who are chosen partially on the basis of being easy to recruit, there is a risk that the preferences expressed by these subjects will not be representative of users in general, and hence may mislead the developer as to how the system should be changed.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 5  Experiment 1: Metric scores against three reference texts (produced by rewriting corpus texts),  for the set of 18 forecasts used in expert evaluation.", "labels": [], "entities": []}, {"text": " Table 6  Experiment 1: Correlation (Pearson's r) between human scores and automatic metrics.", "labels": [], "entities": [{"text": "Pearson's r)", "start_pos": 37, "end_pos": 49, "type": "METRIC", "confidence": 0.9223803281784058}]}, {"text": " Table 7  Experiment 2: Metric scores against three reference texts (written from raw data), for the set of  14 forecasts.", "labels": [], "entities": []}, {"text": " Table 8  Experiment 2: Correlation (Pearson's r) between human score and metrics.", "labels": [], "entities": [{"text": "Pearson's r)", "start_pos": 37, "end_pos": 49, "type": "METRIC", "confidence": 0.9460759311914444}]}, {"text": " Table 9  Number of significant non-parametric differences predicted by each metric.", "labels": [], "entities": []}]}