{"title": [{"text": "A Framework for Fast Incremental Interpretation during Speech Decoding", "labels": [], "entities": [{"text": "Incremental Interpretation during Speech Decoding", "start_pos": 21, "end_pos": 70, "type": "TASK", "confidence": 0.6255247116088867}]}], "abstractContent": [{"text": "This article describes a framework for incorporating referential semantic information from a world model or ontology directly into a probabilistic language model of the sort commonly used in speech recognition, where it can be probabilistically weighted together with phonological and syntactic factors as an integral part of the decoding process.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 191, "end_pos": 209, "type": "TASK", "confidence": 0.7242253422737122}]}, {"text": "Introducing world model referents into the decoding search greatly increases the search space, but by using a single integrated phonological, syntactic, and referential semantic language model, the decoder is able to incrementally prune this search based on probabilities associated with these combined contexts.", "labels": [], "entities": []}, {"text": "The result is a single unified referential semantic probability model which brings several kinds of context to bear in speech decoding, and performs accurate recognition in real time on large domains in the absence of example in-domain training sentences.", "labels": [], "entities": []}], "introductionContent": [{"text": "The capacity to rapidly connect language to referential meaning is an essential aspect of communication between humans.", "labels": [], "entities": []}, {"text": "Eye-tracking studies show that humans listening to spoken directives are able to actively attend to the entities that the words in these directives might refer to, even while the words are still being pronounced (.", "labels": [], "entities": []}, {"text": "This timely access to referential information about input utterances may allow listeners to adjust their preferences among likely interpretations of noisy or ambiguous utterances to favor those that make sense in the current environment or discourse context, before any lower-level disambiguation decisions have been made.", "labels": [], "entities": []}, {"text": "This same capability in a spoken language interface system could allow reliable human-machine interaction in the idiosyncratic language of day-to-day life, populated with proper names of co-workers, objects, and events not found in broad training corpora.", "labels": [], "entities": []}, {"text": "When domain-specific training corpora are not available, a referential semantic interface could still exploit its model of the world: the data to which it is an interface, and patterns characterizing these data.", "labels": [], "entities": []}, {"text": "This article describes a framework for incorporating referential semantic information from a world model or ontology directly into a statistical language model of the sort commonly used in speech recognition, where it can be probabilistically weighted together with phonological and syntactic factors as an integral part of the decoding process.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 189, "end_pos": 207, "type": "TASK", "confidence": 0.7234462201595306}]}, {"text": "Introducing world model referents into the decoding search greatly increases the search space, but by using a single integrated phonological, syntactic, and referential semantic language model, the decoder is able to incrementally prune this search based on probabilities associated with these combined contexts.", "labels": [], "entities": []}, {"text": "Semantic interpretation is defined dynamically in this framework, in terms of transitions overtime from less constrained referents to more constrained referents.", "labels": [], "entities": [{"text": "Semantic interpretation", "start_pos": 0, "end_pos": 23, "type": "TASK", "confidence": 0.8309888541698456}]}, {"text": "Because it is defined dynamically, interpretation in this framework can incorporate dependencies on referential context-for example, constraining interpretations to a presumed set of entities, or a presumed setting-which maybe fixed prior to recognition, or dynamically hypothesized earlier in the recognition process.", "labels": [], "entities": []}, {"text": "This contrasts with other recent systems which interpret constituents only given fixed inter-utterance contexts or explicit syntactic arguments).", "labels": [], "entities": []}, {"text": "Moreover, because it is defined dynamically, in terms of transitions, this context-dependent interpretation framework can be directly integrated into a Viterbi decoding search, like ordinary state transitions in a Hidden Markov Model.", "labels": [], "entities": []}, {"text": "The result is a single unified referential semantic probability model which brings several kinds of referential semantic context to bear in speech decoding, and performs accurate recognition in real time on large domains in the absence of example domain-specific training sentences.", "labels": [], "entities": []}, {"text": "The remainder of this article is organized as follows: Section 2 will describe related approaches to interleaving semantic interpretation with speech recognition.", "labels": [], "entities": [{"text": "interleaving semantic interpretation", "start_pos": 101, "end_pos": 137, "type": "TASK", "confidence": 0.631360391775767}, {"text": "speech recognition", "start_pos": 143, "end_pos": 161, "type": "TASK", "confidence": 0.7204981446266174}]}, {"text": "Section 3 will provide definitions for world models used in semantic interpretation, and language models used in speech decoding, which will form the basis of a referential semantic language model, defined in Section 4.", "labels": [], "entities": [{"text": "semantic interpretation", "start_pos": 60, "end_pos": 83, "type": "TASK", "confidence": 0.7884266078472137}]}, {"text": "Then Section 5 will describe an evaluation of this model in a sample spoken language interface application.", "labels": [], "entities": []}], "datasetContent": [{"text": "Much of the motivation for this approach has been to develop a human-like model of language processing.", "labels": [], "entities": []}, {"text": "But there are practical advantages to this approach as well.", "labels": [], "entities": []}, {"text": "One of the main practical advantages of the referential semantic language model described", "labels": [], "entities": [{"text": "referential semantic language", "start_pos": 44, "end_pos": 73, "type": "TASK", "confidence": 0.8380334973335266}]}], "tableCaptions": [{"text": " Table 2  Per-subject results for Language Model \u0398 LM-Sem with M 240 .", "labels": [], "entities": []}, {"text": " Table 3  Per-subject results for Language Model \u0398 LM-Sem with M 4175 .", "labels": [], "entities": []}, {"text": " Table 4  Per-subject results for Language Model \u0398 LM-NoSem .", "labels": [], "entities": []}, {"text": " Table 5  Per-subject results for Language Model \u0398 LM-Trigram with M 240 .", "labels": [], "entities": []}, {"text": " Table 6  Experimental results with four model configurations.", "labels": [], "entities": []}]}