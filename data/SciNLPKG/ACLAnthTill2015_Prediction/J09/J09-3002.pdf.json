{"title": [], "abstractContent": [{"text": "Multimodal grammars provide an effective mechanism for quickly creating integration and understanding capabilities for interactive systems supporting simultaneous use of multiple input modalities.", "labels": [], "entities": []}, {"text": "However, like other approaches based on hand-crafted grammars, multimodal grammars can be brittle with respect to unexpected, erroneous, or disfluent input.", "labels": [], "entities": []}, {"text": "In this article, we show how the finite-state approach to multimodal language processing can be extended to support multimodal applications combining speech with complex freehand pen input, and evaluate the approach in the context of a multimodal conversational system (MATCH).", "labels": [], "entities": [{"text": "multimodal language processing", "start_pos": 58, "end_pos": 88, "type": "TASK", "confidence": 0.6517374515533447}]}, {"text": "We explore a range of different techniques for improving the robustness of multimodal integration and understanding.", "labels": [], "entities": []}, {"text": "These include techniques for building effective language models for speech recognition when little or no multimodal training data is available, and techniques for robust multimodal understanding that draw on classification, machine translation, and sequence edit methods.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 68, "end_pos": 86, "type": "TASK", "confidence": 0.7872275412082672}, {"text": "machine translation", "start_pos": 224, "end_pos": 243, "type": "TASK", "confidence": 0.7072600871324539}]}, {"text": "We also explore the use of edit-based methods to overcome mismatches between the gesture stream and the speech stream.", "labels": [], "entities": []}], "introductionContent": [{"text": "The ongoing convergence of the Web with telephony, driven by technologies such as voice over IP, broadband Internet access, high-speed mobile data networks, and handheld computers and smartphones, enables widespread deployment of multimodal interfaces which combine graphical user interfaces with natural modalities such as speech and pen.", "labels": [], "entities": []}, {"text": "The critical advantage of multimodal interfaces is that they allow user input and system output to be expressed in the mode or modes to which they are best suited, given the task at hand, user preferences, and the physical and social environment of the interaction.", "labels": [], "entities": []}, {"text": "There is also an increasing body of empirical evidence) showing user preference and task performance advantages of multimodal interfaces.", "labels": [], "entities": []}, {"text": "In order to support effective multimodal interfaces, natural language processing techniques, which have typically operated over linear sequences of speech or text, need to be extended in order to support integration and understanding of multimodal language distributed over multiple different input modes).", "labels": [], "entities": []}, {"text": "Multimodal grammars provide an expressive mechanism for quickly creating language processing capabilities for multimodal interfaces supporting input modes such as speech and gesture (Johnston and Bangalore 2000).", "labels": [], "entities": []}, {"text": "They support composite multimodal inputs by aligning speech input (words) and gesture input (represented as sequences of gesture symbols) while expressing the relation between the speech and gesture input and their combined semantic representation.", "labels": [], "entities": []}, {"text": "show that such grammars can be compiled into finite-state transducers, enabling effective processing of lattice input from speech and gesture recognition and mutual compensation for errors and ambiguities.", "labels": [], "entities": [{"text": "gesture recognition", "start_pos": 134, "end_pos": 153, "type": "TASK", "confidence": 0.6916473507881165}]}, {"text": "In this article, we show how multimodal grammars and their finite-state implementation can be extended to support more complex multimodal applications.", "labels": [], "entities": []}, {"text": "These applications combine speech with complex pen input including both freehand gestures and handwritten input.", "labels": [], "entities": []}, {"text": "More general mechanisms are introduced for representation of gestures and abstraction over specific content in the gesture stream along with anew technique for aggregation of gestures.", "labels": [], "entities": []}, {"text": "We evaluate the approach in the context of the MATCH multimodal conversational system (), an interactive city guide.", "labels": [], "entities": []}, {"text": "In Section 2, we present the MATCH application, the architecture of the system, and our experimental method for collection and annotation of multimodal data.", "labels": [], "entities": [{"text": "MATCH", "start_pos": 29, "end_pos": 34, "type": "TASK", "confidence": 0.5692038536071777}]}, {"text": "In Section 3, we evaluate the baseline approach on the collected data.", "labels": [], "entities": []}, {"text": "The performance of this baseline approach is limited by the use of hand-crafted models for speech recognition and multimodal understanding.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 91, "end_pos": 109, "type": "TASK", "confidence": 0.8256451785564423}, {"text": "multimodal understanding", "start_pos": 114, "end_pos": 138, "type": "TASK", "confidence": 0.7021424770355225}]}, {"text": "Like other approaches based on hand-crafted grammars, multimodal grammars can be brittle with respect to extra-grammatical, erroneous, and disfluent input.", "labels": [], "entities": []}, {"text": "This is particularly problematic for multimodal interfaces if they are to be used in noisy mobile environments.", "labels": [], "entities": []}, {"text": "To overcome this limitation we explore abroad range of different techniques for improving the robustness of both speech recognition and multimodal understanding components.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 113, "end_pos": 131, "type": "TASK", "confidence": 0.7828100323677063}]}, {"text": "For automatic speech recognition (ASR), a corpus-driven stochastic language model (SLM) with smoothing can be builtin order to overcome the brittleness of a grammarbased language model.", "labels": [], "entities": [{"text": "automatic speech recognition (ASR)", "start_pos": 4, "end_pos": 38, "type": "TASK", "confidence": 0.8106509248415629}]}, {"text": "However, for multimodal applications there is often very little training data available and collection and annotation of realistic data can be very expensive.", "labels": [], "entities": []}, {"text": "In Section 5, we examine and evaluate various different techniques for rapid prototyping of the language model for the speech recognizer, including transformation of out-of-domain data, grammar sampling, adaptation from wide-coverage grammars, and speech recognition models built on conversational corpora.", "labels": [], "entities": [{"text": "speech recognizer", "start_pos": 119, "end_pos": 136, "type": "TASK", "confidence": 0.7099233567714691}, {"text": "grammar sampling", "start_pos": 186, "end_pos": 202, "type": "TASK", "confidence": 0.6989498138427734}, {"text": "speech recognition", "start_pos": 248, "end_pos": 266, "type": "TASK", "confidence": 0.7304362654685974}]}, {"text": "Although some of the techniques presented have been reported in the literature, we are not aware of work comparing the effectiveness of these techniques on the same domain and using the same data sets.", "labels": [], "entities": []}, {"text": "Furthermore, the techniques are general enough that they can be applied to bootstrap robust gesture recognition models as well.", "labels": [], "entities": [{"text": "bootstrap robust gesture recognition", "start_pos": 75, "end_pos": 111, "type": "TASK", "confidence": 0.6173564717173576}]}, {"text": "The presentation here focuses on speech recognition models, partly due to the greater impact of speech recognition performance compared to gesture recognition performance on the multimodal application described here.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 33, "end_pos": 51, "type": "TASK", "confidence": 0.8285079598426819}]}, {"text": "However, in Section 7 we explore the use of robustness techniques on gesture input.", "labels": [], "entities": []}, {"text": "Although the use of an SLM enables recognition of out-of-grammar utterances, resulting in improved speech recognition accuracy, this may not help overall system performance unless the multimodal understanding component itself is made robust to unexpected inputs.", "labels": [], "entities": [{"text": "recognition of out-of-grammar utterances", "start_pos": 35, "end_pos": 75, "type": "TASK", "confidence": 0.7846861332654953}, {"text": "speech recognition", "start_pos": 99, "end_pos": 117, "type": "TASK", "confidence": 0.6862954199314117}, {"text": "accuracy", "start_pos": 118, "end_pos": 126, "type": "METRIC", "confidence": 0.8582982420921326}]}, {"text": "In Section 6, we describe and evaluate several different techniques for making multimodal understanding more robust.", "labels": [], "entities": []}, {"text": "Given the success of discriminative classification models in related applications such as natural language call routing) and semantic role labeling, we first pursue a purely data-driven approach where the predicate of a multimodal command and its arguments are determined by classifiers trained on an annotated corpus of multimodal data.", "labels": [], "entities": [{"text": "natural language call routing", "start_pos": 90, "end_pos": 119, "type": "TASK", "confidence": 0.6618239432573318}, {"text": "semantic role labeling", "start_pos": 125, "end_pos": 147, "type": "TASK", "confidence": 0.6413894494374593}]}, {"text": "However, given the limited amount of data available, this approach does not provide an improvement over the grammar-based approach.", "labels": [], "entities": []}, {"text": "We next pursue an approach combining grammar and data where robust understanding is viewed as a statistical machine translation problem where out-of-grammar or misrecognized language must be translated to the closest language the system can understand.", "labels": [], "entities": [{"text": "statistical machine translation", "start_pos": 96, "end_pos": 127, "type": "TASK", "confidence": 0.620982309182485}]}, {"text": "This approach provides modest improvement over the grammar-based approach.", "labels": [], "entities": []}, {"text": "Finally we explore an edit-distance approach which combines grammar-based understanding with knowledge derived from the underlying application database.", "labels": [], "entities": []}, {"text": "Essentially, if a string cannot be parsed, we attempt to identify the in-grammar string that it is most similar to, just as in the translation approach.", "labels": [], "entities": []}, {"text": "This is achieved by using a finite-state edit transducer to compose the output of the ASR with the grammar-based multimodal alignment and understanding models.", "labels": [], "entities": [{"text": "ASR", "start_pos": 86, "end_pos": 89, "type": "TASK", "confidence": 0.8079967498779297}]}, {"text": "We have presented these techniques as methods for improving the robustness of the multimodal understanding by processing the speech recognition output.", "labels": [], "entities": [{"text": "speech recognition output", "start_pos": 125, "end_pos": 150, "type": "TASK", "confidence": 0.798712968826294}]}, {"text": "Given the higher chance of error in speech recognition compared to gesture recognition, we focus on processing the speech recognition output to achieve robust multimodal understanding.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 36, "end_pos": 54, "type": "TASK", "confidence": 0.7705249786376953}, {"text": "gesture recognition", "start_pos": 67, "end_pos": 86, "type": "TASK", "confidence": 0.7820264101028442}]}, {"text": "However, these techniques are also equally applicable to gesture recognition output.", "labels": [], "entities": [{"text": "gesture recognition output", "start_pos": 57, "end_pos": 83, "type": "TASK", "confidence": 0.9106118480364481}]}, {"text": "In Section 7, we explore the use of edit techniques on gesture input.", "labels": [], "entities": []}, {"text": "Section 8 concludes and discusses the implications of these results.", "labels": [], "entities": []}], "datasetContent": [{"text": "To determine the baseline performance of the finite-state approach to multimodal integration and understanding, and to collect data for the experiments on multimodal robustness described in this article, we collected and annotated a corpus of multimodal data as described in Section 2.2.", "labels": [], "entities": [{"text": "multimodal integration and understanding", "start_pos": 70, "end_pos": 110, "type": "TASK", "confidence": 0.8388363867998123}]}, {"text": "To enable this initial experiment and data collection, because no corpus data had already been collected, to bootstrap the process we initially used a handcrafted multimodal grammar using grammar templates combined with data from the underlying application database.", "labels": [], "entities": []}, {"text": "As shown in, the multimodal grammar can be used to create language models for ASR, align the speech and gesture results from the respective recognizers, and transform the multimodal utterance to a meaning representation.", "labels": [], "entities": [{"text": "ASR", "start_pos": 78, "end_pos": 81, "type": "TASK", "confidence": 0.990853488445282}]}, {"text": "All these operations are achieved using finite-state transducer operations.", "labels": [], "entities": []}, {"text": "For the 709 inputs that involve speech (491 unimodal speech and 218 multimodal) we calculated the speech recognition accuracy (word and sentence level) for results using the grammar-based language model projected from the multimodal grammar.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 117, "end_pos": 125, "type": "METRIC", "confidence": 0.7875097990036011}]}, {"text": "We also calculated a series of measures of concept accuracy on the meaning representations resulting from taking the results from speech recognition and combining them with the gesture lattice using the gesture speech alignment model, and then the multimodal understanding model.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 51, "end_pos": 59, "type": "METRIC", "confidence": 0.972913920879364}, {"text": "speech recognition", "start_pos": 130, "end_pos": 148, "type": "TASK", "confidence": 0.7308276295661926}, {"text": "gesture speech alignment", "start_pos": 203, "end_pos": 227, "type": "TASK", "confidence": 0.6029071907202402}]}, {"text": "The concept accuracy measures: Concept Sentence Accuracy, Predicate Sentence Accuracy, and Argument Sentence Accuracy are explained subsequently.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 12, "end_pos": 20, "type": "METRIC", "confidence": 0.9872905611991882}, {"text": "Predicate Sentence Accuracy", "start_pos": 58, "end_pos": 85, "type": "METRIC", "confidence": 0.784810205300649}, {"text": "Argument Sentence Accuracy", "start_pos": 91, "end_pos": 117, "type": "METRIC", "confidence": 0.834675133228302}]}, {"text": "The hierarchically-nested XML representation described in Section 3.1 is effective for processing by the backend application, but is not well suited for the automated determination of the performance of the language understanding mechanism.", "labels": [], "entities": []}, {"text": "We developed an approach, similar to Ciaramella (1993) and, in which the meaning representation, in our case XML, is transformed into a sorted flat list of attribute-value pairs indicating the core contentful concepts of each command.", "labels": [], "entities": []}, {"text": "The attribute-value meaning representation normalizes over multiple different XML representations which correspond to the same underlying meaning.", "labels": [], "entities": []}, {"text": "For example, phone and address and address and phone receive different XML representations but the same attribute-value representation.", "labels": [], "entities": []}, {"text": "For the example phone number of this restaurant, the XML representation is as in, and the corresponding attribute-value representation is as in. cmd:info type:phone object:selection.", "labels": [], "entities": []}, {"text": "(1)  We describe a set of experiments to evaluate the performance of the language model in the MATCH multimodal system.", "labels": [], "entities": []}, {"text": "We use word accuracy and string accuracy for evaluating ASR output.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 12, "end_pos": 20, "type": "METRIC", "confidence": 0.8504202961921692}, {"text": "accuracy", "start_pos": 32, "end_pos": 40, "type": "METRIC", "confidence": 0.6634067893028259}, {"text": "ASR", "start_pos": 56, "end_pos": 59, "type": "TASK", "confidence": 0.9691675305366516}]}, {"text": "All results presented in this section are based on 10-fold crossvalidation experiments run on the 709 spoken and multimodal exchanges collected from the pilot study described in Section 2.2.", "labels": [], "entities": []}, {"text": "presents the performance results for ASR word and sentence accuracy using language models trained on the collected in-domain corpus as well as on corpora derived using the different methods discussed in Sections 5.2-5.7.", "labels": [], "entities": [{"text": "ASR word and sentence accuracy", "start_pos": 37, "end_pos": 67, "type": "TASK", "confidence": 0.8886994957923889}]}, {"text": "For the class-based models mentioned in the table, we defined different classes based on areas of interest (e.g., riverside park, turtle pond), points of interest (e.g., Ellis Island, United Nations Building), type of cuisine (e.g., Afghani, Indonesian), price categories (e.g., moderately priced, expensive), and neighborhoods (e.g., Upper East Side, Chinatown).", "labels": [], "entities": []}, {"text": "It is immediately apparent that the hand-crafted grammar as a language model performs poorly and a language model trained on the collected domain-specific corpus performs significantly better than models trained on derived data.", "labels": [], "entities": []}, {"text": "However, it is encouraging to note that a model trained on a derived corpus (obtained from combining the migrated out-of-domain corpus and a corpus created by sampling the in-domain grammar) is within 10% word accuracy as compared to the model trained on the collected corpus.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 210, "end_pos": 218, "type": "METRIC", "confidence": 0.8727172613143921}]}, {"text": "There are several other noteworthy observations from these experiments.", "labels": [], "entities": []}, {"text": "The performance of the language model trained on data sampled from the grammar is dramatically better as compared to the performance of the hand-crafted grammar.", "labels": [], "entities": []}, {"text": "This technique provides a promising direction for authoring portable grammars that can be sampled subsequently to build robust language models when no in-domain corpora are available.", "labels": [], "entities": []}, {"text": "Furthermore, combining grammar and in-domain data, as described in Section 5.4, outperforms all other models significantly.", "labels": [], "entities": []}, {"text": "For the experiment on the migration of an out-of-domain corpus, we used a corpus from a software help-desk application.", "labels": [], "entities": []}, {"text": "shows that the migration of data using Performance results for ASR word and sentence accuracy using models trained on data derived from different methods of bootstrapping domain-specific data..", "labels": [], "entities": [{"text": "ASR word and sentence accuracy", "start_pos": 63, "end_pos": 93, "type": "TASK", "confidence": 0.8685124278068542}]}, {"text": "We built a trigram model using a 5.4-million-word Switchboard corpus and investigated the effect of adapting the resulting language model on in-domain untranscribed speech utterances.", "labels": [], "entities": []}, {"text": "The adaptation is done by first running the recognizer on the training partition of the in-domain speech utterances and then building a language model from the recognized text.", "labels": [], "entities": []}, {"text": "We observe that although the performance of the Switchboard language model on the MATCH domain is poorer than the performance of a model obtained by migrating data from a related domain, the performance can be significantly improved using the adaptation technique.", "labels": [], "entities": [{"text": "MATCH domain", "start_pos": 82, "end_pos": 94, "type": "DATASET", "confidence": 0.8528265655040741}]}, {"text": "The last row of shows the results of using the MATCH specific lexicon to generate a corpus using a wide-coverage grammar, training a language model, and adapting the resulting model using in-domain untranscribed speech utterances as was done for the Switchboard model.", "labels": [], "entities": []}, {"text": "The class-based trigram model was built using 500,000 randomly sampled paths from the network constructed by the procedure described in Section 5.7.", "labels": [], "entities": []}, {"text": "It is interesting to note that the performance is very similar to the Switchboard model given that the wide-coverage grammar is not designed for conversational speech unlike models derived from Switchboard data.", "labels": [], "entities": []}, {"text": "The data from the domain has some elements of conversational-style speech which the Switchboard model models well, but it also has syntactic constructions that are adequately modeled by the widecoverage grammar.", "labels": [], "entities": []}, {"text": "In this section, we have presented a range of techniques to build language models for speech recognition which are applicable at different development phases of an application.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 86, "end_pos": 104, "type": "TASK", "confidence": 0.7741899788379669}]}, {"text": "Although the utility of in-domain data cannot be obviated, we have shown that there are ways to approximate this data with a combination of grammar and out-ofdomain data.", "labels": [], "entities": []}, {"text": "These techniques are particularly useful in the initial phases of application development when there is very little in-domain data.", "labels": [], "entities": []}, {"text": "The technique of authoring a domain-specific grammar that is sampled for n-gram model building presents a good trade-off between time-to-create and the robustness of the resulting language model.", "labels": [], "entities": [{"text": "n-gram model building", "start_pos": 73, "end_pos": 94, "type": "TASK", "confidence": 0.7177479068438212}]}, {"text": "This method can be extended by incorporating suitably generalized out-of-domain data, in order to approximate the distribution of n-grams in the in-domain data.", "labels": [], "entities": []}, {"text": "If time to develop is of utmost importance, we have shown that using a large out-of-domain corpus (Switchboard) or a wide-coverage domain-independent grammar can yield a reasonable language model.", "labels": [], "entities": []}, {"text": "Before we present the methods for robust understanding, we discuss the issue of data partitions to evaluate these methods on.", "labels": [], "entities": []}, {"text": "Due to the limited amount of data, we run cross-validation experiments in order to arrive at reliable performance estimates for these methods.", "labels": [], "entities": []}, {"text": "However, we have a choice in terms of how the data is split into training and test partitions for the cross-validation runs.", "labels": [], "entities": []}, {"text": "We could randomly split the data for an n-fold (for example, 10-fold) cross-validation test.", "labels": [], "entities": []}, {"text": "However, the data contain several repeated attempts by users performing the six scenarios.", "labels": [], "entities": []}, {"text": "A random partitioning of these data would inevitably have the same multimodal utterances in training and test partitions.", "labels": [], "entities": []}, {"text": "We believe that this would result in an overly optimistic estimate of the performance.", "labels": [], "entities": []}, {"text": "In order to address this issue, we run 6-fold cross-validation experiments by using five scenarios as the training set and the sixth scenario as the test set.", "labels": [], "entities": []}, {"text": "This way of partitioning the data overly handicaps data-driven methods because the distribution of data in the training and test partitions for each cross-validation run would be significantly different.", "labels": [], "entities": []}, {"text": "In the experiment results for each method, we present 10-fold and 6-fold cross-validation results where appropriate in order to demonstrate the strengths and limitations of each method.", "labels": [], "entities": []}, {"text": "For all the experiments in this section, we used a datadriven language model for ASR.", "labels": [], "entities": [{"text": "ASR", "start_pos": 81, "end_pos": 84, "type": "TASK", "confidence": 0.9580761194229126}]}, {"text": "The word accuracy of the ASR is 73.8%, averaged overall scenarios and all speakers.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 9, "end_pos": 17, "type": "METRIC", "confidence": 0.9696496725082397}, {"text": "ASR", "start_pos": 25, "end_pos": 28, "type": "TASK", "confidence": 0.8799653053283691}]}], "tableCaptions": [{"text": " Table 3  ASR and concept accuracy for the grammar-based finite-state approach (10-fold).", "labels": [], "entities": [{"text": "ASR", "start_pos": 10, "end_pos": 13, "type": "METRIC", "confidence": 0.724532961845398}, {"text": "accuracy", "start_pos": 26, "end_pos": 34, "type": "METRIC", "confidence": 0.974105954170227}]}, {"text": " Table 4  Performance results for ASR word and sentence accuracy using models trained on data derived  from different methods of bootstrapping domain-specific data.", "labels": [], "entities": [{"text": "ASR word and sentence accuracy", "start_pos": 34, "end_pos": 64, "type": "TASK", "confidence": 0.8924290299415588}]}, {"text": " Table 4. We built a trigram model using a 5.4-million-word Switch- board corpus and investigated the effect of adapting the resulting language model on  in-domain untranscribed speech utterances. The adaptation is done by first running  the recognizer on the training partition of the in-domain speech utterances and then  building a language model from the recognized text. We observe that although the  performance of the Switchboard language model on the MATCH domain is poorer than  the performance of a model obtained by migrating data from a related domain, the  performance can be significantly improved using the adaptation technique.", "labels": [], "entities": [{"text": "MATCH domain", "start_pos": 459, "end_pos": 471, "type": "DATASET", "confidence": 0.8990591466426849}]}, {"text": " Table 8  Concept accuracy for different edit models on 6-fold cross-validation experiments using a  data-driven language model for ASR.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 18, "end_pos": 26, "type": "METRIC", "confidence": 0.9981526732444763}, {"text": "ASR", "start_pos": 132, "end_pos": 135, "type": "TASK", "confidence": 0.9328415989875793}]}]}