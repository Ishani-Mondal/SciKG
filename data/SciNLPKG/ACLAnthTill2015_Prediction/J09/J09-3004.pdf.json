{"title": [], "abstractContent": [{"text": "This article presents a novel bootstrapping approach for improving the quality of feature vector weighting in distributional word similarity.", "labels": [], "entities": [{"text": "distributional word similarity", "start_pos": 110, "end_pos": 140, "type": "TASK", "confidence": 0.6266804734865824}]}, {"text": "The method was motivated by attempts to utilize distributional similarity for identifying the concrete semantic relationship of lexical entailment.", "labels": [], "entities": []}, {"text": "Our analysis revealed that a major reason for the rather loose semantic similarity obtained by distributional similarity methods is insufficient quality of the word feature vectors, caused by deficient feature weighting.", "labels": [], "entities": []}, {"text": "This observation led to the definition of a bootstrapping scheme which yields improved feature weights, and hence higher quality feature vectors.", "labels": [], "entities": []}, {"text": "The underlying idea of our approach is that features which are common to similar words are also most characteristic for their meanings, and thus should be promoted.", "labels": [], "entities": []}, {"text": "This idea is realized via a bootstrapping step applied to an initial standard approximation of the similarity space.", "labels": [], "entities": []}, {"text": "The superior performance of the bootstrapping method was assessed in two different experiments, one based on direct human gold-standard annotation and the other based on an automatically created disambiguation dataset.", "labels": [], "entities": []}, {"text": "These results are further supported by applying a novel quantitative measurement of the quality of feature weighting functions.", "labels": [], "entities": []}, {"text": "Improved feature weighting also allows massive feature reduction, which indicates that the most characteristic features fora word are indeed concentrated at the top ranks of its vector.", "labels": [], "entities": []}, {"text": "Finally, experiments with three prominent similarity measures and two feature weighting functions showed that the bootstrapping scheme is robust and is independent of the original functions over which it is applied.", "labels": [], "entities": []}], "introductionContent": [], "datasetContent": [{"text": "To test the effectiveness of the bootstrapped weighting scheme, we first evaluated whether it contributes to better prediction of lexical entailment.", "labels": [], "entities": [{"text": "prediction of lexical entailment", "start_pos": 116, "end_pos": 148, "type": "TASK", "confidence": 0.7224294245243073}]}, {"text": "This evaluation was based on gold-standard annotations determined by human judgments of the substitutable lexical entailment relation, as defined in Section 3.", "labels": [], "entities": []}, {"text": "The new similarity scheme, sim B , based on the bootstrapped weights, was first computed using the standard LIN method as the initial similarity measure.", "labels": [], "entities": []}, {"text": "The resulting similarity lists of sim LIN (the original LIN method) and sim B LIN (Bootstrapped LIN) schemes were evaluated fora sample of nouns (Section 5.2).", "labels": [], "entities": []}, {"text": "Then, the evaluation was extended (Section 5.3) to apply the bootstrapping scheme over the two additional similarity measures that were presented in Section 2.2, sim WJ (weighted Jaccard) and sim COS (Cosine).", "labels": [], "entities": []}, {"text": "Along with these lexical entailment evaluations we also analyzed directly the quality of the bootstrapped feature vectors, according to the average common-feature rank ratio measure, which was defined in Section 6.", "labels": [], "entities": []}, {"text": "Our experiments were conducted using statistics from an 18 million token subset of the Reuters RCV1 corpus (known as Reuters Corpus, Volume 1, English Language, 1996-08-20 to 1997-08-19), parsed by Lin's Minipar dependency parser (Lin 1993).", "labels": [], "entities": [{"text": "Reuters RCV1 corpus", "start_pos": 87, "end_pos": 106, "type": "DATASET", "confidence": 0.971915582815806}, {"text": "Reuters Corpus, Volume 1, English Language, 1996-08-20", "start_pos": 117, "end_pos": 171, "type": "DATASET", "confidence": 0.9168717086315155}]}, {"text": "The test set of candidate word similarity pairs was constructed fora sample of 30 randomly selected nouns whose corpus frequency exceeds 500.", "labels": [], "entities": []}, {"text": "In our primary experiment we computed the top 40 most similar words for each noun by the sim LIN and by sim B LIN measures, yielding 1,200 pairs for each method, and 2,400 pairs altogether.", "labels": [], "entities": []}, {"text": "About 800 of these pairs were common for the two methods, therefore leaving approximately 1,600 distinct candidate word similarity pairs.", "labels": [], "entities": []}, {"text": "Because the lexical entailment relation is directional, each candidate pair was duplicated to create two directional pairs, yielding a test set of 3,200 pairs.", "labels": [], "entities": []}, {"text": "Thus, for each pair of words, wand v, the two ordered pairs (w, v) and (v, w) were created to be judged separately for entailment in the specified direction (whether the first word entails the other).", "labels": [], "entities": []}, {"text": "Consequently, a non-directional candidate similarity pair w, v is considered as a correct entailment if it was assessed as an entailing pair at least in one direction.", "labels": [], "entities": []}, {"text": "The assessors were only provided with a list of word pairs without any contextual information and could consult any available dictionary, WordNet, and the Web.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 138, "end_pos": 145, "type": "DATASET", "confidence": 0.9763398766517639}]}, {"text": "The judgment criterion follows the criterion presented in Section 3.", "labels": [], "entities": []}, {"text": "In particular, the judges were asked to apply the two operational conditions, existence and substitutability in context, to each given pair.", "labels": [], "entities": []}, {"text": "Prior to performing the final test of the annotation experiment, the judges were presented with an annotated set of entailing and non-entailing pairs along with the existential statements and sample sentences for substitution, demonstrating how the two conditions could be applied in different cases of entailment.", "labels": [], "entities": []}, {"text": "In addition, they had to judge a training set of several dozen pairs and then discuss their judgment decisions with each other to gain a better understanding of the two criteria.", "labels": [], "entities": []}, {"text": "The following example illustrates this process.", "labels": [], "entities": []}, {"text": "Given a non-directional pair {company, organization} two directional pairs are created: (company, organization) and (organization, company).", "labels": [], "entities": []}, {"text": "The former pair is judged as a correct entailment: the existence of a company entails the existence of an organization, and the meaning of the sentence: John works fora large company entails the meaning of the sentence with substitution: John works fora large organization.", "labels": [], "entities": []}, {"text": "Hence, company lexically entails organization, but not vice versa (as shown in Section 3.3), therefore the second pair is judged as not entailing.", "labels": [], "entities": []}, {"text": "Eventually, the non-directional pair {company, organization} is considered as a correct entailment.", "labels": [], "entities": []}, {"text": "Finally, the test set of 3,200 pairs was split into three disjoint subsets that were judged by three native English speaking assessors, each of whom possessed a Bachelors degree in English Linguistics.", "labels": [], "entities": []}, {"text": "For each subset a different pair of assessors was assigned, each person judging the entire subset.", "labels": [], "entities": []}, {"text": "The judges were grouped into three different pairs (i.e., JudgeI+JudgeII, JudgeII+JudgeIII, and JudgeI+JudgeIII).", "labels": [], "entities": []}, {"text": "Each pair was assigned initially to judge all the word similarities in each subset, and the third assessor was employed in cases of disagreement between the first two.", "labels": [], "entities": []}, {"text": "The majority vote was taken as the final decision.", "labels": [], "entities": []}, {"text": "Hence, each assessor had to fully annotate two thirds of the data and fora third subset she only had to judge the pairs for which there was disagreement between the other two judges.", "labels": [], "entities": []}, {"text": "This was done in order to measure the agreement achieved for different pairs of annotators.", "labels": [], "entities": []}, {"text": "The output pairs from both methods were mixed so the assessors could not associate a pair with the method that proposed it.", "labels": [], "entities": []}, {"text": "We note that this evaluation methodology, in which human assessors judge the correctness of candidate pairs by some semantic substitutability criterion, is similar to common evaluation methodologies used for paraphrase acquisition ().", "labels": [], "entities": [{"text": "paraphrase acquisition", "start_pos": 208, "end_pos": 230, "type": "TASK", "confidence": 0.8829550445079803}]}, {"text": "Measuring human agreement level for this task, the proportions of matching decisions were 93.5% between Judge I and Judge II, 90% for Judge I and Judge III, and 91.2% for Judge II and Judge III.", "labels": [], "entities": []}, {"text": "The corresponding kappa values are 0.83, 0.80, and 0.80, which is regarded as \"very good agreement\".", "labels": [], "entities": [{"text": "agreement", "start_pos": 89, "end_pos": 98, "type": "METRIC", "confidence": 0.9773061871528625}]}, {"text": "It is interesting to note that after some discussion most of the disagreements were settled, and the few remaining mismatches were due to different understandings of word meanings.", "labels": [], "entities": []}, {"text": "These findings seem to have a similar flavor to the human agreement findings reported for the Recognizing Textual Entailment challenges (), in which entailment was judged for pairs of sentences.", "labels": [], "entities": [{"text": "Recognizing Textual Entailment challenges", "start_pos": 94, "end_pos": 135, "type": "TASK", "confidence": 0.7170232832431793}]}, {"text": "In fact, the kappa values obtained in our evaluation are substantially higher than reported for sentencelevel textual entailment, which suggests that it is easier to make entailment judgments at the lexical level than at the full sentence level.", "labels": [], "entities": [{"text": "sentencelevel textual entailment", "start_pos": 96, "end_pos": 128, "type": "TASK", "confidence": 0.5791199704011282}]}, {"text": "The parameter values of the algorithms were tuned using a development set of similarity pairs generated for 10 additional nouns, distinct from the 30 nouns used for the test set.", "labels": [], "entities": []}, {"text": "The parameters were optimized by running the algorithm systematically with various values across the parameter scales and judging a sample subset of the results.", "labels": [], "entities": []}, {"text": "weight MI = 4 was found as the optimal MI threshold for active feature weights (features included in the feature vectors), yielding a 10% precision increase of sim LIN and removing over 50% of the data relative to no feature filtering.", "labels": [], "entities": [{"text": "weight MI", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.7168681025505066}, {"text": "MI threshold", "start_pos": 39, "end_pos": 51, "type": "METRIC", "confidence": 0.9524061381816864}, {"text": "precision", "start_pos": 138, "end_pos": 147, "type": "METRIC", "confidence": 0.9990899562835693}]}, {"text": "Accordingly, this value also serves as the \u03b8 weight threshold in the bootstrapping scheme (Section 4).", "labels": [], "entities": []}, {"text": "As for the \u03b8 sim parameter, the best results on the development set were obtained for \u03b8 sim = 0.04, \u03b8 sim = 0.02, and \u03b8 sim = 0.01 when bootstrapping over the initial similarity measures LIN, WJ, and COS, respectively.", "labels": [], "entities": [{"text": "LIN", "start_pos": 187, "end_pos": 190, "type": "METRIC", "confidence": 0.9722200036048889}, {"text": "COS", "start_pos": 200, "end_pos": 203, "type": "METRIC", "confidence": 0.9950838685035706}]}, {"text": "We measured the contribution of the improved feature vectors to the resulting precision of sim LIN and sim B LIN in predicting lexical entailment.", "labels": [], "entities": [{"text": "precision", "start_pos": 78, "end_pos": 87, "type": "METRIC", "confidence": 0.9995629191398621}, {"text": "predicting lexical entailment", "start_pos": 116, "end_pos": 145, "type": "TASK", "confidence": 0.9013818303743998}]}, {"text": "The results are presented in, where precision and error reduction values were computed for the top 20, 30, and 40 word similarity pairs produced by each method.", "labels": [], "entities": [{"text": "precision", "start_pos": 36, "end_pos": 45, "type": "METRIC", "confidence": 0.9991611242294312}, {"text": "error reduction", "start_pos": 50, "end_pos": 65, "type": "METRIC", "confidence": 0.9267765879631042}]}, {"text": "It can be seen that the Bootstrapped LIN method outperformed the original LIN approach by 6-9 precision points at all top-n levels.", "labels": [], "entities": [{"text": "precision", "start_pos": 94, "end_pos": 103, "type": "METRIC", "confidence": 0.9892041087150574}]}, {"text": "As expected, the precision for the shorter top 20 list is higher for both methods, thus leaving a bit less room for improvement.", "labels": [], "entities": [{"text": "precision", "start_pos": 17, "end_pos": 26, "type": "METRIC", "confidence": 0.9995781779289246}]}, {"text": "Overall, the Bootstrapped LIN method extracted 104 (21%) more correct similarity pairs than the other measure and reduced the number of errors by almost 15%.", "labels": [], "entities": [{"text": "Bootstrapped LIN", "start_pos": 13, "end_pos": 29, "type": "METRIC", "confidence": 0.6224054098129272}, {"text": "number of errors", "start_pos": 126, "end_pos": 142, "type": "METRIC", "confidence": 0.8087830146153768}]}, {"text": "We also computed the relative recall, which shows the percentage of correct word similarities found by each method relative to the joint set of similarities that were extracted by both methods.", "labels": [], "entities": [{"text": "recall", "start_pos": 30, "end_pos": 36, "type": "METRIC", "confidence": 0.9682009220123291}]}, {"text": "The overall relative recall of the Bootstrapped LIN was quite high (94%), exceeding LIN's relative recall (of 78%) by 16 percentage points.", "labels": [], "entities": [{"text": "recall", "start_pos": 21, "end_pos": 27, "type": "METRIC", "confidence": 0.9175753593444824}, {"text": "Bootstrapped LIN", "start_pos": 35, "end_pos": 51, "type": "DATASET", "confidence": 0.9021726548671722}, {"text": "recall", "start_pos": 99, "end_pos": 105, "type": "METRIC", "confidence": 0.8782730102539062}]}, {"text": "We found that the bootstrapped method covers over 90% of the correct similarities learned by the original method, while also identifying many additional correct pairs.", "labels": [], "entities": []}, {"text": "It should be noted at this point that the current limited precision levels are determined not just by the quality of the feature vectors but significantly by the nature of the vector comparison measure itself (i.e., the LIN formula, as well weighted Jaccard and Cosine as reported in Section 5.3).", "labels": [], "entities": [{"text": "precision", "start_pos": 58, "end_pos": 67, "type": "METRIC", "confidence": 0.9909878373146057}, {"text": "LIN", "start_pos": 220, "end_pos": 223, "type": "METRIC", "confidence": 0.9574200510978699}]}, {"text": "It was observed in other work) that these common types of vector comparison schemes exhibit certain flaws in predicting lexical entailment.", "labels": [], "entities": [{"text": "predicting lexical entailment", "start_pos": 109, "end_pos": 138, "type": "TASK", "confidence": 0.8695909182230631}]}, {"text": "Our present work thus shows that the bootstrapping method yields a significant improvement in feature vector quality, but future research is needed to investigate improved vector comparison schemes.", "labels": [], "entities": []}, {"text": "An additional indication of the improved vector quality is the massive feature reduction allowed by having the most characteristic features concentrated at the top ranks of the vectors.", "labels": [], "entities": []}, {"text": "The vectors of active features of LIN, as constructed after standard feature filtering (Section 5.1), could be further reduced by the bootstrapped weighting to about one third of their size.", "labels": [], "entities": []}, {"text": "As illustrated in, changing the vector size significantly affects the similarity results.", "labels": [], "entities": [{"text": "similarity", "start_pos": 70, "end_pos": 80, "type": "METRIC", "confidence": 0.9457634687423706}]}, {"text": "In sim B LIN the best result was obtained with the top 100 features per word, while using less than 100 or more than 150 features caused a 5-10% decrease in performance.", "labels": [], "entities": []}, {"text": "On the other hand, an attempt to cutoff the lower ranked features of the MI weighting always resulted in a noticeable decrease in precision.", "labels": [], "entities": [{"text": "precision", "start_pos": 130, "end_pos": 139, "type": "METRIC", "confidence": 0.999011754989624}]}, {"text": "These results show that for MI weighting many important features appear further down in the ranked vectors, while for the bootstrapped weighting adding too many features adds mostly noise, since most characteristic features are concentrated at the top ranks.", "labels": [], "entities": [{"text": "MI weighting", "start_pos": 28, "end_pos": 40, "type": "TASK", "confidence": 0.939967006444931}]}, {"text": "Thus, in addition to better feature weighting, the bootstrapping step provides effective feature reduction, which improves vector quality and consequently the similarity results.", "labels": [], "entities": []}, {"text": "We note that the optimal vector size we obtained conforms to previous resultsfor example, by,, and-who also used reduced vectors of up to 100 features as optimal for learning hyponymy and synonymy, respectively.", "labels": [], "entities": []}, {"text": "In Widdows the known SVD method for dimension reduction of LSA-based vectors is applied, whereas in Curran, and Curran and Moens, only the strongly associated verbs (direct and indirect objects of the noun) are selected as \"canonical features\" that are expected to be shared by true synonyms.", "labels": [], "entities": []}, {"text": "Finally, we tried executing an additional bootstrapping iteration of weight B calculation over the similarity results of sim B LIN . The resulting increase in precision was much smaller, of about 2%, showing that most of the potential benefit is exploited in the first bootstrapping iteration (which is not uncommon for natural language data).", "labels": [], "entities": [{"text": "precision", "start_pos": 159, "end_pos": 168, "type": "METRIC", "confidence": 0.9993005990982056}]}, {"text": "On the other hand, computing the bootstrapping weight twice increases computation time significantly, which led us to suggest a single bootstrapping iteration as a reasonable cost-effectiveness tradeoff for our data.", "labels": [], "entities": []}, {"text": "To further validate the behavior of the bootstrapping scheme we experimented with two additional similarity measures, weighted Jaccard (sim WJ ) and Cosine (sim COS ) (described in Section 2.2).", "labels": [], "entities": [{"text": "Cosine (sim COS )", "start_pos": 149, "end_pos": 166, "type": "METRIC", "confidence": 0.845073652267456}]}, {"text": "For each of the additional measures the experiment repeats the main three steps described in Section 4: Initially, the basic similarity lists are calculated for each of the measures using MI weighting; then, the bootstrapped weighting, weight B , is computed based on the initial similarities, yielding new word feature vectors; finally, the similarity values are recomputed by the same vector similarity measure using the new feature vectors.", "labels": [], "entities": []}, {"text": "To assess the effectiveness of weight B we computed the four alternative output similarity lists, using the sim WJ and sim COS similarity measures, each with the weight MI Comparative precision values for the top 20 similarity lists of the three selected similarity measures, with MI and Bootstrapped feature weighting for each.", "labels": [], "entities": [{"text": "COS similarity", "start_pos": 123, "end_pos": 137, "type": "METRIC", "confidence": 0.9127268493175507}, {"text": "precision", "start_pos": 184, "end_pos": 193, "type": "METRIC", "confidence": 0.9456782937049866}, {"text": "MI", "start_pos": 281, "end_pos": 283, "type": "METRIC", "confidence": 0.9943040013313293}, {"text": "Bootstrapped feature weighting", "start_pos": 288, "end_pos": 318, "type": "METRIC", "confidence": 0.9252617160479227}]}, {"text": "The lexical entailment evaluation reported herein corresponds to the lexical substitution application of distributional similarity.", "labels": [], "entities": []}, {"text": "The other type of application, as reviewed in the Introduction, is similarity-based prediction of word co-occurrence likelihood, needed for disambiguation applications.", "labels": [], "entities": [{"text": "similarity-based prediction of word co-occurrence likelihood", "start_pos": 67, "end_pos": 127, "type": "TASK", "confidence": 0.6581116418043772}]}, {"text": "Comparative evaluations of distributional similarity methods for this type of application were commonly conducted using a pseudo-word sense disambiguation scheme, which is replicated here.", "labels": [], "entities": []}, {"text": "In the next subsections we first describe how distributional similarity can help improve word sense disambiguation (WSD).", "labels": [], "entities": [{"text": "word sense disambiguation (WSD)", "start_pos": 89, "end_pos": 120, "type": "TASK", "confidence": 0.7269164472818375}]}, {"text": "Then we describe how the pseudo-word sense disambiguation task, which", "labels": [], "entities": [{"text": "pseudo-word sense disambiguation task", "start_pos": 25, "end_pos": 62, "type": "TASK", "confidence": 0.7728292718529701}]}], "tableCaptions": [{"text": " Table 3  The top 10 ranked features for country produced by MI, the weighting function employed in the  LIN method.", "labels": [], "entities": [{"text": "MI", "start_pos": 61, "end_pos": 63, "type": "DATASET", "confidence": 0.6799361109733582}]}, {"text": " Table 4  Lexical entailment precision values for top-n similar words by the Bootstrapped LIN and the  original LIN method.", "labels": [], "entities": [{"text": "precision", "start_pos": 29, "end_pos": 38, "type": "METRIC", "confidence": 0.9232316017150879}, {"text": "Bootstrapped LIN", "start_pos": 77, "end_pos": 93, "type": "DATASET", "confidence": 0.8089281618595123}]}, {"text": " Table 7  LIN (MI) weighting: The top 10 common features for country-state and country-party, along with  their corresponding ranks in each of the two feature vectors. The features are sorted by the sum  of their feature weights with both words.", "labels": [], "entities": [{"text": "LIN (MI)", "start_pos": 10, "end_pos": 18, "type": "METRIC", "confidence": 0.810455247759819}]}, {"text": " Table 8  Top 10 features of country by the Bootstrapped feature weighting.", "labels": [], "entities": []}]}