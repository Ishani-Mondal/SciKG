{"title": [{"text": "Squibs From Annotator Agreement to Noise Models", "labels": [], "entities": []}], "abstractContent": [{"text": "This article discusses the transition from annotated data to a gold standard, that is, a subset that is sufficiently noise-free with high confidence.", "labels": [], "entities": []}, {"text": "Unless appropriately reinterpreted, agreement coefficients do not indicate the quality of the data set as a benchmarking resource: High overall agreement is neither sufficient nor necessary to distill some amount of highly reliable data from the annotated material.", "labels": [], "entities": [{"text": "agreement", "start_pos": 144, "end_pos": 153, "type": "METRIC", "confidence": 0.9452217817306519}]}, {"text": "A mathematical framework is developed that allows estimation of the noise level of the agreed subset of annotated data, which helps promote cautious benchmarking.", "labels": [], "entities": []}], "introductionContent": [{"text": "By and large, the reason a computational linguist engages in an annotation project is to build a reliable data set for the eventual testing, and possibly training, of an algorithm performing the task.", "labels": [], "entities": []}, {"text": "Hence, the crucial question regarding the annotated data set is whether it is good for benchmarking.", "labels": [], "entities": []}, {"text": "For classification tasks, the current practice is to infer this information from the value of an inter-annotator agreement coefficient such as the \u03ba statistic.", "labels": [], "entities": [{"text": "classification tasks", "start_pos": 4, "end_pos": 24, "type": "TASK", "confidence": 0.9348677396774292}]}, {"text": "If agreement is high, the whole of the data set is good for training and testing; the remaining disagreements are typically adjudicated by an expert) or through discussion), or, in case of more than two annotators, the majority label is chosen (Vieira and Poesio 2000).", "labels": [], "entities": []}, {"text": "There are some studies where cases of disagreement were removed from test data).", "labels": [], "entities": []}, {"text": "If agreement is low, the whole data set is discarded as unreliable.", "labels": [], "entities": [{"text": "agreement", "start_pos": 3, "end_pos": 12, "type": "METRIC", "confidence": 0.9756316542625427}]}, {"text": "The threshold of acceptability seems to have stabilized around \u03ba = 0.67.", "labels": [], "entities": []}, {"text": "There is little understanding, however, of exactly how and how well the value of \u03ba reflects the quality of the data for benchmarking purposes.", "labels": [], "entities": []}, {"text": "We develop a model of annotation generation that allows estimation of the level of noise in a specially constructed gold standard.", "labels": [], "entities": [{"text": "annotation generation", "start_pos": 22, "end_pos": 43, "type": "TASK", "confidence": 0.7648616135120392}]}, {"text": "A gold standard with a noise figure supports cautious benchmarking, by requiring that the performance of an algorithm be better than baseline by more than that which can be attributed to noise.", "labels": [], "entities": []}, {"text": "Articulating an annotation generation model also allows us to shed light on the information \u03ba can contribute to benchmarking.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": []}