{"title": [{"text": "Kernel Methods for Minimally Supervised WSD", "labels": [], "entities": [{"text": "WSD", "start_pos": 40, "end_pos": 43, "type": "TASK", "confidence": 0.7256066203117371}]}], "abstractContent": [{"text": "Fondazione Bruno Kessler-IRST We present a semi-supervised technique for word sense disambiguation that exploits external knowledge acquired in an unsupervised manner.", "labels": [], "entities": [{"text": "word sense disambiguation", "start_pos": 73, "end_pos": 98, "type": "TASK", "confidence": 0.7870765924453735}]}, {"text": "In particular, we use a combination of basic kernel functions to independently estimate syntagmatic and domain similarity, building a set of word-expert classifiers that share a common domain model acquired from a large corpus of un-labeled data.", "labels": [], "entities": []}, {"text": "The results show that the proposed approach achieves state-of-the-art performance on a wide range of lexical sample tasks and on the English all-words task of Senseval-3, although it uses a considerably smaller number of training examples than other methods.", "labels": [], "entities": []}], "introductionContent": [{"text": "A significant challenge in many natural language processing tasks is to reduce the need for labeled training data while maintaining an acceptable performance.", "labels": [], "entities": [{"text": "natural language processing tasks", "start_pos": 32, "end_pos": 65, "type": "TASK", "confidence": 0.7017348930239677}]}, {"text": "This is especially true for word sense disambiguation (WSD) because when moving from the somewhat artificial lexical-sample task to the more realistic all-words task it is practically impossible to collect a large number of training examples for each word sense.", "labels": [], "entities": [{"text": "word sense disambiguation (WSD)", "start_pos": 28, "end_pos": 59, "type": "TASK", "confidence": 0.8068306148052216}]}, {"text": "Thus, many supervised approaches, explicitly designed for the lexical-sample task, cannot be applied to the all-words task, even though they exhibit excellent performance.", "labels": [], "entities": []}, {"text": "This has led to the somewhat paradoxical situation in which completely different methods have been developed for the two tasks, although they represent two sides of the same coin.", "labels": [], "entities": []}, {"text": "To address this problem, in recent work we presented a semi-supervised approach based on kernel methods for WSD).", "labels": [], "entities": []}, {"text": "In particular, we explored the following research directions: (1) independently modeling domain and syntagmatic aspects of sense distinction to improve feature representativeness; and (2) exploiting external knowledge acquired from unlabeled data, with the purpose of drastically reducing the amount of labeled training data.", "labels": [], "entities": [{"text": "sense distinction", "start_pos": 123, "end_pos": 140, "type": "TASK", "confidence": 0.7342447340488434}]}, {"text": "The first direction is based on the linguistic assumption that syntagmatic and domain (associative) relations are crucial for representing sense distinctions, but they are originated by different phenomena.", "labels": [], "entities": []}, {"text": "Regarding the second direction, one can hope to obtain a more accurate prediction", "labels": [], "entities": []}], "datasetContent": [{"text": "Experiments were carried out on various tasks of.", "labels": [], "entities": []}, {"text": "First of all, we conducted a preliminary set of experiments on the Catalan, English, Italian, and Spanish lexical-sample tasks; the results are shown in Section 4.1.", "labels": [], "entities": []}, {"text": "Second, in order to show the general applicability of the proposed method, we evaluated the system on the English all-words task; the results are presented in Section 4.2.", "labels": [], "entities": []}, {"text": "All the experiments were performed using the SVM package (Chang and Lin 2001) customized to embed our own kernels.", "labels": [], "entities": []}, {"text": "The parameters were optimized by five-fold cross-validation on the training set.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1  An example of a domain matrix.", "labels": [], "entities": []}, {"text": " Table 2  Description of the lexical-sample tasks of Senseval-3.", "labels": [], "entities": []}, {"text": " Table 3  The performance (F1) of the basic and composite kernels on the Catalan, English, Italian, and  Spanish lexical-sample tasks of Semeval-3.", "labels": [], "entities": [{"text": "F1", "start_pos": 27, "end_pos": 29, "type": "METRIC", "confidence": 0.9987162351608276}]}, {"text": " Table 4  Performance (F1) of the syntagmatic kernel for the Catalan, English, Italian, and Spanish  lexical-sample tasks of Semeval-3.", "labels": [], "entities": [{"text": "F1", "start_pos": 23, "end_pos": 25, "type": "METRIC", "confidence": 0.990601658821106}]}, {"text": " Table 5  Comparative evaluation on the lexical sample tasks.", "labels": [], "entities": []}, {"text": " Table 7  The performance (F1) of\u02c6Kof\u02c6 of\u02c6K sem  wsd at different ranges of polysemy. Most Frequent baseline (MF) is  also reported.", "labels": [], "entities": [{"text": "F1", "start_pos": 27, "end_pos": 29, "type": "METRIC", "confidence": 0.997132420539856}, {"text": "Frequent baseline (MF)", "start_pos": 91, "end_pos": 113, "type": "METRIC", "confidence": 0.974651837348938}]}, {"text": " Table 8  The performance (F1) of\u02c6Kof\u02c6 of\u02c6K sem  wsd on words with a given number of training examples. Most  Frequent baseline (MF) and mean polysemy for each partition are also reported.", "labels": [], "entities": [{"text": "F1", "start_pos": 27, "end_pos": 29, "type": "METRIC", "confidence": 0.9968793392181396}, {"text": "Frequent baseline (MF)", "start_pos": 110, "end_pos": 132, "type": "METRIC", "confidence": 0.9604051828384399}]}]}