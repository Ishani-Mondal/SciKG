{"title": [], "abstractContent": [{"text": "Universitat Jaume I Current machine translation (MT) systems are still not perfect.", "labels": [], "entities": [{"text": "Universitat Jaume I", "start_pos": 0, "end_pos": 19, "type": "DATASET", "confidence": 0.8077463308970133}, {"text": "machine translation (MT)", "start_pos": 28, "end_pos": 52, "type": "TASK", "confidence": 0.8339558243751526}]}, {"text": "In practice, the output from these systems needs to be edited to correct errors.", "labels": [], "entities": []}, {"text": "A way of increasing the productivity of the whole translation process (MT plus human work) is to incorporate the human correction activities within the translation process itself, thereby shifting the MT paradigm to that of computer-assisted translation.", "labels": [], "entities": [{"text": "MT", "start_pos": 201, "end_pos": 203, "type": "TASK", "confidence": 0.9899832010269165}]}, {"text": "This model entails an iterative process in which the human translator activity is included in the loop: In each iteration, a prefix of the translation is validated (accepted or amended) by the human and the system computes its best (or n-best) translation suffix hypothesis to complete this prefix.", "labels": [], "entities": []}, {"text": "A successful framework for MT is the so-called statistical (or pattern recognition) framework.", "labels": [], "entities": [{"text": "MT", "start_pos": 27, "end_pos": 29, "type": "TASK", "confidence": 0.9946842789649963}, {"text": "pattern recognition)", "start_pos": 63, "end_pos": 83, "type": "TASK", "confidence": 0.7992996275424957}]}, {"text": "Interestingly, within this framework, the adaptation of MT systems to the interactive scenario affects mainly the search process, allowing a great reuse of successful techniques and models.", "labels": [], "entities": [{"text": "MT", "start_pos": 56, "end_pos": 58, "type": "TASK", "confidence": 0.9738084673881531}]}, {"text": "In this article, alignment templates, phrase-based models, and stochastic finite-state transducers are used to develop computer-assisted translation systems.", "labels": [], "entities": [{"text": "alignment templates", "start_pos": 17, "end_pos": 36, "type": "TASK", "confidence": 0.9232734441757202}]}, {"text": "These systems were assessed in a European project (TransType2) in two real tasks: The translation of printer manuals; manuals and the translation of the Bulletin of the European Union.", "labels": [], "entities": [{"text": "translation of printer manuals", "start_pos": 86, "end_pos": 116, "type": "TASK", "confidence": 0.8496717363595963}, {"text": "translation of the Bulletin of the European Union", "start_pos": 134, "end_pos": 183, "type": "TASK", "confidence": 0.8035024031996727}]}, {"text": "In each task, the following three pairs of languages were involved (in both translation directions):", "labels": [], "entities": []}], "introductionContent": [], "datasetContent": [{"text": "The models and search procedures introduced in the previous sections were assessed through a series of IPMT experiments with different corpora.", "labels": [], "entities": []}, {"text": "These corpora, along with the corresponding pre-and post-processing and assessment procedures, are presented in this section.", "labels": [], "entities": []}, {"text": "In this section, the translation results obtained using ATs, PBMs, and SFSTs for all six language pairs of the Xerox corpus are reported.", "labels": [], "entities": [{"text": "Xerox corpus", "start_pos": 111, "end_pos": 123, "type": "DATASET", "confidence": 0.9492807984352112}]}, {"text": "Word-based trigram and class-based five-gram target-language models were used for the AT models (the parameters of the log-linear model are tuned so as to minimize WER on a development corpus); wordbased trigram target-language models were used for PBMs and trigrams were used to infer GIATI SFSTs.", "labels": [], "entities": [{"text": "WER", "start_pos": 164, "end_pos": 167, "type": "METRIC", "confidence": 0.9929141998291016}, {"text": "GIATI SFSTs", "start_pos": 286, "end_pos": 297, "type": "TASK", "confidence": 0.5632287263870239}]}, {"text": "MT results with ATs, PBMs, and SFSTs are presented in.", "labels": [], "entities": [{"text": "MT", "start_pos": 0, "end_pos": 2, "type": "TASK", "confidence": 0.9724276065826416}, {"text": "SFSTs", "start_pos": 31, "end_pos": 36, "type": "TASK", "confidence": 0.5246526598930359}]}, {"text": "Results obtained using the PBMs are slightly but consistently better that those achieved using the other models.", "labels": [], "entities": []}, {"text": "In general, the different techniques perform similarly for the various translation directions.", "labels": [], "entities": []}, {"text": "However, the English-Spanish language pair is the one for which the best translations can be produced.", "labels": [], "entities": []}, {"text": "Performance has been measured in terms of KSRs and MARs (KSR and MAR are represented as the lower and upper portions of each bar, respectively, and KSMR is the whole bar length).", "labels": [], "entities": [{"text": "MARs", "start_pos": 51, "end_pos": 55, "type": "METRIC", "confidence": 0.9552364945411682}, {"text": "MAR", "start_pos": 65, "end_pos": 68, "type": "METRIC", "confidence": 0.926621675491333}]}, {"text": "The results are shown in.", "labels": [], "entities": []}, {"text": "According to these results, a human translator assisted by an AT-based or a SFSTbased interactive system would only need an effort equivalent to typing about 20% of the characters in order to produce the correct translations for the Spanish to English task; or even less than 20% if a PBM-based system is used.", "labels": [], "entities": []}, {"text": "For the Xerox task, off-line MT performance and IPMT results show similar tendencies.", "labels": [], "entities": [{"text": "MT", "start_pos": 29, "end_pos": 31, "type": "TASK", "confidence": 0.9478650689125061}]}, {"text": "The PBMs show better performance for both the off-line MT and for the IPMT assessment figures.", "labels": [], "entities": [{"text": "MT", "start_pos": 55, "end_pos": 57, "type": "TASK", "confidence": 0.9531933665275574}, {"text": "IPMT assessment", "start_pos": 70, "end_pos": 85, "type": "DATASET", "confidence": 0.8461646735668182}]}, {"text": "The AT and SFST models perform more or less equivalently.", "labels": [], "entities": []}, {"text": "In both scenarios, the best results were achieved for the Spanish-English language pair followed by French-English and German-English.", "labels": [], "entities": []}, {"text": "The computing times needed by all the systems involved in these experiments were well within the range of the on-line operational requirements.", "labels": [], "entities": []}, {"text": "The average initial time for each source test sentence was very low (less than 50 msec) for PBMs and SFSTs and adequate for ATs (772 msec).", "labels": [], "entities": []}, {"text": "In the case of ATs and SFSTs, this included the time required for the generation of the initial word-graph of each sentence.", "labels": [], "entities": []}, {"text": "Moreover, the most critical times incurred in the successive IPMT iterations were very low in all the cases: 18 msec for ATs, 99 msec for PBMs, and 9 msec for SFSTs.", "labels": [], "entities": []}, {"text": "Note, however, that these average times are not exactly comparable because of the differences in the computer hardware used by each system (2 Ghz AMD, 1.5 Ghz Pentium, and 2.4 Ghz Pentium for ATs, PBMs, and SFSTs, respectively).", "labels": [], "entities": []}, {"text": "The translation results using the AT, PBM, and SFST approaches for all six language pairs of the EU corpus are reported in this section.", "labels": [], "entities": [{"text": "AT", "start_pos": 34, "end_pos": 36, "type": "METRIC", "confidence": 0.849224328994751}, {"text": "EU corpus", "start_pos": 97, "end_pos": 106, "type": "DATASET", "confidence": 0.866399496793747}]}, {"text": "As for the Xerox corpora, in the AT experiments, word-based trigram and class-based five-gram target-language models were used; in the PBM experiments, word-based trigram and class-based five-gram target-language models were also used and five-grams were used to infer GIATI SFSTs.", "labels": [], "entities": [{"text": "GIATI SFSTs", "start_pos": 269, "end_pos": 280, "type": "TASK", "confidence": 0.6321149170398712}]}, {"text": "presents the results obtained using ATs, PBMs, and SFSTs.", "labels": [], "entities": []}, {"text": "Generally speaking, the results are comparable to those obtained on the Xerox corpus with the exception of the English-Spanish language pair, which were better.", "labels": [], "entities": [{"text": "Xerox corpus", "start_pos": 72, "end_pos": 84, "type": "DATASET", "confidence": 0.957638144493103}]}, {"text": "With these corpora, the best results were obtained with the ATs and PBMs for all the pairs and the best translation direction was French-to-English with all the models used.", "labels": [], "entities": [{"text": "ATs", "start_pos": 60, "end_pos": 63, "type": "METRIC", "confidence": 0.9231336712837219}]}, {"text": "shows the performance of the AT, PBM, and SFST systems in terms of KSRs and MARs in a similar way as for the Xerox corpora.", "labels": [], "entities": [{"text": "MARs", "start_pos": 76, "end_pos": 80, "type": "METRIC", "confidence": 0.8089155554771423}]}, {"text": "As in the MT experiments, the results are comparable to those obtained on the Xerox corpus, with the exception of the English-Spanish pair.", "labels": [], "entities": [{"text": "MT", "start_pos": 10, "end_pos": 12, "type": "TASK", "confidence": 0.9215261340141296}, {"text": "Xerox corpus", "start_pos": 78, "end_pos": 90, "type": "DATASET", "confidence": 0.9560612738132477}]}, {"text": "Similarly, as in MT, the best results were obtained for the French-to-English translation direction.", "labels": [], "entities": [{"text": "MT", "start_pos": 17, "end_pos": 19, "type": "TASK", "confidence": 0.853614330291748}]}, {"text": "Although EU is a more open-domain task, the results demonstrate again the potential benefit of computer-assisted translation systems.", "labels": [], "entities": [{"text": "EU", "start_pos": 9, "end_pos": 11, "type": "TASK", "confidence": 0.9424495697021484}]}, {"text": "Using PBMs, a human translator would only need an effort equivalent to typing about 20% of the characters in order to produce the correct translations for French-to-English translation direction, whereas for ATs and SFSTs the effort would be about 30%.", "labels": [], "entities": [{"text": "French-to-English translation direction", "start_pos": 155, "end_pos": 194, "type": "TASK", "confidence": 0.6510693232218424}]}, {"text": "For the other language pairs, the efforts would be about 20-30% and 35% of the characters for PBMs and ATs/SFSTs, respectively.", "labels": [], "entities": [{"text": "PBMs and ATs/SFSTs", "start_pos": 94, "end_pos": 112, "type": "TASK", "confidence": 0.39652071595191957}]}, {"text": "The systemwise correlation between MT and IPMT results on this corpus is not as clear as in the Xerox case.", "labels": [], "entities": [{"text": "MT", "start_pos": 35, "end_pos": 37, "type": "TASK", "confidence": 0.9406205415725708}]}, {"text": "One possible cause is the much larger size of the EU corpus compared to the Xerox corpus.", "labels": [], "entities": [{"text": "EU corpus", "start_pos": 50, "end_pos": 59, "type": "DATASET", "confidence": 0.9628646373748779}, {"text": "Xerox corpus", "start_pos": 76, "end_pos": 88, "type": "DATASET", "confidence": 0.9669949412345886}]}, {"text": "In order to run the EU experiments within reasonable time limits, all the systems have required the use of beam search and/or other  suboptimal pruning techniques, although this was largely unnecessary for the Xerox corpus.", "labels": [], "entities": [{"text": "beam search", "start_pos": 107, "end_pos": 118, "type": "TASK", "confidence": 0.8089001178741455}, {"text": "Xerox corpus", "start_pos": 210, "end_pos": 222, "type": "DATASET", "confidence": 0.9589464366436005}]}, {"text": "Clearly, the pruning effects are different in the off-line (MT) and the on-line (IPMT) search processes and the differences may lead to wide performance variations for the AT, PBM, and SFST approaches.", "labels": [], "entities": []}, {"text": "Nevertheless, as can be seen in, the degradation in system performance due to pruning is generally not too substantial and sufficiently accurate real-time interactive operation could also be achieved in the EU task with the three systems tested.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1  The Xerox corpora. For all the languages, the training/test full-sentence overlap and the rate of  out-of-vocabulary test-set words were less than 10% and 1%, respectively. Trigram models were  used to compute the test word perplexity.", "labels": [], "entities": []}, {"text": " Table 2. The  vocabulary size and the training and test set partitions were obtained in a similar way  as with the Xerox corpora.", "labels": [], "entities": []}, {"text": " Table 3  IPMT results (%) for the Xerox corpus (English-Spanish) using ATs, PBMs, and SFSTs for the  1-best hypothesis and 5-best hypotheses. 95% confidence intervals are shown.", "labels": [], "entities": [{"text": "Xerox corpus", "start_pos": 35, "end_pos": 47, "type": "DATASET", "confidence": 0.945714920759201}]}, {"text": " Table 4  IPMT results (%) for the EU corpus (English-Spanish) using ATs, PBMs, and SFSTs for the 1-best  hypothesis and 5-best hypotheses. 95% confidence intervals are shown.", "labels": [], "entities": [{"text": "EU corpus", "start_pos": 35, "end_pos": 44, "type": "DATASET", "confidence": 0.8578672111034393}, {"text": "ATs", "start_pos": 69, "end_pos": 72, "type": "METRIC", "confidence": 0.9381456971168518}]}]}