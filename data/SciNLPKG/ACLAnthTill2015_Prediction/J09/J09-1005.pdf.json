{"title": [{"text": "Unsupervised Type and Token Identification of Idiomatic Expressions", "labels": [], "entities": [{"text": "Token Identification of Idiomatic Expressions", "start_pos": 22, "end_pos": 67, "type": "TASK", "confidence": 0.7682776868343353}]}], "abstractContent": [{"text": "Idiomatic expressions are plentiful in everyday language, yet they remain mysterious, as it is not clear exactly how people learn and understand them.", "labels": [], "entities": []}, {"text": "They are of special interest to linguists, psycholinguists, and lexicographers, mainly because of their syntactic and semantic idiosyncrasies as well as their unclear lexical status.", "labels": [], "entities": []}, {"text": "Despite a great deal of research on the properties of idioms in the linguistics literature, there is not much agreement on which properties are characteristic of these expressions.", "labels": [], "entities": []}, {"text": "Because of their peculiarities, idiomatic expressions have mostly been overlooked by researchers in computational linguistics.", "labels": [], "entities": []}, {"text": "In this article, we look into the usefulness of some of the identified linguistic properties of idioms for their automatic recognition.", "labels": [], "entities": [{"text": "automatic recognition", "start_pos": 113, "end_pos": 134, "type": "TASK", "confidence": 0.7070239186286926}]}, {"text": "Specifically, we develop statistical measures that each model a specific property of idiomatic expressions by looking at their actual usage patterns in text.", "labels": [], "entities": []}, {"text": "We use these statistical measures in a type-based classification task where we automatically separate idiomatic expressions (expressions with a possible idiomatic interpretation) from similar-on-the-surface literal phrases (for which no idiomatic interpretation is possible).", "labels": [], "entities": [{"text": "type-based classification", "start_pos": 39, "end_pos": 64, "type": "TASK", "confidence": 0.7432331442832947}]}, {"text": "In addition, we use some of the measures in a token identification task where we distinguish idiomatic and literal usages of potentially idiomatic expressions in context.", "labels": [], "entities": [{"text": "token identification task", "start_pos": 46, "end_pos": 71, "type": "TASK", "confidence": 0.8569599390029907}]}], "introductionContent": [{"text": "Idioms form a heterogeneous class, with prototypical examples such as by and large, kick the bucket, and let the cat out of the bag.", "labels": [], "entities": []}, {"text": "It is hard to find a single agreed-upon definition that covers all members of this class, but they are often defined as sequences of words involving some degree of semantic idiosyncrasy or non-compositionality.", "labels": [], "entities": []}, {"text": "That is, an idiom has a different meaning from the simple composition of the meaning of its component words.", "labels": [], "entities": []}, {"text": "Idioms are widely and creatively used by speakers of a language to express ideas cleverly, economically, or implicitly, and thus appear in all languages and in all text genres (.", "labels": [], "entities": []}, {"text": "Many expressions acquire an idiomatic meaning overtime; consequently, new idioms come into existence on a daily basis.", "labels": [], "entities": []}, {"text": "Automatic tools are therefore necessary for assisting lexicographers in keeping lexical resources up to date, as well as for creating and extending computational lexicons for use in natural language processing (NLP) systems.", "labels": [], "entities": []}, {"text": "Though completely frozen idioms, such as by and large, can be represented as words with spaces (, most idioms are syntactically well-formed phrases that allow some variability in expression, such as shoot the breeze and hold fire.", "labels": [], "entities": []}, {"text": "Such idioms allow a varying degree of morphosyntactic flexibility-for example, held fire and hold one's fire allow for an idiomatic reading, whereas typically only a literal interpretation is available for fire was held and held fires.", "labels": [], "entities": []}, {"text": "Clearly, a words-with-spaces approach does notwork for phrasal idioms.", "labels": [], "entities": []}, {"text": "Hence, in addition to requiring NLP tools for recognizing idiomatic expressions (types) to include in a lexicon, methods for determining the allowable and preferred usages (a.k.a. canonical forms) of such expressions are also needed.", "labels": [], "entities": []}, {"text": "Moreover, in many situations, an NLP system will need to distinguish a usage (token) of a potentially idiomatic expression as either idiomatic or literal in order to handle a given sequence of words appropriately.", "labels": [], "entities": []}, {"text": "For example, a machine translation system must translate held fire differently in The army held their fire and The worshippers held the fire up to the idol.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 15, "end_pos": 34, "type": "TASK", "confidence": 0.6867280304431915}]}, {"text": "Previous studies focusing on the automatic identification of idiom types have often recognized the importance of drawing on their linguistic properties, such as their semantic idiosyncrasy or their restricted flexibility, pointed out earlier.", "labels": [], "entities": [{"text": "automatic identification of idiom types", "start_pos": 33, "end_pos": 72, "type": "TASK", "confidence": 0.6988610684871673}]}, {"text": "Some researchers have relied on a manual encoding of idiom-specific knowledge in a lexicon (), whereas others have presented approaches for the automatic acquisition of more general (hence less distinctive) knowledge from corpora.", "labels": [], "entities": []}, {"text": "Recent work that looks into the acquisition of the distinctive properties of idioms has been limited, both in scope and in the evaluation of the methods proposed.", "labels": [], "entities": []}, {"text": "Our goal is to develop unsupervised means for the automatic acquisition of lexical, syntactic, and semantic knowledge about a broadly documented class of idiomatic expressions.", "labels": [], "entities": []}, {"text": "Specifically, we focus on a cross-linguistically prominent class of phrasal idioms which are commonly and productively formed from the combination of a frequent verb and a noun in its direct object position, for example, shoot the breeze, make a face, and push one's luck.", "labels": [], "entities": []}, {"text": "We refer to these as verb+noun idiomatic combinations or VNICs.", "labels": [], "entities": []}, {"text": "We present a comprehensive analysis of the distinctive linguistic properties of phrasal idioms, including VNICs (Section 2), and propose statistical measures that capture each property (Section 3).", "labels": [], "entities": []}, {"text": "We provide a multi-faceted evaluation of the measures (Section 4), showing their effectiveness in the recognition of idiomatic expressions (types)-that is, separating them from similar-on-the-surface literal phrases-as well as their superiority to existing state-of-the-art techniques.", "labels": [], "entities": [{"text": "recognition of idiomatic expressions (types)-", "start_pos": 102, "end_pos": 147, "type": "TASK", "confidence": 0.7932689189910889}]}, {"text": "Drawing on these statistical measures, we also propose an unsupervised method for the automatic acquisition of an idiom's canonical forms (e.g., shoot the breeze as opposed to shoot a breeze), and show that it can successfully accomplish the task (Section 5).", "labels": [], "entities": []}, {"text": "It is possible fora single VNIC to have both idiomatic and non-idiomatic (literal) meanings.", "labels": [], "entities": []}, {"text": "For example, make a face is ambiguous between an idiom, as in The little girl made a funny face at her mother, and a literal combination, as in She made a face on the snowman using a carrot and two buttons.", "labels": [], "entities": []}, {"text": "Despite the common perception that phrases that can be idioms are mainly used in their idiomatic sense, our analysis of 60 idioms has shown otherwise.", "labels": [], "entities": []}, {"text": "We found that close to half of these also have a clear literal meaning; and of those with a literal meaning, on average around 40% of their usages are literal.", "labels": [], "entities": []}, {"text": "Distinguishing token phrases as idiomatic or literal combinations of words is thus essential for NLP tasks, such as semantic parsing and machine translation, which require the identification of multiword semantic units.", "labels": [], "entities": [{"text": "Distinguishing token phrases as idiomatic or literal combinations of words", "start_pos": 0, "end_pos": 74, "type": "TASK", "confidence": 0.7892586648464203}, {"text": "semantic parsing", "start_pos": 116, "end_pos": 132, "type": "TASK", "confidence": 0.7202626764774323}, {"text": "machine translation", "start_pos": 137, "end_pos": 156, "type": "TASK", "confidence": 0.7824977338314056}]}, {"text": "Most recent studies focusing on the identification of idiomatic and non-idiomatic tokens either assume the existence of manually annotated data fora supervised classification), or rely on manually encoded linguistic knowledge about idioms), or even ignore the specific properties of nonliteral language and rely mainly on general purpose methods for the task).", "labels": [], "entities": []}, {"text": "We propose unsupervised methods that rely on automatically acquired knowledge about idiom types to identify their token occurrences as idiomatic or literal (Section 6).", "labels": [], "entities": []}, {"text": "More specifically, we explore the hypothesis that the type-based knowledge we automatically acquire about an idiomatic expression can be used to determine whether an instance of the expression is used literally or idiomatically (token-based knowledge).", "labels": [], "entities": []}, {"text": "Our experimental results show that the performance of the token-based idiom identification methods proposed here is comparable to that of existing supervised techniques (Section 7).", "labels": [], "entities": [{"text": "token-based idiom identification", "start_pos": 58, "end_pos": 90, "type": "TASK", "confidence": 0.6531652013460795}]}], "datasetContent": [{"text": "To evaluate our proposed fixedness measures, we analyze their appropriateness for determining the degree of idiomaticity of a set of experimental expressions (in the form of verb-noun pairs, extracted as described in Section 4.1).", "labels": [], "entities": []}, {"text": "More specifically, we first use each measure to assign scores to the experimental pairs.", "labels": [], "entities": []}, {"text": "We then use the scores assigned by each measure to perform two different tasks, and assess the overall goodness of the measure by looking at its performance in both.", "labels": [], "entities": []}, {"text": "First, we look into the classification performance of each measure by using the scores to separate idiomatic verb-noun pairs from literal ones in a mixed list.", "labels": [], "entities": []}, {"text": "This is done by setting a threshold, here the median score, where all pairs with scores higher than the threshold are labeled as idiomatic and the rest as literal.", "labels": [], "entities": []}, {"text": "For classification, we report accuracy (Acc), as well as the relative error rate reduction (ERR) over a random (chance) baseline, referred to as Rand.", "labels": [], "entities": [{"text": "classification", "start_pos": 4, "end_pos": 18, "type": "TASK", "confidence": 0.9747070670127869}, {"text": "accuracy (Acc)", "start_pos": 30, "end_pos": 44, "type": "METRIC", "confidence": 0.8430486619472504}, {"text": "relative error rate reduction (ERR)", "start_pos": 61, "end_pos": 96, "type": "METRIC", "confidence": 0.9051563739776611}]}, {"text": "Second, we examine the retrieval performance of our fixedness measures by using the scores to rank verb-noun pairs according to their degree of idiomaticity.", "labels": [], "entities": []}, {"text": "For retrieval, we present the precision-recall curves, as well as the interpolated three-point average precision or IAP-that is, the average of the interpolated precisions at the recall levels of 20%, 50%, and 80%.", "labels": [], "entities": [{"text": "precision-recall", "start_pos": 30, "end_pos": 46, "type": "METRIC", "confidence": 0.9986211061477661}, {"text": "interpolated three-point average precision or IAP-that", "start_pos": 70, "end_pos": 124, "type": "METRIC", "confidence": 0.7578866382439932}, {"text": "recall", "start_pos": 179, "end_pos": 185, "type": "METRIC", "confidence": 0.9938766360282898}]}, {"text": "The interpolated average precision and precision-recall curves are commonly used for the evaluation of information retrieval systems (Manning and Sch\u00fctzeSch\u00a8Sch\u00fctze 1999), and reflect the goodness of a measure in placing the relevant items (here, idioms) before the irrelevant ones (here, literals).", "labels": [], "entities": [{"text": "interpolated average precision", "start_pos": 4, "end_pos": 34, "type": "METRIC", "confidence": 0.663311759630839}, {"text": "precision-recall", "start_pos": 39, "end_pos": 55, "type": "METRIC", "confidence": 0.9753088355064392}]}, {"text": "Idioms are often assumed to exhibit collocational behavior to some extent, that is, the components of an idiom are expected to appear together more often than expected by chance.", "labels": [], "entities": []}, {"text": "Hence, some NLP systems have used collocational measures to identify them.", "labels": [], "entities": []}, {"text": "However, as discussed in Section 2, idioms have distinctive syntactic and semantic properties that separate them from simple collocations.", "labels": [], "entities": []}, {"text": "For example, although collocations involve some degree of semantic idiosyncrasy (strong tea vs. ?powerful tea), compared to idioms, they typically have a more transparent meaning, and their syntactic behavior is more similar to that of literal expressions.", "labels": [], "entities": []}, {"text": "We thus expect our fixedness measures that draw on the distinctive linguistic properties of idioms to be more appropriate than measures of collocation for the identification of idioms.", "labels": [], "entities": []}, {"text": "To verify this hypothesis, in both the classification and retrieval tasks, we compare the performance of the fixedness measures with that of two collocation extraction measures: an informed baseline, PMI, and a position-based fixedness measure proposed by, which we refer to as Smadja.", "labels": [], "entities": []}, {"text": "Next, we provide more details on PMI and Smadja.", "labels": [], "entities": [{"text": "PMI", "start_pos": 33, "end_pos": 36, "type": "TASK", "confidence": 0.8755121231079102}, {"text": "Smadja", "start_pos": 41, "end_pos": 47, "type": "TASK", "confidence": 0.8674352169036865}]}, {"text": "PMI is a widely used measure for extracting statistically significant combinations of words or collocations.", "labels": [], "entities": [{"text": "extracting statistically significant combinations of words or collocations", "start_pos": 33, "end_pos": 107, "type": "TASK", "confidence": 0.800945445895195}]}, {"text": "It has also been used for the recognition of idioms, warranting its use as an informed baseline here for comparison.", "labels": [], "entities": [{"text": "recognition of idioms", "start_pos": 30, "end_pos": 51, "type": "TASK", "confidence": 0.8868278662363688}]}, {"text": "As in Equation (1), our calculation of PMI here restricts the counts of the verb-noun pair to the direct object relation.", "labels": [], "entities": [{"text": "PMI", "start_pos": 39, "end_pos": 42, "type": "TASK", "confidence": 0.7919227480888367}]}, {"text": "proposes a collocation extraction method which measures the fixedness of a word sequence (e.g., a verb-noun pair) by examining the relative position of the component words across their occurrences together.", "labels": [], "entities": [{"text": "collocation extraction", "start_pos": 11, "end_pos": 33, "type": "TASK", "confidence": 0.7423828542232513}]}, {"text": "We replicate Smadja's method, where we measure fixedness of a target verb-noun pair as the spread (variance) of the co-occurrence frequency of the verb and the noun over 10 relative positions within a five-word window.", "labels": [], "entities": [{"text": "spread (variance)", "start_pos": 91, "end_pos": 108, "type": "METRIC", "confidence": 0.840807318687439}]}, {"text": "Recall from Section 3.1 that our Fixedness lex measure is intended as an improvement over the non-compositionality measure of Lin (1999).", "labels": [], "entities": []}, {"text": "For the sake of completeness, we also compare the classification performance of our Fixedness lex with that of Lin's (1999) measure, which we refer to as Lin.", "labels": [], "entities": [{"text": "Fixedness lex", "start_pos": 84, "end_pos": 97, "type": "DATASET", "confidence": 0.7307857573032379}]}, {"text": "We first elaborate on the methodological aspects of our experiments in Section 4.1, and then present a discussion of the experimental results in Section 4.2.; to extract verb-noun pairs, along with information on the syntactic patterns they appear in.", "labels": [], "entities": []}, {"text": "We automatically parse the BNC using the Collins parser (Collins 1999), and augment it with information about verb and noun lemmas, automatically generated using WordNet).", "labels": [], "entities": [{"text": "Collins parser (Collins 1999)", "start_pos": 41, "end_pos": 70, "type": "DATASET", "confidence": 0.8174578150113424}, {"text": "WordNet", "start_pos": 162, "end_pos": 169, "type": "DATASET", "confidence": 0.9698656797409058}]}, {"text": "We further process the corpus using TGrep2) in order to extract syntactic dependencies.", "labels": [], "entities": []}, {"text": "For each instance of a transitive verb, we use heuristics to extract the noun phrase (NP) in either the direct object position (if the sentence is active), or the subject position (if the sentence is passive).", "labels": [], "entities": []}, {"text": "We then automatically find the head noun of the extracted NP, its number (singular or plural), and the determiner introducing it.", "labels": [], "entities": []}, {"text": "We select our development and test expressions from verb-noun pairs that involve a member of a predefined list of transitive verbs, referred to as basic verbs.", "labels": [], "entities": []}, {"text": "Basic verbs, in their literal use, refer to states or acts that are central to human experience.", "labels": [], "entities": []}, {"text": "They are thus frequent, highly polysemous, and tend to combine with other words to form idiomatic combinations).", "labels": [], "entities": []}, {"text": "An initial list of such verbs was selected from several linguistic and psycholinguistic studies on basic vocabulary.", "labels": [], "entities": []}, {"text": "We further augmented this initial list with verbs that are semantically related to another 7 PMI has been shown to perform better than or comparable to many other association measures.", "labels": [], "entities": []}, {"text": "In our experiments, we also found that PMI consistently performs better than two other association measures, the Dice coefficient and the log-likelihood measure.", "labels": [], "entities": [{"text": "PMI", "start_pos": 39, "end_pos": 42, "type": "TASK", "confidence": 0.7999446988105774}, {"text": "Dice coefficient", "start_pos": 113, "end_pos": 129, "type": "METRIC", "confidence": 0.9132278859615326}]}, {"text": "Experiments by showed contradicting results for PMI; however, these experiments were performed on small-sized corpora, and on data which contained items with very low frequency.", "labels": [], "entities": [{"text": "PMI", "start_pos": 48, "end_pos": 51, "type": "TASK", "confidence": 0.9818598628044128}]}, {"text": "8 We implement the method as explained in, taking into account the part-of-speech tags of the target component words.", "labels": [], "entities": []}, {"text": "9 We implement the method as explained in Lin (1999), using 95% confidence intervals.", "labels": [], "entities": []}, {"text": "We thus need to ignore variants with frequency lower than 4 for which no confidence interval can be formed.", "labels": [], "entities": []}, {"text": "verb already in the list; for example, lose is added in analogy with find.", "labels": [], "entities": []}, {"text": "Here is the final list of the 28 verbs in alphabetical order: blow, bring, catch, cut, find, get, give, have, hear, hit, hold, keep, kick, lay, lose, make, move, place, pull, push, put, see, set, shoot, smell, take, throw, touch From the corpus, we extract all the verb-noun pairs (lemmas) that contain any of these listed basic verbs, and that appear at least 10 times in the corpus in a direct object relation (irrespective of any intervening determiners or adjectives).", "labels": [], "entities": [{"text": "blow, bring, catch, cut, find, get, give, have, hear, hit, hold, keep, kick, lay, lose, make, move, place, pull, push, put, see, set, shoot, smell, take, throw, touch", "start_pos": 62, "end_pos": 228, "type": "Description", "confidence": 0.8565503358840942}]}, {"text": "From these, we select a subset that are idiomatic, and another subset that are literal, as follows: A verb-noun pair is considered idiomatic if it appears in an idiom listed in a credible dictionary such as the Oxford Dictionary of Current Idiomatic English (ODCIE;, or the Collins COBUILD Idioms Dictionary (CCID; Seaton and Macaulay 2002).", "labels": [], "entities": [{"text": "Oxford Dictionary of Current Idiomatic English (ODCIE", "start_pos": 211, "end_pos": 264, "type": "DATASET", "confidence": 0.8713119402527809}, {"text": "Collins COBUILD Idioms Dictionary (CCID; Seaton and Macaulay 2002)", "start_pos": 274, "end_pos": 340, "type": "DATASET", "confidence": 0.9398980438709259}]}, {"text": "To decide whether a verb-noun pair has appeared in an idiom, we look for all idioms containing the verb and the noun in a direct-object relation, irrespective of any intervening determiners or adjectives, and/or any other arguments.", "labels": [], "entities": []}, {"text": "The pair is considered literal if it involves a physical actor state (i.e., the basic semantics of the verb) and does not appear in any of the mentioned dictionaries as an idiom (or part of an idiom).", "labels": [], "entities": []}, {"text": "From the set of idiomatic pairs, we then randomly pullout 80 development pairs and 100 test pairs, ensuring that we have items of both low and high frequency.", "labels": [], "entities": []}, {"text": "We then double the size of each data set (development and test) by adding equal numbers of literal pairs, with similar frequency distributions.", "labels": [], "entities": []}, {"text": "Some of the idioms corresponding to the experimental idiomatic pairs are: kick the habit, move mountains, lose face, and keep one's word.", "labels": [], "entities": []}, {"text": "Examples of literal pairs include: move carriage, lose ticket, and keep fish.", "labels": [], "entities": []}, {"text": "Development expressions are used in devising the fixedness measures, as well as in determining the values of their parameters as explained in the next subsection.", "labels": [], "entities": []}, {"text": "Test expressions are saved as unseen data for the final evaluation.", "labels": [], "entities": []}, {"text": "We first discuss the overall performance of our proposed unsupervised methods in Section 7.2.1.", "labels": [], "entities": []}, {"text": "Results reported in Section 7.2.1 are on TEST (results on DEV have similar trends, unless noted otherwise).", "labels": [], "entities": [{"text": "TEST", "start_pos": 41, "end_pos": 45, "type": "METRIC", "confidence": 0.8250924944877625}, {"text": "DEV", "start_pos": 58, "end_pos": 61, "type": "DATASET", "confidence": 0.8916571736335754}]}, {"text": "Next, we look into the performance of our methods on expressions with different proportions of idiomatic-to-literal usages in Section 7.2.2, which presents results on TEST and DEV combined, as explained subsequently.", "labels": [], "entities": []}, {"text": "Section 7.2.3 provides an analysis of the errors made because of using canonical forms, and identifies some possible directions for future work.", "labels": [], "entities": []}, {"text": "In Section 7.2.4, we present results on anew data set containing expressions with highly skewed proportion of idiomaticto-literal usages.", "labels": [], "entities": []}, {"text": "shows the macro-averaged accuracy on TEST of our two unsupervised methods, as well as that of the baseline and the supervised method for comparison.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 25, "end_pos": 33, "type": "METRIC", "confidence": 0.9563325643539429}, {"text": "TEST", "start_pos": 37, "end_pos": 41, "type": "METRIC", "confidence": 0.9208042621612549}]}, {"text": "The best unsupervised performance is indicated in boldface.", "labels": [], "entities": []}, {"text": "As the table shows, both of our unsupervised methods as well as the supervised method outperform the baseline, confirming that the canonical forms of an expression, and local context, are both informative in distinguishing literal and idiomatic instances of the expression.", "labels": [], "entities": []}, {"text": "Moreover, CFORM outperforms CONTEXT (difference is marginally significant at p < .06), which is somewhat unexpected, as CONTEXT was proposed as an improvement over CFORM in that it combines contextual information along with the syntactic information provided by CFORM.", "labels": [], "entities": [{"text": "CFORM", "start_pos": 262, "end_pos": 267, "type": "DATASET", "confidence": 0.9428521990776062}]}, {"text": "We return to these results later (Section 7.2.3) to offer some reasons as to why this might be the case.", "labels": [], "entities": []}, {"text": "However, the results using CFORM confirm our hypothesis that canonical forms-which reflect the overall behavior of a verb+noun type-are strongly informative about the class of a token.", "labels": [], "entities": [{"text": "CFORM", "start_pos": 27, "end_pos": 32, "type": "DATASET", "confidence": 0.8995187878608704}]}, {"text": "Importantly, this is the case even though the canonical forms that we use are imperfect knowledge obtained automatically through an unsupervised method.", "labels": [], "entities": []}, {"text": "Comparing CFORM with SUP, we observe that even though on average the latter outperforms the former, the difference is not statistically significant (p > .1).", "labels": [], "entities": [{"text": "CFORM", "start_pos": 10, "end_pos": 15, "type": "DATASET", "confidence": 0.6654380559921265}]}, {"text": "A close look at the performance of these methods on the individual expressions reveals that neither consistently outperforms the other on all (or even most) expressions.", "labels": [], "entities": []}, {"text": "Moreover, as we will see in Section 7.2.2, SUP seems to gain most of its advantage over CFORM on expressions with a low proportion of idiomatic usages, for which canonical forms tend to have less predictive value (see Section 7.2.3 for details).", "labels": [], "entities": [{"text": "SUP", "start_pos": 43, "end_pos": 46, "type": "TASK", "confidence": 0.8982712030410767}]}, {"text": "Recall that both CONTEXT and SUP label each token by comparing its local context to those of its K nearest \"idiomatic\" and its K nearest \"literal\" usages.", "labels": [], "entities": []}, {"text": "The difference is that CONTEXT uses noisy (automatically) labelled data to identify these nearest usages for each token, whereas SUP uses manually labelled data.", "labels": [], "entities": []}, {"text": "One possible direction for future work is thus to investigate whether providing substantially larger amounts of data alleviates the effect of noise, as is often found to be the case by researchers in the field.", "labels": [], "entities": []}, {"text": "To evaluate the performance of our proposed token identification methods, we use each in a classification task, in which the method indicates for each instance of a given expression whether it has an idiomatic or a literal interpretation.", "labels": [], "entities": [{"text": "token identification", "start_pos": 44, "end_pos": 64, "type": "TASK", "confidence": 0.8384671807289124}]}, {"text": "Section 7.1 explains the details of our experimental setup.", "labels": [], "entities": []}, {"text": "Section 7.2 then presents the experimental results as well as some discussion and analysis.", "labels": [], "entities": []}, {"text": "7.1.1 Experimental Expressions and Annotation.", "labels": [], "entities": []}, {"text": "In our token classification experiments, we use a subset of the 180 idiomatic expressions in the development and test data sets used in the type-based experiments of Section 4.", "labels": [], "entities": [{"text": "token classification", "start_pos": 7, "end_pos": 27, "type": "TASK", "confidence": 0.8272075355052948}]}, {"text": "From the original 180 expressions, we discard those whose frequency in the BNC is lower than 20, to increase the likelihood that there are both literal and idiomatic usages of each expression.", "labels": [], "entities": [{"text": "BNC", "start_pos": 75, "end_pos": 78, "type": "DATASET", "confidence": 0.8033279180526733}]}, {"text": "We also discard any expression that is not from the two dictionaries ODCIE and CCID (see Section 4.1.2 for more details on the original data sets).", "labels": [], "entities": [{"text": "ODCIE", "start_pos": 69, "end_pos": 74, "type": "DATASET", "confidence": 0.8708850741386414}]}, {"text": "This process results in the selection of 60 candidate verb-noun pairs.", "labels": [], "entities": []}, {"text": "For each of the selected pairs, 100 sentences containing its usage were randomly extracted from the automatically parsed BNC, using the method described in Section 4.1.1.", "labels": [], "entities": [{"text": "BNC", "start_pos": 121, "end_pos": 124, "type": "DATASET", "confidence": 0.901896595954895}]}, {"text": "For a pair which occurs less than 100 times in the BNC, all of its usages were extracted.", "labels": [], "entities": [{"text": "BNC", "start_pos": 51, "end_pos": 54, "type": "DATASET", "confidence": 0.9693920612335205}]}, {"text": "Two judges were asked to independently label each use of each candidate expression as literal, idiomatic, or unknown.", "labels": [], "entities": []}, {"text": "When annotating a token, the judges had access to only the sentence in which it occurred, and not the surrounding sentences.", "labels": [], "entities": []}, {"text": "If this context was insufficient to determine the class of the expression, the judge assigned the unknown label.", "labels": [], "entities": []}, {"text": "In an effort to assure high agreement between the judges' annotations, the judges were also provided with the dictionary definitions of the idiomatic meanings of the expressions.", "labels": [], "entities": [{"text": "agreement", "start_pos": 28, "end_pos": 37, "type": "METRIC", "confidence": 0.9543893337249756}]}, {"text": "Idiomaticity is not a binary property; rather it is known to fall on a continuum from completely semantically transparent, or literal, to entirely opaque, or idiomatic.", "labels": [], "entities": []}, {"text": "The human annotators were required to pick the label, literal or idiomatic, that best fit the usage in their judgment; they were not to use the unknown label for intermediate cases.", "labels": [], "entities": []}, {"text": "Figurative extensions of literal meanings were classified as literal if their overall meaning was judged to be fairly transparent, as in You turn right when we hit the road at the end of this track (taken from the BNC).", "labels": [], "entities": [{"text": "BNC)", "start_pos": 214, "end_pos": 218, "type": "DATASET", "confidence": 0.9194145202636719}]}, {"text": "Sometimes an idiomatic usage, such as have word in At the moment they only had the word of Nicola's husband for what had happened (also taken from the BNC), is somewhat directly related to its literal meaning, which is not the case for more semantically opaque idioms such as hit the roof.", "labels": [], "entities": [{"text": "BNC", "start_pos": 151, "end_pos": 154, "type": "DATASET", "confidence": 0.9308742880821228}]}, {"text": "This sentence was classified as idiomatic because the idiomatic meaning is much more salient than the literal meaning.", "labels": [], "entities": []}, {"text": "First, our primary judge, a native English speaker and an author of this paper, annotated each use of each candidate expression.", "labels": [], "entities": []}, {"text": "Based on this judge's annotations, we removed the 25 expressions with fewer than 5 instances of either of their literal or idiomatic meanings, leaving 28 expressions.", "labels": [], "entities": []}, {"text": "20 (We will revisit the 25 removed expressions in Section 7.2.4.)", "labels": [], "entities": []}, {"text": "The remaining expressions were then split into development (DEV) and test (TEST) sets of 14 expressions each.", "labels": [], "entities": []}, {"text": "The data was divided such that DEV and TEST would be approximately equal with respect to the frequency of their expressions, as well as their proportion of idiomatic-to-literal usages (according to the primary judge's annotations).", "labels": [], "entities": [{"text": "DEV", "start_pos": 31, "end_pos": 34, "type": "DATASET", "confidence": 0.5360789895057678}, {"text": "TEST", "start_pos": 39, "end_pos": 43, "type": "METRIC", "confidence": 0.9670575261116028}]}, {"text": "At this stage, DEV and TEST contained a total of 813 and 743 tokens, respectively.", "labels": [], "entities": [{"text": "DEV", "start_pos": 15, "end_pos": 18, "type": "DATASET", "confidence": 0.9275718331336975}, {"text": "TEST", "start_pos": 23, "end_pos": 27, "type": "DATASET", "confidence": 0.5698912739753723}]}, {"text": "Our second judge, also a native English-speaking author of this paper, then annotated DEV and TEST sentences.", "labels": [], "entities": [{"text": "DEV", "start_pos": 86, "end_pos": 89, "type": "METRIC", "confidence": 0.49959591031074524}, {"text": "TEST", "start_pos": 94, "end_pos": 98, "type": "METRIC", "confidence": 0.9480705857276917}]}, {"text": "The observed agreement and unweighted kappa score (Cohen 1960) on TEST were 76% and 0.62, respectively.", "labels": [], "entities": [{"text": "agreement", "start_pos": 13, "end_pos": 22, "type": "METRIC", "confidence": 0.9983720183372498}, {"text": "kappa score", "start_pos": 38, "end_pos": 49, "type": "METRIC", "confidence": 0.8362442553043365}, {"text": "TEST", "start_pos": 66, "end_pos": 70, "type": "METRIC", "confidence": 0.5490103363990784}]}, {"text": "The judges discussed tokens on which they disagreed to achieve a consensus annotation.", "labels": [], "entities": []}, {"text": "Final annotations were generated by removing tokens that received the unknown label as the consensus annotation, leaving DEV and TEST with a total of 573 and 607 tokens, and an average of 41 and 43 tokens per expression, respectively.", "labels": [], "entities": [{"text": "DEV", "start_pos": 121, "end_pos": 124, "type": "DATASET", "confidence": 0.893834114074707}]}, {"text": "shows the DEV and the TEST verb-noun pairs used in our experiments.", "labels": [], "entities": [{"text": "DEV", "start_pos": 10, "end_pos": 13, "type": "DATASET", "confidence": 0.8333771824836731}]}, {"text": "The table also contains information on the number of tokens considered for each pair, as well as the percentage of its usages which are idiomatic.", "labels": [], "entities": []}, {"text": "From the original set of 60 expressions, seven were excluded because our primary annotator did not provide any annotations for them.", "labels": [], "entities": []}, {"text": "These include catch one's breath, cut one's losses, and push one's luck (for which our annotator did not have access to a literal interpretation); and blow one's (own) horn, pull one's hair, give a lift, and get the bird (for which our annotator did not have access to an idiomatic meaning).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2  Accuracy and relative error reduction for the two fixedness measures, the two baseline  measures, and Smadja, over all test pairs (TEST all ), and test pairs divided by frequency  (TEST f low and TEST f high ).", "labels": [], "entities": [{"text": "relative error reduction", "start_pos": 23, "end_pos": 47, "type": "METRIC", "confidence": 0.7745642066001892}, {"text": "Smadja", "start_pos": 112, "end_pos": 118, "type": "METRIC", "confidence": 0.917434573173523}]}, {"text": " Table 3  Classification and retrieval performance of the overall fixedness measure over TEST all .", "labels": [], "entities": [{"text": "retrieval", "start_pos": 29, "end_pos": 38, "type": "METRIC", "confidence": 0.9752365350723267}, {"text": "TEST", "start_pos": 89, "end_pos": 93, "type": "METRIC", "confidence": 0.5572237968444824}]}, {"text": " Table 5  Experimental DEV and TEST verb-noun pairs, their token frequency (FRQ), and the percentage of  their usages that are idiomatic (%IDM), ordered in decreasing %IDM.", "labels": [], "entities": [{"text": "token frequency (FRQ)", "start_pos": 59, "end_pos": 80, "type": "METRIC", "confidence": 0.7573671638965607}]}, {"text": " Table 6  Macro-averaged accuracy (%Acc) and relative error rate reduction (%ERR) on TEST expressions.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 25, "end_pos": 33, "type": "METRIC", "confidence": 0.9897708892822266}, {"text": "Acc", "start_pos": 36, "end_pos": 39, "type": "METRIC", "confidence": 0.9839910268783569}, {"text": "relative error rate reduction (%ERR)", "start_pos": 45, "end_pos": 81, "type": "METRIC", "confidence": 0.7618727556296757}]}, {"text": " Table 7  Macro-averaged accuracy (%Acc) and relative error rate reduction (%ERR) on the 28 expressions  in DT (DEV and TEST combined), divided according to the proportion of idiomatic-to-literal  usages (high and low).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 25, "end_pos": 33, "type": "METRIC", "confidence": 0.9834949374198914}, {"text": "Acc", "start_pos": 36, "end_pos": 39, "type": "METRIC", "confidence": 0.9887731671333313}, {"text": "relative error rate reduction", "start_pos": 45, "end_pos": 74, "type": "METRIC", "confidence": 0.8067136853933334}, {"text": "ERR", "start_pos": 77, "end_pos": 80, "type": "METRIC", "confidence": 0.6449846029281616}]}, {"text": " Table 10  Detailed classification performance of all methods over DT I high and DT I low . Performance is given  using four measures: Sens or R idm , PPV or P idm , Spec or R lit , and NPV or P lit , macro-averaged  using 14%-trimmed mean.", "labels": [], "entities": []}, {"text": " Table 11  Macro-averaged accuracy (%Acc) and relative error rate reduction (%ERR) on the 23 expressions  in SKEWED-IDM and on the 37 expressions in the combination of TEST and SKEWED-IDM (ALL).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 26, "end_pos": 34, "type": "METRIC", "confidence": 0.9854539632797241}, {"text": "Acc", "start_pos": 37, "end_pos": 40, "type": "METRIC", "confidence": 0.9883121848106384}, {"text": "relative error rate reduction", "start_pos": 46, "end_pos": 75, "type": "METRIC", "confidence": 0.8299110531806946}, {"text": "ERR", "start_pos": 78, "end_pos": 81, "type": "METRIC", "confidence": 0.6533873081207275}]}, {"text": " Table 12  Performance of CFORM on individual expressions in DT I high and DT I low .", "labels": [], "entities": [{"text": "CFORM", "start_pos": 26, "end_pos": 31, "type": "METRIC", "confidence": 0.5986548662185669}]}, {"text": " Table 13  Performance of CONTEXT on individual expressions in DT I high and DT I low .", "labels": [], "entities": [{"text": "CONTEXT", "start_pos": 26, "end_pos": 33, "type": "METRIC", "confidence": 0.9584490656852722}]}]}