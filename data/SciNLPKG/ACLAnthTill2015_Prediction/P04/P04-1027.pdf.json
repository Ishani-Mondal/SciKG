{"title": [{"text": "An Empirical Study of Information Synthesis Tasks", "labels": [], "entities": [{"text": "Information Synthesis Tasks", "start_pos": 22, "end_pos": 49, "type": "TASK", "confidence": 0.8004553218682607}]}], "abstractContent": [{"text": "This paper describes an empirical study of the \"In-formation Synthesis\" task, defined as the process of (given a complex information need) extracting, organizing and interrelating the pieces of information contained in a set of relevant documents, in order to obtain a comprehensive, non redundant report that satisfies the information need.", "labels": [], "entities": [{"text": "In-formation Synthesis\" task", "start_pos": 48, "end_pos": 76, "type": "TASK", "confidence": 0.8167484477162361}]}, {"text": "Two main results are presented: a) the creation of an Information Synthesis testbed with 72 reports manually generated by nine subjects for eight complex topics with 100 relevant documents each; and b) an empirical comparison of similarity metrics between reports, under the hypothesis that the best metric is the one that best distinguishes between manual and automatically generated reports.", "labels": [], "entities": []}, {"text": "A metric based on key concepts overlap gives better results than metrics based on n-gram overlap (such as ROUGE) or sentence overlap.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 106, "end_pos": 111, "type": "METRIC", "confidence": 0.9609911441802979}]}], "introductionContent": [{"text": "A classical Information Retrieval (IR) system helps the user finding relevant documents in a given text collection.", "labels": [], "entities": [{"text": "Information Retrieval (IR)", "start_pos": 12, "end_pos": 38, "type": "TASK", "confidence": 0.8350790500640869}]}, {"text": "In most occasions, however, this is only the first step towards fulfilling an information need.", "labels": [], "entities": []}, {"text": "The next steps consist of extracting, organizing and relating the relevant pieces of information, in order to obtain a comprehensive, non redundant report that satisfies the information need.", "labels": [], "entities": []}, {"text": "In this paper, we will refer to this process as Information Synthesis.", "labels": [], "entities": [{"text": "Information Synthesis", "start_pos": 48, "end_pos": 69, "type": "TASK", "confidence": 0.8048707246780396}]}, {"text": "It is normally understood as an (intellectually challenging) human task, and perhaps the Google Answer Service 1 is the best general purpose illustration of how it works.", "labels": [], "entities": [{"text": "Google Answer Service 1", "start_pos": 89, "end_pos": 112, "type": "DATASET", "confidence": 0.8069281429052353}]}, {"text": "In this service, users send complex queries which cannot be answered simply by inspecting the first two or three documents returned by a search engine.", "labels": [], "entities": []}, {"text": "These area couple of real, representative examples: a) I'm looking for information concerning the history of text compression both before and with computers.", "labels": [], "entities": [{"text": "text compression", "start_pos": 109, "end_pos": 125, "type": "TASK", "confidence": 0.7674507796764374}]}, {"text": "Answers to such complex information needs are provided by experts which, commonly, search the Internet, select the best sources, and assemble the most relevant pieces of information into a report, organizing the most important facts and providing additional web hyperlinks for further reading.", "labels": [], "entities": []}, {"text": "This Information Synthesis task is understood, in Google Answers, as a human task for which a search engine only provides the initial starting point.", "labels": [], "entities": [{"text": "Information Synthesis task", "start_pos": 5, "end_pos": 31, "type": "TASK", "confidence": 0.8221317132314047}]}, {"text": "Our midterm goal is to develop computer assistants that help users to accomplish Information Synthesis tasks.", "labels": [], "entities": [{"text": "Information Synthesis tasks", "start_pos": 81, "end_pos": 108, "type": "TASK", "confidence": 0.8210614919662476}]}, {"text": "From a Computational Linguistics point of view, Information Synthesis can be seen as a kind of topic-oriented, informative multi-document summarization, where the goal is to produce a single text as a compressed version of a set of documents with a minimum loss of relevant information.", "labels": [], "entities": [{"text": "Information Synthesis", "start_pos": 48, "end_pos": 69, "type": "TASK", "confidence": 0.7282973229885101}]}, {"text": "Unlike indicative summaries (which help to determine whether a document is relevant to a particular topic), informative summaries must be helpful to answer, for instance, factual questions about the topic.", "labels": [], "entities": []}, {"text": "In the remainder of the paper, we will use the term \"reports\" to refer to the summaries produced in an Information Synthesis task, in order to distinguish them from other kinds of summaries.", "labels": [], "entities": [{"text": "Information Synthesis task", "start_pos": 103, "end_pos": 129, "type": "TASK", "confidence": 0.7534370323022207}]}, {"text": "Topic-oriented multi-document summarization has already been studied in other evaluation initiatives which provide testbeds to compare alternative approaches).", "labels": [], "entities": [{"text": "Topic-oriented multi-document summarization", "start_pos": 0, "end_pos": 43, "type": "TASK", "confidence": 0.5622199972470602}]}, {"text": "Unfortunately, those studies have been restricted to very small summaries (around 100 words) and small document sets (10-20 documents).", "labels": [], "entities": []}, {"text": "These are relevant summarization tasks, but hardly representative of the Information Synthesis problem we are focusing on.", "labels": [], "entities": [{"text": "summarization", "start_pos": 19, "end_pos": 32, "type": "TASK", "confidence": 0.9624841809272766}, {"text": "Information Synthesis", "start_pos": 73, "end_pos": 94, "type": "TASK", "confidence": 0.8257512748241425}]}, {"text": "The first goal of our work has been, therefore, to create a suitable testbed that permits qualitative and quantitative studies on the information synthesis task.", "labels": [], "entities": [{"text": "information synthesis task", "start_pos": 134, "end_pos": 160, "type": "TASK", "confidence": 0.890374481678009}]}, {"text": "Section 2 describes the creation of such a testbed, which includes the manual generation of 72 reports by nine different subjects across 8 complex topics with 100 relevant documents per topic.", "labels": [], "entities": []}, {"text": "Using this testbed, our second goal has been to compare alternative similarity metrics for the Information Synthesis task.", "labels": [], "entities": [{"text": "Information Synthesis task", "start_pos": 95, "end_pos": 121, "type": "TASK", "confidence": 0.8976875146230062}]}, {"text": "A good similarity metric provides away of evaluating Information Synthesis systems (comparing their output with manually generated reports), and should also shed some light on the common properties of manually generated reports.", "labels": [], "entities": [{"text": "Information Synthesis", "start_pos": 53, "end_pos": 74, "type": "TASK", "confidence": 0.8031560480594635}]}, {"text": "Our working hypothesis is that the best metric will best distinguish between manual and automatically generated reports.", "labels": [], "entities": []}, {"text": "We have compared several similarity metrics, including a few baseline measures (based on document, sentence and vocabulary overlap) and a stateof-the-art measure to evaluate summarization systems, ROUGE (.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 197, "end_pos": 202, "type": "METRIC", "confidence": 0.9719540476799011}]}, {"text": "We also introduce another proximity measure based on key concept overlap, which turns out to be substantially better than ROUGE fora relevant class of topics.", "labels": [], "entities": [{"text": "ROUGE", "start_pos": 122, "end_pos": 127, "type": "METRIC", "confidence": 0.9850972890853882}]}, {"text": "Section 3 describes these metrics and the experimental design to compare them; in Section 4, we analyze the outcome of the experiment, and Section 5 discusses related work.", "labels": [], "entities": []}, {"text": "Finally, Section 6 draws the main conclusions of this work.", "labels": [], "entities": []}], "datasetContent": [{"text": "The behavior of the two Information Extraction (IE) topics is substantially different from TT topics.", "labels": [], "entities": [{"text": "Information Extraction (IE)", "start_pos": 24, "end_pos": 51, "type": "TASK", "confidence": 0.8193258285522461}]}, {"text": "While the ROUGE measure remains stable (0.53 versus 0.54), the key concept similarity is much worse with IE topics (0.52 versus 0.77).", "labels": [], "entities": [{"text": "ROUGE measure", "start_pos": 10, "end_pos": 23, "type": "METRIC", "confidence": 0.9779487252235413}]}, {"text": "On the other hand, all baselines improve, and some of them (SentenceSim precision and perplexity) give better results than both ROUGE and NICOS.", "labels": [], "entities": [{"text": "precision", "start_pos": 72, "end_pos": 81, "type": "METRIC", "confidence": 0.8644527792930603}, {"text": "ROUGE", "start_pos": 128, "end_pos": 133, "type": "METRIC", "confidence": 0.9455865025520325}, {"text": "NICOS", "start_pos": 138, "end_pos": 143, "type": "DATASET", "confidence": 0.8679835796356201}]}, {"text": "Of course, no reliable conclusion can be obtained from only two IE topics.", "labels": [], "entities": [{"text": "IE", "start_pos": 64, "end_pos": 66, "type": "TASK", "confidence": 0.9554388523101807}]}, {"text": "But the observed differences suggest that TT and IE may need different approaches, both to the automatic generation of reports and to their evaluation.", "labels": [], "entities": [{"text": "TT", "start_pos": 42, "end_pos": 44, "type": "TASK", "confidence": 0.8578119874000549}, {"text": "IE", "start_pos": 49, "end_pos": 51, "type": "TASK", "confidence": 0.927812933921814}]}, {"text": "One possible reason for this different behavior is that IE topics do not have a set of consistent key concepts; every case of a hunger strike, for instance, involves different people, organizations and places.", "labels": [], "entities": [{"text": "IE topics", "start_pos": 56, "end_pos": 65, "type": "TASK", "confidence": 0.9351489841938019}]}, {"text": "The average number of different key concepts is 18.7 for TT topics and 28.5 for IE topics, a difference that reveals less agreement between subjects, supporting this argument.", "labels": [], "entities": [{"text": "TT topics", "start_pos": 57, "end_pos": 66, "type": "TASK", "confidence": 0.6436651945114136}, {"text": "IE topics", "start_pos": 80, "end_pos": 89, "type": "TASK", "confidence": 0.727191835641861}]}], "tableCaptions": []}