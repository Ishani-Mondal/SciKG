{"title": [{"text": "Unsupervised Sense Disambiguation Using Bilingual Probabilistic Models", "labels": [], "entities": [{"text": "Unsupervised Sense Disambiguation", "start_pos": 0, "end_pos": 33, "type": "TASK", "confidence": 0.5843722323576609}]}], "abstractContent": [{"text": "We describe two probabilistic models for unsuper-vised word-sense disambiguation using parallel corpora.", "labels": [], "entities": [{"text": "word-sense disambiguation", "start_pos": 55, "end_pos": 80, "type": "TASK", "confidence": 0.6685999929904938}]}, {"text": "The first model, which we call the Sense model, builds on the work of Diab and Resnik (2002) that uses both parallel text and a sense inventory for the target language, and recasts their approach in a probabilistic framework.", "labels": [], "entities": []}, {"text": "The second model, which we call the Concept model, is a hierarchical model that uses a concept latent variable to relate different language specific sense labels.", "labels": [], "entities": []}, {"text": "We show that both models improve performance on the word sense disambiguation task over previous unsu-pervised approaches, with the Concept model showing the largest improvement.", "labels": [], "entities": [{"text": "word sense disambiguation task", "start_pos": 52, "end_pos": 82, "type": "TASK", "confidence": 0.8024395555257797}]}, {"text": "Furthermore, in learning the Concept model, as a by-product, we learn a sense inventory for the parallel language.", "labels": [], "entities": []}], "introductionContent": [{"text": "Word sense disambiguation (WSD) has been a central question in the computational linguistics community since its inception.", "labels": [], "entities": [{"text": "Word sense disambiguation (WSD)", "start_pos": 0, "end_pos": 31, "type": "TASK", "confidence": 0.8526131709416708}]}, {"text": "WSD is fundamental to natural language understanding and is a useful intermediate step for many other language processing tasks.", "labels": [], "entities": [{"text": "WSD", "start_pos": 0, "end_pos": 3, "type": "TASK", "confidence": 0.6629317402839661}, {"text": "natural language understanding", "start_pos": 22, "end_pos": 52, "type": "TASK", "confidence": 0.6522271235783895}]}, {"text": "Many recent approaches make use of ideas from statistical machine learning; the availability of shared sense definitions (e.g. WordNet) and recent international competitions) have enabled researchers to compare their results.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 127, "end_pos": 134, "type": "DATASET", "confidence": 0.9374210834503174}]}, {"text": "Supervised approaches which make use of a small hand-labeled training set ( typically outperform unsupervised approaches (, but tend to be tuned to a specific corpus and are constrained by scarcity of labeled data.", "labels": [], "entities": []}, {"text": "In an effort to overcome the difficulty of finding sense-labeled training data, researchers have begun investigating unsupervised approaches to wordsense disambiguation.", "labels": [], "entities": [{"text": "wordsense disambiguation", "start_pos": 144, "end_pos": 168, "type": "TASK", "confidence": 0.7543517053127289}]}, {"text": "For example, the use of parallel corpora for sense tagging can help with word sense disambiguation (.", "labels": [], "entities": [{"text": "sense tagging", "start_pos": 45, "end_pos": 58, "type": "TASK", "confidence": 0.7109009772539139}, {"text": "word sense disambiguation", "start_pos": 73, "end_pos": 98, "type": "TASK", "confidence": 0.7355775435765585}]}, {"text": "As an illustration of sense disambiguation from translation data, when the English word bank is translated to Spanish as orilla, it is clear that we are referring to the shore sense of bank, rather than the financial institution sense.", "labels": [], "entities": []}, {"text": "The main inspiration for our work is, who use translations and linguistic knowledge for disambiguation and automatic sense tagging.", "labels": [], "entities": [{"text": "automatic sense tagging", "start_pos": 107, "end_pos": 130, "type": "TASK", "confidence": 0.6006718178590139}]}, {"text": "present a graphical model that is an attempt to formalize probabilistically the main ideas in.", "labels": [], "entities": []}, {"text": "They assume the same semantic hierarchy (in particular, WordNet) for both the languages and assign English words as well as their translations to WordNet synsets.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 56, "end_pos": 63, "type": "DATASET", "confidence": 0.963502824306488}]}, {"text": "Here we present two variants of the graphical model in, along with a method to discover a cluster structure for the Spanish senses.", "labels": [], "entities": []}, {"text": "We also present empirical word sense disambiguation results which demonstrate the gain brought by this probabilistic approach, even while only using the translated word to provide disambiguation information.", "labels": [], "entities": [{"text": "word sense disambiguation", "start_pos": 26, "end_pos": 51, "type": "TASK", "confidence": 0.585437128941218}]}, {"text": "Our first generative model, the Sense Model, groups semantically related words from the two languages into senses, and translations are generated by probabilistically choosing a sense and then words from the sense.", "labels": [], "entities": []}, {"text": "We show that this improves on the results of.", "labels": [], "entities": []}, {"text": "Our next model, which we call the Concept Model, aims to improve on the above sense structure by modeling the senses of the two languages separately and relating senses from both languages through a higher-level, semantically less precise concept.", "labels": [], "entities": []}, {"text": "The intuition here is that not all of the senses that are possible fora word will be relevant fora concept.", "labels": [], "entities": []}, {"text": "In other words, the distribution over the senses of a word given a concept can be expected to have a lower entropy than the distribution over the senses of the word in the language as a whole.", "labels": [], "entities": []}, {"text": "In this paper, we look at translation data as a resource for identification of semantic concepts.", "labels": [], "entities": [{"text": "identification of semantic concepts", "start_pos": 61, "end_pos": 96, "type": "TASK", "confidence": 0.8318396508693695}]}, {"text": "Note that actual translated word pairs are not always good matches semantically, because the translation process is not on a word byword basis.", "labels": [], "entities": []}, {"text": "This introduces a kind of noise in the translation, and an additional hidden variable to represent the shared meaning helps to take it into account.", "labels": [], "entities": []}, {"text": "Improved performance over the Sense Model validates the use of concepts in modeling translations.", "labels": [], "entities": [{"text": "modeling translations", "start_pos": 75, "end_pos": 96, "type": "TASK", "confidence": 0.6982412934303284}]}, {"text": "An interesting by-product of the Concept Model is a semantic structure for the secondary language.", "labels": [], "entities": []}, {"text": "This is automatically constructed using background knowledge of the structure for the primary language and the observed translation pairs.", "labels": [], "entities": []}, {"text": "In the model, words sharing the same sense are synonyms while senses under the same concept are semantically related in the corpus.", "labels": [], "entities": []}, {"text": "An investigation of the model trained over real data reveals that it can indeed group related words together.", "labels": [], "entities": []}, {"text": "It maybe noted that predicting senses from translations need not necessarily bean end result in itself.", "labels": [], "entities": [{"text": "predicting senses from translations", "start_pos": 20, "end_pos": 55, "type": "TASK", "confidence": 0.8850740939378738}]}, {"text": "As we have already mentioned, lack of labeled data is a severe hindrance for supervised approaches to word sense disambiguation.", "labels": [], "entities": [{"text": "word sense disambiguation", "start_pos": 102, "end_pos": 127, "type": "TASK", "confidence": 0.7441445787747701}]}, {"text": "At the same time, there is an abundance of bilingual documents and many more can potentially be mined from the web.", "labels": [], "entities": []}, {"text": "It should be possible using our approach to (noisily) assign sense tags to words in such documents, thus providing huge resources of labeled data for supervised approaches to make use of.", "labels": [], "entities": []}, {"text": "For the rest of this paper, for simplicity we will refer to the primary language of the parallel document as English and to the secondary as Spanish.", "labels": [], "entities": []}, {"text": "The paper is organized as follows.", "labels": [], "entities": []}, {"text": "We begin by formally describing the models in Section 2.", "labels": [], "entities": []}, {"text": "We describe our approach for constructing the senses and concepts in Section 3.", "labels": [], "entities": []}, {"text": "Our algorithm for learning the model parameters is described in Section 4.", "labels": [], "entities": []}, {"text": "We present experimental results in Section 5 and our analysis in Section 6.", "labels": [], "entities": []}, {"text": "We conclude in Section 7.", "labels": [], "entities": []}], "datasetContent": [{"text": "Both the models are generative probabilistic models learned from parallel corpora and are expected to fit the training and subsequent test data.", "labels": [], "entities": []}, {"text": "A good fit should be reflected in good prediction accuracy over a test set.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 50, "end_pos": 58, "type": "METRIC", "confidence": 0.9548784494400024}]}, {"text": "The prediction task of interest is the sense of an English word when its translation is provided.", "labels": [], "entities": [{"text": "sense of an English word when its translation", "start_pos": 39, "end_pos": 84, "type": "TASK", "confidence": 0.7215761318802834}]}, {"text": "We estimate the prediction accuracy and recall of our models on Senseval data.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 27, "end_pos": 35, "type": "METRIC", "confidence": 0.9792330861091614}, {"text": "recall", "start_pos": 40, "end_pos": 46, "type": "METRIC", "confidence": 0.9994107484817505}, {"text": "Senseval data", "start_pos": 64, "end_pos": 77, "type": "DATASET", "confidence": 0.9438900351524353}]}, {"text": "In addition, the Concept Model learns a sense structure for the Spanish 2 Accuracy is the ratio of the number of correct predictions and the number of attempted predictions.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 74, "end_pos": 82, "type": "METRIC", "confidence": 0.5135323405265808}]}, {"text": "Recall is the ratio of the number of correct predictions and the size of the test set. language.", "labels": [], "entities": [{"text": "Recall", "start_pos": 0, "end_pos": 6, "type": "METRIC", "confidence": 0.9916497468948364}]}, {"text": "While it is hard to objectively evaluate the quality of such a structure, we present some interesting concepts that are learned as an indication of the potential of our approach.", "labels": [], "entities": []}, {"text": "In our experiments with real data, we make use of the parallel corpora constructed by for evaluation purposes.", "labels": [], "entities": []}, {"text": "We chose to work on these corpora in order to permit a direct comparison with their results.", "labels": [], "entities": []}, {"text": "The sense-tagged portion of the English corpus is comprised of the English \"allwords\" section of the SENSEVAL-2 test data.", "labels": [], "entities": [{"text": "SENSEVAL-2 test data", "start_pos": 101, "end_pos": 121, "type": "DATASET", "confidence": 0.7947886983553568}]}, {"text": "The remainder of this corpus is constructed by adding the Brown Corpus, the SENSEVAL-1 corpus, the SENSEVAL-2 English Lexical Sample test, trial and training corpora and the Wall Street Journal sections 18-24 from the Penn Treebank.", "labels": [], "entities": [{"text": "Brown Corpus", "start_pos": 58, "end_pos": 70, "type": "DATASET", "confidence": 0.9828521013259888}, {"text": "SENSEVAL-1 corpus", "start_pos": 76, "end_pos": 93, "type": "DATASET", "confidence": 0.6811350882053375}, {"text": "SENSEVAL-2 English Lexical Sample", "start_pos": 99, "end_pos": 132, "type": "TASK", "confidence": 0.44889406114816666}, {"text": "Wall Street Journal sections 18-24 from the Penn Treebank", "start_pos": 174, "end_pos": 231, "type": "DATASET", "confidence": 0.921559022532569}]}, {"text": "This English corpus is translated into Spanish using two commercially available MT systems: Globalink Pro 6.4 and Systran Professional Premium.", "labels": [], "entities": [{"text": "Globalink Pro 6.4", "start_pos": 92, "end_pos": 109, "type": "DATASET", "confidence": 0.9312405983606974}]}, {"text": "The GIZA++ implementation of the IBM statistical MT models was used to derive the most-likely word-level alignments, and these define the English/Spanish word co-occurrences.", "labels": [], "entities": []}, {"text": "To take into account variability of translation, we combine the translations from the two systems for each English word, following in the footsteps of.", "labels": [], "entities": []}, {"text": "For our experiments, we focus only on nouns, of which there are 875 occurrences in our tagged data.", "labels": [], "entities": []}, {"text": "The sense tags for the English domain are derived from the WordNet 1.7 inventory.", "labels": [], "entities": [{"text": "WordNet 1.7 inventory", "start_pos": 59, "end_pos": 80, "type": "DATASET", "confidence": 0.9546870589256287}]}, {"text": "After pruning stopwords, we end up with 16,186 English words, 31,862 Spanish words and 2,385,574 instances of 41,850 distinct translation pairs.", "labels": [], "entities": []}, {"text": "The English words come from 20,361 WordNet senses.", "labels": [], "entities": []}, {"text": "As can be seen from the following table, both our models clearly outperform, which is an improvement over, in both accuracy and recall, while the Concept Model does significantly better than the Sense Model with fewer parameters.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 115, "end_pos": 123, "type": "METRIC", "confidence": 0.999356210231781}, {"text": "recall", "start_pos": 128, "end_pos": 134, "type": "METRIC", "confidence": 0.9990309476852417}]}, {"text": "The comparison is restricted to the same subset of the test data.", "labels": [], "entities": []}, {"text": "For our best results, the Sense Model has 20,361 senses, while the Concept Model has 20,361 English senses, 11,961 Spanish senses and 7,366 concepts.", "labels": [], "entities": []}, {"text": "The Concept Model results are for the version that allows multiple senses fora Spanish word.", "labels": [], "entities": []}, {"text": "Results for the In, we compare the prediction accuracy and recall against those of the 21 Senseval-2 English All Words participants and that of, when restricted to the same set of noun instances from the gold standard.", "labels": [], "entities": [{"text": "prediction", "start_pos": 35, "end_pos": 45, "type": "METRIC", "confidence": 0.870708167552948}, {"text": "accuracy", "start_pos": 46, "end_pos": 54, "type": "METRIC", "confidence": 0.7798731923103333}, {"text": "recall", "start_pos": 59, "end_pos": 65, "type": "METRIC", "confidence": 0.9996600151062012}]}, {"text": "It can be seen that our models outperform all the unsupervised approaches in recall and many supervised ones as well.", "labels": [], "entities": [{"text": "recall", "start_pos": 77, "end_pos": 83, "type": "METRIC", "confidence": 0.9547598361968994}]}, {"text": "No unsupervised approach is better in both accuracy and recall.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 43, "end_pos": 51, "type": "METRIC", "confidence": 0.9995228052139282}, {"text": "recall", "start_pos": 56, "end_pos": 62, "type": "METRIC", "confidence": 0.9984000325202942}]}, {"text": "It needs to be kept in mind that we take into account only bilingual data for our predictions, and not monolingual features like context of the word as most other WSD approaches do. shows some interesting examples of different Spanish senses for discovered concepts.", "labels": [], "entities": []}, {"text": "The context of most concepts, like the ones shown, can be easily understood.", "labels": [], "entities": []}, {"text": "For example, the first concept is about government actions and the second deals with murder and accidental deaths.", "labels": [], "entities": []}, {"text": "The penultimate concept is interesting because it deals with different kinds of association and involves three different senses containing the word conex\u00ed on.", "labels": [], "entities": []}, {"text": "The other words in two of these senses suggest that they are about union and relation respectively.", "labels": [], "entities": []}, {"text": "The third probably involves the link sense of connection.", "labels": [], "entities": []}, {"text": "Conciseness of the concepts depends on the similarity threshold that is selected.", "labels": [], "entities": []}, {"text": "Some may bring together loosely-related topics, which can be separated by a higher threshold.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Comparison with Diab's Model", "labels": [], "entities": []}]}