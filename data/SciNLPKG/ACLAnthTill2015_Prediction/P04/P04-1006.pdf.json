{"title": [{"text": "Attention Shifting for Parsing Speech *", "labels": [], "entities": [{"text": "Attention Shifting", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.8638953864574432}, {"text": "Parsing Speech", "start_pos": 23, "end_pos": 37, "type": "TASK", "confidence": 0.9011618196964264}]}], "abstractContent": [{"text": "We present a technique that improves the efficiency of word-lattice parsing as used in speech recognition language modeling.", "labels": [], "entities": [{"text": "word-lattice parsing", "start_pos": 55, "end_pos": 75, "type": "TASK", "confidence": 0.7108401507139206}, {"text": "speech recognition language modeling", "start_pos": 87, "end_pos": 123, "type": "TASK", "confidence": 0.8166684433817863}]}, {"text": "Our technique applies a probabilistic parser iteratively whereon each iteration it focuses on a different subset of the word-lattice.", "labels": [], "entities": []}, {"text": "The parser's attention is shifted towards word-lattice subsets for which there are few or no syntactic analyses posited.", "labels": [], "entities": []}, {"text": "This attention-shifting technique provides a six-times increase in speed (measured as the number of parser analyses evaluated) while performing equivalently when used as the first-stage of a multi-stage parsing-based language model.", "labels": [], "entities": [{"text": "speed", "start_pos": 67, "end_pos": 72, "type": "METRIC", "confidence": 0.9901784658432007}]}], "introductionContent": [{"text": "Success in language modeling has been dominated by the linear n-gram for the past few decades.", "labels": [], "entities": [{"text": "language modeling", "start_pos": 11, "end_pos": 28, "type": "TASK", "confidence": 0.8159773051738739}]}, {"text": "A number of syntactic language models have proven to be competitive with the n-gram and better than the most popular n-gram, the trigram.", "labels": [], "entities": []}, {"text": "Language modeling for speech could well be the first real problem for which syntactic techniques are useful.", "labels": [], "entities": [{"text": "Language modeling", "start_pos": 0, "end_pos": 17, "type": "TASK", "confidence": 0.7108640521764755}]}, {"text": "John ate the pizza on a plate with a fork . One reason that we expect syntactic models to perform well is that they are capable of modeling long-distance dependencies that simple n-gram * This research was supported in part by NSF grants 9870676 and 0085940.", "labels": [], "entities": []}, {"text": "For example, the model presented by) uses syntactic structure to identify lexical items in the left-context which are then modeled as an n-gram process.", "labels": [], "entities": []}, {"text": "The model presented by) identifies both syntactic structural and lexical dependencies that aid in language modeling.", "labels": [], "entities": [{"text": "language modeling", "start_pos": 98, "end_pos": 115, "type": "TASK", "confidence": 0.707514300942421}]}, {"text": "While there are n-gram models that attempt to extend the left-context window through the use of caching and skip models), we believe that linguistically motivated models, such as these lexical-syntactic models, are more robust.", "labels": [], "entities": []}, {"text": "presents a simple example to illustrate the nature of long-distance dependencies.", "labels": [], "entities": []}, {"text": "Using a syntactic model such as the the Structured Language Model (, we predict the word fork given the context {ate, with} where a trigram model uses the context {with, a}.", "labels": [], "entities": []}, {"text": "Consider the problem of disambiguating between . .", "labels": [], "entities": []}, {"text": "plate with a fork and . .", "labels": [], "entities": []}, {"text": "The syntactic model captures the semantic relationship between the words ate and fork.", "labels": [], "entities": []}, {"text": "The syntactic structure allows us to find lexical contexts for which there is some semantic relationship (e.g., predicateargument).", "labels": [], "entities": []}, {"text": "Unfortunately, syntactic language modeling techniques have proven to be extremely expensive in terms of computational effort.", "labels": [], "entities": [{"text": "syntactic language modeling", "start_pos": 15, "end_pos": 42, "type": "TASK", "confidence": 0.6657482782999674}]}, {"text": "Many employ the use of string parsers; in order to utilize such techniques for language modeling one must preselect a set of strings from the word-lattice and parse each of them separately, an inherently inefficient procedure.", "labels": [], "entities": []}, {"text": "Of the techniques that can process word-lattices directly, it takes significant computation to achieve the same levels of accuracy as the n-best reranking method.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 122, "end_pos": 130, "type": "METRIC", "confidence": 0.9982714653015137}]}, {"text": "This computational cost is the result of increasing the search space evaluated with the syntactic model (parser); the larger space resulting from combining the search for syntactic structure with the search for paths in the word-lattice.", "labels": [], "entities": []}, {"text": "In this paper we propose a variation of a probabilistic word-lattice parsing technique that increases efficiency while incurring no loss of language modeling performance (measured as Word Error Rate -WER).", "labels": [], "entities": [{"text": "word-lattice parsing", "start_pos": 56, "end_pos": 76, "type": "TASK", "confidence": 0.6888751536607742}, {"text": "Word Error Rate -WER)", "start_pos": 183, "end_pos": 204, "type": "METRIC", "confidence": 0.8203027745087942}]}, {"text": "In ( we presented a modular lattice parsing process that operates in two stages.", "labels": [], "entities": []}, {"text": "The first stage is a PCFG word-lattice parser that generates a set of candidate parses over strings in a word-lattice, while the second stage rescores these candidate edges using a lexicalized syntactic language model.", "labels": [], "entities": []}, {"text": "Under this paradigm, the first stage is not only responsible for selecting candidate parses, but also for selecting paths in the word-lattice.", "labels": [], "entities": []}, {"text": "Due to computational and memory requirements of the lexicalized model, the second stage parser is capable of rescoring only a small subset of all parser analyses.", "labels": [], "entities": []}, {"text": "For this reason, the PCFG prunes the set of parser analyses, thereby indirectly pruning paths in the word lattice.", "labels": [], "entities": []}, {"text": "We propose adding a meta-process to the firststage that effectively shifts the selection of wordlattice paths to the second stage (where lexical information is available).", "labels": [], "entities": []}, {"text": "We achieve this by ensuring that for each path in the word-lattice the first-stage parser posits at least one parse.", "labels": [], "entities": []}], "datasetContent": [{"text": "The purpose of attention shifting is to reduce the amount of work exerted by the first stage PCFG parser while maintaining the same quality of language modeling (in the multi-stage system).", "labels": [], "entities": [{"text": "attention shifting", "start_pos": 15, "end_pos": 33, "type": "TASK", "confidence": 0.7118480056524277}]}, {"text": "We have performed a set of experiments on the NIST '93 HUB-1 word-lattices.", "labels": [], "entities": [{"text": "NIST '93 HUB-1 word-lattices", "start_pos": 46, "end_pos": 74, "type": "DATASET", "confidence": 0.9396804571151733}]}, {"text": "The HUB-1 is a collection of 213 word-lattices resulting from an acoustic recognizer's analysis of speech utterances.", "labels": [], "entities": []}, {"text": "Professional readers reading Wall Street Journal articles generated the utterances.", "labels": [], "entities": [{"text": "Wall Street Journal articles", "start_pos": 29, "end_pos": 57, "type": "DATASET", "confidence": 0.9494410008192062}]}, {"text": "The first stage parser is a best-first PCFG parser trained on sections 2 through 22, and 24 of the Penn WSJ treebank.", "labels": [], "entities": [{"text": "PCFG parser", "start_pos": 39, "end_pos": 50, "type": "TASK", "confidence": 0.6460631191730499}, {"text": "Penn WSJ treebank", "start_pos": 99, "end_pos": 116, "type": "DATASET", "confidence": 0.9491016864776611}]}, {"text": "Prior to training, the treebank is transformed into speech-like text, removing punctuation and expanding numerals, etc.", "labels": [], "entities": []}, {"text": "5 Overparsing is performed using an edge pop 6 multiplicative factor.", "labels": [], "entities": []}, {"text": "The parser records the number of edge pops required to reach the first complete parse.", "labels": [], "entities": []}, {"text": "The parser continues to parse a until multiple of the number of edge pops required for the first parse are popped off the agenda.", "labels": [], "entities": []}, {"text": "The second stage parser used is a modified version of the Charniak language modeling parser described in).", "labels": [], "entities": [{"text": "Charniak language modeling parser", "start_pos": 58, "end_pos": 91, "type": "TASK", "confidence": 0.5551644712686539}]}, {"text": "We trained this parser on the BLLIP99 corpus ( ); a corpus of 30million words automatically parsed using the Charniak parser.", "labels": [], "entities": [{"text": "BLLIP99 corpus", "start_pos": 30, "end_pos": 44, "type": "DATASET", "confidence": 0.9019346833229065}]}, {"text": "In order to compare the work done by the n-best reranking technique to the word-lattice parser, we generated a set of n-best lattices.", "labels": [], "entities": []}, {"text": "50-best lists were extracted using the Chelba A* decoder . A 50-best lattice is a sublattice of the acoustic lattice that generates only the strings found in the 50-best list.", "labels": [], "entities": [{"text": "Chelba A* decoder", "start_pos": 39, "end_pos": 56, "type": "DATASET", "confidence": 0.8018739968538284}]}, {"text": "Additionally, we provide the results for parsing the full acoustic lattices (although these work measurements should not be compared to those of n-best reranking).", "labels": [], "entities": [{"text": "parsing", "start_pos": 41, "end_pos": 48, "type": "TASK", "confidence": 0.9747791290283203}]}, {"text": "We report the amount of work, shown as the cumulative # edge pops, the oracle WER for the word-lattices after first stage pruning, and the WER of the complete multi-stage parser.", "labels": [], "entities": [{"text": "WER", "start_pos": 78, "end_pos": 81, "type": "METRIC", "confidence": 0.9896520972251892}, {"text": "WER", "start_pos": 139, "end_pos": 142, "type": "METRIC", "confidence": 0.9992693066596985}]}, {"text": "In all of the word-lattice parsing experiments, we pruned the set of posited hypothesis so that no more than 30,000 local-trees are generated 8 . We chose this threshold due to the memory requirements of the second stage parser.", "labels": [], "entities": [{"text": "word-lattice parsing", "start_pos": 14, "end_pos": 34, "type": "TASK", "confidence": 0.6930491030216217}]}, {"text": "Performing pruning at the end of the first stage prevents the attention shifting parser from reaching the minimum oracle WER (most notable in the full acoustic word-lattice experiments).", "labels": [], "entities": [{"text": "WER", "start_pos": 121, "end_pos": 124, "type": "METRIC", "confidence": 0.9852858185768127}]}, {"text": "While the attention-shifting algorithm ensures all word-lattice arcs are included incomplete parses, forward-backward pruning, as used here, will eliminate some of these parses, indirectly eliminating some of the word-lattice arcs.", "labels": [], "entities": []}, {"text": "To illustrate the need for pruning, we computed the number of states used by the Charniak lexicalized syntactic language model for 30,000 local trees.", "labels": [], "entities": []}, {"text": "An average of 215 lexicalized states were generated for each of the 30,000 local trees.", "labels": [], "entities": []}, {"text": "This means that the lexicalized language model, on average, computes probabilities for over 6.5 million states when provided with 30,000 local trees.: Results for n-best lists and n-best lattices.", "labels": [], "entities": []}, {"text": "shows the results for n-best list reranking and word-lattice parsing of n-best lattices.", "labels": [], "entities": [{"text": "word-lattice parsing", "start_pos": 48, "end_pos": 68, "type": "TASK", "confidence": 0.6896060854196548}]}, {"text": "We recreated the results of the Charniak language model parser used for reranking in order to measure the amount of work required.", "labels": [], "entities": []}, {"text": "We ran the first stage parser with 4-times overparsing for each string in the n-best list.", "labels": [], "entities": []}, {"text": "The LatParse result represents running the word-lattice parser on the n-best lattices performing 100-times overparsing in the first stage.", "labels": [], "entities": []}, {"text": "The AttShift model is the attention shifting parser described in this paper.", "labels": [], "entities": [{"text": "attention shifting parser", "start_pos": 26, "end_pos": 51, "type": "TASK", "confidence": 0.7646475434303284}]}, {"text": "We used 10-times overparsing for both the initial parse and each of the attention shifting iterations.", "labels": [], "entities": []}, {"text": "When run on the n-best lattice, this model achieves a comparable WER, while reducing the amount of parser work sixfold (as compared to the regular word-lattice parser).", "labels": [], "entities": [{"text": "WER", "start_pos": 65, "end_pos": 68, "type": "METRIC", "confidence": 0.9984056353569031}]}, {"text": "In we present the results of the wordlattice parser and the attention shifting parser when run on full acoustic lattices.", "labels": [], "entities": [{"text": "wordlattice parser", "start_pos": 33, "end_pos": 51, "type": "TASK", "confidence": 0.6076291054487228}]}, {"text": "While the oracle WER is reduced, we are considering almost half as many edges as the standard word-lattice parser.", "labels": [], "entities": [{"text": "WER", "start_pos": 17, "end_pos": 20, "type": "METRIC", "confidence": 0.9675866961479187}]}, {"text": "The increased size of the acoustic lattices suggests that it may not be computationally efficient to consider the entire lattice and that an additional pruning phase is necessary.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Results for n-best lists and n-best lattices.", "labels": [], "entities": []}, {"text": " Table 3: Results for acoustic lattices.", "labels": [], "entities": []}]}