{"title": [{"text": "Mining metalinguistic activity in corpora to create lexical resources using Information Extraction techniques: the MOP system", "labels": [], "entities": []}], "abstractContent": [{"text": "This paper describes and evaluates MOP, an IE system for automatic extraction of metalinguistic information from technical and scientific documents.", "labels": [], "entities": [{"text": "MOP", "start_pos": 35, "end_pos": 38, "type": "TASK", "confidence": 0.7002248167991638}, {"text": "IE", "start_pos": 43, "end_pos": 45, "type": "TASK", "confidence": 0.9315893650054932}, {"text": "automatic extraction of metalinguistic information from technical and scientific documents", "start_pos": 57, "end_pos": 147, "type": "TASK", "confidence": 0.820636260509491}]}, {"text": "We claim that such a system can create special databases to boot-strap compilation and facilitate update of the huge and dynamically changing glossaries, knowledge bases and ontologies that are vital to modern-day research.", "labels": [], "entities": []}], "introductionContent": [{"text": "Availability of large-scale corpora has made it possible to mine specific knowledge from free or semi-structured text, resulting in what many consider by now a reasonably mature NLP technology.", "labels": [], "entities": []}, {"text": "Extensive research in Information Extraction (IE) techniques, especially with the series of Message Understanding Conferences of the nineties, has focused on tasks such as creating and updating databases of corporate join ventures or terrorist and guerrilla attacks, while the ACQUILEX project used similar methods for creating lexical databases using the highly structured environment of machine-readable dictionary entries and other resources.", "labels": [], "entities": [{"text": "Information Extraction (IE)", "start_pos": 22, "end_pos": 49, "type": "TASK", "confidence": 0.8309439420700073}]}, {"text": "Gathering knowledge from unstructured text often requires manually crafting knowledgeengineering rules both complex and deeply dependent of the domain at hand, although some successful experiences using learning algorithms have been reported.", "labels": [], "entities": []}, {"text": "Although mining specific semantic relations and subcategorization information from free-text has been successfully carried out in the past, automatically extracting lexical resources (including terminological definitions) from text in special domains has been afield less explored, but recent experiences ( show that compiling the extensive resources that modern scientific and technical disciplines need in order to manage the explosive growth of their knowledge, is both feasible and practical.", "labels": [], "entities": []}, {"text": "A good example of this NLP-based processing need is the MedLine abstract database maintained by the National Library of Medicine 1 (NLM), which incorporates around 40,000 Health Sciences papers each month.", "labels": [], "entities": [{"text": "MedLine abstract database", "start_pos": 56, "end_pos": 81, "type": "DATASET", "confidence": 0.7677903572718302}, {"text": "National Library of Medicine 1 (NLM)", "start_pos": 100, "end_pos": 136, "type": "DATASET", "confidence": 0.9192657470703125}]}, {"text": "Researchers depend on these electronic resources to keep abreast of their rapidly changing field.", "labels": [], "entities": []}, {"text": "In order to maintain and update vital indexing references such as the Unified Medical Language System (UMLS) resources, the MeSH and SPECIALIST vocabularies, the NLM staff needs to review 400,000 highly-technical papers each year.", "labels": [], "entities": [{"text": "MeSH", "start_pos": 124, "end_pos": 128, "type": "DATASET", "confidence": 0.9329882264137268}]}, {"text": "Clearly, neology detection, terminological information update and other tasks can benefit from applications that automatically search text for information, e.g., when anew term is introduced or an existing one is modified due to data or theory-driven concerns, or, in general, when new information about sublanguage usage is being put forward.", "labels": [], "entities": [{"text": "neology detection", "start_pos": 9, "end_pos": 26, "type": "TASK", "confidence": 0.9433648586273193}, {"text": "terminological information update", "start_pos": 28, "end_pos": 61, "type": "TASK", "confidence": 0.6293982664744059}]}, {"text": "But the usefulness of robust NLP applications for special-domain text goes beyond glossary updates.", "labels": [], "entities": []}, {"text": "The kind of categorization information implicit in many definitions can help improve anaphora resolution, semantic typing or acronym identification in these corpora, as well as enhance \"semantic rerendering\" of special-domain ontologies and thesaurii).", "labels": [], "entities": [{"text": "anaphora resolution", "start_pos": 85, "end_pos": 104, "type": "TASK", "confidence": 0.7193690836429596}, {"text": "acronym identification", "start_pos": 125, "end_pos": 147, "type": "TASK", "confidence": 0.7064462751150131}]}, {"text": "In this paper we describe and evaluate the MOP 2 IE system, implemented to automatically create Metalinguistic Information Databases (MIDs) from large collections of special-domain research papers.", "labels": [], "entities": [{"text": "MOP 2 IE", "start_pos": 43, "end_pos": 51, "type": "TASK", "confidence": 0.7266655961672465}, {"text": "Metalinguistic Information Databases (MIDs) from large collections of special-domain research papers", "start_pos": 96, "end_pos": 196, "type": "TASK", "confidence": 0.6667055533482478}]}, {"text": "Section 2 will layout the theory, methodology and the empirical research grounding the application, while Section 3 will describe the first phase of the MOP tasks: accurate location of good candidate metalinguistic sentences for further processing.", "labels": [], "entities": [{"text": "MOP tasks", "start_pos": 153, "end_pos": 162, "type": "TASK", "confidence": 0.8843787908554077}]}, {"text": "We experimented both with manually coded rules and with learning algorithms for this task.", "labels": [], "entities": []}, {"text": "Section 4 focuses on the problem of identifying and organizing into a useful database structure the different linguistic constituents of the candidate predications, a phase similar to what are known in the IE literature as Named-Entity recognition, Element and Scenario template fill-up tasks.", "labels": [], "entities": [{"text": "Named-Entity recognition", "start_pos": 223, "end_pos": 247, "type": "TASK", "confidence": 0.6530825644731522}]}, {"text": "Finally, Section 5 discusses results and problems of our experiments, as well as future lines of research.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1. Best metrics for \"call\" lexeme  sorted by F-measure and classifier accuracy", "labels": [], "entities": [{"text": "F-measure", "start_pos": 52, "end_pos": 61, "type": "METRIC", "confidence": 0.9791259765625}, {"text": "accuracy", "start_pos": 77, "end_pos": 85, "type": "METRIC", "confidence": 0.9274658560752869}]}]}