{"title": [{"text": "Convolution Kernels with Feature Selection for Natural Language Processing Tasks", "labels": [], "entities": []}], "abstractContent": [{"text": "Convolution kernels, such as sequence and tree kernels , are advantageous for both the concept and accuracy of many natural language processing (NLP) tasks.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 99, "end_pos": 107, "type": "METRIC", "confidence": 0.9983206391334534}]}, {"text": "Experiments have, however, shown that the over-fitting problem often arises when these kernels are used in NLP tasks.", "labels": [], "entities": []}, {"text": "This paper discusses this issue of convolution kernels, and then proposes anew approach based on statistical feature selection that avoids this issue.", "labels": [], "entities": []}, {"text": "To enable the proposed method to be executed efficiently, it is embedded into an original kernel calculation process by using sub-structure mining algorithms.", "labels": [], "entities": []}, {"text": "Experiments are undertaken on real NLP tasks to confirm the problem with a conventional method and to compare its performance with that of the proposed method.", "labels": [], "entities": []}], "introductionContent": [{"text": "Over the past few years, many machine learning methods have been successfully applied to tasks in natural language processing (NLP).", "labels": [], "entities": [{"text": "natural language processing (NLP)", "start_pos": 98, "end_pos": 131, "type": "TASK", "confidence": 0.7565336028734843}]}, {"text": "Especially, state-of-the-art performance can be achieved with kernel methods, such as Support Vector Machine.", "labels": [], "entities": []}, {"text": "Examples include text categorization, chunking () and parsing.", "labels": [], "entities": [{"text": "text categorization", "start_pos": 17, "end_pos": 36, "type": "TASK", "confidence": 0.7608829140663147}]}, {"text": "Another feature of this kernel methodology is that it not only provides high accuracy but also allows us to design a kernel function suited to modeling the task at hand.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 77, "end_pos": 85, "type": "METRIC", "confidence": 0.996119499206543}]}, {"text": "Since natural language data take the form of sequences of words, and are generally analyzed using discrete structures, such as trees (parsed trees) and graphs (relational graphs), discrete kernels, such as sequence kernels (), tree kernels, and graph kernels (), have been shown to offer excellent results.", "labels": [], "entities": []}, {"text": "These discrete kernels are related to convolution kernels, which provides the concept of kernels over discrete structures.", "labels": [], "entities": []}, {"text": "Convolution kernels allow us to treat structural features without explicitly representing the feature vectors from the input object.", "labels": [], "entities": []}, {"text": "That is, convolution kernels are well suited to NLP tasks in terms of both accuracy and concept.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 75, "end_pos": 83, "type": "METRIC", "confidence": 0.9989117383956909}]}, {"text": "Unfortunately, experiments have shown that in some cases there is a critical issue with convolution kernels, especially in NLP tasks.", "labels": [], "entities": []}, {"text": "That is, the over-fitting problem arises if large \"substructures\" are used in the kernel calculations.", "labels": [], "entities": []}, {"text": "As a result, the machine learning approach can never be trained efficiently.", "labels": [], "entities": []}, {"text": "To solve this issue, we generally eliminate large sub-structures from the set of features used.", "labels": [], "entities": []}, {"text": "However, the main reason for using convolution kernels is that we aim to use structural features easily and efficiently.", "labels": [], "entities": []}, {"text": "If use is limited to only very small structures, it negates the advantages of using convolution kernels.", "labels": [], "entities": []}, {"text": "This paper discusses this issue of convolution kernels, and proposes anew method based on statistical feature selection.", "labels": [], "entities": []}, {"text": "The proposed method deals only with those features that are statistically significant for kernel calculation, large significant substructures can be used without over-fitting.", "labels": [], "entities": [{"text": "kernel calculation", "start_pos": 90, "end_pos": 108, "type": "TASK", "confidence": 0.818368673324585}]}, {"text": "Moreover, the proposed method can be executed efficiently by embedding it in an original kernel calculation process by using sub-structure mining algorithms.", "labels": [], "entities": []}, {"text": "In the next section, we provide a brief overview of convolution kernels.", "labels": [], "entities": []}, {"text": "Section 3 discusses one issue of convolution kernels, the main topic of this paper, and introduces some conventional methods for solving this issue.", "labels": [], "entities": []}, {"text": "In Section 4, we propose anew approach based on statistical feature selection to offset the issue of convolution kernels using an example consisting of sequence kernels.", "labels": [], "entities": []}, {"text": "In Section 5, we briefly discuss the application of the proposed method to other convolution kernels.", "labels": [], "entities": []}, {"text": "In Section 6, we compare the performance of conventional methods with that of the proposed method by using real NLP tasks: question classification and sentence modality identification.", "labels": [], "entities": [{"text": "question classification", "start_pos": 123, "end_pos": 146, "type": "TASK", "confidence": 0.8146656453609467}, {"text": "sentence modality identification", "start_pos": 151, "end_pos": 183, "type": "TASK", "confidence": 0.6837036808331808}]}, {"text": "The experimental results described in Section 7 clarify the advantages of the proposed method.", "labels": [], "entities": []}], "datasetContent": [{"text": "We evaluated the performance of the proposed method in actual NLP tasks, namely English question classification (EQC), Japanese question classification (JQC) and sentence modality identification (MI) tasks.", "labels": [], "entities": [{"text": "English question classification (EQC)", "start_pos": 80, "end_pos": 117, "type": "TASK", "confidence": 0.7256307403246561}, {"text": "Japanese question classification (JQC)", "start_pos": 119, "end_pos": 157, "type": "TASK", "confidence": 0.7057172954082489}, {"text": "sentence modality identification (MI) tasks", "start_pos": 162, "end_pos": 205, "type": "TASK", "confidence": 0.80714293888637}]}, {"text": "We compared the proposed method (FSSK) with a conventional method (SK), as discussed in Section 3, and with bag-of-words (BOW) Kernel (BOW-K) as baseline methods.", "labels": [], "entities": [{"text": "FSSK", "start_pos": 33, "end_pos": 37, "type": "METRIC", "confidence": 0.8639592528343201}]}, {"text": "Support Vector Machine (SVM) was selected as the kernel-based classifier for training and classification.", "labels": [], "entities": []}, {"text": "shows some of the parameter values that we used in the comparison.", "labels": [], "entities": []}, {"text": "We set thresholds of \u03c4 = 2.7055 (FSSK1) and \u03c4 = 3.8415 (FSSK2) for the proposed methods; these values represent the 10% and 5% level of significance in the \u03c7 2 distribution with one degree of freedom, which used the \u03c7 2 significant test.", "labels": [], "entities": [{"text": "FSSK1", "start_pos": 33, "end_pos": 38, "type": "METRIC", "confidence": 0.5199781060218811}, {"text": "FSSK2", "start_pos": 56, "end_pos": 61, "type": "METRIC", "confidence": 0.7633873224258423}]}], "tableCaptions": [{"text": " Table 2: Parameter values of proposed kernels and  Support Vector Machines", "labels": [], "entities": [{"text": "Parameter", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9327937960624695}]}, {"text": " Table 3: Results of the Japanese question classification (F-measure)", "labels": [], "entities": [{"text": "Japanese question classification", "start_pos": 25, "end_pos": 57, "type": "TASK", "confidence": 0.7077368299166361}, {"text": "F-measure", "start_pos": 59, "end_pos": 68, "type": "METRIC", "confidence": 0.8824595808982849}]}, {"text": " Table 6: Precision and recall of SK: n = \u221e", "labels": [], "entities": [{"text": "Precision", "start_pos": 10, "end_pos": 19, "type": "METRIC", "confidence": 0.9962884187698364}, {"text": "recall", "start_pos": 24, "end_pos": 30, "type": "METRIC", "confidence": 0.998930037021637}]}, {"text": " Table 4: Results of English question classification (Accuracy)", "labels": [], "entities": [{"text": "English question classification", "start_pos": 21, "end_pos": 52, "type": "TASK", "confidence": 0.7882610956827799}, {"text": "Accuracy", "start_pos": 54, "end_pos": 62, "type": "METRIC", "confidence": 0.9991057515144348}]}, {"text": " Table 5: Results of sentence modality identification (F-measure)", "labels": [], "entities": [{"text": "sentence modality identification", "start_pos": 21, "end_pos": 53, "type": "TASK", "confidence": 0.7606877187887827}]}]}