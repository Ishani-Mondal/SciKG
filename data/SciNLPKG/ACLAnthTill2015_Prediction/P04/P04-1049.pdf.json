{"title": [{"text": "Paragraph-, word-, and coherence-based approaches to sentence ranking: A comparison of algorithm and human performance", "labels": [], "entities": [{"text": "sentence ranking", "start_pos": 53, "end_pos": 69, "type": "TASK", "confidence": 0.7184760719537735}]}], "abstractContent": [{"text": "Sentence ranking is a crucial part of generating text summaries.", "labels": [], "entities": [{"text": "Sentence ranking", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.9427950978279114}, {"text": "generating text summaries", "start_pos": 38, "end_pos": 63, "type": "TASK", "confidence": 0.7302195231119791}]}, {"text": "We compared human sentence rankings obtained in a psycholinguistic experiment to three different approaches to sentence ranking: A simple paragraph-based approach intended as a baseline, two word-based approaches, and two coherence-based approaches.", "labels": [], "entities": []}, {"text": "In the paragraph-based approach, sentences in the beginning of paragraphs received higher importance ratings than other sentences.", "labels": [], "entities": []}, {"text": "The word-based approaches determined sentence rankings based on relative word frequencies (Luhn (1958); Salton & Buckley (1988)).", "labels": [], "entities": []}, {"text": "Coherence-based approaches determined sentence rankings based on some property of the coherence structure of a text (Marcu (2000); Page et al. (1998)).", "labels": [], "entities": []}, {"text": "Our results suggest poor performance for the simple paragraph-based approach, whereas word-based approaches perform remarkably well.", "labels": [], "entities": []}, {"text": "The best performance was achieved by a coherence-based approach where coherence structures are represented in a non-tree structure.", "labels": [], "entities": []}, {"text": "Most approaches also outperformed the commercially available MSWord summarizer.", "labels": [], "entities": [{"text": "MSWord summarizer", "start_pos": 61, "end_pos": 78, "type": "TASK", "confidence": 0.6644658148288727}]}], "introductionContent": [{"text": "Automatic generation of text summaries is a natural language engineering application that has received considerable interest, particularly due to the ever-increasing volume of text information available through the internet.", "labels": [], "entities": [{"text": "Automatic generation of text summaries", "start_pos": 0, "end_pos": 38, "type": "TASK", "confidence": 0.8548590540885925}]}, {"text": "The task of a human generating a summary generally involves three subtasks (;): (1) understanding a text; (2) ranking text pieces (sentences, paragraphs, phrases, etc.) for importance; (3) generating anew text (the summary).", "labels": [], "entities": []}, {"text": "Like most approaches to summarization, we are concerned with the second subtask (e.g.;;;;;;;).", "labels": [], "entities": [{"text": "summarization", "start_pos": 24, "end_pos": 37, "type": "TASK", "confidence": 0.9911593794822693}]}, {"text": "Furthermore, we are concerned with obtaining generic rather than query-relevant importance rankings (cf., for that distinction).", "labels": [], "entities": []}, {"text": "We evaluated different approaches to sentence ranking against human sentence rankings.", "labels": [], "entities": [{"text": "sentence ranking", "start_pos": 37, "end_pos": 53, "type": "TASK", "confidence": 0.7020528018474579}]}, {"text": "To obtain human sentence rankings, we asked people to read 15 texts from the Wall Street Journal on a wide variety of topics (e.g. economics, foreign and domestic affairs, political commentaries).", "labels": [], "entities": [{"text": "Wall Street Journal", "start_pos": 77, "end_pos": 96, "type": "DATASET", "confidence": 0.9412508805592855}]}, {"text": "For each of the sentences in the text, they provided a ranking of how important that sentence is with respect to the content of the text, on an integer scale from 1 (not important) to 7 (very important).", "labels": [], "entities": []}, {"text": "The approaches we evaluated area simple paragraph-based approach that serves as a baseline, two word-based algorithms, and two coherencebased approaches . We furthermore evaluated the MSWord summarizer.", "labels": [], "entities": [{"text": "MSWord summarizer", "start_pos": 184, "end_pos": 201, "type": "DATASET", "confidence": 0.890338808298111}]}], "datasetContent": [{"text": "In order to test algorithm performance, we compared algorithm sentence rankings to human sentence rankings.", "labels": [], "entities": []}, {"text": "This section describes the experiments we conducted.", "labels": [], "entities": []}, {"text": "In Experiment 1, the texts were presented with paragraph breaks; in Experiment 2, the texts were presented without paragraph breaks.", "labels": [], "entities": []}, {"text": "This was done to control for the effect of paragraph information on human sentence rankings.", "labels": [], "entities": []}, {"text": "15 participants from the MIT community were paid for their participation.", "labels": [], "entities": []}, {"text": "All were native speakers of English and were na\u00efve as to the purpose of the study (i.e. none of the subjects was familiar with theories of coherence in natural language, for example).", "labels": [], "entities": []}, {"text": "Participants were asked to read 15 texts from the Wall Street Journal, and, for each sentence in each text, to provide a ranking of how important that sentence is with respect to the content of the text, on an integer scale from 1 to 7 (1 = not important; 7 = very important).", "labels": [], "entities": [{"text": "Wall Street Journal", "start_pos": 50, "end_pos": 69, "type": "DATASET", "confidence": 0.9519129792849222}]}, {"text": "The texts were selected so sentence number importance ranking NoParagraph WithParagraph.", "labels": [], "entities": []}, {"text": "Human ranking results for one text (wsj_1306). that there was a coherence tree annotation available in 's database.", "labels": [], "entities": []}, {"text": "Text lengths for the 15 texts we selected ranged from 130 to 901 words (5 to 47 sentences); average text length was 442 words (20 sentences), median was 368 words (16 sentences).", "labels": [], "entities": []}, {"text": "Additionally, texts were selected so that they were about as diverse topics as possible.", "labels": [], "entities": []}, {"text": "The experiment was conducted in front of personal computers.", "labels": [], "entities": []}, {"text": "Texts were presented in a web browser as one webpage per text; for some texts, participants had to scroll to seethe whole text.", "labels": [], "entities": []}, {"text": "Each sentence was presented on anew line.", "labels": [], "entities": []}, {"text": "Paragraph breaks were indicated by empty lines; this was pointed out to the participants during the instructions for the experiment.", "labels": [], "entities": []}, {"text": "The method was the same as in Experiment 1, except that texts in Experiment 2 did not include paragraph information.", "labels": [], "entities": []}, {"text": "Each sentence was presented on anew line.", "labels": [], "entities": []}, {"text": "None of the 15 participants who participated in Experiment 2 had participated in Experiment 1.", "labels": [], "entities": []}, {"text": "Human sentence rankings did not differ significantly between Experiment 1 and Experiment 2 for any of the 15 texts (all Fs < 1).", "labels": [], "entities": []}, {"text": "This suggests that paragraph information does not have a big effect on human sentence rankings, at least not for the 15 texts that we examined.", "labels": [], "entities": []}, {"text": "shows the results from both experiments for one text.", "labels": [], "entities": []}, {"text": "We compared human sentence rankings to different algorithmic approaches.", "labels": [], "entities": []}, {"text": "The paragraphbased rankings do not provide scaled importance rankings but only \"important\" vs. \"not important\".", "labels": [], "entities": []}, {"text": "Therefore, in order to compare human rankings to the paragraph-based baseline approach, we calculated point biserial correlations (cf.).", "labels": [], "entities": []}, {"text": "We obtained significant correlations between paragraph-based rankings and human rankings only for one of the 15 texts.", "labels": [], "entities": []}, {"text": "All other algorithms provided scaled importance rankings.", "labels": [], "entities": []}, {"text": "Many evaluations of scalable sentence ranking algorithms are based on precision/recall/Fscores (e.g.;).", "labels": [], "entities": [{"text": "scalable sentence ranking", "start_pos": 20, "end_pos": 45, "type": "TASK", "confidence": 0.5963389277458191}, {"text": "precision", "start_pos": 70, "end_pos": 79, "type": "METRIC", "confidence": 0.9988065958023071}, {"text": "recall", "start_pos": 80, "end_pos": 86, "type": "METRIC", "confidence": 0.7369973063468933}, {"text": "Fscores", "start_pos": 87, "end_pos": 94, "type": "METRIC", "confidence": 0.816181480884552}]}, {"text": "However, argue that such measures are inadequate because they only distinguish between hits and misses or false alarms, but do not account fora degree of agreement.", "labels": [], "entities": []}, {"text": "For example, imagine a situation where the human ranking fora given sentence is \"7\" (\"very important\") on an integer scale ranging from 1 to 7, and Algorithm A gives the same sentence a ranking of \"7\" on the same scale, Algorithm B gives a ranking of \"6\", and Algorithm C gives a ranking of \"2\".", "labels": [], "entities": []}, {"text": "Intuitively, Algorithm B, although it does not reach perfect performance, still performs better than Algorithm C. Precision/recall/F-scores do not account for that difference and would rate Algorithm A as \"hit\" but Algorithm B as well as Algorithm C as \"miss\".", "labels": [], "entities": [{"text": "Precision", "start_pos": 114, "end_pos": 123, "type": "METRIC", "confidence": 0.9691847562789917}, {"text": "recall", "start_pos": 124, "end_pos": 130, "type": "METRIC", "confidence": 0.7741709351539612}, {"text": "F-scores", "start_pos": 131, "end_pos": 139, "type": "METRIC", "confidence": 0.6311624050140381}]}, {"text": "In order to collect performance measures that are more adequate to the evaluation of scaled importance rankings, we computed Spearman's rank correlation coefficients.", "labels": [], "entities": []}, {"text": "The rank correlation coefficients were corrected for tied ranks because in our rankings it was possible for more than one sentence to have the same importance rank, i.e. to have tied ranks;).", "labels": [], "entities": []}, {"text": "In addition to evaluating word-based and coherence-based algorithms, we evaluated one commercially available summarizer, the MSWord summarizer, against human sentence rankings.", "labels": [], "entities": []}, {"text": "Our reason for including an evaluation of the MSWord summarizer was to have a more useful baseline for scalable sentence rankings than the paragraph-based approach provides.'s algorithm where we calculated sentence rankings as the average of the rankings of all discourse segments that constitute that sentence; for MarcuMin, sentence rankings were the minimum of the rankings of all discourse segments in that sentence; for MarcuMax we selected the maximum of the rankings of all discourse segments in that sentence.", "labels": [], "entities": []}, {"text": "shows that the MSWord summarizer performed numerically worse than most other algorithms, except MarcuMin.", "labels": [], "entities": [{"text": "MSWord summarizer", "start_pos": 15, "end_pos": 32, "type": "TASK", "confidence": 0.5632512867450714}, {"text": "MarcuMin", "start_pos": 96, "end_pos": 104, "type": "DATASET", "confidence": 0.9486322402954102}]}, {"text": "As mentioned above, human sentence rankings did not differ significantly between Experiment 1 and Experiment 2 for any of the 15 texts (all Fs < 1).", "labels": [], "entities": []}, {"text": "Therefore, in order to lend more power to our statistical tests, we collapsed the data for each text for the WithParagraph and the NoParagraph condition, and treated them as one experiment.", "labels": [], "entities": [{"text": "WithParagraph", "start_pos": 109, "end_pos": 122, "type": "DATASET", "confidence": 0.6833974719047546}]}, {"text": "shows that when the data from Experiments 1 and 2 are collapsed, PageRank", "labels": [], "entities": [{"text": "PageRank", "start_pos": 65, "end_pos": 73, "type": "DATASET", "confidence": 0.8978371024131775}]}], "tableCaptions": []}