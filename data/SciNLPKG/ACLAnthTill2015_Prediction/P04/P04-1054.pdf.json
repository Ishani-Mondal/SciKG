{"title": [{"text": "Dependency Tree Kernels for Relation Extraction", "labels": [], "entities": [{"text": "Relation Extraction", "start_pos": 28, "end_pos": 47, "type": "TASK", "confidence": 0.9686757624149323}]}], "abstractContent": [{"text": "We extend previous work on tree kernels to estimate the similarity between the dependency trees of sentences.", "labels": [], "entities": []}, {"text": "Using this kernel within a Support Vector Machine, we detect and classify relations between entities in the Automatic Content Extraction (ACE) corpus of news articles.", "labels": [], "entities": [{"text": "Automatic Content Extraction (ACE) corpus of news articles", "start_pos": 108, "end_pos": 166, "type": "TASK", "confidence": 0.632815557718277}]}, {"text": "We examine the utility of different features such as Wordnet hypernyms, parts of speech, and entity types, and find that the dependency tree kernel achieves a 20% F1 improvement over a \"bag-of-words\" kernel.", "labels": [], "entities": [{"text": "Wordnet", "start_pos": 53, "end_pos": 60, "type": "DATASET", "confidence": 0.9239702820777893}, {"text": "F1", "start_pos": 163, "end_pos": 165, "type": "METRIC", "confidence": 0.999000608921051}]}], "introductionContent": [{"text": "The ability to detect complex patterns in data is limited by the complexity of the data's representation.", "labels": [], "entities": []}, {"text": "In the case of text, a more structured data source (e.g. a relational database) allows richer queries than does an unstructured data source (e.g. a collection of news articles).", "labels": [], "entities": []}, {"text": "For example, current web search engines would not perform well on the query, \"list all California-based CEOs who have social ties with a United States Senator.\"", "labels": [], "entities": []}, {"text": "Only a structured representation of the data can effectively provide such a list.", "labels": [], "entities": []}, {"text": "The goal of Information Extraction (IE) is to discover relevant segments of information in a data stream that will be useful for structuring the data.", "labels": [], "entities": [{"text": "Information Extraction (IE)", "start_pos": 12, "end_pos": 39, "type": "TASK", "confidence": 0.8553024888038635}]}, {"text": "In the case of text, this usually amounts to finding mentions of interesting entities and the relations that join them, transforming a large corpus of unstructured text into a relational database with entries such as those in IE is commonly viewed as a three stage process: first, an entity tagger detects all mentions of interest; second, coreference resolution resolves disparate mentions of the same entity; third, a relation extractor finds relations between these entities.", "labels": [], "entities": [{"text": "coreference resolution resolves disparate mentions", "start_pos": 340, "end_pos": 390, "type": "TASK", "confidence": 0.9264919638633728}]}, {"text": "Entity tagging has been thoroughly addressed by many statistical machine learning techniques, obtaining greater than 90% F1 on many datasets.", "labels": [], "entities": [{"text": "Entity tagging", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.8047542572021484}, {"text": "F1", "start_pos": 121, "end_pos": 123, "type": "METRIC", "confidence": 0.9997109770774841}]}, {"text": "Coreference resolution is an active area of research not investigated here (Pa-", "labels": [], "entities": [{"text": "Coreference resolution", "start_pos": 0, "end_pos": 22, "type": "TASK", "confidence": 0.9556025266647339}]}], "datasetContent": [{"text": "We extract relations from the Automatic Content Extraction (ACE) corpus provided by the National Institute for Standards and Technology (NIST).", "labels": [], "entities": [{"text": "Automatic Content Extraction (ACE)", "start_pos": 30, "end_pos": 64, "type": "TASK", "confidence": 0.7333276073137919}]}, {"text": "As noted from the distribution of relationship types in the training data), data imbalance and sparsity are potential problems.", "labels": [], "entities": []}, {"text": "In addition to the contiguous and sparse tree kernels, we also implement a bag-of-words kernel, which treats the tree as a vector of features over nodes, disregarding any structural information.", "labels": [], "entities": []}, {"text": "We also create composite kernels by combining the sparse and contiguous kernels with the bagof-words kernel.", "labels": [], "entities": []}, {"text": "have shown that given two kernels K 1 , K 2 , the composite kernel is also a kernel.", "labels": [], "entities": []}, {"text": "We find that this composite kernel improves performance when the Gram matrix G is sparse (i.e. our instances are far apart in the kernel space).", "labels": [], "entities": []}, {"text": "The features used to represent each node are shown in.", "labels": [], "entities": []}, {"text": "After initial experimentation, the set of features we use in the matching function is \u03c6 m (t i ) = {general-pos, entity-type, relationargument}, and the similarity function examines the In our experiments we tested the following five kernels: We also experimented with the function C(v q , v r ), the compatibility function between two feature values.", "labels": [], "entities": []}, {"text": "For example, we can increase the importance of two nodes having the same Wordnet hypernym 2 . If v q , v rare hypernym features, then we can define When \u03b1 > 1, we increase the similarity of nodes that share a hypernym.", "labels": [], "entities": [{"text": "Wordnet", "start_pos": 73, "end_pos": 80, "type": "DATASET", "confidence": 0.9487327933311462}]}, {"text": "We tested a number of weighting schemes, but did not obtain a set of weights that produced consistent significant improvements.", "labels": [], "entities": []}, {"text": "See Section 8 for alternate approaches to setting C. shows the results of each kernel within an SVM.", "labels": [], "entities": []}, {"text": "(We augment the LibSVM 3 implementation to include our dependency tree kernel.)", "labels": [], "entities": []}, {"text": "Note that, although training was done overall 24 relation subtypes, we evaluate only over the 5 high-level relation types.", "labels": [], "entities": []}, {"text": "Thus, classifying a RESIDENCE relation as a LOCATED relation is deemed correct 4 . Note also that K 0 is not included in because of burdensome computational time.", "labels": [], "entities": []}, {"text": "shows that precision is adequate, but recall is low.", "labels": [], "entities": [{"text": "precision", "start_pos": 11, "end_pos": 20, "type": "METRIC", "confidence": 0.9997535347938538}, {"text": "recall", "start_pos": 38, "end_pos": 44, "type": "METRIC", "confidence": 0.9997379183769226}]}, {"text": "This is a result of the aforementioned class imbalancevery few of the training examples are relations, so the classifier is less likely to identify a testing instances as a relation.", "labels": [], "entities": []}, {"text": "Because we treat every pair of mentions in a sentence as a possible relation, our training set contains fewer than 15% positive relation instances.", "labels": [], "entities": []}, {"text": "To remedy this, we retrain each SVMs fora binary classification task.", "labels": [], "entities": [{"text": "binary classification task", "start_pos": 42, "end_pos": 68, "type": "TASK", "confidence": 0.7387116352717081}]}, {"text": "Here, we detect, but do not classify, relations.", "labels": [], "entities": []}, {"text": "This allows us to combine all positive relation instances into one class, which provides us more training samples to estimate the class boundary.", "labels": [], "entities": []}, {"text": "We then threshold our output to achieve an optimal operating point.", "labels": [], "entities": []}, {"text": "As seen in, this method of relation detection outperforms that of the multi-class classifier.", "labels": [], "entities": [{"text": "relation detection", "start_pos": 27, "end_pos": 45, "type": "TASK", "confidence": 0.8933488130569458}]}, {"text": "We then use these binary classifiers in a cascading scheme as follows: First, we use the binary SVM to detect possible relations.", "labels": [], "entities": []}, {"text": "Then, we use the SVM trained only on positive relation instances to classify each predicted relation.", "labels": [], "entities": []}, {"text": "These results are shown in.", "labels": [], "entities": []}, {"text": "The first result of interest is that the sparse tree kernel, K 0 , does not perform as well as the contiguous tree kernel, K 1 . Suspecting that noise was introduced by the non-matching nodes allowed in the sparse tree kernel, we performed the experiment with different values for the decay factor \u03bb = {.9, .5, .1}, but obtained no improvement.", "labels": [], "entities": []}, {"text": "The second result of interest is that all tree kernels outperform the bag-of-words kernel, K 2 , most noticeably in recall performance, implying that the   structural information the tree kernel provides is extremely useful for relation detection.", "labels": [], "entities": [{"text": "recall", "start_pos": 116, "end_pos": 122, "type": "METRIC", "confidence": 0.9646552205085754}, {"text": "relation detection", "start_pos": 228, "end_pos": 246, "type": "TASK", "confidence": 0.9675763249397278}]}, {"text": "Note that the average results reported here are representative of the performance per relation, except for the NEAR relation, which had slightly lower results overall due to its infrequency in training.", "labels": [], "entities": [{"text": "NEAR relation", "start_pos": 111, "end_pos": 124, "type": "METRIC", "confidence": 0.7280386984348297}]}], "tableCaptions": [{"text": " Table 4: Kernel performance comparison.", "labels": [], "entities": [{"text": "Kernel performance comparison", "start_pos": 10, "end_pos": 39, "type": "TASK", "confidence": 0.8289210995038351}]}, {"text": " Table 5: Relation detection performance. (B) de- notes binary classification.", "labels": [], "entities": [{"text": "Relation detection", "start_pos": 10, "end_pos": 28, "type": "TASK", "confidence": 0.8171678781509399}, {"text": "binary classification", "start_pos": 56, "end_pos": 77, "type": "TASK", "confidence": 0.6199134737253189}]}, {"text": " Table 6: Results on the cascading classification. D  and C denote the kernel used for relation detection  and classification, respectively.", "labels": [], "entities": [{"text": "cascading classification", "start_pos": 25, "end_pos": 49, "type": "TASK", "confidence": 0.7940252423286438}, {"text": "relation detection", "start_pos": 87, "end_pos": 105, "type": "TASK", "confidence": 0.871060699224472}]}]}