{"title": [{"text": "Experiments in Parallel-Text Based Grammar Induction", "labels": [], "entities": [{"text": "Parallel-Text Based Grammar Induction", "start_pos": 15, "end_pos": 52, "type": "TASK", "confidence": 0.743628516793251}]}], "abstractContent": [{"text": "This paper discusses the use of statistical word alignment over multiple parallel texts for the identification of string spans that cannot be constituents in one of the languages.", "labels": [], "entities": [{"text": "statistical word alignment", "start_pos": 32, "end_pos": 58, "type": "TASK", "confidence": 0.6444081366062164}]}, {"text": "This information is exploited in monolingual PCFG grammar induction for that language, within an augmented version of the inside-outside algorithm.", "labels": [], "entities": [{"text": "PCFG grammar induction", "start_pos": 45, "end_pos": 67, "type": "TASK", "confidence": 0.6551432311534882}]}, {"text": "Besides the aligned corpus, no other resources are required.", "labels": [], "entities": [{"text": "aligned corpus", "start_pos": 12, "end_pos": 26, "type": "DATASET", "confidence": 0.6497477144002914}]}, {"text": "We discuss an implemented system and present experimental results with an evaluation against the Penn Tree-bank.", "labels": [], "entities": [{"text": "Penn Tree-bank", "start_pos": 97, "end_pos": 111, "type": "DATASET", "confidence": 0.9951504468917847}]}], "introductionContent": [{"text": "There have been a number of recent studies exploiting parallel corpora in bootstrapping of monolingual analysis tools.", "labels": [], "entities": []}, {"text": "In the \"information projection\" approach (e.g.,), statistical word alignment is applied to a parallel corpus of English and some other language for which no tagger/morphological analyzer/chunker etc.", "labels": [], "entities": [{"text": "information projection", "start_pos": 8, "end_pos": 30, "type": "TASK", "confidence": 0.7582692205905914}, {"text": "statistical word alignment", "start_pos": 50, "end_pos": 76, "type": "TASK", "confidence": 0.6434992253780365}]}, {"text": "(henceforth simply: analysis tool) exists.", "labels": [], "entities": []}, {"text": "A high-quality analysis tool is applied to the English text, and the statistical word alignment is used to project a (noisy) target annotation to the version of the text.", "labels": [], "entities": []}, {"text": "Robust learning techniques are then applied to bootstrap an analysis tool for , using the annotations projected with high confidence as the initial training data.", "labels": [], "entities": []}, {"text": "(Confidence of both the English analysis tool and the statistical word alignment is taken into account.)", "labels": [], "entities": [{"text": "statistical word alignment", "start_pos": 54, "end_pos": 80, "type": "TASK", "confidence": 0.5989249249299368}]}, {"text": "The results that have been achieved by this method are very encouraging.", "labels": [], "entities": []}, {"text": "Will the information projection approach also work for less shallow analysis tools, in particular full syntactic parsers?", "labels": [], "entities": [{"text": "information projection", "start_pos": 9, "end_pos": 31, "type": "TASK", "confidence": 0.8368705213069916}]}, {"text": "An obvious issue is that one does not expect the phrase structure representation of English (as produced by state-of-the-art treebank parsers) to carryover to less configurational languages.", "labels": [], "entities": []}, {"text": "Therefore, () extract a more language-independent dependency structure from the English parse as the basis for projection to Chinese.", "labels": [], "entities": []}, {"text": "From the resulting (noisy) dependency treebank, a dependency parser is trained using the techniques of.", "labels": [], "entities": []}, {"text": "() report that the noise in the projected treebank is still a major challenge, suggesting that a future research focus should be on the filtering of (parts of) unreliable trees and statistical word alignment models sensitive to the syntactic projection framework.", "labels": [], "entities": [{"text": "statistical word alignment", "start_pos": 181, "end_pos": 207, "type": "TASK", "confidence": 0.6054816842079163}]}, {"text": "Our hypothesis is that the quality of the resulting parser/grammar for language can be significantly improved if the training method for the parser is changed to accomodate for training data which are in part unreliable.", "labels": [], "entities": []}, {"text": "The experiments we report in this paper focus on a specific part of the problem: we replace standard treebank training with an Expectation-Maximization (EM) algorithm for PCFGs, augmented by weighting factors for the reliability of training data, following the approach of (), who apply it for EM training of a text classifier.", "labels": [], "entities": [{"text": "Expectation-Maximization (EM) algorithm", "start_pos": 127, "end_pos": 166, "type": "METRIC", "confidence": 0.8819663286209106}]}, {"text": "The factors are only sensitive to the constituent/distituent (C/D) status of each span of the string in (cp.).", "labels": [], "entities": []}, {"text": "The C/D status is derived from an aligned parallel corpus in away discussed in section 2.", "labels": [], "entities": []}, {"text": "We use the Europarl corpus, and the statistical word alignment was performed with the GIZA++ toolkit.", "labels": [], "entities": [{"text": "Europarl corpus", "start_pos": 11, "end_pos": 26, "type": "DATASET", "confidence": 0.9960902035236359}, {"text": "statistical word alignment", "start_pos": 36, "end_pos": 62, "type": "TASK", "confidence": 0.5636401275793711}, {"text": "GIZA++ toolkit", "start_pos": 86, "end_pos": 100, "type": "DATASET", "confidence": 0.8671436508496603}]}, {"text": "For the current experiments we assume no preexisting parser for any of the languages, contrary to the information projection scenario.", "labels": [], "entities": [{"text": "information projection", "start_pos": 102, "end_pos": 124, "type": "TASK", "confidence": 0.795778214931488}]}, {"text": "While better absolute results could be expected using one or more parsers for the languages involved, we think that it is important to isolate the usefulness of exploiting just crosslinguistic word order divergences in order to obtain partial prior knowledge about the constituent structure of a language, which is then exploited in an EM learning approach (section 3).", "labels": [], "entities": []}, {"text": "Not using a parser for some languages also makes it possible to compare various language pairs at the same level, and specifically, we can experiment with grammar induction for English exploiting various", "labels": [], "entities": [{"text": "grammar induction", "start_pos": 155, "end_pos": 172, "type": "TASK", "confidence": 0.7401041090488434}, {"text": "English exploiting", "start_pos": 177, "end_pos": 195, "type": "TASK", "confidence": 0.7165200263261795}]}], "datasetContent": [{"text": "We applied GIZA++ ( to word-align parts of the Europarl corpus) for English and all other 10 languages.", "labels": [], "entities": [{"text": "GIZA", "start_pos": 11, "end_pos": 15, "type": "METRIC", "confidence": 0.9570533633232117}, {"text": "Europarl corpus", "start_pos": 47, "end_pos": 62, "type": "DATASET", "confidence": 0.9906219244003296}]}, {"text": "For the experiments we report in this paper, we only used the 1999 debates, with the language pairs of English combined with Finnish, French, German, Greek, Italian, Spanish, and Swedish.", "labels": [], "entities": []}, {"text": "For computing the weight factors we used a twostep process implemented in Perl, which first determines the maximal # -block boundaries (by detecting discontinuities in the sequence of the # -projected words).", "labels": [], "entities": []}, {"text": "Words with fertility \u00a2 \u00a1 whose # -correspondents were non-adjacent (modulo NULLprojections) were treated like zero fertility words, i.e., we viewed them as unreliable indicators of block status (compare.", "labels": [], "entities": []}, {"text": "shows the internal representation of the block structure for (6) (compare.", "labels": [], "entities": []}, {"text": "Land R are used for the beginning and end of blocks, when the adjacent boundary zone is empty; land rare used next to non-empty boundary zones.", "labels": [], "entities": []}, {"text": "Words that have correspondents in In the simplest model, we use the factor 0 for spans satisfying the distituent condition underlying hypothesis (4), and factor 1 for all other spans; in other words, parses involving a distituent are cancelled out.", "labels": [], "entities": []}, {"text": "We also experimented with various levels of weight factors: for instance, distituents were assigned factor 0.01, likely distituents factor 0.1, neutral spans 1, and likely constituents factor 2.", "labels": [], "entities": []}, {"text": "Likely constituents are defined as spans for which one end is adjacent to an empty block boundary zone (i.e., there is no zero fertility word in the block boundary zone which could be the actual boundary of constituents in which the block is involved).", "labels": [], "entities": []}, {"text": "Most variations in the weighting scheme did not have a significant effect, but they caused differences in coverage because rules with a probability below a certain threshold were dropped in training.", "labels": [], "entities": [{"text": "coverage", "start_pos": 106, "end_pos": 114, "type": "METRIC", "confidence": 0.9984496831893921}]}, {"text": "Below, we report the results of the 0.01-0.1-1-2 scheme, which had a reasonably high coverage on the test data.", "labels": [], "entities": [{"text": "coverage", "start_pos": 85, "end_pos": 93, "type": "METRIC", "confidence": 0.9935644268989563}]}, {"text": "the normal sequence are encoded as *, zero fertility words as -; A and B are used for the first block in a sentence instead of Land R, unless it arises from \"relocation\", which increases likelihood for constituent status (likewise for the last block: Y and Z).", "labels": [], "entities": [{"text": "likelihood", "start_pos": 187, "end_pos": 197, "type": "METRIC", "confidence": 0.9553713202476501}]}, {"text": "Since we are interested only in first-order blocks here, the compact string-based representation is sufficient.", "labels": [], "entities": []}, {"text": "The second step for computing the weight factors creates a chart of all string spans over the given sentence and marks for each span whether it is a distituent, possible constituent or likely distituent, based on the location of boundary symbols.", "labels": [], "entities": []}, {"text": "(For instance zu Baringdorf has the is marked as a distituent; the floor and has the floor are marked as likely constituents.)", "labels": [], "entities": []}, {"text": "The tests are implemented as simple regular expressions.", "labels": [], "entities": []}, {"text": "The chart of weight factors is represented as an array which is stored in the training corpus file along with the sentences.", "labels": [], "entities": [{"text": "training corpus file", "start_pos": 78, "end_pos": 98, "type": "DATASET", "confidence": 0.7810587684313456}]}, {"text": "We combine the weight factors from various languages, since each of them may contribute distinct (non-)constituent information.", "labels": [], "entities": []}, {"text": "The inside-outside algorithm reads in the weight factor array and uses it in the computation of expected rule counts.", "labels": [], "entities": []}, {"text": "We used the probability of the statistical word alignment as a confidence measure to filter out unreliable training sentences.", "labels": [], "entities": [{"text": "statistical word alignment", "start_pos": 31, "end_pos": 57, "type": "TASK", "confidence": 0.653498649597168}]}, {"text": "Due to the conservative nature of the information we extract from the alignment, the results indicate however that filtering is not necessary.", "labels": [], "entities": []}, {"text": "For evaluation, we ran the PCFG resulting from training with the Viterbi algorithm 10 on parts of the Wall Street Journal (WSJ) section of the Penn Treebank and compared the tree structure for the most We used the LoPar parser  probable parse for the test sentences against the gold standard treebank annotation.", "labels": [], "entities": [{"text": "Wall Street Journal (WSJ) section of the Penn Treebank", "start_pos": 102, "end_pos": 156, "type": "DATASET", "confidence": 0.9449895728718151}]}, {"text": "(Note that one does not necessarily expect that an induced grammar will match a treebank annotation, but it may at least serve as a basis for comparison.)", "labels": [], "entities": []}, {"text": "The evaluation criteria we apply are unlabeled bracketing precision and recall (and crossing brackets).", "labels": [], "entities": [{"text": "precision", "start_pos": 58, "end_pos": 67, "type": "METRIC", "confidence": 0.9834251999855042}, {"text": "recall", "start_pos": 72, "end_pos": 78, "type": "METRIC", "confidence": 0.9994186162948608}]}, {"text": "We follow an evaluation criterion that, footnote 3) discuss for the evaluation of a not fully supervised grammar induction approach based on a binary grammar topology: bracket multiplicity (i.e., non-branching projections) is collapsed into a single set of brackets (since what is relevant is the constituent structure that was induced).", "labels": [], "entities": []}, {"text": "11 For comparison, we provide baseline results that a uniform left-branching structure and a uniform right-branching structure (which encodes some nontrivial information about English syntax) would give rise to.", "labels": [], "entities": []}, {"text": "As an upper boundary for the performance a binary grammar can achieve on the WSJ, we present the scores fora minimal binarized extension of the gold-standard annotation.", "labels": [], "entities": [{"text": "WSJ", "start_pos": 77, "end_pos": 80, "type": "DATASET", "confidence": 0.94680255651474}]}, {"text": "The results we can report at this point are based on a comparatively small training set.", "labels": [], "entities": []}, {"text": "12 So, it maybe too early for conclusive results.", "labels": [], "entities": []}, {"text": "(An issue that arises with the small training set is that smoothing techniques would be required to avoid overtraining, but these tend to dominate the test application, so the effect of the parallel-corpus based information cannot be seen so clearly.)", "labels": [], "entities": []}, {"text": "But we think that the results are rather encouraging.", "labels": [], "entities": []}, {"text": "As the table in shows, the PCFG we induced based on the parallel-text derived weight factors reaches 57.5 as the F -score of unlabeled precision and recall on sentences up to length 10.", "labels": [], "entities": [{"text": "PCFG", "start_pos": 27, "end_pos": 31, "type": "DATASET", "confidence": 0.7839531302452087}, {"text": "F -score", "start_pos": 113, "end_pos": 121, "type": "METRIC", "confidence": 0.992409328619639}, {"text": "precision", "start_pos": 135, "end_pos": 144, "type": "METRIC", "confidence": 0.9869006276130676}, {"text": "recall", "start_pos": 149, "end_pos": 155, "type": "METRIC", "confidence": 0.9987711310386658}]}, {"text": "We show the scores for an experiment without smoothing, trained on c.", "labels": [], "entities": []}, {"text": "Since no smoothing was applied, the resulting coverage (with lowprobability rules removed) on the test set is about 80%.", "labels": [], "entities": [{"text": "coverage", "start_pos": 46, "end_pos": 54, "type": "METRIC", "confidence": 0.9981295466423035}]}, {"text": "It took 74 iterations of the inside-outside algorithm to train the weight-factor-trained grammar; the final version has 1005 rules.", "labels": [], "entities": []}, {"text": "For comparison we induced another PCFG based on the same X-bar topology without using the weight factor mechanism.", "labels": [], "entities": []}, {"text": "This grammar ended up with 1145 rules after 115 iterations.", "labels": [], "entities": []}, {"text": "The F -score is only 51.3 (while the coverage is the same as for the weight-factor-trained grammar).", "labels": [], "entities": [{"text": "F -score", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.9949690103530884}, {"text": "coverage", "start_pos": 37, "end_pos": 45, "type": "METRIC", "confidence": 0.993935763835907}]}, {"text": "shows the complete set of (singular) \"NP rules\" emerging from the weight-factor-trained grammar, which are remarkably well-behaved, in particular when we compare them to the corresponding rules from the PCFG induced in the standard way (figure 7).", "labels": [], "entities": []}, {"text": "(XP categories are written as POS-TAG\u00a1 -P, X head categories are written as POS-TAG\u00a1 -0 -so the most probable NP productions in  Of course we are comparing an unsupervised technique with a mildly supervised technique; but the results indicate that the relatively subtle information discussed in section 2 seems to be indeed very useful.", "labels": [], "entities": []}], "tableCaptions": []}