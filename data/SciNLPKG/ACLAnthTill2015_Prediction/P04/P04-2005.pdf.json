{"title": [{"text": "Automatic Acquisition of English Topic Signatures Based on a Second Language", "labels": [], "entities": []}], "abstractContent": [{"text": "We present a novel approach for automatically acquiring English topic signatures.", "labels": [], "entities": [{"text": "automatically acquiring English topic signatures", "start_pos": 32, "end_pos": 80, "type": "TASK", "confidence": 0.6403212606906891}]}, {"text": "Given a particular concept, or word sense, a topic signature is a set of words that tend to co-occur with it.", "labels": [], "entities": []}, {"text": "Topic signatures can be useful in a number of Natural Language Processing (NLP) applications, such as Word Sense Disambiguation (WSD) and Text Summarisation.", "labels": [], "entities": [{"text": "Topic signatures", "start_pos": 0, "end_pos": 16, "type": "TASK", "confidence": 0.7446998059749603}, {"text": "Word Sense Disambiguation (WSD)", "start_pos": 102, "end_pos": 133, "type": "TASK", "confidence": 0.7409348636865616}, {"text": "Text Summarisation", "start_pos": 138, "end_pos": 156, "type": "TASK", "confidence": 0.8132575154304504}]}, {"text": "Our method takes advantage of the different way in which word senses are lexicalised in English and Chinese, and also exploits the large amount of Chinese text available in corpora and on the Web.", "labels": [], "entities": []}, {"text": "We evaluated the topic signatures on a WSD task, where we trained a second-order vector co-occurrence algorithm on standard WSD datasets, with promising results.", "labels": [], "entities": [{"text": "WSD task", "start_pos": 39, "end_pos": 47, "type": "TASK", "confidence": 0.7991790473461151}, {"text": "WSD datasets", "start_pos": 124, "end_pos": 136, "type": "DATASET", "confidence": 0.8078175187110901}]}], "introductionContent": [{"text": "Lexical knowledge is crucial for many NLP tasks.", "labels": [], "entities": []}, {"text": "Huge efforts and investments have been made to build repositories with different types of knowledge.", "labels": [], "entities": []}, {"text": "Many of them have proved useful, such as WordNet ().", "labels": [], "entities": [{"text": "WordNet", "start_pos": 41, "end_pos": 48, "type": "DATASET", "confidence": 0.9644174575805664}]}, {"text": "However, in some areas, such as WSD, manually created knowledge bases seem never to satisfy the huge requirement by supervised machine learning systems.", "labels": [], "entities": [{"text": "WSD", "start_pos": 32, "end_pos": 35, "type": "TASK", "confidence": 0.8842697143554688}]}, {"text": "This is the so-called knowledge acquisition bottleneck.", "labels": [], "entities": [{"text": "knowledge acquisition", "start_pos": 22, "end_pos": 43, "type": "TASK", "confidence": 0.8587474822998047}]}, {"text": "As an alternative, automatic or semi-automatic acquisition methods have been proposed to tackle the bottleneck.", "labels": [], "entities": []}, {"text": "For example, tried to automatically extract topic signatures by querying a search engine using monosemous synonyms or other knowledge associated with a concept defined in WordNet.", "labels": [], "entities": [{"text": "extract topic signatures", "start_pos": 36, "end_pos": 60, "type": "TASK", "confidence": 0.6131369670232137}, {"text": "WordNet", "start_pos": 171, "end_pos": 178, "type": "DATASET", "confidence": 0.9273148775100708}]}, {"text": "The Web provides further ways of overcoming the bottleneck.", "labels": [], "entities": []}, {"text": "presented a method enabling automatic acquisition of sensetagged corpora, based on WordNet and an Internet search engine.", "labels": [], "entities": [{"text": "automatic acquisition of sensetagged corpora", "start_pos": 28, "end_pos": 72, "type": "TASK", "confidence": 0.6519611001014709}, {"text": "WordNet", "start_pos": 83, "end_pos": 90, "type": "DATASET", "confidence": 0.9710041880607605}]}, {"text": "presented another interesting proposal which turns to Web users to produce sense-tagged corpora.", "labels": [], "entities": []}, {"text": "Another type of method, which exploits differences between languages, has shown great promise.", "labels": [], "entities": []}, {"text": "For example, some work has been done based on the assumption that mappings of words and meanings are different in different languages.", "labels": [], "entities": []}, {"text": "proposed a method which automatically produces sense-tagged data using parallel bilingual corpora.", "labels": [], "entities": []}, {"text": "presented an unsupervised method for WSD using the same type of resource.", "labels": [], "entities": [{"text": "WSD", "start_pos": 37, "end_pos": 40, "type": "TASK", "confidence": 0.9607245922088623}]}, {"text": "One problem with relying on bilingual corpora for data collection is that bilingual corpora are rare, and aligned bilingual corpora are even rarer.", "labels": [], "entities": []}, {"text": "Mining the Web for bilingual text is not likely to provide sufficient quantities of high quality data.", "labels": [], "entities": []}, {"text": "Another problem is that if two languages are closely related, data for some words cannot be collected because different senses of polysemous words in one language often translate to the same word in the other.", "labels": [], "entities": []}, {"text": "In this paper, we present a novel approach for automatically acquiring topic signatures (see, which also adopts the cross-lingual paradigm.", "labels": [], "entities": [{"text": "automatically acquiring topic signatures", "start_pos": 47, "end_pos": 87, "type": "TASK", "confidence": 0.5889590755105019}]}, {"text": "To solve the problem of different senses not being distinguishable mentioned in the previous paragraph, we chose a language very distant to EnglishChinese, since the more distant two languages are, the more likely that senses are lexicalised differently).", "labels": [], "entities": [{"text": "EnglishChinese", "start_pos": 140, "end_pos": 154, "type": "DATASET", "confidence": 0.9499619007110596}]}, {"text": "Because our approach only uses Chinese monolingual text, we also avoid the problem of shortage of aligned bilingual corpora.", "labels": [], "entities": []}, {"text": "We build the topic signatures by using Chinese-English and EnglishChinese bilingual lexicons and a large amount of Chinese text, which can be collected either from the Web or from Chinese corpora.", "labels": [], "entities": []}, {"text": "Since topic signatures are potentially good training data for WSD algorithms, we setup a task to disambiguate 6 words using a WSD algorithm similar to context-group discrimination.", "labels": [], "entities": [{"text": "WSD", "start_pos": 62, "end_pos": 65, "type": "TASK", "confidence": 0.9795118570327759}]}, {"text": "The results show that our topic signatures are useful for WSD.", "labels": [], "entities": [{"text": "WSD", "start_pos": 58, "end_pos": 61, "type": "TASK", "confidence": 0.9845955967903137}]}, {"text": "The remainder of the paper is organised as follows.", "labels": [], "entities": []}, {"text": "Section 2 describes the process of acquisition of the topic signatures.", "labels": [], "entities": []}, {"text": "Section 3 demonstrates the application of this resource on WSD, and presents the results of our experiments.", "labels": [], "entities": [{"text": "WSD", "start_pos": 59, "end_pos": 62, "type": "TASK", "confidence": 0.7865151762962341}]}, {"text": "Section 4 discusses factors that could affect the acquisition process and then we conclude in Section 5.", "labels": [], "entities": [{"text": "acquisition process", "start_pos": 50, "end_pos": 69, "type": "TASK", "confidence": 0.9016101360321045}]}], "datasetContent": [{"text": "We tested our system on 6 nouns, as shown in Table 2, which also shows information on the training and test data we used in the experiments.", "labels": [], "entities": []}, {"text": "The training sets for motion, plant and tank are topic signatures extracted from the CGC; whereas those for bass, crane and palm are obtained from both CGC and the People's Daily On-line.", "labels": [], "entities": [{"text": "CGC", "start_pos": 85, "end_pos": 88, "type": "DATASET", "confidence": 0.9815372228622437}, {"text": "CGC", "start_pos": 152, "end_pos": 155, "type": "DATASET", "confidence": 0.9863944053649902}, {"text": "People's Daily On-line", "start_pos": 164, "end_pos": 186, "type": "DATASET", "confidence": 0.9134821742773056}]}, {"text": "This is because the Chinese translation equivalents of senses of the latter 3 words don't occur frequently in CGC, and we had to seek more data from the Web.", "labels": [], "entities": []}, {"text": "Where applicable, we also limited the training data of each sense to a maximum of 6, 000 instances for efficiency purposes.", "labels": [], "entities": []}, {"text": "The test data is a binary sense-tagged corpus, the TWA Sense Tagged Data Set, manually produced by Rada Mihalcea and Li Yang, from text drawn from the British National Corpus.", "labels": [], "entities": [{"text": "TWA Sense Tagged Data Set", "start_pos": 51, "end_pos": 76, "type": "DATASET", "confidence": 0.9169730186462403}, {"text": "British National Corpus", "start_pos": 151, "end_pos": 174, "type": "DATASET", "confidence": 0.9413531223932902}]}, {"text": "We calculated a 'supervised' baseline from the annotated data by assigning the most frequent sense in the test data to all instances, although it could be argued that the baseline for unsupervised disambiguation should be computed by randomly assigning one of the senses to instances (e.g. it would be 50% for words with two senses).", "labels": [], "entities": []}, {"text": "According to our previous description, the 2, 500 most frequent concepts were selected as dimensions.", "labels": [], "entities": []}, {"text": "The number of features in a Concept Space depends on how many unique concepts actually occur in the training sets.", "labels": [], "entities": []}, {"text": "Larger amounts of training data tend to yield a larger set of features.", "labels": [], "entities": []}, {"text": "At the end of the training stage, for each sense, a sense vector was produced.", "labels": [], "entities": []}, {"text": "Then we lemmatised the test data and extracted a set of context vectors for all instances in the same way.", "labels": [], "entities": []}, {"text": "For each instance in the test data, the cosine scores between its context vector and all possible sense vectors acquired through training were calculated and compared, and then the sense scoring the highest was allocated to the instance.", "labels": [], "entities": []}, {"text": "The results of the experiments are also given in (last column).", "labels": [], "entities": []}, {"text": "Using our topic signatures, we obtained good results: the accuracy for all words exceeds the supervised baseline, except for motion which approaches it.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 58, "end_pos": 66, "type": "METRIC", "confidence": 0.9992169141769409}]}, {"text": "The Chinese translations for motion are also ambiguous, which might be the reason that our WSD system performed less well on this word.", "labels": [], "entities": [{"text": "WSD", "start_pos": 91, "end_pos": 94, "type": "TASK", "confidence": 0.6940014362335205}]}, {"text": "However, as we mentioned, to avoid this problem, we could have expanded motion's Chinese translations, using their Chinese monosemous synonyms, when we query the Chinese corpus or the Web.", "labels": [], "entities": []}, {"text": "Considering our system is unsupervised, the results are very promising.", "labels": [], "entities": []}, {"text": "An indicative comparison might be with the work of Mihalcea (2003), who with a very different approach achieved similar performance on the same test data.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2:Sizes of the training data and the test data, baseline  performance, and the results.", "labels": [], "entities": []}]}