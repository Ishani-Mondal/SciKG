{"title": [{"text": "Learning Noun Phrase Anaphoricity to Improve Coreference Resolution: Issues in Representation and Optimization", "labels": [], "entities": [{"text": "Learning Noun Phrase Anaphoricity", "start_pos": 0, "end_pos": 33, "type": "TASK", "confidence": 0.6412337869405746}, {"text": "Coreference Resolution", "start_pos": 45, "end_pos": 67, "type": "TASK", "confidence": 0.8303969502449036}]}], "abstractContent": [{"text": "Knowledge of the anaphoricity of a noun phrase might be profitably exploited by a coreference system to bypass the resolution of non-anaphoric noun phrases.", "labels": [], "entities": []}, {"text": "Perhaps surprisingly, recent attempts to incorporate automatically acquired anaphoricity information into coreference systems, however, have led to the degradation in resolution performance.", "labels": [], "entities": []}, {"text": "This paper examines several key issues in computing and using anaphoricity information to improve learning-based coreference systems.", "labels": [], "entities": []}, {"text": "In particular , we present anew corpus-based approach to anaphoricity determination.", "labels": [], "entities": [{"text": "anaphoricity determination", "start_pos": 57, "end_pos": 83, "type": "TASK", "confidence": 0.746450811624527}]}, {"text": "Experiments on three standard coreference data sets demonstrate the effectiveness of our approach.", "labels": [], "entities": []}], "introductionContent": [{"text": "Noun phrase coreference resolution, the task of determining which noun phrases (NPs) in a text refer to the same real-world entity, has long been considered an important and difficult problem in natural language processing.", "labels": [], "entities": [{"text": "Noun phrase coreference resolution", "start_pos": 0, "end_pos": 34, "type": "TASK", "confidence": 0.7883913815021515}]}, {"text": "Identifying the linguistic constraints on when two NPs can co-refer remains an active area of research in the community.", "labels": [], "entities": []}, {"text": "One significant constraint on coreference, the non-anaphoricity constraint, specifies that a nonanaphoric NP cannot be coreferent with any of its preceding NPs in a given text.", "labels": [], "entities": [{"text": "coreference", "start_pos": 30, "end_pos": 41, "type": "TASK", "confidence": 0.9723920226097107}]}, {"text": "Given the potential usefulness of knowledge of (non-)anaphoricity for coreference resolution, anaphoricity determination has been studied fairly extensively.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 70, "end_pos": 92, "type": "TASK", "confidence": 0.9822415113449097}, {"text": "anaphoricity determination", "start_pos": 94, "end_pos": 120, "type": "TASK", "confidence": 0.9008678793907166}]}, {"text": "One common approach involves the design of heuristic rules to identify specific types of (non-)anaphoric NPs such as pleonastic pronouns (e.g.,,,, Denber (1998)) and definite descriptions (e.g.,).", "labels": [], "entities": []}, {"text": "More recently, the problem has been tackled using unsupervised (e.g.,) and supervised (e.g.,,) approaches.", "labels": [], "entities": []}, {"text": "Interestingly, existing machine learning approaches to coreference resolution have performed reasonably well without anaphoricity determination (e.g.,,,,).", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 55, "end_pos": 77, "type": "TASK", "confidence": 0.9708811044692993}]}, {"text": "Nevertheless, there is empirical evidence that resolution systems might further be improved with anaphoricity information.", "labels": [], "entities": [{"text": "resolution", "start_pos": 47, "end_pos": 57, "type": "TASK", "confidence": 0.9563004374504089}]}, {"text": "For instance, our coreference system mistakenly identifies an antecedent for many non-anaphoric common nouns in the absence of anaphoricity information).", "labels": [], "entities": []}, {"text": "Our goal in this paper is to improve learningbased coreference systems using automatically computed anaphoricity information.", "labels": [], "entities": []}, {"text": "In particular, we examine two important, yet largely unexplored, issues in anaphoricity determination for coreference resolution: representation and optimization.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 106, "end_pos": 128, "type": "TASK", "confidence": 0.9690902531147003}]}, {"text": "How should the computed anaphoricity information be used by a coreference system?", "labels": [], "entities": []}, {"text": "From a linguistic perspective, knowledge of nonanaphoricity is most naturally represented as \"bypassing\" constraints, with which the coreference system bypasses the resolution of NPs that are determined to be non-anaphoric.", "labels": [], "entities": []}, {"text": "But for learning-based coreference systems, anaphoricity information can be simply and naturally accommodated into the machine learning framework by including it as a feature in the instance representation.", "labels": [], "entities": []}, {"text": "Should the anaphoricity determination procedure be developed independently of the coreference system that uses the computed anaphoricity information (local optimization), or should it be optimized with respect to coreference performance (global optimization)?", "labels": [], "entities": []}, {"text": "The principle of software modularity calls for local optimization.", "labels": [], "entities": []}, {"text": "However, if the primary goal is to improve coreference performance, global optimization appears to be the preferred choice.", "labels": [], "entities": [{"text": "global optimization", "start_pos": 68, "end_pos": 87, "type": "TASK", "confidence": 0.6958696693181992}]}, {"text": "Existing work on anaphoricity determination for anaphora/coreference resolution can be characterized along these two dimensions.", "labels": [], "entities": [{"text": "anaphoricity determination", "start_pos": 17, "end_pos": 43, "type": "TASK", "confidence": 0.6881090104579926}, {"text": "coreference resolution", "start_pos": 57, "end_pos": 79, "type": "TASK", "confidence": 0.8512536287307739}]}, {"text": "Interestingly, most existing work employs constraintbased, locally-optimized methods (e.g., and), leaving the remaining three possibilities largely unexplored.", "labels": [], "entities": []}, {"text": "In particular, to our knowledge, there have been no attempts to (1) globally optimize an anaphoricity determination procedure for coreference performance and (2) incorporate anaphoricity into coreference systems as a feature.", "labels": [], "entities": []}, {"text": "Consequently, as part of our investigation, we propose anew corpus-based method for achieving global optimization and experiment with representing anaphoricity as a feature in the coreference system.", "labels": [], "entities": [{"text": "global optimization", "start_pos": 94, "end_pos": 113, "type": "TASK", "confidence": 0.7037904858589172}]}, {"text": "In particular, we systematically evaluate all four combinations of local vs. global optimization and constraint-based vs. feature-based representation of anaphoricity information in terms of their effectiveness in improving a learning-based coreference system.", "labels": [], "entities": []}, {"text": "Results on three standard coreference data sets are somewhat surprising: our proposed globally-optimized method, when used in conjunction with the constraint-based representation, outperforms not only the commonly-adopted locallyoptimized approach but also its seemingly more natural feature-based counterparts.", "labels": [], "entities": []}, {"text": "The rest of the paper is structured as follows.", "labels": [], "entities": []}, {"text": "Section 2 focuses on optimization issues, discussing locally-and globally-optimized approaches to anaphoricity determination.", "labels": [], "entities": [{"text": "anaphoricity determination", "start_pos": 98, "end_pos": 124, "type": "TASK", "confidence": 0.707096666097641}]}, {"text": "In Section 3, we give an overview of the standard machine learning framework for coreference resolution.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 81, "end_pos": 103, "type": "TASK", "confidence": 0.9753401875495911}]}, {"text": "Sections 4 and 5 present the experimental setup and evaluation results, respectively.", "labels": [], "entities": []}, {"text": "We examine the features that are important to anaphoricity determination in Section 6 and conclude in Section 7.", "labels": [], "entities": [{"text": "anaphoricity determination", "start_pos": 46, "end_pos": 72, "type": "TASK", "confidence": 0.8480589389801025}]}], "datasetContent": [{"text": "In Section 2, we examined how to construct locallyand globally-optimized anaphoricity models.", "labels": [], "entities": []}, {"text": "Recall that, for each of these two types of models, the resulting (non-)anaphoricity information can be used by a learning-based coreference system either as hard bypassing constraints or as a feature.", "labels": [], "entities": []}, {"text": "Hence, given a coreference system that implements the twostep learning approach shown above, we will be able to evaluate the four different combinations of computing and using anaphoricity information for improving the coreference system described in the introduction.", "labels": [], "entities": []}, {"text": "Before presenting evaluation details, we will describe the experimental setup.", "labels": [], "entities": []}, {"text": "In all of our experiments, we use our learning-based coreference system (Ng and Cardie, 2002b).", "labels": [], "entities": []}, {"text": "In both the locally-optimized and the globallyoptimized approaches to anaphoricity determination described in Section 2, an instance is represented by 37 features that are specifically designed for distinguishing anaphoric and non-anaphoric NPs.", "labels": [], "entities": [{"text": "anaphoricity determination", "start_pos": 70, "end_pos": 96, "type": "TASK", "confidence": 0.709580585360527}]}, {"text": "Space limitations preclude a description of these features; see Ng and Cardie (2002a) for details.", "labels": [], "entities": []}, {"text": "For training coreference classifiers and locally-optimized anaphoricity models, we use both RIPPER and MaxEnt as the underlying learning algorithms.", "labels": [], "entities": []}, {"text": "However, for training globally-optimized anaphoricity models, RIPPER is always used in conjunction with Method 1 and MaxEnt with Method 2, as described in Section 2.2.", "labels": [], "entities": [{"text": "RIPPER", "start_pos": 62, "end_pos": 68, "type": "METRIC", "confidence": 0.9854025840759277}]}, {"text": "In terms of setting learner-specific parameters, we use default values for all RIPPER parameters unless otherwise stated.", "labels": [], "entities": []}, {"text": "For MaxEnt, we always train the feature-weight parameters with 100 iterations of the improved iterative scaling algorithm (Della), using a Gaussian prior to prevent overfitting).", "labels": [], "entities": []}, {"text": "We use the Automatic Content Extraction (ACE) Phase II data sets.", "labels": [], "entities": [{"text": "Automatic Content Extraction (ACE) Phase II data sets", "start_pos": 11, "end_pos": 64, "type": "DATASET", "confidence": 0.7884243041276932}]}, {"text": "We choose ACE rather than the more widely-used MUC corpus: Statistics of the three ACE data sets ACE provides much more labeled data for both training and testing.", "labels": [], "entities": [{"text": "MUC corpus", "start_pos": 47, "end_pos": 57, "type": "DATASET", "confidence": 0.8851087391376495}, {"text": "ACE data sets ACE", "start_pos": 83, "end_pos": 100, "type": "DATASET", "confidence": 0.7965393587946892}]}, {"text": "However, our system was setup to perform coreference resolution according to the MUC rules, which are fairly different from the ACE guidelines in terms of the identification of markables as well as evaluation schemes.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 41, "end_pos": 63, "type": "TASK", "confidence": 0.9518473446369171}, {"text": "MUC", "start_pos": 81, "end_pos": 84, "type": "DATASET", "confidence": 0.8612820506095886}, {"text": "ACE", "start_pos": 128, "end_pos": 131, "type": "DATASET", "confidence": 0.8964511752128601}]}, {"text": "Since our goal is to evaluate the effect of anaphoricity information on coreference resolution, we make no attempt to modify our system to adhere to the rules specifically designed for ACE.", "labels": [], "entities": [{"text": "coreference resolution", "start_pos": 72, "end_pos": 94, "type": "TASK", "confidence": 0.973687469959259}]}, {"text": "The coreference corpus is composed of three data sets made up of three different news sources: Broadcast News (BNEWS), Newspaper (NPAPER), and Newswire (NWIRE).", "labels": [], "entities": []}, {"text": "Statistics collected from these data sets are shown in.", "labels": [], "entities": []}, {"text": "For each data set, we train an anaphoricity classifier and a coreference classifier on the (same) set of training texts and evaluate the coreference system on the test texts.", "labels": [], "entities": []}, {"text": "In this section, we will compare the effectiveness of four approaches to anaphoricity determination (see the introduction) in improving our baseline coreference system.", "labels": [], "entities": [{"text": "anaphoricity determination", "start_pos": 73, "end_pos": 99, "type": "TASK", "confidence": 0.7225968092679977}]}], "tableCaptions": [{"text": " Table 1: Statistics of the three ACE data sets", "labels": [], "entities": [{"text": "ACE data sets", "start_pos": 34, "end_pos": 47, "type": "DATASET", "confidence": 0.9445667664210001}]}, {"text": " Table 2: Results of the coreference systems using different approaches to anaphoricity determination on the  three ACE test data sets. Information on which Learner (RIPPER or MaxEnt) is used to train the coreference clas-", "labels": [], "entities": [{"text": "ACE test data sets", "start_pos": 116, "end_pos": 134, "type": "DATASET", "confidence": 0.9762464612722397}]}]}