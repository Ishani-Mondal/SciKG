{"title": [{"text": "Customizing Parallel Corpora at the Document Level", "labels": [], "entities": []}], "abstractContent": [{"text": "Recent research in cross-lingual information retrieval (CLIR) established the need for properly matching the parallel corpus used for query translation to the target corpus.", "labels": [], "entities": [{"text": "cross-lingual information retrieval (CLIR)", "start_pos": 19, "end_pos": 61, "type": "TASK", "confidence": 0.7748264074325562}, {"text": "query translation", "start_pos": 134, "end_pos": 151, "type": "TASK", "confidence": 0.6982996463775635}]}, {"text": "We propose a document-level approach to solving this problem: building a custom-made parallel corpus by automatically assembling it from documents taken from other parallel corpora.", "labels": [], "entities": []}, {"text": "Although the general idea can be applied to any application that uses parallel corpora, we present results for CLIR in the medical domain.", "labels": [], "entities": []}, {"text": "In order to extract the best-matched documents from several parallel corpora, we propose ranking individual documents by using a length-normalized Okapi-based similarity score between them and the target corpus.", "labels": [], "entities": [{"text": "length-normalized Okapi-based similarity score", "start_pos": 129, "end_pos": 175, "type": "METRIC", "confidence": 0.7667299658060074}]}, {"text": "This ranking allows us to discard 50-90% of the training data, while avoiding the performance drop caused by a good but mismatched resource, and even improving CLIR effectiveness by 4-7% when compared to using all available training data.", "labels": [], "entities": []}], "introductionContent": [{"text": "Our recent research in cross-lingual information retrieval (CLIR) established the need for properly matching the parallel corpus used for query translation to the target corpus ().", "labels": [], "entities": [{"text": "cross-lingual information retrieval (CLIR)", "start_pos": 23, "end_pos": 65, "type": "TASK", "confidence": 0.7598031709591547}, {"text": "query translation", "start_pos": 138, "end_pos": 155, "type": "TASK", "confidence": 0.6908031851053238}]}, {"text": "In particular, we showed that using a general purpose machine translation (MT) system such as SYSTRAN, or a general purpose parallel corpus -both of which perform very well for news stories -dramatically fails in the medical domain.", "labels": [], "entities": [{"text": "general purpose machine translation (MT)", "start_pos": 38, "end_pos": 78, "type": "TASK", "confidence": 0.7334450653621128}]}, {"text": "To explore solutions to this problem, we used cosine similarity between training and target corpora as respective weights when building a translation model.", "labels": [], "entities": []}, {"text": "This approach treats a parallel corpus as a homogeneous entity, an entity that is self-consistent in its domain and document quality.", "labels": [], "entities": []}, {"text": "In this paper, we propose that instead of weighting entire resources, we can select individual documents from these corpora in order to build a parallel corpus that is tailor-made to fit a specific target collection.", "labels": [], "entities": []}, {"text": "To avoid confusion, it is helpful to remember that in IR settings the true test data are the queries, not the target documents.", "labels": [], "entities": []}, {"text": "The documents are available off-line and can be (and usually are) used for training and system development.", "labels": [], "entities": []}, {"text": "In other words, by matching the training corpora and the target documents we are not using test data for training.", "labels": [], "entities": []}, {"text": "() also discusses indirectly related work, such as query translation disambiguation and building domain-specific language models for speech recognition.", "labels": [], "entities": [{"text": "query translation disambiguation", "start_pos": 51, "end_pos": 83, "type": "TASK", "confidence": 0.8930210471153259}, {"text": "speech recognition", "start_pos": 133, "end_pos": 151, "type": "TASK", "confidence": 0.7793670892715454}]}, {"text": "We are not aware of any additional related work.", "labels": [], "entities": []}, {"text": "In addition to proposing individual documents as the unit for building custom-made parallel corpora, in this paper we start exploring the criteria used for individual document selection by examining the effect of ranking documents using the length-normalized Okapi-based similarity score between them and the target corpus.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": []}