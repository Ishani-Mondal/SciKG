{"title": [], "abstractContent": [{"text": "Aligning words from sentences which are mutual translations is an important problem in different settings , such as bilingual terminology extraction, Machine Translation, or projection of linguistic features.", "labels": [], "entities": [{"text": "Aligning words from sentences which are mutual translations", "start_pos": 0, "end_pos": 59, "type": "TASK", "confidence": 0.800528347492218}, {"text": "bilingual terminology extraction", "start_pos": 116, "end_pos": 148, "type": "TASK", "confidence": 0.6303763190905253}, {"text": "Machine Translation", "start_pos": 150, "end_pos": 169, "type": "TASK", "confidence": 0.8861669898033142}, {"text": "projection of linguistic features", "start_pos": 174, "end_pos": 207, "type": "TASK", "confidence": 0.8563411235809326}]}, {"text": "Here, we view word alignment as matrix fac-torisation.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 14, "end_pos": 28, "type": "TASK", "confidence": 0.7885234951972961}]}, {"text": "In order to produce proper alignments, we show that factors must satisfy a number of constraints such as orthogonality.", "labels": [], "entities": []}, {"text": "We then propose an algorithm for orthogonal non-negative matrix fac-torisation, based on a probabilistic model of the alignment data, and apply it to word alignment.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 150, "end_pos": 164, "type": "TASK", "confidence": 0.8245352804660797}]}, {"text": "This is illustrated on a French-English alignment task from the Hansard.", "labels": [], "entities": [{"text": "Hansard", "start_pos": 64, "end_pos": 71, "type": "DATASET", "confidence": 0.9231762290000916}]}], "introductionContent": [{"text": "Aligning words from mutually translated sentences in two different languages is an important and difficult problem.", "labels": [], "entities": [{"text": "Aligning words from mutually translated sentences", "start_pos": 0, "end_pos": 49, "type": "TASK", "confidence": 0.8813216984272003}]}, {"text": "It is important because a wordaligned corpus is typically used as a first step in order to identify phrases or templates in phrase-based Machine Translation (), (, (, sec.", "labels": [], "entities": [{"text": "phrase-based Machine Translation", "start_pos": 124, "end_pos": 156, "type": "TASK", "confidence": 0.627159833908081}]}, {"text": "3), or for projecting linguistic annotation across languages ().", "labels": [], "entities": []}, {"text": "Obtaining a word-aligned corpus usually involves training a word-based translation models) in each directions and combining the resulting alignments.", "labels": [], "entities": []}, {"text": "Besides processing time, important issues are completeness and propriety of the resulting alignment, and the ability to reliably identify general Nto-M alignments.", "labels": [], "entities": []}, {"text": "In the following section, we introduce the problem of aligning words from a corpus that is already aligned at the sentence level.", "labels": [], "entities": []}, {"text": "We show how this problem maybe phrased in terms of matrix factorisation.", "labels": [], "entities": []}, {"text": "We then identify a number of constraints on word alignment, show that these constraints entail that word alignment is equivalent to orthogonal non-negative matrix factorisation, and we give a novel algorithm that solves this problem.", "labels": [], "entities": [{"text": "word alignment", "start_pos": 44, "end_pos": 58, "type": "TASK", "confidence": 0.7864466905593872}, {"text": "word alignment", "start_pos": 100, "end_pos": 114, "type": "TASK", "confidence": 0.7129718959331512}]}, {"text": "This is illustrated using data from the shared tasks of the 2003 HLT-NAACL Workshop on Building le droit de permis ne augmente pas the licence fee does not increase and Using Parallel Texts ().", "labels": [], "entities": [{"text": "HLT-NAACL Workshop", "start_pos": 65, "end_pos": 83, "type": "DATASET", "confidence": 0.8011180758476257}]}], "datasetContent": [{"text": "In order to perform a more systematic evaluation of the use of matrix factorisation for aligning words, we tested this technique on the full trial and test data from the 2003 HLT-NAACL Workshop.", "labels": [], "entities": [{"text": "HLT-NAACL Workshop", "start_pos": 175, "end_pos": 193, "type": "DATASET", "confidence": 0.8602450788021088}]}, {"text": "Note that the reference data has both \"Sure\" and \"Probable\" alignments, with about 77% of all alignments in the latter category.", "labels": [], "entities": [{"text": "Probable\" alignments", "start_pos": 50, "end_pos": 70, "type": "METRIC", "confidence": 0.9089194536209106}]}, {"text": "On the other hand, our system proposes only one type of alignment.", "labels": [], "entities": []}, {"text": "The evaluation is done using the performance measures described in: precision, recall and F-score on the probable and sure alignments, as well as the Alignment Error Rate (AER), which in our case is a weighted average of the recall on the sure alignments and the precision on the probable.", "labels": [], "entities": [{"text": "precision", "start_pos": 68, "end_pos": 77, "type": "METRIC", "confidence": 0.9997650980949402}, {"text": "recall", "start_pos": 79, "end_pos": 85, "type": "METRIC", "confidence": 0.9982522130012512}, {"text": "F-score", "start_pos": 90, "end_pos": 97, "type": "METRIC", "confidence": 0.9989228844642639}, {"text": "Alignment Error Rate (AER)", "start_pos": 150, "end_pos": 176, "type": "METRIC", "confidence": 0.9582108656565348}, {"text": "recall", "start_pos": 225, "end_pos": 231, "type": "METRIC", "confidence": 0.9952652454376221}, {"text": "precision", "start_pos": 263, "end_pos": 272, "type": "METRIC", "confidence": 0.9870589971542358}]}, {"text": "Given an alignment A and gold standards G Sand GP (for sure and probable alignments, respectively): where T is either S or P , and: Using these measures, we first evaluate the performance on the trial set (37 sentences): as we produce only one type of alignment and evaluate against \"Sure\" and \"Probable\", we observe, as expected, that the recall is very good on sure alignments, but precision relatively poor, with the reverse situation on the probable alignments.", "labels": [], "entities": [{"text": "Sand GP", "start_pos": 42, "end_pos": 49, "type": "METRIC", "confidence": 0.8885339200496674}, {"text": "recall", "start_pos": 340, "end_pos": 346, "type": "METRIC", "confidence": 0.9993613362312317}, {"text": "precision", "start_pos": 384, "end_pos": 393, "type": "METRIC", "confidence": 0.9993428587913513}]}, {"text": "This is because we generate an intermediate number of alignments.", "labels": [], "entities": []}, {"text": "There are 338 sure and 1446 probable alignments (for 721 French and 661 English words) in the reference trial data, and we produce 707 (AIC) or 766 (BIC) alignments with ONMF.", "labels": [], "entities": [{"text": "BIC", "start_pos": 149, "end_pos": 152, "type": "METRIC", "confidence": 0.900688111782074}, {"text": "ONMF", "start_pos": 170, "end_pos": 174, "type": "DATASET", "confidence": 0.7782924771308899}]}, {"text": "Most of them are at least probably correct, as attested by PP , but only about half of them are in the \"Sure\" subset, yielding a low value of PS . Similarly, because \"Probable\" alignments were generated as the union of alignments produced by two annotators, they sometimes lead to very large M-N alignments, which produce on average 2.5 to 2.7 alignments per word.", "labels": [], "entities": [{"text": "PS", "start_pos": 142, "end_pos": 144, "type": "METRIC", "confidence": 0.9665583372116089}]}, {"text": "By contrast ONMF produces less than 1.2 alignments per word, hence the low value of RP . As the AER is a weighted average of R Sand PP , the resulting AER are relatively low for our method.", "labels": [], "entities": [{"text": "ONMF", "start_pos": 12, "end_pos": 16, "type": "METRIC", "confidence": 0.6678276062011719}, {"text": "RP", "start_pos": 84, "end_pos": 86, "type": "METRIC", "confidence": 0.9521180987358093}, {"text": "AER", "start_pos": 96, "end_pos": 99, "type": "METRIC", "confidence": 0.9984971284866333}, {"text": "AER", "start_pos": 151, "end_pos": 154, "type": "METRIC", "confidence": 0.9990160465240479}]}, {"text": "Method: Performance on the 37 trial sentences for orthogonal non-negative matrix factorisation (ONMF) using the AIC and BIC criterion for choosing the number of cepts, discounting null alignments.", "labels": [], "entities": [{"text": "BIC", "start_pos": 120, "end_pos": 123, "type": "METRIC", "confidence": 0.9809266328811646}]}, {"text": "We also compared the performance on the 447 test sentences to 1/ the intersection of the alignments produced by the top IBM4 alignments in either directions, and 2/ the best systems from).", "labels": [], "entities": []}, {"text": "On limited resources, Ralign.EF.1) produced the best F -score, as well as the best AER when NULL alignments were taken into account, while XRCE.Nolem.EF.3) produced the best AER when NULL alignments were discounted.", "labels": [], "entities": [{"text": "F -score", "start_pos": 53, "end_pos": 61, "type": "METRIC", "confidence": 0.9917968312899271}, {"text": "AER", "start_pos": 83, "end_pos": 86, "type": "METRIC", "confidence": 0.9982278943061829}, {"text": "XRCE.Nolem.EF.3", "start_pos": 139, "end_pos": 154, "type": "DATASET", "confidence": 0.847775399684906}, {"text": "AER", "start_pos": 174, "end_pos": 177, "type": "METRIC", "confidence": 0.996613085269928}]}, {"text": "show that ONMF improves on several of these results.", "labels": [], "entities": [{"text": "ONMF", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.7181275486946106}]}, {"text": "In particular, we get better recall and F -score on the probable alignments (and even a better precision than Ralign.EF.1 in table 2).", "labels": [], "entities": [{"text": "recall", "start_pos": 29, "end_pos": 35, "type": "METRIC", "confidence": 0.9997500777244568}, {"text": "F -score", "start_pos": 40, "end_pos": 48, "type": "METRIC", "confidence": 0.9898077448209127}, {"text": "precision", "start_pos": 95, "end_pos": 104, "type": "METRIC", "confidence": 0.9989822506904602}]}, {"text": "On the other hand, the performance, and in particular the precision, on sure alignments is dismal.", "labels": [], "entities": [{"text": "precision", "start_pos": 58, "end_pos": 67, "type": "METRIC", "confidence": 0.99969482421875}]}, {"text": "We attribute this at least partly to a key difference between our model and the reference data: 49.86% 95.12% 65.42% 84.63% 37.39% 51.87% 11.76% ONMF + BIC 46.50% 96.01% 62.65% 80.92% 38.69% 52.35% 14.16% IBM4 intersection 71.46% 90.04% 79.68% 97.66% 28.44% 44.12% 5.71% HLT-03 best F 72.54% 80.61% 76.36% 77.56% 38.19% 51.18% 18.50% HLT-03 best AER 55.43% 93.81% 69.68% 90.09% 35.30% 50.72% 8.53%: Performance on the 447 English-French test sentences, discounting NULL alignments, for orthogonal non-negative matrix factorisation (ONMF) using the AIC and BIC criterion for choosing the number of cepts.", "labels": [], "entities": [{"text": "ONMF", "start_pos": 145, "end_pos": 149, "type": "METRIC", "confidence": 0.7525560259819031}, {"text": "BIC", "start_pos": 152, "end_pos": 155, "type": "METRIC", "confidence": 0.5885769128799438}, {"text": "F", "start_pos": 283, "end_pos": 284, "type": "METRIC", "confidence": 0.7724540829658508}, {"text": "AER", "start_pos": 346, "end_pos": 349, "type": "METRIC", "confidence": 0.9597299695014954}, {"text": "BIC", "start_pos": 556, "end_pos": 559, "type": "METRIC", "confidence": 0.8356241583824158}]}, {"text": "HLT-03 best F is Ralign.EF.1 and best AER is XRCE.Nolem.EF.3.", "labels": [], "entities": [{"text": "HLT-03", "start_pos": 0, "end_pos": 6, "type": "METRIC", "confidence": 0.7500721216201782}, {"text": "F", "start_pos": 12, "end_pos": 13, "type": "METRIC", "confidence": 0.9918926954269409}, {"text": "Ralign.EF.1", "start_pos": 17, "end_pos": 28, "type": "DATASET", "confidence": 0.6310562491416931}, {"text": "AER", "start_pos": 38, "end_pos": 41, "type": "METRIC", "confidence": 0.9996416568756104}, {"text": "XRCE.Nolem.EF.3", "start_pos": 45, "end_pos": 60, "type": "METRIC", "confidence": 0.97197425365448}]}, {"text": "our model enforces coverage and makes sure that all words are aligned, while the \"Sure\" reference alignments have no such constraints and actually have a very bad coverage.", "labels": [], "entities": [{"text": "coverage", "start_pos": 19, "end_pos": 27, "type": "METRIC", "confidence": 0.9069584012031555}]}, {"text": "Indeed, less than half the words in the test set have a \"Sure\" alignment, which means that a method which ensures that all words are aligned will at best have a sub 50% precision.", "labels": [], "entities": [{"text": "Sure\" alignment", "start_pos": 57, "end_pos": 72, "type": "METRIC", "confidence": 0.9360348582267761}, {"text": "precision", "start_pos": 169, "end_pos": 178, "type": "METRIC", "confidence": 0.9875895380973816}]}, {"text": "In addition, many reference \"Probable\" alignments are not proper alignments in the sense defined above.", "labels": [], "entities": []}, {"text": "Note that the IBM4 intersection has a bias similar to the sure reference alignments, and performs very well in F S , PP and especially in AER, even though it produces very incomplete alignments.", "labels": [], "entities": [{"text": "AER", "start_pos": 138, "end_pos": 141, "type": "METRIC", "confidence": 0.7985304594039917}]}, {"text": "This points to a particular problem with the AER in the context of our study.", "labels": [], "entities": [{"text": "AER", "start_pos": 45, "end_pos": 48, "type": "METRIC", "confidence": 0.9058610200881958}]}, {"text": "In fact, a system that outputs exactly the set of sure alignments achieves a perfect AER of 0, even though it aligns only about 23% of words, clearly an unacceptable drawback in many applications.", "labels": [], "entities": [{"text": "AER", "start_pos": 85, "end_pos": 88, "type": "METRIC", "confidence": 0.9988853335380554}]}, {"text": "We think that this issue maybe addressed in two different ways.", "labels": [], "entities": []}, {"text": "One time-consuming possibility would be to post-edit the reference alignment to ensure coverage and proper alignments.", "labels": [], "entities": [{"text": "coverage", "start_pos": 87, "end_pos": 95, "type": "METRIC", "confidence": 0.9913450479507446}]}, {"text": "Another possibility would be to use the probabilistic model to mimic the reference data and generate both \"Sure\" and \"Probable\" alignments using eg thresholds on the estimated alignment probabilities.", "labels": [], "entities": []}, {"text": "This approach may lead to better performance according to our metrics, but it is not obvious that the produced alignments will be more reasonable or even useful in a practical application.", "labels": [], "entities": []}, {"text": "We also tested our approach on the RomanianEnglish task of the same workshop, cf. table 4.", "labels": [], "entities": [{"text": "RomanianEnglish task", "start_pos": 35, "end_pos": 55, "type": "DATASET", "confidence": 0.9234373271465302}]}, {"text": "The 'HLT-03 best' is our earlier work, simply based on IBM4 alignment using an additional lexicon extracted from the corpus.", "labels": [], "entities": [{"text": "IBM4 alignment", "start_pos": 55, "end_pos": 69, "type": "TASK", "confidence": 0.6278069764375687}]}, {"text": "Slightly better results have been published since), using additional linguistic processing, but those were not presented at the workshop.", "labels": [], "entities": []}, {"text": "Note that the reference alignments for RomanianEnglish contain only \"Sure\" alignments, and therefore we only report the performance on those.", "labels": [], "entities": [{"text": "RomanianEnglish", "start_pos": 39, "end_pos": 54, "type": "DATASET", "confidence": 0.9741371870040894}]}, {"text": "In addition, AER = 1 \u2212 F S in this setting.", "labels": [], "entities": [{"text": "AER", "start_pos": 13, "end_pos": 16, "type": "METRIC", "confidence": 0.9995049238204956}]}, {"text": "shows that the matrix factorisation approach does not offer any quantitative improvements over these results.", "labels": [], "entities": []}, {"text": "A gain of up to 10 points in recall does not offset a large decrease in precision.", "labels": [], "entities": [{"text": "recall", "start_pos": 29, "end_pos": 35, "type": "METRIC", "confidence": 0.9992794394493103}, {"text": "precision", "start_pos": 72, "end_pos": 81, "type": "METRIC", "confidence": 0.9989156723022461}]}, {"text": "As a consequence, the AER for ONMF+AIC is about 10% higher than in our earlier work.", "labels": [], "entities": [{"text": "AER", "start_pos": 22, "end_pos": 25, "type": "METRIC", "confidence": 0.9993953704833984}, {"text": "ONMF+AIC", "start_pos": 30, "end_pos": 38, "type": "TASK", "confidence": 0.5177507301171621}]}, {"text": "This seems mainly due to the fact that the 'HLT-03 best' produces alignments for only about 80% of the words, while our technique ensure coverage and therefore aligns all words.", "labels": [], "entities": [{"text": "coverage", "start_pos": 137, "end_pos": 145, "type": "METRIC", "confidence": 0.9618865847587585}]}, {"text": "These results suggest that remaining 20% seem particularly problematic.", "labels": [], "entities": []}, {"text": "These quantitative results are disappointing given the sofistication of the method.", "labels": [], "entities": []}, {"text": "It should be noted, however, that ONMF provides the qualitative advantage of producing proper alignments, and in particular ensures coverage.", "labels": [], "entities": [{"text": "ONMF", "start_pos": 34, "end_pos": 38, "type": "METRIC", "confidence": 0.4490877389907837}]}, {"text": "This maybe useful in some contexts, eg training a phrasebased translation system.", "labels": [], "entities": [{"text": "phrasebased translation", "start_pos": 50, "end_pos": 73, "type": "TASK", "confidence": 0.7917998135089874}]}], "tableCaptions": [{"text": " Table 1: Performance on the 37 trial sentences for orthogonal non-negative matrix factorisation (ONMF)  using the AIC and BIC criterion for choosing the number of cepts, discounting null alignments.", "labels": [], "entities": [{"text": "BIC", "start_pos": 123, "end_pos": 126, "type": "METRIC", "confidence": 0.9770327806472778}]}, {"text": " Table 3: Performance on the 447 English-French test sentences, taking NULL alignments into account, for  orthogonal non-negative matrix factorisation (ONMF) using the AIC and BIC criterion for choosing the  number of cepts. HLT-03 best is Ralign.EF.1 (Mihalcea and Pedersen, 2003).", "labels": [], "entities": [{"text": "orthogonal non-negative matrix factorisation (ONMF)", "start_pos": 106, "end_pos": 157, "type": "METRIC", "confidence": 0.76302056653159}, {"text": "BIC", "start_pos": 176, "end_pos": 179, "type": "METRIC", "confidence": 0.9764348864555359}]}, {"text": " Table 4: Performance on the 248 Romanian-English test sentences (only sure alignments), for orthogonal  non-negative matrix factorisation (ONMF) using the AIC and BIC criterion for choosing the number of  cepts. HLT-03 best is XRCE.Nolem (Mihalcea", "labels": [], "entities": [{"text": "orthogonal  non-negative matrix factorisation (ONMF)", "start_pos": 93, "end_pos": 145, "type": "METRIC", "confidence": 0.7315748078482491}, {"text": "BIC", "start_pos": 164, "end_pos": 167, "type": "METRIC", "confidence": 0.974528431892395}, {"text": "HLT-03", "start_pos": 213, "end_pos": 219, "type": "METRIC", "confidence": 0.828267514705658}, {"text": "XRCE.Nolem", "start_pos": 228, "end_pos": 238, "type": "METRIC", "confidence": 0.9630494713783264}]}]}