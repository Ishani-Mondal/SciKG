{"title": [], "abstractContent": [{"text": "We present the first application of the head-driven statistical parsing model of Collins (1999) as a simultaneous language model and parser for large-vocabulary speech recognition.", "labels": [], "entities": [{"text": "head-driven statistical parsing", "start_pos": 40, "end_pos": 71, "type": "TASK", "confidence": 0.5895887215932211}, {"text": "large-vocabulary speech recognition", "start_pos": 144, "end_pos": 179, "type": "TASK", "confidence": 0.6803382734457651}]}, {"text": "The model is adapted to an online left to right chart-parser for word lattices, integrating acoustic, n-gram, and parser probabilities.", "labels": [], "entities": []}, {"text": "The parser uses structural and lexical dependencies not considered by n-gram models, conditioning recognition on more linguistically-grounded relationships.", "labels": [], "entities": []}, {"text": "Experiments on the Wall Street Journal treebank and lattice corpora show word error rates competitive with the standard n-gram language model while extracting additional structural information useful for speech understanding.", "labels": [], "entities": [{"text": "Wall Street Journal treebank", "start_pos": 19, "end_pos": 47, "type": "DATASET", "confidence": 0.9599434286355972}, {"text": "speech understanding", "start_pos": 204, "end_pos": 224, "type": "TASK", "confidence": 0.7372618913650513}]}], "introductionContent": [{"text": "The question of how to integrate high-level knowledge representations of language with automatic speech recognition (ASR) is becoming more important as (1) speech recognition technology matures, (2) the rate of improvement of recognition accuracy decreases, and (3) the need for additional information (beyond simple transcriptions) becomes evident.", "labels": [], "entities": [{"text": "automatic speech recognition (ASR)", "start_pos": 87, "end_pos": 121, "type": "TASK", "confidence": 0.8006662825743357}, {"text": "speech recognition", "start_pos": 156, "end_pos": 174, "type": "TASK", "confidence": 0.7727225720882416}, {"text": "accuracy", "start_pos": 238, "end_pos": 246, "type": "METRIC", "confidence": 0.9611672163009644}]}, {"text": "Most of the currently best ASR systems use an n-gram language model of the type pioneered by.", "labels": [], "entities": [{"text": "ASR", "start_pos": 27, "end_pos": 30, "type": "TASK", "confidence": 0.9894573092460632}]}, {"text": "Recently, research has begun to show progress towards application of new and better models of spoken language).", "labels": [], "entities": []}, {"text": "Our goal is integration of head-driven lexicalized parsing with acoustic and n-gram models for speech recognition, extracting high-level structure from speech, while simultaneously selecting the best path in a word lattice.", "labels": [], "entities": [{"text": "head-driven lexicalized parsing", "start_pos": 27, "end_pos": 58, "type": "TASK", "confidence": 0.6349864105383555}, {"text": "speech recognition", "start_pos": 95, "end_pos": 113, "type": "TASK", "confidence": 0.7216738611459732}]}, {"text": "Parse trees generated by this process will be useful for automated speech understanding, such as in higher semantic parsing.", "labels": [], "entities": [{"text": "automated speech understanding", "start_pos": 57, "end_pos": 87, "type": "TASK", "confidence": 0.6155493557453156}, {"text": "higher semantic parsing", "start_pos": 100, "end_pos": 123, "type": "TASK", "confidence": 0.6273252367973328}]}, {"text": "Collins (1999) presents three lexicalized models which consider long-distance dependencies within a sentence.", "labels": [], "entities": []}, {"text": "Grammar productions are conditioned on headwords.", "labels": [], "entities": []}, {"text": "The conditioning context is thus more focused than that of a large n-gram covering the same span, so the sparse data problems arising from the sheer size of the parameter space are less pressing.", "labels": [], "entities": []}, {"text": "However, sparse data problems arising from the limited availability of annotated training data become a problem.", "labels": [], "entities": []}, {"text": "We test the head-driven statistical lattice parser with word lattices from the NIST HUB-1 corpus, which has been used by others in related work).", "labels": [], "entities": [{"text": "NIST HUB-1 corpus", "start_pos": 79, "end_pos": 96, "type": "DATASET", "confidence": 0.9477585752805074}]}, {"text": "Parse accuracy and word error rates are reported.", "labels": [], "entities": [{"text": "Parse", "start_pos": 0, "end_pos": 5, "type": "METRIC", "confidence": 0.988955557346344}, {"text": "accuracy", "start_pos": 6, "end_pos": 14, "type": "METRIC", "confidence": 0.8750991821289062}, {"text": "word error rates", "start_pos": 19, "end_pos": 35, "type": "METRIC", "confidence": 0.7521043618520101}]}, {"text": "We present an analysis of the effects of pruning and heuristic search on efficiency and accuracy and note several simplifying assumptions common to other reported experiments in this area, which present challenges for scaling up to realworld applications.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 88, "end_pos": 96, "type": "METRIC", "confidence": 0.9985363483428955}]}, {"text": "This work shows the importance of careful algorithm and data structure design and choice of dynamic programming constraints to the efficiency and accuracy of a head-driven probabilistic parser for speech.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 146, "end_pos": 154, "type": "METRIC", "confidence": 0.9986182451248169}]}, {"text": "We find that the parsing model of can be successfully adapted as a language model for speech recognition.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 86, "end_pos": 104, "type": "TASK", "confidence": 0.8269499838352203}]}, {"text": "In the following section, we present a review of recent works in high-level language modelling for speech recognition.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 99, "end_pos": 117, "type": "TASK", "confidence": 0.8316749632358551}]}, {"text": "We describe the word lattice parser developed in this work in Section 3.", "labels": [], "entities": []}, {"text": "Section 4 is a description of current evaluation metrics, and suggestions for new metrics.", "labels": [], "entities": []}, {"text": "Experiments on strings and word lattices are reported in Section 5, and conclusions and opportunities for future work are outlined in Section 6.", "labels": [], "entities": []}], "datasetContent": [{"text": "The word lattice parser was evaluated with several metrics -WER, labelled precision and recall, crossing brackets, and time and space resource usage.", "labels": [], "entities": [{"text": "word lattice parser", "start_pos": 4, "end_pos": 23, "type": "TASK", "confidence": 0.5959671835104624}, {"text": "WER", "start_pos": 60, "end_pos": 63, "type": "METRIC", "confidence": 0.9975079298019409}, {"text": "precision", "start_pos": 74, "end_pos": 83, "type": "METRIC", "confidence": 0.9688754677772522}, {"text": "recall", "start_pos": 88, "end_pos": 94, "type": "METRIC", "confidence": 0.9699392914772034}]}, {"text": "Following Roark (2001), we conducted evaluations using two experimental sets -strings and word lattices.", "labels": [], "entities": []}, {"text": "We optimized settings (thresholds, variable beam function, base beam value) for parsing using development test data consisting of strings for which we have annotated parse trees.", "labels": [], "entities": [{"text": "parsing", "start_pos": 80, "end_pos": 87, "type": "TASK", "confidence": 0.982648491859436}]}, {"text": "The parsing accuracy for parsing word lattices was not directly evaluated as we did not have annotated parse trees for comparison.", "labels": [], "entities": [{"text": "parsing", "start_pos": 4, "end_pos": 11, "type": "TASK", "confidence": 0.9449517726898193}, {"text": "accuracy", "start_pos": 12, "end_pos": 20, "type": "METRIC", "confidence": 0.9082992076873779}, {"text": "parsing word lattices", "start_pos": 25, "end_pos": 46, "type": "TASK", "confidence": 0.8991775314013163}]}, {"text": "Furthermore, standard parsing measures such as labelled precision and recall are not directly applicable in cases where the number of words differs between the proposed parse tree and the gold standard.", "labels": [], "entities": [{"text": "precision", "start_pos": 56, "end_pos": 65, "type": "METRIC", "confidence": 0.8707926869392395}, {"text": "recall", "start_pos": 70, "end_pos": 76, "type": "METRIC", "confidence": 0.999197781085968}]}, {"text": "Results show scores for parsing strings which are lower than the original implementation of Collins (1999).", "labels": [], "entities": [{"text": "parsing strings", "start_pos": 24, "end_pos": 39, "type": "TASK", "confidence": 0.9142112731933594}]}, {"text": "The WER scores for this, the first application of the Collins (1999) model to parsing word lattices, are comparable to other recent work in syntactic language modelling, and better than a simple trigram model trained on the same data.", "labels": [], "entities": [{"text": "WER", "start_pos": 4, "end_pos": 7, "type": "METRIC", "confidence": 0.8645681142807007}, {"text": "parsing word lattices", "start_pos": 78, "end_pos": 99, "type": "TASK", "confidence": 0.9054119984308878}, {"text": "syntactic language modelling", "start_pos": 140, "end_pos": 168, "type": "TASK", "confidence": 0.717924435933431}]}, {"text": "Parse trees are commonly scored with the PARSEVAL set of metrics ().", "labels": [], "entities": [{"text": "PARSEVAL", "start_pos": 41, "end_pos": 49, "type": "METRIC", "confidence": 0.8797858953475952}]}], "tableCaptions": [{"text": " Table 1: Results for parsing section 0 (", "labels": [], "entities": [{"text": "parsing section 0", "start_pos": 22, "end_pos": 39, "type": "TASK", "confidence": 0.8943763176600138}]}, {"text": " Table 2: Results for parsing HUB-1 n-best word lattices and lists: OP = overparsing, S = substutitions (%),  D = deletions (%), I = insertions (%), T = total WER (%", "labels": [], "entities": [{"text": "parsing HUB-1 n-best word lattices", "start_pos": 22, "end_pos": 56, "type": "TASK", "confidence": 0.7719142317771912}, {"text": "OP", "start_pos": 68, "end_pos": 70, "type": "METRIC", "confidence": 0.9874230623245239}, {"text": "T", "start_pos": 149, "end_pos": 150, "type": "METRIC", "confidence": 0.9733549952507019}, {"text": "WER", "start_pos": 159, "end_pos": 162, "type": "METRIC", "confidence": 0.8033453822135925}]}, {"text": " Table 3: Comparison of WER for parsing HUB-1 words lattices with best results of other works. SER =  sentence error rate. WER = word error rate. \"Speech-like\" transformations were applied to all training  corpora.", "labels": [], "entities": [{"text": "WER", "start_pos": 24, "end_pos": 27, "type": "METRIC", "confidence": 0.9457683563232422}, {"text": "parsing HUB-1 words lattices", "start_pos": 32, "end_pos": 60, "type": "TASK", "confidence": 0.8381285071372986}, {"text": "SER", "start_pos": 95, "end_pos": 98, "type": "METRIC", "confidence": 0.9989232420921326}, {"text": "sentence error rate", "start_pos": 102, "end_pos": 121, "type": "METRIC", "confidence": 0.6705645521481832}, {"text": "WER", "start_pos": 123, "end_pos": 126, "type": "METRIC", "confidence": 0.9917417764663696}, {"text": "word error rate", "start_pos": 129, "end_pos": 144, "type": "METRIC", "confidence": 0.7181463440259298}]}]}