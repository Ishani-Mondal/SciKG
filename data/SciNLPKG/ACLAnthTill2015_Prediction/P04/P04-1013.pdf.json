{"title": [{"text": "Discriminative Training of a Neural Network Statistical Parser", "labels": [], "entities": []}], "abstractContent": [{"text": "Discriminative methods have shown significant improvements over traditional generative methods in many machine learning applications, but there has been difficulty in extending them to natural language parsing.", "labels": [], "entities": [{"text": "natural language parsing", "start_pos": 185, "end_pos": 209, "type": "TASK", "confidence": 0.6973642309506735}]}, {"text": "One problem is that much of the work on discriminative methods conflates changes to the learning method with changes to the parameterization of the problem.", "labels": [], "entities": []}, {"text": "We show how a parser can be trained with a dis-criminative learning method while still param-eterizing the problem according to a generative probability model.", "labels": [], "entities": []}, {"text": "We present three methods for training a neural network to estimate the probabilities fora statistical parser, one gen-erative, one discriminative, and one where the probability model is generative but the training criteria is discriminative.", "labels": [], "entities": []}, {"text": "The latter model out-performs the previous two, achieving state-of-the-art levels of performance (90.1% F-measure on constituents).", "labels": [], "entities": [{"text": "F-measure", "start_pos": 104, "end_pos": 113, "type": "METRIC", "confidence": 0.9975684285163879}]}], "introductionContent": [{"text": "Much recent work has investigated the application of discriminative methods to NLP tasks, with mixed results.", "labels": [], "entities": []}, {"text": "argue that these results show a pattern where discriminative probability models are inferior to generative probability models, but that improvements can be achieved by keeping a generative probability model and training according to a discriminative optimization criteria.", "labels": [], "entities": []}, {"text": "We show how this approach can be applied to broad coverage natural language parsing.", "labels": [], "entities": [{"text": "broad coverage natural language parsing", "start_pos": 44, "end_pos": 83, "type": "TASK", "confidence": 0.6612197160720825}]}, {"text": "Our estimation and training methods successfully balance the conflicting requirements that the training method be both computationally tractable for large datasets and a good approximation to the theoretically optimal method.", "labels": [], "entities": [{"text": "estimation", "start_pos": 4, "end_pos": 14, "type": "TASK", "confidence": 0.955099880695343}]}, {"text": "The parser which uses this approach outperforms both a generative model and a discriminative model, achieving state-of-the-art levels of performance (90.1% F-measure on constituents).", "labels": [], "entities": [{"text": "F-measure", "start_pos": 156, "end_pos": 165, "type": "METRIC", "confidence": 0.9980206489562988}]}, {"text": "To compare these different approaches, we use a neural network architecture called Simple Synchrony Networks (SSNs)) to estimate the parameters of the probability models.", "labels": [], "entities": []}, {"text": "SSNs have the advantage that they avoid the need to impose hand-crafted independence assumptions on the learning process.", "labels": [], "entities": [{"text": "SSNs", "start_pos": 0, "end_pos": 4, "type": "TASK", "confidence": 0.8857747316360474}]}, {"text": "Training an SSN simultaneously trains a finite representations of the unbounded parse history and a mapping from this history representation to the parameter estimates.", "labels": [], "entities": []}, {"text": "The history representations are automatically tuned to optimize the parameter estimates.", "labels": [], "entities": []}, {"text": "This avoids the problem that any choice of hand-crafted independence assumptions may bias our results towards one approach or another.", "labels": [], "entities": []}, {"text": "The independence assumptions would have to be different for the generative and discriminative probability models, and even for the parsers which use the generative probability model, the same set of independence assumptions maybe more appropriate for maximizing one training criteria over another.", "labels": [], "entities": []}, {"text": "By inducing the history representations specifically to fit the chosen model and training criteria, we avoid having to choose independence assumptions which might bias our results.", "labels": [], "entities": []}, {"text": "Each complete parsing system we propose consists of three components, a probability model for sequences of parser decisions, a Simple Synchrony Network which estimates the parameters of the probability model, and a procedure which searches for the most probable parse given these parameter estimates.", "labels": [], "entities": []}, {"text": "This paper outlines each of these components, but more details can be found in, and, for the discriminative model, in.", "labels": [], "entities": []}, {"text": "We also present the training methods, and experiments on the proposed parsing models.", "labels": [], "entities": [{"text": "parsing", "start_pos": 70, "end_pos": 77, "type": "TASK", "confidence": 0.9656006097793579}]}], "datasetContent": [{"text": "We used the Penn Treebank () to perform empirical experiments on the proposed parsing models.", "labels": [], "entities": [{"text": "Penn Treebank", "start_pos": 12, "end_pos": 25, "type": "DATASET", "confidence": 0.9954235553741455}]}, {"text": "In each case the input to the network is a sequence of tag-word pairs.", "labels": [], "entities": []}, {"text": "5 We report results for three different vocabulary sizes, varying in the frequency with which tagword pairs must occur in the training set in order to be included explicitly in the vocabulary.", "labels": [], "entities": []}, {"text": "A frequency threshold of 200 resulted in a vocabulary of 508 tag-word pairs, a threshold of 20 resulted in 4215 tag-word pairs, and a threshold of 5 resulted in 11,993 tag-word pairs For the generative model we trained networks for the 508 (\"GSSN-Freq\u2265200\") and 4215 (\"GSSN-Freq\u226520\") word vocabularies.", "labels": [], "entities": []}, {"text": "The need to calculate word predictions makes training times for the 11,993 word vocabulary very long, and as of this writing no such network training has been completed.", "labels": [], "entities": [{"text": "word predictions", "start_pos": 22, "end_pos": 38, "type": "TASK", "confidence": 0.6528822183609009}]}, {"text": "The discriminative model does not need to calculate word predictions, so it was feasible to train networks for the 11,993 word vocabulary (\"DSSN-Freq\u22655\").", "labels": [], "entities": []}, {"text": "Previous results indicate that this vocabulary size performs better than the smaller ones, as would be expected.", "labels": [], "entities": []}, {"text": "For the networks trained with the discriminative optimization criteria and the generative probability model, we trained networks for the 508 (\"DGSSN-Freq\u2265200\") and 4215 (\"DGSSNFreq\u226520\") word vocabularies.", "labels": [], "entities": []}, {"text": "For this training, we need to select a small set of the most probable incorrect parses.", "labels": [], "entities": []}, {"text": "When we tried using only the network being trained to choose these top parses, training times were very long and the resulting networks did not outperform their generative counterparts.", "labels": [], "entities": []}, {"text": "In the experiments reported here, we provided the training with a list of the top 20 parses found by a network of the same type which had been trained with the generative criteria.", "labels": [], "entities": []}, {"text": "The network being trained was then used to choose its top 10 parses from this list, and training was performed on these 10 parses and the correct parse.", "labels": [], "entities": []}, {"text": "6 This reduced the time necessary to choose the top parses during training, and helped focus the early stages of training on learning relevant discriminations.", "labels": [], "entities": []}, {"text": "Once the training of these networks was complete, we tested both their ability to parse on their own and their ability to re-rank the top unknown-word vocabulary item which is used for all those words which are not sufficiently frequent with that tag to be included individually in the vocabulary (as well as other words if the unknown-word case itself does not have at least 5 instances).", "labels": [], "entities": []}, {"text": "We did no morphological analysis of unknown words.", "labels": [], "entities": []}, {"text": "The 20 candidate parses and the 10 training parses were found with post-word beam widths of 20 and 10, respectively, so these are only approximations to the top parses.", "labels": [], "entities": []}, {"text": "20 parses of their associated generative model (\"DGSSN-.", "labels": [], "entities": []}, {"text": "We determined appropriate training parameters and network size based on intermediate validation results and our previous experience.", "labels": [], "entities": []}, {"text": "We trained several networks for each of the GSSN models and chose the best ones based on their validation performance.", "labels": [], "entities": []}, {"text": "We then trained one network for each of the DGSSN models and for the DSSN model.", "labels": [], "entities": [{"text": "DGSSN", "start_pos": 44, "end_pos": 49, "type": "DATASET", "confidence": 0.9041568636894226}, {"text": "DSSN model", "start_pos": 69, "end_pos": 79, "type": "DATASET", "confidence": 0.8708232939243317}]}, {"text": "The best post-word beam width was determined on the validation set, which was 5 for the DSSN model and 100 for the other models.", "labels": [], "entities": [{"text": "post-word beam width", "start_pos": 9, "end_pos": 29, "type": "METRIC", "confidence": 0.5832629005114237}]}, {"text": "To avoid repeated testing on the standard testing set, we first compare the different models with their performance on the validation set.", "labels": [], "entities": []}, {"text": "Standard measures of accuracy are shown in table 1. 8 The largest accuracy difference is between the parser with the discriminative probability model (DSSN-Freq\u22655) and those with the generative probability model, despite the larger vocabulary of the former.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 21, "end_pos": 29, "type": "METRIC", "confidence": 0.9989953637123108}, {"text": "accuracy", "start_pos": 66, "end_pos": 74, "type": "METRIC", "confidence": 0.9985528588294983}]}, {"text": "This demonstrates the difficulty of estimating the parameters of a discriminative probability model.", "labels": [], "entities": []}, {"text": "There is also a clear effect of vocabulary size, but there is a slightly larger effect of training method.", "labels": [], "entities": []}, {"text": "When tested in the same way as they were trained (for reranking), the parsers which were trained with a discriminative criteria achieve a 7% and 8% reduction in error rate over their respective parsers with the same generative probability model.", "labels": [], "entities": [{"text": "error rate", "start_pos": 161, "end_pos": 171, "type": "METRIC", "confidence": 0.963132232427597}]}, {"text": "When tested alone, these DGSSN parsers perform only slightly better than their respective GSSN parsers.", "labels": [], "entities": []}, {"text": "Initial experiments on giving these networks exposure to parses outside the top 20 parses of the GSSN parsers at the very end of training did not result in any improvement on this task.", "labels": [], "entities": [{"text": "GSSN", "start_pos": 97, "end_pos": 101, "type": "DATASET", "confidence": 0.8714900612831116}]}, {"text": "This suggests that at least some of the advantage of the DSSN models is due to the fact that re-ranking is a simpler task than parsing from scratch.", "labels": [], "entities": []}, {"text": "But additional experimental work would be necessary to make any definite conclusions about this issue.", "labels": [], "entities": []}, {"text": "7 All the best networks had 80 hidden units for the history representation (and 80 hidden units in the lookahead representation).", "labels": [], "entities": []}, {"text": "Weight decay regularization was applied at the beginning of training but reduced to near 0 by the end of training.", "labels": [], "entities": [{"text": "Weight decay regularization", "start_pos": 0, "end_pos": 27, "type": "METRIC", "confidence": 0.8609933058420817}]}, {"text": "Training was stopped when maximum performance was reached on the validation set, using a post-word beam width of 5. 8 All our results are computed with the evalb program following the standard criteria in, and using the standard training (sections and several other statistical parsers on the entire testing set.", "labels": [], "entities": []}, {"text": "Our best performing model is more accurate than all these previous models except.", "labels": [], "entities": []}, {"text": "This DGSSN parser achieves this result using much less lexical knowledge than other approaches, which mostly use at least the words which occur at least 5 times, plus morphological features of the remaining words.", "labels": [], "entities": []}, {"text": "However, the fact that the DGSSN uses a large-vocabulary tagger) as a preprocessing stage may compensate for its smaller vocabulary.", "labels": [], "entities": [{"text": "DGSSN", "start_pos": 27, "end_pos": 32, "type": "DATASET", "confidence": 0.9397551417350769}]}, {"text": "Also, the main reason for using a smaller vocabulary is the computational complexity of computing probabilities for the shift(w i ) actions on-line, which other models do not require.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Percentage labeled constituent recall  (LR), precision (LP), and a combination of both  (F \u03b2=1 ) on validation set sentences of length at  most 100.", "labels": [], "entities": [{"text": "Percentage labeled constituent recall  (LR)", "start_pos": 10, "end_pos": 53, "type": "METRIC", "confidence": 0.7619307935237885}, {"text": "precision (LP)", "start_pos": 55, "end_pos": 69, "type": "METRIC", "confidence": 0.973404049873352}, {"text": "F \u03b2=1 )", "start_pos": 99, "end_pos": 106, "type": "METRIC", "confidence": 0.9238073348999023}]}, {"text": " Table 2: Percentage labeled constituent recall  (LR), precision (LP), and a combination of both  (F \u03b2=1 ) on the entire testing set.", "labels": [], "entities": [{"text": "recall  (LR)", "start_pos": 41, "end_pos": 53, "type": "METRIC", "confidence": 0.9267864227294922}, {"text": "precision (LP)", "start_pos": 55, "end_pos": 69, "type": "METRIC", "confidence": 0.970926970243454}, {"text": "F \u03b2=1 )", "start_pos": 99, "end_pos": 106, "type": "METRIC", "confidence": 0.9531853914260864}]}]}