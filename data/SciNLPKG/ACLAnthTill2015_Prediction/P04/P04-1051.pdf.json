{"title": [], "abstractContent": [{"text": "We present the first algorithm that computes optimal orderings of sentences into a locally coherent discourse.", "labels": [], "entities": []}, {"text": "The algorithm runs very efficiently on a variety of coherence measures from the literature.", "labels": [], "entities": []}, {"text": "We also show that the discourse ordering problem is NP-complete and cannot be approximated.", "labels": [], "entities": []}], "introductionContent": [{"text": "One central problem in discourse generation and summarisation is to structure the discourse in away that maximises coherence.", "labels": [], "entities": [{"text": "discourse generation", "start_pos": 23, "end_pos": 43, "type": "TASK", "confidence": 0.7326185256242752}, {"text": "summarisation", "start_pos": 48, "end_pos": 61, "type": "TASK", "confidence": 0.9769976139068604}]}, {"text": "Coherence is the property of a good human-authored text that makes it easier to read and understand than a randomlyordered collection of sentences.", "labels": [], "entities": []}, {"text": "Several papers in the recent literature) have focused on defining local coherence, which evaluates the quality of sentence-to-sentence transitions.", "labels": [], "entities": []}, {"text": "This is in contrast to theories of global coherence, which can consider relations between larger chunks of the discourse and e.g. structures them into a tree ().", "labels": [], "entities": []}, {"text": "Measures of local coherence specify which ordering of the sentences makes for the most coherent discourse, and can be based e.g. on Centering Theory () or on statistical models.", "labels": [], "entities": []}, {"text": "But while formal models of local coherence have made substantial progress over the past few years, the question of how to efficiently compute an ordering of the sentences in a discourse that maximises local coherence is still largely unsolved.", "labels": [], "entities": []}, {"text": "The fundamental problem is that any of the factorial number of permutations of the sentences could be the optimal discourse, which makes fora formidable search space for nontrivial discourses. and present algorithms based on genetic programming, and Lapata (2003) uses a graph-based heuristic algorithm, but none of them can give any guarantees about the quality of the computed ordering.", "labels": [], "entities": []}, {"text": "This paper presents the first algorithm that computes optimal locally coherent discourses, and establishes the complexity of the discourse ordering problem.", "labels": [], "entities": [{"text": "discourse ordering", "start_pos": 129, "end_pos": 147, "type": "TASK", "confidence": 0.7097181528806686}]}, {"text": "We first prove that the discourse ordering problem for local coherence measures is equivalent to the Travelling Salesman Problem (TSP).", "labels": [], "entities": [{"text": "discourse ordering", "start_pos": 24, "end_pos": 42, "type": "TASK", "confidence": 0.7061426490545273}]}, {"text": "This means that discourse ordering is NP-complete, i.e. there are probably no polynomial algorithms for it.", "labels": [], "entities": []}, {"text": "Worse, our result implies that the problem is not even approximable; any polynomial algorithm will compute arbitrarily bad solutions on unfortunate inputs.", "labels": [], "entities": []}, {"text": "Note that all approximation algorithms for the TSP assume that the underlying cost function is a metric, which is not the case for the coherence measures we consider.", "labels": [], "entities": [{"text": "TSP", "start_pos": 47, "end_pos": 50, "type": "TASK", "confidence": 0.920678973197937}]}, {"text": "Despite this negative result, we show that by applying modern algorithms for TSP, the discourse ordering problem can be solved efficiently enough for practical applications.", "labels": [], "entities": [{"text": "TSP", "start_pos": 77, "end_pos": 80, "type": "TASK", "confidence": 0.9797729253768921}, {"text": "discourse ordering problem", "start_pos": 86, "end_pos": 112, "type": "TASK", "confidence": 0.8146564165751139}]}, {"text": "We define a branch-and-cut algorithm based on linear programming, and evaluate it on discourse ordering problems based on the GNOME corpus and the BLLIP corpus.", "labels": [], "entities": [{"text": "GNOME corpus", "start_pos": 126, "end_pos": 138, "type": "DATASET", "confidence": 0.9725273251533508}, {"text": "BLLIP corpus", "start_pos": 147, "end_pos": 159, "type": "DATASET", "confidence": 0.9084447622299194}]}, {"text": "If the local coherence measure depends only on the adjacent pairs of sentences in the discourse, we can order discourses of up to 50 sentences in under a second.", "labels": [], "entities": []}, {"text": "If it is allowed to depend on the left-hand context of the sentence pair, computation is often still efficient, but can become expensive.", "labels": [], "entities": []}, {"text": "The structure of the paper is as follows.", "labels": [], "entities": []}, {"text": "We will first formally define the discourse ordering problem and relate our definition to the literature on local coherence measures in Section 2.", "labels": [], "entities": [{"text": "discourse ordering problem", "start_pos": 34, "end_pos": 60, "type": "TASK", "confidence": 0.7874989410241445}]}, {"text": "Then we will prove the equivalence of discourse ordering and TSP (Section 3), and present algorithms for solving it in Section 4.", "labels": [], "entities": [{"text": "discourse ordering", "start_pos": 38, "end_pos": 56, "type": "TASK", "confidence": 0.7308968603610992}]}, {"text": "Section 5 evaluates our algorithms on examples from the literature.", "labels": [], "entities": []}, {"text": "We compare our approach to various others in Section 6, and then conclude in Section 7.", "labels": [], "entities": []}], "datasetContent": [{"text": "We implemented the algorithm and ran it on some examples to evaluate its practical efficiency.", "labels": [], "entities": []}, {"text": "The runtimes are shown in for an implementation using a branch-and-cut ILP solver which is free for all academic purposes (ILP-FS) and a commercial branch-and-cut ILP solver (ILP-CS).", "labels": [], "entities": [{"text": "ILP solver", "start_pos": 71, "end_pos": 81, "type": "TASK", "confidence": 0.5403618663549423}]}, {"text": "Our: Some runtimes ford = 2 (in seconds).", "labels": [], "entities": []}, {"text": "(www.algorithmic-solutions.com) for the data structures and the graph algorithms and on SCIL 0.8 (www.mpi-sb.mpg.de/SCIL) for implementing the ILP-based branch-and-cut algorithm.", "labels": [], "entities": []}, {"text": "SCIL can be used with different branch-and-cut core codes.", "labels": [], "entities": []}, {"text": "We used CPLEX 9.0 (www.ilog.com) as commercial core and SCIP 0.68 (www.zib.de/Optimization/ Software/SCIP/) based on SOPLEX 1.2.2a (www.zib.de/Optimization/Software/ Soplex/) as the free implementation.", "labels": [], "entities": []}, {"text": "Note that all our implementations are still preliminary.", "labels": [], "entities": []}, {"text": "The software is publicly available (www.mpi-sb. mpg.de/\u02dcalthaus/PDOP.html).", "labels": [], "entities": []}, {"text": "We evaluate the implementations on three classes of inputs.", "labels": [], "entities": []}, {"text": "First, we use two discourses from the GNOME corpus, taken from, together with the centering-based cost functions from Section 2: coffers1, containing 10 discourse units, and cabinet1, containing 15 discourse units.", "labels": [], "entities": [{"text": "GNOME corpus", "start_pos": 38, "end_pos": 50, "type": "DATASET", "confidence": 0.9437403678894043}]}, {"text": "Second, we use twelve discourses from the BLLIP corpus taken from, together with M.LAPATA.", "labels": [], "entities": [{"text": "BLLIP corpus", "start_pos": 42, "end_pos": 54, "type": "DATASET", "confidence": 0.8401965796947479}]}, {"text": "These discourses are 4 to 13 discourse units long; the table only shows the instance with the highest running time.", "labels": [], "entities": []}, {"text": "Finally, we generate random instances of 2PDOP of size 20-100, and of 3PDOP of size 10, 15, and 20.", "labels": [], "entities": []}, {"text": "A random instance is the complete graph, where c(u 1 , . .", "labels": [], "entities": []}, {"text": ", u d ) is chosen uniformly at random from {0, . .", "labels": [], "entities": []}, {"text": "The results for the 2-place instances are shown in, and the results for the 3-place instances are shown in.", "labels": [], "entities": []}, {"text": "The numbers are runtimes in seconds on a Pentium 4 (Xeon) processor with 3.06 GHz.", "labels": [], "entities": []}, {"text": "Note that a hypothetical baseline implementation which naively generates and evaluates all permutations would run over 77 years fora discourse of length 20, even on a highly optimistic platform that evaluates one billion permutations per second.", "labels": [], "entities": []}, {"text": "Ford = 2, all real-life instances and all random instances of size up to 50 can be solved in less than one second, with either implementation.", "labels": [], "entities": []}, {"text": "The problem becomes more challenging ford = 3.", "labels": [], "entities": []}, {"text": "Here the algorithm quickly establishes good LP bounds for", "labels": [], "entities": [{"text": "LP", "start_pos": 44, "end_pos": 46, "type": "METRIC", "confidence": 0.9700287580490112}]}], "tableCaptions": [{"text": " Table 3: Some runtimes for d = 2 (in seconds).", "labels": [], "entities": []}, {"text": " Table 4: Some runtimes for d = 3 (in seconds).", "labels": [], "entities": []}]}