{"title": [{"text": "Improving the Accuracy of Subcategorizations Acquired from Corpora", "labels": [], "entities": [{"text": "Improving", "start_pos": 0, "end_pos": 9, "type": "TASK", "confidence": 0.9710147380828857}, {"text": "Accuracy", "start_pos": 14, "end_pos": 22, "type": "METRIC", "confidence": 0.9622715711593628}, {"text": "Corpora", "start_pos": 59, "end_pos": 66, "type": "DATASET", "confidence": 0.5621729493141174}]}], "abstractContent": [{"text": "This paper presents a method of improving the accuracy of subcategorization frames (SCFs) acquired from corpora to augment existing lexicon resources.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 46, "end_pos": 54, "type": "METRIC", "confidence": 0.9982033967971802}]}, {"text": "I estimate a confidence value of each SCF using corpus-based statistics, and then perform clustering of SCF confidence-value vectors for words to capture co-occurrence tendency among SCFs in the lexicon.", "labels": [], "entities": []}, {"text": "I apply my method to SCFs acquired from corpora using lexicons of two large-scale lexicalized grammars.", "labels": [], "entities": [{"text": "SCFs acquired from corpora", "start_pos": 21, "end_pos": 47, "type": "TASK", "confidence": 0.8795626163482666}]}, {"text": "The resulting SCFs achieve higher precision and recall compared to SCFs obtained by naive frequency cutoff .", "labels": [], "entities": [{"text": "precision", "start_pos": 34, "end_pos": 43, "type": "METRIC", "confidence": 0.99931800365448}, {"text": "recall", "start_pos": 48, "end_pos": 54, "type": "METRIC", "confidence": 0.999291181564331}]}], "introductionContent": [{"text": "Recently, a variety of methods have been proposed for acquisition of subcategorization frames (SCFs) from corpora (surveyed in).", "labels": [], "entities": [{"text": "acquisition of subcategorization frames (SCFs)", "start_pos": 54, "end_pos": 100, "type": "TASK", "confidence": 0.778411831174578}]}, {"text": "One interesting possibility is to use these techniques to improve the coverage of existing largescale lexicon resources such as lexicons of lexicalized grammars.", "labels": [], "entities": []}, {"text": "However, there has been little work on evaluating the impact of acquired SCFs with the exception of ().", "labels": [], "entities": []}, {"text": "The problem when we integrate acquired SCFs into existing lexicalized grammars is lower quality of the acquired SCFs, since they are acquired in an unsupervised manner, rather than being manually coded.", "labels": [], "entities": []}, {"text": "If we attempt to compensate for the poor precision by being less strict in filtering out less likely SCFs, then we will end up with a larger number of noisy lexical entries, which is problematic for parsing with lexicalized grammars ().", "labels": [], "entities": [{"text": "precision", "start_pos": 41, "end_pos": 50, "type": "METRIC", "confidence": 0.9982195496559143}, {"text": "parsing", "start_pos": 199, "end_pos": 206, "type": "TASK", "confidence": 0.9638140797615051}]}, {"text": "We thus need some method of selecting the most reliable set of SCFs from the system output as demonstrated in.", "labels": [], "entities": []}, {"text": "In this paper, I present a method of improving the accuracy of SCFs acquired from corpora in order to augment existing lexicon resources.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 51, "end_pos": 59, "type": "METRIC", "confidence": 0.9985619187355042}, {"text": "SCFs acquired from corpora", "start_pos": 63, "end_pos": 89, "type": "TASK", "confidence": 0.8493197709321976}]}, {"text": "I first estimate a confidence value that a word can have each SCF, using corpus-based statistics.", "labels": [], "entities": []}, {"text": "To capture latent co-occurrence tendency among SCFs in the target lexicon, I next perform clustering of SCF confidence-value vectors of words in the acquired lexicon and the target lexicon.", "labels": [], "entities": []}, {"text": "Since each centroid value of the obtained clusters indicates whether the words in that cluster have each SCF, we can eliminate SCFs acquired in error and predict possible SCFs according to the centroids.", "labels": [], "entities": []}, {"text": "I applied my method to SCFs acquired from a corpus of newsgroup posting about mobile phones (), using the XTAG English grammar (XTAG Research) and the LinGO English Resource Grammar (ERG)).", "labels": [], "entities": [{"text": "XTAG English grammar (XTAG Research)", "start_pos": 106, "end_pos": 142, "type": "DATASET", "confidence": 0.9057369572775704}, {"text": "LinGO English Resource Grammar (ERG))", "start_pos": 151, "end_pos": 188, "type": "DATASET", "confidence": 0.9406286222594125}]}, {"text": "I then compared the resulting SCFs with SCFs obtained by naive frequency cut-off to observe the effects of clustering.", "labels": [], "entities": []}], "datasetContent": [{"text": "I applied my method to SCFs acquired from 135,902 sentences of mobile phone newsgroup postings archived by Google.com, which is the same data used in ().", "labels": [], "entities": [{"text": "SCFs acquired from 135,902 sentences of mobile phone newsgroup postings archived", "start_pos": 23, "end_pos": 103, "type": "TASK", "confidence": 0.6901817538521506}]}, {"text": "The number of acquired SCFs was 14,783 for 3,864 word stems, while the number of SCF types in the data was 97.", "labels": [], "entities": []}, {"text": "I then translated the 163 SCF types into the SCF types of the XTAG English grammar) and the LinGO ERG) 5 using translation mappings built by Ted Briscoe and Dan Flickinger from 23 of the SCF types into 13 (out of 57 possible) XTAG SCF types, and 129 into 54 (out of 216 possible) ERG SCF types.", "labels": [], "entities": [{"text": "XTAG English grammar", "start_pos": 62, "end_pos": 82, "type": "DATASET", "confidence": 0.9271121422449747}, {"text": "LinGO ERG)", "start_pos": 92, "end_pos": 102, "type": "DATASET", "confidence": 0.9372424880663554}]}, {"text": "To evaluate my method, I split each lexicon of the two grammars into the training SCFs and the testing SCFs.", "labels": [], "entities": []}, {"text": "The words in the testing SCFs were included in the acquired SCFs.", "labels": [], "entities": []}, {"text": "When I apply my method to the acquired SCFs using the training SCFs and evaluate the resulting SCFs with the 5 I used the same version of the LinGO ERG as () (1.4; April 2003) but the map is updated.", "labels": [], "entities": [{"text": "LinGO ERG", "start_pos": 142, "end_pos": 151, "type": "DATASET", "confidence": 0.94610595703125}]}, {"text": "testing SCFs, we can estimate to what extent my method can preserve reliable SCFs for words unknown to the grammar.", "labels": [], "entities": []}, {"text": "The XTAG lexicon was split into 9,437 SCFs for 8,399 word stems as training and 423 SCFs for 280 word stems as testing, while the ERG lexicon was split into 1,608 SCFs for 1,062 word stems as training and 292 SCFs for 179 word stems as testing.", "labels": [], "entities": [{"text": "XTAG lexicon", "start_pos": 4, "end_pos": 16, "type": "DATASET", "confidence": 0.8877392113208771}, {"text": "ERG lexicon", "start_pos": 130, "end_pos": 141, "type": "DATASET", "confidence": 0.8896442353725433}]}, {"text": "I extracted SCF confidence-value vectors from the training SCFs and the acquired SCFs for the words in the testing SCFs.", "labels": [], "entities": []}, {"text": "The number of the resulting data objects was 8,679 for XTAG and 1,241 for ERG.", "labels": [], "entities": [{"text": "XTAG", "start_pos": 55, "end_pos": 59, "type": "DATASET", "confidence": 0.8937543630599976}]}, {"text": "The number of initial centroids 7 extracted from the training SCFs was 49 for XTAG and 53 for ERG.", "labels": [], "entities": [{"text": "XTAG", "start_pos": 78, "end_pos": 82, "type": "DATASET", "confidence": 0.6962901949882507}]}, {"text": "I then performed clustering of 8,679 data objects into 49 clusters and 1,241 data objects into I here assume that the existing SCFs for the words in the lexicon is more reliable than the other SCFs for those words.", "labels": [], "entities": []}, {"text": "7 I used the vectors that appeared for more than one word.", "labels": [], "entities": []}, {"text": "53 clusters, and then evaluated the resulting SCFs by comparing them to the testing SCFs.", "labels": [], "entities": []}, {"text": "I first compare confidence cut-off with frequency cut-off to observe the effects of Bayesian estimation.", "labels": [], "entities": [{"text": "frequency", "start_pos": 40, "end_pos": 49, "type": "METRIC", "confidence": 0.9609793424606323}]}, {"text": "shows precision and recall of the SCFs obtained using frequency cut-off and confidence cut-off 0.01, 0.03, and 0.05 by varying threshold for the confidence values and the relative frequencies from 0 to 1.", "labels": [], "entities": [{"text": "precision", "start_pos": 6, "end_pos": 15, "type": "METRIC", "confidence": 0.9991135001182556}, {"text": "recall", "start_pos": 20, "end_pos": 26, "type": "METRIC", "confidence": 0.9995501637458801}]}, {"text": "The graph indicates that the confidence cut-offs achieved higher recall than the frequency cut-off, thanks to the a priori distributions.", "labels": [], "entities": [{"text": "recall", "start_pos": 65, "end_pos": 71, "type": "METRIC", "confidence": 0.9994522929191589}]}, {"text": "When we compare the three confidence cut-offs, we can improve precision using higher recognition thresholds while we can improve recall using lower recognition thresholds.", "labels": [], "entities": [{"text": "precision", "start_pos": 62, "end_pos": 71, "type": "METRIC", "confidence": 0.9992542862892151}, {"text": "recall", "start_pos": 129, "end_pos": 135, "type": "METRIC", "confidence": 0.998683512210846}]}, {"text": "This is quite consistent with our expectations.", "labels": [], "entities": []}, {"text": "I then compare centroid cut-off with confidence cut-off to observe the effects of clustering.", "labels": [], "entities": []}, {"text": "shows precision and recall of the resulting SCFs using centroid cut-off 0.05 and the confidence cut-off 0.05 by varying the threshold for the confidence values.", "labels": [], "entities": [{"text": "precision", "start_pos": 6, "end_pos": 15, "type": "METRIC", "confidence": 0.9991773962974548}, {"text": "recall", "start_pos": 20, "end_pos": 26, "type": "METRIC", "confidence": 0.9993602633476257}]}, {"text": "In order to show the effects of the use of the training SCFs, I also performed clustering of SCF confidence-value vectors in the acquired SCFs with random initialization (k = 49 (for XTAG) and 53 (for ERG); centroid cut-off 0.05*).", "labels": [], "entities": []}, {"text": "The graph shows that clustering is meaningful only when we make use of the reliable SCFs in the manually-coded lexicon.", "labels": [], "entities": []}, {"text": "The centroid cutoff using the lexicon of the grammar boosted precision compared to the confidence cut-off.", "labels": [], "entities": [{"text": "precision", "start_pos": 61, "end_pos": 70, "type": "METRIC", "confidence": 0.9996353387832642}]}, {"text": "The difference between the effects of my method on XTAG and ERG would be due to the finer-grained SCF types of ERG.", "labels": [], "entities": [{"text": "XTAG", "start_pos": 51, "end_pos": 55, "type": "DATASET", "confidence": 0.776776134967804}]}, {"text": "This resulted in lower precision of the acquired SCFs for ERG, which prevented us from distinguishing infrequent (correct) SCFs from SCFs acquired in error.", "labels": [], "entities": [{"text": "precision", "start_pos": 23, "end_pos": 32, "type": "METRIC", "confidence": 0.9991484880447388}]}, {"text": "However, since unusual SCFs tend to be included in the lexicon, we will be able to have accurate clusters for unknown words with smaller SCF variations as we achieved in the experiments with XTAG.", "labels": [], "entities": [{"text": "XTAG", "start_pos": 191, "end_pos": 195, "type": "DATASET", "confidence": 0.8848593235015869}]}], "tableCaptions": []}