{"title": [{"text": "A Unified Framework for Automatic Evaluation using N-gram Co-Occurrence Statistics", "labels": [], "entities": []}], "abstractContent": [{"text": "In this paper we propose a unified framework for automatic evaluation of NLP applications using N-gram co-occurrence statistics.", "labels": [], "entities": []}, {"text": "The automatic evaluation metrics proposed to date for Machine Translation and Automatic Summarization are particular instances from the family of metrics we propose.", "labels": [], "entities": [{"text": "Machine Translation", "start_pos": 54, "end_pos": 73, "type": "TASK", "confidence": 0.8411955535411835}, {"text": "Automatic Summarization", "start_pos": 78, "end_pos": 101, "type": "TASK", "confidence": 0.6824948340654373}]}, {"text": "We show that different members of the same family of metrics explain best the variations obtained with human evaluations, according to the application being evaluated (Machine Translation, Automatic Summarization, and Automatic Question Answering) and the evaluation guidelines used by humans for evaluating such applications.", "labels": [], "entities": [{"text": "Machine Translation", "start_pos": 168, "end_pos": 187, "type": "TASK", "confidence": 0.7798768281936646}, {"text": "Automatic Summarization", "start_pos": 189, "end_pos": 212, "type": "TASK", "confidence": 0.6977419257164001}, {"text": "Question Answering", "start_pos": 228, "end_pos": 246, "type": "TASK", "confidence": 0.6799072623252869}]}], "introductionContent": [{"text": "With the introduction of the BLEU metric for machine translation evaluation (), the advantages of doing automatic evaluation for various NLP applications have become increasingly appreciated: they allow for faster implement-evaluate cycles (by by-passing the human evaluation bottleneck), less variation in evaluation performance due to errors inhuman assessor judgment, and, not least, the possibility of hill-climbing on such metrics in order to improve system performance.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 29, "end_pos": 33, "type": "METRIC", "confidence": 0.9926069974899292}, {"text": "machine translation evaluation", "start_pos": 45, "end_pos": 75, "type": "TASK", "confidence": 0.8532349864641825}]}, {"text": "Recently, a second proposal for automatic evaluation has come from the Automatic Summarization community (, with an automatic evaluation metric called ROUGE, inspired by BLEU but twisted towards the specifics of the summarization task.", "labels": [], "entities": [{"text": "Automatic Summarization", "start_pos": 71, "end_pos": 94, "type": "TASK", "confidence": 0.6754307448863983}, {"text": "ROUGE", "start_pos": 151, "end_pos": 156, "type": "METRIC", "confidence": 0.9938510656356812}, {"text": "BLEU", "start_pos": 170, "end_pos": 174, "type": "METRIC", "confidence": 0.9794684052467346}, {"text": "summarization task", "start_pos": 216, "end_pos": 234, "type": "TASK", "confidence": 0.9141241312026978}]}, {"text": "An automatic evaluation metric is said to be successful if it is shown to have high agreement with human-performed evaluations.", "labels": [], "entities": []}, {"text": "Human evaluations, however, are subject to specific guidelines given to the human assessors when performing the evaluation task; the variation inhuman judgment is therefore highly influenced by these guidelines.", "labels": [], "entities": []}, {"text": "It follows that, in order for an automatic evaluation to agree with a humanperformed evaluation, the evaluation metric used by the automatic method must be able to account, at least to some degree, for the bias induced by the human evaluation guidelines.", "labels": [], "entities": []}, {"text": "None of the automatic evaluation methods proposed to date, however, explicitly accounts for the different criteria followed by the human assessors, as they are defined independently of the guidelines used in the human evaluations.", "labels": [], "entities": []}, {"text": "In this paper, we propose a framework for automatic evaluation of NLP applications which is able to account for the variation in the human evaluation guidelines.", "labels": [], "entities": []}, {"text": "We define a family of metrics based on N-gram co-occurrence statistics, for which the automatic evaluation metrics proposed to date for Machine Translation and Automatic Summarization can be seen as particular instances.", "labels": [], "entities": [{"text": "Machine Translation and Automatic Summarization", "start_pos": 136, "end_pos": 183, "type": "TASK", "confidence": 0.718891853094101}]}, {"text": "We show that different members of the same family of metrics explain best the variations obtained with human evaluations, according to the application being evaluated (Machine Translation, Automatic Summarization, and Question Answering) and the guidelines used by humans when evaluating such applications.", "labels": [], "entities": [{"text": "Machine Translation", "start_pos": 168, "end_pos": 187, "type": "TASK", "confidence": 0.784600704908371}, {"text": "Automatic Summarization", "start_pos": 189, "end_pos": 212, "type": "TASK", "confidence": 0.6965294182300568}, {"text": "Question Answering", "start_pos": 218, "end_pos": 236, "type": "TASK", "confidence": 0.792671799659729}]}], "datasetContent": [{"text": "In this section we describe an evaluation plane on which we place various NLP applications evaluated using various guideline packages.", "labels": [], "entities": []}, {"text": "This evaluation plane is defined by two orthogonal axes (see): an Application Axis, on which we order NLP applications according to the faithfulness/compactness ratio that characterizes the application's input and output; and a Guideline Axis, on which we order various human guideline packages, according to the precision/recall ratio that characterizes the evaluation guidelines.", "labels": [], "entities": [{"text": "precision", "start_pos": 313, "end_pos": 322, "type": "METRIC", "confidence": 0.9976919889450073}, {"text": "recall", "start_pos": 323, "end_pos": 329, "type": "METRIC", "confidence": 0.7564988136291504}]}, {"text": "When trying to define what translating and summarizing means, one can arguably suggest that a translation is some \"as-faithful-as-possible\" rendering of some given input, whereas a summary is some \"as-compact-as-possible\" rendering of some given input.", "labels": [], "entities": [{"text": "translating", "start_pos": 27, "end_pos": 38, "type": "TASK", "confidence": 0.976332426071167}, {"text": "summarizing", "start_pos": 43, "end_pos": 54, "type": "TASK", "confidence": 0.6922838091850281}]}, {"text": "As such, Machine Translation (MT) and Automatic Summarization (AS) are on the extremes of a faithfulness/compactness (f/c) ratio between inputs and outputs.", "labels": [], "entities": [{"text": "Machine Translation (MT)", "start_pos": 9, "end_pos": 33, "type": "TASK", "confidence": 0.8685286402702331}, {"text": "Automatic Summarization (AS)", "start_pos": 38, "end_pos": 66, "type": "TASK", "confidence": 0.8322330474853515}, {"text": "faithfulness/compactness (f/c) ratio", "start_pos": 92, "end_pos": 128, "type": "METRIC", "confidence": 0.7010878721872965}]}, {"text": "In between these two extremes lie various other NLP applications: a high f/c ratio, although lower than MT's, characterizes Automatic Paraphrasing (paraphrase: To express, interpret, or translate with latitude); close to the other extreme, a low f/c ratio, although higher than AS's, characterizes Automatic Summarization with view-points (summarization which needs to focus on a given point of view, extern to the document(s) to be summarized).", "labels": [], "entities": [{"text": "Automatic Summarization", "start_pos": 298, "end_pos": 321, "type": "TASK", "confidence": 0.6812798380851746}]}, {"text": "Another NLP application, Automatic Question Answering (QA), has arguably a close-to-1 f/c ratio: the task is to render an answer about the thing(s) inquired for in a question (the faithfulness side), in a manner that is concise enough to be regarded as a useful answer (the compactness side).", "labels": [], "entities": [{"text": "Automatic Question Answering (QA)", "start_pos": 25, "end_pos": 58, "type": "TASK", "confidence": 0.7501155634721121}]}, {"text": "Formal human evaluations make use of various guidelines that specify what particular aspects of the output being evaluated are considered important, for the particular application being evaluated.", "labels": [], "entities": []}, {"text": "For example, human evaluations of MT (e.g., TIDES 2002 evaluation, performed by NIST) have traditionally looked at two different aspects of a translation: adequacy (how much of the content of the original sentence is captured by the proposed translation) and fluency (how correct is the proposed translation sentence in the target language).", "labels": [], "entities": [{"text": "MT", "start_pos": 34, "end_pos": 36, "type": "TASK", "confidence": 0.9781584739685059}]}, {"text": "In many instances, evaluation guidelines can be linearly ordered according to the precision/recall (p/r) ratio they specify.", "labels": [], "entities": [{"text": "precision/recall (p/r) ratio", "start_pos": 82, "end_pos": 110, "type": "METRIC", "confidence": 0.8941284881697761}]}, {"text": "For example, evaluation guidelines for adequacy evaluation of MT have a low p/r ratio, because of the high emphasis on recall (i.e., content is rewarded) and low emphasis on precision (i.e., verbosity is not penalized); on the other hand, evaluation guidelines for fluency of MT have a high p/r ratio, because of the low emphasis on recall (i.e., content is not rewarded) and high emphasis on wording (i.e., extraneous words are penalized).", "labels": [], "entities": [{"text": "MT", "start_pos": 62, "end_pos": 64, "type": "TASK", "confidence": 0.9216013550758362}, {"text": "recall", "start_pos": 119, "end_pos": 125, "type": "METRIC", "confidence": 0.9930570125579834}, {"text": "precision", "start_pos": 174, "end_pos": 183, "type": "METRIC", "confidence": 0.9971592426300049}, {"text": "MT", "start_pos": 276, "end_pos": 278, "type": "TASK", "confidence": 0.9262837171554565}, {"text": "recall", "start_pos": 333, "end_pos": 339, "type": "METRIC", "confidence": 0.985500156879425}]}, {"text": "Another evaluation we consider in this paper, the DUC 2001 evaluation for Automatic Summarization (also performed by NIST), had specific guidelines for coverage evaluation, which means a low p/r ratio, because of the high emphasis on recall (i.e., content is rewarded).", "labels": [], "entities": [{"text": "DUC 2001 evaluation", "start_pos": 50, "end_pos": 69, "type": "DATASET", "confidence": 0.9514828523000082}, {"text": "Automatic Summarization", "start_pos": 74, "end_pos": 97, "type": "TASK", "confidence": 0.6235654652118683}, {"text": "NIST", "start_pos": 117, "end_pos": 121, "type": "DATASET", "confidence": 0.9295194149017334}, {"text": "recall", "start_pos": 234, "end_pos": 240, "type": "METRIC", "confidence": 0.9977895021438599}]}, {"text": "Last but not least, the QA evaluation for correctness we discuss in Section 4 has a close-to-1 p/r ratio for evaluation guidelines (i.e., both correct content and precise answer wording are rewarded).", "labels": [], "entities": []}, {"text": "When combined, the application axis and the guideline axis define a plane in which particular evaluations are placed according to their application/guideline coordinates.", "labels": [], "entities": []}, {"text": "In we illustrate this evaluation plane, and the evaluation examples mentioned above are placed in this plane according to their coordinates.", "labels": [], "entities": []}, {"text": "The precision-focused metric family PS(N) and the recall-focused metric family RS(N) defined in the previous sections are unified under the metric family AEv(\u03b1,N), defined as: This formula extends the well-known F-measure that combines recall and precision numbers into a single number, by combining recall and precision metric families into a single metric family.", "labels": [], "entities": [{"text": "precision-focused", "start_pos": 4, "end_pos": 21, "type": "METRIC", "confidence": 0.9771053791046143}, {"text": "recall-focused metric family RS", "start_pos": 50, "end_pos": 81, "type": "METRIC", "confidence": 0.9037758708000183}, {"text": "AEv", "start_pos": 154, "end_pos": 157, "type": "METRIC", "confidence": 0.8918443918228149}]}, {"text": "For \u03b1=0, AEv(\u03b1,N) is the same as the recall-focused family of metrics RS(N); for \u03b1=1, AEv(\u03b1,\uf02c N) is the same as the precision-focused family of metrics PS(N).", "labels": [], "entities": [{"text": "AEv", "start_pos": 9, "end_pos": 12, "type": "METRIC", "confidence": 0.997511625289917}, {"text": "recall-focused", "start_pos": 37, "end_pos": 51, "type": "METRIC", "confidence": 0.9611708521842957}, {"text": "AEv", "start_pos": 86, "end_pos": 89, "type": "METRIC", "confidence": 0.9926129579544067}, {"text": "precision-focused", "start_pos": 116, "end_pos": 133, "type": "METRIC", "confidence": 0.9487023949623108}]}, {"text": "For \u03b1 in between 0 and 1, AEv(\u03b1,N) are metrics that balance recall and precision according to \u03b1.", "labels": [], "entities": [{"text": "AEv", "start_pos": 26, "end_pos": 29, "type": "METRIC", "confidence": 0.9989670515060425}, {"text": "recall", "start_pos": 60, "end_pos": 66, "type": "METRIC", "confidence": 0.9984562397003174}, {"text": "precision", "start_pos": 71, "end_pos": 80, "type": "METRIC", "confidence": 0.9986793398857117}]}, {"text": "For the rest of the paper, we restrict the parameters of the AEv(\u03b1,N) family as follows: \u03b1 varies continuously in, N varies discretely in {1,2,3,4}, the linear weights w n are 1/N, the brevity constant is 1, the wordiness constant is 2, the list of stop-words SW is our own 626 stop-word list, and the stemming function ST is the one defined by the Porter stemmer.", "labels": [], "entities": [{"text": "AEv", "start_pos": 61, "end_pos": 64, "type": "METRIC", "confidence": 0.7811304926872253}, {"text": "ST", "start_pos": 320, "end_pos": 322, "type": "METRIC", "confidence": 0.976565420627594}]}, {"text": "We establish a correspondence between the parameters of the family of metrics AEv(\u03b1,N) and the evaluation plane in as follows: \u03b1 parameterizes the guideline axis (x-axis) of the plane, such that \u03b1=0 corresponds to a low precision/recall (p/r) ratio, and \u03b1=1 corresponds to a high p/r ratio; N parameterizes the application axis (y-axis) of the plane, such that N=1 corresponds to a low faithfulness/compactness (f/c) ratio (unigram statistics allow fora low representation of faithfulness, but a high representation of compactness), and N=4 corresponds to a high f/c ratio (n-gram statistics up to 4-grams allow fora high representation of faithfulness, but a low representation of compactness).", "labels": [], "entities": [{"text": "precision/recall (p/r) ratio", "start_pos": 220, "end_pos": 248, "type": "METRIC", "confidence": 0.8757627805074056}]}, {"text": "This framework enables us to predict that a human-performed evaluation is best approximated by metrics that have similar f/c ratio as the application being evaluated and similar p/r ratio as the evaluation package used by the human assessors.", "labels": [], "entities": []}, {"text": "For example, an application with a high f/c ratio, evaluated using a low p/r ratio evaluation guideline package (an example of this is the adequacy evaluation for MT in TIDES 2002), is best approximated by the automatic evaluation metric defined by a low \u03b1 and a high N; an application with a close-to-1 f/c ratio, evaluated using an evaluation guideline package characterized by a close-to-1 p/r ratio (such as the correctness evaluation for Question Answering in Section 4.3) is best approximated by an automatic metric defined by a median \u03b1 and a median N.", "labels": [], "entities": [{"text": "MT in TIDES 2002", "start_pos": 163, "end_pos": 179, "type": "TASK", "confidence": 0.6821131706237793}, {"text": "Question Answering in Section 4.3", "start_pos": 443, "end_pos": 476, "type": "TASK", "confidence": 0.7956135690212249}]}, {"text": "In this section, we present empirical results regarding the ability of our family of metrics to approximate human evaluations of various applications under various evaluation guidelines.", "labels": [], "entities": []}, {"text": "We measure the amount of approximation of a human evaluation by an automatic evaluation as the value of the coefficient of determination R 2 between the human evaluation scores and the automatic evaluation scores for various systems implementing Machine Translation, Summarization, and Question Answering applications.", "labels": [], "entities": [{"text": "coefficient of determination R 2", "start_pos": 108, "end_pos": 140, "type": "METRIC", "confidence": 0.6969565987586975}, {"text": "Machine Translation, Summarization", "start_pos": 246, "end_pos": 280, "type": "TASK", "confidence": 0.7785318195819855}, {"text": "Question Answering", "start_pos": 286, "end_pos": 304, "type": "TASK", "confidence": 0.7848457396030426}]}, {"text": "In this framework, the coefficient of determination R 2 is to be interpreted as the percentage from the total variation of the human evaluation (that is, why some system's output is better than some other system's output, from the human evaluator's perspective) that is captured by the automatic evaluation (that is, why some system's output is better than some other system's output, from the automatic evaluation perspective).", "labels": [], "entities": [{"text": "coefficient of determination R 2", "start_pos": 23, "end_pos": 55, "type": "METRIC", "confidence": 0.7094354093074798}]}, {"text": "The values of R 2 vary between 0 and 1, with a value of 1 indicating that the automatic evaluation explains perfectly the human evaluation variation, and a value of 0 indicating that the automatic evaluation explains nothing from the human evaluation variation.", "labels": [], "entities": []}, {"text": "All the results for the values of R 2 for the family of metrics AEv(\u03b1,N) are reported with \u03b1 varying from 0 to 1 in 0.1 increments, and N varying from 1 to 4.", "labels": [], "entities": [{"text": "AEv", "start_pos": 64, "end_pos": 67, "type": "METRIC", "confidence": 0.9690520763397217}]}, {"text": "The Machine Translation evaluation carried out by NIST in 2002 for DARPA's TIDES programme involved 7 systems that participated in the Chinese-English track.", "labels": [], "entities": [{"text": "Machine Translation", "start_pos": 4, "end_pos": 23, "type": "TASK", "confidence": 0.8269942998886108}]}, {"text": "Each system was evaluated by a human judge, using one reference extracted from a list of 4 available reference translations.", "labels": [], "entities": []}, {"text": "Each of the 878 test sentences was evaluated both for adequacy (how much of the content of the original sentence is captured by the proposed translation) and fluency (how correct is the proposed translation sentence in the target language).", "labels": [], "entities": []}, {"text": "From the publicly available data for this evaluation, we compute the values of R 2 for 7 data points (corresponding to the 7 systems participating in the Chinese-English track), using as a reference set one of the 4 sets of reference translations available.", "labels": [], "entities": []}, {"text": "In, we present the values of the coefficient of determination R 2 for the family of metrics AEv(\u03b1,N), when considering only the fluency scores from the human evaluation.", "labels": [], "entities": [{"text": "coefficient of determination R 2", "start_pos": 33, "end_pos": 65, "type": "METRIC", "confidence": 0.6871154963970184}, {"text": "AEv", "start_pos": 92, "end_pos": 95, "type": "METRIC", "confidence": 0.9916219711303711}]}, {"text": "As mentioned in Section 2, the evaluation guidelines for fluency have a high precision/recall ratio, whereas MT is an application with a high faithfulness/compactness ratio.", "labels": [], "entities": [{"text": "precision", "start_pos": 77, "end_pos": 86, "type": "METRIC", "confidence": 0.9988135099411011}, {"text": "recall ratio", "start_pos": 87, "end_pos": 99, "type": "METRIC", "confidence": 0.9643147587776184}, {"text": "MT", "start_pos": 109, "end_pos": 111, "type": "TASK", "confidence": 0.683351993560791}]}, {"text": "In this case, our evaluation framework predicts that the automatic evaluation metrics that explain most of the variation in the human evaluation must have a high \u03b1 and a high N.", "labels": [], "entities": []}, {"text": "As seen in, our evaluation framework correctly predicts the automatic evaluation metrics that explain most of the variation in the human evaluation: metrics AEv, AEv(0.9,3), and AEv(1,4) capture most of the variation: 79.04%, 78.94%, and 78.87%, respectively.", "labels": [], "entities": [{"text": "AEv", "start_pos": 157, "end_pos": 160, "type": "METRIC", "confidence": 0.9940897226333618}, {"text": "AEv", "start_pos": 162, "end_pos": 165, "type": "METRIC", "confidence": 0.9892885684967041}, {"text": "AEv", "start_pos": 178, "end_pos": 181, "type": "METRIC", "confidence": 0.9961942434310913}]}, {"text": "Since metric AEv(1,4) is almost the same as the BLEU metric (modulo stemming and stop word elimination for unigrams), our results confirm the current practice in the Machine Translation community, which commonly uses BLEU for automatic evaluation.", "labels": [], "entities": [{"text": "AEv", "start_pos": 13, "end_pos": 16, "type": "METRIC", "confidence": 0.9611383676528931}, {"text": "BLEU", "start_pos": 48, "end_pos": 52, "type": "METRIC", "confidence": 0.9853687882423401}, {"text": "Machine Translation", "start_pos": 166, "end_pos": 185, "type": "TASK", "confidence": 0.8179640471935272}, {"text": "BLEU", "start_pos": 217, "end_pos": 221, "type": "METRIC", "confidence": 0.9535813927650452}]}, {"text": "For comparison purposes, we also computed the value of R 2 for fluency using the BLEU score formula given in (), for the 7 systems using the same one reference, and we obtained a similar value, 78.52%; computing the value of R 2 for fluency using the BLEU scores computed with all 4 references available yielded a lower value for R 2 , 64.96%, although BLEU scores obtained with multiple references are usually considered more reliable.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 81, "end_pos": 91, "type": "METRIC", "confidence": 0.9755934774875641}, {"text": "BLEU", "start_pos": 251, "end_pos": 255, "type": "METRIC", "confidence": 0.9786118865013123}, {"text": "BLEU", "start_pos": 353, "end_pos": 357, "type": "METRIC", "confidence": 0.9964196681976318}]}, {"text": "In, we present the values of the coefficient of determination R 2 for the family of metrics AEv(\u03b1,N), when considering only the adequacy scores from the human evaluation.", "labels": [], "entities": [{"text": "coefficient of determination R 2", "start_pos": 33, "end_pos": 65, "type": "METRIC", "confidence": 0.7072148203849793}, {"text": "AEv", "start_pos": 92, "end_pos": 95, "type": "METRIC", "confidence": 0.9895122647285461}]}, {"text": "As mentioned in Section 2, the evaluation guidelines for adequacy have a low precision/recall ratio, whereas MT is an application with high faithfulness/compactness ratio.", "labels": [], "entities": [{"text": "precision", "start_pos": 77, "end_pos": 86, "type": "METRIC", "confidence": 0.9988829493522644}, {"text": "recall ratio", "start_pos": 87, "end_pos": 99, "type": "METRIC", "confidence": 0.9568317830562592}, {"text": "MT", "start_pos": 109, "end_pos": 111, "type": "TASK", "confidence": 0.6717391610145569}]}, {"text": "In this case, our evaluation framework predicts that the automatic evaluation metrics that explain most of the variation in the human evaluation must have a low \u03b1 and a high N.", "labels": [], "entities": []}, {"text": "As seen in, our evaluation framework correctly predicts the automatic evaluation metric that explains most of the variation in the human evaluation: metric AEv(0,4) captures most of the variation, 83.04%.", "labels": [], "entities": [{"text": "AEv", "start_pos": 156, "end_pos": 159, "type": "METRIC", "confidence": 0.9758985042572021}]}, {"text": "For comparison purposes, we also computed the value of R 2 for adequacy using the BLEU score formula given in (), for the 7 systems using the same one reference, and we obtain a similar value, 83.91%; computing the value of R 2 for adequacy using the BLEU scores computed with all 4 references available also yielded a lower value for R 2 , 62.21%.", "labels": [], "entities": [{"text": "BLEU score", "start_pos": 82, "end_pos": 92, "type": "METRIC", "confidence": 0.9783665537834167}, {"text": "BLEU", "start_pos": 251, "end_pos": 255, "type": "METRIC", "confidence": 0.9752505421638489}, {"text": "R", "start_pos": 335, "end_pos": 336, "type": "METRIC", "confidence": 0.9158602952957153}]}, {"text": "The Automatic Summarization evaluation carried out by NIST for the DUC 2001 conference involved 15 participating systems.", "labels": [], "entities": [{"text": "Automatic Summarization", "start_pos": 4, "end_pos": 27, "type": "TASK", "confidence": 0.6298041045665741}, {"text": "NIST for the DUC 2001 conference", "start_pos": 54, "end_pos": 86, "type": "DATASET", "confidence": 0.8150111635526022}]}, {"text": "We focus hereon the multi-document summarization task, in which 4 generic summaries (of 50, 100, 200, and 400 words) were required fora given set of documents on a single subject.", "labels": [], "entities": [{"text": "multi-document summarization", "start_pos": 20, "end_pos": 48, "type": "TASK", "confidence": 0.6136927008628845}]}, {"text": "For this evaluation 30 test sets were used, and each system was evaluated by a human judge using one reference extracted from a list of 2 reference summaries.", "labels": [], "entities": []}, {"text": "One of the evaluations required the assessors to judge the coverage of the summaries.", "labels": [], "entities": []}, {"text": "The coverage of a summary was measured by comparing a system's units versus the units of a reference summary, and assessing whether each system unit expresses all, most, some, hardly any, or none of the current reference unit.", "labels": [], "entities": [{"text": "coverage", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.9849366545677185}]}, {"text": "A final evaluation score for coverage was obtained using a coverage score computed as a weighted recall score (see (Lin and Hovy 2003) for more information on the human summary evaluation).", "labels": [], "entities": [{"text": "coverage", "start_pos": 29, "end_pos": 37, "type": "TASK", "confidence": 0.9272704720497131}, {"text": "recall score", "start_pos": 97, "end_pos": 109, "type": "METRIC", "confidence": 0.9694695472717285}]}, {"text": "From the publicly available data for this evaluation, we compute the values of R 2 for 15 data points available (corresponding to the 15 participating systems).", "labels": [], "entities": []}, {"text": "In, for adequacy scores in MT evaluation scores from the human evaluation, for summaries of 200 and 400 words, respectively (the values of R 2 for summaries of 50 and 100 words show similar patterns).", "labels": [], "entities": [{"text": "MT evaluation", "start_pos": 27, "end_pos": 40, "type": "TASK", "confidence": 0.8999989330768585}]}, {"text": "As mentioned in Section 2, the evaluation guidelines for coverage have a low precision/recall ratio, whereas AS is an application with low faithfulness/compactness ratio.", "labels": [], "entities": [{"text": "coverage", "start_pos": 57, "end_pos": 65, "type": "TASK", "confidence": 0.9614063501358032}, {"text": "precision", "start_pos": 77, "end_pos": 86, "type": "METRIC", "confidence": 0.9988904595375061}, {"text": "recall ratio", "start_pos": 87, "end_pos": 99, "type": "METRIC", "confidence": 0.9472208619117737}, {"text": "AS", "start_pos": 109, "end_pos": 111, "type": "METRIC", "confidence": 0.8608626127243042}]}, {"text": "In this case, our evaluation framework predicts that the automatic evaluation metrics that explain most of the variation in the human evaluation must have a low \u03b1 and a low N.", "labels": [], "entities": []}, {"text": "As seen in, our evaluation framework correctly predicts the automatic evaluation metric that explain most of the variation in the human evaluation: metric AEv(0,1) explains 90.77% and 92.28% of the variation in the human evaluation of summaries of length 200 and 400, respectively.", "labels": [], "entities": [{"text": "AEv", "start_pos": 155, "end_pos": 158, "type": "METRIC", "confidence": 0.9746667742729187}]}, {"text": "Since metric AEv(0, 1) is almost the same as the ROUGE metric proposed by Lin and Hovy (2003) (they only differ in the stop-word list they use), our results also confirm the proposal for such metrics to be used for automatic evaluation by the Automatic Summarization community.", "labels": [], "entities": [{"text": "AEv", "start_pos": 13, "end_pos": 16, "type": "METRIC", "confidence": 0.9714177250862122}, {"text": "ROUGE", "start_pos": 49, "end_pos": 54, "type": "METRIC", "confidence": 0.8992893695831299}, {"text": "Automatic Summarization", "start_pos": 243, "end_pos": 266, "type": "TASK", "confidence": 0.6414150595664978}]}, {"text": "One of the most common approaches to automatic question answering (QA) restricts the domain of questions to be handled to so-called factoid questions.", "labels": [], "entities": [{"text": "question answering (QA)", "start_pos": 47, "end_pos": 70, "type": "TASK", "confidence": 0.8581451356410981}]}, {"text": "Automatic evaluation of factoid QA is often straightforward, as the number of correct answers is most of the time limited, and exhaustive lists of correct answers are available.", "labels": [], "entities": [{"text": "evaluation of factoid QA", "start_pos": 10, "end_pos": 34, "type": "TASK", "confidence": 0.7332877516746521}]}, {"text": "When removing the factoid constraint, however, the set of possible answer to a (complex, beyondfactoid) question becomes unfeasibly large, and consequently automatic evaluation becomes a challenge.", "labels": [], "entities": []}, {"text": "In this section, we focus on an evaluation carried out in order to assess the performance of a QA system for answering questions from the Frequently-Asked-Question (FAQ) domain ().", "labels": [], "entities": []}, {"text": "These are generally questions requiring a more elaborated answer than a simple factoid (e.g., questions such as: \"How does a film qualify for an Academy Award?\").", "labels": [], "entities": []}, {"text": "In order to evaluate such a system a humanperformed evaluation was performed, in which 11 versions of the QA system (various modules were implemented using various algorithms) were separately evaluated.", "labels": [], "entities": []}, {"text": "Each version was evaluated by a human evaluator, with no reference answer available.", "labels": [], "entities": []}, {"text": "For this evaluation 115 test questions were used, and the human evaluator was asked to assess whether the proposed answer was correct, somehow related, or wrong.", "labels": [], "entities": []}, {"text": "A unique ranking number was achieved using a weighted average of the scored answers.", "labels": [], "entities": []}, {"text": "(See () for more details concerning the QA task and the evaluation procedure.)", "labels": [], "entities": [{"text": "QA task", "start_pos": 40, "end_pos": 47, "type": "TASK", "confidence": 0.8718813061714172}]}, {"text": "One important aspect in the evaluation procedure was devising criteria for assigning a rating to an answer which was not neither correct nor wrong.", "labels": [], "entities": []}, {"text": "One of such cases involved so-called flooded answers: answers which contain the correct information, along with several other unrelated pieces of information.", "labels": [], "entities": []}, {"text": "A first evaluation has been carried with a guideline package asking the human assessor to assign the rating correct to flooded answers.", "labels": [], "entities": []}, {"text": "In, we present the values of the coefficient of determination R 2 for the family of metrics AEv(\u03b1,N) for this first QA evaluation.", "labels": [], "entities": [{"text": "AEv", "start_pos": 92, "end_pos": 95, "type": "METRIC", "confidence": 0.9881672263145447}]}, {"text": "On the guideline side, the guideline package used in this first QA evaluation has a low precision/recall ratio, because the human judge is asked to evaluate based on the content provided by a given answer (high recall), but is asked to disregard the conciseness (or lack thereof) of the answer (low precision); consequently, systems that focus on   giving correct and concise answers are not distinguished from systems that give correct answers, but have no regard for concision.", "labels": [], "entities": [{"text": "QA evaluation", "start_pos": 64, "end_pos": 77, "type": "TASK", "confidence": 0.8380056917667389}, {"text": "precision", "start_pos": 88, "end_pos": 97, "type": "METRIC", "confidence": 0.9976351261138916}, {"text": "recall ratio", "start_pos": 98, "end_pos": 110, "type": "METRIC", "confidence": 0.966560035943985}, {"text": "recall", "start_pos": 211, "end_pos": 217, "type": "METRIC", "confidence": 0.9846577048301697}, {"text": "precision", "start_pos": 299, "end_pos": 308, "type": "METRIC", "confidence": 0.9594332575798035}]}, {"text": "On the application side, as mentioned in Section 2, QA is arguably an application characterized by a closeto-1 faithfulness/compactness ratio.", "labels": [], "entities": []}, {"text": "In this case, our evaluation framework predicts that the automatic evaluation metrics that explain most of the variation in the human evaluation must have a low \u03b1 and a median N.", "labels": [], "entities": []}, {"text": "As seen in, our evaluation framework correctly predicts the automatic evaluation metric that explain most of the variation in the human evaluation: metric AEv(0,2) explains most of the human variation, 91.72%.", "labels": [], "entities": [{"text": "AEv", "start_pos": 155, "end_pos": 158, "type": "METRIC", "confidence": 0.9753184914588928}]}, {"text": "Note that other members of the AEv(\uf02c \u03b1,N) family do not explain nearly as well the variation in the human evaluation.", "labels": [], "entities": [{"text": "AEv", "start_pos": 31, "end_pos": 34, "type": "METRIC", "confidence": 0.944081723690033}]}, {"text": "For example, the ROUGE-like metric AEv(0,1) explains only 61.61% of the human variation, while the BLEUlike metric AEv(1,4) explains a mere 17.7% of the human variation (to use such a metric in order to automatically emulate the human QA evaluation is close to performing an evaluation assigning random ratings to the output answers).", "labels": [], "entities": [{"text": "ROUGE-like metric AEv(0,1)", "start_pos": 17, "end_pos": 43, "type": "METRIC", "confidence": 0.8872669736544291}, {"text": "BLEUlike metric AEv(1,4)", "start_pos": 99, "end_pos": 123, "type": "METRIC", "confidence": 0.9042935073375702}]}, {"text": "In order to further test the prediction power of our evaluation framework, we carried out a second QA evaluation, using a different evaluation guideline package: a flooded answer was rated only somehow-related.", "labels": [], "entities": []}, {"text": "In, we present the values of the coefficient of determination R 2 for the family of metrics AEv(\u03b1,N) for this second QA evaluation.", "labels": [], "entities": [{"text": "AEv", "start_pos": 92, "end_pos": 95, "type": "METRIC", "confidence": 0.9889277219772339}]}, {"text": "Instead of performing this second evaluation from scratch, we actually simulated it using the following methodology: 2/3 of the output answers rated correct of the systems ranked 1 st , 2 nd , 3 rd , and 6 th by the previous human evaluation have been intentionally over-flooded using two long and out-of-context sentences, while their ratings were changed from correct to somehow-related.", "labels": [], "entities": []}, {"text": "Such a change simulated precisely the change in the guideline package, by downgrading flooded answers.", "labels": [], "entities": []}, {"text": "This means that, on the guideline side, the guideline package used in this second QA evaluation has a close-to-1 precision/recall ratio, because the human judge evaluates now based both on the content and the conciseness of a given answer.", "labels": [], "entities": [{"text": "precision", "start_pos": 113, "end_pos": 122, "type": "METRIC", "confidence": 0.9985235333442688}, {"text": "recall ratio", "start_pos": 123, "end_pos": 135, "type": "METRIC", "confidence": 0.9626929759979248}]}, {"text": "At the same time, the application remains unchanged, which means that on the application side we still have a close-to-1 faithfulness/compactness ratio.", "labels": [], "entities": []}, {"text": "In this case, our evaluation framework predicts that the automatic evaluation metrics that explain most of the variation in the human evaluation must have a median \u03b1 and a median N.", "labels": [], "entities": []}, {"text": "As seen in, our evaluation framework correctly predicts the automatic evaluation metric that explain most of the variation in the human evaluation: metric AEv(0.3,2) explains most of the variation in the human evaluation, 86.26%.", "labels": [], "entities": [{"text": "AEv", "start_pos": 155, "end_pos": 158, "type": "METRIC", "confidence": 0.9744735956192017}]}, {"text": "Also note that, while the R 2 values around AEv(0.3,2) are still reasonable, evaluation metrics that are further and further away from it have increasingly lower R 2 values, meaning that they are more and more unreliable for this task.", "labels": [], "entities": [{"text": "AEv", "start_pos": 44, "end_pos": 47, "type": "METRIC", "confidence": 0.9962266683578491}]}, {"text": "The high correlation of metric AEv(0.3,2) with human judgment, however, suggests that such a metric is a good candidate for performing automatic evaluation of QA systems that go beyond answering factoid questions.", "labels": [], "entities": [{"text": "AEv", "start_pos": 31, "end_pos": 34, "type": "METRIC", "confidence": 0.8287608623504639}]}], "tableCaptions": [{"text": " Table 1: R 2 values for the family of metrics AEv(\u03b1,N), for fluency scores in MT evaluation", "labels": [], "entities": [{"text": "AEv", "start_pos": 47, "end_pos": 50, "type": "METRIC", "confidence": 0.9943364262580872}, {"text": "MT evaluation", "start_pos": 79, "end_pos": 92, "type": "TASK", "confidence": 0.9022451341152191}]}, {"text": " Table 2: R 2 values for the family of metrics AEv(\u03b1,", "labels": [], "entities": [{"text": "AEv", "start_pos": 47, "end_pos": 50, "type": "METRIC", "confidence": 0.9869276881217957}]}, {"text": " Table 3: R 2 for the family of metrics AEv(\u03b1,N), for coverage scores in AS evaluation (200 words)", "labels": [], "entities": [{"text": "AEv", "start_pos": 40, "end_pos": 43, "type": "METRIC", "confidence": 0.9880555272102356}, {"text": "coverage", "start_pos": 54, "end_pos": 62, "type": "METRIC", "confidence": 0.9347575902938843}]}, {"text": " Table 4: R 2 for the family of metrics AEv(\u03b1,N), for coverage scores in AS evaluation (400 words)", "labels": [], "entities": [{"text": "AEv", "start_pos": 40, "end_pos": 43, "type": "METRIC", "confidence": 0.9877052307128906}, {"text": "coverage", "start_pos": 54, "end_pos": 62, "type": "METRIC", "confidence": 0.9380724430084229}]}, {"text": " Table 5: R 2 for the family of metrics AEv(\u03b1,N), for correctness scores, first QA evaluation", "labels": [], "entities": [{"text": "AEv", "start_pos": 40, "end_pos": 43, "type": "METRIC", "confidence": 0.9927480816841125}]}, {"text": " Table 6: R 2 for the family of metrics AEv(\u03b1,", "labels": [], "entities": [{"text": "AEv", "start_pos": 40, "end_pos": 43, "type": "METRIC", "confidence": 0.9773799777030945}]}]}