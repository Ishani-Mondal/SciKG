{"title": [{"text": "Automatic Evaluation of Machine Translation Quality Using Longest Com- mon Subsequence and Skip-Bigram Statistics", "labels": [], "entities": [{"text": "Machine Translation Quality", "start_pos": 24, "end_pos": 51, "type": "TASK", "confidence": 0.769402543703715}]}], "abstractContent": [{"text": "In this paper we describe two new objective automatic evaluation methods for machine translation.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 77, "end_pos": 96, "type": "TASK", "confidence": 0.8098434209823608}]}, {"text": "The first method is based on longest common subsequence between a candidate translation and a set of reference translations.", "labels": [], "entities": []}, {"text": "Longest common subsequence takes into account sentence level structure similarity naturally and identifies longest co-occurring in-sequence n-grams automatically.", "labels": [], "entities": []}, {"text": "The second method relaxes strict n-gram matching to skip-bigram matching.", "labels": [], "entities": []}, {"text": "Skip-bigram is any pair of words in their sentence order.", "labels": [], "entities": []}, {"text": "Skip-bigram co-occurrence statistics measure the overlap of skip-bigrams between a candidate translation and a set of reference translations.", "labels": [], "entities": []}, {"text": "The empirical results show that both methods correlate with human judgments very well in both adequacy and fluency.", "labels": [], "entities": []}], "introductionContent": [{"text": "Using objective functions to automatically evaluate machine translation quality is not new.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 52, "end_pos": 71, "type": "TASK", "confidence": 0.7752273678779602}]}, {"text": "proposed a method based on measuring edit distance) between candidate and reference translations.", "labels": [], "entities": []}, {"text": "extended the idea to accommodate multiple references.", "labels": [], "entities": []}, {"text": "calculated the lengthnormalized edit distance, called word error rate (WER), between a candidate and multiple reference translations.", "labels": [], "entities": [{"text": "lengthnormalized edit distance", "start_pos": 15, "end_pos": 45, "type": "METRIC", "confidence": 0.8782390157381693}, {"text": "word error rate (WER)", "start_pos": 54, "end_pos": 75, "type": "METRIC", "confidence": 0.8928234378496805}]}, {"text": "proposed a related measure called position-independent word error rate (PER) that did not consider word position, i.e. using bag-of-words instead.", "labels": [], "entities": [{"text": "position-independent word error rate (PER)", "start_pos": 34, "end_pos": 76, "type": "METRIC", "confidence": 0.7723377176693508}]}, {"text": "Instead of error measures, we can also use accuracy measures that compute similarity between candidate and reference translations in proportion to the number of common words between them as suggested by.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 43, "end_pos": 51, "type": "METRIC", "confidence": 0.9989768266677856}]}, {"text": "An n-gram co-occurrence measure, BLEU, proposed by that calculates co-occurrence statistics based on n-gram overlaps have shown great potential.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 33, "end_pos": 37, "type": "METRIC", "confidence": 0.997277557849884}]}, {"text": "A variant of BLEU developed by has been used in two recent large-scale machine translation evaluations.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 13, "end_pos": 17, "type": "METRIC", "confidence": 0.9895390868186951}, {"text": "machine translation evaluations", "start_pos": 71, "end_pos": 102, "type": "TASK", "confidence": 0.8376719752947489}]}, {"text": "Recently, indicated that standard accuracy measures such as recall, precision, and the F-measure can also be used in evaluation of machine translation.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 34, "end_pos": 42, "type": "METRIC", "confidence": 0.998120129108429}, {"text": "recall", "start_pos": 60, "end_pos": 66, "type": "METRIC", "confidence": 0.999479353427887}, {"text": "precision", "start_pos": 68, "end_pos": 77, "type": "METRIC", "confidence": 0.9992455244064331}, {"text": "F-measure", "start_pos": 87, "end_pos": 96, "type": "METRIC", "confidence": 0.9950705766677856}, {"text": "machine translation", "start_pos": 131, "end_pos": 150, "type": "TASK", "confidence": 0.7969241142272949}]}, {"text": "However, results based on their method, General Text Matcher (GTM), showed that unigram F-measure correlated best with human judgments while assigning more weight to higher n-gram (n > 1) matches achieved similar performance as Bleu.", "labels": [], "entities": [{"text": "General Text Matcher (GTM", "start_pos": 40, "end_pos": 65, "type": "TASK", "confidence": 0.6883042395114899}]}, {"text": "Since unigram matches do not distinguish words in consecutive positions from words in the wrong order, measures based on position-independent unigram matches are not sensitive to word order and sentence level structure.", "labels": [], "entities": []}, {"text": "Therefore, systems optimized for these unigram-based measures might generate adequate but not fluent target language.", "labels": [], "entities": []}, {"text": "Since BLEU has been used to report the performance of many machine translation systems and it has been shown to correlate well with human judgments, we will explain BLEU in more detail and point out its limitations in the next section.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 6, "end_pos": 10, "type": "METRIC", "confidence": 0.9944688081741333}, {"text": "machine translation", "start_pos": 59, "end_pos": 78, "type": "TASK", "confidence": 0.719115138053894}, {"text": "BLEU", "start_pos": 165, "end_pos": 169, "type": "METRIC", "confidence": 0.9901379346847534}]}, {"text": "We then introduce anew evaluation method called ROUGE-L that measures sentence-to-sentence similarity based on the longest common subsequence statistics between a candidate translation and a set of reference translations in Section 3.", "labels": [], "entities": [{"text": "ROUGE-L", "start_pos": 48, "end_pos": 55, "type": "METRIC", "confidence": 0.9794304370880127}]}, {"text": "Section 4 describes another automatic evaluation method called ROUGE-S that computes skipbigram co-occurrence statistics.", "labels": [], "entities": [{"text": "ROUGE-S", "start_pos": 63, "end_pos": 70, "type": "METRIC", "confidence": 0.8793081045150757}]}, {"text": "Section 5 presents the evaluation results of ROUGE-L, and ROUGE-S and compare them with BLEU, GTM, NIST, PER, and WER in correlation with human judgments in terms of adequacy and fluency.", "labels": [], "entities": [{"text": "ROUGE-L", "start_pos": 45, "end_pos": 52, "type": "METRIC", "confidence": 0.9574344754219055}, {"text": "ROUGE-S", "start_pos": 58, "end_pos": 65, "type": "METRIC", "confidence": 0.8414397239685059}, {"text": "BLEU", "start_pos": 88, "end_pos": 92, "type": "METRIC", "confidence": 0.9986129999160767}, {"text": "PER", "start_pos": 105, "end_pos": 108, "type": "METRIC", "confidence": 0.9912950992584229}, {"text": "WER", "start_pos": 114, "end_pos": 117, "type": "METRIC", "confidence": 0.9932937622070312}]}, {"text": "We conclude this paper and discuss extensions of the current work in Section 6.", "labels": [], "entities": []}], "datasetContent": [{"text": "One of the goals of developing automatic evaluation measures is to replace labor-intensive human evaluations.", "labels": [], "entities": []}, {"text": "Therefore the first criterion to assess the usefulness of an automatic evaluation measure is to show that it correlates highly with human judgments in different evaluation settings.", "labels": [], "entities": []}, {"text": "However, high quality large-scale human judgments are hard to come by.", "labels": [], "entities": []}, {"text": "Fortunately, we have access to eight MT systems' outputs, their human assessment data, and the reference translations from 2003 NIST Chinese MT evaluation).", "labels": [], "entities": [{"text": "MT", "start_pos": 37, "end_pos": 39, "type": "TASK", "confidence": 0.9776268005371094}, {"text": "NIST Chinese MT evaluation", "start_pos": 128, "end_pos": 154, "type": "DATASET", "confidence": 0.7993257790803909}]}, {"text": "There were 919 sentence segments in the corpus.", "labels": [], "entities": []}, {"text": "We first computed averages of the adequacy and fluency scores of each system assigned by human evaluators.", "labels": [], "entities": []}, {"text": "For the input of automatic evaluation methods, we created three evaluation sets from the MT outputs: 1.", "labels": [], "entities": []}, {"text": "Case set: The original system outputs with case information.", "labels": [], "entities": []}, {"text": "2. NoCase set: All words were converted into lowercase, i.e. no case information was used.", "labels": [], "entities": []}, {"text": "This set was used to examine whether human assessments were affected by case information since not all MT systems generate properly cased output.", "labels": [], "entities": [{"text": "MT", "start_pos": 103, "end_pos": 105, "type": "TASK", "confidence": 0.9698716402053833}]}, {"text": "3. Stem set: All words were converted into lowercase and stemmed using the Porter stemmer.", "labels": [], "entities": []}, {"text": "Since ROUGE computed similarity on surface word level, stemmed version allowed ROUGE to perform more lenient matches.", "labels": [], "entities": []}, {"text": "To accommodate multiple references, we use a Jackknifing procedure.", "labels": [], "entities": []}, {"text": "Given N references, we compute the best score over N sets of N-1 references.", "labels": [], "entities": []}, {"text": "The final score is the average of the N best scores using N different sets of N-1 references.", "labels": [], "entities": []}, {"text": "The Jackknifing procedure is adopted since we often need to compare system and human performance and the reference translations are usually the only human translations available.", "labels": [], "entities": []}, {"text": "Using this procedure, we are able to estimate average human performance by averaging N best scores of one reference vs. the rest N-1 references.", "labels": [], "entities": []}, {"text": "We then computed average BLEU1-12 , GTM with exponents of 1.0, 2.0, and 3.0, NIST, WER, and PER scores over these three sets.", "labels": [], "entities": [{"text": "BLEU1-12", "start_pos": 25, "end_pos": 33, "type": "METRIC", "confidence": 0.9942875504493713}, {"text": "GTM", "start_pos": 36, "end_pos": 39, "type": "DATASET", "confidence": 0.9002113938331604}, {"text": "NIST", "start_pos": 77, "end_pos": 81, "type": "METRIC", "confidence": 0.785176157951355}, {"text": "WER", "start_pos": 83, "end_pos": 86, "type": "METRIC", "confidence": 0.9894654750823975}, {"text": "PER", "start_pos": 92, "end_pos": 95, "type": "METRIC", "confidence": 0.9974431991577148}]}, {"text": "Finally we applied ROUGE-L, ROUGE-W with weighting function k 1.2 , and ROUGE-S without skip distance limit and with skip distant limits of 0, 4, and 9.", "labels": [], "entities": [{"text": "ROUGE-L", "start_pos": 19, "end_pos": 26, "type": "METRIC", "confidence": 0.895394504070282}, {"text": "ROUGE-S", "start_pos": 72, "end_pos": 79, "type": "METRIC", "confidence": 0.8727989792823792}]}, {"text": "Correlation analysis based on two different correlation statistics, Pearson's \u03c1 and Spearman's \u03c1, with respect to adequacy and fluency are shown in Table 1.", "labels": [], "entities": [{"text": "Pearson's \u03c1", "start_pos": 68, "end_pos": 79, "type": "METRIC", "confidence": 0.8795546293258667}, {"text": "Spearman's \u03c1", "start_pos": 84, "end_pos": 96, "type": "METRIC", "confidence": 0.7375664313634237}]}, {"text": "The Pearson's correlation coefficient measures the strength and direction of a linear relationship between any two variables, i.e. automatic metric score and human assigned mean coverage score in our case.", "labels": [], "entities": [{"text": "Pearson's correlation coefficient", "start_pos": 4, "end_pos": 37, "type": "METRIC", "confidence": 0.8432871550321579}, {"text": "automatic metric score", "start_pos": 131, "end_pos": 153, "type": "METRIC", "confidence": 0.9186883767445883}, {"text": "human assigned mean coverage score", "start_pos": 158, "end_pos": 192, "type": "METRIC", "confidence": 0.5835173845291137}]}, {"text": "It ranges from +1 to -1.", "labels": [], "entities": []}, {"text": "A correlation of 1 means that there is a perfect positive linear relationship between the two variables, a correlation of -1 means that there is a perfect negative linear relationship between them, and a correlation of 0 means that there is no linear relationship between them.", "labels": [], "entities": []}, {"text": "Since we would like to use automatic evaluation metric not only in comparing systems but also in in-house system development, a good linear correlation with human judgment would enable us to use automatic scores to predict corresponding human judgment scores.", "labels": [], "entities": []}, {"text": "Therefore, Pearson's correlation coefficient is a good measure to look at.", "labels": [], "entities": [{"text": "Pearson's correlation coefficient", "start_pos": 11, "end_pos": 44, "type": "METRIC", "confidence": 0.8647730946540833}]}, {"text": "Spearman's correlation coefficient 6 is also a measure of correlation between two variables.", "labels": [], "entities": [{"text": "Spearman's correlation coefficient 6", "start_pos": 0, "end_pos": 36, "type": "METRIC", "confidence": 0.6413678646087646}]}, {"text": "It is a non-parametric measure and is a special case of the Pearson's correlation coefficient when the values of data are converted into ranks before computing the coefficient.", "labels": [], "entities": [{"text": "Pearson's correlation coefficient", "start_pos": 60, "end_pos": 93, "type": "METRIC", "confidence": 0.7451781332492828}]}, {"text": "Spearman's correlation coefficient does not assume the correlation between the variables is linear.", "labels": [], "entities": [{"text": "correlation coefficient", "start_pos": 11, "end_pos": 34, "type": "METRIC", "confidence": 0.9552268087863922}]}, {"text": "Therefore it is a useful correlation indicator even when good linear correlation, for example, according to Pearson's correlation coefficient between two variables could.", "labels": [], "entities": []}, {"text": "Pearson's \u03c1 and Spearman's \u03c1 correlations of automatic evaluation measures vs. adequacy and fluency: BLEU1, 4, and 12 are BLEU with maximum of 1, 4, and 12 grams, NIST is the NIST score, ROUGE-L is LCS-based F-measure (\u03b2 = 1), ROUGE-W is weighted LCS-based F-measure (\u03b2 = 1).", "labels": [], "entities": [{"text": "BLEU1", "start_pos": 101, "end_pos": 106, "type": "METRIC", "confidence": 0.9980762004852295}, {"text": "BLEU", "start_pos": 122, "end_pos": 126, "type": "METRIC", "confidence": 0.9919203519821167}, {"text": "NIST", "start_pos": 163, "end_pos": 167, "type": "DATASET", "confidence": 0.7072210311889648}, {"text": "NIST score", "start_pos": 175, "end_pos": 185, "type": "DATASET", "confidence": 0.7885241210460663}, {"text": "ROUGE-L", "start_pos": 187, "end_pos": 194, "type": "METRIC", "confidence": 0.9348049759864807}, {"text": "ROUGE-W", "start_pos": 227, "end_pos": 234, "type": "METRIC", "confidence": 0.9569894671440125}]}, {"text": "ROUGE-S* is skip-bigram-based co-occurrence statistics with any skip distance limit, ROUGE-SN is skip-bigram-based F-measure (\u03b2 = 1) with maximum skip distance of N, PER is position independent word error rate, and WER is word error rate.", "labels": [], "entities": [{"text": "ROUGE-SN", "start_pos": 85, "end_pos": 93, "type": "METRIC", "confidence": 0.8998669385910034}, {"text": "F-measure", "start_pos": 115, "end_pos": 124, "type": "METRIC", "confidence": 0.8134403228759766}, {"text": "PER", "start_pos": 166, "end_pos": 169, "type": "METRIC", "confidence": 0.9723198413848877}, {"text": "position independent word error rate", "start_pos": 173, "end_pos": 209, "type": "METRIC", "confidence": 0.6402068495750427}, {"text": "WER", "start_pos": 215, "end_pos": 218, "type": "METRIC", "confidence": 0.9964639544487}, {"text": "word error rate", "start_pos": 222, "end_pos": 237, "type": "METRIC", "confidence": 0.737143317858378}]}, {"text": "GTM 10, 20, and 30 are general text matcher with exponents of 1.0, 2.0, and 3.0.", "labels": [], "entities": [{"text": "GTM 10", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.9224534034729004}]}, {"text": "(Note, only BLEU1, 4, and 12 are shown hereto preserve space.) not be found.", "labels": [], "entities": [{"text": "BLEU1", "start_pos": 12, "end_pos": 17, "type": "METRIC", "confidence": 0.9959068298339844}]}, {"text": "It also suits the NIST MT evaluation scenario where multiple systems are ranked according to some performance metrics.", "labels": [], "entities": [{"text": "NIST MT evaluation", "start_pos": 18, "end_pos": 36, "type": "TASK", "confidence": 0.6156911452611288}]}, {"text": "To estimate the significance of these correlation statistics, we applied bootstrap resampling, generating random samples of the 919 different sentence segments.", "labels": [], "entities": []}, {"text": "The lower and upper values of 95% confidence interval are also shown in the table.", "labels": [], "entities": [{"text": "95% confidence interval", "start_pos": 30, "end_pos": 53, "type": "METRIC", "confidence": 0.6914295256137848}]}, {"text": "Dark (green) cells are the best correlation numbers in their categories and light gray cells are statistically equivalent to the best numbers in their categories.", "labels": [], "entities": []}, {"text": "Analyzing all runs according to the adequacy and fluency table, we make the following observations: Applying the stemmer achieves higher correlation with adequacy but keeping case information achieves higher correlation with fluency except for BLEU7-12 (only BLEU12 is shown).", "labels": [], "entities": [{"text": "BLEU7-12", "start_pos": 244, "end_pos": 252, "type": "METRIC", "confidence": 0.9944531321525574}, {"text": "BLEU12", "start_pos": 259, "end_pos": 265, "type": "METRIC", "confidence": 0.981769323348999}]}, {"text": "For example, the Pearson's \u03c1 (P) correlation of ROUGE-S* with adequacy increases from 0.85 (Case) to 0.95 (Stem) while its Pearson's \u03c1 correlation with fluency drops from 0.84 (Case) to 0.78 (Stem).", "labels": [], "entities": [{"text": "Pearson's \u03c1 (P) correlation", "start_pos": 17, "end_pos": 44, "type": "METRIC", "confidence": 0.9002321192196437}, {"text": "ROUGE-S", "start_pos": 48, "end_pos": 55, "type": "METRIC", "confidence": 0.8078158497810364}, {"text": "Pearson's \u03c1 correlation", "start_pos": 123, "end_pos": 146, "type": "METRIC", "confidence": 0.6450848281383514}]}, {"text": "We will focus our discussions on the Stem set inadequacy and Case set in fluency.", "labels": [], "entities": [{"text": "Case set", "start_pos": 61, "end_pos": 69, "type": "METRIC", "confidence": 0.9255771636962891}]}, {"text": "The Pearson's \u03c1 correlation values in the Stem set of the Adequacy indicates that ROUGE-L and ROUGE-S with a skip distance longer than 0 correlate highly and linearly with adequacy and outperform BLEU and NIST.", "labels": [], "entities": [{"text": "Pearson's \u03c1 correlation", "start_pos": 4, "end_pos": 27, "type": "METRIC", "confidence": 0.8221685737371445}, {"text": "ROUGE-L", "start_pos": 82, "end_pos": 89, "type": "METRIC", "confidence": 0.9453054070472717}, {"text": "ROUGE-S", "start_pos": 94, "end_pos": 101, "type": "METRIC", "confidence": 0.8988039493560791}, {"text": "BLEU", "start_pos": 196, "end_pos": 200, "type": "METRIC", "confidence": 0.9975979924201965}, {"text": "NIST", "start_pos": 205, "end_pos": 209, "type": "DATASET", "confidence": 0.9660571217536926}]}, {"text": "ROUGE-S* achieves that best correlation with a Pearson's \u03c1 of 0.95.", "labels": [], "entities": [{"text": "ROUGE-S", "start_pos": 0, "end_pos": 7, "type": "METRIC", "confidence": 0.9581785798072815}, {"text": "Pearson's \u03c1", "start_pos": 47, "end_pos": 58, "type": "METRIC", "confidence": 0.9178738792737325}]}, {"text": "Measures favoring consecutive matches, i.e. BLEU4 and 12, ROUGE-W, GTM20 and 30, ROUGE-S0 (bigram), and WER have lower Pearson's \u03c1.", "labels": [], "entities": [{"text": "BLEU4", "start_pos": 44, "end_pos": 49, "type": "METRIC", "confidence": 0.9962460398674011}, {"text": "ROUGE-W", "start_pos": 58, "end_pos": 65, "type": "METRIC", "confidence": 0.9906208515167236}, {"text": "ROUGE-S0", "start_pos": 81, "end_pos": 89, "type": "METRIC", "confidence": 0.957745373249054}, {"text": "WER", "start_pos": 104, "end_pos": 107, "type": "METRIC", "confidence": 0.9923807978630066}, {"text": "Pearson's \u03c1", "start_pos": 119, "end_pos": 130, "type": "METRIC", "confidence": 0.9797962109247843}]}, {"text": "Among them WER (0.48) that tends to penalize small word movement is the worst performer.", "labels": [], "entities": [{"text": "WER", "start_pos": 11, "end_pos": 14, "type": "METRIC", "confidence": 0.9982082843780518}, {"text": "penalize small word movement", "start_pos": 36, "end_pos": 64, "type": "TASK", "confidence": 0.5623147487640381}]}, {"text": "One interesting observation is that longer BLEU has lower correlation with adequacy.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 43, "end_pos": 47, "type": "METRIC", "confidence": 0.9987291693687439}]}, {"text": "Spearman's \u03c1 values generally agree with Pearson's \u03c1 but have more equivalents.", "labels": [], "entities": []}, {"text": "The Pearson's \u03c1 correlation values in the Stem set of the Fluency indicates that BLEU12 has the highest correlation (0.93) with fluency.", "labels": [], "entities": [{"text": "Pearson's \u03c1 correlation", "start_pos": 4, "end_pos": 27, "type": "METRIC", "confidence": 0.9211979061365128}, {"text": "Fluency", "start_pos": 58, "end_pos": 65, "type": "METRIC", "confidence": 0.751636266708374}, {"text": "BLEU12", "start_pos": 81, "end_pos": 87, "type": "METRIC", "confidence": 0.9978869557380676}]}, {"text": "However, it is statistically indistinguishable with 95% confidence from all other metrics shown in the Case set of the Fluency GTM10 has good correlation with human judgments inadequacy but not fluency; while GTM20 and GTM30, i.e. GTM with exponent larger than 1.0, has good correlation with human judgment in fluency but not adequacy.", "labels": [], "entities": [{"text": "Case set", "start_pos": 103, "end_pos": 111, "type": "DATASET", "confidence": 0.828020304441452}, {"text": "Fluency GTM10", "start_pos": 119, "end_pos": 132, "type": "DATASET", "confidence": 0.8295621275901794}]}, {"text": "ROUGE-L and ROUGE-S*, 4, and 9 are good automatic evaluation metric candidates since they perform as well as BLEU in fluency correlation analysis and outperform BLEU4 and 12 significantly inadequacy.", "labels": [], "entities": [{"text": "ROUGE-L", "start_pos": 0, "end_pos": 7, "type": "METRIC", "confidence": 0.8598656058311462}, {"text": "ROUGE-S", "start_pos": 12, "end_pos": 19, "type": "METRIC", "confidence": 0.8883283138275146}, {"text": "BLEU", "start_pos": 109, "end_pos": 113, "type": "METRIC", "confidence": 0.99870765209198}, {"text": "BLEU4", "start_pos": 161, "end_pos": 166, "type": "METRIC", "confidence": 0.9954570531845093}]}, {"text": "Among them, ROUGE-L is the best metric in both adequacy and fluency correlation with human judgment according to Spearman's correlation coefficient and is statistically indistinguishable from the best metrics in both adequacy and fluency correlation with human judgment according to Pearson's correlation coefficient.", "labels": [], "entities": [{"text": "ROUGE-L", "start_pos": 12, "end_pos": 19, "type": "METRIC", "confidence": 0.9961417317390442}]}], "tableCaptions": []}