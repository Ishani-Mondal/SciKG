{"title": [{"text": "Extending the BLEU MT Evaluation Method with Frequency Weightings", "labels": [], "entities": [{"text": "BLEU", "start_pos": 14, "end_pos": 18, "type": "METRIC", "confidence": 0.9937280416488647}, {"text": "MT Evaluation", "start_pos": 19, "end_pos": 32, "type": "TASK", "confidence": 0.8765330016613007}]}], "abstractContent": [{"text": "We present the results of an experiment on extending the automatic method of Machine Translation evaluation BLUE with statistical weights for lexical items, such as tf.idf scores.", "labels": [], "entities": [{"text": "Machine Translation evaluation", "start_pos": 77, "end_pos": 107, "type": "TASK", "confidence": 0.8693228960037231}, {"text": "BLUE", "start_pos": 108, "end_pos": 112, "type": "METRIC", "confidence": 0.6582573652267456}]}, {"text": "We show that this extension gives additional information about evaluated texts; in particular it allows us to measure translation Adequacy, which, for statistical MT systems, is often overestimated by the baseline BLEU method.", "labels": [], "entities": [{"text": "translation", "start_pos": 118, "end_pos": 129, "type": "TASK", "confidence": 0.8823686242103577}, {"text": "Adequacy", "start_pos": 130, "end_pos": 138, "type": "METRIC", "confidence": 0.8954735994338989}, {"text": "MT", "start_pos": 163, "end_pos": 165, "type": "TASK", "confidence": 0.9240840673446655}, {"text": "BLEU", "start_pos": 214, "end_pos": 218, "type": "METRIC", "confidence": 0.9969164133071899}]}, {"text": "The proposed model uses a single human reference translation, which increases the usability of the proposed method for practical purposes.", "labels": [], "entities": []}, {"text": "The model suggests a linguistic interpretation which relates frequency weights and human intuition about translation Adequacy and Fluency.", "labels": [], "entities": [{"text": "Fluency", "start_pos": 130, "end_pos": 137, "type": "METRIC", "confidence": 0.8960429430007935}]}], "introductionContent": [{"text": "Automatic methods for evaluating different aspects of MT quality -such as Adequacy, Fluency and Informativeness -provide an alternative to an expensive and time-consuming process of human MT evaluation.", "labels": [], "entities": [{"text": "MT", "start_pos": 54, "end_pos": 56, "type": "TASK", "confidence": 0.9946008920669556}, {"text": "Adequacy", "start_pos": 74, "end_pos": 82, "type": "METRIC", "confidence": 0.9968861937522888}, {"text": "Fluency", "start_pos": 84, "end_pos": 91, "type": "METRIC", "confidence": 0.9952669143676758}, {"text": "Informativeness", "start_pos": 96, "end_pos": 111, "type": "METRIC", "confidence": 0.9613956809043884}, {"text": "MT evaluation", "start_pos": 188, "end_pos": 201, "type": "TASK", "confidence": 0.970346987247467}]}, {"text": "They are intended to yield scores that correlate with human judgments of translation quality and enable systems (machine or human) to be ranked on this basis.", "labels": [], "entities": []}, {"text": "Several such automatic methods have been proposed in recent years.", "labels": [], "entities": []}, {"text": "Some of them use human reference translations, e.g., the BLEU method (), which is based on comparison of N-gram models in MT output and in a set of human reference translations.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 57, "end_pos": 61, "type": "METRIC", "confidence": 0.9977561831474304}]}, {"text": "However, a serious problem for the BLEU method is the lack of a model for relative importance of matched and mismatched items.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 35, "end_pos": 39, "type": "METRIC", "confidence": 0.912727952003479}]}, {"text": "Words in text usually carry an unequal informational load, and as a result are of differing importance for translation.", "labels": [], "entities": [{"text": "translation", "start_pos": 107, "end_pos": 118, "type": "TASK", "confidence": 0.9741668105125427}]}, {"text": "It is reasonable to expect that the choices of right translation equivalents for certain key items, such as expressions denoting principal events, event participants and relations in a text are more important in the eyes of human evaluators then choices of function words and a syntactic perspective for sentences.", "labels": [], "entities": []}, {"text": "Accurate rendering of these key items by an MT system boosts the quality of translation.", "labels": [], "entities": [{"text": "MT", "start_pos": 44, "end_pos": 46, "type": "TASK", "confidence": 0.9396405220031738}]}, {"text": "Therefore, at least for evaluation of translation Adequacy (Fidelity), the proper choice of translation equivalents for important pieces of information should count more than the choice of words which are used for structural purposes and without a clear translation equivalent in the source text.", "labels": [], "entities": [{"text": "Fidelity", "start_pos": 60, "end_pos": 68, "type": "METRIC", "confidence": 0.4472334384918213}]}, {"text": "(The latter maybe more important for Fluency evaluation).", "labels": [], "entities": [{"text": "Fluency evaluation", "start_pos": 37, "end_pos": 55, "type": "TASK", "confidence": 0.7705946862697601}]}, {"text": "The problem of different significance of Ngram matches is related to the issue of legitimate variation inhuman translations, when certain words are less stable than others across independently produced human translations.", "labels": [], "entities": []}, {"text": "BLEU accounts for legitimate translation variation by using a set of several human reference translations, which are believed to be representative of several equally acceptable ways of translating any source segment.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 0, "end_pos": 4, "type": "METRIC", "confidence": 0.9085357785224915}, {"text": "translation variation", "start_pos": 29, "end_pos": 50, "type": "TASK", "confidence": 0.9529833495616913}]}, {"text": "This is motivated by the need not to penalise deviations from the set of Ngrams in a single reference, although the requirement of multiple human references makes automatic evaluation more expensive.", "labels": [], "entities": []}, {"text": "However, the \"significance\" problem is not directly addressed by the BLEU method.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 69, "end_pos": 73, "type": "METRIC", "confidence": 0.9955962300300598}]}, {"text": "On the one hand, the matched items that are present in several human references receive the same weights as items found in just one of the references.", "labels": [], "entities": []}, {"text": "On the other hand the model of legitimate translation variation cannot fully accommodate the issue of varying degrees of \"salience\" for matched lexical items, since alternative synonymic translation equivalents may also be highly significant for an adequate translation from the human perspective ().", "labels": [], "entities": []}, {"text": "Therefore it is reasonable to suggest that introduction of a model which approximates intuitions about the significance of the matched N-grams will improve the correlation between automatically computed MT evaluation scores and human evaluation scores for translation Adequacy.", "labels": [], "entities": [{"text": "MT evaluation", "start_pos": 203, "end_pos": 216, "type": "TASK", "confidence": 0.8384498059749603}, {"text": "translation", "start_pos": 256, "end_pos": 267, "type": "TASK", "confidence": 0.9760326147079468}]}, {"text": "In this paper we present the result of an experiment on augmenting BLEU N-gram comparison with statistical weight coefficients which capture a word's salience within a given document: the standard tf.idf measure used in the vector-space model for Information and the S-score proposed for evaluating MT output corpora for the purposes of Information Extraction (.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 67, "end_pos": 71, "type": "METRIC", "confidence": 0.9954195022583008}, {"text": "MT output corpora", "start_pos": 299, "end_pos": 316, "type": "TASK", "confidence": 0.8786833882331848}, {"text": "Information Extraction", "start_pos": 337, "end_pos": 359, "type": "TASK", "confidence": 0.6900647133588791}]}, {"text": "Both scores are computed for each term in each of the 100 human reference translations from French into English available in DARPA-94 MT evaluation corpus.", "labels": [], "entities": [{"text": "DARPA-94 MT evaluation corpus", "start_pos": 125, "end_pos": 154, "type": "DATASET", "confidence": 0.7703016847372055}]}, {"text": "The proposed weighted N-gram model for MT evaluation is tested on a set of translations by four different MT systems available in the DARPA corpus, and is compared with the results of the baseline BLEU method with respect to their correlation with human evaluation scores.", "labels": [], "entities": [{"text": "MT evaluation", "start_pos": 39, "end_pos": 52, "type": "TASK", "confidence": 0.9628256857395172}, {"text": "MT", "start_pos": 106, "end_pos": 108, "type": "TASK", "confidence": 0.9572769403457642}, {"text": "DARPA corpus", "start_pos": 134, "end_pos": 146, "type": "DATASET", "confidence": 0.9719347357749939}, {"text": "BLEU", "start_pos": 197, "end_pos": 201, "type": "METRIC", "confidence": 0.9966663718223572}]}, {"text": "The scores produced by the N-gram model with tf.idf and S-Score weights are shown to be consistent with baseline BLEU evaluation results for Fluency and outperform the BLEU scores for Adequacy (where the correlation for the S-score weighting is higher).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 113, "end_pos": 117, "type": "METRIC", "confidence": 0.9885983467102051}, {"text": "BLEU", "start_pos": 168, "end_pos": 172, "type": "METRIC", "confidence": 0.9974460601806641}]}, {"text": "We also show that the weighted model may still be reliably used if there is only one human reference translation for an evaluated text.", "labels": [], "entities": []}, {"text": "Besides saving cost, the ability to dependably work with a single human translation has an additional advantage: it is now possible to create Recall-based evaluation measures for MT, which has been problematic for evaluation with multiple reference translations, since only one of the choices from the reference set is used in translation (.", "labels": [], "entities": [{"text": "MT", "start_pos": 179, "end_pos": 181, "type": "TASK", "confidence": 0.9838621020317078}]}, {"text": "Notably, Recall of weighted N-grams is found to be a good estimation of human judgements about translation Adequacy.", "labels": [], "entities": [{"text": "Recall", "start_pos": 9, "end_pos": 15, "type": "METRIC", "confidence": 0.983876645565033}]}, {"text": "Using weighted N-grams is essential for predicting Adequacy, since correlation of Recall for non-weighted N-grams is much lower.", "labels": [], "entities": [{"text": "Adequacy", "start_pos": 51, "end_pos": 59, "type": "METRIC", "confidence": 0.9844682812690735}, {"text": "correlation", "start_pos": 67, "end_pos": 78, "type": "METRIC", "confidence": 0.9922124743461609}, {"text": "Recall", "start_pos": 82, "end_pos": 88, "type": "METRIC", "confidence": 0.9980553388595581}]}, {"text": "It is possible that other automatic methods which use human translations as a reference may also benefit from an introduction of an explicit model for term significance, since so far these methods also implicitly assume that all words are equally important inhuman translation, and use all of them, e.g., for measuring edit distances (.", "labels": [], "entities": []}, {"text": "The weighted N-gram model has been implemented as an MT evaluation toolkit (which includes a Perl script, example files and documentation).", "labels": [], "entities": [{"text": "MT evaluation", "start_pos": 53, "end_pos": 66, "type": "TASK", "confidence": 0.8882807493209839}]}, {"text": "It computes evaluation scores with tf.idf and S-score weights for translation Adequacy and Fluency.", "labels": [], "entities": [{"text": "translation", "start_pos": 66, "end_pos": 77, "type": "TASK", "confidence": 0.9280170798301697}, {"text": "Fluency", "start_pos": 91, "end_pos": 98, "type": "METRIC", "confidence": 0.9702956080436707}]}, {"text": "The toolkit is available at http://www.comp.leeds.ac.uk/bogdan/evalMT.html", "labels": [], "entities": []}], "datasetContent": [{"text": "The experiment used French-English translations available in the DARPA-94 MT evaluation corpus.", "labels": [], "entities": [{"text": "DARPA-94 MT evaluation corpus", "start_pos": 65, "end_pos": 94, "type": "DATASET", "confidence": 0.7951181381940842}]}, {"text": "The corpus contains 100 French news texts (each text is about 350 words long) translated into English by 5 different MT systems: \"Systran\", \"Reverso\", \"Globalink\", \"Metal\", \"Candide\" and scored by human evaluators; there are no human scores for \"Reverso\", which was added to the corpus on a later stage.", "labels": [], "entities": []}, {"text": "The corpus also contains 2 independent human translations of each text.", "labels": [], "entities": []}, {"text": "Human evaluation scores are available for each of the 400 texts translated by the 4 MT systems for 3 parameters of translation quality: \"Adequacy\", \"Fluency\" and \"Informativeness\".", "labels": [], "entities": [{"text": "Fluency", "start_pos": 149, "end_pos": 156, "type": "METRIC", "confidence": 0.9933766722679138}, {"text": "Informativeness", "start_pos": 163, "end_pos": 178, "type": "METRIC", "confidence": 0.9598467946052551}]}, {"text": "The Adequacy (Fidelity) scores are given on a 5-point scale by comparing MT with a human reference translation.", "labels": [], "entities": [{"text": "Adequacy (Fidelity) scores", "start_pos": 4, "end_pos": 30, "type": "METRIC", "confidence": 0.9106705069541932}, {"text": "MT", "start_pos": 73, "end_pos": 75, "type": "TASK", "confidence": 0.7373968958854675}]}, {"text": "The Adequacy parameter captures how much of the original content of a text is conveyed, regardless of how grammatically imperfect the output might be.", "labels": [], "entities": [{"text": "Adequacy", "start_pos": 4, "end_pos": 12, "type": "METRIC", "confidence": 0.9284731149673462}]}, {"text": "The Fluency scores (also given on a 5-point scale) determine intelligibility of MT without reference to the source text, i.e., how grammatical and stylistically natural the translation appears to be.", "labels": [], "entities": [{"text": "Fluency", "start_pos": 4, "end_pos": 11, "type": "METRIC", "confidence": 0.9902907013893127}, {"text": "MT", "start_pos": 80, "end_pos": 82, "type": "TASK", "confidence": 0.9821487665176392}]}, {"text": "The Informativeness scores (which we didn't use for our experiment) determine whether there is enough information in MT output to enable evaluators to answer multiplechoice questions on its content In the first stage of the experiment, each of the two sets of human translations was used to compute tf.idf and S-scores for each word in each of the 100 texts.", "labels": [], "entities": [{"text": "MT output", "start_pos": 117, "end_pos": 126, "type": "TASK", "confidence": 0.8411982655525208}]}, {"text": "The tf.idf score was calculated as: tf.idf(i,j) = (1 + log (tf i,j )) log (N / df i ), if tf i,j \u2265 1; where: -tf i,j is the number of occurrences of the word w i in the document d j ; -df i is the number of documents in the corpus where the word w i occurs; -N is the total number of documents in the corpus.", "labels": [], "entities": []}, {"text": "The S-score was calculated as: where: -P doc(i,j) is the relative frequency of the word in the text; (\"Relative frequency\" is the number of tokens of this word-type divided by the total number of tokens).", "labels": [], "entities": [{"text": "Relative frequency\"", "start_pos": 103, "end_pos": 122, "type": "METRIC", "confidence": 0.9735100865364075}]}, {"text": "-P corp-doc(i) is the relative frequency of the same word in the rest of the corpus, without this text; -(N -df (i) ) / N is the proportion of texts in the corpus, where this word does not occur (number of texts, where it is not found, divided by number of texts in the corpus); -P corp(i) is the relative frequency of the word in the whole corpus, including this particular text.", "labels": [], "entities": []}, {"text": "In the second stage we carried out N-gram based MT evaluation, measuring Precision and Recall of N-grams in MT output using a single human reference translation.", "labels": [], "entities": [{"text": "MT evaluation", "start_pos": 48, "end_pos": 61, "type": "TASK", "confidence": 0.908406138420105}, {"text": "Precision", "start_pos": 73, "end_pos": 82, "type": "METRIC", "confidence": 0.9991616010665894}, {"text": "Recall", "start_pos": 87, "end_pos": 93, "type": "METRIC", "confidence": 0.9886645078659058}, {"text": "MT output", "start_pos": 108, "end_pos": 117, "type": "TASK", "confidence": 0.8772885799407959}]}, {"text": "N-gram counts were adjusted with the tf.idf weights and S-scores for every matched word.", "labels": [], "entities": []}, {"text": "The following procedure was used to integrate the S-scores / tf.idf scores fora lexical item into N-gram counts.", "labels": [], "entities": []}, {"text": "For every word in a given text which received an S-score and tf.idf score on the basis of the human reference corpus, all counts for the N-grams containing this word are increased by the value of the respective score (not just by 1, as in the baseline BLEU approach).", "labels": [], "entities": [{"text": "BLEU", "start_pos": 252, "end_pos": 256, "type": "METRIC", "confidence": 0.9722108840942383}]}, {"text": "The original matches used for BLEU and the weighted matches are both calculated.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 30, "end_pos": 34, "type": "METRIC", "confidence": 0.9717842936515808}]}, {"text": "The following changes have been made to the Perl script of the BLEU tool: apart from the operator which increases counts for every matched N-gram $ngr by 1, i.e.: the following code was introduced: -where the hash data structure: represents the table of tf.idf scores or S-scores for words in every text in the corpus.", "labels": [], "entities": []}, {"text": "The weighted N-gram evaluation scores of Precision, Recall and F-measure maybe produced fora segment, fora text or fora corpus of translations generated by an MT system.", "labels": [], "entities": [{"text": "Precision", "start_pos": 41, "end_pos": 50, "type": "METRIC", "confidence": 0.879859447479248}, {"text": "Recall", "start_pos": 52, "end_pos": 58, "type": "METRIC", "confidence": 0.9575374126434326}, {"text": "F-measure", "start_pos": 63, "end_pos": 72, "type": "METRIC", "confidence": 0.9880374670028687}]}, {"text": "In the third stage of the experiment the weighted Precision and Recall scores were tested for correlation with human scores for the same texts and compared to the results of similar tests for standard BLEU evaluation.", "labels": [], "entities": [{"text": "Precision", "start_pos": 50, "end_pos": 59, "type": "METRIC", "confidence": 0.9438875317573547}, {"text": "Recall", "start_pos": 64, "end_pos": 70, "type": "METRIC", "confidence": 0.7913386821746826}, {"text": "BLEU", "start_pos": 201, "end_pos": 205, "type": "METRIC", "confidence": 0.7514529824256897}]}, {"text": "Finally we addressed the question whether the proposed MT evaluation method allows us to use a single human reference translation reliably.", "labels": [], "entities": [{"text": "MT evaluation", "start_pos": 55, "end_pos": 68, "type": "TASK", "confidence": 0.9264347553253174}]}, {"text": "In order to assess the stability of the weighted evaluation scores with a single reference, two runs of the experiment were carried out.", "labels": [], "entities": []}, {"text": "The first run used the \"Reference\" human translation, while the second run used the \"Expert\" human translation (each time a single reference translation was used).", "labels": [], "entities": []}, {"text": "The scores for both runs were compared using a standard deviation measure.", "labels": [], "entities": []}, {"text": "With respect to evaluating MT systems, the correlation for the weighted N-gram model was found to be stronger, for both Adequacy and Fluency, the improvement being highest for Adequacy.", "labels": [], "entities": [{"text": "MT", "start_pos": 27, "end_pos": 29, "type": "TASK", "confidence": 0.9911592602729797}, {"text": "Adequacy", "start_pos": 120, "end_pos": 128, "type": "METRIC", "confidence": 0.950981080532074}, {"text": "Fluency", "start_pos": 133, "end_pos": 140, "type": "METRIC", "confidence": 0.9929671287536621}]}, {"text": "These results are due to the fact that the weighted N-gram model gives much more accurate predictions about the statistical MT system \"Candide\", whereas the standard BLEU approach tends to over-estimate its performance for translation Adequacy.", "labels": [], "entities": [{"text": "MT", "start_pos": 124, "end_pos": 126, "type": "TASK", "confidence": 0.9483014345169067}, {"text": "BLEU", "start_pos": 166, "end_pos": 170, "type": "METRIC", "confidence": 0.9957446455955505}, {"text": "translation", "start_pos": 223, "end_pos": 234, "type": "TASK", "confidence": 0.9572789669036865}]}, {"text": "present the baseline results for nonweighted Precision, Recall and F-score.", "labels": [], "entities": [{"text": "Precision", "start_pos": 45, "end_pos": 54, "type": "METRIC", "confidence": 0.9916340112686157}, {"text": "Recall", "start_pos": 56, "end_pos": 62, "type": "METRIC", "confidence": 0.9982059001922607}, {"text": "F-score", "start_pos": 67, "end_pos": 74, "type": "METRIC", "confidence": 0.9984920024871826}]}, {"text": "It shows the following figures: -Human evaluation scores for Adequacy and Fluency (the mean scores for all texts produced by each MT system); -BLEU scores produced using 2 human reference translations and the default script settings (N-gram size = 4); -Precision, Recall and F-score for the weighted N-gram model produced with 1 human reference translation and N-gram size = 4.", "labels": [], "entities": [{"text": "Fluency", "start_pos": 74, "end_pos": 81, "type": "METRIC", "confidence": 0.8149978518486023}, {"text": "BLEU", "start_pos": 143, "end_pos": 147, "type": "METRIC", "confidence": 0.9992088675498962}, {"text": "Precision", "start_pos": 253, "end_pos": 262, "type": "METRIC", "confidence": 0.997422456741333}, {"text": "Recall", "start_pos": 264, "end_pos": 270, "type": "METRIC", "confidence": 0.9929854273796082}, {"text": "F-score", "start_pos": 275, "end_pos": 282, "type": "METRIC", "confidence": 0.9963911175727844}]}, {"text": "-Pearson's correlation coefficient r for Precision, Recall and F-score correlated with human scores for Adequacy and Fluency r(2) (with 2 degrees of freedom) for the sets which include scores for the 4 MT systems.", "labels": [], "entities": [{"text": "Pearson's correlation coefficient r", "start_pos": 1, "end_pos": 36, "type": "METRIC", "confidence": 0.6893578767776489}, {"text": "Precision", "start_pos": 41, "end_pos": 50, "type": "METRIC", "confidence": 0.965286910533905}, {"text": "Recall", "start_pos": 52, "end_pos": 58, "type": "METRIC", "confidence": 0.9818294048309326}, {"text": "F-score", "start_pos": 63, "end_pos": 70, "type": "METRIC", "confidence": 0.9953525066375732}, {"text": "Adequacy", "start_pos": 104, "end_pos": 112, "type": "METRIC", "confidence": 0.9918621778488159}, {"text": "Fluency r", "start_pos": 117, "end_pos": 126, "type": "METRIC", "confidence": 0.9413098990917206}]}, {"text": "The scores at the top of each cell show the results for the first run of the experiment, which used the \"Reference\" human translation; the scores at the bottom of the cells represent the results for the second run with the \"Expert\" human translation..", "labels": [], "entities": []}, {"text": "In this section we investigate how reliable is the use of a single human reference translation.", "labels": [], "entities": []}, {"text": "The stability of the scores is central to the issue of computing Recall and reducing the cost of automatic evaluation.", "labels": [], "entities": [{"text": "computing Recall", "start_pos": 55, "end_pos": 71, "type": "TASK", "confidence": 0.5412671864032745}]}, {"text": "We also would like to compare the stability of our results with the stability of the baseline non-weighted N-gram model using a single reference.", "labels": [], "entities": []}, {"text": "In this stage of the experiment we measured the changes that occur for the scores of MT systems if an alternative reference translation is used -both for the baseline N-gram counts and for the weighted N-gram model.", "labels": [], "entities": [{"text": "MT", "start_pos": 85, "end_pos": 87, "type": "TASK", "confidence": 0.968532919883728}]}, {"text": "Standard deviation was computed for each pair of evaluation scores produced by the two runs of the system with alternative human references.", "labels": [], "entities": []}, {"text": "An average of these standard deviations is the measure of stability fora given score.", "labels": [], "entities": []}, {"text": "The results of these calculations are presented in  Standard deviation for weighted scores is generally slightly higher, but both the baseline and the weighted N-gram approaches give relatively stable results: the average standard deviation was not greater than 0.0027, which means that both will produce reliable figures with just a single human reference translation (although interpretation of the score with a single reference should be different than with multiple references).", "labels": [], "entities": [{"text": "Standard", "start_pos": 52, "end_pos": 60, "type": "METRIC", "confidence": 0.973707377910614}]}, {"text": "Somewhat higher standard deviation figures for the weighted N-gram model confirm the suggestion that a word's importance for translation cannot be straightforwardly derived from the model of the legitimate translation variation implemented in BLEU and needs the salience weights, such as tf.idf or S-scores.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 243, "end_pos": 247, "type": "DATASET", "confidence": 0.6233100295066833}]}], "tableCaptions": [{"text": " Table 1. Baseline non-weighted scores.", "labels": [], "entities": []}, {"text": " Table 2. BLEU vs tf.idf weighted scores.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9991199374198914}]}, {"text": " Table 3. BLEU vs S-score weights.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9987897276878357}]}, {"text": " Table 4. Recall, Precision, and weighted scores", "labels": [], "entities": [{"text": "Recall", "start_pos": 10, "end_pos": 16, "type": "METRIC", "confidence": 0.9942504167556763}, {"text": "Precision", "start_pos": 18, "end_pos": 27, "type": "METRIC", "confidence": 0.9990845918655396}, {"text": "weighted scores", "start_pos": 33, "end_pos": 48, "type": "METRIC", "confidence": 0.8992561101913452}]}, {"text": " Table 5. Stability of scores", "labels": [], "entities": []}]}