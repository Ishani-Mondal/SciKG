{"title": [], "abstractContent": [{"text": "Ina multimodal conversation, the way users communicate with a system depends on the available interaction channels and the situated context (e.g., conversation focus, visual feedback).", "labels": [], "entities": []}, {"text": "These dependencies form a rich set of constraints from various perspectives such as temporal alignments between different modalities, coherence of conversation, and the domain semantics.", "labels": [], "entities": []}, {"text": "There is strong evidence that competition and ranking of these constraints is important to achieve an optimal interpretation.", "labels": [], "entities": []}, {"text": "Thus, we have developed an optimization approach for multimodal interpretation, particularly for interpreting multimodal references.", "labels": [], "entities": [{"text": "multimodal interpretation", "start_pos": 53, "end_pos": 78, "type": "TASK", "confidence": 0.7578548491001129}, {"text": "interpreting multimodal references", "start_pos": 97, "end_pos": 131, "type": "TASK", "confidence": 0.8512723445892334}]}, {"text": "A preliminary evaluation indicates the effectiveness of this approach, especially for complex user inputs that involve multiple referring expressions in a speech utterance and multiple gestures.", "labels": [], "entities": []}], "introductionContent": [{"text": "Multimodal systems provide a natural and effective way for users to interact with computers through multiple modalities such as speech, gesture, and gaze.", "labels": [], "entities": []}, {"text": "Since the first appearance of \"Put-That-There\" system, a variety of multimodal systems have emerged, from early systems that combine speech, pointing, and gaze (, to systems that integrate speech with pen inputs (e.g., drawn graphics) (), and systems that engage users in intelligent conversation.", "labels": [], "entities": []}, {"text": "One important aspect of building multimodal systems is multimodal interpretation, which is a process that identifies the meanings of user inputs.", "labels": [], "entities": [{"text": "multimodal interpretation", "start_pos": 55, "end_pos": 80, "type": "TASK", "confidence": 0.7525405585765839}]}, {"text": "Ina multimodal conversation, the way users communicate with a system depends on the available interaction channels and the situated context (e.g., conversation focus, visual feedback).", "labels": [], "entities": []}, {"text": "These dependencies form a rich set of constraints from various aspects (e.g., semantic, temporal, and contextual).", "labels": [], "entities": []}, {"text": "A correct interpretation can only be attained by simultaneously considering these constraints.", "labels": [], "entities": []}, {"text": "In this process, two issues are important: first, a mechanism to combine information from various sources to form an overall interpretation given a set of constraints; and second, a mechanism that achieves the best interpretation among all the possible alternatives given a set of constraints.", "labels": [], "entities": []}, {"text": "The first issue focuses on the fusion aspect, which has been well studied in earlier work, for example, through unificationbased approaches) or finite state approaches).", "labels": [], "entities": []}, {"text": "This paper focuses on the second issue of optimization.", "labels": [], "entities": []}, {"text": "As in natural language interpretation, there is strong evidence that competition and ranking of constraints is important to achieve an optimal interpretation for multimodal language processing.", "labels": [], "entities": [{"text": "natural language interpretation", "start_pos": 6, "end_pos": 37, "type": "TASK", "confidence": 0.6660463611284891}]}, {"text": "We have developed a graph-based optimization approach for interpreting multimodal references.", "labels": [], "entities": [{"text": "interpreting multimodal references", "start_pos": 58, "end_pos": 92, "type": "TASK", "confidence": 0.876291811466217}]}, {"text": "This approach achieves an optimal interpretation by simultaneously applying semantic, temporal, and contextual constraints.", "labels": [], "entities": []}, {"text": "A preliminary evaluation indicates the effectiveness of this approach, particularly for complex user inputs that involve multiple referring expressions in a speech utterance and multiple gestures.", "labels": [], "entities": []}, {"text": "In this paper, we first describe the necessities for optimization in multimodal interpretation, then present our graphbased optimization approach and discuss how our approach addresses key principles in Optimality Theory used for natural language interpretation (Prince and Smolensky 1993).", "labels": [], "entities": [{"text": "natural language interpretation", "start_pos": 230, "end_pos": 261, "type": "TASK", "confidence": 0.6841235160827637}]}], "datasetContent": [{"text": "We conducted several user studies to evaluate the performance of this approach.", "labels": [], "entities": []}, {"text": "Users could interact with our system using both speech and deictic gestures.", "labels": [], "entities": []}, {"text": "Each subject was asked to complete five tasks.", "labels": [], "entities": []}, {"text": "For example, one task was to find the cheapest house in the most populated town.", "labels": [], "entities": []}, {"text": "Data from eleven subjects was collected and analyzed.", "labels": [], "entities": []}, {"text": "shows the evaluation results of 219 inputs.", "labels": [], "entities": []}, {"text": "These inputs were categorized in terms of the number of referring expressions in the speech input and the number of gestures in the gesture inputs.", "labels": [], "entities": []}, {"text": "Out of the total 219 inputs, 137 inputs had their referents correctly interpreted.", "labels": [], "entities": []}, {"text": "For the remaining 82 inputs in which the referents were not correctly identified, the problem did not come from the approach itself, but rather from other sources such as speech recognition and language understanding errors.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 171, "end_pos": 189, "type": "TASK", "confidence": 0.7246457189321518}]}, {"text": "These were two major error sources, which were accounted for 55% and 20% of total errors respectively ().", "labels": [], "entities": []}, {"text": "In our studies, the majority of user references were simple in that they involved only one referring expression and one gesture as in earlier findings.", "labels": [], "entities": []}, {"text": "It is trivial for our approach to handle these simple inputs since the size of the graph is usually very small and there is only one node in the referring graph.", "labels": [], "entities": []}, {"text": "However, we did find 23% complex inputs (the row S3 and the column G3 in), which involved multiple referring expressions from speech utterances and/or multiple gestures.", "labels": [], "entities": []}, {"text": "Our optimization approach is particularly effective to interpret these complex inputs by simultaneously considering semantic, temporal, and contextual constraints.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Overall temporal relations between speech and  gesture", "labels": [], "entities": []}, {"text": " Table 2: Definition of SemType(r xy , \u03b3 mn )", "labels": [], "entities": []}, {"text": " Table 3: Definition of Temp(r xy , \u03b3 mn )", "labels": [], "entities": [{"text": "Definition", "start_pos": 10, "end_pos": 20, "type": "METRIC", "confidence": 0.9232112765312195}, {"text": "Temp", "start_pos": 24, "end_pos": 28, "type": "METRIC", "confidence": 0.9057210087776184}]}, {"text": " Table 4: Evaluation Results. In each entry form \"a(b), c(d)\",  \"a\" indicates the number of inputs in which the referring  expressions were correctly recognized by the speech recog- nizer; \"b\" indicates the number of inputs in which the refer- ring expressions were correctly recognized and were  correctly resolved; \"c\" indicates the number of inputs in  which the referring expressions were not correctly recog- nized; \"d\" indicates the number of inputs in which the refer- ring expressions also were not correctly recognized, but  were correctly resolved. The sum of \"a\" and \"c\" gives the  total number of inputs with a particular combination of  speech and gesture.", "labels": [], "entities": []}]}