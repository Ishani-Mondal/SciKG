{"title": [{"text": "Corpus representativeness for syntactic information acquisition", "labels": [], "entities": [{"text": "syntactic information acquisition", "start_pos": 30, "end_pos": 63, "type": "TASK", "confidence": 0.740069051583608}]}], "abstractContent": [{"text": "This paper refers to part of our research in the area of automatic acquisition of computational lexicon information from corpus.", "labels": [], "entities": [{"text": "automatic acquisition of computational lexicon information from corpus", "start_pos": 57, "end_pos": 127, "type": "TASK", "confidence": 0.8303462341427803}]}, {"text": "The present paper reports the ongoing research on corpus representativeness.", "labels": [], "entities": []}, {"text": "For the task of inducing information out of text, we wanted to fix a certain degree of confidence on the size and composition of the collection of documents to be observed.", "labels": [], "entities": []}, {"text": "The results show that it is possible to work with a relatively small corpus of texts if it is tuned to a particular domain.", "labels": [], "entities": []}, {"text": "Even more, it seems that a small tuned corpus will be more informative for real parsing than a general corpus.", "labels": [], "entities": []}], "introductionContent": [{"text": "The coverage of the computational lexicon used in deep Natural Language Processing (NLP) is crucial for parsing success.", "labels": [], "entities": [{"text": "deep Natural Language Processing (NLP)", "start_pos": 50, "end_pos": 88, "type": "TASK", "confidence": 0.7202294681753431}, {"text": "parsing", "start_pos": 104, "end_pos": 111, "type": "TASK", "confidence": 0.9790220856666565}]}, {"text": "But rather frequently, the absence of particular entries or the fact that the information encoded for these does not cover very specific syntactic contexts --as those found in technical texts-make high informative grammars not suitable for real applications.", "labels": [], "entities": []}, {"text": "Moreover, this poses areal problem when porting a particular application from domain to domain, as the lexicon has to be re-encoded in the light of the new domain.", "labels": [], "entities": []}, {"text": "In fact, in order to minimize ambiguities and possible over-generation, application based lexicons tend to be tuned for every specific domain addressed by a particular application.", "labels": [], "entities": []}, {"text": "Tuning of lexicons to different domains is really a delaying factor in the deployment of NLP applications, as it raises its costs, not only in terms of money, but also, and crucially, in terms of time.", "labels": [], "entities": []}, {"text": "A desirable solution would be a 'plug and play' system that, given a collection of documents supplied by the customer, could induce a tuned lexicon.", "labels": [], "entities": []}, {"text": "By 'tuned' we mean full coverage both in terms of: 1) entries: detecting new items and assigning them a syntactic behavior pattern; and 2) syntactic behavior pattern: adapting the encoding of entries to the observations of the corpus, so as to assign a class that accounts for the occurrences of this particular word in that particular corpus.", "labels": [], "entities": []}, {"text": "The question we have addressed here is to define the size and composition of the corpus we would need in order to get necessary and sufficient information for Machine Learning techniques to induce that type of information.", "labels": [], "entities": []}, {"text": "Representativeness of a corpus is a topic largely dealt with, especially in corpus linguistics.", "labels": [], "entities": [{"text": "corpus linguistics", "start_pos": 76, "end_pos": 94, "type": "TASK", "confidence": 0.7958724498748779}]}, {"text": "One of the standard references is where the author offers guidelines for corpus design to characterize a language.", "labels": [], "entities": []}, {"text": "The size and composition of the corpus to be observed has also been studied by general statistical NLP, and in relation with automatic acquisition methods).", "labels": [], "entities": []}, {"text": "But most of these studies focused in having a corpus that actually models the whole language.", "labels": [], "entities": []}, {"text": "However, we will see in section 3 that for inducing information for parsing we might want to model just a particular subset of a language, the one that corresponds to the texts that a particular application is going to parse.", "labels": [], "entities": [{"text": "parsing", "start_pos": 68, "end_pos": 75, "type": "TASK", "confidence": 0.9755648374557495}]}, {"text": "Thus, the research we report about here refers to aspects related to the quantity and optimal composition of a corpus that will be used for inducing syntactic information.", "labels": [], "entities": []}, {"text": "In what follows, we first will briefly describe the observation corpus.", "labels": [], "entities": []}, {"text": "In section 3, we introduce the phenomena observed and the way we got an objective measure.", "labels": [], "entities": []}, {"text": "In Section 4, we report on experiments done in order to check the validity of this measure in relation with word frequency.", "labels": [], "entities": [{"text": "validity", "start_pos": 66, "end_pos": 74, "type": "METRIC", "confidence": 0.990380048751831}]}, {"text": "In section 5 we address the issue of corpus size and how it affects this measure.", "labels": [], "entities": [{"text": "corpus size", "start_pos": 37, "end_pos": 48, "type": "TASK", "confidence": 0.6744066923856735}]}], "datasetContent": [{"text": "We have used a corpus of technical specialized texts, the CT.", "labels": [], "entities": []}, {"text": "The CT is made of subcorpora belonging to 5 different areas or domains: Medicine, Computing, Law, Economy, Environmental sciences and what is called a General subcorpus made basically of news.", "labels": [], "entities": []}, {"text": "The size of the subcorpora range between 1 and 3 million words per domain.", "labels": [], "entities": []}, {"text": "The CT corpus covers 3 different languages although for the time being we have only worked on Spanish.", "labels": [], "entities": [{"text": "CT corpus", "start_pos": 4, "end_pos": 13, "type": "DATASET", "confidence": 0.8481351733207703}]}, {"text": "For Spanish, the size of the subcorpora is stated in.", "labels": [], "entities": []}, {"text": "All texts have been processed and are annotated with morphosyntactic information.", "labels": [], "entities": []}, {"text": "The CT corpus has been compiled as a test-bed for studying linguistic differences between general language and specialized texts.", "labels": [], "entities": [{"text": "CT corpus", "start_pos": 4, "end_pos": 13, "type": "DATASET", "confidence": 0.8689476847648621}]}, {"text": "Nevertheless, for our purposes, we only considered it as documents that represent the language used in particular knowledge domains.", "labels": [], "entities": []}, {"text": "In fact, we use them to simulate the scenario where a user supplies a collection of documents with no specific sampling methodology behind.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Computing syntactic contexts as  behaviour", "labels": [], "entities": []}, {"text": " Table 2: CD for 'paralelo' compared to the  general corpus", "labels": [], "entities": []}, {"text": " Table 3 shows the  mean and standard deviation for all de subcorpora  (CC is Computing Corpus).", "labels": [], "entities": [{"text": "Computing Corpus", "start_pos": 78, "end_pos": 94, "type": "DATASET", "confidence": 0.8569487929344177}]}]}