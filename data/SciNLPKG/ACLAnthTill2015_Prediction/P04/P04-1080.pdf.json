{"title": [{"text": "Learning Word Senses With Feature Selection and Order Identification Capabilities", "labels": [], "entities": []}], "abstractContent": [{"text": "This paper presents an unsupervised word sense learning algorithm, which induces senses of target word by grouping its occurrences into a \"natural\" number of clusters based on the similarity of their contexts.", "labels": [], "entities": [{"text": "word sense learning", "start_pos": 36, "end_pos": 55, "type": "TASK", "confidence": 0.7423656483491262}]}, {"text": "For removing noisy words in feature set, feature selection is conducted by optimizing a cluster validation criterion subject to some constraint in an unsupervised manner.", "labels": [], "entities": [{"text": "feature selection", "start_pos": 41, "end_pos": 58, "type": "TASK", "confidence": 0.7639359533786774}]}, {"text": "Gaussian mixture model and Minimum Description Length criterion are used to estimate cluster structure and cluster number.", "labels": [], "entities": [{"text": "Minimum Description Length criterion", "start_pos": 27, "end_pos": 63, "type": "METRIC", "confidence": 0.6975452825427055}]}, {"text": "Experimental results show that our algorithm can find important feature subset, estimate model order (cluster number) and achieve better performance than another algorithm which requires cluster number to be provided.", "labels": [], "entities": []}], "introductionContent": [{"text": "Sense disambiguation is essential for many language applications such as machine translation, information retrieval, and speech processing.", "labels": [], "entities": [{"text": "Sense disambiguation", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.8176834881305695}, {"text": "machine translation", "start_pos": 73, "end_pos": 92, "type": "TASK", "confidence": 0.7968057990074158}, {"text": "information retrieval", "start_pos": 94, "end_pos": 115, "type": "TASK", "confidence": 0.8018360733985901}, {"text": "speech processing", "start_pos": 121, "end_pos": 138, "type": "TASK", "confidence": 0.7529071867465973}]}, {"text": "Almost all of sense disambiguation methods are heavily dependant on manually compiled lexical resources.", "labels": [], "entities": []}, {"text": "However these lexical resources often miss domain specific word senses, even many new words are not included inside.", "labels": [], "entities": []}, {"text": "Learning word senses from free text will help us dispense of outside knowledge source for defining sense by only discriminating senses of words.", "labels": [], "entities": []}, {"text": "Another application of word sense learning is to help enriching or even constructing semantic lexicons.", "labels": [], "entities": [{"text": "word sense learning", "start_pos": 23, "end_pos": 42, "type": "TASK", "confidence": 0.7703884243965149}]}, {"text": "The solution of word sense learning is closely related to the interpretation of word senses.", "labels": [], "entities": [{"text": "word sense learning", "start_pos": 16, "end_pos": 35, "type": "TASK", "confidence": 0.7647784749666849}, {"text": "interpretation of word senses", "start_pos": 62, "end_pos": 91, "type": "TASK", "confidence": 0.8328949362039566}]}, {"text": "Different interpretations of word senses result in different solutions to word sense learning.", "labels": [], "entities": [{"text": "word sense learning", "start_pos": 74, "end_pos": 93, "type": "TASK", "confidence": 0.7710214058558146}]}, {"text": "One interpretation strategy is to treat a word sense as a set of synonyms like synset in WordNet.", "labels": [], "entities": [{"text": "WordNet", "start_pos": 89, "end_pos": 96, "type": "DATASET", "confidence": 0.9495332837104797}]}, {"text": "The committee based word sense discovery algorithm (Pantel and) followed this strategy, which treated senses as clusters of words occurring in similar contexts.", "labels": [], "entities": [{"text": "word sense discovery", "start_pos": 20, "end_pos": 40, "type": "TASK", "confidence": 0.7367100119590759}]}, {"text": "Their algorithm initially discovered tight clusters called committees by grouping top n words similar with target word using averagelink clustering.", "labels": [], "entities": []}, {"text": "Then the target word was assigned to committees if the similarity between them was above a given threshold.", "labels": [], "entities": []}, {"text": "Each committee that the target word belonged to was interpreted as one of its senses.", "labels": [], "entities": []}, {"text": "There are two difficulties with this committee based sense learning.", "labels": [], "entities": [{"text": "sense learning", "start_pos": 53, "end_pos": 67, "type": "TASK", "confidence": 0.8107619285583496}]}, {"text": "The first difficulty is about derivation of feature vectors.", "labels": [], "entities": []}, {"text": "A feature for target word here consists of a contextual content word and its grammatical relationship with target word.", "labels": [], "entities": []}, {"text": "Acquisition of grammatical relationship depends on the output of a syntactic parser.", "labels": [], "entities": []}, {"text": "But for some languages, ex.", "labels": [], "entities": []}, {"text": "Chinese, the performance of syntactic parsing is still a problem.", "labels": [], "entities": [{"text": "syntactic parsing", "start_pos": 28, "end_pos": 45, "type": "TASK", "confidence": 0.8032270967960358}]}, {"text": "The second difficulty with this solution is that two parameters are required to be provided, which control the number of committees and the number of senses of target word.", "labels": [], "entities": []}, {"text": "Another interpretation strategy is to treat a word sense as a group of similar contexts of target word.", "labels": [], "entities": []}, {"text": "The context group discrimination (CGD) algorithm presented in adopted this strategy.", "labels": [], "entities": [{"text": "context group discrimination (CGD)", "start_pos": 4, "end_pos": 38, "type": "TASK", "confidence": 0.7666550775369009}]}, {"text": "Firstly, their algorithm selected important contextual words using \u03c7 2 or local frequency criterion.", "labels": [], "entities": []}, {"text": "With the \u03c7 2 based criterion, those contextual words whose occurrence depended on whether the ambiguous word occurred were chosen as features.", "labels": [], "entities": []}, {"text": "When using local frequency criterion, their algorithm selected top n most frequent contextual words as features.", "labels": [], "entities": []}, {"text": "Then each context of occurrences of target word was represented by second order cooccurrence based context vector.", "labels": [], "entities": []}, {"text": "Singular value decomposition (SVD) was conducted to reduce the dimensionality of context vectors.", "labels": [], "entities": [{"text": "Singular value decomposition (SVD)", "start_pos": 0, "end_pos": 34, "type": "TASK", "confidence": 0.7704829672972361}]}, {"text": "Then the reduced context vectors were grouped into a pre-defined number of clusters whose centroids corresponded to senses of target word.", "labels": [], "entities": []}, {"text": "Some observations can be made about their feature selection and clustering procedure.", "labels": [], "entities": [{"text": "feature selection", "start_pos": 42, "end_pos": 59, "type": "TASK", "confidence": 0.6791028380393982}, {"text": "clustering", "start_pos": 64, "end_pos": 74, "type": "TASK", "confidence": 0.9419541954994202}]}, {"text": "One observation is that their feature selection uses only first order information although the second order cooccurrence data is available.", "labels": [], "entities": [{"text": "feature selection", "start_pos": 30, "end_pos": 47, "type": "TASK", "confidence": 0.7944745123386383}]}, {"text": "The other observation is about their clustering procedure.", "labels": [], "entities": [{"text": "clustering", "start_pos": 37, "end_pos": 47, "type": "TASK", "confidence": 0.9635405540466309}]}, {"text": "Similar with committee based sense discovery algorithm, their clustering procedure also requires the predefinition of cluster number.", "labels": [], "entities": [{"text": "committee based sense discovery", "start_pos": 13, "end_pos": 44, "type": "TASK", "confidence": 0.6730203181505203}]}, {"text": "Their method can capture both coarse-gained and fine-grained sense distinction as the predefined cluster number varies.", "labels": [], "entities": []}, {"text": "But from a point of statistical view, there should exist a partitioning of data at which the most reliable, \"natural\" sense clusters appear.", "labels": [], "entities": []}, {"text": "In this paper, we follow the second order representation method for contexts of target word, since it is supposed to be less sparse and more robust than first order information.", "labels": [], "entities": []}, {"text": "We introduce a cluster validation based unsupervised feature wrapper to remove noises in contextual words, which works by measuring the consistency between cluster structures estimated from disjoint data subsets in selected feature space.", "labels": [], "entities": []}, {"text": "It is based on the assumption that if selected feature subset is important and complete, cluster structure estimated from data subset in this feature space should be stable and robust against random sampling.", "labels": [], "entities": []}, {"text": "After determination of important contextual words, we use a Gaussian mixture model (GMM) based clustering algorithm) to estimate cluster structure and cluster number by minimizing Minimum Description Length (MDL) criterion.", "labels": [], "entities": [{"text": "Minimum Description Length (MDL) criterion", "start_pos": 180, "end_pos": 222, "type": "METRIC", "confidence": 0.6650187713759286}]}, {"text": "We construct several subsets from widely used benchmark corpus as test data.", "labels": [], "entities": []}, {"text": "Experimental results show that our algorithm (F SGM M ) can find important feature subset, estimate cluster number and achieve better performance compared with CGD algorithm.", "labels": [], "entities": []}, {"text": "This paper is organized as follows.", "labels": [], "entities": []}, {"text": "In section 2 we will introduce our word sense learning algorithm, which incorporates unsupervised feature selection and model order identification technique.", "labels": [], "entities": [{"text": "word sense learning", "start_pos": 35, "end_pos": 54, "type": "TASK", "confidence": 0.7685583432515463}, {"text": "model order identification", "start_pos": 120, "end_pos": 146, "type": "TASK", "confidence": 0.604128877321879}]}, {"text": "Then we will give out the experimental results of our algorithm and discuss some findings from these results in section 3.", "labels": [], "entities": []}, {"text": "Section 4 will be devoted to a brief review of related efforts on word sense discrimination.", "labels": [], "entities": [{"text": "word sense discrimination", "start_pos": 66, "end_pos": 91, "type": "TASK", "confidence": 0.8090819915135702}]}, {"text": "In section 5 we will conclude our work and suggest some possible improvements.", "labels": [], "entities": []}], "datasetContent": [{"text": "For evaluation of feature selection, we used mutual information between feature subset and class label set to assess the importance of selected feature subset.", "labels": [], "entities": []}, {"text": "Our assessment measure is defined as: where T is the feature subset to be evaluated, T \u2286 W , L is class label set, p(w, l) is the joint distribution of two variables wand l, p(w) and p(l) are marginal probabilities.", "labels": [], "entities": []}, {"text": "p(w, l) is estimated based on contingency table of contextual word set W and class label set L.", "labels": [], "entities": []}, {"text": "Intuitively, if M (T 1 ) > M (T 2 ), T 1 is more important than T 2 since T 1 contains more information about L.", "labels": [], "entities": []}, {"text": "When assessing the agreement between clustering result and hand-tagged senses (ground truth classes) in benchmark data, we encountered the difficulty that there was no sense tag for each cluster.", "labels": [], "entities": []}, {"text": "In (), they defined a permutation procedure for calculating the agreement between two cluster memberships assigned by different unsupervised learners.", "labels": [], "entities": []}, {"text": "In this paper, we applied their method to assign different sense tags to only min(|U |, |C|) clusters by maximizing the accuracy, where |U | is the number of clusters, and |C| is the number of ground truth classes.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 120, "end_pos": 128, "type": "METRIC", "confidence": 0.9993966817855835}]}, {"text": "The underlying assumption here is that each cluster is considered as a class, and for any two clusters, they do not share same class labels.", "labels": [], "entities": []}, {"text": "At most |C| clusters are assigned sense tags, since there are only |C| classes in benchmark data.", "labels": [], "entities": []}, {"text": "Given the contingency table Q between clusters and ground truth classes, each entry Q i,j gives the number of occurrences which fall into both the ith cluster and the j-th ground truth class.", "labels": [], "entities": []}, {"text": "If |U | < |C|, we constructed empty clusters so that |U | = |C|.", "labels": [], "entities": []}, {"text": "Let \u2126 represent a one-to-one mapping function from C to U . It means that \u2126(j 1 ) = \u2126(j 2 ) if j 1 = j 2 and vice versa, 1 \u2264 j 1 , j 2 \u2264 |C|.", "labels": [], "entities": []}, {"text": "Then \u2126(j) is the index of the cluster associated with the j-th class.", "labels": [], "entities": []}, {"text": "Searching a mapping function to maximize the accuracy of U can be formulated as: Then the accuracy of solution U is given by In fact, i,j Q i,j is equal to N , the number of occurrences of target word in test set.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 45, "end_pos": 53, "type": "METRIC", "confidence": 0.998506486415863}, {"text": "accuracy", "start_pos": 90, "end_pos": 98, "type": "METRIC", "confidence": 0.9990260601043701}]}, {"text": "For each dataset, we tested following procedures: CGD term :We implemented the context group discrimination algorithm.", "labels": [], "entities": [{"text": "context group discrimination", "start_pos": 79, "end_pos": 107, "type": "TASK", "confidence": 0.6599840124448141}]}, {"text": "Top max(|W | \u00d7 20%, 100) words in contextual word list was selected as features using frequency or \u03c7 2 based ranking.", "labels": [], "entities": []}, {"text": "Then k-means clustering 2 was performed on context vector matrix using normalized Euclidean distance.", "labels": [], "entities": []}, {"text": "K-means clustering was repeated 5 times We used k-means function in statistics toolbox of Matlab. and the partition with best quality was chosen as final result.", "labels": [], "entities": [{"text": "Matlab.", "start_pos": 90, "end_pos": 97, "type": "DATASET", "confidence": 0.9587401151657104}]}, {"text": "The number of clusters used by k-means was set to be identical with the number of ground truth classes.", "labels": [], "entities": []}, {"text": "We tested CGD term using various word vector weighting methods when deriving context vectors, ex. binary, idf , tf \u00b7 idf . CGD SV D : The context vector matrix was derived using same method in CGD term . Then kmeans clustering was conducted on latent semantic space transformed from context vector matrix, using normalized Euclidean distance.", "labels": [], "entities": [{"text": "kmeans clustering", "start_pos": 209, "end_pos": 226, "type": "TASK", "confidence": 0.8570253849029541}]}, {"text": "Specifically, context vectors were reduced to 100 dimensions using SVD.", "labels": [], "entities": []}, {"text": "If the dimension of context vector was less than 100, all of latent semantic vectors with non-zero eigenvalue were used for subsequent clustering.", "labels": [], "entities": []}, {"text": "We also tested it using different weighting methods, ex. binary, idf , tf \u00b7 idf . F SGM M : We performed cluster validation based feature selection in feature set used by CGD.", "labels": [], "entities": [{"text": "F", "start_pos": 82, "end_pos": 83, "type": "METRIC", "confidence": 0.9753280282020569}]}, {"text": "Then Cluster algorithm was used to group target word's instances using Euclidean distance measure.", "labels": [], "entities": []}, {"text": "\u03c4 was set as 0.90 in feature subset search procedure.", "labels": [], "entities": []}, {"text": "The random splitting frequency is set as 10 for estimation of the score of feature subset.", "labels": [], "entities": []}, {"text": "The initial subclass number was 20 and full covariance matrix was used for parameter estimation of each subclass.", "labels": [], "entities": []}, {"text": "For investigating the effect of different context window size on the performance of three procedures, we tested these procedures using various context window sizes: \u00b11, \u00b15, \u00b115, \u00b125, and all of contextual words.", "labels": [], "entities": []}, {"text": "The average length of sentences in 4 datasets is 32 words before preprocessing.", "labels": [], "entities": []}, {"text": "Performance on each dataset was assessed by equation 19.", "labels": [], "entities": []}, {"text": "The scores of feature subsets selected by F SGM M and CGD are listed in and 4.", "labels": [], "entities": [{"text": "F SGM M", "start_pos": 42, "end_pos": 49, "type": "TASK", "confidence": 0.5459662278493246}, {"text": "CGD", "start_pos": 54, "end_pos": 57, "type": "DATASET", "confidence": 0.7336664199829102}]}, {"text": "The average accuracy of three procedures with different feature ranking and weighting method is given in.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 12, "end_pos": 20, "type": "METRIC", "confidence": 0.9986718893051147}]}, {"text": "Each figure is the average over 5 different context window size and 4 datasets.", "labels": [], "entities": []}, {"text": "We give out the detailed results of these three procedures in.", "labels": [], "entities": []}, {"text": "Several results should be noted specifically: From and 4, we can find that F SGM M achieved better score on mutual information (MI) measure than CGD over 35 out of total 40 cases.", "labels": [], "entities": [{"text": "F SGM M", "start_pos": 75, "end_pos": 82, "type": "TASK", "confidence": 0.5383426447709402}, {"text": "mutual information (MI)", "start_pos": 108, "end_pos": 131, "type": "TASK", "confidence": 0.5573506414890289}]}, {"text": "This is the evidence that our feature selection procedure can remove noise and retain important features.", "labels": [], "entities": [{"text": "feature selection", "start_pos": 30, "end_pos": 47, "type": "TASK", "confidence": 0.7645927667617798}]}, {"text": "As it was shown in, with both \u03c7 2 and freq based feature ranking, F SGM M algorithm performed better than CGD term and CGD SV D if we used average accuracy to evaluate their performance.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 147, "end_pos": 155, "type": "METRIC", "confidence": 0.953914225101471}]}, {"text": "Specifically, with \u03c7 2 based feature ranking, F SGM M attained 55.4% average accuracy, while the best average accuracy of CGD term and CGD SV D were 40.9% and 51.3% respectively.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 77, "end_pos": 85, "type": "METRIC", "confidence": 0.9933739900588989}, {"text": "accuracy", "start_pos": 110, "end_pos": 118, "type": "METRIC", "confidence": 0.9857286810874939}]}, {"text": "With freq based feature ranking, F SGM M achieved 51.2% average accuracy, while the best average accuracy of CGD term and CGD SV D were 45.1% and 50.2%.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 64, "end_pos": 72, "type": "METRIC", "confidence": 0.9859739542007446}, {"text": "accuracy", "start_pos": 97, "end_pos": 105, "type": "METRIC", "confidence": 0.9773988723754883}, {"text": "CGD SV D", "start_pos": 122, "end_pos": 130, "type": "METRIC", "confidence": 0.557632178068161}]}, {"text": "The automatically estimated cluster numbers by F SGM M over 4 datasets are given in.", "labels": [], "entities": [{"text": "F SGM M", "start_pos": 47, "end_pos": 54, "type": "DATASET", "confidence": 0.6159351070721945}]}, {"text": "The estimated cluster number was 2 \u223c 4 for \"hard\", 3 \u223c 6 for \"interest\", 3 \u223c 6 for \"line\", and 2 \u223c 4 for \"serve\".", "labels": [], "entities": [{"text": "cluster number", "start_pos": 14, "end_pos": 28, "type": "METRIC", "confidence": 0.9761323630809784}]}, {"text": "It is noted that the estimated cluster number was less than the number of ground truth classes inmost cases.", "labels": [], "entities": []}, {"text": "There are some reasons for this phenomenon.", "labels": [], "entities": []}, {"text": "First, the data is not balanced, which may lead to that some important features cannot be retrieved.", "labels": [], "entities": []}, {"text": "For example, the fourth sense of \"serve\", and the sixth sense of \"line\", their corresponding features are not up to the selection criteria.", "labels": [], "entities": []}, {"text": "Second, some senses cannot be distinguished using only bag-of-words information, and their difference lies in syntactic information held by features.", "labels": [], "entities": []}, {"text": "For example, the third sense and the sixth sense of \"interest\" maybe distinguished by syntactic relation of feature words, while the bag of feature words occurring in their context are similar.", "labels": [], "entities": []}, {"text": "Third, some senses are determined by global topics, rather than local contexts.", "labels": [], "entities": []}, {"text": "For example, according to global topics, it maybe easier to distinguish the first and the second sense of \"interest\".", "labels": [], "entities": []}, {"text": "shows the average accuracy over three procedures in as a function of context window size for 4 datasets.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 18, "end_pos": 26, "type": "METRIC", "confidence": 0.9993994235992432}]}, {"text": "For \"hard\", the performance dropped as window size increased, and the best accuracy(77.0%) was achieved at window size 1.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 75, "end_pos": 83, "type": "METRIC", "confidence": 0.9994471669197083}]}, {"text": "For \"interest\", sense discrimination did not benefit from large window size and the best accuracy(40.1%) was achieved at window size 5.", "labels": [], "entities": [{"text": "sense discrimination", "start_pos": 16, "end_pos": 36, "type": "TASK", "confidence": 0.7024533301591873}, {"text": "accuracy", "start_pos": 89, "end_pos": 97, "type": "METRIC", "confidence": 0.9991681575775146}]}, {"text": "For \"line\", accuracy dropped when increasing window size and the best accuracy(50.2%) was achieved at window size 1.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 12, "end_pos": 20, "type": "METRIC", "confidence": 0.9995434880256653}, {"text": "accuracy", "start_pos": 70, "end_pos": 78, "type": "METRIC", "confidence": 0.999264657497406}]}, {"text": "For \"serve\", the performance benefitted from large window size and the best accuracy(46.8%) was achieved at window size 15.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 76, "end_pos": 84, "type": "METRIC", "confidence": 0.9993508458137512}]}, {"text": "In (, they used Bayesian approach for sense disambiguation of three ambiguous words, \"hard\", \"line\", and \"serve\", based on cues from topical and local context.", "labels": [], "entities": [{"text": "sense disambiguation", "start_pos": 38, "end_pos": 58, "type": "TASK", "confidence": 0.8291244208812714}]}, {"text": "They observed that local context was more reliable than topical context as an indicator of senses for this verb and adjective, but slightly less reliable for this noun.", "labels": [], "entities": []}, {"text": "Compared with their conclusion, we can find that our result is consistent with it for \"hard\".", "labels": [], "entities": []}, {"text": "But there is some differences for verb \"serve\" and noun \"line\".", "labels": [], "entities": []}, {"text": "For  \"serve\", the possible reason is that we do not use position of local word and part of speech information, which may deteriorate the performance when local context(\u2264 5 words) is used.", "labels": [], "entities": []}, {"text": "For \"line\", the reason might come from the feature subset, which is not good enough to provide improvement when  context window size is no less than 5.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Four ambiguous words, their senses and frequency", "labels": [], "entities": []}, {"text": " Table 3: Mutual information between feature subset and class", "labels": [], "entities": []}, {"text": " Table 4: Mutual information between feature subset and class", "labels": [], "entities": []}, {"text": " Table 5: Average accuracy of three procedures with various", "labels": [], "entities": [{"text": "Average", "start_pos": 10, "end_pos": 17, "type": "METRIC", "confidence": 0.9872183799743652}, {"text": "accuracy", "start_pos": 18, "end_pos": 26, "type": "METRIC", "confidence": 0.9452853798866272}]}, {"text": " Table 6: Automatically determined mixture component num-", "labels": [], "entities": []}]}