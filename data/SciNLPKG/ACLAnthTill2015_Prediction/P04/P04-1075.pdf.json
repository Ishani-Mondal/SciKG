{"title": [{"text": "Multi-Criteria-based Active Learning for Named Entity Recognition", "labels": [], "entities": [{"text": "Named Entity Recognition", "start_pos": 41, "end_pos": 65, "type": "TASK", "confidence": 0.7078921397527059}]}], "abstractContent": [{"text": "In this paper, we propose a multi-criteria-based active learning approach and effectively apply it to named entity recognition.", "labels": [], "entities": [{"text": "named entity recognition", "start_pos": 102, "end_pos": 126, "type": "TASK", "confidence": 0.655617376168569}]}, {"text": "Active learning targets to minimize the human annotation efforts by selecting examples for labeling.", "labels": [], "entities": []}, {"text": "To maximize the contribution of the selected e xamples, we consider the multiple criteria: informative-ness, representativeness and diversity and propose measures to quantify them.", "labels": [], "entities": []}, {"text": "More comprehensively, we incorporate all the criteria using two selection strategies, both of which result in less labeling cost than single-criterion-based method.", "labels": [], "entities": []}, {"text": "The results of the named entity recognition in both MUC-6 and GENIA show that the labeling cost can be reduced by at least 80% without degrading the performance.", "labels": [], "entities": [{"text": "named entity recognition", "start_pos": 19, "end_pos": 43, "type": "TASK", "confidence": 0.6145805219809214}, {"text": "MUC-6", "start_pos": 52, "end_pos": 57, "type": "DATASET", "confidence": 0.8893899917602539}, {"text": "GENIA", "start_pos": 62, "end_pos": 67, "type": "DATASET", "confidence": 0.8824114203453064}]}], "introductionContent": [{"text": "In the machine learning approaches of natural language processing (NLP), models are generally trained on large annotated corpus.", "labels": [], "entities": [{"text": "natural language processing (NLP)", "start_pos": 38, "end_pos": 71, "type": "TASK", "confidence": 0.8246995906035105}]}, {"text": "However, annotating such corpus is expensive and timeconsuming, which makes it difficult to adapt an existing model to anew domain.", "labels": [], "entities": []}, {"text": "In order to overcome this difficulty, active learning (sample sele ction) has been studied in more and more NLP applications such as POS tagging), information extraction (), text classification (, statistical parsing (), noun phrase chunking, etc.", "labels": [], "entities": [{"text": "POS tagging", "start_pos": 133, "end_pos": 144, "type": "TASK", "confidence": 0.7537986040115356}, {"text": "information extraction", "start_pos": 147, "end_pos": 169, "type": "TASK", "confidence": 0.8398544192314148}, {"text": "text classification", "start_pos": 174, "end_pos": 193, "type": "TASK", "confidence": 0.8394856750965118}, {"text": "statistical parsing", "start_pos": 197, "end_pos": 216, "type": "TASK", "confidence": 0.8485550284385681}, {"text": "noun phrase chunking", "start_pos": 221, "end_pos": 241, "type": "TASK", "confidence": 0.7487087647120158}]}, {"text": "Active learning is based on the assumption that a small number of annotated examples and a large number of unannotated examples are available.", "labels": [], "entities": []}, {"text": "This assumption is valid inmost NLP tasks.", "labels": [], "entities": []}, {"text": "Different from supervised learning in which the entire corpus are labeled manually, active learning is to select the most useful example for labe ling and add the labeled example to training set to retrain model.", "labels": [], "entities": []}, {"text": "This procedure is repeated until the model achieves a certain level of performance.", "labels": [], "entities": []}, {"text": "Practically, a batch of examples are selected at a time, called batchedbased sample selection ( since it is time consuming to retrain the model if only one new example is added to the training set.", "labels": [], "entities": []}, {"text": "Many existing work in the area focus on two approaches: certainty-based methods () and committee-based methods to select the most informative examples for which the current model are most uncertain.", "labels": [], "entities": []}, {"text": "Being the first piece of work on active learning for name entity recognition (NER) task, we target to minimize the human annotation efforts yet still reaching the same level of performance as a supervised learning approach.", "labels": [], "entities": [{"text": "name entity recognition (NER) task", "start_pos": 53, "end_pos": 87, "type": "TASK", "confidence": 0.8043836227485112}]}, {"text": "For this purpose, we make a more comprehensive consideration on the contribution of individual examples, and more importantly maximizing the contribution of a batch based on three criteria : informativeness, representativeness and diversity.", "labels": [], "entities": []}, {"text": "First, we propose three scoring functions to quantify the informativeness of an example , which can be used to select the most uncertain examples.", "labels": [], "entities": []}, {"text": "Second, the representativeness measure is further proposed to choose the example s representing the majority.", "labels": [], "entities": []}, {"text": "Third, we propose two diversity considerations (global and local) to avoid repetition among the examples of a batch.", "labels": [], "entities": []}, {"text": "Finally, two combination strategies with the above three criteria are proposed to reach the maximum effectiveness on active learning for NER.", "labels": [], "entities": [{"text": "NER", "start_pos": 137, "end_pos": 140, "type": "TASK", "confidence": 0.9717180728912354}]}, {"text": "We build our NER model using Support Vector Machines (SVM).", "labels": [], "entities": []}, {"text": "The experiment shows that our active learning methods achieve a promising result in this NER task.", "labels": [], "entities": [{"text": "NER task", "start_pos": 89, "end_pos": 97, "type": "TASK", "confidence": 0.9184944033622742}]}, {"text": "The results in both MUC-6 and GENIA show that the amount of the labeled training data can be reduced by at least 80% without degrading the quality of the named entity recognizer.", "labels": [], "entities": [{"text": "MUC-6", "start_pos": 20, "end_pos": 25, "type": "DATASET", "confidence": 0.8687185049057007}, {"text": "GENIA", "start_pos": 30, "end_pos": 35, "type": "DATASET", "confidence": 0.7560436129570007}]}, {"text": "The contributions not only come from the above measures, but also the two sample selection strategies which effectively incorporate informativeness, representativeness and diversity criteria.", "labels": [], "entities": []}, {"text": "To our knowledge, it is the first work on considering the three criteria all together for active learning.", "labels": [], "entities": []}, {"text": "Furthermore, such measures and strategies can be easily adapted to other active learning tasks as well.", "labels": [], "entities": []}], "datasetContent": [{"text": "In order to evaluate the effectiveness of our selection strategies, we apply them to recognize protein (PRT) names in biomedical domain using GENIA corpus V1.1 (  and person (PER), location (LOC), organization (ORG) names in newswire domain using MUC-6 corpus.", "labels": [], "entities": [{"text": "GENIA corpus V1.1", "start_pos": 142, "end_pos": 159, "type": "DATASET", "confidence": 0.9415583610534668}, {"text": "MUC-6 corpus", "start_pos": 247, "end_pos": 259, "type": "DATASET", "confidence": 0.9658008217811584}]}, {"text": "First, we randomly split the whole corpus into three parts: an initial training set to build an initial model, a test set to evaluate the performance of the model and an unlabeled set to select examples.", "labels": [], "entities": []}, {"text": "The size of each data set is shown in.", "labels": [], "entities": []}, {"text": "Then, iteratively, we select a batch of examples following the selection strategie s proposed, require human experts to label them and add them into the training set.", "labels": [], "entities": []}, {"text": "The batch size K = 50 in GENIA and 10 in MUC-6.", "labels": [], "entities": [{"text": "GENIA", "start_pos": 25, "end_pos": 30, "type": "DATASET", "confidence": 0.8518869280815125}, {"text": "MUC-6", "start_pos": 41, "end_pos": 46, "type": "DATASET", "confidence": 0.8628870248794556}]}, {"text": "Each example is defined as a machine-recognized named entity and its context words (previous 3 words and next 3 words).", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1: Experiment settings for active learning using GENIA1.1(PRT) and MUC-6(PER,LOC,ORG)", "labels": [], "entities": [{"text": "GENIA1.1", "start_pos": 56, "end_pos": 64, "type": "DATASET", "confidence": 0.8328015804290771}]}, {"text": " Table 3: Training data sizes for various selection methods to  achieve the same performance level as the supervised learning", "labels": [], "entities": []}, {"text": " Table 4: Comparisons of training data sizes for the multi- criteria-based selection strategies and the informativeness- criterion-based selection (Info_Min) to achieve the same per- formance level as the supervised learning.", "labels": [], "entities": []}]}