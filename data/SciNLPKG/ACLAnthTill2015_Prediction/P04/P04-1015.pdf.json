{"title": [{"text": "Incremental Parsing with the Perceptron Algorithm", "labels": [], "entities": [{"text": "Incremental Parsing", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.9102626144886017}]}], "abstractContent": [{"text": "This paper describes an incremental parsing approach where parameters are estimated using a variant of the perceptron algorithm.", "labels": [], "entities": []}, {"text": "A beam-search algorithm is used during both training and decoding phases of the method.", "labels": [], "entities": []}, {"text": "The perceptron approach was implemented with the same feature set as that of an existing generative model (Roark, 2001a), and experimental results show that it gives competitive performance to the generative model on parsing the Penn treebank.", "labels": [], "entities": [{"text": "Penn treebank", "start_pos": 229, "end_pos": 242, "type": "DATASET", "confidence": 0.9651491045951843}]}, {"text": "We demonstrate that training a perceptron model to combine with the generative model during search provides a 2.1 percent F-measure improvement over the generative model alone, to 88.8 percent.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 122, "end_pos": 131, "type": "METRIC", "confidence": 0.9990221261978149}]}], "introductionContent": [{"text": "In statistical approaches to NLP problems such as tagging or parsing, it seems clear that the representation used as input to a learning algorithm is central to the accuracy of an approach.", "labels": [], "entities": [{"text": "tagging or parsing", "start_pos": 50, "end_pos": 68, "type": "TASK", "confidence": 0.6014126439889272}, {"text": "accuracy", "start_pos": 165, "end_pos": 173, "type": "METRIC", "confidence": 0.9982759952545166}]}, {"text": "In an ideal world, the designer of a parser or tagger would be free to choose any features which might be useful in discriminating good from bad structures, without concerns about how the features interact with the problems of training (parameter estimation) or decoding (search for the most plausible candidate under the model).", "labels": [], "entities": []}, {"text": "To this end, a number of recently proposed methods allow a model to incorporate \"arbitrary\" global features of candidate analyses or parses.", "labels": [], "entities": []}, {"text": "Examples of such techniques are Markov Random Fields, and boosting or perceptron approaches to reranking.", "labels": [], "entities": []}, {"text": "A drawback of these approaches is that in the general case, they can require exhaustive enumeration of the set of candidates for each input sentence in both the training and decoding phases . For example, and use all parses generated by an LFG parser as input to an MRF approach -given the level of ambiguity in natural language, this set can presumably become extremely large. and rerank the top N parses from an existing generative parser, but this kind of approach presupposes that there is an existing baseline model with reasonable performance.", "labels": [], "entities": []}, {"text": "Many of these baseline models are themselves used with heuristic search techniques, so that the potential gain through the use of discriminative re-ranking techniques is further dependent on effective search.", "labels": [], "entities": []}, {"text": "This paper explores an alternative approach to parsing, based on the perceptron training algorithm introduced in.", "labels": [], "entities": [{"text": "parsing", "start_pos": 47, "end_pos": 54, "type": "TASK", "confidence": 0.9843956232070923}]}, {"text": "In this approach the training and decoding problems are very closely related -the training method decodes training examples in sequence, and makes simple corrective updates to the parameters when errors are made.", "labels": [], "entities": []}, {"text": "Thus the main complexity of the method is isolated to the decoding problem.", "labels": [], "entities": []}, {"text": "We describe an approach that uses an incremental, left-to-right parser, with beam search, to find the highest scoring analysis under the model.", "labels": [], "entities": []}, {"text": "The same search method is used in both training and decoding.", "labels": [], "entities": []}, {"text": "We implemented the perceptron approach with the same feature set as that of an existing generative model, and show that the perceptron model gives performance competitive to that of the generative model on parsing the Penn treebank, thus demonstrating that an unnormalized discriminative parsing model can be applied with heuristic search.", "labels": [], "entities": [{"text": "Penn treebank", "start_pos": 218, "end_pos": 231, "type": "DATASET", "confidence": 0.9201854169368744}]}, {"text": "We also describe several refinements to the training algorithm, and demonstrate their impact on convergence properties of the method.", "labels": [], "entities": []}, {"text": "Finally, we describe training the perceptron model with the negative log probability given by the generative model as another feature.", "labels": [], "entities": []}, {"text": "This provides the perceptron algorithm with a better starting point, leading to large improvements over using either the generative model or the perceptron algorithm in isolation (the hybrid model achieves 88.8% f-measure on the WSJ treebank, compared to figures of 86.7% and 86.6% for the separate generative and perceptron models).", "labels": [], "entities": [{"text": "f-measure", "start_pos": 212, "end_pos": 221, "type": "METRIC", "confidence": 0.9919597506523132}, {"text": "WSJ treebank", "start_pos": 229, "end_pos": 241, "type": "DATASET", "confidence": 0.9839783310890198}]}, {"text": "The approach is an extremely simple method for integrating new features into the generative model: essentially all that is needed is a definition of feature-vector representations of entire parse trees, and then the existing parsing algorithms can be used for both training and decoding with the models.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Left-child chain type counts (of length > 2) for  sections of the Wall St. Journal Treebank, and out-of- vocabulary (OOV) rate on the held-out corpus.", "labels": [], "entities": [{"text": "Wall St. Journal Treebank", "start_pos": 76, "end_pos": 101, "type": "DATASET", "confidence": 0.9921479225158691}, {"text": "out-of- vocabulary (OOV) rate", "start_pos": 107, "end_pos": 136, "type": "METRIC", "confidence": 0.8588900566101074}]}, {"text": " Table 3: Parsing results, section 23, all sentences, including labeled precision (LP), labeled recall (LR), and F-measure", "labels": [], "entities": [{"text": "precision (LP)", "start_pos": 72, "end_pos": 86, "type": "METRIC", "confidence": 0.9070349782705307}, {"text": "recall (LR)", "start_pos": 96, "end_pos": 107, "type": "METRIC", "confidence": 0.9477242827415466}, {"text": "F-measure", "start_pos": 113, "end_pos": 122, "type": "METRIC", "confidence": 0.9632614850997925}]}, {"text": " Table 4: Parsing results, section 23, all sentences, in- cluding labeled precision (LP), labeled recall (LR), and  F-measure", "labels": [], "entities": [{"text": "precision (LP)", "start_pos": 74, "end_pos": 88, "type": "METRIC", "confidence": 0.9183303564786911}, {"text": "recall (LR)", "start_pos": 98, "end_pos": 109, "type": "METRIC", "confidence": 0.9478373527526855}, {"text": "F-measure", "start_pos": 116, "end_pos": 125, "type": "METRIC", "confidence": 0.9777290225028992}]}]}