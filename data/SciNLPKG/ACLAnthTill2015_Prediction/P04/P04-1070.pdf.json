{"title": [{"text": "An alternative method of training probabilistic LR parsers", "labels": [], "entities": []}], "abstractContent": [{"text": "We discuss existing approaches to train LR parsers, which have been used for statistical resolution of structural ambiguity.", "labels": [], "entities": [{"text": "LR parsers", "start_pos": 40, "end_pos": 50, "type": "TASK", "confidence": 0.7559212446212769}, {"text": "statistical resolution of structural ambiguity", "start_pos": 77, "end_pos": 123, "type": "TASK", "confidence": 0.8013246655464172}]}, {"text": "These approaches are non-optimal, in the sense that a collection of probability distributions cannot be obtained.", "labels": [], "entities": []}, {"text": "In particular, some probability distributions expressible in terms of a context-free grammar cannot be expressed in terms of the LR parser constructed from that grammar, under the restrictions of the existing approaches to training of LR parsers.", "labels": [], "entities": []}, {"text": "We present an alternative way of training that is provably optimal, and that allows all probability distributions expressible in the context-free grammar to be carried over to the LR parser.", "labels": [], "entities": []}, {"text": "We also demonstrate empirically that this kind of training can be effectively applied on a large treebank.", "labels": [], "entities": []}], "introductionContent": [{"text": "The LR parsing strategy was originally devised for programming languages), but has been used in a wide range of other areas as well, such as for natural language processing).", "labels": [], "entities": [{"text": "LR parsing", "start_pos": 4, "end_pos": 14, "type": "TASK", "confidence": 0.8095004260540009}]}, {"text": "The main difference between the application to programming languages and the application to natural languages is that in the latter case the parsers should be nondeterministic, in order to deal with ambiguous context-free grammars (CFGs).", "labels": [], "entities": []}, {"text": "Nondeterminism can be handled in a number of ways, but the most efficient is tabulation, which allows processing in polynomial time.", "labels": [], "entities": []}, {"text": "Tabular LR parsing is known from the work by, but can also be achieved by the generic tabulation technique due to, which assumes an input pushdown transducer (PDT).", "labels": [], "entities": [{"text": "Tabular LR parsing", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.7417698899904887}]}, {"text": "In this context, the LR parsing strategy can be seen as a particular mapping from context-free grammars to PDTs.", "labels": [], "entities": [{"text": "LR parsing", "start_pos": 21, "end_pos": 31, "type": "TASK", "confidence": 0.878987044095993}]}, {"text": "The acronym 'LR' stands for 'Left-to-right processing of the input, producing a Right-most derivation (in reverse)'.", "labels": [], "entities": []}, {"text": "When we construct a PDT A from a CFG G by the LR parsing strategy and apply it on an input sentence, then the set of output strings of A represents the set of all right-most derivations that G allows for that sentence.", "labels": [], "entities": [{"text": "LR parsing", "start_pos": 46, "end_pos": 56, "type": "TASK", "confidence": 0.6428533047437668}]}, {"text": "Such an output string enumerates the rules (or labels that identify the rules uniquely) that occur in the corresponding right-most derivation, in reversed order.", "labels": [], "entities": []}, {"text": "If LR parsers do not use lookahead to decide between alternative transitions, they are called LR(0) parsers.", "labels": [], "entities": []}, {"text": "More generally, if LR parsers look ahead k symbols, they are called LR(k) parsers; some simplified LR parsing models that use lookahead are called SLR(k) and LALR(k) parsing).", "labels": [], "entities": []}, {"text": "In order to simplify the discussion, we abstain from using lookahead in this article, and 'LR parsing' can further be read as 'LR(0) parsing'.", "labels": [], "entities": []}, {"text": "We would like to point out however that our observations carryover to LR parsing with lookahead.", "labels": [], "entities": [{"text": "LR parsing", "start_pos": 70, "end_pos": 80, "type": "TASK", "confidence": 0.7678556740283966}]}, {"text": "The theory of probabilistic pushdown automata ( can be easily applied to LR parsing.", "labels": [], "entities": [{"text": "LR parsing", "start_pos": 73, "end_pos": 83, "type": "TASK", "confidence": 0.8521736562252045}]}, {"text": "A probability is then assigned to each transition, by a function that we will call the probability function p A , and the probability of an accepting computation of A is the product of the probabilities of the applied transitions.", "labels": [], "entities": []}, {"text": "As each accepting computation produces a right-most derivation as output string, a probabilistic LR parser defines a probability distribution on the set of parses, and thereby also a probability distribution on the set of sentences generated by grammar G.", "labels": [], "entities": []}, {"text": "Disambiguation of an ambiguous sentence can be achieved on the basis of a comparison between the probabilities assigned to the respective parses by the probabilistic LR model.", "labels": [], "entities": [{"text": "Disambiguation of an ambiguous sentence", "start_pos": 0, "end_pos": 39, "type": "TASK", "confidence": 0.8023576498031616}]}, {"text": "The probability function can be obtained on the basis of a treebank, as proposed by) (see also).", "labels": [], "entities": []}, {"text": "The model by however incorporated a mistake involving lookahead, which was corrected by).", "labels": [], "entities": []}, {"text": "As we will not discuss lookahead here, this matter does not play a significant role in the current study.", "labels": [], "entities": []}, {"text": "Noteworthy is that () showed empirically that an LR parser maybe more accurate than the original CFG, if both are trained on the basis of the same treebank.", "labels": [], "entities": [{"text": "CFG", "start_pos": 97, "end_pos": 100, "type": "DATASET", "confidence": 0.930687427520752}]}, {"text": "In other words, the resulting probability function p A on transitions of the PDT allows better disambiguation than the corresponding function p G on rules of the original grammar.", "labels": [], "entities": []}, {"text": "A plausible explanation of this is that stack symbols of an LR parser encode some amount of left context, i.e. information on rules applied earlier, so that the probability function on transitions may encode dependencies between rules that cannot be encoded in terms of the original CFG extended with rule probabilities.", "labels": [], "entities": []}, {"text": "The explicit use of left context in probabilistic context-free models was investigated by e.g. (), who also demonstrated that this may significantly improve accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 157, "end_pos": 165, "type": "METRIC", "confidence": 0.9967151880264282}]}, {"text": "Note that the probability distributions of language maybe beyond the reach of a given context-free grammar, as pointed out by e.g..", "labels": [], "entities": []}, {"text": "Therefore, the use of left context, and the resulting increase in the number of parameters of the model, may narrow the gap between the given grammar and ill-understood mechanisms underlying actual language.", "labels": [], "entities": []}, {"text": "One important assumption that is made by and) is that trained probabilistic LR parsers should be proper, i.e. if several transitions are applicable fora given stack, then the sum of probabilities assigned to those transitions by probability function p A should be 1.", "labels": [], "entities": []}, {"text": "This assumption maybe motivated by pragmatic considerations, as such a proper model is easy to train by relative frequency estimation: count the number of times a transition is applied with respect to a treebank, and divide it by the number of times the relevant stack symbol (or pair of stack symbols) occurs at the top of the stack.", "labels": [], "entities": []}, {"text": "Let us call the resulting probability function p rfe . This function is provably optimal in the sense that the likelihood it assigns to the training corpus is maximal among all probability functions p A that are proper in the above sense.", "labels": [], "entities": []}, {"text": "However, properness restricts the space of probability distributions that a PDT allows.", "labels": [], "entities": []}, {"text": "This means that a (consistent) probability function p A may exist that is not proper and that assigns a higher likelihood to the training corpus than p rfe does.", "labels": [], "entities": []}, {"text": "(By 'consistent' we mean that the probabilities of all strings that are accepted sum to 1.)", "labels": [], "entities": []}, {"text": "It may even be the case that a (proper and consistent) probability function p G on the rules of the input grammar G exists that assigns a higher likelihood to the corpus than p rfe , and therefore it is not guaranteed that LR parsers allow better probability estimates than the CFGs from which they were constructed, if we constrain probability functions p A to be proper.", "labels": [], "entities": []}, {"text": "In this respect, LR parsing differs from at least one other well-known parsing strategy, viz.", "labels": [], "entities": [{"text": "LR parsing", "start_pos": 17, "end_pos": 27, "type": "TASK", "confidence": 0.9038405120372772}]}, {"text": "See () fora discussion of a property that is shared by left-corner parsing but not by LR parsing, and which explains the above difference.", "labels": [], "entities": [{"text": "LR parsing", "start_pos": 86, "end_pos": 96, "type": "TASK", "confidence": 0.5840802192687988}]}, {"text": "As main contribution of this paper we establish that this restriction on expressible probability distributions can be dispensed with, without losing the ability to perform training by relative frequency estimation.", "labels": [], "entities": [{"text": "relative frequency estimation", "start_pos": 184, "end_pos": 213, "type": "TASK", "confidence": 0.5887562135855356}]}, {"text": "What comes in place of properness is reverse-properness, which can be seen as properness of the reversed pushdown automaton that processes input from right to left instead of from left to right, interpreting the transitions of A backwards.", "labels": [], "entities": []}, {"text": "As we will show, reverse-properness does not restrict the space of probability distributions expressible by an LR automaton.", "labels": [], "entities": []}, {"text": "More precisely, assume some probability distribution on the set of derivations is specified by a probability function p A on transitions of PDT A that realizes the LR strategy fora given grammar G.", "labels": [], "entities": []}, {"text": "Then the same probability distribution can be specified by an alternative such function p A that is reverse-proper.", "labels": [], "entities": []}, {"text": "In addition, for each probability distribution on derivations expressible by a probability function p G for G, there is a reverse-proper probability function p A for A that expresses the same probability distribution.", "labels": [], "entities": []}, {"text": "Thereby we ensure that LR parsers become at least as powerful as the original CFGs in terms of allowable probability distributions.", "labels": [], "entities": [{"text": "CFGs", "start_pos": 78, "end_pos": 82, "type": "DATASET", "confidence": 0.9109053015708923}]}, {"text": "This article is organized as follows.", "labels": [], "entities": []}, {"text": "In Section 2 we outline our formalization of LR parsing as a construction of PDTs from CFGs, making some superficial changes with respect to standard formulations.", "labels": [], "entities": [{"text": "LR parsing", "start_pos": 45, "end_pos": 55, "type": "TASK", "confidence": 0.9256355166435242}, {"text": "CFGs", "start_pos": 87, "end_pos": 91, "type": "DATASET", "confidence": 0.755736231803894}]}, {"text": "Properness and reverse-properness are discussed in Section 3, where we will show that reverse-properness does not restrict the space of probability distributions.", "labels": [], "entities": []}, {"text": "Section 4 reports on experiments, and Section 5 concludes this article.", "labels": [], "entities": []}], "datasetContent": [{"text": "We have implemented both the traditional training method for LR parsing and the novel one, and have compared their performance, with two concrete objectives: 1.", "labels": [], "entities": [{"text": "LR parsing", "start_pos": 61, "end_pos": 71, "type": "TASK", "confidence": 0.9594521820545197}]}, {"text": "We show that the number of free parameters is significantly larger with the new training method.", "labels": [], "entities": []}, {"text": "(The number of free parameters is the number of probabilities of transitions that can be freely chosen within the constraints of properness or reverse-properness.)", "labels": [], "entities": []}, {"text": "2. The larger number of free parameters does not make the problem of sparse data any worse, and precision and recall are at least comparable to, if not better than, what we would obtain with the established method.", "labels": [], "entities": [{"text": "precision", "start_pos": 96, "end_pos": 105, "type": "METRIC", "confidence": 0.9994888305664062}, {"text": "recall", "start_pos": 110, "end_pos": 116, "type": "METRIC", "confidence": 0.9995611310005188}]}, {"text": "The experiments were performed on the Wall Street Journal (WSJ) corpus, from the Penn Treebank, version II.", "labels": [], "entities": [{"text": "Wall Street Journal (WSJ) corpus", "start_pos": 38, "end_pos": 70, "type": "DATASET", "confidence": 0.9447466305324009}, {"text": "Penn Treebank, version II", "start_pos": 81, "end_pos": 106, "type": "DATASET", "confidence": 0.9363309025764466}]}, {"text": "Training was done on sections 02-21, i.e., first a context-free grammar was derived from the 'stubs' of the combined trees, taking parts of speech as leaves of the trees, omitting all affixes from the nonterminal names, and removing \u03b5-generating subtrees.", "labels": [], "entities": []}, {"text": "Such preprocessing of the WSJ corpus is consistent with earlier attempts to derive CFGs from that corpus, as e.g. by.", "labels": [], "entities": [{"text": "WSJ corpus", "start_pos": 26, "end_pos": 36, "type": "DATASET", "confidence": 0.967880517244339}]}, {"text": "The obtained CFG has 10,035 rules.", "labels": [], "entities": [{"text": "CFG", "start_pos": 13, "end_pos": 16, "type": "DATASET", "confidence": 0.889519453048706}]}, {"text": "The dimensions of the LR parser constructed from this grammar are given in.", "labels": [], "entities": []}, {"text": "The PDT was then trained on the trees from the same sections 02-21, to determine the number of times that transitions are used.", "labels": [], "entities": [{"text": "PDT", "start_pos": 4, "end_pos": 7, "type": "METRIC", "confidence": 0.5218809843063354}]}, {"text": "At first sight it is not clear how to determine this on the basis of the treebank, as the structure of LR parsers is very different from the structure of the grammars from which they are constructed.", "labels": [], "entities": []}, {"text": "The solution is to construct a second PDT from the PDT to be trained, replacing each transition \u03b1 a,b \u2192 \u03b2 with label r by transition \u03b1 b,r \u2192 \u03b2.", "labels": [], "entities": []}, {"text": "By this second PDT we parse the treebank, encoded as a series of right-most derivations in reverse.", "labels": [], "entities": []}, {"text": "1 For each input string, there is exactly one parse, of which the output is the list of used transitions.", "labels": [], "entities": []}, {"text": "The same method can be used for other parsing strategies as well, such as left-corner parsing, replacing right-most derivations by a suitable alternative representation of parse trees.", "labels": [], "entities": [{"text": "parsing", "start_pos": 38, "end_pos": 45, "type": "TASK", "confidence": 0.9644240736961365}]}, {"text": "By the counts of occurrences of transitions, we may then perform maximum likelihood estimation to obtain probabilities for transitions.", "labels": [], "entities": []}, {"text": "This can be done under the constraints of properness or of reverse-properness, as explained in the previous section.", "labels": [], "entities": []}, {"text": "We have not applied any form of smooth-  ing or back-off, as this could obscure properties inherent in the difference between the two discussed training methods.", "labels": [], "entities": [{"text": "smooth-  ing", "start_pos": 32, "end_pos": 44, "type": "TASK", "confidence": 0.720474362373352}]}, {"text": "(Back-off for probabilistic LR parsing has been proposed by).)", "labels": [], "entities": [{"text": "LR parsing", "start_pos": 28, "end_pos": 38, "type": "TASK", "confidence": 0.9010652601718903}]}, {"text": "All transitions that were not seen during training were given probability 0.", "labels": [], "entities": []}, {"text": "The results are outlined in.", "labels": [], "entities": []}, {"text": "Note that the number of free parameters in the case of reverseproperness is much larger than in the case of normal properness.", "labels": [], "entities": []}, {"text": "Despite of this, the number of transitions that actually receive non-zero probabilities is (predictably) identical in both cases, viz.", "labels": [], "entities": []}, {"text": "However, the potential for fine-grained probability estimates and for smoothing and parameter-tying techniques is clearly greater in the case of reverseproperness.", "labels": [], "entities": []}, {"text": "That in both cases the number of non-zero probabilities is lower than the total number of parameters can be explained as follows.", "labels": [], "entities": []}, {"text": "First, the treebank contains many rules that occur a small number of times.", "labels": [], "entities": []}, {"text": "Secondly, the LR automaton is much larger than the CFG; in general, the size of an LR automaton is bounded by a function that is exponential in the size of the input CFG.", "labels": [], "entities": [{"text": "CFG", "start_pos": 51, "end_pos": 54, "type": "DATASET", "confidence": 0.9586405158042908}]}, {"text": "Therefore, if we use the same treebank to estimate the probability function, then many transitions are never visited and obtain a zero probability.", "labels": [], "entities": []}, {"text": "We have applied the two trained LR automata on section 22 of the WSJ corpus, measuring labelled precision and recall, as done by e.g.).", "labels": [], "entities": [{"text": "WSJ corpus", "start_pos": 65, "end_pos": 75, "type": "DATASET", "confidence": 0.8793425559997559}, {"text": "precision", "start_pos": 96, "end_pos": 105, "type": "METRIC", "confidence": 0.929924488067627}, {"text": "recall", "start_pos": 110, "end_pos": 116, "type": "METRIC", "confidence": 0.9991719722747803}]}, {"text": "We observe that in the case of reverseproperness, precision and recall are slightly better.", "labels": [], "entities": [{"text": "precision", "start_pos": 50, "end_pos": 59, "type": "METRIC", "confidence": 0.9998117089271545}, {"text": "recall", "start_pos": 64, "end_pos": 70, "type": "METRIC", "confidence": 0.9997873902320862}]}, {"text": "The most important conclusion that can be drawn from this is that the substantially larger space of obtainable probability distributions offered by the reverse-properness method does not come at the expense of a degradation of accuracy for large grammars such as those derived from the WSJ.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 227, "end_pos": 235, "type": "METRIC", "confidence": 0.9976842403411865}, {"text": "WSJ", "start_pos": 286, "end_pos": 289, "type": "DATASET", "confidence": 0.963808536529541}]}, {"text": "For comparison, with a standard PCFG we obtain labelled precision and recall of 0.725 and 0.670, respectively.", "labels": [], "entities": [{"text": "PCFG", "start_pos": 32, "end_pos": 36, "type": "DATASET", "confidence": 0.9528705477714539}, {"text": "labelled", "start_pos": 47, "end_pos": 55, "type": "METRIC", "confidence": 0.9416970610618591}, {"text": "precision", "start_pos": 56, "end_pos": 65, "type": "METRIC", "confidence": 0.848293125629425}, {"text": "recall", "start_pos": 70, "end_pos": 76, "type": "METRIC", "confidence": 0.9997398257255554}]}, {"text": "We would like to stress that our experiments did not have as main objective the improvement of state-of-the-art parsers, which can certainly not be done without much additional fine-tuning and the incorporation of some form of lexicalization.", "labels": [], "entities": []}, {"text": "Our main objectives concerned the relation between our newly proposed training method for LR parsers and the traditional one.", "labels": [], "entities": [{"text": "LR parsers", "start_pos": 90, "end_pos": 100, "type": "TASK", "confidence": 0.7998628914356232}]}], "tableCaptions": [{"text": " Table 1: Dimensions of PDT implementing LR  strategy for CFG derived from WSJ, sect. 02-21.", "labels": [], "entities": [{"text": "PDT", "start_pos": 24, "end_pos": 27, "type": "TASK", "confidence": 0.9197441339492798}, {"text": "CFG", "start_pos": 58, "end_pos": 61, "type": "DATASET", "confidence": 0.7971957325935364}, {"text": "WSJ, sect. 02-21", "start_pos": 75, "end_pos": 91, "type": "DATASET", "confidence": 0.9395014524459839}]}, {"text": " Table 2: The two methods of training, based on  properness and reverse-properness.", "labels": [], "entities": [{"text": "properness", "start_pos": 49, "end_pos": 59, "type": "METRIC", "confidence": 0.9779726266860962}]}]}