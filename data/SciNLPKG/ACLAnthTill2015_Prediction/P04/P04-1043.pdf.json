{"title": [{"text": "A Study on Convolution Kernels for Shallow Semantic Parsing", "labels": [], "entities": []}], "abstractContent": [{"text": "In this paper we have designed and experimented novel convolution kernels for automatic classification of predicate arguments.", "labels": [], "entities": [{"text": "automatic classification of predicate arguments", "start_pos": 78, "end_pos": 125, "type": "TASK", "confidence": 0.7233329713344574}]}, {"text": "Their main property is the ability to process struc-tured representations.", "labels": [], "entities": []}, {"text": "Support Vector Machines (SVMs), using a combination of such kernels and the flat feature kernel, classify Prop-Bank predicate arguments with accuracy higher than the current argument classification state-of-the-art.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 141, "end_pos": 149, "type": "METRIC", "confidence": 0.9990283250808716}]}, {"text": "Additionally, experiments on FrameNet data have shown that SVMs are appealing for the classification of semantic roles even if the proposed kernels do not produce any improvement.", "labels": [], "entities": [{"text": "FrameNet data", "start_pos": 29, "end_pos": 42, "type": "DATASET", "confidence": 0.8825110495090485}]}], "introductionContent": [{"text": "Several linguistic theories, e.g. claim that semantic information in natural language texts is connected to syntactic structures.", "labels": [], "entities": []}, {"text": "Hence, to deal with natural language semantics, the learning algorithm should be able to represent and process structured data.", "labels": [], "entities": []}, {"text": "The classical solution adopted for such tasks is to convert syntax structures into flat feature representations which are suitable fora given learning model.", "labels": [], "entities": []}, {"text": "The main drawback is that structures may not be properly represented by flat features.", "labels": [], "entities": []}, {"text": "In particular, these problems affect the processing of predicate argument structures annotated in PropBank () or FrameNet.", "labels": [], "entities": [{"text": "FrameNet", "start_pos": 113, "end_pos": 121, "type": "DATASET", "confidence": 0.8395354747772217}]}, {"text": "shows an example of a predicate annotation in PropBank for the sentence: \"Paul gives a lecture in Rome\".", "labels": [], "entities": [{"text": "PropBank", "start_pos": 46, "end_pos": 54, "type": "DATASET", "confidence": 0.9755472540855408}]}, {"text": "A predicate maybe a verb or a noun or an adjective and most of the time Arg 0 is the logical subject, Arg 1 is the logical object and ArgM may indicate locations, as in our example.", "labels": [], "entities": [{"text": "Arg 0", "start_pos": 72, "end_pos": 77, "type": "METRIC", "confidence": 0.9667328596115112}, {"text": "ArgM", "start_pos": 134, "end_pos": 138, "type": "METRIC", "confidence": 0.9852051138877869}]}, {"text": "FrameNet also describes predicate/argument structures but for this purpose it uses richer semantic structures called frames.", "labels": [], "entities": [{"text": "FrameNet", "start_pos": 0, "end_pos": 8, "type": "DATASET", "confidence": 0.8121987581253052}]}, {"text": "These latter are schematic representations of situations involving various participants, properties and roles in which a word maybe typically used.", "labels": [], "entities": []}, {"text": "Frame elements or semantic roles are arguments of predicates called target words.", "labels": [], "entities": []}, {"text": "In FrameNet, the argument names are local to a particular frame.", "labels": [], "entities": []}, {"text": "Several machine learning approaches for argument identification and classification have been developed (.", "labels": [], "entities": [{"text": "argument identification and classification", "start_pos": 40, "end_pos": 82, "type": "TASK", "confidence": 0.7780580371618271}]}, {"text": "Their common characteristic is the adoption of feature spaces that model predicate-argument structures in a flat representation.", "labels": [], "entities": []}, {"text": "On the contrary, convolution kernels aim to capture structural information in term of sub-structures, providing a viable alternative to flat features.", "labels": [], "entities": []}, {"text": "In this paper, we select portions of syntactic trees, which include predicate/argument salient sub-structures, to define convolution kernels for the task of predicate argument classification.", "labels": [], "entities": [{"text": "predicate argument classification", "start_pos": 157, "end_pos": 190, "type": "TASK", "confidence": 0.8624306917190552}]}, {"text": "In particular, our kernels aim to (a) represent the relation between predicate and one of its arguments and (b) to capture the overall argument structure of the target predicate.", "labels": [], "entities": []}, {"text": "Additionally, we define novel kernels as combinations of the above two with the polynomial kernel of standard flat features.", "labels": [], "entities": []}, {"text": "Experiments on Support Vector Machines using the above kernels show an improvement of the state-of-the-art for PropBank argument classification.", "labels": [], "entities": [{"text": "PropBank argument classification", "start_pos": 111, "end_pos": 143, "type": "TASK", "confidence": 0.7230792045593262}]}, {"text": "On the contrary, FrameNet semantic parsing seems to not take advantage of the structural information provided by our kernels.", "labels": [], "entities": [{"text": "FrameNet semantic parsing", "start_pos": 17, "end_pos": 42, "type": "TASK", "confidence": 0.6896424293518066}]}, {"text": "The remainder of this paper is organized as follows: Section 2 defines the Predicate Argument Extraction problem and the standard solution to solve it.", "labels": [], "entities": [{"text": "Predicate Argument Extraction problem", "start_pos": 75, "end_pos": 112, "type": "TASK", "confidence": 0.7865216284990311}]}, {"text": "In Section 3 we present our kernels whereas in Section 4 we show comparative results among SVMs using standard features and the proposed kernels.", "labels": [], "entities": []}, {"text": "Finally, Section 5 summarizes the conclusions.", "labels": [], "entities": []}], "datasetContent": [{"text": "The aim of our experiments are twofold: On the one hand, we study if the PAF representation produces an accuracy higher than standard features.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 104, "end_pos": 112, "type": "METRIC", "confidence": 0.9991089701652527}]}, {"text": "On the other hand, we study if SCF can be used to classify verbs according to their syntactic realization.", "labels": [], "entities": []}, {"text": "Both the above aims can be carried out by combining PAF and SCF with the standard features.", "labels": [], "entities": [{"text": "PAF", "start_pos": 52, "end_pos": 55, "type": "METRIC", "confidence": 0.83245450258255}]}, {"text": "For this purpose we adopted two ways to combine kernels 3 : (1) K = K 1 \u00b7 K 2 and (2) K = \u03b3K 1 + K 2 . The resulting set of kernels used in the experiments is the following: \u2022 K pd is the polynomial kernel with degree d over the standard features.", "labels": [], "entities": []}, {"text": "\u2022 KP AF is obtained by using PAK function over the PAF structures.", "labels": [], "entities": []}, {"text": "\u2022 e. the sum between the normalized 4 PAF-based kernel and the normalized polynomial kernel.", "labels": [], "entities": []}, {"text": "\u2022 .e. the normalized product between the PAF-based kernel and the polynomial kernel.", "labels": [], "entities": []}, {"text": ".e. the summation between the normalized SCF-based kernel and the normalized polynomial kernel.", "labels": [], "entities": []}, {"text": "\u2022 .e. the normalized product between SCF-based kernel and the polynomial kernel.", "labels": [], "entities": []}, {"text": "To study the impact of our structural kernels we firstly derived the maximal accuracy reachable with standard features along with polynomial kernels.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 77, "end_pos": 85, "type": "METRIC", "confidence": 0.9412469863891602}]}, {"text": "The multi-class accuracies, for PropBank and FrameNet using K pd with d = 1, .., 5, are shown in.", "labels": [], "entities": [{"text": "PropBank", "start_pos": 32, "end_pos": 40, "type": "DATASET", "confidence": 0.9559618234634399}]}, {"text": "We note that (a) the highest performance is reached ford = 3, (b) for PropBank our maximal accuracy (90.5%) 7 f1 assigns equal importance to Precision P and Recall R, i.e. f1 = 2P \u00b7R P +R . is substantially equal to the SVM performance (88%) obtained in () with degree 2 and (c) the accuracy on FrameNet (85.2%) is higher than the best result obtained in literature, i.e. 82.0% in (.", "labels": [], "entities": [{"text": "PropBank", "start_pos": 70, "end_pos": 78, "type": "DATASET", "confidence": 0.9013639092445374}, {"text": "accuracy", "start_pos": 91, "end_pos": 99, "type": "METRIC", "confidence": 0.8929357528686523}, {"text": "Recall R", "start_pos": 157, "end_pos": 165, "type": "METRIC", "confidence": 0.9614030718803406}, {"text": "accuracy", "start_pos": 283, "end_pos": 291, "type": "METRIC", "confidence": 0.9992019534111023}, {"text": "FrameNet", "start_pos": 295, "end_pos": 303, "type": "DATASET", "confidence": 0.8597530722618103}]}, {"text": "This different outcome is due to a different task (we classify different roles) and a different classification algorithm.", "labels": [], "entities": []}, {"text": "Moreover, we did not use the Frame information which is very important 8 . It is worth noting that the difference between linear and polynomial kernel is about 3-4 percent points for both PropBank and FrameNet.", "labels": [], "entities": [{"text": "PropBank", "start_pos": 188, "end_pos": 196, "type": "DATASET", "confidence": 0.9483675956726074}]}, {"text": "This remarkable difference can be easily explained by considering the meaning of standard features.", "labels": [], "entities": []}, {"text": "For example, let us restrict the classification function C Arg0 to the two features Voice and Position.", "labels": [], "entities": [{"text": "Arg0", "start_pos": 59, "end_pos": 63, "type": "METRIC", "confidence": 0.8325830698013306}]}, {"text": "Without loss of generality we can assume: (a) Voice=1 if active and 0 if passive, and (b) Position=1 when the argument is after the predicate and 0 otherwise.", "labels": [], "entities": [{"text": "Position", "start_pos": 90, "end_pos": 98, "type": "METRIC", "confidence": 0.9753487706184387}]}, {"text": "To simplify the example, we also assume that if an argument precedes the target predicate it is a subject, otherwise it is an object 9 . It follows that a constituent is Arg0, i.e. C Arg0 = 1, if only one feature at a time is 1, otherwise it is not an Arg0, i.e. C Arg0 = 0.", "labels": [], "entities": [{"text": "Arg0", "start_pos": 170, "end_pos": 174, "type": "METRIC", "confidence": 0.9604859948158264}]}, {"text": "In other words, C Arg0 = Position XOR Voice, which is the classical example of a non-linear separable function that becomes separable in a superlinear space).", "labels": [], "entities": [{"text": "Arg0", "start_pos": 18, "end_pos": 22, "type": "METRIC", "confidence": 0.9098076224327087}]}, {"text": "After it was established that the best kernel for standard features is K p 3 , we carried out all the other experiments using it in the kernel combinations.", "labels": [], "entities": []}, {"text": "show the single class (f 1 measure) as well as multi-class classifier (accuracy) performance for PropBank and FrameNet respectively.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 71, "end_pos": 79, "type": "METRIC", "confidence": 0.9732809066772461}, {"text": "PropBank", "start_pos": 97, "end_pos": 105, "type": "DATASET", "confidence": 0.9464383721351624}]}, {"text": "Each column of the two tables refers to a different kernel defined in the 8 Preliminary experiments indicate that SVMs can reach 90% by using the frame feature.", "labels": [], "entities": []}, {"text": "9 Indeed, this is true inmost part of the cases.", "labels": [], "entities": []}, {"text": "The overall meaning is discussed in the following points: First, PAF alone has good performance, since in PropBank evaluation it outperforms the linear kernel (K p 1 ), 88.7% vs. 86.7% whereas in FrameNet, it shows a similar performance 79.5% vs. 82.1% (compare tables with).", "labels": [], "entities": [{"text": "PAF", "start_pos": 65, "end_pos": 68, "type": "METRIC", "confidence": 0.6812886595726013}]}, {"text": "This suggests that PAF generates the same information as the standard features in a linear space.", "labels": [], "entities": []}, {"text": "However, when a degree greater than 1 is used for standard features, PAF is outperformed 10 .   Second, SCF improves the polynomial kernel (d = 3), i.e. the current state-of-the-art, of about 3 percent points on PropBank (column SCF\u00b7P).", "labels": [], "entities": [{"text": "PAF", "start_pos": 69, "end_pos": 72, "type": "METRIC", "confidence": 0.7994028925895691}, {"text": "PropBank", "start_pos": 212, "end_pos": 220, "type": "DATASET", "confidence": 0.9849696159362793}]}, {"text": "This suggests that (a) PAK can measure the similarity between two SCF structures and (b) the sub-categorization information provides effective clues about the expected argument type.", "labels": [], "entities": []}, {"text": "The interesting consequence is that SCF together with PAK seems suitable to automatically cluster different verbs that have the same syntactic realization.", "labels": [], "entities": []}, {"text": "We note also that to fully exploit the SCF information it is necessary to use a kernel product (K 1 \u00b7 K 2 ) combination rather than the sum (K 1 + K 2 ), e.g. column SCF+P.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Evaluation of Kernels on PropBank.", "labels": [], "entities": []}, {"text": " Table 3: Evaluation of Kernels on FrameNet se-", "labels": [], "entities": []}]}