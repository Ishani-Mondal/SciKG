{"title": [{"text": "Statistical Modeling for Unit Selection in Speech Synthesis", "labels": [], "entities": [{"text": "Unit Selection", "start_pos": 25, "end_pos": 39, "type": "TASK", "confidence": 0.7064304798841476}, {"text": "Speech Synthesis", "start_pos": 43, "end_pos": 59, "type": "TASK", "confidence": 0.6790128797292709}]}], "abstractContent": [{"text": "Traditional concatenative speech synthesis systems use a number of heuristics to define the target and concatenation costs, essential for the design of the unit selection component.", "labels": [], "entities": [{"text": "concatenative speech synthesis", "start_pos": 12, "end_pos": 42, "type": "TASK", "confidence": 0.6899750431378683}]}, {"text": "In contrast to these approaches , we introduce a general statistical model-ing framework for unit selection inspired by automatic speech recognition.", "labels": [], "entities": [{"text": "unit selection", "start_pos": 93, "end_pos": 107, "type": "TASK", "confidence": 0.7608993351459503}, {"text": "speech recognition", "start_pos": 130, "end_pos": 148, "type": "TASK", "confidence": 0.7200078815221786}]}, {"text": "Given appropriate data, techniques based on that framework can result in a more accurate unit selection, thereby improving the general quality of a speech synthesizer.", "labels": [], "entities": []}, {"text": "They can also lead to a more modular and a substantially more efficient system.", "labels": [], "entities": []}, {"text": "We present anew unit selection system based on statistical modeling.", "labels": [], "entities": []}, {"text": "To overcome the original absence of data, we use an existing high-quality unit selection system to generate a corpus of unit sequences.", "labels": [], "entities": []}, {"text": "We show that the concatenation cost can be accurately estimated from this corpus using a statistical n-gram language model over units.", "labels": [], "entities": []}, {"text": "We used weighted automata and transducers for the representation of the components of the system and designed anew and more efficient composition algorithm making use of string potentials for their combination.", "labels": [], "entities": []}, {"text": "The resulting statistical unit selection is shown to be about 2.6 times faster than the last release of the AT&T Natural Voices Product while preserving the same quality, and offers much flexibility for the use and integration of new and more complex components.", "labels": [], "entities": [{"text": "AT&T Natural Voices Product", "start_pos": 108, "end_pos": 135, "type": "DATASET", "confidence": 0.9307638009389242}]}], "introductionContent": [], "datasetContent": [{"text": "We used the AT&T Natural Voices Product speech synthesis system to synthesize 107,987 AP news articles, generating a large corpus of 8,731,662 unit sequences representing a total of 415,227,388 units.", "labels": [], "entities": [{"text": "AT&T Natural Voices Product speech synthesis", "start_pos": 12, "end_pos": 56, "type": "DATASET", "confidence": 0.8805762082338333}]}, {"text": "We used this corpus to build several n-gram Katz backoff language models with n = 2 or 3.", "labels": [], "entities": []}, {"text": "Table 1 gives the size of the resulting language model weighted automata.", "labels": [], "entities": []}, {"text": "These language models were built using the GRM Library ().", "labels": [], "entities": [{"text": "GRM Library", "start_pos": 43, "end_pos": 54, "type": "DATASET", "confidence": 0.9308278262615204}]}, {"text": "We evaluated these models by using them to synthesize an AP news article of 1,000 words, corresponding to 8250 units or 6 minutes of synthesized speech.: Computation time for each unit selection system when used to synthesize the same AP news article.", "labels": [], "entities": [{"text": "AP news article", "start_pos": 57, "end_pos": 72, "type": "DATASET", "confidence": 0.8807303309440613}]}, {"text": "Experiments were run on a 1GHz Pentium III processor with 256KB of cache and 2GB of memory.", "labels": [], "entities": []}, {"text": "The baseline system mentioned in this table is the AT&T Natural Voices Product which was also used to generate our training corpus using the concatenation cost caching method from ().", "labels": [], "entities": [{"text": "AT&T Natural Voices Product", "start_pos": 51, "end_pos": 78, "type": "DATASET", "confidence": 0.9449030657609304}]}, {"text": "For the new system, both the computation times due to composition and to the search are displayed.", "labels": [], "entities": []}, {"text": "Note that the AT&T Natural Voices Product system was highly optimized for speed.", "labels": [], "entities": [{"text": "AT&T Natural Voices Product system", "start_pos": 14, "end_pos": 48, "type": "DATASET", "confidence": 0.9063222834042141}]}, {"text": "In our new systems, the standard research software libraries already mentioned were used.", "labels": [], "entities": []}, {"text": "The search was performed using the standard speech recognition Viterbi decoder from the DCD library.", "labels": [], "entities": [{"text": "DCD library", "start_pos": 88, "end_pos": 99, "type": "DATASET", "confidence": 0.9264281392097473}]}, {"text": "With a trigram language model, our new statistical unit selection system was about 2.6 times faster than the baseline system.", "labels": [], "entities": [{"text": "statistical unit selection", "start_pos": 39, "end_pos": 65, "type": "TASK", "confidence": 0.6388296286265055}]}, {"text": "A formal test using the standard mean of opinion score (MOS) was used to compare the quality of the high-quality AT&T Natural Voices Product synthesizer and that of the synthesizers based on our new unit selection system with shrunken and unshrunken trigram language models.", "labels": [], "entities": [{"text": "mean of opinion score (MOS)", "start_pos": 33, "end_pos": 60, "type": "METRIC", "confidence": 0.7986202495438712}, {"text": "AT&T Natural Voices Product synthesizer", "start_pos": 113, "end_pos": 152, "type": "DATASET", "confidence": 0.9173558013779777}]}, {"text": "In such tests, several listeners are asked to rank the quality of each utterance from 1 (worst score) to 5 (best).", "labels": [], "entities": []}, {"text": "The MOS results of the three systems with 60 utterances tested by 21 listeners are reported in with their correspondModel raw score normalized score baseline system 3.54 \u00b1 .20 3.09 \u00b1 .22 3-gram, unshrunken 3.45 \u00b1 .20 2.98 \u00b1 .21 3-gram, \u03b3 = \u22121 3.40 \u00b1 .20 2.93 \u00b1 .22: Quality testing results: we report for each system, the mean and standard error of the raw and the listener-normalized scores.", "labels": [], "entities": [{"text": "correspondModel raw score normalized score baseline", "start_pos": 106, "end_pos": 157, "type": "METRIC", "confidence": 0.9077704151471456}]}, {"text": "The difference of scores between the three systems is not statistically significant (first column), in particular, the absolute difference between the two best systems is less than .1.", "labels": [], "entities": []}, {"text": "Different listeners may rank utterances in different ways.", "labels": [], "entities": []}, {"text": "Some may choose the full range of scores (1-5) to rank each utterance, others may select a smaller range near 5, near 3, or some other range.", "labels": [], "entities": []}, {"text": "To factor out such possible discrepancies in ranking, we also computed the listener-normalized scores (second column of the table).", "labels": [], "entities": []}, {"text": "This was done for each listener by removing the average score over the full set of utterances, dividing it by the standard deviation, and by centering it around 3.", "labels": [], "entities": []}, {"text": "The results show that the difference between the normalized scores of the three systems is not significantly different.", "labels": [], "entities": []}, {"text": "Thus, the MOS results show that the three systems have the same quality.", "labels": [], "entities": [{"text": "MOS", "start_pos": 10, "end_pos": 13, "type": "TASK", "confidence": 0.5810953974723816}]}, {"text": "We also measured the similarity of the two best systems by comparing the number of common units they produce for each utterance.", "labels": [], "entities": [{"text": "similarity", "start_pos": 21, "end_pos": 31, "type": "METRIC", "confidence": 0.9561523795127869}]}, {"text": "On the AP news article already mentioned, more than 75% of the units were common.", "labels": [], "entities": [{"text": "AP news article", "start_pos": 7, "end_pos": 22, "type": "DATASET", "confidence": 0.9805201689402262}]}], "tableCaptions": [{"text": " Table 1: Size of the stochastic language models for  different n-gram order and shrinking factor.", "labels": [], "entities": []}, {"text": " Table 2: Computation time for each unit selection  system when used to synthesize the same AP news  article.", "labels": [], "entities": [{"text": "Computation time", "start_pos": 10, "end_pos": 26, "type": "METRIC", "confidence": 0.9136343896389008}, {"text": "AP news  article", "start_pos": 92, "end_pos": 108, "type": "DATASET", "confidence": 0.91687540213267}]}]}