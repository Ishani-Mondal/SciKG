{"title": [{"text": "Multi-Engine Machine Translation with Voted Language Model", "labels": [], "entities": [{"text": "Multi-Engine Machine Translation", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.608911414941152}]}], "abstractContent": [{"text": "The paper describes a particular approach to multi-engine machine translation (MEMT), where we make use of voted language models to selectively combine translation outputs from multiple off-the-shelf MT systems.", "labels": [], "entities": [{"text": "multi-engine machine translation (MEMT)", "start_pos": 45, "end_pos": 84, "type": "TASK", "confidence": 0.7745236953099569}]}, {"text": "Experiments are done using large corpora from three distinct domains.", "labels": [], "entities": []}, {"text": "The study found that the use of voted language models leads to an improved performance of MEMT systems .", "labels": [], "entities": []}], "introductionContent": [{"text": "As the Internet grows, an increasing number of commercial MT systems are getting online ready to serve anyone anywhere on the earth.", "labels": [], "entities": [{"text": "MT", "start_pos": 58, "end_pos": 60, "type": "TASK", "confidence": 0.986703634262085}]}, {"text": "An interesting question we might ponder is whether it is not possible to aggregate the vast number of MT systems available on the Internet into one super MT which surpasses in performance any of those MTs that comprise the system.", "labels": [], "entities": []}, {"text": "And this is what we will be concerned within the paper, with somewhat watered-down settings.", "labels": [], "entities": []}, {"text": "People in the speech community pursued the idea of combining off-the-shelf ASRs (automatic speech recognizers) into a super ASR for sometime, and found that the idea works.", "labels": [], "entities": [{"text": "ASRs (automatic speech recognizers)", "start_pos": 75, "end_pos": 110, "type": "TASK", "confidence": 0.6648239096005758}]}, {"text": "In IR (information retrieval), we find some efforts going (under the name of distributed IR or meta-search) to selectively fuse outputs from multiple search engines on the Internet (.", "labels": [], "entities": [{"text": "IR (information retrieval)", "start_pos": 3, "end_pos": 29, "type": "TASK", "confidence": 0.6942597985267639}]}, {"text": "So it would be curious to see whether we could do the same with MTs.", "labels": [], "entities": [{"text": "MTs", "start_pos": 64, "end_pos": 67, "type": "TASK", "confidence": 0.9014926552772522}]}, {"text": "Now back in machine translation, we do find some work addressing such concern: develop a multi-engine MT or MEMT architecture which operates by combining outputs from three different engines based on the knowledge it has about inner workings of each of the component engines. is a continuation of with an addition of a ngrambased mechanism fora candidate selection., however, explores a different line of research whose goal is to combine black box MTs using statistical confidence models.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 12, "end_pos": 31, "type": "TASK", "confidence": 0.7881421148777008}, {"text": "MTs", "start_pos": 449, "end_pos": 452, "type": "TASK", "confidence": 0.9228105545043945}]}, {"text": "Similar efforts are also found in.", "labels": [], "entities": []}, {"text": "The present paper builds on the prior work by.", "labels": [], "entities": []}, {"text": "We start by reviewing his approach, and goon to demonstrate that it could be improved by capitalizing on dependence of the MEMT model thereon language model.", "labels": [], "entities": []}, {"text": "Throughout the paper, we refer to commercial black box MT systems as OTS (off-the-shelf) systems, or more simply, OTSs.", "labels": [], "entities": [{"text": "MT", "start_pos": 55, "end_pos": 57, "type": "TASK", "confidence": 0.9204837679862976}]}], "datasetContent": [{"text": "We assume here that the MEMT works on a sentence-by-sentence basis.", "labels": [], "entities": []}, {"text": "That is, it takes as input a source sentence, gets it translated by several OTSs, and picks up the best among translations it gets.", "labels": [], "entities": []}, {"text": "Now a problem with using BLEU in this setup is that translations often end up with zero because model translations they refer to do not contain ngrams of a particular length.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 25, "end_pos": 29, "type": "METRIC", "confidence": 0.9959151148796082}]}, {"text": "2 This would make impossible a comparison and selection among possible translations.", "labels": [], "entities": []}, {"text": "One way out of this, suggests, is to back off to a somewhat imprecise yet robust metric for evaluating translations, which he calls mprecision.", "labels": [], "entities": []}, {"text": "The idea of m-precision helps define what an optimal MEMT should look like.", "labels": [], "entities": []}, {"text": "Imagine a system which operates by choosing, among candidates, a translation that gives a best m-precision.", "labels": [], "entities": []}, {"text": "We would reasonably expect the system to outperform any of its component OTSs.", "labels": [], "entities": []}, {"text": "Indeed demonstrates empirically that it is the case.", "labels": [], "entities": []}, {"text": "Moreover, since rFLM \u03c8 and rALM \u03c8 work on a sentence, not on a block of them, what h(\u00b7) relates to is not BLEU, but m-precision.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 106, "end_pos": 110, "type": "METRIC", "confidence": 0.998485267162323}]}, {"text": "Hogan and Frederking (1998) introduces anew kind of yardstick for measuring the effectiveness of MEMT systems.", "labels": [], "entities": []}, {"text": "The rationale for this is that it is often the case that the efficacy of MEMT systems does not translate into performance of outputs that they generate.", "labels": [], "entities": []}, {"text": "We recall that with BLEU, one measures performance of translations, not how often a given MEMT system picks the best translation among candidates.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 20, "end_pos": 24, "type": "METRIC", "confidence": 0.9972713589668274}]}, {"text": "The problem is, even if a MEMT is right about its choices more often than a best component engine, BLEU may not show it.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 99, "end_pos": 103, "type": "METRIC", "confidence": 0.9967055916786194}]}, {"text": "This happens because a best translation may not always get a high score in BLEU.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 75, "end_pos": 79, "type": "METRIC", "confidence": 0.9944643378257751}]}, {"text": "Indeed, differences in BLEU among candidate translations could be very small.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 23, "end_pos": 27, "type": "METRIC", "confidence": 0.9988308548927307}]}, {"text": "Now what Hogan and Frederking (1998) suggest is the following.", "labels": [], "entities": []}, {"text": "where \u03b4(i, j) is the Kronecker delta function, which gives 1 if i = j and 0 otherwise.", "labels": [], "entities": []}, {"text": "Here \u03c8 m represents some MEMT system, \u03c8 m (e) denotes a particular translation \u03c8 m chooses for sentence e, i.e., \u03c8 m (e) = \u03a8(e, J, l).", "labels": [], "entities": []}, {"text": "\u03c3 e M \u2208 J denotes a set of candidate translations.", "labels": [], "entities": []}, {"text": "max here gives a translation with the highest score in m-precision.", "labels": [], "entities": []}, {"text": "N is the number of source sentences.", "labels": [], "entities": []}, {"text": "\u03b4(\u00b7) says that you get 1 if a particular translation the MEMT chooses fora given sentences happens to rank highest among can-3 For a reference translation rand a machine-generated translation t, m-precision is defined as: which is nothing more than's modified n-gram precision applied to a pair of a single reference and the associated translation.", "labels": [], "entities": []}, {"text": "Si there denotes a set of i-grams int, van i-gram.", "labels": [], "entities": []}, {"text": "C(v, t) indicates the count of v int.", "labels": [], "entities": [{"text": "count", "start_pos": 22, "end_pos": 27, "type": "METRIC", "confidence": 0.9527156352996826}]}, {"text": "Let us describe the setup of experiments we have conducted.", "labels": [], "entities": []}, {"text": "The goal here is to learn how the Vby-M affects the overall MEMT performance.", "labels": [], "entities": []}, {"text": "For test sets, we carryover those from the perplexity experiments (see Footnote 6, Section 4), which are derived from CPC, EJP, and PAT.", "labels": [], "entities": [{"text": "CPC", "start_pos": 118, "end_pos": 121, "type": "DATASET", "confidence": 0.8895860910415649}]}, {"text": "(Call them tCPC, tEJP, and tPAT hereafter.)", "labels": [], "entities": []}, {"text": "In experiments, we begin by splitting a test set into equal-sized blocks, each containing 500 sentences for tEJP and tCPC, and 100 abstracts (approximately 200 sentences) for tPAT.", "labels": [], "entities": []}, {"text": "We had the total of 15 blocks for tCPC and tEJP, and 46 blocks for tPAT.", "labels": [], "entities": [{"text": "tPAT", "start_pos": 67, "end_pos": 71, "type": "DATASET", "confidence": 0.9164742827415466}]}, {"text": "We leave one for evaluation and use the rest for training alignment models, i.e., Q(e | j), SV regressors and some inside-data LMs.", "labels": [], "entities": []}, {"text": "(Again we took care not to inadvertently train LMs on test sets.)", "labels": [], "entities": []}, {"text": "We send a test block to OTSs Ai, Lo, At, and Ib, for translation and combine their outputs using the V-by-M scheme, which mayor may not be coupled with regression SVMs.", "labels": [], "entities": [{"text": "translation", "start_pos": 53, "end_pos": 64, "type": "TASK", "confidence": 0.9834998846054077}]}, {"text": "Recall that the MEMT operates on a sentence by sentence basis.", "labels": [], "entities": []}, {"text": "So what happens here is that for each of the sentences in a block, the MEMT works the four MT systems to get translations and picks one that produces the best score under \u03b8.", "labels": [], "entities": []}, {"text": "We evaluate the MEMT performance by running HFA and BLEU on MEMT selected translations block by block, and giving average performance over the blocks.", "labels": [], "entities": [{"text": "MEMT", "start_pos": 16, "end_pos": 20, "type": "TASK", "confidence": 0.8829900622367859}, {"text": "HFA", "start_pos": 44, "end_pos": 47, "type": "METRIC", "confidence": 0.8282734155654907}, {"text": "BLEU", "start_pos": 52, "end_pos": 56, "type": "METRIC", "confidence": 0.9964859485626221}]}, {"text": "provides algorithmic details on how the MEMT actually operates.", "labels": [], "entities": []}, {"text": "8 It is worth noting that the voted language model readily lends itself to a mixture model: P (j) = P m\u2208M \u03bbmP (j | m) where \u03bbm = 1 if m is most voted for and 0 otherwise.", "labels": [], "entities": []}, {"text": "tCPC had the average of 15,478 words per block, whereas tEJP had about 11,964 words on the average in each block.", "labels": [], "entities": [{"text": "tCPC", "start_pos": 0, "end_pos": 4, "type": "DATASET", "confidence": 0.9609187245368958}]}, {"text": "With tPAT, however, the average per block word length grew to 16,150.", "labels": [], "entities": []}, {"text": "We evaluate performance by block, because of some reports in the MT literature that warn that BLEU behaves erratically on a small set of sentences.", "labels": [], "entities": [{"text": "MT literature", "start_pos": 65, "end_pos": 78, "type": "DATASET", "confidence": 0.850370466709137}, {"text": "BLEU", "start_pos": 94, "end_pos": 98, "type": "METRIC", "confidence": 0.9966827034950256}]}, {"text": "See also Section 3 and Footnote 2 for the relevant discussion.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: HF accuracy of MEMT models with V-by- M.", "labels": [], "entities": [{"text": "HF", "start_pos": 10, "end_pos": 12, "type": "METRIC", "confidence": 0.8285316824913025}, {"text": "accuracy", "start_pos": 13, "end_pos": 21, "type": "METRIC", "confidence": 0.8453198075294495}]}, {"text": " Table 3: HF accuracy of MEMT models with ran- domly chosen LMs. Note how FLM \u03c8 and ALM \u03c8  drop in performance.", "labels": [], "entities": [{"text": "HF", "start_pos": 10, "end_pos": 12, "type": "METRIC", "confidence": 0.8783700466156006}, {"text": "accuracy", "start_pos": 13, "end_pos": 21, "type": "METRIC", "confidence": 0.8077515363693237}, {"text": "FLM \u03c8", "start_pos": 74, "end_pos": 79, "type": "METRIC", "confidence": 0.9194228649139404}]}, {"text": " Table 4: Performance in BLEU of MEMT models  with V-by-M.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 25, "end_pos": 29, "type": "METRIC", "confidence": 0.9989088773727417}]}, {"text": " Table 5: Performance in BLEU of MEMT models  with randomly chosen LMs.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 25, "end_pos": 29, "type": "METRIC", "confidence": 0.998399555683136}]}, {"text": " Table 6: HF accuracy of OTS systems", "labels": [], "entities": [{"text": "HF", "start_pos": 10, "end_pos": 12, "type": "METRIC", "confidence": 0.7049084305763245}, {"text": "accuracy", "start_pos": 13, "end_pos": 21, "type": "METRIC", "confidence": 0.8039770126342773}, {"text": "OTS", "start_pos": 25, "end_pos": 28, "type": "TASK", "confidence": 0.83226478099823}]}, {"text": " Table 7: Performance of OTS systems in BLEU.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 40, "end_pos": 44, "type": "METRIC", "confidence": 0.7585794925689697}]}, {"text": " Table 8: HF accuracy of MEMTs with perturbed SV  regressor in the V-by-M scheme.", "labels": [], "entities": [{"text": "HF", "start_pos": 10, "end_pos": 12, "type": "METRIC", "confidence": 0.7067915797233582}, {"text": "accuracy", "start_pos": 13, "end_pos": 21, "type": "METRIC", "confidence": 0.900373101234436}]}, {"text": " Table 9: Performance in BLEU of MEMTs with per- turbed SV regressor in the V-by-M scheme.", "labels": [], "entities": [{"text": "BLEU", "start_pos": 25, "end_pos": 29, "type": "METRIC", "confidence": 0.9985859394073486}]}]}