{"title": [{"text": "Error Mining for Wide-Coverage Grammar Engineering", "labels": [], "entities": [{"text": "Error Mining", "start_pos": 0, "end_pos": 12, "type": "TASK", "confidence": 0.831353485584259}]}], "abstractContent": [{"text": "Parsing systems which rely on hand-coded linguistic descriptions can only perform adequately in as far as these descriptions are correct and complete.", "labels": [], "entities": []}, {"text": "The paper describes an error mining technique to discover problems in hand-coded linguistic descriptions for parsing such as grammars and lexicons.", "labels": [], "entities": [{"text": "error mining", "start_pos": 23, "end_pos": 35, "type": "TASK", "confidence": 0.7400013208389282}]}, {"text": "By analysing parse results for very large unannotated corpora, the technique discovers missing, incorrect or incomplete linguistic descriptions.", "labels": [], "entities": []}, {"text": "The technique uses the frequency of n-grams of words for arbitrary values of n.", "labels": [], "entities": []}, {"text": "It is shown how anew combination of suffix arrays and perfect hash finite automata allows an efficient implementation.", "labels": [], "entities": []}], "introductionContent": [{"text": "As we all know, hand-crafted linguistic descriptions such as wide-coverage grammars and large scale dictionaries contain mistakes, and are incomplete.", "labels": [], "entities": []}, {"text": "In the context of parsing, people often construct sets of example sentences that the system should be able to parse correctly.", "labels": [], "entities": [{"text": "parsing", "start_pos": 18, "end_pos": 25, "type": "TASK", "confidence": 0.9859237670898438}]}, {"text": "If a sentence cannot be parsed, it is a clear sign that something is wrong.", "labels": [], "entities": []}, {"text": "This technique only works in as far as the problems that might occur have been anticipated.", "labels": [], "entities": []}, {"text": "More recently, tree-banks have become available, and we can apply the parser to the sentences of the tree-bank and compare the resulting parse trees with the gold standard.", "labels": [], "entities": []}, {"text": "Such techniques are limited, however, because treebanks are relatively small.", "labels": [], "entities": []}, {"text": "This is a serious problem, because the distribution of words is Zipfian (there are very many words that occur very infrequently), and the same appears to hold for syntactic constructions.", "labels": [], "entities": []}, {"text": "In this paper, an error mining technique is described which is very effective at automatically discovering systematic mistakes in a parser by using very large (but unannotated) corpora.", "labels": [], "entities": [{"text": "error mining", "start_pos": 18, "end_pos": 30, "type": "TASK", "confidence": 0.7109040915966034}]}, {"text": "The idea is very simple.", "labels": [], "entities": []}, {"text": "We run the parser on a large set of sentences, and then analyze those sentences the parser cannot parse successfully.", "labels": [], "entities": []}, {"text": "Depending on the nature of the parser, we define the notion 'successful parse' in different ways.", "labels": [], "entities": []}, {"text": "In the experiments described here, we use the Alpino wide-coverage parser for Dutch ().", "labels": [], "entities": []}, {"text": "This parser is based on a large constructionalist HPSG for Dutch as well as a very large electronic dictionary (partly derived from CELEX, Parole, and CGN).", "labels": [], "entities": []}, {"text": "The parser is robust in the sense that it essentially always produces a parse.", "labels": [], "entities": []}, {"text": "If a full parse is not possible fora given sentence, then the parser returns a (minimal) number of parsed nonoverlapping sentence parts.", "labels": [], "entities": []}, {"text": "In the context of the present paper, a parse is called successful only if the parser finds an analysis spanning the full sentence.", "labels": [], "entities": []}, {"text": "The basic idea is to compare the frequency of words and word sequences in sentences that cannot be parsed successfully with the frequency of the same words and word sequences in unproblematic sentences.", "labels": [], "entities": []}, {"text": "As we illustrate in section 3, this technique obtains very good results if it is applied to large sets of sentences.", "labels": [], "entities": []}, {"text": "To compute the frequency of word sequences of arbitrary length for very large corpora, we use anew combination of suffix arrays and perfect hash finite automata.", "labels": [], "entities": []}, {"text": "This implementation is described in section 4.", "labels": [], "entities": []}, {"text": "The error mining technique is able to discover systematic problems which lead to parsing failure.", "labels": [], "entities": [{"text": "error mining", "start_pos": 4, "end_pos": 16, "type": "TASK", "confidence": 0.7569977343082428}, {"text": "parsing", "start_pos": 81, "end_pos": 88, "type": "TASK", "confidence": 0.9717831611633301}]}, {"text": "This includes missing, incomplete and incorrect lexical entries and grammar rules.", "labels": [], "entities": []}, {"text": "Problems which cause the parser to assign complete but incorrect parses cannot be discovered.", "labels": [], "entities": []}, {"text": "Therefore, tree-banks and hand-crafted sets of example sentences remain important to discover problems of the latter type.", "labels": [], "entities": []}], "datasetContent": [{"text": "For our experiments, we used the Twente Nieuws Corpus, version pre-release 0.1. 1 This corpus contains among others a large collection of news articles from various Dutch newspapers in the period 1994-2001.", "labels": [], "entities": [{"text": "Twente Nieuws Corpus", "start_pos": 33, "end_pos": 53, "type": "DATASET", "confidence": 0.9473432898521423}]}, {"text": "In addition, we used all news articles from the Volkskrant 1997 (available on CD-ROM).", "labels": [], "entities": [{"text": "Volkskrant 1997", "start_pos": 48, "end_pos": 63, "type": "DATASET", "confidence": 0.8527510762214661}]}, {"text": "In order that this material can be parsed relatively quickly, we discarded all sentences of more than 20 words.", "labels": [], "entities": []}, {"text": "Furthermore, a time-out per sentence of twenty CPU-seconds was enforced.", "labels": [], "entities": []}, {"text": "The Alpino parser normally exploits a part-of-speech tag filter for efficient parsing which was switched off, to ensure that the results were not influenced by mistakes due to this filter.", "labels": [], "entities": []}, {"text": "In table 1 we list some basic quantitative facts about this material.", "labels": [], "entities": []}, {"text": "We exploited a cluster of Linux PCs for parsing.", "labels": [], "entities": [{"text": "parsing", "start_pos": 40, "end_pos": 47, "type": "TASK", "confidence": 0.977145791053772}]}, {"text": "If only a single PC had been available, it would have taken in the order of 100 CPU days, to construct the material described in table 1.", "labels": [], "entities": []}, {"text": "These experiments were performed in the autumn of 2002, with the Alpino parser available then.", "labels": [], "entities": []}, {"text": "Below, we report on more recent experiments with the latest version of the Alpino parser, which has been improved quite a lot on the basis of the results of the experiments described here.", "labels": [], "entities": []}, {"text": "For the data described above, we computed the parsability table, using a frequency cutoff of 5.", "labels": [], "entities": []}, {"text": "In the frequencies of parsability scores in the parsability table are presented.", "labels": [], "entities": []}, {"text": "From the figure, it is immediately clear that the relatively high number of word sequences with a parsability of (almost) zero cannot be due to chance.", "labels": [], "entities": []}, {"text": "Indeed, the  parsability table starts with word sequences which constitute systematic problems for the parser.", "labels": [], "entities": []}, {"text": "In quite a lot of cases, these word sequences originate from particular types of newspaper text with idiosyncratic syntax, such as announcements of new books, movies, events, television programs etc.; as well as checkers, bridge and chess diagrams.", "labels": [], "entities": []}, {"text": "Another category consists of (parts of) English, French and German phrases.", "labels": [], "entities": []}, {"text": "We also find frequent spelling mistakes such as de de where only a single de (the definite article) is expected, and heben for hebben (to have), indentiek for identiek (identical), koninging for koningin (queen), etc.", "labels": [], "entities": []}, {"text": "Other examples include wordt ik (becomes I), vindt ik (finds I), vind hij (find he) etc.", "labels": [], "entities": []}, {"text": "We now describe a number of categories of examples which have been used to improve the parser.", "labels": [], "entities": []}, {"text": "A number of n-grams with low parsability scores point towards systematic mistakes during tokenization.", "labels": [], "entities": [{"text": "parsability", "start_pos": 29, "end_pos": 40, "type": "METRIC", "confidence": 0.9542191028594971}, {"text": "tokenization", "start_pos": 89, "end_pos": 101, "type": "TASK", "confidence": 0.9653393626213074}]}, {"text": "Here area number of examples: 2 The @ symbol indicates a sentence boundary.", "labels": [], "entities": []}, {"text": "Many of the errors and omissions that were found on the basis of the parsability table have been corrected.", "labels": [], "entities": [{"text": "parsability", "start_pos": 69, "end_pos": 80, "type": "METRIC", "confidence": 0.8950014710426331}]}, {"text": "As can be seen in table 2, the coverage obtained by the improved parser increased substantially.", "labels": [], "entities": [{"text": "coverage", "start_pos": 31, "end_pos": 39, "type": "METRIC", "confidence": 0.9894920587539673}]}, {"text": "In this experiment, we also measured the coverage on additional sets of sentences (all sentences from the Trouw 1999 and Volkskrant 2001 newspaper, available in the TwNC corpus).", "labels": [], "entities": [{"text": "coverage", "start_pos": 41, "end_pos": 49, "type": "METRIC", "confidence": 0.993162989616394}, {"text": "Trouw 1999 and Volkskrant 2001 newspaper", "start_pos": 106, "end_pos": 146, "type": "DATASET", "confidence": 0.8973825176556905}, {"text": "TwNC corpus", "start_pos": 165, "end_pos": 176, "type": "DATASET", "confidence": 0.956415057182312}]}, {"text": "The results show that coverage is similar on these unseen testsets.", "labels": [], "entities": [{"text": "coverage", "start_pos": 22, "end_pos": 30, "type": "METRIC", "confidence": 0.9980379939079285}]}, {"text": "Obviously, coverage only indicates how often the parser found a full parse, but it does not indicate whether that parse actually was the correct parse.", "labels": [], "entities": [{"text": "coverage", "start_pos": 11, "end_pos": 19, "type": "METRIC", "confidence": 0.9637775421142578}]}, {"text": "For this reason, we also closely monitored the performance of the parser on the Alpino tree-bank 3 (van der), both in terms of parsing accuracy and in terms of average number of parses per sentence.", "labels": [], "entities": [{"text": "Alpino tree-bank 3 (van der)", "start_pos": 80, "end_pos": 108, "type": "DATASET", "confidence": 0.9261923602649144}, {"text": "accuracy", "start_pos": 135, "end_pos": 143, "type": "METRIC", "confidence": 0.9545429944992065}]}, {"text": "The average number of parses increased, which is to be expected if the grammar and lexicon are extended.", "labels": [], "entities": []}, {"text": "Accuracy has been steadily increasing on the Alpino tree-bank.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 0, "end_pos": 8, "type": "METRIC", "confidence": 0.9945098161697388}, {"text": "Alpino tree-bank", "start_pos": 45, "end_pos": 61, "type": "DATASET", "confidence": 0.9872941672801971}]}, {"text": "Accuracy is defined as the proportion of correct named dependency relations of the first parse returned by Alpino.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 0, "end_pos": 8, "type": "METRIC", "confidence": 0.9934091567993164}]}, {"text": "Alpino employs a maximum entropy disambiguation component; the first parse is the most promising parse according to this statistical model.", "labels": [], "entities": []}, {"text": "The maximum entropy disambiguation component of Alpino assigns a score S(x) to each parse x: where f i (x) is the frequency of a particular feature i in parse x and \u03b8 i is the corresponding weight of that feature.", "labels": [], "entities": []}, {"text": "The probability of a parse x for sentence w is then defined as follows, where Y (w) are all the parses of w: The disambiguation component is described in detail in Malouf and van: Development of Accuracy of the Alpino parser on the Alpino Tree-bank displays the accuracy from.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 195, "end_pos": 203, "type": "METRIC", "confidence": 0.9949191212654114}, {"text": "Alpino Tree-bank", "start_pos": 232, "end_pos": 248, "type": "DATASET", "confidence": 0.9590923190116882}, {"text": "accuracy", "start_pos": 262, "end_pos": 270, "type": "METRIC", "confidence": 0.999535322189331}]}, {"text": "During this period many of the problems described earlier were solved, but other parts of the system were improved too (in particular, the disambiguation component was improved considerably).", "labels": [], "entities": []}, {"text": "The point of the graph is that apparently the increase in coverage has not been obtained at the cost of decreasing accuracy.", "labels": [], "entities": [{"text": "coverage", "start_pos": 58, "end_pos": 66, "type": "METRIC", "confidence": 0.9945026636123657}, {"text": "accuracy", "start_pos": 115, "end_pos": 123, "type": "METRIC", "confidence": 0.9980112314224243}]}], "tableCaptions": [{"text": " Table 1: Overview of corpus material; first experi- ment (Autumn 2002).", "labels": [], "entities": []}, {"text": " Table 3: Multiple n-grams indicating same error", "labels": [], "entities": []}]}