{"title": [{"text": "Discriminative Language Modeling with Conditional Random Fields and the Perceptron Algorithm", "labels": [], "entities": [{"text": "Discriminative Language Modeling", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.702931821346283}]}], "abstractContent": [{"text": "This paper describes discriminative language modeling fora large vocabulary speech recognition task.", "labels": [], "entities": [{"text": "large vocabulary speech recognition task", "start_pos": 59, "end_pos": 99, "type": "TASK", "confidence": 0.689843374490738}]}, {"text": "We contrast two parameter estimation methods: the perceptron algorithm, and a method based on conditional random fields (CRFs).", "labels": [], "entities": []}, {"text": "The models are encoded as determin-istic weighted finite state automata, and are applied by intersecting the automata with word-lattices that are the output from a baseline recognizer.", "labels": [], "entities": []}, {"text": "The perceptron algorithm has the benefit of automatically selecting a relatively small feature set in just a couple of passes over the training data.", "labels": [], "entities": []}, {"text": "However, using the feature set output from the perceptron algorithm (initialized with their weights), CRF training provides an additional 0.5% reduction in word error rate, fora total 1.8% absolute reduction from the baseline of 39.2%.", "labels": [], "entities": [{"text": "word error rate", "start_pos": 156, "end_pos": 171, "type": "METRIC", "confidence": 0.6569507320721945}]}], "introductionContent": [{"text": "A crucial component of any speech recognizer is the language model (LM), which assigns scores or probabilities to candidate output strings in a speech recognizer.", "labels": [], "entities": []}, {"text": "The language model is used in combination with an acoustic model, to give an overall score to candidate word sequences that ranks them in order of probability or plausibility.", "labels": [], "entities": []}, {"text": "A dominant approach in speech recognition has been to use a \"source-channel\", or \"noisy-channel\" model.", "labels": [], "entities": [{"text": "speech recognition", "start_pos": 23, "end_pos": 41, "type": "TASK", "confidence": 0.8081080913543701}]}, {"text": "In this approach, language modeling is effectively framed as density estimation: the language model's task is to define a distribution over the source -i.e., the possible strings in the language.", "labels": [], "entities": [{"text": "language modeling", "start_pos": 18, "end_pos": 35, "type": "TASK", "confidence": 0.7157037109136581}]}, {"text": "Markov (n-gram) models are often used for this task, whose parameters are optimized to maximize the likelihood of a large amount of training text.", "labels": [], "entities": []}, {"text": "Recognition performance is a direct measure of the effectiveness of a language model; an indirect measure which is frequently proposed within these approaches is the perplexity of the LM (i.e., the log probability it assigns to some held-out data set).", "labels": [], "entities": []}, {"text": "This paper explores alternative methods for language modeling, which complement the source-channel approach through discriminatively trained models.", "labels": [], "entities": [{"text": "language modeling", "start_pos": 44, "end_pos": 61, "type": "TASK", "confidence": 0.726653665304184}]}, {"text": "The language models we describe do not attempt to estimate a generative model P (w) over strings.", "labels": [], "entities": []}, {"text": "Instead, they are trained on acoustic sequences with their transcriptions, in an attempt to directly optimize error-rate.", "labels": [], "entities": []}, {"text": "Our work builds on previous work on language modeling using the perceptron algorithm, described in.", "labels": [], "entities": [{"text": "language modeling", "start_pos": 36, "end_pos": 53, "type": "TASK", "confidence": 0.7234374135732651}]}, {"text": "In particular, we explore conditional random field methods, as an alternative training method to the perceptron.", "labels": [], "entities": []}, {"text": "We describe how these models can be trained over lattices that are the output from a baseline recognizer.", "labels": [], "entities": []}, {"text": "We also give a number of experiments comparing the two approaches.", "labels": [], "entities": []}, {"text": "The perceptron method gave a 1.3% absolute improvement in recognition error on the Switchboard domain; the CRF methods we describe give a further gain, the final absolute improvement being 1.8%.", "labels": [], "entities": []}, {"text": "A central issue we focus on concerns feature selection.", "labels": [], "entities": [{"text": "feature selection", "start_pos": 37, "end_pos": 54, "type": "TASK", "confidence": 0.8164747655391693}]}, {"text": "The number of distinct n-grams in our training data is close to 45 million, and we show that CRF training converges very slowly even when trained with a subset (of size 12 million) of these features.", "labels": [], "entities": []}, {"text": "Because of this, we explore methods for picking a small subset of the available features.", "labels": [], "entities": []}, {"text": "The perceptron algorithm can be used as one method for feature selection, selecting around 1.5 million features in total.", "labels": [], "entities": [{"text": "feature selection", "start_pos": 55, "end_pos": 72, "type": "TASK", "confidence": 0.6706449687480927}]}, {"text": "The CRF trained with this feature set, and initialized with parameters from perceptron training, converges much more quickly than other approaches, and also gives the optimal performance on the held-out set.", "labels": [], "entities": []}, {"text": "We explore other approaches to feature selection, but find that the perceptron-based approach gives the best results in our experiments.", "labels": [], "entities": [{"text": "feature selection", "start_pos": 31, "end_pos": 48, "type": "TASK", "confidence": 0.7612205147743225}]}, {"text": "While we focus on n-gram models, we stress that our methods are applicable to more general language modeling features -for example, syntactic features, as explored in, e.g.,.", "labels": [], "entities": []}, {"text": "We intend to explore methods with new features in the future.", "labels": [], "entities": []}, {"text": "Experimental results with n-gram models on 1000-best lists show a very small drop inaccuracy compared to the use of lattices.", "labels": [], "entities": []}, {"text": "This is encouraging, in that it suggests that models with more flexible features than n-gram models, which therefore cannot be efficiently used with lattices, may not be unduly harmed by their restriction to n-best lists.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Word-error rate results at convergence iteration for", "labels": [], "entities": []}, {"text": " Table 2: Time (in hours) for one iteration on a single Intel", "labels": [], "entities": [{"text": "Time", "start_pos": 10, "end_pos": 14, "type": "METRIC", "confidence": 0.9904677271842957}]}]}