{"title": [{"text": "A Sentimental Education: Sentiment Analysis Using Subjectivity Summarization Based on Minimum Cuts", "labels": [], "entities": [{"text": "Sentiment Analysis", "start_pos": 25, "end_pos": 43, "type": "TASK", "confidence": 0.922802597284317}, {"text": "Subjectivity Summarization", "start_pos": 50, "end_pos": 76, "type": "TASK", "confidence": 0.6972446143627167}]}], "abstractContent": [{"text": "Sentiment analysis seeks to identify the view-point(s) underlying a text span; an example application is classifying a movie review as \"thumbs up\" or \"thumbs down\".", "labels": [], "entities": [{"text": "Sentiment analysis", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.9366827011108398}]}, {"text": "To determine this sentiment polarity , we propose a novel machine-learning method that applies text-categorization techniques to just the subjective portions of the document.", "labels": [], "entities": []}, {"text": "Extracting these portions can be implemented using efficient techniques for finding minimum cuts in graphs; this greatly facilitates incorporation of cross-sentence contextual constraints.", "labels": [], "entities": []}], "introductionContent": [{"text": "The computational treatment of opinion, sentiment, and subjectivity has recently attracted a great deal of attention (see references), in part because of its potential applications.", "labels": [], "entities": [{"text": "computational treatment of opinion, sentiment, and subjectivity", "start_pos": 4, "end_pos": 67, "type": "TASK", "confidence": 0.8391172223620944}]}, {"text": "For instance, informationextraction and question-answering systems could flag statements and queries regarding opinions rather than facts.", "labels": [], "entities": []}, {"text": "Also, it has proven useful for companies, recommender systems, and editorial sites to create summaries of people's experiences and opinions that consist of subjective expressions extracted from reviews (as is commonly done in movie ads) or even just a review's polarity -positive (\"thumbs up\") or negative (\"thumbs down\").", "labels": [], "entities": []}, {"text": "Document polarity classification poses a significant challenge to data-driven methods, resisting traditional text-categorization techniques).", "labels": [], "entities": [{"text": "Document polarity classification", "start_pos": 0, "end_pos": 32, "type": "TASK", "confidence": 0.8916317820549011}]}, {"text": "Previous approaches focused on selecting indicative lexical features (e.g., the word \"good\"), classifying a document according to the number of such features that occur anywhere within it.", "labels": [], "entities": []}, {"text": "In contrast, we propose the following process: (1) label the sentences in the document as either subjective or objective, discarding the latter; and then (2) apply a standard machine-learning classifier to the resulting extract.", "labels": [], "entities": []}, {"text": "This can prevent the polarity classifier from considering irrelevant or even potentially misleading text: for example, although the sentence \"The protagonist tries to protect her good name\" contains the word \"good\", it tells us nothing about the author's opinion and in fact could well be embedded in a negative movie review.", "labels": [], "entities": []}, {"text": "Also, as mentioned above, subjectivity extracts can be provided to users as a summary of the sentiment-oriented content of the document.", "labels": [], "entities": []}, {"text": "Our results show that the subjectivity extracts we create accurately represent the sentiment information of the originating documents in a much more compact form: depending on choice of downstream polarity classifier, we can achieve highly statistically significant improvement (from 82.8% to 86.4%) or maintain the same level of performance for the polarity classification task while retaining only 60% of the reviews' words.", "labels": [], "entities": [{"text": "polarity classification task", "start_pos": 350, "end_pos": 378, "type": "TASK", "confidence": 0.7988555232683817}]}, {"text": "Also, we explore extraction methods based on a minimum cut formulation, which provides an efficient, intuitive, and effective means for integrating inter-sentencelevel contextual information with traditional bag-ofwords features.", "labels": [], "entities": []}], "datasetContent": [{"text": "Our experiments involve classifying movie reviews as either positive or negative, an appealing task for several reasons.", "labels": [], "entities": []}, {"text": "First, as mentioned in the introduction, providing polarity information about reviews is a useful service: witness the popularity of www.rottentomatoes.com.", "labels": [], "entities": []}, {"text": "Second, movie reviews are apparently harder to classify than reviews of other products.", "labels": [], "entities": []}, {"text": "Third, the correct label can be extracted automatically from rating information (e.g., number of stars).", "labels": [], "entities": []}, {"text": "Our data 4 contains 1000 positive and 1000 negative reviews all written before 2002, with a cap of 20 reviews per author (312 authors total) per category.", "labels": [], "entities": []}, {"text": "We refer to this corpus as the polarity dataset.", "labels": [], "entities": []}, {"text": "Default polarity classifiers We tested support vector machines (SVMs) and Naive Bayes (NB).", "labels": [], "entities": []}, {"text": "Following, we use unigram-presence features: the ith coordinate of a feature vector is 1 if the corresponding unigram occurs in the input text, 0 otherwise.", "labels": [], "entities": []}, {"text": "(For SVMs, the feature vectors are length-normalized).", "labels": [], "entities": []}, {"text": "Each default documentlevel polarity classifier is trained and tested on the extracts formed by applying one of the sentencelevel subjectivity detectors to reviews in the polarity dataset.", "labels": [], "entities": []}, {"text": "Subjectivity dataset To train our detectors, we need a collection of labeled sentences.", "labels": [], "entities": []}, {"text": "state that \"It is [very hard] to obtain collections of individual sentences that can be easily identified as subjective or objective\"; the polarity-dataset sentences, for example, have not been so annotated.", "labels": [], "entities": []}, {"text": "Fortunately, we were able to mine the Web to create a large, automaticallylabeled sentence corpus . To gather subjective sentences (or phrases), we collected 5000 moviereview snippets (e.g., \"bold, imaginative, and impossible to resist\") from www.rottentomatoes.com.", "labels": [], "entities": []}, {"text": "To obtain (mostly) objective data, we took 5000 sentences from plot summaries available from the Internet Movie Database (www.imdb.com).", "labels": [], "entities": [{"text": "Internet Movie Database", "start_pos": 97, "end_pos": 120, "type": "DATASET", "confidence": 0.863569994767507}]}, {"text": "We only selected sentences or snippets at least ten words long and drawn from reviews or plot summaries of movies released post-2001, which prevents overlap with the polarity dataset.", "labels": [], "entities": []}, {"text": "Subjectivity detectors As noted above, we can use our default polarity classifiers as \"basic\" sentencelevel subjectivity detectors (after retraining on the subjectivity dataset) to produce extracts of the original reviews.", "labels": [], "entities": []}, {"text": "We also create a family of cut-based subjectivity detectors; these take as input the set of sentences appearing in a single document and determine the subjectivity status of all the sentences simultaneously using per-item and pairwise relationship information.", "labels": [], "entities": []}, {"text": "Specifically, fora given document, we use the construction in Section 2.2 to build a graph wherein the source sand sink t correspond to the class of subjective and objective sentences, respectively, and each internal node vi corresponds to the document's i th sentence s i . We can set the individual scores ind 1 (s i ) to Pr NB sub (s i ) and ind 2 (s i ) to 1 \u2212 Pr NB sub (s i ), as shown in, where Pr NB sub (s) denotes Naive Bayes' estimate of the probability that sentence sis subjective; or, we can use the weights produced by the SVM classifier instead.", "labels": [], "entities": []}, {"text": "If we set all the association scores to zero, then the minimum-cut classification of the sentences is the same as that of the basic subjectivity detector.", "labels": [], "entities": []}, {"text": "Alternatively, we incorporate the degree of proximity between pairs of sentences, controlled by three parameters.", "labels": [], "entities": []}, {"text": "The threshold T specifies the maximum distance two sentences can be separated by and still be considered proximal.", "labels": [], "entities": []}, {"text": "The We therefore could not directly evaluate sentenceclassification accuracy on the polarity dataset.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 68, "end_pos": 76, "type": "METRIC", "confidence": 0.9796791076660156}]}, {"text": "Available at www.cs.cornell.edu/people/pabo/moviereview-data/ , sentence corpus version 1.0.", "labels": [], "entities": []}, {"text": "We converted SVM output di, which is a signed distance (negative=objective) from the separating hyperplane, to nonnegative numbers by and ind2(si) = 1 \u2212 ind1(si).", "labels": [], "entities": []}, {"text": "Note that scaling is employed only for consistency; the algorithm itself does not require probabilities for individual scores.", "labels": [], "entities": []}, {"text": "non-increasing function f (d) specifies how the influence of proximal sentences decays with respect to distance d; in our experiments, we tried f (d) = 1, e 1\u2212d , and 1/d 2 . The constant c controls the relative influence of the association scores: a larger c makes the minimum-cut algorithm more loath to put proximal sentences in different classes.", "labels": [], "entities": []}, {"text": "With these in hand , we set (for j > i)  Below, we report average accuracies computed by ten-fold cross-validation over the polarity dataset.", "labels": [], "entities": [{"text": "accuracies", "start_pos": 66, "end_pos": 76, "type": "METRIC", "confidence": 0.8462006449699402}]}, {"text": "Section 4.1 examines our basic subjectivity extraction algorithms, which are based on individualsentence predictions alone.", "labels": [], "entities": [{"text": "subjectivity extraction", "start_pos": 31, "end_pos": 54, "type": "TASK", "confidence": 0.7631727159023285}]}, {"text": "Section 4.2 evaluates the more sophisticated form of subjectivity extraction that incorporates context information via the minimum-cut paradigm.", "labels": [], "entities": [{"text": "subjectivity extraction", "start_pos": 53, "end_pos": 76, "type": "TASK", "confidence": 0.7398936599493027}]}, {"text": "As we will see, the use of subjectivity extracts can in the best case provide satisfying improvement in polarity classification, and otherwise can at least yield polarity-classification accuracies indistinguishable from employing the full review.", "labels": [], "entities": [{"text": "polarity classification", "start_pos": 104, "end_pos": 127, "type": "TASK", "confidence": 0.7406421005725861}]}, {"text": "At the same time, the extracts we create are both smaller on average than the original document and more effective as input to a default polarity classifier than the same-length counterparts produced by standard summarization tactics (e.g., first-or last-N sentences).", "labels": [], "entities": []}, {"text": "We therefore conclude that subjectivity extraction produces effective summaries of document sentiment.", "labels": [], "entities": [{"text": "subjectivity extraction", "start_pos": 27, "end_pos": 50, "type": "TASK", "confidence": 0.787329375743866}]}], "tableCaptions": []}