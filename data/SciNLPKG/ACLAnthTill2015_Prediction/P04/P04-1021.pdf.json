{"title": [{"text": "A Joint Source-Channel Model for Machine Transliteration", "labels": [], "entities": []}], "abstractContent": [{"text": "Most foreign names are transliterated into Chinese, Japanese or Korean with approximate phonetic equivalents.", "labels": [], "entities": []}, {"text": "The transliteration is usually achieved through intermediate phonemic mapping.", "labels": [], "entities": []}, {"text": "This paper presents anew framework that allows direct orthographical mapping (DOM) between two different languages, through a joint source-channel model, also called n-gram transliteration model (TM).", "labels": [], "entities": []}, {"text": "With the n-gram TM model, we automate the orthographic alignment process to derive the aligned transliteration units from a bilingual dictionary.", "labels": [], "entities": []}, {"text": "The n-gram TM under the DOM framework greatly reduces system development effort and provides a quantum leap in improvement in transliteration accuracy over that of other state-of-the-art machine learning algorithms.", "labels": [], "entities": [{"text": "DOM framework", "start_pos": 24, "end_pos": 37, "type": "DATASET", "confidence": 0.8591701984405518}, {"text": "accuracy", "start_pos": 142, "end_pos": 150, "type": "METRIC", "confidence": 0.9680227041244507}]}, {"text": "The modeling framework is validated through several experiments for English-Chinese language pair.", "labels": [], "entities": []}], "introductionContent": [{"text": "In applications such as cross-lingual information retrieval (CLIR) and machine translation, there is an increasing need to translate out-of-vocabulary words from one language to another, especially from alphabet language to Chinese, Japanese or Korean.", "labels": [], "entities": [{"text": "cross-lingual information retrieval (CLIR)", "start_pos": 24, "end_pos": 66, "type": "TASK", "confidence": 0.7944913903872172}, {"text": "machine translation", "start_pos": 71, "end_pos": 90, "type": "TASK", "confidence": 0.7991918921470642}]}, {"text": "Proper names of English, French, German, Russian, Spanish and Arabic origins constitute a good portion of out-of-vocabulary words.", "labels": [], "entities": []}, {"text": "They are translated through transliteration, the method of translating into another language by preserving how words sound in their original languages.", "labels": [], "entities": []}, {"text": "For writing foreign names in Chinese, transliteration always follows the original romanization.", "labels": [], "entities": []}, {"text": "Therefore, any foreign name will have only one Pinyin (romanization of Chinese) and thus in Chinese characters.", "labels": [], "entities": []}, {"text": "In this paper, we focus on automatic Chinese transliteration of foreign alphabet names.", "labels": [], "entities": [{"text": "automatic Chinese transliteration of foreign alphabet names", "start_pos": 27, "end_pos": 86, "type": "TASK", "confidence": 0.7675199466092246}]}, {"text": "Because some alphabet writing systems use various diacritical marks, we find it more practical to write names containing such diacriticals as they are rendered in English.", "labels": [], "entities": []}, {"text": "Therefore, we refer all foreign-Chinese transliteration to English-Chinese transliteration, or E2C.", "labels": [], "entities": []}, {"text": "Transliterating English names into Chinese is not straightforward.", "labels": [], "entities": [{"text": "Transliterating English names", "start_pos": 0, "end_pos": 29, "type": "TASK", "confidence": 0.9012677073478699}]}, {"text": "However, recalling the original from Chinese transliteration is even more challenging as the E2C transliteration may have lost some original phonemic evidences.", "labels": [], "entities": []}, {"text": "The Chinese-English backward transliteration process is also called back-transliteration, or C2E.", "labels": [], "entities": []}, {"text": "In machine transliteration, the noisy channel model (NCM), based on a phoneme-based approach, has recently received considerable attention (.", "labels": [], "entities": [{"text": "machine transliteration", "start_pos": 3, "end_pos": 26, "type": "TASK", "confidence": 0.7065126001834869}]}, {"text": "In this paper we discuss the limitations of such an approach and address its problems by firstly proposing a paradigm that allows direct orthographic mapping (DOM), secondly further proposing a joint source-channel model as a realization of DOM.", "labels": [], "entities": [{"text": "direct orthographic mapping (DOM)", "start_pos": 130, "end_pos": 163, "type": "TASK", "confidence": 0.7325248618920644}]}, {"text": "Two other machine learning techniques, NCM and ID3 decision tree, also are implemented under DOM as reference to compare with the proposed n-gram TM.", "labels": [], "entities": [{"text": "DOM", "start_pos": 93, "end_pos": 96, "type": "DATASET", "confidence": 0.8102739453315735}]}, {"text": "This paper is organized as follows: In section 2, we present the transliteration problems.", "labels": [], "entities": []}, {"text": "In section 3, a joint source-channel model is formulated.", "labels": [], "entities": []}, {"text": "In section 4, several experiments are carried out to study different aspects of proposed algorithm.", "labels": [], "entities": []}, {"text": "In section 5, we relate our algorithms to other reported work.", "labels": [], "entities": []}, {"text": "Finally, we conclude the study with some discussions.", "labels": [], "entities": []}], "datasetContent": [{"text": "We use a database from the bilingual dictionary \"Chinese Transliteration of Foreign Personal Names\" which was edited by Xinhua News Agency and was considered the de facto standard of personal name transliteration in today's Chinese press.", "labels": [], "entities": [{"text": "Chinese Transliteration of Foreign Personal Names", "start_pos": 49, "end_pos": 98, "type": "TASK", "confidence": 0.8114244242509207}, {"text": "Xinhua News Agency", "start_pos": 120, "end_pos": 138, "type": "DATASET", "confidence": 0.8827567299207052}, {"text": "personal name transliteration", "start_pos": 183, "end_pos": 212, "type": "TASK", "confidence": 0.7385256091753641}]}, {"text": "The database includes a collection of 37,694 unique English entries and their official Chinese transliteration.", "labels": [], "entities": []}, {"text": "The listing includes personal names of English, French, Spanish, German, Arabic, Russian and many other origins.", "labels": [], "entities": []}, {"text": "The database is initially randomly distributed into 13 subsets.", "labels": [], "entities": []}, {"text": "In the open test, one subset is withheld for testing while the remaining 12 subsets are used as the training materials.", "labels": [], "entities": []}, {"text": "This process is repeated 13 times to yield an average result, which is called the 13-fold open test.", "labels": [], "entities": []}, {"text": "After experiments, we found that each of the 13-fold open tests gave consistent error rates with less than 1% deviation.", "labels": [], "entities": [{"text": "error", "start_pos": 80, "end_pos": 85, "type": "METRIC", "confidence": 0.8830170631408691}]}, {"text": "Therefore, for simplicity, we randomly select one of the 13 subsets, which consists of 2896 entries, as the standard open test set to report results.", "labels": [], "entities": []}, {"text": "In the close test, all data entries are used for training and testing.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 4.  open  (word)", "labels": [], "entities": []}, {"text": " Table 3. E2C error rates for n-gram TM tests.", "labels": [], "entities": [{"text": "E2C error rates", "start_pos": 10, "end_pos": 25, "type": "METRIC", "confidence": 0.6486248771349589}]}, {"text": " Table 4. E2C error rates for n-gram NCM tests", "labels": [], "entities": [{"text": "E2C error rates", "start_pos": 10, "end_pos": 25, "type": "METRIC", "confidence": 0.6459343632062277}]}, {"text": " Table 5. Based on the same alignment  tokenization, we estimate the monolingual  language perplexity for Chinese and English  independently using the n-gram language models", "labels": [], "entities": []}, {"text": " Table 6. C2E error rate for n-gram TM tests", "labels": [], "entities": [{"text": "error rate", "start_pos": 14, "end_pos": 24, "type": "METRIC", "confidence": 0.9287761449813843}]}, {"text": " Table 7. N-best word error rates for 3-gram TM  tests", "labels": [], "entities": [{"text": "N-best word error rates", "start_pos": 10, "end_pos": 33, "type": "METRIC", "confidence": 0.8749604970216751}]}, {"text": " Table 8. E2C transliteration using ID3 decision  tree for transliterating Nice to  \u5c3c\u65af (\u5c3c|NI \u65af|CE)", "labels": [], "entities": []}, {"text": " Table 9. Word error rate ID3 vs. 3-gram TM", "labels": [], "entities": [{"text": "Word error rate ID3", "start_pos": 10, "end_pos": 29, "type": "METRIC", "confidence": 0.7537639737129211}]}, {"text": " Table 10. Performance reference in recent  studies", "labels": [], "entities": [{"text": "Performance reference", "start_pos": 11, "end_pos": 32, "type": "METRIC", "confidence": 0.900147944688797}]}]}