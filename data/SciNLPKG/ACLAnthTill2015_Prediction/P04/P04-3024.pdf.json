{"title": [{"text": "A New Feature Selection Score for Multinomial Naive Bayes Text Classification Based on KL-Divergence", "labels": [], "entities": [{"text": "Multinomial Naive Bayes Text Classification", "start_pos": 34, "end_pos": 77, "type": "TASK", "confidence": 0.7175285995006562}, {"text": "KL-Divergence", "start_pos": 87, "end_pos": 100, "type": "DATASET", "confidence": 0.51751708984375}]}], "abstractContent": [{"text": "We define anew feature selection score for text classification based on the KL-divergence between the distribution of words in training documents and their classes.", "labels": [], "entities": [{"text": "text classification", "start_pos": 43, "end_pos": 62, "type": "TASK", "confidence": 0.7823967635631561}, {"text": "KL-divergence", "start_pos": 76, "end_pos": 89, "type": "METRIC", "confidence": 0.9010978937149048}]}, {"text": "The score favors words that have a similar distribution in documents of the same class but different distributions in documents of different classes.", "labels": [], "entities": []}, {"text": "Experiments on two standard data sets indicate that the new method outperforms mutual information , especially for smaller categories.", "labels": [], "entities": []}], "introductionContent": [{"text": "Text classification is the assignment of predefined categories to text documents.", "labels": [], "entities": [{"text": "Text classification", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.7907326817512512}]}, {"text": "Text classification has many applications in natural language processing tasks such as E-mail filtering, prediction of user preferences and organization of web content.", "labels": [], "entities": [{"text": "Text classification", "start_pos": 0, "end_pos": 19, "type": "TASK", "confidence": 0.8009374737739563}, {"text": "E-mail filtering", "start_pos": 87, "end_pos": 103, "type": "TASK", "confidence": 0.7950554490089417}, {"text": "prediction of user preferences", "start_pos": 105, "end_pos": 135, "type": "TASK", "confidence": 0.8546207547187805}, {"text": "organization of web content", "start_pos": 140, "end_pos": 167, "type": "TASK", "confidence": 0.8586117178201675}]}, {"text": "The Naive Bayes classifier is a popular machine learning technique for text classification because it performs well in many domains, despite its simplicity.", "labels": [], "entities": [{"text": "Naive Bayes classifier", "start_pos": 4, "end_pos": 26, "type": "TASK", "confidence": 0.617954154809316}, {"text": "text classification", "start_pos": 71, "end_pos": 90, "type": "TASK", "confidence": 0.8077479898929596}]}, {"text": "Naive Bayes assumes a stochastic model of document generation.", "labels": [], "entities": [{"text": "document generation", "start_pos": 42, "end_pos": 61, "type": "TASK", "confidence": 0.714922770857811}]}, {"text": "Using Bayes' rule, the model is inverted in order to predict the most likely class fora new document.", "labels": [], "entities": []}, {"text": "We assume that documents are generated according to a multinomial event model.", "labels": [], "entities": []}, {"text": "Thus a document is represented as a vector d i = (x i1 . .", "labels": [], "entities": []}, {"text": "x i|V | ) of word counts where V is the vocabulary and each x it \u2208 {0, 1, 2, . .", "labels": [], "entities": []}, {"text": "} indicates how often wt occurs ind i . Given model parameters p(w t |c j ) and class prior probabilities p(c j ) and assuming independence of the words, the most likely class fora document d i is computed as where n(w t , d i ) is the number of occurrences of wt ind i . p(w t |c j ) and p(c j ) are estimated from training documents with known classes, using maximum likelihood estimation with a Laplacean prior: It is common practice to use only a subset of the words in the training documents for classification to avoid overfitting and make classification more efficient.", "labels": [], "entities": []}, {"text": "This is usually done by assigning each word a score f (w t ) that measures its usefulness for classification and selecting the N highest scored words.", "labels": [], "entities": []}, {"text": "One of the best performing scoring functions for feature selection in text classification is mutual information.", "labels": [], "entities": [{"text": "feature selection", "start_pos": 49, "end_pos": 66, "type": "TASK", "confidence": 0.7736880779266357}, {"text": "text classification", "start_pos": 70, "end_pos": 89, "type": "TASK", "confidence": 0.7477413415908813}]}, {"text": "The mutual information between two random variables, MI(X; Y ), measures the amount of information that the value of one variable gives about the value of the other.", "labels": [], "entities": [{"text": "MI", "start_pos": 53, "end_pos": 55, "type": "METRIC", "confidence": 0.9773508310317993}]}, {"text": "Note that in the multinomial model, the word variable W takes on values from the vocabulary V . In order to use mutual information with a multinomial model, one defines new random variables W t \u2208 {0, 1} with p(W t = 1) = p(W = wt )).", "labels": [], "entities": []}, {"text": "Then the mutual information between a word wt and the class variable C is where p(x, c j ) and p(x) are short for p(W t = x, c j ) and p(W t = x).", "labels": [], "entities": []}, {"text": "p(x, c j ), p(x) and p(c j ) are estimated from the training documents by counting how often wt occurs in each class.", "labels": [], "entities": []}], "datasetContent": [{"text": "We compare KL and dKL to mutual information, using two standard data sets: 20 Newsgroups 2 and Reuters 21578.", "labels": [], "entities": [{"text": "20 Newsgroups 2", "start_pos": 75, "end_pos": 90, "type": "DATASET", "confidence": 0.8495072921117147}, {"text": "Reuters 21578", "start_pos": 95, "end_pos": 108, "type": "DATASET", "confidence": 0.9016374945640564}]}, {"text": "In tokenizing the data, only words consisting of alphabetic characters are used after conversion to lowercase.", "labels": [], "entities": [{"text": "tokenizing", "start_pos": 3, "end_pos": 13, "type": "TASK", "confidence": 0.9637051224708557}]}, {"text": "In addition, all numbers are mapped to a special token NUM.", "labels": [], "entities": []}, {"text": "For 20 Newsgroups we remove the newsgroup headers and use a stoplist consisting of the 100 most frequent words of the British National Corpus.", "labels": [], "entities": [{"text": "British National Corpus", "start_pos": 118, "end_pos": 141, "type": "DATASET", "confidence": 0.9513857364654541}]}, {"text": "We use the ModApte split of Reuters 21578 and use only the 10 largest classes.", "labels": [], "entities": [{"text": "ModApte split of Reuters 21578", "start_pos": 11, "end_pos": 41, "type": "DATASET", "confidence": 0.9135695815086364}]}, {"text": "The vocabulary size is 111868 words for 20 Newsgroups and 22430 words for Reuters.", "labels": [], "entities": [{"text": "Reuters", "start_pos": 74, "end_pos": 81, "type": "DATASET", "confidence": 0.9457288980484009}]}, {"text": "Experiments with 20 Newsgroups are performed with 5-fold cross-validation, using 80% of the data for training and 20% for testing.", "labels": [], "entities": []}, {"text": "We build a single classifier for the 20 classes and vary the number of selected words from 20 to 20000.", "labels": [], "entities": []}, {"text": "compares classification accuracy for the three scoring functions.", "labels": [], "entities": [{"text": "classification", "start_pos": 9, "end_pos": 23, "type": "TASK", "confidence": 0.9138196706771851}, {"text": "accuracy", "start_pos": 24, "end_pos": 32, "type": "METRIC", "confidence": 0.9558904767036438}]}, {"text": "dKL slightly outperforms mutual information, especially for smaller vocabulary sizes.", "labels": [], "entities": []}, {"text": "The difference is statistically significant for 20 to 200 words at the 99% confidence level, and for 20 to 2000 words at the 95% confidence level, using a one-tailed paired t-test.", "labels": [], "entities": []}, {"text": "For the Reuters dataset we build a binary classifier for each of the ten topics and set the number of positively classified documents such that precision equals recall.", "labels": [], "entities": [{"text": "Reuters dataset", "start_pos": 8, "end_pos": 23, "type": "DATASET", "confidence": 0.9791945219039917}, {"text": "precision", "start_pos": 144, "end_pos": 153, "type": "METRIC", "confidence": 0.9989995360374451}, {"text": "recall", "start_pos": 161, "end_pos": 167, "type": "METRIC", "confidence": 0.9977847933769226}]}, {"text": "Precision is the percentage of positive documents among all positively classified documents.", "labels": [], "entities": [{"text": "Precision", "start_pos": 0, "end_pos": 9, "type": "METRIC", "confidence": 0.9916661381721497}]}, {"text": "Recall is the percentage of positive documents that are classified as positive.", "labels": [], "entities": [{"text": "Recall", "start_pos": 0, "end_pos": 6, "type": "METRIC", "confidence": 0.9824604392051697}]}, {"text": "In we report microaveraged and macroaveraged recall for each number of selected words.", "labels": [], "entities": [{"text": "recall", "start_pos": 45, "end_pos": 51, "type": "METRIC", "confidence": 0.9672260284423828}]}, {"text": "Microaveraged recall is the percentage of all positive documents (in all topics) that are classified as positive.", "labels": [], "entities": [{"text": "recall", "start_pos": 14, "end_pos": 20, "type": "METRIC", "confidence": 0.8306077718734741}]}, {"text": "Macroaveraged recall is the average of the recall values of the individual topics.", "labels": [], "entities": [{"text": "recall", "start_pos": 14, "end_pos": 20, "type": "METRIC", "confidence": 0.8306542038917542}, {"text": "recall", "start_pos": 43, "end_pos": 49, "type": "METRIC", "confidence": 0.9960263967514038}]}, {"text": "Microaveraged recall gives equal weight to the documents and thus emphasizes larger topics, while macroaveraged recall gives equal weight to the topics and thus emphasizes smaller topics more than microav- Both KL and dKL achieve slightly higher values for microaveraged recall than mutual information, for most vocabulary sizes.", "labels": [], "entities": []}, {"text": "KL performs best at 20000 words with 90.1% microaveraged recall, compared to 89.3% for mutual information.", "labels": [], "entities": [{"text": "KL", "start_pos": 0, "end_pos": 2, "type": "DATASET", "confidence": 0.662437915802002}, {"text": "recall", "start_pos": 57, "end_pos": 63, "type": "METRIC", "confidence": 0.9167054891586304}]}, {"text": "The largest improvement is found for dKL at 100 words with 88.0%, compared to 86.5% for mutual information.", "labels": [], "entities": []}, {"text": "For smaller categories, the difference between the KL-divergence based scores and mutual information is larger, as indicated by the curves for macroaveraged recall.", "labels": [], "entities": [{"text": "recall", "start_pos": 157, "end_pos": 163, "type": "METRIC", "confidence": 0.8874878287315369}]}, {"text": "KL yields the highest recall at 20000 words with 82.2%, an increase of 3.9% compared to mutual information with 78.3%, whereas dKL has its largest value at 100 words with 78.8%, compared to 76.1% for mutual information.", "labels": [], "entities": [{"text": "recall", "start_pos": 22, "end_pos": 28, "type": "METRIC", "confidence": 0.9994896650314331}]}, {"text": "We find the largest improvement at 5000 words with 5.6% for KL and 2.9% for dKL, compared to mutual information.", "labels": [], "entities": []}], "tableCaptions": []}