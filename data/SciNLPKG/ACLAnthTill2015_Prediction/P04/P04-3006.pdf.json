{"title": [{"text": "An Automatic Filter for Non-Parallel Texts", "labels": [], "entities": []}], "abstractContent": [{"text": "Numerous cross-lingual applications, including state-of-the-art machine translation systems, require parallel texts aligned at the sentence level.", "labels": [], "entities": [{"text": "machine translation", "start_pos": 64, "end_pos": 83, "type": "TASK", "confidence": 0.7684275507926941}]}, {"text": "However, collections of such texts are often polluted by pairs of texts that are comparable but not parallel.", "labels": [], "entities": []}, {"text": "Bitext maps can help to discriminate between parallel and comparable texts.", "labels": [], "entities": []}, {"text": "Bitext mapping algorithms use a larger set of document features than competing approaches to this task, resulting in higher accuracy.", "labels": [], "entities": [{"text": "Bitext mapping", "start_pos": 0, "end_pos": 14, "type": "TASK", "confidence": 0.7073738276958466}, {"text": "accuracy", "start_pos": 124, "end_pos": 132, "type": "METRIC", "confidence": 0.9975348711013794}]}, {"text": "In addition, good bitext mapping algorithms are not limited to documents with structural markup such as web pages.", "labels": [], "entities": []}, {"text": "The task of filtering non-parallel text pairs represents anew application of bitext mapping algorithms.", "labels": [], "entities": []}], "introductionContent": [{"text": "In June 2003, the U.S. government organized a \"Surprise Language Exercise\" for the NLP community.", "labels": [], "entities": [{"text": "Surprise Language Exercise\"", "start_pos": 47, "end_pos": 74, "type": "TASK", "confidence": 0.9053489863872528}]}, {"text": "The goal was to build the best possible language technologies fora \"surprise\" language in just one month.", "labels": [], "entities": []}, {"text": "One of the main technologies pursued was machine translation (MT).", "labels": [], "entities": [{"text": "machine translation (MT)", "start_pos": 41, "end_pos": 65, "type": "TASK", "confidence": 0.873584794998169}]}, {"text": "Statistical MT (SMT) systems were the most successful in this scenario, because their construction typically requires less time than other approaches.", "labels": [], "entities": [{"text": "Statistical MT (SMT)", "start_pos": 0, "end_pos": 20, "type": "TASK", "confidence": 0.7331890761852264}]}, {"text": "On the other hand, SMT systems require large quantities of parallel text as training data.", "labels": [], "entities": [{"text": "SMT", "start_pos": 19, "end_pos": 22, "type": "TASK", "confidence": 0.9952137470245361}]}, {"text": "A significant collection of parallel text was obtained for this purpose from multiple sources.", "labels": [], "entities": []}, {"text": "SMT systems were built and tested; results were reported.", "labels": [], "entities": [{"text": "SMT", "start_pos": 0, "end_pos": 3, "type": "TASK", "confidence": 0.9690583348274231}]}, {"text": "Much later we were surprised to discover that a significant portion of the training data was not parallel text!", "labels": [], "entities": []}, {"text": "Some of the document pairs were on the same topic but not translations of each other.", "labels": [], "entities": []}, {"text": "For today's sentence-based SMT systems, this kind of data is noise.", "labels": [], "entities": [{"text": "SMT", "start_pos": 27, "end_pos": 30, "type": "TASK", "confidence": 0.8649469017982483}]}, {"text": "How much better would the results have been if the noisy training data were automatically filtered out?", "labels": [], "entities": []}, {"text": "This question is becoming more important as SMT systems increase their reliance on automatically collected parallel texts.", "labels": [], "entities": [{"text": "SMT", "start_pos": 44, "end_pos": 47, "type": "TASK", "confidence": 0.9938871264457703}]}, {"text": "There is abundant literature on aligning parallel texts at the sentence level.", "labels": [], "entities": []}, {"text": "To the best of our knowledge, all published methods happily misalign nonparallel inputs, without so much as a warning.", "labels": [], "entities": []}, {"text": "There is also some recent work on distinguishing parallel texts from pairs of unrelated texts.", "labels": [], "entities": [{"text": "distinguishing parallel texts from pairs of unrelated texts", "start_pos": 34, "end_pos": 93, "type": "TASK", "confidence": 0.7819045707583427}]}, {"text": "In this paper, we propose a solution to the more difficult problem of distinguishing parallel texts from texts that are comparable but not parallel.", "labels": [], "entities": []}, {"text": "Definitions of \"comparable texts\" vary in the literature.", "labels": [], "entities": []}, {"text": "Here we adopt a definition that is most suitable for filtering SMT training data: Two texts are \"comparable\" if they are not alignable at approximately the sentence level.", "labels": [], "entities": [{"text": "SMT training", "start_pos": 63, "end_pos": 75, "type": "TASK", "confidence": 0.8772964775562286}]}, {"text": "This definition is also suitable for other applications of parallel texts, such as machine-assisted translation and computerassisted foreign language learning.", "labels": [], "entities": [{"text": "machine-assisted translation", "start_pos": 83, "end_pos": 111, "type": "TASK", "confidence": 0.7207897901535034}, {"text": "foreign language learning", "start_pos": 133, "end_pos": 158, "type": "TASK", "confidence": 0.6041516661643982}]}, {"text": "suggested three approaches to filtering non-parallel texts: STRAND, tsim, and a combination of the two.", "labels": [], "entities": [{"text": "STRAND", "start_pos": 60, "end_pos": 66, "type": "METRIC", "confidence": 0.9898267984390259}]}, {"text": "STRAND relies on mark-up within a document to reveal the document's structure.", "labels": [], "entities": [{"text": "STRAND", "start_pos": 0, "end_pos": 6, "type": "DATASET", "confidence": 0.4643615484237671}]}, {"text": "STRAND then predicts that documents with the same structure are parallel.", "labels": [], "entities": []}, {"text": "Tsim uses a machine-readable bilingual dictionary to find word-to-word matches between two halves of a bitext.", "labels": [], "entities": []}, {"text": "It then computes a similarity score based on the maximum cardinality bipartite matching between the two halves.", "labels": [], "entities": [{"text": "similarity score", "start_pos": 19, "end_pos": 35, "type": "METRIC", "confidence": 0.9715801775455475}]}, {"text": "We chose to compare our method with tsim because we were interested in an approach that works with both marked up and plain text documents.", "labels": [], "entities": []}], "datasetContent": [{"text": "In our first two experiments we limited the points of correspondence to orthographic cognates.", "labels": [], "entities": []}, {"text": "We used the Longest Common Subsequence Ratio (LCSR) to measure similarity.", "labels": [], "entities": [{"text": "Longest Common Subsequence Ratio (LCSR)", "start_pos": 12, "end_pos": 51, "type": "METRIC", "confidence": 0.7907905238015311}]}, {"text": "The LCSR ratio is the length of the longest common subsequence of two tokens, divided by the length of the longer token.", "labels": [], "entities": [{"text": "LCSR ratio", "start_pos": 4, "end_pos": 14, "type": "METRIC", "confidence": 0.9527908861637115}]}, {"text": "In our English-Hindi experiments we used an English-Hindi dictionary because the languages are written in different character sets, limiting the effectiveness of orthographic cognates.", "labels": [], "entities": []}], "tableCaptions": []}