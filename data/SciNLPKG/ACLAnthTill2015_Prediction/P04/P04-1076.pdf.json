{"title": [{"text": "Weakly Supervised Learning for Cross-document Person Name Disambiguation Supported by Information Extraction", "labels": [], "entities": [{"text": "Cross-document Person Name Disambiguation", "start_pos": 31, "end_pos": 72, "type": "TASK", "confidence": 0.7181245610117912}, {"text": "Information Extraction", "start_pos": 86, "end_pos": 108, "type": "TASK", "confidence": 0.7043939083814621}]}], "abstractContent": [{"text": "It is fairly common that different people are associated with the same name.", "labels": [], "entities": []}, {"text": "In tracking person entities in a large document pool, it is important to determine whether multiple mentions of the same name across documents refer to the same entity or not.", "labels": [], "entities": []}, {"text": "Previous approach to this problem involves measuring context similarity only based on co-occurring words.", "labels": [], "entities": []}, {"text": "This paper presents anew algorithm using information extraction support in addition to co-occurring words.", "labels": [], "entities": [{"text": "information extraction", "start_pos": 41, "end_pos": 63, "type": "TASK", "confidence": 0.7827937304973602}]}, {"text": "A learning scheme with minimal supervision is developed within the Bayesian framework.", "labels": [], "entities": []}, {"text": "Maximum entropy modeling is then used to represent the probability distribution of context similarities based on heterogeneous features.", "labels": [], "entities": []}, {"text": "Statistical annealing is applied to derive the final entity coreference chains by globally fitting the pairwise context similarities.", "labels": [], "entities": []}, {"text": "Benchmarking shows that our new approach significantly outperforms the existing algorithm by 25 percentage points in overall F-measure.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 125, "end_pos": 134, "type": "METRIC", "confidence": 0.945838987827301}]}], "introductionContent": [{"text": "Cross document name disambiguation is required for various tasks of knowledge discovery from textual documents, such as entity tracking, link discovery, information fusion and event tracking.", "labels": [], "entities": [{"text": "Cross document name disambiguation", "start_pos": 0, "end_pos": 34, "type": "TASK", "confidence": 0.6982971876859665}, {"text": "knowledge discovery from textual documents", "start_pos": 68, "end_pos": 110, "type": "TASK", "confidence": 0.8212468385696411}, {"text": "entity tracking", "start_pos": 120, "end_pos": 135, "type": "TASK", "confidence": 0.769987940788269}, {"text": "link discovery", "start_pos": 137, "end_pos": 151, "type": "TASK", "confidence": 0.7988511621952057}, {"text": "information fusion", "start_pos": 153, "end_pos": 171, "type": "TASK", "confidence": 0.8057794868946075}, {"text": "event tracking", "start_pos": 176, "end_pos": 190, "type": "TASK", "confidence": 0.7698268890380859}]}, {"text": "This task is part of the co-reference task: if two mentions of the same name refer to same (different) entities, by definition, they should (should not) be co-referenced.", "labels": [], "entities": []}, {"text": "As far as names are concerned, co-reference consists of two sub-tasks: (i) name disambiguation to handle the problem of different entities happening to use the same name; (ii) alias association to handle the problem of the same entity using multiple names (aliases).", "labels": [], "entities": [{"text": "name disambiguation", "start_pos": 75, "end_pos": 94, "type": "TASK", "confidence": 0.7528282999992371}, {"text": "alias association", "start_pos": 176, "end_pos": 193, "type": "TASK", "confidence": 0.7561624348163605}]}, {"text": "Message Understanding Conference (MUC) community has established within-document coreference standards].", "labels": [], "entities": [{"text": "Message Understanding Conference (MUC)", "start_pos": 0, "end_pos": 38, "type": "TASK", "confidence": 0.7850993275642395}]}, {"text": "Compared with within-document name disambiguation which can leverage highly reliable discourse heuristics such as one sense per discourse [, cross-document name disambiguation is a much harder problem.", "labels": [], "entities": [{"text": "within-document name disambiguation", "start_pos": 14, "end_pos": 49, "type": "TASK", "confidence": 0.6953250765800476}, {"text": "cross-document name disambiguation", "start_pos": 141, "end_pos": 175, "type": "TASK", "confidence": 0.7655482292175293}]}, {"text": "Among major categories of named entities (NEs, which in this paper refer to entity names, excluding the MUC time and numerical NEs), company and product names are often trademarked or uniquely registered, and hence less subject to name ambiguity.", "labels": [], "entities": [{"text": "MUC", "start_pos": 104, "end_pos": 107, "type": "DATASET", "confidence": 0.6702152490615845}]}, {"text": "This paper focuses on cross-document disambiguation of person names.", "labels": [], "entities": [{"text": "cross-document disambiguation of person names", "start_pos": 22, "end_pos": 67, "type": "TASK", "confidence": 0.8666958570480346}]}, {"text": "Previous research for cross-document name disambiguation applies vector space model (VSM) for context similarity, only using co-occurring words.", "labels": [], "entities": [{"text": "cross-document name disambiguation", "start_pos": 22, "end_pos": 56, "type": "TASK", "confidence": 0.7859660784403483}]}, {"text": "A pre-defined threshold decides whether two context vectors are different enough to represent two different entities.", "labels": [], "entities": []}, {"text": "This approach faces two challenges: i) it is difficult to incorporate natural language processing (NLP) results in the VSM framework; 1 ii) the algorithm focuses on the local pairwise context similarity, and neglects the global correlation in the data: this may cause inconsistent results, and hurts the performance.", "labels": [], "entities": []}, {"text": "This paper presents anew algorithm that addresses these problems.", "labels": [], "entities": []}, {"text": "A learning scheme with minimal supervision is developed within the Bayesian framework.", "labels": [], "entities": []}, {"text": "Maximum entropy modeling is then used to represent the probability distribution of context similarities based on heterogeneous features covering both co-occurring words and natural language information extraction (IE) results.", "labels": [], "entities": [{"text": "natural language information extraction (IE)", "start_pos": 173, "end_pos": 217, "type": "TASK", "confidence": 0.7984482645988464}]}, {"text": "Statistical annealing is used to derive the final entity co-reference chains by globally fitting the pairwise context similarities.", "labels": [], "entities": []}, {"text": "Both the previous algorithm and our new algorithm are implemented, benchmarked and compared.", "labels": [], "entities": []}, {"text": "Significant performance enhancement up to 25 percentage points in overall F-measure is observed with the new approach.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 74, "end_pos": 83, "type": "METRIC", "confidence": 0.9933119416236877}]}, {"text": "The generality of this algorithm ensures that this approach is also applicable to other categories of NEs.", "labels": [], "entities": []}, {"text": "The remaining part of the paper is structured as follows.", "labels": [], "entities": []}, {"text": "Section 2 presents the algorithm design and task definition.", "labels": [], "entities": []}, {"text": "The name disambiguation algorithm is described in Sections 3, 4 and 5, corresponding to the three key aspects of the algorithm, i.e. minimally supervised learning scheme, maximum entropy modeling and annealing-based optimization.", "labels": [], "entities": [{"text": "maximum entropy modeling", "start_pos": 171, "end_pos": 195, "type": "TASK", "confidence": 0.6282053589820862}]}, {"text": "Benchmarks are shown in Section 6, followed by Conclusion in Section 7.", "labels": [], "entities": [{"text": "Benchmarks", "start_pos": 0, "end_pos": 10, "type": "METRIC", "confidence": 0.7302715182304382}, {"text": "Conclusion", "start_pos": 47, "end_pos": 57, "type": "METRIC", "confidence": 0.973914623260498}]}], "datasetContent": [], "tableCaptions": [{"text": " Table 1. Constructed Testing Corpus I", "labels": [], "entities": [{"text": "Constructed Testing Corpus I", "start_pos": 10, "end_pos": 38, "type": "DATASET", "confidence": 0.7296752631664276}]}, {"text": " Table 2. Testing Corpus I Benchmarking", "labels": [], "entities": [{"text": "Testing Corpus I Benchmarking", "start_pos": 10, "end_pos": 39, "type": "DATASET", "confidence": 0.7263270542025566}]}, {"text": " Table 3. Testing Corpus II Benchmarking", "labels": [], "entities": [{"text": "Testing Corpus II Benchmarking", "start_pos": 10, "end_pos": 40, "type": "DATASET", "confidence": 0.8085980117321014}]}]}