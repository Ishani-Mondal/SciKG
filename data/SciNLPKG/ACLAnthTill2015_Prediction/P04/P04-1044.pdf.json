{"title": [{"text": "Combining Acoustic and Pragmatic Features to Predict Recognition Performance in Spoken Dialogue Systems", "labels": [], "entities": [{"text": "Predict Recognition", "start_pos": 45, "end_pos": 64, "type": "TASK", "confidence": 0.856210321187973}]}], "abstractContent": [{"text": "We use machine learners trained on a combination of acoustic confidence and pragmatic plausi-bility features computed from dialogue context to predict the accuracy of incoming n-best recognition hypotheses to a spoken dialogue system.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 155, "end_pos": 163, "type": "METRIC", "confidence": 0.9933838248252869}]}, {"text": "Our best results show a 25% weighted f-score improvement over a baseline system that implements a \"grammar-switching\" approach to context-sensitive speech recognition.", "labels": [], "entities": [{"text": "context-sensitive speech recognition", "start_pos": 130, "end_pos": 166, "type": "TASK", "confidence": 0.6274432937304179}]}], "introductionContent": [{"text": "A crucial problem in the design of spoken dialogue systems is to decide for incoming recognition hypotheses whether a system should accept (consider correctly recognized), reject (assume misrecognition), or ignore (classify as noise or speech not directed to the system) them.", "labels": [], "entities": []}, {"text": "In addition, a more sophisticated dialogue system might decide whether to clarify or confirm certain hypotheses.", "labels": [], "entities": []}, {"text": "Obviously, incorrect decisions at this point can have serious negative effects on system usability and user satisfaction.", "labels": [], "entities": []}, {"text": "On the one hand, accepting misrecognized hypotheses leads to misunderstandings and unintended system behaviors which are usually difficult to recover from.", "labels": [], "entities": []}, {"text": "On the other hand, users might get frustrated with a system that behaves too cautiously and rejects or ignores too many utterances.", "labels": [], "entities": []}, {"text": "Thus an important feature in dialogue system engineering is the tradeoff between avoiding task failure (due to misrecognitions) and promoting overall dialogue efficiency, flow, and naturalness.", "labels": [], "entities": [{"text": "dialogue system engineering", "start_pos": 29, "end_pos": 56, "type": "TASK", "confidence": 0.7636867165565491}]}, {"text": "In this paper, we investigate the use of machine learners trained on a combination of acoustic confidence and pragmatic plausibility features (i.e. computed from dialogue context) to predict the quality of incoming n-best recognition hypotheses to a spoken dialogue system.", "labels": [], "entities": []}, {"text": "These predictions are then used to select a \"best\" hypothesis and to decide on appropriate system reactions.", "labels": [], "entities": []}, {"text": "We evaluate this approach in comparison with a baseline system that combines fixed recognition confidence rejection thresholds with dialogue-state dependent recognition grammars.", "labels": [], "entities": []}, {"text": "The paper is organized as follows.", "labels": [], "entities": []}, {"text": "After a short relation to previous work, Section 3 introduces the WITAS multimodal dialogue system, which we use to collect data (Section 4) and to derive baseline results (Section 5).", "labels": [], "entities": [{"text": "WITAS multimodal dialogue", "start_pos": 66, "end_pos": 91, "type": "TASK", "confidence": 0.5298438767592112}]}, {"text": "Section 6 describes our learning experiments for classifying and selecting from nbest recognition hypotheses and Section 7 reports our results.", "labels": [], "entities": [{"text": "classifying and selecting from nbest recognition hypotheses", "start_pos": 49, "end_pos": 108, "type": "TASK", "confidence": 0.6432826944759914}]}], "datasetContent": [{"text": "The middle part of shows the classification results for TiMBL and RIPPER when run with default parameter settings (the other results are included for comparison).", "labels": [], "entities": [{"text": "TiMBL", "start_pos": 56, "end_pos": 61, "type": "DATASET", "confidence": 0.830878496170044}]}, {"text": "The individual rows show the performance when different combinations of feature groups are used for training.", "labels": [], "entities": []}, {"text": "The results for the three-way classification are included for comparison with the baseline system and are obtained by combining the two classes clarify and reject.", "labels": [], "entities": []}, {"text": "Note that we do not evaluate the performance of the learners for classifying the individual recognition hypotheses but the classification of (whole) user utterances (i.e. including the selection procedure to choose from the classified hypotheses).", "labels": [], "entities": []}, {"text": "The results show that both learners profit from the addition of more features concerning dialogue context and task context for classifying user speech input appropriately.", "labels": [], "entities": []}, {"text": "The only exception from this trend is a slight performance decrease when task features are added in the four-way classification for RIPPER.", "labels": [], "entities": []}, {"text": "Note that both learners already outperform the baseline results even when only recognition features are considered.", "labels": [], "entities": []}, {"text": "The most striking result is the performance gain for TiMBL (almost 10%) when we include the dialogue features.", "labels": [], "entities": []}, {"text": "As soon as dialogue features are included, TiMBL also performs slightly better than RIPPER.", "labels": [], "entities": [{"text": "TiMBL", "start_pos": 43, "end_pos": 48, "type": "METRIC", "confidence": 0.5030210018157959}]}, {"text": "Note that the introduction of (limited) task features, in addition to the DIAL and UTT features, did not have dramatic impact in this study.", "labels": [], "entities": []}, {"text": "One aim for future work is to define and analyze the influence of further task related features for classification.", "labels": [], "entities": [{"text": "classification", "start_pos": 100, "end_pos": 114, "type": "TASK", "confidence": 0.9630057215690613}]}, {"text": "The baseline accuracy for the 3-class problem is 65.68% (61.81% weighted f-score).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 13, "end_pos": 21, "type": "METRIC", "confidence": 0.9896498918533325}]}, {"text": "Our best results, obtained by using TiMBL with parameter opSystem or features used Acc/wf-score Acc/wf-score Acc/wf-score Acc/wf-score for classification: Classification Results timization, show a 25% weighted f-score improvement over the baseline system.", "labels": [], "entities": [{"text": "TiMBL", "start_pos": 36, "end_pos": 41, "type": "DATASET", "confidence": 0.8213908672332764}, {"text": "Acc", "start_pos": 83, "end_pos": 86, "type": "METRIC", "confidence": 0.9674085974693298}]}, {"text": "We can compare these results to a hypothetical \"oracle\" system in order to obtain an upper bound on classification performance.", "labels": [], "entities": []}, {"text": "This is an imaginary system which performs perfectly on the experimental data given the 10-best recognition output.", "labels": [], "entities": []}, {"text": "The oracle results reveal that for 18 of the in-grammar utterances the 10-best recognition hypotheses do not include the correct logical format all and therefore have to be classified as clarify or reject (i.e. it is not possible to achieve 100% accuracy on the experimental data).", "labels": [], "entities": [{"text": "accuracy", "start_pos": 246, "end_pos": 254, "type": "METRIC", "confidence": 0.9969121217727661}]}, {"text": "shows that our best results are only 8%/12% (absolute) away from the optimal performance.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 1 summarizes the evaluation of the baseline  system.", "labels": [], "entities": []}]}