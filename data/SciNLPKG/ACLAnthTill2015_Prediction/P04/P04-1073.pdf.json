{"title": [{"text": "Question Answering using Constraint Satisfaction: QA-by-Dossier-with-Constraints", "labels": [], "entities": [{"text": "Question Answering", "start_pos": 0, "end_pos": 18, "type": "TASK", "confidence": 0.6978064477443695}]}], "abstractContent": [{"text": "QA-by-Dossier-with-Constraints is anew approach to Question Answering whereby candidate answers' confidences are adjusted by asking auxiliary questions whose answers constrain the original answers.", "labels": [], "entities": [{"text": "Question Answering", "start_pos": 51, "end_pos": 69, "type": "TASK", "confidence": 0.8348464667797089}]}, {"text": "These constraints emerge naturally from the domain of interest, and enable application of real-world knowledge to QA.", "labels": [], "entities": []}, {"text": "We show that our approach significantly improves system performance (75% relative improvement in F-measure on select question types) and can create a \"dossier\" of information about the subject matter in the original question.", "labels": [], "entities": [{"text": "F-measure", "start_pos": 97, "end_pos": 106, "type": "METRIC", "confidence": 0.9935861229896545}]}], "introductionContent": [{"text": "Traditionally, Question Answering (QA) has drawn on the fields of Information Retrieval, Natural Language Processing (NLP), Ontologies, Data Bases and Logical Inference, although it is at heart a problem of NLP.", "labels": [], "entities": [{"text": "Question Answering (QA)", "start_pos": 15, "end_pos": 38, "type": "TASK", "confidence": 0.9228217482566834}, {"text": "Information Retrieval", "start_pos": 66, "end_pos": 87, "type": "TASK", "confidence": 0.7310220301151276}]}, {"text": "These fields have been used to supply the technology with which QA components have been built.", "labels": [], "entities": []}, {"text": "We present here anew methodology which attempts to use QA holistically, along with constraint satisfaction, to better answer questions, without requiring any advances in the underlying fields.", "labels": [], "entities": []}, {"text": "Because NLP is still very much an error-prone process, QA systems make many mistakes; accordingly, a variety of methods have been developed to boost the accuracy of their answers.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 153, "end_pos": 161, "type": "METRIC", "confidence": 0.9981191754341125}]}, {"text": "Such methods include redundancy (getting the same answer from multiple documents, sources, or algorithms), deep parsing of questions and texts (hence improving the accuracy of confidence measures), inferencing (proving the answer from information in texts plus background knowledge) and sanity-checking (verifying that answers are consistent with known facts).", "labels": [], "entities": [{"text": "deep parsing of questions and texts", "start_pos": 107, "end_pos": 142, "type": "TASK", "confidence": 0.802715023358663}, {"text": "accuracy", "start_pos": 164, "end_pos": 172, "type": "METRIC", "confidence": 0.9972726702690125}]}, {"text": "To our knowledge, however, no QA system deliberately asks additional questions in order to derive constraints on the answers to the original questions.", "labels": [], "entities": []}, {"text": "We have found empirically that when our own QA system's ( top answer is wrong, the correct answer is often present later in the ranked answer list.", "labels": [], "entities": []}, {"text": "In other words, the correct answer is in the passages retrieved by the search engine, but the system was unable to sufficiently promote the correct answer and/or deprecate the incorrect ones.", "labels": [], "entities": []}, {"text": "Our new approach of QA-by-Dossier-with-Constraints (QDC) uses the answers to additional questions to provide more information that can be used in ranking candidate answers to the original question.", "labels": [], "entities": [{"text": "QA-by-Dossier-with-Constraints (QDC)", "start_pos": 20, "end_pos": 56, "type": "TASK", "confidence": 0.6261005476117134}]}, {"text": "These auxiliary questions are selected such that natural constraints exist among the set of correct answers.", "labels": [], "entities": []}, {"text": "After issuing both the original question and auxiliary questions, the system evaluates all possible combinations of the candidate answers and scores them by a simple function of both the answers' intrinsic confidences, and how well the combination satisfies the aforementioned constraints.", "labels": [], "entities": []}, {"text": "Thus we hope to improve the accuracy of an essentially NLP task by making an end-run around some of the more difficult problems in the field.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 28, "end_pos": 36, "type": "METRIC", "confidence": 0.9987936019897461}]}, {"text": "We describe QDC and experiments to evaluate its effectiveness.", "labels": [], "entities": []}, {"text": "Our results show that on our test set, substantial improvement is achieved by using constraints, compared with our baseline system, using standard evaluation metrics.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1 shows our system's top answers to  this question, with associated scores in the range  0-1.", "labels": [], "entities": []}, {"text": " Table 2. Answers for auxiliary questions \"When  was Leonardo da Vinci born?\" and \"When did Leo- nardo da Vinci die?\".", "labels": [], "entities": []}, {"text": " Table 3. The baseline  assertions for individual X were:  o Top-ranking birthdate/NIL  o Top-ranking deathdate/NIL  o Set of works W i that passed threshold  o Top-ranking date for W i /NIL", "labels": [], "entities": []}, {"text": " Table 3. Results of Performance Evaluation.  Two calculations of P/R/F are made, depending on  whether the averaging is done over the whole set, or  first by individual; the results are very similar.", "labels": [], "entities": [{"text": "P/R/F", "start_pos": 66, "end_pos": 71, "type": "METRIC", "confidence": 0.8871673583984375}]}]}