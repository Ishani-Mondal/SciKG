{"title": [{"text": "A Geometric View on Bilingual Lexicon Extraction from Comparable Corpora", "labels": [], "entities": [{"text": "Bilingual Lexicon Extraction from Comparable Corpora", "start_pos": 20, "end_pos": 72, "type": "TASK", "confidence": 0.7615038653214773}]}], "abstractContent": [{"text": "We present a geometric view on bilingual lexicon extraction from comparable corpora, which allows to re-interpret the methods proposed so far and identify unresolved problems.", "labels": [], "entities": [{"text": "bilingual lexicon extraction", "start_pos": 31, "end_pos": 59, "type": "TASK", "confidence": 0.6458876430988312}]}, {"text": "This motivates three new methods that aim at solving these problems.", "labels": [], "entities": []}, {"text": "Empirical evaluation shows the strengths and weaknesses of these methods, as well as a significant gain in the accuracy of extracted lexicons.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 111, "end_pos": 119, "type": "METRIC", "confidence": 0.9984690546989441}]}], "introductionContent": [{"text": "Comparable corpora contain texts written in different languages that, roughly speaking, \"talk about the same thing\".", "labels": [], "entities": []}, {"text": "In comparison to parallel corpora, ie corpora which are mutual translations, comparable corpora have not received much attention from the research community, and very few methods have been proposed to extract bilingual lexicons from such corpora.", "labels": [], "entities": []}, {"text": "However, except for those found in translation services or in a few international organisations, which, by essence, produce parallel documentations, most existing multilingual corpora are not parallel, but comparable.", "labels": [], "entities": []}, {"text": "This concern is reflected in major evaluation conferences on crosslanguage information retrieval (CLIR), e.g. CLEF , which only use comparable corpora for their multilingual tracks.", "labels": [], "entities": [{"text": "crosslanguage information retrieval (CLIR)", "start_pos": 61, "end_pos": 103, "type": "TASK", "confidence": 0.7567774007717768}]}, {"text": "We adopt here a geometric view on bilingual lexicon extraction from comparable corpora which allows one to re-interpret the methods proposed thus far and formulate new ones inspired by latent semantic analysis (LSA), which was developed within the information retrieval (IR) community to treat synonymous and polysemous terms).", "labels": [], "entities": [{"text": "bilingual lexicon extraction", "start_pos": 34, "end_pos": 62, "type": "TASK", "confidence": 0.6491584380467733}, {"text": "latent semantic analysis (LSA)", "start_pos": 185, "end_pos": 215, "type": "TASK", "confidence": 0.7380498697360357}, {"text": "information retrieval (IR)", "start_pos": 248, "end_pos": 274, "type": "TASK", "confidence": 0.7714460372924805}]}, {"text": "We will explain in this paper the motivations behind the use of such methods for bilingual lexicon extraction from comparable corpora, and show how to apply them.", "labels": [], "entities": [{"text": "bilingual lexicon extraction from comparable corpora", "start_pos": 81, "end_pos": 133, "type": "TASK", "confidence": 0.7759220252434412}]}, {"text": "Section 2 is devoted to the presentation of the standard approach, ie the approach adopted by most researchers so far, its geometric interpretation, and the unresolved synonymy 1 http://clef.iei.pi.cnr.it:2002/ and polysemy problems.", "labels": [], "entities": [{"text": "geometric interpretation", "start_pos": 123, "end_pos": 147, "type": "TASK", "confidence": 0.7566676139831543}]}, {"text": "Sections 3 to 4 then describe three new methods aiming at addressing the issues raised by synonymy and polysemy: in section 3 we introduce an extension of the standard approach, and show in appendix A how this approach relates to the probabilistic method proposed in; in section 4, we present a bilingual extension to LSA, namely canonical correlation analysis and its kernel version; lastly, in section 5, we formulate the problem in terms of probabilistic LSA and review different associated similarities.", "labels": [], "entities": [{"text": "canonical correlation analysis", "start_pos": 330, "end_pos": 360, "type": "TASK", "confidence": 0.6416814923286438}]}, {"text": "Section 6 is then devoted to a large-scale evaluation of the different methods proposed.", "labels": [], "entities": []}, {"text": "Open issues are then discussed in section 7.", "labels": [], "entities": []}], "datasetContent": [{"text": "We conducted experiments on an English-French corpus derived from the data used in the multilingual track of CLEF2003, corresponding to the newswire of months May 1994 and December 1994 of the Los Angeles Times and Le Monde (1994, French).", "labels": [], "entities": [{"text": "multilingual track of CLEF2003", "start_pos": 87, "end_pos": 117, "type": "DATASET", "confidence": 0.5885891020298004}, {"text": "newswire of months May 1994 and December 1994 of the Los Angeles Times and Le Monde", "start_pos": 140, "end_pos": 223, "type": "DATASET", "confidence": 0.6501398980617523}]}, {"text": "As our bilingual dictionary, we used the ELRA multilingual dictionary, 4 which contains ca.", "labels": [], "entities": [{"text": "ELRA multilingual dictionary", "start_pos": 41, "end_pos": 69, "type": "DATASET", "confidence": 0.8552793661753336}]}, {"text": "13,500 entries with at least one match in our corpus.", "labels": [], "entities": []}, {"text": "In addition, the following linguistic preprocessing steps were performed on both the corpus and the dictionary: tokenisation, lemmatisation and POS-tagging.", "labels": [], "entities": [{"text": "tokenisation", "start_pos": 112, "end_pos": 124, "type": "TASK", "confidence": 0.9615160226821899}]}, {"text": "Only lexical words (nouns, verbs, adverbs, adjectives) were indexed and only single word entries in the dicitonary were retained.", "labels": [], "entities": []}, {"text": "Infrequent words (occurring less than 5 times) were discarded when building the indexing terms and the dictionary entries.", "labels": [], "entities": []}, {"text": "After these steps our corpus contains 34,966 distinct English words, and 21,140 distinct French words, leading to ca.", "labels": [], "entities": []}, {"text": "25,000 English and 13,000 French words not present in the dictionary.", "labels": [], "entities": []}, {"text": "To evaluate the performance of our extraction methods, we randomly split the dictionaries into a training set with 12,255 entries, and a test set with 1,245 entries.", "labels": [], "entities": []}, {"text": "The split is designed in such away that all pairs corresponding to the same source word are in the same set (training or test).", "labels": [], "entities": []}, {"text": "All methods use the training set as the sole available resource and predict the most likely translations of the terms in the source language (English) belonging to the test set.", "labels": [], "entities": []}, {"text": "The context vectors were defined by computing the mutual information association measure between terms occurring in the same context window of size 5 (ie. by considering a neighborhood of +/-2 words around the current word), and summing it overall contexts of the corpora.", "labels": [], "entities": []}, {"text": "Different association measures and context sizes were assessed and the above settings turned out to give the best performance even if the optimum is relatively flat.", "labels": [], "entities": []}, {"text": "For memory space and computational efficiency reasons, context vectors were pruned so that, for each term, the remaining components represented at least 90 percent of the total mutual information.", "labels": [], "entities": []}, {"text": "After pruning, the context vectors were normalised so that their Euclidean norm is equal to 1.", "labels": [], "entities": []}, {"text": "The PLSA-based methods used the raw co-occurrence counts as association measure, to be consistent with the underlying generative model.", "labels": [], "entities": []}, {"text": "In addition, for the extended method, we retained only the N (N = 200 is the value which yielded the best results in our experiments) dictionary entries closest to source and target words when doing the projection with Q.", "labels": [], "entities": []}, {"text": "As discussed below, this allows us to get rid of spurious relationships.", "labels": [], "entities": []}, {"text": "The upper part of table 1 summarizes the results we obtained, measured in terms of F-1 score for different lengths of the candidate list, from 20 to 500.", "labels": [], "entities": [{"text": "F-1 score", "start_pos": 83, "end_pos": 92, "type": "METRIC", "confidence": 0.9788575172424316}]}, {"text": "For each length, precision is based on the number of lists that contain an actual translation of the source word, whereas recall is based on the number of translations provided in the reference set and found in the list.", "labels": [], "entities": [{"text": "precision", "start_pos": 17, "end_pos": 26, "type": "METRIC", "confidence": 0.9995207786560059}, {"text": "recall", "start_pos": 122, "end_pos": 128, "type": "METRIC", "confidence": 0.999345600605011}]}, {"text": "Note that our results differ from the ones previously published, which can be explained by the fact that first our corpus is relatively small compared to others, second that our evaluation relies on a large number of candidates, which can occur as few as 5 times in the corpus, whereas previous evaluations were based on few, high frequent terms, and third that we do not use the same bilingual dictionary, the coverage of which being an important factor in the quality of the results obtained.", "labels": [], "entities": []}, {"text": "Long candidate lists are justified by CLIR considerations, where longer lists might be preferred over shorter ones for query expansion purposes.", "labels": [], "entities": [{"text": "query expansion", "start_pos": 119, "end_pos": 134, "type": "TASK", "confidence": 0.7470832169055939}]}, {"text": "For PLSA, the normalised Fisher kernels provided the best results, and increasing the number of latent classes did not lead in our case to improved results.", "labels": [], "entities": [{"text": "PLSA", "start_pos": 4, "end_pos": 8, "type": "TASK", "confidence": 0.9154714941978455}]}, {"text": "We thus display here the results obtained with the normalised version of the Fisher kernel, using only one component.", "labels": [], "entities": []}, {"text": "For CCA, we empirically optimised the number of dimensions to be used, and display the results obtained with the optimal value (l = 300).", "labels": [], "entities": []}, {"text": "As one can note, the extended approach yields the best results in terms of F1-score.", "labels": [], "entities": [{"text": "F1-score", "start_pos": 75, "end_pos": 83, "type": "METRIC", "confidence": 0.9983088970184326}]}, {"text": "However, its performance for the first 20 candidates are below the standard approach and comparable to the PLSAbased method.", "labels": [], "entities": []}, {"text": "Indeed, the standard approach leads to higher precision at the top of the list, but lower recall overall.", "labels": [], "entities": [{"text": "precision", "start_pos": 46, "end_pos": 55, "type": "METRIC", "confidence": 0.9993151426315308}, {"text": "recall", "start_pos": 90, "end_pos": 96, "type": "METRIC", "confidence": 0.9996895790100098}]}, {"text": "This suggests that we could gain in performance by re-ranking the candidates of the extended approach with the standard and PLSA methods.", "labels": [], "entities": []}, {"text": "The lower part of table 1 shows that this is indeed the case.", "labels": [], "entities": []}, {"text": "The average precision goes up from 0.4 to 0.44 through this combination, and the F1-score is significantly improved for all the length ranges we considered (bold line in table 1).", "labels": [], "entities": [{"text": "precision", "start_pos": 12, "end_pos": 21, "type": "METRIC", "confidence": 0.9973065853118896}, {"text": "F1-score", "start_pos": 81, "end_pos": 89, "type": "METRIC", "confidence": 0.999653697013855}]}], "tableCaptions": [{"text": " Table 1: Results of the different methods; F-1 score at different number of candidate translations. Ext refers  to the extended approach, whereas NFK stands for normalised Fisher kernel.", "labels": [], "entities": [{"text": "F-1", "start_pos": 44, "end_pos": 47, "type": "METRIC", "confidence": 0.9751300811767578}]}]}