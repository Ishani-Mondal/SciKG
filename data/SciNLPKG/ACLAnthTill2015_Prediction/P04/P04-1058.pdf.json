{"title": [{"text": "Alternative Approaches for Generating Bodies of Grammar Rules", "labels": [], "entities": []}], "abstractContent": [{"text": "We compare two approaches for describing and generating bodies of rules used for natural language parsing.", "labels": [], "entities": [{"text": "natural language parsing", "start_pos": 81, "end_pos": 105, "type": "TASK", "confidence": 0.6538634796937307}]}, {"text": "In today's parsers rule bodies do not exist a priori but are generated on the fly, usually with methods based on n-grams, which are one particular way of inducing probabilistic regular languages.", "labels": [], "entities": []}, {"text": "We compare two approaches for inducing such languages.", "labels": [], "entities": []}, {"text": "One is based on n-grams, the other on minimization of the Kullback-Leibler divergence.", "labels": [], "entities": []}, {"text": "The inferred regular languages are used for generating bodies of rules inside a parsing procedure.", "labels": [], "entities": []}, {"text": "We compare the two approaches along two dimensions: the quality of the probabilistic regular language they produce, and the performance of the parser they were used to build.", "labels": [], "entities": []}, {"text": "The second approach outper-forms the first one along both dimensions.", "labels": [], "entities": []}], "introductionContent": [{"text": "N -grams have had a big impact on the state of the art in natural language parsing.", "labels": [], "entities": [{"text": "natural language parsing", "start_pos": 58, "end_pos": 82, "type": "TASK", "confidence": 0.6374796529610952}]}, {"text": "They are central to many parsing models, and despite their simplicity n-gram models have been very successful.", "labels": [], "entities": [{"text": "parsing", "start_pos": 25, "end_pos": 32, "type": "TASK", "confidence": 0.9793047308921814}]}, {"text": "Modeling with n-grams is an induction task.", "labels": [], "entities": []}, {"text": "Given a sample set of strings, the task is to guess the grammar that produced that sample.", "labels": [], "entities": []}, {"text": "Usually, the grammar is not be chosen from an arbitrary set of possible grammars, but from a given class.", "labels": [], "entities": []}, {"text": "Hence, grammar induction consists of two parts: choosing the class of languages amongst which to search and designing the procedure for performing the search.", "labels": [], "entities": [{"text": "grammar induction", "start_pos": 7, "end_pos": 24, "type": "TASK", "confidence": 0.802650511264801}]}, {"text": "By using n-grams for grammar induction one addresses the two parts in one go.", "labels": [], "entities": [{"text": "grammar induction", "start_pos": 21, "end_pos": 38, "type": "TASK", "confidence": 0.7486510276794434}]}, {"text": "In particular, the use of n-grams implies that the solution will be searched for in the class of probabilistic regular languages, since n-grams induce probabilistic automata and, consequently, probabilistic regular languages.", "labels": [], "entities": []}, {"text": "However, the class of probabilistic regular languages induced using n-grams is a proper subclass of the class of all probabilistic regular languages; n-grams are incapable of capturing long-distance relations between words.", "labels": [], "entities": []}, {"text": "At the technical level the restricted nature of n-grams is witnessed by the special structure of the automata induced from them, as we will see in Section 4.2.", "labels": [], "entities": []}, {"text": "N -grams are not the only way to induce regular languages, and not the most powerful way to do so.", "labels": [], "entities": []}, {"text": "There is a variety of general methods capable of inducing all regular languages).", "labels": [], "entities": []}, {"text": "What is their relevance for natural language parsing?", "labels": [], "entities": [{"text": "natural language parsing", "start_pos": 28, "end_pos": 52, "type": "TASK", "confidence": 0.6471149424711863}]}, {"text": "Recall that regular languages are used for describing the bodies of rules in a grammar.", "labels": [], "entities": []}, {"text": "Consequently, the quality and expressive power of the resulting grammar is tied to the quality and expressive power of the regular languages used to describe them.", "labels": [], "entities": []}, {"text": "And the quality and expressive power of the latter, in turn, are influenced directly by the method used to induce them.", "labels": [], "entities": []}, {"text": "These observations give rise to a natural question: can we gain anything in parsing from using general methods for inducing regular languages instead of methods based on n-grams?", "labels": [], "entities": []}, {"text": "Specifically, can we describe the bodies of grammatical rules more accurately and more concisely by using general methods for inducing regular languages?", "labels": [], "entities": []}, {"text": "In the context of natural language parsing we present an empirical comparison between algorithms for inducing regular languages using ngrams on the one hand, and more general algorithms for learning the general class of regular language on the other hand.", "labels": [], "entities": [{"text": "natural language parsing", "start_pos": 18, "end_pos": 42, "type": "TASK", "confidence": 0.6723633805910746}]}, {"text": "We generate our training data from the Wall Street Journal Section of the Penn Tree Bank (PTB), by transforming it to projective dependency structures, following, and extracting rules from the result.", "labels": [], "entities": [{"text": "Wall Street Journal Section of the Penn Tree Bank (PTB)", "start_pos": 39, "end_pos": 94, "type": "DATASET", "confidence": 0.9443772335847219}]}, {"text": "These rules are used as training material for the rule induction algorithms we consider.", "labels": [], "entities": [{"text": "rule induction", "start_pos": 50, "end_pos": 64, "type": "TASK", "confidence": 0.7591415047645569}]}, {"text": "The automata produced this way are then used to build grammars which, in turn, are used for parsing.", "labels": [], "entities": []}, {"text": "We are interested in two different aspects of the use of probabilistic regular languages for natural language parsing: the quality of the induced automata and the performance of the resulting parsers.", "labels": [], "entities": [{"text": "natural language parsing", "start_pos": 93, "end_pos": 117, "type": "TASK", "confidence": 0.6708932320276896}]}, {"text": "For evaluation purposes, we use two different metrics: perplexity for the first aspect and percentage of correct attachments for the second.", "labels": [], "entities": []}, {"text": "The main results of the paper are that, measured in terms of perplexity, the automata induced by algorithms other than n-grams describe the rule bodies better than automata induced using n-gram-based algorithms, and that, moreover, the gain in automata quality is reflected by an improvement in parsing performance.", "labels": [], "entities": []}, {"text": "We also find that the parsing performance of both methods (n-grams vs. general automata) can be substantially improved by splitting the training material into POS categories.", "labels": [], "entities": [{"text": "parsing", "start_pos": 22, "end_pos": 29, "type": "TASK", "confidence": 0.965340793132782}]}, {"text": "As aside product, we find empirical evidence to suggest that the effectiveness of rule lexicalization techniques) and parent annotation techniques () is due to the fact that both lead to a reduction in perplexity in the automata induced from training corpora.", "labels": [], "entities": []}, {"text": "Section 2 surveys our experiments, and later sections provide details of the various aspects.", "labels": [], "entities": []}, {"text": "Section 3 offers details on our grammatical framework, PCW-grammars, on transforming automata to PCW-grammars, and on parsing with PCWgrammars.", "labels": [], "entities": []}, {"text": "Section 4 explains the starting point of this process: learning automata, and Section 5 reports on parsing experiments.", "labels": [], "entities": []}, {"text": "We discuss related work in Section 6 and conclude in Section 7.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1: Automata sizes for the \"One-Automaton\"  case, with alpha = 0.0001.", "labels": [], "entities": []}, {"text": " Table 2: Automata sizes for the three parts of speech  in the \"Many-Automata\" case, with alpha =  0.0002 for parts of speech.", "labels": [], "entities": []}, {"text": " Table 3: Number of rules in the grammars built.", "labels": [], "entities": []}, {"text": " Table 4: Parsing results for the PTB", "labels": [], "entities": [{"text": "Parsing", "start_pos": 10, "end_pos": 17, "type": "TASK", "confidence": 0.7896384596824646}, {"text": "PTB", "start_pos": 34, "end_pos": 37, "type": "DATASET", "confidence": 0.664307177066803}]}]}