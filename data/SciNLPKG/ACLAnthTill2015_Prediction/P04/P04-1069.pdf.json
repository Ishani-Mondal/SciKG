{"title": [], "abstractContent": [{"text": "We present new results on the relation between context-free parsing strategies and their probabilis-tic counterparts.", "labels": [], "entities": []}, {"text": "We provide a necessary condition and a sufficient condition for the probabilistic extension of parsing strategies.", "labels": [], "entities": []}, {"text": "These results generalize existing results in the literature that were obtained by considering parsing strategies in isolation.", "labels": [], "entities": [{"text": "parsing", "start_pos": 94, "end_pos": 101, "type": "TASK", "confidence": 0.9625187516212463}]}], "introductionContent": [{"text": "Context-free grammars (CFGs) are standardly used in computational linguistics as formal models of the syntax of natural language, associating sentences with all their possible derivations.", "labels": [], "entities": [{"text": "Context-free grammars (CFGs)", "start_pos": 0, "end_pos": 28, "type": "TASK", "confidence": 0.6876326918601989}]}, {"text": "Other computational models with the same generative capacity as CFGs are also adopted, as for instance push-down automata (PDAs).", "labels": [], "entities": []}, {"text": "One of the advantages of the use of PDAs is that these devices provide an operational specification that determines which steps must be performed when parsing an input string, something that is not offered by CFGs.", "labels": [], "entities": [{"text": "parsing an input string", "start_pos": 151, "end_pos": 174, "type": "TASK", "confidence": 0.8539233952760696}]}, {"text": "In other words, PDAs can be associated to parsing strategies for contextfree languages.", "labels": [], "entities": []}, {"text": "More precisely, parsing strategies are traditionally specified as constructions that map CFGs to language-equivalent PDAs.", "labels": [], "entities": []}, {"text": "Popular examples of parsing strategies are the standard constructions of top-down PDAs, leftcorner PDAs, shift-reduce PDAs ( and LR PDAs ().", "labels": [], "entities": [{"text": "parsing", "start_pos": 20, "end_pos": 27, "type": "TASK", "confidence": 0.9683351516723633}]}, {"text": "CFGs and PDAs have probabilistic counterparts, called probabilistic CFGs (PCFGs) and probabilistic PDAs (PPDAs).", "labels": [], "entities": []}, {"text": "These models are very popular in natural language processing applications, where they are used to define a probability distribution function on the domain of all derivations for sentences in the language of interest.", "labels": [], "entities": []}, {"text": "In PCFGs and PPDAs, probabilities are assigned to rules or transitions, respectively.", "labels": [], "entities": []}, {"text": "However, these probabilities cannot be chosen entirely arbitrarily.", "labels": [], "entities": []}, {"text": "For example, fora given nonterminal A in a PCFG, the sum of the probabilities of all rules rewriting A must be 1.", "labels": [], "entities": [{"text": "PCFG", "start_pos": 43, "end_pos": 47, "type": "DATASET", "confidence": 0.8968408703804016}]}, {"text": "This means that, out of a total of say m rules rewriting A, only m \u2212 1 rules represent \"free\" parameters.", "labels": [], "entities": []}, {"text": "Depending on the choice of the parsing strategy, the constructed PDA may allow different probability distributions than the underlying CFG, since the set of free parameters may differ between the CFG and the PDA, both quantitatively and qualitatively.", "labels": [], "entities": [{"text": "CFG", "start_pos": 135, "end_pos": 138, "type": "DATASET", "confidence": 0.9594062566757202}, {"text": "CFG", "start_pos": 196, "end_pos": 199, "type": "DATASET", "confidence": 0.971595287322998}]}, {"text": "For example, ( and) have shown that a probability distribution that can be obtained by training the probabilities of a CFG on the basis of a corpus can be less accurate than the probability distribution obtained by training the probabilities of a PDA constructed by a particular parsing strategy, on the basis of the same corpus.", "labels": [], "entities": []}, {"text": "Also the results from, and) could be seen in this light.", "labels": [], "entities": []}, {"text": "The question arises of whether parsing strategies can be extended probabilistically, i.e., whether a given construction of PDAs from CFGs can be \"augmented\" with a function defining the probabilities for the target PDA, given the probabilities associated with the input CFG, in such away that the obtained probabilistic distributions on the CFG derivations and the corresponding PDA computations are equivalent.", "labels": [], "entities": [{"text": "parsing", "start_pos": 31, "end_pos": 38, "type": "TASK", "confidence": 0.9694473743438721}]}, {"text": "Some first results on this issue have been presented by, who shows that the already mentioned left-corner parsing strategy can be extended probabilistically, and later by) who show that the pure top-down parsing strategy and a specific type of shift-reduce parsing strategy can be probabilistically extended.", "labels": [], "entities": []}, {"text": "One might think that any \"practical\" parsing strategy can be probabilistically extended, but this turns out not to be the case.", "labels": [], "entities": []}, {"text": "We briefly discuss here a counter-example, in order to motivate the approach we have taken in this paper.", "labels": [], "entities": []}, {"text": "Probabilistic LR parsing has been investigated in the literature) under the assumption that it would allow more fine-grained probability distributions than the underlying PCFGs.", "labels": [], "entities": [{"text": "LR parsing", "start_pos": 14, "end_pos": 24, "type": "TASK", "confidence": 0.8576936423778534}]}, {"text": "However, this is not the casein general.", "labels": [], "entities": []}, {"text": "Consider a PCFG with rule/probability pairs: There are two key transitions in the associated LR automaton, which represent shift actions over c and d (we denote LR states by their sets of kernel items and encode these states into stack symbols): Assume a proper assignment of probabilities to the transitions of the LR automaton, i.e., the sum of transition probabilities fora given LR state is 1.", "labels": [], "entities": []}, {"text": "It can be easily seen that we must assign probability 1 to all transitions except \u03c4 c and \u03c4 d , since this is the only pair of distinct transitions that can be applied for one and the same top-of-stack symbol, viz.", "labels": [], "entities": []}, {"text": "Thus we conclude that there is no proper assignment of probabilities to the transitions of the LR automaton that would result in a distribution on the generated language that is equivalent to the one induced by the source PCFG.", "labels": [], "entities": []}, {"text": "Therefore the LR strategy does not allow probabilistic extension.", "labels": [], "entities": []}, {"text": "One may seemingly solve this problem by dropping the constraint of properness, letting each transition that outputs a rule have the same probability as that rule in the PCFG, and letting other transitions have probability 1.", "labels": [], "entities": [{"text": "PCFG", "start_pos": 169, "end_pos": 173, "type": "DATASET", "confidence": 0.9593716859817505}]}, {"text": "However, the properness condition for PDAs has been heavily exploited in parsing applications, in doing incremental left-to-right probability computation for beam search, and more generally in integration with other linear probabilistic models.", "labels": [], "entities": [{"text": "parsing", "start_pos": 73, "end_pos": 80, "type": "TASK", "confidence": 0.9682658314704895}, {"text": "beam search", "start_pos": 158, "end_pos": 169, "type": "TASK", "confidence": 0.925075113773346}]}, {"text": "Furthermore, commonly used training algorithms for PCFGS/PPDAs always produce proper probability assignments, and many desired mathematical properties of these methods are based on such an assumption ().", "labels": [], "entities": [{"text": "PCFGS/PPDAs", "start_pos": 51, "end_pos": 62, "type": "DATASET", "confidence": 0.6881862282752991}]}, {"text": "We may therefore discard non-proper probability assignments in the current study.", "labels": [], "entities": []}, {"text": "However, such probability assignments are outside the reach of the usual training algorithms for PDAs, which always produce proper PDAs.", "labels": [], "entities": []}, {"text": "Therefore, we may discard such assignments in the current study, which investigates aspects of the potential of training algorithms for CFGs and PDAs.", "labels": [], "entities": []}, {"text": "What has been lacking in the literature is a theoretical framework to relate the parameter space of a CFG to that of a PDA constructed from the CFG by a particular parsing strategy, in terms of the set of allowable probability distributions over derivations.", "labels": [], "entities": [{"text": "CFG", "start_pos": 144, "end_pos": 147, "type": "DATASET", "confidence": 0.9591076970100403}]}, {"text": "Note that the number of free parameters alone is not a satisfactory characterization of the parameter space.", "labels": [], "entities": []}, {"text": "In fact, if the \"nature\" of the parameters is ill-chosen, then an increase in the number of parameters may lead to a deterioration of the accuracy of the model, due to sparseness of data.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 138, "end_pos": 146, "type": "METRIC", "confidence": 0.9992440938949585}]}, {"text": "In this paper we extend previous results, where only a few specific parsing strategies were considered in isolation, and provide some general characterization of parsing strategies that can be probabilistically extended.", "labels": [], "entities": []}, {"text": "Our main contribution can be stated as follows.", "labels": [], "entities": []}, {"text": "\u2022 We define a theoretical framework to relate the parameter space defined by a CFG and that defined by a PDA constructed from the CFG by a particular parsing strategy.", "labels": [], "entities": [{"text": "CFG", "start_pos": 130, "end_pos": 133, "type": "DATASET", "confidence": 0.9515129923820496}]}, {"text": "\u2022 We provide a necessary condition and a sufficient condition for the probabilistic extension of parsing strategies.", "labels": [], "entities": [{"text": "parsing", "start_pos": 97, "end_pos": 104, "type": "TASK", "confidence": 0.9362095594406128}]}, {"text": "We use the above findings to establish new results about probabilistic extensions of parsing strategies that are used in standard practice in computational linguistics, as well as to provide simpler proofs of already known results.", "labels": [], "entities": []}, {"text": "We introduce our framework in Section 3 and report our main results in Sections 4 and 5.", "labels": [], "entities": []}, {"text": "We discuss applications of our results in Section 6.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": []}