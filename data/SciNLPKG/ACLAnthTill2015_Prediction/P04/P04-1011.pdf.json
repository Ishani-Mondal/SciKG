{"title": [{"text": "Trainable Sentence Planning for Complex Information Presentation in Spoken Dialog Systems", "labels": [], "entities": []}], "abstractContent": [{"text": "A challenging problem for spoken dialog systems is the design of utterance generation modules that are fast, flexible and general, yet produce high quality output in particular domains.", "labels": [], "entities": [{"text": "utterance generation", "start_pos": 65, "end_pos": 85, "type": "TASK", "confidence": 0.8636151850223541}]}, {"text": "A promising approach is trainable generation, which uses general-purpose linguistic knowledge automatically adapted to the application domain.", "labels": [], "entities": [{"text": "trainable generation", "start_pos": 24, "end_pos": 44, "type": "TASK", "confidence": 0.7410581707954407}]}, {"text": "This paper presents a trainable sentence planner for the MATCH dialog system.", "labels": [], "entities": [{"text": "MATCH dialog", "start_pos": 57, "end_pos": 69, "type": "TASK", "confidence": 0.6462548077106476}]}, {"text": "We show that trainable sentence planning can produce output comparable to that of MATCH's template-based generator even for quite complex information presentations.", "labels": [], "entities": [{"text": "sentence planning", "start_pos": 23, "end_pos": 40, "type": "TASK", "confidence": 0.7297880947589874}]}], "introductionContent": [{"text": "One very challenging problem for spoken dialog systems is the design of the utterance generation module.", "labels": [], "entities": [{"text": "utterance generation", "start_pos": 76, "end_pos": 96, "type": "TASK", "confidence": 0.8880179226398468}]}, {"text": "This challenge arises partly from the need for the generator to adapt to many features of the dialog domain, user population, and dialog context.", "labels": [], "entities": []}, {"text": "There are three possible approaches to generating system utterances.", "labels": [], "entities": []}, {"text": "The first is templatebased generation, used inmost dialog systems today.", "labels": [], "entities": []}, {"text": "Template-based generation enables a programmer without linguistic training to program a generator that can efficiently produce high quality output specific to different dialog situations.", "labels": [], "entities": [{"text": "Template-based generation", "start_pos": 0, "end_pos": 25, "type": "TASK", "confidence": 0.8848450779914856}]}, {"text": "Its drawbacks include the need to (1) create templates anew by hand for each application; (2) design and maintain a set of templates that work well together in many dialog contexts; and (3) repeatedly encode linguistic constraints such as subject-verb agreement.", "labels": [], "entities": []}, {"text": "The second approach is natural language generation (NLG), which divides generation into: (1) text (or content) planning, (2) sentence planning, and (3) surface realization.", "labels": [], "entities": [{"text": "natural language generation (NLG)", "start_pos": 23, "end_pos": 56, "type": "TASK", "confidence": 0.8147699336210886}, {"text": "text (or content) planning", "start_pos": 93, "end_pos": 119, "type": "TASK", "confidence": 0.7024905184904734}, {"text": "sentence planning", "start_pos": 125, "end_pos": 142, "type": "TASK", "confidence": 0.7043868750333786}, {"text": "surface realization", "start_pos": 152, "end_pos": 171, "type": "TASK", "confidence": 0.7769280672073364}]}, {"text": "NLG promises portability across domains and dialog contexts by using general rules for each generation module.", "labels": [], "entities": [{"text": "NLG", "start_pos": 0, "end_pos": 3, "type": "DATASET", "confidence": 0.8985351920127869}]}, {"text": "However, the quality of the output fora particular domain, or a particular dialog context, maybe inferior to that of a templatebased system unless domain-specific rules are developed or general rules are tuned for the particular domain.", "labels": [], "entities": []}, {"text": "Furthermore, full NLG maybe too slow for use in dialog systems.", "labels": [], "entities": []}, {"text": "A third, more recent, approach is trainable generation: techniques for automatically training NLG modules, or hybrid techniques that adapt NLG modules to particular domains or user groups, e.g.).", "labels": [], "entities": []}, {"text": "Open questions about the trainable approach include (1) whether the output quality is high enough, and (2) whether the techniques work well across domains.", "labels": [], "entities": []}, {"text": "For example, the training method used in SPoT (Sentence Planner Trainable), as described in), was only shown to work in the travel domain, for the information gathering phase of the dialog, and with simple content plans involving no rhetorical relations.", "labels": [], "entities": []}, {"text": "This paper describes trainable sentence planning for information presentation in the MATCH (Multimodal Access To City Help) dialog system ).", "labels": [], "entities": [{"text": "information presentation", "start_pos": 53, "end_pos": 77, "type": "TASK", "confidence": 0.7751951813697815}, {"text": "MATCH (Multimodal Access To City Help) dialog system", "start_pos": 85, "end_pos": 137, "type": "DATASET", "confidence": 0.5271609097719192}]}, {"text": "We provide evidence that the trainable approach is feasible by showing (1) that the training technique used for SPoT can be extended to anew domain (restaurant information); (2) that this technique, previously used for informationgathering utterances, can be used for information presentations, namely recommendations and comparisons; and (3) that the quality of the output is comparable to that of a template-based generator previously developed and experimentally evaluated with MATCH users ( ).", "labels": [], "entities": [{"text": "SPoT", "start_pos": 112, "end_pos": 116, "type": "TASK", "confidence": 0.9321554899215698}]}, {"text": "Section 2 describes SPaRKy (Sentence Planning with Rhetorical Knowledge), an extension of SPoT that uses rhetorical relations.", "labels": [], "entities": [{"text": "Sentence Planning with Rhetorical Knowledge)", "start_pos": 28, "end_pos": 72, "type": "TASK", "confidence": 0.7673229326804479}]}, {"text": "SPaRKy consists of a randomized sentence plan generator (SPG) and a trainable sentence plan ranker (SPR); these are described in Sections 3 strategy:recommend items: Chanpen Thai relations:justify(nuc:1;sat:2); justify(nuc:1;sat:3); justify(nuc:1;sat:4) content: 1.", "labels": [], "entities": []}, {"text": "assert(has-att(Chanpen Thai, decor(decent))) 3.", "labels": [], "entities": []}, {"text": "assert(has-att(Chanpen Thai, service(good)) 4.", "labels": [], "entities": []}, {"text": "assert(has-att(Chanpen Thai, cuisine(Thai))) Figure 1: A content plan fora recommendation fora restaurant in midtown Manhattan strategy:compare3 items: Above, Carmine's relations:elaboration(1;2); elaboration(1;3); elaboration(1,4); elaboration(1,5); elaboration(1,6); elaboration(1,7); contrast(2;3); contrast(4;5); contrast(6;7) content: 1.", "labels": [], "entities": []}, {"text": "assert(exceptional(Above, Carmine's)) 2.", "labels": [], "entities": []}, {"text": "assert(has-att(Above, decor(good))) 3.", "labels": [], "entities": []}, {"text": "assert(has-att(Carmine's, decor(decent))) 4.", "labels": [], "entities": []}, {"text": "assert(has-att(Above, service(good))) 5.", "labels": [], "entities": []}, {"text": "assert(has-att(Carmine's, service(good))) 6.", "labels": [], "entities": []}, {"text": "assert(has-att(Above, cuisine(New American))) 7.", "labels": [], "entities": []}, {"text": "assert(has-att(Carmine's, cuisine(italian))) Figure 2: A content plan fora comparison between restaurants in midtown Manhattan and 4.", "labels": [], "entities": []}, {"text": "Section 5 presents the results of two experiments.", "labels": [], "entities": []}, {"text": "The first experiment shows that given a content plan such as that in, SPaRKy can select sentence plans that communicate the desired rhetorical relations, are significantly better than a randomly selected sentence plan, and are on average less than 10% worse than a sentence plan ranked highest by human judges.", "labels": [], "entities": []}, {"text": "The second experiment shows that the quality of SPaRKy's output is comparable to that of MATCH's template-based generator.", "labels": [], "entities": []}, {"text": "We sum up in Section 6.", "labels": [], "entities": []}], "datasetContent": [{"text": "We report two sets of experiments.", "labels": [], "entities": []}, {"text": "The first experiment tests the ability of the SPR to select a high quality sentence plan from a population of sentence plans randomly generated by the SPG.", "labels": [], "entities": []}, {"text": "Because the discriminatory power of the SPR is best tested by the largest possible population of sentence plans, we use 2-fold cross validation for this experiment.", "labels": [], "entities": [{"text": "SPR", "start_pos": 40, "end_pos": 43, "type": "TASK", "confidence": 0.9496585130691528}]}, {"text": "The second experiment compares SPaRKy to template-based generation.", "labels": [], "entities": []}, {"text": "Cross Validation Experiment: We repeatedly tested SPaRKy on the half of the corpus of 1756 sp-trees held out as test data for each fold.", "labels": [], "entities": []}, {"text": "The evaluation metric is the humanassigned score for the variant that was rated highest by SPaRKy for each text plan for each task/user combination.", "labels": [], "entities": []}, {"text": "We evaluated SPaRKy on the test sets by comparing three data points for each text plan: HUMAN (the score of the top-ranked sentence plan); SPARKY (the score of the SPR's selected sentence); and RANDOM (the score of a sentence plan randomly selected from the alternate sentence plans).", "labels": [], "entities": [{"text": "HUMAN", "start_pos": 88, "end_pos": 93, "type": "METRIC", "confidence": 0.9961373209953308}, {"text": "SPARKY", "start_pos": 139, "end_pos": 145, "type": "METRIC", "confidence": 0.987754762172699}, {"text": "RANDOM", "start_pos": 194, "end_pos": 200, "type": "METRIC", "confidence": 0.9759427905082703}]}, {"text": "We report results separately for comparisons between two entities and among three or more entities.", "labels": [], "entities": []}, {"text": "These two types of comparison are generated using different strategies in the SPG, and can produce text that is very different both in terms of length and structure.", "labels": [], "entities": [{"text": "SPG", "start_pos": 78, "end_pos": 81, "type": "TASK", "confidence": 0.83637934923172}]}, {"text": "summarizes the difference between SPaRKy, HUMAN and RANDOM for recommendations, comparisons between two entities and comparisons between three or more entities.", "labels": [], "entities": [{"text": "HUMAN", "start_pos": 42, "end_pos": 47, "type": "DATASET", "confidence": 0.6754373908042908}, {"text": "RANDOM", "start_pos": 52, "end_pos": 58, "type": "METRIC", "confidence": 0.6727205514907837}]}, {"text": "For all three presentation types, a paired t-test comparing SPaRKy to HUMAN to RAN-DOM showed that SPaRKy was significantly better than RANDOM (df = 59, p < .001) and significantly worse than HUMAN (df = 59, p < .001).", "labels": [], "entities": []}, {"text": "This demonstrates that the use of a trainable sentence planner can lead to sentence plans that are significantly better than baseline (RANDOM), with less human effort than programming templates.", "labels": [], "entities": [{"text": "RANDOM", "start_pos": 135, "end_pos": 141, "type": "METRIC", "confidence": 0.792792558670044}]}, {"text": "Comparison with template generation: For each content plan input to SPaRKy, the judges also rated the output of a templatebased generator for MATCH.", "labels": [], "entities": [{"text": "template generation", "start_pos": 16, "end_pos": 35, "type": "TASK", "confidence": 0.6873755604028702}, {"text": "MATCH", "start_pos": 142, "end_pos": 147, "type": "DATASET", "confidence": 0.7614941596984863}]}, {"text": "This templatebased generator performs text planning and sentence planning (the focus of the current paper), including some discourse cue insertion, clause combining and referring expression generation; the templates themselves are described in ( ).", "labels": [], "entities": [{"text": "text planning", "start_pos": 38, "end_pos": 51, "type": "TASK", "confidence": 0.7689653933048248}, {"text": "sentence planning", "start_pos": 56, "end_pos": 73, "type": "TASK", "confidence": 0.7494499385356903}, {"text": "discourse cue insertion", "start_pos": 123, "end_pos": 146, "type": "TASK", "confidence": 0.6480997105439504}, {"text": "clause combining", "start_pos": 148, "end_pos": 164, "type": "TASK", "confidence": 0.7227522134780884}, {"text": "referring expression generation", "start_pos": 169, "end_pos": 200, "type": "TASK", "confidence": 0.6306656499703726}]}, {"text": "Because the templates are highly tailored to this domain, this generator can be expected to perform well.", "labels": [], "entities": []}, {"text": "Example template-based and SPaRKy outputs fora comparison between three or more items are shown in. shows the mean HUMAN scores for the template-based sentence planning.", "labels": [], "entities": [{"text": "HUMAN", "start_pos": 115, "end_pos": 120, "type": "METRIC", "confidence": 0.9727720022201538}]}, {"text": "A paired t-test comparing HUMAN and template-based scores showed that HUMAN was significantly better than template-based sentence planning only for compare2 (df = 29, t = 6.2, p < .001).", "labels": [], "entities": []}, {"text": "The judges evidently did not like the template for comparisons between two items.", "labels": [], "entities": []}, {"text": "A paired t-test comparing SPaRKy and template-based sentence planning showed that template-based sentence planning was significantly better than SPaRKy only for recommendations (df = 29, t = 3.55, p < .01).", "labels": [], "entities": []}, {"text": "These results demonstrate that trainable sentence planning shows promise for producing output comparable to that of a template-based generator, with less programming effort and more flexibility.", "labels": [], "entities": []}, {"text": "The standard deviation for all three templatebased strategies was wider than for HUMAN or SPaRKy, indicating that there maybe content-specific aspects to the sentence planning done by SPaRKy that contribute to output variation.", "labels": [], "entities": []}, {"text": "The data show this to be correct; SPaRKy learned content-specific preferences about clause combining and discourse cue insertion that a template-based generator can- Figure 9: Comparisons between 3 or more items, H = Humans' score not easily model, but that a trainable sentence planner can.", "labels": [], "entities": [{"text": "clause combining", "start_pos": 84, "end_pos": 100, "type": "TASK", "confidence": 0.7212375700473785}, {"text": "discourse cue insertion", "start_pos": 105, "end_pos": 128, "type": "TASK", "confidence": 0.6597879429658254}]}, {"text": "For example, shows the nine rules generated on the first test fold which have the largest negative impact on the final RankBoost score (above the double line) and the largest positive impact on the final RankBoost score (below the double line), for comparisons between three or more entities.", "labels": [], "entities": []}, {"text": "The rule with the largest positive impact shows that SPaRKy learned to prefer that justifications involving price be merged with other information using a conjunction.", "labels": [], "entities": []}, {"text": "These rules are also specific to presentation type.", "labels": [], "entities": []}, {"text": "Averaging over both folds of the experiment, the number of unique features appearing in rules is 708, of which 66 appear in the rule sets for two presentation types and 9 appear in the rule sets for all three presentation types.", "labels": [], "entities": []}, {"text": "There are on average 214 rule features, 428 sentence features and 26 leaf features.", "labels": [], "entities": []}, {"text": "The majority of the features are ancestor features (319) followed by traversal features (264) and sister features (60).", "labels": [], "entities": []}, {"text": "The remainder of the features (67) are for specific lexemes.", "labels": [], "entities": []}, {"text": "To sum up, this experiment shows that the ability to model the interactions between domain content, task and presentation type is a strength of the trainable approach to sentence planning.", "labels": [], "entities": [{"text": "sentence planning", "start_pos": 170, "end_pos": 187, "type": "TASK", "confidence": 0.7283160984516144}]}], "tableCaptions": [{"text": " Table 1: Summary of Recommend, Compare2  and Compare3 results (N = 180)", "labels": [], "entities": [{"text": "Summary", "start_pos": 10, "end_pos": 17, "type": "METRIC", "confidence": 0.9178968667984009}]}, {"text": " Table 2: Summary of template-based genera- tion results. N = 180", "labels": [], "entities": []}, {"text": " Table 3: The nine rules generated on the first  test fold which have the largest negative impact  on the final RankBoost score (above the dou- ble line) and the largest positive impact on the  final RankBoost score (below the double line),  for Compare3. \u03b1 s represents the increment or  decrement associated with satisfying the condi- tion.", "labels": [], "entities": [{"text": "Compare3", "start_pos": 246, "end_pos": 254, "type": "DATASET", "confidence": 0.8510860800743103}]}]}