{"title": [{"text": "Modeling Semantic Relations Expressed by Prepositions", "labels": [], "entities": [{"text": "Modeling Semantic Relations Expressed by Prepositions", "start_pos": 0, "end_pos": 53, "type": "TASK", "confidence": 0.8228597044944763}]}], "abstractContent": [{"text": "This paper introduces the problem of predicting semantic relations expressed by prepositions and develops statistical learning models for predicting the relations, their arguments and the semantic types of the arguments.", "labels": [], "entities": [{"text": "predicting semantic relations expressed by prepositions", "start_pos": 37, "end_pos": 92, "type": "TASK", "confidence": 0.8720227976640066}]}, {"text": "We define an inventory of 32 relations, building on the word sense disambiguation task for prepositions and collapsing related senses across prepositions.", "labels": [], "entities": [{"text": "word sense disambiguation", "start_pos": 56, "end_pos": 81, "type": "TASK", "confidence": 0.6348096032937368}]}, {"text": "Given a preposition in a sentence, our computational task to jointly model the preposition relation and its arguments along with their semantic types, as away to support the relation prediction.", "labels": [], "entities": [{"text": "relation prediction", "start_pos": 174, "end_pos": 193, "type": "TASK", "confidence": 0.7304153740406036}]}, {"text": "The annotated data, however, only provides labels for the relation label, and not the arguments and types.", "labels": [], "entities": []}, {"text": "We address this by presenting two models for preposition relation labeling.", "labels": [], "entities": [{"text": "preposition relation labeling", "start_pos": 45, "end_pos": 74, "type": "TASK", "confidence": 0.7852558493614197}]}, {"text": "Our generalization of latent structure SVM gives close to 90% accuracy on relation labeling.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 62, "end_pos": 70, "type": "METRIC", "confidence": 0.9994513392448425}, {"text": "relation labeling", "start_pos": 74, "end_pos": 91, "type": "TASK", "confidence": 0.7563517093658447}]}, {"text": "Further, by jointly predicting the relation, arguments, and their types along with preposition sense, we show that we cannot only improve the relation accuracy, but also significantly improve sense prediction accuracy.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 151, "end_pos": 159, "type": "METRIC", "confidence": 0.9818214774131775}, {"text": "accuracy", "start_pos": 209, "end_pos": 217, "type": "METRIC", "confidence": 0.8462620973587036}]}], "introductionContent": [{"text": "This paper addresses the problem of predicting semantic relations conveyed by prepositions in text.", "labels": [], "entities": [{"text": "predicting semantic relations conveyed by prepositions in text", "start_pos": 36, "end_pos": 98, "type": "TASK", "confidence": 0.894656665623188}]}, {"text": "Prepositions express many semantic relations between their governor and object.", "labels": [], "entities": []}, {"text": "Predicting these can help advancing text understanding tasks like question answering and textual entailment.", "labels": [], "entities": [{"text": "text understanding", "start_pos": 36, "end_pos": 54, "type": "TASK", "confidence": 0.772356778383255}, {"text": "question answering", "start_pos": 66, "end_pos": 84, "type": "TASK", "confidence": 0.8807951211929321}, {"text": "textual entailment", "start_pos": 89, "end_pos": 107, "type": "TASK", "confidence": 0.7457639873027802}]}, {"text": "Consider the sentence: (1) The book of Prof. Alexander on primary school methods is a valuable teaching resource.", "labels": [], "entities": []}, {"text": "Here, the preposition on indicates that the book and primary school methods are connected by the relation Topic and of indicates the CreatorCreation relation between Prof. Alexander and the book.", "labels": [], "entities": []}, {"text": "Predicting these relations can help answer questions about the subject of the book and also recognize the entailment of sentences like Prof. Alexander has written about primary school methods.", "labels": [], "entities": []}, {"text": "Being highly polysemous, the same preposition can indicate different kinds of relations, depending on its governor and object.", "labels": [], "entities": []}, {"text": "Furthermore, several prepositions can indicate the same semantic relation.", "labels": [], "entities": []}, {"text": "For example, consider the sentence: (2) Poor care led to her death from pneumonia.", "labels": [], "entities": []}, {"text": "The preposition from in this sentence expresses the relation Cause(death, pneumonia).", "labels": [], "entities": [{"text": "Cause", "start_pos": 61, "end_pos": 66, "type": "METRIC", "confidence": 0.9695380330085754}]}, {"text": "Ina different context, it can denote other relations, as in the phrases copied from the film (Source) and recognized from the start (Temporal).", "labels": [], "entities": []}, {"text": "On the other hand, the relation Cause can be expressed by several prepositions; for example, the following phrases express a Cause relation: died of pneumonia and tired after the surgery.", "labels": [], "entities": []}, {"text": "We characterize semantic relations expressed by transitive prepositions and develop accurate models for predicting the relations, identifying their arguments and recognizing the semantic types of the arguments.", "labels": [], "entities": []}, {"text": "Building on the word sense disambiguation task for prepositions, we collapse semantically related senses across prepositions to derive our relation inventory.", "labels": [], "entities": [{"text": "word sense disambiguation", "start_pos": 16, "end_pos": 41, "type": "TASK", "confidence": 0.6783916155497233}]}, {"text": "These relations act as predicates in a predicate-argument representation, where the arguments are the governor and the object of the preposition.", "labels": [], "entities": []}, {"text": "While ascertaining the arguments is a largely syntactic decision, we point out that syntactic parsers do not always make this prediction correctly.", "labels": [], "entities": []}, {"text": "However, as illustrated in the examples above, identifying the relation depends on the governor and object of the preposition.", "labels": [], "entities": []}, {"text": "Given a sentence and a preposition, our goal is to model the predicate (i.e. the preposition relation) and its arguments (i.e. the governor and object).", "labels": [], "entities": []}, {"text": "Very often, the relation label is not influenced by the surface form of the arguments but rather by their semantic types.", "labels": [], "entities": []}, {"text": "In sentence (2) above, we want the predicate to be Cause when the object of the preposition is any illness.", "labels": [], "entities": []}, {"text": "We thus suggest to model the argument types along with the preposition relations and arguments, using different notions of types.", "labels": [], "entities": []}, {"text": "These three related aspects of the relation prediction task are further explained in Section 3 leading up to the problem definition.", "labels": [], "entities": [{"text": "relation prediction task", "start_pos": 35, "end_pos": 59, "type": "TASK", "confidence": 0.9352128903071085}]}, {"text": "Though we wish to predict relations, arguments and types, there is no corpus which annotates all three.", "labels": [], "entities": []}, {"text": "The SemEval 2007 shared task of word sense disambiguation for prepositions provides sense annotations for prepositions.", "labels": [], "entities": [{"text": "word sense disambiguation", "start_pos": 32, "end_pos": 57, "type": "TASK", "confidence": 0.6455732782681783}]}, {"text": "We use this data to generate training and test corpora for the relation labels.", "labels": [], "entities": []}, {"text": "In Section 4, we present two models for the prepositional relation identification problem.", "labels": [], "entities": [{"text": "prepositional relation identification problem", "start_pos": 44, "end_pos": 89, "type": "TASK", "confidence": 0.7725864797830582}]}, {"text": "The first model considers all possible argument candidates from various sources along with all argument types to predict the preposition relation label.", "labels": [], "entities": []}, {"text": "The second model treats the arguments and types as latent variables during learning using a generalization of the latent structural SVM of (.", "labels": [], "entities": []}, {"text": "We show in Section 5 that this model not only predicts the arguments and types, but also improves relation prediction performance.", "labels": [], "entities": [{"text": "relation prediction", "start_pos": 98, "end_pos": 117, "type": "TASK", "confidence": 0.885657399892807}]}, {"text": "The primary contributions of this paper are: 1.", "labels": [], "entities": []}, {"text": "We introduce anew inventory of preposition relations that covers the 34 prepositions that formed the basis of the SemEval 2007 task of preposition sense disambiguation.", "labels": [], "entities": [{"text": "SemEval 2007 task of preposition sense disambiguation", "start_pos": 114, "end_pos": 167, "type": "TASK", "confidence": 0.6810112042086465}]}, {"text": "2. We model preposition relations, arguments and their types jointly and propose a learning algorithm that learns to predict all three using training data that annotates only relation labels.", "labels": [], "entities": []}, {"text": "3. We show that jointly predicting relations with word sense not only improves the relation predictor, but also gives a significant improvement in sense prediction.", "labels": [], "entities": [{"text": "sense prediction", "start_pos": 147, "end_pos": 163, "type": "TASK", "confidence": 0.7225115448236465}]}], "datasetContent": [{"text": "The primary research goal of our experiments is to evaluate the different models (Model 1, Model 2 and joint relation-sense inference) for predicting preposition relations.", "labels": [], "entities": [{"text": "predicting preposition relations", "start_pos": 139, "end_pos": 171, "type": "TASK", "confidence": 0.8610261877377828}]}, {"text": "In additional analysis experiments, we also show that the definition of preposition relations indeed captures cross-preposition semantics by taking advantage of shared features and also highlight the need for going beyond the syntactic parser.", "labels": [], "entities": []}, {"text": "All our experiments are based on the SemEval 2007 data for preposition sense disambiguation () comprising word sense annotation over 16176 training and 8058 examples of prepositions labeled with their senses.", "labels": [], "entities": [{"text": "SemEval 2007 data", "start_pos": 37, "end_pos": 54, "type": "DATASET", "confidence": 0.7093262175718943}, {"text": "preposition sense disambiguation", "start_pos": 59, "end_pos": 91, "type": "TASK", "confidence": 0.7395523985226949}]}, {"text": "We pre-processed sentences with part-ofspeech tags using the Illinois POS tagger and dependency graphs using the parser of . For the experiments described below, we used the relation-annotated training set to train the models and evaluate accuracy of prediction on the test set.", "labels": [], "entities": [{"text": "Illinois POS tagger", "start_pos": 61, "end_pos": 80, "type": "DATASET", "confidence": 0.9320092598597208}, {"text": "accuracy", "start_pos": 239, "end_pos": 247, "type": "METRIC", "confidence": 0.9973468780517578}]}, {"text": "We chose the structural SVM parameter C using five-fold cross-validation on a 1000 random examples chosen from the training set.", "labels": [], "entities": []}, {"text": "For Model 2, we picked \u03b1 = 0.1 using a validation set consisting of a separate set of 1000 training examples.", "labels": [], "entities": []}, {"text": "We ran Algorithm 1 for 20 rounds.", "labels": [], "entities": []}, {"text": "Predicting the most frequent relation fora preposition gives an accuracy of 21.18%.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 64, "end_pos": 72, "type": "METRIC", "confidence": 0.9997227787971497}]}, {"text": "Even though the performance of the most-frequent relation label is poor, it does not represent the problem's difficulty and is not a good baseline.", "labels": [], "entities": []}, {"text": "To compare, for preposition senses, using features from the neighboring words, obtained an accuracy of 69.3%, and with features designed for the preposition sense task, () getup to 84.8% accuracy for the task.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 91, "end_pos": 99, "type": "METRIC", "confidence": 0.9994797110557556}, {"text": "accuracy", "start_pos": 187, "end_pos": 195, "type": "METRIC", "confidence": 0.9991040825843811}]}, {"text": "Our re-implementation of the latter system using a different set of pre-processing tools gets an accuracy of 83.53%.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 97, "end_pos": 105, "type": "METRIC", "confidence": 0.9997401833534241}]}, {"text": "For preposition relations, our baseline system for relation labeling uses the typed feature set, but without any type information.", "labels": [], "entities": [{"text": "relation labeling", "start_pos": 51, "end_pos": 68, "type": "TASK", "confidence": 0.7198385298252106}]}, {"text": "This produces an accuracy of 88.01% with Model 1 and 88.64% with Model 2.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 17, "end_pos": 25, "type": "METRIC", "confidence": 0.9997397065162659}]}, {"text": "We report statistical significance of results using our implementation of Dan Bikel's stratified-shuffling based statistical significance tester 7 .  Feature sharing across prepositions In our first analysis experiment, we seek to highlight the utility of sharing features between different prepositions.", "labels": [], "entities": []}, {"text": "To do so, we compare the performance of a system trained without shared features against the typeindependent system, which uses shared features.", "labels": [], "entities": []}, {"text": "To discount the influence of other factors, we use Model 1 in the typed setting without any types.", "labels": [], "entities": []}, {"text": "reports the accuracy of relation prediction for these two feature sets.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 12, "end_pos": 20, "type": "METRIC", "confidence": 0.9995941519737244}, {"text": "relation prediction", "start_pos": 24, "end_pos": 43, "type": "TASK", "confidence": 0.8363415598869324}]}, {"text": "We observed a similar improvement in performance even when type features are added or the setting is changed to typed+gen or with Model 2.", "labels": [], "entities": []}, {"text": "Setting Accuracy Independent 87.17 + Shared 88.01 Different argument candidate generators Our second ablation study looks at the effect of the various argument candidate generators.", "labels": [], "entities": []}, {"text": "Recall that in addition to the dependency governor and object, our models also use the previous word, the previous noun, adjective and verb as governor candidates and the next noun as object candidate.", "labels": [], "entities": []}, {"text": "We refer to the candidates generated by the parser as Parser only and the others as Heuristics only.", "labels": [], "entities": [{"text": "Parser", "start_pos": 54, "end_pos": 60, "type": "METRIC", "confidence": 0.875795304775238}]}, {"text": "compares the performance of these two argument candidate generators against the full set using Model 1 in both the typed and typed+gen settings.", "labels": [], "entities": []}, {"text": "We see that the heuristics give a better accuracy than the parser based system.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 41, "end_pos": 49, "type": "METRIC", "confidence": 0.9990323781967163}]}, {"text": "This is because the heuristics often contain the governor/object predicted by the dependency.", "labels": [], "entities": []}, {"text": "This is not always the case, though, because using all generators gives a slightly better performing system (not statistically significant).", "labels": [], "entities": []}, {"text": "In the overall system, we retain the dependency parser as one of the generators in order to capture long-range governor/object candidates that may not be in the set selected by the heuristics.", "labels": [], "entities": []}], "tableCaptions": [{"text": " Table 2: Identifying governor and object of prepositions  in the Penn Treebank data. Here, Best(Parser, Heuris- tics) reports the performance of an oracle that picks the  true governor and object, if present among the candidates  presented by the parser and the heuristic. This presents  an in-domain upper bound for governor and object detec- tion. See text for further details.", "labels": [], "entities": [{"text": "Penn Treebank data", "start_pos": 66, "end_pos": 84, "type": "DATASET", "confidence": 0.9961335857709249}]}, {"text": " Table 5: Main results: Accuracy of relation labeling.", "labels": [], "entities": [{"text": "Accuracy", "start_pos": 24, "end_pos": 32, "type": "METRIC", "confidence": 0.9887159466743469}, {"text": "relation labeling", "start_pos": 36, "end_pos": 53, "type": "TASK", "confidence": 0.7514220178127289}]}, {"text": " Table 8: The performance of different argument candi- date generators. We see that considering a larger set of  candidate generators gives a big accuracy improvement.", "labels": [], "entities": [{"text": "accuracy", "start_pos": 146, "end_pos": 154, "type": "METRIC", "confidence": 0.9987097978591919}]}]}