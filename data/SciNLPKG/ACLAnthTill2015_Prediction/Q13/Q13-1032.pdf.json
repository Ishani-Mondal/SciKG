{"title": [{"text": "Powergrading: a Clustering Approach to Amplify Human Effort for Short Answer Grading", "labels": [], "entities": [{"text": "Short Answer Grading", "start_pos": 64, "end_pos": 84, "type": "TASK", "confidence": 0.7856203317642212}]}], "abstractContent": [{"text": "We introduce anew approach to the machine assisted grading of short answer questions.", "labels": [], "entities": []}, {"text": "We follow past work in automated grading by first training a similarity metric between student responses, but then goon to use this metric to group responses into clusters and subclusters.", "labels": [], "entities": []}, {"text": "The resulting groupings allow teachers to grade multiple responses with a single action, provide rich feedback to groups of similar answers, and discover modalities of misunderstanding among students; we refer to this amplification of grader effort as \"powergrading.\"", "labels": [], "entities": []}, {"text": "We develop the means to further reduce teacher effort by automatically performing actions when an answer key is available.", "labels": [], "entities": []}, {"text": "We show results in terms of grading progress with a small \"budget\" of human actions, both from our method and an LDA-based approach, on a test corpus of 10 questions answered by 698 respondents.", "labels": [], "entities": []}], "introductionContent": [{"text": "Increasing access to quality education is a global issue, and one of the most exciting developments in recent years has been the introduction of MOOCsmassively online open courses, in which hundreds or thousands of students take a course online.", "labels": [], "entities": [{"text": "MOOCsmassively online open", "start_pos": 145, "end_pos": 171, "type": "TASK", "confidence": 0.6412074168523153}]}, {"text": "While this works wonderfully for lectures, assessment in the form of quizzes and exams presents some significant challenges.", "labels": [], "entities": []}, {"text": "One straightforward solution is to use multiple choice questions, but it is well known that there is far greater educational benefit from fill-in-the-blank and essay questions).", "labels": [], "entities": []}, {"text": "For many domains, then, there could be great value in using short answer questions; the problem is grading those answers without making the cost prohibitive.", "labels": [], "entities": []}, {"text": "Even in relatively small classrooms of a few dozen to a few hundred, the ability to grade such answers more efficiently would be a great boon to teachers.", "labels": [], "entities": []}, {"text": "One approach to addressing this is to automatically grade such answers as right or wrong or with a numerical score-there have been significant past efforts in this space (.", "labels": [], "entities": []}, {"text": "However, in practice this path has some significant obstacles.", "labels": [], "entities": []}, {"text": "The first is that while these approaches have made impressive progress, they are never 100% accurate-some number of misgraded answers are left on the table.", "labels": [], "entities": []}, {"text": "The second is that assigning a score is not really sufficient-in a small classroom, a teacher would give feedback as to why the answer is wrong; ideally she would be able to do this in the MOOC scenario as well.", "labels": [], "entities": [{"text": "MOOC", "start_pos": 189, "end_pos": 193, "type": "TASK", "confidence": 0.8514330387115479}]}, {"text": "A third problem, or at least a lost opportunity, is that there maybe consistent patterns of misunderstanding amongst students that go beyond the fraction getting a question right or wrong.", "labels": [], "entities": []}, {"text": "For instance many students might mistakenly believe that one of the rights afforded by the first amendment to the US constitution is the right to bear arms; a teacher would want to know this so that she could correct their misconception in class.", "labels": [], "entities": []}, {"text": "To address these issues, we propose looking at the problem in a different way.", "labels": [], "entities": []}, {"text": "Instead of trying to grade answers completely automatically, we attempt to leverage the abilities of both the human and the machine.", "labels": [], "entities": []}, {"text": "In particular, instead of classifying individual answers as being right or wrong, we propose to automatically find groupings and subgroupings of similar answers from a large set of answers to the same question, and let teachers apply their expertise to mark the groups.", "labels": [], "entities": []}, {"text": "We found that answers fora particular question often cluster into groups around different modes of understanding or misunderstanding.", "labels": [], "entities": []}, {"text": "Once identified, such groups would allow a teacher to quickly \"divide and conquer\" the grading task-she could mark entire groups as right or wrong, and give rich feedback to a whole group at once.", "labels": [], "entities": []}, {"text": "This has the additional benefit of increasing a grader's self-consistency, found to be a problem in past studies).", "labels": [], "entities": []}, {"text": "The groupings also allow the teacher to get an overview of the level of understanding of her students and the modes of misunderstanding.", "labels": [], "entities": []}, {"text": "In the absence of an answer key, we propose to form these groupings automatically without any model of the question or its answers; however, if a simple text answer key is available, we can use it to automatically mark some groups.", "labels": [], "entities": []}, {"text": "While attractive in principle, the powergrading approach of dividing and conquering contains many questions and challenges of its own, as various students will express the same answer in many different ways.", "labels": [], "entities": [{"text": "dividing and conquering", "start_pos": 60, "end_pos": 83, "type": "TASK", "confidence": 0.8644331296284994}]}, {"text": "Popular approaches to clustering text, such as using LDA to group answers by inferred topics, do their best to explain these variations in terms of distributions over words, but are limited by their word-based representations of text.", "labels": [], "entities": []}, {"text": "Ideally, we would like to learn how to group items together based on data, with an array of features that expand overtime as our technologies grow more mature.", "labels": [], "entities": []}, {"text": "We propose to model this distance function by training a classifier that predicts whether two answers should be grouped together, in the vein of past work which modeled the similarity between student answers and answer key entries ().", "labels": [], "entities": []}, {"text": "The notion of this distance function is subtle.", "labels": [], "entities": []}, {"text": "We want answers that are paraphrases of each other, such as \"the Congress\" and \"the houses of Cngress [sic]\" to be close, but \"the Senate and the House of Representatives\" to be different from these so we can mark out this more precise mode.", "labels": [], "entities": [{"text": "Cngress", "start_pos": 94, "end_pos": 101, "type": "DATASET", "confidence": 0.8256412148475647}]}, {"text": "Because we are modeling the distance between answers as opposed to the answers themselves, we can use \"between-item\" features that measure semantic or spelling differences.", "labels": [], "entities": []}, {"text": "We thus supply our classifier with the best available features that can account for misspellings, tenses, and other variations, with the hopes that we can add more sophisticated features in the future.", "labels": [], "entities": []}, {"text": "Finally, we would like to evaluate the benefit of our cluster-based approach.", "labels": [], "entities": []}, {"text": "In general, evaluating how helpful a given clustering is fora particular task can be difficult, but this scenario offers a very specific criteria-we examine how far a grader can get with a given amount of effort.", "labels": [], "entities": []}, {"text": "One take on this measure is \"grading on a budget,\" where we want to maximize the progress from a fixed number of actions; another is \"effort left for perfection,\" which is the number of additional user actions required to grade all items correctly.", "labels": [], "entities": [{"text": "perfection", "start_pos": 150, "end_pos": 160, "type": "METRIC", "confidence": 0.9706193804740906}]}, {"text": "Under these criteria, we find that using clusters formed via the learned similarity metric leads to substantially better results than using those formed via LDA or individually classifying items.", "labels": [], "entities": []}], "datasetContent": [], "tableCaptions": [{"text": " Table 1. Subset of questions used for evaluating our  method and data characteristics for the 698 responses.", "labels": [], "entities": []}, {"text": " Table 2. Differences in graders' judgments and inter-an- notator agreement (Fleiss' Kappa).", "labels": [], "entities": [{"text": "Differences", "start_pos": 10, "end_pos": 21, "type": "METRIC", "confidence": 0.8974946737289429}, {"text": "Fleiss' Kappa)", "start_pos": 77, "end_pos": 91, "type": "DATASET", "confidence": 0.7243602275848389}]}, {"text": " Table 3. Number of actions left (smaller is better) for each question after automatic actions and three manual  actions when an answer key exists, comparing various grading methods for each individual grader (G1-G3).", "labels": [], "entities": []}, {"text": " Table 4. Number of actions left for metric and LDA  clustering after three user actions when no answer key is  available.", "labels": [], "entities": [{"text": "LDA  clustering", "start_pos": 48, "end_pos": 63, "type": "TASK", "confidence": 0.6981819421052933}]}, {"text": " Table 5. Average number of actions left across settings  of the number of clusters/subclusters after 3 actions.  The setting used for our other results is shaded in grey.", "labels": [], "entities": []}]}