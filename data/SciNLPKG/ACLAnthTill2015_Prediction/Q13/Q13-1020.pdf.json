{"title": [{"text": "Unsupervised Tree Induction for Tree-based Translation", "labels": [], "entities": [{"text": "Unsupervised Tree Induction", "start_pos": 0, "end_pos": 27, "type": "TASK", "confidence": 0.5839837193489075}, {"text": "Tree-based Translation", "start_pos": 32, "end_pos": 54, "type": "TASK", "confidence": 0.7375659048557281}]}], "abstractContent": [{"text": "In current research, most tree-based translation models are built directly from parse trees.", "labels": [], "entities": []}, {"text": "In this study, we go in another direction and build a translation model with an unsupervised tree structure derived from a novel non-parametric Bayesian model.", "labels": [], "entities": []}, {"text": "In the model, we utilize synchronous tree substitution grammars (STSG) to capture the bilingual mapping between language pairs.", "labels": [], "entities": [{"text": "synchronous tree substitution grammars (STSG)", "start_pos": 25, "end_pos": 70, "type": "TASK", "confidence": 0.7189343401363918}]}, {"text": "To train the model efficiently, we develop a Gibbs sampler with three novel Gibbs operators.", "labels": [], "entities": []}, {"text": "The sampler is capable of exploring the infinite space of tree structures by performing local changes on the tree nodes.", "labels": [], "entities": []}, {"text": "Experimental results show that the string-to-tree translation system using our Bayesian tree structures significantly outperforms the strong baseline string-to-tree system using parse trees.", "labels": [], "entities": []}], "introductionContent": [{"text": "In recent years, tree-based translation models 1 are drawing more and more attention in the community of statistical machine translation (SMT).", "labels": [], "entities": [{"text": "statistical machine translation (SMT)", "start_pos": 105, "end_pos": 142, "type": "TASK", "confidence": 0.7888636638720831}]}, {"text": "Due to their remarkable ability to incorporate context structure information and long distance reordering into the translation process, tree-based translation models have shown promising progress in improving translation quality ().", "labels": [], "entities": []}, {"text": "However, tree-based translation models always suffer from two major challenges: 1) They are usually built directly from parse trees, which are generated by supervised linguistic parsers.", "labels": [], "entities": []}, {"text": "However, for many language pairs, it is difficult to acquire such corresponding linguistic parsers due to the lack of Tree-bank resources for training.", "labels": [], "entities": []}, {"text": "2) Parse trees are actually only used to model and explain the monolingual structure, rather than the bilingual mapping between language pairs.", "labels": [], "entities": []}, {"text": "This indicates that parse trees are usually not the optimal choice for training tree-based translation models (.", "labels": [], "entities": []}, {"text": "Based on the above analysis, we can conclude that the tree structure that is independent from Tree-bank resources and simultaneously considers the bilingual mapping inside the bilingual sentence pairs would be a good choice for building treebased translation models.", "labels": [], "entities": []}, {"text": "Therefore, complying with the above conditions, we propose an unsupervised tree structure for treebased translation models in this study.", "labels": [], "entities": []}, {"text": "In the structures, tree nodes are labeled by combining the word classes of their boundary words rather than by syntactic labels, such as NP, VP.", "labels": [], "entities": []}, {"text": "Furthermore, using these node labels, we design a generative Bayesian model to infer the final tree structure based on synchronous tree substitution grammars (STSG) 2 . STSG is derived from the word alignments and thus can grasp the bilingual mapping effectively.", "labels": [], "entities": []}, {"text": "Training the Bayesian model is difficult due to the exponential space of possible tree structures for each training instance.", "labels": [], "entities": []}, {"text": "We therefore develop an efficient Gibbs sampler with three novel Gibbs operators for training.", "labels": [], "entities": []}, {"text": "The sampler is capable of exploring the infinite space of tree structures by performing local changes on the tree nodes.", "labels": [], "entities": []}, {"text": "The tree structure formed in this way is independent from the Tree-bank resources and simultaneously exploits the bilingual mapping effectively.", "labels": [], "entities": []}, {"text": "Experiments show that the proposed unsupervised tree (U-tree) is more effective and reasonable for tree-based translation than the parse tree.", "labels": [], "entities": []}, {"text": "The main contributions of this study are as follows: 1) Instead of the parse tree, we propose a Bayesian model to induce a U-tree for treebased translation.", "labels": [], "entities": []}, {"text": "The U-tree exploits the bilingual mapping effectively and does not rely on any Tree-bank resources.", "labels": [], "entities": []}], "datasetContent": [{"text": "The experiments are conducted on Chinese-toEnglish translation.", "labels": [], "entities": [{"text": "Chinese-toEnglish translation", "start_pos": 33, "end_pos": 62, "type": "TASK", "confidence": 0.5838830769062042}]}, {"text": "The training data are the FBIS corpus with approximately 7.1 million Chinese words and 9.2 million English words.", "labels": [], "entities": [{"text": "FBIS corpus", "start_pos": 26, "end_pos": 37, "type": "DATASET", "confidence": 0.9459351897239685}]}, {"text": "We obtain the bidirectional word alignment with GIZA++, and then adopt the grow-diag-final-and strategy to obtain the final symmetric alignment.", "labels": [], "entities": []}, {"text": "We train a 5-gram language model on the Xinhua portion of the English Gigaword corpus and the English part of the training data.", "labels": [], "entities": [{"text": "English Gigaword corpus", "start_pos": 62, "end_pos": 85, "type": "DATASET", "confidence": 0.8597456614176432}]}, {"text": "For tuning and testing, we use the NIST MT 2003 evaluation data as the development set, and use the NIST MT04 and MT05 data as the test set.", "labels": [], "entities": [{"text": "NIST MT 2003 evaluation data", "start_pos": 35, "end_pos": 63, "type": "DATASET", "confidence": 0.8859547019004822}, {"text": "NIST", "start_pos": 100, "end_pos": 104, "type": "DATASET", "confidence": 0.9898748397827148}, {"text": "MT04", "start_pos": 105, "end_pos": 109, "type": "DATASET", "confidence": 0.5264511704444885}, {"text": "MT05 data", "start_pos": 114, "end_pos": 123, "type": "DATASET", "confidence": 0.8941291570663452}]}, {"text": "We use MERT) to tune parameters.", "labels": [], "entities": [{"text": "MERT", "start_pos": 7, "end_pos": 11, "type": "METRIC", "confidence": 0.8948360085487366}]}, {"text": "Since MERT is prone to search errors, we run MERT 5 times and select the best tuning parameters in the tuning set.", "labels": [], "entities": []}, {"text": "The translation quality is evaluated by case-insensitive BLEU-4 with the shortest length penalty.", "labels": [], "entities": [{"text": "translation", "start_pos": 4, "end_pos": 15, "type": "TASK", "confidence": 0.9512521028518677}, {"text": "BLEU-4", "start_pos": 57, "end_pos": 63, "type": "METRIC", "confidence": 0.9858444929122925}]}, {"text": "The statistical significance testis performed by the re-sampling approach.", "labels": [], "entities": []}, {"text": "To create the baseline system, we use the opensource Joshua 4.0 system ( to build a hierarchical phrase-based (HPB) system, and a syntax-augmented MT (SAMT) 11 system () respectively.", "labels": [], "entities": []}, {"text": "The translation system used for testing the effectiveness of our U-trees is our in-house stringto-tree system (abbreviated as s2t).", "labels": [], "entities": []}, {"text": "The system is implemented based on () and ( ).", "labels": [], "entities": []}, {"text": "In the system, we extract both the minimal GHKM rules (), and the rules of SPMT Model 1 () with phrases up to length L=5 on the source side.", "labels": [], "entities": []}, {"text": "We then obtain the composed rules by composing two or three adjacent minimal rules.", "labels": [], "entities": []}, {"text": "To build the above s2t system, we first use the parse tree, which is generated by parsing the English side of the bilingual data with the Berkeley parser ().", "labels": [], "entities": []}, {"text": "Then, we binarize the English parse trees using the head binarization approach ( and use the resulting binary parse trees to build another s2t system.", "labels": [], "entities": []}, {"text": "For the U-trees, we run the Gibbs sampler for 1000 iterations on the whole corpus.", "labels": [], "entities": []}, {"text": "The sampler uses 1,087s per iteration, on average, using a single core, 2.3 GHz Intel Xeon machine.", "labels": [], "entities": []}, {"text": "For the hyperparameters, we set \ud97b\udf59 to 0.1 and p expand = 1/3 to give a preference to the rules with small fragments.", "labels": [], "entities": []}, {"text": "We built an s2t translation system with the achieved U-trees after the 1000th iteration.", "labels": [], "entities": []}, {"text": "We only use one sample to extract the translation grammar because multiple samples would result in a grammar that would be too large.", "labels": [], "entities": []}, {"text": "From (Zollmann and Vogel, 2011), we find that the performance of SAMT system is similar with the method of labeling SCFG rules with POS tags.", "labels": [], "entities": []}, {"text": "Thus, to be convenient, we only conduct experiments with the SAMT system.", "labels": [], "entities": [{"text": "SAMT", "start_pos": 61, "end_pos": 65, "type": "TASK", "confidence": 0.7497749924659729}]}], "tableCaptions": [{"text": " Table 1. Results (in case-insensitive BLEU-4 scores) of  s2t systems using different types of trees. The \"*\" and  \"#\" denote that the results are significantly better than  the Joshua (SAMT) system and the s2t system using  parse trees (p<0.01).", "labels": [], "entities": [{"text": "BLEU-4", "start_pos": 39, "end_pos": 45, "type": "METRIC", "confidence": 0.986318826675415}]}, {"text": " Table  2. In the scenario with a large data, the string-to- tree system using our U-trees still significantly  outperforms the system using parse trees.", "labels": [], "entities": []}, {"text": " Table 2. Results (in case-insensitive BLEU-4 scores) for  the large training data. The meaning of \"*\" and \"#\" are  similar to Table 1.", "labels": [], "entities": [{"text": "BLEU-4", "start_pos": 39, "end_pos": 45, "type": "METRIC", "confidence": 0.9879311323165894}]}]}